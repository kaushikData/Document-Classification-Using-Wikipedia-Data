{"id": "2958224", "url": "https://en.wikipedia.org/wiki?curid=2958224", "title": "A Vindication of the Rights of Men", "text": "A Vindication of the Rights of Men\n\nA Vindication of the Rights of Men, in a Letter to the Right Honourable Edmund Burke; Occasioned by His Reflections on the Revolution in France (1790) is a political pamphlet, written by the 18th-century British liberal feminist Mary Wollstonecraft, which attacks aristocracy and advocates republicanism. Wollstonecraft's was the first response in a pamphlet war sparked by the publication of Edmund Burke's \"Reflections on the Revolution in France\" (1790), a defense of constitutional monarchy, aristocracy, and the Church of England.\n\nWollstonecraft attacked not only hereditary privilege, but also the rhetoric that Burke used to defend it. Most of Burke's detractors deplored what they viewed as his theatrical pity for Marie Antoinette, but Wollstonecraft was unique in her love of Burke's gendered language. By saying the \"sublime\" and the \"beautiful\", terms first established by Burke himself in \"A Philosophical Enquiry into the Origin of Our Ideas of the Sublime and Beautiful\" (1756), she kept his rhetoric as well as his argument. In her first unabashedly feminist critique, which Wollstonecraft scholar Claudia Johnson describes as unsurpassed in its argumentative force, Wollstonecraft indicts Burke's justification of an equal society founded on the passivity of women.\n\nIn her arguments for republican virtue, Wollstonecraft invokes an emerging middle-class ethos in opposition to what she views as the vice-ridden aristocratic code of manners. Driven by an Enlightenment belief in progress, she derides Burke for relying on tradition and custom. She describes an idyllic country life in which each family has a farm sufficient for its needs. Wollstonecraft contrasts her utopian picture of society, drawn with what she claims is genuine feeling, with Burke's false theatrical \"tableaux\".\n\nThe \"Rights of Men\" was successful: it was reviewed by every major periodical of the day and the first edition, published anonymously, sold out in three weeks. However, upon the publication of the second edition (the first to carry Wollstonecraft's name on the title page), the reviews began to evaluate the text not only as a political pamphlet but also as the work of a female writer. They contrasted Wollstonecraft's \"passion\" with Burke's \"reason\" and spoke condescendingly of the text and its female author. This remained the prevailing analysis of the \"Rights of Men\" until the 1970s, when feminist scholars revisited Wollstonecraft's texts and endeavoured to bring greater attention to their intellectualism.\n\n\"A Vindication of the Rights of Men\" was written against the backdrop of the French Revolution and the debates that it provoked in Britain. In a lively and sometimes vicious pamphlet war, now referred to as the \"Revolution Controversy\", which lasted from 1789 until the end of 1795, British political commentators argued over the validity of monarchy. One scholar has called this debate \"perhaps the last real discussion of the fundamentals of politics in [Britain]\". The power of popular agitation in revolutionary France, demonstrated in events such as the Tennis Court Oath and the storming of the Bastille in 1789, reinvigorated the British reform movement, which had been largely moribund for a decade. Efforts to reform the British electoral system and to distribute the seats in the House of Commons more equitably were revived.\n\nMuch of the vigorous political debate in the 1790s was sparked by the publication of Edmund Burke's \"Reflections on the Revolution in France\" in November 1790. Most commentators in Britain expected Burke to support the French revolutionaries, because he had previously been part of the liberal Whig party, a critic of monarchical power, a supporter of the American revolutionaries, and a prosecutor of government malfeasance in India. When he failed to do so, it shocked the populace and angered his friends and supporters. Burke's book, despite being priced at an expensive three shillings, sold an astonishing 30,000 copies in two years. Thomas Paine's famous response, \"The Rights of Man\" (1792), which became the rallying cry for thousands, however, greatly surpassed it, selling upwards of 200,000 copies.\n\nWollstonecraft's \"Rights of Men\" was published only weeks after Burke's \"Reflections\". While Burke supported aristocracy, monarchy, and the Established Church, liberals such as William Godwin, Paine, and Wollstonecraft, argued for republicanism, agrarian socialism, anarchy, and religious toleration. Most of those who came to be called radicals supported similar aims: individual liberties and civic virtue. They were also united in the same broad criticisms: opposition to the bellicose \"landed interest\" and its role in government corruption, and opposition to a monarchy and aristocracy who they believed were unlawfully seizing the people's power.\n\n1792 was the \"\"annus mirabilis\" of eighteenth-century radicalism\": its most important texts were published and the influence of radical associations, such as the London Corresponding Society (LCS) and the Society for Constitutional Information (SCI), was at its height. However, it was not until these middle- and working-class groups formed an alliance with the genteel Society of the Friends of the People that the government became concerned. After this alliance was formed, the conservative-dominated government prohibited seditious writings. Over 100 prosecutions for sedition took place in the 1790s alone, a dramatic increase from previous decades. The British government, fearing an uprising similar to the French Revolution, took even more drastic steps to quash the radicals: they made ever more political arrests and infiltrated radical groups; they threatened to \"revoke the licences of publicans who continued to host politicised debating societies and to carry reformist literature\"; they seized the mail of \"suspected dissidents\"; they supported groups that disrupted radical events; and they attacked dissidents in the press. Radicals saw this period, which included the 1794 Treason Trials, as \"the institution of a system of TERROR, almost as hideous in its features, almost as gigantic in its stature, and infinitely more pernicious in its tendency, than France ever knew.\"\n\nWhen, in October 1795, crowds threw refuse at George III and insulted him, demanding a cessation of the war with France and lower bread prices, Parliament immediately passed the \"gagging acts\" (the Seditious Meetings Act and the Treasonable Practices Act, also known as the \"Two Acts\"). Under these new laws, it was almost impossible to hold public meetings and speech was severely curtailed at those that were held. British radicalism was effectively muted during the later 1790s and 1800s. It was not until the next generation that any real reform could be enacted.\n\nPublished partially in response to Dissenting clergyman Richard Price's sermon celebrating the French revolution, Burke used the device of a mock-letter to a young Frenchman's plea for guidance in order to defend aristocratic government, paternalism, loyalty, chivalry, and primogeniture. He viewed the French Revolution as the violent overthrow of a legitimate government. In \"Reflections\", he argues that citizens do not have the right to revolt against their government, because civilizations, including governments, are the result of social and political consensus. If a culture's traditions were continually challenged, he contends, the result would be anarchy.\n\nBurke criticizes many British thinkers and writers who welcomed the early stages of the French Revolution. While the radicals likened the revolution to Britain's own Glorious Revolution in 1688, which had restricted the powers of the monarchy, Burke argues that the appropriate historical analogy was the English Civil War (1642–1651), in which Charles I had been executed in 1649. At the time Burke was writing, however, there had been very little revolutionary violence; more concerned with persuading his readers than informing them, he greatly exaggerated this element of the revolution in his text for rhetorical effect. In his \"Enquiry into the Sublime and Beautiful\", he had argued that \"large inexact notions convey ideas best\", and to generate fear in the reader, in \"Reflections\" he constructs the set-piece of Louis XVI and Marie Antoinette forced from their palace at sword point. When the violence actually escalated in France in 1793 with the Reign of Terror, Burke was viewed as a prophet.\n\nBurke also criticizes the learning associated with the French \"philosophes\"; he maintains that new ideas should not, in an imitation of the emerging discipline of science, be tested on society in an effort to improve it, but that populations should rely on custom and tradition to guide them.\n\nIn the advertisement printed at the beginning of the \"Rights of Men\", Wollstonecraft describes how and why she wrote it:\n\nSo the pamphlet could be published as soon as she finished writing it, Wollstonecraft wrote frantically while her publisher Joseph Johnson printed the pages. In fact, Godwin's \"Memoirs\" of Wollstonecraft tells that the sheets of manuscript were delivered to the press as they were written. Halfway through the work, however, she ceased writing. One biographer describes it as a \"loss of nerve\"; Godwin, in his \"Memoirs\", describes it as \"a temporary fit of torpor and indolence\". Johnson, perhaps canny enough at this point in their friendship to know how to encourage her, agreed to dispose of the book and told her not to worry about it. Ashamed, she rushed to finish.\n\nWollstonecraft's \"Rights of Men\" was published anonymously on 29 November 1790, the first of between fifty and seventy responses to Burke by various authors. Only three weeks later, on 18 December, a second edition, with her name printed on the title page, was issued. Wollstonecraft took time to edit the second edition, which, according to biographer Emily Sunstein, \"sharpened her personal attack on Burke\" and changed much of the text from first person to third person; \"she also added a non-partisan code criticising hypocritical liberals who talk equality but scrape before the powers that be.\"\nUntil the 1970s, the \"Rights of Men\" was typically considered disorganized, incoherent, illogical, and replete with \"ad hominem\" attacks (such as the suggestion that Burke would have promoted the crucifixion of Christ if he were a Jew). It had been touted as an example of \"feminine\" emotion tilting at \"masculine\" reason. However, since the 1970s scholars have challenged this view, arguing that Wollstonecraft employed 18th-century modes of writing, such as the digression, to great rhetorical effect. More importantly, as scholar Mitzi Myers argues, \"Wollstonecraft is virtually alone among those who answered Burke in eschewing a narrowly political approach for a wide-ranging critique of the foundation of the \"Reflections\".\" Wollstonecraft makes a primarily moral argument; her \"polemic is not a confutation of Burke's political theories, but an exposure of the cruel inequities which those theories presuppose.\" Wollstonecraft's style was also a deliberate choice, enabling her to respond to Burke's \"Enquiry into the Sublime and Beautiful\" as well as to \"Reflections\".\n\nThe style of the \"Rights of Men\" mirrors much of Burke's own text. It has no clear structure; like \"Reflections\", the text follows the mental associations made by the author as she was writing. Wollstonecraft's political treatise is written, like Burke's, in the form of a letter: his to C. J. F. DePont, a young Frenchman, and hers to Burke himself. Using the same form, metaphors, and style as Burke, she turns his own argument back on him. The \"Rights of Men\" is as much about language and argumentation as it is about political theory; in fact, Wollstonecraft claims that these are inseparable. She advocates, as one scholar writes, \"simplicity and honesty of expression, and argument employing reason rather than eloquence.\" At the beginning of the pamphlet, she appeals to Burke: \"Quitting now the flowers of rhetoric, let us, Sir, reason together.\"\n\nThe \"Rights of Men\" does not aim to present a fully articulated alternative political theory to Burke's, but instead to demonstrate the weaknesses and contradictions in his own argument. Therefore, much of the text is focused on Burke's logical inconsistencies, such as his support of the American revolution and the Regency Bill (which proposed restricting monarchical power during George III's madness in 1788), in contrast to his lack of support for the French revolutionaries. In criticism of Burke's contradictory support of the Regency Bill along with supporting the rule of monarchy in France, she writes:\n\nWollstonecraft's goal, she writes, is \"to shew you [Burke] to yourself, stripped of the gorgeous drapery in which you have enwrapped your tyrannic principles.\" However, she does also gesture towards a larger argument of her own, focusing on the inequalities faced by British citizens because of the class system. As Wollstonecraft scholar Barbara Taylor writes, \"treating Burke as a representative spokesman for old-regime despotism, Wollstonecraft champions the reformist initiatives of the new French government against his 'rusty, baneful opinions', and censures British political elites for their opulence, corruption, and inhumane treatment of the poor.\"\n\nWollstonecraft's attack on rank and hierarchy dominates the \"Rights of Men\". She chastises Burke for his contempt for the people, whom he dismisses as the \"swinish multitude\", and berates him for supporting the elite, most notably Marie Antoinette. In a famous passage, Burke had written: \"I had thought ten thousand swords must have leaped from their scabbards to avenge even a look that threatened her with insult.—But the age of chivalry is gone.\" Wollstonecraft's \"A Vindication of the Rights of Woman\" (1792) and \"An Historical and Moral View of the French Revolution\" (1794) extend the specific arguments made in the \"Rights of Men\" into larger social and political contexts.\n\nContrasting her middle-class values against Burke's aristocratic ones, Wollstonecraft contends that people should be judged on their merits rather than on their birthrights. As Wollstonecraft scholar Janet Todd writes, \"the vision of society revealed [in] \"A Vindication of the Rights of Men\" was one of talents, where entrepreneurial, unprivileged children could compete on equal terms with the now wrongly privileged.\" Wollstonecraft emphasizes the benefits of hard work, self-discipline, frugality, and morality, values she contrasts with the \"vices of the rich\", such as \"insincerity\" and the \"want of natural affections\". She endorses a commercial society that would help individuals discover their own potential as well as force them to realize their civic responsibilities. For her, commercialism would be the great equalizing force. However, several years later, in \"Letters Written in Sweden, Norway, and Denmark\" (1796), she would question the ultimate benefits of commercialism to society.\n\nWhile Dissenting clergyman Richard Price, whose sermon helped spark Burke's work, is the villain of \"Reflections\", he is the hero of the \"Rights of Men\". Both Wollstonecraft and Burke associate him with Enlightenment thinking, particularly the notion that civilization could progress through rational debate, but they interpret that stance differently. Burke believed such relentless questioning would lead to anarchy, while Wollstonecraft connected Price with \"reason, liberty, free discussion, mental superiority, the improving exercise of the mind, moral excellence, active benevolence, orientation toward the present and future, and the rejection of power and riches\"—quintessential middle-class professional values.\n\nWollstonecraft wields the English philosopher John Locke's definition of property (that is, ownership acquired through labour) against Burke's notion of inherited wealth. She contends that inheritance is one of the major impediments to the progress of European civilization, and repeatedly argues that Britain's problems are rooted in the inequity of property distribution. Although she did not advocate a totally equal distribution of wealth, she did desire one that was more equitable.\n\nThe \"Rights of Men\" indicts monarchy and hereditary distinctions and promotes a republican ideology. Relying on 17th- and early 18th-century notions of republicanism, Wollstonecraft maintains that virtue is at the core of citizenship. However, her notion of virtue is more individualistic and moralistic than traditional Commonwealth ideology. The goals of Wollstonecraft's republicanism are the happiness and prosperity of the individual, not the greatest good for the greatest number or the greatest benefits for the propertied. While she emphasizes the benefits that will accrue to the individual under republicanism, she also maintains that reform can only be effected at a societal level. This marks a change from her earlier texts, such as \"Original Stories from Real Life\" (1788), in which the individual plays the primary role in social reform.\n\nWollstonecraft's ideas of virtue revolved around the family, distinguishing her from other republicans such as Francis Hutcheson and William Godwin. For Wollstonecraft, virtue begins in the home: private virtues are the foundation for public virtues. Inspired by Jean-Jacques Rousseau's depictions of the ideal family and the republican Swiss canton, she draws a picture of idyllic family life in a small country village. One scholar describes her plan this way: \"vast estates would be divided into small farms, cottagers would be allowed to make enclosures from the commons and, instead of alms being given to the poor, they would be given the means to independence and self-advancement.\" Individuals would learn and practice virtue in the home, virtue that would not only make them self-sufficient, but also prompt them to feel responsible for the citizens of their society.\n\nOne of the central arguments of Wollstonecraft's \"Rights of Men\" is that rights should be conferred because they are reasonable and just, not because they are traditional. While Burke argued that civil society and government should rely on traditions which had accrued over centuries, Wollstonecraft contends that all civil agreements are subject to rational reassessment. Precedence, she maintains, is no reason to accept a law or a constitution. As one scholar puts it, \"Burke's belief in the antiquity of the British constitution and the impossibility of improvement upon a system that has been tried and tested through time is dismissed as nonsense. The past, for Wollstonecraft, is a scene of superstition, oppression, and ignorance.\" Wollstonecraft believed powerfully in the Enlightenment notion of progress, and rejected the contention that ancient ideas could not be improved upon. Using Burke's own architectural language, she asks, \"why was it a duty to repair an ancient castle, built in barbarous ages, of Gothic materials?\" She also notes, pointedly, that Burke's philosophy condones slavery:\nIn the \"Rights of Men\", Wollstonecraft not only endorses republicanism, but also a social contract based on sympathy and fellow-feeling. She describes the ideal society in these terms: individuals, supported by cohesive families, connect with others through rational sympathy. Strongly influenced by Price, whom she had met at Newington Green just a few years earlier, Wollstonecraft asserts that people should strive to imitate God by practicing universal benevolence.\n\nEmbracing a reasoned sensibility, Wollstonecraft contrasts her theory of civil society with Burke's, which she describes as full of pomp and circumstance and riddled with prejudice. She attacks what she perceives as Burke's false feeling, countering with her own genuine emotion. She argues that to be sympathetic to the French revolution (i.e., the people) is humane while to sympathize with the French clergy, as Burke does, is a mark of inhumanity. She accuses Burke not only of insincerity, but also of manipulation, claiming that his \"Reflections\" is propaganda. In one of the most dramatic moments of the \"Rights of Men\", Wollstonecraft claims to be moved beyond Burke's tears for Marie Antoinette and the monarchy of France to silence for the injustice suffered by slaves, a silence she represents with dashes meant to express feelings more authentic than Burke's:\n\nIn the \"Rights of Men\", Wollstonecraft challenges Burke's rhetoric as much as, or more, than his political theory. She begins by redefining the \"sublime\" and the \"beautiful\", terms he had established in his \"Enquiry into the Sublime and Beautiful\". While Burke associates the beautiful with weakness and femininity, and the sublime with strength and masculinity, Wollstonecraft writes, \"for truth, in morals, has ever appeared to me the essence of the sublime; and, in taste, simplicity the only criterion of the beautiful.\" With this sentence, she calls into question Burke's gendered definitions; convinced that they are harmful, she argues later in the \"Rights of Men\":\n\nAs Wollstonecraft scholar Claudia Johnson has written, \"As feminist critique, these passages have never really been surpassed.\" Burke, Wollstonecraft maintains, describes womanly virtue as weakness, thus leaving women no substantive roles in the public sphere and relegating them to uselessness.\n\nWollstonecraft applies this feminist critique to Burke's language throughout the \"Reflections\". As Johnson argues, \"her pamphlet as a whole refutes the Burkean axiom 'to make us love our country, our country ought to be lovely'\"; Wollstonecraft successfully challenges Burke's rhetoric of the beautiful with the rhetoric of the rational. She also demonstrates how Burke embodies the worst of his own ideas. He becomes the hysterical, illogical, feminine writer, and Wollstonecraft becomes the rational, masculine writer. Ironically, in order to effect this transposition, Wollstonecraft herself becomes passionate at times, for example, in her description of slavery (quoted above).\n\nThe \"Rights of Men\" was successful, its price contributing in no small measure: at one shilling and sixpence it was half the price of Burke's book. After the first edition sold out, Wollstonecraft agreed to have her name printed on the title page of the second. It was her first extensive work as \"a self-supporting professional and self-proclaimed intellectual\", as scholar Mary Poovey writes, and:\n\nCommentaries from the time note this; Horace Walpole, for example, called her a \"hyena in petticoats\" for attacking Marie Antoinette. William Godwin, her future husband, described the book as illogical and ungrammatical; in his \"Memoirs\" of Wollstonecraft, he dedicated only a paragraph to a discussion of the content of the work, calling it \"intemperate\".\n\nAll the major periodicals of the day reviewed the \"Rights of Men\". The \"Analytical Review\" agreed with Wollstonecraft's arguments and praised her \"lively and animated remarks\". The \"Monthly Review\" was also sympathetic, but it pointed out faults in her writing. The \"Critical Review\", the \"sworn foe\" of the \"Analytical Review\", however, wrote in December 1790, after discovering that the author was a woman:\n\nThe \"Gentleman's Magazine\" followed suit, criticizing the book's logic and \"its absurd presumption that men will be happier if free\", as well as Wollstonecraft's own presumption in writing on topics outside of her proper domain, commenting \"the \"rights of men\" asserted by a fair lady! The age of chivalry cannot be over, or the sexes have changed their ground.\" However, the \"Rights of Men\" put Wollstonecraft on the map as a writer; from this point forward in her career, she was well known.\n\nWollstonecraft sent a copy of the book to the historian Catharine Macaulay, whom she greatly admired. Macaulay wrote back that she was \"still more highly pleased that this publication which I have so greatly admired from its pathos & sentiment should have been written by a woman and thus to see my opinion of the powers and talents of the sex in your pen so early verified.\" William Roscoe, a Liverpool lawyer, writer, and patron of the arts, liked the book so much that he included Wollstonecraft in his satirical poem \"The Life, Death, and Wonderful Achievements of Edmund Burke\":\n\nWhile most of the early reviewers of the \"Rights of Men\", as well as most of Wollstonecraft's early biographers, criticized the work's emotionalism, and juxtaposed it with Burke's masterpiece of logic, there has been a recent re-evaluation of her text. Since the 1970s, critics who have looked more closely at both her work and Burke's, have come to the conclusion that they share many rhetorical similarities, and that the masculine/logic and feminine/emotion binaries are unsupportable. Most Wollstonecraft scholars now recognize it was this work that radicalized Wollstonecraft and directed her future writings, particularly \"A Vindication of the Rights of Woman\". It is not until after the halfway point of \"Rights of Men\" that she begins the dissection of Burke's gendered aesthetic; as Claudia Johnson contends, \"it seems that in the act of writing the later portions of \"Rights of Men\" she discovered the subject that would preoccupy her for the rest of her career.\"\n\nTwo years later, when Wollstonecraft published the \"Rights of Woman\", she extended many of the arguments she had begun in \"Rights of Men\". If all people should be judged on their merits, she wrote, women should be included in that group. In both texts, Wollstonecraft emphasizes that the virtue of the British nation is dependent on the virtue of its people. To a great extent, she collapses the distinction between private and public and demands that all educated citizens be offered the chance to participate in the public sphere.\n\n\n\n\n"}
{"id": "10140499", "url": "https://en.wikipedia.org/wiki?curid=10140499", "title": "Algorithm engineering", "text": "Algorithm engineering\n\nAlgorithm engineering focuses on the design, analysis, implementation, optimization, profiling and experimental evaluation of computer algorithms, bridging the gap between algorithm theory and practical applications of algorithms in software engineering.\nIt is a general methodology for algorithmic research.\n\nIn 1995, a report from an NSF-sponsored workshop \"with the purpose of assessing the current goals and directions of the Theory of Computing (TOC) community\" identified the slow speed of adoption of theoretical insights by practitioners as an important issue and suggested measures to\nBut also, promising algorithmic approaches have been neglected due to difficulties in mathematical analysis.\nThe term \"algorithm engineering\" was first used with specificity in 1997, with the first Workshop on Algorithm Engineering (WAE97), organized by Giuseppe F. Italiano.\n\nAlgorithm engineering does not intend to replace or compete with algorithm theory, but tries to enrich, refine and reinforce its formal approaches with experimental algorithmics (also called empirical algorithmics).\n\nThis way it can provide new insights into the efficiency and performance of algorithms in cases where\n\nSome researchers describe algorithm engineering's methodology as a cycle consisting of algorithm design, analysis, implementation and experimental evaluation, joined by further aspects like machine models or realistic inputs.\nThey argue that equating algorithm engineering with experimental algorithmics is too limited, because viewing design and analysis, implementation and experimentation as separate activities ignores the crucial feedback loop between those elements of algorithm engineering.\n\nWhile specific applications are outside the methodology of algorithm engineering, they play an important role in shaping realistic models of the problem and the underlying machine, and supply real inputs and other design parameters for experiments.\n\nCompared to algorithm theory, which usually focuses on the asymptotic behavior of algorithms, algorithm engineers need to keep further requirements in mind: Simplicity of the algorithm, implementability in programming languages on real hardware, and allowing code reuse.\nAdditionally, constant factors of algorithms have such a considerable impact on real-world inputs that sometimes an algorithm with worse asymptotic behavior performs better in practice due to lower constant factors.\n\nSome problems can be solved with heuristics and randomized algorithms in a simpler and more efficient fashion than with deterministic algorithms. Unfortunately, this makes even simple randomized algorithms \"difficult to analyze because there are subtle dependencies to be taken into account\".\n\nHuge semantic gaps between theoretical insights, formulated algorithms, programming languages and hardware pose a challenge to efficient implementations of even simple algorithms, because small implementation details can have rippling effects on execution behavior.\nThe only reliable way to compare several implementations of an algorithm is to spend an considerable amount of time on tuning and profiling, running those algorithms on multiple architectures, and looking at the generated machine code.\n\nSee: Experimental algorithmics\n\nImplementations of algorithms used for experiments differ in significant ways from code usable in applications.\nWhile the former prioritizes fast prototyping, performance and instrumentation for measurements during experiments, the latter requires \"thorough testing, maintainability, simplicity, and tuning for particular classes of inputs\".\n\nStable, well-tested algorithm libraries like LEDA play an important role in technology transfer by speeding up the adoption of new algorithms in applications. \nSuch libraries reduce the required investment and risk for practitioners, because it removes the burden of understanding and implementing the results of academic research.\n\nTwo main conferences on Algorithm Engineering are organized annually, namely:\n\nThe 1997 Workshop on Algorithm Engineering (WAE'97) was held in Venice (Italy) on September 11–13, 1997. The Third International Workshop on Algorithm Engineering (WAE'99) was held in London, UK in July 1999.\nThe first Workshop on Algorithm Engineering and Experimentation (ALENEX99) was held in Baltimore, Maryland on January 15–16, 1999. It was sponsored by DIMACS, the Center for Discrete Mathematics and Theoretical Computer Science (at Rutgers University), with additional support from SIGACT, the ACM Special Interest Group on Algorithms and Computation Theory, and SIAM, the Society for Industrial and Applied Mathematics.\n"}
{"id": "1643003", "url": "https://en.wikipedia.org/wiki?curid=1643003", "title": "Anticathexis", "text": "Anticathexis\n\nIn psychoanalysis, anticathexis, or countercathexis, is the energy used by the ego to bind the primitive impulses of the Id. Sometimes the ego follows the instructions of the superego in doing so; sometimes however it develops a double-countercathexis, so as to block feelings of guilt and anxiety deriving from the superego, as well as id impulses.\n\nFreud saw the establishment of a permanent anticathexis as a prerequisite for successful psychological repression. He also saw countercathexis as playing a central role in isolation.\n\nIn a late work, Freud further distinguished between the external anticathexis of repression and what he called “internal anticathexis\" (i.e. alteration of the ego through reaction formation).\n\nAnticathexis has also been linked to the phenomenon of figure-ground, in that it may entail the suppression of the margin or ground of a perceptual field.\n\n"}
{"id": "2007980", "url": "https://en.wikipedia.org/wiki?curid=2007980", "title": "Apology Project", "text": "Apology Project\n\nThe Apology Project, a 1980 conceptual art project, was created by Allan Bridge who employed the pseudonym Mr. Apology. Bridge used an answering machine to record confessions from anonymous callers. More than 1000 hours of confession were recorded, ranging from common confessions to ritualistic murders.\n\nSome of the confessions were published in Bridge's magazine \"Apology\".\n\nThis Apology Project is not to be confused with the Apology Project of Elliniko Teatro, a theatrical production based on Plato's The Apology of Socrates, featuring Yannis Simonides (see http://www.ellinikotheatro.org.)\n\n\n"}
{"id": "31362947", "url": "https://en.wikipedia.org/wiki?curid=31362947", "title": "Avoided burden", "text": "Avoided burden\n\nAvoided burden is an approach used in life-cycle assessment (LCA), especially in the context of allocating environmental burden in the presence of recycling or reuse. When determining the overall environmental impact of a product, the product is given \"credit\" for the potential recycled material included. For instance, PET bottles may be given environmental credit for the PET they contain, as this material will eventually be recycled back into further PET products. This credit is termed \"avoided burden\" because it refers to the impact of virgin material production that is avoided by the use of recycled material.\n\nThe extent to which recycled material displaces production of virgin material of a similar or different type is the subject of current research. In case studies, practitioners typically utilize heuristic, arbitrary rules, such as assigning 100%, 50%, or 0% displacement of virgin material by recycled material. However, accurate information on the displacement rates of various materials is largely unknown. This information is important to LCA practitioners, as it can often tip the balance between two compared alternative products or materials.\n\nAlthough the calculation of avoided burden is not without some degree of subjectivity, it is usually calculated as follows: \n"}
{"id": "758445", "url": "https://en.wikipedia.org/wiki?curid=758445", "title": "Baler", "text": "Baler\n\nA baler, most often called a hay baler is a piece of farm machinery used to compress a cut and raked crop (such as hay, cotton, flax straw, salt marsh hay, or silage) into compact bales that are easy to handle, transport, and store. Often, bales are configured to dry and preserve some intrinsic (e.g. the nutritional) value of the plants bundled. Several different types of balers are commonly used, each producing a different type of bale – rectangular or cylindrical, of various sizes, bound with twine, strapping, netting, or wire.\n\nIndustrial balers are also used in material recycling facilities, primarily for baling metal, plastic, or paper for transport.\n\nBefore the 19th century, hay was cut by hand and most typically stored in haystacks using hay forks to rake and gather the scythed grasses into optimal sized heaps — neither too large (promoting conditions that might create spontaneous combustion), nor too small, so much of the pile is susceptible to rotting. These haystacks lifted most of the plant fibers up off the ground, letting air in and water drain out, so the grasses could dry and cure, to retain nutrition for livestock feed at a later time. In the 1860s, mechanical cutting devices were developed; from these came the modern devices including mechanical mowers and balers. In 1872, a reaper that used a knotter device to bundle and bind hay was invented by Charles Withington; this was commercialized in 1874 by Cyrus McCormick. In 1936, Innes invented an automatic baler that tied bales with twine using Appleby-type knotters from a John Deere grain binder; In 1938 Edwin Nolt filed a patent for an improved version that was more reliable. \n\nThe most common type of baler in industrialized countries today is the round baler. It produces cylinder-shaped \"round\" or \"rolled\" bales. The design has a \"thatched roof\" effect that withstands weather well. Grass is rolled up inside the baler using rubberized belts, fixed rollers, or a combination of the two. When the bale reaches a predetermined size, either netting or twine is wrapped around it to hold its shape. The back of the baler swings open, and the bale is discharged. The bales are complete at this stage, but they may also be wrapped in plastic sheeting by a bale wrapper, either to keep hay dry when stored outside or convert damp grass into silage. Variable-chamber large round balers typically produce bales from in diameter and up to in width. The bales can weigh anywhere from , depending upon size, material, and moisture content. Common modern small round balers (also called \"mini round balers\" or \"roto-balers\") produce bales in diameter and in width, generally weighing from .\n\nOriginally conceived by Ummo Luebben circa 1910, the first round baler did not see production until 1947 when Allis-Chalmers introduced the Roto-Baler. Marketed for the water-shedding and light weight properties of its hay bales, AC had sold nearly 70,000 units by the end of production in 1960. The next major innovation began in 1965 when a graduate student at Iowa State University, Virgil Haverdink, sought out Wesley F. Buchele, a professor of Agricultural Engineering, seeking a research topic for a master thesis. Over the next year Buchele and Haverdink developed a new design for a large round baler, completed and tested in 1966, and thereafter dubbed the Buchele-Haverdink large round baler. The large round bales were about in diameter, long, and they weighed about after they dried—about 80 kg/m5 lb/ft. The design was promoted as a \"Whale of a Bale\" and Iowa State University now explains the innovative design as follows:\n\"Farmers were saved from the backbreaking chore of slinging hay bales in the 1960s, when Iowa State agricultural engineering professor Wesley Buchele and a group of student researchers invented a baler that produced large, round bales that could be moved by tractor. The baler has become the predominant forage-handling machine in the United States.\"\nIn the summer of 1969, the Australian Econ Fodder Roller baler came out, a design that made a ground-rolled bale. In September of that same year, The Hawkbilt Company of Vinton, Iowa, contacted Dr. Buchele about his design, then fabricated a large ground-rolling round baler which baled hay that had been laid out in a windrow, and began manufacturing large round balers in 1970. In 1972, Gary Vermeer of Pella, Iowa, designed and fabricated a round baler after the design of the A-C Roto-Baler, and the Vermeer Company began selling its model 605 - the first modern round baler. The Vermeer design used belts to compact hay into a cylindrical shape as is seen today. In the early 1980s, collaboration between Walterscheid and Vermeer produced the first effective uses of CV joints in balers, and later in other farm machinery. Due to the heavy torque required for such equipment, double Cardan joints are primarily used. Former Walterscheid engineer Martin Brown is credited with \"inventing\" this use for universal joints.\n\nBy 1975, fifteen American and Canadian companies were manufacturing large round balers.\n\nDue to the ability for round bales to roll away on a slope, they require specific treatment for safe transport and handling. Small round bales can typically be moved by hand or with lower-powered equipment. Large round bales, due to their size and weight (they can weigh a ton or more) require special transport and moving equipment.\n\nThe most important tool for large round bale handling is the bale spear or spike, which is usually mounted on the back of a tractor or the front of a skid-steer. It is inserted into the approximate center of the round bale, then lifted and the bale is hauled away. Once at the destination, the bale is set down, and the spear pulled out. Careful placement of the spear in the center is needed or the bale can spin around and touch the ground while in transport, causing a loss of control. When used for wrapped bales that are to be stored further, the spear makes a hole in the wrapping that must be sealed with plastic tape to maintain a hermetic seal.\n\nAlternatively, a grapple fork may be used to lift and transport large round bales. The grapple fork is a hydraulically driven implement attached to the end of a tractor's bucket loader. When the hydraulic cylinder is extended, the fork clamps downward toward the bucket, much like a closing hand. To move a large round bale, the tractor approaches the bale from the side and places the bucket underneath the bale. The fork is then clamped down across the top of the bale, and the bucket is lifted with the bale in tow. Grab hooks installed on the bucket of a tractor are another tool used to handle round bales, and be done by a farmer with welding skills by welding two hooks and a heavy chain to the outside top of a tractor front loader bucket.\n\nThe rounded surface of round bales poses a challenge for long-haul, flat-bed transport, as they could roll off of the flat surface if not properly supported. This is particularly the case with large round bales; their size makes them difficult to flip, so it may not be feasible to flip many of them onto the flat surface for transport and then re-position them on the round surface at the destination. One option that works with both large and small round bales is to equip the flat-bed trailer with guard-rails at either end, which prevent bales from rolling either forward or backward. Another solution is the saddle wagon, which has closely spaced rounded saddles or support posts in which round bales sit. The tall sides of each saddle prevent the bales from rolling around while on the wagon, as the bale settles down in between posts. On 3 September 2010, on the A381 in Halwell near Totnes, Devon, UK an early member of British rock group ELO Mike Edwards was killed when his van was crushed by a large round bale. The cellist, 62, died instantly when the bale fell from a tractor on nearby farmland before rolling onto the road and crushing his van.\n\nA large round bale can be directly used for feeding animals by placing it in a feeding area, tipping it over, removing the bale wrap, and placing a protective ring (a \"ring feeder\") around the outside so that animals don't walk on hay that has been peeled off the outer perimeter of the bale. The round baler's rotational forming and compaction process also enables both large and small round bales to be fed out by unrolling the bale, leaving a continuous flat strip in the field or behind a feeding barrier.\n\nA recent innovation in hay storage has been the development of the silage or haylage bale, which is a high-moisture bale wrapped in plastic film. These are baled much wetter than hay bales, and are usually smaller than hay bales because the greater moisture content makes them heavier and harder to handle. These bales begin to ferment almost immediately, and the metal bale spear stabbed into the core becomes very warm to the touch from the fermentation process.\n\nSilage or haylage bales may be wrapped by placing them on a rotating bale spear mounted on the rear of a tractor. As the bale spins, a layer of plastic cling film is applied to the exterior of the bale. This roll of plastic is mounted in a sliding shuttle on a steel arm and can move parallel to the bale axis, so the operator does not need to hold up the heavy roll of plastic. The plastic layer extends over the ends of the bale to form a ring of plastic approximately wide on the ends, with hay exposed in the center.\n\nTo stretch the cling-wrap plastic tightly over the bale, the tension is actively adjusted with a knob on the end of the roll, which squeezes the ends of the roll in the shuttle. In this example wrapping video, the operator is attempting to use high tension to get a flat, smooth seal on the right end. However, the tension increases too much and the plastic tears off. The operator recovers by quickly loosening the tension and allows the plastic to feed out halfway around the bale before reapplying the tension to the sheeting.\n\nThese bales are placed in a long continuous row, with each wrapped bale pressed firmly against all the other bales in the row before being set down onto the ground. The plastic wrap on the ends of each bale sticks together to seal out air and moisture, protecting the silage from the elements. The end-bales are hand-sealed with strips of cling plastic across the opening.\n\nThe airtight seal between each bale permits the row of round bales to ferment as if they were in a silo bag, but they are easier to handle than a silo bag, as they are more robust and compact. The plastic usage is relatively high, and there is no way to reuse the silage-contaminated plastic sheeting, although it can be recycled or used as a fuel source via incineration. The wrapping cost is approximately US$5 per bale.\n\nAn alternative form of wrapping uses the same type of bale placed on a bale wrapper, consisting of pair of rollers on a turntable mounted on the three-point linkage of a tractor. It is then spun about two axes while being wrapped in several layers of cling-wrap plastic film. This covers the ends and sides of the bale in one operation, thus sealing it separately from other bales. The bales are then moved or stacked using a special pincer attachment on the front loader of a tractor, which does not damage the film seal. They can also be moved using a standard bale spike, but this punctures the airtight seal, and the hole in the film must be repaired after each move.\n\nPlastic-wrapped bales must be unwrapped before being fed to livestock to prevent accidental ingestion of the plastic. Like round hay bales, silage bales are usually fed using a \"ring feeder\".\nIn 1978, Hesston introduced the first \"large square baler,\" capable of compacting hay into more easily transported large square bales that could be stacked and tarped in the field (to protect them from rain) or loaded on trucks or containers for trucking or export. Depending upon the baler, these bales can weigh from 1000 pounds to 2200 pounds for a 3'x3'x9' or 3'x4'x9' bale (versus 900 pounds for a 3'x4' round bale). As the pickup revolves just above the ground surface, the tines pick up and feed the hay into the flake forming chamber, where a \"flake\" of hay is formed before being pushed up into the path of the plunger, which then compresses it with great force (200 to over 750 kilonewtons, depending on model) against the existing bale in the chamber. Once the desired length is achieved, the knotter arm is mechanically tripped to begin the knotting cycle in which several knotters (4-6 is common) tie the 4-6 strings that maintain the bale's shape.\n\nIn the prairies of Canada, the large rectangular balers are also called \"prairie raptors\".\n\nRectangular bales are easier to transport than round bales, since there is little risk of the bale rolling off the back of a flatbed trailer. The rectangular shape also saves space and allows a complete solid slab of hay to be stacked for transport and storage. Most balers allow adjustment of length and it is common to produce bales of twice the width, allowing stacks with brick-like alternating groups overlapping the row below at right angles, creating a strong structure.\n\nThey are well-suited for large-scale livestock feedlot operations, where many tons of feed are rationed every hour. Most often, they are baled small enough that one person can carry or toss them where needed.\n\nDue to the huge rectangular shape, large spear forks, or squeeze grips are mounted to heavy lifting machinery, such as large fork lifts, tractors equipped with front end loaders, telehandlers, hay squeezes or wheel loaders, to lift these bales.\n\nA type of baler that produces small rectangular (often called \"square\") bales was once the most prevalent form of baler, but is less common today. It is primarily used on small acreages where large equipment is impractical, and also for the production of hay for small operations, particularly horse owners who may not have access to the specialized feeding machinery used for larger bales. Each bale is about . The bales are usually wrapped with two, but sometimes three, or more strands of knotted twine. The bales are light enough for one person to handle, about , depending upon the crop and pressure applied (can be 100 lbs for a 16\"x18\" 2-string bale). Many balers have adjustable bale chamber pressure and bale length, so shorter, less-dense bales can be produced for ease of handling.\n\nTo form the bale, the material to be baled (which is often hay or straw) in the windrow is lifted by tines in the baler's \"reel\". This material is then \"packed\" into the bale chamber, which runs the length of one side of the baler (normally the right hand side when viewed from the front) in offset balers. Balers like Hesston models use an in-line system where the hay goes straight through from the pickup to the flake chamber to the plunger and bale-forming chamber. A combination plunger and knife move back and forth in the front of this chamber, with the knife closing the door into the bale chamber as it moves backwards. The plunger and knife are attached to a heavy asymmetrical flywheel to provide extra force as they pack the bales. A measuring device—normally a spiked wheel that is turned by the emerging bales—measures the amount of material that is being compressed and, at the appropriate length it triggers the \"knotters\" that wrap the twine around the bale and tie it off. As the next bale is formed the tied one is driven out of the rear of the baling chamber, where it can either drop to the ground, or sent to a wagon towed behind the baler. When a wagon is used, the bale may be lifted by hand from the chamber by a worker on the wagon who stacks the bales on the wagon, or the bale may be propelled into the wagon by a mechanism on the baler, commonly either a \"thrower\" (parallel high-speed drive belts which throw the bale into the wagon) or a \"kicker\" (mechanical arm which throws the bale into the wagon). In the case of a thrower or kicker, the wagon has high walls on the left, right, and back sides, and a short wall on the front side, to contain the randomly piled bales. This process continues as long as there is material to be baled, and twine to tie it with.\n\nThis form of bale is not much used in large-scale commercial agriculture, because of the costs involved in handling many small bales. However, it enjoys some popularity in small-scale, low-mechanization agriculture and horse-keeping. Besides using simpler machinery and being easy to handle, these small bales can also be used for insulation and building materials in straw-bale construction. Square bales may generally weather better than round bales because a more much dense stack can be put up. However, they don't shed water as round bales do. Convenience is also a major factor in farmers deciding to continue putting up square bales, as they make feeding and bedding in confined areas (stables, barns, etc.) much easier.\n\nMany of these older balers are still to be found on farms today, particularly in dry areas, where bales can be left outside for long periods.\n\nThe automatic-baler for small square bales took on most of its present form in 1938 with the first such baler sold as Arthur S. Young's Automation Baler. It was manufactured in small numbers until acquired by \"New Holland Ag.\"\n\nIn Europe, in as early as 1939, both Claas of Germany and Rousseau SA of France had automatic twine tying pick-up balers. Most of these produced low density bales though. The first successful pick-up balers were made by the Ann Arbor Company in 1929. Ann Arbor was acquired by the Oliver Farm Equipment Company in 1943. Despite their head start on the rest of the field, no Ann Arbor balers carried automatic knotters or twisters and Oliver didn't produce its own automatic tying baler until 1949.\n\nPrior to 1937 the hay press was the common name of the stationary baling implement, powered with a tractor or stationary engine using a belt on a belt pulley, with the hay being brought to the baler and fed in by hand. Later, balers were made mobile, with a 'pickup' to gather up the hay and feed it into the chamber. These often used air cooled gasoline engines mounted on the baler for power. The biggest change to this type of baler since 1940 is being powered by the tractor through its power take-off (PTO), instead of by a built-in internal combustion engine.\n\nIn present-day production, small square balers can be ordered with twine knotters or wire tie knotters.\n\nNot all stationary wire tying balers used 2 wires. It was not uncommon for the larger bale size (usually 17\" x 22\") machines to use 'boards' that had three slots for wires and hence tied three wires per bale. Most North American manufacturers produced these machines as either regular models or as size options. 'Small square' three wire tying pick-up balers were available from the early 1930s, principally from J. I. Case & Co. and Ann Arbor. These machines were hand tying and hand threading machines.\n\nIn the 1940s most farmers would bale hay in the field with a small tractor with 20 or less horsepower, and the tied bales would be dropped onto the ground as the baler moved through the field. Another team of workers with horses and a flatbed wagon would come by and use a sharp metal hook to grab the bale and throw it up onto the wagon while an assistant stacks the bale, for transport to the barn.\n\nA later time-saving innovation was to tow the flatbed wagon directly behind the baler, and the bale would be pushed up a ramp to a waiting attendant on the wagon. The attendant hooks the bale off the ramp and stacks it on the wagon, while waiting for the next bale to be produced.\n\nEventually, as tractor horsepower increased, the thrower-baler became possible, which eliminated the need for someone to stand on the wagon and pick up the finished bales. The first thrower mechanism used two fast-moving friction belts to grab finished bales and throw them at an angle up in the air onto the bale wagon. The bale wagon was modified from a flatbed into a three-sided skeleton frame open at the front, to act as a catcher's net for the thrown bales.\n\nAs tractor horsepower further increased, the next innovation of the thrower-baler was the hydraulic tossing baler. This employs a flat pan behind the bale knotter. As bales advance out the back of the baler, they are pushed onto the pan one at a time. When the bale has moved fully onto the pan, the pan suddenly pops up, pushed by a large hydraulic cylinder, and tosses the bale up into the wagon like a catapult.\n\nThe pan-thrower method puts much less stress on the bales compared to the belt-thrower. The friction belts of the belt-thrower stress the twine and knots as they grip the bale, and would occasionally cause bales to break apart in the thrower or when the bales landed in the wagon.\n\nBales may be picked up from the field and stacked using a self-powered machine called a \"bale stacker\", \"bale wagon\" or \"harobed\". There are several designs and sizes. One type picks up square bales are dropped by the baler with the strings facing upward. The stacker will drive up to each bale, pick it up and set it on a three-bale-wide table (the strings are now facing upwards). Once three bales are on the table, the table lifts up and back, causing the three bales to face strings to the side again; this happens three more times until there are 16 bales on the main table. This table will lift like the smaller one, and the bales will be up against a vertical table. The machine will hold 160 bales (ten tiers); usually there will be cross-tiers near the center to keep the stack from swaying or collapsing if any weight is applied to the top of the stack. The full load will be transported to a barn; the whole rear of the stacker will tilt upwards until it is vertical. There will be two pushers that will extend through the machine and hold the bottom of the stack from being pulled out from the stacker while it is driven out of the barn.\n\nIn Britain (if small square bales are still to be used), they are usually collected as they fall out of the baler in a \"bale sledge\" dragged behind the baler. This has four channels, controlled by automatic mechanical balances, catches and springs, which sort each bale into its place in a square \"eight\". When the sledge is full, a catch is tripped automatically, and a door at the rear opens to leave the eight lying neatly together on the ground. These may be picked up individually and loaded by hand, or they may be picked up all eight together by a \"bale grab\" on a tractor, a special front loader consisting of many hydraulically powered downward-pointing curved spikes. The square eight will then be stacked, either on a trailer for transport, or in a roughly cubic field stack eight or ten layers high. This cube may then be transported by a large machine attached to the three-point hitch behind a tractor, which clamps the sides of the cube and lifts it bodily.\n\nBefore electrification occurred in rural parts of the United States in the 1940s, some small dairy farms would have tractors but not electric power. Often just one neighbor who could afford a tractor would do all the baling for surrounding farmers still using horses.\n\nTo get the bales up into the hayloft, a pulley system ran on a track along the peak of the barn's hayloft. This track also stuck a few feet out the end of the loft, with a large access door under the track. On the bottom of the pulley system was a bale spear, which is pointed on the end and has retractable retention spikes.\n\nA flatbed wagon would pull up next to the barn underneath the end of the track, the spear lowered down to the wagon, and speared into a single bale. The pulley rope would be used to manually lift the bale up until it could enter the mow through the door, then moved along the track into the barn and finally released for manual stacking in tight rows across the floor of the loft. As the stack filled the loft, the bales would be lifted higher and higher with the pulleys until the hay was stacked all the way up to the peak.\n\nWhen electricity arrived, the bale spear, pulley and track system were replaced by long motorized bale conveyors known as hay elevators. A typical elevator is an open skeletal frame, with a chain that has dull spikes every few feet along the chain to grab bales and drag them along. One elevator replaced the spear track and ran the entire length of the peak of the barn. A second elevator was either installed at a 30-degree slope on the side of the barn to lift bales up to the peak elevator, or used dual front-back chains surrounding the bale to lift bales straight up the side of the barn to the peak elevator.\n\nA bale wagon pulled up next to the lifting elevator, and a farm worker placed bales one at a time onto the angled track. Once bales arrived at the peak elevator, adjustable tipping gates along the length of the peak elevator were opened by pulling a cable from the floor of the hayloft, so that bales tipped off the elevator and dropped down to the floor in different areas of the loft. This permitted a single elevator to transport hay to one part of a loft and straw to another part.\n\nThis complete hay elevator lifting, transport, and dropping system reduced bale storage labor to a single person, who simply pulls up with a wagon, turns on the elevators and starts placing bales on it, occasionally checking to make sure that bales are falling in the right locations in the loft.\n\nThe neat stacking of bales in the loft is often sacrificed for the speed of just letting them fall and roll down the growing pile in the loft, and changing the elevator gates to fill in open areas around the loose pile. But if desired, the loose bale pile dropped by the elevator could be rearranged into orderly rows between wagon loads.\n\nThe process of retrieving bales from a hayloft has stayed relatively unchanged from the beginning of baling. Typically workers were sent up into the loft, to climb up onto the bale stack, pull bales off the stack, and throw or roll them down the stack to the open floor of the loft. Once the bale is down on the floor, workers climb down the stack, open a cover over a bale chute in the floor of the loft, and push the bales down the chute to the livestock area of the barn.\n\nMost barns were equipped with several chutes along the sides and in the center of the loft floor. This permitted bales to be dropped into the area where they were to be used. Hay bales would be dropped through side chutes, to be broken up and fed to the cattle. Straw bales would be dropped down the center chute, to be distributed as bedding in the livestock standing/resting areas.\n\nTraditionally multiple bales were dropped down to the livestock floor and the twine removed by hand. After drying and being stored under tons of pressure in the haystack, most bales are tightly compacted and need to be torn apart and fluffed up for use.\n\nOne recent method of speeding up all this manual bale handling is the bale shredder, which is a large vertical drum with rotary cutting/ripping teeth at the base of the drum. The shredder is placed under the chute and several bales dropped in. A worker then pushes the shredder along the barn aisle as it rips up a bale and spews it out in a continuous fluffy stream of material.\n\nIndustrial balers are typically used to compact similar types of waste, such as office paper, Corrugated fiberboard, plastic, foil and cans, for sale to recycling companies. These balers are made of steel with a hydraulic ram to compress the material loaded. Some balers are simple and labor-intensive, but are suitable for smaller volumes. Other balers are very complex and automated, and are used where large quantities of waste are handled.\n\nUsed in recycling facilities, balers are a packaging step that allows for the aforementioned commodities to be broken down into dense cubes of one type of material at a time. There are different balers used depending on the material type. After a specific material is crushed down into a dense cube, it is tied to a bale by a thick wire and then pushed out of the machine. This process allows for easy transport of all materials involved.\n\nTwo-ram baler: A two-ram baler is a baling machine that contains two cylinders and is able to bundle and package most commodities except for cardboard and clear film. This baler is known for its durability and is able to take in more bulky material.\n\nSingle-ram baler: A single-ram baler is a baling machine that contains one cylinder. Because this baler is relatively smaller than the two-ram baler, it is best for small and medium commodities.\n\nClosed door baler: This baler bales clear plastic film.\n\nAmerican baler: This baler bales corrugated materials.\n\n\n"}
{"id": "2679447", "url": "https://en.wikipedia.org/wiki?curid=2679447", "title": "Bel decomposition", "text": "Bel decomposition\n\nIn semi-Riemannian geometry, the Bel decomposition, taken with respect to a specific timelike congruence, is a way of breaking up the Riemann tensor of a pseudo-Riemannian manifold into lower order tensors with properties similar to the electric field and magnetic field. Such a decomposition was partially described by Alphonse Matte in 1953 and by Lluis Bel in 1958.\n\nThis decomposition is particularly important in general relativity. This is the case of four-dimensional Lorentzian manifolds, for which there are only three pieces with simple properties and individual physical interpretations.\n\nIn four dimensions the Bel decomposition of the Riemann tensor, with respect to a timelike unit vector field formula_1, not necessarily geodesic or hypersurface orthogonal, consists of three pieces:"}
{"id": "1870284", "url": "https://en.wikipedia.org/wiki?curid=1870284", "title": "CPN-AMI", "text": "CPN-AMI\n\nCPN-AMI is a computer-aided software engineering environment based on Petri Net specifications. It provides the ability to specify the behavior of a distributed system—and to evaluate properties such as invariants (preservation of resources), absence of deadlocks, liveness, or temporal logic properties (relations between events in the system).\n\nCPN-AMI relies on AMI-Nets, that are well-formed Petri nets with syntactic facilities. Well Formed Petri nets were jointly elaborated between the University of Paris 6 (Université P. & M. Curie) and the University of Torino in the early 1990s. This Petri net class supports symbolic techniques for model checking, and thus provides a very compressed way to store all states of a system.\n\n\n"}
{"id": "8202685", "url": "https://en.wikipedia.org/wiki?curid=8202685", "title": "Comparatio", "text": "Comparatio\n\nComparatio in Classical rhetoric is strategy that uses \"comparison\" to persuade people. Comparatio relies upon people's knowledge or beliefs about a phenomenon, and then discursively \"links\" that phenomenon to a different phenomenon about which the speaker/writer wishes to make a claim.\n\nFor example, if someone wanted to persuade an audience of the merit of putting in a new freeway system, they could use comparatio as a rhetorical strategy. They might compare the new freeway system to a \"river of life flowing through our community\" or they could call it a \"path to commercial vibrance.\"\n\nComparisons can also be made to phenomena about which an audience could be expected to have negative feelings. For example, if you were opposed to the new freeway because of the environmental damage it would do to waterfowl nesting areas, you might compare the new freeway to \"a corridor of death for our bird species.\"\n\nThe rhetorical success of comparatio hinges upon comparing your claim to a phenomenon that is:\n\nIt would not, for example, work well to call the freeway a \"ham sandwich lodged in the throat of the community.\" The freeway seems more logically related to the throat (a passageway or throughway) than an obstacle in that passageway.\n"}
{"id": "11778679", "url": "https://en.wikipedia.org/wiki?curid=11778679", "title": "Complex response", "text": "Complex response\n\nA complex response refers to an environmental reaction to change that occurs at multiple levels to multiple objects, and can induce a chain reaction of responses to a single initial change. It is akin to the butterfly effect: one small event (change) can cascade through a given system creating new agents of change, and operating at several levels. The term is most commonly used in fluvial geomorphology, or the study of river systems and changes within those systems.\n\n"}
{"id": "1767727", "url": "https://en.wikipedia.org/wiki?curid=1767727", "title": "Conflict management", "text": "Conflict management\n\nConflict management is the process of limiting the negative aspects of conflict while increasing the positive aspects of conflict. The aim of conflict management is to enhance learning and group outcomes, including effectiveness or performance in an organizational setting. Properly managed conflict can improve group outcomes.\n\nConflict resolution involves the reduction, elimination, or termination of all forms and types of conflict. Five styles for conflict management, as identified by Thomas and Kilmann, are: competing, compromising, collaborating, avoiding, and accommodating.\n\nBusinesses can benefit from appropriate types and levels of conflict. That is the aim of conflict management, and not the aim of conflict resolution. Conflict management does not imply conflict resolution.\n\nConflict management minimizes the negative outcomes of conflict and promotes the positive outcomes of conflict with the goal of improving learning in an organization.\n\nProperly managed conflict increases organizational learning by increasing the number of questions asked and encourages people to challenge the status quo.\n\nOrganizational conflict at the interpersonal level includes disputes between peers as well as supervisor-subordinate conflict. Party-directed mediation (PDM) is a mediation approach particularly suited for disputes between co-workers, colleagues or peers, especially deep-seated interpersonal conflict, multicultural or multiethnic disputes. The mediator listens to each party separately in a pre-caucus or pre-mediation before ever bringing them into a joint session. Part of the pre-caucus also includes coaching and role plays. The idea is that the parties learn how to converse directly with their adversary in the joint session. Some unique challenges arise when organizational disputes involve supervisors and subordinates. The Negotiated Performance Appraisal (NPA) is a tool for improving communication between supervisors and subordinates and is particularly useful as an alternate mediation model because it preserves the hierarchical power of supervisors while encouraging dialogue and dealing with differences in opinion.\n\nThere are three orientations to conflict: lose-lose, win-lose, and win-win. The lose-lose orientation is a type of conflict that tends to end negatively for all parties involved. A win-lose orientation results in one victorious party, usually at the expense of the other. The win-win orientation is one of the most essential concepts to conflict resolution. A win-win solution arrived at by integrative bargaining may be close to optimal for both parties. This approach engages in a cooperative approach rather than a competitive one.\n\nAlthough the win-win concept is the ideal orientation, the notion that there can only be one winner is constantly being reinforced in American culture:\n\n\"The win-lose orientation is manufactured in our society in athletic competition, admission to academic programs, industrial promotion systems, and so on. Individuals tend to generalize from their objective win-lose situations and apply these experiences to situations that are not objectively fixed-pies\".\n\nThis kind of mentality can be destructive when communicating with different cultural groups by creating barriers in negotiation, resolution and compromise; it can also lead the \"loser\" to feel mediocre. When the win-win orientation is absent in negotiation, different responses to conflict may be observed.\n\nBlake and Mouton (1964) were among the first to present a conceptual scheme for classifying the modes (styles) for handling interpersonal conflicts in five types: forcing, withdrawing, smoothing, compromising, and problem solving.\n\nIn the 1970s and 1980s, researchers began using the intentions of the parties involved to classify the styles of conflict management that they included in their models. Both Thomas (1976) and Pruitt (1983) put forth a model based on the concerns of the parties involved in the conflict. The combination of the parties' concern for their own interests (i.e. assertiveness) and their concern for the interests of those across the table (i.e. cooperativeness) yielded a particular conflict management style. Pruitt called these styles yielding (low assertiveness/high cooperativeness), problem solving (high assertiveness/high cooperativeness), inaction (low assertiveness/low cooperativeness), and contending (high assertiveness/low cooperativeness). Pruitt argues that problem-solving is the preferred method when seeking mutually beneficial options (win-win).\n\nKhun and Poole (2000) established a similar system of group conflict management. In their system, they split Kozan's confrontational model into two sub models: distributive and integrative.\n\nDeChurch and Marks (2001) examined the literature available on conflict management at the time and Ni established what they claimed was a \"meta-taxonomy\" that encompasses all other models.\n\nThey argued that all other styles have inherent in them into two dimensions:\n\nIn the study DeChurch and Marks conducted to validate this division, activeness did not have a significant effect on the effectiveness of conflict resolution, but the agreeableness of the conflict management style, whatever it was, did have a positive impact on how groups felt about the way the conflict was managed, regardless of the outcome.\n\nRahim (2002) noted that there is agreement among management scholars that there is no one best approach to how to make decisions, lead or manage conflict.\n\nIn a similar vein, rather than creating a very specific model of conflict management, Rahim created a meta-model (in much the same way that DeChurch and Marks, 2001, created a meta-taxonomy) for conflict styles based on two dimensions, concern for self and concern for others.\n\nWithin this framework are five management approaches: integrating, obliging, dominating, avoiding, and compromising.\n\nSpecial consideration should be paid to conflict management between two parties from distinct cultures. In addition to the everyday sources of conflict, \"misunderstandings, and from this counterproductive, pseudo conflicts, arise when members of one culture are unable to understand culturally determined differences in communication practices, traditions, and thought processing\". Indeed, this has already been observed in the business research literature.\n\nRenner (2007) recounted several episodes where managers from developed countries moved to less developed countries to resolve conflicts within the company and met with little success due to their failure to adapt to the conflict management styles of the local culture.\n\nAs an example, in Kozan's study noted above, he noted that Asian cultures are far more likely to use a harmony model of conflict management. If a party operating from a harmony model comes in conflict with a party using a more confrontational model, misunderstandings above and beyond those generated by the conflict itself will arise.\n\nInternational conflict management, and the cultural issues associated with it, is one of the primary areas of research in the field at the time, as existing research is insufficient to deal with the ever-increasing contact occurring between international entities.\n\nWith only 14% of researched universities reporting mandatory courses in this subject, and with up to 25% of the manager day being spent on dealing with conflict, education needs to reconsider the importance of this subject. The subject warrants emphasis on enabling students to deal with conflict management.\n\n\"Providing more conflict management training in undergraduate business programs could help raise the emotional intelligence of future managers.\" The improvement of emotional intelligence found that employees were more likely to use problem-solving skills, instead of trying to bargain.\n\nStudents need to have a good set of social skills. Good communication skills allow the manager to accomplish interpersonal situations and conflict. Instead of focusing on conflict as a behavior issue, focus on the communication of it.\n\nWith an understanding of the communications required, the student will gain the aptitude needed to differentiate between the nature and types of conflicts. These skills also teach that relational and procedural conflict needs a high degree of immediacy to resolution. If these two conflicts are not dealt with quickly, an employee will become dissatisfied or perform poorly.\n\nIt is also the responsibility of companies to react. One option is to identify the skills needed in-house, but if the skills for creating workplace fairness are already lacking, it may be best to seek assistance from an outside organization, such as a developmental assessment center.\n\nAccording to Rupp, Baldwin, and Bashur, these organizations \"have become a popular means for providing coaching, feedback, and experiential learning opportunities\". Their main focus is fairness and how it impacts employees' attitudes and performance.\n\nThese organizations teach competencies and what they mean. The students then participate in simulations. Multiple observers assess and record what skills are being used and then return this feedback to the participant. After this assessment, participants are then given another set of simulations to utilize the skills learned. Once again they receive additional feedback from observers, in hopes that the learning can be used in their workplace.\n\nThe feedback the participant receives is detailed, behaviorally specific, and high quality. This is needed for the participant to learn how to change their behavior. In this regard, it is also important that the participant take time to self-reflect so that learning may occur.\n\nOnce an assessment program is utilized, action plans may be developed based on quantitative and qualitative data.\n\nWhen personal conflict leads to frustration and loss of efficiency, counseling may prove to be a helpful antidote. Although few organizations can afford the luxury of having professional counselors on the staff, given some training, managers may be able to perform this function. Nondirective counseling, or \"listening with understanding\", is little more than being a good listener —\nsomething every manager should be.\n\nSometimes the simple process of being able to vent one's feelings—that is; to express them to a concerned and understanding listener–is enough to relieve frustration and make it possible for the frustrated individual to advance to a problem-solving frame of mind, better able to cope with a personal difficulty that is affecting his work adversely. The nondirective approach is one effective way for managers to deal with frustrated subordinates and co-workers.\n\nThere are other more direct and more diagnostic ways that might be used in appropriate circumstances. The great strength of the nondirective approach (nondirective counseling is based on the client-centered therapy of Carl Rogers), however, lies in its simplicity, its effectiveness, and the fact that it deliberately avoids the manager-counselor's diagnosing and interpreting emotional problems, which would call for special psychological training. No one has ever been harmed by being listened to sympathetically and understandingly. On the contrary, this approach has helped many people to cope with problems that were interfering with their effectiveness on the job.\n\n\n\n"}
{"id": "8900", "url": "https://en.wikipedia.org/wiki?curid=8900", "title": "Discrimination", "text": "Discrimination\n\nIn human social affairs, discrimination is treatment or consideration of, or making a distinction towards, a person based on the group, class, or category to which the person is perceived to belong. These include age, colour, convictions for which a pardon has been granted or a record suspended, height, disability, ethnicity, family status, gender identity, genetic characteristics, marital status, nationality, race, religion, sex, and sexual orientation. Discrimination consists of treatment of an individual or group, based on their actual or perceived membership in a certain group or social category, \"in a way that is worse than the way people are usually treated\". It involves the group's initial reaction or interaction going on to influence the individual's actual behavior towards the group leader or the group, restricting members of one group from opportunities or privileges that are available to another group, leading to the exclusion of the individual or entities based on illogical or irrational decision making.\n\nDiscriminatory traditions, policies, ideas, practices and laws exist in many countries and institutions in every part of the world, including in territories where discrimination is generally looked down upon. In some places, controversial attempts such as quotas have been used to benefit those who are believed to be current or past victims of discrimination—but they have sometimes been called reverse discrimination.\n\nThe term \"discriminate\" appeared in the early 17th century in the English language. It is from the Latin \"discriminat-\" 'distinguished between', from the verb \"discriminare\", from \"discrimen\" 'distinction', from the verb \"discernere\". Since the American Civil War the term \"discrimination\" generally evolved in American English usage as an understanding of prejudicial treatment of an individual based solely on their race, later generalized as membership in a certain socially undesirable group or social category. \"Discrimination\" derives from Latin, where the verb \"discrimire\" means \"to separate, to distinguish, to make a distinction\".\n\nMoral philosophers have defined discrimination as \"disadvantageous\" treatment or consideration. This is a comparative definition. An individual need not be actually harmed in order to be discriminated against. They just need to be treated \"worse\" than others for some arbitrary reason. If someone decides to donate to help orphan children, but decides to donate less, say, to black children out of a racist attitude, then they would be acting in a discriminatory way despite the fact that the people they discriminate against actually benefit by receiving a donation. In addition to this discrimination develops into a source of oppression. It is similar to the action of recognizing someone as 'different' so much that they are treated inhumanly and degraded.\n\nBased on realistic-conflict theory and social-identity theory, Rubin and Hewstone have highlighted a distinction among three types of discrimination:\n\nThe United Nations stance on discrimination includes the statement: \"Discriminatory behaviors take many forms, but they all involve some form of exclusion or rejection.\" International bodies United Nations Human Rights Council work towards helping ending discrimination around the world.\n\nAgeism or age discrimination is discrimination and stereotyping based on the grounds of someone's age. It is a set of beliefs, norms, and values which used to justify discrimination or subordination based on a person's age. Ageism is most often directed towards old people, or adolescents and children.\n\nAge discrimination in hiring has been shown to exist in the United States. Joanna Lahey, professor at The Bush School of Government and Public Service at Texas A&M, found that firms are more than 40% more likely to interview a young adult job applicant than an older job applicant. In Europe, Stijn Baert, Jennifer Norga, Yannick Thuy and Marieke Van Hecke, researchers at Ghent University, measured comparable ratios in Belgium. They found that age discrimination is heterogeneous by the activity older candidates undertook during their additional post-educational years. In Belgium, they are only discriminated if they have more years of inactivity or irrelevant employment.\n\nIn a survey for the University of Kent, England, 29% of respondents stated that they had suffered from age discrimination. This is a higher proportion than for gender or racial discrimination. Dominic Abrams, social psychology professor at the university, concluded that ageism is the most pervasive form of prejudice experienced in the UK population.\n\nAccording to UNICEF and Human Rights Watch, caste discrimination affects an estimated 250 million people worldwide. Discrimination based on caste, as perceived by UNICEF, is mainly prevalent in parts of Asia, (India, Sri Lanka, Bangladesh, China, Pakistan, Nepal, Japan), Africa and others. , there were 200 million Dalits or Scheduled Castes (formerly known as \"untouchables\") in India.\n\nDiscrimination against people with disabilities in favor of people who are not is called ableism or disablism. Disability discrimination, which treats non-disabled individuals as the standard of 'normal living', results in public and private places and services, education, and social work that are built to serve 'standard' people, thereby excluding those with various disabilities. Studies have shown, employment is needed to not only provide a living but to sustain mental health and well-being. Work fulfils a number of basic needs for an individual such as collective purpose, social contact, status, and activity. A person with a disability is often found to be socially isolated and work is one way to reduce isolation.\n\nIn the United States, the Americans with Disabilities Act mandates the provision of equality of access to both buildings and services and is paralleled by similar acts in other countries, such as the Equality Act 2010 in the UK.\n\nDiversity of language is protected and respected by most nations who value cultural diversity. However, people are sometimes subjected to different treatment because their preferred language is associated with a particular group, class or category. Notable examples are the Anti-French sentiment in the United States as well as the Anti-Quebec sentiment in Canada targeting people who speak the French language. Commonly, the preferred language is just another attribute of separate ethnic groups. Discrimination exists if there is prejudicial treatment against a person or a group of people who either do or do not speak a particular language or languages.\n\nAnother noteworthy example of linguistic discrimination is the backdrop to the Bengali Language Movement in erstwhile Pakistan, a political campaign that played a key role in the creation of Bangladesh. In 1948, Mohammad Ali Jinnah declared Urdu as the national language of Pakistan and branded those supporting the use of Bengali, the most widely spoken language in the state, as enemies of the state.\n\nLanguage discrimination is suggested to be labeled linguicism or logocism. Anti-discriminatory and inclusive efforts to accommodate persons who speak different languages or cannot have fluency in the country's predominant or \"official\" language, is bilingualism such as official documents in two languages, and multiculturalism in more than two languages.\n\nDiscrimination based on a person's name may also occur, with research suggesting the presence of discrimination based on name meaning, pronunciation, uniqueness, gender affiliation, and racial affiliation. Research has further shown that real world recruiters spend an average of just six seconds reviewing each résumé before making their initial \"fit/no fit\" screen-out decision and that a person's name is one of the six things they focus on most. France has made it illegal to view a person's name on a résumé when screening for the initial list of most qualified candidates. Great Britain, Germany, Sweden, and the Netherlands have also experimented with name-blind résumé processes. Some apparent discrimination may be explained by other factors such as name frequency. The effects of name discrimination based on name fluency is subtle, small and subject significantly to changing norms.\n\nDiscrimination on the basis of nationality is usually included in employment laws (see above section for employment discrimination specifically). It is sometimes referred to as bound together with racial discrimination although it can be separate. It may vary from laws that stop refusals of hiring based on nationality, asking questions regarding origin, to prohibitions of firing, forced retirement, compensation and pay, etc., based on nationality.\n\nDiscrimination on the basis of nationality may show as a \"level of acceptance\" in a sport or work team regarding new team members and employees who differ from the nationality of the majority of team members.\n\nIn the UAE and other GCC states, for instance, nationality is not frequently given to residents and expatriates. In the workplace, preferential treatment is given to full citizens, even though many of them lack experience or motivation to do the job. State benefits are also generally available for citizens only.\n\nRacial and ethnic discrimination differentiates individuals on the basis of real and perceived racial and ethnic differences and leads to various forms of the ethnic penalty. It has been official government policy in several countries, such as South Africa during the apartheid era. Discriminatory policies towards ethnic minorities include the race-based discrimination of ethnic Indians and Chinese in Malaysia After the Vietnam war, many Vietnamese refugees moved to the United States, where they face discrimination. Many studies report lower private sector earnings for racial minorities, although it is often difficult to determine the extent to which this is the result of racial discrimination.\n\n, aboriginal people (First Nations, Métis, and Inuit) comprise 4 percent of Canada's population, but they account for 23.2 percent of the federal prison population. According to the Australian government's June 2006 publication of prison statistics, Aborigines make up 24% of the overall prison population in Australia.\n\nIn 2004, Māori made up just 15% of the total population of New Zealand but 49.5% of prisoners. Māori were entering prison at eight times the rate of non-Māori. A quarter of the people in England's prisons are from an ethnic minority. The Equality and Human Rights Commission found that in England and Wales , a black person was five times more likely to be imprisoned than a white person. The discrepancy was attributed to \"decades of racial prejudice in the criminal justice system\".\n\nIn the United States, racial profiling of minorities by law-enforcement officials has been called racial discrimination.<br> Within the criminal justice system in the United States, minorities are convicted and imprisoned disproportionately when compared to the majority. As early as 1866, the Civil Rights Act and Civil Rights Act of 1871 provided a remedy for intentional racism in employment by private employers and state and local public employers. The Civil Rights Act of 1991 expanded the damages available in Title VII cases and granted Title VII plaintiffs the right to a jury trial.\n\nRacial discrimination in hiring has been shown to exist in the United States and in Europe. Using a field experiment, Marianne Bertrand and Sendhil Mullainathan showed that applications from job candidates with white-sounding names received 50 percent more callbacks for interviews than those with African-American-sounding names in the United States at the start of this millennium. A 2009 study by Devah Pager, Bruce Western, and Bart Bonikowski found that black applicants to low-wage jobs were half as likely as identically qualified white applicants to receive callbacks or job offers. More recently, Stijn Baert, Bart Cockx, Niels Gheyle and Cora Vandamme replicated and extended their field experiment in Belgium, Europe. They found that racial discrimination in the labour market is heterogeneous by the labour market tightness in the occupation: compared to natives, candidates with a foreign-sounding name are equally often invited to a job interview in Belgium if they apply for occupations for which vacancies are difficult to fill, but they have to send twice as many applications for occupations for which labor market tightness is low.\n\nRegional or geographic discrimination is discrimination based on the region in which a person lives or was born. It differs from national discrimination in that it may not be based on national borders or the country the victim lives in, but is instead based on prejudices against a specific region of one or more countries. Examples include discrimination against Chinese born in countryside far from city within China, and discrimination against Americans from the southern or northern regions of the United States. It is often accompanied by discrimination based on accent, dialect, or cultural differences.\n\nReligious discrimination is valuing or treating a person or group differently because of what they do or do not believe or because of their feelings towards a given religion. For instance, the indigenous Christian population of the Balkans, known as the \"rayah\" or the \"protected flock\", was discriminated against under the Ottoman Kanun–i–Rayah. The word is sometimes translated as 'cattle' rather than 'flock' or 'subjects' in order to emphasize the Christian population's inferior status to that of the Muslim rayah.\n\nRestrictions upon Jewish occupations were imposed by Christian authorities. Local rulers and church officials closed many professions to religious Jews, pushing them into marginal roles considered socially inferior, such as tax and rent collecting and moneylending, occupations only tolerated as a \"necessary evil\". The number of Jews permitted to reside in different places was limited; they were concentrated in ghettos and were not allowed to own land.\n\nIn a 1979 consultation on the issue, the United States commission on civil rights defined religious discrimination in relation to the civil rights guaranteed by the Fourteenth Amendment to the United States Constitution. Whereas religious civil liberties, such as the right to hold or not to hold a religious belief, are essential for Freedom of Religion (in the United States secured by the First Amendment), religious discrimination occurs when someone is denied \"the equal protection of the laws, equality of status under the law, equal treatment in the administration of justice, and equality of opportunity and access to employment, education, housing, public services and facilities, and public accommodation because of their exercise of their right to religious freedom\".\n\nThough gender discrimination and sexism refer to beliefs and attitudes in relation to the gender of a person, such beliefs and attitudes are of a social nature and do not, normally, carry any legal consequences. Sex discrimination, on the other hand, may have legal consequences. Though what constitutes sex discrimination varies between countries, the essence is that it is an adverse action taken by one person against another person that would not have occurred had the person been of another sex. Discrimination of that nature is considered a form of prejudice and in certain enumerated circumstances is illegal in many countries.\n\nSexual discrimination can arise in different contexts. For instance, an employee may be discriminated against by being asked discriminatory questions during a job interview, or by an employer not hiring or promoting, unequally paying, or wrongfully terminating, an employee based on their gender. Sexual discrimination can also arise when the dominant group holds a bias against the minority group. One such example is Wikipedia. In the Wikipedian community, around 13 percent of registered users are women. This creates gender imbalances, and leaves room for systemic bias. Women are not only more harshly scrutinized, but the representation of women authors are also overlooked. Relative to men, across all source lists, women have a 2.6 greater odds of omission in Wikipedia. In an educational setting, there could be claims that a student was excluded from an educational institution, program, opportunity, loan, student group, or scholarship because of their gender. In the housing setting, there could be claims that a person was refused negotiations on seeking a house, contracting/leasing a house or getting a loan based on their gender. Another setting where there have been claims of gender discrimination is banking; for example if one is refused credit or is offered unequal loan terms based on one's gender. As with other forms of unlawful discrimination, there are two types of sex discrimination – direct discrimination and indirect discrimination. Direct sex discrimination is fairly easy to spot – 'Barmaid wanted', but indirect sex discrimination, where an unnecessary requirement puts one sex at a disproportionate disadvantage compared to the opposite sex, is sometimes less easy to spot, although some are obvious – 'Bar person wanted – must look good in a mini skirt'. Another setting where there is usually gender discrimination is when one is refused to extend their credit, refused approval of credit/loan process, and if there is a burden of unequal loan terms based on one's gender.\n\nSocially, sexual differences have been used to justify different roles for men and women, in some cases giving rise to claims of primary and secondary roles. While there are alleged non-physical differences between men and women, major reviews of the academic literature on gender difference find only a tiny minority of characteristics where there are consistent psychological differences between men and women, and these relate directly to experiences grounded in biological difference. Unfair discrimination usually follows the gender stereotyping held by a society.\n\nThe United Nations had concluded that women often experience a \"glass ceiling\" and that there are no societies in which women enjoy the same opportunities as men. The term \"glass ceiling\" is used to describe a perceived barrier to advancement in employment based on discrimination, especially sex discrimination. In the United States in 1995, the Glass Ceiling Commission, a government-funded group, stated: \"Over half of all Master's degrees are now awarded to women, yet 95% of senior-level managers, of the top Fortune 1000 industrial and 500 service companies are men. Of them, 97% are white.\" In its report, it recommended affirmative action, which is the consideration of an employee's gender and race in hiring and promotion decisions, as a means to end this form of discrimination. , women accounted for 51% of workers in high-paying management, professional, and related occupations. They outnumbered men in such occupations as public relations managers, financial managers, and human resource managers.\n\nIn addition, women are found to experience a sticky floor. While a glass ceiling implies that women are less like to reach the top of the job ladder, a sticky floor is defined as the pattern that women are, compared to men, less likely to start to climb the job ladder. A sticky floor is related to gender differences at the bottom of the wage distribution. It might be explained by both employer discrimination and gender differences in career aspirations.\n\nIntersex persons experience discrimination due to innate, atypical sex characteristics. Multiple jurisdictions now protect individuals on grounds of \"intersex status\" or \"sex characteristics\". South Africa was the first country to explicitly add intersex to legislation, as part of the attribute of 'sex'. Australia was the first country to add an independent attribute, of 'intersex status'. Malta was the first to adopt a broader framework of 'sex characteristics', through legislation that also ended modifications to the sex characteristics of minors undertaken for social and cultural reasons.\n\nTransgender individuals, whether male-to-female, female-to-male, or genderqueer, often experience transphobic problems that often lead to dismissals, underachievement, difficulty in finding a job, social isolation, and, occasionally, violent attacks against them. \n\nNevertheless, the problem of gender discrimination does not stop at transgender individuals or with women. Men are often the victim in certain areas of employment as men begin to seek work in office and childcare settings traditionally perceived as \"women's jobs\". One such situation seems to be evident in a recent case concerning alleged YMCA discrimination and a Federal Court Case in Texas. The case actually involves alleged discrimination against both men and black people in childcare, even when they pass the same strict background tests and other standards of employment. It is currently being contended in federal court, as of fall 2009.\n\nDiscrimination in slasher films is relevant. Gloria Cowan had a research group study on 57 different slasher films. Their results showed that the non-surviving females were more frequently sexual than the surviving females and the non-surviving males. Surviving as a female slasher victim was strongly associated with the absence of sexual behavior. In slasher films, the message appears to be that sexual women get killed and only the pure women survive, thus reinforcing the idea that female sexuality can be costly.\n\nOne's sexual orientation is a \"predilection for homosexuality, heterosexuality, or bisexuality\". Like most minority groups, homosexuals and bisexuals are vulnerable to prejudice and discrimination from the majority group. They may experience hatred from others because of their sexual preferences; a term for such hatred based upon one's sexual orientation is often called homophobia. Many continue to hold negative feelings towards those with non-heterosexual orientations and will discriminate against people who have them or are thought to have them. People of other uncommon sexual orientations also experience discrimination. One study found its sample of heterosexuals to be more prejudiced against asexuals than to homosexuals or bisexuals.\n\nEmployment discrimination based on sexual orientation varies by country. Revealing a lesbian sexual orientation (by means of mentioning an engagement in a rainbow organisation or by mentioning one's partner name) lowers employment opportunities in Cyprus and Greece but overall, it has no negative effect in Sweden and Belgium. In the latter country, even a positive effect of revealing a lesbian sexual orientation is found for women at their fertile ages.\n\nBesides these academic studies, in 2009, ILGA published a report based on research carried out by Daniel Ottosson at Södertörn University College, Stockholm, Sweden. This research found that of the 80 countries around the world that continue to consider homosexuality illegal, five carry the death penalty for homosexual activity, and two do in some regions of the country. In the report, this is described as \"State sponsored homophobia\". This happens in Islamic states, or in two cases regions under Islamic authority. On February 5, 2005, the IRIN issued a reported titled \"Iraq: Male homosexuality still a taboo\". The article stated, among other things that honor killings by Iraqis against a gay family member are common and given some legal protection. In August 2009, Human Rights Watch published an extensive report detailing torture of men accused of being gay in Iraq, including the blocking of men's anuses with glue and then giving the men laxatives. Although gay marriage has been legal in South Africa since 2006, same-sex unions are often condemned as \"un-African\". Research conducted in 2009 shows 86% of black lesbians from the Western Cape live in fear of sexual assault.\nA number of countries, especially those in the Western world, have passed measures to alleviate discrimination against sexual minorities, including laws against anti-gay hate crimes and workplace discrimination. Some have also legalized same-sex marriage or civil unions in order to grant same-sex couples the same protections and benefits as opposite-sex couples. In 2011, the United Nations passed its first resolution recognizing LGBT rights.\n\nDrug use discrimination is the unequal treatment people experience because of the drugs they use. People who use or have used illicit drugs may face discrimination in employment, welfare, housing, child custody, and travel, in addition to imprisonment, asset forfeiture, and in some cases forced labor, torture, and execution. Though often prejudicially stereotyped as deviants and misfits, most drug users are well-adjusted and productive members of society. Drug prohibitions may have been partly motivated by racism and other prejudice against minorities, and racial disparities have been found to exist in the enforcement and prosecution of drug laws. Discrimination due to illicit drug use was the most commonly reported type of discrimination among Blacks and Latinos in a 2003 study of minority drug users in New York City, double to triple that due to race. People who use legal drugs such as tobacco and prescription medications may also face discrimination.\n\nIdeas of self-ownership and cognitive liberty affirm rights to use drugs, whether for medicine recreation, or spiritual fulfilment. Those espousing such ideas question the legality of drug prohibition and cite the rights and freedoms enshrined in such documents as the Declaration of Independence, the U.S. Constitution and Bill of Rights, the European Convention on Human Rights, and the Universal Declaration of Human Rights, as protecting personal drug choices. They are inspired by and see themselves following in the tradition of those who have struggled against other forms of discrimination in the past.\n\nDrug policy reform organizations such as the Drug Policy Alliance, the Drug Equality Alliance, the Transform Drug Policy Foundation, and the Beckley Foundation have highlighted the issue of stigma and discrimination in drug policy. The Partnership for Drug-Free Kids also recognizes this issue and shares on its website stories that \"break through the stigma and discrimination that people with drug or drinking problems often face.\"\n\nA report issued by the Global Commission on Drug Policy, critical of the global war on drugs, states, under \"Undermining Human Rights, Fostering Discrimination\":\n\nPunitive approaches to drug policy are severely undermining human rights in every region of the world. They lead to the erosion of civil liberties and fair trial standards, the stigmatization of individuals and groups – particularly women, young people, and ethnic minorities – and the imposition of abusive and inhumane punishments.\n\nAlthough still illegal at the federal level, about half of U.S. states have legalized marijuana for medical use and several of those states have laws, or are considering legislation, specifically protecting medical marijuana patients from discrimination in such areas as education, employment, housing, child custody, and organ transplantation.\n\nIn the US, a government policy known as affirmative action was instituted to encourage employers and universities to seek out and accept groups such as African Americans and women, who have been subject to discrimination for a long time.\n\nSome attempts at antidiscrimination have been criticized as reverse discrimination. In particular, minority quotas (for example, affirmative action) may discriminate against members of a dominant or majority group or other minority groups. In its opposition to race preferences, the American Civil Rights Institute's Ward Connerly stated, \"There is nothing positive, affirmative, or equal about 'affirmative action' programs that give preference to some groups based on race.\"\n\nAustralia\nCanada\nHong Kong\nIsrael\n\nNetherlands\n\nUnited Kingdom\nUnited States\n\nImportant UN documents addressing discrimination include:\n\nSocial theories such as egalitarianism assert that social equality should prevail. In some societies, including most developed countries, each individual's civil rights include the right to be free from government sponsored social discrimination. Due to a belief in the capacity to perceive pain or suffering shared by all animals, \"abolitionist\" or \"vegan\" egalitarianism maintains that the interests of every individual (regardless its species), warrant equal consideration with the interests of humans, and that not doing so is \"speciesist\".\n\nDiscrimination, in labeling theory, takes form as mental categorization of minorities and the use of stereotype. This theory describes difference as deviance from the norm, which results in internal devaluation and social stigma that may be seen as discrimination. It is started by describing a \"natural\" social order. It is distinguished between the fundamental principle of fascism and social democracy. The Nazis in 1930s-era Germany and the pre-1990 Apartheid government of South Africa used racially discriminatory agendas for their political ends. This practice continues with some present day governments.\n\nEconomist Yanis Varoufakis (2013) argues that \"discrimination based on utterly arbitrary characteristics\nevolves quickly and systematically in the experimental laboratory\", and that neither classical game theory nor neoclassical economics can explain this. Varoufakis and Shaun Hargreaves-Heap (2002) ran an experiment where volunteers played a computer-mediated, multiround hawk-dove game (HD game). At the start of each session, each participant was assigned a color at random, either red or blue. At each round, each player learned the color assigned to his or her opponent, but nothing else about the opponent. Hargreaves-Heap and Varoufakis found that the players' behavior within a session frequently developed a discriminatory convention, giving a Nash equilibrium where players of one color (the \"advantaged\" color) consistently played the aggressive \"hawk\" strategy against players of the other, \"disadvantaged\" color, who played the acquiescent \"dove\" strategy against the advantaged color. Players of both colors used a mixed strategy when playing against players assigned the same color as their own.\n\nThe experimenters then added a cooperation option to the game, and found that disadvantaged players usually cooperated with each other, while advantaged players usually did not. They state that while the equilibria reached in the original HD game are predicted by evolutionary game theory, game theory does not explain the emergence of cooperation in the disadvantaged group. Citing earlier psychological work of Matthew Rabin, they hypothesize that a norm of differing entitlements emerges across the two groups, and that this norm could define a \"fairness\" equilibrium within the disadvantaged group.\n\nIt is debated as to whether or not markets discourage discrimination brought about by the state. One argument is that since discrimination restricts access to customers and incurs additional expense, market logic will punish discrimination. Opposition by companies to \"Jim Crow\" segregation laws is an example of this. An alternative argument is that markets don't necessarily undermine discrimination, as it is argued that if discrimination is profitable by catering to the \"tastes\" of individuals (which is the point of the market), then the market will not punish discrimination. It is argued that microeconomic analysis of discrimination uses unusual methods to determine its effects (using explicit treatment of production functions) and that the very existence of discrimination in employment (defined as wages which differ from marginal product of the discriminated employees) in the long run contradicts claims that the market will function well and punish discrimination.\n\n"}
{"id": "23291417", "url": "https://en.wikipedia.org/wiki?curid=23291417", "title": "Environmental organization", "text": "Environmental organization\n\nAn environmental organization is an organization coming out of the conservation or environmental movements\nthat seeks to protect, analyse or monitor the environment against misuse or degradation from human forces.\n\nIn this sense the environment may refer to the biophysical environment, the natural environment or the built environment. The organization may be a charity, a trust, a non-governmental organization or a government organization. Environmental organizations can be global, national, regional or local.\n\nFounded on 28 May 1892 in San Francisco, California, Sierra Club was one of the first large-scale environmental preservation organizations in the world.\n\nMost organizations exert more influence through their involvement in policy making. Green politics is a political ideology which emphasizes the importance of achieving environmental goals. The Green parties have formed to implement environmental policies at a government level.\n\nSome environmental issues that environmental organizations focus on include pollution, waste, resource depletion, human overpopulation and climate change.\n\nNotable global environmental organizations are the United Nations Environment Programme (UNEP), World Wide Fund for Nature, the Intergovernmental Panel on Climate Change (IPCC), Greenpeace and Friends of the Earth.\n\nMany interest groups and associations have been formed, often industry supported, to counter the progressive - capitalism- and economy-skeptic attitude and influence of environmental organizations. However, these interest groups are rarely pure in ideological nature.\n"}
{"id": "1866009", "url": "https://en.wikipedia.org/wiki?curid=1866009", "title": "Environmentalism", "text": "Environmentalism\n\nEnvironmentalism or environmental rights is a broad philosophy, ideology, and social movement regarding concerns for environmental protection and improvement of the health of the environment, particularly as the measure for this health seeks to incorporate the impact of changes to the environment on humans, animals, plants and non-living matter. While environmentalism focuses more on the environmental and nature-related aspects of green ideology and politics, ecologism combines the ideology of social ecology and environmentalism. Ecologism is more commonly used in continental European languages while ‘environmentalism’ is more commonly used in English but the words have slightly different connotations.\n\nEnvironmentalism advocates the preservation, restoration and/or improvement of the natural environment, and may be referred to as a movement to control pollution or protect plant and animal diversity. For this reason, concepts such as a land ethic, environmental ethics, biodiversity, ecology, and the biophilia hypothesis figure predominantly.\n\nAt its crux, environmentalism is an attempt to balance relations between humans and the various natural systems on which they depend in such a way that all the components are accorded a proper degree of sustainability. The exact measures and outcomes of this balance is controversial and there are many different ways for environmental concerns to be expressed in practice. Environmentalism and environmental concerns are often represented by the color green, but this association has been appropriated by the marketing industries for the tactic known as greenwashing.\n\nEnvironmentalism is opposed by anti-environmentalism, which says that the Earth is less fragile than some environmentalists maintain, and portrays environmentalism as overreacting to the human contribution to climate change or opposing human advancement.\n\n\"Environmentalism\" denotes a social movement that seeks to influence the political process by lobbying, activism, and education in order to protect natural resources and ecosystems.\n\nAn \"environmentalist\" is a person who may speak out about our natural environment and the sustainable management of its resources through changes in public policy or individual behavior. This may include supporting practices such as informed consumption, conservation initiatives, investment in renewable resources, improved efficiencies in the materials economy, transitioning to new accounting paradigms such as Ecological economics, renewing and revitalizing our connections with non-human life or even opting to have one less child to reduce consumption and pressure on resources.\n\nIn various ways (for example, grassroots activism and protests), environmentalists and environmental organizations seek to give the natural world a stronger voice in human affairs.\n\nIn general terms, environmentalists advocate the sustainable management of resources, and the protection (and restoration, when necessary) of the natural environment through changes in public policy and individual behavior. In its recognition of humanity as a participant in ecosystems, the movement is centered around ecology, health, and human rights.\n\nA concern for environmental protection has recurred in diverse forms, in different parts of the world, throughout history. \nThe earliest ideas of environment protectionism can be traced in Jainism, which was revived by Mahavira in 6th century BC in ancient India. Jainism offers a view that may seem readily compatible with core values associated with environmental activism, i.e., protection of life by nonviolence; which could form the basis of strong ecological ethos thus adding its voice to global calls for protection of environment.\n\nIn Europe, King Edward I of England banned the burning of sea-coal by proclamation in London in 1272, after its smoke had become a problem. The fuel was so common in England that this earliest of names for it was acquired because it could be carted away from some shores by the wheelbarrow.\n\nEarlier in the Middle East, the Caliph Abu Bakr in the 630s commanded his army to \"Bring no harm to the trees, nor burn them with fire,\" and \"Slay not any of the enemy's flock, save for your food.\" Arabic medical treatises during the 9th to 13th centuries dealing with environmentalism and environmental science, including pollution, were written by Al-Kindi, Qusta ibn Luqa, Al-Razi, Ibn Al-Jazzar, al-Tamimi, al-Masihi, Avicenna, Ali ibn Ridwan, Ibn Jumay, Isaac Israeli ben Solomon, Abd-el-latif, Ibn al-Quff, and Ibn al-Nafis. Their works covered a number of subjects related to pollution, such as air pollution, water pollution, soil contamination, municipal solid waste mishandling, and environmental impact assessments of certain localities.\n\nThe origins of the environmental movement lay in the response to increasing levels of smoke pollution in the atmosphere during the Industrial Revolution. The emergence of great factories and the concomitant immense growth in coal consumption gave rise to an unprecedented level of air pollution in industrial centers; after 1900 the large volume of industrial chemical discharges added to the growing load of untreated human waste. The first large-scale, modern environmental laws came in the form of Britain's Alkali Acts, passed in 1863, to regulate the deleterious air pollution (gaseous hydrochloric acid) given off by the Leblanc process, used to produce soda ash. An Alkali inspector and four sub-inspectors were appointed to curb this pollution. The responsibilities of the inspectorate were gradually expanded, culminating in the Alkali Order 1958 which placed all major heavy industries that emitted smoke, grit, dust and fumes under supervision.\n\nIn industrial cities local experts and reformers, especially after 1890, took the lead in identifying environmental degradation and pollution, and initiating grass-roots movements to demand and achieve reforms. Typically the highest priority went to water and air pollution. The Coal Smoke Abatement Society was formed in 1898 making it one of the oldest environmental NGOs. It was founded by artist Sir William Blake Richmond, frustrated with the pall cast by coal smoke. Although there were earlier pieces of legislation, the Public Health Act 1875 required all furnaces and fireplaces to consume their own smoke. It also provided for sanctions against factories that emitted large amounts of black smoke. The provisions of this law were extended in 1926 with the Smoke Abatement Act to include other emissions, such as soot, ash and gritty particles and to empower local authorities to impose their own regulations.\n\nDuring the Spanish Revolution, anarchist controlled territories undertook several environmental reforms which were possibly the largest in the world at the time. Daniel Guerin notes that anarchist territories would diversify crops, extend irrigation, initiate reforestation, start tree nurseries and helped establish nudist colonies. Once there was a link discovered between air pollution and tuberculosis, the CNT shut down several metal factories.\n\nIt was, however, only under the impetus of the Great Smog of 1952 in London, which almost brought the city to a standstill and may have caused upward of 6,000 deaths that the Clean Air Act 1956 was passed and airborne pollution in the city was first tackled. Financial incentives were offered to householders to replace open coal fires with alternatives (such as installing gas fires), or for those who preferred, to burn coke instead (a byproduct of town gas production) which produces minimal smoke. 'Smoke control areas' were introduced in some towns and cities where only smokeless fuels could be burnt and power stations were relocated away from cities. The act formed an important impetus to modern environmentalism, and caused a rethinking of the dangers of environmental degradation to people's quality of life.\n\nThe late 19th century also saw the passage of the first wildlife conservation laws.\nThe zoologist Alfred Newton published a series of investigations into the \"Desirability of establishing a 'Close-time' for the preservation of indigenous animals\" between 1872 and 1903. His advocacy for legislation to protect animals from hunting during the mating season led to the formation of the Royal Society for the Protection of Birds and influenced the passage of the Sea Birds Preservation Act in 1869 as the first nature protection law in the world.\n\nEarly interest in the environment was a feature of the Romantic movement in the early 19th century. The poet William Wordsworth travelled extensively in the Lake District and wrote that it is a \"sort of national property in which every man has a right and interest who has an eye to perceive and a heart to enjoy\".\nSystematic efforts on behalf of the environment only began in the late 19th century; it grew out of the amenity movement in Britain in the 1870s, which was a reaction to industrialization, the growth of cities, and worsening air and water pollution. Starting with the formation of the Commons Preservation Society in 1865, the movement championed rural preservation against the encroachments of industrialisation. Robert Hunter, solicitor for the society, worked with Hardwicke Rawnsley, Octavia Hill, and John Ruskin to lead a successful campaign to prevent the construction of railways to carry slate from the quarries, which would have ruined the unspoilt valleys of Newlands and Ennerdale. This success led to the formation of the Lake District Defence Society (later to become The Friends of the Lake District).\n\nPeter Kropotkin wrote about ecology in economics, agricultural science, conservation, ethology, criminology, urban planning, geography, geology and biology. He observed in Swiss and Siberian glaciers that they had been slowly melting since the dawn of the industrial revolution, possibly making him one of the first predictors for climate change. He also observed the damage done from deforestation and hunting. Kropotkin's writings would become influential in the 1970s and became a major inspiration for the intentional community movement as well as his ideas becoming the basis for the theory of social ecology.\n\nIn 1893 Hill, Hunter and Rawnsley agreed to set up a national body to coordinate environmental conservation efforts across the country; the \"National Trust for Places of Historic Interest or Natural Beauty\" was formally inaugurated in 1894. The organisation obtained secure footing through the 1907 National Trust Bill, which gave the trust the status of a statutory corporation. and the bill was passed in August 1907.\n\nAn early \"Back-to-Nature\" movement, which anticipated the romantic ideal of modern environmentalism, was advocated by intellectuals such as John Ruskin, William Morris, George Bernard Shaw and Edward Carpenter, who were all against consumerism, pollution and other activities that were harmful to the natural world. The movement was a reaction to the urban conditions of the industrial towns, where sanitation was awful, pollution levels intolerable and housing terribly cramped. Idealists championed the rural life as a mythical Utopia and advocated a return to it. John Ruskin argued that people should return to a \"small piece of English ground, beautiful, peaceful, and fruitful. We will have no steam engines upon it . . . we will have plenty of flowers and vegetables . . . we will have some music and poetry; the children will learn to dance to it and sing it.\"\n\nPractical ventures in the establishment of small cooperative farms were even attempted and old rural traditions, without the \"taint of manufacture or the canker of artificiality\", were enthusiastically revived, including the Morris dance and the maypole.\n\nThese ideas also inspired various environmental groups in the UK, such as the Royal Society for the Protection of Birds, established in 1889 by Emily Williamson as a protest group to campaign for greater protection for the indigenous birds of the island. The Society attracted growing support from the suburban middle-classes as well as support from many other influential figures, such as the ornithologist Professor Alfred Newton. By 1900, public support for the organisation had grown, and it had over 25,000 members. The Garden city movement incorporated many environmental concerns into its urban planning manifesto; the Socialist League and The Clarion movement also began to advocate measures of nature conservation.\nThe movement in the United States began in the late 19th century, out of concerns for protecting the natural resources of the West, with individuals such as John Muir and Henry David Thoreau making key philosophical contributions. Thoreau was interested in peoples' relationship with nature and studied this by living close to nature in a simple life. He published his experiences in the book \"Walden\", which argues that people should become intimately close with nature. Muir came to believe in nature's inherent right, especially after spending time hiking in Yosemite Valley and studying both the ecology and geology. He successfully lobbied congress to form Yosemite National Park and went on to set up the Sierra Club in 1892. The conservationist principles as well as the belief in an inherent right of nature were to become the bedrock of modern environmentalism.\n\nIn the 20th century, environmental ideas continued to grow in popularity and recognition. Efforts were starting to be made to save some wildlife, particularly the American bison. The death of the last passenger pigeon as well as the endangerment of the American bison helped to focus the minds of conservationists and popularize their concerns. In 1916 the National Park Service was founded by US President Woodrow Wilson.\n\nThe Forestry Commission was set up in 1919 in Britain to increase the amount of woodland in Britain by buying land for afforestation and reforestation. The commission was also tasked with promoting forestry and the production of timber for trade. During the 1920s the Commission focused on acquiring land to begin planting out new forests; much of the land was previously used for agricultural purposes. By 1939 the Forestry Commission was the largest landowner in Britain.\nDuring the 1930s the Nazis had elements that were supportive of animal rights, zoos and wildlife, and took several measures to ensure their protection. In 1933 the government created a stringent animal-protection law and in 1934, \"Das Reichsjagdgesetz\" (The Reich Hunting Law) was enacted which limited hunting. Several Nazis were environmentalists (notably Rudolf Hess), and species protection and animal welfare were significant issues in the regime. In 1935, the regime enacted the \"Reich Nature Protection Act\" (\"Reichsnaturschutzgesetz\"). The concept of the \"Dauerwald\" (best translated as the \"perpetual forest\") which included concepts such as forest management and protection was promoted and efforts were also made to curb air pollution.\n\nIn 1949, \"A Sand County Almanac\" by Aldo Leopold was published. It explained Leopold's belief that humankind should have moral respect for the environment and that it is unethical to harm it. The book is sometimes called the most influential book on conservation.\n\nThroughout the 1950s, 1960s, 1970s and beyond, photography was used to enhance public awareness of the need for protecting land and recruiting members to environmental organizations. David Brower, Ansel Adams and Nancy Newhall created the Sierra Club Exhibit Format Series, which helped raise public environmental awareness and brought a rapidly increasing flood of new members to the Sierra Club and to the environmental movement in general. \"This Is Dinosaur\" edited by Wallace Stegner with photographs by Martin Litton and Philip Hyde prevented the building of dams within Dinosaur National Monument by becoming part of a new kind of activism called environmentalism that combined the conservationist ideals of Thoreau, Leopold and Muir with hard-hitting advertising, lobbying, book distribution, letter writing campaigns, and more. The powerful use of photography in addition to the written word for conservation dated back to the creation of Yosemite National Park, when photographs persuaded Abraham Lincoln to preserve the beautiful glacier carved landscape for all time. The Sierra Club Exhibit Format Series galvanized public opposition to building dams in the Grand Canyon and protected many other national treasures. The Sierra Club often led a coalition of many environmental groups including the Wilderness Society and many others. After a focus on preserving wilderness in the 1950s and 1960s, the Sierra Club and other groups broadened their focus to include such issues as air and water pollution, population concern, and curbing the exploitation of natural resources.\n\nIn 1962, \"Silent Spring\" by American biologist Rachel Carson was published. The book cataloged the environmental impacts of the indiscriminate spraying of DDT in the US and questioned the logic of releasing large amounts of chemicals into the environment without fully understanding their effects on human health and ecology. The book suggested that DDT and other pesticides may cause cancer and that their agricultural use was a threat to wildlife, particularly birds. The resulting public concern led to the creation of the United States Environmental Protection Agency in 1970 which subsequently banned the agricultural use of DDT in the US in 1972. The limited use of DDT in disease vector control continues to this day in certain parts of the world and remains controversial. The book's legacy was to produce a far greater awareness of environmental issues and interest into how people affect the environment. With this new interest in environment came interest in problems such as air pollution and petroleum spills, and environmental interest grew. New pressure groups formed, notably Greenpeace and Friends of the Earth (US), as well as notable local organizations such as the Wyoming Outdoor Council, which was founded in 1967.\n\nIn the 1970s, the environmental movement gained rapid speed around the world as a productive outgrowth of the counterculture movement.\n\nThe world's first political parties to campaign on a predominantly environmental platform were the United Tasmania Group Tasmania, Australia and the Values Party of New Zealand. The first green party in Europe was the Popular Movement for the Environment, founded in 1972 in the Swiss canton of Neuchâtel. The first national green party in Europe was PEOPLE, founded in Britain in February 1973, which eventually turned into the Ecology Party, and then the Green Party.\n\nProtection of the environment also became important in the developing world; the Chipko movement was formed in India under the influence of Mohandas Gandhi and they set up peaceful resistance to deforestation by literally hugging trees (leading to the term \"tree huggers\"). Their peaceful methods of protest and slogan \"ecology is permanent economy\" were very influential.\n\nAnother milestone in the movement was the creation of Earth Day. Earth Day was first observed in San Francisco and other cities on March 21, 1970, the first day of spring. It was created to give awareness to environmental issues. On March 21, 1971, United Nations Secretary-General U Thant spoke of a spaceship Earth on Earth Day, hereby referring to the ecosystem services the earth supplies to us, and hence our obligation to protect it (and with it, ourselves). Earth Day is now coordinated globally by the Earth Day Network, and is celebrated in more than 175 countries every year.\n\nThe UN's first major conference on international environmental issues, the United Nations Conference on the Human Environment (also known as the Stockholm Conference), was held on June 5–16, 1972. It marked a turning point in the development of international environmental politics.\n\nBy the mid-1970s, many felt that people were on the edge of environmental catastrophe. The Back-to-the-land movement started to form and ideas of environmental ethics joined with anti-Vietnam War sentiments and other political issues. These individuals lived outside normal society and started to take on some of the more radical environmental theories such as deep ecology. Around this time more mainstream environmentalism was starting to show force with the signing of the Endangered Species Act in 1973 and the formation of CITES in 1975. Significant amendments were also enacted to the United States Clean Air Act and Clean Water Act.\n\nIn 1979, James Lovelock, a British scientist, published \"Gaia: A new look at life on Earth\", which put forth the Gaia hypothesis; it proposes that life on earth can be understood as a single organism. This became an important part of the Deep Green ideology. Throughout the rest of the history of environmentalism there has been debate and argument between more radical followers of this Deep Green ideology and more mainstream environmentalists.\n\nEnvironmentalism continues to evolve to face up to new issues such as global warming, overpopulation and genetic engineering.\n\nResearch demonstrates a precipitous decline in the US public's interest in 19 different areas of environmental concern. Americans are less likely be actively participating in an environmental movement or organization and more likely to identify as \"unsympathetic\" to an environmental movement than in 2000. This is likely a lingering factor of the Great Recession in 2008. Since 2005, the percentage of Americans agreeing that the environment should be given priority over economic growth has dropped 10 points, in contrast, those feeling that growth should be given priority \"even if the environment suffers to some extent\" has risen 12 percent. These numbers point to the growing complexity of environmentalism and its relationship to economics.\n\nTree sitting is a form of activism in which the protester sits in a tree in an attempt to stop the removal of a tree or to impede the demolition of an area with the longest and most famous tree-sitter being Julia Butterfly Hill, who spent 738 days in a California Redwood, saving a three-acre tract of forest.\n\nSit-in is a form of activism where one or any number of people occupy a place as a form of protest. The tactic can be used to encourage social change, such as the Greensboro sit-ins, a series of protests in 1960 to stop racial segregation, but can also be used in ecoactivism, as in the Dakota Access Pipeline Protest.\n\nBefore the Syrian Civil War, Rojava had been ecologically damaged by monoculture, oil extraction, damming of rivers, deforestation, drought, topsoil loss and general pollution. The DFNS launched a campaign titled 'Make Rojava Green Again' (a parody of Make America Great Again) which is attempting to provide renewable energy to communities (especially solar energy), reforestation, protecting water sources, planting gardens, promoting urban agriculture, creating wildlife reserves, water recycling, beekeeping, expanding public transportation and promoting environmental awareness within their communities.\n\nThe Rebel Zapatista Autonomous Municipalities are firmly environmentalist and have stopped the extraction of oil, uranium, timber and metal from the Lacandon Jungle and stopped the use of pesticides and chemical fertilizers in farming.\n\nThe CIPO-RFM has engaged in sabotage and direct action against wind farms, shrimp farms, eucalyptus plantations and the timber industry. They have also set up corn and coffee worker cooperatives and built schools and hospitals to help the local populations. They have also created a network of autonomous community radio stations to educate people about dangers to the environment and inform the surrounding communities about new industrial projects that would destroy more land. In 2001, the CIPO-RFM defeated the construction of a highway that was part of Plan Puebla Panama.\n\nThe \"environmental movement\" (a term that sometimes includes the conservation and green movements) is a diverse scientific, social, and political movement. Though the movement is represented by a range of organizations, because of the inclusion of environmentalism in the classroom curriculum, the environmental movement has a younger demographic than is common in other social movements (see green seniors).\n\nEnvironmentalism as a movement covers broad areas of institutional oppression, including for example: consumption of ecosystems and natural resources into waste, dumping waste into disadvantaged communities, air pollution, water pollution, weak infrastructure, exposure of organic life to toxins, mono-culture, anti-polythene drive (jhola movement) and various other focuses. Because of these divisions, the environmental movement can be categorized into these primary focuses: environmental science, environmental activism, environmental advocacy, and environmental justice.\n\nFree market environmentalism is a theory that argues that the free market, property rights, and tort law provide the best tools to preserve the health and sustainability of the environment. It considers environmental stewardship to be natural, as well as the expulsion of polluters and other aggressors through individual and class action.\n\nEvangelical environmentalism is an environmental movement in the United States of America in which some Evangelicals have emphasized biblical mandates concerning humanity's role as steward and subsequent responsibility for the caretaking of Creation. While the movement has focused on different environmental issues, it is best known for its focus of addressing climate action from a biblically grounded theological perspective. This movement is controversial among some non-Christian environmentalists due to its rooting in a specific religion.\n\nEnvironmental preservation in the United States and other parts of the world, including Australia, is viewed as the setting aside of natural resources to prevent damage caused by contact with humans or by certain human activities, such as logging, mining, hunting, and fishing, often to replace them with new human activities such as tourism and recreation. Regulations and laws may be enacted for the preservation of natural resources.\n\nEnvironmental organizations can be global, regional, national or local; they can be government-run or private (NGO). Environmentalist activity exists in almost every country. Moreover, groups dedicated to community development and social justice also focus on environmental concerns.\n\nSome US environmental organizations, among them the Natural Resources Defense Council and the Environmental Defense Fund, specialize in bringing lawsuits (a tactic seen as particularly useful in that country). Other groups, such as the US-based National Wildlife Federation, the Nature Conservancy, and The Wilderness Society, and global groups like the World Wide Fund for Nature and Friends of the Earth, disseminate information, participate in public hearings, lobby, stage demonstrations, and may purchase land for preservation. Statewide nonprofit organizations such as the Wyoming Outdoor Council often collaborate with these national organizations and employ similar strategies. Smaller groups, including Wildlife Conservation International, conduct research on endangered species and ecosystems. More radical organizations, such as Greenpeace, Earth First!, and the Earth Liberation Front, have more directly opposed actions they regard as environmentally harmful. While Greenpeace is devoted to nonviolent confrontation as a means of bearing witness to environmental wrongs and bringing issues into the public realm for debate, the underground \"Earth Liberation Front\" engages in the clandestine destruction of property, the release of caged or penned animals, and other criminal acts. Such tactics are regarded as unusual within the movement, however.\n\nOn an international level, concern for the environment was the subject of a United Nations Conference on the Human Environment in Stockholm in 1972, attended by 114 nations. Out of this meeting developed UNEP (United Nations Environment Programme) and the follow-up United Nations Conference on Environment and Development in 1992. Other international organizations in support of environmental policies development include the Commission for Environmental Cooperation (as part of NAFTA), the European Environment Agency (EEA), and the Intergovernmental Panel on Climate Change (IPCC).\n\nNotable environmental protests and campaigns include:\nNotable advocates for environmental protection and sustainability include:\n\nEvery year, more than 100 environmental activists are murdered throughout the world. Most recent deaths are in Brazil, where activists combat logging in the Amazon rainforest.\n\n116 environmental activists were assassinated in 2014, and 185 in 2015. (This represents more than two environmentalists assassinated every week in 2014 and three every week in 2015.) More than 200 environmental activists were assassinated worldwide between 2016 and early 2018.\n\n\nMany environmentalists believe that human interference with 'nature' should be restricted or minimised as a matter of urgency (for the sake of life, or the planet, or just for the benefit of the human species), whereas environmental skeptics and anti-environmentalists do not believe that there is such a need. One can also regard oneself as an environmentalist and believe that human 'interference' with 'nature' should be \"increased\". Nevertheless, there is a risk that the shift from emotional environmentalism into the technical management of natural resources and hazards could decrease the touch of humans with nature, leading to less concern with environment preservation.\n\n\n"}
{"id": "10432337", "url": "https://en.wikipedia.org/wiki?curid=10432337", "title": "Farmer-managed natural regeneration", "text": "Farmer-managed natural regeneration\n\nFarmer-managed natural regeneration (FMNR) is a low-cost, sustainable land restoration technique used to combat poverty and hunger amongst poor subsistence farmers in developing countries by increasing food and timber production, and resilience to climate extremes. It involves the systematic regeneration and management of trees and shrubs from tree stumps, roots and seeds. \nFMNR is especially applicable, but not restricted to, the dryland tropics. As well as returning degraded croplands and grazing lands to productivity, it can be used to restore degraded forests, thereby reversing biodiversity loss and reducing vulnerability to climate change. FMNR can also play an important role in maintaining not-yet-degraded landscapes in a productive state, especially when combined with other sustainable land management practices such as conservation agriculture on cropland and holistic management on rangelands.\nFMNR adapts centuries-old methods of woodland management, called coppicing and pollarding, to produce continuous tree-growth for fuel, building materials, food and fodder without the need for frequent and costly replanting. On farmland, selected trees are trimmed and pruned to maximise growth while promoting optimal growing conditions for annual crops (such as access to water and sunlight). When FMNR trees are integrated into crops and grazing pastures there is an increase in crop yields, soil fertility and organic matter, soil moisture and leaf fodder. There is also a decrease in wind and heat damage, and soil erosion.\n\nIn the Sahel region of Africa, FMNR has become a potent tool in increasing food security, resilience and climate change adaptation in poor, subsistence farming communities where much of sub-Saharan Africa's poverty exists. FMNR is also being promoted in East Timor, Indonesia and Myanmar.\n\nFMNR complements the evergreen agriculture, conservation agriculture and agroforestry movements. It is considered a good entry point for resource-poor and risk-averse farmers to adopt a low-cost and low-risk technique. This in turn has acted as a stepping stone to greater agricultural intensification as farmers become more receptive to new ideas.\n\nThroughout the developing world, immense tracts of farmland, grazing lands and forests have become degraded to the point they are no longer productive. Deforestation continues at an alarming rate. In Africa's drier regions, 74 percent of rangelands and 61 percent of rain-fed croplands are damaged by moderate to very severe desertification. In some African countries deforestation rates exceed planting rates by 300:1.\n\nDegraded land has an extremely detrimental effect on the lives of subsistence farmers who depend on it for their food and livelihoods. Subsistence farmers often make up to 70-80 percent of the population in these regions and they regularly suffer from hunger, malnutrition and even famine as a consequence.\nIn the Sahel region of Africa, a band of savannah which runs across the continent immediately south of the Sahara Desert, large tracts of once-productive farmland are turning to desert. In tropical regions across the world, where rich soils and good rainfall would normally assure bountiful harvests and fat livestock, some environments have become so degraded they are no longer productive.\nSevere famines across the African Sahel in the 1970s and 80s led to a global response, and stopping desertification became a top priority. Conventional methods of raising exotic and indigenous tree species in nurseries were used – planting out, watering, protecting and weeding. However, despite investing millions of dollars and thousands of hours labour, there was little overall impact. Conventional approaches to reforestation in such harsh environments faced insurmountable problems and were costly and labour-intensive. Once planted out, drought, sand storms, pests, competition from weeds and destruction by people and animals negated efforts. Low levels of community ownership were another inhibiting factor.\n\nExisting indigenous vegetation was generally dismissed as 'useless bush', and it was often cleared to make way for exotic species. Exotics were planted in fields containing living and sprouting stumps of indigenous vegetation, the presence of which was barely acknowledged, let alone seen as important.\n\nThis was an enormous oversight. In fact, these living tree stumps are so numerous they constitute a vast 'underground forest' just waiting for some care to grow and provide multiple benefits at little or no cost –and each stump can produce between 10 and 30 stems each. During the process of traditional land preparation, farmers saw the stems as weeds and slashed and burnt them before sowing their food crops. The net result was a barren landscape for much of the year with few mature trees remaining. To the casual observer, the land was turning to desert. Most concluded that there were no trees present and that the only way to reverse the problem was through tree planting.\n\nMeanwhile, established indigenous trees continued to disappear at an alarming rate. In Niger, from the 1930s until 1993, forestry laws took tree ownership and responsibility for the care of trees out of the hands of the people; and even though ineffective and uneconomic, reforestation through conventional tree planting seemed to be the only way to address desertification at the time.\n\nIn the early 1980s, in the Maradi region of the Republic of Niger, the missionary organisation, Serving in Mission (SIM), was unsuccessfully attempting to reforest the surrounding districts using conventional means. In 1983, SIM began experimenting and promoting FMNR amongst about 10 farmers. During the severe famine of 1984, a food-for-work program was introduced that saw some 70,000 people exposed to FMNR and its practice on around 12,500 hectares of farmland. From 1985 to 1999, FMNR continued to be promoted locally and nationally as exchange visits and training days were organised for various NGOs, government foresters, Peace Corps Volunteers and farmer and civil society groups. Additionally, SIM project staff and farmers visited numerous locations across Niger to provide training.\n\nBy 2004 it was ascertained that FMNR was being practiced on over five million hectares or 50 percent of Niger's farmland – an average reforestation rate of 250,000 hectares per year over a 20-year period. This transformation prompted Senior Fellow of the World Resources Institute, Chris Reij to comment that \"this is probably the largest positive environmental transformation in the Sahel and perhaps all of Africa”.\n\nAlso in 2004, World Vision Australia and World Vision Ethiopia initiated a forestry-based carbon sequestration project as a potential means to stimulate community development while engaging in environmental restoration. An innovative partnership with the World Bank, the Humbo Community-based Natural Regeneration Project involved the regeneration of 2,728 hectares of degraded native forests. This brought social, economic and ecological benefits to the participating communities. Within two years, communities were collecting wild fruits, firewood and fodder, and reported that wildlife had begun to return and erosion and flooding had been reduced. In addition, the communities are now receiving payments for the sale of carbon credits through the Clean Development Mechanism (CDM) of the Kyoto protocol.\n\nFollowing the success of the Humbo project, FMNR spread to the Tigray region of northern Ethiopia where 20,000 hectares have been set aside for regeneration, including 10-hectare FMNR model sites for research and demonstration in each of 34 sub-districts. In addition, the Government of Ethiopia has committed to reforest 15 million hectares of degraded land using FMNR as part of a climate change and renewable energy plan to become carbon neutral by 2025.\n\nIn Talensi, northern Ghana, FMNR is being practiced on 2,000-3,000 hectares and new projects, initiated by World Vision, are introducing FMNR into three new districts. In the Kaffrine and Diourbel regions of Senegal, FMNR has spread across 50,000 hectares in four years. World Vision is also promoting FMNR in Indonesia, Myanmar and East Timor. There are also examples of both independently promoted and spontaneous FMNR movements occurring. In Burkina Faso, for example, an increasing part of the country is being transformed into agroforestry parkland. And in Mali, an ageing agroforestry parkland of about 6 million hectares is showing signs of regeneration.\n\nFMNR depends on the existence of living tree stumps or roots in crop fields, grazing pastures, woodlands or forests. Each season bushy growth will sprout from the stumps/roots often appearing like small shrubs. Continuous grazing by livestock, regular burning and/or regular harvesting for fuel wood results in these 'shrubs' never attaining tree stature. On farmland, standard practice has been for farmers to slash this regrowth in preparation for planting crops, but with a little attention this growth can be turned into a valuable resource without jeopardising, but in fact, enhancing crop yields.\n\nFor each stump, a decision is made as to how many stems will be chosen to grow. The tallest and straightest stems are selected and the remaining stems culled. Best results are obtained when the farmer returns regularly to prune any unwanted new stems and side branches as they appear. Farmers can then grow other crops between and around the trees. When farmers want wood they can cut the stem(s) they want and leave the rest to continue growing. The remaining stems will increase in size and value each year, and will continue to protect the environment. Each time a stem is harvested, a younger stem is selected to replace it.\n\nVarious naturally occurring tree species can be used which may also provide berries, fruits and nuts or have medicinal qualities. In Niger, commonly used species include: \"Strychnos spinosa\", \"Balanites aegyptiaca\", \"Boscia senegalensis\", \"Ziziphus\" spp., \"Annona senegalensis\", \"Poupartia birrea\" and \"Faidherbia albida\". However, the most important determinants are whatever species are locally available, their ability to re-sprout after cutting, and the value local people place on those species.\n\n\"Faidherbia albida\", also known as the 'fertiliser tree', is popular for intercropping across the Sahel as it fixes nitrogen into the soil, provides fodder for livestock, and shade for crops and livestock. By shedding its leaves in the wet season, \"Faidherbia\" provides beneficial light shade to crops when high temperatures would otherwise damage crops or retard growth. Leaf fall contributes useful nutrients and organic matter to the soil.\n\nThe practice of FMNR is not confined to croplands. It is being practised on grazing land and in degraded communal forests as well. When there are no living stumps, seeds of naturally occurring species are used. In reality, there is no fixed way of practising FMNR and farmers are free to choose which species they will leave, the density of trees they prefer, and the timing and method of pruning.\n\nFMNR depends on the existence of living tree stumps, tree roots and seeds to be re-vegetated. These can be in crop fields, grazing lands or degraded forests. New stems, which sprout from these stumps and tree roots, can be selected and pruned for improved growth. Sprouting tree stumps and roots may look like shrubs and are often ignored or even slashed by farmers or foresters. However, with culling of excess stems and by selecting and pruning of the best stems, the re-growth has enormous potential to rapidly grow into trees.\n\nSeemingly treeless fields may contain seeds and living tree stumps and roots which have the ability to sprout new stems and regenerate trees. Even this 'bare' millet field in West Africa contains hundreds of living stumps per hectare which are buried beneath the surface like an underground forest.\n\nFMNR can restore degraded farmlands, pastures and forests by increasing the quantity and value of woody vegetation, by increasing biodiversity and by improving soil structure and fertility through leaf litter and nutrient cycling. The reforestation also retards wind and water erosion; it creates wind-breaks which decrease soil moisture evaporation, and protects crops and livestock against searing winds and temperatures. Often, dried up springs reappear and the water table rises towards historic levels; insect eating predators including insects, spiders and birds return, helping to keep crop pests in check; the trees can be a source of edible berries and nuts; and over time the biodiversity of plant and animal life is increased. FMNR can be used to combat deforestation and desertification and can also be an important tool in maintaining the integrity and productivity of land that is not yet degraded.\n\nTrials, long-running programs and anecdotal data indicate that FMNR can at least double crop yields on low fertility soils. In the Sahel, high numbers of livestock and an eight-month dry season can mean that pastures are completely depleted before the rains commence. However, with the presence of trees, grazing animals can make it through the dry season by feeding on tree leaves and seed pods of some species, at a time when no other fodder is available. In north east Ghana, more grass became available with the introduction of FMNR because communities worked together to prevent bush fires from destroying their trees.\n\nWell designed and executed FMNR projects can act as catalysts to empower communities as they negotiate land ownership or user rights for the trees in their care. This assists with self-organisation, and with the development of new agriculture-based micro-enterprises (e.g. selling firewood, timber and handcrafts made from timber or woven grasses).\n\nConventional approaches to reversing desertification, such as funding tree planting, rarely spread beyond the project boundary once external funding is withdrawn. By comparison, FMNR is cheap, rapid, locally led and implemented. It uses local skills and resources – the poorest farmers can learn by observation and teach their neighbours. Given an enabling environment, or at least the absence of a 'disabling' environment, FMNR can be done at scale and spread well beyond the original target area without ongoing government or NGO intervention.\n\nWorld Vision evaluations of FMNR conducted in Senegal and Ghana in 2011 and 2012 found that households practising FMNR were less vulnerable to extreme weather shocks such as drought and damaging rain and wind storms.\n\nThe following table summarises FMNR's benefits which fit the sustainable development model of economic, social and environmental benefits:\n\n\"Source: compiled by R Francis, Project Manager FMNR, World Vision Australia from Brown et al, Garrity et al, Haglund et al, McGahuey & Winterbottom, Reij et al, Rinaudo, World Resources Institute.\"\n\nWhile there are numerous accounts of the uptake and spread of FMNR independent of aid and development agencies, the following factors have been found to be beneficial for its introduction and spread: \n\nBrown et al. suggest that the two main reasons why FMNR has spread so widely in Niger are attitudinal change by the community of what constitutes good land management practices, and farmers' ownership of trees. Farmers need the assurance that they will benefit from their labour. Giving farmers either outright ownership of the trees they protect, or tree-user rights, has made it possible for large-scale farmer-led reforestation to take place.\n\nOver nearly 30 years, FMNR has changed the farming landscape in some of the poorest countries in the world, including parts of Niger, Burkina Faso, Mali and Senegal, providing subsistence farmers with the methods necessary to become more food secure and resilient against severe weather events.\n\nThe 2011–12 food crisis in East Africa gave a stark reminder of the importance of addressing root causes of hunger. In the 2011 State of the World Report, Bunch concludes that four major factors – lack of sustainable fertile land, loss of traditional fallowing, cost of fertiliser and climate change – are coming together all at once in a sort of “perfect storm” that will almost surely result in an African famine of unprecedented proportions, probably within the next four to five years. It will most heavily affect the lowland, semi-arid to sub-humid areas of Africa (including the Sahel, parts of eastern Africa, plus a band from Malawi across to Angola and Namibia); and unless the world does something dramatic, 10 to 30 million people could die from famine between 2015 and 2020. Restoration of degraded land through FMNR is one way of addressing these major contributors to hunger.\n\nIn recent years FMNR has come to the attention of global development agencies and grass roots movements alike. The World Bank, World Resources Institute, World Agroforestry Center, USAID and the Permaculture movement are amongst those either actively promoting or advocating for the uptake of FMNR and FMNR has received recognition from a number of quarters including:\n\n\nIn April 2012, World Vision Australia – in partnership with the World Agroforestry Center and World Vision East Africa – held an international conference in Nairobi called Beating Famine to analyse and plan how to improve food security for the world's poor through the use of FMNR and Evergreen Agriculture. The conference was attended by more than 200 participants, including world leaders in sustainable agriculture, five East African ministers of agriculture and the environment, ambassadors and other government representatives from Africa, Europe and Australia, and leaders from non-government and international organisations.\n\nTwo major outcomes of the conference were:\n\nThe conference acted as a catalyst for media coverage of FMNR in some of the world's leading outlets and a noticeable increase in momentum for an FMNR global movement. This heightened awareness of FMNR has created an opportunity for it to spread exponentially worldwide.\n\nWorld Vision and the World Agroforestry Centrer are currently exploring opportunities for conducting conferences and workshops in new regions where FMNR is not yet established in order to stimulate further awareness and adoption.\n\n\n"}
{"id": "57388109", "url": "https://en.wikipedia.org/wiki?curid=57388109", "title": "Further facts", "text": "Further facts\n\nIn philosophy, the phrase further facts refers to facts that do not follow logically from the physical facts of the world. Reductionists who argue that at bottom there is nothing more than the physical facts thus argue against the existence of further facts. The concept of further facts plays a key role in some of the major works in analytic philosophy of the late 20th century, including in Derek Parfit's \"Reasons and Persons\", and David Chalmers's \"The Conscious Mind\".\n\nOne context in which the existence of further facts is debated is that of personal identity across time: in what sense is Alice today really the \"same\" person as Alice yesterday, given that across the two days the state of her brain is different and the atoms that constitute her are different? One may believe that at bottom, there is nothing more than the atoms and their arrangement at different points in time; while we may for practical purposes come up with some notion of sameness of a person, this notion does not reflect anything deeper about reality. Under this view there would be no further facts. Alternatively, one may believe that there is a deeper sense in which Alice yesterday and Alice today really are the same person. For example, if one believes in Cartesian souls, one may believe that Alice yesterday and Alice today are the same person if and only if they correspond to the same soul. Or one may not believe in Cartesian souls, but yet believe that whether Alice yesterday and Alice today are the same person is a question about something other than facts about which atoms constitute them and how they are arranged. These would both be further-fact views.\nThe debate about further facts about personal identity over time is most closely associated with Derek Parfit. In his \"Reasons and Persons\", he describes the non-reductionist's view that \"personal identity is a deep further fact, distinct from physical and psychological continuity\". Parfit takes a reductionist stance and argues against this further-fact view. As a result it is not clear whether a person has any reason to be worried about her future self in a special way that does not also apply to worrying about others, with Parfit arguing that it is plausible that \"only the deep further fact gives me a reason to be specially concerned about my future\" (his so-called \"Extreme Claim\"). Sydney Shoemaker objects that it is not clear how a further fact would give a reason for such special concerns, either. Harold Langsam has attempted to give a positive account of how a further fact would give such a reason.\nDavid Chalmers lists a number of other types of candidates for further facts. One is facts about conscious experience. For example, it is difficult to see how it follows from the physical facts what it is like to experience seeing red; indeed, inverted spectrum scenarios, where we imagine that experiences of colors are swapped without anything else changing, might suggest that things could have been different without the physical facts changing. Another candidate for a further fact is the fact that there is any conscious experience at all, rather than everyone being a philosophical zombie. Christopher Hill and Brian Mclaughlin have argued against the idea that facts about consciousness are further facts, disputing the logical possibility of a world physically identical to ours in which the facts about consciousness are different.\n\nChalmers also considers facts about indexicality. He cites the fact that \"\"I\" am David Chalmers\", noting that its significance seems to go beyond the tautology that David Chalmers is David Chalmers. (See also Caspar Hare's egocentric presentism and Benj Hellie's vertiginous question.) Similarly, in the philosophy of time, what date and time it is \"now\" might be considered a candidate for a further fact, in the sense that a being that knows everything about the full four-dimensional block of spacetime would still not know what time it is \"now\". (See also the A-theory and the B-theory of time.)\n\nA final type of fact that Chalmers considers is that of \"negative\" facts. For example, consider the following statement: there do not exist nonphysical angels. If in fact true, it does not seem that this logically follows from any of the physical facts by themselves; but, he argues, it would follow if one added a \"That is all\" statement at the end of the list of all the physical facts.\n\nVincent Conitzer has used the following computer simulation scenarios to illuminate further facts about qualia (what it is \"like\" to have specific experiences), indexicality, and personal identity. Imagine a person in the real world who is observing a simulated world on a screen, from the perspective of one of the simulated agents in it. The person observing knows that besides the code responsible for the physics of the simulation, there must be \"additional\" code that determines in which colors the simulation is displayed on the screen, and which agent's perspective is displayed. That is, the person can conclude that the facts about the physics of the simulation (which are completely captured by the code governing the physics) do not fully determine her experience by themselves. But then, Conitzer argues, imagine someone who has become so engrossed in the simulation that she has \"forgotten\" that it is a simulation that she is watching. Could she not still reach the same conclusion? And if so, can we not conclude the same in our own daily lives?\n\n\n"}
{"id": "46970664", "url": "https://en.wikipedia.org/wiki?curid=46970664", "title": "Global mode", "text": "Global mode\n\nIn mathematics and physics, a global mode of a system is one in which the system executes coherent oscillations in time. Suppose a quantity formula_1 which depends on space formula_2 and time formula_3 is governed by some partial differential equation which does not have an explicit dependence on formula_3. Then a global mode is a solution of this PDE of the form formula_5, for some frequency formula_6. If formula_6 is complex, then the imaginary part corresponds to the mode exhibiting exponential growth or exponential decay.\n\nThe concept of a global mode can be compared to that of a normal mode; the PDE may be thought of as a dynamical system of infinitely many equations coupled together. Global modes are used in the stability analysis of hydrodynamical systems. Philip Drazin introduced the concept of a global mode in his 1974 paper, and gave a technique for finding the normal modes of a linear PDE problem in which the coefficients or geometry vary slowly in formula_2. This technique is based on the WKBJ approximation, which is a special case of multiple-scale analysis. His method extends the Briggs–Bers technique, which gives a stability analysis for linear PDEs with constant coefficients.\n\nSince Drazin's 1974 paper, other authors have studied more realistic problems in fluid dynamics using a global mode analysis. Such problems are often highly nonlinear, and attempts to analyse them have often relied on laboratory or numerical experiment. Examples of global modes in practice include the oscillatory wakes produced when fluid flows past an object, such as a vortex street.\n"}
{"id": "48604351", "url": "https://en.wikipedia.org/wiki?curid=48604351", "title": "GroceryAid", "text": "GroceryAid\n\nGroceryAid is a registered (1095897) benevolent society for people from all over the United Kingdom who have worked, or are working in the grocery industry, and who find they need extra support to get by. Founded in 1857 the National Grocers Benevolent Fund has been trading under the GroceryAid name since October 2012. The charity over the years has combined with numerous charities, including the London Grocers & Tea Dealers Federation, the Grocers Federation Benevolent Fund and the Grocers Employees National Benevolent Fund. More recently the charity merged with The Confectioners Benevolent Fund in 2012. At this time, the charity ceased to trade under the Caravan name and from autumn 2012 became known as GroceryAid. There are eight regional Branches which help to raise funds for the charity across the UK. There are also two Network Awareness Groups in which industry colleagues focus on raising awareness of the charity.\n\nGroceryAid’s vision is: 'Everyone in need can turn to us.'\n\nTheir mission is: 'Making life better for grocery people in need. From factory to store, we help everyone.'\n\nTheir objectives are: 'Real lives. Real problems. Real help.'\nPeople who work, or have worked, in the grocery industry use GroceryAid for financial support and practical assistance. This includes support and guidance on health and wellbeing, personal issues, benefits, career, housing and legal issues.\n\nGroceryAid also has a 24/7 freephone, confidential Helpline available by freephone and via online live chat.\n\nThe charity, in September 2015, partnered with Relate, which provides free relationship support.\n\nIn February 2016, Gillian M. Barker announced that she would retire from GroceryAid later in the year, after 14 years with the charity.\n\nIn June 2016, Steve Barnes was announced as Gillian's successor.\n\nAndrew Moore, Chief Merchandising Officer at Asda took over as President of the Fundraising Committee in April 2016. Moore replaced Jason Tarry, Chief Product Officer at Tesco who was President from April 2015 to March 2016.\n\nAt the beginning of 2017 GroceryAid unveiled two new Directorial appointments. Jane Hill started as Fundraising Director while Mandi Leonard replaced Cathy Mercer as Welfare Director in the Spring of 2017.\n\nIt was announced in May 2017 that Andrew Higginson, Chairman of Wm Morrison plc, would become GroceryAid President replacing Andrew Moore who served a year in the role. In a new move, GroceryAid named a Vice President in Charles Wilson, Chief Executive of Booker Ltd and David Wheeler, Head of Finance and Logistics at J Sainsbury plc was appointed as Treasurer.\n\nLife Patrons: Sir David Reid, Lord Mark Price, Mike Coupe and Paul Monk.\n\nGroceryAid operates an annual fundraising calendar in the UK.\n\nThis includes:\n\n\nIn addition to this, GroceryAid have a number of regional branches holding events across the country to raise further funds for the charity.\n\nGroceryAid introduced its Achievement Awards in 2007/8 to recognise companies, large or small, who currently support the charity.\n\n"}
{"id": "1072144", "url": "https://en.wikipedia.org/wiki?curid=1072144", "title": "Grothendieck universe", "text": "Grothendieck universe\n\nIn mathematics, a Grothendieck universe is a set \"U\" with the following properties:\n\n\nA Grothendieck universe is meant to provide a set in which all of mathematics can be performed. (In fact, uncountable Grothendieck universes provide models of set theory with the natural ∈-relation, natural powerset operation etc.). Elements of a Grothendieck universe are sometimes called small sets. The idea of universes is due to Alexander Grothendieck, who used them as a way of avoiding proper classes in algebraic geometry.\n\nThe existence of a nontrivial Grothendieck universe goes beyond the usual axioms of Zermelo–Fraenkel set theory; in particular it would imply the existence of strongly inaccessible cardinals.\nTarski–Grothendieck set theory is an axiomatic treatment of set theory, used in some automatic proof systems, in which every set belongs to a Grothendieck universe.\nThe concept of a Grothendieck universe can also be defined in a topos.\n\nAs an example, we will prove an easy proposition.\n\nIt is similarly easy to prove that any Grothendieck universe \"U\" contains:\n\nIn particular, it follows from the last axiom that if \"U\" is non-empty, it must contain all of its finite subsets and a subset of each finite cardinality. One can also prove immediately from the definitions that the intersection of any class of universes is a universe.\n\nThere are two simple examples of Grothendieck universes:\nOther examples are more difficult to construct. Loosely speaking, this is because Grothendieck universes are equivalent to strongly inaccessible cardinals. More formally, the following two axioms are equivalent:\n\nTo prove this fact, we introduce the function c(\"U\"). Define:\nwhere by |\"x\"| we mean the cardinality of \"x\". Then for any universe \"U\", c(\"U\") is either zero or strongly inaccessible. Assuming it is non-zero, it is a strong limit cardinal because the power set of any element of \"U\" is an element of \"U\" and every element of \"U\" is a subset of \"U\". To see that it is regular, suppose that \"c\" is a collection of cardinals indexed by \"I\", where the cardinality of \"I\" and of each \"c\" is less than c(\"U\"). Then, by the definition of c(\"U\"), \"I\" and each \"c\" can be replaced by an element of \"U\". The union of elements of \"U\" indexed by an element of \"U\" is an element of \"U\", so the sum of the \"c\" has the cardinality of an element of \"U\", hence is less than c(\"U\"). By invoking the axiom of foundation, that no set is contained in itself, it can be shown that c(\"U\") equals |\"U\"|; when the axiom of foundation is not assumed, there are counterexamples (we may take for example U to be the set of all finite sets of finite sets etc. of the sets x where the index α is any real number, and \"x\" = {\"x\"} for each \"α\". Then \"U\" has the cardinality of the continuum, but all of its members have finite cardinality and so formula_13 ; see Bourbaki's article for more details).\n\nLet κ be a strongly inaccessible cardinal. Say that a set \"S\" is strictly of type κ if for any sequence \"s\" ∈ ... ∈ \"s\" ∈ \"S\", |\"s\"| < \"κ\". (\"S\" itself corresponds to the empty sequence.) Then the set \"u\"(\"κ\") of all sets strictly of type κ is a Grothendieck universe of cardinality κ. The proof of this fact is long, so for details, we again refer to Bourbaki's article, listed in the references.\n\nTo show that the large cardinal axiom (C) implies the universe axiom (U), choose a set \"x\". Let \"x\" = \"x\", and for each \"n\", let \"x\" = formula_14 \"x\" be the union of the elements of \"x\". Let \"y\" = formula_15\"x\". By (C), there is a strongly inaccessible cardinal κ such that |y| < κ. Let \"u\"(\"κ\") be the universe of the previous paragraph. \"x\" is strictly of type κ, so \"x\" ∈ \"u\"(\"κ\"). To show that the universe axiom (U) implies the large cardinal axiom (C), choose a cardinal κ. κ is a set, so it is an element of a Grothendieck universe \"U\". The cardinality of \"U\" is strongly inaccessible and strictly larger than that of κ.\n\nIn fact, any Grothendieck universe is of the form \"u\"(\"κ\") for some κ. This gives another form of the equivalence between Grothendieck universes and strongly inaccessible cardinals:\n\nSince the existence of strongly inaccessible cardinals cannot be proved from the axioms of Zermelo–Fraenkel set theory (ZFC), the existence of universes other than the empty set and formula_11 cannot be proved from ZFC either. However, strongly inaccessible cardinals are on the lower end of the list of large cardinals; thus, most set theories that use large cardinals (such as \"ZFC plus there is a measurable cardinal\", \"ZFC plus there are infinitely many Woodin cardinals\") will prove that Grothendieck universes exist.\n\n"}
{"id": "39223191", "url": "https://en.wikipedia.org/wiki?curid=39223191", "title": "ISASMELT", "text": "ISASMELT\n\nThe ISASMELT process is an energy-efficient smelting process that was jointly developed from the 1970s to the 1990s by Mount Isa Mines Limited (a subsidiary of MIM Holdings Limited and now part of Glencore plc) and the Australian government’s Commonwealth Scientific and Industrial Research Organisation (\"CSIRO\"). It has relatively low capital and operating costs for a smelting process.\n\nISASMELT technology has been applied to lead, copper, and nickel smelting, and by 2013 fifteen plants were in operation in ten countries, with another five in various stages of development. The installed capacity of the operating plants in 2013 was over 8 million tonnes per year (t/y) of feed materials, with additional capacity to come on line in 2013 and 2014.\n\nSmelters based on the copper ISASMELT process are among the lowest-cost copper smelters in the world.\n\nAn ISASMELT furnace is an upright-cylindrical shaped steel vessel that is lined with refractory bricks. There is a molten bath of slag, matte or metal (depending on the application) at the bottom of the furnace. A steel lance is lowered into the bath through a hole in the roof of the furnace, and air or oxygen-enriched air that is injected through the lance into the bath causes vigorous agitation of the bath.\nMineral concentrates or materials for recycling are dropped into the bath through another hole in the furnace roof or, in some cases, injected down the lance. These feed materials react with the oxygen in the injected gas, resulting in an intensive reaction in a small volume (relative to other smelting technologies).\n\nISASMELT lances contain one or more devices called \"swirlers\" that cause the injected gas to spin within the lance, forcing it against the lance wall, cooling it. The swirler consists of curved vanes around a central pipe forming an annular flow. They are designed to minimize pressure losses changing the angle from axial to tangential thus creating a strong vortex . The vortex helps mix liquids and solids with oxygen in the bath. The cooling effect results in a layer of slag \"freezing\" on the outside of the lance. This layer of solid slag protects the lance from the high temperatures inside the furnace. The tip of the lance that is submerged in the bath eventually wears out, and the worn lance is easily replaced with a new one when necessary. The worn tips are subsequently cut off and a new tip welded onto the lance body before it is returned to the furnace.\n\nISASMELT furnaces typically operate in the range of 1000–1200 °C, depending on the application. The refractory bricks that form the internal lining of the furnace protect the steel shell from the heat inside the furnace.\n\nThe products are removed from the furnace through one or more \"tap holes\" in a process called \"tapping\". This can be either continuous removal or in batches, with the tap holes being blocked with clay at the end of a tap, and then reopened by drilling or with a thermic lance when it is time for the next tap.\n\nThe products are allowed to separate in a settling vessel, such as a rotary holding furnace or an electric furnace.\n\nWhile smelting sulfide concentrates, most of the energy needed to heat and melt the feed materials is derived from the reaction of oxygen with the sulfur and iron in the concentrate. However, a small amount of supplemental energy is required. ISASMELT furnaces can use a variety of fuels, including coal, coke, petroleum coke, oil and natural gas. The solid fuel can be added through the top of the furnace with the other feed materials, or it can be injected down the lance. Liquid and gaseous fuels are injected down the lance.\n\nThe advantages of the ISASMELT process include:\n\n\nThe history of the ISASMELT process began with the invention in 1973 of the Sirosmelt lance by Drs Bill Denholm and John Floyd at the CSIRO. The lance was developed as a result of investigations into improved tin-smelting processes, in which it was found that the use of a top-entry submerged lance would result in greater heat transfer and mass transfer efficiencies.\n\nThe idea of top-entry submerged lances goes back to at least 1902, when such a system was attempted in Clichy, France. However, early attempts failed because of the short lives of the lances on submersion in the bath. The Mitsubishi copper smelting process is one alternative approach, wherein lances are used in a furnace, but they are not submerged into the bath. Instead, they blow oxygen-enriched air onto the surface of the slag (top jetting). Similarly, a water-cooled, top-jetting lance was the basis of the LD (Linz-Donawitz) steelmaking process. This does not produce the same intensity of mixing in the bath as a submerged lance.\n\nThe CSIRO scientists first tried developing a submerged lance system using a water-cooled lance system, but moved to an air-cooled system because \"scale up of the water-cooled lance would have been problematic\". Introducing any water to a system involving molten metals and slags can result in catastrophic explosions, such as that in the Scunthorpe Steelworks in November 1975 in which 11 men lost their lives.\n\nThe inclusion of the swirlers in the Sirosmelt lance and forming a splash coating of slag on the lance were the major innovations that led to the successful development of submerged lance smelting.\n\nFrom 1973, the CSIRO scientists began a series of trials using the Sirosmelt lance to recover metals from industrial slags in Australia, including lead softener slag at the Broken Hill Associated Smelters in Port Pirie (1973), tin slag from Associated Tin Smelters in Sydney (1974), copper converter slag at the Electrolytic Refining and Smelting (\"ER&S\") Port Kembla plant (1975) and copper anode furnace slag at Copper Refineries Limited (another subsidiary of MIM Holdings) in Townsville (1976) and of copper converter slag in Mount Isa (1977). The work then proceeded to smelting tin concentrates (1975) and then sulfidic tin concentrates (1977).\n\nMIM and ER&S jointly funded the 1975 Port Kembla converter slag treatment trials and MIM’s involvement continued with the slag treatment work in Townsville and Mount Isa.\n\nIn parallel with the copper slag treatment work, the CSIRO was continuing to work in tin smelting. Projects included a five tonne (\"t\") plant for recovering tin from slag being installed at Associated Tin Smelters in 1978, and the first sulfidic smelting test work being done in collaboration with Aberfoyle Limited, in which tin was fumed from pyritic tin ore and from mixed tin and copper concentrates. Aberfoyle was investigating the possibility of using the Sirosmelt lance approach to improve the recovery of tin from complex ores, such as its mine at Cleveland, Tasmania, and the Queen Hill ore zone near Zeehan in Tasmania.\n\nThe Aberfoyle work led to the construction and operation in late 1980 of a four t/h tin matte fuming pilot plant at the Western Mining Corporation’s Kalgoorlie Nickel Smelter, located to the south of Kalgoorlie, Western Australia.\n\nIn the early 1970s, the traditional blast furnace and sinter plant technology that was the mainstay of the lead smelting industry was coming under sustained pressure from more stringent environmental requirements, increased energy costs, decreasing metal prices and rising capital and operating costs.\n\nMany smelting companies were seeking new processes to replace sinter plants and blast furnaces. Possibilities included the QSL lead smelting process, the Kivcet process, the Kaldo top-blown rotary converter, and adapting Outokumpu’s successful copper and nickel flash furnace to lead smelting.\n\nMIM was seeking ways to safeguard the future of its Mount Isa lead smelting operations. It did this in two ways:\nMIM investigated new technologies by arranging plant testing of large parcels of Mount Isa lead concentrates for all the then process options except for the Kivcet process. At the same time, it had been aware of the use of top-jetting lances in the Mitsubishi and Kaldo processes, and of top-entry submerged combustion lance investigations undertaken by ASARCO Limited (which had a long association with MIM, including being a shareholder in MIM Holdings) in the 1960s. This stimulated MIM’s interest in the Sirosmelt lance, which was seen as a way to produce a robust submerged lance.\n\nFollowing the copper slag trials of 1976–1978, MIM initiated a joint project with the CSIRO in 1978 to investigate the possibility of applying Sirosmelt lances to lead smelting.\n\nThe work began with computer modelling the equilibrium thermodynamics (1978) and was followed by laboratory bench-scale test work using large alumina silicate crucibles (1978–1979). The results were sufficiently encouraging that MIM built a 120 kg/h test rig in Mount Isa. It began operation in September 1980. This was used to develop a two-stage process to produce lead bullion from Mount Isa lead concentrate. The first stage was an oxidation step that removed virtually all the sulfur from the feed, oxidising the contained lead to lead oxide (PbO) that was largely collected in the slag (some was carried out of the furnace as lead oxide fume that was returned for lead recovery). The second stage was a reduction step in which the oxygen was removed from the lead to form lead metal.\n\nFollowing the 120 kg/h test work, MIM decided to proceed to install a 5 t/h lead ISASMELT pilot plant in its Mount Isa lead smelter. It bought Aberfoyle’s matte fuming furnace and transported it from Kalgoorlie to Mount Isa, where it was rebuilt and commissioned in 1983 to demonstrate the first stage of the process in continuous operation and for testing the reduction step using batches of high-lead slag.\n\nOne of the key features of the pilot plant was that it was run by operations’ personnel in the lead smelter as though it was an operations’ plant. The high lead slag produced by the continuous smelting of the lead concentrate was subsequently treated in the sinter plant, thus increasing the production of the lead smelter by up to 17%. This gave the operations’ people ownership of the plant and an incentive to make it work, thus ensuring management and maintenance priority. It also gave MIM assurance that the process simple enough to be operable in a production environment, with normal staff and supervision, and that it was robust enough to withstand normal control excursions. In addition to the continuous operation of lead concentrate to produce high-lead slag, the pilot plant was used to produce lead metal from batches of the slag, investigate the wear rates of the furnace’s refractory lining and lances, and initial work aimed at developing a low-pressure version of the Sirosmelt lance. The result was a lance design that allowed operation at significantly lower pressure than the initial values of about 250 kilopascal (gauge) (\"kPag\"), thus reducing operating costs.\n\nMIM built a second, identical furnace next to the first, and commissioned it in August 1985. This combination of furnaces was used to demonstrate the two-stage process in continuous operation in mid-1987. However, for most of the time the two furnaces were not able to operate simultaneously due to a constraint in the capacity of the baghouse used to filter the lead dust from the waste gas.\n\nA series of process improvements, particularly in the waste gas handling system, resulted in increasing the throughput of the plant from the initial design of 5 t/h to 10 t/h. The pilot plant had treated more than 125,000 t of lead concentrate by April 1989.\n\nThe two furnaces were also used to develop a process to recover lead from the Mount Isa lead smelter’s drossing operations.\n\nBased on the results of the pilot plant work, the MIM Holdings Board of Directors approved the construction of an A$65 million demonstration plant, capable of producing 60,000 t/y of lead bullion. This plant operated from early 1991 until 1995. It was initially designed to treat 20 t/h of lead concentrate using lance air enriched to 27%. However, the oxygen originally designated for its use was diverted to the more profitable copper smelting operations, and the feed rate to the lead ISASMELT demonstration plant was severely restricted. When there was sufficient oxygen available in 1993 to increase the enrichment level to 33–35%, treatment rates of up to 36 t/h of concentrate were achieved, with residual lead in the final reduction furnace slag being in the range of 2–5%.\n\nThe two-stage approach to ISASMELT lead smelting was partly driven by the relatively low lead content of Mount Isa lead concentrates (typically in the range of 47–52% lead during the lead ISASMELT development period). Trying to produce lead bullion in a single furnace with such low concentrate grades would result in excessive fuming of lead oxide with a huge amount of material that would have to be returned to the furnace to recover the lead and, consequently, a higher energy demand as that material had to be reheated to the furnace temperatures.\n\nConcentrates with higher lead contents can be smelted directly into lead metal in a single furnace without excess fuming. This was demonstrated on the large scale in 1994, when 4000 t of concentrate containing 67% lead were treated at rates up to 32 t/h with lance air enriched to 27%. During these trials, 50% of the lead in the concentrate was converted to lead bullion in the smelting furnace, while most of the rest ended up as lead oxide in the smelting furnace slag.\n\nLike the lead ISASMELT pilot plant, the lead ISASMELT demonstration plant suffered from constraints imposed by the waste gas handling system. In the case of the demonstration plant, the problem was caused by sticky fume that formed an insulating layer on the convection tube bundles of the waste heat boilers, significantly reducing the heat transfer rates and thus the ability of the boilers to reduce the waste gas temperature. As the plant used baghouses to filter lead fume from the waste gas, it was necessary to reduce the temperature of the gas below the point at which the bags would be damaged by high temperatures. The problem was solved by allowing cool air to mix with the hot waste gas to lower the temperature to a level at which the baghouse could operate. This reduced the ISASMELT plant’s capacity because it was again limited by the volume of gas that could be filtered by the baghouse.\n\nThe lead ISASMELT demonstration plant was mothballed in 1995 because there was insufficient concentrate to keep both it and the rest of the lead smelter operating. It was too small to treat all the Mount Isa lead concentrate by itself.\n\nThe first commercial primary-lead ISASMELT furnace was installed at the Yunnan Chihong Zinc and Germanium Company Limited (YCZG) greenfield zinc and lead smelting complex at Qujing in Yunnan Province in China. This furnace was part of a plant consisting of the ISASMELT furnace and a blast furnace specially designed to treat high-lead ISASMELT slag. The ISASMELT furnace was designed to produce both the slag and lead bullion, with about 40% of the lead in the concentrate being converted to lead bullion in the ISASMELT furnace.\n\nThe ISASMELT–blast furnace combination was designed to treat 160,000 t/y of lead concentrate.\n\nThe second commercial primary-lead ISASMELT furnace was commissioned at Kazzinc’s smelting complex at Ust-Kamenogorsk in Kazakhstan in 2012. It is designed to treat 300,000 t/y of lead concentrate, again using an ISASMELT–blast furnace combination.\n\nYCZG is constructing another lead ISASMELT at a new greenfield smelter in Huize in China, and this is due to be commissioned in 2013.\n\nWhile the lead ISASMELT 5 t/h pilot plant was being designed in 1982–1983, MIM continued to use the 120 kg/h test rig to develop other processes, including the dross treatment process previously mentioned, and the treatment of lead-acid battery paste for lead recycling.\n\nThe MIM Holdings Board of Directors approved the construction of an ISASMELT plant at Britannia Refined Metals, the company’s lead refinery at Northfleet in the United Kingdom, for commercial recovery of lead from battery paste to supplement the existing plant, which used a short rotary furnace to produce 10,000 t/y of lead. The new plant increased annual production to 30,000 t/y of recycled lead, and was commissioned in 1991. The ISASMELT furnace was used to produce low-antimony lead bullion from the battery paste and an antimony-rich slag that contained 55–65% lead oxide. While it was possible to recover the lead from the slag in the ISASMELT furnace by a reduction step, the total throughput of the plant was increased by treating the slag in the short rotary furnace when sufficient quantities of the slag had been generated. The plant was designed to treat 7.7 t/h of battery paste, but routinely treated 12 t/h. The plant was shut down in 2004 when Xstrata Zinc, which took over the MIM Holdings lead operations, decided to leave the lead recycling business.\n\nA second lead ISASMELT plant for recovering lead from recycled batteries was commissioned in 2000 in Malaysia at Metal Reclamation Industries’ Pulau Indah plant. This ISASMELT plant has a design capacity of 40,000 t/y of lead bullion.\n\nScientists at the CSIRO conducted small-scale test work on copper sulfide concentrate in 1979, using the CSIRO’s 50 kg Sirosmelt test rig. These trials included producing copper matte containing 40–52% copper and, in some cases, converting the matte to produce blister copper.\n\nThe results of this work were sufficiently encouraging that MIM in 1983 undertook its own copper smelting test work program using its 120 kg/h test rig, which had by then been rerated to 250 kg/h. It was found that the process was easy to control and that copper loss to slag was low. It was also learned that the process could easily recover copper from copper converter slag concentrate, of which there was a large stockpile at Mount Isa.\n\nConstruction of a 15 t/h demonstration copper ISASMELT plant began in 1986. The design was based on MIM’s 250 kg/h test work and operating experience with the lead ISASMELT pilot plant. It cost A$11 million and was commissioned in April 1987. The initial capital cost was recovered in the first 14 months of operation.\n\nAs with the lead ISASMELT pilot plant, the copper ISASMELT demonstration plant was integrated into copper smelter operations and justified by the 20% (30,000 t/y) increase in copper production that it provided. It quickly treated the entire backlog of converter slag concentrate, which could not be treated at high rates in the reverberatory furnaces without generating magnetite (\"FeO\") accretions that would necessitate shutting down the reverberatory furnaces for their removal.\n\nThe demonstration copper ISASMELT plant was used to further develop the copper process. Refractory life was initially shorter than expected and considerable effort was devoted to understanding the reasons and attempting to extend the life of the refractories. At the end of the life of the demonstration plant, the longest refractory life achieved was 90 weeks.\n\nLance life was also low initially. Inexperienced operators could destroy a lance in as little as 10 minutes. However, as a result of modifications to the lance design, the development of techniques to determine the position of the lance in the bath, and a rise in the operating experience, the typically lance life was extended to a week.\n\nThe demonstration plant was commissioned with high-pressure (700 kPag) air injected down the lance. Later, after extensive testing of low-pressure lance designs and trials using oxygen enrichment of the lance air, a 70 t/d oxygen plant and a 5 Nm3/s blower with a discharge pressure of 146 kPag were purchased. The new lance design was capable of operating at pressures below 100 kPag. Using enrichment of the oxygen in the lance air to 35%, the demonstration plant throughput was lifted to 48 t/h of concentrate, and the gross energy used during smelting was reduced from 25.6 GJ/t of contained copper to 4.1 GJ/t.\n\nThe successful operation and development of the demonstration copper ISASMELT, and the degree of interest shown in the new process by the global smelting community, gave MIM Holdings sufficient confidence to license the ISASMELT technology to external companies, so an agreement under which MIM could incorporate the Sirosmelt lance into ISASMELT technology was signed with the CSIRO in 1989.\n\nMIM signed the first ISASMELT licence agreement with Agip Australia Proprietary Limited (\"Agip\") in July 1990. Agip, a subsidiary of the Italian oil company ENI, was developing the Radio Hill nickel-copper deposit near Karratha in Western Australia. MIM and representatives of Agip conducted a series of trials in which 4 tonnes of Radio Hill concentrate was smelted in the 250 kg/h test rig at Mount Isa.\n\nThe Agip ISASMELT plant was designed to treat 7.5 t/h of the Radio Hill concentrate and produce 1.5 t/h of granulated matte with a combined nickel and copper content of 45% for sale., It was the same size as the copper ISASMELT demonstration plant (2.3 m internal diameter) and had a 5.5 Nm3/s blower to provide the lance air. Commissioning of the plant began in September 1991; however, the Radio Hill mine and smelter complex were forced to close by low nickel prices after less than six months, before commissioning was completed. The ISASMELT furnace achieved its design capacity within three months. Subsequent owners of the mine focussed on mining and mineral processing only, and the ISASMELT plant has been dismantled.\n\nIn 1973, the Freeport-McMoRan Copper and Gold Inc. (\"Freeport\") smelter at Miami, Arizona, installed a 51 MW electric furnace at its Miami smelter. The decision was based on a long-term electrical power contract with the Salt River Project that provided the company with a very low rate for electricity. This contract expired in 1990 and the resulting increase in electricity prices prompted the then owners of the smelter, Cyprus Miami Mining Corporation (\"Cyprus\"), to seek alternative smelting technologies to provide lower operating costs.\n\nThe technologies evaluated included the:\nThe Contop, Inco, Mitsubishi and Outokumpu processes \"were all eliminated primarily because of their high dust levels, high capital costs and poor adaptability to the existing facility\". The Teniente converter was ruled out because it required the use of the electric furnace for partial smelting. The Noranda reactor was not selected \"because of its high refractory wear and its poor adaptability to the existing plant due to the handling of the reactor slag\". ISASMELT was chosen as the preferred technology and a licence agreement was signed with MIM in October 1990. The major factor in the decision to select the ISASMELT technology was the ability to fit it into the existing plant and to maximise the use of existing equipment and infrastructure, while the major disadvantage was seen to be the risks associated with scaling up the technology from the Mount Isa demonstration plant.\n\nThe Miami copper ISASMELT furnace was designed to treat 590,000 t/y (650,000 short tons per year) of copper concentrate, a treatment rate that was constrained by the capacity of the sulfuric acid plant used to capture the sulfur dioxide from the smelter’s waste gases. The existing electric furnace was converted from smelting duties to a slag cleaning furnace and providing matte surge capacity for the converters. The ISASMELT furnace was commissioned on 11 June 1992 and in 2002 treated over 700,000 t/y of concentrate. The modernisation of the Miami smelter cost an estimated US$95 million.\n\nIn 1993, the Cyprus Minerals Company merged with AMAX to form the Cyprus Amax Minerals company, which was in turn taken over by the Phelps Dodge Corporation in late 1999. Phelps Dodge was acquired by Freeport in 2006.\n\nThe Miami smelter is one of only three remaining operating copper smelters in the United States, where there were 16 in 1979.\n\nThe third commercial copper ISASMELT plant was installed in MIM’s Mount Isa copper smelter at a cost of approximately A$100 million. It was designed to treat 104 t/h of copper concentrate, containing 180,000 t/y of copper, and it began operation in August 1992.\n\nA significant difference between the Mount Isa copper ISASMELT plant and all the others is that it uses an Ahlstrom Fluxflow waste heat boiler to recover heat from the furnace waste gas. This boiler uses a recirculating fluid bed of particles to rapidly quench the gas as it leaves the furnace, and then uses the enhanced heat transfer properties of solid–solid contact to cool the particles as they are carried past boiler tubes that are suspended in a shaft above the bed. The high heat transfer rate means that the Fluxflow boiler is relatively compact compared with conventional waste heat boilers and the rapid cooling of the waste gas limits the formation of sulfur trioxide (\"SO\"), which in the presence of water forms sulfuric acid that can cause corrosion of cool surfaces.\n\nIn the early years of operation, the Fluxflow boiler was the cause of significant down time, because the rate of wear of the boiler tubes was much higher than expected. The problems were solved by understanding the gas flows within the boiler redesigning the boiler tubes to minimise the effects of erosion.\n\nThe life of the refractory bricks in the ISASMELT furnace was initially shorter than expected and a water cooling system was briefly considered to extend them; however, this was not installed and operational improvements have resulted in a significant extension of the life of the lining without this capital and operating expense. Since 1998, the refractory lining lives have exceeded the two-year design life, with lives of the 8th and 9th linings almost reaching three years. The most recent lining lasted for 50 months, with the one before that lasting for 44 months.\n\nIn the first years of operation at Mount Isa, the throughput of the ISASMELT furnace was constrained by problems with some of the ancillary equipment in the plant, including the boiler, slag granulation system and concentrate filters. The ultimate constraint was the decision during its construction to keep one of the two reverberatory furnaces on line to increase the copper smelter production to 265,000 t/y of anode copper. The smelter’s Peirce-Smith converters became a bottleneck and the feed rate of the ISASMELT furnace had to be restrained to allow sufficient matte to be drawn from the reverberatory furnace to prevent it freezing solid. The ISASMELT 12-month rolling average of the feed rate fell just short of 100 t/h for much of this period, not quite reaching the design annual average of 104 t/h. MIM decided to shut down the reverberatory furnace in 1997, and the ISASMELT plant 12-month rolling mean feed rate quickly exceeded the 104 t/h design when this constraint was lifted.\n\nThe performance of the ISASMELT plant was sufficiently encouraging that MIM decided to expand the ISASMELT treatment rate to 166 t/h by adding a second oxygen plant to allow higher enrichment of the lance air. As a result, by late 2001 it had achieve a peak rate of 190 t/h of concentrate, and the smelter produced a peak annual total of 240,000 t of anode copper. At that time, the Mount Isa copper smelter, together with its copper refinery in Townsville, was among the lowest cost copper smelters in the world.\n\nLance life is typically two weeks, with lance changes taking 30 to 40 minutes, and repairs usually being limited to replacement of the lance tips.\n\nIn 2006, MIM commissioned a second rotary holding furnace that operates in parallel with the existing holding furnace.\n\nSterlite Industries (\"Sterlite\"), now a subsidiary of Vedanta Resources plc (\"Vedanta\"), built a copper smelter in Tuticorin using an ISASMELT furnace and Peirce-Smith converters. The smelter was commissioned in 1996 and was designed to produce 60,000 t/y of copper (450,000 t/y of copper concentrate), but by increasing the oxygen content of the lance air and making modifications to other equipment, the ISASMELT furnace feed rate was increased to the point where the smelter was producing 180,000 t/y of copper.\n\nSterlite commissioned a new ISASMELT furnace in May 2005 that was designed to treat 1.3 million t/y of copper concentrate, and the smelter’s production capacity was expanded to 300,000 t/y of copper. The new plant reached its design capacity, measured over a three-month period, six months after it started treating its first feed. Vedanta’s website states that the new ISASMELT furnace was successfully ramped up \"in a record period of 45 days\".\n\nSince then Sterlite has decided to further expand its copper production by installing a third ISASMELT smelter and new refinery using IsaKidd technology. The new smelter will have a design capacity of 1.36 million t/y of copper concentrate (containing 400,000 t/y of copper), processed through a single ISASMELT furnace.\n\nIn the 1990s, the Chinese government decided to increase the efficiency of the Chinese economy and reduce the environmental effects of heavy industry by modernising plants. As a response, the Yunnan Copper Corporation Limited (\"YCC\") upgraded its existing plant, which was based on a sinter plant and an electric furnace, with a copper ISASMELT furnace. As with the Miami smelter, the electric furnace was converted from smelting duty to separation of matte and slag and providing matte surge capacity for the converters, and again, the small footprint of the ISASMELT furnace was very important in retrofitting it to the existing smelter.\n\nThe YCC ISASMELT plant had a design capacity of 600,000 dry t/y of copper concentrate and started smelting concentrate on 15 May 2002. YCC placed a lot of emphasis on training its operators, sending people to Mount Isa for training over a seven-month period during 2001 ahead of the ISASMELT commissioning. The total cost of the smelter modernisation program, including the ISASMELT furnace, was 640 million yuan (approximately US$80 million) and the smelter’s concentrate treatment rate increased from 470,000 t/y to 800,000 t/y as a result.\n\nThe transfer of operating knowledge from MIM to YCC was sufficient for the first ISASMELT furnace refractory lining to last for two years, a marked improvement on the life of the initial lining of other plants.\n\nYCC described the modernisation project as \"a great success, achieving all that was expected.\" Energy consumption per tonne of blister copper produced decreased by 34% as a result of installing the ISASMELT furnace, and YCC estimated that during the first 38 months of operation, it saved approximately US$31.4 million through reduced energy costs alone, giving the modernisation a very short payback by industry standards.\n\nIn 2004, YCC’s management was presented with awards for Innovation in Project Management and the National Medal for High Quality Projects by the Chinese government to mark the success of the smelter modernisation project.\n\nXstrata subsequently licensed YCC to build three more ISASMELT plants, one in Chuxiong in Yunnan Province, China to treat 500,000 t/y of copper concentrate, one in Liangshan in Sichuan Province, China and the other in Chambishi in Zambia to treat 350,000 t/y of concentrate. Chuxiong and Chambishi were commissioned in 2009. Liangshan was commissioned in 2012.\n\nMopani Copper Mines (\"Mopani\") was part of Zambia Consolidated Copper Mines Limited until it was privatised in 2000. It owns the Mufulira smelter, which operated with an electric furnace with a nominal capacity of 420,000 t/y of copper concentrate (180,000 t/y of new copper). Mopani decided to install a copper ISASMELT plant that could treat 850,000 t/y of copper concentrate, including a purpose-designed electric matte settling furnace to separate the ISASMELT matte and slag and also return slag from the smelter’s Peirce-Smith converters.\n\nBefore committing to the ISASMELT technology, Mopani considered the following process options:\nMopani considered electric furnaces unproven at the proposed concentrate feed rates, and the low sulfur dioxide concentration in the waste gas would make its capture very expensive. Flash furnaces and the Mitsubishi process were excluded because:\nMopani excluded the Teniente converter and Noranda reactor because of the poor performance of the Teniente converter at the other Zambian smelter operating at the time and because of \"the relatively inexperienced technical resources available at the time\".\n\nMopani selected ISASMELT technology over Ausmelt technology after visits to operating plants in Australia, the United States of America, and China. The total cost of the project was US$213 million. The first feed was smelted in September 2006.\n\nThe Southern Peru Copper Corporation (\"SPCC\") is a subsidiary of the Southern Copper Corporation (\"SCC\"), one of the world’s largest copper companies and currently 75.1% owned by Grupo México. Grupo México acquired the shares in SPCC when it bought ASARCO in November 1999\n\nIn the 1990s, SPCC was seeking to modernise its smelter at Ilo in southern Peru as part of 1997 commitment to the Peruvian government to capture at least 91.7% of the sulfur dioxide generated in its smelting operations by January 2007. It initially selected flash smelting technology to replace its reverberatory furnaces, at a cost of almost US$1 billion; however, one of the first actions following Grupo México’s acquisition of ASARCO was to review the proposed Ilo smelter modernisation plans.\n\nSix different technologies were evaluated during the review. These were:\nThe ISASMELT technology was selected as a result of the review, resulting in a reduction in the capital cost of almost 50% and was also the alternative with the lowest operating costs.\n\nThe plant was commissioned in February 2007. In June 2009, the plant had an average feed rate of 165.2 t/h of concentrate and 6.3 t/h of reverts (cold copper-bearing materials that arise from spillage and accretions in the pots used to transport matte or other molten materials).\n\nSPCC has reported a cost of approximately $600 million for the smelter modernization.\n\nKazzinc selected the copper ISASMELT process for its Ust-Kamenogorsk metallurgical complex. It is designed to treat 290,000 t/y of copper concentrate and was commissioned in 2011. A projected capital cost for the smelter and refinery in 2006 was US$178 million.\n\nIn the fourth quarter of 2011, the First Quantum Minerals board approved the construction of an ISASMELT-based smelter at Kansanshi in Zambia. The smelter is to process 1.2 million t/y of copper concentrate to produce over 300,000 t/y of copper and 1.1 million t/y of sulfuric acid as a by-product. Construction is expected to be completed by mid-2014, and the capital cost is estimated at US$650 million. The estimated operating cost was given as US$69 per tonne of concentrate.\n\nThe Kansanshi copper smelter project is estimated to be worth US$340–500 million per year in reduced concentrate freight costs, export duties and sulfuric acid costs.\n\nIn addition to treating copper concentrates, ISASMELT furnaces have also been built to treat secondary (scrap) copper materials.\n\nIn the early 1990s, technical personnel from the then Union Miniére worked with MIM Holdings personnel to develop an ISASMELT-based process to treat scrap materials and residues containing copper and lead. Union Miniére operated a smelter at Hoboken, near Antwerpen in Belgium, that specialised in recycling scrap non-ferrous materials. The test work program was undertaken using an ISASMELT test rig at MIM Holdings’ lead refinery, Britannia Refined Metals, at Northfleet in the United Kingdom.\n\nA demonstration plant was designed by MIM Holdings personnel and operated for several months at the Hoboken smelter site. The new smelter was commissioned in the final quarter of 1997 and in 2007 was treating up to 300,000 t/y of secondary materials. The installation of the ISASMELT furnace replaced \"a large number of unit processes\" and substantially reduced operating costs at the Hoboken smelter.\n\nUmicore’s Hoboken plant uses a two-step process in a single furnace. The first step involves the oxidation of the feed to form a copper matte and a lead-rich slag. The slag is then tapped and the remaining copper matte is then converted to blister copper. The lead-rich slag is subsequently reduced in a blast furnace to produce lead metal, while the copper is refined and the contained precious metals recovered.\n\nThe then Hüttenwerke Kayser smelter at Lünen in Germany installed an ISASMELT plant in 2002 to replace three blast furnaces and one Peirce-Smith converter used for smelting scrap copper. The company was subsequently bought by Norddeutsche Affinerie AG, which in turn became Aurubis AG.\n\nThe process used at the Lünen smelter involves charging the furnace with copper residues and scrap containing between 1 and 80% copper and then melting it in a reducing environment. This produces a \"black copper phase\" and a low-copper silica slag. Initially the black copper was converted to blister copper in the ISASMELT furnace. However, in 2011 the smelter was expanded as part of the \"KRS Plus\" project. A top-blown rotary converter is now used to convert the black copper and the ISASMELT furnace runs continuously in smelting mode.\n\nThe installation of the ISASMELT furnace increased the overall copper recovery in the plant by reducing losses to slag, reduced the number of furnaces in operation, decreased the waste gas volume, and decreased energy consumption by more than 50%. The production capacity exceeds the original design by 40%.\n"}
{"id": "32607746", "url": "https://en.wikipedia.org/wiki?curid=32607746", "title": "Imagined speech", "text": "Imagined speech\n\nImagined speech (silent speech or covert speech) is thinking in the form of sound – “hearing” one’s own voice silently to oneself, without the intentional movement of any extremities such as the lips, tongue, or hands. Logically, imagined speech has been possible since the emergence of language, however, the phenomenon is most associated with the signal processing and detection within electroencephalograph (EEG) data as well as data obtained using alternative non-invasive, brain–computer interface (BCI) devices.\n\nIn 2008, the US Defense Advanced Research Projects Agency (DARPA) provided a $4 million grant to the University of California (Irvine), with the intent of providing a foundation for synthetic telepathy. According to DARPA, the project “will allow user-to-user communication on the battlefield without the use of vocalized speech through neural signals analysis. The brain generates word-specific signals prior to sending electrical impulses to the vocal cords. These imagined speech signals would be analyzed and translated into distinct words allowing covert person-to-person communication.”\n\nDARPA's program outline has three major goals:\n\nThe process for analyzing subjects' \"silent speech\" is composed of recording subjects’ brain waves, and then using a computer to process the data and determine the content of the subjects' \"covert speech\".\n\nSubject neural patterns (brain waves) can be recorded using BCI devices; currently, use of non-invasive devices, specifically the EEG, is of greater interest to researchers than invasive and partially invasive types. This is because non-invasive types pose the least risk to subject health; EEG's have attracted the greatest interest because they offer the most user-friendly approach in addition to having far less complex instrumentation than that of functional magnetic resonance imaging (fMRI’s), another commonly used non-invasive BCI.\n\nThe first step in processing non-invasive data is to remove artifacts such as eye movement and blinking, as well as other electromyographic activity. After artifact-removal, a series of algorithms is used to translate raw data into the imagined speech content. Processing is also intended to occur in real-time—the information is processed as it is recorded, which allows for near-simultaneous viewing of the content as the subject imagines it.\n\nPresumably, “thinking in the form of sound” recruits auditory and language areas whose activation profiles may be extracted from the EEG, given adequate processing. The goal is to relate these signals to a template that represents “what the person is thinking about”. This template could for instance be the acoustic envelope (energy) timeseries corresponding to sound if it were physically uttered. Such mapping from EEG to stimulus is an example of neural decoding techniques.\n\nA major problem however is the many variations that the very same message can have under diverse physical conditions (speaker or noise, for example). Hence one can have the same EEG signal, but it is uncertain, at least in acoustic terms, what stimulus to map it to. This in turn makes it difficult to train the relevant decoder.\n\nThis process could instead be approached using higher-order (‘linguistic’) representations of the message. The mappings to such representations are non-linear and can be heavily context-dependent, therefore further research may be necessary. Nevertheless, it is known that an 'acoustic' strategy can still be maintained by pre-setting a “template” by making it known to the listener exactly what message to think about, even if passively, and in a non-explicit form. In these circumstances it is possible to partially decode the acoustic envelope of speech message from neural timeseries if the listener is induced to think in the form of sound.\n\nIn detection of other imagined actions, such as imagined physical movements, greater brain activity occurs in one hemisphere over the other. This presence of asymmetrical activity acts as a major aid in identifying the subject's imagined action. In imagined speech detection however, equal levels of activity commonly occur in both the left and right hemispheres simultaneously. This lack of lateralization demonstrates a significant challenge in analyzing neural signals of this type.\n\nAnother unique challenge is a relatively low signal-to-noise ratio (SNR) in the recorded data. A SNR represents the amount of meaningful signals found in a data set, compared to the amount of arbitrary or useless signals present in the same set. Artifacts present in EEG data are just one of many significant sources of noise.\n\nTo further complicate matters, the relative placement of EEG electrodes will vary amongst subjects. This is because the anatomical details of people's heads will differ; therefore, the signals recorded will vary in each subject, regardless of individuals-specific imagined speech characteristics.\n\nForemost, EEG use requires meticulously securing electrodes onto a subject’s head; the electrodes are connected through a web of wires tethered to a CPU. So, creating an everyday, user-friendly communicator requires a further development of compacting EEGs and their signal-processors into an easy-to-use, lightweight, and fashionable device. (e.g. a headband with Wi-Fi or Bluetooth)\n\nIn addition, current detection methods cannot distinguish between more than two signals (i.e. /ba/ or /ku/, yes or no). Therefore, a significant advancement in EEG processing algorithms is still required. This may suggest that an overall understanding of human-information-processing patterns must be better understood first, as it would offer insight into classifying word-specific neural-patterns common to all people.\n"}
{"id": "3223340", "url": "https://en.wikipedia.org/wiki?curid=3223340", "title": "Imputation (law)", "text": "Imputation (law)\n\nIn law, the principle of imputation or attribution underpins the concept that \"ignorantia juris non excusat\"—ignorance of the law does not excuse. All laws are published and available for study in all developed states. The content of the law is imputed to all persons who are within the jurisdiction, no matter how transiently.\n\nThis fiction tries to negate the unfairness of someone avoiding liability for an act or omission by simply denying knowledge of the law. The principle also arises in specific areas of law, such as criminal law and commercial law, to describe the need for the law to hold a person liable, even when they may not have known the particular circumstances that caused another person to sustain loss or damage.\n\nTo incur liability for a crime, a person must have both committed a prohibited act (the \"actus reus\", which must be willed: see automatism) and have had an appropriate mental element (the \"mens rea\") at the relevant time (see the technical requirement for concurrence). A key component of the \"mens rea\" is any knowledge that the alleged criminal might have had. For these purposes, knowledge can be both actual and constructive—i.e., the court can impute knowledge where appropriate.\n\nThere is no problem when the alleged criminal actually intended to cause the particular harm. Things are more difficult when the defendant denies actual knowledge. When evaluating behavior, the legal process assumes the defendant was aware of their immediate physical surroundings and understood practical cause and effect. A \"mens rea\" is imputed when a person with reasonable foresight in the same circumstances would have foreseen that the \"actus reus\" would occur. This prevents a person from raising a defence based on willful blindness (note that in the United States, wilful blindness has a slightly different meaning).\n\nA problem arises when the defendant is a corporation. By its nature, a fictitious person can only act through the human agency of the natural persons that it employs. Equally, it has no mind to constitute the \"mens rea\". Hence, the notion of vicarious liability for companies and other business entities exclusively depends on the ability to impute knowledge.\n\nThe test is one of identification. If the natural person who acts can be \"identified with the mind of the company\" when performing the actions forming the \"actus reus\", all the relevant mental elements will be imputed to the company. This test, sometimes termed the \"alter ego\" test, is objective and cannot be distracted by the job title or description formally held by the human agent. This prevents evasion of liability by the simple expedient of naming the real director of affairs as the janitor.\n\nHowever, not all actions trigger this transference. When acting, the human agent identified as the mind must be promoting the company's interests in some practical way. If they are engaged in an entirely personal activity—e.g., attacking a fellow employee out of anger or stealing from the company—the courts do not impute the relevant \"mens rea\" to the company.\n\nIn the United States, the courts use a three-pronged test to determine whether a corporation is vicariously liable for the acts of its employees: \n\nA standard example of imputation arises through the principle of \"joint endeavour\". Where two or more people embark on a joint exercise, they are equally liable for everything that happens during the execution of their plan. For this purpose, joint principals are treated as knowing everything that happens, whether they were present or not. The requisite \"mens rea\" formed by one is imputed to the others to enable a conviction. For example, suppose that a gang conspire to rob a bank. One remains outside in the car to ensure a quick escape. If the others kill a guard inside the bank, the driver is jointly liable for the homicide.\n\nIn the majority of agency situations, Agents must be allowed some degree of discretion in the conduct of routine transactions. Hence, there is no need to seek specific authorisation for every deal or detail within a deal. But, when the Agent acts with actual or apparent authority, all the Agent's knowledge will be imputed to the Principal. If Principals were allowed to hide behind their agents' ignorance, mistakes, or failures to communicate, they could achieve better results than if they acted personally. For example, if the particular deal turned out well, the Principal could adopt the transaction—if it turned out badly, the Principal could disavow it. If not for imputation, there would be a perverse incentive to conduct business through Agents rather than personally. Consequently, the Principal cannot exploit ignorance to advantage by instructing the Agent to withhold key information, or by appointing an Agent known to be secretive.\n\nThis rule in favour of imputation relates to the generality of the duties an Agent owes to a Principal, in particular the Agent's duty to communicate material facts to the Principal. Since the purpose of the law is to offer protection to Third Parties who act in good faith, it is reasonable to allow them to believe that, in most cases, the Agents have fulfilled this duty. After all, the Principal selects the Agents and has the power to control their actions both through express instructions and incentives intended to influence their behaviour which will include laying down routines for how Agents should handle information, and the extent to which Agents will be rewarded for transmitting information of commercial value. The result is a form of strict liability in which the legal consequences of an Agent's acts or omissions are attributed to a Principal even when the Principal was without fault in appointing or supervising the Agent.\n\nIn English law, a corporation can only act through its employees and agents so it is necessary to decide in which circumstances the law of agency or vicarious liability will apply to hold the corporation liable in tort for the frauds of its directors or senior officers. If liability for the particular tort requires a state of mind, then to be liable, the director or senior officer must have that state of mind and it must be attributed to the company.\n\nIn \"Meridian Global Funds Management Asia Ltd v Securities Commission\" [1995] 2 AC 500, two employees of the company, acting within the scope of their authority but unknown to the directors, used company funds to acquire some shares. The question was whether the company knew, or ought to have known that it had acquired those shares. The Privy Council held that it did. Whether by virtue of their actual or ostensible authority as agents acting within their authority (see \"Lloyd v Grace, Smith & Co.\" [1912] AC 716) or as employees acting in the course of their employment (see \"Armagas Limited v Mundogas S.A.\" [1986] 1 AC 717), their acts and omissions and their knowledge could be attributed to the company, and this could give rise to liability as joint tortfeasors where the directors have assumed responsibility on their own behalf and not just on behalf of the company.\n\nSo, if a director or officer is expressly authorised represent a particular class on behalf of the company, and makes a fraudulent representation that causes loss to a Third Party, the company is liable, even though the representation was an improper way of doing what he was authorised to do. The extent of authority is a question fact and is significantly more than the fact of an employment that gave the employee the opportunity to carry out the fraud.\n\nIn \"Panorama Developments (Guildford) Ltd v Fidelis Furnishing Fabrics Ltd\" [1971] 2 QB 711, a company secretary fraudulently hired cars for his own use without the managing director knowing. A company secretary routinely enters into contracts in the company's name and has administrative responsibilities that would give apparent authority to hire cars. Hence, the company was liable.\n\nDemott, Deborah A. \"When is a Principal Charged with an Agent's Knowledge?\" 13 \"Duke Journal of Comparative & International Law\". 291\n"}
{"id": "40649292", "url": "https://en.wikipedia.org/wiki?curid=40649292", "title": "Jan Sangharsh Manch", "text": "Jan Sangharsh Manch\n\nJan Sangharsh Manch is a voluntary civil rights organisation established in Gujarat, India. It was founded by Mukul Sinha, Senior Advocate and Trade Union Leader and his wife Nirjhari Sinha, human rights activist. The group has fought to expose the complicity of the former Gujarat Chief Minister and current Prime Minister of India Narendra Modi and his ministers and police in the communal pogrom of 2002. Their judicial struggle has also exposed the complicity of the cabinet minister Amit Shah in the fake encounters from 2002-2007, thereby pointing to the Modi government's hand in these murders. Jan Sangharsh Manch represented victims of the 2002 Gujarat violence in the Shah-Nanavati inquiry. The organisation has also fought for justice to the families of the victims in the fake encounter cases and exposed claims of the police and Modi's government branding them as terrorists to the public. The legal interventions by the organisation led the Supreme Court in the Sohrabuddin Sheikh case to pass the investigation from the Gujarat police to the Central Bureau of Investigation. This set a precedent as all the other cases taken up by JSM were handed over to the CBI and the investigation in all these cases established them as extrajudicial killings.\n\nJan Sangharsh Manch also defended those accused in the Godhra Train incident\n"}
{"id": "35628858", "url": "https://en.wikipedia.org/wiki?curid=35628858", "title": "John D. Dunne", "text": "John D. Dunne\n\nJohn D. Dunne (born 1961) is the Distinguished Chair in Contemplative Humanities through the Center for Investigating Healthy Minds at the University of Wisconsin-Madison. He also holds a co-appointment in the Department of East Asian Languages and Literature. Until January 2016, he was Associate Professor in the Department of Religion and the Graduate Division of Religion at Emory University. \n\nDunne went to Amherst College and received his Ph.D. in the Study of Religion from Harvard University in 1999. According to his faculty profile, his work \"focuses on Buddhist philosophy and contemplative practice, especially in dialog with Cognitive Science. His publications range from technical works on Buddhist epistemology to broader works on the nature of Buddhist contemplative practices such as Mindfulness.\" He occasionally teaches for Buddhist communities, most notably the Upaya Institute and Zen Center. He was a student of Tulku Urgyen Rinpoche and now studies with Tulku Urgyen's sons Chökyi Nyima Rinpoche and Yongey Mingyur Rinpoche.\n\nDunne is the author of \"Foundations of Dharmakīrti's Philosophy\", part of Wisdom Publications' \"Studies in Indian and Tibetan Buddhism Series,\" a study of the thought of the important 7th century Buddhist philosopher Dharmakīrti.\n"}
{"id": "10586126", "url": "https://en.wikipedia.org/wiki?curid=10586126", "title": "Jus inter gentes", "text": "Jus inter gentes\n\nJus inter gentes, is the body of treaties, U.N. conventions, and other international agreements. Originally a Roman law concept, it later became a major part of public international law. The other major part is \"jus gentium\", the Law of Nations (municipal law). \"Jus inter gentes\", literally, means \"law between the peoples\".\n\nThis is \"not\" the same as \"jus gentium\", argues Francisco Martin and his co-authors in \"International Human Rights and Humanitarian Law\" (2006), because \"jus inter gentes\" includes internationally recognized human rights.\n\n"}
{"id": "4759267", "url": "https://en.wikipedia.org/wiki?curid=4759267", "title": "Legend of the Octopus", "text": "Legend of the Octopus\n\nThe Legend of the Octopus is a sports tradition during Detroit Red Wings home playoff games involving dead octopuses thrown onto the ice rink. The origins of the activity go back to the 1952 playoffs, when a National Hockey League team played two best-of-seven series to capture the Stanley Cup. Having eight arms, the octopus symbolized the number of playoff wins necessary for the Red Wings to win the Stanley Cup. The practice started April 15, 1952, when Pete and Jerry Cusimano, brothers and storeowners in Detroit's Eastern Market, hurled an octopus into the rink of The Old Red Barn. The team swept the Toronto Maple Leafs and Montreal Canadiens en route to winning the championship. \n\nSince 1952, the practice has persisted with each passing year. In one 1995 game, fans threw 36 octopuses, including a specimen weighing . The Red Wings' unofficial mascot is a purple octopus named Al, and during playoff runs, two of these mascots were also hung from the rafters of Joe Louis Arena, symbolizing the 16 wins now needed to take home the Stanley Cup. The practice has become such an accepted part of the team's lore, fans have developed various techniques and \"octopus etiquette\" for launching the creatures onto the ice.\n\nThe octopus tradition has launched several other object-tossing moments:\n\nDuring Game 3 of the 1995 Stanley Cup Finals between the Detroit Red Wings and the New Jersey Devils, Devils fans threw a lobster, a dead fish, and other objects onto the ice.\n\nIn the 2006 Stanley Cup playoffs, during the opening-round series between the Wings and the Edmonton Oilers, an Edmonton radio host suggested throwing Alberta Beef on the ice before the game. Oilers fans continued throwing steaks, even at away games, resulting in several arrests at the away cities.\n\nIn the 2002–03 season, the Nashville Predators fans began throwing catfish onto their home ice, in response to the Red Wings tradition. The first recorded instance occurred on October 26, 2002 in a game between the Red Wings and the Predators. Jessica Hanley, who helps clean the ice in the Gaylord Entertainment Center, told the press: \"They are so gross. They're huge, they're heavy, they stink and they leave this slimy trail on the ice. But, hey, if it's good for the team, I guess we can deal with it.\" This tradition continued in Game 3 of the 2008 Western Conference Quarterfinals matchup between the Detroit Red Wings and the Nashville Predators when Predator fans threw four catfish onto the ice.\n\nDuring Game 4 of the 2007 Stanley Cup Western Conference Semifinals between the Detroit Red Wings and the San Jose Sharks, a Sharks fan threw a 3-foot leopard shark onto the ice at the HP Pavilion at San Jose after the Sharks scored their first goal with 2 minutes left in the first period.\n\nDuring the 2008 Stanley Cup Finals, in which the Red Wings defeated the Pittsburgh Penguins, seafood wholesalers in Pittsburgh, led by Wholey's Fish Market, began requiring identification from customers who purchased octopuses, refusing to sell to buyers from Michigan. This also took place in the lead up to the 2017 Stanley Cup Finals with markets refusing to sell catfish to Tennessee residents.\n\nIn Game 1 of the 2010 Western Conference Quarterfinals between the Detroit Red Wings and the Phoenix Coyotes, a rubber snake was thrown onto the ice after a goal by the Coyotes' Keith Yandle.\n\nIn Game 2 of the 2010 Western Conference Semifinals between the Detroit Red Wings and San Jose Sharks, a small shark was tossed onto the ice with an octopus inside its mouth.\n\nIn Game 3 of the 2017 Western Conference Finals between the Anaheim Ducks and the Nashville Predators, a Predators fan threw a skinned duck on the ice. \n\nIn Game 1 of the 2017 Stanley Cup Finals between the Pittsburgh Penguins and the Nashville Predators, a fan threw a catfish on the ice in the second period, and was escorted out of the arena.\n\nAl Sobotka, the head ice manager at Little Caesars Arena and one of the two Zamboni drivers, is the person who retrieves the thrown octopuses from the ice. When the Red Wings played at Joe Louis Arena, he was known to twirl an octopus above his head as he walked across the ice rink to the Zamboni entrance. On April 19, 2008, NHL director of hockey operations Colin Campbell sent the Red Wings a memo that forbade Zamboni drivers from cleaning up any octopuses thrown onto the ice and imposed a $10,000 fine for violating the mandate. The linesmen were instead instructed to perform any clean-up duties. In an email to the \"Detroit Free Press\", NHL spokesman Frank Brown justified the ban because \"matter flies off the octopus and gets on the ice\" when Sobotka swings it above his head. In an article describing the effects of the new rule, the \"Detroit Free Press\" dubbed the NHL's prohibition as \"Octopus-gate\". By the beginning of the third round of the 2008 Playoffs, the NHL loosened the ban to allow for the octopus twirling to take place at the Zamboni entrance.\n\n"}
{"id": "1461082", "url": "https://en.wikipedia.org/wiki?curid=1461082", "title": "Letter to the editor", "text": "Letter to the editor\n\nA letter to the editor (sometimes abbreviated LTTE or LTE) is a letter sent to a publication about issues of concern from its readers. Usually, letters are intended for publication. In many publications, letters to the editor may be sent either through conventional mail or electronic mail.\n\nLetters to the editor are most frequently associated with newspapers and newsmagazines. However, they are sometimes published in other periodicals (such as entertainment and technical magazines), and radio and television stations. In the latter instance, letters are sometimes read on the air (usually, on a news broadcast or on talk radio). In that presentation form, it can also be described as viewer mail or listener mail, depending on the medium.\n\nIn academic publishing, letters to the editor of an academic journal are usually open postpublication reviews of a paper, often critical of some aspect of the original paper. The authors of the original paper sometimes respond to these with a letter of their own. Controversial papers in mainstream journals often attract numerous letters to the editor. Good citation indexing services list the original papers together with all replies. Depending on the length of the letter and the journal's style, other types of headings may be used, such as peer commentary. There are some variations on this practice. Some journals request open commentaries as a matter of course, which are published together with the original paper, and any authors' reply, in a process called open peer commentary. The introduction of the \"epub ahead of print\" practice in many journals now allows unsolicited letters to the editor (and authors' reply) to appear in the same print issue of the journal, as long as they are sent in the interval between the electronic publication of the original paper and its appearance in print.\n\nThe subject matter of letters to the editor vary widely. However, the most common topics include:\n\n\nLTEs always have been a feature of American newspapers. Much of the earliest news reports and commentaries published by early-American newspapers were delivered in the form of letters, and by the mid-18th century, LTEs were a dominant carrier of political and social discourse. Many influential essays about the role of government in matters such as personal freedoms and economic development took the form of letters — consider Cato's Letters or Letters from a Farmer in Pennsylvania, which were widely reprinted in early American newspapers. Through the 19th century, LTEs were increasingly centralized near the editorials of newspapers, so that by the turn of the 20th century LTEs had become permanent fixtures of the opinion pages.\n\nModern LTE forums differ little from those earlier counterparts. A typical forum will include a half-dozen to a dozen letters (or excerpts from letters). The letters chosen for publication usually are only a sample of the total letters submitted, with larger-circulation publications running a much smaller percentage of submissions and small-circulation publications running nearly all of the relatively few letters they receive. Editors generally read all submissions, but in general most will automatically reject letters that include profanity, libelous statements, personal attacks against individuals or specific organizations, that are unreasonably long (most publications suggest length limits ranging from 200 to 500 words) or that are submitted anonymously.\n\nThe latter criterion is a fairly recent development in LTE management. Prior to the Cold War paranoia of the mid-20th century, anonymous LTEs were common; in fact, the right to write anonymously was central to the free-press/free-speech movement (as in the 1735 trial against John Peter Zenger, which started with an anonymous essay). By the 1970s, editors had developed strong negative attitudes toward anonymous letters, and by the end of the 20th century, about 94 percent of newspapers automatically rejected anonymous LTEs. Some newspapers in the 1980s and '90s created special anonymous opinion forums that allowed people to either record short verbal opinions via telephone (which were then transcribed and published) or send letters that were either unsigned or where the author used a pseudonym. Although many journalists derided the anonymous call-in forums as unethical (for instance, someone could make an unfounded opinion without worry of the consequences or having to back the comment up with hard facts), defenders argued that such forums upheld the free-press tradition of vigorous, uninhibited debate similar to that found in earlier newspapers.\n\nAlthough primarily considered a function of print publications, LTEs also are present in electronic media. In broadcast journalism, LTEs have always been a semi-regular feature of 60 Minutes and the news programs of National Public Radio. LTE's also are widespread on the Internet in various forms.\n\nBy the early 21st century, the Internet had become a delivery system for many LTEs via e-mail and news Web sites (in fact, after several envelopes containing a powder suspected to be anthrax were mailed to lawmakers and journalists, several news organizations announced they would only accept e-mail LTEs). Because the Internet broadly expanded the potential readership of editorials and opinion columns at small newspapers, their controversial editorials or columns could sometimes attract much more e-mail than they were used to handling — so much so that a few newspapers had their e-mail servers crash.\n\nEditors are a frequent target of letter-writing campaigns, also called “astroturfing,” or “fake grass-roots” operations where sample letters are distributed on the Internet or otherwise, to be copied or rewritten and submitted as personal letters.\n\nAlthough LTE management gets little attention in trade journals, one organization, the National Conference of Editorial Writers, often includes essays on LTE management in its newsletter, The Masthead, and at its annual meetings. Among the NCEW's strongest champions for LTEs was Ronald D. Clark of the St. Paul Pioneer Press, who wrote, \"Consider letters as a barometer of how well (you are) engaging readers or viewers. The more you receive, the more you're connecting. The fewer you receive, the stronger the sign that you're putting the masses to sleep.\"\n\nOn the other hand many editors will allow the publication of anonymous letters where the details of name and address of the author are not printed, but are disclosed to the editor. This can promote a debate of issues that are personal, contentious or embarrassing, yet are of importance to raise in a public debate.\n\nSometimes a letter to the editor in a local newspaper, such as the Dear IRS letter written by Ed Barnett to the \"Wichita Falls Times Record News\" in Wichita Falls, Texas, will end up receiving attention from the national media.\n\nSubmitting a letter under a false name to shill in support or to criticize an opponent can have significant consequences. For example, Canadian politician Paul Reitsma's career ended in scandal in 1999, after he signed letters addressed to newspapers as \"Warren Betanko\" praising himself and attacking his political opponents. His local paper wrote a front-page story under the headline of \"MLA Reitsma is a liar and we can prove it.\" \n\nIn 1966 Israel, the Herut Party of then opposition leader Menachem Begin was shaken by scandal when letters sharply attacking Begin, which had been published in major dailies, were proven to have been authored by Begin's rivals for the party leadership and sent to the papers under various aliases and false names. As a result, the rivals were discredited and eventually expelled from the party, which helped buttress Begin's leadership position up to win the 1977 general elections and become Prime Minister of Israel.\n\n\n"}
{"id": "10849414", "url": "https://en.wikipedia.org/wiki?curid=10849414", "title": "Lever rule", "text": "Lever rule\n\nThe lever rule is a tool used to determine the mole fraction (\"x\") or the mass fraction (\"w\") of each phase of a binary equilibrium phase diagram. It can be used to determine the fraction of liquid and solid phases for a given binary composition and temperature that is between the liquidus and solidus line.\n\nIn an alloy or a mixture with two phases, α and β, which themselves contain two elements, A and B, the lever rule states that the mass fraction of the α phase is\n\nwhere\nall at some fixed temperature or pressure.\n\nSuppose an alloy at an equilibrium temperature \"T\" consists of formula_4 mass fraction of element B. Suppose also that at temperature \"T\" the alloy consists of two phases, α and β, for which the α consists of formula_2, and β consists of formula_3. Let the mass of the α phase in the alloy be formula_8 so that the mass of the β phase is formula_9, where formula_10 is the total mass of the alloy.\n\nBy definition, then, the mass of element B in the α phase is formula_11, while the mass of element B in the β phase is formula_12. Together these two quantities sum to the total mass of element B in the alloy, which is given by formula_13. Therefore,\n\nBy rearranging, one finds that\n\nThis final fraction is the mass fraction of the α phase in the alloy.\n\nBefore any calculations can be made, a \"tie line\" is drawn on the phase diagram to determine the mass fraction of each element; on the phase diagram to the right it is line segment LS. This tie line is drawn horizontally at the composition's temperature from one phase to another (here the liquid to the solid). The mass fraction of element B at the liquidus is given by \"w\" (represented as \"w\" in this diagram) and the mass fraction of element B at the solidus is given by \"w\" (represented as \"w\" in this diagram). The mass fraction of solid and liquid can then be calculated using the following lever rule equations:\n\nwhere \"w\" is the mass fraction of element B for the given composition (represented as \"w\" in this diagram).\n\nThe numerator of each equation is the original composition we are interested in +/- the opposite \"lever arm\". That is if you want the mass fraction of solid then take the difference between the liquid composition and the original composition. And then the denominator is the overall length of the arm so the difference between the solid and liquid compositions. If you're having difficulty realising why this is so, try visualising the composition when \"w\" approaches \"w\". Then the liquid concentration will start increasing.\n\nThere is now more than one two-phase region. The tie line drawn is from the solid alpha to the liquid and by dropping a vertical line down at these points the mass fraction of each phase is directly read off the graph, that is the mass fraction in the x axis element. The same equations can be used to find the mass fraction of alloy in each of the phases, i.e. w is the mass fraction of the whole sample in the liquid phase.\n"}
{"id": "600366", "url": "https://en.wikipedia.org/wiki?curid=600366", "title": "Libertine", "text": "Libertine\n\nA libertine is one devoid of most moral principles, a sense of responsibility, or sexual restraints, which are seen as unnecessary or undesirable, especially one who ignores or even spurns accepted morals and forms of behaviour sanctified by the larger society. Libertinism is described as an extreme form of hedonism. Libertines put value on physical pleasures, meaning those experienced through the senses. As a philosophy, libertinism gained new-found adherents in the 17th, 18th, and 19th centuries, particularly in France and Great Britain. Notable among these were John Wilmot, 2nd Earl of Rochester and the Marquis de Sade.\n\nThe word \"Libertine\" was originally coined by John Calvin to negatively describe opponents of his policies in Geneva, Switzerland. This group, led by Ami Perrin, argued against Calvin's \"insistence that church discipline should be enforced uniformly against all members of Genevan society\". Perrin and his allies were elected to the town council in 1548, and \"broadened their support base in Geneva by stirring up resentment among the older inhabitants against the increasing number of religious refugees who were fleeing France in even greater numbers\". By 1555, Calvinists were firmly in place on the Genevan town council, so the Libertines, led by Perrin, responded with an \"attempted coup against the government and called for the massacre of the French. This was the last great political challenge Calvin had to face in Geneva\".\n\nDuring the 18th and 19th centuries, the term became more associated with debauchery. Charles-Maurice de Talleyrand wrote that Joseph Bonaparte \"sought only life's pleasures and easy access to libertinism\" while on the throne of Naples.\n\n\"Les Liaisons dangereuses\" (\"Dangerous Liaisons\", 1782), an epistolary novel by Pierre Choderlos de Laclos, is a trenchant description of sexual libertinism. Wayland Young argues:\nAgreeable to Calvin's emphasis on the need for uniformity of discipline in Geneva, Samuel Rutherford (Professor of Divinity in the University of St. Andrews, and Christian minister in 17th Century Scotland) offered a rigorous treatment of \"Libertinism\" in his polemical work \"A Free Disputation against pretended Liberty of Conscience\" (1649).\n\n\"A Satyr Against Reason and Mankind\" is a poem by John Wilmot, 2nd Earl of Rochester which addresses the question of the proper use of reason, and is generally assumed to be a Hobbesian critique of rationalism. The narrator subordinates reason to sense. It is based to some extent on Boileau's version of Juvenal's eighth or fifteenth satire, and is also indebted to Hobbes, Montaigne, Lucretius and Epicurus, as well as the general libertine tradition. Confusion has arisen in its interpretation as it is ambiguous as to whether the speaker is Rochester himself, or a satirised persona. It criticises the vanities and corruptions of the statesmen and politicians of the court of Charles II.\n\nThe libertine novel was an 18th century literary genre of which the roots lay in the European but mainly French libertine tradition. The genre effectively ended with the French Revolution. Themes of libertine novels were anti-clericalism, anti-establishment and eroticism.\n\nAuthors include Claude Prosper Jolyot de Crébillon (\"Les Égarements du cœur et de l'esprit\", 1736; \"Le Sopha, conte moral\", 1742), Denis Diderot (\"Les bijoux indiscrets\", 1748), Marquis de Sade (\"L'Histoire de Juliette\", 1797–1801), Choderlos de Laclos (\"Les Liaisons dangereuses\", 1782), John Wilmot (\"Sodom, or the Quintessence of Debauchery\", 1684).\n\nOther famous titles are \"Histoire de Dom Bougre, Portier des Chartreux\" (1741) and \"Thérèse Philosophe\" (1748).\n\nPrecursors to the libertine writers were Théophile de Viau (1590–1626) and Charles de Saint-Evremond (1610–1703), who were inspired by Epicurus and the publication of Petronius.\n\nRobert Darnton is a cultural historian who has covered this genre extensively.\n\nCritics have been divided as to the literary merits of William Hazlitt's \"Liber Amoris\", a deeply personal account of frustrated Lolita-like love that is quite unlike anything else Hazlitt ever wrote. Wardle suggests that it was compelling but marred by sickly sentimentality, and also proposes that Hazlitt might even have been anticipating some of the experiments in chronology made by later novelists.\n\nOne or two positive reviews appeared, such as the one in the \"Globe\", 7 June 1823: \"The \"Liber Amoris\" is unique in the English language; and as, possibly, the first book in its fervour, its vehemency, and its careless exposure of passion and weakness—of sentiments and sensations which the common race of mankind seek most studiously to mystify or conceal—that exhibits a portion of the most distinguishing characteristics of Rousseau, it ought to be generally praised\". Dan Cruickshank in his book \"London's Sinful Secret\" summarized Hazlitt's infatuation stating: \"Decades after her death Batsy (Careless) still haunted the imagination of the essayist William Hazlitt, a man who lodged near Covent Garden during the 1820s, where he became unpleasantly intimate with the social consequences of unconventional sexual obsession that he revealed in his \"Liber Amoris\" of 1823, in which he candidly confessed to his infatuation with his landlord's young daughter.\"\n\nDuring the Baroque era in France, there existed a freethinking circle of philosophers and intellectuals who were collectively known as \"libertinage érudit\" and which included Gabriel Naudé, Élie Diodati and François de La Mothe Le Vayer. The critic Vivian de Sola Pinto linked John Wilmot, 2nd Earl of Rochester's libertinism to Hobbesian materialism.\n\nSome notable libertines include:\n"}
{"id": "18090043", "url": "https://en.wikipedia.org/wiki?curid=18090043", "title": "List of discrete event simulation software", "text": "List of discrete event simulation software\n\nThis is a list of notable discrete event simulation software.\n\n"}
{"id": "25236027", "url": "https://en.wikipedia.org/wiki?curid=25236027", "title": "Mars to Stay", "text": "Mars to Stay\n\nMars to Stay missions propose astronauts sent to Mars for the first time should intend to stay. Unused emergency return vehicles would be recycled into settlement construction as soon as the habitability of Mars becomes evident to the initial pioneers. Mars to Stay missions are advocated both to reduce cost and to ensure permanent settlement of Mars. Among many notable Mars to Stay advocates, former Apollo astronaut Buzz Aldrin has been particularly outspoken, suggesting in numerous forums \"Forget the Moon, Let’s Head to Mars!\" and, in June 2013, Aldrin promoted a manned mission \"to homestead Mars and become a two-planet species\". In August 2015, Aldrin, in association with the Florida Institute of Technology, presented a \"master plan\", for NASA consideration, for astronauts, with a \"tour of duty of ten years\", to colonize Mars before the year 2040. The Mars Underground, Mars Homestead Foundation, Mars One, and Mars Artists Community advocacy groups and business organizations have also adopted Mars to Stay policy initiatives.\n\nThe earliest formal outline of a Mars to Stay mission architecture was given at the Case for Mars VI Workshop in 1996, during a presentation by George Herbert titled \"One Way to Mars\".\n\nSince returning the astronauts from the surface of Mars is one of the most difficult parts of a Mars mission, the idea of a one-way trip to Mars has been proposed several times. Space activist Bruce Mackenzie, for example, proposed a one-way trip to Mars in a presentation \"One Way to Mars – a Permanent Settlement on the First Mission\" at the 1998 International Space Development Conference, arguing that since the mission could be done with less difficulty and expense if the astronauts were not required to return to Earth, the first mission to Mars should be a settlement, not a visit.\n\nPaul Davies, writing in the \"New York Times\" in 2004, made similar arguments. Under Davies' plan, an initial colony of four astronauts equipped with a small nuclear reactor and a couple of rover vehicles would make their own oxygen, grow food, and even initiate building projects using local raw materials. Supplemented by food shipments, medical supplies, and replacement gadgets from Earth, the colony would be indefinitely sustained.\n\nUnder Mars to Stay mission architectures, the first humans to travel to Mars would typically be in six-member teams. After this initial landing, subsequent missions would raise the number of persons on Mars to 30, thereby beginning a Martian settlement. Since the Martian surface offers all the natural resources and elements necessary to sustain a robust, mature, industrialized human settlement—unlike, for example the Moon—a permanent Martian settlement is thought to be the most effective way to ensure that humanity becomes a space-faring, multi-planet species. Through the use of digital fabricators and in vitro fertilisation it is assumed a permanent human settlement on Mars can grow organically from an original thirty to forty pioneers.\n\nA Mars to Stay mission following Aldrin's proposal would enlist astronauts in the following timeline:\n\n\nAs Aldrin has said, \"who knows what advances will have taken place. The first generation can retire there, or maybe we can bring them back.\"\n\nAn article by Dirk Schulze-Makuch (Washington State University) and Paul Davies (Arizona State University) from the book \"The Human Mission to Mars: Colonizing the Red Planet\" highlights their mission plans as:\n\nThe astronauts would be sent supplies from Earth regularly. This proposal was picked up for discussion in a number of public sources.\n\nA proposal for a one-way human settlement mission to Mars was put forward in 2012 by the Mars One, a private spaceflight project led by Dutch entrepreneur Bas Lansdorp to establish a permanent human colony on Mars.\nMars One is a Dutch not-for-profit foundation, a \"Stichting\".\nThe proposal is to send a communication satellite and path finder lander to the planet by 2018 and, after several stages, land four humans on Mars for permanent settlement in 2027. \nA new set of four astronauts would then arrive every two years. 200,000 applications were started; about 2,500 were complete enough for consideration, from which one hundred applicants have been chosen so far. Further selections are planned to narrow this down to six groups of four before training begins in 2016. It is hoped their reality television show, participant fees, and donations will generate the funding for the project.\n\nThe project has been criticized by experts as a 'scam' and as 'delusional'.\n\nIn response to feedback following the EarthLight Institute's \"Mars Colony 2030\" project at NewSpace 2012 and the announcement of Mars One, Eric Machmer proposed conjunction-class missions be planned with a bias to stay (if low gravity, radiation, and other factors present no pressing health issues), so that, if at the end of each 550-day period during a conjunction-class launch window no adverse health effects were observed, settlers would continue research and construction through another 550-day period. In the meantime, additional crews and supplies would continue to arrive, starting their own 550-day evaluation periods. Health tests would be repeated during subsequent 550-day periods until the viability of human life on Mars was proven. Once settlers determine that humans can live on Mars without negative health effects, emergency return vehicles would be recycled into permanent research bases.\n\nInitial explorers leave equipment in orbit and at landing zones scattered considerable distances from the main settlement. Subsequent missions therefore are assumed to become easier and safer to undertake, with the likelihood of back-up equipment being present if accidents in transit or landing occur.\n\nLarge subsurface, pressurized habitats would be the first step toward human settlement; as Dr. Robert Zubrin suggests in the first chapter of his book \"Mars Direct\", these structures can be built as Roman-style atria in mountainsides or underground with easily produced Martian brick. During and after this initial phase of habitat construction, hard-plastic radiation and abrasion-resistant geodesic domes could be deployed on the surface for eventual habitation and crop growth. Nascent industry would begin using indigenous resources: the manufacture of plastics, ceramics and glass could be easily achieved.\n\nThe longer-term work of terraforming Mars requires an initial phase of global warming to release atmosphere from the Martian regolith and to create a water-cycle. Three methods of global warming are described by Zubrin, who suggests they are best deployed in tandem: orbital mirrors to heat the surface; factories on the ground to pump halocarbons into the atmosphere; and the seeding of bacteria that can metabolize water, nitrogen and carbon to produce ammonia and methane (these gases would aid in global warming). While the work of terraforming Mars is on-going, robust settlement of Mars would continue.\n\nZubrin, in his 1996 book (revised 2011) \"The Case for Mars\", acknowledges any Martian colony will be partially Earth-dependent for centuries. However, Zubrin suggests Mars may be profitable for two reasons. First, it may contain concentrated supplies of metals equal to or of greater value than silver, which have not been subjected to millennia of human scavenging; it is suggested such ores may be sold on Earth for profit. Secondly, the concentration of deuterium—an extremely expensive but essential fuel for the as-yet non-existent nuclear fusion power industry—is five times greater on Mars. Humans emigrating to Mars, under this paradigm, are presumed to have an industry; it is assumed the planet will be a magnet for settlers as wage costs will be high. Because of the labor shortage on Mars and its subsequent high pay-scale, Martian civilization and the value placed upon each individual's productivity is proposed as a future engine of both technological and social advancement.\n\nIn the fifth chapter of \"Mars Direct\", Zubrin addresses the idea that radiation and zero-gravity are unduly hazardous. He claims cancer rates \"do\" increase for astronauts who have spent extensive time in space, but only marginally. Similarly, while zero-gravity presents challenges, near total recovery of musculature and immune system vitality is presumed by all Mars to Stay mission plans once settlers are on the Martian surface. Several experiments, such as the Mars Gravity Biosatellite, have been proposed to test this hypothetical assumption, but until humans have lived in Martian gravity conditions (38% of Earth's), human long-term viability in such low gravity will remain only a working assumption. Back-contamination—humans acquiring and spreading hypothetical Martian viruses—is described as \"just plain nuts\", because there are no host organisms on Mars for disease organisms to have evolved.\n\nIn the same chapter, Zubrin rejects suggestions the Moon should be used as waypoint to Mars or as a preliminary training area. \"It is ultimately much easier to journey to Mars from low Earth orbit than from the Moon and using the latter as a staging point is a pointless diversion of resources.\" While the Moon may superficially appear a good place to perfect Mars exploration and habitation techniques, the two bodies are radically different. The Moon has no atmosphere, no analogous geology and a much greater temperature range and rotational period of illumination. It is argued Antarctica, deserts of Earth, and precisely controlled chilled vacuum chambers on easily accessible NASA centers on Earth provide much better training grounds at lesser cost.\n\n\"Should the United States space program send a mission to Mars, those astronauts should be prepared to stay there,\" said Lunar astronaut Buzz Aldrin during an interview on \"Mars to Stay\" initiative. The time and expense required to send astronauts to Mars, argues Aldrin, \"warrants more than a brief sojourn, so those who are on board should think of themselves as pioneers. Like the Pilgrims who came to the New World or the families who headed to the Wild West, they should not plan on coming back home.\" The Moon is a shorter trip of two or three days, but according to Mars advocates it offers virtually no potential for independent settlements. Studies have found that Mars, on the other hand, has vast reserves of frozen water, all of the basic elements, and more closely mimics both gravitational (roughly of Earth's while the moon is ) and illumination conditions on Earth. \"It is easier to subsist, to provide the support needed for people there than on the Moon.\" In an interview with reporters, Aldrin said Mars offers greater potential than Earth's satellite as a place for habitation:\n\nA comprehensive statement of a rationale for \"Mars to Stay\" was laid out by Dr. Aldrin in a May 2009 Popular Mechanics article, as follows:\nThe agency's current Vision for Space Exploration will waste decades and hundreds of billions of dollars trying to reach the Moon by 2020—a glorified rehash of what we did 40 years ago. Instead of a steppingstone to Mars, NASA's current lunar plan is a detour. It will derail our Mars effort, siphoning off money and engineering talent for the next two decades. If we aspire to a long-term human presence on Mars—and I believe that should be our overarching goal for the foreseeable future—we must drastically change our focus. Our purely exploratory efforts should aim higher than a place we've already set foot on six times. In recent years my philosophy on colonizing Mars has evolved. I now believe that human visitors to the Red Planet should commit to staying there permanently. One-way tickets to Mars will make the missions technically easier and less expensive and get us there sooner. More importantly, they will ensure that our Martian outpost steadily grows as more homesteaders arrive. Instead of explorers, one-way Mars travelers will be 21st-century pilgrims, pioneering a new way of life. It will take a special kind of person. Instead of the traditional pilot/scientist/engineer, Martian homesteaders will be selected more for their personalities—flexible, inventive and determined in the face of unpredictability. In short, survivors.\n\nThe Mars Artists Community has adopted Mars to Stay as their primary policy initiative. During a 2009 public hearing of the U.S. Human Space Flight Plans Committee at which Dr. Robert Zubrin presented a summary of the arguments in his book \"The Case for Mars\", dozens of placards reading \"Mars Direct Cowards Return to the Moon\" were placed throughout the Carnegie Institute. The passionate uproar among space exploration advocates—both favorable and critical—resulted in the Mars Artists Community creating several dozen more designs, with such slogans as, \"Traitors Return to Earth\" and \"What Would Zheng He Do?\"\n\nIn October 2009, Eric Berger of the Houston Chronicle wrote of \"Mars to Stay\" as perhaps the only program that can revitalize the United States' space program:\n\nWhat if NASA could land astronauts on Mars in a decade, for not ridiculously more money than the $10 billion the agency spends annually on human spaceflight? It's possible ... relieving NASA of the need to send fuel and rocketry to blast humans off the Martian surface, which has slightly more than twice the gravity of the moon, would actually reduce costs by about a factor of 10, by some estimates.\n\nHard Science Fiction writer Mike Brotherton has found \"Mars to Stay\" appealing for both economic and safety reasons, but more emphatically, as a fulfillment of the ultimate mandate by which \"our manned space program is sold, at least philosophically and long-term, as a step to colonizing other worlds\". Two-thirds of the respondents to a poll on his website expressed interest in a one-way ticket to Mars \"if mission parameters are well-defined\" (not suicidal).\n\nIn June 2010, Buzz Aldrin gave an interview to Vanity Fair in which he restated \"Mars to Stay\":\n\nDid the Pilgrims on the Mayflower sit around Plymouth Rock waiting for a return trip? They came here to settle. And that's what we should be doing on Mars. When you go to Mars, you need to have made the decision that you're there permanently. The more people we have there, the more it can become a sustaining environment. Except for very rare exceptions, the people who go to Mars shouldn't be coming back. Once you get on the surface, you're there.\n\nAn article by Dirk Schulze-Makuch (Washington State University) and Paul Davies (Arizona State University) from the book \"The Human Mission to Mars: Colonizing the Red Planet\" summarizes their rationale for Mars to Stay:\n\n[Mars to stay] would for years of rehabilitation for returning astronauts, which would not be an issue if the astronauts were to remain in the low-gravity environment of Mars. We envision that Mars exploration would begin and proceed for a long time on the basis of outbound journeys only.\n\nIn November 2010, Keith Olbermann started an interview with Derrick Pitts, Planetarium Director at the Franklin Institute in Philadelphia, by quoting from the Dirk Schulze-Makuch and Paul Davies article, saying, \"The Astronauts would go to Mars with the intention of staying for the rest of their lives, as trailblazers of a permanent human Mars colony.\" In response to Olbermann's statement that \"the authors claim a one-way ticket to Mars is no more outlandish than a one-way ticket to America was in 1620\", Pitts defends Mars to Stay initiatives by saying \"they begin to open the doors in a way that haven't been opened before\".\n\nIn a January 2011 interview, X Prize founder Peter Diamandis expressed his preference for Mars to Stay research settlements:\n\nPrivately funded missions are the only way to go to Mars with humans because I think the best way to go is on \"one-way\" colonization flights and no government will likely sanction such a risk. The timing for this could well be within the next 20 years. It will fall within the hands of a small group of tech billionaires who view such missions as the way to leave their mark on humanity.\n\nIn March 2011, Apollo 14 pilot Edgar Mitchell and Apollo 17's geologist Harrison Schmitt, among other noted Mars exploration advocates published an anthology of Mars to Stay architectures titled, \"A One Way Mission to Mars: Colonizing the Red Planet\". From the publisher's review:\n\nAnswers are provided by a veritable who's who of the top experts in the world. And what would it be like to live on Mars? What dangers would they face? Learn first hand, in the final, visionary chapter about life in a Martian colony, and the adventures of a young woman, Aurora, who is born on Mars. Exploration, discovery, and journeys into the unknown are part of the human spirit. Colonizing the cosmos is our destiny. The Greatest Adventure in the History of Humanity awaits us. Onward to Mars!\n\nAugust 2011, Professor Paul Davies gave a plenary address to the opening session of the 14th Annual International Mars Society Convention on cost-effective human mission plans for Mars titled \"One-Way Mission to Mars\".\n\n\"Mars to Stay\" has been explicitly proposed by two op-ed pieces in the \"New York Times\".\n\nFollowing a similar line of argument to Buzz Aldrin, Lawrence Krauss asks in an op-ed, \"Why are we so interested in bringing the Mars astronauts home again?\". While the idea of sending astronauts aloft never to return may be jarring upon first hearing, the rationale for one-way exploration and settlement trips has both historical and practical roots. For example, colonists and pilgrims seldom set off to the New World without the expectation of a return trip. As Lawrence Krauss writes, \"To boldly go where no one has gone before does not require coming home again.\"\n\nIf it sounds unrealistic to suggest that astronauts would be willing to leave home never to return ... consider the results of several informal surveys I and several colleagues have conducted recently. One of my peers in Arizona recently accompanied a group of scientists and engineers from the Jet Propulsion Laboratory on a geological survey. He asked how many would be willing to go on a one-way mission into space. Every member of the group raised their hand.\n\nAdditional immediate and pragmatic reasons to consider one-way human space exploration missions are explored by Krauss. Since much of the cost of a voyage to Mars will be spent on returning to Earth, if the fuel for the return is carried on board, this greatly increases the mission mass requirement – that in turn requires even more fuel. According to Krauss, \"Human space travel is so expensive and so dangerous ... we are going to need novel, even extreme solutions if we really want to expand the range of human civilization beyond our own planet.\" Delivering food and supplies to pioneers via unmanned spacecraft is less expensive than designing an immediate return trip.\n\nIn an earlier 2004 op-ed for the \"New York Times\", Paul Davies says motivation for the less expensive, permanent \"one-way to stay option\" arises from a theme common in \"Mars to Stay\" advocacy: \"Mars is one of the few accessible places beyond Earth that could have sustained life [... and] alone among our sister planets, it is able to support a permanent human presence.\"\n\nDavies argues that since \"some people gleefully dice with death in the name of sport or adventure [and since] dangerous occupations that reduce life expectancy through exposure to hazardous conditions or substances are commonplace\", we ought to not find the risks involved in a Mars to Stay architecture unusual. \"A century ago, explorers set out to trek across Antarctica in the full knowledge that they could die in the process, and that even if they succeeded their health might be irreversibly harmed. Yet governments and scientific societies were willing sponsors of these enterprises.\" Davies then asks, \"Why should it be different today?\"\n\n\n"}
{"id": "547857", "url": "https://en.wikipedia.org/wiki?curid=547857", "title": "Mere-exposure effect", "text": "Mere-exposure effect\n\nThe mere-exposure effect is a psychological phenomenon by which people tend to develop a preference for things merely because they are familiar with them. In social psychology, this effect is sometimes called the familiarity principle. The effect has been demonstrated with many kinds of things, including words, Chinese characters, paintings, pictures of faces, geometric figures, and sounds. In studies of interpersonal attraction, the more often a person is seen by someone, the more pleasing and likeable that person appears to be.\n\nThe earliest known research on the effect was conducted by Gustav Fechner in 1876. Edward B. Titchener also documented the effect and described the \"glow of warmth\" felt in the presence of something that is familiar. However, Titchener's hypothesis was thrown out once tested and results showed that the enhancement of preferences for objects did not depend on the individual's subjective impressions of how familiar the objects were. The rejection of Titchener's hypothesis spurred further research and the development of current theory.\n\nThe scholar who is best known for developing the mere-exposure effect is Robert Zajonc. Before conducting his research, he observed that exposure to a novel stimulus initially elicits a fear/avoidance response by all organisms. Each repeated exposure to the novel stimulus causes less fear and more of an approach tactic by the observing organism. After repeated exposure, the observing organism will begin to react fondly to the once novel stimulus. This observation led to the research and development of the mere-exposure effect.\n\nIn the 1960s, a series of laboratory experiments by Robert Zajonc demonstrated that simply exposing subjects to a familiar stimulus led them to rate it more positively than other, similar stimuli which had not been presented. In the beginning of his research, Zajonc looked at language and the frequency of words used. He found that overall positive words received more usage than their negative counterparts. In later years, he moved on to show similar results for a variety of stimuli such as polygons, drawings, photographs of expressions, nonsense words, and idiographs, as well as when being judged by a variety of procedures such as liking, pleasantness, and forced-choice measures.\n\nIn 1980, Zajonc proposed the affective primacy hypothesis, which hypothesizes that affective reactions (i.e. liking) can be \"elicited with minimal stimulus input\". Through mere-exposure experiments, Zajonc sought to provide evidence for the affective-primacy hypothesis, namely, that affective judgments are made without prior cognitive processes. Zajonc tested this hypothesis by presenting repeated stimuli to participants at suboptimal thresholds such that they did not show conscious awareness or recognition of the repeated stimuli (when asked whether they had seen the image, responses at chance level), but continued to show affective bias towards the repeatedly exposed stimuli. Zajonc compared results from primes exposed longer which allowed for conscious awareness to stimuli shown briefly such that participants did not show conscious awareness. He found that the primes shown more briefly and not recognized prompted faster responses for liking as compared to primes shown at conscious levels.\n\nOne experiment that was conducted to test the mere-exposure effect used fertile chicken eggs for the test subjects. Tones of two different frequencies were played to different groups of chicks while they were still unhatched. Once hatched, each tone was played to both groups of chicks. Each set of chicks consistently chose the tone prenatally played to it. Zajonc tested the mere-exposure effect by exposing Chinese characters for shorts amounts of time to two groups of individuals. The individuals were then told that these symbols represented adjectives and were asked to rate whether the symbols held positive or negative connotations. The symbols that had been previously seen by the test subjects were consistently rated more positively than those unseen. In a similar experiment, the individuals were not asked to rate the connotations of the symbols, but to describe their mood after the experiment. Members of the group with repeated exposure to certain characters reported being in better moods and felt more positive than those who did not receive repeated exposure.\n\nIn another variation, subjects were shown an image on a tachistoscope for a very brief duration that could not be perceived consciously. This subliminal exposure produced the same effect, though it is important to note that subliminal effects are unlikely to occur without controlled laboratory conditions.\n\nAccording to Zajonc, the mere-exposure effect is capable of taking place without conscious cognition, and that \"preferences need no inferences\". This statement by Zajonc has spurred much research in the relationship between cognition and affect. Zajonc explains that if preferences (or attitudes) were merely based upon information units with affect attached to them, then persuasion would be fairly simple. He argues that this is not the case: such simple persuasion tactics have failed miserably. Zajonc states that affective responses to stimuli happen much more quickly than cognitive responses, and that these responses are often made with much more confidence. He states that thought (cognition) and feeling (affect) are distinct, and that cognitions are not free from affect, nor is affect free of cognition. Zajonc states, \"...the form of experience that we came to call feeling accompanies all cognitions, that it arises early in the process of registration and retrieval, albeit weakly and vaguely, and that it derives from a parallel, separate, and partly independent system in the organism.\"\n\nIn regards to the mere-exposure effect and decision making, Zajonc states that there has been no empirical proof that cognition precedes any form of decision making. While this is a common assumption, Zajonc argues that the opposite is more likely: decisions are made with little to no cognitive process. He equates deciding upon something with liking it, meaning that more often we cognize reasons to rationalize a decision instead of deciding upon it. Therefore, we make judgments first, and then seek to justify those judgments by rationalization.\n\nCharles Goetzinger conducted an experiment using the mere-exposure effect on his class at Oregon State University. Goetzinger had a student come to class in a large black bag with only his feet visible. The black bag sat on a table in the back of the classroom. Goetzinger's experiment was to observe if the students would treat the black bag in accordance to Zajonc's mere-exposure effect. His hypothesis was confirmed. The students in the class first treated the black bag with hostility, which over time turned into curiosity, and eventually friendship. This experiment confirms Zajonc's mere-exposure effect, by simply presenting the black bag over and over again to the students their attitudes were changed, or as Zajonc states \"mere repeated exposure of the individual to a stimulus is a sufficient condition for the enhancement of his attitude toward it\".\n\nA meta-analysis of 208 experiments found that the mere-exposure effect is robust and reliable, with an effect size of \"r\"=0.26. This analysis found that the effect is strongest when unfamiliar stimuli are presented briefly. Mere exposure typically reaches its maximum effect within 10–20 presentations, and some studies even show that liking may decline after a longer series of exposures. For example, people generally like a song more after they have heard it a few times, but many repetitions can reduce this preference. A delay between exposure and the measurement of liking actually tends to increase the strength of the effect. The effect is weaker on children, and for drawings and paintings as compared to other types of stimuli. One social psychology experiment showed that exposure to people we initially dislike makes us dislike them even more.\n\nIn support of Zajonc's claim that affect does not need cognition to occur, Zola–Morgan conducted experiments on monkeys with lesions to the amygdala (the brain structure that is responsive to affective stimuli). In his experiments, Zola–Morgan proved that lesions to the amygdala impair affective functioning, but not cognitive processes. However, lesions in the hippocampus (the brain structure responsible for memory) impair cognitive functions but leave emotional responses fully functional.\n\nThe mere-exposure effect has been explained by a two-factor theory that posits that repeated exposure of a stimulus increases \"perceptual fluency\" which is the ease with which a stimulus can be processed. Perceptual fluency, in turn, increases positive affect. Studies showed that repeated exposure increases perceptual fluency, confirming the first part of the two-factor theory. Later studies observed that perceptual fluency is affectively positive, confirming the second part of the fluency account of the mere-exposure effect.\n\nThe most obvious application of the mere-exposure effect is found in advertising, but research has been mixed as to its effectiveness at enhancing consumer attitudes toward particular companies and products. One study tested the mere-exposure effect with banner ads seen on a computer screen. The study was conducted on college-aged students whom were asked to read an article on the computer while banner ads flashed at the top of the screen. The results showed that each group exposed to the \"test\" banner rated the ad more favorably than other ads shown less frequently or not at all. This research supports the evidence for the mere-exposure effect.\n\nA different study showed that higher levels of media exposure are associated with lower reputations for companies, even when the mere exposure is mostly positive. A subsequent review of the research concluded that exposure leads to ambivalence because it brings about a large number of associations, which tend to be both favorable and unfavorable. Exposure is most likely to be helpful when a company or product is new and unfamiliar to consumers. An 'optimal' level of exposure to an advertisement may or may not exist. In a third study, experimenters primed consumers with affective motives. One group of thirsty consumers were primed with a happy face before being offered a beverage, while a second group was primed with an unpleasant face. The group primed with the happy face bought more beverages, and were also willing to pay more for the beverage than their unhappy counterparts. This study bolsters Zajonc's claim that choices are not in need of cognition. Buyers often choose what they 'like' instead of what they have substantially cognized.\n\nIn the advertising world, the mere-exposure effect suggests that consumers need not cognize advertisements: the simple repetition is enough to make a 'memory trace' in the consumer's mind and unconsciously affect their consuming behavior. One scholar explains this relationship as follows: \"The approach tendencies created by mere exposure may be preattitudinal in the sense that they do not require the type of deliberate processing that is required to form brand attitude.\"\n\nThe mere-exposure effect exists in most areas of human decision making. For example, many stock traders tend to invest in securities of domestic companies merely because they are more familiar with them despite the fact that international markets offer similar or even better alternatives. The mere-exposure effect also distorts the results of journal ranking surveys; those academics who previously published or completed reviews for a particular academic journal rate it dramatically higher than those who did not. There are mixed results on the question of whether mere exposure can promote good relations between different social groups. When groups already have negative attitudes to each other, further exposure can increase hostility. A statistical analysis of voting patterns found that a candidate's exposure has a strong effect on the number of votes they receive, distinct from the popularity of the policies.\n\n\n"}
{"id": "4556457", "url": "https://en.wikipedia.org/wiki?curid=4556457", "title": "Michael Messner", "text": "Michael Messner\n\nMichael Alan Messner (born 1952) is an American sociologist. His main areas of research are gender (especially men's studies) and the sociology of sports. He is the author of several books, he gives public speeches and teaches on issues of gender-based violence, the lives of men and boys, and gender and sports.\n\nSince 1987, Messner has worked as a professor of sociology and gender studies at the University of Southern California. He was head of the department, and still retains his dual faculty appointment. He was the president of the Pacific Sociological Association in 2010-2011, and in 2011 the California Women's Law Center presented him with the Pursuit of Justice Award.\n\nMessner was born in Salinas, California.\n\nMessner was educated from kindergarten to his Ph.D. in California's public schools. He has a bachelor's degree in social science and a master's degree in sociology from California State University, Chico. He obtained a Ph.D. in sociology in 1985 from the University of California, Berkeley with a dissertation titled \"Masculinity and Sports: An Exploration of the Changing Meaning of Male Identity in the Lifecourse of the Athlete\".\n\nIn the late 1970s, he started to deal with feminist theory and the construction of gender. He took part in one of the first ever classes about men and masculinity in the USA, held by Bob Blauner in Berkeley.\n\nMessner lives in South Pasadena with his wife, Pierrette Hondagneu-Sotelo (a sociologist and author). They have two sons, named Miles and Sasha.\n\nIn an article about a \"fight club\" in Menlo Park, California, Messner remarked that men involved in them \"often carry bottled-up violent impulses learned in childhood from video games, cartoons and movies. [...] Boys have these warrior fantasies picked up from popular culture, and schools sort of force that out of them.\" In these fantasies: \"The good guys always resort to violence, and they always get the glory and the women.\"\n\nMessner establishes the emphasis society puts on sexuality and gender roles in various works upholding the notions that roles in society are predetermined by these categorizations. Gender role is the set of characteristics prescribed by a culture and expressed through direct communication and through media. These predetermined roles can lead to inequivalent advantages to people classified in certain categories over others. In many societies, this has become a systematic oppression. He discusses these ideas through various articles and explores different topics such as the participation of women in sports as well as what it means to be 100% straight.\n\nMessner upholds the theory that sexuality is, \"a constructed identity, a performance, and an institution\". It is not necessary and it influences behaviors in society every day. He discusses this idea in depth in his article, \"Becoming 100 percent Straight\". In this article, he discusses how he as a teenager was influenced to become a jock because of his height and build and, as a result, repressed his sexual fluidity in fear of being outcast. He talks about his experience with basketball and his relationship with his friend because of his repressed romantic feelings for him. As a result of these feelings, he targets him as an external source of his frustrations and even goes as far as bullying him to express these frustrations. Although Messner does not identify as bisexual, he describes his experiences based on the Freudian model of bisexuality which states that most people go through a stage in life in which they are attracted to people of the same sex. Adult experiences eventually lead them to shift their sexual desires to people of the same sex. This suppression of sexual desires can lead to aggression and violence in order to clarify boundaries between one's self and others. His experience serves as an example of a new perspective on sexuality that others in dominant gender/sexuality categories can identify with: one that is not dominated by socially constructed values.\n\nAlong with sexuality, Messner views gender to be a determining factor of the roles we exhibit in society. The world of sports is a prime example of the inherent differences that are constructed for different genders. Sports are a terrain of contested gender relations to Messner. Before the 1970s, girls did not feel as if they belonged in the sporting world because they were not made for it. However, Title IX passed in the early 70's gave them the equal opportunity to pursue sports in school. The participation in sports by women has a strong correlation with the presence of women in the workplace. While the participation of women in sports is a triumph in a movement toward equality, the sports world is still inherently different for women and men. It is fueled by \"soft-essentialism\", Messner's theory of the shared belief that boys and girls are inherently different as opposed to \"hard essentialism,\" which basically creates a more categorical structure for men. Men are pushed to be more competitive than women through this notion of hard essentialism leading them to be driven by a linear notion that success and leadership in the workplace is the ultimate goal. Women, however, are routed towards sports like softball or even in different leagues altogether giving them choices and complicating the meaning of equality. He argues that separate can never truly be equal because of these components. This furthers the frame of mind that women and men are inherently different, and girls should not be where the boys are.\n\nWhile integration of sports sound like an opportunity for equality, this idea of merging boys and girls together in youth sports would be potentially counterproductive. Sexists attitudes and presumptions develop in either scenario; if sports become coed, girls may be seen as disadvantaged, but if sports stay segregated the theory of soft essentialism can only help but run up against the social barriers of equal choice for women today. In order to dismantle the notion of soft essentialism, a coupled strategy of de-gendering boys sports along with a strategic categorization of women's sports can help break down gender based inequalities exhibited in society.\n\nGender also plays a key role in the development of gifted students. Gender identities in gifted students usually differ from those of their peers. For example, gifted girls are more like gifted boys than other girls. This complicates the development of their gender identity. Girls are more likely to underachieve because of stereotypes such as, \"girls are bad at math\" even though the math gap has shrunk significantly where the discrepancy between boys and girls used to be extreme. For this reason it is important for counselors to be practiced in understanding gender educational practices and support students.\n\nSports and education provide a look at the inequalities found based on gender in boys and girls, but it does not paint the picture of gender and sexuality relations in society as a whole. Messner's views of gender predetermining roles in society is seen in extreme ways when looking at other cultures such as the Congolese. For example, when Moore wrote of her experiences in the Congo with women's rights, she found disturbing evidence that women were only used to birth children. When a women's workshop was held demonstrating that it is wrong for a husband to beat his wife, one man asked, \"But if we can't beat our wives what can we do when they won't have sex with us?\" This shows a universal, somewhat exaggerated, example of how women are seen in society, as childbearers. Male entitlement to women's bodies has become a standard in societies such as these and reflects the values that feminists like Messner aim to change.\n\nSexuality is a powerful tool used to control nations especially when it is paired with orthodox religious thinking. Feki states, \"If you want to understand a people, look in their bedrooms\". Her hypothesis is correct when looking at societies such as Egypt. Governments and parents do not trust their children enough to learn about sex education. As a result, religious views of sexuality are established and followed strictly. Women are expected to remain virgins until marriage and chastity thereafter even though their sexuality is a family matter, usually determined by the patriarch on when and who she weds. It is this thinking that has kept women from leadership positions in the workplace and in politics. If women are not deemed responsible enough to control their own bodies, they will not be deemed capable of leading in any aspect. This affirms Messner's belief that roles in society are determined by one's gender and sexuality.\n\nMessner examines gender and sexuality and concludes that they play an important role in determining one's place in society. They can shape the way people behave and develop and can mold views and actions to fit society's standards. It is fueled by notion that women and men are inherently different, a term Messner coins as soft essentialism. This theory often leads to sexuality and gender used as tools of oppression in some cultures which leads to views of women as sexual objects for men who do not have control of their bodies. Religion is sometimes paired with this theory to further the old hegemonic patriarchies established in emerging democracies such as Egypt. Changing the frame of mind of what sexuality and gender truly mean in society will help take steps towards equality. Equal opportunity for people of differing genders and sexuality can only benefit a society. For example, countries such as Germany, Denmark, and Norway have the closest gender gap in the workplace and they have some of the most powerful economies because of it according to Reding who states, \"Humans are distinguished in their contributions to society by character, ability, and motivation, and there is no meaningful correlation between those traits and sex and sexuality\".\n\nMessner has spoken out as a strong advocate for Title IX. Having interests in both gender studies and sports, he has analyzed it both from a feminist and sports perspective. Published in the \"Journal of Sport and Social Issues,\" his article \"Social justice and Men's Interests: The Case of Title IX\" reviews the effect Title IX has on men's interest in sports. He argues that although \"men's superordinate status sets the stage for them to understand their interests as opposed to those of women\", men gain much from integration of women into sports. He states that \"these kinds of experiences can provide a commitment for men to take action with girls, women, and other men who are interested in building a more equitable and just world\". Overall, Messner believes that only through integration and mutual respect in all institutions (particularly sport) can we begin to grow relationships between men and women that allow for a better future for both parties.\n\nMessner has conducted research in several subcategories of Sociology over four decades. Primarily, his research was influenced by several events that took place in the mid 20th century. He states in his online biography: \"My teaching and research were sparked and continue to be animated by the movements for social justice that erupted in the 1960s, 1970s and beyond, especially feminism\". Messner's research can be broken up into three main categories: gender and sport; sports media; and men, feminism and politics.\n\nMessner has written four books and over eleven articles on gender and sport. His main article contributions have been to the \"Sociology of Sport Journal\", \"Gender & Society\", and other scholarly journals. His website features many of these articles.\n\nIn his paper, \"Gender ideologies, youth sports, and the production of soft essentialism\" Messner introduces the concept of soft essentialism as \"a currently ascendant hegemonic ideology… that valorizes the liberal feminist ideal of individual choice for girls, while retaining a largely naturalized view of boys and men\". He argues that essentially, especially in youth sports, we assume natural differences between boys and girls. However, by offering girls equal opportunity, soft essentialism does not \"endorse categorical social containment of women in domestic life\". While this type of thought is less restrictive for girls and women than hard essentialism, because it still uses institutions (particularly sport) to reinforce a \"natural difference\" between boys and girls, it is still counterproductive to the feminism movement as a whole.\nThrough extensive research and interviewing of youth soccer, baseball, and softball coaches about boys, girls, and gender, Messner found that most adults had a tendency to describe girls' lives as being full of choices – a way of thinking that Messner argues is a major accomplishment of liberal feminism. However, he found that when asked about boys, the responses were less sophisticated and assumed that boys were simply driven by testosterone. In his conclusion, Messner determines that there exist three main sources of strain that proliferate hegemonic gender inequality in the form of soft essentialism: working class mothers, today's largely unreconstructed and categorical view of boys, and the celebration of equal opportunity and free choice for girls. He argues that strategic categoricalism in girls' sports coupled with a de-gendering of boys' sports.\n\nMessner has conducted research in sports media for over twenty years, focusing on what it covers and ignores. On his website he breaks his research down into three main areas: \"First, [he] has conducted a longitudinal content and textual analysis of gender in televised news and highlights programs. Second, [he] is interested in how the sports media handles a particular story, especially a \"scandal.\" Third, [he] is interested in the dominant gendered messages that are pitched to boys and men as consumers through sport broadcasts\". Overall, Messner believes that sports media is yet another institution that promotes patriarchal sexist ideology.\n\nMost of Messner's research in sports media revolves around the way the media portrays females and female athletes. In his most recent article on sports media, \"Women Play Sport, But Not on TV: A Longitudinal Study of Televised News Media\" he and co-authors Cheryl Cooky and Robin Hextrum analyze 6 weeks of local and national news coverage. Their evidence found that despite \"tremendous increased participation of girls and women in sport at the high school, collegiate, and professional level,\" coverage of women's sport on television is \"the lowest ever\". Through their research, Messner, Cooky, and Hextrum determined that sports media doesn't simply show what people want to see, but rather proliferates hegemonic gender asymmetries by contributing to a specific reception of sport that idealizes it as a man's world.\n\nIn his research on both men and women's studies, Messner has analyzed the feminist movement from a male perspective since the 1970s. His research particularly focuses on men's personal, organizational and political responses to feminism. His most recent contribution to feminism is his co-authored book, \"Some Men: Feminist Allies and the Movement to End Violence Against Women\", which was released in March 2015 through the Oxford University Press. As a general trend, Messner believes that it is in the best interest of men to support the feminist movement and the end of sexism.\nIn his 2004 article \"On Patriarchs and Losers: Rethinking Men's Interests\", Messner explores the concept of \"men's interests', deciding whether or not there exists a universal interest for men and how that plays into the role of feminism in the United States. Furthermore, he discusses the development of the scholarly focus on \"men and masculinity\", observing exactly how men's interests in the United States are being articulated both in commercial and political discourse. Messner believes that all men must be on board with feminism. In particular, he points out an example with a young white guy speeding by in a pick-up truck with a gun rack. He writes, \"I want that guy in the men's movement… and to get him involved, we have to convince him that the masculinity he has learned is self-destructive and toxic, and that feminist change is in his interest\". Messner argues that, at its core, feminism is in the best interest of every person because systemic, hegemonic oppression harms everyone, not just women.\n\n\n\n\n2. Anekphong. Sports Team. Digital image. WikiGender. OECD, 29 Nov. 2014. Web. 17 Apr. 2015.\n\n3. \"Michael Messner.\" Digital image. USC Dornsife. University of Southern California, n.d. Web. 17 Apr. 2015.\n\n"}
{"id": "15683423", "url": "https://en.wikipedia.org/wiki?curid=15683423", "title": "Monadic plane", "text": "Monadic plane\n\nIn Theosophy, the Monadic Plane is the plane in which the Monad (also called the Holy Spirit or the Oversoul) is said to exist. The term \"Monad\" is from the Greek word μονάς (monas), which means \"singularity\", and was used by Ancient Greek philosophers such as Plato. The Arcane School ideas also use the concept of the Monad. Some schools of thought equate the Monadic Plane with the Hyperplane, while others view the Monadic Plane as enclosing and interpenetrating many hyperplanes.\n\n'Classical' 1800s Theosophy does not say the monad is human, but Annie Besant & Charles Leadbeater may have said so, and Alice Bailey or others, who uses similar ideas, did say so.\n\n"}
{"id": "9968842", "url": "https://en.wikipedia.org/wiki?curid=9968842", "title": "Mycobacterium avium hominissuis", "text": "Mycobacterium avium hominissuis\n\n\"Mycobacterium avium hominissuis\" is a subspecies of the phylum actinobacteria (Gram-positive bacteria with high guanine and cytosine content, one of the dominant phyla of all bacteria), belonging to the genus mycobacterium.\n\nSuggested name for \"Mycobacterium avium avium\" isolates from humans and pigs.\n\nBased on differences in IS1245 RFLP, 16S-23S rDNA ITS and growth temperature, Mijs \"et al.\" 2002. propose to reserve the designation \"Mycobacterium avium\" subsp. \"avium\" for bird-type isolates. These authors suggest, but not formally propose, the designation \"Mycobacterium avium\" subsp. \"hominissuis\" for the isolates from humans and pigs.\n"}
{"id": "10862494", "url": "https://en.wikipedia.org/wiki?curid=10862494", "title": "Organic unity", "text": "Organic unity\n\nOrganic unity is the idea that a thing is made up of interdependent parts. For example, a body is made up of its constituent organs, and a society is made up of its constituent social roles.\n\nOrganic unity was propounded by the philosopher Plato as a theory of literature. He explored the idea in such works as \"The Republic\", \"Phaedrus\", and \"Gorgias\". But it was Aristotle, one of Plato's students, who advanced the idea and discussed it more explicitly.\n\nIn \"Poetics\" (c. 335 BCE), Aristotle describes organic unity by explaining how writing relies internally on narration and drama to be cohesive; but without balance between the two sides, the work suffers. The main theme of organic unity relies on a free spirited style of writing and by following any guidelines or genre-based habits, the true nature of a work becomes stifled and unreliable on an artistic plane.\n\nThe concept of organic unity gained popularity through the New Critics movement. Cleanth Brooks (1906–94) played an integral role in modernizing the organic unity principle. Using the poem \"The Well Wrought Urn\" as an example, Brooks related the importance of a work’s ability to flow and maintain a theme, so that the work gains momentum from beginning to end. Organic unity is the common thread that keeps a theme from becoming broken and disjointed as a work moves forward.\n\n"}
{"id": "35038685", "url": "https://en.wikipedia.org/wiki?curid=35038685", "title": "Prasrabhi", "text": "Prasrabhi\n\nPrasrabhi (Sanskrit; Tibetan: ཤིན་ཏུ་སྦྱང་བ་, Tibetan Wylie: \"shin tu sbyang ba\", Pali: passaddhi) is a Mahayana Buddhist term translated as \"pliancy\", \"flexibility\", or \"alertness\". It is defined as the ability to apply body and mind towards virtuous activity. \"Prasrabhi\" is identified as:\n\nThe Abhidharma-samuccaya states: \n\n\n\n"}
{"id": "23941767", "url": "https://en.wikipedia.org/wiki?curid=23941767", "title": "Prosolvable group", "text": "Prosolvable group\n\nIn mathematics, more precisely in algebra, a prosolvable group (less common: prosoluble group) is a group that is isomorphic to the inverse limit of an inverse system of solvable groups. Equivalently, a group is called prosolvable, if, viewed as a topological group, every open neighborhood of the identity contains a normal subgroup whose corresponding quotient group is a solvable group.\n\n\n"}
{"id": "58429637", "url": "https://en.wikipedia.org/wiki?curid=58429637", "title": "Racial battle fatigue", "text": "Racial battle fatigue\n\nRacial battle fatigue is a psychological concept that was coined in 2003 as a theoretical framework to explain the social and psychological stress responses from being an African American male on a historically White campus. It was introduced by William A. Smith, an associate professor in the Division of Ethnic Studies and Department of Education, Culture, and Society at the University of Utah. The social and psychological stress is proposed to develop from the burden of a lifetime of blatant racism and microaggressions that \"can theoretically contribute to diminished mortality, augmented morbidity, and flattened confidence.\" The framework offers a lens to better understand racial undertones of a campus environment and educational experiences for people of color, though Smith's research primarily focuses on African American men. The phenomenon builds off of existing research connecting African Americans and other people of color with oppression and discrimination experienced at historically White institutions. It also takes into account that African Americans are often the primary recipient of hate crimes, physical abuse, and verbal abuse, as reported by the U.S. Department of Justice. Smith incorporates literature on combat trauma and combat stress syndrome to help understand the effects of managing a hostile environment(s) and persistent extreme stress. \n\nFor a person of color, being on the receiving end of racial slights can manifest in an unabating balance of time and energy in determining if they are motivated by a racist intent and if they are worthy of responding to. Smith stated that many African American boys and adults \"will perceive their environment as extremely stressful, exhausting, and diminishing to their senses of control, comfort, and meaning while eliciting feelings of loss, ambiguity, strain, frustration, and injustice\" because of chronic racial microaggressions and overt racism (also called, racial macroaggressions). The accumulation of emotional and physiological symptoms resulting from subtle and overt forms of racial verbal and nonverbal microaggressions at the societal, interpersonal, and institutional level can lead to traumatic psychological and physiological stress symptoms. \n\nPsychological symptoms can include but are not limited to: depression, chronic anxiety, anger, frustration, shock, disturbed sleep, disappointment, resentment, emotional or social withdrawal, intrusive thoughts or images, avoidance, helplessness, and fear. Acceptance of racist attributions, or internalized racism, may also be a psychosocial response. Physiological symptoms such as high blood pressure, headaches, increased breathing and heart rate in anticipation of racial conflict, upset stomach, ulcers, fatigue, exhaustion, and muscle tension around the neck, shoulders, and head may be present due to the persistent nature of the stress experienced. Clark and colleagues proposed that these stress responses are also related to cardiovascular reactivity and higher rates of hypertension among African Americans. Moreover, prolonged activation of sympathetic responses may result in higher resting systolic blood pressure and increases in mean arterial blood pressure. \n\nIt has been well-documented that Black male college students experience greater dropout and lower grades. When compared with their Black female counterparts, Black males were also more likely to drop out of high school and college. Researchers asserted that the distress and academic attrition that may be present with Black males at historically White universities should not be attributed to their lack of academic preparedness, rather the aftermath of subtle and cumulative racial discrimination that occurs in those places. One of Smith's earliest studies on racial battle fatigue gathered 36 African American college students enrolled in historically white university campuses into focus groups with guided discussions. During the time of the study, the students had been enrolled at: Harvard University; University of California, Berkeley; University of Michigan, University of Michigan Law School; University of Illinois at Urbana-Champaign; and Michigan State University. The students reported psychological responses aligned with racial battle fatigue and all perceived the college environment to be more hostile towards African American males than other groups. Consistent patterns described by the students involved experiences of hypersurvellience and control from white people and anti-Black stereotyping. Another published article on the findings from the study expressed stereotyping and scrutinizing from campus police officers. One student recounted an incident while at UC Berkeley: \"[At] Underhill [residence hall], all last semester, almost every night, there’s Whites, there’s Asians in Underhill playing Frisbee, or playing football, or what have you at one o’clock in the morning. [They are] out there yelling, having a good time, and never [having] any problems. So, me and my friends [all Black males] are out there about to play some football, and it’s like 11 o’clock. All of a sudden, UCBP [UC-Berkeley Police] sweeps up. First, it’s one car, and they get out the car and it’s like, ‘We got some complaints. You guys need to leave.’ Mind you, there’s about maybe 10 of us and we’re out there still just tossing the football around. Then, after [the UCBP officer] is there for maybe about two minutes, all of a sudden from this entrance over here, we have two other [UCBP squad] cars swooping in on Underhill lot.\"Despite stating to the police rationally that they were using campus property, the student and his friends were asked to leave or be arrested. The messages perceived by the students is that they as [Black males] were unwanted, unvalued, and not as respected compared to his other non-Black college peers. This experience may also reflects community policing tactics employed by the police against Black males as a larger systemic issue.\n\nA 2014 research study assessed if Latino/a/x students experience similar psychological and physiological stress responses on college campuses following racialized incidents that Smith described for African American males. They found that the common experiences of racial microaggressions were interpersonal, non-verbal, institutional, racial jokes and remarks, low teacher expectations, and false assumptions based on stereotypes. Moreover, they upheld that Latino/a/x experience more psychological stress because of racial microaggressions. As such, the stress responses highlighted by racial battle fatigue is quantitively linked to Latina/o/x students. \n\nThe African American male experiences described in the earlier studies conducted cannot be generalized to other people of color, females, or African American females. The study assessing Latina/o students with racial battle fatigue had a small sample size and did not differentiate between gender. \n"}
{"id": "18388936", "url": "https://en.wikipedia.org/wiki?curid=18388936", "title": "Self-righteousness", "text": "Self-righteousness\n\nSelf-righteousness (also called sanctimoniousness, sententiousness, and holier-than-thou attitudes) is a feeling or display of (usually smug) moral superiority derived from a sense that one's beliefs, actions, or affiliations are of greater virtue than those of the average person. Self-righteous individuals are often intolerant of the opinions and behaviors of others.\n\nThe term \"self-righteous\" is often considered derogatory (see, for example, journalist and essayist James Fallows' description of self-righteousness in regard to Nobel Peace Prize winners) particularly because self-righteous individuals are often thought to exhibit hypocrisy due to the belief that humans are imperfect and can therefore never be infallible, an idea similar to that of the Freudian defense mechanism of reaction formation. The connection between self-righteousness and hypocrisy predates Freud's views, however, as evidenced by the 1899 book \"Good Mrs. Hypocrite\" by the pseudonymous author \"Rita\".\n\n\n"}
{"id": "2022903", "url": "https://en.wikipedia.org/wiki?curid=2022903", "title": "Semi-cursive script", "text": "Semi-cursive script\n\nSemi-cursive script is a cursive style of Chinese characters. Because it is not as abbreviated as cursive, most people who can read regular script can read semi-cursive. It is highly useful and also artistic.\n\nAlso referred to in English both as running script and by its Mandarin Chinese name, xíngshū, it is derived from clerical script, and was for a long time after its development in the 1st centuries AD the usual style of handwriting.\n\nSome of the best examples of semi-cursive can be found in the work of Wang Xizhi (321-379) of the Eastern Jin Dynasty.\n"}
{"id": "15826297", "url": "https://en.wikipedia.org/wiki?curid=15826297", "title": "Seriousness", "text": "Seriousness\n\nSeriousness (noun; adjective: serious) is an attitude of gravity, solemnity, persistence, and earnestness toward something considered to be of importance. Some notable philosophers and commentators have criticised excessive seriousness, while others have praised it. Seriousness is often contrasted with comedy, as in the seriocomedy. In the theory of humor, one must have a sense of humor and a sense of seriousness to distinguish what is supposed to be taken literally or not, or of being important or not. Otherwise, it may also be contrasted with a sense of play. How children learn a sense of seriousness to form values and differentiate between the serious and that which is not is studied in developmental psychology and educational psychology. There is a distinction between the degree of seriousness of various crimes in sentencing under the law, and also in law enforcement. There is a positive correlation with the degree of seriousness of a crime and viewer ratings of news coverage. What is or is not considered serious varies widely with different cultures.\n\nSometimes fields studying degrees of seriousness overlap, such as developmental psychology studies of development of the sense of degrees of seriousness as it relates to transgressions, which has overlap with criminology and the seriousness of crimes. \nSource:Alchilagyo.com\n\nSome use \"seriousness\" as a term of praise for scholarship or in literary review. 19th century poet, cultural critic, and literary critic, Matthew Arnold said that the most important criteria used to judge the value of a poem were \"high truth\" and \"high seriousness\".\n\nMany have expressed an attitude of disdain toward taking things too seriously, as opposed to viewing things with an attitude of humor. Poet, playwright, and philosopher Joseph Addison said that being serious is dull, \"we are growing serious, and let me tell you, that's the next step to being dull.\" Political satirist P.J. O'Rourke said that \"Seriousness is stupidity sent to college.\" Epigramist, poet, and playwright Oscar Wilde said that \"life is too important to be taken seriously.\" In a play on words, novelist Samuel Butler indicated that the \"central serious conviction in life\" is that nothing should be taken with too much seriousness, \"the one serious conviction that a man should have is that nothing is to be taken too seriously.\"\n\nIn some ascetic or puritan religious sects, an attitude of seriousness is always to be taken, and solemnity, sobriety, and puritanism with its hostility to social pleasures and indulgences are the only acceptable attitudes. Perry Miller, \"the master of American intellectual history\", wrote of excessive seriousness of the Puritans, \"simple humanity cries at last for some relief from the interminable high seriousness of the Puritan code.\"\n\nExistentialist philosopher Jean-Paul Sartre called the \"spirit of seriousness’’ the belief that there is an objective and independent goodness in things for people to discover, and that this belief leads to bad faith. He argued that people forget that values are not absolute, but are contingent and subjectively determined. In Sartre’s words, \"the spirit of seriousness has two characteristics: it considers values as transcendent ‘'givens’’, independent of human subjectivity, and it transfers the quality of ‘desirable’ from the ontological structure of things to their simple material constitution.\"\n\nSeriousness is sometimes contrasted with the comical in humor. In the performing arts and literature, the seriocomedy is a genre which blends seriousness with the comical, drama with comedy.\n\nIn the theory of humor, one must have a sense of humor and a sense of seriousness to distinguish what is supposed to be taken literally or not. An even more keen sense is needed when humor is used to make a serious point. Psychologists have studied how humor is intended to be taken as having seriousness, as when court jesters used humor to convey serious information. Conversely, when humor is not intended to be taken seriously, bad taste in humor may cross a line after which it is taken seriously, though not intended.\n\nIn Developmental psychology and educational psychology, seriousness is studied as it relates to how children develop an ability to distinguish levels of seriousness as it relates to transgressions and expenditure of time; for example, a child must learn to distinguish between levels of seriousness in admonitions such as between \"don't fidget\" and \"don't forget to look both ways when crossing the street\", which have the same linguistic and normative structure, but different levels of seriousness.\n\nThe degree of seriousness of crimes is an important factor relating to crime. One standard for measurement is the degree to which a crime affects others or society. A felony is generally considered to be a crime of \"high seriousness\", while a misdemeanor is not.\n\nIn criminal law the degree of seriousness is considered when meting out punishment to fit the crime, and in considering to what extent overcrowded prison facilities will be used. Seriousness of a crime is a major factor in considerations of the allocation of scarce law enforcement funds.\n\nThe meaning and measurement of seriousness is a major concern in public policy considerations. A quantitative scoring system called the \"seriousness score\" has been developed for use in allocating law enforcement resources and sentencing.\n\nAs to England and Wales, see section 143 of the Criminal Justice Act 2003.\n\nDegrees of seriousness are used in medicine to make decisions about care. Seriousness is related to the effects of delaying or not having medical care. In an emergency hospital, the triage nurse must evaluate levels of seriousness of medical emergencies and rank them to determine order of care. Seriousness of illness is used to make decisions as to whether to perform invasive procedures such as surgery.\n\nThere is a positive correlation between the degree of seriousness of a crime and viewer ratings of news coverage.\n\nWhat is considered serious varies widely across cultures and is studied in sociology, cultural anthropology, and criminology; being of the wrong religious faith may be considered a serious crime in some cultures; smoking marijuana may be a serious crime in some cultures and not others; homosexuality a serious crime in some cultures; and prostitution is a serious crime in some cultures. Perception of seriousness is measured in assessing varying cultural perceptions on health risks.\n"}
{"id": "7961225", "url": "https://en.wikipedia.org/wiki?curid=7961225", "title": "Shapley–Sawyer Concentration Class", "text": "Shapley–Sawyer Concentration Class\n\nThe Shapley–Sawyer Concentration Class is a classification system on a scale of one to twelve using Roman numerals for globular clusters according to their concentration. The most highly concentrated clusters such as M75 are classified as Class I, with successively diminishing concentrations ranging to Class XII, such as Palomar 12. (The class is sometimes given with numbers [Class 1–12] rather than with Roman numerals.)\n\nFrom 1927–1929, Harlow Shapley and Helen Sawyer Hogg began categorizing clusters according to the degree of concentration the system has toward the core using this scale. This became known as the \"Shapley–Sawyer Concentration Class\".\n"}
{"id": "984081", "url": "https://en.wikipedia.org/wiki?curid=984081", "title": "Space rendezvous", "text": "Space rendezvous\n\nA space rendezvous is an orbital maneuver during which two spacecraft, one of which is often a space station, arrive at the same orbit and approach to a very close distance (e.g. within visual contact). Rendezvous requires a precise match of the orbital velocities and position vectors of the two spacecraft, allowing them to remain at a constant distance through orbital station-keeping. Rendezvous may or may not be followed by docking or berthing, procedures which bring the spacecraft into physical contact and create a link between them.\n\nThe same rendezvous technique can be used for spacecraft \"landing\" on natural objects with a weak gravitational field, e.g. landing on one of the Martian moons would require the same matching of orbital velocities, followed by a \"descent\" that shares some similarities with docking.\n\nIn its first human spaceflight program Vostok, the Soviet Union launched pairs of spacecraft from the same launch pad, one or two days apart (Vostok 3 and 4 in 1962, and Vostok 5 and 6 in 1963). In each case, the launch vehicles' guidance systems inserted the two craft into nearly identical orbits; however, this was not nearly precise enough to achieve rendezvous, as the Vostok lacked maneuvering thrusters to adjust its orbit to match that of its twin. The initial separation distances were in the range of , and slowly diverged to thousands of kilometers (over a thousand miles) over the course of the missions.\n\nIn 1963 Buzz Aldrin submitted his doctoral thesis titled, \" Line-Of-Sight Guidance Techniques For Manned Orbital Rendezvous.\" As a NASA astronaut, Aldrin worked to \"translate complex orbital mechanics into relatively simple flight plans for my colleagues.\" \n\nThe first attempt at rendezvous was made on June 3, 1965, when US astronaut Jim McDivitt tried to maneuver his Gemini 4 craft to meet its spent Titan II launch vehicle's upper stage. McDivitt was unable to get close enough to achieve station-keeping, due to depth-perception problems, and stage propellant venting which kept moving it around.\nHowever, the Gemini 4 attempts at rendezvous were unsuccessful largely because NASA engineers had yet to learn the orbital mechanics involved in the process. Simply pointing the active vehicle's nose at the target and thrusting was unsuccessful. If the target is ahead in the orbit and the tracking vehicle increases speed, its altitude also increases, actually moving it away from the target. The higher altitude then increases orbital period due to Kepler's third law, putting the tracker not only above, but also behind the target. The proper technique requires changing the tracking vehicle's orbit to allow the rendezvous target to either catch up or be caught up with, and then at the correct moment changing to the same orbit as the target with no relative motion between the vehicles (for example, putting the tracker into a lower orbit, which has a shorter orbital period allowing it to catch up, then executing a Hohmann transfer back to the original orbital height).\n\nRendezvous was first successfully accomplished by US astronaut Wally Schirra on December 15, 1965. Schirra maneuvered the Gemini 6 spacecraft within of its sister craft Gemini 7. The spacecraft were not equipped to dock with each other, but maintained station-keeping for more than 20 minutes. Schirra later commented:\n\nThe first docking of two spacecraft was achieved on March 16, 1966 when Gemini 8, under the command of Neil Armstrong, rendezvoused and docked with an unmanned Agena Target Vehicle. Gemini 6 was to have been the first docking mission, but had to be cancelled when that mission's Agena vehicle was destroyed during launch.\n\nThe Soviets carried out the first automated, unmanned docking between Cosmos 186 and Cosmos 188 on October 30, 1967.\n\nThe first Soviet cosmonaut to attempt a manual docking was Georgy Beregovoy who unsuccessfully tried to dock his Soyuz 3 craft with the unmanned Soyuz 2 in October 1968. He was able to bring his craft from to as close as , but was unable to dock before exhausting his maneuvering fuel.\n\nThe Soviet's first successful manned docking occurred on January 16, 1969 when Soyuz 4 and Soyuz 5 docked and exchanged two crew members.\n\nThe first rendezvous of two spacecraft from different countries took place on July 17, 1975, when an Apollo spacecraft docked with a Soyuz spacecraft as part of the Apollo-Soyuz Test Project.\n\nThe first multiple space docking took place when both Soyuz 26 and Soyuz 27 were docked to the Salyut 6 space station during January 1978.\n\nA rendezvous takes place each time a spacecraft brings crew members or supplies to an orbiting space station. The first spacecraft to do this was Soyuz 11, which successfully docked with the Salyut 1 station on June 7, 1971. Human spaceflight missions have successfully made rendezvous with six Salyut stations, with Skylab, with \"Mir\" and with the International Space Station (ISS). Currently Soyuz spacecraft are used at approximately six month intervals to transport crew members to and from ISS.\nRobotic spacecraft are also used to rendezvous with and resupply space stations. Soyuz and Progress spacecraft have automatically docked with both \"Mir\" and the ISS using the Kurs docking system, Europe's Automated Transfer Vehicle also uses this system to dock with the Russian segment of the ISS. Several unmanned spacecraft use NASA's berthing mechanism rather than a docking port. The Japanese H-II Transfer Vehicle (HTV), SpaceX Dragon, and Orbital Sciences' Cygnus spacecraft all maneuver to a close rendezvous and maintain station-keeping, allowing the ISS Canadarm2 to grapple and move the spacecraft to a berthing port on the US segment. The Russian segment only uses docking ports so it is not possible for HTV, Dragon and Cygnus to find a berth there.\n\nSpace rendezvous has been used for a variety of other purposes, including recent service missions to the Hubble Space Telescope. Historically, for the missions of Project Apollo that landed astronauts on the Moon, the ascent stage of the Apollo Lunar Module would rendezvous and dock with the Apollo Command/Service Module in lunar orbit rendezvous maneuvers. Also, the STS-49 crew rendezvoused with and attached a rocket motor to the Intelsat VI F-3 communications satellite to allow it to make an orbital maneuver.\n\nPossible future rendezvous may be made by a yet to be developed automated Hubble Robotic Vehicle (HRV), and by the CX-OLEV, which is being developed for rendezvous with a geosynchronous satellite that has run out of fuel. The CX-OLEV would take over orbital stationkeeping and/or finally bring the satellite to a graveyard orbit, after which the CX-OLEV can possibly be reused for another satellite. Gradual transfer from the geostationary transfer orbit to the geosynchronous orbit will take a number of months, using Hall effect thrusters.\n\nAlternatively the two spacecraft are already together, and just undock and dock in a different way:\n\nThe standard technique for rendezvous and docking is to dock an active vehicle, the \"chaser\", with a passive \"target\". This technique has been used successfully for the Gemini, Apollo, Apollo/Soyuz, Salyut, Skylab, Mir, ISS, and Tiāngōng programs.\n\nTo properly understand spacecraft rendezvous it is essential to understand the relation between spacecraft velocity and orbit. A spacecraft in a certain orbit cannot arbitrarily alter its velocity. Each orbit correlates to a certain orbital velocity. If the spacecraft fires thrusters and increases (or decreases) its velocity it will obtain a different orbit, one that correlates to the higher (or lower) velocity. For circular orbits, higher orbits have a lower orbital velocity. Lower orbits have a higher orbital velocity.\n\nFor orbital rendezvous to occur, both spacecraft must be in the same orbital plane, and the phase of the orbit (the position of the spacecraft in the orbit) must be matched. The \"chaser\" is placed in a slightly lower orbit than the target. The lower the orbit, the higher the orbital velocity. The difference in orbital velocities of chaser and target is therefore such that the chaser is faster than the target, and catches up with it.\n\nOnce the two spacecraft are sufficiently close, the chaser's orbit is synchronized with the target's orbit. That is, the chaser will be accelerated. This increase in velocity carries the chaser to a higher orbit. The increase in velocity is chosen such that the chaser approximately assumes the orbit of the target. Stepwise, the chaser closes in on the target, until proximity operations (see below) can be started.\nIn the very final phase, the closure rate is reduced by use of the active vehicle's reaction control system.\nDocking typically occurs at a rate of to .\n\nSpace rendezvous of an active, or \"chaser,\" spacecraft with an (assumed) passive spacecraft may be divided into several phases, and typically starts with the two spacecraft in separate orbits, typically separated by more than :\n\nA variety of spacecraft control techniques may be used to effect the translational and rotational maneuvers necessary for proximity operations and docking.\n\nThe two most common methods of approach for proximity operations are in-line with the flight path of the spacecraft (called V-bar, as it is along the velocity vector of the target) and perpendicular to the flight path along the line of the radius of the orbit (called R-bar, as it is along the radial vector, with respect to Earth, of the target).\nThe chosen method of approach depends on safety, spacecraft / thruster design, mission timeline, and, especially for docking with the ISS, on the location of the assigned docking port.\n\nThe V-bar approach is an approach of the \"chaser\" horizontally along the passive spacecraft's velocity vector. That is, from behind or from ahead, and in the same direction as the orbital motion of the passive target. The motion is parallel to the target's orbital velocity.\nIn the V-bar approach from behind, the chaser fires small thrusters to increase its velocity in the direction of the target. This, of course, also drives the chaser to a higher orbit. To keep the chaser on the V-vector, other thrusters are fired in the radial direction. If this is omitted (for example due to a thruster failure), the chaser will be carried to a higher orbit, which is associated with an orbital velocity lower than the target's. Consequently, the target moves faster than the chaser and the distance between them increases. This is called a \"natural braking effect\", and is a natural safeguard in case of a thruster failure.\n\nSTS-104 was the third Space Shuttle mission to conduct a V-bar arrival at the International Space Station. The V-bar, or velocity vector, extends along a line directly ahead of the station. Shuttles approach the ISS along the V-bar when docking at the PMA-2 docking port.\n\nThe R-bar approach consists of the chaser moving below or above the target spacecraft, along its radial vector. The motion is orthogonal to the orbital velocity of the passive spacecraft.\nWhen below the target the chaser fires radial thrusters to close in on the target. By this it increases its altitude. However, the orbital velocity of the chaser remains unchanged (thruster firings in the radial direction have no effect on the orbital velocity). Now in a slightly higher position, but with an orbital velocity that does not correspond to the local circular velocity, the chaser slightly falls behind the target. Small rocket pulses in the orbital velocity direction are necessary to keep the chaser along the radial vector of the target. If these rocket pulses are not executed (for example due to a thruster failure), the chaser will move away from the target. This is a \"natural braking effect\". For the R-bar approach, this effect is stronger than for the V-bar approach, making the R-bar approach the safer one of the two.\nGenerally, the R-bar approach from below is preferable, as the chaser is in a lower (faster) orbit than the target, and thus \"catches up\" with it. For the R-bar approach from above, the chaser is in a higher (slower) orbit than the target, and thus has to wait for the target to approach it.\n\nAstrotech proposed meeting ISS cargo needs with a vehicle which would approach the station, \"using a traditional nadir R-bar approach.\" The nadir R-bar approach is also used for flights to the ISS of H-II Transfer Vehicles, and of SpaceX Dragon vehicles.\n\nAn approach of the active, or \"chaser,\" spacecraft horizontally from the side and orthogonal to the orbital plane of the passive spacecraft—that is, from the side and out-of-plane of the orbit of the passive spacecraft—is called a Z-bar approach.\n\n\n"}
{"id": "12594834", "url": "https://en.wikipedia.org/wiki?curid=12594834", "title": "Steady state visually evoked potential", "text": "Steady state visually evoked potential\n\nIn neurology and neuroscience research, steady state visually evoked potentials (SSVEP) are signals that are natural responses to visual stimulation at specific frequencies. When the retina is excited by a visual stimulus ranging from 3.5 Hz to 75 Hz, the brain generates electrical activity at the same (or multiples of) frequency of the visual stimulus.\n\nThis technique is used widely with electroencephalographic research regarding vision and attention. SSVEPs are useful in research because of the excellent signal-to-noise ratio and relative immunity to artifacts. SSVEPs also provide a means to characterize preferred frequencies of neocortical dynamic processes. SSVEP is generated by stationary localized sources and distributed sources that exhibit characteristics of wave phenomena.\n\n\n"}
{"id": "17422687", "url": "https://en.wikipedia.org/wiki?curid=17422687", "title": "Stop Abuse for Everyone", "text": "Stop Abuse for Everyone\n\nStop Abuse For Everyone (SAFE) is a domestic violence and conservative lobbying organization started in 1996. It was formed in Portland, Oregon and now is based out of Bakersfield, California. Stop Abuse For Everyone advocates for what they term an \"inclusive\" model of domestic violence. They state that their focus is on groups that are \"lacking in services\", such as men, gay, lesbian, and transgender victims, and the elderly.\n\nSAFE claims that men are more victimized by physical abuse than women and \"men were also the victims of psychological aggression and control over sexual or reproductive health more often than women.\"\n\nStop Abuse For Everyone is an organization that professes to advocate for the plight of abused men and other \"underserved\" domestic violence groups, such as LGBT victims.\n\nSAFE has made appearances at national conferences on domestic violence, media coverage, and lobbies to government entities in the United States.\n\nSAFE was founded by Jade Rubick in 1996, and became a non-profit organization in 2001 with co-founders Phil Cook and Stanley Green.\n\nSAFE is a national organization. One of the many community organizations working to extend SAFE to the community is Women in Distress (WID), a nationally accredited, state-certified, full service domestic violence center in Broward County, Florida.\n"}
{"id": "24538180", "url": "https://en.wikipedia.org/wiki?curid=24538180", "title": "Superficial charm", "text": "Superficial charm\n\nSuperficial charm (or insincere charm or glib charm) is the tendency to be smooth, engaging, charming, slick and verbally facile.\n\nThe phrase often appears in lists of attributes of psychopathic personalities, such as in Hervey M. Cleckley's \"The Mask of Sanity\", and Robert D. Hare's Hare Psychopathy Checklist.\n\nAssociated expressions are \"charm offensive\", \"turning on the charm\" and \"superficial smile\".\n\nClassical rhetoric had early singled out the critical category of the superficial charmer, whose merit was purely verbal, without underlying substance.\n\nContemporary interest in superficial charm goes back to Hervey M. Cleckley's classic study (1941) of the sociopath: since his work it has become widely accepted that the sociopath/psychopath was characterised by superficial charm and a disregard for other people's feelings. According to Hare, \"Psychopathic charm is not in the least shy, self-conscious, or afraid to say anything.\" \n\nSubsequent studies have refined, but not perhaps fundamentally altered, Cleckley's initial assessment. In the latest diagnostic review, Cleckley's mix of intelligence and superficial charm has been redefined to reflect a more deviant demeanour, talkative, slick, and insincere. A distinction can also be drawn between a subtle, self-effacing kind of sociopathic charm, and a more expansive, exhilarating spontaneity which serves to give the sociopath a sort of animal magnetism.\n\nThe authors of the book describe a five phase model of how a typical workplace psychopath climbs to and maintains power. In phase one (entry), the psychopath will use highly developed social skills and charm to obtain employment by an organisation. Corporate psychopaths within organizations may be singled out for rapid promotion because of their polish, charm, and cool decisiveness. Their superficial charm may be misinterpreted as charisma.\n\nThe term also occurs in Hotchkiss' discussion of narcissists: \"Their superficial charm can be enchanting.\" For such figures, however, there is no substance behind the romantic gestures, which only serve to feed the narcissist's own ego. \n\nNarcissists are known as manipulative in a charming way, entrapping their victims through a façade of understanding into suspending their self-protective behaviour and lowering their personal boundaries. Closely related is the way impostors are able to make people fall in love with them to satisfy their narcissistic needs, without reciprocating in any real sense or returning their feelings.\n\nSocial chameleons have been described as adept in social intelligence, able to make a charming good impression, yet at the price of their own true motivations. Their ability to manage impressions well often leads to success in areas like the theatre, salesmanship, or politics and diplomacy. But when lacking a sense of their own inner needs, such superficial extraverts may end up (despite their charm) as rootless chameleons, endlessly taking their social cues from other people.\n\nSimilarly, for the histrionic personality, the attention seeking through superficial charm may only reinforce the splitting of the real self from the public presentation in a vicious circle.\n\nSuperficial charmers, in their more benign manifestations, can produce a variety of positive results, their conversational skills providing light-hearted entertainment in social settings through their ability to please.\n\nA \"charm offensive\" is a related concept meaning a publicity campaign, usually by politicians, that attempts to attract supporters by emphasizing their charisma or trustworthiness. The first recorded use of the expression is in the California newspaper \"The Fresno Bee Republican\" in October 1956.\n\nF. Scott Fitzgerald explored the destructive consequences of excess charm in stories like \"Magnetism\", maintaining that charm, for those who had it, had a life of its own, demanding constant use to keep it in peak condition.\n\nCritics object that there are few objective criteria whereby to distinguish superficial from genuine charm; and that as part of the conventional niceties of politeness, we all regularly employ superficial charm in everyday life: conveying superficial solidarity and fictitious benevolence to all social interactions.\n\n\n"}
{"id": "1635732", "url": "https://en.wikipedia.org/wiki?curid=1635732", "title": "Torture by proxy", "text": "Torture by proxy\n\nTorture by proxy is collusion by one government in the abuse of prisoners by another. The United States has \"renditioned\" prisoners to nations known to practice torture. In the case of the United Kingdom, the government of Prime Minister Tony Blair is alleged to have colluded in the torture of prisoners by Libya.\n\nThe frequency with which the US government has chosen to employ the practice of transferring prisoners to countries that practice torture has fluctuated from one administration to the next. Before the September 11 attacks, renditions to countries that practice torture were sporadic and ad hoc. Afterwards, the Bush administration created a dedicated rendition bureaucracy and streamlined procedures which radically expanded abductions for torture by proxy, most commonly sending victims to be abused in Egypt, sometimes to Syria and Morocco. Despite protestations that it does not condone torture, recently the Obama administration has been accused of transferring prisoners to face brutal treatment in Afghanistan, Iraq, and Somalia.\n\nColluding governments use proxy torturers in order to support the deception that they have no knowledge of, or participation in, torture.\n"}
{"id": "5363", "url": "https://en.wikipedia.org/wiki?curid=5363", "title": "Video game", "text": "Video game\n\nA video game is an electronic game that involves interaction with a user interface to generate visual feedback on a video device such as a TV screen or computer monitor. The word \"video\" in \"video game\" traditionally referred to a raster display device, but as of the 2000s, it implies any type of display device that can produce two- or three-dimensional images. Some theorists categorize video games as an art form, but this designation is controversial.\n\nThe electronic systems used to play video games are known as platforms; in addition to general-purpose computers like a laptop/desktop being used, there are devices created exclusively for the playing of video games. Platforms range from large mainframe computers to small handheld computing devices. Video games are developed and released for specific platforms; for example, a video game that is available to Steam may not be available to Xbox One. Specialized video games such as arcade games, in which the video game components are housed in a large, typically coin-operated chassis, while common in the 1980s in video arcades, have gradually declined due to the widespread availability of affordable home video game consoles (e.g., PlayStation 4, Xbox One and Nintendo Switch) and video games on desktop/laptops and smartphones.\n\nThe input device used for games, the game controller, varies across platforms. Common controllers include gamepads, joysticks, mouse devices, keyboards, the touchscreens of mobile devices, or even a person's body, using a Kinect sensor. Players view the game on a display device such as a television or computer monitor or sometimes on virtual reality head-mounted display goggles. There are often game sound effects, music and voice actor lines which come from loudspeakers or headphones. Some games in the 2000s include haptic, vibration-creating effects, force feedback peripherals and virtual reality headsets.\n\nIn the 2010s, the commercial importance of the video game industry is increasing. The emerging Asian markets and mobile games on smartphones in particular are driving the growth of the industry. As of 2015, video games generated sales of US$74 billion annually worldwide, and were the third-largest segment in the U.S. entertainment market, behind broadcast and cable TV.\n\nEarly games used interactive electronic devices with various display formats. The earliest example is from 1947—a \"Cathode ray tube Amusement Device\" was filed for a patent on 25 January 1947, by Thomas T. Goldsmith Jr. and Estle Ray Mann, and issued on 14 December 1948, as U.S. Patent 2455992. Inspired by radar display technology, it consisted of an analog device that allowed a user to control a vector-drawn dot on the screen to simulate a missile being fired at targets, which were drawings fixed to the screen. Other early examples include: The Nimrod computer at the 1951 Festival of Britain; \"OXO\" a tic-tac-toe Computer game by Alexander S. Douglas for the EDSAC in 1952; \"Tennis for Two\", an electronic interactive game engineered by William Higinbotham in 1958; \"Spacewar!\", written by MIT students Martin Graetz, Steve Russell, and Wayne Wiitanen's on a DEC PDP-1 computer in 1961; and the hit ping pong-style \"Pong\", a 1972 game by Atari. Each game used different means of display: NIMROD used a panel of lights to play the game of Nim, OXO used a graphical display to play tic-tac-toe \"Tennis for Two\" used an oscilloscope to display a side view of a tennis court, and \"Spacewar!\" used the DEC PDP-1's vector display to have two spaceships battle each other.\n\nIn 1971, \"Computer Space\", created by Nolan Bushnell and Ted Dabney, was the first commercially sold, coin-operated video game. It used a black-and-white television for its display, and the computer system was made of 74 series TTL chips. The game was featured in the 1973 science fiction film \"Soylent Green\". \"Computer Space\" was followed in 1972 by the Magnavox Odyssey, the first home console. Modeled after a late 1960s prototype console developed by Ralph H. Baer called the \"Brown Box\", it also used a standard television. These were followed by two versions of Atari's \"Pong\"; an arcade version in 1972 and a home version in 1975 that dramatically increased video game popularity. The commercial success of \"Pong\" led numerous other companies to develop \"Pong\" clones and their own systems, spawning the video game industry.\n\nA flood of \"Pong\" clones eventually led to the video game crash of 1977, which came to an end with the mainstream success of Taito's 1978 shooter game \"Space Invaders\", marking the beginning of the golden age of arcade video games and inspiring dozens of manufacturers to enter the market. The game inspired arcade machines to become prevalent in mainstream locations such as shopping malls, traditional storefronts, restaurants, and convenience stores. The game also became the subject of numerous articles and stories on television and in newspapers and magazines, establishing video gaming as a rapidly growing mainstream hobby. \"Space Invaders\" was soon licensed for the Atari VCS (later known as Atari 2600), becoming the first \"killer app\" and quadrupling the console's sales. This helped Atari recover from their earlier losses, and in turn the Atari VCS revived the home video game market during the second generation of consoles, up until the North American video game crash of 1983. The home video game industry was revitalized shortly afterwards by the widespread success of the Nintendo Entertainment System, which marked a shift in the dominance of the video game industry from the United States to Japan during the third generation of consoles.\n\nThe term \"platform\" refers to the specific combination of electronic components or computer hardware which, in conjunction with software, allows a video game to operate. The term \"system\" is also commonly used. The distinctions below are not always clear and there may be games that bridge one or more platforms. In addition to laptop/desktop computers and mobile devices, there are other devices which have the ability to play games but are not primarily video game machines, such as PDAs and graphing calculators.\n\nIn common use a \"PC game\" refers to a form of media that involves a player interacting with a personal computer connected to a video monitor. Personal computers are not dedicated game platforms, so there may be differences running the same game in different hardware, also the openness allows some features to developers like reduced software cost, increased flexibility, increased innovation, emulation, creation of modifications (\"mods\"), open hosting for online gaming (in which a person plays a video game with people who are in a different household) and others.\n\nA \"console game\" is played on a specialized electronic device (\"home video game console\") that connects to a common television set or composite video monitor, unlike PCs, which can run all sorts of computer programs, a console is a dedicated video game platform manufactured by a specific company. Usually consoles only run games developed for it, or games from other platform made by the same company, but never games developed by its direct competitor, even if the same game is available on different platforms. It often comes with a specific game controller. Major console platforms include Xbox, PlayStation, and Nintendo.\n\nA \"handheld\" gaming device is a small, self-contained electronic device that is portable and can be held in a user's hands. It features the console, a small screen, speakers and buttons, joystick or other game controllers in a single unit. Like consoles, handhelds are dedicated platforms, and share almost the same characteristics. Handheld hardware usually is less powerful than PC or console hardware. Some handheld games from the late 1970s and early 1980s could only play one game. In the 1990s and 2000s, a number of handheld games used cartridges, which enabled them to be used to play many different games.\n\n\"Arcade game\" generally refers to a game played on an even more specialized type of electronic device that is typically designed to play only one game and is encased in a special, large coin-operated cabinet which has one built-in console, controllers (joystick, buttons, etc.), a CRT screen, and audio amplifier and speakers. Arcade games often have brightly painted logos and images relating to the theme of the game. While most arcade games are housed in a vertical cabinet, which the user typically stands in front of to play, some arcade games use a tabletop approach, in which the display screen is housed in a table-style cabinet with a see-through table top. With table-top games, the users typically sit to play. In the 1990s and 2000s, some arcade games offered players a choice of multiple games. In the 1980s, video arcades were businesses in which game players could use a number of arcade video games. In the 2010s, there are far fewer video arcades, but some movie theaters and family entertainment centers still have them.\n\nThe web browser has also established itself as platform in its own right in the 2000s, while providing a cross-platform environment for video games designed to be played on a wide spectrum of platforms. In turn, this has generated new terms to qualify classes of web browser-based games. These games may be identified based on the website that they appear, such as with \"Miniclip\" games. Others are named based on the programming platform used to develop them, such as Java and Flash games.\n\nWith the advent of standard operating systems for mobile devices such as iOS and Android and devices with greater hardware performance, mobile gaming has become a significant platform. These games may utilize unique features of mobile devices that are not necessary present on other platforms, such as global positing information and camera devices to support augmented reality gameplay. Mobile games also led into the development of microtransactions as a valid revenue model for casual games.\n\nVirtual reality (VR) games generally require players to use a special head-mounted unit that provides stereoscopic screens and motion tracking to immerse a player within virtual environment that responds to their head movements. Some VR systems include control units for the player's hands as to provide a direct way to interact with the virtual world. VR systems generally require a separate computer, console, or other processing device that couples with the head-mounted unit.\n\nA new platform of video games emerged in late 2017 in which users could take ownership of game assets (digital assets) using Blockchain technologies. An example of this is Cryptokitties.\n\nA video game, like most other forms of media, may be categorized into genres. Video game genres are used to categorize video games based on their gameplay interaction rather than visual or narrative differences. A video game genre is defined by a set of gameplay challenges and are classified independent of their setting or game-world content, unlike other works of fiction such as films or books. For example, a shooter game is still a shooter game, regardless of whether it takes place in a fantasy world or in outer space.\n\nBecause genres are dependent on content for definition, genres have changed and evolved as newer styles of video games have come into existence. Ever advancing technology and production values related to video game development have fostered more lifelike and complex games which have in turn introduced or enhanced genre possibilities (e.g., virtual pets), pushed the boundaries of existing video gaming or in some cases add new possibilities in play (such as that seen with games specifically designed for devices like Sony's EyeToy). Some genres represent combinations of others, such as massively multiplayer online role-playing games, or, more commonly, MMORPGs. It is also common to see higher level genre terms that are collective in nature across all other genres such as with action, music/rhythm or horror-themed video games.\n\nCasual games derive their name from their ease of accessibility, simple to understand gameplay and quick to grasp rule sets. Additionally, casual games frequently support the ability to jump in and out of play on demand. Casual games as a format existed long before the term was coined and include video games such as Solitaire or Minesweeper which can commonly be found pre-installed with many versions of the Microsoft Windows operating system. Examples of genres within this category are match three, hidden object, time management, puzzle or many of the tower defense style games. Casual games are generally available through app stores and online retailers such as PopCap, Zylom and GameHouse or provided for free play through web portals such as Newgrounds. While casual games are most commonly played on personal computers, phones or tablets, they can also be found on many of the on-line console system download services (e.g., the PlayStation Network, WiiWare or Xbox Live).\n\nSerious games are games that are designed primarily to convey information or a learning experience to the player. Some serious games may even fail to qualify as a video game in the traditional sense of the term. Educational software does not typically fall under this category (e.g., touch typing tutors, language learning programs, etc.) and the primary distinction would appear to be based on the game's primary goal as well as target age demographics. As with the other categories, this description is more of a guideline than a rule. Serious games are games generally made for reasons beyond simple entertainment and as with the core and casual games may include works from any given genre, although some such as exercise games, educational games, or propaganda games may have a higher representation in this group due to their subject matter. These games are typically designed to be played by professionals as part of a specific job or for skill set improvement. They can also be created to convey social-political awareness on a specific subject.\nOne of the longest-running serious games franchises is \"Microsoft Flight Simulator\", first published in 1982 under that name. The United States military uses virtual reality based simulations, such as VBS1 for training exercises, as do a growing number of first responder roles (e.g., police, firefighters, EMTs). One example of a non-game environment utilized as a platform for serious game development would be the virtual world of \"Second Life\", which is currently used by several United States governmental departments (e.g., NOAA, NASA, JPL), Universities (e.g., Ohio University, MIT) for educational and remote learning programs and businesses (e.g., IBM, Cisco Systems) for meetings and training.\n\nTactical media in video games plays a crucial role in making a statement or conveying a message on important relevant issues. This form of media allows for a broader audience to be able to receive and gain access to certain information that otherwise may not have reached such people. An example of tactical media in video games would be newsgames. These are short games related to contemporary events designed to illustrate a point. For example, Take Action Games is a game studio collective that was co-founded by Susana Ruiz and has made successful serious games. Some of these games include \"Darfur is Dying\", \"Finding Zoe\", and \"In The Balance\". All of these games bring awareness to important issues and events.\n\nOn 23 September 2009, U.S. President Barack Obama launched a campaign called \"Educate to Innovate\" aimed at improving the technological, mathematical, scientific and engineering abilities of American students. This campaign states that it plans to harness the power of interactive games to help achieve the goal of students excelling in these departments. This campaign has stemmed into many new opportunities for the video game realm and has contributed to many new competitions. Some of these competitions include the Stem National Video Game Competition and the Imagine Cup. Both of these bring a focus to relevant and important current issues through gaming. www.NobelPrize.org entices the user to learn about information pertaining to the Nobel prize achievements while engaging in a fun video game. There are many different types and styles of educational games, including counting to spelling to games for kids, to games for adults. Some other games do not have any particular targeted audience in mind and intended to simply educate or inform whoever views or plays the game.\n\nVideo game can use several types of input devices to translate human actions to a game, the most common game controllers are keyboard and mouse for \"PC games, consoles usually come with specific gamepads, handheld consoles have built in buttons. Other game controllers are commonly used for specific games like racing wheels, light guns or dance pads. Digital cameras can also be used as game controllers capturing movements of the body of the player.\n\nAs technology continues to advance, more can be added onto the controller to give the player a more immersive experience when playing different games. There are some controllers that have presets so that the buttons are mapped a certain way to make playing certain games easier. Along with the presets, a player can sometimes custom map the buttons to better accommodate their play style. On keyboard and mouse, different actions in the game are already preset to keys on the keyboard. Most games allow the player to change that so that the actions are mapped to different keys that are more to their liking. The companies that design the controllers are trying to make the controller visually appealing and also feel comfortable in the hands of the consumer.\n\nAn example of a technology that was incorporated into the controller was the touchscreen. It allows the player to be able to interact with the game differently than before. The person could move around in menus easier and they are also able to interact with different objects in the game. They can pick up some objects, equip others, or even just move the objects out of the players path. Another example is motion sensor where a persons movement is able to be captured and put into a game. Some motion sensor games are based on where the controller is. The reason for that is because there is a signal that is sent from the controller to the console or computer so that the actions being done can create certain movements in the game. Other type of motion sensor games are webcam style where the player moves around in front of it, and the actions are repeated by a game character.\n\nVideo game development and authorship, much like any other form of entertainment, is frequently a cross-disciplinary field. Video game developers, as employees within this industry are commonly referred, primarily include programmers and graphic designers. Over the years this has expanded to include almost every type of skill that one might see prevalent in the creation of any movie or television program, including sound designers, musicians, and other technicians; as well as skills that are specific to video games, such as the game designer. All of these are managed by producers.\n\nIn the early days of the industry, it was more common for a single person to manage all of the roles needed to create a video game. As platforms have become more complex and powerful in the type of material they can present, larger teams have been needed to generate all of the art, programming, cinematography, and more. This is not to say that the age of the \"one-man shop\" is gone, as this is still sometimes found in the casual gaming and handheld markets, where smaller games are prevalent due to technical limitations such as limited RAM or lack of dedicated 3D graphics rendering capabilities on the target platform (e.g., some PDAs).\n\nWith the growth of the size of development teams in the industry, the problem of cost has increased. Development studios need to be able to pay their staff a competitive wage in order to attract and retain the best talent, while publishers are constantly looking to keep costs down in order to maintain profitability on their investment. Typically, a video game console development team can range in sizes of anywhere from 5 to 50 people, with some teams exceeding 100. In May 2009, one game project was reported to have a development staff of 450. The growth of team size combined with greater pressure to get completed projects into the market to begin recouping production costs has led to a greater occurrence of missed deadlines, rushed games and the release of unfinished products.\nA phenomenon of additional game content at a later date, often for additional funds, began with digital video game distribution known as downloadable content (DLC). Developers can use digital distribution to issue new storylines after the main game is released, such as Rockstar Games with \"Grand Theft Auto IV\" (\"\" and \"\"), or Bethesda with \"Fallout 3\" and its expansions. New gameplay modes can also become available, for instance, \"Call of Duty\" and its zombie modes, a multiplayer mode for \"Mushroom Wars\" or a higher difficulty level for \"\". Smaller packages of DLC are also common, ranging from better in-game weapons (\"Dead Space\", \"Just Cause 2\"), character outfits (\"LittleBigPlanet\", \"Minecraft\"), or new songs to perform (\"SingStar\", \"Rock Band\", \"Guitar Hero\").\n\nA variation of downloadable content is expansion packs. Unlike DLC, expansion packs add a whole section to the game that either already exists in the game's code or is developed after the game is released. Expansions add new maps, missions, weapons, and other things that weren't previously accessible in the original game. An example of an expansion is Bungie's \"Destiny\", which had the \"\" expansion. The expansion added new weapons, new maps, and higher levels, and remade old missions.\n\nExpansions are added to the base game to help prolong the life of the game itself until the company is able to produce a sequel or a new game altogether. Developers may plan out their game's life and already have the code for the expansion in the game, but inaccessible by players, who later unlock these expansions, sometimes for free and sometimes at an extra cost. Some developers make games and add expansions later, so that they could see what additions the players would like to have. There are also expansions that are set apart from the original game and are considered a stand-alone game, such as Ubisoft's expansion \" Freedom's Cry\", which features a different character than the original game.\n\nMany games produced for the PC are designed such that technically oriented consumers can modify the game. These mods can add an extra dimension of replayability and interest. Developers such as id Software, Valve Corporation, Crytek, Bethesda, Epic Games and Blizzard Entertainment ship their games with some of the development tools used to make the game, along with documentation to assist mod developers. The Internet provides an inexpensive medium to promote and distribute mods, and they may be a factor in the commercial success of some games. This allows for the kind of success seen by popular mods such as the \"Half-Life\" mod \"Counter-Strike\".\n\nCheating in computer games may involve cheat codes and hidden spots implemented by the game developers, modification of game code by third parties, or players exploiting a software glitch. Modifications are facilitated by either cheat cartridge hardware or a software trainer. Cheats usually make the game easier by providing an unlimited amount of some resource; for example weapons, health, or ammunition; or perhaps the ability to walk through walls. Other cheats might give access to otherwise unplayable levels or provide unusual or amusing features, like altered game colors or other graphical appearances.\n\nSoftware errors not detected by software testers during development can find their way into released versions of computer and video games. This may happen because the glitch only occurs under unusual circumstances in the game, was deemed too minor to correct, or because the game development was hurried to meet a publication deadline. Glitches can range from minor graphical errors to serious bugs that can delete saved data or cause the game to malfunction. In some cases publishers will release updates (referred to as \"patches\") to repair glitches. Sometimes a glitch may be beneficial to the player; these are often referred to as exploits.\n\nEaster eggs are hidden messages or jokes left in games by developers that are not part of the main game. Easter eggs are secret responses that occur as a result of an undocumented set of commands. The results can vary from a simple printed message or image, to a page of programmer credits or a small videogame hidden inside an otherwise serious piece of software. Videogame cheat codes are a specific type of Easter egg, in which entering a secret command will unlock special powers or new levels for the player.\n\nAlthough departments of computer science have been studying the technical aspects of video games for years, theories that examine games as an artistic medium are a relatively recent development in the humanities. The two most visible schools in this emerging field are ludology and narratology. Narrativists approach video games in the context of what Janet Murray calls \"Cyberdrama\". That is to say, their major concern is with video games as a storytelling medium, one that arises out of interactive fiction. Murray puts video games in the context of the Holodeck, a fictional piece of technology from \"Star Trek\", arguing for the video game as a medium in which the player is allowed to become another person, and to act out in another world. This image of video games received early widespread popular support, and forms the basis of films such as \"Tron\", \"eXistenZ\" and \"The Last Starfighter\".\n\nLudologists break sharply and radically from this idea. They argue that a video game is first and foremost a game, which must be understood in terms of its rules, interface, and the concept of play that it deploys. Espen J. Aarseth argues that, although games certainly have plots, characters, and aspects of traditional narratives, these aspects are incidental to gameplay. For example, Aarseth is critical of the widespread attention that narrativists have given to the heroine of the game \"Tomb Raider\", saying that \"the dimensions of Lara Croft's body, already analyzed to death by film theorists, are irrelevant to me as a player, because a different-looking body would not make me play differently... When I play, I don't even see her body, but see through it and past it.\" Simply put, ludologists reject traditional theories of art because they claim that the artistic and socially relevant qualities of a video game are primarily determined by the underlying set of rules, demands, and expectations imposed on the player.\n\nWhile many games rely on emergent principles, video games commonly present simulated story worlds where emergent behavior occurs within the context of the game. The term \"emergent narrative\" has been used to describe how, in a simulated environment, storyline can be created simply by \"what happens to the player.\" However, emergent behavior is not limited to sophisticated games. In general, any place where event-driven instructions occur for AI in a game, emergent behavior will exist. For instance, take a racing game in which cars are programmed to avoid crashing, and they encounter an obstacle in the track: the cars might then maneuver to avoid the obstacle causing the cars behind them to slow and/or maneuver to accommodate the cars in front of them and the obstacle. The programmer never wrote code to specifically create a traffic jam, yet one now exists in the game.\n\nAn emulator is a program that replicates the behavior of a video game console, allowing games to run on a different platform from the original hardware. Emulators exist for PCs, smartphones and consoles other than the original. Emulators are generally used to play old games, hack existing games, translate unreleased games in a specific region, or add enhanced features to games like improved graphics, speed up or down, bypass regional lockouts, or online multiplayer support.\n\nSome manufacturers have released official emulators for their own consoles. For example, Nintendo's Virtual Console allows users to play games for old Nintendo consoles on the Wii, Wii U, and 3DS. Virtual Console is part of Nintendo's strategy for deterring video game piracy. In November 2015, Microsoft launched backwards compatibility of Xbox 360 games on Xbox One console via emulation. Also, Sony announced relaunching PS2 games on PS4 via emulation. According to \"Sony Computer Entertainment America v. Bleem\", creating an emulator for a proprietary video game console is legal. However, Nintendo claims that emulators promote the distribution of illegally copied games.\n\nThe November 2005 Nielsen Active Gamer Study, taking a survey of 2,000 regular gamers, found that the U.S. games market is diversifying. The age group among male players has expanded significantly in the 25–40 age group. For casual online puzzle-style and simple mobile cell phone games, the gender divide is more or less equal between men and women. More recently there has been a growing segment of female players engaged with the aggressive style of games historically considered to fall within traditionally male genres (e.g., first-person shooters). According to the ESRB, almost 41% of PC gamers are women. Participation among African-Americans is lower. One survey of over 2000 game developers returned responses from only 2.5% who identified as black.\n\nWhen comparing today's industry climate with that of 20 years ago, women and many adults are more inclined to be using products in the industry. While the market for teen and young adult men is still a strong market, it is the other demographics which are posting significant growth. The Entertainment Software Association (ESA) provides the following summary for 2011 based on a study of almost 1,200 American households carried out by Ipsos MediaCT:\n\nA 2006 academic study, based on a survey answered by 10,000 gamers, identified the gaymers (gamers that identify as gay) as a demographic group. A follow-up survey in 2009 studied the purchase habits and content preferences of people in the group. Based on the study by NPD group in 2011, approximately 91 percent of children aged 2–17 play games.\n\nVideo game culture is a worldwide new media subculture formed around video games and game playing. As computer and video games have increased in popularity over time, they have had a significant influence on popular culture. Video game culture has also evolved over time hand in hand with internet culture as well as the increasing popularity of mobile games. Many people who play video games identify as gamers, which can mean anything from someone who enjoys games to someone who is passionate about it. As video games become more social with multiplayer and online capability, gamers find themselves in growing social networks. Gaming can both be entertainment as well as competition, as a new trend known as electronic sports is becoming more widely accepted. In the 2010s, video games and discussions of video game trends and topics can be seen in social media, politics, television, film and music.\n\nMultiplayer video games are those that can be played either competitively, sometimes in Electronic Sports, or cooperatively by using either multiple input devices, or by hotseating. \"Tennis for Two\", arguably the first video game, was a two player game, as was its successor \"Pong\". The first commercially available game console, the Magnavox Odyssey, had two controller inputs. Since then, most consoles have been shipped with two or four controller inputs. Some have had the ability to expand to four, eight or as many as 12 inputs with additional adapters, such as the Multitap. Multiplayer arcade games typically feature play for two to four players, sometimes tilting the monitor on its back for a top-down viewing experience allowing players to sit opposite one another.\n\nMany early computer games for non-PC descendant based platforms featured multiplayer support. Personal computer systems from Atari and Commodore both regularly featured at least two game ports. PC-based computer games started with a lower availability of multiplayer options because of technical limitations. PCs typically had either one or no game ports at all. Network games for these early personal computers were generally limited to only text based adventures or MUDs that were played remotely on a dedicated server. This was due both to the slow speed of modems (300-1200-bit/s), and the prohibitive cost involved with putting a computer online in such a way where multiple visitors could make use of it. However, with the advent of widespread local area networking technologies and Internet based online capabilities, the number of players in modern games can be 32 or higher, sometimes featuring integrated text and/or voice chat. Massively multiplayer online game (MMOs) can offer extremely high numbers of simultaneous players; \"Eve Online\" set a record with 65,303 players on a single server in 2013.\n\nIt has been shown that action video game players have better hand–eye coordination and visuo-motor skills, such as their resistance to distraction, their sensitivity to information in the peripheral vision and their ability to count briefly presented objects, than nonplayers. Researchers found that such enhanced abilities could be acquired by training with action games, involving challenges that switch attention between different locations, but not with games requiring concentration on single objects.\nIt has been suggested by a few studies that online/offline video gaming can be used as a therapeutic tool in the treatment of different mental health concerns.\n\nIn Steven Johnson's book, \"Everything Bad Is Good for You\", he argues that video games in fact demand far more from a player than traditional games like \"Monopoly\". To experience the game, the player must first determine the objectives, as well as how to complete them. They must then learn the game controls and how the human-machine interface works, including menus and HUDs. Beyond such skills, which after some time become quite fundamental and are taken for granted by many gamers, video games are based upon the player navigating (and eventually mastering) a highly complex system with many variables. This requires a strong analytical ability, as well as flexibility and adaptability. He argues that the process of learning the boundaries, goals, and controls of a given game is often a highly demanding one that calls on many different areas of cognitive function. Indeed, most games require a great deal of patience and focus from the player, and, contrary to the popular perception that games provide instant gratification, games actually delay gratification far longer than other forms of entertainment such as film or even many books. Some research suggests video games may even increase players' attention capacities.\n\nLearning principles found in video games have been identified as possible techniques with which to reform the U.S. education system. It has been noticed that gamers adopt an attitude while playing that is of such high concentration, they do not realize they are learning, and that if the same attitude could be adopted at school, education would enjoy significant benefits. Students are found to be \"learning by doing\" while playing video games while fostering creative thinking.\n\nThe U.S. Army has deployed machines such as the PackBot and UAV vehicles, which make use of a game-style hand controller to make it more familiar for young people. According to research discussed at the 2008 Convention of the American Psychological Association, certain types of video games can improve the gamers' dexterity as well as their ability to do problem solving. A study of 33 laparoscopic surgeons found that those who played video games were 27 percent faster at advanced surgical procedures and made 37 percent fewer errors compared to those who did not play video games. A second study of 303 laparoscopic surgeons (82 percent men; 18 percent women) also showed that surgeons who played video games requiring spatial skills and hand dexterity and then performed a drill testing these skills were significantly faster at their first attempt and across all 10 trials than the surgeons who did not play the video games first.\n\nAn experiment carried out by Richard De Lisi and Jennifer Woldorf demonstrates the positive effect that video games may have on spatial skills. De Lisi and Woldorf took two groups of third graders, one control group and one experiment group. Both groups took a paper-and-pencil test of mental rotation skills. After this test, the experiment group only played 11 sessions of the game \"Tetris\". This game was chosen as it requires mental rotation. After this game, both groups took the test again. The result showed that the scores of the experiment group raised higher than that of the control group, thereby confirming this theory.\n\nThe research showing benefits from action games has been questioned due to methodological shortcomings, such as recruitment strategies and selection bias, potential placebo effects, and lack of baseline improvements in control groups. In addition, many of the studies are cross-sectional, and of the longitudinal interventional trials, not all have found effects. A response to this pointed out that the skill improvements from action games are more broad than predicted, such as mental rotation, which is not a common task in action games. Action gamers are not only better at ignoring distractions, but also at focusing on the main task.\n\nLike other media, such as rock music (notably heavy metal music and gangsta rap), video games have been the subject of objections, controversies and censorship, for instance because of depictions of violence, criminal activities, sexual themes, alcohol, tobacco and other drugs, propaganda, profanity or advertisements. Critics of video games include parents' groups, politicians, religious groups, scientists and other advocacy groups. Claims that some video games cause addiction or violent behavior continue to be made and to be disputed.\n\nThere have been a number of societal and scientific arguments about whether the content of video games change the behavior and attitudes of a player, and whether this is reflected in video game culture overall. Since the early 1980s, advocates of video games have emphasized their use as an expressive medium, arguing for their protection under the laws governing freedom of speech and also as an educational tool. Detractors argue that video games are harmful and therefore should be subject to legislative oversight and restrictions. The positive and negative characteristics and effects of video games are the subject of scientific study. Results of investigations into links between video games and addiction, aggression, violence, social development, and a variety of stereotyping and sexual morality issues are debated. A study was done that showed that young people who have had a greater exposure to violence in video games ended up behaving more aggressively towards people in a social environment.\n\nIn 2018, the World Health Organization declared \"gaming disorder\" a mental disorder for people who are addicted to video games. Studies have shown video games can negatively effect health and mental state for some players.\n\nIn spite of the negative effects of video games, certain studies indicate that they may have value in terms of academic performance, perhaps because of the skills that are developed in the process. “When you play ... games you’re solving puzzles to move to the next level and that involves using some of the general knowledge and skills in maths, reading and science that you’ve been taught during the day,” said Alberto Posso an Associate Professor at the Royal Melbourne Institute of Technology, after analysing data from the results of standardized testing completed by over 12,000 high school students across Australia. As summarized by \"The Guardian\", the study (published in the \"International Journal of Communication\") \"found that students who played online games almost every day scored 15 points above average in maths and reading tests and 17 points above average in science.\" However, the reporter added an important comment that was not provided by some of the numerous Web sites that published a brief summary of the Australian study: \"[the] methodology cannot prove that playing video games were the cause of the improvement.\" \"The Guardian\" also reported that a Columbia University study indicated that extensive video gaming by students in the 6 to 11 age group provided a greatly increased chance of high intellectual functioning and overall school competence.\n\nIn an interview with CNN, Edward Castronova, a professor of Telecommunications at Indiana University Bloomington said he was not surprised by the outcome of the Australian study but also discussed the issue of causal connection. \"Though there is a link between gaming and higher math and science scores, it doesn't mean playing games caused the higher scores. It could just be that kids who are sharp are looking for a challenge, and they don't find it on social media, and maybe they do find it on board games and video games,\" he explained.\n\nVideo games have also been proven to raise self-esteem and build confidence. It gives people an opportunity to do things that they cannot do offline, and to discover new things about themselves. There is a social aspect to gaming as well – research has shown that a third of video game players make good friends online. As well as that, video games are also considered to be therapeutic as it helps to relieve stress. Although short term, studies have shown that children with developmental delays gain a temporary physical improvement in health when they interact and play video games on a regular, and consistent basis due to the cognitive benefits and the use of hand eye coordination.\n\nSelf-determination theory (SDT) is a macro theory of human motivation based around competence, autonomy, and relatedness to facilitate positive outcomes. SDT provides a framework for understanding the effects of playing video games; well-being, problem solving, group relations, physical activities. These factors can be measured to determine the effect video games can have on people.\n\nThe ability to create an ideal image of ones self and being given multiple options to change that image gives a sense of satisfaction. This topic has much controversy; it is unknown whether this freedom can be beneficial to ones character or detrimental. With increased game usage, a players can become too invested in a fictionally generated character, where the desire to look that way overpowers the enjoyment of the game. Players see this character creation as entertainment and a release, creating a self-image they could not obtain in reality, bringing comfort outside of the game from lack of investment to the fictional character. Problems that arise based on character design may be link to personality disorders.\n\nCognitive skills can be enhanced through repetition of puzzles, memory games, spatial abilities and attention control. Most video games present opportunities to use these skills with the ability to try multiple times even after failure. Many of these skills can be translated to reality and problem solving. This allows the player to learn from mistakes and fully understand how and why a solution to a problem may work. Some researchers believe that continual exposure to challenges may lead players to develop greater persistence over time after a study was shown that frequent players spent more time on puzzles in task that did not involve video games. Although players were shown to spend more time on puzzles, much of that could have been due to the positive effects of problem solving in games, which involve forming strategy and weighing option before testing a solution.\n\nIn a study that followed students through school, students that played video games showed higher levels of problem solving than students who did not. This contradicts the previous study in that higher success rate was seen in video game players. Time being a factor for problem solving led to different conclusions in the different studies. See video game controversies for more.\n\nOnline gaming being on the rise allows for video game players to communicate and work together in order to accomplish a certain task. Being able to work as a group in a game translates well to reality and jobs, where people must work together to accomplish a task. Research on players in violent and non-violent games show similar results, where the players relations improved to improve synergy.\n\nWith the introduction of Wii Fit and VR (virtual reality), \"exergame\" popularity has been increasing, allowing video game players to experience more active rather than sedentary game play. Mobile apps have tried to expand this concept with the introduction of \"Pokémon Go,\" which involves walking to progress in the game. Due to \"exergaming\" being relatively new, there is still much to be researched. No major differences were seen in tests with children that played on the Wii vs. a non-active game after 12 weeks. Testing a larger range of ages may show better results.\n\nVideo game laws vary from country to country. Console manufacturers usually exercise tight control over the games that are published on their systems, so unusual or special-interest games are more likely to appear as PC games. Free, casual, and browser-based games are usually played on available computers, mobile phones, tablet computers or PDAs. \n\nVarious organisations in different regions are responsible for giving content ratings to video games.\n\nThe Entertainment Software Rating Board (ESRB) gives video games maturity ratings based on their content. For example, a game might be rated \"T\" for \"Teen\" if the game contained obscene words or violence. If a game contains explicit violence or sexual themes, it is likely to receive an \"M\" for \"Mature\" rating, which means that no one under 17 should play it. The rating \"A/O\", for \"Adults Only\", indicates games with massive violence or nudity. There are no laws that prohibit children from purchasing \"M\" rated games in the United States. Laws attempting to prohibit minors from purchasing \"M\" rated games were established in California, Illinois, Michigan, Minnesota, and Louisiana, but all were overturned on the grounds that these laws violated the First Amendment. However, many stores have opted to not sell such games to children anyway. One of the most controversial games of all time, \"Manhunt 2\" by Rockstar Studios, was given an AO rating by the ESRB until Rockstar could make the content more suitable for a mature audience.\n\nPan European Game Information (PEGI) is a system that was developed to standardize the game ratings in all of Europe (not just European Union, although the majority are EU members), the current members are: all EU members, except Germany and the 10 accession states; Norway; Switzerland. Iceland is expected to join soon, as are the 10 EU accession states. For all PEGI members, they use it as their sole system, with the exception of the UK, where if a game contains certain material, it must be rated by BBFC. The PEGI ratings are legally binding in Vienna and it is a criminal offence to sell a game to someone if it is rated above their age.\n\nStricter game rating laws mean that Germany does not operate within the PEGI. Instead, they adopt their own system of certification which is required by law. The Unterhaltungssoftware Selbstkontrolle (USK) checks every game before release and assigns an age rating to it – either none (white), 6 years of age (yellow), 12 years of age (green), 16 years of age (blue) or 18 years of age (red). It is forbidden for anyone, retailers, friends or parents alike, to allow a child access to a game for which he or she is underage. If a game is considered to be harmful to young people (for example because of extremely violent, pornographic or racist content), it may be referred to the Bundesprüfstelle für jugendgefährdende Medien (BPjM) who may opt to place it on the Index upon which the game may not be sold openly or advertised in the open media. It is considered a felony to supply these games to a child.\n\nThe Computer Entertainment Rating Organization (CERO) that rates video games and PC games (except dating sims, visual novels, and eroge) in Japan with levels of rating that informs the customer of the nature of the product and for what age group it is suitable. It was established in July 2002 as a branch of Computer Entertainment Supplier's Association, and became an officially recognized non-profit organization in 2003. These ratings are: \n\nAccording to the market research firm SuperData, as of May 2015, the global games market was worth US$74.2 billion. By region, North America accounted for $23.6 billion, Asia for $23.1 billion, Europe for $22.1 billion and South America for $4.5 billion. By market segment, mobile games were worth $22.3 billion, retail games 19.7 billion, free-to-play MMOs 8.7 billion, social games $7.9 billion, PC DLC 7.5 billion, and other categories $3 billion or less each.\n\nIn the United States, also according to SuperData, the share of video games in the entertainment market grew from 5% in 1985 to 13% in 2015, becoming the third-largest market segment behind broadcast and cable television. The research firm anticipated that Asia would soon overtake North America as the largest video game market due to the strong growth of free-to-play and mobile games.\n\nSales of different types of games vary widely between countries due to local preferences. Japanese consumers tend to purchase much more handheld games than console games and especially PC games, with a strong preference for games catering to local tastes. Another key difference is that, despite the decline of arcades in the West, arcade games remain an important sector of the Japanese gaming industry. In South Korea, computer games are generally preferred over console games, especially MMORPG games and real-time strategy games. Computer games are also popular in China.\n\nGaming conventions are an important showcase of the industry. The annual gamescom in Cologne in August is the world's leading expo for video games in attendance. The E3 in June in Los Angeles is also of global importance, but is an event for industry insiders only. The Tokyo Game Show in September is the main fair in Asia. Other notable conventions and trade fairs include Brasil Game Show in October, Paris Games Week in October–November, EB Games Expo (Australia) in October, KRI, ChinaJoy in July and the annual Game Developers Conference. Some publishers, developers and technology producers also host their own regular conventions, with BlizzCon, QuakeCon, Nvision and the X shows being prominent examples.\n\nShort for electronic sports, are video game competitions played most by professional players individually or in teams that gained popularity from the late 2000s, the most common genres are fighting, first-person shooter (FPS), multiplayer online battle arena (MOBA) and real-time strategy. There are certain games that are made for just competitive multiplayer purposes. With those type of games, players focus entirely one choosing the right character or obtaining the right equipment in the game to help them when facing other players. Tournaments are held so that people in the area or from different regions can play against other players of the same game and see who is the best. Major League Gaming (MLG) is a company that reports tournaments that are held across the country. The players that compete in these tournaments are given a rank depending on their skill level in the game that they choose to play in and face other players that play that game. The players that also compete are mostly called professional players for the fact that they have played the game they are competing in for many, long hours. Those players have been able to come up with different strategies for facing different characters. The professional players are able to pick a character to their liking and be able to master how to use that character very effectively. With strategy games, players tend to know how to get resources quick and are able to make quick decisions about where their troops are to be deployed and what kind of troops to create.\n\nCreators will nearly always copyright their games. Laws that define copyright, and the rights that are conveyed over a video game, vary from country to country. Usually a fair use copyright clause allows consumers some ancillary rights, such as for a player of the game to stream a game online. This is a vague area in copyright law, as these laws predate the advent of video games. This means that rightsholders often must define what they will allow a consumer to do with the video game.\n\nThere are many video game museums around the world, including the National Videogame Museum in Frisco, Texas, which serves as the largest museum wholly dedicated to the display and preservation of the industry's most important artifacts. Europe hosts video game museums such as the Computer Games Museum in Berlin and the Museum of Soviet Arcade Machines in Moscow and Saint-Petersburg. The Museum of Art and Digital Entertainment in Oakland, California is a dedicated video game museum focusing on playable exhibits of console and computer games. The Video Game Museum of Rome is also dedicated to preserving video games and their history. The International Center for the History of Electronic Games at The Strong in Rochester, New York contains one of the largest collections of electronic games and game-related historical materials in the world, including a exhibit which allows guests to play their way through the history of video games. The Smithsonian Institution in Washington, D.C. has three video games on permanent display: \"Pac-Man\", \"Dragon's Lair\", and \"Pong\".\n\nThe Museum of Modern Art has added a total of 20 video games and one video game console to its permanent Architecture and Design Collection since 2012. In 2012, the Smithsonian American Art Museum ran an exhibition on \"The Art of Video Games\". However, the reviews of the exhibit were mixed, including questioning whether video games belong in an art museum.\n\n\n"}
{"id": "57768170", "url": "https://en.wikipedia.org/wiki?curid=57768170", "title": "You Belong to Me: Sex, Race and Murder in the South", "text": "You Belong to Me: Sex, Race and Murder in the South\n\nYou Belong to Me: Sex, Race and Murder in the South is a 2015 American documentary film produced by Hilary Saltzman, Kitty Potapow, and Jude Hagin and directed by John Cork. The film works to uncover the hidden truths of the Ruby McCollum case of 1952. McCollum, the richest African American woman in Suwannee County, Florida, shot White physician and politician Clifford Leroy Adams four times with her revolver.\n\nHagin discovered this story when meeting with the late Dr. James Haskins, award-winning author and English professor at The University of Florida. Haskins provided Hagin with a copy of the William Bradford Huie book , saying “this story needs to be told.” Upon reading Huie’s book, Hagin disagreed with the assertion that McCollum had been in a consensual, sexual relationship with Adams and that the Adams shooting was over a money dispute. “I could not get that into my head,” she said. “I could not wrap my head around the notion that an African-American woman in 1952 would willingly have a relationship with a town’s sainted physician.”\n\nHagin’s pursuit of the truth led to an exploration of “paramour rights,” the assumption that White men had the right to use Black women for sex, in the Jim Crow South. While there had been previous books and documentaries on Ruby McCollum, You Belong to Me: Sex, Race and Murder in the South is the first project to get witness accounts, including interviews with McCollum’s friends and family and the last surviving juror from the trial. \"I wanted to get family members on both sides to tell their side of the story,\" Hagin said.\n\nThe film premiered at the 2014 LA Femme International Film Festival on October 17, 2014 and was released for VOD on February 1, 2015 in time for Black History Month.\n\nOn the morning of August 3rd, 1952, Ruby McCollum left her children in her car and walked into the “colored entrance” of Dr. Clifford Leroy Adams’ office and shot him four times into his body. The White doctor and state Senator-elect died on the scene while McCollum returned home to warm milk for her baby.\n\nProsecutors claimed that McCollum shot Dr. Adams in a dispute over a $116 medical bill, despite the fact that she had $1,800 in her purse and was the wealthiest Black woman in town. The judge ordered the jury to disregard McCollum’s testimony that Dr. Adams repeatedly raped her and that the shooting was in self-defense. In fact, Dr. Adams fathered a daughter by McCollum, but the jury was not allowed to even look at her when she was presented as evidence of the rape. McCollum was found guilty and sentenced to death. Years later, she won an insanity plea and was released from prison in 1974.\n\nThe documentary uses first-hand accounts of both the McCollum and Adams families as well as the last living juror in an attempt to uncover the truth behind the murder. Was McCollum actually insane or was she truly acting in self-defense after years of rape? You Belong to Me: Sex, Race and Murder in the South looks at previous conclusions and offers up its own in an effort to answer that question.\n\nThe film received mostly positive response from critics. Mike Spain of Irish Film Critic gave it 4 out of 5 stars and wrote, “There is plenty to the story to keep the interest of the audience. One of the film’s strengths is a lot of the story is told by people who were in Live Oak at the time or had a stake in the outcome. A juror, town’s people, and Ruby’s relatives. It adds to the authenticity of the documentary.” In her review for Emertainment Monthly, Emily Solomon graded the film with an A- and wrote, “What is so intriguing about the way this film is put together is how the mystery unravels. The murder is established early on in the film, with new twists and turns revealed throughout that shows McCollum’s motivation to commit in the first place.”\n\n"}
