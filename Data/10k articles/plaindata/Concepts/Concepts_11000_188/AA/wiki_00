{"id": "1873897", "url": "https://en.wikipedia.org/wiki?curid=1873897", "title": "Ajiva", "text": "Ajiva\n\nAjiva (Sanskrit) is anything that has no soul or life, the polar opposite of \"jīva\" (soul). Because \"ajiva\" has no life, it does not accumulate \"karma\" and cannot die. Examples of ajiva include chairs, computers, paper, plastic, etc. According to Jain philosophy, \"Ajiva\" can be divided into two kinds, with form and without form.\n\nIn Jainism, there are five categories which \"ajīva\" can be placed into. Out of these, four categories, \"Dharma\" (medium of motion), \"Adharma\" (medium of rest), \"Akasha\" (space) and \"Pudgala\" (matter) are described as the \"asti-kaya dravya's\" (substances which possess constituent parts extending in space) while the fifth category \"Kala\" is an \"anasti-kaya dravya\" (which has no extension in space).\n\nDharmastikaya is formed from the two words: Dharma & Astikaya. Dharma here isn't referring to religion, but instead its referring to the medium of motion. Astikay itself is formed of two words: Asti & Kaya. Asti means space, body or mode and Kaya means collection. So Astikaya means a collection of spaces or regions. Dharmastikaya denotes the medium of motion for things in the universe. In the absence of this medium, living things (i.e. jiva) would not be able to move.\n\nAdharmastikaya is also formed from two words: Adharma & Astikaya. Adharma in this case means the\nmedium of rest. In the absence of this medium, Living things or jiva would continuously move.\n\nThe infinity of space, called \"ākāśa\" in Sanskrit, is divided by the Jain philosophy into two parts, namely, the \"lokākāśa\" (loka+ākāśa), that is the space occupied by the universe, and the \"alokākāśa\" (a not, and lokākāśa), the portion beyond the universe. The \"lokākāśa\" is the portion in which are to be found the remaining five substances, i.e., \"Jīvas\", Matter, Time, Dharma and Adharma; but the \"alokākāśa\" is the region of pure space containing no other substance and lying stretched on all sides beyond bounds of the three worlds (the entire universe). At the summit of the \"lokākāśa\" is the \"Siddhashila\" (abode of the liberated souls).\n\nThe word Pudgala is made up of two terms: \"Pud\" means supplement (integration) and \"Gala\" means disintegration (division). In other words, what continuously changes by supplementation and/or division (\"purayanti galanti cha\") is called Pudgala or matter. All matter in the universe is called Pudgala. Pudgala has form and shape. Pudgala can be experienced by touching, tasting, smelling, or seeing. Like Jiva Pudgala is also mobile. According to Jainism, The karma particles that attach to our souls are also Pudgala. Pudgala can only be divided and subdivided to a certain extent that it is not possible to further subdivide it. This indivisible part of Pudgala, which is separated from the main pudgala, is called Paramanu. A paramanu is much more minute than even an atom. When a Paramanu is attached to the main pudgala, it is called a Pradesha. These subatomic Paramanus are too minute to be detected by normal vision, but they can be combined. Thus when a paramanu is combined with one or more other paramanus, they are called a skandha which are more or less like a molecules. Part of skandha is called desha. Such sknadhas may be large or small. Small skandhas may be invisible to the eye, but they can be seen when the combinations are larger.\n\nKala refers to time that brings forth changes. Past, present, and future are the different modes of time and are measured in terms of years, months, days, hours, minutes or seconds. For practical purposes a second happens to be the finest measurement of time. Jainism however, recognizes a very small measurement of time known as \"samaya\" which is an infinitely small part of a second. \n\n\"Kala\" (time) is infinite, but there are cycles (\"kalachakra\"s) in it. Each cycle having two eras of equal duration described as the \"avasarpini\" and the \"utsarpini\". The former is a descending era in which virtue gradually decreases. The latter is an ascending era in which the reverse takes place. The present era is stated to be the former.\n\n<br>Indivisible time = 1 Samaya (smallest unit of measurement)\n<br>Countless Samayas = 1 Avalika\n<br>16,777,216 Avalikas = 1 Muhurt (48 minutes)\n<br>30 Muhurtas = 1 day and night\n<br>15 Days and nights = 1 Paksha\n<br>2 Pakshas = 1 Month\n<br>12 Months = 1 Year\n<br>Countless years = 1 Palyopam\n<br>1,000,000,000,000,000 (one quintillion) Palyopams = 1 Sagaropam; one quintillion is 1,000,000,000,000,000,000\n<br>1,000,000,000,000,000 Sagaropams = 1 Utsarpini or Avasarpini\n<br>1 Utsarpini or Avasarpini = 1 Kalchakra (One time cycle)\n\n"}
{"id": "33665214", "url": "https://en.wikipedia.org/wiki?curid=33665214", "title": "Alluvial diagram", "text": "Alluvial diagram\n\nAlluvial diagrams are a type of flow diagram originally developed to represent changes in network structure over time.\nIn allusion to both their visual appearance and their emphasis on flow, alluvial diagrams are named after alluvial fans that are \nnaturally formed by the soil deposited from streaming water.\n\nIn an alluvial diagram, blocks represent clusters of nodes, and stream fields between the blocks represent changes in the composition of these clusters over time. The height of a block represents the size of the cluster and the height of a stream field represents the size of the components contained in both blocks connected by the stream field.\n\nAlluvial diagrams were originally developed to visualize structural change in large complex networks. They can be used to visualize any type of change in group composition between states or over time and include statistical information to reveal significant change. \nAlluvial diagrams highlight important structural changes that can be further emphasized by color, and make identification of major transitions easy.\n\nAlluvial diagrams can also be used to illustrate patterns of flow on a fixed network over time. The \"Users Flow\" feature of Google Analytics uses alluvial diagrams to graphically represent how visitors move among the nodes (individual pages) on a web sites.\n\n\n\n"}
{"id": "32193640", "url": "https://en.wikipedia.org/wiki?curid=32193640", "title": "Antaryamin", "text": "Antaryamin\n\nThe Antaryāmin, in terms of Hindu philosophy, is related to the \"inner-self\", the \"inner-controller\" or the \"inner-guidance\" that exists in a person and itself manifests on an intuitive way to the one manifesting it. It recalls a teacher or a guru that resides within and once it is manifested - for a higher context of knowledge guidance - usually after one summons or prays for it, the Antaryamin comes to help. In some cases, the Antaryamin is asked to be manifested in order to resolve non-intellectual issues, also performing miracles to the one asking.\n\nOn the Hindu scriptures, we find the concept of the Antaryamin as the Inner Controller after the building of the elementary concepts regarding a deity or an ego - on a unity way - of which is controlling, somehow to a compared intelligence, the universe and all that is. This intelligence may also refer to what is called consciousness or awareness that is far superior, hence capable of doing superior things.\n\n\n"}
{"id": "1483944", "url": "https://en.wikipedia.org/wiki?curid=1483944", "title": "Battered woman syndrome", "text": "Battered woman syndrome\n\nBattered woman syndrome (BWS) emerged in the 1990s from several murder cases in England in which women had killed violent partners in response to what they claimed was cumulative abuse, rather than in response to a single provocative act. Feminist groups, particularly Southall Black Sisters and Justice for Women, challenged the legal definition of \"provocation\", and in a series of appeals against murder convictions secured the courts' recognition of battered woman syndrome. An early work describing the syndrome is Lenore E. Walker's \"The Battered Woman\" (1979).\n\nUntil the mid-1990s, the legal definition of \"provocation\" in England had relied on Devlin J in \"R v Duffy\" [1949] 1 All ER 932: \"Provocation is some act, or series of acts done (or words spoken) ... which would cause in any reasonable person and actually causes in the accused, a sudden and temporary loss of self-control, rendering the accused so subject to passion as to make him or her for the moment not master of his or her mind.\" Three cases helped to change this: \"R v Ahluwalia\" [1992] 4 AER 889; \"R v Humphreys\" [1995] 4 All ER 1008); and \"R v Thornton (No 2)\" [1996] 2 AER 1023.\n\nThe courts in Australia, Canada, New Zealand, the United Kingdom, and the United States have accepted the extensive and growing body of research showing that battered women can use force to defend themselves and sometimes kill their abusers because of the abusive and sometimes life-threatening situation in which they find themselves, acting in the firm belief that there is no other way than to kill for self-preservation. The courts have recognized that this evidence may support a variety of defenses to a charge of murder or to mitigate the sentence if convicted of lesser offenses. Battered woman syndrome is not a legal defense in and of itself, but may legally constitute:\n\n\nHowever, in 1994, as part of the Violence Against Women Act, the United States Congress ordered an investigation into the role of battered woman syndrome expert testimony in the courts to determine its validity and usefulness. In 1997, they published the report of their investigation, titled \"The Validity and Use of Evidence Concerning Battering and Its Effects in Criminal Trials\". \"The federal report ultimately rejected all terminology related to the battered woman syndrome...noting that these terms were 'no longer useful or appropriate (Rothenberg, \"Social Change\", 782). Instead of using the term \"battered woman\", the terminology \"battering and its effects\" became acceptable. The decision to change this terminology was based on a changing body of research indicating there is more than one pattern to battering and a more inclusive definition was necessary to more accurately represent the realities of domestic violence.\n\nIn \"R v Ahluwalia\" (1992) 4 AER 889 a woman (Kiranjit Ahluwalia), created napalm and set fire to the bed of her husband, Deepak, after he had gone to sleep. He suffered severe burns over 40% of his body and died 10 days later in the hospital. He allegedly had attempted to break her ankles and burn her with a hot iron on the night of her attack. Accusing him of domestic violence and marital rape, she claimed provocation. The judge directed the jury to consider whether, if she did lose her self-control, a reasonable person having the characteristics of a well-educated married Asian woman living in England would have lost her self-control given her husband's provocation. On appeal, it was argued that he should have directed the jury to consider a reasonable person suffering from 'battered woman syndrome'. Having considered fresh medical evidence, the Court of Appeal ordered a retrial on the basis that the new evidence showed an arguable case of diminished responsibility in English law.\n\nSimilarly, in \"R v Thornton (No 2)\" (1996) 2 AER 1023 the battered wife adduced fresh evidence that she had a personality disorder and the Court of Appeal ordered a retrial considering that, if the evidence had been available at the original trial, the jury might have reached a different decision. The victim does not have to be in a position to carry out the threats immediately.\n\nIn \"R v Charlton\" (2003) EWCA Crim 415, following threats of sexual and violent abuse against herself and her daughter, the defendant killed her obsessive, jealous, controlling partner while he was restrained by handcuffs, blindfolded and gagged as part of their regular sexual activity. The term of five years' imprisonment was reduced to three and a half years because of the terrifying threats made by a man determined to dominate and control the defendant's life. The threats created a genuine fear for the safety of herself and more significantly, her daughter, and this caused the defendant to lose control and make the ferocious attack.\n\nIn \"HM's AG for Jersey v Holley\" (2005) 3 AER 371, the Privy Council regarded the Court of Appeal precedent in \"Smith\" as wrongly decided, interpreting the Act as setting a purely objective standard. Thus, although the accused's characteristics were to be taken into account when assessing the gravity of the provocation, the standard of self-control to be expected was invariable except for the accused's age and sex. The defendant and the deceased both suffered from chronic alcoholism and had a violent and abusive relationship. The evidence was that the deceased was drunk and taunted him by telling him that she had sex with another man. The defendant then struck the deceased with an axe which was an accident of availability. Psychiatric evidence was that his consumption of alcohol was involuntary and that he suffered from a number of other psychiatric conditions which, independently of the effects of the alcohol, might have caused the loss of self-control and induced him to kill. Lord Nicholls said:\nSince the passage of the Coroners and Justice Act 2009, the defence of provocation—used in a number of the aforementioned cases—has been replaced with 'loss of control'.\n\nThe Law Commission Report on \"Partial Defences to Murder\" (2004) rejects the notion of creating a mitigatory defence to cover the use of excessive force in self-defence but accepts that the \"all or nothing\" effect of self-defence can produce unsatisfactory results in the case of murder.\n\nIn Australia, self-defence might be considered the most appropriate defence to a charge of murder for a woman who kills to protect her life or the lives of her children in a domestic violence context. It is about the rational act of a person who kills in order to save her (or his) own life. But the lack of success in raising self-defence in Australia for battered women has meant that provocation has been the main focus of the courts. In 2005, based on the Victorian Law Reform Commission's \"Defences to Homicide: Final Report\", the Victorian government announced changes to the homicide laws in that jurisdiction, which are intended to address this perceived imbalance. Under the new laws, victims of family violence will be able to put evidence of their abuse before the court as part of their defence, and argue self-defence even in the absence of an immediate threat, and where the response of killing involved greater force than the threatened harm.\n\nIn 1911 in Sault Ste. Marie, Angelina Napolitano, a 28-year-old, pregnant immigrant, killed her abusive husband Pietro with an axe after he tried to force her into prostitution. She confessed and was sentenced to hang after a brief trial, but during the delay before the sentence was carried out (a delay necessary to allow her to give birth to her child), a public campaign for her release began. Napolitano's supporters argued that the judge in the case had been wrong to throw out evidence of her long-standing abuse at Pietro's hands (including an incident five months before when he stabbed her nine times with a pocket knife). The federal cabinet eventually commuted her sentence to life imprisonment. She was the first woman in Canada to use the battered woman defense on a murder charge.\n\nThe Supreme Court of Canada set a precedent for the use of the battered women defence in the 1990 case of \"R. v. Lavallee\".\n\nIn \"R v Fate\" (1998) 16 CRNZ 88 a woman who had come to New Zealand from the small island of Nanumea, which is part of the Tuvalu Islands, received a two-year sentence for manslaughter by provocation. Mrs. Fate spoke no English and was isolated within a small close-knit Wellington community of 12 families, so she felt trapped in the abusive relationship.\n\nSimilarly, \"The Queen v Epifania Suluape\" (2002) NZCA 6, deals with a wife who pleaded provocation after she killed her husband with an axe when he proposed to leave her for another woman. There was some evidence of neglect, humiliation, and abuse but the court concluded that this was exaggerated. On appeal, the court was very conscious of the Samoan culture in New Zealand in restricting the power of the wife to act independently of her husband and reduced her sentence for manslaughter to five years.\n\nA report of the New Zealand Law Commission examines not only violence by men against women, but also violence by women against men and in same-sex relationships.\n\n"}
{"id": "53802984", "url": "https://en.wikipedia.org/wiki?curid=53802984", "title": "Birthday-number effect", "text": "Birthday-number effect\n\nThe birthday-number effect is the subconscious tendency of people to prefer the numbers in the date of their birthday over other numbers. First reported in 1997 by Japanese psychologists Shinobu Kitayama and Mayumi Karasawa, the birthday-number effect has been replicated in various countries. It holds across age and gender. The effect is most prominent for numbers over 12.\n\nMost people like themselves; the birthday is associated with the self, and hence the numbers in the birthday are preferred, despite the fact that they appear in many other contexts. People who do not like themselves tend not to exhibit the birthday-number effect. A similar effect, the name-letter effect, has been found for letters: people tend to prefer the letters that are part of their name. The birthday-number effect and the name-letter effect are significantly correlated. In psychological assessments, the Number Preference Task is used to estimate implicit self-esteem.\n\nThere is some evidence that the effect has implications for real-life decisions. One lab study revealed an increase in a favourable attitude towards prices when they were secretly manipulated to match subjects' birthday dates, thus resulting in a higher chance of purchase. However a second study using birth year as price did not lead to the same result. A study of the liking of products found that participants with high self-esteem liked products better if the product names unknowingly involved their birthday number and letters of their name. Some field research into the impact of the birthday-number effect on bigger life decisions (e.g. where to live) is controversial.\n\nThroughout history, societies have had numbers they consider special. For example, in ancient Rome the number 7 was auspicious, in Maya civilisation the number 13 was sacred, in modern-day Japan people give three, five, or seven gifts for luck, and in China the number 8 is considered lucky and 4 is avoided whenever possible. In Western cultures the number 13 is often considered unlucky, hence the term triskaidekaphobia, fear of the number 13.\n\nControlled experiments with numbers date back to 1933 when the researcher Dietz asked Dutch people to name the first number to come to mind between 0 and 99. The number 7 was mentioned most, as it was in various later replicas of the study in other countries. The number 7 also came out on top in studies that asked people to name their favourite number. In an online poll by Alex Bellos, a columnist for \"The Guardian\", more than 30,000 people from all over the world submitted numbers, with 7 the most popular. All numbers under 100 were submitted at least once and nearly half of the numbers under 1,000. Marketing researchers King and Janiszewski investigated number preference in a different way. They showed undergraduate students random numbers and asked them to say quickly whether they liked the number, disliked it, or felt neutral. The number 100 had the highest proportion of people liking it (70%) and the lowest proportion of people disliking it (5%). The numbers 1 to 20 were liked by 9% more people than the higher numbers; the numbers that are the result of rote-learned multiplication tables (i.e. 2 × 2 to 10 × 10) were liked by 15% more people than the remaining numbers. The researchers concluded that number fluency predicts number preference: hence multiplication table numbers are preferred over prime numbers.\n\nThe closely related field of letter-preference research dates back to the 1950s. In 1985, Belgian psychologist Nuttin reported the unexpected finding that people tend to disproportionately prefer, unknowingly, the letters of their own name. The name-letter effect has been replicated in dozens of follow-up studies in different languages, cultures and alphabets, no matter whether participants selected their preferred letter from a random pair, or picked the top six of all letters in the alphabet, or rated each individual letter. Nuttin predicted that because the driving force behind the name-letter effect is an unconscious preference for anything connected to the self, there would also be a birthday-number effect.\n\nIn 1997, researchers Shinobu Kitayama and Mayumi Karasawa observed that studies repeatedly showed that Japanese people do not seek to maintain and enhance their self-esteem, unlike Europeans and Americans. Whereas research with Western participants found that, on average, people falsely believe they are better than average, that they take credit for successes and blame others for failures, and that they overestimate the chances of good fortune happening to them, studies with Japanese did not reveal such self-enhancing tendencies. In addition, in cross-cultural studies, Japanese reported self-esteem to be hurt more by failures than boosted by successes, the opposite of what was reported by Americans. All these studies involved participants being aware that their self-esteem was being evaluated, and hence they are said to be measures of explicit self-esteem. This made Kitayama and Karasawa wonder. It seemed unlikely to them that Japanese have no positive feelings attached to their selves. They hypothesized that somehow Japanese do not allow these feelings to be detected overtly. To test this, they ran two experiments that hid the aim of assessing self-esteem, measuring instead implicit self-esteem. Because by definition implicit self-esteem is not accessible to introspection, measures of it do not rely on direct self-reports but on the degree to which objects associated with the self generate positive versus negative thoughts. The first experiment was a replica of Nuttin's 1987 study of letter preference, looking for an effect tied to letters of the participant's name. The second experiment involved numbers, looking for an effect tied to numbers representing the day of the month a participant was born (between 1 and 31) and the month of their birthday (between 1 and 12).\n\nFor the letter experiment, they asked 219 Japanese undergraduate students to rate each of the 45 \"hiragana\", part of the Japanese writing system, according to how much they liked it. For the number experiment, they asked 269 Japanese undergraduate students to rate the numbers between 0 and 49 on attractiveness. The number 49 was chosen as the upper limit to mask the true aim of the study, which 31 (being the maximum number of days in a month) might have hinted at. Likewise, the number 0 was included for disguise. Participants had to give ratings on a six-point scale, ranging from 1, if they disliked the number very much, to 6, if they liked it very much. Once done, participants were asked for various demographic data, including their birthdays.\n\nAnalysis of the letter preference data revealed a name-letter effect: an enhanced liking for letters in the participant's own name. Analysis of the number preference data revealed a birthday-number effect. For each number, the researchers first calculated the mean liking by participants who did not have that number in their birthday. These means served as a baseline. For each participant 50 relative liking scores were computed between the baseline of a number and the actual preference. The mean liking scores for different types of numbers showed that participants disproportionately preferred numbers in their birthday. The effect was stronger for higher numbers, over 12, than for lower numbers. The effect was weakest for males and their birth month (only a 0.03 difference from the mean), and strongest for females and the day of their birthday (0.77 difference with the mean). Overall, women showed a greater liking for the numbers in their birthday than men did.\n\nKitayama and Karasawa concluded that the patterns in the findings from both experiments were most consistent with the hypothesis that the preference is due to an attachment to the self. These feelings leak out to stimuli that are closely associated with the self, not just names and birthdates, but also, implicitly, their constituent letters and numbers. Since most people like themselves, most people are found to have positive feelings for these constituent parts. The researchers suggested that the effect is stronger for higher numbers because in daily life these numbers are less saturated with other meanings, other than their associations with birthdays.\n\nAn alternative explanation for the birthday-number effect that had to be tested is mere exposure. If it were true that the numbers in one's birthday are used disproportionately in one's daily life, then the preference for numbers in one's birthday could simply be a preference for what is most frequent. Zajonc found in his 1960s and 1980s lab studies that familiarity can strongly influence preference, and coined the term \"mere exposure effect\". But Kitayama and Karasawa argued that even if people did see numbers from their own birthday more, this would still be negligible in comparison to the overall quantity of numbers they encounter in daily life. This is in line with the argument other researchers have used to rule out mere exposure as an explanation for the name-letter effect.\n\nKitayama and Karasawa concluded that Japanese people do indeed have warm feelings towards themselves, just like Americans and Europeans, but that these feelings are masked when explicitly asked for. They speculated that the reason for this masking lies in the Japanese tendency to attend to negative, undesirable features by way of improving the self.\n\nBy 2017, Kitayama and Karasawa's original study had been cited in over 300 scientific papers.\n\nThe first follow-up study looked at cultural differences. Blass, Schmitt, Jones, and O'Connell used US undergraduate students as participants to replicate the original study. In their paper presented at the American Psychological Association's annual conference in Chicago, in August 1997, they reported the same result: a preference for birthday numbers. They did find a much stronger effect though, which according to the researchers could be due to Americans' tendency towards self-enhancement.\n\nThe second follow-up study was done in 1998 by Kitayama and Uchida. They sought to investigate the relationship between a person's name-letter effect and his or her birthday-number effect, given that Kitayama and Karasawa had suspected a single driving force behind both. As they had predicted, Kitayama and Uchida found that within a person the two effects were correlated. Later studies confirmed this finding.\n\nIn 2000, Bosson, Swann and Pennebaker tested seven measures of implicit self-esteem, including the birthday-number task and name-letter task, and four measures of explicit self-esteem. They used a seven-point rating scale instead of the six-point scale Kitayama and Karasawa had used, and they only looked at the day of the birthday. On average, respondents scored their birthday number 0.73 higher than the other numbers. When the researchers retested all seven implicit self-esteem measures, the birthday-number task was one of three that produced similar results. From the weak or non-significant correlations between the implicit and explicit self-esteem measures they concluded that implicit and explicit self-esteem are tapping different underlying constructs.\n\nLater studies investigated aspects of the effect. Koole, Dijksterhuis, and van Knippenberg sought to explore how automatic the preference process was. They did this with both numbers and letters. They divided participants into two groups. The first group was asked to give quick, intuitive reactions stating preferences for the stimuli. The second group was asked to reason why they liked some numbers better than others and to analyse which features of the numbers they liked. As the researchers had predicted, they found that both the birthday-number effect and name-letter effect disappeared in the ‘thinking’ condition. They argued that thinking about reasons instigates deliberative overriding of implicit self-esteem effects. This conclusion was supported by looking at correlations between the effects: whereas in the feeling condition the strength of a participant's birthday-number effect was correlated to his or her name-letter effect, no such correlation was found in the thinking condition.\n\nJones, Pelham, Mirenberg, and Hetts investigated how the effect held up under so-called 'threats' to the self. Earlier research by Koole, Smeets, van Knippenberg, and Dijksterhuis had already shown that the name-letter effect is influenced by a perceived threat. Jones, Pelham, Mirenberg, and Hetts first made some participants write about a personal flaw and then gave all participants the Number Preference Task and the Letter Preference Task. What they found was consistent with previous findings: people who liked themselves a lot liked the numbers in their birthday and the letters of their name even more when an aspect of their self seemed under threat. This is predicted by the theory of unconscious self-enhancement. It can not be explained by mere exposure theory.\n\nNickell, Pederson, and Rossow looked for effects with significant years. They asked 83 undergraduate students to rate, on a scale from 1 to 7, how much they liked the years between 1976 and 2001, the months of the year, the seasons, times of day, and even types of pet in an attempt to disguise the aim of the study. Analysis of the data showed that participants liked the year of their birth much more than the average of the four years after they were born. The researchers also found that the year of high school graduation was also liked better than average. Of the months of the year, the most liked month was the one in which the participants were born.\n\nFalk, Heine, Takemura, Zhang and Hsu investigated the validity of implicit self-esteem measures to assess cultural differences. They subjected Canadian and Japanese participants to a series of tests, one of which was rating the numbers to 40 by how much participants liked them. Because the researchers saw little to no correlation between the various implicit self-esteem measures, they did not draw any conclusions about cultural differences. Stieger and Krizan explored cross-cultural differences in number preferences, specifically the day on which Christmas is celebrated as a contributor to number preference. They asked participants from six countries to rate numbers between 1 and 36. They found that in countries where gifts are exchanged on 24 December participants disproportionately preferred the number 24, whereas in countries that do this on 25 December participants preferred 25. They concluded that cultural influences need to be taken into account if these preferences are used to reflect individual differences.\n\nIn psychological assessments, the birthday-number effect has been exploited to measure implicit self-esteem. The Number Preference Task is often used in combination with the more popular Letter Preference Task, sometimes jointly called the Initials and Birthday Preference Task (IBPT). The most popular method to measure implicit self-esteem is the Implicit Association Test.\n\nThere is no standard method for applying the task. The most commonly used one is a rating task, which involves having participants judge all the numbers under a certain threshold (typically over 31 to mask the purpose of assessing connections to dates), indicating how much they like them on a 7-point rating scale. There is no standard algorithm for calculating implicit self-esteem. At least six algorithms are in use. In their meta-analysis of the name-letter effect, Stieger, Voracek, and Formann recommend using the ipsatized double-correction algorithm. The algorithms are typically applied to both the number of the day and of the month.\n\nStieger, Voracek, and Formann recommend that the task involve both letter preference and number preference, that it be administered twice, and that the instructions focus on liking rather than attractiveness. The Number Preference Task has been used to measure implicit self-esteem in contexts as diverse as parenting and mental habits.\n\nResearchers have looked for wider implications of the birthday-number effect on preferences, both inside and outside the lab. A body of controversial research under the umbrella of implicit egotism, starting with Pelham, Mirenberg, and Jones, has investigated whether people unknowingly make life decisions based on their name letters or birthday numbers. Skeptics say that the claim that letters influence life decisions is an extraordinary one that requires extraordinary evidence. Based on analysis of US field data Pelham et al. concluded that people disproportionately live in towns whose names contain numbers that match the numbers in their birthday. They looked at people who were born on 2 February, 3 March, 4 April, etc. up to 8 August, and at people who lived in towns with numbers in them, such as Two Rivers, Three Oaks, Four Corners, etc. But in his critical analysis of this study Simonsohn argued that its reliability is questionable, as it found only a small number of residents with matching numbers. Simonsohn tried to replicate the finding in different ways but without success. He found no effect of just the day of birthday on the town (e.g. the second of any month, not just February). He also found no effect of birthday number on street, address, or apartment number.\n\nJones, Pelham, Carvallo and Mirenberg investigated the influence of number preference on interpersonal attraction. In a lab study they showed US participants text profiles of people. The profiles came with a prominently displayed, seemingly arbitrary code that was explained as merely to help the researchers keep track of the profiles. One half of the participants were shown a code that matched their birthday (e.g. someone born on 8 September saw a partner profile with the code \"09-08\"); the other half a non-matching code (e.g. \"03-23\"). All participants were shown exactly the same profile. They had to rate how much they thought they would like the person in the profile. The results showed that participants liked the profiles significantly more when the code matched their own birthday numbers. Relative to participants in the control condition, participants in the birthday-association condition could at the end of the test more accurately recall the code, but only 5 of 110 participants mentioned the matching code as a potential influence. Jones et al. concluded that people’s preferences for their own birthday numbers are potent enough to influence people’s attraction to other people. Pelham and Carvallo subsequently looked at interpersonal attraction using field data. They used statewide marriage records to conclude that people disproportionately marry people who share their birthday numbers. They also found that brides disproportionately chose their own birthday numbers and birth months as wedding dates.\n\nCoulter and Grewal investigated if the birthday-number effect could be exploited in sales and marketing. Over 200 participants of an online survey were asked about an advertisement for a pasta dinner, where the price was secretly matched to the day of the month of their birthday. For example, someone born on the 16th of a certain month would see the price \"$39.16\". The researchers found that matching numbers increased price liking and purchase intention. When introducing a perceived threat to the self into the task, they found an exaggerated effect. From this they concluded that the positive affect linked to birthday-numbers transfers directly to consumers’ price predilections, and ultimately affects their purchase intentions. Keller and Gierl sought to replicate Coulter and Grewal's study. They manipulated the prices in advertisements for pizza and a music streaming service to match the birthday (day, year) of the participants in their lab study. They did not find any disproportionate liking of matching prices, neither for the year the participant was born in or the day. Keller and Gierl concluded that there must be some prerequisites such as priming stimuli to trigger the effect, although they suggested it is possible that their participants, who all happened to have been born between 1990 and 1999, saw their birthyear as price so often in real life that it had become too common.\n\nSmeets used name and birthday matching in a product-liking experiment. He made up product names for a DVD that matched both part of the participant's name and his or her birthday. For example, a participant named Mariëlle, born on 14 May, would get an ad for a DVD-player named \"DVD-Ma 14\" in the self-relevant condition and \"DVD-Pu 30\" in the control condition. He found that high self-esteem participants liked products more if the product names were self-relevant than if they were not. He also found the opposite happened among low self-esteem participants: they liked products better if they were not self-relevant.\n\n"}
{"id": "555206", "url": "https://en.wikipedia.org/wiki?curid=555206", "title": "By any means necessary", "text": "By any means necessary\n\nBy any means necessary is a translation of a phrase used by French intellectual Jean-Paul Sartre in his play \"Dirty Hands\". It entered the popular civil rights culture through a speech given by Malcolm X at the Organization of Afro-American Unity founding rally on June 28, 1964. It is generally considered to leave open all available tactics for the desired ends, including violence; however, the “necessary” qualifier adds a caveat—if violence is not necessary, then presumably, it should not be used.\n\nThe phrase is translation of a sentence used in French intellectual Jean-Paul Sartre's play \"Dirty Hands\":\n\nIt entered the popular culture through a speech given by Malcolm X in the last year of his life.\n\nIn the final scene of the 1992 movie \"Malcolm X\", Nelson Mandela—recently released after 27 years of political imprisonment—appears as a schoolteacher in a Soweto classroom. Yet Mandela informed director Spike Lee that he could not utter the famous final phrase \"by any means necessary\" on camera; fearing that the apartheid government would use it against him if he did. Lee obliged, and the final seconds of the film feature black-and-white footage of Malcolm X himself delivering the phrase.\n\n"}
{"id": "358276", "url": "https://en.wikipedia.org/wiki?curid=358276", "title": "Class (philosophy)", "text": "Class (philosophy)\n\nIn at least one source, a class is a set in which an individual member can be recognized in one or both of two ways: a) it is included in an extensional definition of the whole set (a list of set members) b) it matches an Intensional definition of one set member. By contrast, a \"type\" is an intensional definition; it is a description that is sufficiently generalized to fit every member of a set.\n\nPhilosophers sometimes distinguish classes from types and kinds. We can talk about the \"class\" of human beings, just as we can talk about the \"type\" (or \"natural kind\"), human being, or humanity. How, then, might classes differ from types? One might well think they are not actually different categories of being, but typically, while both are treated as abstract objects, classes are not usually treated as universals, whereas types usually are. Whether natural kinds ought to be considered universals is vexed; see natural kind.\n\nThere is, in any case, a difference in how we talk about types or kinds. We say that Socrates is a \"token\" of a type, or an \"instance\" of the natural kind, \"human\" \"being\". But notice that we say instead that Socrates is a \"member\" of the class of human beings. We would not say that Socrates is a \"member\" of the type or kind, human beings. Nor would we say he is a type (or kind) of a class. He is a token (instance) of the type (kind). So the linguistic difference is: types (or kinds) have tokens (or instances); classes, on the other hand, have members.\n\nThe concept of a class is similar to the concept of a set defined by its members. Here, the class is extensional. If, however, a set is defined intensionally, then it is a set of things that meet some requirement to be a member. Thus, such a set can be seen as creating a type. Note that it also creates a class from the extension of the intensional set. A type always has a corresponding class (though that class might have no members), but a class does not necessarily have a corresponding type.\n\n"}
{"id": "38889765", "url": "https://en.wikipedia.org/wiki?curid=38889765", "title": "Coherence (units of measurement)", "text": "Coherence (units of measurement)\n\nA coherent system of units is a system of units based on a system of quantities in such a way that the equations between the numerical values expressed in the units of the system have exactly the same form, including numerical factors, as the corresponding equations between the quantities. Equivalently, it is a system in which every quantity has a unique unit, or one that does not use conversion factors.\n\nA coherent derived unit is a derived unit that, for a given system of quantities and for a chosen set of base units, is a product of powers of base units with the proportionality factor being one.\n\nIf a system of units has both equations and base units, with only one base unit for each base quantity, then it is coherent if and only if every derived unit of the system is coherent.\n\nThe concept of coherence was developed in the mid-nineteenth century by, amongst others, Kelvin and James Clerk Maxwell and promoted by the British Association for the Advancement of Science. The concept was initially applied to the centimetre–gram–second (CGS) in 1873 and the foot–pound–second systems (FPS) of units in 1875. The International System of Units (1960) was designed around the principle of coherence.\n\nIn SI, which is a coherent system, the unit of power is the watt, which is defined as one joule per second. In the US customary system of measurement, which is non-coherent, the unit of power is the horsepower, which is defined as 550 foot-pounds per second (the pound in this context being the pound-force); similarly the gallon is not equal to a cubic yard (nor is it the cube of any length unit of the system).\n\nThe earliest units of measure devised by humanity bore no relationship to each other. As both humanity's understanding of philosophical concepts and the organisation of society developed, so units of measurement were standardised – first particular units of measure had the same value across a community then different units of the same quantity (for example feet and inches) were given a fixed relationship. Apart from Ancient China where the units of capacity and of mass are linked to red millet seed, there is little evidence of the linking of different quantities until the Age of Reason.\n\nThe history of the measurement of length dates back to the early civilisations of the Middle East (10000 BC – 8000 BC). Archeologists have been able to reconstruct the units of measure in use in Mesopotamia, India, the Jewish culture and many others. Archaeological and other evidence shows that in many civilisations, the ratios between different units for the same quantity of measure were adjusted so that they were integer numbers. In many early cultures such as Ancient Egypt, multiples of 2, 3 and 5 were not always used—the Egyptian royal cubit being 28 fingers of 7 hands. In 2150 BC, the Akkadian emperor Naram-Sin rationalised the Babylonian system of measure, adjusting the ratios of many units of measure to multiples of 2, 3 or 5, for example there were 6 \"she\" (barleycorns) in a \"shu-si\" (finger) and 30 shu-si in a \"kush\" (cubit).\nNon-commensurable quantities have different physical dimensions which means that adding or subtracting them is not meaningful. For instance, adding the mass of an object to its volume has no physical meaning. However, new quantities (and, as such, units) can be derived via multiplication and exponentiation of other units. As an example, the SI unit for force is the newton, which is defined as kg⋅m⋅s. Since a coherent derived unit is one which is defined by means of multiplication and exponentiation of other units but not multiplied by any scaling factor other than 1, the pascal is a coherent unit of pressure (defined as kg⋅m⋅s), but the bar (defined as ) is not. \n\nNote that coherence of a given unit depends on the definition of the base units. Should the meter's definition change such that it is shorter by a factor of , then the bar would be a coherent derived unit. However, a coherent unit remains coherent (and a non-coherent unit remains non-coherent) if the base units are redefined in terms of other units with the numerical factor always being unity.\n\nThe concept of coherence was only introduced into the metric system in the third quarter of the nineteenth century; in its original form the metric system was non-coherent – in particular the litre was 0.001 m and the are (from which we get the hectare) was 100 m. A precursor to the concept of coherence was however present in that the units of mass and length were related to each other through the physical properties of water, the gram having been designed as being the mass of one cubic centimetre of water at its freezing point.\n\nThe CGS system had two units of energy, the erg that was related to mechanics and the calorie that was related to thermal energy, so only one of them (the erg, equivalent to the g⋅cm/s) could bear a coherent relationship to the base units. By contrast, coherence was a design aim of the SI, resulting in only one unit of energy being defined – the joule.\n\nWork of James Clerk Maxwell and others\n\nEach variant of the metric system has a degree of coherence – the various derived units being directly related to the base units without the need of intermediate conversion factors. An additional criterion is that, for example, in a coherent system the units of force, energy and power be chosen so that the equations\nhold without the introduction of constant factors. Once a set of coherent units have been defined, other relationships in physics that use those units will automatically be true – Einstein's mass–energy equation, , does not require extraneous constants when expressed in coherent units.\n\nIsaac Asimov wrote, \"In the cgs system, a unit force is described as one that will produce an acceleration of 1 cm/sec on a mass of 1 gm. A unit force is therefore 1 cm/sec multiplied by 1 gm.\" These are independent statements. The first is a definition; the second is not. The first implies that the constant of proportionality in the force law has a magnitude of one; the second implies that it is dimensionless. Asimov uses them both together to prove that it is the pure number one.\n\nAsimov's conclusion is not the only possible one. In a system that uses the units foot (ft) for length, second (s) for time, pound (lb) for mass, and pound-force (lbf) for force, the law relating force (\"F\"), mass (\"m\"), and acceleration (\"a\") is . Since the proportionality constant here is dimensionless and the units in any equation must balance without any numerical factor other than one, it follows that 1 lbf = 1 lb⋅ft/s. This conclusion appears paradoxical from the point of view of competing systems, according to which and . Although the pound-force is a coherent derived unit in this system according to the official definition, the system itself is not considered to be coherent because of the presence of the proportionality constant in the force law.\n\nA variant of this system applies the unit s/ft to the proportionality constant. This has the effect of identifying the pound-force with the pound. The pound is then both a base unit of mass and a coherent derived unit of force. One may apply any unit one pleases to the proportionality constant. If one applies the unit s/lb to it, then the foot becomes a unit of force. In a four-unit system (English engineering units), the pound and the pound-force are distinct base units, and the proportionality constant has the unit lbf⋅s/(lb⋅ft).\n\nAll these systems are coherent. One that is not is a three-unit system (also called English engineering units) in which \"F\" = \"ma\" that uses the pound and the pound-force, one of which is a base unit and the other, a noncoherent derived unit. In place of an explicit proportionality constant, this system uses conversion factors derived from the relation 1 lbf = 32.174 lb⋅ft/s. In numerical calculations, it is indistinguishable from the four-unit system, since what is a proportionality constant in the latter is a conversion factor in the former. The relation among the numerical values of the quantities in the force law is {\"F\"} = 0.031081 {\"m\"} {\"a\"}, where the braces denote the numerical values of the enclosed quantities. Unlike in this system, in a coherent system, the relations among the numerical values of quantities are the same as the relations among the quantities themselves.\n\nThe following example concerns definitions of quantities and units. The (average) velocity (\"v\") of an object is defined as the quantitative physical property of the object that is directly proportional to the distance (\"d\") traveled by the object and inversely proportional to the time (\"t\") of travel, i.e., \"v\" = \"kd\"/\"t\", where \"k\" is a constant that depends on the units used. Suppose that the meter (m) and the second (s) are base units; then the kilometer (km) and the hour (h) are noncoherent derived units. The meter per second (mps) is defined as the velocity of an object that travels one meter in one second, and the kilometer per hour (kmph) is defined as the velocity of an object that travels one kilometer in one hour. Substituting from the definitions of the units into the defining equation of velocity we obtain, 1 mps = \"k\" m/s and 1 kmph = \"k\" km/h = 1/3.6 \"k\" m/s = 1/3.6 mps. Now choose \"k\" = 1; then the meter per second is a coherent derived unit, and the kilometer per hour is a noncoherent derived unit. Suppose that we choose to use the kilometer per hour as the unit of velocity in the system. Then the system becomes noncoherent, and the numerical value equation for velocity becomes {\"v\"} = 3.6 {\"d\"}/{\"t\"}. Coherence may be restored, without changing the units, by choosing \"k\" = 3.6; then the kilometer per hour is a coherent derived unit, with 1 kmph = 1 m/s, and the meter per second is a noncoherent derived unit, with 1 mps = 3.6 m/s.\n\nA definition of a physical quantity is a statement that determines the ratio of any two instances of the quantity. The specification of the value of any constant factor is not a part of the definition since it does not affect the ratio. The definition of velocity above satisfies this requirement since it implies that \"v\"/\"v\" = (\"d\"/\"d\")/(\"t\"/\"t\"); thus if the ratios of distances and times are determined, then so is the ratio of velocities. A definition of a unit of a physical quantity is a statement that determines the ratio of any instance of the quantity to the unit. This ratio is the numerical value of the quantity or the number of units contained in the quantity. The definition of the meter per second above satisfies this requirement since it, together with the definition of velocity, implies that \"v\"/mps = (\"d\"/m)/(\"t\"/s); thus if the ratios of distance and time to their units are determined, then so is the ratio of velocity to its unit. The definition, by itself, is inadequate since it only determines the ratio in one specific case; it may be thought of as exhibiting a specimen of the unit.\n\nA new coherent unit cannot be defined merely by expressing it algebraically in terms of already defined units. Thus the statement, \"the meter per second equals one meter divided by one second\", is not, by itself, a definition. It does not imply that a unit of velocity is being defined, and if that fact is added, it does not determine the magnitude of the unit, since that depends on the system of units. In order for it to become a proper definition both the quantity and the defining equation, including the value of any constant factor, must be specified. After a unit has been defined in this manner, however, it has a magnitude that is independent of any system of units.\n\nThis list catalogues coherent relationships in various systems of units.\nThe following is a list of coherent SI units:\n\nThe following is a list of coherent centimetre–gram–second (CGS) system of units:\nThe following is a list of coherent foot–pound–second (FPS) system of units:\n\n"}
{"id": "7533645", "url": "https://en.wikipedia.org/wiki?curid=7533645", "title": "Cone (category theory)", "text": "Cone (category theory)\n\nIn category theory, a branch of mathematics, the cone of a functor is an abstract notion used to define the limit of that functor. Cones make other appearances in category theory as well.\n\nLet \"F\" : \"J\" → \"C\" be a diagram in \"C\". Formally, a diagram is nothing more than a functor from \"J\" to \"C\". The change in terminology reflects the fact that we think of \"F\" as indexing a family of objects and morphisms in \"C\". The category \"J\" is thought of as an \"index category\". One should consider this in analogy with the concept of an indexed family of objects in set theory. The primary difference is that here we have morphisms as well. Thus, for example, when \"J\" is a discrete category, it corresponds most closely to the idea of an indexed family in set theory. Another common and more interesting example takes \"J\" to be a span. \"J\" can also be taken to be the empty category, leading to the simplest cones.\n\nLet \"N\" be an object of \"C\". A cone from \"N\" to \"F\" is a family of morphisms\nfor each object \"X\" of \"J\" such that for every morphism \"f\" : \"X\" → \"Y\" in \"J\" the following diagram commutes:\n\nThe (usually infinite) collection of all these triangles can\nbe (partially) depicted in the shape of a cone with the apex \"N\". The cone ψ is sometimes said to have vertex \"N\" and base \"F\".\n\nOne can also define the dual notion of a cone from \"F\" to \"N\" (also called a co-cone) by reversing all the arrows above. Explicitly, a co-cone from \"F\" to \"N\" is a family of morphisms\nfor each object \"X\" of \"J\" such that for every morphism \"f\" : \"X\" → \"Y\" in \"J\" the following diagram commutes:\n\nAt first glance cones seem to be slightly abnormal constructions in category theory. They are maps from an \"object\" to a \"functor\" (or vice versa). In keeping with the spirit of category theory we would like to define them as morphisms or objects in some suitable category. In fact, we can do both.\n\nLet \"J\" be a small category and let \"C\" be the category of diagrams of type \"J\" in \"C\" (this is nothing more than a functor category). Define the diagonal functor Δ : \"C\" → \"C\" as follows: Δ(\"N\") : \"J\" → \"C\" is the constant functor to \"N\" for all \"N\" in \"C\".\n\nIf \"F\" is a diagram of type \"J\" in \"C\", the following statements are equivalent:\n\nThe dual statements are also equivalent:\n\nThese statements can all be verified by a straightforward application of the definitions. Thinking of cones as natural transformations we see that they are just morphisms in \"C\" with source (or target) a constant functor.\n\nBy the above, we can define the category of cones to \"F\" as the comma category (Δ ↓ \"F\"). Morphisms of cones are then just morphisms in this category. This equivalence is rooted in the observation that a natural map between constant functors Δ(\"N\"), Δ(\"M\") corresponds to a morphism between \"N\" and \"M\". In this sense, the diagonal functor acts trivially on arrows. In similar vein, writing down the definition of a natural map from a constant functor Δ(\"N\") to \"F\" yields the same diagram as the above. As one might expect, a morphism from a cone (\"N\", ψ) to a cone (\"L\", φ) is just a morphism \"N\" → \"L\" such that all the \"obvious\" diagrams commute (see the first diagram in the next section).\n\nLikewise, the category of co-cones from \"F\" is the comma category (\"F\" ↓ Δ).\n\nLimits and colimits are defined as universal cones. That is, cones through which all other cones factor. A cone φ from \"L\" to \"F\" is a universal cone if for any other cone ψ from \"N\" to \"F\" there is a unique morphism from ψ to φ.\n\nEquivalently, a universal cone to \"F\" is a universal morphism from Δ to \"F\" (thought of as an object in \"C\"), or a terminal object in (Δ ↓ \"F\").\n\nDually, a cone φ from \"F\" to \"L\" is a universal cone if for any other cone ψ from \"F\" to \"N\" there is a unique morphism from φ to ψ.\n\nEquivalently, a universal cone from \"F\" is a universal morphism from \"F\" to Δ, or an initial object in (\"F\" ↓ Δ).\n\nThe limit of \"F\" is a universal cone to \"F\", and the colimit is a universal cone from \"F\". As with all universal constructions, universal cones are not guaranteed to exist for all diagrams \"F\", but if they do exist they are unique up to a unique isomorphism (in the comma category (Δ ↓ \"F\")).\n\n"}
{"id": "5326", "url": "https://en.wikipedia.org/wiki?curid=5326", "title": "Creationism", "text": "Creationism\n\nCreationism is the religious belief that the universe and life originated \"from specific acts of divine creation\", as opposed to the scientific conclusion that they came about through natural processes. Creationism covers a spectrum of views including \"evolutionary creationism\", a theological variant of theistic evolution which asserts that both evolutionary science and a belief in creation are true, but the term is commonly used for literal creationists who reject various aspects of science, and instead promote pseudoscientific beliefs.\n\nLiteral creationists base their beliefs on a fundamentalist reading of religious texts, including the creation myths found in Genesis and the Quran. For young Earth creationists, these beliefs are based on a literalist interpretation of the Genesis creation narrative and rejection of the scientific theory of evolution. Literalist creationists believe that evolution cannot adequately account for the history, diversity, and complexity of life on Earth. Pseudoscientific branches of creationism include creation science, flood geology, and intelligent design, as well as subsets of pseudoarchaeology, pseudohistory, and pseudolinguistics.\n\nThe first use of the term \"creationist\" to describe a proponent of creationism is found in an 1856 letter of Charles Darwin describing those who objected on religious grounds to the then emerging science of evolution.\n\nThe basis for many creationists' beliefs is a literal or quasi-literal interpretation of the Old Testament, especially from stories from the book of Genesis:\n\n\nA further important element is the interpretation of the Biblical chronology, the elaborate system of life-spans, \"generations,\" and other means by which the Bible measures the passage of events from the creation (Genesis 1:1) to the Book of Daniel, the last biblical book in which it appears. Recent decades have seen attempts to de-link creationism from the Bible and recast it as science; these include creation science and intelligent design. There are also non-Christian forms of creationism, notably Islamic creationism and Hindu creationism.\n\nSeveral attempts have been made to categorize the different types of creationism, and create a \"taxonomy\" of creationists. Creationism (broadly construed) covers a spectrum of beliefs which have been categorized into the general types listed below.\n\nYoung Earth creationists such as Ken Ham and Doug Phillips believe that God created the Earth within the last ten thousand years, literally as described in the Genesis creation narrative, within the approximate time-frame of biblical genealogies (detailed for example in the Ussher chronology). Most young Earth creationists believe that the universe has a similar age as the Earth. A few assign a much older age to the universe than to Earth. Creationist cosmologies give the universe an age consistent with the Ussher chronology and other young Earth time frames. Other young Earth creationists believe that the Earth and the universe were created with the appearance of age, so that the world appears to be much older than it is, and that this appearance is what gives the geological findings and other methods of dating the Earth and the universe their much longer timelines.\n\nThe Christian organizations Institute for Creation Research (ICR) and the Creation Research Society (CRS) both promote young Earth creationism in the US. Another organization with similar views, Answers in Genesis (AiG)—based in both the U.S. and the United Kingdom—has opened the Creation Museum in Petersburg, Kentucky, to promote young Earth creationism. Creation Ministries International promotes young Earth views in Australia, Canada, South Africa, New Zealand, the US, and the UK. Among Roman Catholics, the Kolbe Center for the Study of Creation promotes similar ideas. In 2007, Ken Ham founded the Creation Museum and Ark Encounter in northern Kentucky.\n\nOld Earth creationism holds that the physical universe was created by God, but that the creation event described in the Book of Genesis is to be taken figuratively. This group generally believes that the age of the universe and the age of the Earth are as described by astronomers and geologists, but that details of modern evolutionary theory are questionable.\n\nOld Earth creationism itself comes in at least three types:\n\nGap creationism, also called \"restoration creationism,\" holds that life was recently created on a pre-existing old Earth. This version of creationism relies on a particular interpretation of . It is considered that the words \"formless\" and \"void\" in fact denote waste and ruin, taking into account the original Hebrew and other places these words are used in the Old Testament. Genesis 1:1–2 is consequently translated:\n\nThus, the six days of creation (verse 3 onwards) start sometime after the Earth was \"without form and void.\" This allows an indefinite \"gap\" of time to be inserted after the original creation of the universe, but prior to the creation according to Genesis, (when present biological species and humanity were created). Gap theorists can therefore agree with the scientific consensus regarding the age of the Earth and universe, while maintaining a literal interpretation of the biblical text.\n\nSome gap creationists expand the basic version of creationism by proposing a \"primordial creation\" of biological life within the \"gap\" of time. This is thought to be \"the world that then was\" mentioned in 2 Peter 3:3–7. Discoveries of fossils and archaeological ruins older than 10,000 years are generally ascribed to this \"world that then was,\" which may also be associated with Lucifer's rebellion. These views became popular with publications of Hebrew Lexicons such as \"Strong's Concordance\", and Bible commentaries such as the \"Scofield Reference Bible\" and \"The Companion Bible\".\n\nDay-age creationism states that the \"six days\" of the Book of Genesis are not ordinary 24-hour days, but rather much longer periods (for instance, each \"day\" could be the equivalent of millions, or billions of years of human time). The physicist Gerald Schroeder is one such proponent of this view. This version of creationism often states that the Hebrew word \"yôm,\" in the context of Genesis 1, can be properly interpreted as \"age.\"\n\nStrictly speaking, day-age creationism is not so much a version of creationism as a hermeneutic option which may be combined with other versions of creationism such as progressive creationism.\n\nProgressive creationism holds that species have changed or evolved in a process continuously guided by God, with various ideas as to how the process operated—though it is generally taken that God directly intervened in the natural order at key moments in Earth history. This view accepts most of modern physical science including the age of the Earth, but rejects much of modern evolutionary biology or looks to it for evidence that evolution by natural selection alone is incorrect. Organizations such as Reasons To Believe, founded by Hugh Ross, promote this version of creationism.\n\nProgressive creationism can be held in conjunction with hermeneutic approaches to the Genesis creation narrative such as the day-age creationism or framework/metaphoric/poetic views.\n\nCreation science, or initially scientific creationism, is a pseudoscience that emerged in the 1960s with proponents aiming to have young Earth creationist beliefs taught in school science classes as a counter to teaching of evolution. Common features of creation science argument include: creationist cosmologies which accommodate a universe on the order of thousands of years old, criticism of radiometric dating through a technical argument about radiohalos, explanations for the fossil record as a record of the Genesis flood narrative (see flood geology), and explanations for the present diversity as a result of pre-designed genetic variability and partially due to the rapid degradation of the perfect genomes God placed in \"created kinds\" or \"Baramin\" (see creationist biology) due to mutations.\n\nNeo-creationism is a pseudoscientific movement which aims to restate creationism in terms more likely to be well received by the public, by policy makers, by educators and by the scientific community. It aims to re-frame the debate over the origins of life in non-religious terms and without appeals to scripture. This comes in response to the 1987 ruling by the United States Supreme Court in \"Edwards v. Aguillard\" that creationism is an inherently religious concept and that advocating it as correct or accurate in public-school curricula violates the Establishment Clause of the First Amendment.\n\nOne of the principal claims of neo-creationism propounds that ostensibly objective orthodox science, with a foundation in naturalism, is actually a dogmatically atheistic religion. Its proponents argue that the scientific method excludes certain explanations of phenomena, particularly where they point towards supernatural elements, thus effectively excluding religious insight from contributing to understanding the universe. This leads to an open and often hostile opposition to what neo-creationists term \"Darwinism\", which they generally mean to refer to evolution, but which they may extend to include such concepts as abiogenesis, stellar evolution and the Big Bang theory.\n\nUnlike their philosophical forebears, neo-creationists largely do not believe in many of the traditional cornerstones of creationism such as a young Earth, or in a dogmatically literal interpretation of the Bible.\n\nIntelligent design (ID) is the pseudoscientific view that \"certain features of the universe and of living things are best explained by an intelligent cause, not an undirected process such as natural selection.\" All of its leading proponents are associated with the Discovery Institute, a think tank whose Wedge strategy aims to replace the scientific method with \"a science consonant with Christian and theistic convictions\" which accepts supernatural explanations. It is widely accepted in the scientific and academic communities that intelligent design is a form of creationism, and is sometimes referred to as \"intelligent design creationism.\"\n\nID originated as a re-branding of creation science in an attempt to avoid a series of court decisions ruling out the teaching of creationism in American public schools, and the Discovery Institute has run a series of campaigns to change school curricula. In Australia, where curricula are under the control of state governments rather than local school boards, there was a public outcry when the notion of ID being taught in science classes was raised by the Federal Education Minister Brendan Nelson; the minister quickly conceded that the correct forum for ID, if it were to be taught, is in religious or philosophy classes.\n\nIn the US, teaching of intelligent design in public schools has been decisively ruled by a federal district court to be in violation of the Establishment Clause of the First Amendment to the United States Constitution. In Kitzmiller v. Dover, the court found that intelligent design is not science and \"cannot uncouple itself from its creationist, and thus religious, antecedents,\" and hence cannot be taught as an alternative to evolution in public school science classrooms under the jurisdiction of that court. This sets a persuasive precedent, based on previous US Supreme Court decisions in \"Edwards v. Aguillard\" and \"Epperson v. Arkansas\" (1968), and by the application of the Lemon test, that creates a legal hurdle to teaching intelligent design in public school districts in other federal court jurisdictions.\n\nIn astronomy, the geocentric model (also known as geocentrism, or the Ptolemaic system), is a description of the Cosmos where Earth is at the orbital center of all celestial bodies. This model served as the predominant cosmological system in many ancient civilizations such as ancient Greece. As such, they assumed that the Sun, Moon, stars, and naked eye planets circled Earth, including the noteworthy systems of Aristotle (see Aristotelian physics) and Ptolemy.\n\nArticles arguing that geocentrism was the biblical perspective appeared in some early creation science newsletters associated with the Creation Research Society pointing to some passages in the Bible, which, when taken literally, indicate that the daily apparent motions of the Sun and the Moon are due to their actual motions around the Earth rather than due to the rotation of the Earth about its axis for example, Joshua 10:12 where the Sun and Moon are said to stop in the sky, and Psalms 93:1 where the world is described as immobile. Contemporary advocates for such religious beliefs include Robert Sungenis, co-author of the self-published \"Galileo Was Wrong: The Church Was Right\" (2006). These people subscribe to the view that a plain reading of the Bible contains an accurate account of the manner in which the universe was created and requires a geocentric worldview. Most contemporary creationist organizations reject such perspectives.\n\nThe Omphalos hypothesis argues that in order for the world to be functional, God must have created a mature Earth with mountains and canyons, rock strata, trees with growth rings, and so on; therefore \"no\" evidence that we can see of the presumed age of the Earth and age of the universe can be taken as reliable. The idea has seen some revival in the 20th century by some modern creationists, who have extended the argument to address the \"starlight problem\". The idea has been criticised as Last Thursdayism, and on the grounds that it requires a deliberately deceptive creator.\n\nTheistic evolution, or evolutionary creation, is a belief that \"the personal God of the Bible created the universe and life through evolutionary processes.\" According to the American Scientific Affiliation:\n\nThrough the 19th century the term \"creationism\" most commonly referred to direct creation of individual souls, in contrast to traducianism. Following the publication of \"Vestiges of the Natural History of Creation\", there was interest in ideas of Creation by divine law. In particular, the liberal theologian Baden Powell argued that this illustrated the Creator's power better than the idea of miraculous creation, which he thought ridiculous. When \"On the Origin of Species\" was published, the cleric Charles Kingsley wrote of evolution as \"just as noble a conception of Deity.\" Darwin's view at the time was of God creating life through the laws of nature, and the book makes several references to \"creation,\" though he later regretted using the term rather than calling it an unknown process. In America, Asa Gray argued that evolution is the secondary effect, or \"modus operandi\", of the first cause, design, and published a pamphlet defending the book in theistic terms, \"Natural Selection not inconsistent with Natural Theology\". Theistic evolution, also called, evolutionary creation, became a popular compromise, and St. George Jackson Mivart was among those accepting evolution but attacking Darwin's naturalistic mechanism. Eventually it was realised that supernatural intervention could not be a scientific explanation, and naturalistic mechanisms such as neo-Lamarckism were favoured as being more compatible with purpose than natural selection.\n\nSome theists took the general view that, instead of faith being in opposition to biological evolution, some or all classical religious teachings about Christian God and creation are compatible with some or all of modern scientific theory, including specifically evolution; it is also known as \"evolutionary creation.\" In Evolution versus Creationism, Eugenie Scott and Niles Eldredge state that it is in fact a type of evolution.\n\nIt generally views evolution as a tool used by God, who is both the first cause and immanent sustainer/upholder of the universe; it is therefore well accepted by people of strong theistic (as opposed to deistic) convictions. Theistic evolution can synthesize with the day-age creationist interpretation of the Genesis creation narrative; however most adherents consider that the first chapters of the Book of Genesis should not be interpreted as a \"literal\" description, but rather as a literary framework or allegory.\n\nFrom a theistic viewpoint, the underlying laws of nature were designed by God for a purpose, and are so self-sufficient that the complexity of the entire physical universe evolved from fundamental particles in processes such as stellar evolution, life forms developed in biological evolution, and in the same way the origin of life by natural causes has resulted from these laws.\n\nIn one form or another, theistic evolution is the view of creation taught at the majority of mainline Protestant seminaries. For Roman Catholics, human evolution is not a matter of religious teaching, and must stand or fall on its own scientific merits. Evolution and the Roman Catholic Church are not in conflict. The Catechism of the Catholic Church comments positively on the theory of evolution, which is neither precluded nor required by the sources of faith, stating that scientific studies \"have splendidly enriched our knowledge of the age and dimensions of the cosmos, the development of life-forms and the appearance of man.\" Roman Catholic schools teach evolution without controversy on the basis that scientific knowledge does not extend beyond the physical, and scientific truth and religious truth cannot be in conflict. Theistic evolution can be described as \"creationism\" in holding that divine intervention brought about the origin of life or that divine laws govern formation of species, though many creationists (in the strict sense) would deny that the position is creationism at all. In the creation–evolution controversy, its proponents generally take the \"evolutionist\" side. This sentiment was expressed by Fr. George Coyne, (the Vatican's chief astronomer between 1978 and 2006):...in America, creationism has come to mean some fundamentalistic, literal, scientific interpretation of Genesis. Judaic-Christian faith is radically creationist, but in a totally different sense. It is rooted in a belief that everything depends upon God, or better, all is a gift from God.\n\nWhile supporting the methodological naturalism inherent in modern science, the proponents of theistic evolution reject the implication taken by some atheists that this gives credence to ontological materialism. In fact, many modern philosophers of science, including atheists, refer to the long-standing convention in the scientific method that observable events in nature should be explained by natural causes, with the distinction that it does not assume the actual existence or non-existence of the supernatural. \n\n, most Christians around the world accepted evolution as the most likely explanation for the origins of species, and did not take a literal view of the Genesis creation myth. The United States is an exception where belief in religious fundamentalism is much more likely to affect attitudes towards evolution than it is for believers elsewhere. Political partisanship affecting religious belief may be a factor because political partisanship in the US is highly correlated with fundamentalist thinking, unlike in Europe.\n\nMost contemporary Christian leaders and scholars from mainstream churches, such as Anglicans and Lutherans, consider that there is no conflict between the spiritual meaning of creation and the science of evolution. According to the former Archbishop of Canterbury, Rowan Williams, \"...for most of the history of Christianity, and I think this is fair enough, most of the history of the Christianity there's been an awareness that a belief that everything depends on the creative act of God, is quite compatible with a degree of uncertainty or latitude about how precisely that unfolds in creative time.\"\n\nLeaders of the Anglican and Roman Catholic churches have made statements in favor of evolutionary theory, as have scholars such as the physicist John Polkinghorne, who argues that evolution is one of the principles through which God created living beings. Earlier supporters of evolutionary theory include Frederick Temple, Asa Gray and Charles Kingsley who were enthusiastic supporters of Darwin's theories upon their publication, and the French Jesuit priest and geologist Pierre Teilhard de Chardin saw evolution as confirmation of his Christian beliefs, despite condemnation from Church authorities for his more speculative theories. Another example is that of Liberal theology, not providing any creation models, but instead focusing on the symbolism in beliefs of the time of authoring Genesis and the cultural environment.\n\nMany Christians and Jews had been considering the idea of the creation history as an allegory (instead of historical) long before the development of Darwin's theory of evolution. For example, Philo, whose works were taken up by early Church writers, wrote that it would be a mistake to think that creation happened in six days, or in any set amount of time. Augustine of the late fourth century who was also a former neoplatonist argued that everything in the universe was created by God at the same moment in time (and not in six days as a literal reading of the Book of Genesis would seem to require); It appears that both Philo and Augustine felt uncomfortable with the idea of a seven-day creation because it detracted from the notion of God's omnipotence. In 1950, Pope Pius XII stated limited support for the idea in his encyclical \"Humani generis\". In 1996, Pope John Paul II stated that \"new knowledge has led to the recognition of the theory of evolution as more than a hypothesis,\" but, referring to previous papal writings, he concluded that \"if the human body takes its origin from pre-existent living matter, the spiritual soul is immediately created by God.\"\n\nIn the US, Evangelical Christians have continued to believe in a literal Genesis. Members of evangelical Protestant (70%), Mormon (76%) and Jehovah's Witnesses (90%) denominations are the most likely to reject the evolutionary interpretation of the origins of life.\n\nJehovah's Witnesses adhere to a combination of gap creationism and day-age creationism, asserting that scientific evidence about the age of the universe is compatible with the Bible, but that the 'days' after Genesis 1:1 were each thousands of years in length.\n\nThe historic Christian literal interpretation of creation requires the harmonization of the two creation stories, Genesis 1:1–2:3 and Genesis 2:4–25, for there to be a consistent interpretation. They sometimes seek to ensure that their belief is taught in science classes, mainly in American schools. Opponents reject the claim that the literalistic biblical view meets the criteria required to be considered scientific. Many religious groups teach that God created the Cosmos. From the days of the early Christian Church Fathers there were allegorical interpretations of the Book of Genesis as well as literal aspects.\n\nChristian Science, a system of thought and practice derived from the writings of Mary Baker Eddy, interprets the Book of Genesis figuratively rather than literally. It holds that the material world is an illusion, and consequently not created by God: the only real creation is the spiritual realm, of which the material world is a distorted version. Christian Scientists regard the story of the creation in the Book of Genesis as having symbolic rather than literal meaning. According to Christian Science, both creationism and evolution are false from an absolute or \"spiritual\" point of view, as they both proceed from a (false) belief in the reality of a material universe. However, Christian Scientists do not oppose the teaching of evolution in schools, nor do they demand that alternative accounts be taught: they believe that both material science and literalist theology are concerned with the illusory, mortal and material, rather than the real, immortal and spiritual. With regard to material theories of creation, Eddy showed a preference for Darwin's theory of evolution over others.\n\nAccording to Hindu creationism, all species on Earth including humans have \"devolved\" or come down from a high state of pure consciousness. Hindu creationists claim that species of plants and animals are material forms adopted by pure consciousness which live an endless cycle of births and rebirths. Ronald Numbers says that: \"Hindu Creationists have insisted on the antiquity of humans, who they believe appeared fully formed as long, perhaps, as trillions of years ago.\" Hindu creationism is a form of old Earth creationism, according to Hindu creationists the universe may even be older than billions of years. These views are based on the Vedas, the creation myths of which depict an extreme antiquity of the universe and history of the Earth.\n\nIslamic creationism is the belief that the universe (including humanity) was directly created by God as explained in the Qur'an. It usually views the Book of Genesis as a corrupted version of God's message. The creation myths in the Qur'an are vaguer and allow for a wider range of interpretations similar to those in other Abrahamic religions.\n\nIslam also has its own school of theistic evolutionism, which holds that mainstream scientific analysis of the origin of the universe is supported by the Qur'an. Some Muslims believe in evolutionary creation, especially among liberal movements within Islam.\n\nWriting for \"The Boston Globe\", Drake Bennett noted: \"Without a Book of Genesis to account for ... Muslim creationists have little interest in proving that the age of the Earth is measured in the thousands rather than the billions of years, nor do they show much interest in the problem of the dinosaurs. And the idea that animals might evolve into other animals also tends to be less controversial, in part because there are passages of the Koran that seem to support it. But the issue of whether human beings are the product of evolution is just as fraught among Muslims.\" However, some Muslims, such as Adnan Oktar (also known as Harun Yahya), do not agree that one species can develop from another.\n\nSince the 1980s, Turkey has been a site of strong advocacy for creationism, supported by American adherents.\n\nThere are several verses in the Qur'an which some modern writers have interpreted as being compatible with the expansion of the universe, Big Bang and Big Crunch theories:\n\nThe Ahmadiyya movement actively promotes evolutionary theory. Ahmadis interpret scripture from the Qur'an to support the concept of macroevolution and give precedence to scientific theories. Furthermore, unlike orthodox Muslims, Ahmadis believe that humans have gradually evolved from different species. Ahmadis regard Adam as being the first Prophet of Godas opposed to him being the first man on Earth. Rather than wholly adopting the theory of natural selection, Ahmadis promote the idea of a \"guided evolution,\" viewing each stage of the evolutionary process as having been selectively woven by God. Mirza Tahir Ahmad, Fourth Caliph of the Ahmadiyya Muslim Community has stated in his magnum opus \"Revelation, Rationality, Knowledge & Truth\" (1998) that evolution did occur but only through God being the One who brings it about. It does not occur itself, according to the Ahmadiyya Muslim Community.\n\nFor Orthodox Jews who seek to reconcile discrepancies between science and the creation myths in the Bible, the notion that science and the Bible should even be reconciled through traditional scientific means is questioned. To these groups, science is as true as the Torah and if there seems to be a problem, epistemological limits are to blame for apparently irreconcilable points. They point to discrepancies between what is expected and what actually is to demonstrate that things are not always as they appear. They note that even the root word for \"world\" in the Hebrew language—עולם (Olam)—means hidden—נעלם (Neh-Eh-Lahm). Just as they know from the Torah that God created man and trees and the light on its way from the stars in their observed state, so too can they know that the world was created in its over the six days of Creation that reflects progression to its currently-observed state, with the understanding that physical ways to verify this may eventually be identified. This knowledge has been advanced by Rabbi Dovid Gottlieb, former philosophy professor at Johns Hopkins University. Also, relatively old Kabbalistic sources from well before the scientifically apparent age of the universe was first determined are in close concord with modern scientific estimates of the age of the universe, according to Rabbi Aryeh Kaplan, and based on Sefer Temunah, an early kabbalistic work attributed to the first-century Tanna Nehunya ben HaKanah. Many kabbalists accepted the teachings of the Sefer HaTemunah, including the medieval Jewish scholar Nahmanides, his close student Isaac ben Samuel of Acre, and David ben Solomon ibn Abi Zimra. Other parallels are derived, among other sources, from Nahmanides, who expounds that there was a Neanderthal-like species with which Adam mated (he did this long before Neanderthals had even been discovered scientifically). Reform Judaism does not take the Torah as a literal text, but rather as a symbolic or open-ended work.\n\nSome contemporary writers such as Rabbi Gedalyah Nadel have sought to reconcile the discrepancy between the account in the Torah, and scientific findings by arguing that each day referred to in the Bible was not 24 hours, but billions of years long. Others claim that the Earth was created a few thousand years ago, but was deliberately made to look as if it was five billion years old, e.g. by being created with ready made fossils. The best known exponent of this approach being Rabbi Menachem Mendel Schneerson Others state that although the world was physically created in six 24 hour days, the Torah accounts can be interpreted to mean that there was a period of billions of years before the six days of creation.\n\nIn the creation myth taught by Bahá'u'lláh, the Bahá'í Faith founder, the universe has \"neither beginning nor ending,\" and that the component elements of the material world have always existed and will always exist. With regard to evolution and the origin of human beings, `Abdu'l-Bahá gave extensive comments on the subject when he addressed western audiences in the beginning of the 20th century. Transcripts of these comments can be found in \"Some Answered Questions\", \"Paris Talks\" and \"The Promulgation of Universal Peace\". `Abdu'l-Bahá described the human species as having evolved from a primitive form to modern man, but that the capacity to form human intelligence was always in existence.\n\nMost vocal literalist creationists are from the US, and strict creationist views are much less common in other developed countries. According to a study published in \"Science\", a survey of the US, Turkey, Japan and Europe showed that public acceptance of evolution is most prevalent in Iceland, Denmark and Sweden at 80% of the population. There seems to be no significant correlation between believing in evolution and understanding evolutionary science.\n\nA 2009 Nielsen poll, showed that almost a quarter of Australians believe \"the biblical account of human origins.\" Forty-two percent believe in a \"wholly scientific\" explanation for the origins of life, while 32 percent believe in an evolutionary process \"guided by God.\"\n\nA 2012 survey, by Angus Reid Public Opinion revealed that 61 percent of Canadians believe in evolution. The poll asked \"Where did human beings come fromdid we start as singular cells millions of year ago and evolve into our present form, or did God create us in his image 10,000 years ago?\"\n\nIn Europe, literalist creationism is more widely rejected, though regular opinion polls are not available. Most people accept that evolution is the most widely accepted scientific theory as taught in most schools. In countries with a Roman Catholic majority, papal acceptance of evolutionary creationism as worthy of study has essentially ended debate on the matter for many people.\n\nIn the UK, a 2006 poll on the \"origin and development of life\", asked participants to choose between three different perspectives on the origin of life: 22% chose creationism, 17% opted for intelligent design, 48% selected evolutionary theory, and the rest did not know. A subsequent 2010 YouGov poll on the correct explanation for the origin of humans found that 9% opted for creationism, 12% intelligent design, 65% evolutionary theory and 13% didn't know. The former Archbishop of Canterbury Rowan Williams, head of the worldwide Anglican Communion, views the idea of teaching creationism in schools as a mistake.\n\nIn Italy, Education Minister Letizia Moratti wanted to retire evolution from the secondary school level; after one week of massive protests, she reversed her opinion.\n\nThere continues to be scattered and possibly mounting efforts on the part of religious groups throughout Europe to introduce creationism into public education. In response, the Parliamentary Assembly of the Council of Europe has released a draft report titled \"The dangers of creationism in education\" on June 8, 2007, reinforced by a further proposal of banning it in schools dated October 4, 2007.\n\nSerbia suspended the teaching of evolution for one week in September 2004, under education minister Ljiljana Čolić, only allowing schools to reintroduce evolution into the curriculum if they also taught creationism. \"After a deluge of protest from scientists, teachers and opposition parties\" says the BBC report, Čolić's deputy made the statement, \"I have come here to confirm Charles Darwin is still alive\" and announced that the decision was reversed. Čolić resigned after the government said that she had caused \"problems that had started to reflect on the work of the entire government.\"\n\nPoland saw a major controversy over creationism in 2006, when the Deputy Education Minister, Mirosław Orzechowski, denounced evolution as \"one of many lies\" taught in Polish schools. His superior, Minister of Education Roman Giertych, has stated that the theory of evolution would continue to be taught in Polish schools, \"as long as most scientists in our country say that it is the right theory.\" Giertych's father, Member of the European Parliament Maciej Giertych, has opposed the teaching of evolution and has claimed that dinosaurs and humans co-existed.\n\nA 2017 poll by Pew Research found that 62% of Americans believe humans have evolved over time and 34% of Americans believe humans and other living things have existed in their present form since the beginning of time. Another 2017 Gallup creationism survey found that 38% of adults in the United States inclined to the view that \"God created humans in their present form at one time within the last 10,000 years\" when asked for their views on the origin and development of human beings, which Gallup noted was the lowest level in 35 years.\n\nAccording to a 2014 Gallup poll, about 42% of Americans believe that \"God created human beings pretty much in their present form at one time within the last 10,000 years or so.\" Another 31% believe that \"human beings have developed over millions of years from less advanced forms of life, but God guided this process,\"and 19% believe that \"human beings have developed over millions of years from less advanced forms of life, but God had no part in this process.\"\n\nBelief in creationism is inversely correlated to education; of those with postgraduate degrees, 74% accept evolution. In 1987, \"Newsweek\" reported: \"By one count there are some 700 scientists with respectable academic credentials (out of a total of 480,000 U.S. earth and life scientists) who give credence to creation-science, the general theory that complex life forms did not evolve but appeared 'abruptly.'\"\n\nA 2000 poll for People for the American Way found 70% of the US public felt that evolution was compatible with a belief in God.\n\nAccording to a study published in \"Science\", between 1985 and 2005 the number of adult North Americans who accept evolution declined from 45% to 40%, the number of adults who reject evolution declined from 48% to 39% and the number of people who were unsure increased from 7% to 21%. Besides the US the study also compared data from 32 European countries, Turkey, and Japan. The only country where acceptance of evolution was lower than in the US was Turkey (25%).\n\nAccording to a 2011 Fox News poll, 45% of Americans believe in Creationism, down from 50% in a similar poll in 1999. 21% believe in 'the theory of evolution as outlined by Darwin and other scientists' (up from 15% in 1999), and 27% answered that both are true (up from 26% in 1999).\n\nIn September 2012, educator and television personality Bill Nye spoke with the Associated Press and aired his fears about acceptance of creationism, believing that teaching children that creationism is the only true answer without letting them understand the way science works will prevent any future innovation in the world of science. In February 2014, Nye defended evolution in the classroom in a debate with creationist Ken Ham on the topic of whether creation is a viable model of origins in today's modern, scientific era.\n\nIn the US, creationism has become centered in the political controversy over creation and evolution in public education, and whether teaching creationism in science classes conflicts with the separation of church and state. Currently, the controversy comes in the form of whether advocates of the intelligent design movement who wish to \"Teach the Controversy\" in science classes have conflated science with religion.\n\nPeople for the American Way polled 1500 North Americans about the teaching of evolution and creationism in November and December 1999. They found that most North Americans were not familiar with Creationism, and most North Americans had heard of evolution, but many did not fully understand the basics of the theory. The main findings were:\nIn such political contexts, creationists argue that their particular religiously based origin belief is superior to those of other belief systems, in particular those made through secular or scientific rationale. Political creationists are opposed by many individuals and organizations who have made detailed critiques and given testimony in various court cases that the alternatives to scientific reasoning offered by creationists are opposed by the consensus of the scientific community.\n\nMost Christians disagree with the teaching of creationism as an alternative to evolution in schools. Several religious organizations, among them the Catholic Church, hold that their faith does not conflict with the scientific consensus regarding evolution. The Clergy Letter Project, which has collected more than 13,000 signatures, is an \"endeavor designed to demonstrate that religion and science can be compatible.\"\n\nIn his 2002 article \"Intelligent Design as a Theological Problem,\" George Murphy argues against the view that life on Earth, in all its forms, is direct evidence of God's act of creation (Murphy quotes Phillip E. Johnson's claim that he is speaking \"of a God who acted openly and left his fingerprints on all the evidence.\"). Murphy argues that this view of God is incompatible with the Christian understanding of God as \"the one revealed in the cross and resurrection of Christ.\" The basis of this theology is Isaiah 45:15, \"Verily thou art a God that hidest thyself, O God of Israel, the Saviour.\"\n\nMurphy observes that the execution of a Jewish carpenter by Roman authorities is in and of itself an ordinary event and did not require divine action. On the contrary, for the crucifixion to occur, God had to limit or \"empty\" himself. It was for this reason that Paul the Apostle wrote, in Philippians 2:5-8:\n\nLet this mind be in you, which was also in Christ Jesus: Who, being in the form of God, thought it not robbery to be equal with God: But made himself of no reputation, and took upon him the form of a servant, and was made in the likeness of men: And being found in fashion as a man, he humbled himself, and became obedient unto death, even the death of the cross.\n\nMurphy concludes that,Just as the Son of God limited himself by taking human form and dying on a cross, God limits divine action in the world to be in accord with rational laws which God has chosen. This enables us to understand the world on its own terms, but it also means that natural processes hide God from scientific observation.For Murphy, a theology of the cross requires that Christians accept a \"methodological\" naturalism, meaning that one cannot invoke God to explain natural phenomena, while recognizing that such acceptance does not require one to accept a \"metaphysical\" naturalism, which proposes that nature is all that there is.\n\nThe Jesuit priest George Coyne has stated that is \"unfortunate that, especially here in America, creationism has come to mean...some literal interpretation of Genesis.\" He argues that \"...Judaic-Christian faith is radically creationist, but in a totally different sense. It is rooted in belief that everything depends on God, or better, all is a gift from God.\"\n\nOther Christians have expressed qualms about teaching creationism. In March 2006, then Archbishop of Canterbury Rowan Williams, the leader of the world's Anglicans, stated his discomfort about teaching creationism, saying that creationism was \"a kind of category mistake, as if the Bible were a theory like other theories.\" He also said: \"My worry is creationism can end up reducing the doctrine of creation rather than enhancing it.\" The views of the Episcopal Churcha major American-based branch of the Anglican Communionon teaching creationism resemble those of Williams.\n\nThe National Science Teachers Association is opposed to teaching creationism as a science, as is the Association for Science Teacher Education, the National Association of Biology Teachers, the American Anthropological Association, the American Geosciences Institute, the Geological Society of America, the American Geophysical Union, and numerous other professional teaching and scientific societies.\n\nIn April 2010, the American Academy of Religion issued \"Guidelines for Teaching About Religion in K‐12 Public Schools in the United States\", which included guidance that creation science or intelligent design should not be taught in science classes, as \"Creation science and intelligent design represent worldviews that fall outside of the realm of science that is defined as (and limited to) a method of inquiry based on gathering observable and measurable evidence subject to specific principles of reasoning.\" However, they, as well as other \"worldviews that focus on speculation regarding the origins of life represent another important and relevant form of human inquiry that is appropriately studied in literature or social sciences courses. Such study, however, must include a diversity of worldviews representing a variety of religious and philosophical perspectives and must avoid privileging one view as more legitimate than others.\"\n\nRandy Moore and Sehoya Cotner, from the biology program at the University of Minnesota, reflect on the relevance of teaching creationism in the article \"The Creationist Down the Hall: Does It Matter When Teachers Teach Creationism?\" They conclude that \"Despite decades of science education reform, numerous legal decisions declaring the teaching of creationism in public-school science classes to be unconstitutional, overwhelming evidence supporting evolution, and the many denunciations of creationism as nonscientific by professional scientific societies, creationism remains popular throughout the United States.\"\n\nScience is a system of knowledge based on observation, empirical evidence, and the development of theories that yield testable explanations and predictions of natural phenomena. By contrast, creationism is often based on literal interpretations of the narratives of particular religious texts. Some creationist beliefs involve purported forces that lie outside of nature, such as supernatural intervention, and often do not allow predictions at all. Therefore, these can neither be confirmed nor disproved by scientists. However, many creationist beliefs can be framed as testable predictions about phenomena such as the age of the Earth, its geological history and the origins, distributions and relationships of living organisms found on it. Early science incorporated elements of these beliefs, but as science developed these beliefs were gradually falsified and were replaced with understandings based on accumulated and reproducible evidence that often allows the accurate prediction of future results.\n\nSome scientists, such as Stephen Jay Gould, consider science and religion to be two compatible and complementary fields, with authorities in distinct areas of human experience, so-called non-overlapping magisteria. This view is also held by many theologians, who believe that ultimate origins and meaning are addressed by religion, but favor verifiable scientific explanations of natural phenomena over those of creationist beliefs. Other scientists, such as Richard Dawkins, reject the non-overlapping magisteria and argue that, in disproving literal interpretations of creationists, the scientific method also undermines religious texts as a source of truth. Irrespective of this diversity in viewpoints, since creationist beliefs are not supported by empirical evidence, the scientific consensus is that any attempt to teach creationism as science should be rejected.\n\n\n\n"}
{"id": "853778", "url": "https://en.wikipedia.org/wiki?curid=853778", "title": "Dispersion relation", "text": "Dispersion relation\n\nIn physical sciences and electrical engineering, dispersion relations describe the effect of dispersion in a medium on the properties of a wave traveling within that medium. A dispersion relation relates the wavelength or wavenumber of a wave to its frequency. From this relation the phase velocity and group velocity of the wave have convenient expressions which then determine the refractive index of the medium. More general than the geometry-dependent and material-dependent dispersion relations, there are the overarching Kramers–Kronig relations that describe the frequency dependence of wave propagation and attenuation.\n\nDispersion may be caused either by geometric boundary conditions (waveguides, shallow water) or by interaction of the waves with the transmitting medium. Elementary particles, considered as matter waves, have a nontrivial dispersion relation even in the absence of geometric constraints and other media.\n\nIn the presence of dispersion, wave velocity is no longer uniquely defined, giving rise to the distinction of phase velocity and group velocity.\n\nDispersion occurs when pure plane waves of different wavelengths have different propagation velocities, so that a wave packet of mixed wavelengths tends to spread out in space. The speed of a plane wave, \"v\", is a function of the wave's wavelength formula_1:\n\nThe wave's speed, wavelength, and frequency, \"f\", are related by the identity\n\nThe function formula_4 expresses the dispersion relation of the given medium. Dispersion relations are more commonly expressed in terms of the angular frequency formula_5 and wavenumber formula_6. Rewriting the relation above in these variables gives\n\nwhere we now view \"f\" as a function of \"k\". The use of ω(\"k\") to describe the dispersion relation has become standard because both the phase velocity ω/\"k\" and the group velocity dω/d\"k\" have convenient representations via this function.\n\nThe plane waves being considered can be described by\n\nwhere \n\nPlane waves in vacuum are the simplest case of wave propagation: no geometric constraint, no interaction with a transmitting medium.\n\nFor electromagnetic waves in vacuum, the angular frequency is proportional to the wavenumber:\n\nThis is a \"linear\" dispersion relation. In this case, the phase velocity and the group velocity are the same:\n\nthey are given by \"c\", the speed of light in vacuum, a frequency-independent constant.\n\nTotal energy, momentum, and mass of particles are connected through the relativistic dispersion relation:\n\nwhich in the ultrarelativistic limit is\n\nand in the nonrelativistic limit is\n\nwhere formula_14 is the rest mass. In the nonrelativistic limit, formula_15 is a constant and formula_16 is the familiar kinetic energy expressed in terms of the momentum formula_17.\n\nThe transition from ultrarelativistic to nonrelativistic behaviour shows up as a slope change from \"p\" to \"p\" as shown in the log-log dispersion plot of \"E\" vs. \"p\".\n\nElementary particles, atomic nuclei, atoms, and even molecules behave in some contexts as matter waves. According to the de Broglie relations, their kinetic energy \"E\" can be expressed as a frequency \"ω\", and their momentum \"p\" as a wavenumber \"k\", using the reduced Planck constant ħ:\n\nAccordingly, angular frequency and wavenumber are connected through a dispersion relation,\nwhich in the nonrelativistic limit reads\n\nAs mentioned above, when the focus in a medium is on refraction rather than absorption—that is, on the real part of the refractive index—it is common to refer to the functional dependence of angular frequency on wavenumber as the \"dispersion relation\". For particles, this translates to a knowledge of energy as a function of momentum.\n\nThe name \"dispersion relation\" originally comes from optics. It is possible to make the effective speed of light dependent on wavelength by making light pass through a material which has a non-constant index of refraction, or by using light in a non-uniform medium such as a waveguide. In this case, the waveform will spread over time, such that a narrow pulse will become an extended pulse, i.e., be dispersed. In these materials, formula_20 is known as the group velocity and corresponds to the speed at which the peak of the pulse propagates, a value different from the phase velocity.\n\nThe dispersion relation for deep water waves is often written as \n\nwhere \"g\" is the acceleration due to gravity. Deep water, in this respect, is commonly denoted as the case where the water depth is larger than half the wavelength. In this case the phase velocity is\n\nand the group velocity is\n\nFor an ideal string, the dispersion relation can be written as\n\nwhere \"T\" is the tension force in the string and \"μ\" is the string's mass per unit length. As for the case of electromagnetic waves in a vacuum, ideal strings are thus a non-dispersive medium i.e., the phase and group velocities are equal and independent (to first order) of vibration frequency.\n\nFor a nonideal string, where stiffness is taken into account, the dispersion relation is written as\n\nwhere formula_26 is a constant that depends on the string.\n\nIn the study of solids, the study of the dispersion relation of electrons is of paramount importance. The periodicity of crystals means that many levels of energy are possible for a given momentum and that some energies might not be available at any momentum. The collection of all possible energies and momenta is known as the band structure of a material. Properties of the band structure define whether the material is an insulator, semiconductor or conductor.\n\nPhonons are to sound waves in a solid what photons are to light: they are the quanta that carry it. The dispersion relation of phonons is also non-trivial and important, being directly related to the acoustic and thermal properties of a material. For most systems, the phonons can be categorized into two main types: those whose bands become zero at the center of the Brillouin zone are called acoustic phonons, since they correspond to classical sound in the limit of long wavelengths. The others are optical phonons, since they can be excited by electromagnetic radiation.\n\nWith high energy (e.g., ) electrons in a transmission electron microscope, the energy dependence of higher order Laue zone (HOLZ) lines in convergent beam electron diffraction (CBED) patterns allows one, in effect, to \"directly image\" cross-sections of a crystal's three-dimensional dispersion surface. This dynamical effect has found application in the precise measurement of lattice parameters, beam energy, and more recently for the electronics industry: lattice strain.\n\nIsaac Newton studied refraction in prisms but failed to recognize the material dependence of the dispersion relation, dismissing the work of another researcher whose measurement of a prism's dispersion did not match Newton's own.\n\nDispersion of waves on water was studied by Pierre-Simon Laplace in 1776.\n\nThe universality of the Kramers-Kronig relations (1926–27) became apparent with subsequent papers on the dispersion relation's connection to causality in the scattering theory of all types of waves and particles.\n\n\n"}
{"id": "630795", "url": "https://en.wikipedia.org/wiki?curid=630795", "title": "Ecstasy (emotion)", "text": "Ecstasy (emotion)\n\nEcstasy (from Ancient Greek ἔκστασις \"ékstasis\") is a subjective experience of total involvement of the subject, with an object of his or her awareness. In classical Greek literature it refers to removal of the mind or body \"from its normal place of function.\"\n\nTotal involvement with an object of interest is not an ordinary experience because of being aware of other objects, thus ecstasy is an example of an altered state of consciousness characterized by diminished awareness of other objects or the total lack of the awareness of surroundings and everything around the object. The word is also used to refer to any heightened state of consciousness or intensely pleasant experience. It is also used more specifically to denote states of awareness of non-ordinary mental spaces, which may be perceived as spiritual (the latter type of ecstasy often takes the form of religious ecstasy).\n\nFrom a psychological perspective, ecstasy is a loss of self-control and sometimes a temporary loss of consciousness, which is often associated with religious mysticism, sexual intercourse and the use of certain drugs.\nFor the duration of the ecstasy the ecstatic is out of touch with ordinary life and is capable neither of communication with other people nor of undertaking normal actions. The experience can be brief in physical time, or it can go on for hours. Subjective perception of time, space or self may strongly change or disappear during ecstasy. For instance, if one is concentrating on a physical task, then any intellectual thoughts may cease. On the other hand, making a spirit journey in an ecstatic trance involves the cessation of voluntary bodily movement.\n\nEcstasy can be deliberately induced using religious or creative activities, meditation, music, dancing, breathing exercises, physical exercise, sexual intercourse or consumption of psychotropic drugs. The particular technique that an individual uses to induce ecstasy is usually also associated with that individual's particular religious and cultural traditions. Sometimes an ecstatic experience takes place due to occasional contact with something or somebody perceived as extremely beautiful or holy, or without any known reason. \"In some cases, a person might obtain an ecstatic experience 'by mistake'. Maybe the person unintentionally triggers one of the, probably many, physiological mechanisms through which such an experience can be reached. In such cases, it is not rare to find that the person later, by reading, looks for an interpretation and maybe finds it within a tradition.\"\n\nPeople interpret the experience afterward according to their culture and beliefs (as a revelation from God, a trip to the world of spirits or a psychotic episode). \"When a person is using an ecstasy technique, he usually does so within a tradition. When he reaches an experience, a traditional interpretation of it already exists.\" The experience together with its subsequent interpretation may strongly and permanently change the value system and the worldview of the subject (e.g. to cause religious conversion).\n\nIn 1925, James Leuba wrote: \"Among most uncivilized populations, as among civilized peoples, certain ecstatic conditions are regarded as divine possession or as union with the Divine. These states are induced by means of drugs, by physical excitement, or by psychical means. But, however produced and at whatever level of culture they may be found, they possess certain common features which suggest even to the superficial observer some profound connection. Always described as delightful beyond expression, these awesome ecstatic experiences end commonly in mental quiescence or even in total unconsciousness.\" He prepares his readers \"... to recognize a continuity of impulse, of purpose, of form and of result between the ecstatic intoxication of the savage and the absorption in God of the Christian mystic.\"\n\n\"In everyday language, the word 'ecstasy' denotes an intense, euphoric experience. For obvious reasons, it is rarely used in a scientific context; it is a concept that is extremely hard to define.\"\n\n\n"}
{"id": "45456836", "url": "https://en.wikipedia.org/wiki?curid=45456836", "title": "Extended mind thesis", "text": "Extended mind thesis\n\nThe extended mind thesis (EMT) says that an agent's mind and associated cognitive processing are neither skull-bound nor even body-bound, but extend into the agent's world. As Clark and Chalmers put it:\n\nThe question is raised as to the division point between the mind and the environment. The EMT proposes that some objects in the external environment are utilized by the mind in such a way that the objects can be seen as extensions of the mind itself. Specifically, the mind is seen to encompass every level of the cognitive process, which will often include the use of environmental aids.\n\nPhilosophical arguments against the extended mind thesis include the following.\n\nEach of these arguments is addressed in Clark (2008), in which he notes:\n\nWhile in \"Supersizing the Mind\" Clark defends a strong version of the hypothesis of extended cognition (contrasted with a hypothesis of embedded cognition) in other work, some of these objections have inspired more moderate reformulations of the extended mind thesis. Thus, the extended mind thesis may no longer depend on the parity considerations of Clark and Chalmers' original argument but, instead, emphasize the \"complementarity\" of internal and external elements of cognitive systems or processes. This version might be understood as emphasizing the explanatory value of the extended mind thesis for cognitive science rather than maintaining it as an ontological claim about the nature of mind or cognition.\n\n"}
{"id": "22757157", "url": "https://en.wikipedia.org/wiki?curid=22757157", "title": "French and Raven's bases of power", "text": "French and Raven's bases of power\n\nIn a notable study of power conducted by social psychologists John R. P. French and Bertram Raven in 1959, power is divided into five separate and distinct forms. In 1965 Raven revised this model to include a sixth form by separating the informational power base as distinct from the expert power base.\n\nRelating to social communication studies, \"power\" in social influence settings has introduced a large realm of research pertaining to persuasion tactics and leadership practices. Through social communication studies, it has been theorized that leadership and power are closely linked. It has been further presumed that different forms of power affect one's leadership and success. This idea is used often in organizational communication and throughout the workforce. In a notable study of power conducted by social psychologists John R. P. French and Bertram Raven in 1959, power is divided into five separate and distinct forms. They identified those five bases of power as \"coercive, reward, legitimate, referent, and expert\". This was followed by Raven's subsequent identification in 1965 of a sixth separate and distinct base of power: informational power. Furthermore, French and Raven defined social influence as a change in the belief, attitude, or behavior of a person (the target of influence) which results from the action of another person (an influencing agent), and they defined social power as the potential for such influence, that is, the ability of the agent to bring about such a change using available resources.\n\nThough there have been many formal definitions of leadership that did not include social influence and power, any discussion of leadership must inevitably deal with the means by which a leader gets the members of a group or organization to act and move in a particular direction.\n\nWhereby, this is to be considered \"power\" in social influential situations.\nThe original French and Raven (1959) model included five bases of power – reward, coercion, legitimate, expert, and referent – however, informational power was added by Raven in 1965, bringing the total to six. Since then, the model has gone through very significant developments: coercion and reward can have personal as well as impersonal forms. Expert and referent power can be negative or positive. Legitimate power, in addition to position power, may be based on other normative obligations: reciprocity, equity, and responsibility. Information may be utilized in direct or indirect fashion.\n\nFrench and Raven defined social power as the potential for influence (a change in the belief, attitude or behavior of a someone who is the target of influence.\n\nAs we know leadership and power are closely linked. This model shows how the different forms of power affect one's leadership and success. This idea is used often in organizational communication and throughout the workforce. \"The French-Raven power forms are introduced with consideration of the level of observability and the extent to which power is dependent or independent of structural conditions. Dependency refers to the degree of internalization that occurs among persons subject to social control. Using these considerations it is possible to link personal processes to structural conditions\".\n\nThe bases of social power have evolved over the years with benefits coming from advanced research and theoretical developments in related fields. On the basis of research and evidence, there have been many other developments and elaborations on the original theory. French and Raven developed an original model outlining the change dependencies and also further delineating each power basis.\n\nThough it is a common understanding that most social influence can still be understood by the original six bases of power, the foundational bases have been elaborated and further differentiated. Further Differentiating the Bases of Social Power\n\nAs mentioned above, there are now six main concepts of power strategies consistently studied in social communication research. They are described as Coercive, Reward, Legitimate, Referent, Expert, and Informational. Additionally, research has shown that source credibility has an explicit effect on the bases of power used in persuasion.\n\nSource credibility, the bases of power, and objective power, which is established based on variables such as position or title, are interrelated. The levels of each have a direct relationship in the manipulation and levels of one another.\n\nThe bases of power differ according to the manner in which social changes are implemented, the permanence of such changes, and the ways in which each basis of power is established and maintained.\n\nThe effectiveness of power is situational. Given there are six bases of power studied in the Communication field, it is very important to know the situational uses of each power, focusing on when each is most effective. According to French and Raven, \"it is of particular practical interest to know what bases of power or which power strategies are most likely to be effective, but it is clear that there is no simple answer.\n\nFor example, a power strategy that works immediately but relies on surveillance (for example, reward power or coercive power) may not last once surveillance ends. One organizational study found that reward power tended to lead to greater satisfaction on the part of employees, which means that it might increase influence in a broad range of situations. Coercive power was more effective in influencing a subordinate who jeopardized the success of the overall organization or threatened the leader's authority, even though in the short term it also led to resentment on the part of the target. A power strategy that ultimately leads to private acceptance and long-lasting change (for example, information power) may be difficult to implement, and consume considerable time and energy. In the short term, complete reliance on information power might even be dangerous (for example, telling a small child not to run into the street unattended). A military officer leading his troops into combat might be severely handicapped if he had to give complete explanations for each move. Instead, he would want to rely on unquestioned legitimate position power, backed up by coercive power. Power resources, which may be effective for one leader, dealing with one target or follower, may not work for a different leader and follower. The manner in which the power strategy is utilized will also affect its success or failure. Where coercion is deemed necessary, a leader might soften its negative effects with a touch of humor. There have been studies indicating that cultural factors may determine the effectiveness of power strategies.\"\nCoercive power uses the threat of force to gain compliance from another. Force may include physical, social, emotional, political, or economic means. Coercion is not always recognized by the target of influence. This type of power is based upon the idea of coercion. The main idea behind this concept is that someone is forced to do something that he/she does not desire to do. The main goal of coercion is compliance. Coercive power's influence is socially dependent on how the target relates to the change being desired by the influence agent. Furthermore, a person would have to be consistently watched by the influencing agent in order for the change to remain in effect.\n\nAn example of impersonal coercion relates a person's belief that the influencing agent has the real power to physically threaten, impose a monetary fine or dismiss an employee.\n\nAn example of personal coercion relates to a threat of rejection or the possibility of disapproval from a person whom is highly valued.\n\nAccording to Changingminds.org \"demonstrations of the harm are often used to illustrate what will happen if compliance is not gained\". The power of coercion has been proven to be related with punitive behavior that may be outside one's normal role expectations. However coercion has also been associated positively with generally punitive behavior and negatively associated to contingent reward behavior. This source of power can often lead to problems and in many circumstances it involves abuse. These type of leaders rely on the use of threats in their leadership style. Often the threats involve saying someone will be fired or demoted.\n\nReward power is based on the right of some to offer or deny tangible, social, emotional, or spiritual rewards to others for doing what is wanted or expected of them. Some examples of reward power (positive reward) are: (a) a child is given a dollar for earning better grades; (b) a student is admitted into an honor society for excellent effort; (c) a retiree is praised and feted for lengthy service at a retirement party; and (d) New York firefighters were heralded as heroes for their acts on 9-11-01. Some examples of reward power (negative reward) are: (a) a driver is fined for illegal parking; (b) a teenager grounded for a week for misbehaving; (c) a rookie player is ridiculed for not following tradition; and (d) President Harding's name is commonly invoked whenever political scandal is mentioned. Some pitfalls can emerge when a too heavy reliance is placed on reward power; these include: (a) some people become fixated and too dependent on rewards to do even mundane activities; (b) too severe fears of punishment can immobilize some people; (c) as time passes, past rewards become insufficient to motivate or activate desired outcomes; and (d) negative rewards may be perverted into positive attention.\n\nAn example of impersonal reward relates to promises of promotions, money and rewards from various social areas.\n\nAn example of personal reward relates to the reward of receiving approval from a desired person and building relationships with romantic partners.\n\nLegitimate power comes from an elected, selected, or appointed position of authority and may be underpinned by social norms. This power which means the ability to administer to another certain feelings of obligation or the notion of responsibility. \"Rewarding and Punishing subordinates is generally seen as a legitimate part of the formal or appointed leadership role and most managerial positions in work organizations carry with them, some degree of expected reward and punishment.\" This type of formal power relies on position in an authority hierarchy. Occasionally, those possessing legitimate power fail to recognize they have it, and may begin to notice others going around them to accomplish their goals. Three bases of legitimate power are cultural values, acceptance of social structure, and designation. Cultural values comprise a general basis for legitimate power of one entity over another. Such legitimacy is conferred by others and this legitimacy can be revoked by the original granters, their designees, or their inheritors.\n\nLegitimate power originates from a target of influence accepting the power of the influencing agent whereas behavioral change or compliance occurs based on target's obligation. One who uses legitimate power may have a high need for power which is their motivator to use this base for change in behavior and influence. There may be a range of legitimate power.\n\nThe legitimate position power is based on the social norm which requires people to be obedient to those who hold superior positions in a formal or informal social structure. Examples may include: a police officer's legitimacy to make arrests; a parent's legitimacy to restrict a child's activities; the President's legitimacy to live in the White House; and the Congress' legitimacy to declare war. Some pitfalls can arise when too heavy reliance is placed on legitimate power; these include: (a) unexpected exigencies call for non-legitimized individuals to act in the absence of a legitimate authority – such as a citizen's arrest in the absence of a police official; and (b) military legitimacy\n\nThe legitimate power of reciprocity is based on the social norm of reciprocity. Which states how we feel obligated to do something in return for someone who does something beneficial for us.\n\nThe legitimate power of equity is based on the social norm of equity (or compensatory damages) The social norm of equity makes people feel compelled to compensate someone who has suffered or worked hard. As well as someone whom we have harmed in some way is based on the premise that there is a wrong that can be made right, which may be a compensatory form of righting the wrong.\n\nThe legitimate power of dependence is based on the social norm of social responsibility. Social responsibility norm states how people feel obligated to help someone who is in need of assistance.\n\nPeople traditionally obey the person with this power solely based on their role, position or title rather than the person specifically as a leader. Therefore, this type of power can easily be lost and the leader does not have his position or title anymore. This power is therefore not strong enough to be one's only form of influencing/persuading.\n\nReferent power is rooted in the affiliations we make and/or the groups and organizations we belong to. Our affiliation with a group and the beliefs of the group are shared to some degree. As Referent power emphasizes similarity, respect for an agent of influence's superiority may be undermined by a target of influence. Use of this power base and its outcomes may be negative or positive. An agent for change motivated with a strong need for affiliation and concern of likeability will prefer this power base and will influence their leadership style. Ingratiation or flattery and sense of community may be used by an agent of influence to enhance their influence.\nReferent power in a positive form utilizes the shared personal connection or shared belief between the influencing agent and target with the intention of positively correlated actions of the target.\n\nReferent power in a negative form produces actions in opposition to the intent of the influencing agent, this is the result from the agent's creation of cognitive dissonance between the referent influencing agent and the target's perception of that influence.\n\nExamples of referent power include: (a) each of the last seven White House press secretaries have been paid handsomely for their memoirs relating to their presence at the seat of government; (b) Mrs. Hillary Clinton gained political capital by her marriage to the President; (c) Reverend Pat Robertson lost a bid for the Republican Party's nomination for President due, in significant part, to his religious affiliation; and (4) national firefighters have received vocational acclaim due to the association with the heroic NYC firefighters. Some pitfalls can occur related to referent assumptions; these include: (a) guilt or glory by association where little or no true tie is established; (b) associative traits tend to linger long after real association ends; (c) some individuals tend to pay dearly for associates' misdeeds or terrible reputations. It is important to distinguish between referent power and other bases of social power involving control or conformity. According to Fuqua, Payne, and Cangemi, referent power acts a little like role model power. It depends on respecting, liking, and holding another individual in high esteem. It usually develops over a long period of time.\n\nThe power of holding the ability to administer to another a sense of personal acceptance or personal approval. This type of power is strong enough that the power-holder is often looked up to as a role model. This power is often regarded as admiration, or charm. The responsibility involved is heavy and the power easily lost, but when combined with other forms of power it can be very useful. Referent power is commonly seen in political and military figures, although celebrities often have this as well.\n\nExpert power is based on what one knows, experience, and special skills or talents. Expertise can be demonstrated by reputation, credentials certifying expertise, and actions. The effectiveness and impacts of the Expert power base may be negative or positive. According to Raven, there will be more use of Expert power if the motive is a need for achievement. The ability to administer to another information, knowledge or expertise. (Example: Doctors, lawyers). As a consequence of the expert power or knowledge, a leader is able to convince their subordinates to trust them. The expertise does not have to be genuine – it is the perception of expertise that provides the power base. When individuals perceive or assume that a person possesses superior skills or abilities, they award power to that person.\n\nExpert power in a positive form influences the target to act accordingly as instructed by the expert, based on the assumption of the expert's correct knowledge.\n\nExpert power in a negative form can result from a person acting in opposition to the experts instructions if the target feels that the expert has personal gain motives.\n\nSome examples include: (a) a violinist demonstrating through audition skill with music; (b) a professor submits school transcripts to demonstrate discipline expertise; (c) a bricklayer relies on 20+ years of experience to prove expertise. Some pitfalls can emerge when too heavy a reliance is made on expertise; these include: (a) sometimes inferences are made suggesting expertise is wider in scope than it actually is; for example, an expert in antique vases may have little expertise in antique lamps; (b) one's expertise is not everlasting; for example, a physician who fails to keep up with medical technology and advances may lose expertise; and (c) expertise does not necessarily carry with it common sense or ethical judgement.\n\nFrench and Raven's original five powers brought about change after many years, by which Raven added a sixth base of power. Informational is the ability of an agent of influence to bring about change through the resource of information. Raven arguably believed that power as a potential influence logically meant that information was a form of influence and the social power base of Information Power was derived. Informational influence results in cognition and acceptance by the target of influence. The ability for altered behavior initiated through information rather than a specific change agent is called socially independent change. In order to establish Information Power, an agent of influence would likely provide a baseline of information to a target of influence to lay the groundwork in order to be effective with future persuasion. A link between informational power, control, cooperation, and satisfaction have been hypothesized and tested in a lab study. The findings indicate that a channel member's control over another's strategy increases with its informational power source. According to Raven, there will be more use of Information power if the motive is a need for achievement and can also be affected by an agent's self-esteem. Feldman summarizes informational power as the most transitory type of power. If one gives information away, then the power is given away, which differs from other forms of power because it's grounded in what you know about the content of a specific situation. Other forms of power are independent of the content.\n\nInformation power comes as a result of possessing knowledge that others need or want. In the age of Information technology, information power is increasingly relevant as an abundance of information is readily available. There may be a cost-benefit analysis by an agent of influence to determine if Information Power or influence is the best strategy. Informational influence or persuasion would generally be favorable however it may not be best suited if timing and effort lacks. Information possessed that no one needs or wants is powerless. Information power extends to the ability to get information not presently held such as a case with a librarian or data base manager. Not all information is readily available; some information is closely controlled by few people. Examples of information that is sensitive or limits accessibility: (a) national security data; (b) personnel information for government or business; (c) corporate trade secrets; (d) juvenile court records; (e) many privately settled lawsuit documents; (f) Swiss bank account owners; and (g) private phone conversations. Of course, legally obtained phone tap warrants, spying, eavesdropping, group and group member leaks can allow others not intended to be privy to information. Possessing information is not, typically, the vital act; it is what one can and does do or potentially can do with the information that typically is of vital importance. Information can, and often is, used as a weapon as in a divorce, a child custody case, business dissolution, or in civil suits discoveries. Information has been used by some to extort action, utterance, agreement, or settlement by others.\n\nInformation power is a form of personal or collective power that is based on controlling information needed by others in order to reach an important goal. Our society is now reliant on information power as knowledge for influence, decision making, credibility, and control. Timely and relevant information delivered on demand can be the most influential way to acquire power. Information may be readily available through public records, research, and but information is sometimes assumed privileged or confidential. The target of influence accepts,comprehends and internalizes the change independently, without have to go back to the influencing agent.\n\nInformational power is based on the potential to utilize information. Providing rational arguments, using information to persuade others, using facts and manipulating information can create a power base. How information is used – sharing it with others, limiting it to key people, keeping it secret from key people, organizing it, increasing it, or even falsifying it – can create a shift in power within a group.\n\nInformation presented by the influencing agent directly to the target of change.\n\nInformation presented influencing agent indirectly to the target of change void of attempting influence, such as hints or suggestions.\n\nThe ability for altered behavior initiated through information rather than a specific change agent is called socially independent change. Power socially independent of change may reflect the target continuing changed behavior without referring to, or even remembering, the supervisor or individual of authority as an agent of change because the target understands and accepts the reasoning of information received.\n\nRaven acknowledged leaders can attempt to influence subordinates by access and control of information. Information power may be used in both personal and positional classifications and is among the most preferable power bases.\n\nInformational power includes not only possessing information, but also the ability to obtain relevant information in a timely way to amass a power base. The use of tools or technological mechanisms such as internet, smart phones, and Social media progresses society's access to information but informational power as a base is derived by determining the usefulness and appropriateness of the information.\n\nTradition power is that force that is exerted upon us to conform to traditional ways. Traditions, for the most part, are social constructs; they invite, seduce, or compel us to conform and act in predictable, patterned ways. Breaking with traditions put people at risk of social alienation. Traditions can blunt rationality; they can block innovation; and they can appear to outsiders as silly when original traditions' rationales become outdated or forgotten.\n\nThe power of traditions, rather than being typically vested in particular individuals, is ordinarily focused on group conformity\n\nCharismatic power is that aura possessed by only a few individuals in our midst; it is characterized by super confidence, typical physical attractiveness, social adroitness, amiability, sharpened leadership skills, and heightened charm. Some charisma has dark and sinister overtones such as that shown by Adolf Hitler, Jim Jones, Idi Amin, Osama bin Laden, David Koresh, and many confidence tricksters. Others demonstrate more positive displays of charisma such as that displayed by Jacqueline Kennedy, Charles de Gaulle, Diana, Princess of Wales, Michael Jordan, and Bruce Springsteen. Charisma has, in many cases, short circuited rationality; that is, others have been fooled into or lulled into not rationally considering what a charismatic requests or demands but going along as a result of the charismatic attraction. It must be remembered that power is effective only when the target of powerful actions agrees [implicitly or explicitly] to the relevant power dynamic; we are all technically able to resist the power of others; at times, however, we may feel powerless to resist or the social, political, personal, and/or emotional price to be paid is too high or we fear failure in resisting\n"}
{"id": "1163240", "url": "https://en.wikipedia.org/wiki?curid=1163240", "title": "Harihara", "text": "Harihara\n\nHarihara (Sanskrit: हरिहर) is the fused representation of Vishnu (Hari) and Shiva (Hara) from the Hindu tradition. Also known as Shankaranarayana (\"Shankara\" is Shiva, and \"Narayana\" is Vishnu) like Brahmanarayana\n(Half represents Brahma and half represents Vishnu), Harihara is thus revered by both Vaishnavites and Shaivites as a form of the Supreme God. \n\n\"Harihara\" is also sometimes used as a philosophical term to denote the unity of Vishnu and Shiva as different aspects of the same Ultimate Reality called Brahman. This concept of equivalence of various gods as one principle and \"oneness of all existence\" is discussed as Harihara in the texts of Advaita Vedanta school of Hindu philosophy.\n\nSome of the earliest sculptures of Harihara, with one half of the image as Shiva and other half as Vishnu, are found in the surviving cave temples of India, such as in the cave 1 and cave 3 of the 6th-century Badami cave temples.\n\nThe diversity within Hinduism encourages a wide variety of beliefs and traditions, of which two important and large traditions are associated with Vishnu and Shiva. Some schools focus on Vishnu (including his associated avatars such as Rama and Krishna) as the Supreme God, and others on Shiva (including his different avatars such as Mahadeva and Pashupata). The Puranas and various Hindu traditions treat both Shiva and Vishnu as being different aspects of the one Brahman. Harihara is a symbolic representation of this idea. A similar idea, called Ardhanarishvara or \"Naranari\", fuses masculine and feminine deities as one and equivalent representation in Hinduism.\n\nDepending on which scriptures (and translations) are quoted, evidence is available to support each of the different arguments. In most cases, even if one personality is taken as being superior over the other, much respect is still offered to both Vishnu and Shiva by the other's worshippers (i.e. Shiva is still regarded as being above the level of an ordinary jiva and 'the greatest of the Vaishnavas' by Vaishnavas who worship only Vishnu).\n\nSivananda states: \"Shiva and Vishnu are one and the same entity. They are essentially one and the same. They are the names given to the different aspects of the all-pervading Supreme Parabrahman the Supreme Being or the Absolute. ‘Sivasya hridayam vishnur-vishnoscha hridayam sivah—Vishnu is the heart of Shiva and likewise Shiva is the heart of Vishnu’.\"\n\nSwaminarayan holds that Vishnu and Shiva are different aspects of the same God. Notably, the Swaminarayan view is a minority view among Vaishnavites, but the dominant view in contemporary Hinduism which follows the Smarta view in general.\n\nHarihara is depicted in art as split down the middle, one half representing Shiva, the other half representing Vishnu. The Shiva half will have the matted locks of a yogic master piled high on his head and sometimes will wear a tiger skin, reserved for the most revered ascetics. Shiva's pale skin may be read as ash-covered in his role as an ascetic. The Vishnu half will wear a tall crown and other jewelry, representing his responsibility for maintaining world order. Vishnu's black skin represents holiness. Broadly, these distinctions serve to represent the duality of humble religious influence in the ascetic and authoritative secular power in the king or householder. However, in other aspects Shiva also takes on the authoritative position of householder, a position which is directly at odds with the ascetic position depicted in his Harihara manifestation.\n\nHarihara has been part of temple iconography throughout South Asia and Southeast Asia, with some illustrations listed in the following table. In some states, the concept of Harihara appears through alternate names and its progeny; for example, temples incorporating Ayyappan and Shasta deities in Kerala illustrate this Hindu tradition there since at least the 7th century.\n\n\n\n"}
{"id": "32612385", "url": "https://en.wikipedia.org/wiki?curid=32612385", "title": "Hindley–Milner type system", "text": "Hindley–Milner type system\n\nA Hindley–Milner (HM) type system is a classical type system for the lambda calculus with parametric polymorphism. It is also known as Damas–Milner or Damas–Hindley–Milner. It was first described by J. Roger Hindley and later rediscovered by Robin Milner. Luis Damas contributed a close formal analysis and proof of the method in his PhD thesis.\n\nAmong HM's more notable properties are its completeness and its ability to infer the most general type of a given program without programmer-supplied type annotations or other hints. Algorithm W is an efficient type inference method that performs in almost linear time with respect to the size of the source, making it practically useful to type large programs. HM is preferably used for functional languages. It was first implemented as part of the type system of the programming language ML. Since then, HM has been extended in various ways, most notably with type class constraints like those in Haskell.\n\nOne and the same thing can be used for many purposes. A chair might be used to support a sitting person but also as a ladder to stand on while changing a light bulb or as a clothes valet. Beside having particular material qualities, which make a chair usable as such, it also has the particular designation for its use. When no chair is at hand, other things might be used as a seat, and so the designation of a thing can be changed as fast as one can turn an empty bottle crate upside down to change its purpose from a container to that of a support.\n\nDifferent uses of physically near-identical things are usually accompanied by giving those things different names to emphasize the intended purpose. Depending on the use, seamen have a dozen or more words for a rope though it might materially be the same thing. The same in everyday language, where a leash indicates a use different to a line.\n\nIn computer science, this practice of naming things by its intended use is put to an extreme called \"typing\" and the names or expressions called \"types\":\n\n\nBeside structuring objects, (data) types serve as means to validate that these objects are used as intended. Much like a crate that could only be used as a support or a container at a time, a particular arrangement of bytes designated for one purpose might exclude other possible uses.\n\nIn programming, these uses are expressed as \"functions\" or \"procedures\" which serve the role of verbs in natural language. As an example for typing verbs, an English dictionary might define \"gift\" as \"to give so. sth.\", indicating that the object must be a person and the indirect object a physical thing. In programming, \"so.\" (someone) and \"sth.\" (something) would be called types and using a thing into the place of \"so.\" would be indicated as a programming error by a type checker.\n\nBeside checking, one can use the types in this example to gain knowledge about an unknown word. Reading the sentence \"Mary gifts John a bilber\" the types could be used to conclude that a \"bilber\" is likely a physical thing. This activity and conclusion is called \"type inference\". As the story unfolds, more and more information about the unknown \"bilber\" may be gained, and eventually enough details become known to form a complete image of that kind of thing.\n\nThe type inference method designed by Hindley and Milner does just this for programming languages. The advantage of type inference over type checking is that it allows a more natural and dense style of programming. Instead of starting a program text with a glossary defining what a bilber and everything else is, one can distribute this information over the text simply by using the yet undefined words and let a program collect all the details about them. The method works for both nouns (data types) and for verbs (functions types). As a consequence, a programmer can proceed without ever mentioning types at all, while still having the full support of a type checker that validates their writing. When reading a program, the programmer can use type inference to query the full definition of anything named in the program whenever needed.\n\nHistorically, type inference to this extent was developed for a particular group of programming languages, called functional languages. These started in 1958 with Lisp, a programming language based on the lambda calculus and that compares well with modern scripting languages like Python or Lua. Lisp was mainly used for computer science research, often for symbol manipulation purposes where large, tree-like data structures were common.\n\nData in Lisp is dynamically typed and the types are only available to some degree while running a program. Debugging type errors was no less of a concern than it is with modern script languages. But, being completely untyped, i.e. written without any explicit type information, maintaining large programs written in Lisp soon became a problem because the many complicated types of the data were mentioned only in the program documentation and comments at best.\n\nThus, the need to have a Lisp-like language with machine-checkable types became more and more pressing. At some point, programming language development faced two challenges:\n\n\nAs an example, polymorphically constructing the list \"(1 2)\" of two numbers would mean writing:\n\nThis example was quite typical. Every third word a type, monotonously serving the type checker in every step. This worsens when the types become more complex. Then, the methods to be expressed in code become buried in types.\n\nTo handle this issue, effective methods for type inference were the subject of research, and Hindley-Milner's method was one of them. Their method was first used in ML (1973) and is also used in an extended form in Haskell (1990). The HM type inference method is strong enough to infer types not only for expressions, but for whole programs including the procedures and local definitions, providing a type-less style of programming.\n\nThe following text gives an impression of the resulting programming style for the quicksort procedure in Haskell:\n\nThough all of the functions in the above example need type parameters, types are nowhere mentioned. The code is statically type-checked even though the type of the function defined is unknown and must be inferred to type-check the applications in the body.\n\nOver the years, other programming languages added their own version of parametric types. C++ templates were introduced in 1998 and Java introduced generics in 2004. As programming with type parameters became more common, problems similar to the ones sketched for Lisp surfaced in imperative languages too, perhaps not as pressing as it was for the functional languages. As a consequence, these languages obtained support for some type inference techniques, for instance \"auto\" in C++11 (2014). Unfortunately, the stronger type inference methods developed for functional programming cannot simply be integrated in the imperative languages, as their type systems' features are in part incompatible, and a programming language must be designed from the ground up when continuous type inference is wanted.\n\nBefore presenting the HM type system and related algorithms, the following sections make some features of HM more formal and precise.\n\nIn a typing, an expression E is opposed to a type T, formally written as E : T. Usually a typing only makes sense within some context, which is omitted here.\n\nIn this setting, the following questions are of particular interest:\n\n\nFor the simply typed lambda calculus, all three questions are decidable. The situation is not as comfortable when more expressive types are allowed. Additionally, the simply typed lambda calculus makes the types of the parameters of each function explicit, while they are not needed in HM. While HM is a method for type inference, it can be used also for type checking and answer the first question. To do that, a type is first inferred from E and then compared with the type wanted. The third question becomes of interest when looking at recursively-defined functions at the end of this article.\n\nIn the simply typed lambda calculus, types formula_1 are either atomic type constants or function types of form formula_2. Such types are \"monomorphic\". Typical examples are the types used in arithmetic values:\n\nContrary to this, the untyped lambda calculus is neutral to typing at all, and many of its functions can be meaningfully applied to all type of arguments. The trivial example is the identity function\n\nwhich simply returns whatever value it is applied to. Less trivial examples include parametric types like lists.\n\nWhile polymorphism in general means that operations accept values of more than one type, the polymorphism used here is parametric. One finds the notation of \"type schemes\" in the literature, too, emphasizing the parametric nature of the polymorphism. Additionally, constants may be typed with (quantified) type variables. E.g.:\n\nPolymorphic types can become monomorphic by consistent substitution of their variables. Examples of monomorphic \"instances\" are:\n\nMore generally, types are polymorphic when they contain type variables, while types without them are monomorphic.\n\nContrary to the type systems used for example in Pascal (1970) or C (1972), which only support monomorphic types, HM is designed with emphasis on parametric polymorphism. The successors of the languages mentioned, like C++ (1985), focused on different types of polymorphism, namely subtyping in connection with object-oriented programming and overloading. While subtyping is incompatible with HM, a variant of systematic overloading is available in the HM-based type system of Haskell.\n\nWhen extending the simply-typed lambda calculus towards polymorphism, one has to define when deriving an instance of a value is admissible. Ideally, this would be allowed with any use of a bound variable, as in:\n\nUnfortunately, type inference in such a system is not decidable. Instead, HM provides \"let-polymorphism\" of the form\n\nrestricting the binding mechanism in an extension of the expression syntax. Only values bound in a let construct are subject to instantiation, i.e. are polymorphic, while the parameters in lambda-abstractions are treated as being monomorphic.\n\nThe remainder of the article is more technical as it has to present the HM method as it is handled in the literature. It proceeds as follows:\n\n\nThe same description of the deduction system is used throughout, even for the two algorithms, to make the various forms in which the HM method is presented directly comparable.\n\nThe type system can be formally described by syntax rules that fix a language for the expressions, types, etc. The presentation here of such a syntax is not too formal, in that it is written down not to study the surface grammar, but rather the depth grammar, and leaves some syntactical details open. This form of presentation is usual. Building on this, type rules are used to define how expressions and types are related. As before, the form used is a bit liberal.\n\nThe expressions to be typed are exactly those of the lambda calculus extended with a let-expression as shown in the adjacent table. Parentheses can be used to disambiguate an expression. The application is left-binding and binds stronger than abstraction or the let-in construct.\n\nTypes are syntactically split into two groups, monotypes and polytypes.\n\nMonotypes always designate a particular type. Monotypes formula_4 are syntactically represented as terms.\n\nExamples of monotypes include type constants like formula_5 or formula_6, and parametric types like formula_7. The later types are examples of \"applications\" of type functions, for example, from the set\nformula_8, \nwhere the superscript indicates the number of type parameters. The complete set of type functions formula_9 is arbitrary in HM, except that it \"must\" contain at least formula_10, the type of functions. It is often written in infix notation for convenience. For example, a function mapping integers to strings has type formula_11. Again, parentheses can be used to disambiguate a type expression. The application binds stronger than the infix arrow, which is right-binding.\n\nType variables are admitted as monotypes. Monotypes are not to be confused with monomorphic types, which exclude variables and allow only ground terms.\n\nTwo monotypes are equal if they have identical terms.\n\n\"Polytypes\" (or \"type schemes\") are types containing variables bound by one or more for-all quantifiers, e.g. formula_12.\n\nA function with polytype formula_12 can map \"any\" value of the same type to itself,\nand the identity function is a value for this type.\n\nAs another example formula_14 is the type of a function mapping all finite sets to integers. A function which returns the cardinality of a set would be a value of this type.\n\nNote that quantifiers can only appear top level, i.e. a type formula_15 for instance, is excluded by the syntax of types. Note also that monotypes are included in the polytypes, thus a type has the general form formula_16, where formula_4 is a monotype.\n\nEquality of polytypes is up to reordering the quantification and renaming the quantified variables (formula_18-conversion). Further, quantified variables not occurring in the monotype can be dropped.\n\nTo meaningfully bring together the still disjoint parts (syntax expressions and types) a third part is needed: context. Syntactically, a context is a list of pairs formula_19, called assignments, assumptions or bindings, each pair stating that value variable formula_20has type formula_21. All three parts combined give a \"typing judgment\" of the form formula_22,\nstating that under assumptions formula_23, the expression formula_24 has type formula_25.\n\nIn a type formula_16, the symbol formula_27 is the quantifier binding the type variables formula_28 in the monotype formula_4. The variables formula_28 are called \"quantified\" and any occurrence of a quantified type variable in formula_4 is called \"bound\" and all unbound type variables in formula_4 are called \"free\". Additionally to the quantification formula_27 in polytypes, type variables can also be bound by occurring in the context, but with the inverse effect on the right hand side of the formula_34. Such variables then behave like type constants there. Finally, a type variable may legally occur unbound in a typing, in which case they are implicitly all-quantified.\n\nThe presence of both bound and unbound type variables is a bit uncommon in programming languages. Often, all type variables are implicitly treated all-quantified. For instance, one does not have clauses with free variables in Prolog. Likely in Haskell, in the absence of the ScopedTypeVariables language extension, all type variables implicitly occur quantified, i.e. a Haskell type codice_4 means formula_12 here.\n\nPolymorphism means that one and the same expression can have (perhaps\ninfinitely) many types. But in this type system, these types are not completely\nunrelated, but rather orchestrated by the parametric polymorphism.\n\nAs an example, the identity formula_36 can have formula_37 as its type as well as\nformula_38 or formula_39 and many others, but not formula_40. The most general type for this function is\nformula_41, while the\nothers are more specific and can be derived from the general one by consistently\nreplacing another type for the \"type parameter\", i.e. the quantified\nvariable formula_18. The counter-example fails because the\nreplacement is not consistent.\n\nThe consistent replacement can be made formal by applying a substitution formula_43 to the term of a type formula_4, written formula_45. As the example suggests, substitution is not only strongly related to an order, that expresses that a type is more or less special, but also with the all-quantification which allows the substitution to be applied.\nFormally, in HM, a type formula_25 is \nmore general than formula_47, formally formula_48 if some quantified variable in formula_25 is\nconsistently substituted such that one gains formula_47 as shown in the side bar.\nThis order is part of the type definition of the type system.\n\nWhile substituting a monomorphic (ground) type for a quantified variable is\nstraight forward, substituting a polytype has some pitfalls caused by the\npresence of free variables. Most particularly, unbound variables must not be\nreplaced. They are treated as constants here. Additionally, note\nthat quantifications can only occur top-level. Substituting a parametric type,\none has to lift its quantors. The table on the right makes the rule precise.\n\nAlternatively, consider an equivalent notation for the polytypes without\nquantors in which quantified variables are represented by a different set of\nsymbols. In such a notation, the specialization reduces to plain consistent\nreplacement of such variables.\n\nThe relation formula_51 is a partial order\nand formula_52 is its smallest element.\n\nWhile specialization of a type scheme is one use of the order, it plays a\ncrucial second role in the type system. Type inference with polymorphism\nfaces the challenge of summarizing all possible types an expression may have.\nThe order guarantees that such a summary exists as the most general type\nof the expression.\n\nThe type order defined above can be extended to typings because the implied all-quantification of typings enables consistent replacement:\nContrary to the specialisation rule, this is not part of the definition, but like the implicit all-quantification rather a consequence of the type rules defined next.\nFree type variables in a typing serve as placeholders for possible refinement. The binding effect of the environment to free type\nvariables on the right hand side of formula_34 that prohibits their substitution in the specialisation rule is again\nthat a replacement has to be consistent and would need to include the whole typing.\n\nThe syntax of HM is carried forward to the syntax of the inference rules that form the body of the formal system, by using the typings as judgments. Each of the rules define what conclusion could be drawn from what premises. Additionally to the judgments, some extra conditions introduced above might be used as premises, too.\n\nA proof using the rules is a sequence of judgments such that all premises are listed before a conclusion. The examples below show a possible format of proofs. From left to right, each line shows the conclusion, the formula_55 of the rule applied and the premises, either by referring to an earlier line (number) if the premise is a judgment or by making the predicate explicit.\n\nThe side box shows the deduction rules of the HM type system. One can roughly divide the rules into two groups:\n\nThe first four rules formula_56 (variable or function access), formula_57 (\"application\", i.e. function call with one parameter), formula_58 (\"abstraction\", i.e. function declaration) and formula_59 (variable declaration) are centered around the syntax, presenting one rule for each of the expression forms. Their meaning is obvious at the first glance, as they decompose each expression, prove their sub-expressions and finally combine the individual types found in the premises to the type in the conclusion.\n\nThe second group is formed by the remaining two rules formula_60 and formula_61.\nThey handle specialization and generalization of types. While the rule formula_60 should be clear from the section on specialization above, formula_61 complements the former, working in the opposite direction. It allows generalization, i.e. to quantify monotype variables not bound in the context.\nThe following two examples exercise the rule system in action. Since both the expression and the type are given, they are a type-checking use of the rules.\n\nExample: A proof for formula_64 where formula_65,\ncould be written\n\nExample: To demonstrate generalization,\nformula_67\nis shown below:\n\nNot visible immediately, the rule set encodes a regulation under which circumstances a type might be generalized or not by a slightly varying use of mono- and polytypes in the rules formula_58 and formula_59. Remember that formula_25 and formula_4 denote poly- and monotypes respectively.\n\nIn rule formula_58, the value variable of the parameter of the function formula_74 is added to the context with a monomorphic type through the premise formula_75, while in the rule formula_59, the variable enters the environment in polymorphic form formula_77. Though in both cases the presence of formula_78 in the context prevents the use of the generalisation rule for any free variable in the assignment, this regulation forces the type of parameter formula_78 in a formula_80-expression to remain monomorphic, while in a let-expression, the variable could be introduced polymorphic, making specializations possible.\n\nAs a consequence of this regulation, formula_81 cannot be typed,\nsince the parameter formula_82 is in a monomorphic position, while formula_83 has type formula_84, because formula_82 has been introduced in a let-expression and is treated polymorphic therefore.\n\nThe generalisation rule is also worth for closer look. Here, the all-quantification implicit in the premise formula_86 is simply moved to the right hand side of formula_87 in the conclusion. This is possible, since formula_18 does not occur free in the context. Again, while this makes the generalisation rule plausible, it is not really a consequence. Vis versa, the generalisation rule is part of the definition of HM's type system and the implicit all-quantification a consequence.\n\nNow that the deduction system of HM is at hand, one could present an algorithm and validate it with respect to the rules.\nAlternatively, it might be possible to derive it by taking a closer look on how the rules interact and proof are\nformed. This is done in the remainder of this article focusing on the possible decisions one can make while proving a typing.\n\nIsolating the points in a proof, where no decision is possible at all,\nthe first group of rules centered around the syntax leaves no choice since\nto each syntactical rule corresponds a unique typing rule, which determines\na part of the proof, while between the conclusion and the premises of these\nfixed parts chains of formula_60 and formula_61\ncould occur. Such a chain could also exist between the conclusion of the\nproof and the rule for topmost expression. All proofs must have\nthe so sketched shape.\n\nBecause the only choice in a proof with respect of rule selection are the\nformula_60 and formula_61 chains, the\nform of the proof suggests the question whether it can be made more precise,\nwhere these chains might be needed. This is in fact possible and leads to a\nvariant of the rules system with no such rules.\n\nA contemporary treatment of HM uses a purely syntax-directed rule system due to\nClement\nas an intermediate step. In this system, the specialization is located directly after the original formula_56 rule\nand merged into it, while the generalization becomes part of the formula_59 rule. There the generalization is\nalso determined to always produce the most general type by introducing the function formula_95, which quantifies\nall monotype variables not bound in formula_23.\n\nFormally, to validate, that this new rule system formula_97 is equivalent to the original formula_87, one has\nto show that formula_99, which falls apart into two sub-proofs:\n\n\nWhile consistency can be seen by decomposing the rules formula_59 and formula_56\nof formula_97 into proofs in formula_87, it is likely visible that formula_97 is incomplete, as\none cannot show formula_107 in formula_97, for instance, but only\nformula_109. An only slightly weaker version of completeness is provable\n\n\nimplying, one can derive the principal type for an expression in formula_97 allowing us to generalize the proof in the end.\n\nComparing formula_87 and formula_97, note that now only monotypes appear in the judgments of all rules. Additionally, the shape of any possible proof with the deduction system is now identical to the shape of the expression (both seen as trees). Thus the expression fully determines the shape of the proof. In formula_87 the shape would likely be determined with respect to all rules except formula_60 and formula_61, which allow building arbitrarily long branches (chains) between the other nodes.\n\nNow that the shape of the proof is known, one is already close to formulating a type inference algorithm.\nBecause any proof for a given expression must have the same shape, one can assume the monotypes in the\nproof's judgements to be undetermined and consider how to determine them.\n\nHere, the substitution (specialisation) order comes into play. Although at the first glance one cannot determine the types locally, the hope is that it is possible to refine them with the help of the order while traversing the proof tree, additionally assuming, because the resulting algorithm is to become an inference method, that the type in any premise will be determined as the best possible. And in fact, one can, as looking at the rules of formula_97 suggests:\n\n\nThe first premise forces the outcome of the inference to be of the form formula_129.\n\nThe second premise requires that the inferred type is equal to formula_4 of the first premise. Now there are two possibly different types, perhaps with open type variables, at hand to compare and to make equal if it is possible. If it is, a refinement is found, and if not, a type error is detected again. An effective method is known to \"make two terms equal\" by substitution, Robinson's Unification in combination with the so-called Union-Find algorithm.\n\nTo briefly summarize the union-find algorithm, given the set of all types in a proof, it allows one to group them together into equivalence classes by means of a formula_132\nprocedure and to pick a representative for each such class using a formula_133 procedure. Emphasizing the word procedure in the sense of side effect, we're clearly leaving the realm of logic in order to prepare an effective algorithm. The representative of a formula_134 is determined such that, if both formula_135 and formula_136 are type variables then the representative is arbitrarily one of them, but while uniting a variable and a term, the term becomes the representative. Assuming an implementation of union-find at hand, one can formulate the unification of two monotypes as follows:\n\nNow having a sketch of an inference algorithm at hand, a more formal presentation is given in the next section. It is described in Milner P. 370 ff. as algorithm J.\n\nThe presentation of Algorithm J is a misuse of the notation of logical rules, since it includes side effects but allows a direct comparison with formula_97 while expressing an efficient implementation at the same time. The rules now specify a procedure with parameters formula_138 yielding formula_4 in the conclusion where the execution of the premises proceeds from left to right.\n\nThe procedure formula_140 specializes the polytype formula_25 by copying the term and replacing the bound type variables consistently by new monotype variables. 'formula_142' produces a new monotype variable. Likely, formula_95 has to copy the type introducing new variables for the quantification to avoid unwanted captures. Overall, the algorithm now proceeds by always making the most general choice leaving the specialization to the unification, which by itself produces the most general result. As noted above, the final result formula_4 has to be generalized to formula_95 in the end, to gain the most general type for a given expression.\n\nBecause the procedures used in the algorithm have nearly O(1) cost, the overall cost of the algorithm is close to linear in the size of the expression for which a type is to be inferred. This is in strong contrast to many other attempts to derive type inference algorithms, which often came out to be NP-hard, if not undecidable with respect to termination. Thus the HM performs as well as the best fully informed type-checking algorithms can. Type-checking here means that an algorithm does not have to find a proof, but only to validate a given one.\n\nEfficiency is slightly reduced because the binding of type variables in the context has to be maintained to allow computation of formula_95 and enable an occurs check to prevent the building of recursive types during formula_147.\nAn example of such a case is formula_148, for which no type can be derived using HM. Practically, types are only small terms and do not build up expanding structures. Thus, in complexity analysis, one can treat comparing them as a constant, retaining O(1) costs.\n\nIn the previous section, while sketching the algorithm its proof was hinted at with metalogical argumentation. While this leads to an efficient algorithm J, it is\nnot clear whether the algorithm properly reflects the deduction systems D or S\nwhich serve as a semantic base line.\n\nThe most critical point in the above argumentation is the refinement of monotype\nvariables bound by the context. For instance, the algorithm boldly changes the\ncontext while inferring e.g. formula_149,\nbecause the monotype variable added to the context for the parameter formula_82 later needs to be refined\nto formula_151 when handling application.\nThe problem is that the deduction rules do not allow such a refinement.\nArguing that the refined type could have been added earlier instead of the\nmonotype variable is an expedient at best.\n\nThe key to reaching a formally satisfying argument is to properly include\nthe context within the refinement. Formally,\ntyping is compatible with substitution of free type variables.\n\nTo refine the free variables thus means to refine the whole typing.\n\nFrom there, a proof of algorithm J leads to algorithm W, which only makes the\nside effects imposed by the procedure formula_153 explicit by\nexpressing its serial composition by means of the substitutions\nformula_154. The presentation of algorithm W in the sidebar still makes use of side effects\nin the operations set in italic, but these are now limited to generating\nfresh symbols. The form of judgement is formula_155,\ndenoting a function with a context and expression as parameter producing a monotype together with\na substitution. formula_156 is a side-effect free version\nof formula_153 producing a substitution which is the most general unifier.\n\nWhile algorithm W is normally considered to be \"the\" HM algorithm and is\noften directly presented after the rule system in literature, its purpose is\ndescribed by Milner on P. 369 as follows:\n\nWhile he considered W more complicated and less efficient, he presented it \nin his publication before J. It has its merits when side effects are unavailable or unwanted.\nBy the way, W is also needed to prove completeness, which is factored by him into the soundness proof.\n\nBefore formulating the proof obligations, a deviation between the rules systems\nD and S and the algorithms presented needs to be emphasized.\n\nWhile the development above sort of misused the monotypes as \"open\" proof variables, the possibility that proper monotype variables might be harmed was sidestepped by introducing fresh variables and hoping for the best. But there's a catch: One of the promises made was that these fresh variables would be \"kept in mind\" as such. This promise is not fulfilled by the algorithm.\n\nHaving a context formula_158, the expression formula_159\ncannot be typed in either formula_87 or formula_97, but the algorithms come up with\nthe type formula_162, where W additionally delivers the substitution formula_163,\nmeaning that the algorithm fails to detect all type errors. This omission can easily be fixed by more carefully distinguishing proof\nvariables and monotype variables.\n\nThe authors were well aware of the problem but decided not to fix it. One might assume a pragmatic reason behind this.\nWhile more properly implementing the type inference would have enabled the algorithm to deal with abstract monotypes,\nthey were not needed for the intended application where none of the items in a preexisting context have free\nvariables. In this light, the unneeded complication was dropped in favor of a simpler algorithm.\nThe remaining downside is that the proof of the algorithm with respect to the rule system is less general and can only be made\nfor contexts with formula_164 as a side condition.\n\nformula_165\n\nThe side condition in the completeness obligation addresses how the deduction may give many types, while the algorithm always produces one. At the same time, the side condition demands that the type inferred is actually the most general.\n\nTo properly prove the obligations one needs to strengthen them first to allow activating the substitution lemma threading the substitution formula_166 through formula_97 and formula_168. From there, the proofs are by induction over the expression.\n\nAnother proof obligation is the substitution lemma itself, i.e. the substitution of the typing, which finally establishes the all-quantification. The later cannot formally be proven, since no such syntax is at hand.\n\nTo make programming practical recursive functions are needed.\nA central property of the lambda calculus is that recursive definitions\nare not directly available, but can instead be expressed with a fixed point combinator.\nBut unfortunately, the fixpoint combinator cannot be formulated in a typed version\nof the lambda calculus without having a disastrous effect on the system as outlined\nbelow.\n\nThe original paper notes that recursion can be realized by a combinator\nformula_169. A possible recursive definition could thus be formulated as\nformula_170.\n\nAlternatively an extension of the expression syntax and an extra typing rule is possible:\n\nwhere\nbasically merging formula_58 and formula_59 while including the recursively defined\nvariables in monotype positions where they occur to the left of the formula_176 but as polytypes to the right of it. This\nformulation perhaps best summarizes the essence of let-polymorphism.\n\nWhile the above is straightforward it does come at a price.\n\nType theory connects lambda calculus computation and logic.\nThe easy modification above has effects on both:\n\n\nPrograms in simply typed lambda calculus are guaranteed to always terminate. Moreover, they\nare even guaranteed to terminate under any evaluation strategy, be it top down, bottom up, breadth first, whatever. The same is true for expressions that have types in HM. It is well-known that separating\nterminating from non-terminating programs is most difficult, and especially in lambda calculus,\nwhich is so expressive that it can formulate recursion with just a few symbols. Thus the initial\ninability of HM to provide recursive functions was not an omission, but a feature. Adding\nrecursion enables normal programming but the guarantee is not longer valid.\n\nAnother reading of the typing is given by the Curry–Howard isomorphism. Here\nthe types are interpreted as logical expressions. Let's look at the type of the fixpoint combinator from this\nperspective, assuming the variables to have logical value:\nBut this is invalid.\nAdding an invalid axiom will break the logic in the sense that\nevery formula can then be shown to be true in it, e.g. formula_178.\nThus the ability to distinguish even two simple things is no longer given. Everything is the same and\ncollapses into 42. The fixpoint\ncombinator that came in so handy above also plays a role in Curry's paradox.\n\nLogic aside, does this matter for typing programs? It does. Since one is now able to formulate\nnon-terminating functions, one can make a function that would return whatever one wants but never really returns:\nIn practical programming such a function can come in handy when breaking out of a computation,\nlike with codice_5 in C, while silencing the type checker in\nthe current branch by returning essentially nothing but with a suitable type.\n\nLess desirable is that the type checker (type inferencer) now succeeds with a type for a function that in fact never returns any value, like formula_180. The function \"would\" return a value of this type, but it \"cannot\" because no terminating function with this type exists. The type checker's claim that everything is ok thus has to be taken with a grain of salt. The types might only be \"claimed\" to be checked, but the program can still be typed wrong. Only if all functions are terminating does formula_18 in the logic above have a \"true\" value, and the assertions of the type checker become strong again.\n\nOverloading means, that different functions still can be defined and used with the same name. Most programming languages at least provide overloading with the built-in arithmetic operations (+,<,etc.), to allow the programmer to write arithmetic expressions in the same form, even for different numerical types like codice_6 or codice_7. Because a mixture of these different types within the same expression also demands for implicit conversion, overloading especially for these operations is often built into the programming language itself. In some languages, this feature is generalized and made available to the user, e.g. in C++.\n\nWhile ad-hoc overloading has been avoided in functional programming for the computation costs both in type checking and inference, a means to systematise overloading has been introduced that resembles both in form and naming to object oriented programming, but works one level upwards. \"Instances\" in this systematic are not objects (i.e. on value level), but rather types.\nThe quicksort example mentioned in the introduction uses the overloading in the orders, having the following type annotation in Haskell:\n\nHerein, the type codice_8 is not only polymorphic, but also restricted to be an instance of some type class codice_9, that provides the order predicates codice_10 and codice_11 used in the functions body. The proper implementations of these predicates are then passed to quicksorts as additional parameters, as soon as quicksort is used on more concrete types providing a single implementation of the overloaded function quickSort.\n\nBecause the \"classes\" only allow a single type as their argument, the resulting type system can still provide inference. Additionally, the type classes can then be equipped with some kind of overloading order allowing one to arrange the classes as a lattice.\n\nParametric polymorphism implies that types themselves are passed as parameters as if they were proper values. Passed as arguments into a proper functions as in the introduction, but also into \"type functions\" as in the \"parametric\" type constants, leads to the question how to more properly type types themselves. A meta type, the \"type of types\" would be useful to create an even more expressive type system.\n\nThough this would be a straight forward extension, unfortunately, only unification is not longer decidable in the presence of meta types, rendering type inference impossible in this extend of generality.\nAdditionally, assuming a type of all types that includes itself as type leads into a paradox, as in the set of all sets, so one must proceed in steps of levels of abstraction.\nResearch in second order lambda calculus, one step upwards, showed, that type inference is undecidable in this generality.\n\nParts of one extra level has been introduced into Haskell named kind, where it is used helping to type monads. Kinds are left implicit, working behind scene in the inner mechanics of the extended type system.\n\nAttempts to combine subtyping and type inference have caused quite some frustration. While type inference is needed in object-oriented programming for the same reason as in functional languages, methods like HM cannot be made going for this purpose. It is not difficult to setup a type system with subtyping enabling object-oriented style, as\ne.g. Cardelli\n\n\nSuch objects would be immutual in a functional language context, but the type system would enable object-oriented programming style and the type inference method could be reused in imperative languages.\n\nThe subtyping rule for the record types is:\nSyntatically, record expressions would have form\nand have a type rule leading to the above type.\nSuch record values could then be used the same way as objects in object oriented programming.\n\n\n"}
{"id": "794550", "url": "https://en.wikipedia.org/wiki?curid=794550", "title": "Homo sacer", "text": "Homo sacer\n\nHomo sacer (Latin for \"the sacred man\" or \"the accursed man\") is a figure of Roman law: a person who is banned and may be killed by anybody, but may not be sacrificed in a religious ritual. \n\nThe meaning of the term \"sacer\" in Ancient Roman religion is not fully congruent with the meaning it took after Christianization, and which was adopted into English as \"sacred\". In early Roman religion \"sacer\", much like the Hebrew \"qadoš\", denotes anything \"set apart\" from common society and encompasses both the sense of \"hallowed\" and that of \"cursed\". This concept of the sacred is more in line with the Islamic notion of \"haram\". The \"homo sacer\" could thus also simply mean a person expunged from society and deprived of all rights and all functions in civil religion. \"Homo sacer\" is defined in legal terms as someone who can be killed without the killer being regarded as a murderer; and a person who cannot be sacrificed. The sacred human may thus be understood as someone outside the law, or beyond it. With respect to certain monarchs, in certain western legal traditions, the concepts of the sovereign and of the Homo Sacer have been conflated.\n\nThe term \"sacred man\" could also have been used because the condemned could only rely on protection of gods.\n\nThe status of \"homo sacer\" could fall upon one as a consequence of oath-breaking. An oath in antiquity was essentially a conditional self-cursing, i.e. invoking one or several deities and asking for their punishment in the event of breaking the oath. An oathbreaker was consequently considered the property of the gods whom he had invoked and then deceived. If the oathbreaker was killed, this was understood as the revenge of the gods into whose power he had given himself. Since the oathbreaker was already the property of the oath deity, he could no longer belong to human society, or be consecrated to another deity.\n\nA direct reference to this status is found in the Twelve Tables (8.21), laws of the early Roman Republic written in the 5th century BC. The paragraph states that a patron who deceives his clients is to be regarded as \"sacer\".\n\nThe idea of the status of an outlaw, a criminal who is declared as unprotected by the law and can consequently be killed by anyone with impunity, persists throughout the Middle Ages. Medieval perception condemned the entire human intrinsic moral worth of the outlaw, dehumanizing the outlaw literally as a \"wolf\" or \"wolf's-head\" (in an era where hunting of wolves existed strongly, including a commercial element) and is first revoked only by the English \"Habeas Corpus\" act of 1679 which declares that any criminal must be judged by a tribunal before being punished.\n\nItalian philosopher Giorgio Agamben takes the concept as the starting point of his main work \"\" (1998).\n\n\n\n"}
{"id": "227107", "url": "https://en.wikipedia.org/wiki?curid=227107", "title": "Intellectual", "text": "Intellectual\n\nAn intellectual is a person who engages in critical thinking, research, and reflection about society and proposes solutions for its normative problems, and gain authority as public intellectuals. Coming from the world of culture, either as a creator or as a mediator, the intellectual participates in politics either to defend a concrete proposition or to denounce an injustice, usually by rejecting, producing or extending an ideology, and by defending a system of values.\n\nSocially, intellectuals constitute the intelligentsia, a status class organised either by ideology (conservative, fascist, socialist, liberal, reactionary, revolutionary, democratic, communist intellectuals, \"et al.\"), or by nationality (American intellectuals, French intellectuals, Ibero–American intellectuals, \"et al.\"). The contemporary intellectual class originated from the \"intelligentsiya\" of Tsarist Russia (–1870s), the social stratum of those possessing intellectual formation (schooling, education, Enlightenment), and who were Russian society's counterpart to the German \"Bildungsbürgertum\" and to the French \"bourgeoisie éclairée\", the enlightened middle classes of those realms.\n\nIn the late 19th century, amidst the Dreyfus affair (1894–1906), an identity crisis of anti-semitic nationalism for the French Third Republic (1870–1940), the reactionary anti–Dreyfusards (Maurice Barrès, Ferdinand Brunetière, \"et al.\") used the terms \"intellectual\" and \"the intellectuals\" to deride the liberal Dreyfusards (Émile Zola, Octave Mirbeau, Anatole France, \"et al.\") as political dilettantes from the realms of French culture, art, and science, who had become involved in politics, by publicly advocating for the exoneration and liberation of Alfred Dreyfus, a Jewish French artillery captain falsely accused of betraying France to Germany.\n\nIn the 20th century, the term \"Intellectual\" acquired positive connotations of social prestige, derived from possessing intellect and intelligence, especially when the intellectual's activities exerted positive consequences in the public sphere and so increased the intellectual understanding of the public, by means of moral responsibility, altruism, and solidarity, without resorting to the manipulations of demagoguery, paternalism, and incivility (condescension). Hence, for the educated person of a society, participating in the public sphere—the political affairs of the city-state—is a civic responsibility dating from the Græco–Latin Classical era:\n\nThe determining factor for \"a Thinker\" (historian, philosopher, scientist, writer, artist, \"et al.\") to be considered a public intellectual is the degree to which he or she is implicated and engaged with the vital reality of the contemporary world; that is to say, participation in the public affairs of society. Consequently, being designated as a public intellectual is determined by the degree of influence of the designator's motivations, opinions, and options of action (social, political, ideological), and by affinity with the given thinker; therefore:\n\nAnalogously, the application and the conceptual value of the terms \"Intellectual\" and \"the Intellectuals\" are socially negative when the practice of intellectuality is exclusively in service to The Establishment who wield power in a society, as such:\n\nNoam Chomsky's negative view of the Establishment Intellectual suggests the existence of another kind of intellectual one might call \"the public intellectual,\" which is:\n\nThe intellectual is a type of intelligent person, who is associated with reason and critical thinking. Many everyday roles require the application of intelligence to skills that may have a psychomotor component, for example, in the fields of medicine or the arts, but these do not necessarily involve the practitioner in the \"world of ideas\". The distinctive quality of the intellectual person is that the mental skills, which one demonstrates, are not simply intelligent, but even more, they focus on thinking about the abstract, philosophical and esoteric aspects of human inquiry and the value of their thinking.\n\nThe intellectual and the scholarly classes are related; the intellectual usually is not a teacher involved in the production of scholarship, but has an academic background, and works in a profession, practices an art, or a science. The intellectual person is one who applies critical thinking and reason in either a professional or a personal capacity, and so has authority in the public sphere of their society; the term \"intellectual\" identifies three types of person, one who:\n\n\nThe term \"man of letters\" derives from the French term \"belletrist\" or \"homme de lettres\" but is not synonymous with \"an academic\". A \"man of letters\" was a literate man (\"able to read and write\") as opposed to an illiterate man, in a time when literacy was a rare form of cultural capital. In the 17th and 18th centuries, the \"Belletrists\" were the \"literati\", the French \"citizens of the Republic of Letters\", which evolved into the salon, a social institution, usually run by a hostess, meant for the edification, education, and cultural refinement of the participants.\n\nIn English, the term \"intellectual\" identifies a \"literate thinker\"; its earlier usage, as in the book title \"The Evolution of an Intellectual\" (1920), by John Middleton Murry, denotes literary activity, rather than the activities of the public intellectual.\n\nIn the late 19th century, when literacy was relatively common in European countries such as the United Kingdom, the \"Man of Letters\" (\"littérateur\") denotation broadened to mean \"specialized\", a man who earned his living writing intellectually (not creatively) about literature: the essayist, the journalist, the critic, et al. In the 20th century, such an approach was gradually superseded by the academic method, and the term \"Man of Letters\" became disused, replaced by the generic term \"intellectual\", describing the intellectual person. In late 19th century, the term \"intellectual\" became common usage to denote the defenders of the falsely accused artillery officer Alfred Dreyfus.\n\nIn early 19th century Britain, Samuel Taylor Coleridge coined the term \"clerisy\", the intellectual class responsible for upholding and maintaining the national culture, the secular equivalent of the Anglican clergy. Likewise, in Tsarist Russia, there arose the \"intelligentsia\" (1860s–70s), who were the status class of white-collar workers. The theologian Alister McGrath said that \"the emergence of a socially alienated, theologically literate, antiestablishment lay intelligentsia is one of the more significant phenomena of the social history of Germany in the 1830s\", and that \"three or four theological graduates in ten might hope to find employment\" in a church post. As such, politically radical thinkers already had participated in the French Revolution (1789–1799); Robert Darnton said that they were not societal outsiders, but \"respectable, domesticated, and assimilated\".\n\nThenceforth, in Europe, an intellectual class was socially important, especially to self-styled intellectuals, whose participation in society's arts, politics, journalism, and education—of either nationalist, internationalist, or ethnic sentiment—constitute \"vocation of the intellectual\". Moreover, some intellectuals were anti-academic, despite universities (the Academy) being synonymous with intellectualism.\n\nIn France, the Dreyfus affair marked the full emergence of the \"intellectual in public life\", especially Émile Zola, Octave Mirbeau, and Anatole France directly addressing the matter of French antisemitism to the public; thenceforward, \"intellectual\" became common, yet occasionally derogatory, usage; its French noun usage is attributed to Georges Clemenceau in 1898.\n\nHabermas' \"Structural Transformation of Public Sphere\" (1963) made significant contribution to the notion of public intellectual by historically and conceptually delineating the idea of private and public.\n\nIn Imperial China, in the period from 206 BC until AD 1912, the intellectuals were the \"Scholar-officials\" (\"Scholar-gentlemen\"), who were civil servants appointed by the Emperor of China to perform the tasks of daily governance. Such civil servants earned academic degrees by means of imperial examination, and also were skilled calligraphers, and knew Confucian philosophy. Historian Wing-Tsit Chan concludes that:\n\nIn Joseon Korea (1392–1910), the intellectuals were the \"literati\", who knew how to read and write, and had been designated, as the chungin (the \"middle people\"), in accordance with the Confucian system. Socially, they constituted the petite bourgeoisie, composed of scholar-bureaucrats (scholars, professionals, and technicians) who administered the dynastic rule of the Joseon dynasty.\n\nAddressing their role as a social class, Jean-Paul Sartre said that intellectuals are the moral conscience of their age; that their moral and ethical responsibilities are to observe the socio-political moment, and to freely speak to their society, in accordance with their consciences. Like Sartre and Noam Chomsky, public intellectuals usually are polymaths, knowledgeable of the international order of the world, the political and economic organization of contemporary society, the institutions and laws that regulate the lives of the layman citizen, the educational systems, and the private networks of mass communication media that control the broadcasting of information to the public.\n\nWhereas, intellectuals (political scientists and sociologists), liberals, and democratic socialists usually hold, advocate, and support the principles of democracy (liberty, equality, fraternity, human rights, social justice, social welfare, environmental conservation), and the improvement of socio-political relations in domestic and international politics, the conservative public-intellectuals usually defend the social, economic, and political \"status quo\" as the realisation of the \"perfect ideals\" of Platonism, and present a static dominant ideology, in which utopias are unattainable and politically destabilizing of society.\n\nIn Marxist philosophy, the social class function of the intellectuals (the intelligentsia) is to be the source of progressive ideas for the transformation of society; to provide advice and counsel to the political leaders; to interpret the country's politics to the mass of the population (urban workers and peasants); and, as required, to provide leaders from within their own ranks.\n\nThe Italian Communist theoretician Antonio Gramsci (1891–1937) developed Karl Marx's conception of the intelligentsia to include political leadership in the public sphere. That, because \"all knowledge is existentially-based\", the intellectuals, who create and preserve knowledge, are \"spokesmen for different social groups, and articulate particular social interests\". That intellectuals occur in each social class and throughout the right wing, the centre, and the left wing of the political spectrum. That, as a social class, the \"intellectuals view themselves as autonomous from the ruling class\" of their society. That, in the course of class struggle meant to achieve political power, every social class requires a native intelligentsia who shape the ideology (world view) particular to the social class from which they originated. Therefore, the leadership of intellectuals is required for effecting and realizing social change, because:\n\nIn the pamphlet \"What Is to Be Done?\" (1902), Lenin (1870–1924) said that vanguard-party revolution required the participation of the intellectuals to explain the complexities of socialist ideology to the uneducated proletariat and the urban industrial workers, in order to integrate them to the revolution; because \"the history of all countries shows that the working class, exclusively by its own efforts, is able to develop only trade-union consciousness\", and will settle for the limited, socio-economic gains so achieved. In Russia, as in Continental Europe, Socialist theory was the product of the \"educated representatives of the propertied classes\", of \"revolutionary socialist intellectuals\", such as were Karl Marx and Friedrich Engels.\n\nIn the formal codification of Leninism, the Hungarian Marxist philosopher, György Lukács (1885–1971) identified the intelligentsia as the privileged social class who provide revolutionary leadership. By means of intelligible and accessible interpretation, the intellectuals explain to the workers and peasants the \"Who?\", the \"How?\", and the \"Why?\" of the social, economic, and political \"status quo\"—the ideological totality of society—and its practical, revolutionary application to the transformation of their society.\n\nThe term public intellectual describes the intellectual participating in the public-affairs discourse of society, in addition to an academic career. Regardless of the academic field or the professional expertise, the public intellectual addresses and responds to the normative problems of society, and, as such, is expected to be an impartial critic who can \"rise above the partial preoccupation of one's own profession—and engage with the global issues of truth, judgment, and taste of the time.\" In \"Representations of the Intellectual\" (1994), In summarizing a quote by Edward Saïd, Jennings and Kemp-Welch state that the \"… true intellectual is, therefore, always an outsider, living in self-imposed exile, and on the margins of society\".\n\nAn intellectual usually is associated with an ideology or with a philosophy; e.g., the Third Way centrism of Anthony Giddens in the Labour Government of Tony Blair. The Czech intellectual Václav Havel said that politics and intellectuals can be linked, but that moral responsibility for the intellectual's ideas, even when advocated by a politician, remains with the intellectual. Therefore, it is best to avoid utopian intellectuals who offer 'universal insights' to resolve the problems of political economy with public policies that might harm and that have harmed civil society; that intellectuals be mindful of the social and cultural ties created with their words, insights, and ideas; and should be heard as social critics of politics and power.\n\nThe American academic Peter H. Smith describes the intellectuals of Latin America as people from an identifiable social class, who have been conditioned by that common experience, and thus are inclined to share a set of common assumptions (values and ethics); that ninety-four per cent of intellectuals come either from the middle class or from the upper class, and that only six per cent come from the working class. In \"The Intellectual\" (2005), philosopher Steven Fuller said that, because cultural capital confers power and social status, as a status group, they must be autonomous in order to be credible as intellectuals:\n\nThe political importance and effective consequence of Émile Zola in the Dreyfus affair (1894–1906) derived from his being a leading French thinker; thus, \"J'accuse\" (I Accuse), his open letter to the French government and the nation proved critical to achieving the exoneration of Captain Alfred Dreyfus of the false charges of treason, which were facilitated by institutional anti-Semitism, among other ideological defects of the French Establishment.\n\nIn journalism, the term \"intellectual\" usually connotes \"a university academic\" of the humanities—especially a philosopher—who addresses important social and political matters of the day. Hence, such an academic functions as a public intellectual who explains the theoretic bases of said problems and communicates possible answers to the policy makers and executive leaders of society. The sociologist Frank Furedi said that \"Intellectuals are not defined according to the jobs they do, but [by] the manner in which they act, the way they see themselves, and the [social and political] values that they uphold. Public intellectuals usually arise from the educated élite of a society; although the North American usage of the term \"intellectual\" includes the university academics. The difference between \"intellectual\" and \"academic\" is participation in the realm of public affairs.\n\nIn the matters of public policy, the public intellectual connects scholarly research to the practical matters of solving societal problems. The British sociologist Michael Burawoy, an exponent of public sociology, said that professional sociology has failed, by giving insufficient attention to resolving social problems, and that a dialogue between the academic and the layman would bridge the gap. An example is how Chilean intellectuals worked to reestablish democracy within the right-wing, neoliberal governments of the Military dictatorship of Chile (1973–90), the Pinochet régime allowed professional opportunities for some liberal and left-wing social scientists to work as politicians and as consultants in effort to realize the theoretical economics of the Chicago Boys, but their access to power was contingent upon political pragmatism, abandoning the political neutrality of the academic intellectual.\n\nIn \"The Sociological Imagination\" (1959), C. Wright Mills said that academics had become ill-equipped for participating in public discourse, and that journalists usually are \"more politically alert and knowledgeable than sociologists, economists, and especially ... political scientists\". That, because the universities of the U.S. are bureaucratic, private businesses, they \"do not teach critical reasoning to the student\", who then does not \"how to gauge what is going on in the general struggle for power in modern society\". Likewise, Richard Rorty criticized the participation of intellectuals in public discourse as an example of the \"civic irresponsibility of intellect, especially academic intellect\".\n\nThe American legal scholar Richard Posner said that the participation of academic public intellectuals in the public life of society is characterized by logically untidy and politically biased statements of the kind that would be unacceptable to academia. That there are few ideologically and politically independent public intellectuals, and disapproves that public intellectuals limit themselves to practical matters of public policy, and not with values or public philosophy, or public ethics, or public theology, not with matters of moral and spiritual outrage.\n\nIn \"An Interview with Milton Friedman\" (1974), the American libertarian economist Milton Friedman said that businessmen and the intellectuals are enemies of capitalism; the intellectuals, because most believed in socialism, while the businessman expected economic privileges:\n\nIn \"The Intellectuals and Socialism\" (1949), the British libertarian economist Friedrich Hayek, said that \"journalists, teachers, ministers, lecturers, publicists, radio commentators, writers of fiction, cartoonists, and artists\", are the intellectual social class whose function is to communicate the complex and specialized knowledge of the scientist to the general public. That, in the twentieth century, the intellectuals were attracted to socialism and to social democracy, because the socialists offered \"broad visions; the spacious comprehension of the social order, as a whole, which a planned system promises\" and that such broad-vision philosophies \"succeeded in inspiring the imagination of the intellectuals\" to change and improve their societies.\n\nAccording to Hayek, intellectuals disproportionately support socialism for idealistic and utopian reasons that cannot be realized in practical terms. Nonetheless, in the article \"Why Socialism?\" (1949), Albert Einstein said that the economy of the world is not private property because it is a \"planetary community of production and consumption\". In U.S. society, the intellectual status class are demographically characterized as people who hold liberal-to-leftist political perspectives about guns-or-butter fiscal policy.\n\nIn \"The Heartless Lovers of Humankind\" (1987), the journalist and popular historian Paul Johnson said:\nThe public- and private-knowledge dichotomy originated in Ancient Greece, from Socrates's rejection of the Sophist concept that the pursuit of knowledge (truth) is a \"public market of ideas\", open to all men of the city, not only to philosophers. In contradiction to the Sophist's public market of knowledge, Socrates proposed a knowledge monopoly for and by the philosophers; thus, \"those who sought a more penetrating and rigorous intellectual life rejected, and withdrew from, the general culture of the city, in order to embrace a new model of professionalism\"; the private market of ideas.\n\nIn the 19th century, addressing the societal place, roles, and functions of intellectuals in American society, the Congregational theologian Edwards Amasa Park said, \"We do wrong to our own minds, when we carry out scientific difficulties down to the arena of popular dissension\". That for the stability of society (social, economic, political) it is necessary \"to separate the serious, technical role of professionals from their responsibility [for] supplying usable philosophies for the general public\"; thus operated Socrate's cultural dichotomy of public-knowledge and private-knowledge, of \"civic culture\" and \"professional culture\", the social constructs that describe and establish the intellectual sphere of life as separate and apart from the civic sphere of life.\n\nThe American historian Norman Stone said that the intellectual social class misunderstand the reality of society and so are doomed to the errors of logical fallacy, ideological stupidity, and poor planning hampered by ideology. In her memoirs, the Conservative politician Margaret Thatcher said that the anti-monarchical French Revolution (1789–1799) was \"a utopian attempt to overthrow a traditional order ... in the name of abstract ideas, formulated by vain intellectuals\". Yet, as Prime Minister, Thatcher asked Britain's academics to help her government resolve the social problems of British society—whilst she retained the populist opinion of \"The Intellectual\" as being a man of un-British character, a thinker, not a doer; Thatcher's anti-intellectualist perspective was shared by the mass media, especially \"The Spectator\" and \"The Sunday Telegraph\" newspapers, whose reportage documented a \"lack of intellectuals\" in Britain.\n\nIn his essay \"Why do intellectuals oppose capitalism?\" (1998), libertarian philosopher Robert Nozick of the Cato Institute argued that intellectuals become embittered leftists because their academic skills, much rewarded at school and at university, are under-valued and under-paid in the capitalist market economy; so, the intellectuals turned against capitalism—despite enjoying a more economically and financially comfortable life in a capitalist society than they might enjoy in either a socialist or a communist society.\n\nIn post-Communist Europe, the social attitude perception of the intelligentsia became anti-intellectual; in the Netherlands, the word \"intellectual\" negatively connotes an overeducated person of \"unrealistic visions of the World\". In Hungary, the intellectual is perceived as an \"egghead\", a person who is \"too-clever\" for the good of society. In the Czech Republic, the intellectual is a cerebral person, aloof from reality. Such derogatory connotations of \"intellectual\" are not definitive, because, in the \"case of English usage, positive, neutral, and pejorative uses can easily coexist\"; the example is Václav Havel who, \"to many outside observers, [became] a favoured instance of The Intellectual as National Icon\" in the early history of the post-Communist Czech Republic.\n\nIn the book, \"Intellectuals and Society\" (2010), the economist Thomas Sowell said that, lacking disincentives in professional life, the intellectual (producer of knowledge, not material goods) tends to speak outside his or her area of expertise, and expects social and professional benefits from the halo effect, derived from possessing professional expertise. That, in relation to other professions, the public intellectual is socially detached from the negative and unintended consequences of public policy derived from his or her ideas. As such, the philosopher and mathematician Bertrand Russell (1872–1970) advised the British government against national rearmament in the years before World War I (1914–1918), while the German Empire prepared for war. Yet, the post-war intellectual reputation of Bertrand Russell remained almost immaculate and his opinions respected by the general public because of the halo effect.\n\n\n"}
{"id": "37709760", "url": "https://en.wikipedia.org/wiki?curid=37709760", "title": "Law of the Czech Republic", "text": "Law of the Czech Republic\n\nCzech law, often referred to as the legal order of the Czech Republic (\"\"), is the system of legal rules in force in the Czech Republic, and in the international community it is a member of. Czech legal system belongs to the Germanic branch of continental legal culture (civil law). Major areas of public and private law are divided into branches, among them civil, criminal, administrative, procedural and labour law, and systematically codified.\n\nWritten law is the basis of the legal order, and the most important source of law are: legal regulations (acts of parliament, as well as delegated legislation), international treaties (once they have been ratified by the parliament and promulgated), and such findings of the Constitutional Court of the Czech Republic, in which a statute or its part has been nullified as unconstitutional. It is made public by the periodically published ', abbreviated \"Sb.\" (“Collection of Law”, “Coll.”), and ', abbreviated \"Sb. m. s.\" (“Collection of International Treaties”).\n\nThe system of law and justice in the Czech Republic has been in constant development since the 1989 regime change. In 1993, the Constitution of the Czech Republic has been enacted, which postulates the rule of law, outlines the structure and principles of democratic government, and declares human rights and rights of the citizen. Since 2004, the membership in the EU means the priority of European Union law over Czech law in some areas. Recently, a brand new Criminal Code entered into force in 2010, and the Civil Code followed in 2014.\n\nSources of Czech law are (in this hierarchical order):\n\n\nActs of parliament and other legal regulations enter into force on the day they are promulgated (published) in the official \"Collection of Law\" (', abbreviated as ' – \"Coll.\" – when referring to statutes), although they may take effect at a later date. International treaties are similarly published in the \"Collection of International Treaties\" (\"\", abbreviated \"Sb. m. s.\").\n\nCzech constitution is written, and it consists of several constitutional acts (one of them the Constitution of the Czech Republic), together they are known as the constitutional order of the Czech Republic (\"\").\n\nThe constitution can be viewed as entrenched, because constitutional statutes are more difficult to adopt, amend, supplement or repeal them than ordinary laws of the country. A special majority (constitutional supermajority) is required of three-fifths of all Deputies and a qualified majority of three-fifths of all Senators present. This is to promote continuity and stability of the political system.\n\nMost important constitutional acts are:\n\n\nThe 1992 Constitution of the Czech Republic declares sovereignty (self-government) of the people and the values of freedom and democracy. It defines the separation of the three powers in a system of Checks and balances. It gives the legislative power to the popularly elected Czech Parliament consisting of two chambers, the Chamber of Deputies and the Senate. The executive power is divided between the President and the Prime Minister. It describes the functioning of the judiciary, especially the Constitutional Court. Two more institutions are established, the Czech National Bank and the Supreme Audit Office.\n\nThe Charter of Fundamental Rights and Basic Freedoms (\"\") is a bill of rights document enacted in 1991 by the Czechoslovak Federative Republic. In the Czech Republic it was kept in its entirety and forms a part of the constitutional order (i.e. has the same legal force as the Constitution). It postulates the sanctity of natural human rights and fundamental freedoms as well as citizens' (political) rights, the equality before the law, rights of minorities and so on.\n\nThe Constitution has been seriously modified in February 2012, introducing a controversial popular vote of the President of the Czech Republic:\n\n\nOn joining the European Union, the Czech Republic committed itself to respect the principle of the supremacy of European law over Czech law in defined areas.\n\nThe most important treaty in this category is the Council of Europe’s Convention for the Protection of Human Rights and Fundamental Freedoms.\n\nMajor areas of Czech law are codified in a systematic manner.\n\nMost important statutes in the criminal justice field are:\n\n\nSince 2010, Czech criminal law is regulated mainly, though not only, by the Criminal Code Act 2009 ('), which codifies substantive criminal law. The comprehensive regulatory statute for criminal procedure is the Criminal Proceedings Code Act 1961 ('). The two codes are complemented by the Juvenile Criminal Justice Act 2003, which deals with alleged offenders aged between 15 (the threshold of criminal liability as well as age of legal consent) and 17.\n\nUntil 2009, the criminal code statute in force dated back to 1961. In spite of numerous amendments made since the 1989, it still conformed to the communist ideology, oriented on punishment of perceived enemies of the socialist regime. As a part of an effort to comprehensively reform law and justice, a brand new codification has been written. The Ministry of Justice announced it as an \"elaborate piece of work boiled down from the experience and knowledge of foremost expert on criminal law\", which can be proudly compared to modern criminal codes of other democratic countries.\n\nThe purpose of the 2009 codification was to bring the criminal justice system in line with the rest of Europe. The main change introduced by the code was transition from material to formal conception of criminal offence. Another thing the lawyers drafting the 2009 code had in mind, and expressed it in the area of sentencing tariffs, was a new balance of punishment against rehabilitation effect on the criminal. On one hand we see the sentences for felony (') which carry a sentence of at least 5 years. On the other a newly created category of misdemeanour (') introduces a host of community sentences ('), including community payback (') or house arrest (\"\"). The code also outlaws doping.\n\nLess serious breaches of law are called contraventions (\"\"). These are not normally dealt with by the courts of law, but rather punished either by the police on the spot, or through a procedure in front of a committee at the local or municipal authority, or sometimes other administration body. The result is not a conviction, and so such a punishment will not blemish the culprit's criminal record. Like with other administrative acts, the outcome of this procedure can be challenged through the administrative justice process. A typical example of contraventions in Czech law this are driving offences. (In the common law world, acts falling into this category would be classed as summary offences.)\n\nSince 2012, Czech law also recognizes criminal liability of corporations for offences committed by employees of a corporation, when acting on its behalf, being instructed by the board of directors or other representative of the company, or through lack of their action. In such cases, not only individuals will be prosecuted, but the corporation can be prosecuted as well for the same offence and punished by a host of measures, starting from cetrain injunctions, through fines a forfeitures, to liquidation of the company.\n\n\nMain statutes regulating this area are:\n\n\nThe Civil Code codifies core areas of private law. It has five parts. The first part is dedicated to a legal status of a person as an individual. The second regulates family law – e.g. the institute of marriage and the rights and obligations of husband and wife, parents and children. Although the Code does include registered partnership, it explicitly prohibits adoption to a person in a registered partnership. The rest in concerned mainly with property rights.\n\nThe civil code regulation effective since 1 January 2014 is a result of eleven years of work of the recodification committee of the Ministry of Justice. Together with the Commercial Corporations Act and the Private International Law Act it constitutes a complete recodification of private law in the Czech Republic. It brings new and modern regulation of relations governed by civil law, with emphasis on personality rights, free will / more choice when writing a will or unified regulation of obligation laws.\n\nTheoretically, case law is not defined as a source of law in the Czech Republic. Despite that, the decisions of courts, namely supreme courts and the Constitutional Court, have a significant influence over the Czech legal system since 1989. Findings of the Constitutional Court are considered a source of law, and are binding for general courts. If a lower court is to rule against \"consistent adjudications\" of the Supreme Court on the point of law, it must give detailed reasoning, and this can often be a reason for a successful appeal. This system is largely based on judiciary's own interpretation of the Constitution, arguing that it gives citizens predictability and fairness. The most significant judgements of both courts are published in the official Collection of Laws.\n\nIn Czech lands, the process of formation of modern legislation dates back to the era of enlightened absolutism, when they were a core part of the Austrian Empire. Reforms of government were to a large extent the work of empress Maria Theresa and her son Joseph II, who participated in creating the first civil code of the country in 1787, called Josephinisches Gesetzbuch. This was a forbearer to the comprehensive codification of civil law in 1811, known as the Allgemeines bürgerliches Gesetzbuch (ABGB). ABGB was received in 1918 by Czechoslovakia, among other successor states, but only for Czech lands (Bohemia, Moravia and Austrian Silesia), while Slovakia kept the customary law of Hungary. Albeit updated many times, this Austrian law was kept in Czechoslovakia, along with the legal duality, until 1950, when the \"Middle Civil Code\" was promulgated, soon to be superseded by the civil code of 1964, which will be in force until the \"New Civil Code\", enacted in 2009, will enter in force in 2014.\n\nFollowing in the tracks of their enlightenment predecessors, the Constitutio criminalis Josephina of 1707 and the Constitutio criminalis Theresiana of 1768, the most important Czechoslovakian criminal codes were enacted in 1950, 1961 and 2009.\n\nThe history of Czechoslovak constitutionality starts with the formation of independent Czechoslovakia out of the ruins of Austria-Hungary. In 1918, the Interim Constitution of Czechoslovakia has been enacted hastily, establishing the republic with its president and temporary parliament. The Czechoslovak Constitution of 1920 succeeded it, inspired by western democratic constitutions, and, controversially, postulating the Czechoslovak nation. This lasted over the First Republic and the second world war. In 1948, the so-called Ninth-of-May Constitution was enacted after the communist coup – preserving some democratic institutions (human rights, partial division of the three powers, independent judiciary), though the political reality of the country departed from it radically. The 1960 Constitution of Czechoslovakia, influenced by soviet constitutions, is often dubbed the \"Socialist Constitution\". It changed the name of the country to Czechoslovak \"Socialist\" Republic and defined socialist, rather than democratic, character of the state, and introduced the leading role of the Communist Party of Czechoslovakia. The 1960 constitution remained in force until 1992, although in 1968 it was substantially modified by the Constitutional Act on the Czechoslovak Federation – this transformed the unitary state into a federation of the Czech Socialist Republic and the Slovak Socialist Republic. Further radical modifications were enacted after the regime change of 1989, but they turned out to be short-lived, as in 1992 the Constitution of the Czech Republic has been promulgated after the division of Czechoslovakia, and entered into force in 1993.\n\n\n"}
{"id": "11347178", "url": "https://en.wikipedia.org/wiki?curid=11347178", "title": "List of avant-garde artists", "text": "List of avant-garde artists\n\nAvant-garde () is French for \"vanguard\". The term is commonly used in French, English, and German to refer to people or works that are experimental or innovative, particularly with respect to art and culture.\n\nAvant-garde represents a pushing of the boundaries of what is accepted as the norm or the status quo, primarily in the cultural realm. The notion of the existence of the avant-garde is considered by some to be a hallmark of modernism, as distinct from postmodernism. Postmodernism posits that the age of the constant pushing of boundaries is no longer with us and that avant-garde has little to no applicability in the age of Postmodern art.\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "10266438", "url": "https://en.wikipedia.org/wiki?curid=10266438", "title": "MERODE", "text": "MERODE\n\nMERODE is an Object Oriented Enterprise Modeling method developed at KU Leuven (Belgium). Its name is the abbreviation of Model driven, Existence dependency Relation, Object oriented DEvelopment. MERODE is a method for creating domain models (also called conceptual models) as basis for building information systems making use of two prominent UML diagramming techniques - class diagram and state diagrams. Starting from a high-level PIM (close to a Computational Independent Model (CIM)) allows removing or hiding details irrelevant for a conceptual modelling view which makes the approach easier to understand. \nThe method is grounded in process algebra, which enables mathematical reasoning on models. Thanks to this, models can be checked for internal consistency and mutual completeness, i.e. inter/intra model consistency and syntactical quality. The automated reasoning (\"consistency by construction\") also caters for autocomplete functionality, which allows creating correct models faster.\n\nA typical MERODE analysis or conceptualisation consists of three views or diagrams: a so-called existence dependency graph (EDG) similar to a UML class diagram, a proprietary concept namely an object event table (OET) and a group of finite state machines.\n\nMERODE fosters a model-driven engineering approach to software development. It targets platform independent domain models that are sufficiently complete for execution, i.e. transformation to platform-specific models and to code. In order to achieve automated transformation of models, MERODE limits the use of UML to a number of well-defined constructs with clear semantics and complements this with the notion of \"existence dependency\" and a proprietary approach to object interaction modelling.\n\nMERODE-models can be created with the opensource case tool JMermaid. The tool also allows checking the models for consistency and readiness for transformation.\n\nA companion code generator allows to generate a fully working prototype. One-click prototype production lowers the required skill-set for its useful application. By embedding the models into the application, the behaviour of the prototype can be traced back to the models, i.e. making it possible to validate the semantic quality of models. MERODE prototypes are augmented with feedback (textual and graphical) that links the test results to their causes in the model.\n\n"}
{"id": "182727", "url": "https://en.wikipedia.org/wiki?curid=182727", "title": "Mach's principle", "text": "Mach's principle\n\nIn theoretical physics, particularly in discussions of , Mach's principle (or Mach's conjecture) is the name given by Einstein to an imprecise hypothesis often credited to the physicist and philosopher Ernst Mach. The idea is that the existence of absolute rotation (the distinction of local inertial frames vs. rotating reference frames) is determined by the large-scale distribution of matter, as exemplified by this anecdote:\n\nYou are standing in a field looking at the stars. Your arms are resting freely at your side, and you see that the distant stars are not moving. Now start spinning. The stars are whirling around you and your arms are pulled away from your body. Why should your arms be pulled away when the stars are whirling? Why should they be dangling freely when the stars don't move?\nMach's principle says that this is not a coincidence—that there is a physical law that relates the motion of the distant stars to the local inertial frame. If you see all the stars whirling around you, Mach suggests that there is some physical law which would make it so you would feel a centrifugal force. There are a number of rival formulations of the principle. It is often stated in vague ways, like \"mass out there influences inertia here\". A very general statement of Mach's principle is \"local physical laws are determined by the large-scale structure of the universe\".\n\nThis concept was a guiding factor in Einstein's development of the general theory of relativity. Einstein realized that the overall distribution of matter would determine the metric tensor, which tells you which frame is rotationally stationary. Frame-dragging and conservation of gravitational angular momentum makes this into a true statement in the general theory in certain solutions. But because the principle is so vague, many distinct statements can be (and have been) made that would qualify as a Mach principle, and some of these are false. The Gödel rotating universe is a solution of the field equations that is designed to disobey Mach's principle in the worst possible way. In this example, the distant stars seem to be revolving faster and faster as one moves further away. This example doesn't completely settle the question, because it has closed timelike curves.\n\nThe basic idea also appears before Mach's time, in the writings of George Berkeley. The book \"Absolute or Relative Motion?\" (1896) by Benedict Friedländer and his brother Immanuel contained ideas similar to Mach's principle.\n\nThere is a fundamental issue in relativity theory. If all motion is relative, how can we measure the inertia of a body? We must measure the inertia with respect to something else. But what if we imagine a particle completely on its own in the universe? We might hope to still have some notion of its state of motion. Mach's principle is sometimes interpreted as the statement that such a particle's state of motion has no meaning in that case.\n\nIn Mach's words, the principle is embodied as follows:\n\nAlbert Einstein seemed to view Mach's principle as something along the lines of:\n\nIn this sense, at least some of Mach's principles are related to philosophical holism. Mach's suggestion can be taken as the injunction that gravitation theories should be relational theories. Einstein brought the principle into mainstream physics while working on general relativity. Indeed, it was Einstein who first coined the phrase \"Mach's principle\". There is much debate as to whether Mach really intended to suggest a new physical law since he never states it explicitly.\n\nThe writing in which Einstein found inspiration from Mach was \"The Science of Mechanics\", where the philosopher criticized Newton's idea of absolute space, in particular the argument that Newton gave sustaining the existence of an advantaged reference system: what is commonly called \"Newton's bucket argument\".\n\nIn his \"Philosophiae Naturalis Principia Mathematica\", Newton tried to demonstrate that:\n\nMach, in his book, says that:\n\nThis same thought had been expressed by the philosopher George Berkeley in his \"De Motu\". It is then not clear, in the passages from Mach just mentioned, if the philosopher intended to formulate a new kind of physical action between heavy bodies. This physical mechanism should determine the inertia of bodies, in a way that the heavy and distant bodies of our universe should contribute the most to the inertial forces. More likely, Mach only suggested a mere \"redescription of motion in space as experiences that do not invoke the term \"space\"\". What is certain is that Einstein interpreted Mach's passage in the former way, originating a long-lasting debate.\n\nMost physicists believe Mach's principle was never developed into a quantitative physical theory that would explain a mechanism by which the stars can have such an effect. It was never made clear by Mach himself exactly what his principle was. Although Einstein was intrigued and inspired by Mach's principle, Einstein's formulation of the principle is not a fundamental assumption of general relativity.\n\nBecause intuitive notions of distance and time no longer apply, what exactly is meant by \"Mach's principle\" in general relativity is even less clear than in Newtonian physics and at least 21 formulations of Mach's principle are possible, some being considered more strongly Machian than others. A relatively weak formulation is the assertion that the motion of matter in one place should affect which frames are inertial in another.\n\nEinstein, before completing his development of the general theory of relativity, found an effect which he interpreted as being evidence of Mach's principle. We assume a fixed background for conceptual simplicity, construct a large spherical shell of mass, and set it spinning in that background. The reference frame in the interior of this shell will precess with respect to the fixed background. This effect is known as the Lense–Thirring effect. Einstein was so satisfied with this manifestation of Mach's principle that he wrote a letter to Mach expressing this:\nThe Lense–Thirring effect certainly satisfies the very basic and broad notion that \"matter there influences inertia here\". The plane of the pendulum would not be dragged around if the shell of matter were not present, or if it were not spinning. As for the statement that \"inertia originates in a kind of interaction between bodies\", this too could be interpreted as true in the context of the effect.\n\nMore fundamental to the problem, however, is the very existence of a fixed background, which Einstein describes as \"the fixed stars\". Modern relativists see the imprints of Mach's principle in the initial-value problem. Essentially, we humans seem to wish to separate spacetime into slices of constant time. When we do this, Einstein's equations can be decomposed into one set of equations, which must be satisfied on each slice, and another set, which describe how to move between slices. The equations for an individual slice are elliptic partial differential equations. In general, this means that only part of the geometry of the slice can be given by the scientist, while the geometry everywhere else will then be dictated by Einstein's equations on the slice.\n\nIn the context of an asymptotically flat spacetime, the boundary conditions are given at infinity. Heuristically, the boundary conditions for an asymptotically flat universe define a frame with respect to which inertia has meaning. By performing a Lorentz transformation on the distant universe, of course, this inertia can also be transformed.\n\nA stronger form of Mach's principle applies in Wheeler–Mach–Einstein spacetimes, which require spacetime to be spatially compact and globally hyperbolic. In such universes Mach's principle can be stated as \"the distribution of matter and field energy-momentum (and possibly other information) at a particular moment in the universe determines the inertial frame at each point in the universe\" (where \"a particular moment in the universe\" refers to a chosen Cauchy surface).\n\nThere have been other attempts to formulate a theory that is more fully Machian, such as the Brans–Dicke theory and the Hoyle–Narlikar theory of gravity, but most physicists argue that none have been fully successful. At an exit poll of experts, held in Tübingen in 1993, when asked the question \"Is general relativity perfectly Machian?\", 3 respondents replied \"yes\", and 22 replied \"no\". To the question \"Is general relativity with appropriate boundary conditions of closure of some kind very Machian?\" the result was 14 \"yes\" and 7 \"no\".\n\nHowever, Einstein was convinced that a valid theory of gravity would necessarily have to include the relativity of inertia:\nThe broad notion that \"mass there influences inertia here\" has been expressed in several forms.\nHermann Bondi and Joseph Samuel have listed eleven distinct statements that can be called Mach principles, labelled by \"Mach0\" through \"Mach10\". Though their list is not necessarily exhaustive, it does give a flavor for the variety possible.\n\n\n\n"}
{"id": "659632", "url": "https://en.wikipedia.org/wiki?curid=659632", "title": "McKenzie friend", "text": "McKenzie friend\n\nA McKenzie friend assists a litigant in person in a court of law in England and Wales, Northern Ireland, the Republic of Ireland and Australia. This person does not need to be legally qualified. The crucial point is that litigants in person (known as \"party litigants\" in Scotland) are entitled to have assistance, lay or professional, unless there are exceptional circumstances.\n\nTheir role was set out most clearly in the eponymous 1970 case \"McKenzie v McKenzie\". Although in many cases a McKenzie friend may be an actual friend, it is often somebody with knowledge of the area, and the trend is heavily in favour of admitting McKenzie friends. He or she may be liable for any misleading advice given to the litigant in person but is not covered by professional indemnity insurance.\n\nThe role is distinct from that of a next friend or of an \"amicus curiae\". A similar, modified principle exists in Singapore.\n\n\"McKenzie v. McKenzie\" was a divorce case in England. Levine McKenzie, who was petitioning for divorce, had been legally aided but the legal aid had been withdrawn before the case went to court. Unable to fund legal representation, McKenzie had broken off contact with his solicitors, Geoffrey Gordon & Co.. However, one day before the hearing, Geoffrey Gordon & Co. sent the case to an Australian barrister in London, Ian Hanger, whose qualifications in law in Australia did not allow him to practise as a barrister in London. Hanger hoped to sit with his client to prompt him, take notes and suggest questions in cross-examination, thereby providing what quiet assistance he could from the bar table to a man representing himself. The trial judge ordered Hanger not to take any active part in the case (except to advise McKenzie during adjournments) and to sit in the public gallery of the court. Hanger assumed his limited role was futile and did not return for the second day of the trial.\n\nThe case went against McKenzie, who then appealed to the Court of Appeal on the basis that he had been denied representation. On 12 June 1970, the Court of Appeal ruled that the judge's intervention had deprived McKenzie of the assistance to which he was entitled and ordered a retrial.\n\nIan Hanger AM QC, the original McKenzie friend, is now a Queen's Counsel (QC) at the Queensland Bar.\n\nIn September 2006, the Subordinate Courts of Singapore started a pilot project called the Lay Assistant Scheme in which persons, usually with some legal knowledge, attend hearings with litigants who are not represented by lawyers to advise them on non-legal issues and help them with administrative tasks. The scheme, a modification of the U.K.'s McKenzie friend system, is intended to assist litigants who are not eligible for legal aid as they have an annual salary exceeding S$10,000 but cannot afford a lawyer. For the litigant to qualify, the other party must be legally represented.\n\nLay assistants are not allowed to act as lawyers and may not address the court; any breach of court rules may render them liable to a maximum fine of $1,000 or imprisonment of up to six months.\n\nPlans for introducing McKenzie friends in court proceedings were first announced by Chief Justice Chan Sek Keong in May 2006. Students from the Pro Bono Group of the Faculty of Law, National University of Singapore, have been participating in the scheme.\n\nIn English courts, where a case is being heard in private, the use of a McKenzie friend has sometimes been contentious. This is a particular problem in family court hearings, where it has been held that the nature of the case is so confidential that no one other than the litigants and their professional legal representatives should be admitted to the court.\n\nA 2005 Court of Appeal case, \"In the matter of the children of Mr O'Connell, Mr Whelan and Mr Watson\", clarified the law in this area. The result of the appeal has legitimised the use of McKenzie friends in the family court and allowed the litigant to disclose confidential court papers to the McKenzie friend.\n\nEngland and Wales allow fee charging McKenzie friends, who may charge for their services, including the giving of legal advice. A recent report by the Legal Service Consumer Panel found that fee charging McKenzie friends were a net benefit. The report stated, \"They should be viewed as providing valuable support that improves access to justice in the large majority of cases.\"\n\nScotland has a separate legal system from England and Wales. The first recorded use of McKenzie friends in Scotland was in April 2007 in the case \"TB & AM v. The Authority Reporter\". The one major difference between England and Scotland is that in Scotland McKenzie friends are not permitted to receive remuneration.\n\n"}
{"id": "146263", "url": "https://en.wikipedia.org/wiki?curid=146263", "title": "Monotone convergence theorem", "text": "Monotone convergence theorem\n\nIn the mathematical field of real analysis, the monotone convergence theorem is any of a number of related theorems proving the convergence of monotonic sequences (sequences that are increasing or decreasing) that are also bounded. Informally, the theorems state that if a sequence is increasing and bounded above by a supremum, then the sequence will converge to the supremum; in the same way, if a sequence is decreasing and is bounded below by an infimum, it will converge to the infimum.\n\nIf a sequence of real numbers is increasing and bounded above, then its supremum is the limit.\n\nLet formula_1 be such a sequence. By assumption, formula_1 is non-empty and bounded above. By the least-upper-bound property of real numbers, formula_3 exists and is finite. Now, for every formula_4, there exists formula_5 such that formula_6, since otherwise formula_7 is an upper bound of formula_1, which contradicts to the definition of formula_9. Then since formula_1 is increasing, and formula_9 is its upper bound, for every formula_12, we have formula_13. Hence, by definition, the limit of formula_1 is formula_15\n\nIf a sequence of real numbers is decreasing and bounded below, then its infimum is the limit.\n\nThe proof is similar to the proof for the case when the sequence is increasing and bounded above,\n\nIf formula_1 is a monotone sequence of real numbers (i.e., if \"a\" ≤ \"a\" for every \"n\" ≥ 1 or \"a\" ≥ \"a\" for every \"n\" ≥ 1), then this sequence has a finite limit if and only if the sequence is bounded.\n\n\nIf for all natural numbers \"j\" and \"k\", \"a\" is a non-negative real number and \"a\" ≤ \"a\", then\n\nThe theorem states that if you have an infinite matrix of non-negative real numbers such that\nthen the limit of the sums of the rows is equal to the sum of the series whose term \"k\" is given by the limit of column \"k\" (which is also its supremum). The series has a convergent sum if and only if the (weakly increasing) sequence of row sums is bounded and therefore convergent.\n\nAs an example, consider the infinite series of rows\n\nwhere \"n\" approaches infinity (the limit of this series is e). Here the matrix entry in row \"n\" and column \"k\" is\n\nthe columns (fixed \"k\") are indeed weakly increasing with \"n\" and bounded (by 1/\"k\"!), while the rows only have finitely many nonzero terms, so condition 2 is satisfied; the theorem now says that you can compute the limit of the row sums formula_22 by taking the sum of the column limits, namely formula_23.\n\nThe following result is due to Beppo Levi and Henri Lebesgue. In what follows, formula_24 denotes the formula_25-algebra of Borel sets on formula_26. By definition, formula_24 contains the set formula_28 and all Borel subsets of formula_29\n\nLet formula_30 be a measure space, and formula_31. Consider a pointwise non-decreasing sequence formula_32 of formula_33-measurable non-negative functions formula_34, i.e., for every formula_35 and every formula_36,\n\nSet the pointwise limit of the sequence formula_38 to be formula_39. That is, for every formula_40,\n\nThen formula_39 is formula_33-measurable and\n\nRemark 1. The integrals may be finite or infinite.\n\nRemark 2. The theorem remains true if its assumptions hold formula_45-almost everywhere. In other words, it is enough that there is a null set formula_5 such that the sequence formula_47 non-decreases for every formula_48 To see why this is true, we start with an observation that allowing the sequence formula_49 to pointwise non-decrease almost everywhere causes its pointwise limit formula_39 to be undefined on some null set formula_5. On that null set, formula_39 may then be defined arbitrarily, e.g. as zero, or in any other way that preserves measurability. To see why this will not affect the outcome of the theorem, note that since formula_53 we have, for every formula_54\n\nprovided that formula_39 is formula_33-measurable. (These equalities follow directly from the definition of Lebesgue integral for a non-negative function).\n\nRemark 3. Under assumptions of the theorem,\n(Note that the second chain of equalities follows from Remark 5).\n\nRemark 4. The proof below does not use any properties of Lebesgue integral except those established here. The theorem, thus, can be used to prove other basic properties, such as linearity, pertaining to Lebesgue integration.\n\nRemark 5 (monotonicity of Lebesgue integral). In the proof below, we apply the monotonic property of Lebesgue integral to non-negative functions only. Specifically (see Remark 4), let the functions formula_59 be formula_33-measurable.\n\n\n\nProof. Denote formula_67 the set of simple formula_68-measurable functions formula_69 such that\nformula_70 everywhere on formula_71 \n\n1. Since formula_72 we have\n\nBy definition of Lebesgue integral and the properties of supremum,\n\n2. Let formula_75 be the indicator function of the set formula_76 It can be deduced from the definition of Lebesgue integral that\n\nif we notice that, for every formula_78 formula_79 outside of formula_76 Combined with the previous property, the inequality formula_81 implies\n\nThis proof does \"not\" rely on Fatou's lemma. However, we do explain how that lemma might be used.\n\nFor those not interested in independent proof, the intermediate results below may be skipped.\nLemma 1. Let formula_30 be a measurable space. Consider a simple formula_33-measurable non-negative function formula_85. For a subset formula_86, define\nThen formula_88 is a measure on formula_89.\n\nMonotonicity follows from Remark 5. Here, we will only prove countable additivity, leaving the rest up to the reader. Let formula_90, where all the sets formula_91 are pairwise disjoint. Due to simplicity,\nfor some finite non-negative constants formula_93 and pairwise disjoint sets formula_94 such that formula_95. By definition of Lebesgue integral,\nSince all the sets formula_97 are pairwise disjoint, the countable additivity of formula_45\ngives us\nSince all the summands are non-negative, the sum of the series, whether this sum is finite or infinite, cannot change if summation order does. For that reason,\nas required.\n\nThe following property is a direct consequence of the definition of measure.\n\nLemma 2. Let formula_45 be a measure, and formula_102, where\nis a non-decreasing chain with all its sets formula_45-measurable. Then\n\nStep 1. We begin by showing that formula_39 is formula_107–measurable.\n\nNote. If we were using Fatou's lemma, the measurability would follow easily from Remark 3(a).\n\nTo do this \"without\" using Fatou's lemma, it is sufficient to show that the inverse image of an interval formula_108 under formula_39 is an element of the sigma-algebra formula_110 on formula_111, because (closed) intervals generate the Borel sigma algebra on the reals. Since formula_108 is a closed interval, and, for every formula_113, formula_114,\n\nThus,\n\nBeing the inverse image of a Borel set under a formula_33-measurable function formula_118, each set in the countable intersection is an element of formula_110. Since formula_25-algebras are, by definition, closed under countable intersections, this shows that formula_39 is formula_33-measurable, and the integral formula_123 is well defined (and possibly infinite).\n\nStep 2. We will first show that formula_124\n\nThe definition of formula_39 and monotonicity of formula_126 imply that formula_127, for every formula_113 and every formula_40. By monotonicity (or, more precisely, its narrower version established in Remark 5; see also Remark 4) of Lebesgue integral,\nand\nNote that the limit on the right exists (finite or infinite) because, due to monotonicity (see Remark 5 and Remark 4), the sequence is non-decreasing.\n\nEnd of Step 2.\n\nWe now prove the reverse inequality. We seek to show that\n\nProof using Fatou's lemma. Per Remark 3, the inequality we want to prove is equivalent to\nBut the latter follows immediately from Fatou's lemma, and the proof is complete.\n\nIndependent proof. To prove the inequality \"without\" using Fatou's lemma, we need some extra machinery. Denote formula_134 the set of simple formula_33-measurable functions formula_69 such that\nformula_137 on formula_111.\n\nStep 3. Given a simple function formula_139 and a real number formula_140, define\nThen formula_142, formula_143, and formula_144.\n\nStep 3a. To prove the first claim, let formula_145, for some finite collection of pairwise disjoint measurable sets formula_94 such that formula_147, some (finite) non-negative constants formula_148, and formula_149 denoting the indicator function of the set formula_150.\n\nFor every formula_151 formula_152 holds if and only if formula_153 Given that the sets formula_150 are pairwise disjoint,\n\nSince the pre-image formula_156 of the Borel set\nformula_157 under the measurable function formula_118 is measurable, and formula_25-algebras, by definition, are closed under finite intersection and unions, the first claim follows.\n\nStep 3b. To prove the second claim, note that, for each formula_113 and every formula_40, formula_162\n\nStep 3c. To prove the third claim, we show that formula_163.\n\nIndeed, if, to the contrary, formula_164, then an element\nexists such that formula_166, for every formula_113. Taking the limit as formula_168, we get\n\nStep 4. For every simple formula_33-measurable non-negative function formula_171,\nTo prove this, define formula_173. By Lemma 1, formula_174 is a measure on formula_89. By \"continuity from below\" (Lemma 2),\nas required.\n\nStep 5. We now prove that, for every formula_139,\n\nIndeed, using the definition of formula_179, the non-negativity of formula_118, and the monotonicity of Lebesgue integral (see Remark 5 and Remark 4), we have\nfor every formula_182. In accordance with Step 4, as formula_168, the inequality becomes\nTaking the limit as formula_185 yields\nas required.\n\nStep 6. We are now able to prove the reverse inequality, i.e.\n\nIndeed, by non-negativity, formula_188 and formula_189 For the calculation below, the non-negativity of formula_39 is essential. Applying the definition of Lebesgue integral and the inequality established in Step 5, we have\nThe proof is complete.\n\n"}
{"id": "1619306", "url": "https://en.wikipedia.org/wiki?curid=1619306", "title": "Multisensory integration", "text": "Multisensory integration\n\nMultisensory integration, also known as multimodal integration, is the study of how information from the different sensory modalities, such as sight, sound, touch, smell, self-motion and taste, may be integrated by the nervous system. A coherent representation of objects combining modalities enables us to have meaningful perceptual experiences. Indeed, multisensory integration is central to adaptive behavior because it allows us to perceive a world of coherent perceptual entities. Multisensory integration also deals with how different sensory modalities interact with one another and alter each other's processing.\n\nMultimodal perception is a scientific term that describes how humans form coherent, valid, and robust perception by processing sensory stimuli from various modalities. Surrounded by multiple objects and receiving multiple sensory stimulations, the brain is faced with the decision of how to categorize the stimuli resulting from different objects or events in the physical world. The nervous system is thus responsible for whether to integrate or segregate certain groups of temporally coincident sensory signals based on the degree of spatial and structural congruence of those stimulations. Multimodal perception has been widely studied in cognitive science, behavioral science, and neuroscience.\n\nThere are four attributes of stimulus: modality, intensity, location, and duration. The neocortex in the mammalian brain has parcellations that primarily process sensory input from one modality. For example, primary visual area, V1, or primary somatosensory area, S1. These areas mostly deal with low-level stimulus features such as brightness, orientation, intensity, etc. These areas have extensive connections to each other as well as to higher association areas that further process the stimuli and are believed to integrate sensory input from various modalities. However, recently multisensory effects have been shown to occur in primary sensory areas as well.\n\nThe relationship between the binding problem and multisensory perception can be thought of as a question – the binding problem, and potential solution – multisensory perception. The binding problem stemmed from unanswered questions about how mammals (particularly higher primates) generate a unified, coherent perception of their surroundings from the cacophony of electromagnetic waves, chemical interactions, and pressure fluctuations that forms the physical basis of the world around us. It was investigated initially in the visual domain (colour, motion, depth, and form), then in the auditory domain, and recently in the multisensory areas. It can be said therefore, that the binding problem is central to multisensory perception.\n\nHowever, considerations of how unified conscious representations are formed are not the full focus of multisensory Integration research. It is obviously important for the senses to interact in order to maximize how efficiently people interact with the environment. For perceptual experience and behavior to benefit from the simultaneous stimulation of multiple sensory modalities, integration of the information from these modalities is necessary. Some of the mechanisms mediating this phenomenon and its subsequent effects on cognitive and behavioural processes will be examined hereafter. Perception is often defined as one's conscious experience, and thereby combines inputs from all relevant senses and prior knowledge. Perception is also defined and studied in terms of feature extraction, which is several hundred milliseconds away from conscious experience. Notwithstanding the existence of Gestalt psychology schools that advocate a holistic approach to the operation of the brain, the physiological processes underlying the formation of percepts and conscious experience have been vastly understudied. Nevertheless, burgeoning neuroscience research continues to enrich our understanding of the many details of the brain, including neural structures implicated in multisensory integration such as the superior colliculus (SC) and various cortical structures such as the superior temporal gyrus (GT) and visual and auditory association areas. Although the structure and function of the SC are well known, the cortex and the relationship between its constituent parts are presently the subject of much investigation. Concurrently, the recent impetus on integration has enabled investigation into perceptual phenomena such as the ventriloquism effect, rapid localization of stimuli and the McGurk effect; culminating in a more thorough understanding of the human brain and its functions.\n\nStudies of sensory processing in humans and other animals has traditionally been performed one sense at a time, and to the present day, numerous academic societies and journals are largely restricted to considering sensory modalities separately ('Vision Research', 'Hearing Research' etc.). \nHowever, there is also a long and parallel history of multisensory research. An example is the Stratton's (1896) experiments on the somatosensory effects of wearing vision-distorting prism glasses.\nMultisensory interactions or crossmodal effects\nin which the perception of a stimulus is influenced by\nthe presence of another type of stimulus are referred since very\nearly in the past. They were reviewed by Hartmann in a fundamental book where, among several\nreferences to different types of multisensory interactions,\nreference is made to the work of Urbantschitsch in 1888 who reported on the improvement of visual acuity by auditive stimuli in subjects\nwith damaged brain. This effect was also found latter in normals by Krakov and Hartmann, as well as the fact that the visual acuity could be improved by\nother type of stimuli. It is also noteworthy the amount\nof work in the early thirties on intersensory relations in Soviet\nUnion, reviewed by London. A remarkable multisensory research is the extensive work of Gonzalo in the forties on the characterization of a multisensory syndrome in patients with parieto-occipital cortical lesions. In this syndrome, all the sensory functions are affected, and with symmetric bilaterality, in spite of being a unilateral lesion where the primary areas were not involved. A feature of this syndrome is the great permeability to crossmodal effects between visual, tactile, auditive stimuli as well as muscular effort to improve the perception, also decreasing the reaction times. The improvement by crossmodal effect was found to be greater as\nthe primary stimulus to be perceived was weaker, and as the cortical lesion was greater (Vol I and II of reference). This author interpreted these phenomena under a dynamic physiological concept, and from a model based on functional gradients through the cortex and scaling laws of dynamical systems, thus highlighting the functional unity of the cortex. According to the functional cortical gradients, the specificity of the cortex would be distributed in gradation, and the overlap of different specific gradients would be related to multisensory interactions.\n\nMultisensory research has recently gained enormous interest and popularity.\n\nWhen we hear a car honk, we would determine which car triggers the honk by which car we see is the spatially closest to the honk. It's a spatial congruent example by combining visual and auditory stimuli. On the other hand, the sound and the pictures of a TV program would be integrated as structural congruent by combining visual and auditory stimuli. However, if the sound and the pictures were not meaningfully fit, we would segregate the two stimuli. Therefore, whether spatial or structural congruent should not only combine the stimuli but also be determined by understanding.\n\nLiterature on spatial crossmodal biases suggests that visual modality often influences information from other senses. Some research indicates that vision dominates what we hear, when varying the degree of spatial congruency. This is known as the ventriloquist effect. In cases of visual and haptic integration, children younger than 8 years of age show visual dominance when required to identify object orientation. However, haptic dominance occurs when the factor to identify is object size.\n\nAccording to Welch and Warren (1980), the Modality Appropriateness Hypothesis states that the influence of perception in each modality in multisensory integration depends on that modality's appropriateness for the given task. Thus, vision has a greater influence on integrated localization than hearing, and hearing and touch have a greater bearing on timing estimates than vision.\n\nMore recent studies refine this early qualitative account of multisensory integration. Alais and Burr (2004), found that following progressive degradation in the quality of a visual stimulus, participants' perception of spatial location was determined progressively more by a simultaneous auditory cue. However, they also progressively changed the temporal uncertainty of the auditory cue; eventually concluding that it is the uncertainty of individual modalities that determine to what extent information from each modality is considered when forming a percept. This conclusion is similar in some respects to the 'inverse effectiveness rule'. The extent to which multisensory integration occurs may vary according to the ambiguity of the relevant stimuli. In support of this notion, a recent study shows that weak senses such as olfaction can even modulate the perception of visual information as long as the reliability of visual signals is adequately compromised.\n\nThe theory of Bayesian integration is based on the fact that the brain must deal with a number of inputs, which vary in reliability. In dealing with these inputs, it must construct a coherent representation of the world that corresponds to reality. The Bayesian integration view is that the brain uses a form of Bayesian inference. This view has been backed up by computational modeling of such a Bayesian inference from signals to coherent representation, which shows similar characteristics to integration in the brain.\n\nWith the assumption of independence between various sources, traditional cue combination model is successful in modality integration. However, depending on the discrepancies between modalities, there might be different forms of stimuli fusion: integration, partial integration, and segregation. To fully understand the other two types, we have to use causal inference model without the assumption as cue combination model. This freedom gives us general combination of any numbers of signals and modalities by using Bayes' rule to make causal inference of sensory signals.\n\nThe difference between two models is that hierarchical model can explicitly make causal inference to predict certain stimulus while non-hierarchical model can only predict joint probability of stimuli. However, hierarchical model is actually a special case of non-hierarchical model by setting joint prior as a weighted average of the prior to common and independent causes, each weighted by their prior probability. Based on the correspondence of these two models, we can also say that hierarchical is a mixture modal of non-hierarchical model.\n\nFor Bayesian model, the prior and likelihood generally represent the statistics of the environment and the sensory representations. The independence of priors and likelihoods is not assured since the prior may vary with likelihood only by the representations. However, the independence has been proved by Shams with series of parameter control in multi sensory perception experiment.\n\nThe contributions of Barry Stein, Alex Meredith, and their colleagues (e.g.\"The merging of the senses\" 1993,) are widely considered to be the groundbreaking work in the modern field of multisensory integration. Through detailed long-term study of the neurophysiology of the superior colliculus, they distilled three general principles by which multisensory integration may best be described.\n\n\nA unimodal approach dominated scientific literature until the beginning of this century. Although this enabled rapid progression of neural mapping, and an improved understanding of neural structures, the investigation of perception remained relatively stagnant, with a few exceptions. The recent revitalized enthusiasm into perceptual research is indicative of a substantial shift away from reductionism and toward gestalt methodologies. Gestalt theory, dominant in the late 19th and early 20th centuries espoused two general principles: the 'principle of totality' in which conscious experience must be considered globally, and the 'principle of psychophysical isomorphism' which states that perceptual phenomena are correlated with cerebral activity. Just these ideas were already applied by Justo Gonzalo in his work of brain dynamics, where a sensory-cerebral correspondence is considered in the formulation of the \"development of the sensory field due to a psychophysical isomorphism\" (pag. 23 of the English translation of ref.). Both ideas 'principle of totality' and 'psychophysical isomorphism' are particularly relevant in the current climate and have driven researchers to investigate the behavioural benefits of multisensory integration.\n\nIt has been widely acknowledged that uncertainty in sensory domains results in an increased dependence of multisensory integration. Hence, it follows that cues from multiple modalities that are both temporally and spatially synchronous are viewed neurally and perceptually as emanating from the same source. The degree of synchrony that is required for this 'binding' to occur is currently being investigated in a variety of approaches. It should be noted here that the integrative function only occurs to a point beyond which the subject can differentiate them as two opposing stimuli. Concurrently, a significant intermediate conclusion can be drawn from the research thus far. Multisensory stimuli that are bound into a single percept, are also bound on the same receptive fields of multisensory neurons in the SC and cortex.\n\nResponses to multiple simultaneous sensory stimuli can be faster than responses to the same stimuli presented in isolation. Hershenson (1962) presented a light and tone simultaneously and separately, and asked human participants to respond as rapidly as possible to them. As the asynchrony between the onsets of both stimuli was varied, it was observed that for certain degrees of asynchrony, reaction times were decreased. These levels of asynchrony were quite small, perhaps reflecting the temporal window that exists in multisensory neurons of the SC. Further studies have analysed the reaction times of saccadic eye movements; and more recently correlated these findings to neural phenomena. In patients studied by Gonzalo, with lesions in the parieto-occipital cortex, the decrease in the reaction time to a given stimulus by means of intersensory facilitation was shown to be very remarkable.\n\nThe redundant target effect is the observation that people typically respond faster to double targets (two targets presented simultaneously) than to either of the targets presented alone. This difference in latency is termed the redundancy gain (RG).\n\nIn a study done by Forster, Cavina-Pratesi, Aglioti, and Berlucchi (2001), normal observers responded faster to simultaneous visual and\ntactile stimuli than to single visual or tactile stimuli. RT to simultaneous visual and tactile stimuli was also faster than RT to simultaneous dual visual or tactile stimuli. The advantage for RT to combined visual-tactile stimuli over RT to the other types of stimulation could be accounted for by intersensory neural facilitation rather than by probability summation. These effects can be ascribed to the convergence of tactile and visual inputs onto neural centers which contain flexible multisensory representations of body parts.\n\nIt has been found that two converging bimodal stimuli can produce a perception that is not only different in magnitude than the sum of its parts, but also quite different in quality. In a classic study labeled the McGurk effect, a person's phoneme production was dubbed with a video of that person speaking a different phoneme. The end result was the perception of a third, different phoneme. McGurk and MacDonald (1976) explained that phonemes such as ba, da, ka, ta, ga and pa can be divided into four groups, those that can be visually confused, i.e. (da, ga, ka, ta) and (ba and pa), and those that can be audibly confused. Hence, when ba – voice and ga lips are processed together, the visual modality sees ga or da, and the auditory modality hears ba or da, combining to form the percept da.\n\nVentriloquism has been used as the evidence for the modality appropriateness hypothesis. Ventriloquism describes the situation in which auditory location perception is shifted toward a visual cue. The original study describing this phenomenon was conducted by Howard and Templeton, (1966) after which several studies have replicated and built upon the conclusions they reached. In conditions in which the visual cue is unambiguous, visual capture reliably occurs. Thus to test the influence of sound on perceived location, the visual stimulus must be progressively degraded. Furthermore, given that auditory stimuli are more attuned to temporal changes, recent studies have tested the ability of temporal characteristics to influence the spatial location of visual stimuli. Some types of EVP – electronic voice phenomenon, mainly the ones using sound bubles are considered a kind of modern ventriloquism technique and is played by the use of sophisticated software, computers and sound equipment.\n\nThe double flash illusion was reported as the first illusion to show that visual stimuli can be qualitatively altered by audio stimuli. In the standard paradigm participants are presented combinations of one to four flashes accompanied by zero to 4 beeps. They were then asked to say how many flashes they perceived. Participants perceived illusory flashes when there were more beeps than flashes. fMRI studies have shown that there is crossmodal activation in early, low level visual areas, which was qualitatively similar to the perception of a real flash. This suggests that the illusion reflects subjective perception of the extra flash. Further, studies suggest that timing of multisensory activation in unisensory cortexes is too fast to be mediated by a higher order integration suggesting feed forward or lateral connections. One study has revealed the same effect but from vision to audition, as well as fission rather than fusion effects, although the level of the auditory stimulus was reduced to make it less salient for those illusions affecting audition.\n\nIn the rubber hand illusion (RHI), human participants view a dummy hand being stroked with a paintbrush, while they feel a series of identical brushstrokes applied to their own hand, which is hidden from view. If this visual and tactile information is applied synchronously, and if the visual appearance and position of the dummy hand is similar to one's own hand, then people may feel that the touches on their own hand are coming from the dummy hand, and even that the dummy hand is, in some way, their own hand. This is an early form of body transfer illusion. The RHI is an illusion of vision, touch, and posture (proprioception), but a similar illusion can also be induced with touch and proprioception. It has also been found that the illusion may not require tactile stimulation at all, but can be completely induced using mere vision of the rubber hand being in a congruent posture with the hidden real hand. The very first report of this kind of illusion may have been as early as 1937 (Tastevin, 1937).\n\nBody transfer illusion typically involves the use of virtual reality devices to induce the illusion in the subject that the body of another person or being is the subject's own body.\n\nThe superior colliculus (SC) or optic tectum (OT) is part of the tectum, located in the midbrain, superior to the brainstem and inferior to the thalamus. It contains seven layers of alternating white and grey matter, of which the superficial contain topographic maps of the visual field; and deeper layers contain overlapping spatial maps of the visual, auditory and somatosensory modalities. The structure receives afferents directly from the retina, as well as from various regions of the cortex (primarily the occipital lobe), the spinal cord and the inferior colliculus. It sends efferents to the spinal cord, cerebellum, thalamus and occipital lobe via the lateral geniculate nucleus (LGN). The structure contains a high proportion of multisensory neurons and plays a role in the motor control of orientation behaviours of the eyes, ears and head.\n\nReceptive fields from somatosensory, visual and auditory modalities converge in the deeper layers to form a two-dimensional multisensory map of the external world. Here, objects straight ahead are represented caudally and objects on the periphery are represented rosterally. Similarly, locations in superior sensory space are represented medially, and inferior locations are represented laterally.\n\nHowever, in contrast to simple convergence, the SC integrates information to create an output that differs from the sum of its inputs. Following a phenomenon labelled the 'spatial rule', neurons are excited if stimuli from multiple modalities fall on the same or adjacent receptive fields, but are inhibited if the stimuli fall on disparate fields. Excited neurons may then proceed to innervate various muscles and neural structures to orient an individual's behaviour and attention toward the stimulus. Neurons in the SC also adhere to the 'temporal rule', in which stimulation must occur within close temporal proximity to excite neurons. However, due to the varying processing time between modalities and the relatively slower speed of sound to light, it has been found the neurons may be optimally excited when stimulated some time apart.\n\nSingle neurons in the macaque putamen have been shown to have visual and somatosensory responses closely related to those in the polysensory zone of the premotor cortex and area 7b in the parietal lobe.\n\nMultisensory neurons exist in a large number of locations, often integrated with unimodal neurons. They have recently been discovered in areas previously thought to be modality specific, such as the somatosensory cortex; as well as in clusters at the borders between the major cerebral lobes, such as the occipito-parietal space and the occipito-temporal space.\n\nHowever, in order to undergo such physiological changes, there must exist continuous connectivity between these multisensory structures. It is generally agreed that information flow within the cortex follows a hierarchical configuration. Hubel and Wiesel showed that receptive fields and thus the function of cortical structures, as one proceeds out from V1 along the visual pathways, become increasingly complex and specialized. From this it was postulated that information flowed outwards in a feed forward fashion; the complex end products eventually binding to form a percept. However, via fMRI and intracranial recording technologies, it has been observed that the activation time of successive levels of the hierarchy does not correlate with a feed forward structure. That is, late activation has been observed in the striate cortex, markedly after activation of the prefrontal cortex in response to the same stimulus.\n\nComplementing this, afferent nerve fibres have been found that project to early visual areas such as the lingual gyrus from late in the dorsal (action) and ventral (perception) visual streams, as well as from the auditory association cortex. Feedback projections have also been observed in the opossum directly from the auditory association cortex to V1. This last observation currently highlights a point of controversy within the neuroscientific community. Sadato \"et al.\" (2004) concluded, in line with Bernstein \"et al.\" (2002), that the primary auditory cortex (A1) was functionally distinct from the auditory association cortex, in that it was void of any interaction with the visual modality. They hence concluded that A1 would not at all be effected by cross modal plasticity. This concurs with Jones and Powell's (1970) contention that primary sensory areas are connected only to other areas of the same modality.\n\nIn contrast, the dorsal auditory pathway, projecting from the temporal lobe is largely concerned with processing spatial information, and contains receptive fields that are topographically organized. Fibers from this region project directly to neurons governing corresponding receptive fields in V1. The perceptual consequences of this have not yet been empirically acknowledged. However, it can be hypothesized that these projections may be the precursors of increased acuity and emphasis of visual stimuli in relevant areas of perceptual space. Consequently, this finding rejects Jones and Powell's (1970) hypothesis and thus is in conflict with Sadato \"et al.\"'s (2004) findings. A resolution to this discrepancy includes the possibility that primary sensory areas can not be classified as a single group, and thus may be far more different from what was previously thought.\n\nThe multisensory syndrome with symmetric bilaterality, characterized\nby Gonzalo and called by this author `central syndrome of the cortex', was originated from a unilateral parieto-occipital cortical lesion equidistant from the visual, tactile, and auditory projection areas\n(the middle of area 19, the anterior part of area 18 and the most posterior of area 39, in Brodmann terminology) that was called `central zone'. The gradation observed between syndromes led this author to propose a functional gradient scheme in which the specificity of the cortex is distributed with a continuous variation, the overlap of the specific gradients would be high or maximum in that ` central zone'.\n\nFurther research is necessary for a definitive resolution.\n\nArea F4 in macaques\n\nArea F5 in macaques\n\nPolysensory zone of premotor cortex (PZ) in macaques\n\nPrimary visual cortex (V1)\n\nLingual gyrus in humans\n\nLateral occipital complex (LOC), including lateral occipital tactile visual area (LOtv)\n\nVentral intraparietal sulcus (VIP) in macaques\n\nLateral intraparietal sulcus (LIP) in macaques\n\nArea 7b in macaques\n\nSecond somatosensory cortex (SII)\n\nPrimary auditory cortex (A1)\n\nSuperior temporal cortex (STG/STS/PT) Audio visual cross modal interactions are known to occur in the auditory association cortex which lies directly inferior to the Sylvian fissure in the temporal lobe. Plasticity was observed in the superior temporal gyrus (STG) by Petitto \"et al.\" (2000). Here, it was found that the STG was more active during stimulation in native deaf signers compared to hearing non signers. Concurrently, further research has revealed differences in the activation of the Planum temporale (PT) in response to non linguistic lip movements between the hearing and deaf; as well as progressively increasing activation of the auditory association cortex as previously deaf participants gain hearing experience via a cochlear implant.\n\nAnterior ectosylvian sulus (AES) in cats\n\nRostral lateral suprasylvian sulcus (rLS) in cats\n\nThe most significant interaction between these two systems (corticotectal interactions) is the connection between the anterior ectosylvian sulcus (AES), which lies at the junction of the parietal, temporal and frontal lobes, and the SC. The AES is divided into three unimodal regions with multisensory neurons at the junctions between these sections. (Jiang & Stein, 2003). Neurons from the unimodal regions project to the deep layers of the SC and influence the multiplicative integration effect. That is, although they can receive inputs from all modalities as normal, the SC can not enhance or depress the effect of multisensory stimulation without input from the AES.\n\nConcurrently, the multisensory neurons of the AES, although also integrally connected to unimodal AES neurons, are not directly connected to the SC. This pattern of division is reflected in other areas of the cortex, resulting in the observation that cortical and tectal multisensory systems are somewhat dissociated. Stein, London, Wilkinson and Price (1996) analysed the perceived luminance of an LED in the context of spatially disparate auditory distracters of various types. A significant finding was that a sound increased the perceived brightness of the light, regardless of their relative spatial locations, provided the light's image was projected onto the fovea. Here, the apparent lack of the spatial rule, further differentiates cortical and tectal multisensory neurons. Little empirical evidence exists to justify this dichotomy. Nevertheless, cortical neurons governing perception, and a separate sub cortical system governing action (orientation behavior) is synonymous with the perception action hypothesis of the visual stream. Further investigation into this field is necessary before any substantial claims can be made.\n\nResearch suggests the existence of two multisensory routes for \"what\" and \"where\". The \"what\" route identifying the identity of things involving area Brodmann area 9 in the right inferior frontal gyrus and right middle frontal gyrus, Brodmann area 13 and Brodmann area 45 in the right insula-inferior frontal gyrus area, and Brodmann area 13 bilaterally in the insula. The \"where\" route detecting their spatial attributes involving the Brodmann area 40 in the right and left inferior parietal lobule and the Brodmann area 7 in the right precuneus-superior parietal lobule and Brodmann area 7 in\nthe left superior parietal lobule.\n\nAll species equipped with multiple sensory systems, utilize them in an integrative manner to achieve action and perception. However, in most species, especially higher mammals and humans, the ability to integrate develops in parallel with physical and cognitive maturity. Children until certain ages do not show mature integration patterns. Classically, two opposing views that are principally modern manifestations of the nativist/empiricist dichotomy have been put forth. The integration (empiricist) view states that at birth, sensory modalities are not at all connected. Hence, it is only through active exploration that plastic changes can occur in the nervous system to initiate holistic perceptions and actions. Conversely, the differentiation (nativist) perspective asserts that the young nervous system is highly interconnected; and that during development, modalities are gradually differentiated as relevant connections are rehearsed and the irrelevant are discarded.\n\nUsing the SC as a model, the nature of this dichotomy can be analysed. In the newborn cat, deep layers of the SC contain only neurons responding to the somatosensory modality. Within a week, auditory neurons begin to occur, but it is not until two weeks after birth that the first multisensory neurons appear. Further changes continue, with the arrival of visual neurons after three weeks, until the SC has achieved its fully mature structure after three to four months. Concurrently in species of monkey, newborns are endowed with a significant complement of multisensory cells; however, along with cats there is no integration effect apparent until much later. This delay is thought to be the result of the relatively slower development of cortical structures including the AES; which as stated above, is essential for the existence of the integration effect.\n\nFurthermore, it was found by Wallace (2004) that cats raised in a light deprived environment had severely underdeveloped visual receptive fields in deep layers of the SC. Although, receptive field size has been shown to decrease with maturity, the above finding suggests that integration in the SC is a function of experience. Nevertheless, the existence of visual multisensory neurons, despite a complete lack of visual experience, highlights the apparent relevance of nativist viewpoints. Multisensory development in the cortex has been studied to a lesser extent, however a similar study to that presented above was performed on cats whose optic nerves had been severed. These cats displayed a marked improvement in their ability to localize stimuli through audition; and consequently also showed increased neural connectivity between V1 and the auditory cortex. Such plasticity in early childhood allows for greater adaptability, and thus more normal development in other areas for those with a sensory deficit.\n\nIn contrast, following the initial formative period, the SC does not appear to display any neural plasticity. Despite this, habituation and sensititisation over the long term is known to exist in orientation behaviors. This apparent plasticity in function has been attributed to the adaptability of the AES. That is, although neurons in the SC have a fixed magnitude of output per unit input, and essentially operate an all or nothing response, the level of neural firing can be more finely tuned by variations in input by the AES.\n\nAlthough there is evidence for either perspective of the integration/differentiation dichotomy, a significant body of evidence also exists for a combination of factors from either view. Thus, analogous to the broader nativist/empiricist argument, it is apparent that rather than a dichotomy, there exists a continuum, such that the integration and differentiation hypotheses are extremes at either end.\n\nNot much is known about the development of the ability to integrate multiple estimates such as vision and touch. Some multisensory abilities are present from early infancy, but it is not until children are eight years or older before they use multiple modalities to reduce sensory uncertainty.\nOne study demonstrated that cross-modal visual and auditory integration is present from within 1 year of life. This study measured response time for orientating towards a source. Infants who were 8–10 months old showed significantly decreased response times when the source was presented through both visual and auditory information compared to a single modality. Younger infants, however, showed no such change in response times to these different conditions. Indeed, the results of the study indicates that children potentially have the capacity to integrate sensory sources at any age. However, in certain cases, for example visual cues, intermodal integration is avoided.\nAnother study found that cross-modal integration of touch and vision for distinguishing size and orientation is available from at least 8 years of age. For pre-integration age groups, one sense dominates depending on the characteristic discerned (see visual dominance).\n\nA study investigating sensory integration \"within\" a single modality (vision) found that it cannot be established until age 12 and above. This particular study assessed the integration of disparity and texture cues to resolve surface slant. Though younger age groups showed a somewhat better performance when combining disparity and texture cues compared to using only disparity or texture cues, this difference was not statistically significant. In adults, the sensory integration can be mandatory, meaning that they no longer have access to the individual sensory sources.\n\nAcknowledging these variations, many hypotheses have been established to reflect why these observations are task-dependent. Given that different senses develop at different rates, it has been proposed that cross-modal integration does not appear until both modalities have reached maturity. The human body undergoes significant physical transformation throughout childhood. Not only is there growth in size and stature (affecting viewing height), but there is also change in inter-ocular distance and eyeball length. Therefore, sensory signals need to be constantly re-evaluated to appreciate these various physiological changes. Some support comes from animal studies that explore the neurobiology behind integration. Adult monkeys have deep inter-neuronal connections within the superior colliculus providing strong, accelerated visuo-auditory integration. Young animals conversely, do not have this enhancement until unimodal properties are fully developed.\n\nAdditionally, to rationalize sensory dominance, Gori et al. (2008) advocates that the brain utilises the most direct source of information during sensory immaturity. In this case, orientation is primarily a visual characteristic. It can be derived directly from the object image that forms on the retina, irrespective of other visual factors. In fact, data shows that a functional property of neurons within primate visual cortices' are their discernment to orientation. In contrast, haptic orientation judgements are recovered through collaborated patterned stimulations, evidently an indirect source susceptible to interference. Likewise, when size is concerned haptic information coming from positions of the fingers is more immediate. Visual-size perceptions, alternatively, have to be computed using parameters such as slant and distance. Considering this, sensory dominance is a useful instinct to assist with calibration. During sensory immaturity, the more simple and robust information source could be used to tweak the accuracy of the alternate source. Follow-up work by Gori et al. (2012) showed that, at all ages, vision-size perceptions are near perfect when viewing objects within the haptic workspace (i.e. at arm's reach). However, systematic errors in perception appeared when the object was positioned beyond this zone. Children younger than 14 years tend to underestimate object size, whereas adults overestimated. However, if the object was returned to the haptic workspace, those visual biases disappeared. These results support the hypothesis that haptic information may educate visual perceptions. If sources are used for cross-calibration they cannot, therefore, be combined (integrated). Maintaining access to individual estimates is a trade-off for extra plasticity over accuracy, which could be beneficial in retrospect to the developing body.\nAlternatively, Ernst (2008) advocates that efficient integration initially relies upon establishing correspondence – which sensory signals belong together. Indeed, studies have shown that visuo-haptic integration fails in adults when there is a perceived spatial separation, suggesting sensory information is coming from different targets. Furthermore, if the separation can be explained, for example viewing an object through a mirror, integration is re-established and can even be optimal. Ernst (2008) suggests that adults can obtain this knowledge from previous experiences to quickly determine which sensory sources depict the same target, but young children could be deficient in this area. Once there is a sufficient bank of experiences, confidence to correctly integrate sensory signals can then be introduced in their behaviour.\n\nLastly, Nardini et al. (2010) recently hypothesised that young children have optimized their sensory appreciation for speed over accuracy. When information is presented in two forms, children may derive an estimate from the fastest available source, subsequently ignoring the alternate, even if it contains redundant information. Nardini et al. (2010) provides evidence that children's (aged 6 years) response latencies are significantly lower when stimuli are presented in multi-cue over single-cue conditions. Conversely, adults showed no change between these conditions. Indeed, adults display mandatory fusion of signals, therefore they can only ever aim for maximum accuracy. However, the overall mean latencies for children were not faster than adults, which suggests that speed optimization merely enable them to keep up with the mature pace. Considering the haste of real-world events, this strategy may prove necessary to counteract the general slower processing of children and maintain effective vision-action coupling. Ultimately the developing sensory system may preferentially adapt for different goals – \"speed and detecting sensory conflicts\" – those typical of objective learning.\n\nThe late development of efficient integration has also been investigated from computational point of view. Daee et al. (2014) showed that having one dominant sensory source at early age, rather than integrating all sources, facilitates the overall development of cross-modal integrations.\n\nProsthetics designers should carefully consider the nature of dimensionality alteration of sensorimotor signaling from and to the CNS when designing prothesitic devices. As reported in literatures, neural signaling from the CNS to the motors is organized in a way that the dimensionalities of the signals are gradually increased as you approach the muscles, also called muscle synergies. In the same principal, but in opposite ordering, on the other hand, signals dimensionalities from the sensory receptors are gradually integrated, also called sensory synergies, as they approaches the CNS. This bow tie like signaling formation enables the CNS to process abstract yet valuable information only. Such as process will decrease complexity of the data, handle the noises and guarantee to the CNS the optimum energy consumption. \nAlthough the current commercially available prosthetic devices mainly focusing in implementing the motor side by simply uses EMG sensors to switch between different activation states of the prosthesis. Very limited works have proposed a system to involve by integrating the sensory side. The integration of tactile sense and proprioception is regarded as essential for implementing the ability to perceive environmental input.\n\n\n\n"}
{"id": "15417735", "url": "https://en.wikipedia.org/wiki?curid=15417735", "title": "Net cafe refugee", "text": "Net cafe refugee\n\n, also known as , are a class of homeless people in Japan who do not own or rent a residence (thus having no permanent address) and sleep in 24-hour Internet cafés or manga cafés. Although such cafés originally provided only Internet services, some have expanded their services to include food, drink, and showers. They are often used by commuters who miss the last train; however, the net café refugee trend has seen large numbers of people use them as their homes.\n\nA Japanese government study estimated that over 5,400 people are spending at least half of their week staying in net cafés. It has been alleged that this phenomenon is part of an increasing wealth gap in Japan, which has historically boasted of having a very economically equal society.\nAccording to the Japanese government survey, those staying have little interest in manga or the Internet, and are instead using the place because of the low price relative to any of the competition for temporary housing, business hotels, capsule hotels, hostels, or any other option besides sleeping on the street. It was also estimated that about half of those staying have no job, while the other half work in low-paid temporary jobs, which paid around 100,000 yen ($1000) per month – lower than what is needed to rent an apartment and pay for transportation in a city like Tokyo.\n\nSome internet cafés offer free showers and sell underwear and other personal items, enabling net café refugees to use the internet cafés like a hotel or hostel.\n\nAnother word for Net café refugees is \"cyber-homeless\", a Japanese word based on English. Typically, the cyber-homeless are unemployed or underemployed and cannot afford to rent even the cheapest apartment, which is more than the cost per month to rent an internet booth daily. The cyber-homeless may even use the address of the internet café on resumes when applying for jobs to conceal their present form of accommodation.\n\nThe fee of around ¥1400 to ¥2400 yen for a night – which may include free soft drinks, TV, comics and internet access – is less than for capsule hotels. Some cyber-homeless may also be freeters.\n\nInternet cafés in Dublin, Ireland are also known to allow homeless people to sleep in chairs or on desks overnight, typically charging €6–10.\n\n\n\n \n"}
{"id": "3603035", "url": "https://en.wikipedia.org/wiki?curid=3603035", "title": "Nielsen theory", "text": "Nielsen theory\n\nNielsen theory is a branch of mathematical research with its origins in topological fixed point theory. Its central ideas were developed by Danish mathematician Jakob Nielsen, and bear his name.\n\nThe theory developed in the study of the so-called \"minimal number\" of a map \"f\" from a compact space to itself, denoted \"MF\"[\"f\"]. This is defined as:\nwhere \"~\" indicates homotopy of mappings, and #Fix(\"g\") indicates the number of fixed points of \"g\". The minimal number was very difficult to compute in Nielsen's time, and remains so today. Nielsen's approach is to group the fixed point set into classes, which are judged \"essential\" or \"nonessential\" according to whether or not they can be \"removed\" by a homotopy.\n\nNielsen's original formulation is equivalent to the following:\nWe define an equivalence relation on the set of fixed points of a self-map \"f\" on a space \"X\". We say that \"x\" is equivalent to \"y\" if and only if there exists a path \"c\" from \"x\" to \"y\" with \"f\"(\"c\") homotopic to \"c\" as paths. The equivalence classes with respect to this relation are called the Nielsen classes of \"f\", and the Nielsen number \"N\"(\"f\") is defined as the number of Nielsen classes having non-zero fixed point index sum.\n\nNielsen proved that\nmaking his invariant a good tool for estimating the much more difficult \"MF\"[\"f\"]. This leads immediately to what is now known as the Nielsen fixed point theorem: \"Any map f has at least N(f) fixed points.\"\n\nBecause of its definition in terms of the fixed point index, the Nielsen number is closely related to the Lefschetz number. Indeed, shortly after Nielsen's initial work, the two invariants were combined into a single \"generalized Lefschetz number\" (more recently called the Reidemeister trace) by Wecken and Reidemeister.\n\n"}
{"id": "43482265", "url": "https://en.wikipedia.org/wiki?curid=43482265", "title": "Norton's dome", "text": "Norton's dome\n\nNorton's dome is a thought experiment that exhibits a non-deterministic system within the bounds of Newtonian mechanics. It was devised by John D. Norton and first discussed in his 2003 paper \"Causation as Folk Science\". Norton's dome problem can be regarded as a problem in physics, mathematics, or philosophy. It poses interesting philosophical questions about the concepts of causality, determinism, and probability theory.\n\nThe model consists of an idealized particle initially sitting motionless at the apex of an idealized radially symmetrical frictionless dome described by the equation\n\nwhere \"h\" is the vertical displacement from the top of the dome to a point on the dome, \"r\" is the geodesic distance from the dome's apex to that point (in other words, a radial coordinate \"r\" is \"inscribed\" on the surface), and \"g\" the acceleration due to gravity.\n\nNorton shows that there are two classes of mathematical solutions to the system under Newtonian physics. In the first, the particle stays sitting at the apex of the dome forever. In the second, the particle sits at the apex of the dome for a while, and then after an arbitrary period of time starts to slide down the dome in an arbitrary direction. The apparent paradox in this second case is that this would seem to occur for no discernible reason, and without any radial force being exerted on it by any other entity, apparently contrary to both physical intuition and normal intuitive concepts of cause and effect, yet the motion is still entirely consistent with the mathematics of Newton's laws of motion.\n\nWhile many criticisms have been made of Norton's thought experiment, such as it being a violation of the principle of Lipschitz continuity (the 2nd solution type, where the ball starts rolling after an arbitrary period of time, is not Lipschitz continuous), or in violation of the principles of physical symmetry, or that it is somehow in some other way \"unphysical\", there is no consensus among its critics as to why they regard it as invalid.\n\n\n"}
{"id": "340238", "url": "https://en.wikipedia.org/wiki?curid=340238", "title": "Occasionalism", "text": "Occasionalism\n\nOccasionalism is a philosophical theory about causation which says that created substances cannot be efficient causes of events. Instead, all events are taken to be caused directly by God. (A related theory, which has been called \"occasional causation\", also denies a link of efficient causation between mundane events, but may differ as to the identity of the true cause that replaces them.) The theory states that the illusion of efficient causation between mundane events arises out of God's causing of one event after another. However, there is no necessary connection between the two: it is not that the first event \"causes\" God to cause the second event: rather, God first causes one and then causes the other.\n\nThe doctrine first reached prominence in the Islamic theological schools of Iraq, especially in Basra. The ninth century theologian Abu al-Hasan al-Ash'ari argued that there is no Secondary Causation in the created order. The world is sustained and governed through direct intervention of a divine primary causation. As such the world is in a constant state of recreation by God.\n\nThe most famous proponent of the Asharite occasionalist doctrine was Abu Hamid Muhammad ibn Muhammad al-Ghazali, an 11th-century theologian based in Baghdad. In \"The Incoherence of the Philosophers\", Al-Ghazali launched a philosophical critique against Neoplatonic-influenced early Islamic philosophers such as Al-Farabi and Ibn Sina. In response to the philosophers' claim that the created order is governed by secondary efficient causes (God being, as it were, the Primary and Final Cause in an ontological and logical sense), Ghazali argues that what we observe as regularity in nature based presumably upon some natural law is actually a kind of constant and continual regularity. There is no independent necessitation of change and becoming, other than what God has ordained. To posit an independent causality outside of God's knowledge and action is to deprive Him of true agency, and diminish his attribute of power. In his famous example, when fire and cotton are placed in contact, the cotton is burned not because of the heat of the fire, but through God's direct intervention, a claim which he defended using logic. In the 12th century, this theory was defended and further strengthened by the Islamic theologian Fakhr al-Din al-Razi, using his expertise in the natural sciences of astronomy, cosmology and physics.\n\nBecause God is usually seen as rational, rather than arbitrary, his behaviour in normally causing events in the same sequence (i.e., what appears to us to be efficient causation) can be understood as a natural outworking of that principle of reason, which we then describe as the laws of nature. Properly speaking, however, these are not laws of nature but laws by which God chooses to govern his own behaviour (his autonomy, in the strict sense) — in other words, his rational will. This is not, however, an essential element of an occasionalist account, and occasionalism can include positions where God's behaviour (and thus that of the world) is viewed as ultimately inscrutable, thus maintaining God's essential transcendence. On this understanding, apparent anomalies such as miracles are not really such: they are simply God behaving in a way that \"appears\" unusual \"to us\". Given his transcendent freedom, he is not bound even by his own nature. Miracles, as breaks in the rational structure of the universe, can occur, since God's relationship with the world is not mediated by rational principles.\n\nIn 1993, Karen Harding's paper \"Causality Then and Now: Al Ghazali and Quantum Theory\" described several \"remarkable\" similarities between Ghazali's concept of occasionalism and the (disputed) Copenhagen interpretation of quantum mechanics. She stated: \"In both cases, and contrary to common sense, objects are viewed as having no inherent properties and no independent existence. In order for an object to exist, it must be brought into being either by God (al Ghazili) or by an observer (the Copenhagen Interpretation).\"\n\nIn a 1978 article in \"Studia Islamica\", Lenn Goodman asks the question, \"Did Al-Ghazâlî Deny Causality?\" and demonstrates that Ghazali did not deny the existence of observed, \"worldly\" causation. According to Goodman's analysis, Ghazali does not claim that there is never any link between observed cause and observed effect: rather, Ghazali argues that there is no \"necessary\" link between observed cause and effect.\n\nOne of the motivations for the theory is the dualist belief that mind and matter are so utterly different in their essences that one cannot affect the other. Thus, a person's mind cannot be the true cause of his hand's moving, nor can a physical wound be the true cause of mental anguish. In other words, the mental cannot cause the physical and vice versa. Also, occasionalists generally hold that the physical cannot cause the physical either, for no necessary connection can be perceived between physical causes and effects. The will of God is taken to be necessary.\n\nThe doctrine is, however, more usually associated with certain seventeenth century philosophers of the Cartesian school. There are hints of an occasionalist viewpoint here and there in Descartes's own writings, but these can mostly be explained away under alternative interpretations. However, many of his later followers quite explicitly committed themselves to an occasionalist position. In one form or another, the doctrine can be found in the writings of: Johannes Clauberg, Claude Clerselier, Gerauld de Cordemoy, Arnold Geulincx, Louis de La Forge, François Lamy, and (most notably), Nicolas Malebranche.\n\nThese occasionalists' negative argument, that no necessary connections could be discovered between mundane events, was anticipated by certain arguments of Nicholas of Autrecourt in the fourteenth century, and were later taken up by David Hume in the eighteenth. Hume, however, stopped short when it came to the positive side of the theory, where God was called upon to replace such connections, complaining that \"We are got into fairy land [...] Our line is too short to fathom such immense abysses.\" Instead, Hume felt that the only place to find necessary connections was in the subjective associations of ideas within the mind itself. George Berkeley was also inspired by the occasionalists, and he agreed with them that no efficient power could be attributed to bodies. For Berkeley, bodies merely existed as ideas in percipient minds, and all such ideas were, as he put it, \"visibly inactive\". However, Berkeley disagreed with the occasionalists by continuing to endow the created minds themselves with efficient power. Gottfried Wilhelm Leibniz agreed with the occasionalists that there could be no efficient causation between distinct created substances, but he did not think it followed that there was no efficient power in the created world at all. On the contrary, every simple substance had the power to produce changes in \"itself\". The illusion of transeunt efficient causation, for Leibniz, arose out of the pre-established harmony between the alterations produced immanently within different substances.\n\n\n"}
{"id": "27352320", "url": "https://en.wikipedia.org/wiki?curid=27352320", "title": "Open-source appropriate technology", "text": "Open-source appropriate technology\n\nOpen-source appropriate technology (OSAT) is appropriate technology developed through the principles of the open-design movement. OSAT refers to, on the one hand, technology designed with special consideration to the environmental, ethical, cultural, social, political, and economic aspects of the community it is intended for. On the other hand, OSAT is developed in the open and licensed in such a way as to allow their designs to be used, modified and distributed freely.\n\nOpen source is a development method for appropriate technology that harnesses the power of distributed peer review and transparency of process. is an example of open-source appropriate technology. There anyone can both learn how to make and use AT free of concerns about patents. At the same time anyone can also add to the collective open-source knowledge base by contributing ideas, observations, experimental data, deployment logs, etc. It has been claimed that the potential for open-source-appropriate technology to drive applied sustainability is enormous. The built in continuous peer-review can result in better quality, higher reliability, and more flexibility than conventional design/patenting of technologies. The free nature of the knowledge also obviously provides lower costs, particularly for those technologies that do not benefit to a large degree from scale of manufacture. Finally, OSAT also enables the end to predatory intellectual property lock-in. This is particularly important in the context of technology focused on relieving suffering and saving lives in the developing world.\n\nThe \"open-source\" model can act as a driver of sustainable development. Reasons include:\n\n\nFor solutions, many researchers, companies, and academics do work on products meant to assist sustainable development. Vinay Gupta has suggested that those developers agree to three principles:\n\n\nThe ethics of information sharing in this context has been explored in depth.\n\n\n\n\nAppropriate technology is designed to promote decentralized, labor-intensive, energy-efficient and environmentally sound businesses. Carroll Pursell says that the movement declined from 1965 to 1985, due to an inability to counter advocates of agribusiness, large private utilities, and multinational construction companies. Recently (2011), several barriers to OSAT deployment have been identified:\n\n"}
{"id": "55861324", "url": "https://en.wikipedia.org/wiki?curid=55861324", "title": "Osram ne nsoromma", "text": "Osram ne nsoromma\n\nOsram ne nsoromma is one of the Asante Adinkra symbols, which is interpreted to mean \"Osram\" Moon \"Ne\" and \"Nsoromma\" Star. This symbol signifies love, bonding and faithfulness in marriage. The symbol is represented by a half moon with a star slightly hanging within the circumference of the moon. Adinkra are symbols that carry a message or a concept. They are very much used by the Ashantis of the Ashanti kingdom and the Baoulés of Côte d'Ivoire. Osram ne nsoromma symbols are incorporated into walls and other architectural features and quite recently has become common with tattoo designers. The most common ways through which the Adinkra messages are carried out or conveyed is having them printed extensively in fabrics and used in pottery. Adinkra is an Akan name which means farewell or goodbye. Osram ne nsoromma are two different powerful objects of creation put together (Moon and Star). Both co exist in the sky to produce magnificent light or brightness at night. There are some wise sayings closely related to the Osram ne nsoromma symbol,often linked with proverbs\n\nThe Akans of Ghana use an Adinkra symbol to express proverbs and other philosophical ideas or traditional wisdom, aspects of life or the environment.\n\nSome of the familiar proverbs are:\n\n\"Awaree nye nsafufuo na waka ahwe\", which means marriage is not palm-wine that you can decide to have a taste before you get served. It can also be interpreted to mean marriage is not a venture,committee or an organization that you can be a member today and withdraw your membership tomorrow. This proverb frowns upon break ups or separations in marriages or relationships.\n\n\"Woreko awaree a bisa\", This proverb talks about due diligence before marriage. it is wise to do a background check on your partner before marriage or the worse will happen. Making the wrong choice is sometimes costly. Similarly,most of the proverbs seem to put value on the object being addressed. This symbol signifies Love,bonding and faithfulness but one must carefully choose the right partner.\n\n\"Ahwenepa nkasa,\" can be interpreted to mean \"Ahwenepa\"( Good waist beads) \"nkasa\" (makes no noise). This can be related to mean a good man or a good woman needs no introduction to be known. You can notice them when you find them. This proverb talks about the character of the would be partner. Good man or woman is hard to find but when you meet that special person,you will notice.\n\n"}
{"id": "4574535", "url": "https://en.wikipedia.org/wiki?curid=4574535", "title": "Paremiology", "text": "Paremiology\n\nParemiology () is the collection and study of proverbs.\n\nParemiology can be dated back as far as Aristotle. Paremiography, on the other hand, is the collection of proverbs. The proverb scholar Wolfgang Mieder defines the term \"proverb\" as follows:\n\nAs well as actual proverbs, the following may be considered proverbial phrases:\n\nTypical stylistic features of proverbs (as Shirley Arora points out in her article, \"The Perception of Proverbiality\" (1984)) are:\n\n\nIn some languages, assonance, the repetition of a vowel, is also exploited in forming artistic proverbs, such as the following extreme example from Oromo, of Ethiopia.\nSimilarly, from Tajik:\nNotice that in all of these cases of complete assonance, the vowel is , the most common vowel in human languages.\n\nAlso in Amharic, complete assonance is not infrequent when verbs are in the 3rd person masculine singular, past tense. The vowel <ä> is the most frequent vowel in the language.\n\nInternal features that can be found quite frequently include:\n\nTo make the respective statement more general most proverbs are based on a metaphor. Further typical features of the proverb are its shortness and the fact that its author is generally unknown.\nIn the article \"Tensions in Proverbs: More Light on International Understanding\", Joseph Raymond comments on what common Russian proverbs from the 18th and 19th centuries portray: Potent antiauthoritarian proverbs reflected tensions between the Russian people and the Czar. The rollickingly malicious undertone of these folk verbalizations constitutes what might be labeled a \"paremiological revolt\". To avoid openly criticizing a given authority or cultural pattern, folk take recourse to proverbial expressions which voice personal tensions in a tone of generalized consent. Proverbs that speak to the political disgruntlement include: \"When the Czar spits into the soup dish, it fairly bursts with pride\"; \"If the Czar be a rhymester, woe be to the poets\"; and \"The hen of the Czarina herself does not lay swan's eggs\". While none of these proverbs state directly, \"I hate the Czar and detest my situation\" (which would have been incredibly dangerous), they do get their points across.\n\nProverbs are found in many parts of the world, but some areas seem to have richer stores of proverbs than others (such as West Africa), while others have hardly any (North and South America) (Mieder 2004b:108,109).\n\nProverbs are used by speakers for a variety of purposes. Sometimes they are used as a way of saying something gently, in a veiled way (Obeng 1996). Other times, they are used to carry more weight in a discussion, a weak person is able to enlist the tradition of the ancestors to support his position, or even to argue a legal case. Proverbs can also be used to simply make a conversation/discussion more lively. In many parts of the world, the use of proverbs is a mark of being a good orator.\n\nThe study of proverbs has application in a number of fields. Clearly, those who study folklore and literature are interested in them, but scholars from a variety of fields have found ways to profitably incorporate the study proverbs. For example, they have been used to study abstract reasoning of children, acculturation of immigrants, intelligence, the differing mental processes in mental illness, cultural themes, etc. Proverbs have also been incorporated into the strategies of social workers, teachers, preachers, and even politicians. (For the deliberate use of proverbs as a propaganda tool by Nazis, see Mieder 1982.)\n\nThere are collections of sayings that offer instructions on how to play certain games, such as dominoes (Borajo \"et al.\" 1990) and the Oriental board game go (Mitchell 2001). However, these are not prototypical proverbs in that their application is limited to one domain.\n\nOne of the most important developments in the study of proverbs (as in folklore scholarship more generally) was the shift to more ethnographic approaches in the 1960s. This approach attempted to explain proverb use in relation to the context of a speech event, rather than only in terms of the content and meaning of the proverb.\n\nAnother important development in scholarship on proverbs has been applying methods from cognitive science to understand the uses and effects of proverbs and proverbial metaphors in social relations.\n\n"}
{"id": "31103500", "url": "https://en.wikipedia.org/wiki?curid=31103500", "title": "Reasoning system", "text": "Reasoning system\n\nIn information technology a reasoning system is a software system that generates conclusions from available knowledge using logical techniques such as deduction and induction. Reasoning systems play an important role in the implementation of artificial intelligence and knowledge-based systems.\n\nBy the everyday usage definition of the phrase, all computer systems are reasoning systems in that they all automate some type of logic or decision. In typical use in the Information Technology field however, the phrase is usually reserved for systems that perform more complex kinds of reasoning. For example, not for systems that do fairly straightforward types of reasoning such as calculating a sales tax or customer discount but making logical inferences about a medical diagnosis or mathematical theorem. Reasoning systems come in two modes: interactive and batch processing. Interactive systems interface with the user to ask clarifying questions or otherwise allow the user to guide the reasoning process. Batch systems take in all the available information at once and generate the best answer possible without user feedback or guidance.\n\nReasoning systems have a wide field of application that includes scheduling, business rule processing, problem solving, complex event processing, intrusion detection, predictive analytics, robotics, computer vision, and natural language processing.\n\nThe first reasoning systems were theorem provers, systems that represent axioms and statements in First Order Logic and then use rules of logic such as modus ponens to infer new statements. Another early type of reasoning system were general problem solvers. These were systems such as the General Problem Solver designed by Newell and Simon. General problem solvers attempted to provide a generic planning engine that could represent and solve structured problems. They worked by decomposing problems into smaller more manageable sub-problems, solving each sub-problem and assembling the partial answers into one final answer. Another example general problem solver was the SOAR family of systems.\n\nIn practice these theorem provers and general problem solvers were seldom useful for practical applications and required specialized users with knowledge of logic to utilize. The first practical application of automated reasoning were expert systems. Expert systems focused on much more well defined domains than general problem solving such as medical diagnosis or analyzing faults in an aircraft. Expert systems also focused on more limited implementations of logic. Rather than attempting to implement the full range of logical expressions they typically focused on modus-ponens implemented via IF-THEN rules. Focusing on a specific domain and allowing only a restricted subset of logic improved the performance of such systems so that they were practical for use in the real world and not merely as research demonstrations as most previous automated reasoning systems had been. The engine used for automated reasoning in expert systems were typically called inference engines. Those used for more general logical inferencing are typically called theorem provers.\n\nWith the rise in popularity of expert systems many new types of automated reasoning were applied to diverse problems in government and industry. Some such as case-based reasoning were off shoots of expert systems research. Others such as constraint satisfaction algorithms were also influenced by fields such as decision technology and linear programming. Also, a completely different approach, one not based on symbolic reasoning but on a connectionist model has also been extremely productive. This latter type of automated reasoning is especially well suited to pattern matching and signal detection types of problems such as text searching and face matching.\n\nThe term reasoning system can be used to apply to just about any kind of sophisticated decision system as illustrated by the specific areas described below. However, the most common use of the term reasoning system implies the computer representation of logic. Various implementations demonstrate significant variation in terms of systems of logic and formality. Most reasoning systems implement variations of propositional and symbolic (predicate) logic. These variations may be mathematically precise representations of formal logic systems (e.g., FOL), or extended and hybrid versions of those systems (e.g., Courteous logic). Reasoning systems may explicitly implement additional logic types (e.g., modal, deontic, temporal logics). However, many reasoning systems implement imprecise and semi-formal approximations to recognised logic systems. These systems typically support a variety of procedural and semi-declarative techniques in order to model different reasoning strategies. They emphasise pragmatism over formality and may depend on custom extensions and attachments in order to solve real-world problems.\n\nMany reasoning systems employ deductive reasoning to draw inferences from available knowledge. These inference engines support forward reasoning or backward reasoning to infer conclusions via modus ponens. The recursive reasoning methods they employ are termed ‘forward chaining’ and ‘backward chaining’, respectively. Although reasoning systems widely support deductive inference, some systems employ abductive, inductive, defeasible and other types of reasoning. Heuristics may also be employed to determine acceptable solutions to intractable problems.\n\nReasoning systems may employ the closed world assumption (CWA) or open world assumption (OWA). The OWA is often associated with ontological knowledge representation and the Semantic Web. Different systems exhibit a variety of approaches to negation. As well as logical or bitwise complement, systems may support existential forms of strong and weak negation including negation-as-failure and ‘inflationary’ negation (negation of non-ground atoms). Different reasoning systems may support monotonic or non-monotonic reasoning, stratification and other logical techniques.\n\nMany reasoning systems provide capabilities for reasoning under uncertainty. This is important when building situated reasoning agents which must deal with uncertain representations of the world. There are several common approaches to handling uncertainty. These include the use of certainty factors, probabilistic methods such as Bayesian inference or Dempster–Shafer theory, multi-valued (‘fuzzy’) logic and various connectionist approaches.\n\nThis section provides a non-exhaustive and informal categorisation of common types of reasoning system. These categories are not absolute. They overlap to a significant degree and share a number of techniques, methods and algorithms.\n\nConstraint solvers solve constraint satisfaction problems (CSPs). They support constraint programming. A constraint is a condition which must be met by any valid solution to a problem. Constraints are defined declaratively and applied to variables within given domains. Constraint solvers use search, backtracking and constraint propagation techniques to find solutions and determine optimal solutions. They may employ forms of linear and nonlinear programming. They are often used to perform optimization within highly combinatorial problem spaces. For example, they may be used to calculate optimal scheduling, design efficient integrated circuits or maximise productivity in a manufacturing process.\n\nTheorem provers use automated reasoning techniques to determine proofs of mathematical theorems. They may also be used to verify existing proofs. In addition to academic use, typical applications of theorem provers include verification of the correctness of integrated circuits, software programs, engineering designs, etc.\n\nLogic programs (LPs) are software programs written using programming languages whose primitives and expressions provide direct representations of constructs drawn from mathematical logic. An example of a general-purpose logic programming language is Prolog. LPs represent the direct application of logic programming to solve problems. Logic programming is characterised by highly declarative approaches based on formal logic, and has wide application across many disciplines.\n\nRule engines represent conditional logic as discrete rules. Rule sets can be managed and applied separately to other functionality. They have wide applicability across many domains. Many rule engines implement reasoning capabilities. A common approach is to implement production systems to support forward or backward chaining. Each rule (‘production’) binds a conjunction of predicate clauses to a list of executable actions. At run-time, the rule engine matches productions against facts and executes (‘fires’) the associated action list for each match. If those actions remove or modify any facts, or assert new facts, the engine immediately re-computes the set of matches. Rule engines are widely used to model and apply business rules, to control decision-making in automated processes and to enforce business and technical policies.\n\nDeductive classifiers arose slightly later than rule-based systems and were a component of a new type of artificial intelligence knowledge representation tool known as frame languages. A frame language describes the problem domain as a set of classes, subclasses, and relations among the classes. It is similar to the object-oriented model. Unlike object-oriented models however, frame languages have a formal semantics based on first order logic. They utilize this semantics to provide input to the deductive classifier. The classifier in turn can analyze a given model (known as an ontology) and determine if the various relations described in the model are consistent. If the ontology is not consistent the classifier will highlight the declarations that are inconsistent. If the ontology is consistent the classifier can then do further reasoning and draw additional conclusions about the relations of the objects in the ontology. For example, it may determine that an object is actually a subclass or instance of additional classes as those described by the user. Classifiers are an important technology in analyzing the ontologies used to describe models in the Semantic web.\nMachine learning systems evolve their behavior over time based on experience. This may involve reasoning over observed events or example data provided for training purposes. For example, machine learning systems may use inductive reasoning to generate hypotheses for observed facts. Learning systems search for generalised rules or functions that yield results in line with observations and then use these generalisations to control future behavior.\n\nCase-based reasoning (CBR) systems provide solutions to problems by analysing similarities to other problems for which known solutions already exist. They use analogical reasoning to infer solutions based on case histories. CBR systems are commonly used in customer/technical support and call centre scenarios and have applications in industrial manufacture, agriculture, medicine, law and many other areas.\n\nA procedural reasoning system (PRS) uses reasoning techniques to select plans from a procedural knowledge base. Each plan represents a course of action for achievement of a given goal. The PRS implements a belief-desire-intention model by reasoning over facts (‘beliefs’) to select appropriate plans (‘intentions’) for given goals (‘desires’). Typical applications of PRS include management, monitoring and fault detection systems.\n"}
{"id": "43372083", "url": "https://en.wikipedia.org/wiki?curid=43372083", "title": "Repair café", "text": "Repair café\n\nA repair café is a meeting in which people repair household electrical and mechanical devices, computers, bicycles, clothing, etc. They are organised by and for local residents. Repair cafés are held at a fixed location where tools are available and where they can fix their broken goods with the help of volunteers. Its objectives are to reduce waste, to maintain repair skills and to strengthen social cohesion.\n\nThe concept was devised by Martine Postma in 2009. On 18 October 2009, the first Repair Café was held at Fijnhout Theater, Amsterdam-West. On 2 March 2010, the Repair Café Foundation was set up. The foundation was formed to support local groups around the world in setting up their own Repair Cafés. Since then, the number of Repair Cafés has grown quickly. In March 2016 Postma registered more than 1,000 Repair Cafés worldwide, 327 in the Netherlands, 309 in Germany, 22 in the UK, 21 in the US, 15 in Canada, four Australia and one in India. In January 2017 the number of Repair Cafés climbed over 1,200, in March 2018 the number reached 1,500 in 33 countries.\n\nIn 2017, the first International Repair Day was announced. It is intended to be an annual event, taking place on the third Saturday of October each year.\n\nIn 2017, the Repair Café Foundation developed an online tool - the RepairMonitor - enabling volunteers to collect and share repair data via a central database. In March 2018, information about almost 4,000 repairs had been entered into this system, aiming to promote reparability and durability of products. \n\nSome repair cafés have begun to use 3D printers for replicating broken parts. Broken pieces of domestic appliances may be able to be pieced or glued back together, after which they can be scanned with a 3D scanner. Examples of 3D scanners include David Starter-Kit, 3D Systems Sense, MakerBot Digitizer, Fuel 3D, Microsoft Kinect, and Asus Xtion. Once the physical object is scanned, the 3D model is rendered. It can be converted to a .stl or .obj format and revised using geometry processing software such as makeprintable, netfabb, MeshLab, Meshmixer, Cura, or Slic3r. It is printed using a 3D printer client, creating a physical part using the 3D printer. The complete process takes some time to complete.\n\nTo reduce the time needed in the repair café, people might choose to use a pre-made part from a website with 3D models (skipping the scanning step), or make the 3D model themselves by taking many photographs of the part and using something like 123D Catch, and/or choose (in the event the repair café does not have its own 3D printer) to have the 3D \"model\" made in the repair café, but printed using a 3D printer elsewhere. Alternatively a 3D printing service like Ponoko, Shapeways, and others can be used, and a person then return to the repair café to have the new part fitted to the broken equipment.\n\n\n"}
{"id": "161975", "url": "https://en.wikipedia.org/wiki?curid=161975", "title": "Responsibility", "text": "Responsibility\n\nResponsibility may refer to:\n\n"}
{"id": "20252921", "url": "https://en.wikipedia.org/wiki?curid=20252921", "title": "Silver lining (idiom)", "text": "Silver lining (idiom)\n\nA silver lining is a metaphor for optimism in the common English-language which means a negative occurrence may have a positive aspect to it.\n\nThe origin of the phrase is most likely traced to John Milton's \"Comus\" (1634) with the lines,\n"}
{"id": "939680", "url": "https://en.wikipedia.org/wiki?curid=939680", "title": "Single-elimination tournament", "text": "Single-elimination tournament\n\nA single-elimination, knockout, or sudden death tournament is a type of elimination tournament where the loser of each match-up is immediately eliminated from the tournament. Each winner will play another in the next round, until the final match-up, whose winner becomes the tournament champion. Each match-up may be a single match or several, for example two-legged ties in European football or best-of series in American pro sports. Defeated competitors may play no further part after losing, or may participate in \"consolation\" or \"classification\" matches against other losers to determine the lower final rankings; for example, a third place playoff between losing semi-finalists. In a shootout poker tournament, there are more than two players competing at each table, and sometimes more than one progressing to the next round. Some competitions are held with a pure single-elimination tournament system. Others have many phases, with the last being a single-elimination final stage, often called playoffs.\n\nIn English, the round in which only eight competitors remain is generally called (with or without hyphenation) the \"quarter-final\" round; this is followed by the \"semi-final\" round, in which only four are left, the two winners of which then meet in the final or \"championship round\".\n\nThe round before the quarterfinals has multiple designations. Often it's called the \"round of sixteen\", \"last sixteen\", or (in South Asia) \"pre quarter-finals\". In many other languages the term used to describe these eight matches translates to \"eighth-final\", though this term is rare in English itself.\n\nEarlier rounds are typically numbered counting forwards from the first round, or by the number of remaining competitors. If some competitors get a bye, the round at which they enter may be named the \"first round\", with the earlier matches called a \"preliminary round\", \"qualifying round\", or the \"play-in games\"\".\n\nExamples of the diverse names given to concurrent rounds in various select disciplines:\n\nNotes:\nThe final three rounds of the 2014 Australian Open – Women's Singles knock-out tournament:\n\nWhen matches are held to determine places or prizes lower than first and second (the loser of the final-round match gaining the latter position), these typically include a match between the losers of the semifinal matches called third place playoffs, the winner therein placing third and the loser fourth. Many Olympic single-elimination tournaments feature the bronze medal match if they do not award bronze medals to both losing semifinalists. The FIFA World Cup has long featured the third place match, though the UEFA Euro has not held one since the 1980 edition.\n\nSometimes, contests are also held among the losers of the quarterfinal matches to determine fifth to eighth places – this is most commonly encountered in the Olympic Games, with the exception of boxing, where both fighters are deemed to be third place. In one scenario, two \"consolation semifinal\" matches may be conducted, with the winners of these then facing off to determine fifth and sixth places and the losers playing for seventh and eighth; those are used often in qualifying tournaments where only the top five teams advance to the next round; or some method of ranking the four quarterfinal losers might be employed, in which case only one round of additional matches would be held among them, the two highest-ranked therein then playing for fifth and sixth places and the two lowest for seventh and eighth.\n\nThe number of distinct ways of arranging a single-elimination tournament (as an abstract structure, prior to seeding the players into the tournament) is given by the Wedderburn–Etherington numbers. Thus, for instance, there are three different arrangements for five players:\nHowever, the number of arrangements grows quickly for larger numbers of players and not all of them are commonly used.\n\nOpponents may be allocated randomly (such as in the FA Cup); however, since the \"luck of the draw\" may result in the highest-rated competitors being scheduled to face each other early in the competition, \"seeding\" is often used to prevent this. Brackets are set up so that the top two seeds could not possibly meet until the final round (should both advance that far), none of the top four can meet prior to the semifinals, and so on. If no seeding is used, the tournament is called a random knockout tournament.\n\nOne version of seeding is where brackets are set up so that the quarterfinal pairings (barring any upsets) would be the 1 seed vs. the 8 seed, 2 vs. 7, 3 vs. 6 and 4 vs. 5; however, this is not the procedure that is followed in most tennis tournaments, where the 1 and 2 seeds are placed in separate brackets, but then the 3 and 4 seeds are assigned to their brackets randomly, and so too are seeds 5 through 8, and so on. This may result in some brackets consisting of stronger players than other brackets, and since only the top 32 players are seeded at all in Tennis Grand Slam tournaments, it is conceivable that the 33rd-best player in a 128-player field could end up playing the top seed in the first round. A good example of this occurring was when World No. 33 Florian Mayer was drawn against (and eventually defeated by) then-World No. 1 Novak Djokovic in the first round of the 2013 Wimbledon Championships, in what was also a rematch of a quarterfinal from the previous year. While this may seem unfair to a casual observer, it should be pointed out that rankings of tennis players are generated by computers, and players tend to change ranking positions very gradually, so that a more equitable method of determining the pairings might result in many of the same head-to-head matchups being repeated over and over again in successive tournaments.\n\nSometimes the remaining competitors in a single-elimination tournament will be \"re-seeded\" so that the highest surviving seed is made to play the lowest surviving seed in the next round, the second-highest plays the second-lowest, etc. This may be done after each round, or only at selected intervals. In American team sports, for example, the MLS, NFL and WNBA employ this tactic, but the NBA does not (and neither does the NCAA college basketball tournament). MLB does not have enough teams (10) in its playoff tournament where re-seeding would make a large difference in the matchups; (MLS and NFL are at the minimum, which is six from each conference [or league in MLB] for a total of 12). The NBA's format calls for the winner of the first-round series between the first and eighth seeds (within each of the two conferences the league has) to face the winner of the first-round series between the fourth and fifth seeds in the next round, even if one or more of the top three seeds had been upset in their first-round series; critics have claimed that this gives a team fighting for the fifth and sixth seeding positions near the end of the regular season an incentive to tank (deliberately lose) games, so as to finish sixth and thus avoid a possible matchup with the top seed until one round later.\n\nIn some situations, a seeding restriction may be implemented; from 1975 until 1989 in the NFL, and from 1994 until 2011 in MLB there was a rule where at the conference or league semifinal, should the top seed and last seed (wild card) be from the same division, they could not play each other; in that case, the top seed plays the worst division champion; the second-best division champion plays the wild card team. This is due to the scheduling employed for the regular season, in which a team faces any given divisional opponent more often than any given non-divisional opponent – the tournament favors matchups that took place fewer times in the regular season (or did not take place, in some cases). \n\nIn international fencing competitions, it is common to have a group stage. Participants are divided in groups of 6–7 fencers who play a round-robin tournament, and a ranking is calculated from the consolidated group results. Single elimination is seeded from this ranking.\n\nThe single-elimination format enables a relatively large number of competitors to participate. There are no \"dead\" matches (perhaps excluding \"classification\" matches), and no matches where one competitor has more to play for than the other.\n\nThe format is less suited to games where draws are frequent. In chess, each fixture in a single-elimination tournament must be played over multiple matches, because draws are common, and because white has an advantage over black. In association football, games ending in a draw may be settled in extra time and eventually by a penalty shootout or by replaying the fixture.\n\nAnother perceived disadvantage is that most competitors are eliminated after relatively few games. Variations such as the double-elimination tournament allow competitors a single loss while remaining eligible for overall victory.\n\nIn a random knockout tournament (single-elimination without any seeding), awarding the second place to the loser of the final is unjustified: any of the competitors knocked out by the tournament winner might have been the second strongest one, but they never got the chance to play against the losing finalist. In general, it is only fair to use a single-elimination tournament to determine first place. To fairly determine lower places requires some form of round-robin in which each player/team gets the opportunity to face every other player/team.\n\nAlso, if the competitors' performance is variable, that is, it depends on a small, varying factor in addition to the actual strength of the competitors, then not only will it become less likely that the strongest competitor actually wins the tournament, in addition the seeding done by the tournament organizers will play a major part in deciding the winner. As a random factor is always present in a real-world competition, this might easily cause accusations of unfairness.\n\nVariations of the single-elimination tournament include:\nOther common tournament types include:\n"}
{"id": "6139533", "url": "https://en.wikipedia.org/wiki?curid=6139533", "title": "Slip ratio", "text": "Slip ratio\n\nSlip ratio is a means of calculating and expressing the slipping behavior of the wheel of an automobile. It is of fundamental importance in the field of vehicle dynamics, as it allows to understand the relationship between the deformation of the tire and the longitudinal forces (i.e. the forces responsible for forward acceleration and braking) acting upon it. Furthermore, it is essential to the effectiveness of any anti-lock braking system. \n\nWhen accelerating or braking a vehicle equipped with tires, the observed angular velocity of the tire does not match the expected velocity for pure rolling motion, which means there appears to be apparent sliding between outer surface of the rim and the road in addition to rolling due to deformation of the part of tire above the area in contact with the road. When driving on dry pavement the fraction of slip that is caused by actual sliding taking place between road and tire contact patch is negligible in magnitude and thus does not in practice make slip ratio dependent on speed. It is only relevant in soft or slippery surfaces, like snow, mud, ice, etc and results constant speed difference in same road and load conditions independently of speed, and thus fraction of slip ratio due to that cause is inversely related to speed of the vehicle. The difference between theoretically calculated forward speed based on angular speed of the rim and rolling radius, and actual speed of the vehicle, expressed as a percentage of the latter, is called ‘slip ratio’. This slippage is caused by the forces at the contact patch of the tire, not the opposite way, and is thus of fundamental importance to determine the accelerations a vehicle can produce.\n\nThere is no universally agreed upon definition of slip ratio. The SAE J670 definition is, for tires pointing straight ahead:\n\nformula_1\n\nWhere Ω is the angular velocity of the wheel, R is the effective radius of the corresponding free-rolling tire, which can be calculated from the revolutions per kilometer, and V is the forward velocity of the vehicle.\n"}
{"id": "48501922", "url": "https://en.wikipedia.org/wiki?curid=48501922", "title": "Society for the Experimental Analysis of Behavior", "text": "Society for the Experimental Analysis of Behavior\n\nThe Society for the Experimental Analysis of Behavior was founded in 1957 by a group of researchers in the field of behaviorism. It publishes the \"Journal of the Experimental Analysis of Behavior\" and the \"Journal of Applied Behavior Analysis\".\n\nThe Certificate of Incorporation (dated October 29, 1957) of the society states that:\n\nThe \"Journal of the Experimental Analysis of Behavior\" was established to meet the needs of those who were attracted to the behavior-analytic approach but were unhappy with the lack of a journal specializing in that rapidly growing area. As described on its inside front page ever since, the journal is \"primarily for the original publication of experiments relevant to the behavior of individual organisms.\" It started as a quarterly in 1958 but has appeared bimonthly since 1964. The initial Board of Editors also served as the first Board of Directors of the society.\n\nIn 1968, the society established the \"Journal of Applied Behavior Analysis\" for \"the original publication of reports of experimental research involving applications of the experimental analysis of behavior to problems of social importance.\" It appears quarterly.\n"}
{"id": "638664", "url": "https://en.wikipedia.org/wiki?curid=638664", "title": "Strasserism", "text": "Strasserism\n\nStrasserism ( or \"Straßerismus\") is a strand of Nazism that calls for a more radical, mass-action and worker-based form of Nazism—hostile to Jews not from a racial, ethnic, cultural or religious perspective, but from an anti-capitalist basis—to achieve a national rebirth. It derives its name from Gregor and Otto Strasser, the two Nazi brothers initially associated with this position. \n\nOpposed on strategic views to Adolf Hitler, Otto Strasser was expelled from the Nazi Party in 1930 and went into exile in Czechoslovakia while Gregor Strasser was murdered in Germany on 30 June 1934 during the Night of the Long Knives. Strasserism remains an active position within strands of neo-Nazism.\n\nGregor Strasser (1892–1934) began his career in ultranationalist politics by joining the \"Freikorps\" after serving in World War I. Strasser was involved in the Kapp Putsch and formed his own \"völkischer Wehrverband\" (\"popular defense union\") which he merged into the NSDAP in 1921. Initially a loyal supporter of Adolf Hitler, he took part in the Beer Hall Putsch and held a number of high positions in the Nazi Party. However, Strasser soon became a strong advocate of the socialist wing of the party, arguing that the national revolution should also include strong action to tackle poverty and should seek to build working class support. After Hitler's rise to power, Ernst Röhm, who headed the \"Sturmabteilung\" (SA), then the most important paramilitary wing of the Nazi Party, called for a second revolution aimed at removing the elites from control. This was opposed by the conservative movement as well as by some Nazis who preferred an ordered authoritarian regime to the radical and disruptive program proposed by the party's left-wing.\n\nOtto Strasser (1897–1974) had also been a member of the \"Freikorps\", but he joined the Social Democratic Party and fought against the Kapp Putsch. Strasser joined the Nazi Party in 1925, but he nonetheless retained his ideas about the importance of socialism. Considered more of a radical than his brother, Strasser was expelled by the Nazi Party in 1930 and set up his own dissident group, the Black Front, which called for a specifically German nationalist form of socialist revolution. Strasser fled Germany in 1933 to live firstly in Czechoslovakia and then Canada before returning to West Germany in later life, all the while writing prolifically about Hitler and what he saw as his betrayal of national socialist ideals.\n\nThe name Strasserism came to be applied to the form of Nazism that developed around the Strasser brothers. Although they had been involved in the creation of the National Socialist Program of 1920, both called on the party to commit to \"breaking the shackles of finance capital\". This opposition to Jewish finance capitalism, which they contrasted to \"productive capitalism”, was shared by Hitler himself, who borrowed it from Gottfried Feder.\n\nThis populist and antisemitic form of anti-capitalism was further developed in 1925 when Otto Strasser published the \"Nationalsozialistische Briefe\", which discussed notions of class conflict, wealth redistribution and a possible alliance with the Soviet Union. His 1930 follow-up \"Ministersessel oder Revolution\" (\"Cabinet Seat or Revolution\") went further by attacking Hitler's betrayal of the socialist aspect of Nazism as well as criticizing the notion of the \"Führerprinzip\". Whilst Gregor Strasser echoed many of the calls of his brother, his influence on the ideology is less, owing to his remaining in the Nazi Party longer and to his early death. Meanwhile, Otto Strasser continued to expand his argument, calling for the break-up of large estates and the development of something akin to a guild system and the related establishment of a \"Reich\" cooperative chamber to take a leading role in economic planning.\n\nStrasserism therefore became a distinct strand of Nazism that whilst holding on to previous Nazi ideals such as palingenetic ultranationalism and antisemitism, added a strong critique of capitalism and framed this in the demand for a more socialist-based approach to economics.\n\nHowever, it is disputed whether Strasserism was a distinct form of Nazism. According to historian Ian Kershaw, \"the leaders of the SA [which included Gregor Strasser] did not have another vision of the future of Germany or another politic to propose\". The Strasserites advocated the radicalization of the Nazi regime and the toppling of the German elites, calling Hitler's rise to power a half-revolution which needed to be completed.\n\nDuring the 1970s, the ideas of Strasserism began to be referred to more in European far-right groups as younger members with no ties to Hitler and a stronger sense of anti-capitalism came to the fore. Strasserite thought in Germany began to emerge as a tendency within the National Democratic Party (NPD) during the late 1960s. These Strasserites played a leading role in securing the removal of Adolf von Thadden from the leadership and after his departure the party became stronger in condemning Hitler for what it saw as his move away from socialism in order to court business and army leaders.\n\nAlthough initially adopted by the NPD, Strasserism soon became associated with more peripheral extremist figures, notably Michael Kühnen who produced a 1982 pamphlet \"Farewell to Hitler\", which included a strong endorsement of the idea. The People's Socialist Movement of Germany/Labour Party (a minor extremist movement that was outlawed in 1982) adopted the policy while its successor movement, the Nationalist Front, did likewise, with its ten-point programme calling for an \"anti-materialist cultural revolution\" and an \"anti-capitalist social revolution\" to underline its support for the idea. The Free German Workers' Party also moved towards these ideas under the leadership of Friedhelm Busse in the late 1980s. \n\nThe flag of the Strasserite movement Black Front and its symbol a crossed hammer and a sword has been used by German and other European neo-Nazis abroad as a substitute for the more infamous Nazi flag which is banned in some countries such as Germany.\n\nStrasserism emerged in the United Kingdom in the early 1970s and centred on the National Front (NF) publication \"Britain First\", the main writers of which were David McCalden, Richard Lawson and Denis Pirie. Opposing the leadership of John Tyndall, they formed an alliance with John Kingsley Read and ultimately followed him into the National Party (NP). The NP called for British workers to seize the right to work and offered a fairly Strasserite economic policy. Nonetheless, the NP failed to last for very long. Due in part to Read's lack of enthusiasm for Strasserism, the main exponents of the idea drifted away.\n\nThe idea was reintroduced to the NF by Andrew Brons in the early 1980s when he decided to make the party's ideology clearer. However, Strasserism was soon to become the province of the radicals in the Official National Front, with Richard Lawson brought in a behind-the-scenes role to help direct policy. This Political Soldier wing ultimately opted for the indigenous alternative of distributism, but their strong anti-capitalist rhetoric as well as that of their International Third Position successor demonstrated influences from Strasserism. From this background, Troy Southgate emerged, whose own ideology and those of related groups such as the English Nationalist Movement and National Revolutionary Faction were influenced by Strasserism. He has also described himself as a post-Strasserite.\n\nThird Position groups, whose inspiration is generally more Italian in derivation, have often looked to Strasserism, owing to their strong opposition to capitalism. This was noted in France, where the student group \"Groupe Union Défense\" and the more recent \"Renouveau français\" both extolled Strasserite economic platforms.\n\nAttempts to reinterpret Nazism as having a left-wing base have also been heavily influenced by this school of thought, notably through the work of Povl Riis-Knudsen, who produced the Strasser-influenced work \"National Socialism: A Left-Wing Movement\" in 1984.\n\nIn the United States, Tom Metzger also flirted with Strasserism, having been influenced by Kühnen's pamphlet.\n\n\n"}
{"id": "29289445", "url": "https://en.wikipedia.org/wiki?curid=29289445", "title": "The Big Issue (website)", "text": "The Big Issue (website)\n\nThe Big Issue is a web documentary on the obesity epidemic, directed by Samuel Bollendorff and Olivia Colo. In this interactive documentary the user goes on a personal journey into the causes of the explosive increase in obesity in contemporary society.\n\n\n\n"}
{"id": "31976311", "url": "https://en.wikipedia.org/wiki?curid=31976311", "title": "Three poisons", "text": "Three poisons\n\nThe three poisons (Sanskrit: \"triviṣa\"; Tibetan: \"dug gsum\") or the three unwholesome roots (Sanskrit: \"akuśala-mūla\"; Pāli: \"akusala-mūla\"), in Buddhism, refer to the three root kleshas of \"Moha\" (delusion, confusion), \"Raga\" (greed, sensual attachment), and \"Dvesha\" (aversion, ill will). These three poisons are considered to be three afflictions or character flaws innate in a being, the root of \"Taṇhā\" (craving), and thus in part the cause of \"Dukkha\" (suffering, pain, unsatisfactoriness) and rebirths.\n\nThe three poisons are symbolically drawn at the center of Buddhist \"Bhavachakra\" artwork, with rooster, snake and pig, representing greed, ill will and delusion respectively.\n\nIn the Buddhist teachings, the three poisons (of ignorance, attachment, and aversion) are the primary causes that keep sentient beings trapped in samsara. These three poisons are said to be the root of all of the other kleshas.\n\nThe three poisons are represented in the hub of the wheel of life as a pig, a bird, and a snake (representing ignorance, attachment, and aversion, respectively). As shown in the wheel of life (Sanskrit: \"bhavacakra\"), the three poisons lead to the creation of karma, which leads to rebirth in the six realms of samsara.\nThe three wholesome mental factors that are identified as the opposites of the three poisons are:\n\nBuddhist path considers these essential for liberation.\n\nThe three kleshas of ignorance, attachment and aversion are referred to as the \"three poisons\" (Skt. \"triviṣa\"; Tibetan: \"dug gsum\") in the Mahayana tradition and as the \"three unwholesome roots\" (Pāli, \"akusala-mūla\"; Skt. \"akuśala-mūla\" ) in the Theravada tradition.\n\nThe Sanskrit, Pali, and Tibetan terms for each of the three poisons are as follows:\nIn the Mahayana tradition \"moha\" is identified as a subcategory of \"avidya\". Whereas \"avidya\" is defined as a fundamental ignorance, \"moha\" is defined as delusion, confusion and incorrect beliefs. In the Theravada tradition, \"moha\" and \"avidya\" are equivalent terms, but they are used in different contexts; \"moha\" is used when referring to mental factors, and \"avidya\" is used when referring to the twelve links.\n\n\n\n\n"}
{"id": "28867511", "url": "https://en.wikipedia.org/wiki?curid=28867511", "title": "Trance", "text": "Trance\n\nTrance is an abnormal state of wakefulness in which a person is not self-aware and is either altogether unresponsive to external stimuli but is nevertheless capable of pursuing and realizing an aim, or is selectively responsive in following the directions of the person who has induced the trance. Trance states may occur involuntarily and unbidden.\n\nThe term \"trance\" may be associated with hypnosis, meditation, magic, flow, and prayer. It may also be related to the earlier generic term, altered states of consciousness, which is no longer used in \"consciousness studies\" discourse.\n\nTrance in its modern meaning comes from an earlier meaning of \"a dazed, half-conscious or insensible condition or state of fear\", via the Old French \"transe\" \"fear of evil\", from the Latin \"transīre\" \"to cross\", \"pass over\". This definition is now obsolete.\n\nWier, in his 1995 book, \"Trance: from magic to technology\", defines a simple trance (p. 58) as a state of mind being caused by cognitive loops where a cognitive object (thoughts, images, sounds, intentional actions) repeats long enough to result in various sets of disabled cognitive functions. Wier represents all trances (which include sleep and watching television) as taking place on a dissociated trance plane where at least some cognitive functions such as volition are disabled; as is seen in what is typically termed a 'hypnotic trance'. With this definition, meditation, hypnosis, addictions and charisma are seen as being trance states. In Wier's 2007 book, \"The Way of Trance\", he elaborates on these forms, adds ecstasy as an additional form and discusses the ethical implications of his model, including magic and government use which he terms \"trance abuse\".\n\nJohn Horgan in \"Rational Mysticism\" (2003) explores the neurological mechanisms and psychological implications of trances and other mystical manifestations. Horgan incorporates literature and case-studies from a number of disciplines in this work: chemistry, physics, psychology, radiology and theology.\n\nThe following are some examples of trance states:\n\nTrance conditions include all the different states of mind, emotions, moods and daydreams that human beings experience. All activities which engage a human involve the filtering of information coming into sense modalities, and this influences brain functioning and consciousness. Therefore, trance may be understood as a way for the mind to change the way it filters information in order to provide more efficient use of the mind's resources.\n\nTrance states may also be accessed or induced by various modalities and is a way of accessing the unconscious mind for the purposes of relaxation, healing, intuition and inspiration. There is an extensive documented history of trance as evidenced by the case-studies of anthropologists and ethnologists and associated and derivative disciplines. Hence trance may be perceived as endemic to the human condition and a Human Universal. Principles of trance are being explored and documented as are methods of trance induction. Benefits of trance states are being explored by medical and scientific inquiry. Many traditions and rituals employ trance. Trance also has a function in religion and mystical experience.\n\nCastillo (1995) states that: \"Trance phenomena result from the behavior of intense focusing of attention, which is the key psychological mechanism of trance induction. Adaptive responses, including institutionalized forms of trance, are 'tuned' into neural networks in the brain and depend to a large extent on the characteristics of culture. Culture-specific organizations exist in the structure of individual neurons and in the organizational formation of neural networks.\"\n\nHoffman (1998: p. 9) states that: \"Trance is still conventionally defined as a state of reduced consciousness, or a somnolent state. However, the more recent anthropological definition, linking it to 'altered states of consciousness' (Charles Tart), is becoming increasingly accepted.\"\n\nHoffman (1998, p. 9) asserts that: \"...the trance state should be discussed in the plural, because there is more than one altered state of consciousness significantly different from everyday consciousness.\"\n\nAccording to Hoffman (1998: p. 10), pilgrims visited the Temple of Epidaurus, an asclepeion, in Greece for healing sleep. Seekers of healing would make pilgrimage and be received by a priest who would welcome and bless them. This temple housed an ancient religious ritual promoting dreams in the seeker that endeavored to promote healing and the solutions to problems, as did the oracles. This temple was built in honor of Asclepios, the Greek god of medicine. The Greek treatment was referred to as incubation, and focused on prayers to Asclepios for healing. The asclepion at Epidaurus is both extensive and well-preserved, and is traditionally regarded as the birthplace of Asclepius. (For a comparable modern tool see Dreamwork.)\n\nThe Oracle at Delphi was also famous for trances in the ancient Greek world; priestesses there would make predictions about the future in exchange for gold.\n\nStories of the saints in the Middle Ages, myths, parables, fairy tales, oral lore and storytelling from different cultures are themselves potentially inducers of trance. Often literary devices such as repetition are employed which is evident in many forms of trance induction. Milton Erickson used stories to induce trance as do many NLP practitioners.\n\nFrom at least the 16th century it was held that march music may induce soldiers marching in unison into trance states where according to apologists, they bond together as a unit engendered by the rigors of training, the ties of comradeship and the chain of command. This had the effect of making the soldiers become automated, an effect which was widely evident in the 16th, 17th and 18th century due to the increasing prevalence of firearms employed in warcraft. Military instruments, especially the snare drum and other drums were used to entone a monotonous ostinato at the pace of march and heartbeat. High-pitched fifes, flutes and bagpipes were used for their \"piercing\" effect to play the melody. This would assist the morale and solidarity of soldiers as they marched to battle.\n\nJoseph Jordania recently proposed a term battle trance for this mental state, when combatants do not feel fear and pain, and when they lose their individual identity and acquire a collective identity.\n\nThe Norse Berserkers induced a trance-like state before battle, called \"Berserkergang\". It is said to have given the warriors superhuman strength and made them impervious to pain during battle. This form of trance could have been induced partly due to ingestion of hallucinogenic mushrooms.\n\nAs the mystical experience of mystics generally entails direct connection, communication and communion with Deity, Godhead and/or god; trance and cognate experience are endemic. (see Yoga, Sufism, Shaman, Umbanda, Crazy Horse, etc.)\n\nAs shown by Jonathan Garb, trance techniques also played a role in Lurianic Kabbalah, the mystical life of the circle of Moshe Hayyim Luzzatto and Hasidism.\n\nMany Christian mystics are documented as having experiences that may be considered as cognate with trance, such as: Hildegard of Bingen, John of the Cross, Meister Eckhart, Saint Theresa (as seen in the Bernini sculpture) and Francis of Assisi.\n\n\nTaves (1999) charts the synonymic language of trance in the American Christian traditions: \"power\" or \"presence\" or \"indwelling\" of God, or Christ, or the Spirit, or spirits. Typical expressions include \"the indwelling of the Spirit\" (Jonathan Edwards), \"the witness of the Spirit\" (John Wesley), \"the power of God\" (early American Methodists), being \"filled with the Spirit of the Lord\" (early Adventists; see charismatic Adventism), \"communing with spirits\" (Spiritualists), \"the Christ within\" (New Thought), \"streams of holy fire and power\" (Methodist holiness), \"a religion of the Spirit and Power\" (the Emmanuel Movement), and \"the baptism of the Holy Spirit\" (early Pentecostals). (Taves, 1999: 3)\n\nTaves (1999) well-referenced book on trance charts the experience of Anglo-American Protestants and those who left the Protestant movement beginning with the transatlantic awakening in the early 18th century and ending with the rise of the psychology of religion and the birth of Pentecostalism in the early 20th century. This book focuses on a class of seemingly involuntary acts alternately explained in religious and secular terminology. These involuntary experiences include uncontrolled bodily movements (fits, bodily exercises, falling as dead, catalepsy, convulsions); spontaneous vocalizations (crying out, shouting, speaking in tongues); unusual sensory experiences (trances, visions, voices, clairvoyance, out-of-body experiences); and alterations of consciousness and/or memory (dreams, somnium, somnambulism, mesmeric trance, mediumistic trance, hypnosis, possession, alternating personality) (Taves, 1999: 3).\n\nTrance-like states are often interpreted as religious ecstasy or visions and can be deliberately induced using a variety of techniques, including prayer, religious rituals, meditation, pranayama (breathwork or breathing exercises), physical exercise, coitus (and/or sex), music, dancing, sweating (e.g. sweat lodge), fasting, thirsting, and the consumption of psychotropic drugs such as cannabis. Sensory modality is the channel or conduit for the induction of the trance. Sometimes an ecstatic experience takes place in occasion of contact with something or somebody perceived as extremely beautiful or holy. It may also happen without any known reason. The particular technique that an individual uses to induce ecstasy is usually one that is associated with that individual's particular religious and cultural traditions. As a result, an ecstatic experience is usually interpreted within the context of a particular individual's religious and cultural traditions. These interpretations often include statements about contact with supernatural or spiritual beings, about receiving new information as a revelation, also religion-related explanations of subsequent change of values, attitudes and behavior (e.g. in case of religious conversion).\n\nBenevolent, neutral and malevolent trances may be induced (intentionally, spontaneously and/or accidentally) by different methods:\n\nCharles Tart provides a useful working definition of auditory driving. It is the induction of trance through the sense of hearing. Auditory driving works through a process known as entrainment.\n\nThe usage of repetitive rhythms to induce trance states is an ancient phenomenon. Throughout the world, shamanistic practitioners have been employing this method for millennia. Anthropologists and other researchers have documented the similarity of shamanistic auditory driving rituals among different cultures.\n\nSaid simply, entrainment is the synchronization of different rhythmic cycles. Breathing and heart rate have been shown to be affected by auditory stimulus, along with brainwave activity. The ability of rhythmic sound to affect human brainwave activity, especially theta brainwaves, is the essence of auditory driving, and is the cause of the altered states of consciousness that it can induce.\n\nNowack and Feltman have recently published an article entitled \"Eliciting the Photic Driving Response\" which states that the EEG photic driving response is a sensitive neurophysiological measure which has been employed to assess chemical and drug effects, forms of epilepsy, neurological status of Alzheimer's patients, and physiological arousal. Photic driving also impacts upon the psychological climate of a person by producing increased visual imagery and decreased physiological and subjective arousal. In this research by Nowack and Feltman, all participants reported increased visual imagery during photic driving, as measured by their responses to an imagery questionnaire.\n\nDennis Wier (https://web.archive.org/web/20060915232957/http://www.trance.edu/papers/theory.htm Accessed: 6 December 2006) states that over two millennia ago Ptolemy and Apuleius found that differing rates of flickering lights affected states of awareness and sometimes induced epilepsy. Wier also asserts that it was discovered in the late 1920s that when light was shined on closed eyelids it resulted in an echoing production of brainwave frequencies. Wier also opined that in 1965 Grey employed a stroboscope to project rhythmic light flashes into the eyes at a rate of 10–25 Hz (cycles per second). Grey discovered that this stimulated similar brainwave activity.\n\nResearch by Thomas Budzynski, Oestrander et al., in the use of brain machines suggest that photic driving via the suprachiasmatic nucleus and direct electrical stimulation and driving via other mechanisms and modalities, may entrain processes of the brain facilitating rapid and enhanced learning, produce deep relaxation, euphoria, an increase in creativity, problem solving propensity and may be associated with enhanced concentration and accelerated learning. The theta range and the border area between alpha and theta has generated considerable research interest.\n\nCharles Tart provides a useful working definition of kinesthetic driving. It is the induction of trance through the sense of touch, feeling or emotions. Kinesthetic driving works through a process known as entrainment.\n\nThe rituals practiced by some athletes in preparing for contests are dismissed as superstition, but this is a device of sport psychologists to help them to attain an ecstasy-like state. Joseph Campbell had a peak experience whilst running. Roger Bannister on breaking the four-minute mile (Cameron, 1993: 185): \"No longer conscious of my movement, I discovered a new unity with nature. I had found a new source of power and beauty, a source I never dreamt existed.\" Roger Bannister later became a distinguished neurologist.\n\nMechanisms and disciplines that include kinesthetic driving may include: dancing, walking meditation, yoga and asana, mudra, juggling, poi (juggling), etc.\n\nSufism (the mystical branch of Islam) has theoretical and metaphoric texts regarding ecstasy as a state of connection with Allah. Sufi practice rituals (\"dhikr\", \"sema\") use body movement and music to achieve the state.\n\n\nDivination is a cultural universal which anthropologists have observed as being present in many religions and cultures in all ages up to the present day (see sibyl). Divination may be defined as a mechanism for fortune-telling by ascertaining information by interpretation of omens or an alleged supernatural agency. Divination often entails ritual, and is often facilitated by trance.\n\nIn Tibet, oracles have played, and continue to play, an important part in religion and government. The word \"oracle\" is used by Tibetans to refer to the spirit, deity or entity that enters those men and women who act as media between the natural and the spiritual realms. The media are, therefore, known as \"kuten\", which literally means, \"the physical basis\".\n\nThe Dalai Lama, who lives in exile in northern India, still consults an oracle known as the \"Nechung Oracle\", which is considered the official state oracle of the government of Tibet. He gives a complete description of the process of trance and possession in his book \"Freedom in Exile\".\n\nConvergent disciplines of neuroanthropology, ethnomusicology, electroencephalography (EEG), neurotheology and cognitive neuroscience, amongst others, are conducting research into the trance induction of altered states of consciousness resulting from neuron entrainment with the driving of sensory modalities, for example polyharmonics, multiphonics, and percussive polyrhythms through the channel of the auditory and kinesthetic modality.\n\nNeuroanthropology and cognitive neuroscience are conducting research into the trance induction of altered states of consciousness (possibly engendering higher consciousness) resulting from neuron firing entrainment with these polyharmonics and multiphonics. Related research has been conducted into neural entraining with percussive polyrhythms. The timbre of traditional singing bowls and their polyrhythms and multiphonics are considered meditative and calming, and the harmony inducing effects of this tool to potentially alter consciousness are being explored by scientists, medical professionals and therapists.\n\nScientific advancement and new technologies such as computerized EEG, positron emission tomography, regional cerebral blood flow, and nuclear magnetic resonance imaging, are providing measurable tools to assist in understanding trance phenomena.\n\nThough a source of contention, there appear to be three current streams of inquiry: neurophysiology, social psychology and cognitive behaviorism. The neurophysiological approach is awaiting the development of a mechanism to map physiological measurements to human thought. The social-psychological approach currently measures gross subjective and social effects of thoughts and some critique it for lack of precision. Cognitive behaviorialists employ systems theory concepts and analytical techniques.\n\nThere are four principal brainwave states that range from high-amplitude, low-frequency delta to low-amplitude, high-frequency beta. These states range from deep dreamless sleep to a state of high arousal. These four brainwave states are common throughout humans. All levels of brainwaves exist in everyone at all times, even though one is foregrounded depending on the activity level. When a person is in an aroused state and exhibiting a beta brainwave pattern, their brain also exhibits a component of alpha, theta and delta, even though only a trace may be present.\n\nThe University of Philadelphia study on some Christians at the Freedom Valley Worship Center in Gettysburg, Pennsylvania, revealed that glossolalia-speaking (vocalizing or praying in unrecognizable form of language which is seen in members of certain Christian sects) activates areas of the brain out of voluntary control. In addition, the frontal lobe of the brain, which monitors speech, significantly diminished in activity as the study participants spoke glossolalia. Dr. Andrew B. Newberg, in analysis of his earlier studies as opposed to the MRI scans of the test subjects, stated that Buddhist monks in meditation and Franciscan nuns in prayer exhibited increased activity in the frontal lobe, and subsequently their behaviors, very much under voluntary control. The investigation found this particular beyond-body-control characteristic only in tongue-speakers (also see xenoglossia).\n\n"}
{"id": "45067", "url": "https://en.wikipedia.org/wiki?curid=45067", "title": "Transnational Radical Party", "text": "Transnational Radical Party\n\nThe Transnational Radical Party (TRP), whose official name is Nonviolent Radical Party, Transnational and Transparty (NRPTT), is a political association of citizens, members of parliament and members of government of various national and political backgrounds who intend to adopt nonviolent means to create an effective body of international law with respect for individuals, human, civil and political rights, as well as the affirmation of democracy and political freedom in the world.\n\nThe TRP does not participate in elections, and despite being named \"party\", is a non-governmental organization (NGO), adept in building synergies among political forces aimed at achieving the goals of its congressional motions.\n\nThe TRP is the direct evolution of the Italian Radical Party (1955–89) and is separate from the once-connected Italian Radicals party (founded in 2001), has been an NGO at the Economic and Social Council (ECOSOC) of the United Nations (UN) since 1995, listed in the general consultative status' category.\n\nThe TRP often advocates the international use of Esperanto in its literature.\n\nThe TRP's forerunner, the Radical Party (PR), was established in 1955 by a left-wing splinter group from the centre-right Italian Liberal Party (PLI). In 1989 the PR was transformed into the TRP. In 1992 a majority of the Radicals formed, at the national-level in Italian politics, the Pannella List, as its most senior figure was Marco Pannella. Since 1999 the List ran in elections under the banner of Bonino List, named after Emma Bonino. In 2001 the Radicals in Italy formed the Italian Radicals (RI).\n\nIn the first European Parliament election in 1979 the PR obtained its best result ever countrywide (3.7% of the vote, resulting in the election of three MEPs, including Pannella). Following the election, the PR was involved with the Coordination of European Green and Radical Parties (CEGRP) and its unsuccessful efforts to create a single pan-European platform for green and radical politics. More importantly, since then, the party projected itself into international politics.\n\nIn 1988, after a decade during which transnational issues and values were emphasised, the PR's congress decided that the party would be transformed during 1989 into the TRP and that the latter would not present itself in elections (in order to avoid competition with the other parties and stimulate cooperation instead), while permitting \"dual membership\" with other parties. The new symbol featuring the stylised face of Mahatma Gandhi was the point of no return in the transformation of the PR into an instrument of political fight completely at disposal of issue-oriented campaigns.\n\nAll this provoked great controversy among Radicals. Some long-time members left in order to continue their own activity in other parties or retire from public life. However, also most TRP Radicals continued to be actively engaged in politics, sometimes supported by the TRP itself, sometimes seeking hospitality in traditional parties or creating entirely new electoral lists. In the 1989 European Parliament election Pannella stood as a successful candidate of the joint list between the PLI (his former party) and the Italian Republican Party, some Radicals formed the \"Anti-prohibition List on Drugs\" (1 MEP), while others joined the Rainbow Greens (2 MEPs). In the run-up of the 1992 general election the Pannella List was formed.\n\nSergio Stanzani and Emma Bonino were the first secretary and president of the party, respectively. In 1993 Bonino, who would be appointed to the European Commission in 1995, replaced Stanzani, and Pannella became president.\n\nThe TRP was soon involved in comparing the conditions of the rule of law among different democracies throughout the world. While its members and economic resources continued to come primarily from Italy, the party strengthened its activities worldwide, especially in the countries of post-communist Eastern Europe. In this respect, the TRP launched the Multilingual Telematics System, one of the first bulletin board systems in Italy to allow multiple connections at the same time with the many countries where the party had influence and membership.\n\nIn 1995, after an intense institutional work, the TRP became a non-governmental organization for the promotion of human rights' legislation and the affirmation of democracy and freedom worldwide. As such, it was granted the general consultative status at the Economic and Social Council (ECOSOC) of the United Nations (UN).\n\nAlso in 1995 Olivier Dupuis, a long-time Radical from Belgium and founding member of the TRP who had moved to Budapest, Hungary in 1988 and from there had coordinated TRP's activities in Eastern Europe, was elected secretary of the party, while Jean-François Hory, a French MEP of the Radical Party of the Left within the TRP-sponsored European Radical Alliance group, was president. In 1996 Dupuis took Pannella's seat in the European Parliament.\n\nWithin the UN, the party has carried out high-profile battles on several issues: the moratorium on the death penalty and the proposal of his complete abolition, anti-prohibition against global mafias, fair justice, freedom of scientific research and the ban on female genital mutilation. Additionally, the TRP has allowed access to UN meetings to some stateless people, including Tibetans, Uyghurs and Montagnards., and led active monitoring of the conflicts against despotic regimes, such as the case of Ukraine versus Russia, or gave voice to dissidents opposed to authoritarian regimes like Cuba and Turkey. For its proposal of a peace plan in the Chechen–Russian conflict, the party has collided with Russia and its members risked expulsion from the country.\n\nDespite its successes worldwide, the TRP and its initiaves mostly failed to find space on the local press in Italy, which has often focused merely on the internal conflicts within the Radical world. The inadequate information on Radical initiatives by the Italian media has been meticulously verified by the TRP-sponsored \"Centro d'Ascolto dell'Informazione Radiotelevisiva\" and produced several sentences featuring compensations to be paid by RAI, the Italian public broadcaster, and commercial TV as well. Some confusion came from the fact that since the 2000s, rather than highlighting its expansion abroad, the TRP has preferred to focus on the \"Case Italy\", emblematic of the decadence of a constitutional political system into a \"real democracy\", that is to say a formal democracy in which its very institutions substantially act in contrast with the constitution. According to the TRP, Italy has become a \"partycratic regime\" and, as such, has started to spread the \"plague\" of \"real democracy\" around the democratic world. That was denounced by Radicals within international organisations and though the publication of a \"yellow book\" on the \"Italian plague\".\n\nHowever, the TRP effectively suffered internal problems too. In 2003 Dupuis resigned from secretary because of serious political differences with Pannella. In 2011–14 Demba Traoré, a politician from Mali, served briefly as secretary: he left the party without officially resigning, after being returned to the government of his own country. The TRP was later provisionally run by a committee, known as the \"Senate\" (\"Senato\"), led by Pannella and composed of the party's leading members.\n\nAfter the death of Pannella in May 2016, an extraordinary congress was convened in September to overcome the long inactivity due to the absence of the secretary, as well as the economic problems undermining the party's viability. The congress adopted with 178 votes in favour, 79 against and 13 abstentions on the final resolution:\n\nThe minority faction, led by Bonino and Marco Cappato, in turn, controlled the RI, as confirmed in their November 2016 congress. In February 2017 the TRP severed its ties with the RI (accused of boycotting the TRP, using its assets without paying for them and pursuing an Italian-only electoral agenda), and the latter could no longer use the Radical headquarters. However, the RI's congress invited its members to adhere to the TRP. The 3,000-member target for 2017 was achieved by the TRP in December, while the RI had launched a pro-Europeanist electoral list named More Europe for the 2018 Italian general election.\n\n\nThe 2016 congress elected a collective Presidency, composed of the following members: Matteo Angioli, Angiolo Bandinelli, Marco Beltrandi, Rita Bernardini (coordinator), Maurizio Bolognetti, Antonella Casu (coordinator), Antonio Cerrone, Deborah Cianfanelli, Maria Antonietta Coscioni, Sergio D'Elia (coordinator), Mariano Giustino, Giuseppe Rippa, Giuseppe Rossodivita, Irene Testa, Maurizio Turco (coordinator and legal representative), Valter Vecellio, and Elisabetta Zamparutti.\n\nCurrent prominent members (as of December 2017):\n\nFormer prominent members:\n\n\n"}
{"id": "34556475", "url": "https://en.wikipedia.org/wiki?curid=34556475", "title": "Typing environment", "text": "Typing environment\n\nIn type theory a typing environment (or typing context) represents the association between variable names and data types.\n\nMore formally an environment formula_1 is a set or ordered list of pairs formula_2, usually written as formula_3, where formula_4 is a variable and formula_5 its type.\n\nThe judgement\nis read as \"e has type τ in context Γ\".\n\nIn statically typed programming languages these environments are used and maintained by type rules to type check a given program or expression.\n"}
{"id": "997511", "url": "https://en.wikipedia.org/wiki?curid=997511", "title": "Vigilance committee", "text": "Vigilance committee\n\nA vigilance committee was a group formed of private citizens to administer law and order where they considered governmental structures to be inadequate. The term is commonly associated with the frontier areas of the American West in the mid-19th century, where groups attacked cattle rustlers and gangs, and people at gold mining claims. As non-state organizations no functioning checks existed to protect against excessive force or safeguard due process from the committees. In the years prior to the Civil War, some committees worked to free slaves and transport them to freedom.\n\nIn the western United States, both before and after the Civil War, the primary purpose of these committees was to maintain law and order and administer summary justice where governmental law enforcement was inadequate. In the newly settled areas, vigilance committees provided security, and mediated land disputes. In ranching areas, they ruled on ranch boundaries, registered brands, and protected cattle and horses. In the mining districts, they protected claims, settled claim disputes, and attempted to protect miners and other residents. In California, some residents formed vigilance committees to take control from officials whom they considered to be corrupt. This took place during the trial of Charles Cora (Husband of Belle Cora) and James Casey in San Francisco during 1856.\n\nVigilance committees were generally abandoned when the conditions favoring their creation ceased to exist. In the west, as governmental jurisdiction increased to the degree that courts could dispense justice, residents abandoned the committees.\n\nVigilance committees, by their nature, lacked an outside set of \"checks and balances\", leaving them open for excesses and abuse.\n\nIn the West, the speed of the vigilance committees and lack of safeguards sometimes led to the innocent being hanged or to their just disappearing. A few committees were taken over by fraudulent individuals seeking profit or political office.\n\n\n\n\n\n\n\n"}
{"id": "4719741", "url": "https://en.wikipedia.org/wiki?curid=4719741", "title": "Well-founded phenomenon", "text": "Well-founded phenomenon\n\nWell-founded phenomena (), in the philosophy of Gottfried Leibniz, are ways in which the world falsely appears to us, but which are grounded in the way the world is (as opposed to dreams or hallucinations, which are false appearances that are not thus grounded).\n\nFor Leibniz, the universe is made up of an infinite number of simple substances or monads, each of which contains a representation of the entire universe (past, present, and future), and which are all causally isolated from one another (\"Monads have no windows through which anything could enter or depart.\") For the most part the monads' perceptions are more or less confused and obscure, but some of them correspond either to the ways in which other monads are related or to the ways that the representation is genuinely ordered; these are the well-founded phenomena.\n\nIn the world of ordinary experience we might call a rainbow a well-ordered phenomenon; it appears to us to be a coloured arch in the sky, though there is in fact no arch there. We are not suffering from hallucinations, though, for the appearance is grounded in the way the world is ordered – in the behaviour of light, dust motes, water particles, etc.\n\nFor Leibniz, there are two main categories of well-founded phenomena: the ordinary world of individual objects and their interactions, and more abstract phenomena such as space, time, and causality. This is also found in his expression of pre-established harmony being the basis of causation.\n\n"}
{"id": "1301424", "url": "https://en.wikipedia.org/wiki?curid=1301424", "title": "Wood-plastic composite", "text": "Wood-plastic composite\n\nWood-plastic composites (WPCs) are composite materials made of wood fiber/wood flour and thermoplastic(s) (includes PE, PP, PVC, PLA etc.).\n\nIn addition to wood fiber and plastic, WPCs can also contain other ligno-cellulosic and/or inorganic filler materials. WPCs are a subset of a larger category of materials called natural fiber plastic composites (NFPCs), which may contain no cellulose-based fiber fillers such as pulp fibers, peanut hulls, bamboo, straw, digestate, etc.\n\nChemical additives seem practically \"invisible\" (except mineral fillers and pigments, if added) in the composite structure. They provide for integration of polymer and wood flour (powder) while facilitating optimal processing conditions.\n\nIn recent years, people in the flooring industry starts referring to WPC as a type of floor that has a basic structure of top vinyl veneer plus a rigid extruded core (the core can be made without any wood fiber). WPC is now an established product category within LVT. This type of WPC is different than the WPC decking and is not intended for outdoor usage.\n\nWood-plastic composites are still new materials relative to the long history of natural lumber as a building material. The most widespread use of WPCs in North America is in outdoor deck floors, but it is also used for railings, fences, landscaping timbers, cladding and siding, park benches, molding and trim, window and door frames, and indoor furniture. Wood-plastic composites were first introduced into the decking market in the early 1990s. Manufacturers claim that wood-plastic composite is more environmentally friendly and requires less maintenance than the alternatives of solid wood treated with preservatives or solid wood of rot-resistant species. These materials can be molded with or without simulated wood grain details.\n\nWood-plastic composites (WPCs) are produced by thoroughly mixing ground wood particles and heated thermoplastic resin. The most common method of production is to extrude the material into the desired shape, though injection molding is also used. WPCs may be produced from either virgin or recycled thermoplastics including HDPE, LDPE, PVC, PP, ABS, PS, and PLA. Polyethylene-based WPCs are by far the most common. Additives such as colorants, coupling agents, UV stabilizers, blowing agents, foaming agents, and lubricants help tailor the end product to the target area of application. Extruded WPCs are formed into both solid and hollow profiles. A large variety of injection molded parts are also produced, from automotive door panels to cell phone covers.\n\nIn some manufacturing facilities, the constituents are combined and processed in a pelletizing extruder, which produces pellets of the new material. The pellets are then re-melted and formed into the final shape. Other manufacturers complete the finished part in a single step of mixing and extrusion.\n\nDue to the addition of organic material, WPCs are usually processed at far lower temperatures than traditional plastics during extrusion and injection molding. WPCs tend to process at temperatures of about 50 °F (28 °C) lower than the same, unfilled material, for instance. Most will begin to burn at temperatures around 400 °F (204 °C). Processing WPCs at excessively high temperatures increases the risk of shearing, or burning and discoloration resulting from pushing a material that’s too hot through a gate which is too small, during injection molding. The ratio of wood to plastic in the composite will ultimately determine the melt flow index (MFI) of the WPC, with larger amounts of wood generally leading to a lower MFI.\n\nWPCs do not corrode and are highly resistant to rot, decay, and Marine Borer attack, though they do absorb water into the wood fibers embedded within the material. Water absorption is more pronounced in WFCs with a hydrophilic matrix such as PLA and also leads to decreased mechanical stiffness and strength. The mechanical performance in a wet environment can be enhanced by an acetylation treatment. WFCs have good workability and can be shaped using conventional woodworking tools. WPCs are often considered a sustainable material because they can be made using recycled plastics and the waste products of the wood industry. Although these materials continue the lifespan of used and discarded materials, they have their own considerable half life; the polymers and adhesives added make wood-plastic composite difficult to recycle again after use. They can however be recycled easily in a new wood-plastic composite, much like concrete. One advantage over wood is the ability of the material to be molded to meet almost any desired shape. A WPC member can be bent and fixed to form strong arching curves. Another major selling point of these materials is their lack of need for paint. They are manufactured in a variety of colors, but are widely available in grays and earth tones. Despite up to 70 percent cellulose content (although 50/50 is more common), the mechanical behavior of WPCs is most similar to neat polymers. Neat polymers are polymerized without added solvents. This means that WPCs have a lower strength and stiffness than wood, and they experience time and temperature-dependent behavior. The wood particles are susceptible to fungal attack, though not as much so as solid wood, and the polymer component is vulnerable to UV degradation. It is possible that the strength and stiffness may be reduced by freeze-thaw cycling, though testing is still being conducted in this area. Some WPC formulations are sensitive to staining from a variety of agents.\n\nWPC boards show good set of performance but monolithic composite sheets are relatively heavy (most often heavier than pure plastics) which limits their use to applications where low weight is not essential. WPC in a sandwich-structured composite form allow for combination of the benefits of traditional wood polymer composites with the lightness of a sandwich panel technology. WPC sandwich boards consist of wood polymer composite skins and usually low density polymer core which leads to a very effective increase of panel's rigidity. WPC sandwich boards are used mainly in automotive, transportation and building applications, but furniture applications are also being developed. New efficient and often in-line integrated production processes allow to produce stronger and stiffer WPC sandwich boards at lower costs compared to traditional plastic sheets or monolithic WPC panels.\n\nThe environmental impact of WPCs is directly affected by the ratio of renewable to non-renewable materials. The commonly used petroleum-based polymers have a negative environmental impact because they rely on non-renewable raw materials and the non-biodegradability of plastics.\n\nThe types of plastic normally used in WPC formulations have higher fire hazard properties than wood alone, as plastic has a higher chemical heat content and can melt. The inclusion of plastic as a portion of the composite results in the potential for higher fire hazards in WPCs as compared with wood. Some code officials are becoming increasingly concerned with the fire performance of WPCs.\n\n"}
{"id": "322623", "url": "https://en.wikipedia.org/wiki?curid=322623", "title": "Zermelo set theory", "text": "Zermelo set theory\n\nZermelo set theory (sometimes denoted by Z), as set out in an important paper in 1908 by Ernst Zermelo, is the ancestor of modern set theory. It bears certain differences from its descendants, which are not always understood, and are frequently misquoted. This article sets out the original axioms, with the original text (translated into English) and original numbering.\n\nZermelo's axioms are stated for a model some (but not necessarily all) of whose objects are called sets, and the remaining objects are urelements and do not contain any elements. Zermelo's language implicitly includes a membership relation ∈, an equality relation = (if it is not included in the underlying logic), and a unary predicate saying whether an object is a set. Later versions of set theory often assume that all objects are sets so there are no urelements and there is no need for the unary predicate.\n\nThe most widely used and accepted set theory is known as ZFC, which consists of Zermelo–Fraenkel set theory with the addition of the axiom of choice. The links show where the axioms of Zermelo's theory correspond. There is no exact match for \"elementary sets\". (It was later shown that the singleton set could be derived from what is now called \"Axiom of pairs\". If \"a\" exists, \"a\" and \"a\" exist, thus {\"a\",\"a\"} exists. By extensionality {\"a\",\"a\"} = {\"a\"}.) The empty set axiom is already assumed by axiom of infinity, and is now included as part of it.\n\nZermelo set theory does not include the axioms of replacement and regularity. The axiom of replacement was first published in 1922 by Abraham Fraenkel and Thoralf Skolem, who had independently discovered that Zermelo's axioms cannot prove the existence of the set {\"Z\", \"Z\", \"Z\", ...} where \"Z\" is the set of natural numbers and \"Z\" is the power set of \"Z\". They both realized that the axiom of replacement is needed to prove this. The following year, John von Neumann pointed out that this axiom is necessary to build his theory of ordinals. The axiom of regularity was stated by von Neumann in 1925.\n\nIn the modern ZFC system, the \"propositional function\" referred to in the axiom of separation is interpreted as \"any property definable by a first order formula with parameters\", so the separation axiom is replaced by an axiom scheme. The notion of \"first order formula\" was not known in 1904 when Zermelo published his axiom system, and he later rejected this interpretation as being too restrictive. Zermelo set theory is usually taken to be a first-order theory with the separation axiom replaced by an axiom scheme with an axiom for each first-order formula. It can also be considered as a theory in second-order logic, where now the separation axiom is just a single axiom. The second-order interpretation of Zermelo set theory is probably closer to Zermelo's own conception of it, and is stronger than the first-order interpretation.\n\nIn the usual cumulative hierarchy \"V\" of ZFC set theory (for ordinals α), any one of the sets\n\"V\" for α a limit ordinal larger than the first infinite ordinal ω (such as \"V\") forms a model of Zermelo set theory. So the consistency of Zermelo set theory is a theorem of ZFC set theory. Zermelo's axioms do not imply the existence of ℵ or larger infinite cardinals, as the model \"V\" does not contain such cardinals. (Cardinals have to be defined differently in Zermelo set theory, as the usual definition of cardinals and ordinals does not work very well: with the usual definition it is not even possible to prove the existence of the ordinal ω2.)\n\nThe axiom of infinity is usually now modified to assert the existence of the first infinite\nvon Neumann ordinal formula_2; the original Zermelo\naxioms cannot prove the existence of this set, nor can the modified Zermelo axioms prove Zermelo's\naxiom of infinity. Zermelo's axioms (original or modified) cannot prove the existence of formula_3 as a set nor of any rank of the cumulative hierarchy of sets with infinite index.\n\nZermelo allowed for the existence of urelements that are not sets and contain no elements; these are now usually omitted from set theories.\n\nMac Lane set theory, introduced by , is Zermelo set theory with the axiom of separation restricted to first-order formulas in which every quantifier is bounded,\nMac Lane set theory is similar in strength to topos theory with a natural number object, or to the system in Principia mathematica. It is strong enough to carry out almost all ordinary mathematics not directly connected with set theory or logic.\n\nThe introduction states that the very existence of the discipline of set theory \"seems to be threatened by certain contradictions or \"antinomies\", that can be derived from its principles – principles necessarily governing our thinking, it seems – and to which no entirely satisfactory solution has yet been found\". Zermelo is of course referring to the \"Russell antinomy\".\n\nHe says he wants to show how the original theory of Georg Cantor and Richard Dedekind can be reduced to a few definitions and seven principles or axioms. He says he has \"not\" been able to prove that the axioms are consistent.\n\nA non-constructivist argument for their consistency goes as follows. Define \"V\" for α one of the ordinals 0, 1, 2, ...,ω, ω+1, ω+2..., ω·2 as follows:\nThen the axioms of Zermelo set theory are consistent because they are true in the model \"V\". While a non-constructivist might regard this as a valid argument, a constructivist would probably not: while there are no problems with the construction of the sets up to \"V\", the construction of \"V\" is less clear because one cannot constructively define every subset of \"V\". This argument can be turned into a valid proof in Zermelo–Frenkel set theory, but this does not really help because the consistency of Zermelo–Frenkel set theory is less clear than the consistency of Zermelo set theory.\n\nZermelo comments that Axiom III of his system is the one responsible for eliminating the antinomies. It differs from the original definition by Cantor, as follows.\n\nSets cannot be independently defined by any arbitrary logically definable notion. They must be constructed in some way from previously constructed sets. For example, they can be constructed by taking powersets, or they can be \"separated\" as subsets of sets already \"given\". This, he says, eliminates contradictory ideas like \"the set of all sets\" or \"the set of all ordinal numbers\".\n\nHe disposes of the Russell paradox by means of this Theorem: \"Every set formula_4 possesses at least one subset formula_5 that is not an element of formula_4 \". Let formula_5 be the subset of formula_4 for which, by AXIOM III, is separated out by the notion \"formula_9\". Then formula_5 cannot be in formula_4. For\n\n\nTherefore, the assumption that formula_5 is in formula_4 is wrong, proving the theorem. Hence not all objects of the universal domain \"B\" can be elements of one and the same set. \"This disposes of the Russell antinomy as far as we are concerned\".\n\nThis left the problem of \"the domain \"B\"\" which seems to refer to something. This led to the idea of a proper class.\n\nZermelo's paper is notable for what may be the first mention of Cantor's theorem explicitly and by name. This appeals strictly to set theoretical notions, and is thus not exactly the same as Cantor's diagonal argument.\n\nCantor's theorem: \"If \"M\" is an arbitrary set, then always \"M\" < P(\"M\") [the power set of \"M\"]. Every set is of lower cardinality than the set of its subsets\".\n\nZermelo proves this by considering a function φ: \"M\" → P(\"M\"). By Axiom III this defines the following set \"M' \":\n\nBut no element \"m' \" of \"M \" could correspond to \"M' \", i.e. such that φ(\"m' \") = \"M' \". Otherwise we can construct a contradiction:\n\nso by contradiction \"m' \" does not exist. Note the close resemblance of this proof to the way Zermelo disposes of Russell's paradox.\n\n\n"}
