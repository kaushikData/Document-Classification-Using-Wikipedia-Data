{"id": "2021591", "url": "https://en.wikipedia.org/wiki?curid=2021591", "title": "Altruism (biology)", "text": "Altruism (biology)\n\nIn biology, altruism refers to behaviour by an individual that increases the fitness of another individual while decreasing the fitness of the actor. Altruism in this sense is different from the philosophical concept of altruism, in which an action would only be called \"altruistic\" if it was done with the conscious intention of helping another. In the behavioural sense, there is no such requirement. As such, it is not evaluated in moral terms—it is the consequences of an action for reproductive fitness that determine whether the action is considered altruistic, not the intentions, if any, with which the action is performed.\n\nThe term altruism was coined by the French philosopher Auguste Comte in French, as \"altruisme\", for an antonym of egoism. He derived it from an Italian \"altrui\", which in turn was derived from Latin \"alteri\", meaning \"other people\" or \"somebody else\".\n\nAltruistic behaviours appear most obviously in kin relationships, such as in parenting, but may also be evident among wider social groups, such as in social insects. They allow an individual to increase the success of its genes by helping relatives that share those genes. Obligate altruism is the permanent loss of direct fitness (with potential for indirect fitness gain). For example, honey bee workers may forage for the colony. Facultative altruism is temporary loss of direct fitness (with potential for indirect fitness gain followed by personal reproduction); for example, a Florida scrub jay helping at the nest, then gaining parental territory.\n\nIn the science of ethology (the study of behavior), and more generally in the study of social evolution, on occasion, some animals do behave in ways that reduce their individual fitness but increase the fitness of other individuals in the population; this is a functional definition of altruism. Research in evolutionary theory has been applied to social behaviour, including altruism. Cases of animals helping individuals to whom they are closely related can be explained by kin selection, and are not considered true altruism. Beyond the physical exertions that in some species mothers and in some species fathers undertake to protect their young, extreme examples of sacrifice may occur. One example is matriphagy (the consumption of the mother by her offspring) in the spider \"Stegodyphus\"; another example is a male spider allowing a female fertilized by him to eat him. Hamilton's rule describes the benefit of such altruism in terms of Wright's coefficient of relationship to the beneficiary and the benefit granted to the beneficiary minus the cost to the sacrificer. Should this sum be greater than zero a fitness gain will result from the sacrifice.\n\nWhen apparent altruism is not between kin, it may be based on reciprocity. A monkey will present its back to another monkey, who will pick out parasites; after a time the roles will be reversed. Such reciprocity will pay off, in evolutionary terms, as long as the costs of helping are less than the benefits of being helped and as long as animals will not gain in the long run by \"cheating\"—that is to say, by receiving favours without returning them. This is elaborated on in evolutionary game theory and specifically the prisoner's dilemma as social theory.\n\nThe existence of altruism in nature is at first sight puzzling, because altruistic behaviour reduces the likelihood that an individual will reproduce. The idea that \"group selection\" might explain the evolution of altruism was first broached by Darwin himself in The Descent of Man, and Selection in Relation to Sex, (1871). The concept of group selection has had a chequered and controversial history in evolutionary biology but the uncritical 'good of the species' tradition came to an abrupt halt in the 1960s, due largely to the work of George C. Williams, and John Maynard Smith as well as Richard Dawkins. These evolutionary theorists pointed out that natural selection acts on the individual, and that it is the individual's fitness (number of offspring and grand-offspring produced compared to the rest of the population) that drives evolution. A group advantage (e.g. hunting in a pack) that is disadvantageous to the individual (who might be harmed during the hunt, when it could avoid injury by hanging back from the pack but still share in the spoils) cannot evolve, because the selfish individual will leave, on average, more offspring than those who join the pack and suffer injuries as a result. If the selfishness is hereditary, this will ultimately result in the population consisting entirely of selfish individuals. However, in the 1960s and 1970s an alternative to the \"group selection\" theory emerged. This was the kin selection theory, due originally to W. D. Hamilton. Kin selection is an instance of inclusive fitness, which is based on the notion that an individual shares only half its genes with each offspring, but also with each full sib (See footnote). From an evolutionary genetic point of view it is therefore as advantageous to help with the upbringing of full sibs as it is to produce and raise one's own offspring. The two activities are evolutionarily entirely equivalent. Co-operative breeding (i.e. helping one's parents raise sibs—provided they are full sibs) could thus evolve without the need for group-level selection. This quickly gained prominence among biologists interested in the evolution of social behaviour.\n\nIn 1971 Robert Trivers introduced his reciprocal altruism theory to explain the evolution of helping at the nest of an unrelated breeding pair of birds. He argued that an individual might act as a helper if there was a high probabilistic expectation of being helped by the recipients at some later date. If, however, the recipients did not reciprocate when it was possible to do so, the altruistic interaction with these recipients would be permanently terminated. But if the recipients did not cheat then the reciprocal altruism would continue indefinitely to both parties' advantage. This model was considered by many (e.g. West-Eberhard and Dawkins) to be evolutionarily unstable because it is prone to invasion by cheats for the same reason that cooperative hunting can be invaded and replaced by cheats. However, Trivers did make reference to the Prisoner's Dilemma Game which, 10 years later, would restore interest in Trivers' reciprocal altruism theory, but under the title of \"tit-for-tat\".\n\nIn its original form the Prisoner's Dilemma Game (PDG) described two awaiting trial prisoners, A and B, each faced with the choice of betraying the other or remaining silent. The \"game\" has four possible outcomes: (a) they both betray each other, and are both sentenced to two years in prison; (b) A betrays B, which sets A free and B is sentenced to four years in prison; (c) B betrays A, with the same result as (b) except that it is B who is set free and the other spends four years in jail; (d) both remain silent, resulting in a six-month sentence each. Clearly (d) (\"cooperation\") is the best mutual strategy, but from the point of view of the individual betrayal is unbeatable (resulting in being set free, or getting only a two-year sentence). Remaining silent results in a four-year or six-month sentence. This is exemplified by a further example of the PDG: two strangers attend a restaurant together and decide to split the bill. The mutually best ploy would be for both parties to order the cheapest items on the menu (mutual cooperation). But if one member of the party exploits the situation by ordering the most expensive items, then it is best for the other member to do likewise. In fact, if the fellow diner's personality is completely unknown, and the two diners are unlikely ever to meet again, it is always in one's own best interests to eat as expensively as possible. Situations in nature that are subject to the same dynamics (rewards and penalties) as the PDG define cooperative behaviour: it is never in the individual's fitness interests to cooperate, even though mutual cooperation rewards the two contestants (together) more highly than any other strategy. Cooperation cannot evolve under these circumstances.\n\nHowever, in 1981 Axelrod and Hamilton noted that if the same contestants in the PDG meet repeatedly (the so-called Iterated Prisoner's Dilemma game, IPD) then tit-for-tat (foreshadowed by Robert Triver's reciprocal altruism theory) is a robust strategy which promotes altruism. In \"tit-for-tat\" both players' opening moves are cooperation. Thereafter each contestant repeats the other player's last move, resulting in a seemingly endless sequence of mutually cooperative moves. However, mistakes severely undermine tit-for-tat's effectiveness, giving rise to prolonged sequences of betrayal, which can only be rectified by another mistake. Since these initial discoveries, all the other possible IPD game strategies have been identified (16 possibilities in all, including, for instance, \"generous tit-for-tat\", which behaves like \"tit-for-tat\", except that it cooperates with a small probability when the opponent's last move was \"betray\".), but all can be outperformed by at least one of the other strategies, should one of the players switch to such a strategy. The result is that none is evolutionarily stable, and any prolonged series of the iterated prisoner's dilemma game, in which alternative strategies arise at random, gives rise to a chaotic sequence of strategy changes that never ends.\n\nIn the light of the Iterated Prisoner's Dilemma Game failing to provide a full answer to the evolution of cooperation or altruism, several alternative explanations have been proposed.\n\nThere are striking parallels between altruistic acts and exaggerated sexual ornaments displayed by some animals, particularly certain bird species, such as, amongst others, the peacock. Both are costly in fitness terms, and both are generally conspicuous to other members of the population or species. This led Amotz Zahavi to suggest that both might be fitness signals rendered evolutionarily stable by his handicap principle. If a signal is to remain reliable, and generally resistant to falsification, the signal has to be evolutionarily costly. Thus, if a (low fitness) liar were to use the highly costly signal, which seriously eroded its real fitness, it would find it difficult to maintain a semblance or normality. Zahavi borrowed the term \"handicap principle\" from sports handicapping systems. These systems are aimed at reducing disparities in performance, thereby making the outcome of contests less predictable. In a horse handicap race, provenly faster horses are given heavier weights to carry under their saddles than inherently slower horses. Similarly, in amateur golf, better golfers have fewer strokes subtracted from their raw scores than the less talented players. The handicap therefore correlates with unhandicapped performance, making it possible, if one knows nothing about the horses, to predict which unhandicapped horse would win an open race. It would be the one handicapped with the greatest weight in the saddle. The handicaps in nature are highly visible, and therefore a peahen, for instance, would be able to deduce the health of a potential mate by comparing its handicap (the size of the peacock's tail) with those of the other males. The loss of the male's fitness caused by the handicap is offset by its increased access to females, which is as much of a fitness concern as is its health. An altruistic act is, by definition, similarly costly. It would therefore also signal fitness, and is probably as attractive to females as a physical handicap. If this is the case altruism is evolutionarily stabilized by sexual selection.\nThere is an alternate strategy for identifying fit mates which does not rely on one gender having exaggerated sexual ornaments or other handicaps, but is generally applicable to most, if not all sexual creatures. It derives from the concept that the change in appearance and functionality caused by a non-silent mutation will generally stand out in a population. This is because that altered appearance and functionality will be unusual, peculiar, and different from the norm within that population. The norm against which these unusual features are judged is made up of fit attributes that have attained their plurality through natural selection, while less adaptive attributes will be in the minority or frankly rare. Since the overwhelming majority of mutant features are maladaptive, and it is impossible to predict evolution's future direction, sexual creatures would be expected to prefer mates with the least number of unusual or minority features. This will have the effect of a sexual population rapidly shedding peripheral phenotypic features and canalizing the entire outward appearance and behaviour so that all the members of that population will begin to look remarkably similar in every detail, as illustrated in the accompanying photograph of the African pygmy kingfisher, \"Ispidina picta\". Once a population has become as homogeneous in appearance as is typical of most species, its entire repertoire of behaviours will also be rendered evolutionarily stable, including any altruistic, cooperative and social characteristics. Thus, in the example of the selfish individual who hangs back from the rest of the hunting pack, but who nevertheless joins in the spoils, that individual will be recognized as being different from the norm, and will therefore find it difficult to attract a mate. Its genes will therefore have only a very small probability of being passed on to the next generation, thus evolutionarily stabilizing cooperation and social interactions at whatever level of complexity is the norm in that population.\n\nAltruism in animals describes a range of behaviors performed by animals that may be to their own disadvantage but which benefit others. The costs and benefits are measured in terms of reproductive fitness, or expected number of offspring. So by behaving altruistically, an organism reduces the number of offspring it is likely to produce itself, but boosts the likelihood that other organisms are to produce offspring. There are other forms of altruism in nature other than risk-taking behavior, such as reciprocal altruism. This biological notion of altruism is not identical to the everyday human concept. For humans, an action would only be called 'altruistic' if it was done with the conscious intention of helping another. Yet in the biological sense there is no such requirement. Instead, until we can communicate directly with other species, an accurate theory to describe altruistic acts between species is Biological Market Theory.\nHumans and other animals exchange benefits in several ways, known technically as reciprocity mechanism. No matter what the mechanism, the common thread is that benefits find their way back to the original giver.\n\nAlso known as the \"buddy-system\", mutual affection between two parties prompts similar behavior in both directions without need to track of daily give-and-take, so long as the overall relationship remains satisfactory. This is one of the most common mechanism of reciprocity in nature, this kind is present in humans, primates, and many other mammals.\n\nAlso known as, \"If you're nice, I'll be nice too.\" This mechanism of reciprocity is similar to the heuristic of the golden rule, \"Treat others how you would like to be treated.\" Parties mirror one another's attitudes, exchanging favors on the spot. Instant attitudinal reciprocity occurs among monkeys, and people often rely on it with strangers and acquaintances.\n\nAlso known as, \"what have you done for me lately?\" Individuals keep track of the benefits they exchange with particular partners, which helps them decide to whom to return favors. This mechanism is typical of chimpanzees and very common among human relationships.\nYet some opposing experimental research suggests that calculated or contingent reciprocity does not spontaneously arise in laboratory experimental settings, despite patterns of behavior.\n\nBiological market theory is an extension of the idea of reciprocal altruism, as a mechanism to explain altruistic acts between unrelated individuals in a more flexible system of exchanging commodities. The term 'biological market' was first used by Ronald Noe and Hammerstein in 1994 to refer to all the interactions between organisms in which different organisms function as 'traders' that exchange goods and services such as food and water, grooming, warning calls, shelter, etc. Biological market theory consists of five formal characteristics which present a basis for altruism.\n\n\nThe applicability of biological market theory with its emphasis on partner choice is evident in the interactions between the cleaner wrasse and its \"client\" reef fish. Cleaners have small territories, which the majority of reef fish species actively visit to invite inspection of their surface, gills, and mouth. Clients benefit from the removal of parasites while cleaners benefit from the access to a food source. Some particularly choosy client species have large home ranges that cover several cleaning stations, whereas other clients have small ranges and have access to one cleaning station only (resident clients). Field observations, field manipulations, and laboratory experiments revealed that whether or not a client has choice options influences several aspects of both cleaner and client behaviour. Cleaners give choosy clients priority of access. Choosy clients switch partners if cheated by a cleaner by taking a bite of out of the cleaner, whereas resident clients punish cheats. Cleaners and resident clients, but not choosy clients, build up relationships before normal cleaning interactions take place. Cleaners are particularly cooperative if choosy clients are bystanders of an interaction but less so when resident clients are bystanders.\n\nResearchers tested whether wild white-handed gibbon males from Khao Yai National Park, Thailand, increased their grooming activity when the female partner was fertile. Adult females and males of our study population are codominant (in terms of aggression), they live in pairs or small multi male groups and mate promiscuously. They found that males groomed females more than vice versa and more grooming was exchanged when females were cycling than during pregnancy or lactation. The number of copulations/day was elevated when females were cycling, and females copulated more frequently with males on days when they received more grooming. When males increased their grooming efforts, females also increased their grooming of males, perhaps to equalize give and take. Although grooming might be reciprocated because of intrinsic benefits of receiving grooming, males also interchange grooming as a commodity for sexual opportunities during a female's fertile period.\n\n\n\n\n\nAn interesting example of altruism is found in the cellular slime moulds, such as \"Dictyostelium mucoroides\". These protists live as individual amoebae until starved, at which point they aggregate and form a multicellular fruiting body in which some cells sacrifice themselves to promote the survival of other cells in the fruiting body.\n\n\n"}
{"id": "44910573", "url": "https://en.wikipedia.org/wiki?curid=44910573", "title": "Anubandha chatushtaya", "text": "Anubandha chatushtaya\n\nAnubandha chatushtaya (Sanskrit: अनुबन्ध चतुष्टय) literally means four connections, and therefore, it is four-fold in nature and content viz, – a) \"adhikāri\" ('the qualified student') who has developed \"ekāgrata\" ('single pointed mind'), \"chitta shuddhi\" ('purity of the mind') and \"vikshepa\" ('freedom from restlessness and impurity') or \"adhikāra\" (\"aptitude\"); b) \"vishaya\" ('subject matter' or 'the theme') pertaining to the \" Jiva-Brahman identity\"; c) \"prayojana\" or \" phalasruti\" ('result' or 'fruit') which is \"atyantika-dukha-nivritti\" ('complete cessation of sorrow') and \"paramānanda-prāpti\" ('attainment of supreme happiness'), and d) \"sambandha\" ('relationship' or 'intertextuality') between \"adhikāra\", \"vishaya\" and \"prayojana\".\n\nSadananda reminds that:-\n\na)- \"Adhikāra\" is the capacity to study (\"adhyana\") the subject-matter in which respect there exist doubts (\"sandigdhata\") which is the basis of the intended inquiry into that particular subject-matter for the eventual removal of all doubts and gain of its proper understanding. Madhavacharya speaks about three levels of \"adhikāra\" – \"adhama\" ('lower'), \"madhyma\" ('middle') and \"uttama\" ('higher'), the last two levels indicate that the student is of peaceful mind (\"śanta\"), is understanding and has non-repressed sense-control (\"danta\"), has withdrawn from worldly objects (\"uparata\"), indifferent (\"titikśu\") and composed and tranquil (\"samhita\"), and knows that he is adequately prepared to meet the truth face to face. The Mimāmsikas consider \"adhikāra\" to be a notion of appropriateness. All things and actions have identifiable \"adhikāra\". \"Adhikāra\" indicates the level of preparation necessary for making one skilled for higher gains. The concept of \"adhikāra\" also helps describe the relationship between \"purusha\" and \"prakrti\" and explain the reason for activation of creative power of nature, and thus brings about the experience of the world (\"darśana bhoga\") and liberation from this experience (\"kaivalya\"). The eagerness to know incites the obligation (\"adhikāra\") to bring about realization of knowledge. In this direction the student must firstly develop \" Ekāgratā \" ('one-pointed'). \"Ekagrata\" is the state of determined and continuous concentration obtained by integrating the psycho-mental flux (\"sarvārthatā\" i.e. variously directed, discontinuous, diffused attention) which is – योगश्चित्तवृत्तिनिरोधः (Yoga Sutra)- meaning – Yoga is suppression of psycho-mental states i.e. disappearance of all possible misconceptions. In this yogic state of \"ekagrata\" the yogi gains genuine will. \" Upasana \" brings about \"ekagrata\", and \"upasana\" in its own turn is brought about by \"chitta shuddhi\" ('the purity of heart'); and controls \" vikshepa \" (all forms of mental wanderings or distractions).\n\nIn Sanskrit grammar, an \"adhikara\" is a 'heading rule' made belonging to it, by the accent of \"swarita\" with the object of avoiding a repeated designation.\n\nb)- \"Vishaya\" is the object of knowledge, here the subject-matter; the universal Self (Brahman) seemingly different from the individual self (Jiva). Knowledge reveals the real to be superimposed by the unreal on account of ignorance. As explained by Sadananda the subject is the one-ness of the individual soul and the universal soul (the Absolute) as explained by the Upanishads; this is the essence of the Vedanta. Consciousness, which is infinite and indivisible, is able to come into contact with objects and is inherent in objects which are thus very many varying phases of consciousness; such objects that are by themselves phases of consciousness are \"vishaya-caitanya\", outside consciousness no object can exist.\n\nc)- \"Prayojana\" means - the 'purpose', 'end result' or 'resolution of the inquiry'; it is the established result arrived at after practice of devotion conducted with a peaceful mind and body (\"śanta bhāva\"). Uddyotakara explains that the basic urges for attainment of happiness and avoidance of sorrow, such as the \"chaturvarga\" composed of \"dharma\", \"artha\", \"kama\" and \"moksha\", prompt human activity, and which motivation leads to the end results or \"prayojana\". Vedānta Paribhāśa, recognizing this \"chaturvarga\" describes \"prayojana\" as that which when known is desired as one’s own, Prayojana is the realizable purpose, the desired reason or motive which prompts actions but which cannot be indicated separately from \"sambandha\", both being inter-related,\n\nd)- \"Sambandha\" does not necessarily identify either \"vishaya\" or \"prayojana\" though it can be factual and reasonable (\"siddha\") or contrary to fact and reason (\"asiddha\"). Shankara tells us that the self is bodyless (\"aśarira\") but its relationship (\"sambandha\") with the body is \"asiddha\" because the self is unrelated to any activity of the body and unconnected with merit and demerit arising from activity notwithstanding the fact that the \"jiva\" erroneously identifies itself with the body. Vedantasara afore-cited statement as an exposition of these four \"anubandhas\" ('connections') and in particular the connection called \"sambandha\" to mean – (quote) \"the relation of what has to be made known in the Vedantic system, the identity between Brahman and the individual soul\" (unquote).\n\nIn Hindu philosophy, \"Anubandha chatushtaya\" is a traditional Indian rhetorical mode connected with the gain of knowledge and supreme perfection, which mode is based on four fundamental aspects of thoughts and actions working in-tandem which are – a) the proposed subject or theme, b) the intended goal and its nature, c) why for that goal is sought in the light of d) the subject opted and the anticipated apprehension of truth. Krishna assures Arjuna:-\n\nand thereafter, proceeds to explain how that knowledge can be gained and developed which would eventually free a \"mumukshu\" ('seeker') who due to association with the modes of nature (\"prakrti\") is entangled in this material world, and therefore, explains the nature of the three \" gunas\". This particular statement is in the form of an \"Anubandha chatushtaya\" the subject-matter of which is \"Brahma Vidya\", the goal is liberation from sorrow, and \"pryojanam\" is the knowledge of Brahman which gives spontaneous release. Krishna leads Arjuna to that knowledge of the self which reveals what the subject is, what the \"I\" is – \" Tat Tvam Asi \", this is the relationship that helps the seeker, possessing basic qualities, to decide whether he should connect or associate with the subject-matter or not.\n"}
{"id": "185966", "url": "https://en.wikipedia.org/wiki?curid=185966", "title": "Bellum omnium contra omnes", "text": "Bellum omnium contra omnes\n\n, a Latin phrase meaning \"the war of all against all\", is the description that Thomas Hobbes gives to human existence in the state of nature thought experiment that he conducts in \"De Cive\" (1642) and \"Leviathan\" (1651). The common modern English usage is a war of \"each against all\" where war is rare and terms such as \"competition\" or \"struggle\" are more common.\n\nIn \"Leviathan\" itself, Hobbes speaks of 'warre of every one against every one', of 'a war [...] of every man against every man' and of 'a perpetuall warre of every man against his neighbour', but the Latin phrase occurs in \"De Cive\":\nLater on, two slightly modified versions are presented in \"Libertas\" (liberty):\n\nIn chapter XIII of \"Leviathan\", Hobbes explains the concept with these words:\nThe thought experiment places people in a pre-social condition, and theorizes what would happen in such a condition. According to Hobbes, the outcome is that people choose to enter a social contract, giving up some of their liberties in order to enjoy peace. This thought experiment is a test for the legitimation of a state in fulfilling its role as \"sovereign\" to guarantee social order, and for comparing different types of states on that basis.\n\nHobbes distinguishes between war and battle: war does not only consist of actual battle; it points to the situation in which one knows there is a 'Will to contend by Battle'.\n\nIn his \"Notes on the State of Virginia\" (1785) Thomas Jefferson uses the phrase \"bellum omnium in omnia\" (\"war of all things against all things,\" assuming \"omnium\" is intended to be neuter like \"omnia\") as he laments that the constitution of that state was twice at risk of being sacrificed to the nomination of a dictator after the manner of the Roman Republic.\n\nSometimes the phrase is used by Marx and Engels.\n\n\n\n\nIt was also used by Friedrich Nietzsche in \"On Truth and Lies in a Nonmoral Sense\" (1873):\n"}
{"id": "41200884", "url": "https://en.wikipedia.org/wiki?curid=41200884", "title": "Betty Gram Swing", "text": "Betty Gram Swing\n\nBetty Gram Swing, born Myrtle Eveline Gram (March 16, 1893 – September 1, 1969), was an American militant suffragist.\nBetty Gram participated in landmark events of the women's rights movement in the early to mid-twentieth century. She joined the women’s rights movement in 1917 and was subsequently jailed five times and went on two hunger strikes. When she was sent to Tennessee to push for the last vote needed to ratify the 19th amendment, a local newspaper announced, “110 Pounds of Femininity Hit Memphis”. Contemporary newspaper accounts often noted her attractiveness. She remained active for many decades in women’s rights and other causes. In 1921, Gram married well-known radio commentator Raymond Gram Swing. She insisted that he take her last name as well if she was taking his and became Betty Gram Swing. The couple were the parents of two sons and a daughter. They divorced in 1944.\n\nAfter graduating from the University of Oregon, Betty Gram embarked on a stage career, which she left when she came east to join the National Woman's Party (NWP), led by Alice Paul. The NWP had split with other women's suffrage groups over its focus on a constitutional amendment guaranteeing women the right to vote, as well as on its aggressive tactics. Other groups, namely the National American Woman's Suffrage Association, had built a state by state suffrage strategy and opposed the NWP's targeting of President Wilson and Democrats as well as its picketing effort.\n\nThe National Woman's Party began its picketing of the White House in January 1917. In March of that year, over one thousand women took part. By June, the White House decided to arrest the picketers. Since holding signs is not illegal, the women demanded to know the reason for their arrest. The eventual charge was obstructing traffic. Those charges were dismissed and no trial was held. But a new pattern had begun - the women picketers were not deterred. Upon their release, they went straight back to picketing the White House and were arrested again. A front page photo in the Evening Tribune of San Diego bore the caption, \"Washington has found that the only way to keep a picket away from the White House is to put her in jail.\" This time the picketers were found guilty of obstructing traffic and fined $25 or 3 days in jail. The women refused to pay the fine. A rapid escalation followed until the leader of the NWP, Alice Paul was sentenced to 7 months in jail. Betty and her sister, Alice Gram, and 39 others were jailed for obstructing traffic on November 10, 1917; that picket was part of a protest of Alice Paul's sentence. The charge of obstructing traffic was widely understood as a sham - the Oregonian newspaper clearly stated as much in an article covering the Gram sisters' arrest. In the same article, Betty was quoted as saying, \"I am going to prison for the suffrage case, with the full knowledge that I have committed no offense.\" \n\nInstead of being sent to the local jail, the November 10th picketers were sent to the Occoquan Workhouse in Virginia, where the inmates were forced to do prison labor. Horrible conditions faced the women in the workhouse - they were beaten, kept in isolation, and given rancid food. After being refused recognition as political prisoners, the women declared a hunger strike. The leaders of the NWP, including Alice Paul, were force fed. The Gram sisters were released after 17 days in jail, along with 22 other hunger strikers, although they had been sentenced to 30 days.\n\nThe arrests and hunger strike were widely covered in newspapers across the country and Betty was featured prominently. The Oregonian noted that, by the time they were released, Betty and Alice had lost 12 and 19 lbs respectively. But their arrests and ill-treatment did not deter the Grams or the other picketers, who continue pressuring President Wilson and Congress to take up a constitutional amendment granting women the right to vote. In January 1919, Betty Gram was one of two sentries outside the White House standing guard over a bonfire in which they burned President Wilson's speeches. The next month she was arrested in Boston when 21 members of the National Woman's Party gathered in front of the State House with suffrage banners. They were there to protest President Wilson's arrival in Boston. The women were arrested when they refused to comply with police orders to move. In newspaper accounts, Betty was singled out as resisting arrest and had to be \"lifted bodily\" into the police wagon.\n\nAfter multiple attempts through 1918 and the beginning of 1919, on June 4, 1919, Congress finally passed the Nineteenth Amendment to the United States Constitution granting women the right to vote, pending ratification by the states. Betty Gram and the National Woman's Party quickly shifted their efforts from the White House to the states. Betty became a leader in the ratification campaign, working in New Jersey, Massachusetts, Delaware, West Virginia, Tennessee, and other states. In January 1920, suffragists defeated a New Jersey referendum movement that would have put the question of ratification directly to voters. Delaware became an important battleground state, with an active anti-suffrage movement. According to the Boston Herald, the NWP sent its \"most experienced ratification campaigners\" there, including Betty Gram. In June 1920, suffragists picketed the Republican National Convention in Chicago, hoping to pressure the Republican legislature of Delaware and Republican governors of Connecticut and Vermont for their failure to ratify the amendment. The Cleveland Plain Dealer, covering the convention, described Betty Gram as \"young, vivacious, and pretty\" and referenced the miniature steel jail door she wore as a breast pin, symbolizing her time in jail for the cause of women's suffrage. The NWP did not picket the Democratic National Convention in San Francisco.\n\nIn July 1920, Betty Gram was on her way to Tennessee to capture the 36th, and last, state needed for ratification. Her hometown paper, the Oregonian, interviewed her on her work to get women the right to vote. Betty said, \"I joined the picket group because I realized suffrage was at a standstill and unless something dramatic was done, nothing could be accomplished.\". On August 18, 1920, Tennessee became the 36th state to ratify the 19th Amendment to the US Constitution. The long battle was won.\n\nHaving finally succeeded in their years-long effort, the National Woman's Party and other women's groups had to reconsider the makeup and mission of their organizations. In addition, the NWP lost some members as women focused on their careers or other causes. Still others \"found that marriage and family responsibilities at least temporarily curtailed their NWP activities.\" Shortly after the ratification of the 19th amendment, Betty Gram set off for Europe to resume her music studies. After marrying Raymond Swing in 1921, her husband's career as a radio commentator and foreign correspondent took them to England, where they lived for over a decade. Now Betty Gram Swing, Betty continued her work for women's rights in the UK and in the international arena.\n\nEven with the right to vote, there were many other barriers to the full participation of women in business and society. The NWP, the League of Women Voters, the Business and Professional Women, the Women's Trade Union League and other groups all worked to improve women's lives, but they had very different philosophies and approaches. The NWP faction are referred to as \"equalitarian feminists\", because they adopted a platform of equal rights for women, as opposed to the \"social feminists\" who supported a broader set of humanitarian social reforms, including child labor laws and protectionist legislation such as limiting a woman's work week, prohibiting night work for women, and a minimum wage for women and children. There were other legal issues facing women as well, such as the right to serve on a jury, equal rights of inheritance and property ownership, and questions of a married woman's citizenship independent of her husband's. However, like the fight for suffrage, much disagreement remained among the various feminist groups as to the best way to achieve economic opportunity or economic independence for women. \nAn essential element of this rift concerned employment rights for married women, specifically labor laws protecting women versus NWP's “equality” argument. The NWP was against protectionist legislation, arguing it hurt women in the workplace by causing employers not to hire them. As historian Susan Becker describes the feminist movement in the 1920s and 1930s the NWP \"stood alone throughout these two decades in its single-minded dedication to obtaining legal equality between men and women.\" \n\nIn 1926, Betty Gram Swing, along with two other American members of the NWP, formed an English outpost of the group in order to aid British feminists in their efforts to repeal the laws limiting women's voting rights to women over the age of 30. Having also fought (and picketed) for equal pay for women and married women's employment rights during her years in England, Betty Gram Swing returned to the United States in 1934. In the fall of that year, she gave a speech to the Inter-American Commission of Woman at the Pan-American Union warning that women would lose their hard-earned rights unless they were ready to fight for them. A Washington Post article covering the speech described Betty as one of \"the youngest and most beautiful leaders in international feminism.\" The article notes that Betty was focused at that time on having the Equal Rights Treaty \"ratified and adhered to by all nations.\"\n\nIn the post-suffrage time period, American women's groups divided their attention between new legislative agendas, the Equal Rights Amendment (either supporting or opposing it) and the growing focus on international women's issues, highlighted by the NWP's efforts to lobby for approval of the Equal Rights Treaty, which was modeled on the Equal Rights Amendment but was intended as an international document to be ratified by nations. In the 1930s both equalitarian and social feminist agendas were hampered by the Great Depression and the rise of fascism.\n\nThe NWP drafted the first version of the Equal Rights Amendment in 1921 and it provoked disputes almost immediately, even before it was introduced in Congress in 1923. It was addressed in Congressional subcommittees throughout the 1920s and 1930s but was not reported out until 1936. Hearings and committee reports followed into the 1940s, with the League of Women Voters and other social feminist groups consistently opposing it. At the same time, the NWP was working internationally on the Treaty, which was presented to the Sixth Pan American conference in 1928 and, unofficially, at international talks in Paris. (With shades of their picketing days, Betty Gram Swing, NWP veteran Doris Stevens, and a few others attempted to present the Equal Rights Treaty to President Gaston Doumergue of France, while he was hosting negotiations of the Kellogg-Briand Pact. They were refused entry and when they refused to leave the gates, they were held in custody for a few hours.)\n\nEven though the United States was not a member of the League of Nations, the NWP focused its Treaty efforts there, using its relationships with Latin American women's groups for support. Again, social feminists opposed the Treaty because of they believed it would undermine international protectionist labor laws benefitting women. Ultimately, the League of Nations reported the Treaty back to national governments, women's groups, and the International Labor Office, which was a social feminist organization. With the NWP excluded from the decision-making, in 1938 Alice Paul formed the World Women's Party with European equalitarian feminists but the organization was unable to make great strides in the near-term because of the outbreak of World War II.\n\nThe post-war battles between women’s groups resolved themselves along the same lines as the pre-war battles, with the NWP and equalitarians on one side and social feminists on the other. This time, they would battle over the establishment of the UN Commission on the Status of Women. NWP's position as a strong backer of a UN Commission placed it at odds with other groups, including the League of Women Voters.\n\nIn 1946, Betty was in London as a National Woman's Party representative to the World Women’s Party along with British feminist Emmeline Pethick-Lawrence. The NWP supported international efforts like creating a UN commission on women partly in the hope that it would secure the ERA at home. Others opposed those efforts for the same reason. Eleanor Roosevelt, who was anti-ERA and viewed as an opponent of the NWP, was a representative to the UN Conference. These disputes left American women on the sidelines to some extent and the effort to create a UN commission for women was led by New Zealand. On February 16, 1946 the sub-Commission on the Status of Women was formed as part of the Human Rights Commission, and granted full status after its first meeting. Betty's work at the UN was cited in a December 1946 newspaper article on the World Women's Party and the UN General Assembly's resolution calling for equality of political rights for all women. A small collection of Betty Gram Swing's papers are housed at the Arthur and Elizabeth Schlesinger Library on the History of Women in America.\n\nAlice Paul, National Woman's Party, Suffrage, Nineteenth Amendment, Equal Rights Amendment, Equal Rights Treaty, UN Commission on the Status of Women, Carrie Chapman Catt, National American Women's Suffrage Association (NAWSA), Six Point Group, Doris Stevens\n\n"}
{"id": "5951960", "url": "https://en.wikipedia.org/wiki?curid=5951960", "title": "Cashiering", "text": "Cashiering\n\nCashiering (or degradation ceremony), generally within military forces, is a ritual dismissal of an individual from some position of responsibility for a breach of discipline.\n\nFrom the Flemish 'Kasseren' the phrase entered the English language in the late 16th century, during the wars in the Low Countries. Although the O.E.D. states that the first printed use in this sense appears in Shakespeare's \"Othello\" (1603), it appeared in the 1595 tract \"The Estate of English Fugitives\" by Lewes Lewkenor, 'imploring his help and assistance in so hard an extremity, who for recompence, very charitably cashiered them all without the receipt of one penny'.\n\nIt is especially associated with the public degradation of disgraced military officers. Prior to World War I this aspect of cashiering sometimes involved a parade-ground ceremony in front of assembled troops with the destruction of symbols of status: epaulettes ripped off shoulders, badges and insignia stripped, swords broken, caps knocked away, and medals torn off and dashed upon the ground.\n\nThe term originated in the era when British Army officers generally bought their commissions; being cashiered meant that the amount they had paid was lost, as they could not \"sell-out\" afterwards. Essentially, the commission purchase price was a cash bond for good behaviour, forfeited to the Army's cashiers (accountants) in the event of cowardice, desertion or gross misbehaviour.\n\nFamous victims of cashiering include Francis Mitchell (1621), Thomas Cochrane, 10th Earl of Dundonald (after the Great Stock Exchange Fraud of 1814), Alfred Dreyfus (1894, see trial and conviction of Alfred Dreyfus and Dreyfus affair), and Philippe Pétain (1945).\n\nWhile most closely associated with Captain Dreyfus, the ceremony of formal degradation (\"Degradation militaire\") occurred several times in the French military under the Third Republic. At least one other army officer and a naval officer were subjected to the ritual of having their swords broken and the insignia, braid and buttons publicly torn from their uniforms, after being found guilty of charges of treason. More commonly a number of NCOs and private soldiers underwent similar punishments for committing various serious offenses, before execution or imprisonment.\n\n"}
{"id": "42243433", "url": "https://en.wikipedia.org/wiki?curid=42243433", "title": "Chanchala", "text": "Chanchala\n\nChanchala is a Sanskrit adjective basically referring to the unsteady vacillating nature of human mind and actions which need to be stilled, neutralized or controlled for gaining right speech and vision.\n\nChanchala (Sanskrit: चञ्चल) means - 'inconsiderate', 'nimble', 'shaking', 'inconstant', 'moveable', 'flickering', 'moving', 'unsteady', 'fortune', 'wind', 'long pepper', 'goddess of fortune' \n\n\"Chanchala\" is the good word for 'vacillation' in Sanskrit language; in poetry the girl with the dancing eyes is called \"chanchalakshi\", which is considered to be rare attribute. However, as part of the literary evidence of Kusana period, the word \"Chanchala\", like \"Dhavani\" and \"Rodini\", indicates the nature or action of Mother goddess. In the Bhagavad Gita(Sloka VI.26):\nthe word Chanchala used in the first line refers to the restless and the unsteady mind that wanders away.\n\nDasam Granth, which like the Guru Granth Sahib is an important book of Sikhism, it is not composed in ragas (its first composition dates 1684 A.D.) tells us that \"Chanchala\" is the name a \"chhand\" or metre of sixteen syllables having \"ragan\", \"jagan\", \"ragan\", \"jagan\" and \"laghu\" consecutively in each quarter, this metre is also known as \"Chitra\", \"Biraj and \"Brahmrupak\", and has been used twice in \"Choubis Autar\".\n\n\"Chanchala\", meaning, 'the fickle-fortune', is one of the many names of Lakshmi. There is no mention of \"Lakshmi\" in the Rig Veda. Sri of the Rig Veda is deified as a personified being in the Yajurveda, and in the Atharvaveda (I.18) she is prayed to secure prosperity. \"Jatavedas Agni\" is repeatedly asked to make the goddess come to the votary; the epithet \"anapagamini\" reflects the \"chanchala\" i.e. fleet or fickle aspect of the goddess. \"Lakshmi\" or \"Chanchala\" as the mobile one associates only with the rich and the dynamic, no matter what their caste, creed or colour. Because \"Lakshmi\" is \"chanchala\" i.e. quick on her feet, to make her \"achala\" i.e. 'immobile', she needs to be worshipped quietly so that she does not get distracted.\n\nIn Yoga, \"vritti\" indicates the contents of mental awareness that are disturbances in the medium of consciousness. The \"vrittis\" of the \"gunas\" are ever-active and swift, the \"gunas\" serve as parts of \"buddhi\", their habitual conduct is fickle, restless, tremulous (\"chanchala\") activity, which activity can be controlled through \"Abhyasa\", \"Vairagya\" and \"Ishvarapranidhana\". \"Sri Narada Pancharatnam\" (Sloka VIII.15) tells us that \"Chanchala\" is the \"nadi\" which along with \"Medhya\" resides in the \"Visuddha Chakra\" on the throat.\n"}
{"id": "9726169", "url": "https://en.wikipedia.org/wiki?curid=9726169", "title": "Charity (name)", "text": "Charity (name)\n\nCharity is an English feminine given name derived from the English word \"charity\". It was used by the Puritans as a virtue name. An earlier form of the name, Caritas, was an early Christian name in use by Romans.\n\nCharity is also the usual English form of the name of Saint Charity, an early Christian child martyr, who was tortured to death with her sisters Faith and Hope. She is known as Agape in Biblical Greek and as Caritas in Church Latin and her name is translated differently in other languages.\n\nFaith, Hope and Charity, the three theological virtues, are names traditionally given to triplet girls, just as Faith and Hope remain common names for twin girls. One example were the American triplets Faith, Hope and Charity Cardwell, who were born in 1899 in Texas and were recognized in 1994 by the Guinness Book of World Records as the world's longest lived triplets.\n\nCharity has never been as popular a name in the United States as Faith or Hope. It ranked in the top 500 names for American girls between 1880 and 1898 and in the top 1,000 between 1880 and 1927, when it disappeared from the top 1,000 names until it reemerged among the top 1,000 names in 1968 at No. 968. It was most popular between 1973 and 1986, when it ranked among the top 300 names in the United States. It has since declined in popularity and was ranked at No. 852 in 2011.\n\nThis is a list of people and characters named Charity:\n\n\n"}
{"id": "1530689", "url": "https://en.wikipedia.org/wiki?curid=1530689", "title": "Complementarity (physics)", "text": "Complementarity (physics)\n\nIn physics, complementarity is both a theoretical and an experimental result of quantum mechanics, also referred to as principle of complementarity. It holds that objects have certain pairs of complementary properties which cannot all be observed or measured simultaneously.\n\nThe complementarity principle was formulated by Niels Bohr, a leading founder of quantum mechanics. Examples of complementary properties that Bohr considered:\n\n\nFor example, the particle and wave aspects of physical objects are such complementary phenomena. Both concepts are borrowed from classical mechanics, where it is impossible to be a particle and wave at the same time. Therefore, it is impossible to measure the \"full\" properties of the wave and particle at a particular moment. Moreover, Bohr implies that it is not possible to regard objects governed by quantum mechanics as having intrinsic properties independent of determination with a measuring device, a viewpoint supported by the Kochen-Specker theorem. The type of measurement determines which property is shown. However the single and double-slit experiment and other experiments show that \"some\" effects of wave and particle can be measured in one measurement.\n\nAn aspect of complementarity is that it not only applies to measurability or knowability of some property of a physical entity, but more importantly it applies to the limitations of that physical entity’s very manifestation of the property in the physical world. All properties of physical entities exist only in pairs, which Bohr described as complementary or conjugate pairs. Physical reality is determined and defined by manifestations of properties which are limited by trade-offs between these complementary pairs. For example, an electron can manifest a greater and greater accuracy of its position only in even trade for a complementary loss in accuracy of manifesting its momentum. This means that there is a limitation on the precision with which an electron can possess (i.e., manifest) position, since an infinitely precise position would dictate that its manifested momentum would be infinitely imprecise, or undefined (i.e., non-manifest or not possessed), which is not possible. The ultimate limitations in precision of property manifestations are quantified by the Heisenberg uncertainty principle and Planck units. Complementarity and Uncertainty dictate that therefore all properties and actions in the physical world manifest themselves as non-deterministic to some degree.\n\nPhysicists F.A.M. Frescura and Basil Hiley have summarized the reasons for the introduction of the principle of complementarity in physics as follows:\nThe emergence of complementarity in a system occurs when one considers the circumstances under which one attempts to measure its properties; as Bohr noted, the principle of complementarity \"implies the impossibility of any sharp separation between the behaviour of atomic objects and the interaction with the measuring instruments that serve to define the conditions under which the phenomena appear.\" It is important to distinguish, as did Bohr in his original statements, the principle of complementarity from a statement of the uncertainty principle. For a technical discussion of contemporary issues surrounding complementarity in physics see, e.g., Bandyopadhyay (2000), from which parts of this discussion were drawn.\n\nIn his original lecture on the topic, Bohr pointed out that just as the finitude of the speed of light implies the impossibility of a sharp separation between space and time (relativity), the finitude of the quantum of action implies the impossibility of a sharp separation between the behavior of a system and its interaction with the measuring instruments and leads to the well known difficulties with the concept of 'state' in quantum theory; the notion of complementarity is intended to symbolize this new situation in epistemology created by quantum theory. Some people consider it a philosophical adjunct to quantum mechanics, while others consider it to be a discovery that is as important as the formal aspects of quantum theory. Examples of the latter include Leon Rosenfeld, who claimed that \"[C]omplementarity is not a philosophical superstructure invented by Bohr to be placed as a decoration on top of the quantal formalism, it is the bedrock of the quantal description.\", and John Wheeler, who opined that \"Bohr's principle of complementarity is the most revolutionary scientific concept of this century and the heart of his fifty-year search for the full significance of the quantum idea.\"\n\nThe quintessential example of wave–particle complementarity in the laboratory is the double slit. The crux of the complementary behavior is the question: \"What information exists – embedded in the constituents of the universe – that can reveal the history of the signal particles as they pass through the double slit?\" If information exists (even if it is not measured by a conscious observer) that reveals \"which slit\" each particle traversed, then each particle will exhibit no wave interference with the other slit. This is the particle-like behavior. But if \"no information\" exists about which slit – so that no conscious observer, no matter how well equipped, will ever be able to determine which slit each particle traverses – then the signal particles will interfere with themselves as if they traveled through both slits at the same time, as a wave. This is the wave-like behavior. These behaviors are complementary, according to the Englert–Greenberger duality relation, because when one behavior is observed the other is absent. Both behaviors \"can\" be observed at the same time, but each only as lesser manifestations of their full behavior (as determined by the duality relation). This superposition of complementary behaviors exists whenever there is partial \"which slit\" information. While there is some contention to the duality relation, and thus complementarity itself, the contrary position is not accepted by mainstream physics. Double slit experiments with single photons show clearly that photons are particles at the same time as they are waves. Photons impact the screen where they are detected in points, and when enough points have accumulated the wave aspect is clearly visible. Also the particle and wave aspect is seen at the same time in photons that are stationary.\n\nVarious neutron interferometry experiments demonstrate the subtlety of the notions of duality and complementarity. By passing through the interferometer, the neutron appears to act as a wave. Yet upon passage, the neutron is subject to gravitation. As the neutron interferometer is rotated through Earth's gravitational field a phase change between the two arms of the interferometer can be observed, accompanied by a change in the constructive and destructive interference of the neutron waves on exit from the interferometer. Some interpretations claim that understanding the interference effect requires one to concede that a single neutron takes both paths through the interferometer at the same time; a single neutron would \"be in two places at once\", as it were. Since the two paths through a neutron interferometer can be as far as to apart, the effect is hardly microscopic. This is similar to traditional double-slit and mirror interferometer experiments where the slits (or mirrors) can be arbitrarily far apart. So, in interference and diffraction experiments, neutrons behave the same way as photons (or electrons) of corresponding wavelength.\n\nNiels Bohr apparently conceived of the principle of complementarity during a skiing vacation in Norway in February and March 1927, during which he received a letter from Werner Heisenberg regarding the latter's newly discovered (and not yet published) uncertainty principle. Upon returning from his vacation, by which time Heisenberg had already submitted his paper on the uncertainty principle for publication, he convinced Heisenberg that the uncertainty principle was a manifestation of the deeper concept of complementarity. Heisenberg duly appended a note to this effect to his paper on the uncertainty principle, before its publication, stating:\n\nBohr has brought to my attention [that] the uncertainty in our observation does not arise exclusively from the occurrence of discontinuities, but is tied directly to the demand that we ascribe equal validity to the quite different experiments which show up in the [particulate] theory on one hand, and in the wave theory on the other hand.\n\nBohr publicly introduced the principle of complementarity in a lecture he delivered on 16 September 1927 at the International Physics Congress held in Como, Italy, attended by most of the leading physicists of the era, with the notable exceptions of Einstein, Schrödinger, and Dirac. However, these three were in attendance one month later when Bohr again presented the principle at the Fifth Solvay Congress in Brussels, Belgium. The lecture was published in the proceedings of both of these conferences, and was republished the following year in \"Naturwissenschaften\" (in German) and in \"Nature\" (in English).\n\nAn article written by Bohr in 1949 titled \"Discussions with Einstein on Epistemological Problems in Atomic Physics\" is considered by many to be a definitive description of the notion of complementarity.\n\n\n\n"}
{"id": "1357045", "url": "https://en.wikipedia.org/wiki?curid=1357045", "title": "Complex event processing", "text": "Complex event processing\n\nEvent processing is a method of tracking and analyzing (processing) streams of information (data) about things that happen (events), and deriving a conclusion from them. Complex event processing, or CEP, is event processing that combines data from multiple sources to infer events or patterns that suggest more complicated circumstances. The goal of complex event processing is to identify meaningful events (such as opportunities or threats) and respond to them as quickly as possible.\n\nThese events may be happening across the various layers of an organization as sales leads, orders or customer service calls. Or, they may be news items, text messages, social media posts, stock market feeds, traffic reports, weather reports, or other kinds of data. An event may also be defined as a \"change of state,\" when a measurement exceeds a predefined threshold of time, temperature, or other value. Analysts suggest that CEP will give organizations a new way to analyze patterns in real-time and help the business side communicate better with IT and service departments.\n\nThe vast amount of information available about events is sometimes referred to as the event cloud.\n\nAmong thousands of incoming events, a monitoring system may for instance receive the following three from the same source:\n\n\nFrom these events the monitoring system may infer a \"complex event\": a wedding. CEP as a technique helps discover complex events by analyzing and correlating other events: the bells, the man and woman in wedding attire and the rice flying through the air.\n\nCEP relies on a number of techniques, including:\n\n\nCommercial applications of CEP exist in variety of industries and include algorithmic stock-trading, the detection of credit-card fraud, business activity monitoring, and security monitoring.\n\nThe CEP area has roots in discrete event simulation, the active database area and some programming languages. The activity in the industry was preceded by a wave of research projects in the 1990s. According to the first project that paved the way to a generic CEP language and execution model was the Rapide project in Stanford University, directed by David Luckham. In parallel there have been two other research projects: Infospheres in California Institute of Technology, directed by K. Mani Chandy, and Apama in University of Cambridge directed by John Bates. The commercial products were dependents of the concepts developed in these and some later research projects. Community efforts started in a series of event processing symposiums organized by the Event Processing Technical Society, and later by the ACM DEBS conference series. One of the community efforts was to produce the event processing manifesto.\n\nCEP is used in operational intelligence (OI) products to provide insight into business operations by running query analysis against live feeds and event data. OI collects real-time data and correlates against historical data to provide insight and analysis. Multiple sources of data can be combined to provide a common operating picture that uses current information. \n\nIn network management, systems management, application management and service management, people usually refer instead to event correlation. As CEP engines, event correlation engines (\"event correlators\") analyze a mass of events, pinpoint the most significant ones, and trigger actions. However, most of them do not produce new inferred events. Instead, they relate high-level events with low-level events.\n\nInference engines, e.g., rule-based reasoning engines, typically produce inferred information in artificial intelligence. However, they do not usually produce new information in the form of complex (i.e., inferred) events.\n\nA more systemic example of CEP involves a car, some sensors and various events and reactions. Imagine that a car has several sensors—one that measures tire pressure, one that measures speed, and one that detects if someone sits on a seat or leaves a seat.\n\nIn the first situation, the car is moving and the pressure of one of the tires moves from 45 psi to 41 psi over 15 minutes. As the pressure in the tire is decreasing, a series of events containing the tire pressure is generated. In addition, a series of events containing the speed of the car is generated. The car's Event Processor may detect a situation whereby a loss of tire pressure over a relatively long period of time results in the creation of the \"lossOfTirePressure\" event. This new event may trigger a reaction process to note the pressure loss into the car's maintenance log, and alert the driver via the car's portal that the tire pressure has reduced.\n\nIn the second situation, the car is moving and the pressure of one of the tires drops from 45 psi to 20 psi in 5 seconds. A different situation is detected—perhaps because the loss of pressure occurred over a shorter period of time, or perhaps because the difference in values between each event were larger than a predefined limit. The different situation results in a new event \"blowOutTire\" being generated. This new event triggers a different reaction process to immediately alert the driver and to initiate onboard computer routines to assist the driver in bringing the car to a stop without losing control through skidding.\n\nIn addition, events that represent detected situations can also be combined with other events in order to detect more complex situations. For example, in the final situation the car is moving normally and suffers a blown tire which results in the car leaving the road and striking a tree, and the driver is thrown from the car. A series of different situations are rapidly detected. The combination of \"blowOutTire\", \"zeroSpeed\" and \"driverLeftSeat\" within a very short period of time results in a new situation being detected: \"occupantThrownAccident\". Even though there is no direct measurement that can determine conclusively that the driver was thrown, or that there was an accident, the combination of events allows the situation to be detected and a new event to be created to signify the detected situation. This is the essence of a complex (or composite) event. It is complex because one cannot directly detect the situation; one has to infer or deduce that the situation has occurred from a combination of other events.\n\nA natural fit for CEP has been with business process management (BPM). BPM focuses on end-to-end business processes, in order to continuously optimize and align for its operational environment.\n\nHowever, the optimization of a business does not rely solely upon its individual, end-to-end processes. Seemingly disparate processes can affect each other significantly. Consider this scenario:\nIn the aerospace industry, it is good practice to monitor breakdowns of vehicles to look for trends (determine potential weaknesses in manufacturing processes, material, etc.). Another separate process monitors current operational vehicles' life cycles and decommissions them when appropriate. One use for CEP is to link these separate processes, so that in the case of the initial process (breakdown monitoring) discovering a malfunction based on metal fatigue (a significant event), an action can be created to exploit the second process (life cycle) to issue a recall on vehicles using the same batch of metal discovered as faulty in the initial process.\n\nThe integration of CEP and BPM must exist at two levels, both at the business awareness level (users must understand the potential holistic benefits of their individual processes) and also at the technological level (there needs to be a method by which CEP can interact with BPM implementation). For a recent state of the art review on the integration of CEP with BPM, which is frequently labeled as Event-Driven Business Process Management, refer to.\n\nComputation-oriented CEP's role can arguably be seen to overlap with Business Rule technology.\n\nFor example, customer service centers are using CEP for click-stream analysis and customer experience management. CEP software can factor real-time information about millions of events (clicks or other interactions) per second into business intelligence and other decision-support applications. These \"recommendation applications\" help agents provide personalized service based on each customer's experience. The CEP application may collect data about what customers on the phone are currently doing, or how they have recently interacted with the company in other various channels, including in-branch, or on the Web via self-service features, instant messaging and email. The application then analyzes the total customer experience and recommends scripts or next steps that guide the agent on the phone, and hopefully keep the customer happy.\n\nThe financial services industry was an early adopter of CEP technology, using complex event processing to structure and contextualize available data so that it could inform trading behavior, specifically algorithmic trading, by identifying opportunities or threats that indicate traders (or automatic trading systems) should buy or sell. For example, if a trader wants to track stocks that have five up movements followed by four down movements, CEP technology can track such an event. CEP technology can also track drastic rise and fall in number of trades. Algorithmic trading is already a practice in stock trading. It is estimated that around 60% of Equity trading in the United States is by way of algorithmic trades. CEP is expected to continue to help financial institutions improve their algorithms and be more efficient.\n\nRecent improvements in CEP technologies have made it more affordable, helping smaller firms to create trading algorithms of their own and compete with larger firms. CEP has evolved from an emerging technology to an essential platform of many capital markets. The technology's most consistent growth has been in banking, serving fraud detection, online banking, and multichannel marketing initiatives.\n\nToday, a wide variety of financial applications use CEP, including profit, loss, and risk management systems, order and liquidity analysis, quantitative trading and signal generation systems, and others.\n\nA time series database is a software system that is optimized for the handling of data organized by time. Time series are finite or infinite sequences of data items, where each item has an associated timestamp and the sequence of timestamps is non-decreasing. Elements of a time series are often called ticks. The timestamps are not required to be ascending (merely non-decreasing) because in practice the time resolution of some systems such as financial data sources can be quite low (milliseconds, microseconds or even nanoseconds), so consecutive events may carry equal timestamps.\n\nTime series data provides a historical context to the analysis typically associated with complex event processing. This can apply to any vertical industry such as finance and cooperatively with other technologies such as BPM.\n\nConsider the scenario in finance where there is a need to understand historic price volatility to determine statistical thresholds of future price movements. This is helpful for both trade models and transaction cost analysis.\n\nThe ideal case for CEP analysis is to view historical time series and real-time streaming data as a single time continuum. What happened yesterday, last week or last month is simply an extension of what is occurring today and what may occur in the future. An example may involve comparing current market volumes to historic volumes, prices and volatility for trade execution logic. Or the need to act upon live market prices may involve comparisons to benchmarks that include sector and index movements, whose intra-day and historic trends gauge volatility and smooth outliers.\n\nComplex event processing is a key enabler in Internet of Things (IoT) settings and Smart Cyber-physical systems (CPS) as well. Processing dense and heterogeneous streams from various sensors and matching patterns against those streams is a typical task in such cases. The majority of these techniques rely on the fact that representing the IoT system's state and its changes is more efficient in the form of a data stream, instead of having a static, materialized model. Reasoning over such stream-based models fundamentally differs from traditional reasoning techniques and typically require the combination of model transformations and CEP.\n\n\n\n"}
{"id": "15938221", "url": "https://en.wikipedia.org/wiki?curid=15938221", "title": "Darwin among the Machines", "text": "Darwin among the Machines\n\n\"Darwin among the Machines\" is the name of an article published in \"The Press\" newspaper on 13 June 1863 in Christchurch, New Zealand, which references the work of Charles Darwin in the title. Written by Samuel Butler but signed \"Cellarius\" (q.v.), the article raised the possibility that machines were a kind of \"mechanical life\" undergoing constant evolution, and that eventually machines might supplant humans as the dominant species:\nThe article ends by urging that, \"War to the death should be instantly proclaimed against them. Every machine of every sort should be destroyed by the well-wisher of his species. Let there be no exceptions made, no quarter shown; let us at once go back to the primeval condition of the race.\"\n\nButler developed this and subsequent articles into \"The Book of the Machines\", three chapters of \"Erewhon\", published anonymously in 1872. The Erewhonian society Butler envisioned had long ago undergone a revolution that destroyed most mechanical inventions. The narrator of the story finds a book that details the reasons for this revolution, which he translates for the reader. In \nchapter xxiii: the book of the machines, a number of quotes from this imaginary book discuss the possibility of machine consciousness:\n\nLater, in chapter xxiv: the machines—continued, the imaginary book also discusses the notion that machines can \"reproduce\" like living organisms:\n\nThis notion of machine \"reproduction\" anticipates the later notion of self-replicating machines, although in chapter xxv: the machines—concluded, the imaginary book supposes that while there is a danger that humans will become subservient to machines, the machines will still need humans to assist in their reproduction and maintenance:\n\nThe author of the imaginary book goes on to say that while life under machine rule might be materially comfortable for humans, the thought of the human race being superseded in the future is just as horrifying to him as the thought that his distant ancestors were anything other than fully human (apparently Butler imagines the author to be an Anti-evolutionist), so he urges that all machines which have been in use for less than 300 years be destroyed to prevent this future from coming to pass:\n\nErewhonian society came to the conclusion \"...that the machines were ultimately destined to supplant the race of man, and to become instinct with a vitality as different from, and superior to, that of animals, as animal to vegetable life. So... they made a clean sweep of all machinery that had not been in use for more than two hundred and seventy-one years...\" (from \nchapter ix: to the metropolis.)\n\nDespite the initial popularity of \"Erewhon\", Butler commented in the preface to the second edition that reviewers had \"in some cases been inclined to treat the chapters on Machines as an attempt to reduce Mr. Darwin’s theory to an absurdity.\" He protested that \"few things would be more distasteful to me than any attempt to laugh at Mr. Darwin\", but also added \"I am surprised, however, that the book at which such an example of the specious misuse of analogy would seem most naturally levelled should have occurred to no reviewer; neither shall I mention the name of the book here, though I should fancy that the hint given will suffice\", which may suggest that the chapter on Machines was in fact a satire intended to illustrate the \"specious misuse of analogy\", even if the target was not Darwin; Butler, fearing that he had offended Darwin, wrote him a letter explaining that the actual target was Joseph Butler's 1736 \"The Analogy of Religion, Natural and Revealed, to the Constitution and Course of Nature\". The Victorian scholar Herbert Sussman has suggested that although Butler's exploration of machine evolution was intended to be whimsical, he may also have been genuinely interested in the notion that living organisms are a type of mechanism and was exploring this notion with his writings on machines, while the philosopher Louis Flaccus called it \"a mixture of fun, satire, and thoughtful speculation.\"\n\nGeorge Dyson applies Butler's original premise to the artificial life and intelligence of Alan Turing in Darwin Among the Machines: The Evolution of Global Intelligence (1998) , to suggest that the internet is a living, sentient being.\n\nDyson's main claim is that the evolution of a conscious mind from today's technology is inevitable. It is not clear whether this will be a single mind or multiple minds, how smart that mind would be, and even if we will be able to communicate with it. He also clearly suggests that there are forms of intelligence on Earth that we are currently unable to understand.\n\nFrom the book: \"What mind, if any, will become apprehensive of the great coiling of ideas now under way is not a meaningless question, but it is still too early in the game to expect an answer that is meaningful to us.\"\n\nThe theme of humanity at war or otherwise in conflict with machines is found in a number of later creative works:\n\n\n\n\n"}
{"id": "588531", "url": "https://en.wikipedia.org/wiki?curid=588531", "title": "Différance", "text": "Différance\n\nDerrida first uses the term in his 1963 paper \"\". The term then played a key role in Derrida's engagement with the philosophy of Edmund Husserl in \"Speech and Phenomena\". The term was then elaborated in various other works, notably in his essay \"\" and in various interviews collected in \"Positions\".\n\nThe of is a deliberate misspelling of , though the two are pronounced identically, ( plays on the fact that the French word means both \"to defer\" and \"to differ\"). This misspelling highlights the fact that its written form is not heard, and serves to further subvert the traditional privileging of speech over writing (see archi-writing and logocentrism), as well as the distinction between the sensible and the intelligible. The difference articulated by the in is not apparent to the senses via sound, \"but neither cannot it belong to intelligibility, to the ideality which is not fortuitously associated with the objectivity of \"theorein\" or understanding.\" This is because the language of understanding is already caught up in sensible metaphors—for example, () means \"to see\" in Ancient Greek.\n\nIn the essay \"\" Derrida indicates that gestures at a number of heterogeneous features that govern the production of textual meaning. The first (relating to deferral) is the notion that words and signs can never fully summon forth what they mean, but can only be defined through appeal to additional words, from which they differ. Thus, meaning is forever \"deferred\" or postponed through an endless chain of signifiers. The second (relating to difference, sometimes referred to as \"espacement\" or \"spacing\") concerns the force that differentiates elements from one another, and in so doing engenders binary oppositions and hierarchies that underpin meaning itself.\n\nDerrida developed the concept of deeper in the course of an argument against the phenomenology of Husserl, who sought a rigorous analysis of the role of memory and perception in our understanding of sequential items such as music or language. Derrida's approach argues that because the perceiver's mental state is constantly in flux and differs from one re-reading to the next, a general theory describing this phenomenon is unachievable.\n\nA term related to the idea of in Derrida's thought is the \"supplement\", \"itself bound up in a supplementary play of meaning which defies semantic reduction.\"\n\nDerrida approaches texts as constructed around elemental oppositions which all speech has to articulate if it intends to make any sense whatsoever. This is so because identity is viewed in non-essentialist terms as a construct, and because constructs only produce meaning through the interplay of differences inside a \"system of distinct signs\". This approach to text, in a broad sense, emerges from semiology advanced by Ferdinand de Saussure.\n\nSaussure is considered one of the fathers of structuralism when he explained that terms get their meaning in reciprocal determination with other terms inside language:\nSaussure explicitly suggested linguistics was only a branch of a more general semiology, of a science of signs in general, being human codes only one among others. Nevertheless, in the end, as Derrida pointed out, he made of linguistics \"the regulatory model\", and \"for essential, and essentially metaphysical, reasons had to privilege speech, and everything that links the sign to phone\": Derrida will prefer to follow the more \"fruitful paths (formalization)\" of a general semiotics without falling in what he considered \"a hierarchizing teleology\" privileging linguistics, and speak of 'mark' rather than of language, not as something restricted to mankind, but as prelinguistic, as the pure possibility of language, working every where there is a relation to something else.\n\nDerrida sees these differences as elemental oppositions working in all languages, systems of distinct signs, and codes, where terms don't have absolute meanings but instead draw meaning from reciprocal determination with other terms. This structural difference is the first component that Derrida will take into account when articulating the meaning of , a mark he felt the need to create and will become a fundamental tool in his lifelong work: deconstruction:\n\nBut structural difference will not be considered without him already destabilizing from the start its static, synchronic, taxonomic, ahistoric motifs, remembering that all structure already refers to the generative movement in the play of differences:\n\nThe other main component of is deferring, which takes into account the fact that meaning is not only synchrony with all the other terms inside a structure, but also diachrony, with everything that \"was\" and \"will be\" said, in History, difference as structure and deferring as genesis:\nThis confirms the subject as not present to itself and constituted on becoming space, in temporizing and also, as Saussure said, that \"language [which consists only of differences] is not a function of the speaking subject\":\nQuestioned this myth of the presence of meaning in itself (\"objective\") and/or for itself (\"subjective\") Derrida will start a long deconstruction of all texts where conceptual oppositions are put to work in the actual construction of meaning and values based on the subordination of the movement of \"\":\nBut, as Derrida also points out, these relations with other terms express not only meaning but also values. The way elemental oppositions are put to work in all texts it is not only a theoretical operation but also a practical option.\nThe first task of deconstruction, starting with philosophy and afterwards revealing it operating in literary texts, juridical texts, etc., would be to overturn these oppositions:\nIt is not that the final task of deconstruction is to surpass all oppositions, because they are structurally necessary to produce sense. They simply cannot be suspended once and for all. But this does not obviate their need to be analyzed and criticized in all its manifestations, showing the way these oppositions, logical and axiological, are at work in all discourse for it to be able to produce meaning and values.\n\nFor example, the word \"house\" derives its meaning more as a function of how it differs from \"shed\", \"mansion\", \"hotel\", \"building\", etc. (Form of Content, which Louis Hjelmslev distinguished from Form of Expression) than how the word \"house\" may be tied to a certain image of a traditional house (i.e. the relationship between signifier and signified) with each term being established in reciprocal determination with the other terms than by an ostensive description or definition.\n\nWhen can we talk about a \"house\" or a \"mansion\" or a \"shed\"? The same can be said about verbs, in all the world languages: when should we stop saying \"walk\" and start saying \"run\"? The same happens, of course, with adjectives: when must we stop saying \"yellow\" and start saying \"orange\", or stop defining as \"black\" and start saying \"white\", or \"rich\" and \"poor\", \"entrepreneur\" and \"worker\", \"civilized\" and \"primitive\", \"man\" and \"animal\", \"beast\" and \"sovereign\", \"christian\" and \"pagan\", or \"beautiful\" and start saying \"ugly\", or \"bad\" and start saying \"good\", or \"truth\" and start saying \"false\", \"determined\" and \"free\"? Or \"in\" and \"out\", \"here\" and \"there\", \"now\" and \"then\", \"past\" and \"present\" and \"future\" and \"eternal\"? Not only are the topological differences between the words relevant here, but the differentials between what is signified is also covered by différance. Deferral also comes into play, as the words that occur following \"house\" or \"white\" in any expression will revise the meaning of that word, sometimes dramatically so. This is true not only with syntagmatic succession in relation with paradigmatic simultaneity, but also, in a broader sense, between diachronic succession in History related with synchronic simultaneity inside a \"system of distinct signs\".\n\nThus, complete meaning is always \"differential\" and postponed in language; there is never a moment when meaning is complete and total. A simple example would consist of looking up a given word in a dictionary, then proceeding to look up the words found in that word's definition, etc., also comparing with older dictionaries from different periods in time, and such a process would never end.\n\nThis is also true with all ontological oppositions and its many declensions, not only in philosophy as in human sciences in general, cultural studies, theory of Law, et cetera. For example: the intelligible and the sensible, the spontaneous and the receptive, autonomy and heteronomy, the empirical and the transcendental, immanent and transcendent, as the interior and exterior, or the founded and the founder, normal and abnormal, phonetic and writing, analysis and synthesis, the literal sense and figurative meaning in language, reason and madness in psychoanalysis, the masculine and feminine in gender theory, man and animal in ecology, the beast and the sovereign in the political field, theory and practice as distinct dominions of thought itself. In all speeches in fact (and by right) we can make clear how they were dramatized, how the cleavages were made during the centuries, each author giving it different centers and establishing different hierarchies between the terms in the opposition.\n\nIt may seem contradictory to suggest that is neither a word nor a concept. The difference itself between words cannot only be another word. If that is the case then appeals to ontology, creating an even bigger problem. So is either an appeal to an infinite mystery (similar to God in theology) or becomes empty of any and all meaning and is thus rendered superfluous.\n\nWe reside, according to this philosophy, in a web of language, or at least one of interpretation, that has been laid down by tradition and which shifts each time we hear or read an utterance—even if it is the same utterance. and deconstruction are attempts to understand this web of language, to search, in Derrida's words, for the \"other of language\". This \"other of language\" is close to what Anglophone Philosophy calls the Reference of a word. There is a \"deferment\" of meaning with each act of re-reading. There is a \"difference\" of readings with each re-reading. In Derrida's words, \"there is nothing outside the [con]text\" of a word's use and its place in the lexicon. Text, in Derrida's parlance, refers to context and includes all about the \"real-life\" situation of the speech/text (cf. speech act theory).\n\nFor Derrida, the relationship between the Signifier and the Signified is not understood to be exactly like Saussure's. For Derrida, there was a deferral, a continual and indefinite postponement as the Signified can never be achieved. The formation of the linguistic sign is marked by movement, and is not static. The easiest way to understand this is to imagine Saussure's model as a two dimensional plane, where each signified is separated due to the difference in its sound image. (If two sound-images are exactly alike, one could not distinguish between the two.) Each signifier then would be a particular point. Derrida adds a third dimension, time. Now, the act of formation is accounted for. This is not to say that there is no relationship between the two. However, Derrida felt that the old model focused too heavily on the signifier, rather than on utterance and occurrence. The Signifier and the Signified are severed completely and irrevocably.\n\nAn example of this effect occurred in England during the Renaissance, when oranges began to be imported from the Mediterranean. Yellow and red came to be differentiated from a new colour term, \"orange\". What was the meaning of these words before 1600? – What is their meaning afterwards? Such effects go on often in the use of language and frequently this effect forms the basis of language/meaning. Such changes of meaning are also often centres of political violence, as is apparent in the differences invested in male/female, master/slave, citizen/foreigner etc. Derrida seeks to modulate and question these \"violent hierarchies\" through deconstruction.\n\nPerhaps it is a misconception that seeks contradictory meanings. It does not necessarily do so. It can, but what it usually describes is the re-experience, the re-arrival of the moment of reading. Roland Barthes remarked that \"those who fail to reread are obliged to read the same story everywhere\". This wry comment summarizes the phenomenon of different experience for each iteration.\n\nWe are discussing just one text—every text. No distinction is necessarily made between texts in this \"basic\" level. The difference/deferral can be between one text and itself, or between two texts; this is the crucial distinction between traditional perspectives and deconstruction.\n\nDerrida's neographism (rather than neologism because \"neologism\" would propose a \"logos\", a metaphysical category; and (more simply) because, when uttered in French, \"\" is indistinguishable from \"difference\"—it is thus only a graphical modification, having nothing to do with a spoken \"logos\") is, of course, not just an attempt at linguistics or to discuss written texts and how they are read. It is, most importantly, an attempt to escape the history of metaphysics; a history that has always prioritised certain concepts, e.g., those of substance, essence, soul, spirit (idealism), matter (realism), becoming, freedom, sense-experience, language, science etc. All such ideas imply self-presence and totality. Différance, instead, focuses on the play of presence and absence, and, in effecting a concentration of certain thinking, Derrida takes on board the thought of Freud's unconscious (the trace), Heidegger's destruction of ontotheology, Nietzsche's play of forces, and Bataille's notion of sacrifice in contrast to Hegel's .\n\nYet he does not approach this absence and loss with the nostalgia that marks Heidegger's attempt to uncover some original truths beneath the accretions of a false metaphysics that have accumulated since Socrates. Rather it is with the moods of play and affirmation that Derrida approaches the issue.\n\nHowever, Derrida himself never claimed to have escaped from the metaphysics with what he has done. To the contrary, he criticises others for claiming to have demolished metaphysics thoroughly.\n\nDerrida's non-concept of différance, resembles, but is not, negative theology, an attempt to present a tacit metaphysics without pointing to any existent essence as the first cause or transcendental signified. Following his presentation of his paper \"\" in 1968, Derrida was faced with an annoyed participant who said, \"It [différance] is the source of everything and one cannot know it: it is the God of negative theology.\" Derrida's answer was, \"It is and it is not.\"\n\nIn contrast to negative theology, which posits something supereminent and yet concealed and ineffable, is not quite transcendental, never quite \"real\", as it is always and already deferred from being made present. As John Caputo writes, \" is but a quasi-transcendental anteriority, not a supereminent, transcendental ulteriority.\" The differences and deferrings of différance, Derrida points out, are not merely ideal, they are not inscribed in the contours of the brain nor do they fall from the sky, the closest approximation would be to consider them as historical, that is, if the word history itself did not mean what it does, the airbrushing speech of the victor/vanquished.\n\nDerrida has shown an interest in negative or apophatic theology, one of his most important works on the topic being his essay \"Sauf le nom\".\n\nIn \"Of Grammatology\", Derrida states that grammatology is not a \"science of man\" because it is concerned with the question of \"the name of man.\" This leads Derrida into a consideration of the work of André Leroi-Gourhan, and in particular his concepts of \"program,\" \"exteriorisation,\" and \"liberation of memory.\" Derrida writes: \"Leroi-Gourhan no longer describes the unity of man and the human adventure thus by the simple possibility of the in general; rather as a stage or an articulation in the history of life—of what I have called différance—as the history of the .\" Derrida thus explicitly refers the term to life, and in particular to life as the history of inscription and retention, whether this is genetic or technological (from writing to \"electronic card indexes\"). And thus grammatology is not a science of man because it deconstructs any anthropocentrism, in the sense that the inscription in question falls on both sides of the divide human/non-human.\n\nYet, in the article \"\", Derrida refers not to \"physis\", that is, \"life\", but to \"all the others of \"physis\"—\"tekhnè\", \"nomos\", \"thesis\", society, freedom, history, mind, etc.—as \"physis\" differed and deferred, or as \"physis\" differing and deferring.\" Bernard Stiegler argues in his book, \"Technics and Time, 1\", that this represents a hesitation in Derrida: \"Now phusis as life was already différance. There is an indecision, a passage remaining to be thought. At issue is the specificity of the temporality of life in which life is inscription in the nonliving, spacing, temporalisation, differentiation, and deferral by, of and in the nonliving, in the dead.\" What this suggests to Stiegler is that grammatology—a logic of the —must be supplemented with a history of grammatisation, a history of all the forms and techniques of inscription, from genetics to technics, each stage of which will be found to possess its own logic. Only in this way can be thought as the differing and deferral \"of\" life (life as the emergence of a difference from non-life, specifically as the deferral of entropy), \"and\" as the difference \"from\" \"physis\" through which the human must inevitably be defined (the human as the inauguration of \"another\" memory, neither the memory of genetics nor that of the individual, but rather a memory consisting in \"inscription in the nonliving,\" that is, \"technical\" memory).\n\n\n"}
{"id": "5007598", "url": "https://en.wikipedia.org/wiki?curid=5007598", "title": "Ellipsoidal coordinates", "text": "Ellipsoidal coordinates\n\nEllipsoidal coordinates are a three-dimensional orthogonal coordinate system formula_1 that generalizes the two-dimensional elliptic coordinate system. Unlike most three-dimensional orthogonal coordinate systems that feature quadratic coordinate surfaces, the ellipsoidal coordinate system is based on confocal quadrics.\n\nThe Cartesian coordinates formula_2 can be produced from the ellipsoidal coordinates \nformula_3 by the equations\n\nwhere the following limits apply to the coordinates\n\nConsequently, surfaces of constant formula_8 are ellipsoids\n\nwhereas surfaces of constant formula_10 are hyperboloids of one sheet\n\nbecause the last term in the lhs is negative, and surfaces of constant formula_12 are hyperboloids of two sheets\n\nbecause the last two terms in the lhs are negative.\n\nThe orthogonal system of quadrics used for the ellipsoidal coordinates are confocal quadrics.\n\nFor brevity in the equations below, we introduce a function\n\nwhere formula_15 can represent any of the three variables formula_16. \nUsing this function, the scale factors can be written\n\nHence, the infinitesimal volume element equals\n\nand the Laplacian is defined by\n\nOther differential operators such as formula_23 \nand formula_24 can be expressed in the coordinates formula_1 by substituting \nthe scale factors into the general formulae \nfound in orthogonal coordinates.\n\n\n\n\n"}
{"id": "1860870", "url": "https://en.wikipedia.org/wiki?curid=1860870", "title": "Environmental justice", "text": "Environmental justice\n\nEnvironmental justice emerged as a concept in the United States in the early 1980s. The term has two distinct uses with the more common usage describing a social movement that focuses on the fair distribution of environmental benefits and burdens. The other use is an interdisciplinary body of social science literature that includes theories of the environment and justice, environmental laws and their implementations, environmental policy and planning and governance for development and sustainability, and political ecology.\n\nThe United States Environmental Protection Agency defines environmental justice as follows:\nEnvironmental justice is the fair treatment and meaningful involvement of all people regardless of race, color, national origin, or income with respect to the development, implementation, and enforcement of environmental laws, regulations, and policies. EPA has this goal for all communities and persons across this Nation. It will be achieved when everyone enjoys the same degree of protection from environmental and health hazards and equal access to the decision-making process to have a healthy environment in which to live, learn, and work.\nOther definitions include: equitable distribution of environmental risks and benefits; fair and meaningful participation in environmental decision-making; recognition of community ways of life, local knowledge, and cultural difference; and the capability of communities and individuals to function and flourish in society. An alternative meaning, used in social sciences, of the term \"justice\" is \"the distribution of social goods\".\n\nEnvironmental discrimination is one issue that environmental justice seeks to address. Racism and discrimination against minorities center on a socially-dominant group's belief in its superiority, often resulting in privilege for the dominant group and the mistreatment of non-dominant minorities. The combined impact of these privileges and prejudices are just one of the potential reasons that waste management and high pollution sites tend to be located in minority-dominated areas. A disproportionate quantity of minority communities (for example in Warren County, North Carolina) play host to landfills, incinerators, and other potentially toxic facilities. Environmental discrimination can also be the placement of a harmful factory in a place of minority. This can be seen as environmental discrimination because it is placing a harmful entity in a place where the people often don't have the means to fight back against big corporations.\n\nEnvironmental discrimination has historically been evident in the process of selecting and building environmentally hazardous sites, including waste disposal, manufacturing, and energy production facilities. The location of transportation infrastructures, including highways, ports, and airports, has also been viewed as a source of environmental injustice. Among the earliest documentation of environmental racism was a study of the distribution of toxic waste sites across the United States. Due to the results of that study, waste dumps and waste incinerators have been the target of environmental justice lawsuits and protests.\n\nSome environmental justice lawsuits are based on violations of civil rights laws.\n\nTitle VI of the Civil Rights Act of 1964 is often used in lawsuits that claim environmental inequality. Section 601 prohibits discrimination based on race, color, or national origin by any government agency receiving federal assistance. To win an environmental justice case that claims an agency violated this statute, the plaintiff must prove the agency intended to discriminate. Section 602 requires agencies to create rules and regulations that uphold section 601. This section is useful because the plaintiff must only prove that the rule or regulation in question had a discriminatory impact. There is no need to prove discriminatory intent. \"Seif v. Chester Residents Concerned for Quality Living\" set the precedent that citizens can sue under section 601. There has not yet been a case in which a citizen has sued under section 602, which calls into question whether this right of action exists.\n\nThe Equal Protection Clause of the Fourteenth Amendment, which was used many times to defend minority rights during the 1960s, has also been used in numerous environmental justice cases.\n\nWhen environmentalism first became popular during the early 20th century, the focus was wilderness protection and wildlife preservation. These goals reflected the interests of the movement's initial supporters. The actions of many mainstream environmental organizations still reflect these early principles.\n\nNumerous low-income minorities felt isolated or negatively impacted by the movement, exemplified by the Southwest Organizing Project's (SWOP) Letter to the Group of 10, a letter sent to major environmental organizations by several local environmental justice activists. The letter argued that the environmental movement was so concerned about cleaning up and preserving nature that it ignored the negative side-effects that doing so caused communities nearby, namely less job growth. In addition, the NIMBY movement has transferred locally unwanted land uses (LULUs) from middle-class neighborhoods to poor communities with large minority populations. Therefore, vulnerable communities with fewer political opportunities are more often exposed to hazardous waste and toxins. This has resulted in the PIBBY principle, or at least the PIMBY (Place-in-minorities'-backyard), as supported by the United Church of Christ's study in 1987.\nAs a result, some minorities have viewed the environmental movement as elitist. Environmental elitism manifested itself in three different forms:\n\n\nSupporters of economic growth have taken advantage of environmentalists' neglect of minorities. They have convinced minority leaders looking to improve their communities that the economic benefits of industrial facility and the increase in the number of jobs are worth the health risks. In fact, both politicians and businesses have even threatened imminent job loss if communities do not accept hazardous industries and facilities. Although in many cases local residents do not actually receive these benefits, the argument is used to decrease resistance in the communities as well as avoid expenditures used to clean up pollutants and create safer workplace environments.\n\nOne of the prominent barriers to minority participation in environmental justice is the initial costs of trying to change the system and prevent companies from dumping their toxic waste and other pollutants in areas with high numbers of minorities living in them. There are massive legal fees involved in fighting for environmental justice and trying to shed environmental racism. For example, in the United Kingdom, there is a rule that the claimant may have to cover the fees of their opponents, which further exacerbates any cost issues, especially with lower income minority groups; also, the only way for environmental justice groups to hold companies accountable for their pollution and breaking any licensing issues over waste disposal would be to sue the government for not enforcing rules. This would lead to the forbidding legal fees that most could not afford. This can be seen by the fact that out of 210 judicial review cases between 2005 and 2009, 56% did not proceed due to costs.\n\nDuring the Civil Rights Movement in the 1960s, activists participated in a social movement that created a unified atmosphere and advocated goals of social justice and equality. The community organization and the social values of the era have translated to the Environmental Justice movement.\n\nThe Environmental Justice movement and the Civil Rights Movement have many commonalities. At their core, the movements' goals are the same: \"social justice, equal protection, and an end to institutional discrimination.\" By stressing the similarities of the two movements, it emphasizes that environmental equity is a right for all citizens. Because the two movements have parallel goals, it is useful to employ similar tactics that often emerge on the grassroots level. Common confrontational strategies include protests, neighborhood demonstrations, picketing, political pressure, and demonstration.\n\nJust as the civil rights movement of the 1960s began in the South, the fight for environmental equity has been largely based in the South, where environmental discrimination is most prominent. In these southern communities, black churches and other voluntary associations are used to organize resistance efforts, including research and demonstrations, such as the protest in Warren County, North Carolina. As a result of the existing community structure, many church leaders and civil rights activists, such as Reverend Benjamin Chavis Muhammad, have spearheaded the Environmental Justice movement.\n\nThe Bronx, in New York City, has become a recent example of Environmental Justice succeeding. Majora Carter spearheaded the South Bronx Greenway Project, bringing local economic development, local urban heat island mitigation, positive social influences, access to public open space, and aesthetically stimulating environments. The New York City Department of Design and Construction has recently recognized the value of the South Bronx Greenway design, and consequently utilized it as a widely distributed smart growth template. This venture is the ideal shovel-ready project with over $50 million in funding.\n\nSeveral of the most successful Environmental Justice lawsuits are based on violations of civil rights laws. The first case to use civil rights as a means to legally challenge the siting of a waste facility was in 1979. With the legal representation of Linda McKeever Bullard, the wife of Robert D. Bullard, residents of Houston's Northwood Manor opposed the decision of the city and Browning Ferris Industries to construct a solid waste facility near their mostly African-American neighborhood.\n\nIn 1979, Northeast Community Action Group, or NECAG, was formed by African American homeowners in a suburban, middle income neighborhood in order to keep a landfill out of their home town. This group was the first organization that found the connection between race and pollution. The group, alongside their attorney Linda McKeever Bullard started the lawsuit Bean v. Southwestern Waste Management, Inc., which was the first of its kind to challenge the sitting of a waste facility under civil rights law.\nThe Equal Protection Clause of the Fourteenth Amendment, which was used many times to defend minority rights during the 1960s, has also been used in numerous Environmental Justice cases.\n\nTitle VI of the Civil Rights Act of 1964 is often used in lawsuits that claim environmental inequality. The two most paramount sections in these cases are sections 601 and 602. Section 601 prohibits discrimination based on race, color, or national origin by any government agency receiving federal assistance. To win an Environmental Justice case that claims an agency violated this statute, the plaintiff must prove the agency intended to discriminate. Section 602 requires agencies to create rules and regulations that uphold section 601; in \"Alexander v. Sandoval\", the Supreme Court held that plaintiffs must also show intent to discriminate to successfully challenge the government under 602.\n\nAmong the affected groups of Environmental Justice, those in high-poverty and racial minority groups have the most propensity to receive the harm of environmental injustice. Poor people account for more than 20% of the human health impacts from industrial toxic air releases, compared to 12.9% of the population nationwide. This does not account for the inequity found among individual minority groups. Some studies that test statistically for effects of race and ethnicity, while controlling for income and other factors, suggest racial gaps in exposure that persist across all bands of income.\n\nAfrican-Americans are affected by a variety of Environmental Justice issues. One notorious example is the \"Cancer Alley\" region of Louisiana. This 85-mile stretch of the Mississippi River between Baton Rouge and New Orleans is home to 125 companies that produce one quarter of the petrochemical products manufactured in the United States. The United States Commission on Civil Rights has concluded that the African-American community has been disproportionately affected by Cancer Alley as a result of Louisiana's current state and local permit system for hazardous facilities, as well as their low socio-economic status and limited political influence. Another incidence of long-term environmental injustice occurred in the \"West Grove\" community of Miami, Florida. From 1925 to 1970, the predominately poor, African American residents of the \"West Grove\" endured the negative effects of exposure to carcinogenic emissions and toxic waste discharge from a large trash incinerator called Old Smokey. Despite official acknowledgement as a public nuisance, the incinerator project was expanded in 1961. It was not until the surrounding, predominantly white neighborhoods began to experience the negative impacts from Old Smokey that the legal battle began to close the incinerator.\n\nIndigenous groups are often the victims of environmental injustices. Native Americans have suffered abuses related to uranium mining in the American West. Churchrock, New Mexico, in Navajo territory was home to the longest continuous uranium mining in any Navajo land. From 1954 until 1968, the tribe leased land to mining companies who did not obtain consent from Navajo families or report any consequences of their activities. Not only did the miners significantly deplete the limited water supply, but they also contaminated what was left of the Navajo water supply with uranium. Kerr-McGee and United Nuclear Corporation, the two largest mining companies, argued that the Federal Water Pollution Control Act did not apply to them, and maintained that Native American land is not subject to environmental protections. The courts did not force them to comply with US clean water regulations until 1980.\n\nThe most common example of environmental injustice among Latinos is the exposure to pesticides faced by farmworkers. After DDT and other chlorinated hydrocarbon pesticides were banned in the United States in 1972, farmers began using more acutely toxic organophosphate pesticides such as parathion. A large portion of farmworkers in the US are working as undocumented immigrants, and as a result of their political disadvantage, are not able to protest against regular exposure to pesticides or benefit from the protections of Federal laws. Exposure to chemical pesticides in the cotton industry also affects farmers in India and Uzbekistan. Banned throughout much of the rest of the world because of the potential threat to human health and the natural environment, Endosulfan is a highly toxic chemical, the safe use of which cannot be guaranteed in the many developing countries it is used in. Endosulfan, like DDT, is an organochlorine and persists in the environment long after it has killed the target pests, leaving a deadly legacy for people and wildlife.\n\nResidents of cities along the US-Mexico border are also affected. Maquiladoras are assembly plants operated by American, Japanese, and other foreign countries, located along the US-Mexico border. The maquiladoras use cheap Mexican labor to assemble imported components and raw material, and then transport finished products back to the United States. Much of the waste ends up being illegally dumped in sewers, ditches, or in the desert. Along the Lower Rio Grande Valley, maquiladoras dump their toxic wastes into the river from which 95 percent of residents obtain their drinking water. In the border cities of Brownsville, Texas and Matamoros, Mexico, the rate of anencephaly (babies born without brains) is four times the national average.\n\nStates may also see placing toxic facilities near poor neighborhoods as preferential from a Cost Benefit Analysis (CBA) perspective. A CBA may favor placing a toxic facility near a city of 20,000 poor people than near a city of 5,000 wealthy people. Terry Bossert of Range Resources reportedly has said that it deliberately locates its operations in poor neighbourhoods instead of wealthy areas where residents have more money to challenge its practices. Northern California's East Bay Refinery Corridor is an example of the disparities associated with race and income and proximity to toxic facilities.\n\nIt has been argued that environmental justice issues generally tend to affect women in communities more so than they affect men. This is due to the way that women typically interact more closely with their environments at home, such as through handling food preparation and childcare. Women also tend to be the leaders in environmental justice activist movements. Despite this, it tends not to be considered a mainstream feminist issue.\n\nIn its 2012 environmental justice strategy documents, the U.S. Department of Agriculture (USDA) stated an ongoing desire to integrate environmental justice into its core mission, internal operations and programming. It identified ambitious timeframes for action and promised improved efforts to highlight, track and coordinate EJ activities among its many sub-agencies. Agency-wide the USDA expanded its perspective on EJ, so that in addition to preventing disproportionate environmental impacts on EJ communities, USDA voiced a commitment to improve public participation processes and use its technical and financial assistance programs to improve the quality of life in all communities. In 2011, Secretary of Agriculture Tom Vilsack emphasized the USDA's focus on EJ in rural communities around the United States. USDA funds or implements many creative programs with social and environmental equity goals, however it has no staff dedicated solely to EJ, and faces the challenges of limited budgets and coordinating the efforts of a highly diverse agency.\n\nThe USDA is the executive agency responsible for federal policy on food, agriculture, natural resources, and quality of life in rural America. The USDA has more than 100,000 employees and delivers over $96.5 billion in public services to programs worldwide. To fulfill its general mandate, USDA's departments are organized into seven mission areas:1) Farm and Foreign Agricultural Services; 2) Food, Nutrition and Consumer Services; 3) Food Safety; 4) Marketing and Regulatory Programs; 5) Natural Resources and Environment; 6) Research, Education and Economics and; 7) Rural Development.\n\nIn 1994, President Clinton issued Executive Order 12898, \"Federal Actions to Address Environmental Justice in Minority Populations and Low-Income Populations.\" Executive Order 12898 requires that achieving EJ must be part of each federal agency's mission. Agency programs, policies and activities can lead to health and environmental effects that disproportionately impact minority and low-income populations. Under Executive Order 12898 agencies must develop strategies that identify and address these effects by:\n\n\nTitle VI of the Civil Rights Act of 1964 requires that federal funds be used in a fair and equitable manner. Under Title VI any federal agency that receives federal funding cannot discriminate. Title VI also forbids federal agencies from providing grants or funding opportunities to programs that discriminate. An agency that violates Title VI can lose its federal funding.\n\nFollowing E.O. 12898 and USDA's initial EJ strategic plan, USDA issued its internal Environmental Justice Department Regulation (DR 5600-002) in 1997. Although the definition of EJ was undergoing updates in 2012, DR 5600-002 defines environmental justice as \"to the greatest extent practicable and permitted by law, all populations are provided the opportunity to comment before decisions are rendered on, are allowed to share in the benefits of, are not excluded from, and are not affected in a disproportionately high and adverse manner by, government programs and activities affecting human health or the environment.\" Patrick Holmes, Special Assistant to the Under Secretary for Natural Resources and Environment at USDA, notes that this definition will be broadened in 2012 so that EJ also includes efforts to improve quality of life in all communities. In other words, USDA will consider EJ to include avoiding adverse impacts \"and\" ensuring access to environmental benefits. Further, DR 5600-002 identified USDA's goals in implementing Executive Order 12898 as:\n\n\nDR 5600-002 is \"intended only to improve the internal management of USDA,\" and although it described concrete, mandatory actions by the agency, it did not establish new rights or benefits enforceable in court. In April 2011, USDA Secretary Tom Vilsack has stated a more concrete priority to fulfill its mission of environmental justice in rural areas.\n\nIn compliance with the August 2011 Memorandum of Understanding on Environmental Justice and Executive Order 12898 (MOU), USDA released a final Environmental Justice Strategic Plan: 2012 to 2014 on February 7, 2012 (Strategic Plan), which identifies new and updated goals and performance measures beyond what USDA identified in a 1995 EJ strategy it adopted in response to E.O. 12898. In the same week, it also released its first annual implementation progress report (Progress Report), as the MOU also required. The Secretary's message accompanying the Strategic Plan described two immediate tasks: 1) each agency within USDA is required to identify a point of contact for EJ issues, at the Senior Executive Service (SES) level; and 2) each agency must develop its own EJ strategy prior to April 15, 2012, and begin implementing it as soon as possible. As of May 2012, it did not appear that such strategies had been made public, although sub-agencies provided internal reports to the USDA's EJ steering committee on April 9, 2012, according to Holmes.<nowiki> The Secretary's message contained strong language that, \"Given that USDA programs touch almost every American every day, the Department is well positioned to help in [the environmental justice] effort.\"</nowiki> USDA has determined that it can achieve the requirements of the Executive Order by integrating EJ into its programs, rather than implementing new and costly programs. The agency took this same approach in an EJ strategy it adopted in 1995. In some areas, such as agricultural chemicals and effects to migrant workers, USDA reviews its practices to identify potential disproportionate, adverse impacts on EJ communities, according to Blake Velde, Senior Environmental Scientist with the USDA Hazardous Materials Management Division. Generally, however, USDA believes its existing technical and financial assistance programs provide solutions to environmental inequity, such as its initiatives on education, food deserts, and economic development in impacted communities, and ensuring access to environmental benefits is the focus of USDA's EJ efforts.\n\nNatural Resources and Environment (NRE) Under Secretary Harris Sherman is the political appointee generally responsible for USDA's EJ strategy, with Patrick Holmes, a senior staffer to the Under Secretary, playing a coordinating role. Although USDA has no staff dedicated solely to EJ, its sub-agencies have many offices dedicated to civil rights compliance, outreach and communication and environmental review whose responsibilities incorporate EJ issues. The Strategic Plan was developed with the input of an Environmental Justice Working Group, made up of staff and leadership representing the USDA's seven mission areas and the SES-level contacts, which were appointed in early 2012, serve as a steering committee for the agency's efforts. The Strategic Plan is organized according to six goals, which were purposefully left broad, and lists specific objectives and agency performance measures under each goal. The details and specific implementation of many of these programs and the performance measures are left to the departments and sub-agencies to develop. The six goals are to:\n\n\nThe Strategic Plan also lists existing programs that either currently support the goal, or are expected to in the future. According to Holmes, some of the challenges of the Strategic Plan process have stemmed from the diverse programs and missions that the agency serves, limitations on staff time, and budgets.\n\nThe Strategic Plan requires that EJ must be integrated into the strategies and evaluations for sub-agencies' technical and financial assistance programs. It also emphasizes public participation, community capacity-building, EJ awareness and training within the USDA.\n\nA stated goal of USDA's Strategic Plan is to expand public participation in agency activities, to enhance the \"credibility and public trust\" of the USDA. Specifically, the agency will update its public participation guidelines to include EJ, beginning this process by April 15, 2012. The Strategic Plan emphasizes capacity-building in EJ communities, and includes objectives that emphasize communication between USDA and environmental justice communities, including Tribal consultation. Sub-agencies must announce schedules for training programs in EJ communities and to develop new, preliminary outreach materials on USDA programs by April 15, 2012. An additional performance standard is to encourage EJ communities to participate in the NEPA process, an effort the Strategic Plan requires on or before February 29, 2012,<nowiki> although the Strategic Plan does not articulate a standard by which this could be measured. The Strategic Plan also reiterates compliance with the Executive Orders on Tribal consultation and outreach to non-proficient English speakers, and seeks more diverse representation on regional forest advisory committees. [community participation, outreach]</nowiki>\n\nGenerally, the USDA's process for developing the Strategic Plan demonstrates a commitment to public involvement. The USDA EJ documents are currently housed obscurely within the Departmental Management section of the USDA website, under the Hazardous Materials Management Division, although the agency plans to update its entire site in 2012 and create a more robust EJ page. The Strategic Plan was released in draft form in December 2011 for a 30-day public comment period, and responses to general types of comments received are in the Progress Report, although the comments themselves are not online. The Secretary's message accompanying the Strategic Plan requests that organizations and individuals to continue to contact USDA with comments on the Strategic Plan and to identify USDA programs that have been the most beneficial to their communities. The agency has a dedicated email address for this purpose. Agency leadership has asked its sub-agencies to prepare responses to additional comments that have been received, and the agency will release an interim progress report, prior to winter 2013. <nowiki>[community participation, outreach, education]</nowiki>\n\nThe Strategic Plan also seeks to increase the awareness of environmental justice issues among USDA employees. The Strategic Plan does not list any existing programs in this area, but does list a series of performance measures going forward, most of which must be met by April 15, 2012. The measures include environmental justice trainings, new web pages, and potential revisions to staff manuals and handbooks. Sub-agencies began reviewing their existing training in 2012 and in their April 9, 2012 reports to the USDA EJ steering committee, sub-agencies were asked to describe their goals for enhanced EJ training.<nowiki> This internal, educational undertaking appears to be new in the 2012 Strategic Plan. The Strategic Plan targets Responsible Officials, meaning office and program managers, for the trainings, as well as the SES-level points of contact required by the Secretary's message. [education, study, compliance and enforcement]</nowiki>\n\nThe EJ Strategy tasked each sub-agency with developing its own EJ strategy document by spring 2012, although as of May 2012 the sub-agencies were still in an evaluation stage and had not issued final documents. For many sub-agencies, the 2012 process has been their first focused assessment of their EJ impact and opportunities. Going forward, sub-agencies will submit twice-yearly reports to NRE about their implementation of the Strategic Plan's goals; the first of these was due April 9, 2012, and as of May 2012, the USDA's EJ steering committee was evaluating the first reports.\n\nAs part of its effort to ensure that EJ communities have the opportunity to participate in USDA programs, the Strategic Plan requires each sub-agency to set measurements through which it can track increased EJ community participation in USDA technical and financial assistance programs. This must be done by April 15, 2012. As of late April 2012, the sub-agencies were still in the process of describing a baseline of current activities and determining the metrics to evaluate improvement, such as staff time, grant funding or increased programming. The ultimate metrics are likely to be somewhat subjective, and must be flexible given the broad range of undertakings by the sub-agencies. Also related to evaluation, the Strategic Plan requires the sub-agencies to determine an effective methodology with which they can evaluate whether USDA programs have disproportionate impacts.<nowiki> [study, redressing environmental racism, compliance and enforcement]</nowiki>\n\nUSDA has had a role in implementing Michelle Obama's \"Let's Move\" campaign in Tribal Areas, by increasing participation by Bureau of Indian Education schools in Federal nutrition programs, in the development of community gardens on Tribal lands, and in the development of Tribal food policy councils. This is combined with measures to provide Rural Development funding for community infrastructure in Indian Country.<nowiki> [children's issues, education, diet, grants, Native Americans, public health].</nowiki>\n\nThe U.S. Forest Service (USFS) is working to update its policy on protection and management of Native American Sacred Sites, an effort that has included listening sessions and government-to-government consultation. The Animal and Plant Health Inspection Service (APHIS) has also consulted with Tribes regarding management of reintroduced of species, where Tribes may have a history of subsistence-level hunting of those species. Meanwhile, the Agricultural Marketing Service (AMS) is exploring a program to use meat from bisons raised on Tribal land to supply AMS food distribution programs to Tribes.<nowiki> [Native Americans, diet, subsistence, community participation]</nowiki>\n\nThe Intertribal Technical Assistance Network works to improve access of Tribal governments, communities and individuals to USDA technical assistance programs.\n\nThe Progress Report highlights the NRCS Strike Force Initiative, which has identified impoverished counties in Mississippi, Georgia and Arkansas to receive increased outreach and training regarding USDA assistance programs. USDA credits this increased outreach with generating a 196 percent increase in contracts, representing more than 250,000 acres of farmland, in its Environmental Quality Incentives Program.<nowiki> [economic benefit, equitable development, grants, outreach, ej as evaluation criteria] NRCS works with </nowiki>\"private landowners protect their natural resources\" through conservation planning and assistance with the goal of maintaining \"productive lands and healthy ecosystems.\" NRCS has its own civil rights compliance guidance document, and in 2001 NRCS funded and published a study, \"Environmental Justice: Perceptions of Issues, Awareness and Assistance,\" focused on rural, Southern \"Black Belt\" counties and analyzing how the NRCS workforce could more effectively integrate environmental justice into impacted communities.<nowiki> [compliance and enforcement, redressing environmental racism, grants, study, ej as evaluation criteria]</nowiki>\n\nThe Farm Services Agency in 2011 devoted $100,000 of its Socially Disadvantaged Farmers and Ranchers program budget to improving its outreach to counties with persistent poverty, including improving its materials and building relationships with local universities and community groups.<nowiki> [economic benefit, equitable development, grants, outreach, ej as evaluation criteria]</nowiki>\n\nIn addition, USDA's Risk Management Agency has initiated education and outreach to low-income farmers regarding use of biological controls, rather than pesticides, for pest control, efforts that the agency believes are valuable in the face of climate change.<nowiki> [climate change, agricultural chemicals, education]</nowiki>\n\nA 2011 MOU between a USDA sub-agency, the Food Safety Inspection Service (FSIS) and the American Indian Science and Engineering Society that aims to increase the number of Native Americans entering the FSIS career path;<nowiki> [education, community participation, economic benefit, green jobs, Native Americans, diet, interagency collaboration]</nowiki>\n\nA partnership between APHIS and the Rural Coalition (Coalicion)--an alliance of regionally and culturally diverse organizations working to build a more just and sustainable food system. The partnership focuses on outreach, fair returns to minority and other small farmers and rural communities, farmworker working conditions, environmental protection and food safety.<nowiki> [agricultural chemicals, community participation, diet, economic benefit, outreach, improving health and safety, ej as evaluation criteria]</nowiki>\n\nUSFS is also funding pilot initiatives, such as its Urban Water Ambassadors, summer internship positions for youth who coordinate and implement urban tree planting projects. In 2011, USFS provided a grant to the Maryland Department of Natural Resources that funded 14 summer jobs for youth in Baltimore to work on urban watershed restoration programs.<nowiki> [community participation, green jobs, mapping, water]</nowiki>\n\nUSFS has established several Urban Field Stations, to research urban natural resources' structure, function, stewardship, and benefits. By mapping urban tree coverage, the agency hopes to identify and prioritize EJ communities for urban forest projects.<nowiki> [community education, mapping, diet, improving health and safety, ej as evaluation criteria]</nowiki>\n\nAnother initiative highlighted by the agency is the Food and Nutrition Service and Economic Research Service's Food Desert Locator. The Locator provides a spatial view of food deserts, defined as a low-income census tract where a substantial number or share of residents has low access to a supermarket or large grocery store. It also shows, by census tract, the number and percentage of certain populations, such as children, seniors, or households without a vehicle, with low access to grocery stores. The mapped deserts can be used to direct agency resources to increase access to fresh fruits and vegetables and other food assistance programs, according to Blake Velde, an agency scientist and spokesperson on EJ issues.<nowiki> [diet, mapping, improving health and safety, study, ej as evaluation criteria, services and data available to others]</nowiki>\n\nUSDA Secretary Tom Vilsack has placed a clear emphasis on supporting EJ in rural areas. Although<nowiki> \"often the highest profile battles on [environmental justice] issue[s] are waged in at-risk neighborhoods in major cities or at Superfund sites located near populated urban and </nowiki>suburban areas\" Vilsack highlighted the often overlooked rural areas where environmental justice is largely ignored.\n\nThrough its Rural Utilities Service, the USDA supports a number of Water and Environmental Programs. These programs work to administer water and wastewater loans or grants to rural areas and cities to support water and wastewater, stormwater and solid waste disposal systems, including SEARCH grants that are targeted to financially distressed, small rural communities and other opportunities specifically for Alaskan Native villages and designated Colonias.; In his speech, Secretary Vilsack said that the USDA funded 2,575 clean water projects in rural areas during a two-year period to address problems ranging from wastewater treatment to sewage treatment.<nowiki> [water, land use, compliance and enforcement, improving health and safety, pollution cleanup, ej as evaluation criteria]</nowiki>\n\nThe USDA also supports the Rural Energy for America Grant Program. This program provides grants and loans to farmers, ranchers and rural small businesses to finance renewable energy systems and energy efficiency improvements.<nowiki>[grants, economic benefit, ej as evaluation criteria]</nowiki>\n\nIn 1997 the USDA promulgated a departmental regulation providing \"direction to<nowiki> [</nowiki>sub-]agencies for integrating environmental justice considerations into USDA programs and activities\" (DR 5600-002). Issuance of this regulation was a primary goal of USDA's 1995 EJ strategy document. DR 5600-002 includes guidelines for consideration of EJ in the NEPA process, but also stated that \"efforts to address environmental justice are not limited to NEPA compliance.\" It requires evaluation of activities for potential disproportionate EJ impacts, outreach, and performance-metric based evaluation and reporting on sub-agencies' implementation of EJ goals. DR 5600-002 is a forward-looking, permanent directive that applies to all USDA programs and activities. However, it was not published in the Federal Register as a formal rulemaking and does not create a private right of action or enforcement tool. A Strategic Plan goal is to update this regulation, as well as other departmental regulations and policies on EJ. According to USDA, the EJ definition in DR 5600-002 will be modified in 2012—EJ to include measures to avoid disproportionate negative impacts as well as quality-of-life improvements that the agency believes can benefit impacted communities.\n\nThe Strategic Plan also has established a performance standard requiring that existing and new USDA regulations are evaluated for EJ impacts or benefits. Sub-agencies are required to develop a process for this evaluation by April 15, 2012. This performance standard reflects a requirement in DR 5600-002 that required the USDA departmental regulation on rulemaking, DR 1521-1, to be revised to require an EJ evaluation in the rulemaking process. As of 2012, DR 1521-1 requires that a cost-benefit analysis of major human health, safety and environmental regulations include analysis of risks to \"persons who are disproportionately exposed or particularly sensitive,\" although DR 1521-1 does not mention EJ or impacts to minority or low-income communities explicitly. <nowiki>[Land Use - permitting, community participation, compliance and enforcement, study]</nowiki>\n\nThe Strategic Plan sets an enforcement-specific goal, which includes objectives to \"effectively resolve or adjudicate all environmental justice-related Title VI complaints\" and to include environmental justice as a key component of civil rights compliance reviews. Agencies are also required to identify an assessment methodology by April 15, 2012, which can be used to determine whether programs have disproportionately high and adverse environmental and human health impacts. The NRCS has published and updated a Civil Rights Compliance Review Guide, which guides the NRCS Civil Rights Division's review of the compliance with Title VI and 12898 in the agency's state offices, field offices and other facilities. The guide was updated in November 2011 and it does not mention EJ explicitly. However, the Strategic Plan identifies the NRCS compliance review and other outreach and research programs as supporting its EJ enforcement goals.<nowiki> [compliance and enforcement]</nowiki>\n\nThe 1997 Regulation, DR 5600-2 required USDA sub-agencies to develop their own NEPA environmental justice guidance documents. The sub-agencies have done so, with some additional details, such as a reminder that the EJ community should be involved in identifying the alternatives, suggested stakeholders and resources, and guidance to hold meetings at times when working people can get to them, and to translate notices. However, when DR 5600-02 is updated as required by the Strategic Plan, changes could be made to the NEPA section of the Regulation. The Strategic Plan sets a performance standard to encourage interested environmental justice communities to be involved in the public participation process for NEPA documents, although the Strategic Plan does not require updates to the NEPA portions of DR 5600-02.\n\nAlthough the USDA has integrated EJ into each step of the NEPA process as required by Executive Order 12898, many of the NEPA documents completed by the USDA include only cursory analysis of environmental justice effects. This analysis most often includes a rote paragraph as to what Executive Order 12898 requires and a quick conclusion that the agency action does not affect minority and low-income populations. Some examples where the USDA included more in-depth analysis are:\n\n\nThe USDA does not have any permitting initiatives specific to EJ.\n\nThe USDA has an Office of the Assistant Secretary for Civil Rights whose mission it is to provide leadership and direction \"for the fair and equitable treatment of all USDA customers.\"\n\nIn 2003 the USDA revised DR 4300-4, internal regulations requiring a Civil Rights Impact Analysis of all \"policies, actions or decisions\" affecting the USDA's federally conducted and federally assisted programs or activities. The analysis is used to determine the \"scope, intensity, direction, duration, and significance of the effects of an agency's proposed ... policies, actions or decisions.\" USDA's departmental regulation on EJ, DR 5600-002, required DR 4300-4 to be revised to \"require that Civil Rights Impact Analyses include a finding as to whether proposed or new actions have or do not have a disproportionately high and adverse effect on the human health or the environment of minority populations, and whether such effects can be prevented or mitigated\". Although DR 4300-4 was revised in 2003, the revised regulation does not explicitly require a finding on adverse environmental or health impacts. <nowiki>[</nowiki>study, compliance and enforcement]\n\nA new movement, bent on educating the people, was born after the Bhopal disaster, called the \"right-to-know\" movement. A series of laws and reports was created, all built to inform the people of the pollutants being dumped into our neighborhoods and atmosphere, and exactly how much of each chemical is being exposed and dumped. The theory behind \"right-to-know\" is that once people are informed on what is polluting their neighborhood, then they will begin to take action in both bringing down their own emissions, as well as begin to make the companies causing the most pollution, through means such as protests, to take into account their actions.\n\nAfter the Bhopal disaster, where a Union Carbide plant released forty tons of methyl isocyanate into the atmosphere in a village just south of Bhopal, India, the U.S. government passed the Emergency Planning and Right to Know Act of 1986. Introduced by Henry Waxman, the act required all corporations to report their toxic chemical pollution annually, which was then gathered into a report known as the Toxics Release Inventory (TRI). By collecting this data, the government was able to make sure that companies were no longer releasing excessive amounts of deadly toxins into populated areas, so to prevent another incident like that of the thousands of people killed and the tens of thousands of people injured in the Bhopal disaster.\n\nThe Corporate Toxics Information Project (CTIP) was founded on the guidelines that they will \"[develop] and [disseminate] information and analysis on corporate releases of pollutants and the consequences for communities\". The overarching goal was to help take corporations into account for their pollution habits, by collecting information and putting it in databases so to make it available to the general public. The four goals of the project were to develop 1) corporate rankings, 2) regional reports, based on state, region, and metropolitan areas, 3) industry reports, based on industrial sectors, and 4) to create a web-based resource open to the entire population, that can depict all the collected data. The data collection would be done by the Environmental Protection Agency (EPA) and then analyzed and disseminated by the PERI institute.\n\nOne of the biggest projects of CTIP was the Toxic 100. The Toxic 100 is an index of the top 100 air polluters around the United States in terms of the country's largest corporations. The list is based on the EPA's Risk Screening Environmental Indicators (RSEI), which \"assesses the chronic human health risk from industrial toxic releases\", as well as the Toxics Release Inventory (TRI), which is where the corporations must report their chemical releases to the US government. Since its original publishing date in 2004, the Toxic 100 has been updated four more times, with the latest publishing date being August 2013.\n\nIn recent years environmental justice campaigns have also emerged in other parts of the world, such as India, South Africa, Israel, Nigeria, Mexico, Hungary, Uganda, and the United Kingdom. In Europe for example, there is evidence to suggest that the Romani people and other minority groups of non-European descent are suffering from environmental inequality and discrimination.\n\n\"For further information, see Environmental racism in Europe\"\n\nIn Europe, the Romani peoples are ethnic minorities and differ from the rest of the European people by their culture, language, and history. The environmental discrimination that they experience ranges from the unequal distribution of environmental harms as well as the unequal distribution of education, health services and employment. In many countries Romani peoples are forced to live in the slums because many of the laws to get residence permits are discriminatory against them. This forces Romani people to live in urban \"ghetto\" type housing or in shantytowns. In the Czech Republic and Romania, the Romani peoples are forced to live in places that have less access to running water and sewage, and in Ostrava, Czech Republic, the Romani people live in apartments located above an abandoned mine, which emits methane. Also in Bulgaria, the public infrastructure extends throughout the town of Sofia until it reaches the Romani village where there is very little water access or sewage capacity.\n\nThe European Union is trying to strive towards environmental justice by putting into effect declarations that state that all people have a right to a healthy environment. The Stockholm Declaration, the 1987 Brundtland Commission's Report – \"Our Common Future\", the Rio Declaration, and Article 37 of the Charter of Fundamental Rights of the European Union, all are ways that the Europeans have put acts in place to work toward environmental justice. Europe also funds action-oriented projects that work on furthering Environmental Justice throughout the world. For example, EJOLT (Environmental Justice Organisations, Liabilities and Trade) is a large multinational project supported through the FP7 Science in Society budget line from the European Commission. From March 2011 to March 2015, 23 civil society organizations and universities from 20 countries in Europe, Africa, Latin-America, and Asia are, and have promised to work together on advancing the cause of Environmental Justice. EJOLT is building up case studies, linking organisations worldwide, and making an interactive global map of Environmental Justice.\n\nSweden became the first country to ban DDT in 1969 due to the efforts of women protesting its usage in forests. In the 1980s, women activists organized around preparing jam made from pesticide-tainted berries, which they offered to the members of parliament. Parliament members refused, and this has often been cited as an example of direct action within ecofeminism.\n\nWhilst the predominant agenda of the Environmental Justice movement in the United States has been tackling issues of race, inequality, and the environment, environmental justice campaigns around the world have developed and shifted in focus. For example, the EJ movement in the United Kingdom is quite different. It focuses on issues of poverty and the environment, but also tackles issues of health inequalities and social exclusion. A UK-based NGO, named the Environmental Justice Foundation, has sought to make a direct link between the need for environmental security and the defense of basic human rights. They have launched several high profile campaigns that link environmental problems and social injustices. A campaign against illegal, unreported and unregulated (IUU) fishing highlighted how 'pirate' fisherman are stealing food from local, artisanal fishing communities. They have also launched a campaign exposing the environmental and human rights abuses involved in cotton production in Uzbekistan. Cotton produced in Uzbekistan is often harvested by children for little or no pay. In addition, the mismanagement of water resources for crop irrigation has led to the near eradication of the Aral Sea. The Environmental Justice Foundation has successfully petitioned large retailers such as Wal-mart and Tesco to stop selling Uzbek cotton.\n\nIn France, numerous Alternatiba events, or villages of alternatives, are providing hundreds of alternatives to climate change and lack of environmental justice, both in order to raise people's awareness and to stimulate behaviour change. They have been or will be organized in over sixty different French and European cities, such as Bilbao, Brussels, Geneva, Lyon or Paris.\n\nUnder colonial and apartheid governments in South Africa, thousands of black South Africans were removed from their ancestral lands to make way for game parks. Earthlife Africa was formed in 1988 (www.earthlife.org.za), making it Africa's first environmental justice organisation. In 1992, the Environmental Justice Networking Forum (EJNF), a nationwide umbrella organization designed to coordinate the activities of environmental activists and organizations interested in social and environmental justice, was created. By 1995, the network expanded to include 150 member organizations and by 2000, it included over 600 member organizations.\n\nWith the election of the African National Congress (ANC) in 1994, the environmental justice movement gained an ally in government. The ANC noted \"poverty and environmental degradation have been closely linked\" in South Africa. The ANC made it clear that environmental inequalities and injustices would be addressed as part of the party's post-apartheid reconstruction and development mandate. The new South African Constitution, finalized in 1996, includes a Bill of Rights that grants South Africans the right to an \"environment that is not harmful to their health or well-being\" and \"to have the environment protected, for the benefit of present and future generations through reasonable legislative and other measures that\n\nSouth Africa's mining industry is the largest single producer of solid waste, accounting for about two-thirds of the total waste stream. Tens of thousands of deaths have occurred among mine workers as a result of accidents over the last century. There have been several deaths and debilitating diseases from work-related illnesses like asbestosis. For those who live next to a mine, the quality of air and water is poor. Noise, dust, and dangerous equipment and vehicles can be threats to the safety of those who live next to a mine as well. These communities are often poor and black and have little choice over the placement of a mine near their homes. The National Party introduced a new Minerals Act that began to address environmental considerations by recognizing the health and safety concerns of workers and the need for land rehabilitation during and after mining operations. In 1993, the Act was amended to require each new mine to have an Environmental Management Program Report (EMPR) prepared before breaking ground. These EMPRs were intended to force mining companies to outline all the possible environmental impacts of the particular mining operation and to make provision for environmental management.\n\nIn October 1998, the Department of Minerals and Energy released a White Paper entitled \"A Minerals and Mining Policy for South Africa\", which included a section on Environmental Management. The White Paper states \"Government, in recognition of the responsibility of the State as custodian of the nation's natural resources, will ensure that the essential development of the country's mineral resources will take place within a framework of sustainable development and in accordance with national environmental policy, norms, and standards\". It adds that any environmental policy \"must ensure a cost-effective and competitive mining industry.\"\n\nIn Australia, the \"Environmental Justice Movement\" is not defined as it is in the United States. Australia does have some discrimination mainly in the siting of hazardous waste facilities in areas where the people are not given proper information about the company. The injustice that takes place in Australia is defined as environmental politics on who get the unwanted waste site or who has control over where factory opens up. The movement towards equal environmental politics focuses more on who can fight for companies to build, and takes place in the parliament; whereas, in the United States Environmental Justice is trying to make nature safer for all people.\n\nAn example of the environmental injustices that indigenous groups face can be seen in the Chevron-Texaco incident in the Amazon rainforest. Texaco, which is now Chevron, found oil in Ecuador in 1964 and built sub-standard oil wells to cut costs. The deliberately used inferior technology to make their operations cheaper, even if detrimental to the local people and environment. After the company left in 1992, they left approximately one thousand toxic waste pits open and dumped billions of gallons of toxic water into the rivers.\n\nSouth Korea has a relatively short history of environmental justice compared to other countries in the west. As a result of rapid industrialization, people started to have awareness on pollution, and from the environmental discourses the idea of environmental justice appeared. The concept of environmental justice appeared in South Korea in late 1980s.\n\nSouth Korea experienced rapid economic growth (which is commonly referred to as the 'Miracle on the Han River') in the 20th century as a result of industrialization policies adapted by Park Chung-hee after 1970s. The policies and social environment had no room for environmental discussions, which aggravated the pollution in the country.\n\nEnvironmental movements in South Korea started from air pollution campaigns. As the notion of environment pollution spread, the focus on environmental activism shifted from existing pollution to preventing future pollution, and the organizations eventually started to criticize the government policies that are neglecting the environmental issues.\nThe concept of environmental justice was introduced in South Korea among the discussions of environment after 1990s. While the environmental organizations analyzed the condition of pollution in South Korea, they noticed that the environmental problems were inequitably focused especially on regions where people with low social and economic status were concentrated.\n\nThe problems of environmental injustice have arisen by environment related organizations, but approaches to solve the problems were greatly supported by the government, which developed various policies and launched institution. These actions helped raise awareness of environmental justice in South Korea. Existing environment policies were modified to cover environmental justice issues.\n\nEnvironmental justice began to be widely recognized in the 1990s through policy making and researches of related institutions. For example, the Ministry of Environment, which was founded in 1992, launched Citizen's Movement for Environmental Justice (CMEJ) to raise awareness of the problem and figure out appropriate plans. As a part of its activities, Citizen's Movement for Environmental Justice (CMEJ) held Environmental Justice forum in 1999, to gather and analyze the existing studies on the issue which were done sporadically by various organizations. Citizen's Movement for Environmental Justice (CMEJ) started as a small organization, but it is keep growing and expanding. In 2002, CMEJ had more than 5 times the numbers of members and 3 times the budget it had in the beginning year.\n\nEnvironmental injustice is still an ongoing problem. One example is the construction of Saemangeum Seawall. The construction of Saemangeum Seawall, which is the world's longest dyke (33 kilometers) runs between Yellow Sea and Saemangeum estuary, was part of a government project initiated in 1991. The project raised concerns on the destruction of ecosystem and taking away the local residential regions. It caught the attention of environmental justice activists because the main victims were low-income fishing population and their future generations. This is considered as an example of environmental injustice which was caused by the execution of exclusive development-centered policy.\n\nThe construction of Seoul-Incheon canal also raised environmental justice controversies. The construction took away the residential regions and farming areas of the local residents. Also, the environment worsened in the area because of the appearance of wet fogs which was caused by water deprivation and local climate changes caused by the construction of canal. The local residents, mostly people with weak economic basis, were severely affected by the construction and became the main victims of such environmental damages. While the socially and economically weak citizens suffered from the environmental changes, most of the benefits went to the industries and conglomerates with political power.\n\nConstruction of industrial complex was also criticized in the context of environmental justice. The conflict in Wicheon region is one example. The region became the center of controversy when the government decided to build industrial complex of dye houses, which were formerly located in Daegu metropolitan region. As a result of the construction, Nakdong River, which is one of the main rivers in South Korea, was contaminated and local residents suffered from environmental changes caused by the construction.\n\nEnvironmental justice is a growing issue in South Korea. Although the issue is not yet widely recognized compared to other countries, many organizations beginning to recognize the issue.\n\nEnvironmental discrimination in a global perspective is also an important factor when examining the Environmental Justice movement. Even though the Environmental Justice movement began in the United States, the United States also contributes to expanding the amount of environmental injustice that takes place in less-developed countries. Some companies in the United States and in other developed nations around the world contribute to the injustice by shipping the toxic waste and byproducts of factories to less-developed countries for disposal. This act increases the amount of waste in the third world countries, most of which do not have proper sanitation for their own waste much less the waste of another country. Often, the people of the less-developed countries are exposed to toxins from this waste and do not even realize what kind of waste they are encountering or the health problems that could come with it.\n\nOne prominent example of northern countries shipping their waste to southern countries took place in Haiti. Philadelphia, Pennsylvania had ash from the incineration of toxic waste that they did not have room to dump. Philadelphia decided to put the ash into the hands of a private company, which shipped the ash and dumped it in various other parts of the world, outside of the United States. \"The Khian Sea\", the ship the ash was put on, sailed around the world and many countries would not accept the waste because it was hazardous for the environment and the people. The ship owners finally dumped the waste, labeled Fertilizer, in Haiti, on the beach, and sailed away in the night. The government of Haiti was infuriated and called for the waste to be removed, but the company would not come to take the ash away. The fighting over who was responsible for the waste and who would remove the waste went on for many years. After debating for over ten years, the waste was removed and taken back to a site just outside Philadelphia to be disposed of permanently.\n\nThe reason that this transporting of waste from Northern countries to the Southern countries takes place is because it is cheaper to transport waste to another country and dump it there, than to pay to dump the waste in the producing country because the third world countries do not have the same strict industry regulations as the more developed countries. The countries that the waste is taken to are usually impoverished and the governments have little or no control over the happenings in the country or do not care about the people.\n\nMany of the Environmental Justice Networks that began in the United States expanded their horizons to include many other countries and became Transnational Networks for Environmental Justice. These networks work to bring Environmental Justice to all parts of the world and protect all citizens of the world to reduce the environmental injustice happening all over the world. Listed below are some of the major Transnational Social Movement Organizations.\n\n\n\n\n"}
{"id": "268020", "url": "https://en.wikipedia.org/wiki?curid=268020", "title": "Evolutionary computation", "text": "Evolutionary computation\n\nIn computer science, evolutionary computation is a family of algorithms for global optimization inspired by biological evolution, and the subfield of artificial intelligence and soft computing studying these algorithms. In technical terms, they are a family of population-based trial and error problem solvers with a metaheuristic or stochastic optimization character.\n\nIn evolutionary computation, an initial set of candidate solutions is generated and iteratively updated. Each new generation is produced by stochastically removing less desired solutions, and introducing small random changes. In biological terminology, a population of solutions is subjected to natural selection (or artificial selection) and mutation. As a result, the population will gradually evolve to increase in fitness, in this case the chosen fitness function of the algorithm.\n\nEvolutionary computation techniques can produce highly optimized solutions in a wide range of problem settings, making them popular in computer science. Many variants and extensions exist, suited to more specific families of problems and data structures. Evolutionary computation is also sometimes used in evolutionary biology as an \"in silico\" experimental procedure to study common aspects of general evolutionary processes.\n\nThe use of evolutionary principles for automated problem solving originated in the 1950s. It was not until the 1960s that three distinct interpretations of this idea started to be developed in three different places.\n\n\"Evolutionary programming\" was introduced by Lawrence J. Fogel in the US, while John Henry Holland called his method a \"genetic algorithm\". In Germany Ingo Rechenberg and Hans-Paul Schwefel introduced \"evolution strategies\". These areas developed separately for about 15 years. From the early nineties on they are unified as different representatives (\"dialects\") of one technology, called \"evolutionary computing\". Also in the early nineties, a fourth stream following the general ideas had emerged – \"genetic programming\". Since the 1990s, nature-inspired algorithms are becoming an increasingly significant part of the evolutionary computation.\n\nThese terminologies denote the field of evolutionary computing and consider evolutionary programming, evolution strategies, genetic algorithms, and genetic programming as sub-areas.\n\nSimulations of evolution using evolutionary algorithms and artificial life started with the work of Nils Aall Barricelli in the 1960s, and was extended by Alex Fraser, who published a series of papers on simulation of artificial selection. Artificial evolution became a widely recognised optimisation method as a result of the work of Ingo Rechenberg in the 1960s and early 1970s, who used evolution strategies to solve complex engineering problems. Genetic algorithms in particular became popular through the writing of John Holland. As academic interest grew, dramatic increases in the power of computers allowed practical applications, including the automatic evolution of computer programs. Evolutionary algorithms are now used to solve multi-dimensional problems more efficiently than software produced by human designers, and also to optimise the design of systems.\n\nEvolutionary computing techniques mostly involve metaheuristic optimization algorithms. Broadly speaking, the field includes:\n\n\nEvolutionary algorithms form a subset of evolutionary computation in that they generally only involve techniques implementing mechanisms inspired by biological evolution such as reproduction, mutation, recombination, natural selection and survival of the fittest. Candidate solutions to the optimization problem play the role of individuals in a population, and the cost function determines the environment within which the solutions \"live\" (see also fitness function). Evolution of the population then takes place after the repeated application of the above operators.\n\nIn this process, there are two main forces that form the basis of evolutionary systems: Recombination and mutation create the necessary diversity and thereby facilitate novelty, while selection acts as a force increasing quality.\n\nMany aspects of such an evolutionary process are stochastic. Changed pieces of information due to recombination and mutation are randomly chosen. On the other hand, selection operators can be either deterministic, or stochastic. In the latter case, individuals with a higher fitness have a higher chance to be selected than individuals with a lower fitness, but typically even the weak individuals have a chance to become a parent or to survive.\n\nGenetic algorithms deliver methods to model biological systems and systems biology that are linked to the theory of dynamical systems, since they are used to predict the future states of the system. This is just a vivid (but perhaps misleading) way of drawing attention to the orderly, well-controlled and highly structured character of development in biology.\n\nHowever, the use of algorithms and informatics, in particular of computational theory, beyond the analogy to dynamical systems, is also relevant to understand evolution itself. \n\nThis view has the merit of recognizing that there is no central control of development; organisms develop as a result of local interactions within and between cells. The most promising ideas about program-development parallels seem to us to be ones that point to an apparently close analogy between processes within cells, and the low-level operation of modern computers . Thus, biological systems are like computational machines that process input information to compute next states, such that biological systems are closer to a computation than classical dynamical system .\n\nFurthermore, following concepts from computational theory, micro processes in biological organisms are fundamentally incomplete and undecidable (completeness (logic)), implying that “there is more than a crude metaphor behind the analogy between cells and computers .\n\nThe analogy to computation extends also to the relationship between inheritance systems and biological structure, which is often thought to reveal one of the most pressing problems in explaining the origins of life.\n\nThe list of active researchers is naturally dynamic and non-exhaustive. A network analysis of the community was published in 2007.\n\n\nThe main conferences in the evolutionary computation area include \n\n"}
{"id": "14222722", "url": "https://en.wikipedia.org/wiki?curid=14222722", "title": "Explanandum and explanans", "text": "Explanandum and explanans\n\nAn explanandum (a Latin term) is a sentence describing a phenomenon that is to be explained, and the explanans are the sentences adduced as explanations of that phenomenon. For example, one person may pose an \"explanandum\" by asking \"Why is there smoke?\", and another may provide an \"explanans\" by responding \"Because there is a fire\". In this example, \"smoke\" is the \"explanandum\", and \"fire\" is the \"explanans\".\n\nCarl Gustav Hempel and Paul Oppenheim (1948), in their deductive-nomological model of scientific explanation, motivated the distinction between explanans and explanandum in order to answer why-questions, rather than simply what-questions:\n\nSpecifically, they define the concepts as follows:\n\nThe crucial comment, with respect to the scientific method, is given as follows:\n\n"}
{"id": "191933", "url": "https://en.wikipedia.org/wiki?curid=191933", "title": "Exponential growth", "text": "Exponential growth\n\nExponential growth is exhibited when the rate of change—the change per instant or unit of time—of the value of a mathematical function is proportional to the function's current value, resulting in its value at any time being an exponential function of time, i.e., a function in which the time value is the exponent.\nExponential decay occurs in the same way when the growth rate is negative. In the case of a discrete domain of definition with equal intervals, it is also called geometric growth or geometric decay, the function values forming a geometric progression. In either exponential growth or exponential decay, the ratio of the rate of change of the quantity to its current size remains constant over time.\n\nThe formula for exponential growth of a variable \"x\" at the growth rate \"r\", as time \"t\" goes on in discrete intervals (that is, at integer times 0, 1, 2, 3, ...), is\n\nwhere \"x\" is the value of \"x\" at time 0. This formula is transparent when the exponents are converted to multiplication. For instance, with a starting value of 50 and a growth rate of per interval, the passage of one interval would give ; two intervals would give ; and three intervals would give . In this way, each increase in the exponent by a full interval can be seen to increase the previous total by another five percent. (The order of multiplication does not change the result based on the associative property of multiplication.)\n\nSince the time variable, which is the input to this function, occurs as the exponent, this is an exponential function. This contrasts with growth based on a power function, where the time variable is the base value raised to a fixed exponent, such as cubic growth (or in general terms denoted as polynomial growth).\n\n\nA quantity \"x\" depends exponentially on time \"t\" if\n\nwhere the constant \"a\" is the initial value of \"x\",\n\nthe constant \"b\" is a positive growth factor, and \"τ\" is the time constant—the time required for \"x\" to increase by one factor of \"b\":\n\nIf and , then \"x\" has exponential growth. If and , or and 0 < , then \"x\" has exponential decay.\n\nExample: \"If a species of bacteria doubles every ten minutes, starting out with only one bacterium, how many bacteria would be present after one hour?\" The question implies \"a\" = 1, \"b\" = 2 and \"τ\" = 10 min.\n\nAfter one hour, or six ten-minute intervals, there would be sixty-four bacteria.\n\nMany pairs (\"b\", \"τ\") of a dimensionless non-negative number \"b\" and an amount of time \"τ\" (a physical quantity which can be expressed as the product of a number of units and a unit of time) represent the same growth rate, with \"τ\" proportional to log \"b\". For any fixed \"b\" not equal to 1 (e.g. \"e\" or 2), the growth rate is given by the non-zero time \"τ\". For any non-zero time \"τ\" the growth rate is given by the dimensionless positive number \"b\".\n\nThus the law of exponential growth can be written in different but mathematically equivalent forms, by using a different base. The most common forms are the following:\n\nwhere \"x\" expresses the initial quantity \"x\"(0).\n\nParameters (negative in the case of exponential decay):\nThe quantities \"k\", \"τ\", and \"T\", and for a given \"p\" also \"r\", have a one-to-one connection given by the following equation (which can be derived by taking the natural logarithm of the above):\n\nwhere \"k\" = 0 corresponds to \"r\" = 0 and to \"τ\" and \"T\" being infinite.\n\nIf \"p\" is the unit of time the quotient \"t\"/\"p\" is simply the number of units of time. Using the notation \"t\" for the (dimensionless) number of units of time rather than the time itself, \"t\"/\"p\" can be replaced by \"t\", but for uniformity this has been avoided here. In this case the division by \"p\" in the last formula is not a numerical division either, but converts a dimensionless number to the correct quantity including unit.\n\nA popular approximated method for calculating the doubling time from the growth rate is the rule of 70,\ni.e. formula_9.\n\nIf a variable \"x\" exhibits exponential growth according to formula_10, then the log (to any base) of \"x\" grows linearly over time, as can be seen by taking logarithms of both sides of the exponential growth equation:\n\nThis allows an exponentially growing variable to be modeled with a log-linear model. For example, if one wishes to empirically estimate the growth rate from intertemporal data on \"x\", one can linearly regress log \"x\" on \"t\".\n\nThe exponential function formula_12 satisfies the linear differential equation:\n\nsaying that the change per instant of time of \"x\" at time \"t\" is proportional to the value of \"x\"(\"t\"), and \"x\"(\"t\") has the initial value\n\nThe differential equation is solved by direct integration:\n\nso that\n\nIn the above differential equation, if , then the quantity experiences exponential decay.\n\nFor a nonlinear variation of this growth model see logistic function.\n\nThe difference equation\n\nhas solution\n\nshowing that \"x\" experiences exponential growth.\n\nIn the long run, exponential growth of any kind will overtake linear growth of any kind (the basis of the Malthusian catastrophe) as well as any polynomial growth, i.e., for all α:\n\nThere is a whole hierarchy of conceivable growth rates that are slower than exponential and faster than linear (in the long run). See Degree of a polynomial#The degree computed from the function values.\n\nGrowth rates may also be faster than exponential. In the most extreme case, when growth increases without bound in finite time, it is called hyperbolic growth. In between exponential and hyperbolic growth lie more classes of growth behavior, like the hyperoperations beginning at tetration, and formula_20, the diagonal of the Ackermann function.\n\nExponential growth models of physical phenomena only apply within limited regions, as unbounded growth is not physically realistic. Although growth may initially be exponential, the modelled phenomena will eventually enter a region in which previously ignored negative feedback factors become significant (leading to a logistic growth model) or other underlying assumptions of the exponential growth model, such as continuity or instantaneous feedback, break down.\n\nAccording to an old legend, vizier Sissa Ben Dahir presented an Indian King Sharim with a beautiful, hand-made chessboard. The king asked what he would like in return for his gift and the courtier surprised the king by asking for one grain of rice on the first square, two grains on the second, four grains on the third etc. The king readily agreed and asked for the rice to be brought. All went well at first, but the requirement for 2 grains on the \"n\"th square demanded over a million grains on the 21st square, more than a million million ( trillion) on the 41st and there simply was not enough rice in the whole world for the final squares. (From Swirski, 2006)\n\nThe second half of the chessboard is the time when an exponentially growing influence is having a significant economic impact on an organization's overall business strategy.\n\nFrench children are told a story in which they imagine having a pond with water lily leaves floating on the surface. The lily population doubles in size every day and, if left unchecked, it will smother the pond in thirty days killing all the other living things in the water. Day after day, the plant's growth is small and so it is decided that it shall be cut down when the water lilies cover half of the pond. The children are then asked on what day will half of the pond be covered in water lilies. The solution is simple when one considers that the water lilies must double to completely cover the pond on the thirtieth day. Therefore, the water lilies will cover half of the pond on the twenty-ninth day. There is only one day to save the pond. (From Meadows \"et al\". 1972)\n\n\n"}
{"id": "557913", "url": "https://en.wikipedia.org/wiki?curid=557913", "title": "Freedom of thought", "text": "Freedom of thought\n\nFreedom of thought (also called freedom of conscience or ideas) is the freedom of an individual to hold or consider a fact, viewpoint, or thought, independent of others' viewpoints. \n\nFreedom of thought is the precursor and progenitor of—and thus is closely linked to—other liberties, including freedom of religion, freedom of speech, and freedom of expression. Though freedom of thought is axiomatic for many other freedoms they are in no way required for it to operate and exist. The conception of a freedom or a right does not guarantee its inclusion, legality, or protection via a philosophical caveat. It is a very important concept in the Western world and nearly all democratic constitutions protect these freedoms. For instance, the Bill of Rights contains the famous guarantee in the First Amendment that laws may not be made that interfere with religion \"or prohibiting the free exercise thereof\". U.S. Supreme Court Justice Benjamin Cardozo reasoned in \"Palko v. Connecticut\" (1937):\n\nSuch ideas are also a vital part of international human rights law. In the Universal Declaration of Human Rights (UDHR), which is legally binding on member states of the International Covenant on Civil and Political Rights (ICCPR), \"freedom of thought\" is listed under Article 18:\n\nThe United Nations' Human Rights Committee states that this, \"distinguishes the freedom of thought, conscience, religion or belief from the freedom to manifest religion or belief. It does not permit any limitations whatsoever on the freedom of thought and conscience or on the freedom to have or adopt a religion or belief of one's choice. These freedoms are protected unconditionally\". Similarly, Article 19 of the UDHR guarantees that \"Everyone has the right to freedom of opinion and expression; this right includes freedom to hold opinions without interference\".\n\nIt is impossible to know with certainty what another person is thinking, making suppression difficult. The concept is developed throughout the Bible, most fully in the writings of Paul of Tarsus (e.g., \"For why should my freedom [\"eleutheria\"] be judged by another's conscience [\"suneideseos\"]?\" 1 Corinthians 10:29).\nAlthough Greek philosophers Plato and Socrates had discussed Freedom of Thought minimally, the edicts of King Ashoka (3rd century BC) have been called the first decree respecting Freedom of Conscience. In European tradition, aside from the decree of religious toleration by Constantine I at Milan in 313, the philosophers Themistius, Michel de Montaigne, Baruch Spinoza, John Locke, Voltaire, Alexandre Vinet, and John Stuart Mill have been considered major proponents of the idea of Freedom of Conscience.\n\nQueen Elizabeth I revoked a thought censorship law in the late sixteenth century, because, according to Sir Francis Bacon, she did \"not [like] to make windows into men's souls and secret thoughts\". During her reign, philosopher, mathematician, astrologer, and astronomer Giordano Bruno took refuge in England from the Italian Inquisition, where he published a number of his books regarding an infinite universe and topics banned by the Catholic Church. After leaving the safety of England, Bruno was eventually burned as a heretic in Rome for refusing to recant his ideas. For this reason, he is considered by some to be a martyr for free thought.\n\nHowever, freedom of expression can be limited through censorship, arrests, book burning, or propaganda, and this tends to discourage freedom of thought. Examples of effective campaigns against freedom of expression are the Soviet suppression of genetics research in favor of a theory known as Lysenkoism, the book-burning campaigns of Nazi Germany, the radical anti-intellectualism enforced in Cambodia under Pol Pot, the strict limits on freedom of expression imposed by the Communist governments of the People's Republic of China and Cuba or by right-wing authoritarian dictatorships such as those of Augusto Pinochet in Chile and Francisco Franco in Spain.\n\nThe Sapir–Whorf hypothesis, which states that thought is inherently embedded in language, would support the claim that an effort to limit the use of words of language is actually a form of restricting freedom of thought. This was explored in George Orwell's novel\" 1984\", with the idea of Newspeak, a stripped-down form of the English language lacking the capacity for metaphor and limiting expression of original ideas.\n\n\n"}
{"id": "325232", "url": "https://en.wikipedia.org/wiki?curid=325232", "title": "Greenwashing", "text": "Greenwashing\n\nGreenwashing (a compound word modelled on \"whitewash\"), also called \"green sheen\", is a form of spin in which green PR or green marketing is deceptively used to promote the perception that an organization's products, aims or policies are environmentally friendly. Evidence that an organization is greenwashing often comes from pointing out the spending differences: when significantly more money or time has been spent advertising being \"green\" (that is, operating with consideration for the environment), than is actually spent on environmentally sound practices. Greenwashing efforts can range from changing the name or label of a product to evoke the natural environment on a product that contains harmful chemicals to multimillion-dollar advertising campaigns portraying highly polluting energy companies as eco-friendly.\nPublicized accusations of greenwashing have contributed to the term's increasing use.\n\nWhile greenwashing is not new, its use has increased over recent years to meet consumer demand for environmentally friendly goods and services. The problem is compounded by lax enforcement by regulatory agencies such as the Federal Trade Commission in the United States, the Competition Bureau in Canada, and the Committee of Advertising Practice and the Broadcast Committee of Advertising Practice in the United Kingdom. Critics of the practice suggest that the rise of greenwashing, paired with ineffective regulation, contributes to consumer skepticism of all green claims, and diminishes the power of the consumer in driving companies toward greener solutions for manufacturing processes and business operations. Many corporate structures use greenwashing as a way to repair public perception of their brand. The structuring of corporate disclosure is often set up so as to maximize perceptions of legitimacy. However, a growing body of social and environmental accounting research finds that, in the absence of external monitoring a verification, greenwashing strategies amount to corporate posturing and deception.\n\nThe term \"greenwashing\" was coined by New York environmentalist Jay Westervelt in a 1986 essay regarding the hotel industry's practice of placing placards in each room promoting reuse of towels ostensibly to \"save the environment.\" Westervelt noted that, in most cases, little or no effort toward reducing energy waste was being made by these institutions—as evidenced by the lack of cost reduction this practice effected. Westervelt opined that the actual objective of this \"green campaign\" on the part of many hoteliers was, in fact, increased profit. Westervelt thus labeled this and other outwardly environmentally conscientious acts with a greater, underlying purpose of profit increase as \"greenwashing\".\n\nIn addition, the political term \"linguistic detoxification\" describes when, through legislation or other government action, the definitions of toxicity for certain substances are changed, or the name of the substance is changed, so that fewer things fall under a particular classification as toxic. The origin of this phrase has been attributed to environmental activist and author Barry Commoner.\n\nSimilarly, introduction of a Carbon Emission Trading Scheme may feel good, but may be counterproductive if the cost of carbon is priced too low, or if large emitters are given \"free credits.\" For example, Bank of America subsidiary MBNA offers an Eco-Logique MasterCard for Canadian consumers that rewards customers with carbon offsets as they continue using the card. Customers may feel that they are nullifying their carbon footprint by purchasing polluting goods with the card. However, only 0.5 percent of purchase price goes into purchasing carbon offsets, while the rest of the interchange fee still goes to the bank.\n\nSuch campaigns and marketing communications, designed to publicize and highlight organizational CSR policies to various stakeholders, affect corporate reputation and brand image, but the proliferation of unsubstantiated ethical claims and greenwashing by some companies has resulted in increasing consumer cynicism and mistrust.\n\nIn the mid 1960s, the environmental movement gained momentum. This popularity prompted many companies to create a new green image through advertising. Jerry Mander, a former Madison Avenue advertising executive, called this new form of advertising \"ecopornography.\"\n\nThe first Earth Day was held on April 22, 1970. This encouraged many industries to advertise themselves as being friendly to the environment. Public utilities spent 300 million dollars advertising themselves as clean green companies. This was eight times more than the money they spent on pollution reduction research.\n\nIn 1985, the Chevron Corporation launched one of the most famous greenwashing ad campaigns in history. Chevron's \"People Do\" advertisements were aimed at a \"hostile audience\" of \"societally conscious\" people. Two years after the launch of the campaign, surveys found people in California trusted Chevron more than other oil companies to protect the environment. In the late 1980s The American Chemistry Council started a program called Responsible Care, which shone light on the environmental performances and precautions of the group's members. The loose guidelines of responsible care caused industries to adopt self-regulation over government regulation.\n\nIn 1991, a study published in the Journal of Public Policy and Marketing (American Marketing Association) found that 58% of environmental ads had at least one deceptive claim. Another study found that 77% of people said the environmental reputation of company affected whether they would buy their products. One fourth of all household products marketed around Earth Day advertised themselves as being green and environmentally friendly. In 1998 the Federal Trade Commission created the \"Green Guidelines,\" which defined terms used in environmental marketing. The following year the FTC found that the Nuclear Energy Institute claims of being environmentally clean were not true. The FTC did nothing about the ads because they were out of their jurisdiction. This caused the FTC to realize they needed new clear enforceable standards. In 1999, according to environmental activist organizations, the word \"greenwashing\" was added to the \"Oxford English Dictionary\".\n\nIn 2002, during the World Summit on Sustainable Development in Johannesburg, the Greenwashing Academy hosted the Greenwash Academy Awards. The ceremony awarded companies like BP, ExxonMobil, and even the US Government for their elaborate greenwashing ads and support for greenwashing.\n\nMore recently, social scientists have been investigating claims of and the impact of greenwashing. In 2005, Ramus and Monteil conducted secondary data analysis of two databases to uncover corporate commitment to implementation of environmental policies as opposed to greenwashing. They found while companies in the oil and gas are more likely to implement environmental policies than service industry companies, they are less likely to commit to fossil fuel reduction.\n\nIn 2010 a study was done showing that 4.5% of products tested were found to be truly green as opposed to 2% in 2009. In 2009 2,739 products claimed to be green while in 2010 the number rose to 4,744. The same study in 2010 found that 95% percent of the consumer products claiming to be green were not green at all.\n\nThe Australian Trade Practices Act has been modified to include punishment of companies that provide misleading environmental claims. Any organization found guilty of such could face up $6 million in fines. In addition, the guilty party must pay for all expenses incurred while setting the record straight about their product or company's actual environmental impact.\n\nCanada's Competition Bureau along with the Canadian Standards Association are discouraging companies from making \"vague claims\" towards their products' environmental impact. Any claims must be backed up by \"readily available data.\"\n\nNorway's consumer ombudsman has targeted automakers who claim that their cars are \"green,\" \"clean\" or \"environmentally friendly\" with some of the world's strictest advertising guidelines. Consumer Ombudsman official Bente Øverli said: \"Cars cannot do anything good for the environment except less damage than others.\" Manufacturers risk fines if they fail to drop the words. Øverli said she did not know of other countries going so far in cracking down on cars and the environment.\n\nThe Federal Trade Commission (FTC) provides voluntary guidelines for environmental marketing claims. These guidelines give the FTC the right to prosecute false and misleading advertisement claims. The green guidelines were not created to be used as an enforceable guideline but instead were intended to be followed voluntarily. Listed below are the green guidelines set by the FTC.\n\nThe FTC has said in 2010 that it will update its guidelines for environmental marketing claims in an attempt to reduce greenwashing. The revision to the FTC's Green Guides covers a wide range of public input, including hundreds of consumer and industry comments on previously proposed revisions. The updates and revision to the existing Guides include a new section of carbon offsets, \"green\" certifications and seals renewable energy and renewable materials claims. According to FTC Chairman Jon Leibowitz, \"The introduction of environmentally friendly products into the marketplace is a win for consumers who want to purchase greener products and producers who wants to sell them.\" Leibowitz also says the win-win can only claim if marketers' claims are straightforward and proven.\n\nIn 2013, the FTC began enforcing the revisions put forth in the Green Guides. The FTC cracked down on six different companies, in which five of the cases were concerned with the false or misleading advertising surrounding the biodegradability of plastics. The FTC charged ECM Biofilms, American Plastic Manufacturing, CHAMP, Clear Choice Housewares, and Carnie Cap, for misrepresenting the biodegradability of their plastics treated with additives.\n\nThe FTC charged a sixth company, AJM Packaging Corporation, for violating a commission consent order put in place that prohibits companies from using advertising claims based on the product or packaging being \"degradable, biodegradable, or photodegradable\" without reliable scientific information. The FTC now requires companies to disclose and provide the information that qualifies their environmental claims to ensure transparency.\n\n\nOrganizations and individuals are making attempts to reduce the impact of greenwashing by exposing it to the public. The Greenwashing Index, created by the University of Oregon in partnership with EnviroMedia Social Marketing, allows examples of greenwashing to be uploaded and rated by the public. The British Code of Advertising, Sales Promotion and Direct Marketing has a specific section (section 49) targeting environmental claims.\n\nAccording to some organizations opposing greenwashing, there has been a significant increase in its use by companies over the last decade. TerraChoice Environmental Marketing, an advertising consultancy company, issued a report denoting a 79% increase in the usage of corporate greenwashing between 2007 and 2009. Additionally, it has begun to manifest itself in new varied ways. Within the non-residential building products market in the United States, some companies are beginning to claim that their environmentally minded policy changes will allow them to earn points through the U.S. Green Building Council's Leadership in Energy and Environmental Design rating program. This point system has been held up as an example of the \"gateway effect\" that the drive to market products as environmentally friendly is having on company policies. Some have claimed that the greenwashing trend may be enough to eventually effect a genuine reduction in environmentally damaging practices. \n\nAccording to the Home and Family Edition, 95% consumer products claiming to be green were discovered to commit at least one of the \"Sins of Greenwashing\". The Seven Sins of Greenwashing are as follows:\n\nIn 2008, Ed Gillespie identified \"ten signs of greenwashing\", which are similar to the \"Seven Sins\" listed above, but with three additional indicators.\n\nCompanies may pursue environmental certification to avoid greenwashing through independent verification of their green claims. For example, the Carbon Trust Standard launched in 2007 with the stated aim \"to end 'greenwash' and highlight firms that are genuine about their commitment to the environment\".\n\n\n\n"}
{"id": "46187368", "url": "https://en.wikipedia.org/wiki?curid=46187368", "title": "History of modernisation theory", "text": "History of modernisation theory\n\nThis article delineates the history of modernisation theory. Modernisation refers to a model of a progressive transition from a 'pre-modern' or 'traditional' to a 'modern' society. The theory looks at the internal factors of a country while assuming that, with assistance, \"traditional\" countries can be brought to development in the same manner more developed countries have. Modernisation theory attempts to identify the social variables that contribute to social progress and development of societies, and seeks to explain the process of social evolution. Modernisation theory is subject to criticism originating among socialist and free-market ideologies, world-systems theorists, globalisation theorists and dependency theorists among others. Modernisation theory not only stresses the process of change, but also the responses to that change. It also looks at internal dynamics while referring to social and cultural structures and the adaptation of new technologies. \n\nThe basic principles of Modernisation theory can be derived from the Idea of Progress, which emerged in the 18th century Age of Enlightenment with the idea that people themselves could develop and change their society. The French philosopher Marquis de Condorcet was involved in the origins of the theory with the idea that technological advancements and economic changes can enable changes in moral and cultural values. Condorcet was the first to make the connection between economic and social development and to suggest that there can be continuous progress and improvement in human affairs. The logic of this view implies that new processes and improvements are continually needed to keep pace with a constantly changing world. Furthermore, Condorcet advocated technological advancement as a means of giving people further control over their environments, arguing that technological progress would eventually spur social progress. \n\nIn addition to social structure and the evolution of societies, the French sociologist Émile Durkheim developed the concept of functionalism, which stresses the interdependence of the institutions of a society and their interaction in maintaining cultural and social unity. His most famous work is \"The Division of Labour in Society\", which described mechanisms for the maintenance of social order and the ways in which primitive societies might make the transition to becoming more economically advanced industrial societies. Durkheim suggested that in a capitalist society, with a complex division of labour, economic regulation would be needed to maintain order. He stressed that the major transition from a primitive social order to a more advanced industrial society could otherwise bring crisis and disorder. Durkheim furthermore developed the idea of social evolution, which was coined by Herbert Spencer, which indicates how societies and cultures develop over time; for Durkheim, social evolution is like biological evolution with reference to the development of its components. As with living organisms and species, societies progress through several stages, generally beginning at a simple level and developing toward a more complex level of organisation. Societies adapt to surrounding environments, but also interact with other societies, which further contributes to progress and development. \nModern sociology evolved in part as a reaction to the problems associated with modernity, such as industrialisation and the process of 'rationalisation'. \n\nModernisation theory emerged further in the late 19th century and was especially popular among scholars in the mid-20th century. One notable advocate was Harvard sociologist Talcott Parsons whose \"Mandarins of the Future\" (2003) stressed the importance of societies remaining open to change and saw reactionary forces as restricting development. Maintaining tradition for tradition's sake was thought to be harmful to progress and development. Proponents of modernisation tend to fall into two camps, optimists and pessimists. The former view holds that what some see as a setback for the theory (events such as the Iranian Revolution or the persistence of instability in the Democratic Republic of Congo) are invariably temporary setbacks on the road to progress. Pessimists argue that certain 'non-modern' areas of the world are incapable of becoming modern.\n\nThe eminent sociologist Max Weber also made important contributions to development theory. Weber's concept of \"rationalisation\" was mobilised by those who held that the most important factor behind modernisation was the growth of rationality as a core value. Normally rationality denotes the universally available logic underpinning thought and deliberation in a particular society. Most theorists consider it indispensable for the modernisation process. Rationality allows people to think in new and innovative ways; innovation is thus coeval with modernisation.\n\nAmong the academics who contributed much to this theory are W. W. Rostow, who in his \"The Stages of Economic Growth: A Non-Communist Manifesto\" (1960) concentrates on the economic system side of the modernisation, trying to show factors needed for a country to reach the path to modernisation in his Rostovian take-off model. David Apter concentrated on the political system and history of democracy, researching the connection between democracy, good governance and efficiency and modernisation. Seymour Martin Lipset in \"Some Social Requisites of Democracy\" (1959) argued that economic development sets off a series of profound social changes that together tend to produce democracy. David McClelland (\"The Achieving Society\", 1967) approached this subject from the psychological perspective, with his motivations theory, arguing that modernisation cannot happen until a given society values innovation, striving for improvement and entrepreneurship. Alex Inkeles (\"Becoming Modern\", 1974) similarly creates a model of \"modern personality\", which needs to be independent, active, interested in public policies and cultural matters, open for new experiences, rational and being able to create long-term plans for the future. Edward Said's \"Orientalism\" (1978) interprets modernisation from the point of view of societies that are quickly and radically transformed. \n\nThe Progressives in the United States in the early 20th century were avid modernisers. They believed in science, technology, expertise—and especially education—as the grand solution to society's weaknesses. Characteristics of progressivism included a favorable attitude toward urban-industrial society, belief in mankind's ability to improve the environment and conditions of life, belief in obligation to intervene in economic and social affairs, and a belief in the ability of experts and in efficiency of government intervention.\n\nPaul Monroe, a professor of history at Columbia University, was a member of The Inquiry—a team of American experts at the Paris Peace Conference in 1919. He drew on his experience in the Philippines to assess the educational needs of developing areas such as Albania, Turkey and central Africa. Presenting educational development as instrumental to nation-building and socioeconomic development, Monroe recommended the implementation of a progressive curriculum - with an emphasis on practical, adult, and teacher training - in a national system of education, as a basis for self-development, except in Africa. His approach shaped American cooperation with developing countries in the 1920s and modernisation efforts during the 1920s-1930s.\n\nKocka (1988) and Sheri Berman are historians who emphasise the central importance of a German \"Sonderweg\" (\"special path\") or \"exceptionalism\" as the root of Nazism and the German catastrophe in the 20th century. Fritz Fischer and his students emphasised Germany’s primary guilt for causing World War I.\n\nHans-Ulrich Wehler, a leader of the Bielefeld School of social history, places the origins of Germany's path to disaster in the 1860s-1870s, when economic modernisation took place, but political modernisation did not happen and the old Prussian rural elite remained in firm control of the army, diplomacy and the civil service. Traditional, aristocratic, premodern society battled an emerging capitalist, bourgeois, modernising society. Recognising the importance of modernising forces in industry and the economy and in the cultural realm, Wehler argues that reactionary traditionalism dominated the political hierarchy of power in Germany, as well as social mentalities and in class relations (Klassenhabitus). The catastrophic German politics between 1914 and 1945 are interpreted in terms of a delayed modernisation of its political structures. At the core of Wehler's interpretation is his treatment of \"the middle class\" and \"revolution,\" each of which was instrumental in shaping the 20th century. Wehler's examination of Nazi rule is shaped by his concept of \"charismatic domination,\" which focuses heavily on Adolf Hitler.\n\nThe historiographical concept of a German Sonderweg has had a turbulent history. Scholars of the 19th century who emphasised a separate German path to modernity saw it as a positive factor that differentiated Germany from the \"western path\" typified by Great Britain. The stressed the strong bureaucratic state, reforms initiated by Bismarck and other strong leaders, the Prussian service ethos, the high culture of philosophy and music, and Germany's pioneering of a social welfare state. In the 1950s, historians in West Germany argued that the Sonderweg led Germany to the disaster of 1933-1945. The special circumstances of German historical structures and experiences, were interpreted as preconditions that, while not directly causing National Socialism, did hamper the development of a liberal democracy and facilitate the rise of fascism. The Sonderweg paradigm has provided the impetus for at least three strands of research in German historiography: the long 19th century, the history of the bourgeoisie, and comparisons with the West. After 1990, increased attention to cultural dimensions and to comparative and relational history moved German historiography to different topics, with much less attention paid to the Sonderweg. While some historians have abandoned the Sonderweg thesis, they have not provided a generally accepted alternative interpretation.\n\nIn his seminal book \"Peasants Into Frenchmen: The Modernisation of Rural France, 1880–1914\" (1976), historian Eugen Weber traced the modernisation of French villages and argued that rural France went from backward and isolated to modern and possessing a sense of French nationhood during the late 19th and early 20th centuries. He emphasised the roles of railroads, republican schools, and universal military conscription. He based his findings on school records, migration patterns, military service documents and economic trends. Weber argued that until 1900 or so a sense of French nationhood was weak in the provinces. Weber then looked at how the policies of the Third Republic created a sense of French nationality in rural areas. The book was widely praised, but was criticised by some who argued that a sense of Frenchness existed in the provinces before 1870.\n\nThe modernising force of the post-civil war Greek society came primarily as a result of the European and US geopolitical strategy for the region of eastern Mediterranean. Greece ought to be a modern capitalistic state to counter the proximity of several eastern and third world bloc countries and the strong national communist movement. According to Truman doctrine and with the support of local elites, a great economical leap forward took place along with the severe repression that led to the 1967 coup d'état.\nThis dramatic change covered the long-standing cultural divide of greek academia, comparing modern and neo-greek to ancient and traditional identities. Music, art and cinema, influenced by the pioneers of American and European tendencies thrived, until the milestone of 1967, in contrast to the authoritarian and traditionalist military and paramilitary structures. This dimension is vital as it reveals the process of modernisation under the western directives in all social levels that came in fact in opposition to the political directives of the same source.\n\nMany studies of modernisation have focused on the history of Japan in the late 19th century, and China and India in the late 20th century. For example, the process of borrowing science and technology from the West has been explored.\n\nModernisation theory failed to explain the experience of China. Mao modernised the People's Republic of China with massive industrialisation projects and social transformation. However, China did not become a democratic country after its modernisation. Nowadays, even though the Soviet-style authoritarian regimes have already collapsed worldwide, China did not have any major political reforms after Mao's death. The country remained authoritarian, despite the size of its economic sector.\n\nChina has been attempting to modernise ever since the Revolution of 1911 and the end of the Qing Dynasty, the last dynasty of China. Before the Qing Dynasty was overthrown, it attempted to reform from 1902 to 1908 to save itself and instigated reforms in infrastructure, transportation, and government. These reforms were based on Western models and even included aspects of democracy, which are often associated with the process of modernisation. However, these reforms were largely unsuccessful and resulted in the Revolution of 1911. Following the Revolution of 1911, other movements such as the May 4th Movement of 1919 advocated for modernisation, iconoclasm, and a rejection of foreign influence and imperialism. From the beginning of the 20th century until the establishment of the People’s Republic of China in 1949, China has been delayed in efforts to modernise due to an era of warlordism, the Second Sino-Japanese War, and civil war between the CCP and KMT.\n\nWhen the communist party came to power in 1949, Mao Zedong used the Soviet Union as China’s example for modernisation. The Great Leap Forward from 1958-1961 was Mao’s version of the Soviet Union’s Five year Plan, and its goals were to create a modern communist society through industrialisation and collectivisation. Mao Zedong aimed to become a world power without foreign, mainly western, involvement, ideas, or capitalism and preached the idea of self-reliance. Mao did contribute to the modernisation of China, however The Great Leap Forward is regarded as a failure and the Cultural Revolution from 1966 to 1976 was detrimental to China's industrialisation progress. However, during Mao's era, he transformed China from a predominantly agrarian country to an industrialised power. In the 1970s, China was able to produce most of commodities and goods by its own industry. Mao laid the foundation for China's economic development in Deng's era.\n\nThe economic reforms of Chinese Supreme Leader Deng Xiaoping are attributed to China’s economic success in the 21st century. Deng focused on four modernisations: agriculture, industry, national defense, and science and technology. The West was used as an example for several of these modernisations, however their management was completely Chinese. Deng began de-collectivisation and allowed Township and Village Enterprises (TVE), Special Economic Zones (SEZ), foreign investment, profit incentive, and even privatisation.\n\nWhile Mao advocated self-reliance, Deng generated foreign exchange to finance modernisation. His famous quote is, \"It doesn’t matter whether a cat is white or black, as long as it catches mice.\" Post Deng Reforms continued on this path, which is acknowledged as a shift from the iron rice bowl to the porcelain rice bowl, or government owned to privatised. Although China’s economy has shifted towards privatisation and capitalism, the PRC remains an authoritative regime, which is contradictory in comparison to other examples of countries that have modernised. Democracy is the political characteristic that has defined modernised nations in the past and the modernisation theory suggests democracy follows with the development of a modernised state. China was late in modernisation and has thus had many other countries as examples to base its model of modernisation off of.\n\nThe One-Child Policy has also been a technique to contribute or even force the modernisation of China. Instigated in 1978, the one-child policy has created a generation known as \"singletons\" or \"little emperors\" (\"xiao huangdi\"). \"The Chinese state enforced a rapid fertility transition designed to cultivate a generation of \"high-quality\" people with resources and ambition to join the global elite.\" These little emperors are expected to compete with the first-world countries having no siblings to compete with for parental investment. Normally with modernisation and urbanisation smaller or nuclear families evolve as the result. China has switched this logic, hoping that creating the culture of the nuclear family with the one-child policy it will produce modernisation.\n\nAt the beginning of the 21st century, China is still in the process of modernisation. In 2010 it had the third greatest GDP and GDP (PPP) in the world with the world’s largest labour force, and is acknowledged as the world’s second largest economy. In 2010 its economy was still increasing in growth at 10.3%. China has also successfully joined the largely Western international arena with its membership of the UN in 1971, the WTO in 2001, and hosted the Olympics in 2008. China’s goal is to continue modernising until it joins the first-world and becomes the core instead of the semi-periphery or periphery, from the core-periphery model.\n\nThe modernisation of China through urbanisation, industrialisation, and economic policy has benefited the country economically as it rises as a world power in the 21st century. However it now is experiencing the problems associated the other modern countries and capitalism. These problems include the growing disparity between the rich and poor, urban vs. rural and migration, and ecological issues.\n\nModernisers in South Korea in the late 19th century were torn between the American and the Japanese models. Most of the Koreans involved were educated Christians who saw America as their ideal model of civilisation. However, most used Japan as a practical model - as an example of how a fellow East Asian country, which 30 years before was also backward, could succeed in modernising itself. At the same time, reformists' nationalist reaction against the domineering, colonial behaviour of the Japanese in Korea often took the form of an appeal to international (Western) standards of civilisation. The Western-oriented worldview of the early Christian nationalist reformers was complex, multilayered, and often self-contradictory - with 'oppressive' features not easily distinguishable from 'liberational' ones. Their idealised image of the West as the only true, ideal civilisation relegated much of Korea's traditional culture to a position of 'oriental'.\n\nThe self-image of Koreans was formed through complex relationships with modernity, colonialism, Christianity, and nationalism. This formation was initiated by a change in the notion of 'civilisation' due to the transformation of 'international society' and thereafter was affected by the trauma of Japanese colonisation. Through the process of transition from a traditional Confucian notion of civilisation to a Western notion of acceptance and resistance, Koreans shaped their civilisation as well as their notions of the racial, cultural, and individual modern self. Western Orientalism, in particular, accompanied the introduction of the Western notion of civilisation, which served as the background for forming the self-identity of Koreans. The fact that the Japanese version of Orientalism emerged from the domination of Korea by Japan played a critical role in shaping the self-identity of Koreans. Consequently, Korea still maintains an inferiority complex toward Western culture, ambivalent feelings toward Japanese culture, and biased - positive or negative - views of their own cultural traditions. Thus both modernisation and colonisation have shaped the formation or distortion of self-consciousness of non-Western peoples.\n\nThe US launched a decades-long intensive development starting in 1945 to modernise South Korea, with the goal of helping it become a model nation-state and an economic success. Agents of modernisation at work in Korea included the US Army, the Economic Cooperation Administration, the UN Korean Reconstruction Agency, and a number of nongovernmental organisations, among them the Presbyterian Church, the YMCA, Boy Scouts and the Ford Foundation. Many Koreans migrated to California and Hawaii, and brought back firsthand accounts of modern business and governmental practices that they sought to adapt to Korean conditions.\n\nModernisation theory was not compatible with Japan's experience. After industrialisation and economic modernisation, a democratic society did not appear in Japan. Japan instead became an aggressive authoritarian fascist state, up until the Allied Forces defeated Japan in the Second World War. There were almost no democratic principles or practices in pre-war Japanese politics. Prior to the liberation of Japan by the United States from its military government, Japanese culture always honoured obedience and hierarchy, and Japanese people also despised individualism and liberty.\n\nJapan had already modernised and became an industrialised country during its Meiji period, which happened long before Japan's defeat in the Second World War. Japan was probably the first country in East Asia that industrialised successfully. It quickly became one of the imperialist and colonial powers. Japan defeated the Qing dynasty of China in 1894, and subsequently defeated Russia in 1905. Korea and Taiwan were annexed by the Japanese Empire later. When Japan invaded China in 1931, it had already finished its industrialising process and had enough industrial power to wage a war itself. Japan's navy was among one of the world's most advanced navies, with almost no challenger in the Pacific Ocean.\n\nThe performance of Japan after the Second War was simply a result of the military supremacy of the United States. The United States absolutely ruled over Japan after its defeat, with General MacArthur as the de facto emperor of Japan at the time. Japan had no choice but listen to the orders of General MacArthur and his colleagues. In order to be compatible with the foreign policy and national interest of the United States, SCAP and General MacArthur forced Japan to demilitarise and democratise after 1945.\n\nJapan quickly went through a lot of changes concerning modernisation after their defeat. This happened starting with United States quickly intervening with Japan internally. American demilitarisation and democratisation was accepted by the people of Japan without strong resistance.\n\nUnder the occupation of United States, SCAP (Supreme Commander for the Allied Powers) was established, which was an institution in charge of formulating policies. The goal of SCAP was to dissolve the army and navy, and also to punish responsible leaders, and it established the New constitution of 1946. SCAP could not function without the assistance of Japanese government because the language barrier was too high. However, this actually helped make bigger changes in creating demilitarisation and democratisation. SCAP helped people to gain freedoms of speech, press and the right to organise labour or farmer unions.\n\nAs such, the new constitution created by SCAP granted civil and political rights to the people; consequently, leading to modernisation. It also allowed freedom of speech and association, right to organise labour association and movements, or create unions, and implemented grounds for incredible civil right for women.\n\nThe government also held an important role in modernising Japan. Japan concentrated on their industry and technology to achieve growth. For that, the state gave assistances to several industries and adopted protectionism policies. Government stepped in as lender, as facilitator of access to foreign exchange, raw materials, or technology licenses, and as rescuer when problems arise. To illustrate, the most important guiding agency was the MITI (Ministry of International Trade and Industry). In addition, through the 1960s, Japanese government used economic policies such as tariffs to obstruct imports and protect Japanese firms from foreign competitors in domestic market to become a modernised country.\n\nTurkey, under Kemal Atatürk in the 1920s and 1930s, engaged in a systematic modernisation programme called \"Kemalism\". Hundreds of European scholars came to help. Together with Turkish intellectuals they developed a successful model of development.\n\nSince independence, modernisation has been a driving force for Chile's political elites. Ree (2007) analyses projects of modernisation that have been implemented from above since 1964. Despite their ideological differences and very different understandings of what modernity is, these projects shared key characteristics in their construction and implementation, such as the use of developmental theories, their state-orientation, the prominent role of technocrats and state-planning, and the capacity of adaptation in sight of civil unrest. These projects have produced patterns of modernity that have proven particularly stable.\n\nModernisation has been attributed with creating positive development around the world, but in Modern times its ability to promote development, specifically in Africa, has been less than so. Modernisation that has taken place in Africa can be described as something that has yet to benefit most of the African countries.\n\nModernisation through development has led to problems in Nigeria by bringing in private, foreign owned oil companies that have been exploiting the natural resource wealth of the country. Because the oil companies are generally owned by a different nation, the profits are mostly being exported from Nigeria with only one fifteenth of the wealth produced in the region returning to it. Shell, the oil company operating in Ogoniland, Nigeria has helped the country develop and industrialise on a small scale, but it has primarily challenged the sovereignty and autonomy of Nigeria.\n\nA lot of scholars view modernisation as a sort of westernisation where western institutions such as national parks and industries are brought into existing cultures where their use does not make as much sense. Along with modernisation comes a loss of culture and society, and the individual is strengthened. An African tribe known as the Ik was forced to change their habits due to modernisation and the creation of individual countries caused by colonialism. Nationalisation, as a tool of modernisation, was imparted on Africa by colonialists who wanted to westernise and modernise tribal Africa. The creation of individual countries made life for the tribal Ik more difficult because they were forced out of their nomadic lifestyle into a settlement based around a newly founded national park that practically destroyed their livelihood by restricting their hunting grounds to specific non-park areas. The creation of national parks have increased cultivation, which can be seen as good development because people no longer depend solely on livestock. This creation of a new sort of livelihood has mixed improvements, because the tribal setting is not removed, but is put into a single place.\n\n"}
{"id": "40338559", "url": "https://en.wikipedia.org/wiki?curid=40338559", "title": "Hybrid algorithm", "text": "Hybrid algorithm\n\nA hybrid algorithm is an algorithm that combines two or more other algorithms that solve the same problem, either choosing one (depending on the data), or switching between them over the course of the algorithm. This is generally done to combine desired features of each, so that the overall algorithm is better than the individual components.\n\n\"Hybrid algorithm\" does not refer to simply combining multiple algorithms to solve a different problem – many algorithms can be considered as combinations of simpler pieces – but only to combining algorithms that solve the same problem, but differ in other characteristics, notably performance.\n\nIn computer science, hybrid algorithms are very common in optimized real-world implementations of recursive algorithms, particularly implementations of \ndivide and conquer or decrease and conquer algorithms, where the size of the data decreases as one moves deeper in the recursion. In this case, one algorithm is used for the overall approach (on large data), but deep in the recursion, it switches to a different algorithm, which is more efficient on small data. A common example is in sorting algorithms, where the insertion sort, which is inefficient on large data, but very efficient on small data (say, five to ten elements), is used as the final step, after primarily applying another algorithm, such as merge sort or quicksort. Merge sort and quicksort are asymptotically optimal on large data, but the overhead becomes significant if applying them to small data, hence the use of a different algorithm at the end of the recursion. A highly optimized hybrid sorting algorithm is Timsort, which combines merge sort, insertion sort, together with additional logic (including binary search) in the merging logic.\n\nA general procedure for a simple hybrid recursive algorithm is \"short-circuiting the base case,\" also known as \"arm's-length recursion.\" In this case whether the next step will result in the base case is checked before the function call, avoiding an unnecessary function call. For example, in a tree, rather than recursing to a child node and then checking if it is null, checking null before recursing. This is useful for efficiency when the algorithm usually encounters the base case many times, as in many tree algorithms, but is otherwise considered poor style, particularly in academia, due to the added complexity.\n\nAnother example of hybrid algorithms for performance reasons are introsort and introselect, which combine one algorithm for fast average performance, falling back on another algorithm to ensure (asymptotically) optimal worst-case performance. Introsort begins with a quicksort, but switches to a heap sort if quicksort is not progressing well; analogously introselect begins with quickselect, but switches to median of medians if quickselect is not progressing well.\n\nCentralized distributed algorithms can often be considered as hybrid algorithms, consisting of an individual algorithm (run on each distributed processor), and a combining algorithm (run on a centralized distributor) – these correspond respectively to running the entire algorithm on one processor, or running the entire computation on the distributor, combining trivial results (a one-element data set from each processor). A basic example of these algorithms are distribution sorts, particularly used for external sorting, which divide the data into separate subsets, sort the subsets, and then combine the subsets into totally sorted data; examples include bucket sort and flashsort.\n\nHowever, in general distributed algorithms need not be hybrid algorithms, as individual algorithms or combining or communication algorithms may be solving different problems. For example, in models such as MapReduce, the Map and Reduce step solve different problems, and are combined to solve a different, third problem.\n\n"}
{"id": "1968588", "url": "https://en.wikipedia.org/wiki?curid=1968588", "title": "Hypervelocity", "text": "Hypervelocity\n\nHypervelocity is very high velocity, approximately over 3,000 meters per second (6,700 mph, 11,000 km/h, 10,000 ft/s, or Mach 8.8). In particular, hypervelocity is velocity so high that the strength of materials upon impact is very small compared to inertial stresses. Thus, even metals behave like fluids under hypervelocity impact. Extreme hypervelocity results in vaporization of the impactor and target. For structural metals, hypervelocity is generally considered to be over 2,500 m/s (5,600 mph, 9,000 km/h, 8,200 ft/s, or Mach 7.3). Meteorite craters are also examples of hypervelocity impacts.\n\n\"Hypervelocity\" refers to velocities in the range from a few kilometers per second to some tens of kilometers per second. This is especially relevant in the field of space exploration and military use of space, where hypervelocity impacts (e.g. by space debris or an attacking projectile) can result in anything from minor component degradation to the complete destruction of a spacecraft or missile. The impactor, as well as the surface it hits, can undergo temporary liquefaction. The impact process can generate plasma discharges, which can interfere with spacecraft electronics.\n\nHypervelocity usually occurs during meteor showers and deep space reentries, as carried out during the Zond, Apollo and Luna programs. Given the intrinsic unpredictability of the timing and trajectories of meteors, space capsules are prime data gathering opportunities for the study of thermal protection materials at hypervelocity (in this context, hypervelocity is defined as greater than escape velocity). Given the rarity of such observation opportunities since the 1970s, the Genesis and Stardust Sample Return Capsule (SRC) reentries as well as the recent Hayabusa SRC reentry have spawned observation campaigns, most notably at NASA Ames Research Center.\n\nHypervelocity collisions can be studied by examining the results of naturally occurring collisions (between micrometeorites and spacecraft, or between meteorites and planetary bodies), or they may be performed in laboratories. Currently the primary tool for laboratory experiments is a light-gas gun, but some experiments have used linear motors to accelerate projectiles to hypervelocity. The properties of metals under hypervelocity have been integrated with weapons, such as explosively formed penetrator. The vaporization upon impact and liquefaction of surfaces allow metal projectiles formed under hypervelocity forces to penetrate vehicle armor better than conventional bullets.\n\nNASA studies the effects of simulated orbital debris at the White Sands Test Facility Remote Hypervelocity Test Laboratory (RHTL). Objects smaller than a softball cannot be detected on radar. This has prompted spacecraft designers to develop shields to protect spacecraft from unavoidable collisions. At RHTL, micrometeoroid and orbital debris (MMOD) impacts are simulated on spacecraft components and shields allowing designers to test threats posed by the growing orbital debris environment and evolve shield technology to stay one step ahead. At RHTL, four two-stage light-gas guns propel 0.05 mm to 22.2 mm diameter projectiles to velocities as fast as 8.5 km/s.\n\nAccording to the United States Army, \"hypervelocity\" can also refer to the muzzle velocity of a weapon system, with the exact definition dependent upon the weapon in question. When discussing small arms a muzzle velocity of 5,000 ft/s (1524 m/s) or greater is considered hypervelocity, while for tank cannons the muzzle velocity must meet or exceed 3,350 ft/s (1021.08 m/s) to be considered hypervelocity, and the threshold for artillery cannons is 3,500 ft/s (1066.8 m/s).\n\n"}
{"id": "10271359", "url": "https://en.wikipedia.org/wiki?curid=10271359", "title": "Implicit cognition", "text": "Implicit cognition\n\nImplicit cognition refers to unconscious influences such as knowledge, perception, or memory, that influence a person's behavior, even though they themselves have no conscious awareness whatsoever of those influences.\n\nImplicit cognition is everything one does and learns unconsciously or without any awareness that one is doing it. An example of implicit cognition could be when a person first learns to ride a bike: at first they are aware that they are learning the required skills. After having stopped for many years, when the person starts to ride the bike again they do not have to relearn the motor skills required, as their implicit knowledge of the motor skills takes over and they can just start riding the bike as if they had never stopped. In other words, they do not have to think about the actions that they are performing in order to ride the bike. It can be seen with this example that implicit cognition is involved with many of the different mental activities and everyday situations of people's daily lives. There are many processes in which implicit memory works, which include learning, our social cognition, and our problem solving skills.\n\nImplicit cognition was first discovered in the year of 1649 by Descartes in his \"Passions of the Soul\". He said in one of his writings that he saw that unpleasant childhood experiences remain imprinted in a child's brain until its death without any conscious memory of it remaining. Even though this idea was never accepted by any of his peers, in 1704 Gottfried Wilhelm Leibniz in his \"New Essays Concerning Human Understanding\" stressed the importance of unconscious perceptions which he said were the ideas that we are not consciously aware of yet still influence people's behavior. He claimed that people have residual effects of prior impressions without any remembrance of them. In 1802 French philosopher Maine de Biran in his \"The Influence of Habit on the Faculty of Thinking\" was the first person after Leibniz to systematically discuss implicit memory stating that after enough repetition, a habit can become automatic or completed without any conscious awareness. In 1870 Ewald Hering said that it was essential to consider unconscious memory, which is involved in involuntary recall, and the development of automatic and unconscious habitual actions.\n\nImplicit learning starts in our early childhood, this means that people are not able to learn the proper grammar and rules to speaking a language until the age of seven. So if this is the case then how do we learn to talk by the age of four? One of the ways that this is possible is through implicit learning and association. Children learn their first language from what they hear when they are listening to the adults and through their own talking activities. This goes to show that the way children learn language involves implicit learning.\n\nA study was conducted with amnesiac patients in an attempt to demonstrate that amnesiac patients that were unable to learn a list of words or pictures when their performance was tested were able to complete or put together fragmented words and incomplete pictures. This was found to be true as the patients were able to perform better when asked to complete words or pictures. A possible explanation for this could be that implicit memory should be less susceptible to damage that may happen to the brain than explicit memory. There was a case where a 54-year-old man that had bitemporal damage worse on the right side had a hard time remembering things from his own life as well as famous events, names and even; yet he was able to perform within the normal limits with a word completion task involving famous names and with judgments of famous faces. This is a prime example that implicit memory can be less vulnerable to brain damage.\n\nA famous study investigated the Identification blindsight effect with individuals who had suffered damage to one half of the visual cortex and were blind in the opposite half of the visual field. It was discovered that when objects or pictures were shown to these blind areas, the participants said that they saw no object or picture, but a certain number were able to identify the stimulus as either a cross or circle when asked to guess at a considerably higher rate than would have been expected by chance. The reason that this happened is because the information was able to be process through the first three stages of selection, organization, and interpretation or comprehension of the perceptual cycle but failed at only the last stage of retention and memory where the identified image is entered into their awareness. Thus stimuli can enter implicit memory even when people are unable to consciously perceive them.\n\nImplicit cognition also plays a role in social cognition. People tend to see objects and individuals as more encouraging or acceptable the more often that people are exposed to them. An example includes the False-fame Effect. Graf and Masson (1993) conducted a study where they showed participants a list with both famous and non-famous names. When it was shown around people were able to recall the famous names more than the non-famous names initially, but after about a 24-hour delay participants began to associate the non-famous names with famous people. This supports implicit cognition because the participants began to unconsciously associate the non-famous names with famous people.\n\nAlthough the process is unconscious, implicit cognition influences how people view each other as well as their interactions with one another. People tend to view those who look alike as belonging together or to similar groups and associate them with the social groups that existed in their high school years. These groups represented different relations between the students and were made up of students who were perceived as having similarities among each other. \nA study was conducted to see the amount of distance that participants put between individuals given certain circumstances. The participants were asked to place figures of individuals where they thought the figures should be standing given certain circumstances. It was found that people typically place men and women close to each other, to make little families formed with the figures of a woman, a man and of children. The participants did the same when asked to show friends and/or acquaintances, the two figures were placed relatively close to one another rather than if they were asked to represent strangers. When asked to represent strangers the participants placed the figures far apart. There are two parts to the social relations view that is liking relations were the ultimate goal is to be together, then there is the disliking relation view which is separation from the person. An example of this could be when someone is walking down a hall way and see someone whom they know and like that person is more likely to wave and say hello to them. On the other hand, say the person they see is someone whom they dislike, their response will be the opposite as they try to either avoid them or get away from them as quickly as possible showing the separation between the two of them. There are two views to the social relations theory, one of them is that people are out to mainly seek dominance of those around them, while the other view is that people mainly see the relations as either belonging or not belonging or liking and disliking on another. It is seen that males mainly seek dominance against one another as they are competitive and looking to outdo one another. For females on the other hand it is seen that women perceive their social views and values as more of the belonging or liking scale in terms of their closeness to one another. Implicit cognition not only involved how people view each other but also how they view themselves. This means that our own image is constructed from what others see of us rather than our own views. The way that we view ourselves is from what others see us as, or from the times that we compare ourselves to other people. The way that this plays a role in implicit cognition is because all of these actions people do unconsciously, or they are unaware that they are making these decision. Men do not consciously seek to be dominant over one another as women do not consciously arrange their social views or values in terms of their closeness. These are each things that people do without their conscious knowledge of these actions, which ties in with implicit cognition.\n\nThere are scenarios when we act on something and then think back about handling it in different situation or manner. That is implicit cognition coming into play, the mind will then go based off ethical and similar situations when interacting with a certain thought. Implicit cognition and its automated thought process allows a person to decide something out of impulse. It is often defined as an involuntary process where tasks are easily absent of consciousness. There are plenty of factors that influence behaviors and thought processes. Such as social learning, to stigmas, and two major aspects of implicit and explicit cognition. Implicit on one hand is obtained through social aspects and association, while explicit cognition is gained through propositional attitudes or beliefs of certain thoughts, Implicit cognition can be incorporated with a mixture of attention, goals, self-association, and at times even motivational processes. Researchers have used different methods to test these theories of behavior correlation with implicit cognition. Using Implicit Association Tests (ITA's) is a method that is significantly used, according to Fazio & Olsen (2003) and Richetin & Richardson (2008). Since published, approximately ten years or so, it has been widely used influencing research on implicit attitudes. Implicit cognition is a process based off automatic mental interpretations. It's what a person really thinks, yet is not consciously aware of. Behavior is then affected, usually causing negative influences, both theoretically and empirical reasons presume that automatic cognitive processes are contributed to aggressive behaviors.\n\nImpulse behaviors are often created without awareness. Negativity is a characteristic of implicit cognition, since it is an automated response. Explicit cognition is rarely used when trying to discover behavior of ones thought process. Researchers again use IAT's to determine ones thoughts and how a person incorporates these automatic processes, findings consider that implicit cognition may direct what behaviors a person may choose when facing extreme stimuli. For example, death can be perceived as positive, negative, or a combination of the two. Depending on the attributes of death, it can include a general perspective or a \"me\" attribute. implied that implicit association with death and or suicide initiates a final process when deciding how to cope to these extreme measures. Self- harm is another characteristic associated with implicit cognition. Because although we may think of it, it is controlled subconsciously. IAT's showed that there was a stronger correlation of implicit cognition and death/suicide than self-harm. The idea of pain may influence a person to think twice, while suicidal may seem quick, thus the automatic process can show how effective this negative behavior and implicit cognition come hand in hand. Automated processes doesn't allow a person to thoroughly create a conscious choice, therefore creating negative influence to behavior. Another negative behavior that can be associated with implicit cognition is depression. Whether a person takes a positive or negative outlook on the certain situation can produce if a person will be associated with depression. It is easier to determine an implicit mindset simple because it is outside of awareness. Implicit processes are considered critical when determining a person's reactions to a certain schema. Implicit cognition is often immediately affective towards a person's reaction. Implicit cognitions also consisted of negative schemas that included hidden cognitive frameworks, and activation of stress. Awareness was often misinterpreted and implicit cognition emerged because of these negative schemas. Behaviors merged through implicit cognition involve a variety of addictive behaviors, problematic thinking, depression, aggression, suicide, death, and other negative factors. Certain life situations add to this schema, whether it be stressful situations, sudden, or anything along these lines, aspects of implicit cognition are used and evaluated.\n\nImplicit cognition can also be associated with mental illness and the way thoughts are processed. Automatic stigmas and attitudes may anticipate other cognitive and behavior tendencies. A person with mental illness may be correlated with a guilt-related, self-associated personality. Because of these associations it may be managed outside one's own control and awareness, showing how implicit cognition is affected. However a dual process can be assessed within implicit and explicit cognition. An agreement between the two thought processes may be an issue, explicit may not be in contact with implicit, therefore causing more of a problem. Mental illness can include both implicit and explicit attitudes, however implicit self-concepts gave more negative consequence when dealing with mental illness. Much of implicit problems happened to be associated with alcohol, however this isn't the goal in order to describe a mental process and implicit cognition. The most widely influenced mental illness in association with implicit cognition would be Schizophrenia. Since a person of this illness has a problem of detecting what is real and what is not, implicit memory is often used with these patients. However, since it cannot really be detected if it is emotionally, mentally, or a combination of both some aspects of this illness are usually exercised uninterrupted, and unconsciously. Since schizophrenia is widely varied and has different characteristics, we cannot quite measure the outcome of implicit cognition.\n\nImplicit cognition refers to perceptual, memory, comprehension, and performance processes that occur through unconscious awareness. For example, when a patient is discharged after a surgery, the effects of the anesthesia can cause abnormal behaviors without any conscious awareness. According to Wiers et al. (2006) some scholars argue implicit cognition is misinterpreted and could be used to improve behaviors while others highlight the dangers of it. Research studies have shown implicit cognition is a strong predictor for several issues like substance abuse, misconduct, and mental disorders. These inherent thoughts are influenced from early adolescent experiences primarily a negative impact from culture. Adolescents who experience a rough childhood early on develop low levels of self-esteem. Therefore, the cognition to act dangerously is an oblivious development. Research for implicit cognition has started to grow especially within mental disorders.\n\nSchemas are used to interpret the functions involved when individuals would make sense of their surroundings. This cognition happens through an explicit process of recalling an item routinely or implicit process that is outside conscious awareness control. A recent study suggests individuals who have experienced a difficult upbringing develop schemata of fear as well as anxiety and will react almost immediately when they feel threatened. People who are anxious predominantly focus on any peril-related stimuli since they are hyper vigilant. For example, an anxious individual who is about to cross the street at the same time a car drives to a stop sign. The anxious person will automatically assume the driver will not stop. This is recognition of threat through a semantic process that instantaneously occurs. Ambiguous cues are viewed as a threat since there is no relevant knowledge to make sense of. People will have a difficult time to understand and will respond negatively. This kind of behavior can explain how implicit cognition may be an influence for pathological anxiety.\n\nThe ideas of psychotic patients who have low self-esteem are prone to more serious illnesses. This concept was examined through both implicit and explicit perspectives by measuring the self-esteem of paranoia and depression patients. Previous research suggests that negative implicit cognition is not the symptoms for depression and paranoia, but it is an antecedent for the onset. Current research proposes that high implicit self-esteem is linked to less paranoia. It is imperative for patients who have low self-esteem to become more overt about these situations. Another study found a substantial association between adverse self-questioning in implicit cognition and depression. People who do not think highly of themselves are more likely to be depressed because of this involuntary implicit learning.\n\nImplicit cognition is another influential predictor for bipolar disorder and unipolar disorder. Research proposes patients with bipolar disorder show more common implicit and depressive self-referencing than unipolar patients. Implicit cognition plays a strong role for patients with both bipolar and unipolar disorder. These patients have dysfunctional self-schemata, which is viewed as vulnerability for potential illnesses. Patients who have this vulnerability usually do not seek mental assistance which can later become more problematic to treat. Bipolar disorder patients with low implicit self-esteem are more defensive. This is an unconscious reaction to be manic protective when they feel threatened in any way.\nSince the growing research of implicit cognition is associated with abnormalities, researchers attempted to find a connection between implicit neuroticism and schizophrenia. Indeed, there was a correlation; participants with schizophrenia were high in implicit neuroticism and low in implicit extraversion when compared to people who were mentally healthy. Participants were given questionnaires that ask personality questions such as \"I enjoy being the center of attention\". Implicit cognition constitutes low levels of extraversion because these participants are known to avoid any coping. Schizophrenia patients and healthy individuals differ in associative representation pertaining to themselves in neuroticism features. People who are schizophrenic develop an implicit learning, meaning they have an error free learning style so they never take feedback from anyone else.\n\nResearch on suicide can be a difficult process because suicidal patients are commonly covert about their intentions to avoid hospitalization. An implicit cognition associated self-task was applied in one experiment to unveil any suspicious behavior of people who might attempt suicide. This study found patients who were released from mental hospitals showed significant implicit association to attempt suicide. The Implicit Association Task would predict whether a patient was likely to attempt suicide depending on they respond. An individual's implicit cognition may lead to a behavior to best cope with stress. This behavior may be suicide, substance abuse, or even violence. However, implicit association with death will show those most at risk for attempting suicide because this individual is looking for the best solution for ending their stress.\n\nImplicit cognition is measured in different ways to find the most accurate outcomes. The task used for patients with anxiety disorders was a modified Stroop task to observe attention biases in anxiety. A participant's reaction time was measured by the reported color of each word. Participants would name the color for each risk relevant words and risk irrelevant words and depending on the color of each word might slow the reaction time. Colors like red were used to see if anxious participants would have a slower reaction time. These colors are known as aposematic implying a threat warning. Most studies used the Implicit Association Test which varies for each study. For example, implicit self-esteem can be tested by giving participants questionnaires that are self-referent. These ask questions like \"I am known to be suicidal\". Depending on the response of the participants, the researcher can assessed the current state for each participant. Patients that were rated high in suicide immediately received psychiatric treatment. According to these experiments implicit cognition may be a strong predictor for mental disorders.\n\n"}
{"id": "57750049", "url": "https://en.wikipedia.org/wiki?curid=57750049", "title": "Induction-induction", "text": "Induction-induction\n\nIn intuitionistic type theory (ITT), some discipline within mathematical logic, induction-induction is for simultaneously declaring some inductive type and some inductive predicate over this type.\n\nAn inductive definition is given by rules for generating elements of some type. One can then define some predicate on that type by providing constructors for forming the elements of the predicate , such inductively on the way the elements of the type are generated. Induction-induction generalizes this situation since one can \"simultaneously\" define the type and the predicate, because the rules for generating elements of the type formula_1 are allowed to refer to the predicate formula_2.\n\nInduction-induction can be used to define larger types including various universe constructions in type theory. and limit constructions in category/topos theory\n\nPresent the type formula_3 as having the following constructors , note the early reference to the predicate formula_4 :\n\n\nand-simultaneously present the predicate formula_4 as having the following constructors :\n\n\nAs you can see, we gave rules for generating the elements of formula_4 which amount to saying that formula_18 is (isomorphic to) the booleans, and formula_19 is (isomorphic to) the natural numbers.\n\nA simple common example is the Universe à la Tarski type former. It creates some inductive type formula_20 and some inductive predicate formula_21. For every type in the type theory (except formula_22 itself!), there will be some element of formula_22 which may be seen as some code for this corresponding type ; The predicate formula_24 inductively encodes each possible type to the corresponding element of formula_22 ; and constructing new codes in formula_22 will require referring to the decoding-as-type of earlier codes , via the predicate formula_24 .\n\n\n"}
{"id": "48710732", "url": "https://en.wikipedia.org/wiki?curid=48710732", "title": "Julieta Aranda", "text": "Julieta Aranda\n\nJulieta Aranda (born in 1975 in Mexico City, Mexico) is a conceptual artist that lives and works in Berlin and New York City. She received a BFA in filmmaking from the School of Visual Arts (2001) and an MFA from Columbia University (2006), both in New York. Her explorations span installation, video, and print media, with a special interest in the creation and manipulation of artistic exchange and the subversion of traditional notions of commerce through art making.\n\nAranda's complex body of work exists outside the boundaries of the object, and is characterized by the struggle of catching sight of elusive concepts such as time, circulation, and imagination. Her installations and temporary projects, which often examine social interactions and the role that the circulation of objects plays in the cycles of production and consumption, are intensely site-specific. Much of her work takes up the concept of time, sometimes to consider alternative notions of the temporal experience, and other times to approach the arbitrariness of time and freedom from time.\n\nIn \"You had no ninth of May!\" (2006), Aranda addresses the artificiality of the homogeneous construction of time through the case of Kirbati, an archipelago in the Pacific that, in 1995, changed the position of the International Date Line (IDL). Through a series of installation pieces that conceptually and formally map the international date line at Kiribati, the artist investigates officially assigned time and calls into question concepts such as \"today\" or \"tomorrow\".\n\nAranda's 2007 work \"There has been a miscalculation (Flattened Ammunition)\" is an experiment on the functioning of time. This work consists in a transparent Plexiglas cube containing approximately 100 science-fiction novels with a story line taking place before 2007 (the year in which the work was first produced), which have been shredded, almost pulverized. It also contains a hidden computerized air compressor that unexpectedly and violently blows the dust around at random intervals, recalling a sudden sandstorm. This way, Aranda's work makes books endlessly circulate and swirl in an empty cube, leaving them incessantly suspended in a past future.\n\nFor \"Intervals\" (2009), a solo presentation of four works installed in the Solomon R. Guggenheim Museum, Aranda explored and inverted the notion of time as a strictly assigned linear designation marked by clocks and calendars. In this exhibition, all the works were proposed to partially describe, in the artist's words \"a sense of time's passage according to subjective experience, rather than subscribed to a strict system of measurement that assigns a fixed duration to any given event\". Each piece captures time's passage in an individualized sense, addressing what the artist conceives as \"subject formation\" and the assertion of one's dominion over one's own time as a condition for individualism.\n\nJulieta Aranda has been actively collaborating on e-flux since 2003, which is a publishing platform, archive, artist project, curatorial platform, and cultural enterprise founded by Anton Vidokle in 1998. Aranda is both a contributor and editor of e-flux journal, and in collaboration with Vidokle, has produced several e-flux art projects that explore unusual models for the circulation and distribution of art.\n\nConceived in 2004 in collaboration with Anton Vidokle, e-flux video rental (EVR) comprises a free VHS video rental store, a public screening room and an archive. Its collection is selected in collaboration with a large group of international curators, and consists of over 500 art films and video works that are available to the public for home viewing free of charge. The project was originally presented in a storefront in New York, and has been presented at various locations around the world, with the inventory of videos continuously increasing. After seven years as a traveling project, EVR was donated to the Moderna Galerija in Ljubljana in 2011 for its permanent display, which is a reconstruction of the original storefront at 53 Ludlow Street, New York. As a video rental store entering a museum collection, EVR will preserve and make available for future study not only the videos that comprise it, but also the social form of video rental stores, and the technology that originally made it possible.\n\nReleased in 2007, \"Pawnshop\" is an e-flux project by artists Liz Linden, Julieta Aranda, and Anton Vidokle. Both an exhibition and an artwork in itself, this project was originally located in e-flux's 53 Ludlow Street storefront, which temporarily became a pawnshop dedicated to the pawning of artworks. Its initial inventory consisted of over 60 pawned works from a group of artists invited to participate in the project, and after it was opened for business, further artists were able walk in with a work they wanted to pawn. After the initial 30 days, the artworks that have not been retrieved by their original owners became available for sale.\n\nAn online platform initiated by Aranda and Vidokle, \"Time/Bank\" is based on the premise that everyone in the field of culture has something to contribute and that it is possible to develop and sustain an alternative economy by connecting existing needs with unacknowledged resources. On a practical level, it is a platform where artists, curators, writers and other people in the field, can exchange time and skills—help each other get things done without using money. Idealistically, \"Time/Bank\" can become a place where certain types of actions and ideas, that seem to have no value in our market-driven society, can gain a sense of worth.\n\nIt is possible to open a time bank account at http://www.e-flux.com/timebank/user/register\n\nSUPERCOMMUNITY is an editorial project by e-flux journal commissioned for the 56th Venice Biennale, which was run from May to August 2015. Julieta Aranda, Anton Vidokle, and Brian Kuan Wood have been co-editors of this project, which addresses e-flux journal and its readership as the supercommunity and presents a daily piece of writing that often adopts the form of poetry, short fiction or screenplay. It has featured contributions from nearly one hundred authors such as anthropologists, artists, philosophers, poets, theorists and writers.\n\nAranda has been awarded numerous grants and merit scholarships, from institutions such as FONCA, the National Foundation for the Culture and the Arts in Mexico (1995–1996), and both the School of Visual Arts (1995–1999), the National Board of Review (1996–1999) and Columbia University (2004) in New York. She has also been an artist in residence at UNIDEE, the International Program by Fondazione Pistoletto in Biella, Italy (2006), as well as at IAPSIS, the International Artists Studio Program in Stockholm (2006) and at the International Residence of Recollets in Paris (2008). Her work has been shown in internationally renowned institutions such as the Museum of Contemporary Art, North Miami (2009); the Solomon R. Guggenheim Museum, New York (2009); the National Museum of Art, Architecture and Design, Oslo (2010); and Museo de Arte Contemporáneo de Castilla y León, Spain (2010), as well as at international art festivals such as the Liverpool Biennial (2010); the Kassel Documenta, Germany (2012); and the Shanghai Biennale (2012).\n"}
{"id": "32577539", "url": "https://en.wikipedia.org/wiki?curid=32577539", "title": "Kiki Bokassa", "text": "Kiki Bokassa\n\nPrincess Kiki Bokassa (born 1975, Paris, France) is an autodidact conceptual artist, who works in the expressionist, figurative art genre. She has paintings in private collections in the Persian Gulf and USA. She was brought up in Lebanon and has exhibited widely in Beirut and overseas.\n\nIn April 2009, Bokassa created an immersive art event in Beirut entitled ‘72 hrs’, in which she painted for 72 hours continuously as a peaceful form of expression in self-imposed incarceration. The work took place in a giant canvas cube at Laboratoire d'Art. The event came to the attention of more than 30 international media outlets and was reported in at least 74 countries.\n\nShe is the daughter of Emperor of the Central African Empire Jean-Bédel Bokassa.\n"}
{"id": "44564297", "url": "https://en.wikipedia.org/wiki?curid=44564297", "title": "Law without the state", "text": "Law without the state\n\nLaw without the state (also called transnational stateless law, stateless law, or private legal orderings) is law made primarily outside of the power of a state. \n\nSuch law may be established in several ways:\n\n\n\n\n"}
{"id": "33033852", "url": "https://en.wikipedia.org/wiki?curid=33033852", "title": "List of English-language metaphors", "text": "List of English-language metaphors\n\nA list of metaphors in the English language organised by type. A metaphor is a literary figure of speech that uses an image, story or tangible thing to represent a less tangible thing or some intangible quality or idea; e.g., \"Her eyes were glistening jewels\". \"Metaphor\" may also be used for any rhetorical figures of speech that achieve their effects via association, comparison or resemblance. In this broader sense, antithesis, hyperbole, metonymy and simile would all be considered types of metaphor. Aristotle used both this sense and the regular, current sense above.\nWith metaphor, unlike analogy, specific interpretations are not given explicitly.\n\n\n\n\n\n\n\nRichard Honeck described three forms of scientific metaphors: \"mixed scientific metaphor, the scientific metaphor theme, and the scientific metaphor that redefines a concept from a theory.\"\n\n\n\n\n\n\n"}
{"id": "25494798", "url": "https://en.wikipedia.org/wiki?curid=25494798", "title": "Magnetorquer", "text": "Magnetorquer\n\nA magnetorquer or magnetic torquer (also known as torque rod) is a satellite system for attitude control, detumbling, and stabilization built from electromagnetic coils. The magnetorquer creates a magnetic dipole that interfaces with an ambient magnetic field, usually Earth's, so that the counter-forces produced provide useful torque.\n\nMagnetorquers are essentially sets of electromagnets which are laid out to yield a rotationally asymmetric (anisotropic) magnetic field over an extended area. That field is controlled by switching current flow through the coils on or off, usually under computerized feedback control. The magnets themselves are mechanically anchored to the craft, so that any magnetic force they exert on the surrounding magnetic field will lead to a magnetic reverse force and result in mechanical torque about the vessel's center of gravity. This makes it possible to freely pivot the craft around in a known local gradient of the magnetic field by only using electrical energy.\nThe magnetic dipole generated by the \"magnetorquer\" is expressed by the formula:\n\nformula_1\n\nwhere \"n\" is the number of turns of the wire, \"I\" is the current provided, and A is the vector area of the coil. \nThe dipole interacts with the magnetic field generating a torque whose expression is:\n\nformula_2\n\nwhere m is the magnetic dipole vector, B the magnetic field vector (for a spacecraft it is the Earth magnetic field vector) and τ is the generated torque vector.\n\nThe construction of a \"magnetorquer\" is based on the realization of a coil with a defined area and number of turns according to the required performances. However, there are different ways to obtain the coil, thus according to the construction strategy it is possible to find three type of magnetorquer, apparently very different from each other but based on the same concept : \n\nTypically three coils are used, although reduced configurations of two or even one magnet can suffice where full attitude control is not needed or external forces like asymmetric drag allow underactuated control. The three coil assembly usually takes the form of three perpendicular coils, because this setup equalizes the rotational symmetry of the fields which can be generated; no matter how the external field and the craft are placed with respect to each other, approximately the same torque can always be generated simply by using different amounts of current on the three different coils.\n\nAs long as current is passing through the coils and the spacecraft has not yet been stabilized in a fixed orientation with respect to the external field, the craft's spinning will continue.\n\nVery small satellites may use permanent magnets instead of coils.\n\nMagnetorquers are lightweight, reliable, and energy-efficient. Unlike thrusters, they do not require expendable propellant either, so they could in theory work indefinitely as long as sufficient power is available to match the resistive load of the coils. In Earth orbit, sunlight is one such practically inexhaustible energy source, using solar panels.\n\nA further advantage over momentum wheels and control moment gyroscopes is the absence of moving parts and therefore significantly higher reliability.\n\nThe main disadvantage of magnetorquers is that very high magnetic flux densities are needed if large craft have to be turned very fast. This either necessitates a very high current in the coils, or much higher ambient flux densities than are available in Earth orbit. Subsequently, the torques provided are very limited and only serve to accelerate or decelerate the change in a spacecraft's attitude by minute amounts. Over time active control can produce very fast spinning even here, but for accurate attitude control and stabilization the torques provided often aren't enough.\n\nA broader disadvantage is the dependence on Earth's magnetic field strength, making this approach unsuitable for deep space missions, and also more suitable for low Earth orbits as opposed to higher ones like the geosynchronous. The dependence on the highly variable intensity of Earth's magnetic field is also problematic because then the attitude control problem becomes highly nonlinear. It is also impossible to control attitude in all three axes even if the full three coils are used, because the torque can be generated only perpendicular to the Earth's magnetic field vector.\n\nAny spinning satellite made of a conductive material will lose rotational momentum in Earth's magnetic field due to generation of eddy currents in its body and the corresponding braking force proportional to its spin rate. Aerodynamic friction losses can also play a part. This means that the magnetorquer will have to be continuously operated, and at a power level which is enough to counter the resistive forces present. This is not always possible within the energy constraints of the vessel.\n\nThe Michigan Exploration Laboratory (MXL) suspects that the M-Cubed CubeSat, a joint project run by MXL and JPL, became magnetically conjoined to Explorer-1 Prime, a second CubeSat released at the same time, via strong onboard magnets used for passive attitude control, after deploying on October 28, 2011.\nThis is the first non-destructive latching of two satellites.\n\n"}
{"id": "1022109", "url": "https://en.wikipedia.org/wiki?curid=1022109", "title": "McIntyre System", "text": "McIntyre System\n\nThe McIntyre System, or systems as there have been five of them, is a playoff system that gives an advantage to teams or competitors qualifying higher. The systems were developed by Ken McIntyre, an Australian lawyer, historian and English lecturer, for the Victorian Football League in 1931.\n\nThe first McIntyre System, the Page–McIntyre system, also known as the McIntyre Final Four System, was adopted by the VFL in 1931, after using three systems since its foundation in 1897, the major system and predecessor to the Page–McIntyre system being the \"amended \"Argus\" system\" that had operated from 1907 to 1923 and 1925 to 1930.\n\nMcIntyre also devised the McIntyre Final Five System for the VFL for 1972, the McIntyre Final Six System for 1991 (which was revised for 1992) and the McIntyre Final Eight System for the 1994 season.\n\nThe AFL and its fans grew dissatisfied with some of the outcomes the McIntyre Final Eight system might allow, and replaced it with another final eight system in 2000.\n\nMcIntyre finals systems are used prominently throughout Australia. Most Australian rules football leagues, from professional down to suburban, use a McIntyre finals system. The New South Wales Rugby League/National Rugby League has used the McIntyre Final Four and Final Five at different times throughout its history, and used the McIntyre Final Eight System from 1999 until 2011. The Page–McIntyre system is also used in the ANZ Championships (netball), the Australian Baseball League and Women's National Basketball League. It was also used in the A-League (soccer) before that competition expanded its finals series to a top-six format. It is also used in the Indian Premier League (cricket).\n\nUnder the name Page playoff system, the McIntyre Final Four is commonly used in softball and curling events, especially in Canada. The system was also used in the Rugby League National League Three in Great Britain for the 2004 season.\n\nThe Page–McIntyre system feature four teams. In the first round of the Page–McIntyre system, the highest two ranked teams play each other, with the winner going straight through to the grand final and the loser going through to the preliminary final. The lowest two ranked teams play each other, and the winner advances to the preliminary final. The winner of preliminary final gets through to the grand final. In this system, the top two teams are able to lose a match and still qualify for the Grand Final, this is referred to as a 'double chance'.\n\nAssuming that each team has an even chance of winning each match, the probability for both the highest ranked teams winning the competition is 37.5%, compared to 12.5% for the third and fourth placed teams.\n\nAs its name states, the McIntyre final five system features five teams. From the second round the McIntyre final five system is the same as the Page–McIntyre system, however, in the first round the lowest two ranked teams play to eliminate one team and the second and third ranked teams determine which match they will play in the second round. The highest ranked team has a bye in the first round.\n\nIn this case, if all teams have an even chance of winning each match, the highest ranked team has a 37.5% chance, ranks two and three have a 25% chance and the lowest two ranked teams have a 6.25% chance of winning the competition.\n\nThe first McIntyre final six system was also the same as the Page–McIntyre system from the second round. In this case, two of the four lowest ranked teams are eliminated in the first round, while the top two determine which match they will play in the second round. Under this system the top two teams receive a double chance, as does the winner of match B.\n\nThis adaptation of the first McIntyre System corrected for the anomaly that, in the first week, the team who finished 4th would have a more difficult opponent than the team who finished 5th, and was hence more likely to be eliminated, despite finishing higher. This was achieved by adding flexibility to the second round draw, so that the two elimination final winners were re-ranked to determine which played the winner of the qualifying final and which played the loser.\n\nHowever, both McIntyre final six systems had another weakness: the loser of the Qualifying Final (which is the most difficult game of the first round), ended up facing elimination in the First-Semi Final, while the higher-ranked Elimination Final winner (who has had the easiest game of the first round) has a double chance in the Second-Semi Final.\n\nThe McIntyre final eight bears little in common with the other McIntyre Systems. At no stage does it follow the Page–McIntyre structure, and at no stage after the first week does any team retain a double chance. The system allows for 26 of the 28 combinations of the eight finalists to feature in the Grand Final (the two combinations not possible are 1st v 7th and 2nd v 8th). It gives 18.75% to 1st and 2nd, 15.625% to 3rd, 12.5% to 4th and 5th, 9.375% to 6th and 6.25% to 7th and 8th.\n\n\n"}
{"id": "317827", "url": "https://en.wikipedia.org/wiki?curid=317827", "title": "Morton's fork", "text": "Morton's fork\n\nA Morton's fork is a type of false dilemma in which contradictory observations lead to the same conclusion. It is said to have originated with the collecting of taxes by John Morton.\n\nThe earliest known use of the term dates from the mid-19th century and the only known earlier mention is a claim by Francis Bacon of an extant tradition.\n\nUnder Henry VII John Morton was made archbishop of Canterbury in 1486, then Lord Chancellor in 1487. He raised taxation funds for his king by holding that someone living modestly must be saving money and, therefore, could afford taxes, whereas someone living extravagantly obviously was rich and, therefore, could afford taxes.\n\nIn some instances, such as Morton's original use of the fallacy, it may be that one of the two observations is probably valid, but the other is pure sophistry: evidence of possessing wealth may be genuinely irrelevant to having a source of taxable income.\n\nIn other cases, it may be that neither observation may be relied upon to support the conclusion properly. For example, asserting that a person suspected of a crime who is acting nervously must have something to feel guilty about, while a person who acts calmly and confidently must be practised or skilled at hiding guilt. Either observation therefore has little, if any, probative value, as each could equally be evidence for the opposite conclusion.\n\n\"Morton's fork coup\" is a maneuver in the game of bridge that uses the principle of Morton's Fork.\n\nAn episode of the television series \"Fargo\" is entitled \"Morton's Fork\", after the dilemma. It is also mentioned in NCIS Los Angeles season 5 episode 16, \"Fish Out of Water.\"\n\n"}
{"id": "2225085", "url": "https://en.wikipedia.org/wiki?curid=2225085", "title": "Naturphilosophie", "text": "Naturphilosophie\n\nNaturphilosophie (German for \"nature-philosophy\") is a term used in English-language philosophy to identify a current in the philosophical tradition of German idealism, as applied to the study of nature in the earlier 19th century. German speakers use the clearer term \"Romantische Naturphilosophie\", the philosophy of nature developed at the time of the founding of German Romanticism. It is particularly associated with the philosophical work of Friedrich Wilhelm Joseph von Schelling and Georg Wilhelm Friedrich Hegel—though it has some clear precursors also. More particularly it is identified with some of the initial works of Schelling during the period 1797–9, in reaction to the views of Fichte, and subsequent developments from Schelling's position. Always controversial, some of Schelling's ideas in this direction are still considered of philosophical interest, even if the subsequent development of experimental natural science had a destructive impact on the credibility of the theories of his followers in \"Naturphilosophie\".\n\n\"Naturphilosophie\" attempted to comprehend nature in its totality and to outline its general theoretical structure, thus attempting to lay the foundations for the natural sciences. In developing their theories, the German \"Naturphilosophen\" found their inspiration in the natural philosophy of the Ancient Greek Ionian philosophers.\n\nAs an approach to philosophy and science, \"Naturphilosophie\" has had a difficult reception. In Germany, Neo-Kantians came to distrust its developments as speculative and overly metaphysical. For most of the 19th and early 20th centuries, it was poorly understood in Anglophone countries. Over the years, it has been subjected to continuing criticism. Since the 1960s, improved translations have appeared, and scholars have developed a better appreciation of the objectives of \"Naturphilosophie\".\n\nThe German Idealist philosopher Fichte had attempted to show that the whole structure of reality follows necessarily from the fact of self-consciousness. Schelling took Fichte's position as his starting-point, and in his earliest writings posited that nature must have reality for itself. In this light Fichte's doctrines appeared incomplete. On the one hand, they identified the ultimate ground of the universe of reason too closely with finite, individual Spirit. On the other, they threatened the reality of the world of nature by seeing it too much in the manner of subjective idealism. Fichte, in this view, had not managed to unite his system with the aesthetic view of nature to which Immanuel Kant's \"Critique of Judgment\" had pointed.\n\n\"Naturphilosophie\" is therefore one possible theory of the unity of nature. Nature as the sum of what is objective, and intelligence as the complex of all the activities making up self-consciousness, appear as equally real. The philosophy of nature and transcendental idealism would be the two complementary portions making up philosophy as a whole.\n\n\"Naturphilosophie\" translated into English would mean just \"philosophy of nature\", and its scope began to be taken in a broad way. Johann Gottfried Herder, particularly taken in opposition to Immanuel Kant, was a precursor of Schelling:\n\nHerder's dynamic view of nature was developed by Goethe and Schelling and led to the tradition of \"Naturphilosophie\"[...]\n\nLater Friedrich Schlegel theorised about a particular German strand in philosophy of nature, citing Jakob Böhme, Johannes Kepler and Georg Ernst Stahl, with Jan Baptist van Helmont as an edge case. Frederick Beiser instead traces \"Naturphilosophie\" as developed by Schelling, Hegel, Schlegel and Novalis to a crux in the theory of matter, and identifies the origins of the line they took with the \"vis viva\" theory of matter in the work of Gottfried Leibniz.\n\nSubsequently Schelling identified himself with Baruch de Spinoza, to whose thought he saw himself as approaching. The \"Darstellung meines Systems\", and the expanded treatment in the lectures on a \"System der gesamten Philosophie und der Naturphilosophie insbesondere\" given in Würzburg in 1804, contain elements of Spinoza's philosophy.\n\nIn a short space of time Schelling produced three works: \"Ideen zu einer Philosophie der Natur als Einleitung in das Studium dieser Wissenschaft\", 1797 (\"Ideas for a Philosophy of Nature as Introduction to the Study of this Science\"); \"Von der Weltseele\", 1798 (\"On the World Soul\"); and \"Erster Entwurf eines Systems der Naturphilosophie\", 1799 (First Plan of a System of the Philosophy of Nature). As criticism of scientific procedure, these writings retain a relevance. Historically, according to Richards:\n\nDespite the tentativeness of their titles, these monographs introduced radical interpretations of nature that would reverberate through the sciences, and particularly the biology, of the next century. They developed the fundamental doctrines of \"Naturphilosophie\".\n\nIn \"System des transzendentalen Idealismus\", 1800 (\"System of Transcendental Idealism\") Schelling included ideas on matter and the organic in Part III. They form just part of a more ambitious work that takes up other themes, in particular aesthetics. From this point onwards \"Naturphilosophie\" was less of a research concern for him, as he reformulated his philosophy. However, it remained an influential aspect of his teaching. For a short while, he edited a journal, the \"Neue Zeitschrift für speculative Physik\" (bound volume 1802).\n\nSchelling's \"Naturphilosophie\" was a way in which he worked himself out of the tutelage of Fichte, with whom he quarrelled decisively towards the end of the 1790s. More than that, however, it brought him within the orbit of Johann Wolfgang von Goethe, both intellectually and (as a direct consequence of Goethe's sympathetic attitude) by a relocation; and it broke with basic Kantian tenets. Grant writes:\n\nSchelling's postkantian confrontation with nature itself begins with the overthrow of the Copernican revolution ...\n\nSchelling held that the divisions imposed on nature, by our ordinary perception and thought, do not have absolute validity. They should be interpreted as the outcome of the single formative energy which is the soul or inner aspect of nature. In other words he was a proponent of a variety of organicism. The dynamic series of stages in nature, the forms in which the ideal structure of nature is realized, are matter, as the equilibrium of the fundamental expansive and contractive forces; light, with its subordinate processes (magnetism, electricity, and chemical action); organism, with its component phases of reproduction, irritability and sensibility. The continual change presented to us by experience, taken together with the thought of unity in productive force of nature, leads to the conception of the duality through which nature expresses itself in its varied products.\n\nIn the introduction to the \"Ideen\" he argues against dogmatism, in the terms that a dogmatist cannot explain the organic; and that recourse to the idea of a cosmic creator is a feature of dogmatic systems imposed by the need to explain nature as purposive and unified. Fichte's system, called the \"Wissenschaftslehre\", had begun with a fundamental distinction between dogmatism (fatalistic) and criticism (free), as his formulation of idealism.\n\nBeiser divides up the mature form of Schelling's \"Naturphilosophie\" into the attitudes of transcendental realism (the thesis that \"nature exists independent of all consciousness, even that of the transcendental subject\") and transcendental naturalism (the thesis that \"everything is explicable according to the laws of nature, including the rationality of the transcendental subject\"). He notes how \"Naturphilosophie\" was first a counterbalance to \"Wissenschaftslehre\", and then in Schelling's approach became the senior partner. After that, it was hardly to be avoided that Schelling would become an opponent of Fichte, having been a close follower in the early 1790s.\n\nWe are able to apprehend and represent nature to ourselves in the successive forms which its development assumes, since it is the same spirit of which we become aware in self-consciousness, though here unconsciously. The variety of its forms is not imposed on it externally, since there is no external teleology in nature. Nature is a self-forming whole, within which only natural explanations can be sought. The function of \"Naturphilosophie\" is to exhibit the ideal as springing from the real, not to deduce the real from the ideal.\n\nCriticism of \"Naturphilosophie\" has been widespread, over two centuries. Schelling's theories, however influential in terms of the general culture of the time, have not survived in scientific terms. Like other strands of speculation in the life sciences, in particular, such as vitalism, they retreated in the face of experiment, and then were written out of the history of science as Whig history. But critics were initially not scientists (a term not used until later); rather they came largely from within philosophy and Romantic science, a community including many physicians. Typically, the retrospective views of scientists of the 19th century on \"Romantic science\" in general erased distinctions:\n\nScientific criticism in the nineteenth century took hardly any notice of the distinctions between Romantic, speculative and transcendental, scientific and aesthetic directions.\n\nOne outspoken critic was the chemist Justus von Liebig, who compared \"Naturphilosophie\" with the Black Death. Another critic, the physiologist Emil du Bois-Reymond, frequently dismissed \"Naturphilosophie\" as \"bogus.\"\n\nIsaiah Berlin summed up the reasons why \"Naturphilosophie\" had a wide-ranging impact on views of art and artists:\n\nif everything in nature is living, and if we ourselves are simply its most self-conscious representatives, the function of the artist is to delve within himself, and above all to delve within the dark and unconscious forces which move within him, and to bring these to consciousness by the most agonising and violent internal struggle.\n\nFichte was very critical of the opposition set up in Schelling's \"Naturphilosophie\" to his own conception of \"Wissenschaftslehre\". In that debate, Hegel then intervened, largely supporting his student friend Schelling, with the work usually called his \"Differenzschrift\", the \"Differenz des Fichteschen und Schellingschen Systems der Philosophie\" (The Difference Between Fichte's and Schelling's System of Philosophy); a key publication in his own philosophical development, his first book, it was published in September 1801.\n\nSchelling's Absolute was left with no other function than that of removing all the differences which give form to thought. The criticisms of Fichte, and more particularly of Hegel (in the Preface to the \"Phenomenology of Spirit\"), pointed to a defect in the conception of the Absolute as mere featureless identity. It was ridiculed by Hegel as \"the night in which all cows are black.\"\n\nIgnaz Paul Vitalis Troxler, a follower of Schelling, later broke with him. He came to the view that the Absolute in nature and mind is beyond the intellect and reason.\n\n\n"}
{"id": "2015211", "url": "https://en.wikipedia.org/wiki?curid=2015211", "title": "Nelson rules", "text": "Nelson rules\n\nNelson rules are a method in process control of determining if some measured variable is out of control (unpredictable versus consistent). Rules, for detecting \"out-of-control\" or non-random conditions were first postulated by Walter A. Shewhart in the 1920s. The Nelson rules were first published in the October 1984 issue of the \"Journal of Quality Technology\" in an article by Lloyd S Nelson.\n\nThe rules are applied to a control chart on which the magnitude of some variable is plotted against time. The rules are based on the mean value and the standard deviation of the samples.\n\nThe above eight rules apply to a chart of a variable value.\n\nA second chart, the moving range chart, can also be used but only with rules 1, 2, 3 and 4. Such a chart plots a graph of the maximum value - minimum value of N adjacent points against the time sample of the range.\n\nAn example moving range: if N = 3 and values are 1, 3, 5, 3, 3, 2, 4, 5 then the sets of adjacent points are (1,3,5) (3,5,3) (5,3,3) (3,3,2) (3,2,4) (2,4,5) resulting in moving range values of (5-1) (5-3) (5-3) (3-2) (4-2) (5-2) = 4, 2, 2, 1, 2, 3.\n\nApplying these rules indicates when a potential \"out of control\" situation has arisen. However, there will always be some false alerts and the more rules applied the more will occur. For some processes, it may be beneficial to omit one or more rules. Equally there may be some missing alerts where some specific \"out of control\" situation is not detected. Empirically, the detection accuracy is good.\n\n\n"}
{"id": "1690921", "url": "https://en.wikipedia.org/wiki?curid=1690921", "title": "Nursing process", "text": "Nursing process\n\nThe nursing process is a modified scientific method. Nursing practise was first described as a four-stage nursing process by Ida Jean Orlando in 1958. It should not be confused with nursing theories or health informatics. The diagnosis phase was added later.\n\nThe nursing process uses clinical judgement to strike a balance of epistemology between personal interpretation and research evidence in which critical thinking may play a part to categorize the clients issue and course of action. Nursing offers diverse patterns of knowing. Nursing knowledge has embraced pluralism since the 1970s.\n\nSome authors refer to a mind map or abductive reasoning as a potential alternative strategy for organizing care. Intuition plays a part for experienced nurses.\n\nThe nursing process is goal-oriented method of caring that provides a framework to nursing care. It involves seven major steps:\n\n\nAccording to some theorists, this seven-steps description of the nursing process is outdated and misrepresents nursing as linear and atomic.\n\nThe nurse completes an holistic nursing assessment of the needs of the individual/family/community, regardless of the reason for the encounter. The nurse collects subjective data and objective data using a nursing framework, such as Marjory Gordon's functional health patterns.\n\nNursing assessments provide the starting point for determining nursing diagnoses. It is vital that a recognized nursing assessment framework is used in practice to identify the patient's* problems, risks and outcomes for enhancing health. The use of an evidence-based nursing framework such as Gordon's Functional Health Pattern Assessment should guide assessments that support nurses in determination of NANDA-I nursing diagnoses. For accurate determination of nursing diagnoses, a useful, evidence-based assessment framework is best practice.\n\n\nNursing diagnoses represent the nurse's clinical judgment about actual or potential health problems/life process occurring with the individual, family, group or community. The accuracy of the nursing diagnosis is validated when a nurse is able to clearly identify and link to the defining characteristics, related factors and/or risk factors found within the patients assessment. Multiple nursing diagnoses may be made for one client.\n\nIn agreement with the client, the nurse addresses each of the problems identified in the diagnosing phase. When there are multiple nursing diagnoses to be addressed, the nurse prioritizes which diagnoses will receive the most attention first according to their severity and potential for causing more serious harm. For each problem a measurable goal/outcome is set. For each goal/outcome, the nurse selects nursing interventions that will help achieve the goal/outcome. A common method of formulating the expected outcomes is to use the evidence-based Nursing Outcomes Classification to allow for the use of standardized language which improves consistency of terminology, definition and outcome measures. The interventions used in the Nursing Interventions Classification again allow for the use of standardized language which improves consistency of terminology, definition and ability to identify nursing activities, which can also be linked to nursing workload and staffing indices. The result of this phase is a nursing care plan.\n\nThe nurse implements the nursing care plan, performing the determined interventions that were selected to help meet the goals/outcomes that were established. Delegated tasks and the monitoring of them is included here as well.\n\nActivities\n\nThe nurse evaluates the progress toward the goals/outcomes identified in the previous phases. If progress towards the goal is slow, or if regression has occurred, the nurse must change the plan of care accordingly. Conversely, if the goal has been achieved then the care can cease. New problems may be identified at this stage, and thus the process will start all over again.\n\nThe nursing process is a cyclical and ongoing process that can end at any stage if the problem is solved. The nursing process exists for every problem that the individual/family/community has. The nursing process not only focuses on ways to improve physical needs, but also on social and emotional needs as well.\n\nThe entire process is recorded or documented in order to inform all members of the health care team.\n\nThe PIE method is a system for documenting actions, especially in the field of nursing. The name comes from the acronym \"PIE\", meaning Problem, Intervention, Evaluation.\n\n"}
{"id": "16966680", "url": "https://en.wikipedia.org/wiki?curid=16966680", "title": "Occupational segregation", "text": "Occupational segregation\n\nOccupational segregation is the distribution of workers across and within occupations, based upon demographic characteristics, most often gender. Occupational segregation levels differ on a basis of perfect segregation and integration. Perfect segregation occurs where any given occupation employs only one group. Perfect integration, on the other hand, occurs where each group holds the same proportion of positions in an occupation as it holds in the labor force.\n\nMany scholars, such as Biblarz et al., argue that occupational segregation is most likely caused by gender-based discrimination that often occurs in patterns, either horizontally (across occupations) or vertically (within the hierarchy of occupations). Both of these contribute to the gender pay gap.\n\nHorizontal segregation refers to differences in the number of people of each gender present across occupations. Horizontal segregation is likely to be increased by post-industrial restructuring of the economy (post-industrial society), in which the expansion of service industries has called for many women to enter the workforce. The millions of housewives who entered the economy during post-industrial restructuring primarily entered into service sector jobs where they could work part-time and having flexible hours. While these options are often appealing to mothers, who are often responsible for the care work of their children and their homes, they are also unfortunately most available in lower-paying and lower status occupations. The idea that nurses and teachers are often pictured as women whereas doctors and lawyers are often assumed to be men are examples of how highly engrained horizontal segregation is in our society.\n\nThe term \"vertical segregation\" describes men's domination of the highest status jobs in both traditionally male and traditionally female occupations. Colloquially, the existence of vertical segregation is referred to as allowing men to ride in a \"glass escalator\" through which women must watch as men surpass them on the way to the top positions. Generally, the more occupational segregation present in a country, the less vertical segregation there is because women have a better chance of obtaining the highest positions in a given occupation as their share of employment in that particular occupation increases.\n\nVertical segregation can be somewhat difficult to measure across occupations because it refers to hierarchies within individual occupations. For example, the category of Education Professionals, (a category in the Australian Standard Classification of Occupations, Second Edition), is broken down into \"School Teachers,\" \"University and Vocational Education Teachers,\" and \"Miscellaneous Education Professionals.\" These categories are then further broken down into subcategories. While these categories aptly describe the divisions within education, they are not comparable to the hierarchical categories within other occupations, and thus make comparisons of levels of vertical segregation quite difficult.\n\nSome women self-select out of higher status positions, choosing instead to have more time to spend at home and with their families. According to Sarah Damaske, this choice is often made because high status positions do not allow time for the heavy domestic workload that many women expect to take on due to the gendered division of labor in the home. Working class women, in particular, also sometimes self-select out of more time-intensive or higher -status positions in order to maintain the traditional gender hierarchy and household accord.\n\nHuman capital explanations are those that argue that an individual's and a group's occupational and economic success can be at least partially attributed to accumulated abilities developed through formal and informal education and experiences. Human capital explanations for occupational segregation, then, posit that a difference in educational levels of men and women is responsible for persistent occupational segregation. Contrary to this theory, however, over past 40 years, women's educational attainment has outpaced men's. One area of education that might play a substantial role in occupational segregation, however, is the dearth of women in science and mathematics. STEM fields tend to be pipelines to higher paying jobs. Therefore, the lack of women in higher paying jobs might be partially because they do not pursue science and mathematics in school. This can be seen in areas such as finance, which is very mathematics heavy and is also a very popular field for those who eventually rise to high status positions in the private sector. This choice, like others, is often a personal preference or made because of the cultural idea that women are not as good as men at mathematics.\n\nHuman capital explanations also posit that men tend to rise to higher positions than women because of a disparity in work experience between the genders. Indeed, the gap between men and women's tenure rises with age, and female college graduates are more likely than males to interrupt their careers to raise children. Such choices may also be attributed to the gendered division of labor which holds women primarily responsible for domestic duties.\n\nHuman capital explanations posit additionally that men are more likely than women to preference their work life over their family life. However, the General Social Survey found that men were only slightly less likely than women to value short hours, and that preferences for particular job characteristics depended mostly on age, education, race, and other characteristics rather than on gender. In addition, other research has shown that men and women likely hold endogenous job preferences, meaning that their preferences are due to the jobs they hold and those they have held in the past rather than related inherently to gender. After taking into consideration men and women's jobs, there is no difference in their job preferences. Men and women engaged in similar types of work have similar levels of commitment to work and display other similar preferences.\n\nAccording to sociologists Hanson and Pratt, men and women employ different strategies in their job searches that play a role in occupational segregation. These differing strategies are influenced by power relations in the household, the gendered nature of social life, and women's domestic responsibilities. The last factor, in particular, leads women to prioritize the geographical proximity of paid employment when searching for a job. In addition, most people have been found to find their jobs through informal contacts. The gendered nature of social life leads women to have networks with smaller geographical reach than men. Thus, the location of women in female-dominated occupations which are lower-status and lower pay is the result of \"severe day-to-day time constraints\" rather than a conscious and long-term choice made that would be able to maximize pay and prestige.\n\nWomen in female-dominated jobs pay two penalties: the average wage of their jobs is lower than that in comparable male-dominated jobs, and they earn less relative to men in the same jobs. In addition, women's wages are negatively affected by the percentage of females in a job, but men's wages are essentially unaffected.\nThe crowding hypothesis postulates that occupational segregation lowers all women's earnings as a result of women's exclusion from primarily male occupations and segregation into a number of predominantly female-dominated occupations. Given that feminine skills are traditionally rewarded less both in salary and prestige, the crowding of women into certain occupations makes these occupations valued less in both pay and prestige.\n\nCrowding is found to be alleviated through macro-changes in occupational segregation. Teaching, for example, at least in recent generations, is traditionally a female- dominated profession. However, when positions open up for women in business and other high-earning occupations, school boards must raise the salaries of potential teachers to attract candidates. This is an example of how even women in traditionally female-dominated professions still benefit salary-wise from the gendered integration of the market.\n\nThe gendered division of labor helps to explain the hierarchy of power across gender identity, class, race, ethnicity, and sexual orientation. Socialist feminists contribute to this ideology through a Marxist frame of alienated labor and the means of production. Heidi Hartmann emphasized the gendered division of labor as patriarchal control over women's labor. Wally Saccombe suggested the mode of production should become a unity of production and reproduction, in which women's reproductive abilities are viewed as a valuable source of labor or income.The \"wages for housework\" movement in the late 1970s showcased the importance of gender inequality in the workplace. Socialist feminists critiqued the exploitation of women's household and reproductive labor, since it was not viewed as a commodity that deserved payment in the market economy. Women often experience working a \"double day\" or \"second shift\" when they go to a wage-earning job and then come home to take care of children and the home.\n\nOccupational segregation is measured using Duncan's D (or the index of dissimilarity), which serves as a measure of dissimilarity between two distributions. D has to be calculated first:\n\n\nOver the last century in the United States, there has been a surprising stability of segregation-index scores, which measure the level of occupational segregation of the labor market. There were declines in occupational segregation in the 1970s and 1980s, as technologies that made the care work of the home quicker and easier allowed more women time to enter the workforce. However, occupational segregation remains a fixed element of the United States workforce today.\nAs recently as 1996, it has been found that gender occupational segregation over the past three generations has not decreased. In one study, grown women working in the 1980s were likely to have faced the same occupational segregation faced by their mothers working in the 1960s and their grandmothers working in the 1940s.\n\nGender egalitarian cultural principles, or changes in traditional gender norms, are one possible solution to occupational segregation in that they reduce discrimination, affect women's self-evaluations, and support structural changes. Horizontal segregation, however, is more resistant to change from simply modern egalitarian pressures. Changes in norms may reinforce the impact of occupational integration in that once people see women in traditionally male-dominated occupations, their expectations about women in the labor market might be changed.\n\nSome scholars, such as Haveman and Beresford, therefore argue that any policies aimed at reducing occupational inequality must focus on culture changes. According to Haveman and Beresford, people in the United States have historically tended to reject policies that only support one group (unless that group is them). Therefore, effective policies for limiting occupational segregation must aim to provide benefits across groups. Therefore, policies that aim at capping work hours for salaried workers or mandate on-site employer sponsored childcare might be most effective.\n\nIn addition, the more occupational integration that occurs, the more women are in the positions to make powerful decisions affecting occupational segregation. If the overall market becomes less segregated, those who make personnel decisions in traditionally female-dominated occupations will have to make jobs, even higher status jobs, more attractive to women to retain them. School boards, for example, will have to appoint more women to department head positions and other positions of authority in order to retain women workers, whereas those jobs might previously have gone to men.\n\nIn addition to gender, sexual orientation can also be a significant basis for occupational segregation: there is a disproportionately high number of gay and lesbian workers in certain occupations. Research shows that gay men are more likely to be in female-majority occupations than are heterosexual men, and lesbians are more represented in male-majority occupations than are heterosexual women, but even after accounting for this tendency, common to both gay men and lesbians is a propensity to concentrate in occupations that provide task independence or require social perceptiveness, or both. \n\n"}
{"id": "4188995", "url": "https://en.wikipedia.org/wiki?curid=4188995", "title": "Play-in game", "text": "Play-in game\n\nA \"'play-in game'\" is a game at the beginning of a tournament that forces the lowest qualifiers for the tournament to play each other before the main portion of the tournament begins. This gives an added advantage to the higher qualifiers, allowing them to rest, while the lower teams extend themselves by playing. Further, teams that participate in the play-in must usually play the next game against the highest qualifier in the tournament and play on the road. Having a play-in game allows for a tournament to have a number of teams that is not a power of two, and gives an extra advantage for teams to play for, as they try to win to avoid having to play in the extra game.\n\n\n"}
{"id": "294340", "url": "https://en.wikipedia.org/wiki?curid=294340", "title": "Poisson bracket", "text": "Poisson bracket\n\nIn mathematics and classical mechanics, the Poisson bracket is an important binary operation in Hamiltonian mechanics, playing a central role in Hamilton's equations of motion, which govern the time evolution of a Hamiltonian dynamical system. The Poisson bracket also distinguishes a certain class of coordinate transformations, called \"canonical transformations\", which map canonical coordinate systems into canonical coordinate systems. A \"canonical coordinate system\" consists of canonical position and momentum variables (below symbolized by formula_1 and formula_2, respectively) that satisfy canonical Poisson bracket relations. The set of possible canonical transformations is always very rich. For instance, it is often possible to choose the Hamiltonian itself formula_3 as one of the new canonical momentum coordinates.\n\nIn a more general sense, the Poisson bracket is used to define a Poisson algebra, of which the algebra of functions on a Poisson manifold is a special case. There are other general examples, as well: it occurs in the theory of Lie algebras, where the tensor algebra of a Lie algebra forms a Poisson algebra; a detailed construction of how this comes about is given in the universal enveloping algebra article. Quantum deformations of the universal enveloping algebra lead to the notion of quantum groups.\n\nAll of these objects are named in honour of Siméon Denis Poisson.\n\nGiven two functions formula_4 that depend on phase space and time, their Poisson bracket formula_5 is another function that depends on phase space and time. The following rules hold for any three functions formula_6 of phase space and time:\n\nAlso, if a function formula_11 is constant over phase space (but may depend on time), then formula_12 for any formula_13.\n\nIn canonical coordinates (also known as Darboux coordinates) formula_14 on the phase space, given two functions formula_15 and formula_16, the Poisson bracket takes the form\n\nThe Poisson brackets of the canonical coordinates are\n\nwhere \"formula_19\" is the Kronecker delta.\n\nHamilton's equations of motion have an equivalent expression in terms of the Poisson bracket. This may be most directly demonstrated in an explicit coordinate frame. Suppose that formula_20 is a function on the manifold. Then from the multivariable chain rule,\n\nFurther, one may take formula_22 and formula_23 to be solutions to Hamilton's equations; that is,\n\nThen \n\nThus, the time evolution of a function on a symplectic manifold can be given as a one-parameter family of symplectomorphisms (i.e., canonical transformations, area-preserving diffeomorphisms), with the time \"t\" being the parameter: Hamiltonian motion is a canonical transformation generated by the Hamiltonian. That is, Poisson brackets are preserved in it, so that \"any time t\" in the solution to Hamilton's equations, \n\ncan serve as the bracket coordinates. \"Poisson brackets are canonical invariants\".\n\nDropping the coordinates, \n\nThe operator in the convective part of the derivative, \"L̂\" = , is sometimes referred to as the Liouvillian (see Liouville's theorem (Hamiltonian)).\n\nAn integrable dynamical system will have constants of motion in addition to the energy. Such constants of motion will commute with the Hamiltonian under the Poisson bracket. Suppose some function formula_28 is a constant of motion. This implies that if formula_29 is a trajectory or solution to the Hamilton's equations of motion, then\n\nalong that trajectory. Then\n\nwhere, as above, the intermediate step follows by applying the equations of motion and we supposed that formula_13 does not explicitly depend on time. This equation is known as the Liouville equation. The content of Liouville's theorem is that the time evolution of a measure (or \"distribution function\" on the phase space) is given by the above.\n\nIf the Poisson bracket of formula_13 and formula_34 vanishes (formula_35), then formula_13 and formula_34 are said to be in involution. In order for a Hamiltonian system to be completely integrable, formula_38 independent constants of motion must be in mutual involution, where formula_38 is the number of degrees of freedom.\n\nFurthermore, according to the Poisson's Theorem, if two quantities formula_40 and formula_41 are explicitly time independent (formula_42) constants of motion, so is their Poisson bracket formula_43. This does not always supply a useful result, however, since the number of possible constants of motion is limited (formula_44 for a system with formula_38 degrees of freedom), and so the result may be trivial (a constant, or a function of formula_40 and formula_41.)\n\nLet formula_48 be symplectic manifold, that is, a manifold equipped with a symplectic form: a 2-form formula_49 which is both closed (i.e., its exterior derivative formula_50 vanishes) and non-degenerate. For example, in the treatment above, take formula_48 to be formula_52 and take\n\nIf formula_54 is the interior product or contraction operation defined by formula_55, then non-degeneracy is equivalent to saying that for every one-form formula_56 there is a unique vector field formula_57 such that formula_58. Alternatively, formula_59. Then if formula_60 is a smooth function on formula_48, the Hamiltonian vector field formula_62 can be defined to be formula_63. It is easy to see that\n\nThe Poisson bracket formula_65 on (\"M\", ω) is a bilinear operation on differentiable functions, defined by formula_66; the Poisson bracket of two functions on \"M\" is itself a function on \"M\". The Poisson bracket is antisymmetric because:\n\nFurthermore,\n\n\\omega)(X_g) = df(X_g) \\\\\n\nHere \"Xf\" denotes the vector field \"X\" applied to the function \"f\" as a directional derivative, and formula_68 denotes the (entirely equivalent) Lie derivative of the function \"f\".\n\nIf α is an arbitrary one-form on \"M\", the vector field Ω generates (at least locally) a flow formula_69 satisfying the boundary condition formula_70 and the first-order differential equation\n\nThe formula_69 will be symplectomorphisms (canonical transformations) for every \"t\" as a function of \"x\" if and only if formula_73; when this is true, Ω is called a symplectic vector field. Recalling Cartan's identity formula_74 and \"d\"ω = 0, it follows that formula_75. Therefore, Ω is a symplectic vector field if and only if α is a closed form. Since formula_76, it follows that every Hamiltonian vector field \"X\" is a symplectic vector field, and that the Hamiltonian flow consists of canonical transformations. From above, under the Hamiltonian flow \"X\",\n\nThis is a fundamental result in Hamiltonian mechanics, governing the time evolution of functions defined on phase space. As noted above, when \"{f,H} = 0\", \"f\" is a constant of motion of the system. In addition, in canonical coordinates (with formula_78 and formula_79), Hamilton's equations for the time evolution of the system follow immediately from this formula.\n\nIt also follows from that the Poisson bracket is a derivation; that is, it satisfies a non-commutative version of Leibniz's product rule:\n\nThe Poisson bracket is intimately connected to the Lie bracket of the Hamiltonian vector fields. Because the Lie derivative is a derivation,\n\nThus if \"v\" and \"w\" are symplectic, using formula_81, Cartan's identity, and the fact that formula_82 is a closed form,\n\nIt follows that formula_84, so that\n\nThus, the Poisson bracket on functions corresponds to the Lie bracket of the associated Hamiltonian vector fields. We have also shown that the Lie bracket of two symplectic vector fields is a Hamiltonian vector field and hence is also symplectic. In the language of abstract algebra, the symplectic vector fields form a subalgebra of the Lie algebra of smooth vector fields on \"M\", and the Hamiltonian vector fields form an ideal of this subalgebra. The symplectic vector fields are the Lie algebra of the (infinite-dimensional) Lie group of symplectomorphisms of \"M\".\n\nIt is widely asserted that the Jacobi identity for the Poisson bracket,\n\nfollows from the corresponding identity for the Lie bracket of vector fields, but this is true only up to a locally constant function. However, to prove the Jacobi identity for the Poisson bracket, it is sufficient to show that:\n\nwhere the operator formula_87 on smooth functions on \"M\" is defined by formula_88 and the bracket on the right-hand side is the commutator of operators, formula_89. By , the operator formula_87 is equal to the operator \"X\". The proof of the Jacobi identity follows from because the Lie bracket of vector fields is just their commutator as differential operators.\n\nThe algebra of smooth functions on M, together with the Poisson bracket forms a Poisson algebra, because it is a Lie algebra under the Poisson bracket, which additionally satisfies Leibniz's rule . We have shown that every symplectic manifold is a Poisson manifold, that is a manifold with a \"curly-bracket\" operator on smooth functions such that the smooth functions form a Poisson algebra. However, not every Poisson manifold arises in this way, because Poisson manifolds allow for degeneracy which cannot arise in the symplectic case.\n\nGiven a smooth vector field \"X\" on the configuration space, let \"P\" be its conjugate momentum. The conjugate momentum mapping is a Lie algebra anti-homomorphism from the Poisson bracket to the Lie bracket:\n\nThis important result is worth a short proof. Write a vector field \"X\" at point \"q\" in the configuration space as\n\nwhere the formula_93 is the local coordinate frame. The conjugate momentum to \"X\" has the expression\n\nwhere the \"p\" are the momentum functions conjugate to the coordinates. One then has, for a point \"(q,p)\" in the phase space,\n\nThe above holds for all \"(q, p)\", giving the desired result.\n\nPoisson brackets deform to Moyal brackets upon quantization, that is, they generalize to a different Lie algebra, the Moyal algebra, or, equivalently in Hilbert space, quantum commutators. The Wigner-İnönü group contraction of these (the classical limit, ) yields the above Lie algebra.\n\nTo state this more explicitly and precisely, the universal enveloping algebra of the Heisenberg algebra is the Weyl algebra (modulo the relation that the center be the unit). The Moyal product is then a special case of the star product on the algebra of symbols. An explicit definition of the algebra of symbols, and the star product is given in the article on the universal enveloping algebra.\n\n\n"}
{"id": "752941", "url": "https://en.wikipedia.org/wiki?curid=752941", "title": "Project network", "text": "Project network\n\nA project network is a graph (weighted directed graph) depicting the sequence in which a project's terminal elements are to be completed by showing terminal elements and their dependencies. It is always drawn from left to right to reflect project chronology.\n\nThe work breakdown structure or the product breakdown structure show the \"part-whole\" relations. In contrast, the project network shows the \"before-after\" relations.\n\nThe most popular form of project network is activity on node (AON)(as shown above), the other one is activity on arrow (AOA).\n\nThe condition for a valid project network is that it doesn't contain any circular references.\n\nProject dependencies can also be depicted by a predecessor table. Although such a form is very inconvenient for human analysis, project management software often offers such a view for data entry.\n\nAn alternative way of showing and analyzing the sequence of project work is the design structure matrix.\n\n"}
{"id": "2059427", "url": "https://en.wikipedia.org/wiki?curid=2059427", "title": "Ruskin Colony", "text": "Ruskin Colony\n\nThe Ruskin Colony (or Ruskin Commonwealth Association) was a utopian socialist colony which existed near Tennessee City in Dickson County, Tennessee from 1894 to 1896. The colony moved to a slightly more permanent second settlement on an old farm five miles north from 1896 to 1899, and saw another brief incarnation near Waycross, in southern Georgia, from 1899 until it finally dissolved in 1901. Its regional location within the Southern United States set it apart from many other similar utopian projects of the era. At its high point, the population was around 250. The colony was named after John Ruskin, the English socialist writer. A cave on the colony's second property in Dickson County still carries his name. The site of the colony's second settlement in Dickson County is listed on the National Register of Historic Places.\n\nThe Ruskin Colony was founded by Julius Augustus Wayland (1854-1912), a newspaper editor and socialist from Indiana. The roots of the Ruskin project can be found in the movement within American socialism at the time towards the creation of new model colonies which would, in theory, challenge the American industrial system by creating ethical alternatives built in rural settings. The idea that new settlements such as Ruskin would eventually bring forth a revolution referred to as the \"co-operative commonwealth\" stood in contrast to socialists who believed that it was more important to do political and social organizing within the cities, the centers of industry. According to W. Fitzhugh Brundage in the book \"A Socialist Utopia in the New South\": \"The wastefulness and ugliness of competitive individualism, so glaringly apparent in late nineteenth-century cities, would be replaced by the efficient creation and collective control of wealth and technology\" in this new settlement.\n\nBy requiring that all members of the colony become equal shareholders in the endeavor, Wayland constructed Ruskin so that it operated more as a legally-sanctioned corporation. Every colonist was then, in essence, a stockholder. The colony, with its elected board of directors, was to run much like any other company, except that it would \"do all things necessary to make a success, financially and socially, of a co-operative colony.\" (\"The Coming Nation\", Feb. 3, 1894) Ruskin colonists manufactured and marketed pants, \"cereal coffee,\" a vapor bath cabinet, chewing gum, belts and suspenders. The system of work itself changed little from that of the world outside Ruskin in terms of hours devoted to the various industries, however the hours, schedules and rates of pay, and industries selected were all determined by the workers. Ruskinites eventually abolished cash wages and adopted a system of scrip which was used in exchange for goods within the colony. In securing their economic dependence, members of the settlement also gave ample time to creative crafts, theater, and other intellectual pursuits. At one time late in the history of the colony, there even existed a band which toured southern Georgia.\n\nMany of the products created in Ruskin were intended to supplement the income from the newspaper, \"The Coming Nation\", which was the primary source of financial stability. The majority of the colony's money and time was put into the paper, which had at its peak in 1896 around 60,000 subscribers. Besides being the chief flow of assets, the paper also gave voice to the men and women of the colony throughout its many editors. Although Julius Wayland almost single-handedly founded the colony, he left in 1895 due to conflicts about ownership of the newspaper that ran counter to his claims of collective ownership. Under Alfred S. Edwards, who succeeded Wayland, \"The Coming Nation\" included articles from the likes of George D. Herron, Charlotte Perkins Gilman, and Herbert Casson (who later took up the editorial reigns after Edwards left the colony).\n\nThe eventual breakup of the Ruskin colony was due to several elements, the most problematic being the unequal distribution of membership rights of colonists complicated by the \"shareholder\"-type initial investment fees. Much of the blame lay with the original charter members, who had become entrenched in leadership and direction of Ruskin. One particular issue which drew ideological divisions through the colony was that of polyamorous relationships, or the practice of \"free love\", within members of the colony. Ruskinites opposed to these beliefs brought harsh criticism down on those who harbored free love sympathies, which were in many ways linked to anarchist currents that had been growing within the colony. This could be traced to Alfred Edward's editorial slant towards anarchism during his time as editor of \"The Coming Nation\".\n\nThe colony eventually became mired in constant litigation over issues of property, with charter members who were now being pushed out of Ruskin seeking to dismantle the group through legal means. The final auction of the Ruskin Colony site at Cave Mills and most of the communal property left the remaining members with only a fraction of what they had spent five years struggling to build. The 240 members moved what they did have, which still included the newspaper and the printing apparatus for it, 613 miles on a chartered train to their new home in Georgia, where they merged with the Duke Colony in Ware County and formed the Ruskin Commonwealth.\n\nHowever, after its first year in Georgia, the number of colonists dropped by half. The new settlement, an old lumber mill, was not surrounded by the fertile land and good sources of water that the previous location had. Ruskinites were plagued with disease, unprofitable business ventures, and a continual slide into poverty that eventually led to the auction of the property by the county sheriff to settle its debts. The Ruskin Commonwealth was effectively disbanded in the autumn of 1901.\n\nThe Ruskin Commongood Society platted Ruskin, Florida on February 19, 1910.\n\n\n\n"}
{"id": "50408659", "url": "https://en.wikipedia.org/wiki?curid=50408659", "title": "St Sebastian (Bohumil Kubishta painting)", "text": "St Sebastian (Bohumil Kubishta painting)\n\nSt Sebastian is a 20th-century painting by the Bohemian artist Bohumil Kubišta. Saint Sebastian was a Christian matryr. It is held in the National Gallery in Prague.\n\nThe picture is painted on in oils on canvas and has dimensions of 98 x 74,5 cm. It is one of the most important works of Bohumil Kubishta, member of the pre-First World War Czech avant-garde. Unlike traditional Christian interpretations of the theme of suffering of the Christian martyr, Bohumil Kubishta combines cubist forms and expressiveness, but integrates spiritual strength.\n"}
{"id": "53571494", "url": "https://en.wikipedia.org/wiki?curid=53571494", "title": "Surrogation", "text": "Surrogation\n\nSurrogation is a psychological phenomenon in which the measure(s) of a construct of interest evolve to replace the construct itself. Research on performance measurement in management accounting has identified surrogation as \"the tendency for managers to lose sight of the strategic construct(s) the [performance] measures are intended to represent, and subsequently act as though the measures \"are\" the constructs of interest\" (emphasis in original). An everyday example of surrogation is a manager tasked with increasing customer satisfaction who begins to believe that the customer satisfaction survey score actually \"is\" customer satisfaction.\n\nInspired by work by Yuji Ijiri, the term \"surrogation\" was coined by Willie Choi, Gary Hecht, and William Tayler in their paper, \"Lost in Translation: The Effects of Incentive Compensation on Strategy Surrogation\". They show managers tend to use measures as surrogates for strategy, acting as if measures were in fact the strategy when making optimization decisions. This appears to occur even if a measure-maximizing choice ultimately works against the strategy.\n\nThey also show surrogation is exacerbated by incentive compensation. But, the phenomenon is distinct from wealth-maximizing behavior, since it persists both when incentives are removed and when they are changed to create an opportunity cost for maximizing the surrogate. The additional tendency to surrogate in the presence of incentives is reduced when managers are compensated based on multiple measures of a strategy rather than on a single measure.\n\nChoi et al. proposed attribute substitution as a mechanism for surrogation. Attribute substitution in decision-making involves a complex \"target attribute\" being replaced by a more easily accessible \"heuristic attribute\". For this to occur, the target attribute must be relatively inaccessible, the heuristic attribute must be readily accessible, and the mental substitution must not be consciously rejected by the person. In the case of surrogation, the two attributes are related in that some party intends the heuristic attribute to serve as proxy for the target attribute.\n\nIn a follow-up study, Choi, Hecht, and Tayler demonstrate involving managers in the selection of a strategy reduces their tendency to surrogate. Merely involving managers in the strategy deliberation process does not appear to have the same surrogation-reducing effect as involving them in the actual selection of the strategy.\n\nJeremiah Bentley shows the effects of incentive compensation on surrogation are partially explained by a mechanism in which measure-based incentive compensation (in this case using a single measure) and wealth-maximizing behavior lead agents to distort their operational decisions (see Campbell's law). That operational distortion, in turn, leads them to change their beliefs about the compensated measure's causal relationship with the outcome—in other words, to surrogate—possibly as a means of reducing cognitive dissonance arising from inconsistency between beliefs and actions. He demonstrates that allowing people to provide narrative explanations for their decisions reduces the amount of operational distortion observed under an incentive compensation scheme, and also reduces surrogation. He also finds that the effect is larger for people who have a high preference for consistency, which supports the argument that surrogation is due to an attempt to reduce cognitive dissonance. Robert Bloomfield had proposed a link between cognitive dissonance and surrogation in an earlier paper.\n\nIn a subsequent study, Paul Black, Tom Meservy, William Tayler, and Jeff Williams show that surrogation can occur simply when a measure is provided to managers, even if they do not receive incentive compensation based on the measure. That is, if managers know that something is being measured, they will begin to surrogate on that measure, \"even if they are told that the measure is no more nor less important that other measures when determining their compensation.\" This implies that firms must be careful in determining what measures are communicated to managers, as managers may surrogate on a measure just because they hear that it is being measured.\n\nWilliam Tayler discusses everyday examples of surrogation and incentive compensation on BYU News Radio.\n\nSurrogation is conceptually related to Plato's Allegory of the Cave in that people are failing to distinguish the shadow (i.e. the measure) from the form (i.e. the construct).\n\nSurrogation is also related to Baudrillard's concept of simulacra, in his order-of-simulacra theory. The connection to this concept is discussed in Macintosh, Shearer, Thornton and Welker (2000).\n"}
{"id": "30315445", "url": "https://en.wikipedia.org/wiki?curid=30315445", "title": "Systematic process", "text": "Systematic process\n\nA systematic process is often closely associated with critical thinking.\n\nIn general the application of a systematic process is regarded as a means of management aimed at reducing the number and severity of mistakes, errors and failures due to either human or technological functions involved. \n\nUse of systematic process in strategic planning has been both challenged, due to rapid change in market conditions, and advocated as a source of improvement. For example, \"Many OECD countries have a transparent and systematic process of public consultation to enhance the quality of the regulatory process by guaranteeing that the impact on citizens and businesses is taken into account.\"\n"}
{"id": "55008561", "url": "https://en.wikipedia.org/wiki?curid=55008561", "title": "The NeuroGenderings Network", "text": "The NeuroGenderings Network\n\nThe NeuroGenderings Network is an international group of researchers in neuroscience and gender studies. Members of the network study how the complexities of social norms, varied life experiences, details of laboratory conditions and biology all interact to affect the results of neuroscientific research. Working under the label of \"neurofeminism\", they aim to critically analyze how the field of neuroscience operates, and to build an understanding of brain and gender that goes beyond gender essentialism while still treating the brain as fundamentally material. Its founding was part of a period of increased interest and activity in interdisciplinary research connecting neuroscience and the social sciences.\n\nThe group, comprising critical gender, feminist, and queer scholars, formed to tackle \"neurosexism\" as defined by Cordelia Fine in her 2010 book \"Delusions of Gender\" \"uncritical biases in [neuroscientific] research and public perception, and their societal impacts on an individual, structural, and symbolic level.\"\n\nBy contrast they practice \"neurofeminism\" to critically evaluate heteronormative assumptions of contemporary brain research and examine the impact and cultural significance of neuroscientific research on society's views about gender. The network advocate greater emphasis to be placed on neuroplasticity rather than biological determinism.\n\nIn March 2010, the first conference – NeuroGenderings: Critical Studies of the Sexed Brain – was held in Uppsala, Sweden. Organisers Anelis Kaiser and Isabelle Dussauge described its long terms goals \"to elaborate a new conceptual approach of the relation between gender and the brain, one that could help to head gender theorists and neuroscientists to an innovative interdisciplinary place, far away from social and biological determinisms but still engaging with the materiality of the brain.\" The NeuroGenderings Network was established at this event, with the group's first results published in a special issue of the journal \"Neuroethics\".\n\nFurther conferences have since been held on a biennial basis: NeuroCultures — NeuroGenderings II, September 2012 at the University of Vienna's physics department; NeuroGenderings III – The First International Dissensus Conference on Brain and Gender, May 2014 in Lausanne, Switzerland; and NeuroGenderings IV in March 2016, at Barnard College, New York City.\n\nThe members of the NeuroGenderings Network are:\n\nNon-network member, Simon Baron-Cohen, conducted a test on newborns – on average a day-and-a-half old – to see if they were more interested a human face (social object) or a mobile (physical-mechanical object). Baron-Cohen concluded that the results, based on the eye-gaze of the babies, \"showed that the male infants showed a stronger interest in the physical-mechanical mobile while the female infants showed a stronger interest in the face\" and the \"research clearly demonstrates that sex differences are in part biological in origin.\"\n\nNetwork member, Cordelia Fine, criticized the experiment as being flawed for a number of reasons including: the expectations of the experimenter (graduate student, Jennifer Connellan), for example, unconsciously moving her head more / smiling more for a female baby. In a review of Fine's book Baron-Cohen \"we included a panel of independent judges coding the videotapes of just the eye-region of the baby’s face, from which it is virtually impossible to judge the sex of the baby.\" In response Fine argued, \"clearly, if the behaviour of the experimenter-cum-stimulus has already inadvertently influenced the babies’ eye gaze behaviour, the introduction of sex-blind judges is to close the stable door after the horse has bolted,\" and fellow network member Gina Rippon added, \"the potential for misuse of our [neuroscientists] research should make us doubly careful of how (and why) we do research into these kinds of arenas, and how we report our findings. [...] Whether we like it or not, science and politics do mix.\"\n\nIn an fMRI study into the gender differences when processing fear, Schienle \"et al.\" (who are not part of The NeuroGenderings Network) began with an hypothesis that women would show a greater emotional response than men. In fact, the results showed the opposite. The researchers' explanation for the unexpected response pattern was that it \"may reflect greater attention from males to cues of aggression in their environment.\"\n\nNetwork member Robyn Bluhm criticized the initial hypothesis, that women would show a greater emotional response than men, as stereotyping. She also criticized the researchers' explanation for the unexpected results, as they discounted the possibility that men may be more sensitive to fear than women.\n\nAn article by Tracey Shors found \"sex differences in emotional and cognitive responses in rats\" and she advised that her findings \"be used to better understand and promote mental health in humans\" as \"a greater appreciation of our [sex] differences can only enhance our ability to treat our common afflictions.\"\n\nBy comparison, the research carried out by network member Daphna Joel \"et al.\" found that \"between 55% and 70% of people had a “mosaic” of gender characteristics, compared to less than one per cent who had only “masculine” or only “feminine” characteristics.\"\n\n\n\n\n\nBelow is a list of works which cause the network concern due to their \"neurodeterminist notions of a ‘sexed brain’ [which] are being transported into public discourse without reflecting the biases in empirical work.\"\n"}
{"id": "5767900", "url": "https://en.wikipedia.org/wiki?curid=5767900", "title": "Total maximum daily load", "text": "Total maximum daily load\n\nA Total Maximum Daily Load (TMDL) is a regulatory term in the U.S. Clean Water Act, describing a plan for restoring impaired waters that identifies the maximum amount of a pollutant that a body of water can receive while still meeting water quality standards.\n\nThe Clean Water Act requires that state environmental agencies complete TMDLs for impaired waters and that the United States Environmental Protection Agency (EPA) review and approve / disapprove those TMDLs. Because both state and federal governments are involved in completing TMDLs, the TMDL program is an example of cooperative federalism. If a state doesn't take action to develop TMDLs, or if EPA disapproves state-developed TMDLs, the EPA is responsible for issuing TMDLs. EPA published regulations in 1992 establishing TMDL procedures. Application of TMDLs has broadened significantly in the last decade to include many watershed-scale efforts, including the Chesapeake Bay TMDL. TMDLs identify all point source and nonpoint source pollutants within a watershed.\n\nThe Clean Water Act requires states to compile lists of water bodies that do not fully support beneficial uses such as aquatic life, fisheries, drinking water, recreation, industry, or agriculture; and to prioritize those water bodies for TMDL development. These inventories are known as 303(d) Lists and characterize waters as fully supporting, impaired, or in some cases threatened for beneficial uses.\n\nBeneficial use determinations must have sufficient credible water quality data for TMDL planning.\n\nThroughout the U.S., data are often lacking adequate spatial or temporal coverage to reliably establish the sources and magnitude of water quality degradation.\n\nTMDL planning in large watersheds is a process that typically involves the following steps: \n\nThe purpose of water quality targets is to protect or restore beneficial uses and protect human health. These targets may include state/federal numerical water quality standards or narrative standards, i.e. within the range of \"natural\" conditions. Establishing targets to restore beneficial uses is challenging and sometimes controversial. For example, the restoration of a fishery may require reducing temperatures, nutrients, sediments, and improving habitat.\n\nNecessary values for each pollutant target to restore fisheries can be uncertain. The potential for a water body to support a fishery even in a pristine state can be uncertain.\n\nCalculating the TMDL for any given body of water involves the combination of factors that contribute to the problem of nutrient concentrated runoff. Bodies of water are tested for contaminants based on their intended use. Each body of water is tested similarly but designated with a different TMDL. Drinking water reservoirs are designated differently from areas for public swimming and water bodies intended for fishing are designated differently from water located in wildlife conservation areas. The size of the water body also is taken into consideration when TMDL calculating is undertaken. The larger the body of water, the greater the amounts of contaminants can be present and still maintain a Margin of Safety. The \"Margin of Safety (MOS)\" is numeric estimate included in the TMDL calculation, sometimes 10% of the TMDL, intended to allow a safety buffer between the calculated TMDL and the actual load that will allow the water body to meet its beneficial use (since the natural world is complex and several variables may alter future conditions). TMDL is the end product of all point and non-point source pollutants of a single contaminant. Pollutants that originate from a point source are given allowable levels of contaminants to be discharged; this is the \"Waste Load Allocation (WLA).\" Nonpoint source pollutants are also calculated into the TMDL equation with \"Load Allocation (LA).\"\n\nThe calculation of a TMDL is as follows:\nwhere WLA is the waste load allocation for point sources, LA is the load allocation for nonpoint sources, and MOS is the margin of safety.\n\nLoad allocations are equally challenging as setting targets. Load allocations provide a framework for determining the relative share of natural sources and human sources of pollution.\n\nThe natural background load for a pollutant may be imprecisely understood. Industrial dischargers, farmers, land developers, municipalities, natural resource agencies, and other watershed stakeholders each have a vested interest in the outcome.\n\nTo implement TMDLs with point sources, wasteload allocations are incorporated into discharge permits for these sources. The permits are issued by EPA or delegated state agencies under the National Pollutant Discharge Elimination System (NPDES). Nonpoint source discharges (e.g. agriculture) are generally in a voluntary compliance scenario. The TMDL implementation plan is intended to help bridge this divide and ensure that watershed beneficial uses are restored and maintained. Local watershed groups play a critical role in educating stakeholders, generating funding, and implementing projects to reduce nonpoint sources of pollution.\n\n\n\n"}
{"id": "44379844", "url": "https://en.wikipedia.org/wiki?curid=44379844", "title": "Useful Jew", "text": "Useful Jew\n\nThe term useful Jew was used in various historical contexts, typically describing a Jewish person useful in implementing an official authority's policy, sometimes by oppressing other Jews.\n\n\n"}
{"id": "51672", "url": "https://en.wikipedia.org/wiki?curid=51672", "title": "Vacuous truth", "text": "Vacuous truth\n\nIn mathematics and logic, a vacuous truth is a statement that asserts that all members of the empty set have a certain property. For example, the statement \"all cell phones in the room are turned off\" will be true whenever there are no cell phones in the room. In this case, the statement \"all cell phones in the room are turned \"on\"\" would also be vacuously true, as would the conjunction of the two: \"all cell phones in the room are turned on \"and\" turned off\".\n\nMore formally, a relatively well-defined usage refers to a conditional statement with a false antecedent. One example of such a statement is \"if Uluru is in France, then the Eiffel Tower is in Bolivia\". Such statements are considered vacuous because the fact that the antecedent is false prevents using the statement to infer anything about the truth value of the consequent. They are true because a material conditional is defined to be true when the antecedent is false (regardless of whether the conclusion is true).\n\nIn pure mathematics, vacuously true statements are not generally of interest by themselves, but they frequently arise as the base case of proofs by mathematical induction. This notion has relevance in pure mathematics, as well as in any other field which uses classical logic.\n\nOutside of mathematics, statements which can be characterized informally as vacuously true can be misleading. Such statements make reasonable assertions about qualified objects which do not actually exist. For example, a child might tell their parent \"I ate every vegetable on my plate\", when there were no vegetables on the child's plate to begin with.\n\nA statement formula_1 is \"vacuously true\" if it resembles the statement formula_2, where formula_3 is known to be false.\n\nStatements that can be reduced (with suitable transformations) to this basic form include the following universally quantified statements:\n\nVacuous truth most commonly appears in classical logic, which in particular is two-valued. However, vacuous truth also appears in, for example, intuitionistic logic in the same situations given above. Indeed, if formula_3 is false, formula_2 will yield vacuous truth in any logic that uses the material conditional; if formula_3 is a necessary falsehood, then it will also yield vacuous truth under the strict conditional.\n\nOther non-classical logics (for example, relevance logic) may attempt to avoid vacuous truths by using alternative conditionals (for example, the counterfactual conditional).\n\nThese examples, one from mathematics and one from natural language, illustrate the concept:\n\n\"For any integer x, if x > 5 then x > 3.\" – This statement is true non-vacuously (since some integers are greater than 5), but some of its implications are only vacuously true: for example, when x is the integer 2, the statement implies the vacuous truth that \"if 2 > 5 then 2 > 3\".\n\n\"All my children are cats\" is a vacuous truth when spoken by someone without children.\n\n\n\n"}
{"id": "32356", "url": "https://en.wikipedia.org/wiki?curid=32356", "title": "Vexatious litigation", "text": "Vexatious litigation\n\nVexatious litigation is legal action which is brought, regardless of its merits, solely to harass or subdue an adversary. It may take the form of a primary frivolous lawsuit or may be the repetitive, burdensome, and unwarranted filing of meritless motions in a matter which is otherwise a meritorious cause of action. Filing vexatious litigation is considered an abuse of the judicial process and may result in sanctions against the offender.\n\nA single action, even a frivolous one, is usually not enough to raise a litigant to the level of being declared vexatious. Rather, a pattern of frivolous legal actions is typically required to rise to the level of vexatious. Repeated and severe instances by a single lawyer or firm can result in eventual disbarment.\n\nSome jurisdictions have a list of vexatious litigants: people who have repeatedly abused the legal system. Because lawyers could be disbarred for participating in this abuse of the legal process, vexatious litigants are often unable to retain legal counsel, and such litigants therefore represent themselves in court. Those on the vexatious litigant list are usually either forbidden from any further legal action, or are required to obtain prior permission from a senior judge before taking any legal action. The process by which a person is added to the list varies among jurisdictions. In liberal democratic jurisdictions, declaring someone a vexatious litigant is considered to be a serious measure and rarely occurs, as judges and officials are reluctant to curtail a person's access to the courts.\nThese legal actions occur in some countries of the former British Empire, where the Common Law System still remains: Australia, Canada, Ireland, New Zealand, UK, and US, which are specified below. The Civil (Codified/Continental) Law does not provide this kind of gate to limit equal access of citizens to the justice system.\n\nThe concept of vexatious litigation entered into law in 1896 with the Vexatious Actions Act, enacted in England and soon extended to Scotland and Ireland. This was primarily a response to the actions of Alexander Chaffers, a solicitor who filed numerous actions against leading members of Victorian society. When costs were awarded against him, he failed to pay.\n\nThe first such law outside Britain, the Supreme Court Act, 1927 was passed in Australia nearly thirty years later. This too was prompted by the behaviour of an individual, Rupert Millane. The first Vexatious Litigant law in the United States was enacted in California in 1963. By 2007 five US states had passed similar legislation: California, Florida, Hawaii, Ohio, and Texas.\n\nThe High Court of Australia has declared only three people to be vexatious litigants in its century-old existence, whereas the Australian Federal Court system, established in 1976, has at least 36 names on its barred registry.\n\nIn New South Wales, there are currently 35 people on the New South Wales Supreme Court's vexatious litigants register.\n\nIn Queensland, the process for having someone declared a vexatious litigant is governed by the \"Vexatious Proceedings Act 2005\", which supplanted an earlier Act. Importantly, the Act defines a vexatious proceeding to include a proceeding brought without merit or any prospect of success, with the consequence that it is not necessary to prove the existence of any improper motive in order to obtain relief under the Act.\n\nIn South Australia, vexatious litigation laws were enacted in the mid-1930s with the \"Supreme Court Act 1935-1936\", following similar laws enacted in Victoria. In 2010 the Rann government acted to strengthen the ability of the courts to act against vexatious litigants by \"increasing the range of courts and tribunals that can declare people as vexatious\". Prior to that date, few people had been banned from bringing litigation to South Australian courts – by 2005, only two people were listed as having been declared as vexatious litigants, the first in 1997 and the second declared during that year. Subsequently, at least two other South Australians have been found to be vexatious litigants.\n\n, only 13 people—including convicted mass-murderer Julian Knight—had been declared vexatious litigants since the law was introduced in 1930.\n\nLegislation has existed since 1930, but is under review as of limited use.\n\nUnder the \"Constitution Act, 1867\", section 92(14), each province is vested with the power to enact and apply laws relating to the administration of justice within its own territory.\n\nIn Canada, Section 40 of the \"Federal Court Act\" and in Ontario Section 140 of the \"Courts of Justice Act\", restrict the ability to introduce or continue proceedings for those who have instituted vexatious proceedings or conducted proceedings in a vexatious manner.\n\nIn Quebec, the Code of Civil Procedure is the principal legislation that sets rules related to civil procedure.\n\nUnder section 46 of the Code of Civil Procedure, all judicial courts and judges in Quebec are vested with \"...all the powers necessary for the exercise of their jurisdiction\". Furthermore, they may:\n\"…at any time and in all matters, whether in first instance or in appeal, issue orders to safeguard the rights of the parties, for such time and on such conditions as they may determine. As well, they may, in the matters brought before them, even on their own initiative, issue injunctions or reprimands, suppress writings or declare them libellous, and make such orders as are appropriate to deal with cases for which no specific remedy is provided by law.\"\nSection 46 vests a very broad power on judicial courts and judges to ensure that the administration of justice is conducted according to decorum and according to the remedial nature of justice. As the courts's decisions have shown it, the authority to declare a litigant as vexatious is directly tributary to the power conferred by section 46.\n\nCases illustrating the application of section 46 are numerous. Among them, there are: \"Nguiagain v. Commission de la fonction publique\", in which the judge rejected the plaintiff's motion for a \"mandamus\" to enjoin his union to revise the grievance that he had filed on the grounds that the motion was groundless and abusive; \"De Niverville c. Descôteaux\", where an injunction was rendered declaring the respondent, disbarred lawyer Descôteaux, as a vexatious litigant due to the multiple unfounded and frivolous actions that he had sought against the plaintiff De Niverville; and in \"Fabrikant v. Corbin\", a motion to declare the plaintiff Valery Fabrikant as a vexatious litigant was granted to the defendant, Dr. Corbin. In all of the above cited cases, a litigant was only declared vexatious following a proceeding instated by the opposite party.\n\nMoreover, section 46’s scope is limited to judicial courts and judges. Administrative tribunals are legislative creations and they can only exist and function within the limits that are imposed by law. Administrative tribunals in Quebec cannot declare a person a vexatious litigant.\n\nAs per section 90 of the \"Rules of Practice of the Superior Court of Québec in Civil Matters\", such litigants are now indexed in a registry kept by the Chief Justice in the judiciary district of Montreal. Lawyer and author Claude Duchesnay has reported in May 2003 that a document on the Quebec attorney general’s intranet contains the name of 58 persons who must obtain permission prior to instating proceedings before the courts.\n\nIn Ireland, a court may, of its own motion or on application, order that no proceedings, either of a certain type or at all, may be issued by a certain person without leave of that court or some other court, for a specified time, or indefinitely. Such an order is referred to in legal circles as an Isaac Wunder order after Isaac Wunder who made several claims against the Hospitals Trust claiming sweepstakes prizes, but the claims were found to be groundless and the case deemed frivolous or vexatious. He was prohibited from taking further High Court proceedings in the action without leave of the court.\n\nIn New Zealand a person may be declared a vexatious litigant by a High Court Judge on the application of the Attorney-General. A vexatious litigant must then apply to a High Court Judge for leave to commence any action. A decision by the High Court whether or not to grant leave cannot be appealed.\n\nCourts in England and Wales have the means of escalating the sanctions against a litigant who makes applications to the court that are \"totally without merit\": Civil restraint orders allow courts to forbid applications for court hearings without the permission of a judge. There are three types of CRO: limited, extended and general, with different scopes of application. Further applications totally without merit can lead to withdrawal of the right of appeal. Harassment of the court and court officials can lead to a penal prohibition notice, prohibiting the litigant from contacting or approaching the court without permission.\n\nHM Courts Service maintains a list of vexatious litigants.\n\nUnder the Vexatious Actions (Scotland) Act 1898 the Lord Advocate can apply for an order under section 1 of that Act to prevent any person accused of vexatious litigation from raising any and all legal proceedings \"unless he obtains the leave of a judge sitting in the Outer House on the Bills in the Court of Session, having satisfied the judge that such legal proceeding is not vexatious.\" A list of people who have had such an order brought against them is published on the Scottish Courts website. As of June 2018 there are ten names on this list.\n\nUnder California law a vexatious litigant is someone who does any of the following, most of which require that the litigant be proceeding \"pro se\", i.e., representing himself:\n\nAppeals of an existing action do not count as “final determinations”. Appeals and writs that are related to a current action do not count as “final determinations” or additional determinations, because until all avenues of appeal have been exhausted the determinations cannot be construed as “final”. A judgment is final for all purposes when all avenues for direct review have been exhausted. Interlocutory decisions before a judgment cannot be considered “final determinations”. Docket lists show nothing about qualifying merit of interim motions (Id.)\n\nTo meet the unspecified criteria for \"repeated\" motions or litigations, the number must be much more than two, and the rule based on case law seems to be around 12. \"While there is no bright line rule as to what constitutes “repeatedly,” most cases affirming the vexatious litigant designation involve situations where litigants have filed dozens of motions either during the pendency of an action or relating to the same judgment.\" \n\nRepeated motions must be \"so devoid of merit and be so frivolous that they can be described as a flagrant abuse of the system, have no reasonable probability of success, lack reasonable or probable cause or excuse, and are clearly meant to abuse the processes of the courts and to harass the adverse party than other litigants.\" Evidence that a litigant is a frequent plaintiff or defendant alone is insufficient to support a vexatious litigant designation. The moving party, in addition to demonstrating that the plaintiff is vexatious, must make an affirmative showing based on evidence that the case has little chance of prevailing on the merits. If the plaintiff is so determined, a bond may be required, and if the bond requirement is not met within a specified time period, a judgment of dismissal is ordered. A finding of vexatiousness is not an appealable order, but a dismissal for failure to post a bond requirement based on a judgment of vexatiousness is appealable.\n\nHabeas petitions do not count towards vexatious litigant determination. Vexatiousness in Probate Actions are governed by a different standard (Cal. Prob. Code §1611).\n\n"}
{"id": "5276029", "url": "https://en.wikipedia.org/wiki?curid=5276029", "title": "Víctor Grippo", "text": "Víctor Grippo\n\nVictor Grippo (10 May 1936 – February 2002) was an Argentine painter, engraver and sculptor, considered the father of conceptual art in Argentina. He was born in Junín, province of Buenos Aires, the elder of two sons of an Italian immigrant father and an Argentine mother of Albanese origin. \n\nGrippo was raised in his birthtown, and in his youth he moved to the capital of the province, La Plata, and then to the city of Buenos Aires. He studied Chemistry and Pharmacy in the University of La Plata, and attended seminars by Héctor Cartier at the Fine Arts School. He had been interested in art from an early age. He started off as a painter and engraver in the 1950s, and in the 1960s he began experimenting with sculpture, and produced animated pieces (with engines and lighting).\n\nIn 1966 he held his first solo exhibition at the Lirolay gallery in Buenos Aires, presenting oils linked to geometric abstraction.\n\nIn his work he has sought a convergence between science and art. His work, one of whose main motives is the idea of transformation, has always revolved around daily life, the world of work, food and energy. From the beginning he used materials and unconventional means in his objects, sculptures and installations, to reflect on the social and spiritual conditions of the workers, of the artists. It belonged to the group of the thirteen, integrated by Jacques Bedel, Luis Fernando Benedit, Gregorio Dujovny, among others. Finally, this group becomes the CAYC Group. \n\nBetween the years of 1968 and 1970 his works led to a luminous kinetic, machines were wrapped in fabrics, subjected to the surface passage of light.\n\nVictor Grippo researches the energy generation processes through very simple technical resources. One of the raw materials used in his work has been the potato (which he uses as a metaphor) food that was born in America, but after the conquest spread throughout Europe. \n\nGrippo, based on its cultural symbolism, used the energy contained in potatoes, forming electric batteries and connecting them with cables to operate different devices, from a radio to a multimeter that measured the energy generated. That kind of works became a classic of the artist who was developing in different facilities around the world.\n\nUsing electrodes and measuring systems, the artist shows the latent power in plant life.\n\nHe creates facilities that are at the same time spaces of liberated energy, behind these works is also found a meaning referring to alchemy and to the efforts to transfigure the material world.\n\nOne of his most well-known works is Analogía I (1970), presented in the collective exhibition \"Art Systems I\", at the Museum of Modern Art, and which has been presented later in different versions in different parts of the world.\n\nIn 2002 Konex Foundation from Argentina granted him the Diamond Konex Award, one of the most prestigious awards in Argentina, as the most important personality in the Visual Arts of his country in the last decade.\n\nHe made invaluable calculations for quantum physics, relating the atoms and the theories of the absorbent nebula in a totally renewing way with conceptual art, thus achieving a kind of tactile hologram.\n\nThe works of this artist no longer speak of the power that technology has to transform the lives of people, but rather of its articulation as an instrument of thought.\n\nGrippo died in Buenos Aires in February 2002.\n\n\n\n\n"}
{"id": "33407069", "url": "https://en.wikipedia.org/wiki?curid=33407069", "title": "Womance", "text": "Womance\n\nA womance, also called sismance, is a close but non-sexual relationship between two or more women. It is an exceptionally tight affectional, homosocial female bonding relationship exceeding that of usual friendship, and is distinguished by a particularly high level of emotional intimacy.\n\nThe word \"womance\" is a portmanteau of the words \"woman\" and \"romance\". The emergence of the terms bromance and womance has been seen as reflecting increased relationship-seeking as a modern behavior. Although womance is sometimes seen as the female flip side of \"bromance\", some have seen different nuances in the social construction of the two concepts. Hammarén sees \"different values assigned to male and female friendships\" and a dissimilarity in the \"underlying power relation between the concepts\", and Winch has asserted several differences in the social construction.\n\nStephanie and Care are great of film womances seem to be less prevalent than bromances. \"In Her Shoes\" (2005), \"Baby Mama\" (2008), \"The Women\" (2008), \"Bride Wars\" (2009), \"The Sisterhood Of The Travelling Pants\" (2005), \"Desatanakkili Karayarilla\" (1986), and \"Bridesmaids\" (2011) have been seen as womances, and their characteristics and tropes discussed. Winch expands on the assertion that \"The womance can be distinguished from earlier friendship films because of its focus on the female self as entrepreneurial self-project.\" She sees differences from bromance, in \"practices of consumption and hypervisability differentiates their togetherness from the togetherness of the buddies of the bromance\" as well as dissimilar themes - girlfriend competition, female solidarity in the face of concerns about economic security and bridezilla behavior.\n\nThe Australian feature film \"Jucy\" (2010) is billed as a \"womantic comedy\". \"Frances Ha\" (2013) has been seen as a character study, with two close female protagonists, who \"have quite a womance going\".\n\nSeveral current TV series feature notable \"womances\" as well as the TV series \"Laverne and Shirley\" and \"Mel & Sue\".\nThe first all-female podcast is a comedy duo from Brisbane.\n\nIt has also been used to describe the real life friendship between celebrities.\n\n"}
{"id": "6920598", "url": "https://en.wikipedia.org/wiki?curid=6920598", "title": "Xtracycle", "text": "Xtracycle\n\nXtracycle is the name of a company and the name commonly used for the variety of load-carrying bicycle, a longtail or a longbike, that results from use of the company's products: the FreeRadical kit, complete long-frame bicycles and associated accessories. Web forums and blogs often use the shorthand \"Xtrabike\", \"Xtra\", or simply \"X\" to refer to either the FreeRadical extension or the entire extended bicycle. An Xtracycle may be constructed by modifying an existing bicycle with a Free Radical extension, or by custom-building an extended-tail bicycle frame.\n\nWhile the Xtracycle is based on a standard hardtail diamond frame with 26-inch or 622 mm (700c) wheels, the Xtracycle differs from other load-carrying bicycles in that it does not employ a handlebar basket, panniers, or a bicycle trailer. Baskets are easy to attach and allow stowing of cargo in plain view. Panniers are often watertight or water resistant, and can be easily removed from their racks and carried as luggage. While trailers come in many designs and allow a bicycle to pull a significant amount of cargo, an Xtracycle allows for more stowage of large containers directly on the extended frame of the bicycle itself. Xtracycles are also called \"sport utility bicycles\" for their ability to carry larger loads than a normal bicycles having baskets or panniers, while also maintaining the \"sporty\" maneuverability of conventional \"short\" bikes.\n\nThe FreeRadical was conceived by Ross Evans at Stanford University and developed during his work in the mid-1990s managing a \"Bikes Not Bombs\" project in Nicaragua, where having a bicycle enhances a person's employment opportunities. In 1998 Evans and his friend Kipchoge Spencer created Xtracycle Inc to manufacture and market the invention, as well as a nonprofit organization, Worldbike, devoted to encourage a bicycle-centric lifestyle and culture.\n\nDespite the fact that the FreeRadical qualifies as an aftermarket bike accessory, its growing acceptance has sparked an Xtracycle aftermarket not formally connected with Xtracycle Inc: varieties of specialized kickstands, electric-assist motors, and even bike-mounted blenders have come to market, even though each requires the prior purchase of a FreeRadical or other Xtracycle-compatible frame to function properly.\n\nXtracycle Inc has worked with various bicycle manufacturers to build purpose-built extended bicycles compatible with their accessories. The first to actually produce and market an integrated Xtracycle frameset was Surly Bikes with the Big Dummy. XInc continues to form similar covenants with manufacturers in all price ranges, with the goal of making the Xtracycle less of a niche product and more mainstream. XInc is also working on FreeRadical attachments sized for children's and youths' bicycles on the theory of \"catch 'em young and train 'em right.\"\n\nOther applications for the FreeRadical have included linking two Xtracycles to support a mobile stage for use in parades and street fairs, and a computerized chalkpowder-printer device mounted on an Xtracycle that leaves a dot-matrix trail of messages on the street.\n\nIn 2008, Xtracycle put their longtail bike frame specifications online. as part of their project to open source their longtail frame design. They’ve created a Longtail Standard and logoing to allow vendors to design their products to fit in the Xtracycle FreeRadical ecosystem. The \"Longtail Technology\" logo can be used on bikes, accessories or packaging. The open sourcing of the patented technology was meant to stimulate the cargo bike movement, while developing a standard for \"longtail\" frames and accessories. Several frame and accessory makers have adopted the standards, while others have developed competing and incompatible long-frame cargo bike designs.\nHowever, the documents are no longer freely available, and now require an agreement with Xtracycle first.\n\nThe FreeRadical is an extender for a bicycle.\n\nIn 2009 the Radish was launched by Xtracycle. It is a production long-tail bicycle with a low-standover height frame and matching FreeRadical.\n\nIn 2013 the EdgeRunner is a second generation cargo bicycle with a 20\" rear wheel. The EdgeRunner has been called the \"Best longtail ever. No contest.\"\n\nIn 2013 the CargoJoe is a folding cargo bike developed in a partnership between Xtracycle and Tern.\n\nIn 2011 Xtracycle created a sidecar for cargo transport that can carry up to 250 pounds.\n\n\n"}
