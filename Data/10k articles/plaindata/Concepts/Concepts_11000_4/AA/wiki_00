{"id": "49415112", "url": "https://en.wikipedia.org/wiki?curid=49415112", "title": "Amy Barbour-James", "text": "Amy Barbour-James\n\nAmy Barbour-James (25 January 1906 – 4 May 1988) was a London-born Guyanese Black civil rights activist and civil servant.\n\nCaroline Amy Aileen Barbour-James was born in Acton, London, on 25 January 1906 to Guyanese parents, John and Caroline Barbour-James, one of their eight children. The Barbour-James family were a middle-class family who lived in west London in the early 20th century. Her father, John Barbour-James worked as administrator in West Africa and had access to a large network of contacts throughout the continent. In 1918, he founded the African Patriotic Intelligence Bureau.\n\nInspired by her father, Barbour-James became active in the civil rights movements and was involved in the African Progress Union and the League of Coloured Peoples, becoming secretary of later organisation in 1942.\n\nIn 2011, a short drama based on Barbour-James's life was broadcast by BBC Radio 4.\n\nBarbour-James died in Harrow on 4 May 1988.\n"}
{"id": "1359360", "url": "https://en.wikipedia.org/wiki?curid=1359360", "title": "Ansatz", "text": "Ansatz\n\nIn physics and mathematics, an ansatz (; , meaning: \"initial placement of a tool at a work piece\", plural ansätze ; or ansatzes) is an educated guess that is verified later by its results.\n\nAn ansatz is the establishment of the starting equation(s), the theorem(s), or the value(s) describing a mathematical or physical problem or solution. It can take into consideration boundary conditions. After an ansatz has been established (constituting nothing more than an assumption), the equations are solved for the general function of interest (constituting a confirmation of the assumption).\n\nGiven a set of experimental data that looks to be clustered about a line, a linear ansatz could be made to find the parameters of the line by a least squares curve fit. Variational approximation methods use ansätze and then fit the parameters.\n\nAnother example could be the mass, energy, and entropy balance equations that, considered simultaneous for purposes of the elementary operations of linear algebra, are the \"ansatz\" to most basic problems of thermodynamics.\n\nAnother example of an ansatz is to suppose the solutions of a homogeneous linear differential equation and difference equation to have, respectively, exponential and power form. More generally, one can guess a particular solution of a system of equations and test such an ansatz by direct substitution of the solution in the system of equations.\n\n"}
{"id": "48834853", "url": "https://en.wikipedia.org/wiki?curid=48834853", "title": "Anti-information", "text": "Anti-information\n\nInformation is that which reduces uncertainty, wholly or in part.\nSimilarly, anti-information is that which increases uncertainty.\nIt is negative information.\n\nNoise on a noisy communication channel is an example of anti-information.\nAccording to Shannon's Channel Coding Theorem\nthe entropy of the noise must be subtracted to obtain the channel capacity\nthat remains available for reliable communication.\n\nThe gambling industry has made a business out of selling anti-information.\nPeople are willing to pay for anti-information.\nThe increase in uncertainty enables them to savor the information that they subsequently receive\nwhen the uncertainty is finally resolved.\n\nThe term anti-information was introduced by Koos Verhoeff in the 1970s\nwhile teaching informatics at the Erasmus University Rotterdam.\n"}
{"id": "30577937", "url": "https://en.wikipedia.org/wiki?curid=30577937", "title": "Attractiveness principle", "text": "Attractiveness principle\n\nAttractiveness Principle is one of System Dynamics archetypes. System archetypes describe common patterns of behavior in dynamic complex systems. Attractiveness principle is a variation of Limits to Growth archetype, with restrictions caused by multiple limits. The limiting factors here are each of different character and usually cannot be dealt with the same way and/or (and very likely) they cannot be all addressed.\n\nAttractiveness principle is a concept that incorporates the fact that any product or kind of business cannot ever be \"“all things to all people”\" though companies very often strive to follow this way. One needs to make necessary decisions on the characteristics of the product as it cannot be perfect in all dimensions. If she doesn’t, the product is not going to be successful as of the natural constraints (limited resources) it will have to face – sooner or later. It is a fact of life that (assuming we know the relationships among the system’s elements) we can influence, inhibit or remove some of these limits through making expert changes in the system. The archetype can help us to get the insight into the system behavior so we could identify and decide which limiting factors to inhibit before they inhibit the results we want to achieve. But there will always be some limits we are not able to reduce and simply “we have to learn to live with them” and make compromises between our goals.\n\nKnowledge of the attractiveness principle system archetype is essential in management of various projects and businesses. Managers decide which problem is more attractive in terms of possible future improvement of the company – the origin of the archetype’s name actually came from this point. Manager as a decision-maker needs efficient support in solving such complex problems, and system dynamics can play this role. Its main advantage is the ability to reach higher complexity and to provide simultaneous calculations. These can be used to determine the future possible behavior of the system. If managers are able to recognize the archetype in a problem it often helps them to solve it with less cost and they are also often able to change its structure.\n\nThe term attractiveness principle was first used by inventor of system dynamics Jay W. Forrester. According to Forrester, the only way to control growth is to control attractiveness. Other references on this topic can be found in The Systems Thinker and in The Fifth Discipline Fieldbook in articles and parts by Michael Goodman and Art Kleiner.\n\nThe system is made up of a reinforcing loop and at least two balancing loops. See causal loop diagram and stock and flow diagram for the insight into model fundamentals.\n\nThe Reinforcing Loop (R1 in Figure 1 and 2) represents accelerating growth – a growing action is producing results. This is a positive feedback loop – the more the growing action taken, the higher the results level, and yet the result itself produces even more of growing action.\nBalancing loops (B2 and B3 in Figure 1 and 2) represent the way the system turns back to its original state. Result produced in the reinforcing loop is influenced within the balancing loop. There are (at least) two limits causing the slowing actions in the system and adding to them. Limiting actions start to influence the system at various levels of results, generally. Since that moment slowing actions act in the system simultaneously. Both the slowing actions contribute to the total slowing action. Total slowing action then inhibits the results (this process is delayed in time).\nIf we get back to the reinforcing loop then we can see the inhibited results are reducing growing action which is leading to the reduced results again.\n\nTrade-offs must be calculated to decide which ones of limits to focus on and address first. The one that is more attractive in terms of future benefit to the results should be chosen to be dealt with. It is necessary to compare the future situations after removing each of the slowing actions and their values in terms of reaching the desired result. But not only the one that will have a greater impact should be chosen but a possible synergetic effect when removing interdependent limits should be considered when making a decision.\n\nGraphs in Figure 3 and 4 show the results of simulation in Simgua simulation tool.\n\nYou can also run the Simgua Attractiveness principle model.\n\nThere is a project with a negative impact of a risk. Some people were taken off the team, there are unexpected changes in the project content and the economic circumstances have changed, too. The indicators of its quality, schedule and costs need to be kept up. Management’s task is to allocate the resources as well as possible in terms of the project’s indicators. Due to the fact the resources are limited it is necessary to make tradeoffs among opportunities.\n\nJay W. Forrester studied the attractiveness principle of geographical areas. He states that all the places in the world tend to the equilibrium where they are all equally attractive, no matter the population class. Let attractivity be the overall rating of a city in terms of its desirability for potential inhabitants. If a city has high attractivity people move to this city, which increases the prices of housing which is getting scarce, cause overloading of job opportunities (leading to unemployment), the environmental stress is rising, city getting overcrowded etc. These changes demonstrate the impact of the movement as of an equalizing process which makes the mentioned city less and less attractive – to the (idealized) point when no one wants to move to it anymore.\nWe can illustrate this situation by Forester’s words:\nAs stated by Richard C. Duncan, using Forrester’s Word dynamics model to predict the behavior of Third World countries shows that it is not possible to stop the immigration from these countries to USA as these countries can never reach the USA’s level of geographical attractiveness (and so there always will be a tendency to immigrate).\n\nHere is a list of possible effective strategies to deal with Attractiveness principle in praxis based on.\n\nIt is important to have in mind that dynamic complexity is very often counterintuitive – cause and effect are distant in time and space, but decision-makers rather tend to look for causes “near” their effects. The solution is not to concentrate on the symptoms of the problem, but on its causes.\n\n\n"}
{"id": "19627992", "url": "https://en.wikipedia.org/wiki?curid=19627992", "title": "Benni Efrat", "text": "Benni Efrat\n\nBenni Efrat (born 1936) is an Israeli painter, sculptor, printmaker and filmmaker who was born in Beirut, Lebanon. \nBenni Efrat immigrated to Palestine in 1947. From 1959 to 1961, he studied at the Avni Institute of Art and Design in Tel Aviv under Yehezkel Streichman (1906–1993). From 1966 to 1976, he lived in London, where he studied at Saint Martin's School of Art in London. \nBenni Efrat was one of the first of Israeli Conceptual artists and influenced others in this direction (e.g., Joshua Neustein, Michael Gitlin, Buky Schwartz). His works were systems of components which spoke for themselves and sought to represent no more than the sum of their parts. In the mid-1970s his displays were accompanied by films, on the back of which the artist had painted. After settling in New York City in 1976, became involved with conceptual art, producing drawings, prints and photographs that explore energy, space and the perception in sculpture. Efrat currently lives in Belgium.\n\n\n"}
{"id": "38867", "url": "https://en.wikipedia.org/wiki?curid=38867", "title": "Bill of Rights 1689", "text": "Bill of Rights 1689\n\nThe Bill of Rights, also known as the English Bill of Rights, is an Act of the Parliament of England that sets out certain basic civil rights and clarifies who would be next to inherit the Crown. It received the Royal Assent on 16 December 1689 and is a restatement in statutory form of the Declaration of Right presented by the Convention Parliament to William III and Mary II in February 1689, inviting them to become joint sovereigns of England. The Bill of Rights lays down limits on the powers of the monarch and sets out the rights of Parliament, including the requirement for regular parliaments, free elections, and freedom of speech in Parliament. It sets out certain rights of individuals including the prohibition of cruel and unusual punishment and reestablished the right of Protestants to have arms for their defence within the rule of law. Furthermore, the Bill of Rights described and condemned several misdeeds of James II of England.\n\nThese ideas reflected those of the political thinker John Locke and they quickly became popular in England. It also sets out – or, in the view of its drafters, restates – certain constitutional requirements of the Crown to seek the consent of the people, as represented in Parliament.\n\nIn the United Kingdom, the Bill of Rights is further accompanied by Magna Carta, the Petition of Right, the Habeas Corpus Act 1679 and the Parliament Acts 1911 and 1949 as some of the basic documents of the uncodified British constitution. A separate but similar document, the Claim of Right Act 1689, applies in Scotland. The Bill of Rights 1689 was one of the inspirations for the United States Bill of Rights.\n\nAlong with the Act of Settlement 1701, the Bill of Rights is still in effect in all Commonwealth realms. Following the Perth Agreement in 2011, legislation amending both of them came into effect across the Commonwealth realms on 26 March 2015.\n\nDuring the 17th century, there was renewed interest in Magna Carta. The Parliament of England passed the Petition of Right in 1628 which established certain liberties for subjects. The English Civil War (1642–1651) was fought between the King and an oligarchic but elected Parliament, during which the idea of a political party took form with groups debating rights to political representation during the Putney Debates of 1647. Subsequently, the Protectorate (1653–1659) and the English Restoration (1660) restored more autocratic rule although Parliament passed the Habeas Corpus Act in 1679, which strengthened the convention that forbade detention lacking sufficient cause or evidence.\n\nObjecting to the policies of King James II of England (James VII of Scotland and James II of Ireland), a group of English Parliamentarians invited the Dutch stadtholder William III of Orange-Nassau (William of Orange) to overthrow the King. William's successful invasion with a Dutch fleet and army led to James fleeing to France. In December 1688, William took over the provisional government by appointment of the peers of the realm, as was the legal right of the latter in circumstances when the King was incapacitated, and summoned an assembly of certain members of parliament. This assembly called for an English Convention Parliament to be elected, which convened on 22 January 1689.\n\nThe proposal to draw up a statement of rights and liberties and James's violation of them was first made on 29 January 1689 in the House of Commons, with members arguing that the House \"cannot answer it to the nation or Prince of Orange till we declare what are the rights invaded\" and that William \"cannot take it ill if we make conditions to secure ourselves for the future\" in order to \"do justice to those who sent us hither\". On 2 February a committee specially convened reported to the Commons 23 Heads of Grievances, which the Commons approved and added some of their own. However, on 4 February the Commons decided to instruct the committee to differentiate between \"such of the general heads, as are introductory of new laws, from those that are declaratory of ancient rights\". On 7 February the Commons approved this revised Declaration of Right, and on 8 February instructed the committee to put into a single text the Declaration (with the heads which were \"introductory of new laws\" removed), the resolution of 28 January and the Lords' proposal for a revised oath of allegiance. It passed the Commons without division.\n\nOn 13 February the clerk of the House of Lords read the Declaration of Right, and the Marquess of Halifax, in the name of all the estates of the realm, asked William and Mary to accept the throne. William replied for his wife and himself: \"We thankfully accept what you have offered us\". They then went in procession to the great gate at Whitehall. The Garter King at Arms proclaimed them King and Queen of England, France, and Ireland, whereupon they adjourned to the Chapel Royal, with the Bishop of London preaching the sermon. They were crowned on 11 April, swearing an oath to uphold the laws made by Parliament. The Coronation Oath Act 1688 had provided a new coronation oath, whereby the monarchs were to \"solemnly promise and swear to govern the people of this kingdom of England, and the dominions thereunto belonging, according to the statutes in parliament agreed on, and the laws and customs of the same\". They were also to maintain the laws of God, the true profession of the Gospel, and the Protestant Reformed faith established by law. This replaced an oath which had deferred more to the monarch. The previous oath required the monarch to rule based on \"the laws and customs ... granted by the Kings of England\".\n\nThe Declaration of Right was enacted in an Act of Parliament, the Bill of Rights 1689, which received the Royal Assent in December 1689. The Act asserted \"certain ancient rights and liberties\" by declaring that:\n\nThe Act declared James' flight from England following the Glorious Revolution to be an abdication of the throne. It listed twelve of James's policies by which James designed to \"endeavour to subvert and extirpate the protestant religion, and the laws and liberties of this kingdom\". These were:\n\nIn a prelude to the Act of Settlement to come twelve years later, the Bill of Rights barred Roman Catholics from the throne of England as \"it hath been found by experience that it is inconsistent with the safety and welfare of this Protestant kingdom to be governed by a papist prince\"; thus William III and Mary II were named as the successors of James II and that the throne would pass from them first to Mary's heirs, then to her sister, Princess Anne of Denmark and her heirs (and, thereafter, to any heirs of William by a later marriage).\n\nThe Bill of Rights is commonly dated in legal contexts to 1688. This convention arises from the legal fiction (prior to the passage of the Acts of Parliament (Commencement) Act 1793) that an Act of Parliament came into force on the first day of the session in which it was passed. The Bill was therefore deemed to be effective from 13 February 1689 (New Style), or, under the Old Style calendar in use at the time, 13 February 1688. Under the Short Titles Act 1896, the Bill was given the official short title of \"The Bill of Rights\", without a calendar year suffix.\n\nThe Bill of Rights was later supplemented by the Act of Settlement 1701 (which was agreed to by the Parliament of Scotland as part of the Treaty of Union). The Act of Settlement altered the line of succession to the throne laid out in the Bill of Rights. However, both the Bill of Rights and the Claim of Right contributed a great deal to the establishment of the concept of parliamentary sovereignty and the curtailment of the powers of the monarch. Leading, ultimately, to the establishment of constitutional monarchy, while also (along with the penal laws) settling the political and religious turmoil that had convulsed Scotland, England and Ireland in the 17th century.\n\nThe Bill of Rights (1689) reinforced the Petition of Right (1628) and the Habeas Corpus Act (1679) by codifying certain rights and liberties. Described by William Blackstone as \"Fundamental Laws of England\", the rights expressed in these Acts became associated with the idea of the rights of Englishmen. The Bill of Rights directly influenced the 1776 Virginia Declaration of Rights, which in turn influenced the Declaration of Independence.\n\nAlthough not a comprehensive statement of civil and political liberties, the Bill of Rights stands as one of the landmark documents in the development of civil liberties in the United Kingdom and a model for later, more general, statements of rights; these include the United States Bill of Rights, the French Declaration of the Rights of Man and of the Citizen, the United Nations Universal Declaration of Human Rights and the European Convention on Human Rights. For example, as with the Bill of Rights 1689, the US Constitution prohibits excessive bail and \"cruel and unusual punishment\". Similarly, \"cruel, inhuman or degrading treatment or punishment\" is banned under Article 5 of the Universal Declaration of Human Rights and Article 3 of the European Convention on Human Rights.\n\nThe Bill of Rights remains in statute and continues to be cited in legal proceedings in the United Kingdom and other Commonwealth realms, particularly Article 9 on parliamentary freedom of speech. Following the Perth Agreement in 2011, legislation amending the Bill of Rights and the Act of Settlement 1701 came into effect across the Commonwealth realms on 26 March 2015 which changed the laws of succession to the British throne.\n\nPart of the Bill of Rights remains in statute in the Republic of Ireland.\n\nThe Bill of Rights applies in England and Wales; it was enacted in the Kingdom of England which at the time included Wales. Scotland has its own legislation, the Claim of Right Act 1689, passed before the Act of Union between England and Scotland. There are doubts as to whether, or to what extent, the Bill of Rights applies in Northern Ireland.\n\nOn 21 July 1995 a libel case brought by Neil Hamilton (then a member of parliament) against \"The Guardian\" was stopped after Justice May ruled that the Bill of Rights' prohibition on the courts' ability to question parliamentary proceedings would prevent \"The Guardian\" from obtaining a fair hearing. Section 13 of the Defamation Act 1996, was subsequently enacted to permit MPs to waive their parliamentary privilege and thus cite their own speeches if relevant to litigation.\n\nFollowing the United Kingdom European Union membership referendum in 2016, the Bill of Rights was quoted in a court ruling on a legal challenge seeking a judicial declaration that triggering EU exit must first be authorised by an act of Parliament.\n\nThe ninth article, regarding parliamentary freedom of speech, is actively used in Australia.\n\nThe article on parliamentary freedom of speech is in active use in Canada.\n\nThe Bill of Rights was invoked in New Zealand in the 1976 case of \"Fitzgerald v Muldoon and Others\", which centred on the purporting of newly appointed Prime Minister Robert Muldoon that he would advise the Governor-General to abolish a superannuation scheme established by the New Zealand Superannuation Act, 1974, without new legislation. Muldoon felt that the dissolution would be immediate and he would later introduce a bill in parliament to retroactively make the abolition legal. This claim was challenged in court and the Chief Justice declared that Muldoon's actions were illegal as they had violated Article 1 of the Bill of Rights, which provides \"that the pretended power of dispensing with laws or the execution of laws by regal authority ... is illegal.\"\n\nThe Act was retained in the Republic of Ireland although sections were repealed by the Statute Law Revision Act 2007 section 2(2)(a), and Part 2 of Schedule 1. Section 2(3) of that Act repealed:\n\nTwo special designs of commemorative two pound coins were issued in the United Kingdom in 1989 to celebrate the tercentenary of the Glorious Revolution. One referred to the Bill of Rights and the other to the Claim of Right. Both depict the Royal Cypher of William and Mary and the mace of the House of Commons, one also shows a representation of the St Edward's Crown and the other the Crown of Scotland.\n\nIn May 2011, the Bill of Rights was inscribed in UNESCO's UK Memory of the World Register recognizing that:\n\nAs part of the Parliament in the Making programme, the Bill of Rights was on display at the Houses of Parliament in February 2015 and at the British Library from March through September 2015.\n\n\n\n"}
{"id": "726885", "url": "https://en.wikipedia.org/wiki?curid=726885", "title": "By-product", "text": "By-product\n\nA by-product is a secondary product derived from a manufacturing process or chemical reaction. It is not the primary product or service being produced. In the context of production, a by-product is the 'output from a joint production process that is minor in quantity and/or net realizable value (NRV) when compared with the main products'. Because they are deemed to have no influence on reported financial results, by-products do not receive allocations of joint costs. By-products also by convention are not inventoried, but the NRV from by-products is typically recognized as 'other income' or as a reduction of joint production processing costs when the by-product is produced. A by-product can be useful and marketable or it can be considered waste. For example, bran is a byproduct of the milling of refined flour, sometimes composted or burned for disposal but in other cases used as a nutritious ingredient in food or feed, and gasoline was once a byproduct of oil refining that later became a desirable commodity as motor fuel.\n\nIEA defines \"by-product\" in the context of life-cycle assessment: \"... main products, co-products (which involve similar revenues to the main product), by-products (which result in smaller revenues), and waste products (which provide little or no revenue).\"\n\nWhile some chemists treat \"by-product\" and \"side-product\" as synonyms in the above sense of a generic secondary (untargeted) product, others find it useful to distinguish the two. When the two terms are distinguished, \"by-product\" is used to refer to a product that is not desired but inevitably results from molecular fragments of starting materials and/or reagents that are not incorporated into the desired product, as a consequence of conservation of mass. In contrast, \"side-product\" is used to refer to a product that is formed from a competitive process that could, in principle, be suppressed by optimization of reaction conditions.\n\n"}
{"id": "408777", "url": "https://en.wikipedia.org/wiki?curid=408777", "title": "Canadian Arrow", "text": "Canadian Arrow\n\nThe Canadian Arrow was a privately funded, early-2000s rocket and space tourism project concept founded by London, Ontario, Canada entrepreneurs Geoff Sheerin, Dan McKibbon and Chris Corke. The project's objective was to take the first civilians into space, on a vertical sub-orbital spaceflight reaching an altitude of 112 km.\n\nCanadian Arrow was considered one of the top three candidates for the X-Prize competition, along with Scaled Composites (Burt Rutan), and Armadillo Aerospace (John Carmack). Scaled Composites won the competition on October 4, 2004.\n\nThe Canadian Arrow team's motto was \"making SPACE for you\". They have completed the first series of tests on their 57,000 lbf (254 kN) thrust engine and have built a space training centre and a full-scale mock-up of their rocket. After an open nomination process, they also recruited a team of six astronauts from around the world, including several seasoned military pilots and a NASA trained astronaut from Ukraine. Astronaut candidates – the group \"Arrow Six\" includes David Ballinger, Ted Gow, Terry Wong, Jason Dyer, Larry Clark and Yaroslav \"Yarko\" Pustovyi, the only member of the team with actual space training.\n\nIn November 2010 Geoff Sheerin, the president of Canadian Arrow stated the company is unlikely to fly a Canadian Arrow rocket as a space tourism vehicle.\n\nThe Canadian Arrow is a 16.5 m tall two-stage rocket, where the second stage is a three-person space capsule. The Canadian Arrow team's somewhat conservative approach has been to base the design of their rocket engine and aerodynamics on the well proven V-2 design from WWII.\n\nThe rocket's first stage is 10.2 m long and 1.7 m in diameter. It is propelled by a single liquid fuel rocket engine. It produces a thrust of 254 kN. Graphite jet vanes are used for stabilisation before the rocket has reached a velocity high enough for the four fins to be effective. About one minute after ignition, the fuel is depleted and the engine shuts off.\n\nThe rocket's second stage is 6 m long and 1.7 m in diameter at the base. It carries three astronauts and is propelled by four JATO-type solid rocket engines. These are ignited immediately after stage separation, and will carry the capsule to an altitude of ~112 km. Cold gas jets were planned to be used for attitude control.\n\nThe design proposed four solid rocket engines in the second stage that could be fired at any time, even when the rocket stands on its launch pad. This constitutes an escape system, which can, in a case of an emergency, quickly separate the second stage from the rocket and propel it to an altitude of 1.5 km, where its parachutes can be deployed.\n\nThe rocket engine was to use alcohol and liquid oxygen as propellants, and produces a maximum thrust of 254 kN, and burns for 55 s. It is constructed of low carbon steel, with propellant injectors made out of brass.\n\nThe Canadian Arrow rocket will launch vertically from the ground. Initial thrust is ~75.5 kN, but the rocket quickly reaches maximum thrust. After 55 s, the propellant is depleted and stage separation occurs. The solid fuel rockets in the second stage are ignited and boosts it up to an altitude of ~112 km, where the crew and passengers will experience a few minutes of \"zero-G\", or weightlessness.\n\nAfter stage separation the first stage reaches an apogee of over 80 km before descent begins. Four parachutes slow the Canadian Arrow's first stage down before splashdown occurs at a speed of ~9 m/s, after which recovery of the spacecraft can take place.\n\nDuring descent, the crew cabin (the second stage) was planned to use a ballute to reduce its speed. When its velocity becomes subsonic, the second stage's ballute was to be released and pull out the three parachutes before splashdown.\n\n\nCanadian Arrow started as a team competing in the international X-Prize competition, with the ultimate goal of continuing past the X-Prize into the commercial sector providing private access to space. Funding during the X-Prize was provided by sponsorship and private investment.\nIn early 2003 the company would receive a major infusion of financial support by Canadian Arrow partner and Director of Spacecraft Development - Lou van Amelsvoort.\nAs a result, during the next two years The Company would also proceed to open the world's first private Astronaut training facility, continue vehicle development, and test propulsion and recovery systems.\n\nGeoff Sheerin, president and CEO of Canadian Arrow, and Dr. Chirinjeev Kathuria announced on May 17, 2005, the creation of PlanetSpace Corporation. It is through this enterprise that Canadian Arrow will complete the construction of their space craft, and within 24 months offer suborbital space flight to aspiring space tourists. Planetspace expects to fly about 2,000 new astronauts within five years of operation. The price is expected to be $250,000 for each flight, including fourteen days of training. Cape Breton Island, in Nova Scotia is being considered as a launch site, and a contract has been signed with the government of Nova Scotia to provide of land for the project.\n\nA requirement of the X-Prize for each participating company was to propose other possible markets for their spacecraft. Canadian Arrow coined the term \"Spacediving\", while investigating the possible use of Canadian Arrow spacecraft for a high altitude version of skydiving.\n\nOn November 11, 2005 Canadian Arrow teamed up with former X-Prize competitor Romanian aerospace company, ARCASPACE, to develop privately built spacecraft.\n\nOn December 15, 2005 PlanetSpace Corporation unveiled plans for an orbital commercial vehicle capable of carrying eight passengers. This vehicle to be called the Silver Dart is based on the U.S. Air Force's Flight Dynamics Laboratory-7 lifting body program from the 1970s.\nPlanetSpace Corporation defunct as of 6 February 2013\nOn June 21, 2013 Blackburn news reported that the full scale engineering mock-up of the Canadian Arrow rocket was purchased by Sarnia Ontario's Preferred Towing. Having spent several years at Chris Hatfield Airport in Sarnia, Ontario, Preferred Towing expressed interest in hopes of restoring the rocket for display in Sarnia. This plan has since been scrapped and the mock-up no longer exists.\n\n<br>\n"}
{"id": "8118527", "url": "https://en.wikipedia.org/wiki?curid=8118527", "title": "Carltona doctrine", "text": "Carltona doctrine\n\nThe Carltona doctrine (or Carltona principle) expresses the idea that, in United Kingdom law, the acts of government departmental officials are synonymous with the actions of the minister in charge of that department. The point was established in \"Carltona Ltd v Commissioners of Works.\"\n\nFaced with the requisition of their factory by the war-time government, the factory owners raised a judicial review action to challenge the legality of the requisition order. The order had been made under the auspices of the Defence (General) Regulations 1939, which authorised the Commissioners of Works to requisition such land as they deemed necessary in the national interest. The Regulations specified that the Commissioner's powers were exercisable by, \"inter alia\" the Minister of Works and Planning. The factory owners sought to argue that the requisition was invalid because the order had not in fact been signed by the minister, but by an official within the Ministry of Works and Planning. In rejecting this contention, the Master of the Rolls, Lord Greene, acknowledged the realities of government in the 20th century:\n\nThis statement of the way government operates has only become more true in recent decades as increased state interventionism and juridification have produced a rapid growth in the use of delegated legislation. Clearly, confronted with this reality, it would have been preposterous for the Court to construe the wording of the Regulations so narrowly that only the Minister, in person, could exercise the powers. Thus Lord Greene explained that:\n\nIt should be emphasised that the essence of the \"Carltona\" doctrine therefore lies in the elision of the identity of departmental officials with the relevant Minister. It is emphatically not the case that the Minister has delegated his decision-making power to a subordinate and therefore the doctrine achieves consistency with the principle that Parliament's delegatees have, unless specifically provided by statute, no power to delegate (\"delegatus non potest delegare\").\n\nLord Greene proceeded to reconcile this with the doctrine of parliamentary accountability on the basis that:\n\nDespite suggestions to the contrary by some academic commentators, it seems that there is no restriction on the applicability of the doctrine on account of the nature of the power being wielded. In \"HMA v. Copeland\" it was opined, by the highest criminal court in Scotland that:\n\nHowever, in some instances Parliament has chosen to statutorily override this position by providing that the relevant minister must exercise the power in person.\n\nThe Supreme Court of Ireland has confirmed that the Carltona doctrine applies to its fullest extent to the Irish civil service also- see \"Devanney v\" \"Shields\" [1998] 2 I.R. 230.\n\n\n\n"}
{"id": "151577", "url": "https://en.wikipedia.org/wiki?curid=151577", "title": "Causality (physics)", "text": "Causality (physics)\n\nCausality is the relationship between causes and effects. It is considered to be fundamental to all natural science, especially physics. Causality is also a topic studied from the perspectives of philosophy and statistics. Causality means that an effect cannot occur from a cause that is not in the back (past) light cone of that event. Similarly, a cause cannot have an effect outside its front (future) light cone.\n\nIn classical physics, an effect cannot occur \"before\" its cause. In Einstein's theory of special relativity, causality means that an effect can not occur from a cause that is not in the back (past) light cone of that event. Similarly, a cause cannot have an effect outside its front (future) light cone. These restrictions are consistent with the grounded belief (or assumption) that causal influences cannot travel faster than the speed of light and/or backwards in time. In quantum field theory, observables of events with a spacelike relationship, \"elsewhere\", have to commute, so the order of observations or measurements of such observables do not impact each other.\n\nCausality in this context should not be confused with Newton's second law, which is related to the conservation of momentum, and is a consequence of the spatial homogeneity of physical laws. The word causality in this context means that all effects must have specific causes. As discussed below, this is a principle that is violated in some theories of modern physics.\n\nAnother requirement, at least valid at the level of human experience, is that cause and effect be mediated across space and time (requirement of \"contiguity\"). This requirement has been very influential in the past, in the first place as a result of direct observation of causal processes (like pushing a cart), in the second place as a problematic aspect of Newton's theory of gravitation (attraction of the earth by the sun by means of action at a distance) replacing mechanistic proposals like Descartes' vortex theory; in the third place as an incentive to develop dynamic field theories (e.g., Maxwell's electrodynamics and Einstein's general theory of relativity) restoring contiguity in the transmission of influences in a more successful way than did Descartes' theory.\n\nThe empiricists' aversion to metaphysical explanations (like Descartes' vortex theory) lends heavy influence against the idea of the importance of causality. Causality has accordingly sometimes been downplayed (e.g., Newton's \"Hypotheses non fingo\"). According to Ernst Mach the notion of force in Newton's second law was pleonastic, tautological and superfluous. Indeed, it is possible to consider the Newtonian equations of motion of the gravitational interaction of two bodies,\n\nas two coupled equations describing the positions formula_2 and formula_3 of the two bodies, \"without interpreting the right hand sides of these equations as forces\"; the equations just describe a process of interaction, without any necessity to interpret one body as the cause of the motion of the other, and allow one to predict the states of the system at later (as well as earlier) times.\n\nThe ordinary situations in which humans singled out some factors in a physical interaction as being prior and therefore supplying the \"because\" of the interaction were often ones in which humans decided to bring about some state of affairs and directed their energies to producing that state of affairs—a process that took time to establish and left a new state of affairs that persisted beyond the time of activity of the actor. It would be difficult and pointless, however, to explain the motions of binary stars with respect to each other in that way.\n\nThe possibility of such a time-independent view is at the basis of the deductive-nomological (D-N) view of scientific explanation, considering an event to be explained if it can be subsumed under a scientific law. In the D-N view, a physical state is considered to be explained if, applying the (deterministic) law, it can be derived from given initial conditions. (Such initial conditions could include the momenta and distance from each other of binary stars at any given moment.) Such 'explanation by determinism' is sometimes referred to as causal determinism. A disadvantage of the D-N view is that causality and determinism are more or less identified. Thus, in classical physics, it was assumed that all events are caused by earlier ones according to the known laws of nature, culminating in Pierre-Simon Laplace's claim that if the current state of the world were known with precision, it could be computed for any time in the future or the past (see Laplace's demon). However, this is usually referred to as Laplace \"determinism\" (rather than `Laplace causality') because it hinges on determinism in mathematical models as dealt with in the mathematical Cauchy problem. Confusion of causality and determinism is particularly acute in quantum mechanics, this theory being acausal in the sense that it is unable in many cases to identify the causes of actually observed effects or to predict the effects of identical causes, but arguably deterministic in some interpretations (e.g. if the wave function is presumed not to actually collapse as in the many-worlds interpretation, or if its collapse is due to hidden variables, or simply redefining determinism as meaning that probabilities rather than specific effects are determined).\n\nIn modern physics, the notion of causality had to be clarified. The insights of the theory of special relativity confirmed the assumption of causality, but they made the meaning of the word \"simultaneous\" observer-dependent. Consequently, the relativistic principle of causality says that the cause must precede its effect \"according to all inertial observers\". This is equivalent to the statement that the cause and its effect are separated by a timelike interval, and the effect belongs to the future of its cause. If a timelike interval separates the two events, this means that a signal could be sent between them at less than the speed of light. On the other hand, if signals could move faster than the speed of light, this would violate causality because it would allow a signal to be sent across spacelike intervals, which means that at least to some inertial observers the signal would travel \"backward in time\". For this reason, special relativity does not allow communication faster than the speed of light.\n\nIn the theory of general relativity, the concept of causality is generalized in the most straightforward way: the effect must belong to the future light cone of its cause, even if the spacetime is curved. New subtleties must be taken into account when we investigate causality in quantum mechanics and relativistic quantum field theory in particular. In quantum field theory, causality is closely related to the principle of locality. However, the principle of locality is disputed: whether it strictly holds depends on the interpretation of quantum mechanics chosen, especially for experiments involving quantum entanglement that satisfy Bell's Theorem.\n\nDespite these subtleties, causality remains an important and valid concept in physical theories. For example, the notion that events can be ordered into causes and effects is necessary to prevent (or at least outline) causality paradoxes such as the grandfather paradox, which asks what happens if a time-traveler kills his own grandfather before he ever meets the time-traveler's grandmother. See also Chronology protection conjecture.\n\nTheories in physics like the butterfly effect from chaos theory open up the possibility of a type of distributed parameter systems in causality. The butterfly effect theory proposes:\n\"Small variations of the initial condition of a nonlinear dynamical system may produce large variations in the long term behavior of the system.\" This opens up the opportunity to understand a distributed causality.\n\nA related way to interpret the butterfly effect is to see it as highlighting the difference between the application of the notion of causality in physics and a more general use of causality as represented by Mackie's INUS conditions. In classical (Newtonian) physics, in general, only those conditions are (explicitly) taken into account, that are both necessary and sufficient. For instance, when a massive sphere is caused to roll down a slope starting from a point of unstable equilibrium, then its velocity is assumed to be caused by the force of gravity accelerating it; the small push that was needed to set it into motion is not explicitly dealt with as a cause. In order to be a physical cause there must be a certain proportionality with the ensuing effect. A distinction is drawn between triggering and causation of the ball's motion. By the same token the butterfly can be seen as triggering a tornado, its cause being assumed to be seated in the atmospherical energies already present beforehand, rather than in the movements of a butterfly.\n\nCausal dynamical triangulation (abbreviated as \"CDT\") invented by Renate Loll, Jan Ambjørn and Jerzy Jurkiewicz, and popularized by Fotini Markopoulou and Lee Smolin, is an approach to quantum gravity that like loop quantum gravity is background independent. This means that it does not assume any pre-existing arena (dimensional space), but rather attempts to show how the spacetime fabric itself evolves. The Loops '05 conference, hosted by many loop quantum gravity theorists, included several presentations which discussed CDT in great depth, and revealed it to be a pivotal insight for theorists. It has sparked considerable interest as it appears to have a good semi-classical description. At large scales, it re-creates the familiar 4-dimensional spacetime, but it shows spacetime to be 2-d near the Planck scale, and reveals a fractal structure on slices of constant time. Using a structure called a simplex, it divides spacetime into tiny triangular sections. A simplex is the generalized form of a triangle, in various dimensions. A 3-simplex is usually called a tetrahedron, and the 4-simplex, which is the basic building block in this theory, is also known as the pentatope, or pentachoron. Each simplex is geometrically flat, but simplices can be \"glued\" together in a variety of ways to create curved spacetimes. Where previous attempts at triangulation of quantum spaces have produced jumbled universes with far too many dimensions, or minimal universes with too few, CDT avoids this problem by allowing only those configurations where cause precedes any effect. In other words, the timelines of all joined edges of simplices must agree.\n\nThus, maybe, causality lies in the foundation of the spacetime geometry.\n\nIn causal set theory, causality takes an even more prominent place. The basis for this approach to quantum gravity is in a theorem by David Malament. This theorem states that the causal structure of a spacetime suffices to reconstruct its conformal class. So knowing the conformal factor and the causal structure is enough to know the spacetime. Based on this, Rafael Sorkin proposed the idea of Causal Set Theory, which is a fundamentally discrete approach to quantum gravity. The causal structure of the spacetime is represented as a Poset, while the conformal factor can be reconstructed by identifying each poset element with a unit volume.\n\n\n\n"}
{"id": "46877898", "url": "https://en.wikipedia.org/wiki?curid=46877898", "title": "Chinese Whispers (clustering method)", "text": "Chinese Whispers (clustering method)\n\nChinese Whispers is a clustering method used in network science named after the famous whispering game. Clustering methods are basically used to identify communities of nodes or links in a given network. This algorithm was designed by Chris Biemann and Sven Teresniak in 2005. The name comes from the fact that the process can be modeled as a separation of communities where the nodes send the same type of information to each other.\n\nChinese Whispers is a hard partitioning, randomized, flat clustering (no hierarchical relations between clusters) method. The random property means that running the process on the same network several times can lead to different results, while because of hard partitioning one node can only belong to one cluster at a given moment. The original algorithm is applicable to undirected, weighted and unweighted graphs. Chinese Whispers is time linear which means that it is extremely fast even if the number of nodes and links are very high in the network.\n\nThe algorithm works in the following way in an undirected unweighted graph:\n\nThe predetermined threshold for the number of the iterations is needed because it is possible that process does not converge. On the other hand in a network with approximately 10000 nodes the clusters does not change significantly after 40-50 iterations even if there is no convergence.\n\nThe main strength of Chinese Whispers lies in its time linear property. Because of the processing time increases linearly with the number of nodes, the algorithm is capable of identifying communities in a network very fast. For this reason Chinese Whispers is a good tool to analyze community structures in graph with a very high number of nodes. The effectiveness of the method increases further if the network has the small world property.\n\nOn the other hand because the algorithm is not deterministic in the case of small node number the resulting clusters often significantly differ from each other. The reason for this is that in the case of a small network it matters more from which node the iteration process starts while in large networks the relevance of starting points disappears. For this reason for small graphs other clustering methods are recommended.\n\nChinese Whispers is used in many subfield of network science. Most frequently it is mentioned in the context of Natural Language Processing problems. On the other hand the algorithm is applicable to any kind of community identification problem which is related to a network framework. Chinese Whispers is available for personal use as an extension package for Gephi which is an open source program designed for network analysis.\n"}
{"id": "587339", "url": "https://en.wikipedia.org/wiki?curid=587339", "title": "Circuit diagram", "text": "Circuit diagram\n\nA circuit diagram (electrical diagram, elementary diagram, electronic schematic) is a graphical representation of an electrical circuit. A pictorial circuit diagram uses simple images of components, while a schematic diagram shows the components and interconnections of the circuit using standardized symbolic representations. The presentation of the interconnections between circuit components in the schematic diagram does not necessarily correspond to the physical arrangements in the finished device.\n\nUnlike a block diagram or layout diagram, a circuit diagram shows the actual electrical connections. A drawing meant to depict the physical arrangement of the wires and the components they connect is called \"artwork\" or \"layout\", \"physical design\", or \"wiring diagram\".\n\nCircuit diagrams are used for the design (circuit design), construction (such as PCB layout), and maintenance of electrical and electronic equipment.\n\nIn computer science, circuit diagrams are useful when visualizing expressions using Boolean algebra.\n\nCircuit diagrams are pictures with symbols that have differed from country to country and have changed over time, but are now to a large extent internationally standardized. Simple components often had symbols intended to represent some feature of the physical construction of the device. For example, the symbol for a resistor shown here dates back to the days when that component was made from a long piece of wire wrapped in such a manner as to not produce inductance, which would have made it a coil. These wirewound resistors are now used only in high-power applications, smaller resistors being cast from \"carbon composition\" (a mixture of carbon and filler) or fabricated as an insulating tube or chip coated with a metal film. The internationally standardized symbol for a resistor is therefore now simplified to an oblong, sometimes with the value in ohms written inside, instead of the zig-zag symbol. A less common symbol is simply a series of peaks on one side of the line representing the conductor, rather than back-and-forth as shown here.\n\nThe linkages between leads were once simple crossings of lines. With the arrival of computerized drafting, the connection of two intersecting wires was shown by a crossing of wires with a \"dot\" or \"blob\" to indicate a connection. At the same time, the crossover was simplified to be the same crossing, but without a \"dot\". However, there was a danger of confusing the wires that were connected and not connected in this manner, if the dot was drawn too small or accidentally omitted (e.g. the \"dot\" could disappear after several passes through a copy machine). As such, the modern practice for representing a 4-way wire connection is to draw a straight wire and then to draw the other wires staggered along it with \"dots\" as connections (see diagram), so as to form two separate T-junctions that brook no confusion and are clearly not a crossover.\n\nFor crossing wires that are insulated from one another, a small semi-circle symbol is commonly used to show one wire \"jumping over\" the other wire (similar to how jumper wires are used).\n\nA common, hybrid style of drawing combines the T-junction crossovers with \"dot\" connections and the wire \"jump\" semi-circle symbols for insulated crossings. In this manner, a \"dot\" that is too small to see or that has accidentally disappeared can still be clearly differentiated from a \"jump\".\n\nOn a circuit diagram, the symbols for components are labelled with a descriptor or reference designator matching that on the list of parts. For example, C1 is the first capacitor, L1 is the first inductor, Q1 is the first transistor, and R1 is the first resistor. Often the value or type designation of the component is given on the diagram beside the part, but detailed specifications would go on the parts list.\n\nDetailed rules for reference designations are provided in the International standard IEC 61346.\n\nIt is a usual although not universal convention that schematic drawings are organized on the page from left to right and top to bottom in the same sequence as the flow of the main signal or power path. For example, a schematic for a radio receiver might start with the antenna input at the left of the page and end with the loudspeaker at the right. Positive power supply connections for each stage would be shown towards the top of the page, with grounds, negative supplies, or other return paths towards the bottom. Schematic drawings intended for maintenance may have the principal signal paths highlighted to assist in understanding the signal flow through the circuit. More complex devices have multi-page schematics and must rely on cross-reference symbols to show the flow of signals between the different sheets of the drawing.\n\nDetailed rules for the preparation of circuit diagrams, and other document types used in electrotechnology, are provided in the international standard IEC 61082-1.\n\nRelay logic line diagrams, also called ladder logic diagrams, use another common standardized convention for organizing schematic drawings, with a vertical power supply rail on the left and another on the right, and components strung between them like the rungs of a ladder.\n\nOnce the schematic has been made, it is converted into a layout that can be fabricated onto a printed circuit board (PCB). Schematic-driven layout starts with the process of schematic capture. The result is what is known as a rat's nest. The rat's nest is a jumble of wires (lines) criss-crossing each other to their destination nodes. These wires are routed either manually or automatically by the use of electronics design automation (EDA) tools. The EDA tools arrange and rearrange the placement of components and find paths for tracks to connect various nodes. This results in the final layout artwork for the integrated circuit or printed circuit board.\n\nA generalized design flow may be as follows:\n\nTeaching about the functioning of electrical circuits is often on primary and secondary school curricula. Students are expected to understand the rudiments of circuit diagrams and their functioning. Use of diagrammatic representations of circuit diagrams can aid understanding of principles of electricity. \n\nPrinciples of the physics of circuit diagrams are often taught with the use of analogies, such as comparing functioning of circuits to other closed systems such as water heating systems with pumps being the equivalent to batteries.\n\n"}
{"id": "9470237", "url": "https://en.wikipedia.org/wiki?curid=9470237", "title": "David Orrell", "text": "David Orrell\n\nDavid John Orrell (born 1962 in Edmonton) is a Canadian writer and mathematician. He received his doctorate in mathematics from the University of Oxford. His work in the prediction of complex systems such as the weather, genetics and the economy has been featured in New Scientist, the Financial Times, Adbusters, BBC Radio, Russia-1, and CBC TV. He now conducts research and writes in the areas of systems biology and economics, and runs a mathematical consultancy Systems Forecasting. He is the son of theatre historian and English professor John Orrell.\n\nHis books have been translated into over ten languages. \"Apollo's Arrow: The Science of Prediction and the Future of Everything\" was a national bestseller and finalist for the 2007 Canadian Science Writers' Award. \"Economyths: Ten Ways Economics Gets It Wrong\" was a finalist for the 2011 National Business Book Award.\n\nA consistent topic in Orrell’s work is the limitations of mathematical models, and the need to acknowledge these limitations if we are to understand the causes of forecast error. He argues for example that errors in weather prediction are caused primarily by model error, rather than the butterfly effect. Economic models are seen as particularly unrealistic. In \"Truth or Beauty: Science and the Quest for Order\", he suggests that many such theories, along with areas of physics such as string theory, are motivated largely by the desire to conform with a traditional scientific aesthetic, that is currently being subverted by developments in complexity science.\n\nIn \"The Evolution of Money\" (coauthored with journalist Roman Chlupatý) and two articles Orrell proposed a quantum theory of money and value, which states that money has dualistic properties because it combines the properties of an owned and valued thing, with those of abstract number. The fact that these two sides of money are incompatible leads to its complex and often unpredictable behavior. In \"Quantum Economics: The New Science of Money\" he argues that these dualistic properties feed up to affect the economy as a whole.\n\n\n\n"}
{"id": "2990836", "url": "https://en.wikipedia.org/wiki?curid=2990836", "title": "De Brevitate Vitae (Seneca)", "text": "De Brevitate Vitae (Seneca)\n\nDe Brevitate Vitae () is a moral essay written by Seneca the Younger, a Roman Stoic philosopher, sometime around the year 49 AD, to his father-in-law Paulinus. The philosopher brings up many Stoic principles on the nature of time, namely that people waste much of it in meaningless pursuits. According to the essay, nature gives people enough time to do what is really important and the individual must allot it properly. In general, time is best used by living in the present moment in pursuit of the intentional, purposeful life.\n\nSimilar ideas can be found in Seneca's treatise \"De Otio\" (\"On Leisure\") and discussion of these themes can often be found in his \"Letters to Lucilius\" (letter 49, 101, etc.).\n\nThe work is addressed to a man called Paulinus—probably Pompeius Paulinus, a knight of Arelate—and is usually dated to around 49 AD. It is clear from chapters 18 and 19 of \"De Brevitate Vitae\" that Paulinus was \"praefectus annonae\", the official who superintended the grain supply of Rome, and was, therefore, a man of importance. He was likely a near relative of Seneca's wife, Pompeia Paulina, and quite plausibly her father. He is also thought to be the father of another Pompeius Paulinus, who held high public posts under Nero (Pliny, \"Nat. Hist.\" xxxiii. 143; Tacitus, \"Annals\", xiii. 53. 2; xv. 18. 4).\n\nAs for the date of composition, it must have been after the death of Caligula (41 AD), which Seneca mentions in §18.5. Furthermore, there are two known periods when Paulinus could have served as \"praefectus annonae\", 48–55 and 62–71 AD, and scholars prefer the earlier period. A date of 49 AD has previously been suggested because Seneca writes in §13.8 \"that Sulla was the last of the Romans who extended the \"pomerium\"\" (the boundary of Rome). Since Claudius extended this in 49/50 AD, it would have been written before this. However Miriam Griffin has argued that Seneca is quoting a pedant who is asserting that Claudius' extension was illegal, which would mean that Seneca was writing after this date. Griffin has suggested that Seneca wrote \"De Brevitate Vitae\" as an excuse for Paulinus to retire early in 55 AD.\n\nIn chapter 1 Seneca counters the complaint that life is too short with the view that life is long enough if well-managed. Chapters 2 to 9 survey the many ways in which life is squandered and time frittered away by those people (\"occupati\") engrossed in pointless pursuits. Chapters 10 to 17 contrast the philosophical approach to leisure (\"otium\") with the deluded common approach. This culminates in chapters 18 to 20 showing the emancipation of the wise, who can soar above the lives of others mired in endless preoccupation.\n\nAfter the introduction (§1), Seneca reviews (§2–3) the distractions which make life seem short, and explains that people are great wasters of time. He then offers (§4–6) three examples of famous Romans (Augustus, Cicero and Livius Drusus) who, in various ways, were victims of the engrossed life. He explains (§7–8) that the engrossed do not know how to live or have awareness, and that they waste time because they do not know its value. One should purposefully live for the moment (§9), because tomorrow will be too late. In contrast (§10) the lives of the engrossed seem so short to them because they are constrained to the fleeting present, and recollect the past in pain. They desperately cling on to life (§11) because they haven't lived, unlike the wise, who are always ready to leave life behind. The engrossed include those who live in leisure and luxury (§12), and Seneca explains (§13) that even those who devote themselves to scholarship are wasting their time if their efforts are directed to no end. Accordingly, (§14–15) only those who dedicate their time properly truly live, becoming equal with the great minds of the past, allowing the mind of the sage to even transcend time, like a god. The engrossed, on the other hand, (§16–17) are prey to fidgety and contradictory moods, and their joys and pleasures are bitter with the sense of precariousness. Finally (§18–19) Seneca exhorts Paulinus to abandon public occupations and adopt the contemplative life of the wise, free from the passions. This is contrasted (§20) with the suffering of the engrossed: they die without having ever lived.\n\nIn the treatise Seneca argues that we waste so much time because we do not properly value it. We expend great effort in protecting other valuables such as money and property, but because time appears intangible, we allow others to occupy it and take time away from us. Wise people, on the other hand, understand that time is the most valuable of all resources, and with effort can free themselves from external control to engage in meaningful introspection and create an intentional life.\n\nSeneca urges his readers to live in the present, and adapt themselves to a purposeful life in agreement with Nature. Only by doing so, can one then truly unlock both past and future. The completeness of each present moment allows one's awareness to expand to the equal of that of the universe, and achieve true virtue and happiness.\n\nThe statements which urge Paulinus to retire from public life are in notable contrast to Seneca's advice in his \"De Tranquillitate Animi\" (to his friend Annaeus Serenus) to seek public employments in order to render life attractive. However, in his related treatise, \"De Otio\", Seneca makes the point that there is no inconsistency, and that one can serve the greater community in either or both roles.\n\n\n"}
{"id": "21424701", "url": "https://en.wikipedia.org/wiki?curid=21424701", "title": "Defaunation", "text": "Defaunation\n\nDefaunation is the global, local or functional extinction of animal populations or species from ecological communities. The growth of the human population, combined with advances in harvesting technologies, has led to more intense and efficient exploitation of the environment. This has resulted in the depletion of large vertebrates from ecological communities, creating what has been termed \"empty forest\". Defaunation differs from extinction; it includes both the disappearance of species and declines in abundance. Defaunation effects were first implied at the Symposium of Plant-Animal Interactions at the University of Campinas, Brazil in 1988 in the context of neotropical forests. Since then, the term has gained broader usage in conservation biology as a global phenomenon.\n\nIt is estimated that more than 50 percent of all wildlife has been lost in the last 40 years. in 2020 it is estimated that 68% of the world's wildlife will be lost. In South America, there is believed to be a 70 percent loss.\n\nIn November 2017, over 15,000 scientists around the world issued a second warning to humanity, which, among other things, urged for the development and implementation of policies to halt \"defaunation, the poaching crisis, and the exploitation and trade of threatened species.\"\n\nThe intensive hunting and harvesting of animals threatens endangered vertebrate species across the world. Game vertebrates are considered valuable products of tropical forests and savannas. In Brazilian Amazonia, 23 million vertebrates are killed every year; large-bodied primates, tapirs, white-lipped peccaries, giant armadillos, and tortoises are some of the animals most sensitive to harvest. Overhunting can reduce the local population of such species by more than half, as well as reducing population density. Populations located nearer to villages are significantly more at risk of depletion. Abundance of local game species declines as density of local settlements, such as villages, increases.\n\nHunting and poaching may lead to local population declines or extinction in some species. Most affected species undergo pressure from multiple sources but the scientific community is still unsure of the complexity of these interactions and their feedback loops.\n\nOne case study in Panama found an inverse relationship between poaching intensity and abundance for 9 of 11 mammal species studied. In addition, preferred game species experienced greater declines and had higher spatial variation in abundance.\n\nHuman population growth results in changes in land-use, which can cause natural habitats to become fragmented, altered, or destroyed. Large mammals are often more vulnerable to extinction than smaller animals because they require larger home ranges and thus are more prone to suffer the effects of deforestation. Large species such as elephants, rhinoceroses, large primates, tapirs and peccaries are the first animals to disappear in fragmented rainforests.\n\nA case study from Amazonian Ecuador analyzed two oil-road management approaches and their effects on the surrounding wildlife communities. The free-access road had forests that were cleared and fragmented and the other had enforced access control. Fewer species were found along the first road with density estimates being almost 80% lower than at the second site that which had minimal disturbance. This finding suggests that disturbances affected the local animals' willingness and ability to travel between patches.\n\nFragmentation lowers populations while increasing extinction risk when the remaining habitat size is small. When there is more unfragmented land, there is more habitat for more diverse species. A larger land patch also means it can accommodate more species with larger home ranges. However, when patch size decreases, there is an increase in the number of isolated fragments which can remain unoccupied by local fauna. If this persists, species may become extinct in the area.\n\nA study on deforestation in the Amazon looked at two patterns of habitat fragmentation: \"fish-bone\" in smaller properties and another unnamed large property pattern. The large property pattern contained fewer fragments than the smaller fish-bone pattern. The results suggested that higher levels of fragmentation within the fish-bone pattern led to the loss of species and decreased diversity of large vertebrates. Human impacts, such as the fragmentation of forests, may cause large areas to lose the ability to maintain biodiversity and ecosystem function due to loss of key ecological processes. This can consequently cause changes within environments and skew evolutionary processes.\n\nHuman influences, such as colonization and agriculture, have caused species to become distributed outside of their native ranges. Fragmentation also has cascading effects on native species, beyond reducing habitat and resource availability; it leaves areas vulnerable to non-native invasions. Invasive species can out-compete or directly prey upon native species, as well as alter the habitat so that native species can no longer survive.\n\nIn extinct animal species for which the cause of extinction is known, over 50% were affected by invasive species. For 20% of extinct animal species, invasive species are the only cited cause of extinction. Invasive species are the second-most important cause of extinction for mammals.\n\nTropical regions are the most heavily impacted by defaunation. These regions, which include the Brazilian Amazon, the Congo Basin of Central Africa, and Indonesia, experience the greatest rates of overexploitation and habitat degradation. However, specific causes are varied, and areas with one endangered group (such as birds) do not necessarily also have other endangered groups (such as mammals, insects, or amphibians).\n\nDeforestation of the Brazilian Amazon leads to habitat fragmentation and overexploitation. Hunting pressure in the Amazon rainforest has increased as traditional hunting techniques have been replaced by modern weapons such as shotguns. Access roads built for mining and logging operations fragment the forest landscape and allow hunters to move into forested areas which previously were untouched. The bushmeat trade in Central Africa incentivizes the overexploitation of local fauna. Indonesia has the most endangered animal species of any area in the world. International trade in wild animals, as well as extensive logging, mining and agriculture operations, drive the decline and extinction of numerous species.\n\nInbreeding and genetic diversity loss often occur with endangered species populations because they have small and/or declining populations. Loss of genetic diversity lowers the ability of a population to deal with change in their environment and can make individuals within the community homogeneous. If this occurs, these animals are more susceptible to disease and other occurrences that may target a specific genome. Without genetic diversity, one disease could eradicate an entire species. Inbreeding lowers reproduction and survival rates. It is suggested that these genetic factors contribute to the extinction risk in threatened/endangered species.\n\nThe consequences of defaunation can be expected to affect the plant community. There are three non-mutually exclusive conclusions as to the consequences on tropical forest plant communities:\n\n\nOne recent study analyzed seedling density and composition from two areas, Los Tuxtlas and Montes Azules. Los Tuxtlas, which is affected more by human activity, showed higher seedling density and a smaller average number of different species than in the other area. Results suggest that an absence of vertebrate dispersers can change the structure and diversity of forests. As a result, a plant community that relies on animals for dispersal could potentially have an altered biodiversity, species dominance, survival, demography, and spatial and genetic structure.\n\nPoaching is likely to alter plant composition because the interactions between game and plant species varies in strength. Some game species interact strongly, weakly, or not at all with species. A change in plant species composition is likely to be a result because the net effect removal of game species varies among the plant species they interact with.\n\nAs large-bodied vertebrates are increasingly lost from seed-dispersal networks, small-bodied seed dispersers (i.e. bats, birds, dung beetles) and seed predators (i.e. rodents) are affected. Defaunation leads to reduced species diversity. This is due to relaxed competition; small-bodied species normally compete with large-bodied vertebrates for food and other resources. As an area becomes defaunated, dominant small-bodied species take over, crowding out other similar species and leading to an overall reduced species diversity. The loss of species diversity is reflective of a larger loss of biodiversity, which has consequences for the maintenance of ecosystem services.\n\nThe quality of the physical habitat may also suffer. Bird and bat species (many of who are small bodied seed dispersers) rely on mineral licks as a source of sodium, which is not available elsewhere in their diets. In defaunated areas in the Western Amazon, mineral licks are more thickly covered by vegetation and have lower water availability. Bats were significantly less likely to visit these degraded mineral licks. The degradation of such licks will thus negatively affect the health and reproduction of bat populations.\n\nDefaunation has negative consequences for seed dispersal networks as well. In the western Amazon, birds and bats have separate diets and thus form separate guilds within the network. It is hypothesized that large-bodied vertebrates, being generalists, connect separate guilds, creating a stable, resilient network. Defaunation results in a highly modular network in which specialized frugivores instead act as the connector hubs.\n\nChanges in predation dynamics, seed predation, seed dispersal, carrion removal, dung removal, vegetation trampling, and other ecosystem processes as a result of defaunation can affect ecosystem supporting and regulatory services, such as nutrient cycling and decomposition, crop pollination, pest control, and water quality.\n\nEfforts against defaunation include wildlife overpasses and riparian corridors. Both of these can be otherwise known as wildlife crossing mechanisms. Wildlife overpasses are specifically used for the purpose of protecting many animal species from the roads. Many countries use them and they have been found to be very effective in protecting species and allowing forests to be connected. These overpasses look like bridges of forest that cross over many roads, like a walk bridge for humans, allowing animals to migrate from one side of the forest to the other safely since the road cut off the original connectivity. It was concluded in a study done by Pell and Jones, looking at bird use of these corridors in Australia, that many birds did, in fact, use these corridors to travel from one side of forest to the other and although they did not spend much time in the corridor specifically, they did commonly use them. Riparian corridors are very similar to overpasses they are just on flat land and not on bridges, however, they also work as connective \"bridges\" between fragmented pieces of forest. One study done connected the corridors with bird habitat and use for seed dispersal. The conclusions of this study showed that some species of birds are highly dependent on these corridors as connections between forest, as flying across the open land is not ideal for many species. Overall both of these studies agree that some sort of connectivity needs to be established between fragments in order to keep the forest ecosystem in the best health possible and that they have in fact been very effective.\n\nDefaunation in the ocean has occurred later and less intensely than on land. A relatively small number of marine species have been driven to extinction. However, many species have undergone local, ecological, and commercial extinction. Most large marine animal species still exist, such that the size distribution of global species assemblages has changed little since the Pleistocene, but individuals of each species are smaller on average, and overfishing has caused reductions in genetic diversity. Most extinctions and population declines to date have been driven by human overexploitation.\n\nMarine defaunation has a wide array of effects on ecosystem structure and function. The loss of animals can have both top-down (cascading) and bottom-up effects, as well as consequences for biogeochemical cycling, ecosystem connectivity, and ecosystem stability.\n\nTwo of the most important ecosystem services threatened by marine defaunation are the provision of food and coastal storm protection.\n\n\n\n"}
{"id": "4525524", "url": "https://en.wikipedia.org/wiki?curid=4525524", "title": "Directed mutagenesis", "text": "Directed mutagenesis\n\nDirected mutagenesis, also known as directed mutation, was a hypothesis proposing that organisms can respond to environmental stresses by orthogenetically directing mutations to certain genes or areas of the genome.\n\nThe Russian ichthyologist Lev Berg proposed directed mass mutations as the main mechanism for evolution in his anti-Darwinian book \"Nomogenesis; or, Evolution Determined by Law\" (1922). Berg's theory was both mutationist (involving mutation in place of selection) and orthogenetic (involving direction). Early studies of \"directed mutation\" were performed by German geneticists. Richard Goldschmidt claimed to have produced evidence for directed mutation in 1929 in his experiments on \"Drosophila\" fruit flies exposed to elevated temperatures. Viktor Jollos (1887–1941) in the 1930s had also carried out experiments on \"Drosophila\" and written that his results had confirmed Goldschmidt's work which was evidence for directed mutation in contrast to natural selection. However, later American geneticists were unable to replicate these experiments and the concepts fell out of favour compared to the standard Darwinian mechanism of randomly occurring mutations.\n\nDirected mutagenesis was re-proposed in 1988 by John Cairns who was studying \"Escherichia coli\" that lacked the ability to metabolize lactose. He grew these bacteria in media in which lactose was the only source of energy. In doing so, he found that the rate at which the bacteria evolved the ability to metabolize lactose was many orders of magnitude higher than would be expected if the mutations were truly random. This inspired him to propose that the mutations that had occurred had been \"directed\" at those genes involved in lactose utilization.\n\nLater support for this hypothesis came from Susan Rosenberg, then at the University of Alberta, who found that an enzyme involved in DNA recombinational repair, recBCD, was necessary for the directed mutagenesis observed by Cairns and colleagues in 1989. The directed mutagenesis hypothesis was challenged in 2002, by work showing that the phenomenon was due to general hypermutability due to selected gene amplification, followed by natural selection, and was thus a standard Darwinian process. Later research from 2007 however, concluded that amplification could not account for the adaptive mutation and that \"mutants that appear during the first few days of lactose selection are true revertants that arise in a single step\".\n"}
{"id": "39043368", "url": "https://en.wikipedia.org/wiki?curid=39043368", "title": "Germ theory denialism", "text": "Germ theory denialism\n\nGerm theory denialism is the belief that germs do not cause infectious disease, and that the germ theory of disease is wrong. It usually involves arguing that Louis Pasteur's model of infectious disease was wrong, and that Antoine Béchamp's was right. In fact, its origins are rooted in Béchamp's empirically disproved (in the context of disease) theory of pleomorphism. Another obsolete variation is known as terrain theory and postulates that diseased tissue attracts germs rather than being caused by it.\n\nGerm theory denialism (GTD) is as old as germ theory itself beginning with the rivalry of Pasteur and Béchamp. Pasteur's work in preventing beverage contamination led him to discover that it was due to microorganisms and led him down the path to be the first scientist to show the theory was valid and popularize it in Europe. Although he was not the first to have the idea and scientists such as Girolamo Fracastoro (had the idea that fomites could harbor the seeds of contagion), Agostino Bassi (discovered the muscardine disease of silkworms was caused by a fungus that was named Beauveria bassiana), Friedrich Henle (developed the concepts of contagium vivum and contagium animatum), and others had earlier proposed ideas similar to germ theory.\n\nBéchamp strongly contested this view offering up a competing idea known as the pleomorphic theory of disease. This theory says that all life is based on forms that a certain class of organisms take during stages of their life-cycles and that germs are attracted to the environment of diseased tissue rather than being the cause of it. Proponents of this idea insist microbes that live in an organism go through the same stages of their development. According to Günther Enderlein they are as follows: Colloid — microbe (primitive phase), bacteria (middle phase), and fungus (end phase).\n\nEarlier non-germ theories, in addition to the earlier idea of miasma, focused on spontaneous generation - the idea that living matter could arise from non-living - and the terrain theory variation of Béchamp's ideas. Pasteur disproved spontaneous generation with a series of experiments in the 1870s. However, understanding the cause of a sickness does not always immediately lead to effective treatment of sickness and the great decline in mortality during the 19th century is mostly associated with improvement in hygiene and cleanliness. In fact, one of the first movements to deny the germ theory was the Sanitary Movement, and was nevertheless central in developing America's public health infrastructure. By providing clean water and sanitation there was less of an environment for pathogens to develop and mortality rates fell dramatically.\n\nGTD has significant overlap with chiropractic practice. Many chiropractors believe immunity to be a function of spine alignment and the brain's ability to communicate efficiently with the body and that it has little to nothing to do with external pathogens.\n\nA common thread among many alternative medicine proponents is opposition to vaccines and many use GTD to justify their claims. Germ theory deniers make many claims about the biological underpinnings of the theory and the historical record that are at odds with what is accepted by most modern scientists and historians. Another popular claim from the anti-vaccine community is that all diseases are caused by toxemia due to inadequate diet and health practices.\n\nHarriet Hall published an article in \"Skeptic\" where she describes her experience arguing with germ theory deniers.\n\nMembers of the medical community that are also skeptics, such as David Gorski and Steven Novella, point out that denying germ theory is counter to years of experiments and the prevailing opinion of most doctors and scientists.\n\n"}
{"id": "29432063", "url": "https://en.wikipedia.org/wiki?curid=29432063", "title": "Giulio Paolini", "text": "Giulio Paolini\n\nGiulio Paolini (born 5 November 1940) is an Italian artist associated with both Arte Povera and Conceptual Art.\n\nPaolini was born in Genoa. After a childhood spent in Bergamo, he moved with his family to Turin where he still lives today. He attended the Giambattista Bodoni State Industrial Technical School of Graphics and Photography, graduating in the Graphics department in 1959. He had been interested in art from an early age, visiting museums and galleries and reading art periodicals. Towards the end of the 1950s he approached painting, trying some pictures of an abstract nature, close to monochrome. The discovery of modern graphics during his studies and the fact that there were architecture magazines around the house – his elder brother Cesare (1937–1983) was an architect – contributed to orienting him towards a line of research aimed at zeroing the image.\n\nHe did his first work in 1960, \"Disegno geometrico\" (\"Geometrical Drawing\"), which consists of the squaring in ink of a canvas painted with white tempera. This preliminary gesture of any representation whatever would remain the point of “eternal recurrence” in the universe of Paolini’s thought: topical moment and original instant that revealed the artist to himself, representing the conceptual foundation of all his future work.\n\nIn the early 1960s, Paolini developed his research by focusing on the very components of the picture: on the painter’s tools and on the space of representation. For his first solo show – in 1964 at Gian Tommaso Liverani’s La Salita gallery in Rome – he presented some rough wooden panels leant against or hanging on the wall, suggesting an exhibition in the process of being set up. The show was seen by Carla Lonzi and Marisa Volpi who would shortly afterwards write the first critical texts on the young artist. In 1965 Paolini began to use photography, which allowed him to extend his inquiry to the relationship between artist and work (\"Delfo\", 1965; \"1421965\", 1965). In the same year, through Carla Lonzi, he met Luciano Pistoi, owner of the Galleria Notizie in Turin, who introduced him to a new circle of friends and collectors and became his main dealer until the beginning of the 1970s.\n\nBetween 1967 and 1972, the critic Germano Celant invited him to take part in Arte Povera exhibitions which resulted in his name being associated with that movement. In fact Paolini’s position was clearly distinct from the vitalistic climate and “existential phenomenology” that distinguished the propositions of Celant’s artists. He repeatedly declared an intimate belonging to the history of art, identifying programmatically with the lineage of all the artists who had preceded him. Some of his best known works can be traced back to this purpose, extraneous to the militant scene of the late 1960s: \"Giovane che guarda Lorenzo Lotto\" (\"Young Man Looking at Lorenzo Lotto\", 1967), the “self-portraits” from Poussin and Rousseau (1968) and the pictures in which he reproduces details of old masters’ paintings (\"L’ultimo quadro di Diego Velázquez\", 1968; \"Lo studio\", 1968). Among Paolini’s main references in those years were Jorge Luis Borges, to whom he paid homage on several occasions, and Giorgio de Chirico from whom he borrowed the constituent phrase of the work \"Et.quid.amabo.nisi.quod.ænigma est\" (1969).\n\nHis first official acknowledgements came with the 1970s: from shows abroad, which placed him on the international avant-garde gallery circuit, to his first museum exhibitions. In 1970 he took part in the Venice Biennale with \"Elegia\" (\"Elegy\", 1969), the first work in which he used the plaster cast of a classic subject: the eye of Michelangelo’s \"David\" with a fragment of mirror applied to the pupil. One of the outstanding themes in this decade was a backward glance at his own work: from literal citation of celebrated paintings he arrived at self-citation, proposing a historicizing in perspective of his oeuvre. Works such as \"La visione è simmetrica?\" (\"Is Vision Symmetrical?\", 1972) or \"Teoria delle apparenze\" (\"Theory of Appearances\", 1972) allude to the idea of the picture as potential container of all past and future works. Another theme investigated with particular interest in this period was that of the double and the copy, which found expression above all in the group of works entitled \"Mimesi\" (\"Mimesis\", 1975–76) consisting of two plaster casts of the same classical statue set face to face, calling into question the concept of reproduction and representation itself.\n\nThe period most dense in exhibitions and retrospectives, with the publication of important monographs, was the 1980s. In the first half of the decade an explicitly theatrical dimension began to establish itself with works marked by fragmentation and dispersion (\"La caduta di Icaro\", 1982; \"Melanconia ermetica\", 1983) or distinguished by theatrical figures such as eighteenth century valets de chambre (\"Trionfo della rappresentazione\", 1984). Paolini’s poetics was considerably enriched by literary attributions and mythological references, as well as by the introduction of cosmic images. In the late 1980s the artist's reflections turned mainly on the very act of exhibiting. Starting with his solo show at the Musée des Beaux-Arts in Nantes (1987) the concept of the exhibition – its premises and its promises – became progressively configured as the actual subject of the works themselves.\n\nIn the course of the 1990s, further inquiry into the idea of exhibiting spread into other, new modalities. The increasingly complex set-ups often followed a typology that was additive (seriality, juxtaposition) or centrifugal (dispersion or dissemination from a central nucleus) or centripetal (concentration and implosive superimposition). The place of the exhibition became the stage par excellence of the “theatre of the opus”, meaning of the work in its doing and undoing: the place that defined the very eventuality of its happening (\"Esposizione universale\", 1992; \"Teatro dell’opera\", 1993; \"Essere o non essere\", 1995). Completion of the work was moreover constantly deferred, leaving the spectator in perennial expectation: just what the artist always feels from the start at his worktable, waiting for the work to manifest itself.\n\nIn the 2000s, another theme especially dear to Paolini took on special importance, as much in his artwork as in his writings: the identity of the author, his condition as spectator, his lack of contact with a work that always precedes and supersedes him.\n\nPaolini’s poetics and artistic practice as a whole may be characterised as a self-reflective meditation on the dimension of art, on its timeless “classicality” and its perspective without vanishing point. By means of photography, collage, plaster casts and drawing his intention is always to inquire, with great conceptual rigour, into the tautological and at the same time metaphysical nature of artistic practice.\n\nSince his first solo show (Rome, 1964) Paolini has exhibited in art galleries and museums worldwide. Collaboration with avant-garde Italian galleries of the 1960s and 1970s (La Salita, Rome; Galleria Notizie, Turin; Galleria dell'Ariete, Milan; Galleria del Leone, Venice; La Tartaruga, Rome; L'Attico, Rome; Studio Marconi, Milan; Modern Art Agency, Naples) was swiftly integrated with regular presence in important foreign galleries (from 1971 Paul Maenz, Cologne; from 1972 Sonnabend, New York City; from 1973 Annemarie Verna, Zurich; from 1976 Yvon Lambert, Paris; from 1977 Lisson Gallery, London). Since the 1980s Paolini has mainly been represented by the galleries Christian Stein, Milan; Massimo Minini, Brescia; Alfonso Artiaco, Naples; Yvon Lambert, Paris and Marian Goodman, New York City.\n\nThe great anthological exhibitions took off towards the late 1970s (Istituto di Storia dell'Arte dell'Università di Parma, Parma, 1976; Städtisches Museum, Mönchengladbach, 1977; Mannheimer Kunstverein, Mannheim, 1977; Museo Diego Aragona Pignatelli Cortes, Naples, 1978; Stedelijk Museum, Amsterdam, touring to The Museum of Modern Art, Oxford, 1980) and culminated in the second half of the 1980s (Le Nouveau Musée, Villeurbanne, 1984, touring to Montreal, Vancouver and Charleroi; Staatsgalerie, Stuttgart, 1986; Castello di Rivoli, Rivoli, 1986; Galleria Nazionale d'Arte Moderna, Rome, 1988; Galleria Comunale d'Arte Moderna, Villa delle Rose, Bologna, 1990). Outstanding recent solo shows were held in Graz (Neue Galerie im Landesmuseum Joanneum, 1998), Turin (Galleria Civica d'Arte Moderna e Contemporanea, 1999), Verona (Galleria d'Arte Moderna e Contemporanea Palazzo Forti, 2001), Milan (Fondazione Prada, 2003), Winterthur (Kunstmuseum Winterthur, 2005) and Münster (Westfälisches Landesmuseum für Kunst und Kulturgeschichte, 2005). For the season 2002/2003 in the Vienna State Opera Giulio Paolini designed a large scale picture (176 sqm) as part of the exhibition series \"Safety Curtain\", conceived by museum in progress.\n\nGroup exhibitions, innumerable since his participation in the 1961 \"Premio Lissone\", include the shows connected with Arte Povera (1967–1971, 1984–85, 1997, 2001–02), the main international exhibitions of Italian art and many of the most significant shows dedicated to artistic development in the second half of the 20th century (for example: \"Vitalità del negativo\", Rome 1970; \"Contemporanea\", Rome 1973; \"Projekt '74\", Cologne 1974; \"Europe in the Seventies\", Chicago and touring through the United States 1977–78; \"Westkunst\", Cologne 1981; \"‘60–'80': Attitudes/concepts/images\", Amsterdam 1982; \"An International Survey of Recent Painting and Sculpture\", New York City 1984; \"The European Iceberg\", Toronto 1985; \"Transformations in Sculpture\", New York City 1985; \"Bilderstreit\", Cologne 1989; \"1965–1975: Reconsidering the Object of Art\", Los Angeles 1995; \"The Last Picture Show: Artists Using Photography, 1960–82\", Minneapolis and touring 2003–05). Paolini has appeared several times at documenta Kassel (1972, 1977, 1982, 1992) and the Venice Biennale (1970, 1976, 1978, 1980, 1984, 1986, 1993, 1995, 1997). In 2014, the Whitechapel Gallery in London staged Giulio Paolini: To Be or Not To Be, an exhibition of Paolini's sculptures, exhibitions and installations. \"Giorgio De Chirico-Giulio Paolini Giuilo Paolini Giorgio De Chirico\" Center for Italian Modern Art, Oct. 13, 2016-June 24, 2017, New York, NY, italianmodernart.org\n\nIn the course of his career Paolini has also worked in the theatre, from the sets and costumes for Vittorio Alfieri’s \"Bruto II\", directed by Gualtiero Rizzi (1969), to his collaboration with Carlo Quartucci and the Zattera di Babele in the 1980s. Outstanding recent projects include the sets for Wagner’s Die Walküre (2005) and Parsifal (2007) at the Teatro di San Carlo in Naples, directed by Federico Tiezzi.\n\nRight from the start Paolini’s productions have been accompanied by written reflections and comments, seen as elements complementary to and parallel with the image. His first collection of texts, \"Idem\", was published by Einaudi in 1975 with an essay by Italo Calvino. Recent collections include \"Quattro passi. Nel museo senza muse\" (Einaudi, Turin 2006) and \"Dall'Atlante al Vuoto (in ordine alfabetico)\" published by Mondadori Electa, Milan 2010. In 1995 Maddalena Disch edited a complete edition of his writings and interviews (\"Giulio Paolini: la voce del pittore. Scritti e interviste 1965–1995\", ADV Publishing House, Lugano).\n\nThe first monograph on the artist, by Germano Celant, was published in 1972 in New York City by Sonnabend Press. The most significant books on Giulio Paolini, including critical anthologies and a wealth of documentation, are the catalogues brought out on the occasion of his solo shows in Parma (1976), Ravenna (1985, \"Giulio Paolini. Tutto qui\", Edizioni Essegi, Ravenna), Stuttgart (1986), Rome (1988), Graz (1998) and Milan (2003). In 1990 Francesco Poli edited a monograph for Edizioni Lindau of Turin. In 1992 Marco Noire published \"Impressions graphiques. L’opera grafica 1967–1992 di Giulio Paolini\", a general catalogue of his prints and multiples. In 2008 the publisher Skira of Milan brought out a two volume Catalogue Raisonné of Paolini's works from 1960 to 1999, edited by Maddalena Disch.\n\n"}
{"id": "4413754", "url": "https://en.wikipedia.org/wiki?curid=4413754", "title": "Gravitational energy", "text": "Gravitational energy\n\nGravitational energy is the potential energy a body with mass has in relation to another massive object due to gravity. It is potential energy associated with the gravitational field. Gravitational energy is dependent on the masses of two bodies, their distance apart and the gravitational constant ().\n\nIn everyday cases only one body is accelerating measurably, and its acceleration is constant (for example, dropping a ball on Earth). For such scenarios the Newtonian formula can – for the potential energy of the accelerating body with respect to the stationary – be reduced to:\n\nwhere formula_2 is the gravitational potential energy, formula_3 is the mass of the object accelerating, formula_4 is the acceleration of the object, and formula_5 is the distance between the bodies. Note that this formula treats the potential energy as a positive quantity.\n\nIn classical mechanics, two or more masses always have a gravitational potential. Conservation of energy requires that this gravitational field energy is always negative. The gravitational potential energy is the potential energy an object has because it is within a gravitational field.\n\nThe force one point mass formula_6 exerts onto another point mass formula_3 is given by Newton's law of gravitation: formula_8\n\nTo get the total work done by an external force to bring point mass formula_3 from infinity to the final distance formula_10 (for example the radius of Earth) of the two mass points, the force is integrated with respect to displacement:\n\nBecause formula_13, the total work done on the object can be written as:\n\nIn general relativity gravitational energy is extremely complex, and there is no single agreed upon definition of the concept. It is sometimes modeled via the Landau–Lifshitz pseudotensor that allows retention for the energy-momentum conservation laws of classical mechanics. Addition of the matter stress–energy–momentum tensor to the Landau–Lifshitz pseudotensor results in a combined matter plus gravitational energy pseudotensor that has a vanishing 4-divergence in all frames - ensuring the conservation law. Some people object to this derivation on the grounds that pseudotensors are inappropriate in general relativity, but the divergence of the combined matter plus gravitational energy pseudotensor is a tensor.\n\n"}
{"id": "4880917", "url": "https://en.wikipedia.org/wiki?curid=4880917", "title": "Grey-collar", "text": "Grey-collar\n\nGrey-collar refers to the balance of employed people not classified as white- or blue collar. It is used to refer to occupations that incorporate some of the elements of both blue- and white-collar, and generally are in between the two categories in terms of income-earning capability.\n\nExamples of grey-collar industries:\n\nGrey-collar workers often have licenses, associate degrees or diplomas from a trade or technical school in a particular field. They are unlike blue-collar workers in that blue-collar workers can often be trained on the job within several weeks whereas grey-collar workers already have a specific skill set and require more specialized knowledge than their blue-collar counterparts.\n\nThe field which most recognizes the diversity between these two groups is that of human resources and the insurance industry. These different groups must be insured differently for liability as the potential for injury is different.\n\nThe Pittsburgh Post-Gazette wrote that another definition for grey collar could be the underemployed white collar worker.\n\nCharle Brecher of the Citizens Budget Commission and the Partnership for New York City defined it sub-blue-collar jobs: \"maintenance and custodial\".\n\n"}
{"id": "14080364", "url": "https://en.wikipedia.org/wiki?curid=14080364", "title": "HINDRAF", "text": "HINDRAF\n\nHINDRAF or Hindu Rights Action Force (, Tamil: இந்து உரிமைகள் போராட்டக் குழு, Chinese: 兴权 ) with its slogan of \"People's Power\" (மக்கள் சக்தி translated as \"Makkal Sakthi\") began as a coalition of 30 Hindu non-governmental organisations committed to the preservation of Hindu community rights and heritage in a multiracial Malaysia.\nHINDRAF has made a major impact to the political landscape of Malaysia in staging the 2007 HINDRAF rally. In late 2007, several prominent members of the HINDRAF were arrested, some on charges of sedition; following an enormous rally organised by HINDRAF in November. The charges were dismissed by the courts. Five people have since been detained without trial under the Internal Security Act. The group has over the last two years developed a broader political program to preserve and to push for equal rights and opportunities for the minority Indians. It has been successful in continuing to focus attention on the racist aspects of the Malaysian Government policies.\n\nBetween April to May 2006, several legal Hindu temples were demolished by city hall authorities in the country. On 21 April 2006, the Malaimel Sri Selva Kaliamman Temple in Kuala Lumpur was reduced to rubble after the city hall sent in bulldozers.\n\nThe Hindu Rights Action Force or HINDRAF, a coalition of several NGO's, have protested these demolitions by lodging complaints with the Prime Minister of Malaysia but with no response. Many Hindu advocacy groups have protested what they allege is a systematic plan of temple cleansing in Malaysia. The official reason given by the Malaysian government has been that the temples were built illegally. However, several of the temples are centuries old.\nAccording to a lawyer for HINDRAF, a Hindu temple is demolished in Malaysia once every three weeks.\n\n\nOn 30 October, four HINDRAF Group fellows and human rights, namely M. Manoharan, P. Uthayakumar, P. Waytha Moorthy and V. Ganabathirau, were arrested and detained for taking part in the 2007 HINDRAF demonstration against the demolishing of a Hindu shrine in Kuala Lumpur. However, they were acquitted due to a lack of evidence of incitement and sedition.\n\nA series of peaceful weekend forums were organised throughout Malaysia to increase the awareness of Hindu human rights by HINDRAF. A previous forum held near central Kuala Lumpur had been disrupted by the Royal Malaysian Police, according to HINDRAF.\nSubsequently, HINDRAF appealed directly to the Inspector General of the Malaysian Police in an attempt to ensure future forums went on peacefully.\n\nOn 23 November 2007, three HINDRAF, P. Uthayakumar, Waytha Moorthy, and V. Ganabathirau, were arrested and charged under the Sedition Act. However, in a series of repeated arrests and releases, the courts could not prove that they had incited racial hatred. The only evidence against them were unreliable translations of their Tamil speeches into Bahasa Malaysia presented by the Attorney-General's Chambers, which the courts deemed as unverifiable. Eventually, they were all acquitted due to a shaky prosecution and the lack of evidence of any wrongdoing or crime.\n\nOn 31 August 2007, the 50th anniversary of Malaysia's independence, P. Waytha Moorthy, a HINDRAF lawyer filed a class action suit against the Government of the United Kingdom at the Royal Courts of Justice in London for US$4 trillion (US$1 million for every Malaysian Indian) for \"withdrawing after granting independence and leaving us (Indians) unprotected and at the mercy of a majority Malay-Muslim government that has violated our rights as minority Indians\" as guaranteed in the Federal Constitution when independence was granted.\n\nThe lawsuit is not only claiming 4 trillion British Pounds as compensation, it is also seeking to strike out Article 153 of the Malaysian Constitution which acknowledges Malay Supremacy and for the court to declare that Malaysia is a secular state and not an Islamic state as declared by former Prime Minister Tun Dr. Mahathir Mohamad who is quarterly of Indian-Muslim descent.\n\nAs the group, which represents mainly working class Malaysian Indians, could not afford the legal fees required, a petition was circulated with 100,000 signatures to be presented to Queen Elizabeth II to appoint a Queen's Counsel to argue the case. The purpose of the rally was to hand over a 100,000 signature memorandum to the British Embassy in Kuala Lumpur.\n\nHINDRAF organised the rally on Sunday, 25 November 2007 to submit the petition at the British High Commission. Malaysian police refused to grant a permit for the rally, and set up roadblocks in Klang Valley along roads leading up to the rally to screen motorists entering the city centre and identify \"troublemakers\". They also advised the public not to participate in the rally, and arrested three leaders of HINDRAF. Many shops around Kuala Lumpur including Suria KLCC were closed on that day in fear of trouble from the rally.\n\nOne day before the rally, police arrested three HINDRAF lawyers, P. Uthayakumar, P. Waytha Moorthy and V. Ganabathirau for sedition charges. Uthayakumar and Ganabathirau posted bail of 800 Malaysian ringgits each, but Waytha Moorthy refused bail as a sign of protest.\n\nThe police roadblocks started the week before the rally to create massive traffic jams across the city and the outskirts of Kuala Lumpur. The Malaysian Opposition leader Lim Kit Siang of the DAP pointed out that this high-handed act by the police was unnecessary as it caused major inconvenience to everyone.\nOn the morning of the rally, an estimated twenty thousand people gathered near the Petronas Twin Towers in Kuala Lumpur, carrying life-size portraits of Queen Elizabeth and Mahatma Gandhi, to indicate the nonviolent nature of their protest. Five thousand members riot police dispatched to the scene used tear gas and water cannon to disperse the crowds. 136 people were arrested.\n\nAl-Jazeera's coverage of the event showed police officers using tear gas to disperse the protesters. A few hundred protesters and three police officers were injured.\n\nThe protest at the Batu Caves resulted in minor property damages, although the Hindu temple itself was not damaged.\n\nHINDRAF later claimed to have faxed the petition to the British High Commission staff. However, as of 28 November, the British Envoy had not yet received any petition from the HINDRAF, though they did say they had received some unspecified information by fax.\n\nMalaysian prime minister, Najib Tun Razak warned that the government will invoke the Internal Security Act against the demonstrators if they needed. The prime minister further criticised the demonstrators, after he made a promise that he will listen to everyone even if they have unpleasant words to say, the government of Abdullah also attempted to link terrorism with the Hindraf rally via the media.\n\nAs of 11 December 2007, the HINDRAF leaders were all acquitted by the judicial courts due to lack of evidence and a flimsy prosecution case against their allegations. To contain the movement while not being able to charge them according to valid evidence-based legal processes, on 12 December 2007 Prime Minister Abdullah Badawi personally signed the detention letters to imprison the HINDRAF leaders under the ISA for two years, in which their detention terms are subject to infinite renewal. The reason given for this arrest was that the HINDRAF leadership has had links with international terrorist organisations such as LTTE and also supposedly militant organisations in the mould of RSS in India. The invocation of the ISA to capture the HINDRAF leaders was seen as a strategic move by the UMNO government to arrest the momentum generated by HINDRAF.\nThe UMNO lead Government has threatened the Malaysian Indian community with sweeping arrests under the Emergency Act and ISA (similar to Operasi Lalang of the 1987, which targeted anti-BN elements in Malaysia, mostly of Malaysian Chinese extraction). This hardline approach is also softened by the MIC reconciliatory approach to blunt HINDRAF's thrust as the champion of the Malaysian Indian community.\n\nEven as Prime Minister Abdullah Ahmad Badawi started threatening to use the ISA against the HINDRAF leaders for bringing Malaysia's racist policies out into the open for all to see, foreign news outlets criticised Badawi's lack of initiative to tackle the root cause of the problem. The detention without trial of the HINDRAF leaders drew negative comments in the foreign press about Badawi's administration and the poor way that the Government of Malaysia were handling the issue.\n\nThe Democratic Action Party vowed to challenge the detention of the HINDRAF leaders. Despite the arrests, the opposition and most of the NGOs were unfazed and continued to challenge UMNO's deconstruction of democracy in Malaysia. The United States had also voiced their disapproval of this latest round of ISA arrests.\n\nThe official HINDRAF website at http://www.policewatchmalaysia.com has been allowed by Malaysian ISPs again, after a brief ban. However this site is constantly plagued by faults and downtime. In response to the ban, sites such as http://www.hindraf.org, http://www.myhindraf.com were spawned to maintain awareness of this movement, in addition to the many blogs available. The movement started in Malaysia, has grown global and now has following in UK, Australia, Canada and USA.\n\nThere has also been candlelight vigils at Hindu temples throughout Malaysia to protest the detention of five leaders of the HINDRAF. This was condemned by Malaysian minister Samy Vellu.\n\nThe Rose to the PM campaign was mooted to present a humanistic element in HINDRAF's campaign. The central focus of this campaign was the delivery of a rose, as a symbol of love and compassion, to the Malaysian Prime Minister at the Malaysian Parliament by Vwaishnavi Wathya Moorthy (aged 5). This symbolic act was to occur on 16 February 2008, but the Malaysian Lower House was dissolved for the Federal Elections on 13 February 2008.\n\nIn a dramatic show of force, the police fired teargas and targeted water cannon at several hundred ethnic Tamils at the centre of Kuala Lumpur. More than 200 people were detained by the authorities after being attacked by the police near the site of an Indian temple.\n\nThe 12th general Election showed how HINDRAF had become one of the triggers for a major change in the course of the country. The general dissatisfaction with the regime ruled by UMNO which had been brewing for some years was the kerosene and Hindraf Rally of 25 November was the tinder that sparked off the kerosene into a major explosion in what has come to be called a Political Tsunami in Malaysian politics.\n\nThe ruling UMNO led government lost its two-thirds majority in Parliament and came close to getting just over half the seats in Parliament from the Peninsula. HINDRAF which had barely existed for 3 years up to that time, and which was barely known up till August 2007 suddenly had caught the mood of a large proportion of Malaysians, not only Tamils and Hindus but the Chinese and a sizeable section of the Malays as well, causing a major upset in the process.\n\nAfter several warnings by the Malaysian government HINDRAF was officially banned on 15 October 2008, confirmed by Malaysian Home Minister Datuk Seri Syed Hamid Albar.\n\nIn a statement issued at the ministry, Syed Hamid said the decision to declare HINDRAF as an illegal organisation was made following the ministry being satisfied with facts and evidence that showed HINDRAF had and was being used for unlawful purposes and posed a threat to public order and morality.\n\n\"Based on powers vested under Section 5(1) of the Societies Act, HINDRAF from today is declared an illegal organisation,\" he said.\n\nHe said the order was being made as a result of monitoring and investigation on the organisation's activities by the Registrar of Societies (ROs) and Home Ministry, since Hindraf's inception.\n\nOn 23 October 2008, a group comprising eight men, three women and a child, were arrested by the police after they tried to hand a memorandum to the Prime Minister's office. The memorandum called for the release of the five Hindraf leaders from detention under the Internal Security Act.\n\nOn 27 February 2011, HINDRAF organised a demonstration in Kuala Lumpur protesting against the government's decision to include the Malay language novel \"Interlok\" in the school curriculum as compulsory reading for the Malay literature subject for students in secondary 5. HINDRAF alleges that \"Interlok\" contained disparaging remarks against Malaysian Indians and is deemed racist. The police arrested 109 people for allegedly taking part in an illegal demonstration.\n\nThe ban imposed on this minority right group was later lifted by the Malaysian Home Ministry on 26 January 2013 and on 8 March 2013 the Malaysian Registrar of Societies had approved the registration of Hindraf. Recently on 18 April 2013, fractions of Hindraf which were led by P. Waythamoorthy signed a memorandum of understanding (MoU) with BN in which they would work together to uplift displaced estate workers, resolve the issue of stateless persons and provide business opportunities thus bringing poor Indians into the mainstream of the country's development while others spreading out to parties within the federal opposition Pakatan Rakyat (PR), mostly to DAP or PKR\n\n\n\n"}
{"id": "14091", "url": "https://en.wikipedia.org/wiki?curid=14091", "title": "Habeas corpus", "text": "Habeas corpus\n\nHabeas corpus (; Medieval Latin meaning literally \"that you have the body\") is a recourse in law through which a person can report an unlawful detention or imprisonment to a court and request that the court order the custodian of the person, usually a prison official, to bring the prisoner to court, to determine whether the detention is lawful.\n\nThe writ of \"habeas corpus\" is known as \"the great and efficacious writ in all manner of illegal confinement\". It is a summons with the force of a court order; it is addressed to the custodian (a prison official, for example) and demands that a prisoner be taken before the court, and that the custodian present proof of authority, allowing the court to determine whether the custodian has lawful authority to detain the prisoner. If the custodian is acting beyond his or her authority, then the prisoner must be released. Any prisoner, or another person acting on his or her behalf, may petition the court, or a judge, for a writ of \"habeas corpus\". One reason for the writ to be sought by a person other than the prisoner is that the detainee might be held incommunicado. Most civil law jurisdictions provide a similar remedy for those unlawfully detained, but this is not always called \"habeas corpus\". For example, in some Spanish-speaking nations, the equivalent remedy for unlawful imprisonment is the \"amparo de libertad\" (\"protection of freedom\").\n\n\"Habeas corpus\" has certain limitations. Though a writ of right, it is not a writ of course. It is technically only a procedural remedy; it is a guarantee against any detention that is forbidden by law, but it does not necessarily protect other rights, such as the entitlement to a fair trial. So if an imposition such as internment without trial is permitted by the law, then \"habeas corpus\" may not be a useful remedy. In some countries, the writ has been temporarily or permanently suspended under the pretext of war or state of emergency.\n\nThe right to petition for a writ of \"habeas corpus\" has nonetheless long been celebrated as the most efficient safeguard of the liberty of the subject. The jurist Albert Venn Dicey wrote that the British Habeas Corpus Acts \"declare no principle and define no rights, but they are for practical purposes worth a hundred constitutional articles guaranteeing individual liberty\".\n\nThe writ of \"habeas corpus\" is one of what are called the \"extraordinary\", \"common law\", or \"prerogative writs\", which were historically issued by the English courts in the name of the monarch to control inferior courts and public authorities within the kingdom. The most common of the other such prerogative writs are \"quo warranto\", \"prohibito\", \"mandamus\", \"procedendo\", and \"certiorari\". The due process for such petitions is not simply civil or criminal, because they incorporate the presumption of non-authority. The official who is the respondent must prove his authority to do or not do something. Failing this, the court must decide for the petitioner, who may be any person, not just an interested party. This differs from a motion in a civil process in which the movant must have standing, and bears the burden of proof.\n\nFrom Latin \"habeas\", 2nd person singular present subjunctive active of \"habere\", \"to have\", \"to hold\"; and \"corpus\", accusative singular of \"corpus\", \"body\". In reference to more than one person, \"habeas corpora\".\n\nLiterally, the phrase means \"[we command] that you should have the [detainee's] body [brought to court]\". The complete phrase \"habeas corpus ad subjiciendum\" means \"that you have the person for the purpose of subjecting him/her to (examination)\". These are the opening words of writs in 14th century Anglo-French documents requiring a person to be brought before a court or judge, especially to determine if that person is being legally detained.\n\nThe full name of the writ is often used to distinguish it from similar ancient writs, also named \"habeas corpus\". These include:\n\n\"Habeas corpus\" originally stems from the Assize of Clarendon, a re-issuance of rights during the reign of Henry II of England. In the 17th century, the foundations for \"habeas corpus\" were \"wrongly thought\" to have originated in Magna Carta. This charter declared that:\n\nWilliam Blackstone cites the first recorded usage of \"habeas corpus ad subjiciendum\" in 1305, during the reign of King Edward I. However, other writs were issued with the same effect as early as the reign of Henry II in the 12th century. Blackstone explained the basis of the writ, saying \"[t]he king is at all times entitled to have an account, why the liberty of any of his subjects is restrained, wherever that restraint may be inflicted.\" The procedure for issuing a writ of \"habeas corpus\" was first codified by the Habeas Corpus Act 1679, following judicial rulings which had restricted the effectiveness of the writ. A previous law (the Habeas Corpus Act 1640) had been passed forty years earlier to overturn a ruling that the command of the King was a sufficient answer to a petition of \"habeas corpus\". The cornerstone purpose of the <nowiki>\"writ of habeas corpus\" was to limit the King's Chancery's ability to undermine the surety of law by allowing courts of justice decisions to be overturned in favor and application of \"equity\", a process managed by the Chancelor (a bishop) with the King'</nowiki>s authority.\n\nThe 1679 codification of \"habeas corpus\" took place in the context of a sharp confrontation between King Charles II and the Parliament, which was dominated by the then sharply oppositional, nascent Whig Party. The Whig leaders had good reasons to fear the King moving against them through the courts (as indeed happened in 1681) and regarded \"habeas corpus\" as safeguarding their own persons. The short-lived Parliament which made this enactment came to be known as the \"Habeas Corpus Parliament\" - being dissolved by the King immediately afterwards.\n\nThen, as now, the writ of \"habeas corpus\" was issued by a superior court in the name of the Sovereign, and commanded the addressee (a lower court, sheriff, or private subject) to produce the prisoner before the royal courts of law. A \"habeas corpus\" petition could be made by the prisoner him or herself or by a third party on his or her behalf and, as a result of the Habeas Corpus Acts, could be made regardless of whether the court was in session, by presenting the petition to a judge. Since the 18th century the writ has also been used in cases of unlawful detention by private individuals, most famously in \"Somersett's Case\" (1772), where the black slave Somersett was ordered to be freed. During that case, these famous words are said to have been uttered: \"The air of England has long been too pure for a slave, and every man is free who breathes it\". During the Seven Years' War and later conflicts, the Writ was used on behalf of soldiers and sailors pressed into military and naval service. The Habeas Corpus Act 1816 introduced some changes and expanded the territoriality of the legislation.\n\nThe privilege of \"habeas corpus\" has been suspended or restricted several times during English history, most recently during the 18th and 19th centuries. Although internment without trial has been authorised by statute since that time, for example during the two World Wars and the Troubles in Northern Ireland, the \"habeas corpus\" procedure has in modern times always technically remained available to such internees. However, as \"habeas corpus\" is only a procedural device to examine the lawfulness of a prisoner's detention, so long as the detention is in accordance with an Act of Parliament, the petition for \"habeas corpus\" is unsuccessful. Since the passage of the Human Rights Act 1998, the courts have been able to declare an Act of Parliament to be incompatible with the European Convention on Human Rights, but such a declaration of incompatibility has no legal effect unless and until it is acted upon by the government.\n\nThe wording of the writ of \"habeas corpus\" implies that the prisoner is brought to the court for the legality of the imprisonment to be examined. However, rather than issuing the writ immediately and waiting for the return of the writ by the custodian, modern practice in England is for the original application to be followed by a hearing with both parties present to decide the legality of the detention, without any writ being issued. If the detention is held to be unlawful, the prisoner can usually then be released or bailed by order of the court without having to be produced before it. With the development of modern public law, applications for habeas corpus have been to some extent discouraged, in favour of applications for judicial review. The writ, however, maintains its vigour, and was held by the UK Supreme Court to be available in respect of a prisoner captured by British forces in Afghanistan, albeit that the Secretary of State made a valid return to the writ justifying the detention of the claimant.\n\nThe writ of \"habeas corpus\" as a procedural remedy is part of Australia's English law inheritance. In 2005, the Australian parliament passed the Australian Anti-Terrorism Act 2005. Some legal experts questioned the constitutionality of the act, due in part to limitations it placed on \"habeas corpus\".\n\n\"Habeas corpus\" rights are part of the British legal tradition inherited by Canada. The rights exist in the common law but have been enshrined in the Constitution Act 1982, under Section Ten of the Charter of Rights and Freedoms. This states that \"Everyone has the right on arrest or detention ... (c) to have the validity of the detention determined by way of \"habeas corpus\" and to be released if the detention is not lawful\".\n\nSuspension of the writ in Canadian history occurred famously during the October Crisis, during which the War Measures Act was invoked by the Governor General of Canada on the constitutional advice of Prime Minister Pierre Trudeau, who had received a request from the Quebec Cabinet. The Act was also used to justify German, Slavic, and Ukrainian Canadian internment during the First World War, and the internment of German-Canadians, Italian-Canadians and Japanese-Canadians during the Second World War. The writ was suspended for several years following the Battle of Fort Erie (1866) during the Fenian Rising, though the suspension was only ever applied to suspects in the Thomas D'Arcy McGee assassination.\n\nThe writ is available where there is no other adequate remedy. However, a superior court always has the discretion to grant the writ even in the face of an alternative remedy (see \"May v Ferndale Institution\"). Under the Criminal Code the writ is largely unavailable if a statutory right of appeal exists, whether or not this right has been exercised.\n\nA fundamental human right in the \"1789 Declaration of the Rights of Man\" drafted by Lafayette in cooperation with Thomas Jefferson, the guarantees against arbitrary detention are enshrined in the French Constitution and regulated by the Penal Code. The safeguards are equivalent to those found under the Habeas-Corpus provisions found in Germany, the United States and several Commonwealth countries. The French system of accountability prescribes severe penalties for ministers, police officers and civil and judiciary authorities who either violate or fail to enforce the law.\n\n\"Article 7 of [1789] Declaration also provides that 'No individual may be accused, arrested, or detained except where the law so prescribes, and in accordance with the procedure it has laid down.' ... The Constitution further states that 'No one may be arbitrarily detained. The judicial authority, guardian of individual liberty, ensures the observance of this principle under the condition specified by law.' Its article 5 provides that everyone has the right to liberty and sets forth permissible circumstances under which people may be deprived of their liberty and procedural safeguards in case of detention. In particular, it states that 'anyone deprived of his liberty by arrest or detention shall be entitled to take proceedings by which the lawfulness of his detention shall be decided speedily by a court and his release ordered if the detention is not lawful'.\"\n\nFrance and the United States played a synergistic role in the international team, led by Eleanor Roosevelt, which crafted the Universal Declaration of Human Rights. The French judge and Nobel Peace Laureate René Cassin produced the first draft and argued against arbitrary detentions. René Cassin and the French team subsequently championed the \"habeas corpus\" provisions enshrined in the European Convention for the Protection of Human Rights and Fundamental Freedoms.\n\nGermany has constitutional guarantees against improper detention and these have been implemented in statutory law in a manner that can be considered as equivalent to writs of habeas corpus.\n\nArticle 104, paragraph 1 of the Basic Law for the Federal Republic of Germany provides that deprivations of liberty may be imposed only on the basis of a specific enabling statute that also must include procedural rules. Article 104, paragraph 2 requires that any arrested individual be brought before a judge by the end of the day following the day of the arrest. For those detained as criminal suspects, article 104, paragraph 3 specifically requires that the judge must grant a hearing to the suspect in order to rule on the detention.\n\nRestrictions on the power of the authorities to arrest and detain individuals also emanate from article 2 paragraph 2 of the Basic Law which guarantees liberty and requires a statutory authorization for any deprivation of liberty. In addition, several other articles of the Basic Law have a bearing on the issue. The most important of these are article 19, which generally requires a statutory basis for any infringements of the fundamental rights guaranteed by the Basic Law while also guaranteeing judicial review; article 20, paragraph 3, which guarantees the rule of law; and article 3 which guarantees equality.\n\nIn particular, a constitutional obligation to grant remedies for improper detention is required by article 19, paragraph 4 of the Basic Law, which provides as follows: \"Should any person's right be violated by public authority, he may have recourse to the courts. If no other jurisdiction has been established, recourse shall be to the ordinary courts.\"\n\nThe Indian judiciary, in a catena of cases, has effectively resorted to the writ of \"habeas corpus\" to secure release of a person from illegal detention. For example, in October 2009, the Karnataka High Court heard a \"habeas corpus\" petition filed by the parents of a girl who married a Muslim boy from Kannur district and was allegedly confined in a \"madrasa\" in Malapuram town. Usually, in most other jurisdictions, the writ is directed at police authorities. The extension to non-state authorities has its grounds in two cases: the 1898 Queen's Bench case of \"Ex Parte Daisy Hopkins\", wherein the Proctor of Cambridge University did detain and arrest Hopkins without his jurisdiction, and Hopkins was released, and that of \"Somerset v Stewart\", in which an African slave whose master had moved to London was freed by action of the writ.\n\nThe Indian judiciary has dispensed with the traditional doctrine of \"locus standi\", so that if a detained person is not in a position to file a petition, it can be moved on his behalf by any other person. The scope of \"habeas\" relief has expanded in recent times by actions of the Indian judiciary.\n\nIn 1976, the \"habeas\" writ was used in the Rajan case, a student victim of torture in local police custody during the nationwide Emergency in India. On 12 March 2014, Subrata Roy's counsel approached the Chief Justice moving a \"habeas corpus\" petition. It was also filed by the Panthers Party to protest the imprisonment of Anna Hazare, a social activist.\n\nIn the Republic of Ireland, the writ of \"habeas corpus\" is available at common law and under the Habeas Corpus Acts of 1782 and 1816. A remedy equivalent to \"habeas corpus\" is also guaranteed by Article 40 of the 1937 constitution.\n\nThe article guarantees that \"no citizen shall be deprived of his personal liberty save in accordance with law\" and outlines a specific procedure for the High Court to enquire into the lawfulness of any person's detention. It does not mention the Latin term, \"habeas corpus\", but includes the English phrase \"produce the body\".\n\nArticle 40.4.2° provides that a prisoner, or anyone acting on his behalf, may make a complaint to the High Court (or to any High Court judge) of unlawful detention. The court must then investigate the matter \"forthwith\" and may order that the defendant bring the prisoner before the court and give reasons for his detention. The court must immediately release the detainee unless it is satisfied that he is being held lawfully. The remedy is available not only to prisoners of the state, but also to persons unlawfully detained by any private party. However the constitution provides that the procedure is not binding on the Defence Forces during a state of war or armed rebellion.\n\nThe full text of Article 40.4.2° is as follows: \n\nThe writ of \"habeas corpus\" continued as part of the Irish law when the state seceded from the United Kingdom in 1922. A remedy equivalent to \"habeas corpus\" was also guaranteed by Article 6 of the Constitution of the Irish Free State, enacted in 1922. That article used similar wording to Article 40.4 of the current constitution, which replaced it 1937.\n\nThe relationship between the Article 40 and the Habeas Corpus Acts of 1782 and 1816 is ambiguous, and Forde and Leonard write that \"The extent if any to which Art 40.4 has replaced these Acts has yet to be determined\". In \"The State (Ahern) v Cotter\" (1982) Walsh J opined that the ancient writ referred to in the Habeas Corpus Acts remains in existence in Irish law as a separate remedy from that provided for in Article 40.\n\nIn 1941, the Article 40 procedure was restricted by the Second Amendment. Prior to the amendment, a prisoner had the constitutional right to apply to any High Court judge for an enquiry into her detention, and to as many High Court judges as she wished. If the prisoner successfully challenged her detention before the High Court she was entitled to immediate, unconditional release.\n\nThe Second Amendment provided that a prisoner has only the right to apply to a single judge, and, once a writ has been issued, the President of the High Court has authority to choose the judge or panel of three judges who will decide the case. If the High Court finds that the prisoner's detention is unlawful due to the unconstitutionality of a law the judge must refer the matter to the Supreme Court, and until the Supreme's Court's decision is rendered the prisoner may be released only on bail.\n\nThe power of the state to detain persons prior to trial was extended by the Sixteenth Amendment, in 1996. In 1965, the Supreme Court ruled in the \"O'Callaghan\" case that the constitution required that an individual charged with a crime could be refused bail only if she was likely to flee or to interfere with witnesses or evidence. Since the Sixteenth Amendment, it has been possible for a court to take into account whether a person has committed serious crimes while on bail in the past.\n\nThe right to freedom from arbitrary detention is guaranteed by Article 13 of the Constitution of Italy, which states: \n\nIn Malaysia, the remedy of \"habeas corpus\" is guaranteed by the federal constitution, although not by name. Article 5(2) of the Constitution of Malaysia provides that \"Where complaint is made to a High Court or any judge thereof that a person is being unlawfully detained the court shall inquire into the complaint and, unless satisfied that the detention is lawful, shall order him to be produced before the court and release him\".\n\nAs there are several statutes, for example, the Internal Security Act 1960, that still permit detention without trial, the procedure is usually effective in such cases only if it can be shown that there was a procedural error in the way that the detention was ordered.\n\nIn New Zealand, \"habeas corpus\" may be invoked against the government or private individuals. In 2006, a child was allegedly kidnapped by his maternal grandfather after a custody dispute. The father began \"habeas corpus\" proceedings against the mother, the grandfather, the grandmother, the great grandmother, and another person alleged to have assisted in the kidnap of the child. The mother did not present the child to the court and so was imprisoned for contempt of court. She was released when the grandfather came forward with the child in late January 2007.\n\nIssuance of a writ is an exercise of an extraordinary jurisdiction of the superior courts in Pakistan. A writ of habeas corpus may be issued by any High Court of a province in Pakistan. Article 199 of the 1973 Constitution of the Islamic Republic of Pakistan, specifically provides for the issuance of a writ of habeas corpus, empowering the courts to exercise this prerogative. Subject to the Article 199 of the Constitution, \"A High Court may, if it is satisfied that no other adequate remedy is provided by law, on the application of any person, make an order that a person in custody within the territorial jurisdiction of the Court be brought before it so that the Court may satisfy itself that he is not being held in custody without a lawful authority or in an unlawful manner\". The hallmark of extraordinary constitutional jurisdiction is to keep various functionaries of State within the ambit of their authority. Once a High Court has assumed jurisdiction to adjudicate the matter before it, justiciability of the issue raised before it is beyond question. The Supreme Court of Pakistan has stated clearly that the use of words \"in an unlawful manner\" implies that the court may examine, if a statute has allowed such detention, whether it was a colorable exercise of the power of authority. Thus, the court can examine the malafides of the action taken.\n\nIn the Bill of Rights of the Philippine constitution, \"habeas corpus\" is guaranteed in terms almost identically to those used in the U.S. Constitution. in Article 3, Section 15 of the Constitution of the Philippines states that \"The privilege of the writ of \"habeas corpus\" shall not be suspended except in cases of invasion or rebellion when the public safety requires it\".\n\nIn 1971, after the Plaza Miranda bombing, the Marcos administration, under Ferdinand Marcos, suspended \"habeas corpus\" in an effort to stifle the oncoming insurgency, having blamed the Filipino Communist Party for the events of August 21. Many considered this to be a prelude to martial law. After widespread protests, however, the Arroyo administration decided to reintroduce the writ. In December 2009, \"habeas corpus\" was suspended in Maguindanao as the province was placed under martial law. This occurred in response to the Maguindanao massacre.\n\nIn 2016, President Rodrigo Duterte said he was planning on suspending the habeas corpus.\n\nOn May 23, 2017 at 10 pm Philippine time, President Rodrigo Duterte declared martial law in the whole island of Mindanao including Sulu and Tawi-tawi for the period of 60 days due to the series of attacks mounted by the Maute group, an ISIS-linked terrorist organization. The declaration suspends the writ.\n\nThe Parliament of Scotland passed a law to have the same effect as \"habeas corpus\" in the 18th century. This is now known as the Criminal Procedure Act 1701 c.6. It was originally called \"the Act for preventing wrongful imprisonment and against undue delays in trials\". It is still in force although certain parts have been repealed.\n\nThe present Constitution of Spain states that \"A \"habeas corpus\" procedure shall be provided for by law to ensure the immediate handing over to the judicial authorities of any person illegally arrested\". The statute which regulates the procedure is the \"Law of Habeas Corpus of 24 May 1984\", which provides that a person imprisoned may, on her or his own or through a third person, allege that she or he is imprisoned unlawfully and request to appear before a judge. The request must specify the grounds on which the detention is considered to be unlawful, which can be, for example, that the custodian holding the prisoner does not have the legal authority, that the prisoner's constitutional rights have been violated, or that he has been subjected to mistreatment. The judge may then request additional information if needed, and may issue a \"habeas corpus\" order, at which point the custodian has 24 hours to bring the prisoner before the judge.\n\nHistorically, many of the territories of Spain had remedies equivalent to the \"habeas corpus\", such as the privilege of \"manifestación\" in the Crown or Aragon or the right of the Tree in Biscay.\n\nThe United States inherited \"habeas corpus\" from the English common law. In England, the writ was issued in the name of the monarch. When the original thirteen American colonies declared independence, and became a republic based on popular sovereignty, any person, in the name of the people, acquired authority to initiate such writs. The U.S. Constitution specifically includes the \"habeas\" procedure in the Suspension Clause (Clause 2), located in Article One, Section 9. This states that \"The privilege of the writ of \"habeas corpus\" shall not be suspended, unless when in cases of rebellion or invasion the public safety may require it\".\n\nThe writ of \"habeas corpus ad subjiciendum\" is a civil, not criminal, \"ex parte\" proceeding in which a court inquires as to the legitimacy of a prisoner's custody. Typically, \"habeas corpus\" proceedings are to determine whether the court that imposed sentence on the defendant had jurisdiction and authority to do so, or whether the defendant's sentence has expired. \"Habeas corpus\" is also used as a legal avenue to challenge other types of custody such as pretrial detention or detention by the United States Bureau of Immigration and Customs Enforcement pursuant to a deportation proceeding.\n\nPresidents Abraham Lincoln and Ulysses Grant suspended \"habeas corpus\" during the Civil War and Reconstruction for some places or types of cases. During World War II, President Franklin D. Roosevelt suspended habeas corpus. Following the September 11 attacks, President George W. Bush attempted to place Guantanamo Bay detainees outside of the jurisdiction of \"habeas corpus\", but the Supreme Court of the United States overturned this action in \"Boumediene v. Bush\".\n\nIn 1526, the \"Fuero Nuevo of the Señorío de Vizcaya\" (\"New Charter of the Lordship of Biscay\") established a form of \"habeas corpus\" in the territory of the \"Señorío de Vizcaya\", nowadays part of Spain. This revised version of the \"Fuero Viejo\" (Old Charter) of 1451 codified the medieval custom whereby no person could be arbitrarily detained without being summoned first to the Oak of Gernika, an ancestral oak tree located in the outskirts of Gernika under which all laws of the Lordship of Biscay were passed.\n\nThe New Charter formalised that no one could be detained without a court order (Law 26 of Chapter 9) nor due to debts (Law 3 of Chapter 16). It also established that no one could be arrested without previously having been summoned to the Oak of Gernika and given 30 days to answer the said summon, and that upon presenting themselves under the Tree, they had to be provided with all evidence and accusations so that they could defend themselves (Law 7 of Chapter 9). No one could be sent to prison or deprived of their freedom until being formally trialed, and no one could be accused of a different crime until their current court trial was over (Law 5 of Chapter 5). Those fearing they were being arrested illegally could appeal to the \"Regimiento General\" that their rights could be upheld. The \"Regimiento\" (the executive arm of the Juntas Generales of Biscay) would demand the prisoner be handed over to them, and thereafter the prisoner would be released and placed under the protection of the Regimiento while awaiting for trial.\n\nThe Crown of Aragon also had a remedy equivalent to the \"habeas corpus\" called the \"manifestación de personas\" (literally, \"demonstration of persons\"). According to the right of \"manifestación\", the Justicia de Aragon (lit. \"Justice of Aragon\", an Aragonese judiciary figure similar to an ombudsman, but with far reaching executive powers) could require a judge, a court of justice, or any other official that they handed over to the \"Justicia\" (i.e., that they \"demonstrated\") anyone being prosecuted so as to guarantee that this person's rights were upheld, and that no violence would befall this person prior to him being sentenced. Furthermore, the \"Justicia\" retained the right to examine the judgement and decide whether it satisfied the conditions of a fair trial; if the \"Justicia\" was not satisfied, he could refuse to hand the accused back to the authorities. The right of \"manifestación\" acted like an habeas corpus: knowing that the appeal to the \"Justicia\" would immediately follow any unlawful detention, these were effectively illegal. Equally, torture (which had been banned since 1325 in Aragon) could never take place. In some cases, people exerting their right of manifestación were kept under the Justicia's watch in \"manifestación\" prisons (famous for their mild and easy conditions) or house arrest; more generally however, the person was released from confinement and placed under the \"Justicia's protection\", awaiting trial. The \"Justicia\" always granted the right of \"manifestación\" by default, but they only really had to act in extreme cases, as for instance famously happened in 1590 when Antonio Pérez, the disgraced secretary to Philip II of Spain, fled from Castile to Aragon and used his Aragonese ascendency to appeal to the \"Justicia\" for manifestación right, and therefore prevent his arrest at the King's behest.\n\nThe right of \"manifestación\" was codified in 1325 in the Declaratio Privilegii generalis passed by the Aragonese Corts under king James II of Aragon. It had been practiced since the inception of the kingdom of Aragon in the 11th century, and therefore predates the \"habeas corpus\" itself.\n\nIn 1430, King Władysław II Jagiełło of Poland granted the Privilege of Jedlnia, which proclaimed, \"Neminem captivabimus nisi iure victum\" (\"We will not imprison anyone except if convicted by law\"). This revolutionary innovation in civil libertarianism gave Polish citizens due process-style rights that did not exist in any other European country for another 250 years. Originally, the Privilege of Jedlnia was restricted to the nobility (the szlachta), but it was extended to cover townsmen in the 1791 Constitution. Importantly, social classifications in the Polish–Lithuanian Commonwealth were not as rigid as in other European countries; townspeople and Jews were sometimes ennobled. The Privilege of Jedlnia provided broader coverage than many subsequently enacted habeas corpus laws because Poland's nobility constituted an unusually large percentage of the country's total population, which was Europe's largest. As a result, by the 16th century, it was protecting the liberty of between five hundred thousand and a million Poles.\n\nIn South Africa and other countries whose legal systems are based on Roman-Dutch law, the \"interdictum de homine libero exhibendo\" is the equivalent of the writ of \"habeas corpus\". In South Africa, it has been entrenched in the Bill of Rights, which provides in section 35(2)(d) that every detained person has the right to challenge the lawfulness of the detention in person before a court and, if the detention is unlawful, to be released.\n\nIn the 1950s, American lawyer Luis Kutner began advocating an international writ of \"habeas corpus\" to protect individual human rights. In 1952, he filed a petition for a \"United Nations Writ of Habeas Corpus\" on behalf of William N. Oatis, an American journalist jailed the previous year by the Communist government of Czechoslovakia. Alleging that Czechoslovakia had violated Oatis's rights under the United Nations Charter and the Universal Declaration of Human Rights and that the United Nations General Assembly had \"inherent power\" to fashion remedies for human rights violations, the petition was filed with the United Nations Commission on Human Rights. The Commission forwarded the petition to Czechoslovakia, but no other United Nations action was taken. Oatis was released in 1953. Kutner went on to publish numerous articles and books advocating the creation of an \"International Court of Habeas Corpus\".\n\nArticle 3 of the Universal Declaration of Human Rights provides that \"everyone has the right to life, liberty and security of person\". Article 5 of the European Convention on Human Rights goes further and calls for persons detained to have the right to challenge their detention, providing at article 5.4: \n\n\n"}
{"id": "1643583", "url": "https://en.wikipedia.org/wiki?curid=1643583", "title": "Hydraulic analogy", "text": "Hydraulic analogy\n\nThe electronic–hydraulic analogy (derisively referred to as the drain-pipe theory by Oliver Lodge) is the most widely used analogy for \"electron fluid\" in a metal conductor. Since electric current is invisible and the processes at play in electronics are often difficult to demonstrate, the various electronic components are represented by hydraulic equivalents. Electricity (as well as heat) was originally understood to be a kind of fluid, and the names of certain electric quantities (such as current) are derived from hydraulic equivalents. As with all analogies, it demands an intuitive and competent understanding of the baseline paradigms (electronics and hydraulics).\n\nThere is no unique paradigm for establishing this analogy. Two paradigms can be used to introduce the concept to students using pressure induced by gravity or by pumps.\n\nIn the version with pressure induced by gravity, large tanks of water are held up high, or are filled to differing water levels, and the potential energy of the water head is the pressure source. This is reminiscent of electrical diagrams with an up arrow pointing to +V, grounded pins that otherwise are not shown connecting to anything, and so on. This has the advantage of associating electric potential with gravitational potential.\n\nA second paradigm is a completely enclosed version with pumps providing pressure only and no gravity. This is reminiscent of a circuit diagram with a voltage source shown and the wires actually completing a circuit. This paradigm is further discussed below.\n\nOther paradigms highlight the similarities between equations governing the flow of fluid and the flow of charge. Flow and pressure variables can be calculated in both steady and transient fluid flow situations with the use of the hydraulic ohm analogy. Hydraulic ohms are the units of hydraulic impedance, which is defined as the ratio of pressure to volume flow rate. The pressure and volume flow variables are treated as phasors in this definition, so possess a phase as well as magnitude.\n\nA slightly different paradigm is used in acoustics, where acoustic impedance is defined as a relationship between pressure and air speed. In this paradigm, a large cavity with a hole is analogous to a capacitor that stores compressional energy when the time-dependent pressure deviates from atmospheric pressure. A hole (or long tube) is analogous to an inductor that stores kinetic energy associated with the flow of air.\n\nA circuit was used to model feedback stabilization of a hydrodynamic plasma instability in a magnetic mirror In this application, the effort was to keep the plasma column centered by applying voltages to the plates, and except for the presence of turbulence and non-linear effects, the plasma was an actual electric circuit element (not really an analog).\n\nIn general, electric potential is equivalent to hydraulic head. This model assumes that the water is flowing horizontally, so that the force of gravity can be ignored. In this case electric potential is equivalent to pressure. The voltage (or voltage drop or \"potential difference\") is a difference in pressure between two points. Electric potential and voltage are usually measured in volts. \nElectric current is equivalent to a hydraulic volume flow rate; that is, the volumetric quantity of flowing water over time. Usually measured in amperes.\n\nElectric charge is equivalent to a quantity of water.\n\nA relatively wide pipe completely filled with water is equivalent to conducting wire. When comparing to a piece of wire, the pipe should be thought of as having semi-permanent caps on the ends. Connecting one end of a wire to a circuit is equivalent to un-capping one end of the pipe and attaching it to another pipe. With few exceptions (such as a high-voltage power source), a wire with only one end attached to a circuit will do nothing; the pipe remains capped on the free end, and thus adds nothing to the circuit.\n\nA resistor is equivalent to a constriction in the bore of the pipe which requires more pressure to pass the same amount of water. All pipes have some resistance to flow, just as all wires have some resistance to current.\n\nA node (or junction) in Kirchhoff's junction rule is equivalent to a pipe tee. The net flow of water into a piping tee (filled with water) must equal the net flow out.\nA capacitor is equivalent to a tank with one connection at each end and a rubber sheet dividing the tank in two lengthwise (a hydraulic accumulator). When water is forced into one pipe, equal water is simultaneously forced out of the other pipe, yet no water can penetrate the rubber diaphragm. Energy is stored by the stretching of the rubber. As more current flows \"through\" the capacitor, the back-pressure (voltage) becomes greater, thus current \"leads\" voltage in a capacitor. As the back-pressure from the stretched rubber approaches the applied pressure, the current becomes less and less. Thus capacitors \"filter out\" constant pressure differences and slowly varying, low-frequency pressure differences, while allowing rapid changes in pressure to pass through. \n\nAn ideal voltage source (ideal battery) or ideal current source is a dynamic pump with feedback control. A pressure meter on both sides shows that regardless of the current being produced, this kind of pump produces constant pressure difference. If one terminal is kept fixed at ground, another analogy is a large body of water at a high elevation, sufficiently large that the drawn water does not affect the water level. To create the analog of an ideal current source, use a positive displacement pump: A current meter (little paddle wheel) shows that when this kind of pump is driven at a constant speed, it maintains a constant speed of the little paddle wheel.\n\nA diode is equivalent to a one-way check valve with a slightly leaky valve seat. As with a diode, a small pressure difference is needed before the valve opens. And like a diode, too much reverse bias can damage or destroy the valve assembly.\n\nA transistor is a valve in which a diaphragm, controlled by a low-current signal (either constant current for a BJT or constant pressure for a FET), moves a plunger which affects the current through another section of pipe.\n\nCMOS is a combination of two MOSFET transistors. As the input pressure changes, the pistons allow the output to connect to either zero or positive pressure.\n\nA memristor is a needle valve operated by a flow meter. As water flows through in the forward direction, the needle valve restricts flow more; as water flows the other direction, the needle valve opens further providing less resistance.\n\nEM wave speed (velocity of propagation) is equivalent to the speed of sound in water. When a light switch is flipped, the electric wave travels very quickly through the wires.\n\nCharge flow speed (drift velocity) is equivalent to the particle speed of water. The moving charges themselves move rather slowly.\n\nDC is equivalent to the a constant flow of water in a circuit of pipes.\n\nLow frequency AC is equivalent to water oscillating back and forth in a pipe\n\nHigher-frequency AC and transmission lines is somewhat equivalent to sound being transmitted through the water pipes, though this does not properly mirror the cyclical reversal of alternating electric current. As described, the fluid flow conveys pressure fluctuations, but fluids do not reverse at high rates in hydraulic systems, which the above \"low frequency\" entry does accurately describe. A better concept (if sound waves are to be the phenomenon) is that of direct current with high-frequency \"ripple\" superimposed.\n\nInductive spark used in induction coils is similar to water hammer, caused by the inertia of water\n\nSome examples of analogous electrical and hydraulic equations:\n\nIf the differential equations have the same form, the response will be similar.\n\nIf taken too far, the water analogy can create misconceptions. For it to be useful, one must remain aware of the regions where electricity and water behave very differently.\n\nFields (Maxwell equations, Inductance): Electrons can push or pull other distant electrons via their fields, while water molecules experience forces only from direct contact with other molecules. For this reason, waves in water travel at the speed of sound, but waves in a sea of charge will travel much faster as the forces from one electron are applied to many distant electrons and not to only the neighbors in direct contact. In a hydraulic transmission line, the energy flows as mechanical waves through the water, but in an electric transmission line the energy flows as fields in the space surrounding the wires, and does not flow inside the metal. Also, an accelerating electron will drag its neighbors along while attracting them, both because of magnetic forces.\n\nCharge: Unlike water, movable charge carriers can be positive or negative, and conductors can exhibit an overall positive or negative net charge. The mobile carriers in electric currents are usually electrons, but sometimes they are charged positively, such as the positive ions in an electrolyte, the H ions in proton conductors or holes in p-type semiconductors and some (very rare) conductors.\n\nLeaking pipes: The electric charge of an electrical circuit and its elements is usually almost equal to zero, hence it is (almost) constant. This is formalized in Kirchhoff's current law, which does not have an analogy to hydraulic systems, where amount of the liquid is not usually constant. Even with incompressible liquid the system may contain such elements as pistons and open pools, so the volume of liquid contained in a part of the system can change. For this reason, continuing electric currents require closed loops rather than hydraulics' open source/sink resembling spigots and buckets.\n\nFluid velocity and resistance of metals: As with water hoses, the carrier drift velocity in conductors is directly proportional to current. However, water only experiences drag via the pipes' inner surface, while charges are slowed at all points within a metal, as with water forced through a filter. Also, typical velocity of charge carriers within a conductor is less than centimeters per minute, and the \"electrical friction\" is extremely high. If charges ever flowed as fast as water can flow in pipes, the electric current would be immense, and the conductors would become incandescently hot and perhaps vaporize. To model the resistance and the charge-velocity of metals, perhaps a pipe packed with sponge, or a narrow straw filled with syrup, would be a better analogy than a large-diameter water pipe. Resistance in most electrical conductors is a linear function: as current increases, voltage drop increases proportionally (Ohm's Law). Liquid resistance in pipes is not linear with volume, varying as the square of volumetric flow (see Darcy–Weisbach equation).\n\nQuantum Mechanics: Solid conductors and insulators contain charges at more than one discrete level of atomic orbit energy, while the water in one region of a pipe can only have a single value of pressure. For this reason there is no hydraulic explanation for such things as a battery's charge pumping ability, a diode's depletion layer and voltage drop, solar cell functions, Peltier effect, etc., however equivalent devices can be designed which exhibit similar responses, although some of the mechanisms would only serve to regulate the flow curves rather than to contribute to the component's primary function.\n\nIn order for the model to be useful, the reader or student must have a substantial understanding of the model (hydraulic) system's principles. It also requires that the principles can be transferred to the target (electrical) system. Hydraulic systems are deceptively simple: the phenomenon of pump cavitation is a known, complex problem that few people outside of the fluid power or irrigation industries would understand. For those who do, the hydraulic analogy is amusing, as no \"cavitation\" equivalent exists in electrical engineering. The hydraulic analogy can give a mistaken sense of understanding that will be exposed once a detailed description of electrical circuit theory is required.\n\nOne must also consider the difficulties in trying to make an analogy match reality completely. The above \"electrical friction\" example, where the hydraulic analog is a pipe filled with sponge material, illustrates the problem: the model must be increased in complexity beyond any realistic scenario.\n\n\n"}
{"id": "153767", "url": "https://en.wikipedia.org/wiki?curid=153767", "title": "I = PAT", "text": "I = PAT\n\nI = PAT is the mathematical notation of a formula put forward to describe the impact of human activity on the environment.\n\nThe expression equates human impact on the environment to the product of three factors: Population, Affluence, and Technology. It is similar in form to the Kaya identity which applies specifically to emissions of the greenhouse gas carbon dioxide.\n\nThe validity of expressing environmental impact as a simple product of independent factors, and the factors that should be included and their comparative importance, have been the subject of debate among environmentalists. In particular, some have drawn attention to potential inter-relationships among the three factors; and others have wished to stress other factors not included in the formula, such as political and social structures, and the scope for beneficial, as well as harmful, environmental actions.\n\nThe equation was developed in 1970 during the course of a debate between Barry Commoner, Paul R. Ehrlich and John Holdren. Commoner argued that environmental impacts in the United States were caused primarily by changes in its production technology following World War II and aimed his thoughts on present day deteriorating environmental conditions in the U.S. Ehrlich and Holdren argued that all three factors were important and emphasized in particular the role of human population growth, but focused more on a broader scale, being less specific in space and time.\n\nThe equation can aid in understanding some of the factors affecting human impacts on the environment, but it has also been cited as a basis for many of the dire environmental predictions of the 1970s by Paul Ehrlich, George Wald, Denis Hayes, Lester Brown, René Dubos, and Sidney Ripley that did not come to pass. Neal Koblitz classified equations of this type as \"mathematical propaganda\" and criticized Ehrlich's use of them in the media (e.g. on The Tonight Show) to sway the general public.\n\nThe variable \"I\" in the \"I=PAT\" equation represents environmental impact. The environment may be viewed as a self-regenerating system that can sustain a certain level of impact sustainably. The maximum sustainable impact is called the carrying capacity. As long as \"I\" is less than this amount the associated population, affluence, and technology that make up \"I\" are sustainable. If \"I\" exceeds the carrying capacity, then the system is said to be in overshoot, which can only be a temporary state. Overshoot may degrade the ability of the environment to sustain impact, therefore reducing the carrying capacity.\n\nImpact may be measured using ecological footprint analysis in units of global hectares (gha). Ecological footprint per capita is a measure of the quantity of Earth's biologically productive surface that is needed to regenerate the resources consumed per capita.\n\nImpact is modeled as the product of three terms, giving gha as a result. Population is expressed in human numbers, therefore Affluence is measured in units of gha per capita. Technology is a unitless efficiency factor.\n\nIn the I=PAT equation, the variable P represents the population of an area, such as the world. Since the rise of industrial societies, human population has been increasing exponentially. This has caused Thomas Malthus, Paul Ehrlich and many others to postulate that this growth would continue until checked by widespread hunger and famine (see Malthusian growth model).\n\nThe United Nations project that world population will increase from 7.6 billion today (2018) to 9.8 billion in 2050 and about 11.2 billion in 2100.\nThese projections take into consideration that population growth has slowed in recent years as women are having fewer children. This phenomenon is the result of demographic transition all over the world. The UN projects that human population might stabilize around 11.2 billion in 2100 (up from 9.2 billion). However, since the world population is set to keep rising for the next few decades, this factor of the I=PAT equation will likely keep increasing human impact on the environment for the near future.\n\nIncreased population increases humans' environmental impact in many ways, which include but are not limited to:\n\n\nThe variable A, in the I=PAT equation stands for affluence. It represents the average consumption of each person in the population. As the consumption of each person increases, the total environmental impact increases as well. A common proxy for measuring consumption is through GDP per capita. While GDP per capita measures production, it is often assumed that consumption increases when production increases. GDP per capita has been rising steadily over the last few centuries and is driving up human impact in the I=PAT equation.\n\nIncreased consumption significantly increases human environmental impact. This is because each product consumed has wide-ranging effects on the environment. For example, the construction of a car has the following environmental impacts:\n\n\nThe more cars per capita, the greater the impact.\nEcological impacts of each product are far reaching, increases in consumption quickly result in large impacts on the environment through direct and indirect sources.\n\nThe T variable in the I=PAT equation represents how resource intensive the production of affluence is; how much environmental impact is involved in creating, transporting and disposing of the goods, services and amenities used. Improvements in efficiency can reduce resource intensiveness, reducing the T multiplier. Since technology can affect environmental impact in many different ways, the unit for T is often tailored for the situation I=PAT is being applied to. For example, for a situation where the human impact on climate change is being measured, an appropriate unit for T might be greenhouse gas emissions per unit of GDP.\n\nIncreases in efficiency can reduce overall environmental impact. However, since P has increased exponentially, and A has also increased drastically, the overall environmental impact, I, has still increased.\n\nCriticisms of the \"I=PAT\" formula\n\nThe I=PAT equation has been criticized for being too simplistic by assuming that P, A, and T are independent of each other. In reality, at least 7 interdependencies between P, A, and T could exist, indicating that it is more correct to rewrite the equation as I = f(P,A,T). For example, a doubling of technological efficiency, or equivalently a reduction of the T-factor by 50%, does not necessarily reduce the environmental impact (I) by 50% if efficiency induced price reductions stimulate additional consumption of the resource that was supposed to be conserved, a phenomenon called the rebound effect (conservation) or Jevons Paradox. As was shown by Alcott, despite significant improvements in the carbon intensity of GDP (i.e., the efficiency in carbon use) since 1980, world fossil energy consumption has increased in line with economic and population growth. Similarly, an extensive historical analysis of technological efficiency improvements has conclusively shown that improvements in the efficiency of energy and material use were almost always outpaced by economic growth, resulting in a net increase in resource use and associated pollution.\n\nThere have also been comments that this model depicts people as being purely detrimental to the environment, ignoring any conservation or restoration efforts that societies have made.\n\nAnother major criticism of the I=PAT model is that it ignores the political context and decision making structures of countries and groups. This means the equation does not account for varying degrees of power, influence, and responsibility of individuals over environmental impact. Also, the P factor does not account for the complexity of social structures or behaviors, resulting in blame being placed on the global poor. I=PAT does not account for sustainable resource use among some poor and indigenous populations, unfairly characterizing these populations whose cultures support low impact practices. However, the latter criticism not only assumes low impacts for indigenous populations, but also misunderstands the I=PAT equation itself. Environmental impact is a function of human numbers, affluence (ie resources consumed per capita) and technology. It is assumed that small scale societies have low environmental impacts due to their practices and orientations alone but there is little evidence to support this. In fact the generally low impact of small scale societies compared to state societies is due to a combination of their small numbers and low level technology. Thus, the environmental sustainability of these societies is largely an epiphenomenon due their \"inability\" to significantly affect their environment. That all types of societies are subject to I=PAT was actually made clear in Ehrlich and Holdren's 1972 dialogue with Commoner in \"The Bulletin of the Atomic Scientists\" where they examine the pre-industrial (and indeed prehistoric) impact of human beings on the environment. Their position is further clarified by Holdren's 1993 paper, \"A Brief History of \"IPAT\"\".\n\nAs a result of the interdependencies between P, A, and T and potential rebound effects, policies aimed at decreasing environmental impacts through reductions in P, A, and T may not only be very difficult to implement (e.g., population control and material sufficiency and degrowth movements have been very controversial) but also are likely to be rather ineffective compared to rationing (i.e., quotas) or Pigouvian taxation of resource use or pollution.\n\n"}
{"id": "7438937", "url": "https://en.wikipedia.org/wiki?curid=7438937", "title": "Indefinite monism", "text": "Indefinite monism\n\nIndefinite Monism is a philosophical conception of reality that asserts that only Awareness is real and that the wholeness of Reality can be conceptually thought of in terms of immanent and transcendent aspects. The immanent aspect is denominated simply as Awareness, while the transcendent aspect is referred to as \"Omnific Awareness\". \n\nAwareness in this system is not equivalent to consciousness. Rather, Awareness is the venue for consciousness, and the transcendent aspect of Reality, \"Omnific Awareness\", is what consciousness is of. \n\nIn this system, what is real is distinguished from that which exists by showing that everything that we are conscious of exists but is not real since it is contingent upon awareness for its existence. Awareness is the source of its own energetic display -- its omneity. Rather than leading to a solipsistic account of reality, it is claimed through an analysis of consciousness that it is an error on our part to conceive of individuated awareness. That error being found in a conflation of the objects of consciousness with the subject of consciousness within an assumed form of reality of separate physical things. Proceeding from the one necessarily true and unquestionable fact – that we are present to our experiences – an understanding of reality is developed that is neither a materialist nor an idealist conceptualization. This way of viewing the world is referred to as surjective, a metaphorical use of a concept found in mathematical set theory that means a function that works upon every member of a set, where Awareness is the function and \"Omnific Awareness\" is the set, in order to distinguish this position from both subjectivity and objectivity.\n\nWithin this system anything whatsoever can arise from \"Omnific Awareness\", thus the use of the term “indefinite” in labeling this monism. What does arise as the existents that we are conscious of is conditioned by the affections of Awareness for its display. Thus this system does away with the idea of an active, creative force called Free Will and replaces it with an active volitional component known as affections, that does not itself create anything, whether movement or structure, but instead, constrains the possibilities of what arises naturally. Arguably, the concept of Free Will necessitates a world of separation as it implies an actor and that which is acted upon. In this conception there is no such separation. Yet our intuitive modeling of the existents of reality as arising from natural processes, as well as our intuitive understanding that we can ‘cause’ things to happen by our ‘will’, are both cleanly supported.\n\nThe distinction between physical phenomena and mental phenomena is also removed by this system. \"Omnific Awareness\" gives rise to everything – thus the use of the term \"omnific\" – and this includes thoughts that phenomenally arise in brains as well as existents that arise phenomenally as things in the world. By removing this distinction this system cuts off the inevitable paradoxes that otherwise arise in philosophical systems. The implications of this move create a number of novel, but necessary, modifications in current categorizations of ideas about reality and our study of it. For instance, ontology – the study of being – is necessitated by the assumption of a physical world of separate things, but when viewed surjectively ontology collapses into epistemology – the study of the methods or grounds of knowledge. Similarly, by removing the distinction between mental and physical phenomena the tensions created in dualist understandings of reality of how the mental and physical interact with one another are dispelled. Surprisingly, the removal of this distinction also completely removes the need for claims of metaphysical realms of being or metaphysical processes, thus collapsing all of reality into this reality.\n\nThe implications of this view of Reality are carried as far as ethics, where the lack of separation between Awareness and that which it gives rise to necessitate a far-reaching adjustment in our ethical beliefs. One such difference that is highlighted for instance is that all conscious beings, which are called “knowings” in deference to this new conception of reality, are qualitatively the same; thus our current distinction between “human beings” and “animals” is based upon a false dichotomy, and this new understanding will necessitate an adjustment in our ideas of who or what can be “property” and who or what can be a “person.”\n\n\n"}
{"id": "473779", "url": "https://en.wikipedia.org/wiki?curid=473779", "title": "Injective object", "text": "Injective object\n\nIn mathematics, especially in the field of category theory, the concept of injective object is a generalization of the concept of injective module. This concept is important in cohomology, in homotopy theory and in the theory of model categories. The dual notion is that of a projective object.\n\nAn object formula_1 in the category formula_2 is said to be injective if for every monomorphism formula_3 and every morphism formula_4 there exists a morphism formula_5 extending formula_6 to formula_7, i.e. such that formula_8.\n\nThe morphism formula_9 in the above definition is not required to be uniquely determined by formula_10 and formula_6.\n\nIn a locally small category, it is equivalent to require that the hom functor formula_12 carries monomorphisms in formula_2 to surjective set maps.\n\nThe notion of injectivity was first formulated for abelian categories, and this is still one of its primary areas of application. When formula_14 is an abelian category, an object \"Q\" of formula_14 is injective if and only if its hom functor Hom(–,\"Q\") is exact.\n\nIf\nis an exact sequence in formula_14 such that \"Q\" is injective, then the sequence splits.\n\nThe category formula_2 is said to \"have enough injectives\" if for every object \"X\" of formula_2, there exists a monomorphism from \"X\" to an injective object. \n\nA monomorphism \"g\" in formula_2 is called an essential monomorphism if for any morphism \"f\", the composite \"fg\" is a monomorphism only if \"f\" is a monomorphism.\n\nIf \"g\" is an essential monomorphism with domain \"X\" and an injective codomain \"G\", then \"G\" is called an injective hull of \"X\". The injective hull is then uniquely determined by \"X\" up to a non-canonical isomorphism.\n\n\nIf an abelian category has enough injectives, we can form injective resolutions, i.e. for a given object \"X\" we can form a long exact sequence\nand one can then define the derived functors of a given functor \"F\" by applying \"F\" to this sequence and computing the homology of the resulting (not necessarily exact) sequence. This approach is used to define Ext and Tor functors and also the various cohomology theories in group theory, algebraic topology and algebraic geometry. The categories being used are typically functor categories or categories of sheaves of O modules over some ringed space (\"X\",O) or in general any Grothendieck category.\n\nLet formula_2 be a category and let formula_23 be a class of morphisms of formula_2.\n\nAn object formula_1 of formula_2 is said to be \"formula_23\"-injective if for every morphism formula_28 and every morphism formula_29 in formula_23 there exists a morphism formula_31 with formula_32.\n\nIf formula_23 is the class of monomorphisms, we are back to the injective objects that were treated above.\n\nThe category formula_2 is said to \"have enough formula_23-injectives\" if for every object \"X\" of formula_2, there exist a \"formula_23\"-morphism from \"X\" to an \"formula_23\"-injective object.\n\nA \"formula_23\"-morphism \"g\" in formula_2 is called \"formula_23\"-essential if for any morphism \"f\", the composite \"fg\" is in \"formula_23\" only if \"f\" is in \"formula_23\".\n\nIf \"g\" is a \"formula_23\"-essential morphism with domain \"X\" and an \"formula_23\"-injective codomain \"G\", then \"G\" is called an \"H\"-injective hull of \"X\". \n\n\n\n"}
{"id": "49010479", "url": "https://en.wikipedia.org/wiki?curid=49010479", "title": "Interactive street theatre", "text": "Interactive street theatre\n\nInteractive street theatre is a combination of street theatre and interactive art.\nThere are different kinds of interactive street theatre. The biggest difference between “normal interactive art” and \"interactive street theatre\" is that the presentation. \nThe presentation of is always outside in a public place and most of the time on a festival. The audience are people how just walk by and stop and participate. You can participate but if you don’t want you can just watch. It is temporary, only a few hours. And it is easier to participate than in a gallery or a museum. People who might not have ever to a theatre or museum can now participate in an interactive street theatre.\n\nSome interactive art installations visitor can \"walk\" in, on, and around them. And to “play” with the object of the installations. Another form is that spectators themselves become part of the artwork. And a third option is that spectator become active and work to gather to great a collective art piece.\n"}
{"id": "2874626", "url": "https://en.wikipedia.org/wiki?curid=2874626", "title": "Interleave sequence", "text": "Interleave sequence\n\nIn mathematics, an interleave sequence is obtained by merging two sequences via an in shuffle.\n\nLet formula_1 be a set, and let formula_2 and formula_3, formula_4 be two sequences in formula_5 The interleave sequence is defined to be the sequence formula_6 Formally, it is the sequence formula_7 given by\n\n"}
{"id": "8476625", "url": "https://en.wikipedia.org/wiki?curid=8476625", "title": "Knicks–Nuggets brawl", "text": "Knicks–Nuggets brawl\n\nThe Knicks–Nuggets brawl was an on-court altercation at a National Basketball Association (NBA) game between the New York Knicks and Denver Nuggets at Madison Square Garden in New York City on Saturday, December 16, 2006. This altercation became the most penalized on-court fight in the NBA since the Pacers–Pistons brawl from two years before.\n\nThe fight began with a flagrant foul by Knicks guard Mardy Collins on Nuggets guard J. R. Smith in the closing seconds of the game. Several players joined in the confrontation, and began to make physical contact. The fight briefly spilled into the stands, and also stretched to the other end of the court. All ten players on the floor at the time were ejected after the altercation was finished. When suspensions were announced, seven players were suspended without pay for a combined total of 47 games.\n\nAlthough they were not penalized, Nuggets coach George Karl and Knicks coach Isiah Thomas were both scrutinized for their part in the brawl. Carmelo Anthony was also criticized for harming his image as a star, and several writers said the league had penalized the players excessively because it wanted to keep its image free from violence.\n\nEntering the game, the New York Knicks had a record of 9–17 while the Denver Nuggets sported a 13–9 record. Despite trailing the entire game, the Knicks came as close as 2 points in the first half but the Nuggets regrouped and closed the half with a 13-point advantage and continued to lead in the second half by as much as 26 points in the third quarter. The Knicks briefly came within 10 points with ten minutes left in the game, but the Nuggets went on a 12–2 run and were never threatened again. Carmelo Anthony scored 24 points to lead the Nuggets, and Marcus Camby added 24 points and 9 rebounds; Stephon Marbury scored a season-high 31 points for the Knicks.\n\nThe incident occurred with 1:15 remaining in the Knicks' home game at Madison Square Garden, where the Nuggets were leading 119–100. The Knicks' Mardy Collins fouled the Nuggets' J. R. Smith on a fast break by slapping his arms around Smith's neck, knocking him to the floor. As Smith stood up to confront Collins, Nate Robinson pulled Smith away, and then began pushing and shouting at him. David Lee tried to hold Smith back, but Smith broke free and charged into Robinson, causing both players to fall into the photographers and front row courtside seats, before they were quickly separated by teammates.\n\nAs the fighting was seemingly coming to an end, Carmelo Anthony confronted Collins and punched him in the face, knocking him to the ground. Jared Jeffries immediately tried to attack Anthony but tripped over Marcus Camby, before being restrained by coaches and teammates, while Anthony backed up towards the Nuggets' bench. Collins also ran down the court to get at Anthony but was blocked by Nenê and Smith. All ten players on the court at the time of the incident were ejected by the officiating crew that consisted of Dick Bavetta, Violet Palmer, and Robbie Robinson. No fans came onto the court during the entire ordeal, which prevented any repetition of the Pacers–Pistons brawl from 2 years ago.\n\nNBA commissioner David Stern reacted with strict penalties for the players involved, stating, \"It is our obligation to take the strongest possible steps to avoid such failures in the future.\" Seven players were suspended for a total of 47 games, and the players lost in excess of $1.2 million in salary. Each team was also fined $500,000.\n\nBecause Anthony's suspension was longer than 12 games, he was eligible to appeal to an arbitrator; however, Anthony eventually announced he would not attempt one, saying he did not \"want to be a further distraction\".\n\nSeveral sportswriters said the brawl was not as violent as the Pacers–Pistons brawl two years before, and 81% of respondents in a SportsNation poll said the biggest difference between the two brawls was that it \"didn't involve players going into the stands and fighting fans\". However, MSNBC's Michael Ventre said that the Knicks and Nuggets brawl was worse because \"it was touched off by the actions of players, and it escalated because of them.\" Several writers said that the penalties were more severe because of the Pacers–Pistons brawl, because the league was on a \"very serious image-cleanup campaign.\"\n\nSteve Francis claimed that the media reaction to the fight and the suspensions itself were \"racially motivated.\" He argued that Major League Baseball and the National Hockey League had \"incidents that are way worse than basketball,\" but did not face the scrutiny that the NBA received \"because there are more black players in the NBA.\" This was echoed by several writers, and sportswriter-television personality Michael Wilbon said that \"NBA players have endured more scrutiny, pertaining to image, than any other professional athletes in America.\" Martin Luther King III called for a meeting to end the violence in the NBA, stating, \"Individuals who play a game should be able to conduct themselves appropriately.\" However, the NBA said through a spokesman that they \"don't think that meeting is necessary.\"\n\nMinutes before the brawl started, Knicks coach Isiah Thomas asked Carmelo Anthony not to go into the painted area around the basket, despite the fact that they were not members of the same team. Thomas later said that because Denver head coach George Karl kept his team's starting players on the court for the closing minutes of the game, which Thomas thought showed a lack of sportsmanship, his orders to Anthony were to \"show some class.\" However, Karl responded by saying the brawl \"was directed by Isiah\".\n\nThomas was not penalized after the brawl, as an NBA investigation ruled that they did not have \"adequate evidence upon which to make a determination,\" but several writers criticized the NBA for not including Thomas in the suspensions. ESPN analyst Marc Stein called Thomas' explanations of his comments \"laughable,\" and commentator Greg Anthony, a former Knicks player, said he \"never had a coach say that to an opponent.\" It was also suggested that Thomas was attempting to resurrect the physical tactics of his former team, the \"Bad Boy\" Pistons.\n\nIn response to Thomas saying that keeping the Nuggets starters on the floor in the final minutes of the game was unsportsmanlike, Karl said that he \"never thought about running up the score,\" and only wanted to \"get a big win on the road.\" However, several sportswriters criticized his decision, and some said that he should also have been penalized. It was also suggested that Karl was trying to humiliate Thomas due to the perception that Thomas had mistreated Larry Brown, a friend of Karl's. Karl was also blamed for putting his players in a position to start a fight.\n\nThe day following the brawl, Anthony issued a statement and apology to his family, the league, and fans. He also specifically apologized to Mardy Collins, whom he directly struck during the incident. At the time of the brawl, Anthony was the league's leading scorer; his suspension was also the longest of the players suspended, and the sixth-longest in NBA history. According to former NBA player Steve Kerr, Anthony had \"tarnished\" his image, and basketball analyst Ric Bucher said that Anthony had \"torched his own career\". Sports Illustrated writer Marty Burns said that Anthony faced becoming known by sports fans across America as the player who punched Collins in the face and then ran away. An example of the backlash was Northwest Airlines pulling Anthony from its in-flight magazine cover, as it said it did not want \"to condone the behavior of Anthony\".\n\nA day after Anthony was suspended, Denver acquired Allen Iverson, who was then second in the league in scoring behind Anthony. After Anthony and Smith returned from their suspensions, the trio led the Nuggets to 45 wins and the sixth seed in the Western Conference for the playoffs. However, they were eliminated in the first round by the San Antonio Spurs. The Knicks finished 33–49, 12th in the Eastern Conference, and did not make the playoffs.\n\nThe two teams faced each other for the first time since the altercation on November 17, 2007, which the Nuggets won 115–83. Opposing players Renaldo Balkman and Linas Kleiza began arguing with each other after Balkman was called for a hard foul on Kleiza, but the incident was defused after Balkman was given a technical foul. Iverson, Anthony, and Marcus Camby were all removed early in the fourth quarter. Balkman and Kleiza later became teammates after Balkman was traded to the Nuggets in the 2008 off-season.\n\nAs of the 2010–11 season, of the seven suspended players, three were still with their respective teams. Mardy Collins was traded to the Los Angeles Clippers in 2008. Jerome James was traded to the Chicago Bulls a year later, but never played for the Bulls as he suffered a torn Achilles tendon. Jeffries and Robinson were traded at the 2010 trade deadline to the Houston Rockets and Boston Celtics, respectively. Robinson then signed a multi-year deal with the Denver Nuggets on July 26, 2013. Carmelo Anthony was traded to the Knicks before the 2011 trade deadline. Meanwhile, Jeffries' contract was bought out by the Rockets and he rejoined the Knicks. In addition, J.R. Smith was signed by the Knicks in February of the 2012 season, which will make him the second former Nugget involved in the brawl to join the Knicks. Though not directly involved in the brawl, 2006 Nuggets C/PF Marcus Camby also joined the Knicks during the 2012 off-season, making him the third 2006 Nugget to join the Knicks for the 2012-2013 season. Also not directly involved in the brawl, PF Kenyon Martin joined the Knicks on February 21, 2013 (initially on a 10-day contract), making him the 4th member of the 2012-2013 Knicks to have been on the '06 Nuggets team.\n\n"}
{"id": "155759", "url": "https://en.wikipedia.org/wiki?curid=155759", "title": "Launch window", "text": "Launch window\n\nIn the context of spaceflight, launch period is the collection of days and launch window is the time period on a given day during which a particular vehicle (rocket, Space Shuttle, etc.) must be launched in order to reach its intended target. If the rocket is not launched within a given window, it has to wait for the window on the next day of the period. Launch periods and launch windows are very dependent on both the rocket's capability and the orbit it is going.\n\nA launch period refers to the days that the rocket can launch to reach its intended orbit. A mission could have a period of 365 days in a year, a few weeks each month, a few weeks every 26 months (e.g. Mars launch periods), or a short period time that won't be repeated.\n\nA launch window indicates the time frame on a given day in the launch period that the rocket can launch to reach its intended orbit. This can be as short as a second (referred to as an instantaneous window) or even the entire day. For operational reasons, the window almost always is limited to no more than a few hours. The launch window can stretch over two calendar days (ex: start at 11:46 p.m. and end at 12:14 a.m.). Launch windows are rarely the exact same times each day.\n\nLaunch windows and launch periods are often used interchangeably in the public sphere, even within the same organization. However, these definitions are the ones used by NASA (and other space agency) launch directors and trajectory analysts. \n\nTo go to another planet using the simple low-energy Hohmann transfer orbit, if eccentricity of orbits is not a factor, launch periods are periodic according to the synodic period; for example, in the case of Mars, the period is 2.135 years, (780 days). In more complex cases, including the use of gravitational slingshots, launch windows are irregular. Sometimes rare opportunities arise, such as when \"Voyager 2\" took advantage of a 175-year planetary alignment (launch window) to visit Jupiter, Saturn, Uranus, and Neptune. When such an opportunity is missed, another target may be selected. For example, the ESA's \"Rosetta\" mission was originally intended for comet 46P/Wirtanen, but a launcher problem delayed it and a new target had to be selected (comet 67P/Churyumov-Gerasimenko).\n\nLaunch periods are often calculated from porkchop plots, which show the delta-v needed to achieve the mission plotted against the launch time.\n\nThe launch window is defined by the first launch point and ending launch point. It may be continuous (i.e. able to launch every second in the launch window) or may be a collection of discrete instantaneous points between the open and close. Launch windows and days are usually calculated in UTC and then converted to the local time of where the rocket and spacecraft operators are located (frequently multiple time zones for USA launches).\n\nFor trips into largely arbitrary Earth orbits, no specific launch time is required. But if the spacecraft intends to rendezvous with an object already in orbit, the launch must be carefully timed to occur around the times that the target vehicle's orbital plane intersects the launch site.\n\nEarth observation satellites are often launched into sun-synchronous orbits which are near-polar. For these orbits, the launch window occurs at the time of day when the launch site location is aligned with the plane of the required orbit. To launch at another time would require an orbital plane change maneuver which would require a large amount of propellant.\n\nFor launches above low Earth orbit (LEO), the actual launch time can be somewhat flexible if a parking orbit is used, because the inclination and time the spacecraft initially spends in the parking orbit can be varied. See the launch window used by the \"Mars Global Surveyor\" spacecraft to the planet Mars at .\n\nSpace Shuttle missions to the International Space Station were restricted by beta angle cutout. Beta angle (formula_1) is defined as the angle between the orbit plane and the vector from the Sun. Due to the relationship between an orbiting object's beta angle (in this case, the ISS) and the percent of its orbit that is spent in sunlight, solar power generation and thermal control are affected by that beta angle. Shuttle launches to the ISS were normally attempted only when the ISS was in an orbit with a beta angle of less than 60 degrees.\n\n"}
{"id": "35014449", "url": "https://en.wikipedia.org/wiki?curid=35014449", "title": "Le Beurre et l'argent du beurre", "text": "Le Beurre et l'argent du beurre\n\nLe Beurre et l'argent du beurre is a 2007 documentary film directed by Alidou Badini and Philippe Baqué. The title, which translates to \"Butter and the money from butter\", derives from a French idiom equivalent to the English phrase \"Have one's cake and eat it too\".\n\nFair trading is very much in fashion today. The concept is to help the most underprivileged populations on our planet to emerge from this state thanks to a fairer distribution of revenues. The Shea butter is produced by the poorest women of Burkina Faso, and is more and more appreciated in Europe. It is used in the cosmetic industry and as a cocoa substitute. Various fair trading experiences claim to help them but, who really profits from the money of the butter?\n\n\"Le Beurre et l'argent du beurre\" was shown at the Amiens International Film Festival and at the African Film Festival of Cordoba.\nThe film received the Jury Grand Prize at the International Environmental Film Festival of Niamey.\nIt has been used as the basis for discussions by groups who support fair trade.\n\n"}
{"id": "31951282", "url": "https://en.wikipedia.org/wiki?curid=31951282", "title": "List of rampage killers (home intruders)", "text": "List of rampage killers (home intruders)\n\nThis section of the list of rampage killers contains those cases that either occurred mostly within a single household, or where most of the victims were members of a single family not related to the perpetrator. Cases where the primary motive for the murders was to facilitate or cover up another felony, like robbery, are not included.\n\nA rampage killer has been defined as follows:\n\nThis list should contain every case with at least one of the following features:\n\nAll abbreviations used in the table are explained below. \nW – A basic description of the weapons used in the murders\n"}
{"id": "384893", "url": "https://en.wikipedia.org/wiki?curid=384893", "title": "Love–hate relationship", "text": "Love–hate relationship\n\nA love–hate relationship is an interpersonal relationship involving simultaneous or alternating emotions of love and hate—something particularly common when emotions are intense.\n\nThe term is used frequently in psychology, popular writing, and journalism. It can be applied to relationships with inanimate objects, or even concepts, as well as those of a romantic nature or between siblings and parents/children.\n\nA love–hate relationship has been linked to the occurrence of emotional ambivalence in early childhood; to conflicting responses by different ego states within the same person; or to the inevitable co-existence of egoistic conflicts with the object of love.\n\nNarcissists have been seen as particularly prone to aggressive reactions towards love objects, not least when issues of self-identity are involved: in extreme instances, hate at the very existence of the other may be the only emotion felt, until love breaks through behind it.\n\nResearch from Yale University suggests love–hate relationships may be the result of poor self-esteem.\n\nThe term is sometimes employed by writers to refer to relationships between celebrity couples who have been divorced, then who reunite (notably Elizabeth Taylor and Richard Burton, or Eminem and Kimberly Scott), as well as to their relationship with fame itself.\n\nA love–hate relationship may develop when people have completely lost the intimacy within a loving relationship, yet still retain some passion for, or perhaps some commitment to, each other, before degenerating into a hate–love relationship leading to divorce.\n\nTony Blair and Gordon Brown's political friendship took on at times all the characteristics of a love–hate relationship, if one between friends and allies. Sigmund Freud said of himself that “an intimate friend and a hated enemy have always been indispensable to my emotional life...not infrequently…friend and enemy have coincided in the same person”.\n\nAristotle warned of the conflicts that can arise from conflicting claims within friendships.\n\n\n"}
{"id": "3566823", "url": "https://en.wikipedia.org/wiki?curid=3566823", "title": "Massachusetts Body of Liberties", "text": "Massachusetts Body of Liberties\n\nThe Massachusetts Body of Liberties was the first legal code established by European colonists in New England. Compiled by the Puritan minister Nathaniel Ward, the laws were established by the Massachusetts General Court in 1641. The Body of Liberties begins by establishing the exclusive right of the General Court to legislate and dictate the \"Countenance of Authority\".\n\nIn 1684 King Charles II revoked the Body of Liberties and reinstated English law over the Commonwealth. When King James II established the Massachusetts Colony the Body of Liberties took effect and remained so until it was replaced by the 1691 Provincial Charter.\n\nThe Body of Liberties was one of the earliest protections of individual rights in America. Unlike many of the English sources of the time, the Body of Liberties were express in many of their grants and far more supportive of individual rights. Despite these grants, the rights were modifiable by the General Court.\n\nTo varying degrees, the document contained rights that would later be included in the Bill of Rights. Many of the other rights are now considered fundamental components of procedural due process, such as rights to notice and hearing before the court. The rights also contained in the Bill of Rights included freedom of speech, a right against uncompensated takings, a right to bail, a right to jury trial, a right against cruel and unusual punishment, and a right against double jeopardy.\n\nIn addition to those, the Body of Liberties also contained other individual rights, including: a prohibition of a compulsory draft except for territorial defense; a prohibition of monopolies, \"No monopolies shall be granted or allowed amongst us, but of such new Inventions that are profitable to the Countrie, and that for a short time.\"; prohibition of an estate tax; the freedom of all \"house holders\" to fishing and fowling on public land.\n\nSome of the liberties legislated are explicitly cited as originating from biblical sources. While many of the liberties established still exist in both and law and practice in the Commonwealth today, some do not. The justification for slavery of Africans in Passage 91 of the Body of Liberties was likely based on an interpretation of scriptural passages of the New Testament, such as Ephesians 6:5 and Titus 2:9. Liberties were only extended to Caucasian Northern Europeans, particularly protestant Christian men.\n\n"}
{"id": "1108867", "url": "https://en.wikipedia.org/wiki?curid=1108867", "title": "Meena Keshwar Kamal", "text": "Meena Keshwar Kamal\n\nMeena Keshwar Kamal (Pashto/; February 27, 1956 – February 4, 1987), commonly known as Meena, was an Afghan revolutionary political activist, feminist, women's rights activist and founder of Revolutionary Association of the Women of Afghanistan (RAWA), who was assassinated in 1987.\n\nIn 1977, when she was a student at Kabul University, she founded Revolutionary Association of the Women of Afghanistan (RAWA), an organization formed to promote equality and education for women and continues to \"give voice to the deprived and silenced women of Afghanistan\". Even after the Saur Revolution there was no vast changes of women's deprivation in Afghanistan. In 1979 she campaigned against DRA, and organized meetings in schools to mobilize support against it, and in 1981, she launched a bilingual feminist magazine, \"Payam-e-Zan\" (Women's Message). She also founded Watan Schools to aid refugee children and their mothers, offering both hospitalization and the teaching of practical skills.\n\nAt the end of 1981, by invitation of the French Government Meena represented the Afghan resistance movement at the French Socialist Party Congress. The Soviet delegation at the Congress, headed by Boris Ponamaryev, left the hall as participants cheered when Meena started waving a victory sign.\nKamal was married to Afghanistan Liberation Organization leader Faiz Ahmad, who was murdered by agents of Gulbuddin Hekmatyar on November 12, 1986. They had three children, whose whereabouts are unknown.\n\nKamal was assassinated in Quetta, Pakistan on February 4, 1987. Reports vary as to who the assassins were, but are believed to have been agents of the Afghan Intelligence Service KHAD, the Afghan secret police, or of fundamentalist Mujahideen leader Gulbuddin Hekmatyar.\n\nIn a special issue of the Time Magazine on November 13, 2006, included Meena among \"60 Asian Heroes\" and wrote that \"Although she was only 30 when she died, Meena had already planted the seeds of an Afghan women's rights movement based on the power of knowledge.\" \n\nRAWA says of her \"Meena gave 12 years of her short but brilliant life to struggle for her homeland and her people. She had a strong belief that despite the darkness of illiteracy, ignorance of fundamentalism, and corruption and decadence of sell outs imposed on our women under the name of freedom and equality, finally that half of population will be awaken and cross the path towards freedom, democracy and women's rights. The enemy was rightly shivering with fear by the love and respect that Meena was creating within the hearts of our people. They knew that within the fire of her fights all the enemies of freedom, democracy and women would be turned to ashes.\"\n\nAn enduring quote from Meena states:\n\n\n"}
{"id": "19770", "url": "https://en.wikipedia.org/wiki?curid=19770", "title": "Memetics", "text": "Memetics\n\nMemetics (also referred to colloquially as memeology) is the study of information and culture based on an analogy with Darwinian evolution. Proponents describe memetics as an approach to evolutionary models of cultural information transfer. Critics regard memetics as a pseudoscience. Memetics describes how an idea can propagate successfully, but doesn't necessarily imply a concept is factual.\n\nThe term meme was coined in Richard Dawkins' 1976 book \"The Selfish Gene,\" but Dawkins later distanced himself from the resulting field of study. Analogous to a gene, the meme was conceived as a \"unit of culture\" (an idea, belief, pattern of behaviour, etc.) which is \"hosted\" in the minds of one or more individuals, and which can reproduce itself in the sense of jumping from the mind of one person to the mind of another. Thus what would otherwise be regarded as one individual influencing another to adopt a belief is seen as an idea-replicator reproducing itself in a new host. As with genetics, particularly under a Dawkinsian interpretation, a meme's success may be due to its contribution to the effectiveness of its host.\n\nThe Usenet newsgroup alt.memetics started in 1993 with peak posting years in the mid to late 1990s. The \"Journal of Memetics\" was published electronically from 1997 to 2005.\n\nIn his book \"The Selfish Gene\" (1976), the evolutionary biologist Richard Dawkins used the term \"meme\" to describe a unit of human cultural transmission analogous to the gene, arguing that replication also happens in culture, albeit in a different sense. Bella Hiscock outlined a similar hypothesis in 1975, which Dawkins referenced. Cultural evolution itself is a much older topic, with a history that dates back at least as far as Darwin's era.\n\nDawkins (1976) proposed that the meme is a unit of information residing in the brain and is the mutating replicator in human cultural evolution. It is a pattern that can influence its surroundings – that is, it has causal agency – and can propagate. This proposal resulted in debate among sociologists, biologists, and scientists of other disciplines. Dawkins himself did not provide a sufficient explanation of how the replication of units of information in the brain controls human behaviour and ultimately culture, and the principal topic of the book was genetics. Dawkins apparently did not intend to present a comprehensive theory of \"memetics\" in \"The Selfish Gene\", but rather coined the term \"meme\" in a speculative spirit. Accordingly, different researchers came to define the term \"unit of information\" in different ways.\n\nThe modern memetics movement dates from the mid-1980s. A January 1983 \"Metamagical Themas\" column by Douglas Hofstadter, in \"Scientific American\", was influential – as was his 1985 book of the same name. \"Memeticist\" was coined as analogous to \"geneticist\" – originally in \"The Selfish Gene.\" Later Arel Lucas suggested that the discipline that studies memes and their connections to human and other carriers of them be known as \"memetics\" by analogy with \"genetics\". Dawkins' \"The Selfish Gene\" has been a factor in attracting the attention of people of disparate intellectual backgrounds. Another stimulus was the publication in 1991 of \"Consciousness Explained\" by Tufts University philosopher Daniel Dennett, which incorporated the meme concept into a theory of the mind. In his 1991 essay \"Viruses of the Mind\", Richard Dawkins used memetics to explain the phenomenon of religious belief and the various characteristics of organised religions. By then, memetics had also become a theme appearing in fiction (e.g. Neal Stephenson's \"Snow Crash\").\n\nThe idea of \"language as a virus\" had already been introduced by William S. Burroughs as early as 1962 in his book \"The Ticket That Exploded\", and later in \"The Electronic Revolution\", published in 1970 in \"\". Douglas Rushkoff explored the same concept in \"Media Virus: Hidden Agendas in Popular Culture\" in 1995.\n\nHowever, the foundation of memetics in its full modern incarnation originated in the publication in 1996 of two books by authors outside the academic mainstream: \"Virus of the Mind: The New Science of the Meme\" by former Microsoft executive turned motivational speaker and professional poker-player, Richard Brodie, and \"Thought Contagion: How Belief Spreads Through Society\" by Aaron Lynch, a mathematician and philosopher who worked for many years as an engineer at Fermilab. Lynch claimed to have conceived his theory totally independently of any contact with academics in the cultural evolutionary sphere, and apparently was not even aware of Dawkins' \"The Selfish Gene\" until his book was very close to publication.\n\nAround the same time as the publication of the books by Lynch and Brodie the e-journal Journal of Memetics – \"Evolutionary Models of Information Transmission\" appeared on the web. It was first hosted by the Centre for Policy Modelling at Manchester Metropolitan University but later taken over by Francis Heylighen of the CLEA research institute at the Vrije Universiteit Brussel. The e-journal soon became the central point for publication and debate within the nascent memeticist community. (There had been a short-lived paper-based memetics publication starting in 1990, the \"Journal of Ideas\" edited by Elan Moritz.) In 1999, Susan Blackmore, a psychologist at the University of the West of England, published \"The Meme Machine\", which more fully worked out the ideas of Dennett, Lynch, and Brodie and attempted to compare and contrast them with various approaches from the cultural evolutionary mainstream, as well as providing novel, and controversial, memetics-based theories for the evolution of language and the human sense of individual selfhood.\n\nThe term \"meme\" derives from the Ancient Greek μιμητής (\"mimētḗs\"), meaning \"imitator, pretender\". The similar term \"mneme\" was used in 1904, by the German evolutionary biologist Richard Semon, best known for his development of the engram theory of memory, in his work \"Die mnemischen Empfindungen in ihren Beziehungen zu den Originalempfindungen\", translated into English in 1921 as \"The Mneme\" . Until Daniel Schacter published \"Forgotten Ideas, Neglected Pioneers: Richard Semon and the Story of Memory\" in 2000, Semon's work had little influence, though it was quoted extensively in Erwin Schrödinger’s prescient 1956 Tarner Lecture “Mind and Matter”. Richard Dawkins (1976) apparently coined the word \"meme\" independently of Semon, writing this:\n\"'Mimeme' comes from a suitable Greek root, but I want a monosyllable that sounds a bit like 'gene'. I hope my classicist friends will forgive me if I abbreviate mimeme to meme. If it is any consolation, it could alternatively be thought of as being related to 'memory', or to the French word même.\" \n\nIn 2005, the \"Journal of Memetics – Evolutionary Models of Information Transmission\" ceased publication and published a set of articles on the future of memetics. The website states that although \"there was to be a relaunch...after several years nothing has happened\". Susan Blackmore has left the University of the West of England to become a freelance science-writer and now concentrates more on the field of consciousness and cognitive science. Derek Gatherer moved to work as a computer programmer in the pharmaceutical industry, although he still occasionally publishes on memetics-related matters. Richard Brodie is now climbing the world professional poker rankings. Aaron Lynch disowned the memetics community and the words \"meme\" and \"memetics\" (without disowning the ideas in his book), adopting the self-description \"thought contagionist\". He died in 2005.\n\nSusan Blackmore (2002) re-stated the definition of meme as: whatever is copied from one person to another person, whether habits, skills, songs, stories, or any other kind of information. Further she said that memes, like genes, are replicators in the sense as defined by Dawkins.\nThat is, they are information that is copied. Memes are copied by imitation, teaching and other methods. The copies are not perfect: memes are copied with variation; moreover, they compete for space in our memories and for the chance to be copied again. Only some of the variants can survive. The combination of these three elements (copies; variation; competition for survival) forms precisely the condition for Darwinian evolution, and so memes (and hence human cultures) evolve. Large groups of memes that are copied and passed on together are called co-adapted meme complexes, or \"memeplexes\". In Blackmore's definition, the way that a meme replicates is through imitation. This requires brain capacity to generally imitate a model or selectively imitate the model. Since the process of social learning varies from one person to another, the imitation process cannot be said to be completely imitated. The sameness of an idea may be expressed with different memes supporting it. This is to say that the mutation rate in memetic evolution is extremely high, and mutations are even possible within each and every iteration of the imitation process. It becomes very interesting when we see that a social system composed of a complex network of microinteractions exists, but at the macro level an order emerges to create culture.\n\nThe memetics movement split almost immediately into two. The first group were those who wanted to stick to Dawkins' definition of a meme as \"a unit of cultural transmission\". Gibron Burchett, another memeticist responsible for helping to research and co-coin the term memetic engineering, along with Leveious Rolando and Larry Lottman, has stated that a meme can be defined, more precisely, as \"a unit of cultural information that can be copied, located in the brain\". This thinking is more in line with Dawkins' second definition of the meme in his book \"The Extended Phenotype\". The second group wants to redefine memes as observable cultural artifacts and behaviors. However, in contrast to those two positions, Blackmore does not reject either concept of external or internal memes.\n\nThese two schools became known as the \"internalists\" and the \"externalists.\" Prominent internalists included both Lynch and Brodie; the most vocal externalists included Derek Gatherer, a geneticist from Liverpool John Moores University, and William Benzon, a writer on cultural evolution and music. The main rationale for externalism was that internal brain entities are not observable, and memetics cannot advance as a science, especially a quantitative science, unless it moves its emphasis onto the directly quantifiable aspects of culture. Internalists countered with various arguments: that brain states will eventually be directly observable with advanced technology, that most cultural anthropologists agree that culture is about beliefs and not artifacts, or that artifacts cannot be replicators in the same sense as mental entities (or DNA) are replicators. The debate became so heated that a 1998 Symposium on Memetics, organised as part of the 15th International Conference on Cybernetics, passed a motion calling for an end to definitional debates. McNamara demonstrated in 2011 that functional connectivity profiling using neuroimaging tools enables the observation of the processing of internal memes, \"i-memes\", in response to external \"e-memes\".\n\nAn advanced statement of the internalist school came in 2002 with the publication of \"The Electric Meme\", by Robert Aunger, an anthropologist from the University of Cambridge. Aunger also organised a conference in Cambridge in 1999, at which prominent sociologists and anthropologists were able to give their assessment of the progress made in memetics to that date. This resulted in the publication of \"Darwinizing Culture: The Status of Memetics as a Science\", edited by Aunger and with a foreword by Dennett, in 2001.\n\nThis evolutionary model of cultural information transfer is based on the concept that units of information, or \"memes\", have an independent existence, are self-replicating, and are subject to selective evolution through environmental forces. Starting from a proposition put forward in the writings of Richard Dawkins, this model has formed the basis of a new area of study, one that looks at the self-replicating units of culture. It has been proposed that just as memes are analogous to genes, memetics is analogous to genetics.\n\nCritics contend that some proponents' assertions are \"untested, unsupported or incorrect.\" Luis Benitez-Bribiesca, a critic of memetics, calls it \"a pseudoscientific dogma\" and \"a dangerous idea that poses a threat to the serious study of consciousness and cultural evolution\" among other things. As factual criticism, he refers to the lack of a \"code script\" for memes, as the DNA is for genes, and to the fact that the meme mutation mechanism (i.e., an idea going from one brain to another) is too unstable (low replication accuracy and high mutation rate), which would render the evolutionary process chaotic. This, however, has been demonstrated (e.g. by Daniel C. Dennett, in \"Darwin's Dangerous Idea\") to not be the case, in fact, due to the existence of self-regulating correction mechanisms (vaguely resembling those of gene transcription) enabled by the redundancy and other properties of most meme expression languages, which do stabilize information transfer. (E.g. spiritual narratives—including music and dance forms—can survive in full detail across any number of generations even in cultures with oral tradition only.) Memes for which stable copying methods are available will inevitably get selected for survival more often than those which can only have unstable mutations, therefore going extinct. (Notably, Benitez-Bribiesca's claim of \"no code script\" is also irrelevant, considering the fact that there is nothing preventing the information contents of memes from being coded, encoded, expressed, preserved or copied in all sorts of different ways throughout their life-cycles.)\n\nAnother criticism comes from semiotics, (e.g., Deacon, Kull) stating that the concept of meme is a primitivized concept of Sign. Meme is thus described in memetics as a sign without its triadic nature. In other words, meme is a degenerate sign, which includes only its ability of being copied. Accordingly, in the broadest sense, the objects of copying are memes, whereas the objects of translation and interpretation are signs.\nMary Midgley criticises memetics for at least two reasons: \"One, culture is not best understood by examining its smallest parts, as culture is pattern-like, comparable to an ocean current. Many more factors, historical and others, should be taken into account than only whatever particle culture is built from. Two, if memes are not thoughts (and thus not cognitive phenomena), as Daniel C. Dennett insists in \"Darwin's Dangerous Idea\", then their ontological status is open to question, and memeticists (who are also reductionists) may be challenged whether memes even exist. Questions can extend to whether the idea of \"meme\" is itself a meme, or is a true concept. Fundamentally, memetics is an attempt to produce knowledge through organic metaphors, which as such is a questionable research approach, as the application of metaphors has the effect of hiding that which does not fit within the realm of the metaphor. Rather than study actual reality, without preconceptions, memetics, as so many of the socio-biological explanations of society, believe that saying that the apple is like an orange is a valid analysis of the apple.\"\n\nHenry Jenkins, Joshua Green, and Sam Ford, in their book \"Spreadable Media\" (2013), criticize Dawkins' idea of the meme, writing that \"while the idea of the meme is a compelling one, it may not adequately account for how content circulates through participatory culture.\" The three authors also criticize other interpretations of memetics, especially those which describe memes as \"self-replicating\", because they ignore the fact that \"culture is a human product and replicates through human agency.\"\n\nLike other critics, Maria Kronfeldner has criticized memetics for being based on an allegedly inaccurate analogy with the gene; alternately, she claims it is \"heuristically trivial\", being a mere redescription of what is already known without offering any useful novelty.\n\nDawkins in \"A Devil's Chaplain\" responded that there are actually two different types of memetic processes (controversial and informative). The first is a type of cultural idea, action, or expression, which does have high variance; for instance, a student of his who had inherited some of the mannerisms of Wittgenstein. However, he also describes a self-correcting meme, highly resistant to mutation. As an example of this, he gives origami patterns in elementary schools – except in rare cases, the meme is either passed on in the exact sequence of instructions, or (in the case of a forgetful child) terminates. This type of meme tends not to evolve, and to experience profound mutations in the rare event that it does.\n\nAnother definition, given by Hokky Situngkir, tried to offer a more rigorous formalism for the meme, \"memeplexes\", and the \"deme\", seeing the meme as a cultural unit in a cultural complex system. It is based on the Darwinian genetic algorithm with some modifications to account for the different patterns of evolution seen in genes and memes. In the method of memetics as the way to see culture as a complex adaptive system, he describes a way to see memetics as an alternative methodology of cultural evolution. However, there are as many possible definitions that are credited to the word \"meme\". For example, in the sense of computer simulation the term \"memetic algorithm\" is used to define a particular computational viewpoint.\n\nThe possibility of quantitative analysis of memes using neuroimaging tools and the suggestion that such studies have already been done was given by McNamara (2011). This author proposes hyperscanning (concurrent scanning of two communicating individuals in two separate MRI machines) as a key tool in the future for investigating memetics.\n\nVelikovsky (2013) proposed the \"holon\" as the structure of the meme, synthesizing the major theories on memes of Richard Dawkins, Mihaly Csikszentmihalyi, E. O. Wilson, Frederick Turner (poet) and Arthur Koestler.\n\nProponents of memetics as described in the Journal of Memetics (out of print since 2005 ) – \"Evolutionary Models of Information Transmission\" believe that 'memetics' has the potential to be an important and promising analysis of culture using the framework of evolutionary concepts. \nKeith Henson in \"Memetics and the Modular-Mind\" (Analog Aug. 1987) makes the case that memetics needs to incorporate evolutionary psychology to understand the psychological traits of a meme's host. This is especially true of time-varying, meme-amplification host-traits, such as those leading to wars.\n\nDiCarlo () has developed the idea of 'memetic equilibrium' to describe a cultural compatible state with biological equilibrium. In \"Problem Solving and Neurotransmission in the Upper Paleolithic\" (in press), diCarlo argues that as human consciousness evolved and developed, so too did our ancestors' capacity to consider and attempt to solve environmental problems in more conceptually sophisticated ways. Understood in this way, problem solving amongst a particular group, when considered satisfactory, often produces a feeling of environmental control, stability, in short—memetic equilibrium. \nBut the pay-off is not merely practical, providing purely functional utility—it is biochemical and it comes in the form of neurotransmitters. The relationship between a gradually emerging conscious awareness and sophisticated languages in which to formulate representations combined with the desire to maintain biological equilibrium, generated the necessity for equilibrium to fill in conceptual gaps in terms of understanding three very important aspects in the Upper Paleolithic: causality, morality, and mortality. The desire to explain phenomena in relation to maintaining survival and reproductive stasis, generated a normative stance in the minds of our ancestors—Survival/Reproductive Value (or S-R Value).\n\nHouben (2014) has argued on several occasions that the exceptional resilience of Vedic ritual and its interaction with a changing ecological and economic environment over several millennia can be profitably dealt with in a ‘cultural evolution’ perspective in which the Vedic mantra is the ‘meme’ or unit of cultural replication.\nThis renders superfluous attempts to explain the phenomenon of Vedic tradition in genetic terms. The domain of Vedic ritual should be able to fulfil to a large extent the three challenges posed to memetics by B. Edmonds (2002 and 2005).\n\nResearch methodologies that apply memetics go by many names: Viral marketing, cultural evolution, the history of ideas, social analytics, and more. Many of these applications do not make reference to the literature on memes directly but are built upon the evolutionary lens of idea propagation that treats semantic units of culture as self-replicating and mutating patterns of information that are assumed to be relevant for scientific study. For example, the field of public relations is filled with attempts to introduce new ideas and alter social discourse. One means of doing this is to design a meme and deploy it through various media channels. One historic example of applied memetics is the PR campaign conducted in 1991 as part of the build-up to the first Gulf War in the United States.\n\nThe application of memetics to a difficult complex social system problem, environmental sustainability, has recently been attempted at thwink.org Using meme types and memetic infection in several stock and flow simulation models, Jack Harich has demonstrated several interesting phenomena that are best, and perhaps only, explained by memes. One model, The Dueling Loops of the Political Powerplace, argues that the fundamental reason corruption is the norm in politics is due to an inherent structural advantage of one feedback loop pitted against another. Another model, The Memetic Evolution of Solutions to Difficult Problems, uses memes, the evolutionary algorithm, and the scientific method to show how complex solutions evolve over time and how that process can be improved. The insights gained from these models are being used to engineer memetic solution elements to the sustainability problem.\n\nAnother application of memetics in the sustainability space is the crowdfunded Climate Meme Project conducted by Joe Brewer and Balazs Laszlo Karafiath in the spring of 2013. This study was based on a collection of 1000 unique text-based expressions gathered from Twitter, Facebook, and structured interviews with climate activists. The major finding was that the global warming meme is not effective at spreading because it causes emotional duress in the minds of people who learn about it. Five central tensions were revealed in the discourse about [climate change], each of which represents a resonance point through which dialogue can be engaged. The tensions were Harmony/Disharmony (whether or not humans are part of the natural world), Survival/Extinction (envisioning the future as either apocalyptic collapse of civilization or total extinction of the human race), Cooperation/Conflict (regarding whether or not humanity can come together to solve global problems), Momentum/Hesitation (about whether or not we are making progress at the collective scale to address climate change), and Elitism/Heretic (a general sentiment that each side of the debate considers the experts of its opposition to be untrustworthy).\n\nBen Cullen, in his book \"Contagious Ideas\", brought the idea of the meme into the discipline of archaeology. He coined the term \"Cultural Virus Theory\", and used it to try to anchor archaeological theory in a neo-Darwinian paradigm. Archaeological memetics could assist the application of the meme concept to material culture in particular.\n\nFrancis Heylighen of the Center Leo Apostel for Interdisciplinary Studies has postulated what he calls \"memetic selection criteria\". These criteria opened the way to a specialized field of \"applied memetics\" to find out if these selection criteria could stand the test of quantitative analyses. In 2003 Klaas Chielens carried out these tests in a Masters thesis project on the testability of the selection criteria.\n\nIn \"Selfish Sounds and Linguistic Evolution\", Austrian linguist Nikolaus Ritt has attempted to operationalise memetic concepts and use them for the explanation of long term sound changes and change conspiracies in early English. It is argued that a generalised Darwinian framework for handling cultural change can provide explanations where established, speaker centred approaches fail to do so. The book makes comparatively concrete suggestions about the possible material structure of memes, and provides two empirically rich case studies.\n\nAustralian academic S.J. Whitty has argued that project management is a memeplex with the language and stories of its practitioners at its core. This radical approach sees a project and its management as an illusion; a human construct about a collection of feelings, expectations, and sensations, which are created, fashioned, and labeled by the human brain. Whitty's approach requires project managers to consider that the reasons for using project management are not consciously driven to maximize profit, and are encouraged to consider project management as naturally occurring, self-serving, evolving process which shapes organizations for its own purpose.\n\nSwedish political scientist Mikael Sandberg argues against \"Lamarckian\" interpretations of institutional and technological evolution and studies creative innovation of information technologies in governmental and private organizations in Sweden in the 1990s from a memetic perspective. Comparing the effects of active (\"Lamarckian\") IT strategy versus user–producer interactivity (Darwinian co-evolution), evidence from Swedish organizations shows that co-evolutionary interactivity is almost four times as strong a factor behind IT creativity as the \"Lamarckian\" IT strategy.\n\n\n\n"}
{"id": "54213951", "url": "https://en.wikipedia.org/wiki?curid=54213951", "title": "Moral universalizability", "text": "Moral universalizability\n\nThe general concept or principle of moral universalizability is that moral principles, maxims, norms, facts, predicates, rules, etc., are universally true; that is, the basis of their being true in any particular case can in some sense be applied to all cases (to all other persons, actions, etc.) Some philosophers, like Immanuel Kant, Richard Hare, and Alan Gewirth, have argued that moral universalizability is the foundation of all moral facts. Others have argued that moral universalizability is a necessary, but not a sufficient, test of morality. A few philosophers have also argued that morality is not constrained by universalizability at all.\n\nThe general concept can be distinguished into two main versions, which can be called \"universal applicability\" and \"universal practice\". Any particular universalizability test requires that some criterion be satisfied within this universalizability condition. A universalization condition combined with a specific satisfaction criterion constitutes a universalizability test. The two versions can be modelled in formal logic as:\nwhere C is a modal operating meaning \"criterion C is satisfied by...\" and Mx means \"an agent/action follows principle (or maxim) M\". For a strong universalizability test, satisfying this formula makes M permissible; for a weak universalizability test (or universalizability principle), satisfying this formula is necessary for M to be permissible, but may not be sufficient. Both are universalization conditions in virtue of having a universal quantifier, (x), but the role of each is different.\n\nIn a UA test, M is only permissible if the criterion C is satisfied for each possible individual, in any possible situation or world if she follows (or were to follow) the principle M. In particular, C must be satisfied anytime an individual does M even if other agents do not do M. That is, the requirement that M must satisfy C is universally applicable to each possible person's behavior, considered individually.\n\nFor a UP test, however, whether C is satisfied when not all persons are doing M is irrelevant; it is only required that C be satisfied whenever it is the case that everyone does M: that is, when M is universally practiced.\n\nIn addition to using different universalization conditions, universalizability tests use a variety of different satisfaction criteria. For example, consequentialists typically use criteria like \"produces at least as much good as any alternative would\" or \"has at least as much expected value as any alternative.\" These tend to be aggregative, allowing the addition of value across different agents. Deontologists tend to use non-aggregative criteria like \"is not impossible\" (Kant's contradiction in conception test), \"would make the satisfaction of your ends impossible\" (Kant's contradiction in will test), \"would disrespect humanity in yourself or another\" (Kant's formula of humanity), or \"would be reasonable to reject\" (Scanlon's contractualist test).\n\nIn this condition, a moral predicate (like obligatory, permissible, forbidden, etc.) always applies to a given behavior in virtue of some reason, and whenever the same reason is present, the same predicate applies.\n\nAccording to the principle of moral supervenience, moral properties of actions (obligatory, permissible, forbidden, etc.) supervene on--that is, depend upon or are functions of--non-moral properties. The principle itself does not specify which moral properties these are, so it does not constitute a universalizability test. However it is often considered a necessary feature of any moral truth, and hence is often thought to rule out certain general theories of morality (see meta-ethics), even if it cannot forbid many particular actions.\n\nIn a series of books, R.M. Hare (who introduced the term into philosophical literature) made moral supervenience the basis of his derivation of a version of utilitarianism, but this was actually a universal applicability condition combined with the criterion that the universalized behavior would not produce a greater balance of satisfied over frustrated preferences of all affected agents (including animal agents as well as persons) than any alternative behavior would. Other act consequentialists also use versions of this argument, often expressing this in terms of the golden rule or the universality of reasons, where this is described as a universal applicability condition. J.S. Mill was also an act consequentialist, using the universal applicability condition and rejecting the universal practice condition as part of any fundamental moral principle, although he thought that the results of everyone's acting the same way was a useful guide for determining when an individual act was likely to produce good consequences.\n\nThe universal applicability condition is also embodied in the colloquial question, \"How would you like it if somebody else did that to you?\" Here, the presumption is that the behavior in question causes some harm or offense to other people, even though it may benefit or please the person performing it. When the person reflects upon how someone else's performance of the same behavior might harm herself, she finds she cannot approve of this, which suggests that if she is consistent she should also disapprove of herself doing it, judging it morally wrong. The question is imprecise in that it does not specify exactly what effects of the behavior would be grounds for considering it impermissible, and therefore, like the principle of moral supervenience, does not specify a complete universalizability test. Likewise, the phrase \"What's good (or: is sauce) for the goose is good (sauce) for the gander,\" suggesting that minor, irrelevant differences do not affect the permissibility of some behavior, so if it is permissible for one person (the \"goose\") then it is also permissible for any very similar person (a \"gander\")-with the implication that if someone rejects the second judgment, they must either explain why the different between the two cases is morally relevant, or retract their judgment of the first case.\n\nAlan Gewirth uses a universal applicability condition in his \"principle of generic consistency,\", who added to it the criterion that the effects of anyone's actions may not deny to any other person the necessary conditions of their successful agency, most notably including \"freedom and well-being.\"\n\nThe 18th-century German philosopher Immanuel Kant's second formulation of a categorical imperative or fundamental moral principle, the formula of humanity as an end in itself, uses a UA condition. It requires all persons to always respect humanity in oneself or another as an end in itself. \"Always\" presumably means: in any possible situation, and hence implicitly invokes a UA condition: even if some instance of a behavior is not disrespectful, if some possible instance of it would be disrespectful, then we must follow a principle of avoiding it in the latter type of case.\n\nIn this condition, a behavior if permissible if and only if its universal practice by all persons necessarily satisfies some criterion. An imagined world in which everyone in it conform to the same kind of behavior is often called an \"ideal world,\" and so moral or political theories appealing to it are sometimes called \"ideal theories\" as opposed to \"non-ideal theories.\" This should not be taken to mean that the former theories are themselves more ideal or better than the latter by definition; the ideality in question refers to the worlds the theories describe, are based on, or intended to apply to, not necessarily to how well they describe moral facts and obligations in the actual world, which is much disputed.\n\nFor many behaviors, universalizability tests using the same satisfaction criteria but different universalization conditions will reach the same conclusion. For instance, if every person's M-ing causes the same amount of harm and good as anyone else's, no matter what anyone else does, then the total effect of everyone's M-ing will be the effect of one person's M-ing multiplied by the number of persons; if the criterion is that the effect cause no more harm than good, then the same behaviors will satisfy or fail this criterion under either universalization condition we use. However some behaviors cause different amounts of harm depending upon how many other people are performing them. For these behaviors, universal practice tests generally give counter-intuitive, and often quite harmful recommendations, for cases in which not everyone else is doing the same thing we are doing. This occurs in two general kinds of cases: responding to evil-doers, and solving coordination problems. Together they are typically called \"partial compliance\" problems, because they deal with the question of what would be a morally-correct response to a situation where other persons are not doing the same thing you are doing (or considering doing).\n\nThe first is illustrated by a disposition to complete pacifism. In any world where everyone is pacifistic, little harm is done, so it passes most universal practice tests. But in a realistic world containing many non-pacifists, an individual's commitment to complete pacifism is likely to not only make him a victim of evil-doers, but unable to defend innocent third parties from the latter, resulting in much greater harm than a more complex conditional disposition, like \"be pacifist only if all others are pacifistic, but defend yourself and others from aggression if necessary.\"\n\nThe second is exemplified by the need to choose which side of the road to drive on. A rule of always driving on the left passes most universal practice tests. It follows that such tests permit such behavior--even if not everyone else has chosen this option. Of course, the rule of always driving on the right, as well as the more intuitively plausible rule of driving on whatever side conforms to local custom or law, also passes such tests. So while such tests permit such intuitively plausible rules, they also permit some which will lead to catastrophe in a world where not everyone has picked the same permissible rule. Hence, like the rule of absolute pacifism, such a rule will typically pass a universal practice test, but fails a universal applicability test, because while everyone's following the rule is not harmful, its practice by some persons \"while some other people are not doing so\" does produce significant harm. Any behavior condemned by a universal practice test will also be condemned by a universal approval test using the same satisfaction criteria, so the latter is always at least as strong as the former.\n\nA variety of proposals have been made to try to rescue universal practice tests from these implications, but none have been universally considered successful.\n\nThe UP condition is expressed by the colloquial question \"What if everybody did that?\" Like the colloquial \"somebody\" question above, it leaves unstated precisely what result of everyone's performance of the behavior would make this result unacceptable and hence make the behavior wrong. \n\nImmanuel Kant's first formulation of the categorical imperative, the \"Formula of Universal Law,\" as well as his third \"Kingdom of Ends\" formulation, also use a universal practice condition. The first formula states that the only morally acceptable maxims of our actions are those that could rationally be willed to practiced as a universal law, or in a variant \"Law of Nature\" formulation, one whose practice by all persons we could will to have been a law of nature (and hence necessarily governing the behavior of all persons throughout all time and space). Kant appealed to two criteria which must be satisfied under such a condition: first, the universalization must be conceivable, and second that this universalization will not necessarily frustrate the ends of any agent practicing the maxim (and hence such an agent can both will his own practice of the maxim, and its practice by all other agents). The first is violated by maxims, e.g., of always lying or making false promises, for if (\"per impossibile\") everyone did the same thing, no one would even consider any form of words given by another to count as a statement or promise, so it would be impossible to even try to make such statements with the intent to deceive. The second is violated by maxims, e.g, of never helping another person in need, for while we could imagine a world in which no one gave such help to anyone else, no agent could possibly want others to treat him that way, for there are bound to be occasions which the lack of such help will inevitably result in the frustration of the first person's ends-which, by hypothesis, he wills to satisfy.\n\nKant called the first kind of violation a contradiction in conception, the second a contradiction in will. Like Gewirth's idea of frustrating the necessary conditions of agency, they involve a performative contradiction, because the practice of the maxim by others would undermine one's own attempt to practice it, and willing the former (even when this does not cause others to practice it) is tantamount to willing the frustration of one's own agency. However Kant's Formula of Universal Law only identifies these contradictions in cases where the maxim is universally practiced; for Gewirth they can also occur in cases where some (but not all) persons' performing the behavior would deprive you of the necessary condition of agency.\n\nDefenders of Kant's ethics have often appealed to his second version of the categorical imperative, the formula of humanity, in order to successfully deal with problems involving evil-doers and coordination problems. As noted above, this formula can successfully do so because it involves a universal applicability condition, and hence is sensitive to the harm done by various maxims in non-ideal conditions even if their universal practice is harmless.\n\nAnother moral theory using a universal practice test is rule consequentialism, or more precisely that version of it sometimes called \"ideal rule consequentialism\", where a moral rule is permissible if and only if its practice by all persons would produce at least as much of a balance of good over bad results than the universal practice of any other rule would. This theory was first expounded by Jurist John Austin , and defended at greater length by R.F. Harrod in 1936, on the grounds that some behaviors cause \"more\" harm when everyone (or almost everyone) else is doing the same thing than in the more common case where not everyone else is doing so; since the harm can get so very bad in such cases, he argued that we should use its effects in this universal practice case to condemn the same behavior even when almost no one else is doing it, and even when it actually causes little or no harm in such cases. He illustrated this with Kant's example of telling a lie, arguing that the practice of lying by many individual people may actually only be modestly harmful, and in some cases may produce more good than the truth, as long as enough truth-tellers are still around to provide a moral example and give us some confidence that we will usually be told the truth (and be believed when we speak); but if everyone lies, then communication becomes impossible, a devastating effect which Harrod thought justified our condemnation of lying even in the normal case where it may cause little or no harm.\n\nThis view has been much criticized, however, on the grounds that it involves \"rule-worship\" which forbids us from performing such behaviors precisely when they are harmless or even beneficial, just because they would be less beneficial in circumstances which we know we are not in, and that an adequate response (justified by a UA test) is to follow the slightly more but not unmanageable complex rule of usually telling the truth, but lying just when doing so will definitely produce more good than telling the truth. Considerations of the aforementioned need to handle evil-doers has also motivated some defenders of ideal rule consequentialism to suggest modified versions which permit only rules which produce at least as great a balance of good over bad results when 90% (or some other fraction) of any given population practice it, which requires us to follow rules which contain provisions for how to respond to the 10% (criminals, etc.) who may be violating the rules in question. Such proposals have in turn been criticized as a 90% practice level is just as arbitrary as a 100% practice level for the test condition, which can be avoided by appealing to a \"variable-rate\" test condition where the rule must be optimal regardless of the fraction of the population following it. These proposals in effect abandon the universal practice condition for the universal applicability condition.\n\nOther versions of a universal practice test are found in M.G. Singer's \"generalization argument,\", J. Habermas's \"principle U,\" and T.M. Scanlon's \"contractualism.\" These have each in turn been occasionally criticized for their inability to handle non-ideal cases\n\n"}
{"id": "16909978", "url": "https://en.wikipedia.org/wiki?curid=16909978", "title": "Movement for Justice by Any Means Necessary", "text": "Movement for Justice by Any Means Necessary\n\nThe Movement for Justice was set up in 1995 by people around the Kingsway College Student Union in the London Borough of Camden to tackle racism in institutional and established forms. The group confronted organised fascism as well as death in custody and wider racism to black people as well as travellers, refugees and asylum seekers. It is also the sister group to the American organization The Coalition to Defend Affirmative Action, Integration & Immigrant Rights, and Fight for Equality By Any Means Necessary (BAMN). Movement for Justice is headed up by members of the Revolutionary Internationalist League (RIL), a Trotskyist group.\n\nThe group first grew following the murder of Stephen Lawrence and the campaign that followed. The group went on to campaign for justice for the murders by racists and in police custody of many others including Rolan Adams, Michael Tachie-Menson,\n\nThe group took part in the demonstrations to close down the British National Party headquarters in the area and helped the Youth against Racism in Europe to build an anti-racist campaign through this. They also campaigned with the YRE and local community to fight drove the BNP off Brick Lane.\n\nIn 1993 a Kingsway student Shah Alam was nearly killed in a racist attack in Poplar, East London and they organised the Justice for Shah Alam Campaign which organised a march, public meetings, press conferences and court pickets to get the racists convicted and jailed.\n\nAfter the death of Brian Douglas they helped get Lambeth Unison (public service workers union) and Kingsway College Student Union with the campaign.\n\nThe organisation also concentrated on mass non-co-operation with the Asylum Bill and in September 1995, published a pamphlet \"Howard's Racist Immigration and Asylum Bill - What it is and how to fight it\". In 1995 the group orchestrated a paint attack on Brian Mawhinney, Tory MP outside Parliament at the state opening and Queen's speech, because of what they saw as his deliberate use of emotive and misrepresentative language about 'British people fearing immigrants flooding the country' which were seen by many as an incitement to racial hatred. The group continued to oppose immigration, asylum legislation after Labour took power.\n\nAfter the death of Oscar Okoye the group came under attack from Lee Jasper and Brian Paddick. The group's chairman Alex Olowade was sacked from his job with Lambeth Council.\n\nIn 2001 after the police shot dead a man with a replica gun cigarette lighter in Brixton, the group organised a demonstration. The demonstration was covered by the BBC.\n\nFormer Metropolitan Police Special Branch undercover officer Peter Francis, who as part of the secretive Special Demonstration Squad infiltrated the YRE and associated groups between 1993 and 1997, claimed in a 2014 statement issued through his solicitor that he had been a founding member of Movement for Justice.\n\nThe group organised the first London student civil rights conference on 13 July 2006 and according to \"The Daily Telegraph\" was the key organiser behind the \"Day of Rage\" protests following the Grenfell Tower fire.\n\n"}
{"id": "20156816", "url": "https://en.wikipedia.org/wiki?curid=20156816", "title": "Personal Information Agent", "text": "Personal Information Agent\n\nA Personal Information Agent (PIA) is an individual, business, or organization who is expressly authorized by another identifiable individual in dealings with third persons, businesses or organizations concerning Personally identifiable information (PII). PIA status allows access to information pertaining to an identifiable individual and the records and associated files of that identifiable individual. This normally includes, but is not limited to, financial files, correspondence, memorandum, machine-readable records and any other documentary material, regardless of physical form or characteristics. Access of these records extends to any copy of any of those things, pertaining to that identifiable individual and including the right to audit and monitor activities that involve the process for notification and reporting of unauthorized disclosure or PII breaches.\n\n"}
{"id": "5550368", "url": "https://en.wikipedia.org/wiki?curid=5550368", "title": "Pixel aspect ratio", "text": "Pixel aspect ratio\n\nPixel aspect ratio (often abbreviated PAR) is a mathematical ratio that describes how the width of a pixel in a digital image compares to the height of that pixel.\n\nMost digital imaging systems display an image as a grid of tiny, square pixels. However, some imaging systems, especially those that must be compatible with standard-definition television motion pictures, display an image as a grid of rectangular pixels, in which the pixel width and height are different. Pixel aspect ratio describes this difference.\n\nUse of pixel aspect ratio mostly involves pictures pertaining to standard-definition television and some other exceptional cases. Most other imaging systems, including those that comply with SMPTE standards and practices, use square pixels.\n\nThe ratio of the width to the height of an image is known as the aspect ratio, or more precisely the display aspect ratio (DAR) – the aspect ratio of the image \"as displayed;\" for TV, DAR was traditionally 4:3 (a.k.a. fullscreen), with 16:9 (a.k.a. widescreen) now the standard for HDTV. In digital images, there is a distinction with the storage aspect ratio (SAR), which is the ratio of pixel dimensions. If an image is displayed with square pixels, then these ratios agree; if not, then non-square, \"rectangular\" pixels are used, and these ratios disagree. The aspect ratio of the pixels themselves is known as the \"pixel aspect ratio\" (PAR) – for square pixels this is 1:1 – and these are related by the identity:\n\nRearranging (solving for PAR) yields:\n\nFor example, a 640 × 480 VGA image has a SAR of 640/480 = 4:3, and if displayed on a 4:3 display (DAR = 4:3) has square pixels, hence a PAR of 1:1. By contrast, a 720 × 576 D-1 PAL image has a SAR of 720/576 = 5:4, but is displayed on a 4:3 display (DAR = 4:3).\n\nIn analog images such as film there is no notion of pixel, nor notion of SAR or PAR, but in the \"digitization\" of analog images the resulting digital image has pixels, hence SAR (and accordingly PAR, if displayed at the same aspect ratio as the original).\n\nNon-square pixels arise often in early digital TV standards, related to digitalization of analog TV signals – whose vertical and \"effective\" horizontal resolutions differ and are thus best described by non-square pixels – and also in some digital video cameras and computer display modes, such as Color Graphics Adapter (CGA). Today they arise also in transcoding between resolutions with different SARs.\n\nActual displays do not generally have non-square pixels, though digital sensors might; they are rather a mathematical abstraction used in resampling images to convert between resolutions.\n\nThere are several complicating factors in understanding PAR, particularly as it pertains to digitization of analog video:\n\nVideo is presented as a sequential series of images called video frames. Historically, video frames were created and recorded in analog form. As digital display technology, digital broadcast technology, and digital video compression evolved separately, it resulted in video frame differences that must be addressed using pixel aspect ratio. Digital video frames are generally defined as a grid of pixels used to present each sequential image. The horizontal component is defined by pixels (or samples), and is known as a video line. The vertical component is defined by the number of lines, as in 480 lines.\n\nStandard-definition television standards and practices were developed as broadcast technologies and intended for terrestrial broadcasting, and were therefore not designed for digital video presentation. Such standards define an image as an array of well-defined horizontal \"Lines\", well-defined vertical \"Line Duration\" and a well-defined picture center. However, there is not a standard-definition television standard that properly defines image edges or explicitly demands a certain number of picture elements per line. Furthermore, analog video systems such as NTSC 480i and PAL 576i, instead of employing progressively displayed frames, employ fields or interlaced half-frames displayed in an interwoven manner to reduce flicker and double the image rate for smoother motion.\n\nAs a result of computers becoming powerful enough to serve as video editing tools, video digital-to-analog converters and analog-to-digital converters were made to overcome this incompatibility. To convert analog video lines into a series of square pixels, the industry adopted a default sampling rate at which luma values were extracted into pixels. The luma sampling rate for 480i pictures was  MHz and for 576i pictures was  MHz.\n\nThe term \"pixel aspect ratio\" was first coined when ITU-R BT.601 (commonly known as \"Rec. 601\") specified that standard-definition television pictures are made of lines of exactly 720 non-square pixels. ITU-R BT.601 did not define the exact pixel aspect ratio but did provide enough information to calculate the exact pixel aspect ratio based on industry practices: The standard luma sampling rate of precisely  MHz. Based on this information:\n\nSMPTE RP 187 further attempted to standardize the pixel aspect ratio values for 480i and 576i. It designated 177:160 for 480i or 1035:1132 for 576i. However, due to significant difference with practices in effect by industry and the computational load that they imposed upon the involved hardware, SMPTE RP 187 was simply ignored. SMPTE RP 187 information annex A.4 further suggested the use of 10:11 for 480i.\n\nAs of this writing, ITU-R BT.601-6, which is the latest edition of ITU-R BT.601, still implies that the pixel aspect ratios mentioned above are correct.\nAs stated above, ITU-R BT.601 specified that standard-definition television pictures are made of lines of 720 non-square pixels, sampled with a precisely specified sampling rate. A simple mathematical calculation reveals that a 704 pixel width would be enough to contain a 480i or 576i standard 4:3 picture:\n\n\nUnfortunately, not all standard TV pictures are exactly 4:3: As mentioned earlier, in analog video, the center of a picture is well-defined but the edges of the picture are not standardized. As a result, some analog devices (mostly PAL devices but also some NTSC devices) generated motion pictures that were horizontally (slightly) wider. This also proportionately applies to anamorphic widescreen (16:9) pictures. Therefore, to maintain a safe margin of error, ITU-R BT.601 required sampling 16 more non-square pixels per line (8 more at each edge) to ensure saving all video data near the margins.\n\nThis requirement, however, had implications for PAL motion pictures. PAL pixel aspect ratios for standard (4:3) and anamorphic wide screen (16:9), respectively 59:54 and 118:81, were awkward for digital image processing, especially for mixing PAL and NTSC video clips. Therefore, video editing products chose the almost equivalent values, respectively 12:11 and 16:11, which were more elegant and could create PAL digital images at exactly 704 pixels wide, as illustrated:\n\nCommonly found on the Internet and in various other published media are numerous sources that introduce different and highly incompatible values as the pixel aspect ratios of various video pictures and video systems. (See the Supplementary sources section.)\n\nTo neutrally judge the accuracy and/or feasibility of these sources, please note that as the digital motion picture was invented years after the traditional motion picture, all video pictures targeted for standard definition television and compatible media, digital or otherwise, have (and must have) specifications compatible with standard definition television. Therefore, the pixel aspect ratio of digital video must be calculated from the specification of common traditional equipment rather than the specifications of digital video. Otherwise, any pixel aspect ratio that is calculated from a digital video source is only usable in certain cases for the same kind of video sources and cannot be considered/used as a general pixel aspect ratio of any standard definition television system.\n\nIn addition, unlike digital video that has well-defined picture edges, traditional video systems have never standardized a well-defined edge for the picture. Therefore, the pixel aspect ratio of common standard television systems cannot be calculated based on edges of pictures. Such a calculated aspect ratio value would not be entirely wrong, but also cannot be considered as the general pixel aspect ratio of any specific video system. The use of such values would be restricted only to certain cases.\n\nIn modern digital imaging systems and high-definition televisions, especially those that comply with SMPTE standards and practices, only square pixels are used for broadcast and display. However, some formats (ex., HDV, DVCPRO HD) use non-square pixels internally for image storage, as a way to reduce the amount of data that must be processed, thus limiting the necessary transfer rates and maintaining compatibility with existing interfaces.\n\nDirectly mapping an image with a certain pixel aspect ratio on a device whose pixel aspect ratio is different makes the image look unnaturally stretched or squashed in either the horizontal or vertical direction. For example, a circle generated for a computer display with square pixels looks like a vertical ellipse on a standard-definition NTSC television that uses vertically rectangular pixels. This issue is more evident on wide-screen TVs.\n\nPixel aspect ratio must be taken into consideration by video editing software products that edit video files with non-square pixels, especially when mixing video clips with different pixel aspect ratios. This would be the case when creating a video montage from various cameras employing different video standards (a relatively rare situation). Special effects software products must also take the pixel aspect ratio into consideration, since some special effects require calculation of the distances from a certain point so that they look visually correct. An example of such effects would be radial blur, motion blur, or even a simple image rotation.\n\nPixel aspect ratio value is used mainly in digital video software, where motion pictures must be converted or reconditioned to use video systems other than the original. The video player software may use pixel aspect ratio to properly render digital video on screen. Video editing software uses pixel aspect ratio to properly scale and render a video into a new format.\n\nThe pixel aspect ratio support is also required to display, without distortion, legacy digital images from computer standards and video-games what existed in the 80s. In that generation, square pixels were too expensive to produce, so machines and video cards like the SNES, CGA, EGA, Hercules, C64, MSX, PC-88, X68000 etc had non-square pixels.\n\nPixel aspect ratio is often confused with different types of image aspect ratios; the ratio of the image width and height. Due to non-squareness of pixels in Standard-definition TV, there are two types of such aspect ratios: \"storage aspect ratio\" (\"SAR\") and \"display aspect ratio\" (abbreviated \"DAR\", also known as \"image aspect ratio\" and \"picture aspect ratio\"). Note the reuse of the abbreviation \"PAR\". This article uses only the terms pixel aspect ratio and display aspect ratio to avoid ambiguity.\n\nStorage aspect ratio is the ratio of the image width to height in pixels, and can be easily calculated from the video file. Display aspect ratio is the ratio of image width to height (in a unit of length such as centimeters or inches) when displayed on screen, and is calculated from the combination of pixel aspect ratio and storage aspect ratio.\n\nHowever, users who know the definition of these concepts may get confused as well. Poorly crafted user-interfaces or poorly written documentations can easily cause such confusion: Some video-editing software applications often ask users to specify an \"aspect ratio\" for their video file, presenting him or her with the choices of \"4:3\" and \"16:9\". Sometimes, these choices may be \"PAL 4:3\", \"NTSC 4:3\", \"PAL 16:9\" and \"NTSC 16:9\". In such situations, the video editing program is implicitly asking for the pixel aspect ratio of the video file by asking for information about the video system from which the video file originated. The program then uses a table (similar to the one below) to determine the correct pixel aspect ratio value.\n\nGenerally speaking, to avoid confusion, it can be assumed that video editing products never ask for the storage aspect ratio as they can directly retrieve or calculate it. Non-square-pixel–aware applications also need only to ask for either pixel aspect ratio or display aspect ratio, from either of which they can calculate the other.\n\nPixel aspect ratio values for common standard-definition video formats are listed below. Note that for PAL video formats, two different types of pixel aspect ratio values are listed:\n\nNote that sources differ on PARs for common formats – for example, 576 lines (PAL) displayed at 4:3 (DAR) corresponds to either PAR of 12:11 (if 704×576, SAR = 11:9), or a PAR of 16:15 (if 720×576, SAR = 5:4). See references for sources giving both, and SDTV: Resolution for a table of storage, display and pixel aspect ratios.\n\n\n"}
{"id": "19959057", "url": "https://en.wikipedia.org/wiki?curid=19959057", "title": "Samson Kambalu", "text": "Samson Kambalu\n\nSamson Kambalu (born 1975) is a Malawi-born artist, academic and author who trained as a fine artist and ethnomusicologist at the University of Malawi's Chancellor College.\n\nKambalu was born in Malawi, where he attended Kamuzu Academy, the \"Eton of Africa\". He graduated from the University of Malawi's Chancellor College, Zomba in 1999. Kambalu completed his MA in Fine Art at Nottingham Trent University in 2003 and wrote his PhD at Chelsea College of Art and Design, looking at how the problematic of the gift and the general economy animates various aspects of his art practice.\n\nKambalu's work, which references Situationism and the Chewa Nyau culture of his native Malawi, manifests in various media, from drawing, painting, installation, video to literature and performance.\n\nOne of his most well known artworks is \"Holy Ball\", a football plastered in pages of the Bible. Kambalu held an exhibition of 24 \"Holy Balls\" at Chancellor College in 2000 at which he invited the visitors to “exercise and exorcise”. He has since shown his work internationally. In 2015 he was included in Okwui Enwezor's \"All the World's Futures\" at the 56th Venice Biennale. In November 2015 a judge in Venice dismissed a complaint filed by the Italian situationist Gianfranco Sanguinetti against the Venice Biennale and Kambalu with regard to one of his installations, \"Sanguinetti Breakout Area\".\n\nKambalu's \"Nyau Cinema\" is a series of short film clips of psychogeographical performances, shared as interventions on social networking sites and as installations in galleries. These have been described as \"cinematic fragments that blend slapstick and spiritual ritual\".\n\nHis first book, an autobiographical narrative entitled \"The Jive Talker or How to Get a British Passport\", was published by Jonathan Cape (Random House) in July 2008, and in August 2008 by Free Press (Simon & Schuster). His second novel, \"Uccello's Vineyard\", published in 2012, is in \"The Book Lovers\", a collection of artist novels at the Museum of Contemporary Art in Antwerp.\n\nKambalu is represented by Kate MacGarry in London and Galerie Nordenhake in Stockholm.\n\n\n\n\n\n\n"}
{"id": "73415", "url": "https://en.wikipedia.org/wiki?curid=73415", "title": "Sieve of Eratosthenes", "text": "Sieve of Eratosthenes\n\nIn mathematics, the sieve of Eratosthenes is a simple, ancient algorithm for finding all prime numbers up to any given limit.\n\nIt does so by iteratively marking as composite (i.e., not prime) the multiples of each prime, starting with the first prime number, . The multiples of a given prime are generated as a sequence of numbers starting from that prime, with constant difference between them that is equal to that prime. This is the sieve's key distinction from using trial division to sequentially test each candidate number for divisibility by each prime.\n\nThe earliest known reference to the sieve (, \"kóskinon Eratosthénous\") is in Nicomachus of Gerasa's \"Introduction to Arithmetic\", which describes it and attributes it to Eratosthenes of Cyrene, a Greek mathematician.\n\nOne of a number of prime number sieves, it is one of the most efficient ways to find all of the smaller primes. It may be used to find primes in arithmetic progressions.\n\nA prime number is a natural number that has exactly two distinct natural number divisors: 1 and itself.\n\nTo find all the prime numbers less than or equal to a given integer by Eratosthenes' method:\n\n\nThe main idea here is that every value given to will be prime, because if it were composite it would be marked as a multiple of some other, smaller prime. Note that some of the numbers may be marked more than once (e.g., 15 will be marked both for 3 and 5).\n\nAs a refinement, it is sufficient to mark the numbers in step 3 starting from , as all the smaller multiples of will have already been marked at that point. This means that the algorithm is allowed to terminate in step 4 when is greater than . \n\nAnother refinement is to initially list odd numbers only, , and count in increments of from in step 3, thus marking only odd multiples of . This actually appears in the original algorithm. This can be generalized with wheel factorization, forming the initial list only from numbers coprime with the first few primes and not just from odds (i.e., numbers coprime with 2), and counting in the correspondingly adjusted increments so that only such multiples of are generated that are coprime with those small primes, in the first place.\n\nTo find all the prime numbers less than or equal to 30, proceed as follows.\n\nFirst, generate a list of integers from 2 to 30:\n\nThe first number in the list is 2; cross out every 2nd number in the list after 2 by counting up from 2 in increments of 2 (these will be all the multiples of 2 in the list):\n\nThe next number in the list after 2 is 3; cross out every 3rd number in the list after 3 by counting up from 3 in increments of 3 (these will be all the multiples of 3 in the list):\n\nThe next number not yet crossed out in the list after 3 is 5; cross out every 5th number in the list after 5 by counting up from 5 in increments of 5 (i.e. all the multiples of 5):\n\nThe next number not yet crossed out in the list after 5 is 7; the next step would be to cross out every 7th number in the list after 7, but they are all already crossed out at this point, as these numbers (14, 21, 28) are also multiples of smaller primes because 7 × 7 is greater than 30. The numbers not crossed out at this point in the list are all the prime numbers below 30:\n\nThe sieve of Eratosthenes can be expressed in pseudocode, as follows:\n\nThis algorithm produces all primes not greater than . It includes a common optimization, which is to start enumerating the multiples of each prime from . The time complexity of this algorithm is , provided the array update is an operation, as is usually the case.\n\nAs Sorenson notes, the problem with the sieve of Eratosthenes is not the number of operations it performs but rather its memory requirements. For large , the range of primes may not fit in memory; worse, even for moderate , its cache use is highly suboptimal. The algorithm walks through the entire array , exhibiting almost no locality of reference.\n\nA solution to these problems is offered by \"segmented\" sieves, where only portions of the range are sieved at a time. These have been known since the 1970s, and work as follows:\n\n\nIf is chosen to be , the space complexity of the algorithm is , while the time complexity is the same as that of the regular sieve.\n\nFor ranges with upper limit so large that the sieving primes below as required by the page segmented sieve of Eratosthenes cannot fit in memory, a slower but much more space-efficient sieve like the sieve of Sorenson can be used instead.\n\nAn incremental formulation of the sieve generates primes indefinitely (i.e., without an upper bound) by interleaving the generation of primes with the generation of their multiples (so that primes can be found in gaps between the multiples), where the multiples of each prime are generated directly by counting up from the square of the prime in increments of (or for odd primes). The generation must be initiated only when the prime's square is reached, to avoid adverse effects on efficiency. It can be expressed symbolically under the dataflow paradigm as\n\nPrimes can also be produced by iteratively sieving out the composites through [[Trial division|divisibility testing]] by sequential primes, one prime at a time. It is not the sieve of Eratosthenes but is often confused with it, even though the sieve of Eratosthenes directly generates the composites instead of testing for them. Trial division has worse theoretical [[Analysis of algorithms|complexity]] than that of the sieve of Eratosthenes in generating ranges of primes.\n\nWhen testing each prime, the \"optimal\" trial division algorithm uses all prime numbers not exceeding its square root, whereas the sieve of Eratosthenes produces each composite from its prime factors only, and gets the primes \"for free\", between the composites. The widely known 1975 [[functional programming|functional]] sieve code by [[David Turner (computer scientist)|David Turner]] is often presented as an example of the sieve of Eratosthenes but is actually a sub-optimal trial division sieve.\n\nThe work performed by this algorithm is almost entirely the operations to cull the composite number representations which for the basic non-optimized version is the sum of the range divided by each of the primes up to that range or\nwhere is the sieving range in this and all further analysis.\n\nBy rearranging Mertens' second theorem, this is equal to as approaches infinity, where M is the Meissel–Mertens constant of about ...\n\nThe optimization of starting at the square of each prime and only culling for primes less than the square root changes the \"\" in the above expression to (or ) and not culling until the square means that the sum of the base primes each minus two is subtracted from the operations. As the sum of the first primes is and the prime number theorem says that is approximately , then the sum of primes to is , and therefore the sum of base primes to is expressed as a factor of . The extra offset of two per base prime is , where is the prime-counting function in this case, or ; expressing this as a factor of as are the other terms, this is . Combining all of this, the expression for the number of optimized operations without wheel factorization is\n\nFor the wheel factorization cases, there is a further offset of the operations not done of\nwhere is the highest wheel prime and a constant factor of the whole expression is applied which is the fraction of remaining prime candidates as compared to the repeating wheel circumference. The wheel circumference is\nand it can easily be determined that this wheel factor is\nas is the fraction of remaining candidates for the highest wheel prime, , and each succeeding smaller prime leaves its corresponding fraction of the previous combined fraction.\n\nCombining all of the above analysis, the total number of operations for a sieving range up to including wheel factorization for primes up to is approximately\n\nTo show that the above expression is a good approximation to the number of composite number cull operations performed by the algorithm, following is a table showing the actually measured number of operations for a practical implementation of the sieve of Eratosthenes as compared to the number of operations predicted from the above expression with both expressed as a fraction of the range (rounded to four decimal places) for different sieve ranges and wheel factorizations (Note that the last column is a maximum practical wheel as to the size of the wheel gaps Look Up Table - almost 10 million values):\n\nThe above table shows that the above expression is a very good approximation to the total number of culling operations for sieve ranges of about a hundred thousand (10) and above.\n\nThe sieve of Eratosthenes is a popular way to benchmark computer performance. As can be seen from the above by removing all constant offsets and constant factors and ignoring terms that tend to zero as n approaches infinity, the time complexity of calculating all primes below in the random access machine model is operations, a direct consequence of the fact that the prime harmonic series asymptotically approaches . It has an exponential time complexity with regard to input size, though, which makes it a pseudo-polynomial algorithm. The basic algorithm requires of memory.\n\nThe bit complexity of the algorithm is bit operations with a memory requirement of .\n\nThe normally implemented page segmented version has the same operational complexity of as the non-segmented version but reduces the space requirements to the very minimal size of the segment page plus the memory required to store the base primes less than the square root of the range used to cull composites from successive page segments of size .\n\nA special rarely if ever implemented segmented version of the sieve of Eratosthenes, with basic optimizations, uses operations and bits of memory.\n\nTo show that the above approximation in complexity is not very accurate even for about as large as practical a range, the following is a table of the estimated number of operations as a fraction of the range rounded to four places, the calculated ratio for a factor of ten change in range based on this estimate, and the factor based on the estimate for various ranges and wheel factorizations (the combo column uses a frequently practically used pre-cull by the maximum wheel factorization but only the 2/3/5/7 wheel for the wheel factor as the full factorization is difficult to implement efficiently for page segmentation):\n\nThe above shows that the estimate is not very accurate even for maximum practical ranges of about 10. One can see why it does not match by looking at the computational analysis above and seeing that within these practical sieving range limits, there are very significant constant offset terms such that the very slowly growing term does not get large enough so as to make these terms insignificant until the sieving range approaches infinity – well beyond any practical sieving range. Within these practical ranges, these significant constant offsets mean that the performance of the Sieve of Eratosthenes is much better than one would expect just using the asymptotic time complexity estimates by a significant amount, but that also means that the slope of the performance with increasing range is steeper than predicted as the benefit of the constant offsets becomes slightly less significant.\n\nOne should also note that in using the calculated operation ratios to the sieve range, it must be less than about 0.2587 in order to be faster than the often compared sieve of Atkin if the operations take approximately the same time each in CPU clock cycles, which is a reasonable assumption for the one huge bit array algorithm. Using that assumption, the sieve of Atkin is only faster than the maximally wheel factorized sieve of Eratosthenes for ranges of over 10 at which point the huge sieve buffer array would need about a quarter of a terabyte (about 250 gigabytes) of RAM memory even if bit packing were used. An analysis of the page segmented versions will show that the assumption that the time per operation stays the same between the two algorithms does not hold for page segmentation and that the sieve of Atkin operations get slower much faster than the sieve of Eratosthenes with increasing range. Thus for practical purposes, the maximally wheel factorized Sieve of Eratosthenes is faster than the Sieve of Atkin although the Sieve of Atkin is faster for lesser amounts of wheel factorization.\n\nUsing big O notation is also not the correct way to compare practical performance of even variations of the Sieve of Eratosthenes as it ignores constant factors and offsets that may be very significant for practical ranges: The sieve of Eratosthenes variation known as the Pritchard wheel sieve has an performance, but its basic implementation requires either a \"one large array\" algorithm which limits its usable range to the amount of available memory else it needs to be page segmented to reduce memory use. When implemented with page segmentation in order to save memory, the basic algorithm still requires about bits of memory (much more than the requirement of the basic page segmented sieve of Eratosthenes using bits of memory). Pritchard's work reduced the memory requirement to the limit as described above the table, but the cost is a fairly large constant factor of about three in execution time to about three quarters the sieve range due to the complex computations required to do so. As can be seen from the above table for the basic sieve of Eratosthenes, even though the resulting wheel sieve has performance and an acceptable memory requirement, it will never be faster than a reasonably Wheel Factorized basic sieve of Eratosthenes for any practical sieving range by a factor of about two. Other than that it is quite complex to implement, it is rarely practically implemented because it still uses more memory than the basic Sieve of Eratosthenes implementations described here as well as being slower for practical ranges. It is thus more of an intellectual curiosity than something practical.\n\nEuler's proof of the zeta product formula contains a version of the sieve of Eratosthenes in which each composite number is eliminated exactly once. The same sieve was rediscovered and observed to take linear time by . It, too, starts with a list of numbers from 2 to in order. On each step the first element is identified as the next prime and the results of multiplying this prime with each element of the list are marked in the list for subsequent deletion. The initial element and the marked elements are then removed from the working sequence, and the process is repeated:\nHere the example is shown starting from odds, after the first step of the algorithm. Thus, on the th step all the remaining multiples of the th prime are removed from the list, which will thereafter contain only numbers coprime with the first primes (cf. wheel factorization), so that the list will start with the next prime, and all the numbers in it below the square of its first element will be prime too.\n\nThus, when generating a bounded sequence of primes, when the next identified prime exceeds the square root of the upper limit, all the remaining numbers in the list are prime. In the example given above that is achieved on identifying 11 as next prime, giving a list of all primes less than or equal to 80.\n\nNote that numbers that will be discarded by a step are still used while marking the multiples in that step, e.g., for the multiples of 3 it is , , , , ..., , ..., so care must be taken dealing with this.\n\n\n"}
{"id": "839968", "url": "https://en.wikipedia.org/wiki?curid=839968", "title": "Six-hour clock", "text": "Six-hour clock\n\nThe six-hour clock is a traditional timekeeping system used in the Thai and formerly the Lao language and the Khmer language, alongside the official 24-hour clock. Like other common systems, it counts twenty-four hours in a day, but divides the day into four quarters, counting six hours in each. The hours in each quarter (with the exception of the sixth hour in each quarter) are told with period-designating words or phrases, which are:\n\n\nThese terms are thought to have originated from the sounds of traditional timekeeping devices. The gong was used to announce the hours in daytime, and the drum at night. Hence the terms \"mong\", an onomatopoeia of the sound of the gong, and \"thum\", that of the sound of the drum. \"Ti\" is a verb meaning \"to hit or strike\", and is presumed to have originated from the act of striking the timekeeping device itself. \"Chao\" and \"bai\" translate as \"morning\" and \"afternoon\" respectively, and help to differentiate the two daytime quarters.\n\nThe sixth hours of each quarter are told by a different set of terms. The sixth hour at dawn is called \"yam rung\" (, ), and the sixth hour at dusk is called \"yam kham\" (, ), both references to the act of striking the gong or drum in succession to announce the turning of day (\"yam\"), where \"rung\" and \"kham\", meaning \"dawn\" and \"dusk\", denote the time of these occurrences. The midday and midnight hours are respectively known as \"thiang\" (, , or \"thiang wan\", , ) and \"thiang khuen\" (, ), both of which literally translate as \"midday\" and \"midnight\".\n\nMidnight is also called \"song yam\" (, ; note that \"yam\" is a different word), a reference to the end of the second three-hour period of the night watch (\"song\" translates as the number \"two\"). In addition, \"hok (6) thum\" and \"ti hok\" may also be used to refer to the hours of midnight and dawn, following general usage for the other hours, although more rarely; and the fourth to sixth hours of the second daytime half may also be told as ...\"mong yen\" (, ), \"yen\" meaning \"evening\".\n\nThe system has been used in some form since the days of the Ayutthaya Kingdom, but was codified similarly to its present form only in 1901 by King Chulalongkorn in \"Royal Gazette\" 17:206. Nowadays, it is used only in colloquial speech. However, a corrupted form of the six-hour clock is more frequently encountered, where usually the first half of daytime (including the sixth hour of the preceding quarter) is counted as in the twelve-hour clock, i.e. \"hok (6) mong chao\", \"chet (7) mong\", etc., up to \"sip et (11) mong\".\n\nThe six-hour clock system was abolished in Laos and Cambodia during the French protectorate, and the French 24-hour clock system (for example, 3h00) has been used.\n\nA comparison of the systems is as follows:\n\n"}
{"id": "47751845", "url": "https://en.wikipedia.org/wiki?curid=47751845", "title": "Straw feminism", "text": "Straw feminism\n\nStraw feminism is a straw man argument whereby exaggerated or fabricated elements of feminism are used in an attempt to refute and/or derail feminist arguments. A straw feminist then is a fabricated character used by those arguing against feminism to devalue and derail feminist arguments. Straw feminism often incorporates oversimplifications, misrepresentations and stereotypes of feminism in order to easily discredit feminism as a whole. \n\n"}
{"id": "7096466", "url": "https://en.wikipedia.org/wiki?curid=7096466", "title": "Structure tensor", "text": "Structure tensor\n\nIn mathematics, the structure tensor, also referred to as the second-moment matrix, is a matrix derived from the gradient of a function. It summarizes the predominant directions of the gradient in a specified neighborhood of a point, and the degree to which those directions are coherent. The structure tensor is often used in image processing and computer vision.\n\nFor a function formula_1 of two variables , the structure tensor is the 2×2 matrix\n\nwhere formula_3 and formula_4 are the partial derivatives of formula_1 with respect to \"x\" and \"y\"; the integrals range over the plane formula_6; and \"w\" is some fixed \"window function\", a distribution on two variables. Note that the matrix formula_7 is itself a function of .\n\nThe formula above can be written also as formula_8, where formula_9 is the matrix-valued function defined by\n\nIf the gradient formula_11 of formula_1 is viewed as a 2×1 (single-column) matrix, where formula_13 denotes transpose operation, turning a row vector to a column vector, the matrix formula_9 can be written as the matrix product formula_15, also known as an outer product, or tensor product. Note however that the structure tensor formula_16 cannot be factored in this way in general except if formula_17 is a Dirac delta function. \n\nIn image processing and other similar applications, the function formula_1 is usually given as a discrete array of samples formula_19, where \"p\" is a pair of integer indices. The 2D structure tensor at a given pixel is usually taken to be the discrete sum\n\nHere the summation index \"r\" ranges over a finite set of index pairs (the \"window\", typically formula_21 for some \"m\"), and \"w\"[\"r\"] is a fixed \"window weight\" that depends on \"r\", such that the sum of all weights is 1. The values formula_22 are the partial derivatives sampled at pixel \"p\"; which, for instance, may be estimated from by formula_1 by finite difference formulas.\n\nThe formula of the structure tensor can be written also as formula_24, where formula_9 is the matrix-valued array such that\n\nThe importance of the 2D structure tensor formula_7 stems from the fact eigenvalues formula_28 (which can be ordered so that formula_29) and the corresponding eigenvectors formula_30 summarize the distribution of the gradient formula_31 of formula_1 within the window defined by formula_17 centered at formula_34.\n\nNamely, if formula_35, then formula_36 (or formula_37) is the direction that is maximally aligned with the gradient within the window. \n\nIn particular, if formula_38 then the gradient is always a multiple of formula_36 (positive, negative or zero); this is the case if and only if formula_1 within the window varies along the direction formula_36 but is constant along formula_42. This condition of eigenvalues is also called linear symmetry condition because then the iso-curves of formula_1 consist in parallel lines, i.e there exists a one dimensional function function formula_44 which can generate the two dimensional function formula_1 as formula_46 for some constant vector formula_47 and the coordinates formula_48.\n\nIf formula_49, on the other hand, the gradient in the window has no predominant direction; which happens, for instance, when the image has rotational symmetry within that window. This condition of eigenvalues is also called balanced body, or directional equilibrium condition because it holds when all gradient directions in the window are equally frequent/probable. \n\nFurthermore, the condition formula_50 happens if and only if the function formula_1 is constant (formula_52) within formula_53.\n\nMore generally, the value of formula_54, for \"k\"=1 or \"k\"=2, is the formula_17-weighted average, in the neighborhood of \"p\", of the square of the directional derivative of formula_1 along formula_57. The relative discrepancy between the two eigenvalues of formula_7 is an indicator of the degree of anisotropy of the gradient in the window, namely how strongly is it biased towards a particular direction (and its opposite). This attribute can be quantified by the coherence, defined as\n\nif formula_60. This quantity is 1 when the gradient is totally aligned, and 0 when it has no preferred direction. The formula is undefined, even in the limit, when the image is constant in the window (formula_61). Some authors define it as 0 in that case.\n\nNote that the average of the gradient formula_62 inside the window is not a good indicator of anisotropy. Aligned but oppositely oriented gradient vectors would cancel out in this average, whereas in the structure tensor they are properly added together. This is a reason for why formula_15 is used in the averaging of the structure tensor to optimize the direction instead of formula_62.\n\nBy expanding the effective radius of the window function formula_17 (that is, increasing its variance), one can make the structure tensor more robust in the face of noise, at the cost of diminished spatial resolution. The formal basis for this property is described in more detail below, where it is shown that a multi-scale formulation of the structure tensor, referred to as the multi-scale structure tensor, constitutes a \"true multi-scale representation of directional data under variations of the spatial extent of the window function\".\n\nThe interpretation and implementation of the 2D structure tensor becomes particularly accessible using complex numbers. The structure tensor consists in 3 real numbers\n\nwhere formula_67 , formula_68 and formula_69 in which integrals can be replaced by summations for discrete representation. Using Parseval relationship it is clear that the three real numbers are the second order moments of the power spectrum of formula_1. The following second order complex moment of the power spectrum of formula_1 can then be written as \n\nformula_72\n\nwhere formula_73 and formula_74 is the direction angle of the most significant eigenvector of the structure tensor formula_75 whereas formula_76 and formula_77 are the most and the least significant eigenvalues. From, this it follows that formula_78 contains both a certainty formula_79 and the optimal direction in double angle representation since it is a complex number consisting of two real numbers. It follows also that if the gradient is represented as a complex number, and is remapped by squaring (i.e. the argument angle of the complex gradient is doubled), then averaging acts as an optimizer in the mapped domain, since it directly delivers both the optimal direction (in double angle representation) and the associated certainty. The complex number represents thus how much linear structure (linear symmetry) there is in image formula_1, and the complex number is obtained directly by averaging the gradient in its (complex) double angle representation without computing the eigenvalues and the eigenvectors explicitly.\n\nLikewise the following second order complex moment of the power spectrum of formula_1, which happens to be always real because formula_1 is real,\n\nformula_83\n\ncan be obtained, with formula_76 and formula_77 being the eigenvalues as before. Notice that this time the magnitude of the complex gradient is squared (which is always real). \n\nHowever, decomposing the structure tensor in its eigenvectors yields its tensor components as\n\nwhere formula_87 is the identity matrix in 2D because the two eigenvectors are always orthogonal (and sum to unity). The first term in the last expression of the decomposition, formula_88, represents the linear symmetry component of the structure tensor containing all directional information (as a rank-1 matrix), whereas the second term represents the balanced body component of the tensor, which lacks any directional information (containing an identity matrix formula_87). To know how much directional information there is in formula_1 is then the same as checking how large formula_91is compared to formula_77. \n\nEvidently, formula_93 is the complex equivalent of the first term in the tensor decomposition, whereas formula_94is the equivalent of the second term. Thus the two scalars, comprising three real numbers, \n\nwhere formula_96 is the (complex) gradient filter, and formula_97 is convolution, constitute a complex representation of the 2D Structure Tensor. As discussed here and elsewhere formula_17 defines the local image which is usually a Gaussian (with a certain variance defining the outer scale), and formula_99 is the (inner scale) parameter determining the effective frequency range in which the orientation formula_100 is to be estimated.\n\nThe elegance of the complex representation stems from that the two components of the structure tensor can be obtained as averages and independently. In turn, this means that formula_93 and formula_102 can be used in a scale space representation to describe the evidence for presence of unique orientation and the evidence for the alternative hypothesis, the presence of multiple balanced orientations, without computing the eigenvectors and eigenvalues. A functional, such as squaring the complex numbers have to this date not been shown to exist for structure tensors with dimensions higher than two. In Bigun 91, it has been put forward with due argument that this is because complex numbers are commutative algebras whereas quaternions, the possible candidate to construct such a functional by, constitute a non-commutative algebra.\n\nThe complex representation of the structure tensor is frequently used in fingerprint analysis to obtain direction maps containing certainties which in turn are used to enhance them, to find the locations of the global (cores and deltas) and local (minutia) singularities, as well as automatically evaluate the quality of the fingerprints.\n\nThe structure tensor can be defined also for a function formula_1 of three variables \"p\"=(\"x\",\"y\",\"z\") in an entirely analogous way. Namely, in the continuous version we have formula_104, where\nwhere formula_106 are the three partial derivatives of formula_1, and the integral ranges over formula_108.\n\nIn the discrete version,formula_24, where \nand the sum ranges over a finite set of 3D indices, usually formula_111 for some \"m\".\n\nAs in the three-dimensional case, the eigenvalues formula_112 of formula_113, and the corresponding eigenvectors formula_114, summarize the distribution of gradient directions within the neighborhood of \"p\" defined by the window formula_17. This information can be visualized as an ellipsoid whose semi-axes are equal to the eigenvalues and directed along their corresponding eigenvectors.\n\nIn particular, if the ellipsoid is stretched along one axis only, like a cigar (that is, if formula_76 is much larger than both formula_77 and formula_118), it means that the gradient in the window is predominantly aligned with the direction formula_36, so that the isosurfaces of formula_1 tend to be flat and perpendicular to that vector. This situation occurs, for instance, when \"p\" lies on a thin plate-like feature, or on the smooth boundary between two regions with contrasting values.\n\nIf the ellipsoid is flattened in one direction only, like a pancake (that is, if formula_118 is much smaller than both formula_76 and formula_77), it means that the gradient directions are spread out but perpendicular to formula_124; so that the isosurfaces tend to be like tubes parallel to that vector. This situation occurs, for instance, when \"p\" lies on a thin line-like feature, or on a sharp corner of the boundary between two regions with contrasting values.\n\nFinally, if the ellipsoid is roughly spherical (that is, if formula_125), it means that the gradient directions in the window are more or less evenly distributed, with no marked preference; so that the function formula_1 is mostly isotropic in that neighborhood. This happens, for instance, when the function has spherical symmetry in the neighborhood of \"p\". In particular, if the ellipsoid degenerates to a point (that is, if the three eigenvalues are zero), it means that formula_1 is constant (has zero gradient) within the window.\n\nThe structure tensor is an important tool in scale space analysis. The multi-scale structure tensor (or multi-scale second moment matrix) of a function formula_1 is in contrast to other one-parameter scale-space features an image descriptor that is defined over \"two\" scale parameters.\nOne scale parameter, referred to as \"local scale\" formula_129, is needed for determining the amount of pre-smoothing when computing the image gradient formula_130. Another scale parameter, referred to as \"integration scale\" formula_131, is needed for specifying the spatial extent of the window function formula_132 that determines the weights for the region in space over which the components of the outer product of the gradient by itself formula_15 are accumulated.\n\nMore precisely, suppose that formula_1 is a real-valued signal defined over formula_135. For any local scale formula_136, let a multi-scale representation formula_137 of this signal be given by formula_138 where formula_139 represents a pre-smoothing kernel. Furthermore, let formula_130 denote the gradient of the scale space representation.\nThen, the \"multi-scale structure tensor/second-moment matrix\" is defined by\n\nConceptually, one may ask if it would be sufficient to use any self-similar families of smoothing functions formula_139 and formula_132. If one naively would apply, for example, a box filter, however, then non-desirable artifacts could easily occur. If one wants the multi-scale structure tensor to be well-behaved over both increasing local scales formula_129 and increasing integration scales formula_131, then it can be shown that both the smoothing function and the window function \"have to\" be Gaussian. The conditions that specify this uniqueness are similar to the scale-space axioms that are used for deriving the uniqueness of the Gaussian kernel for a regular Gaussian scale space of image intensities.\n\nThere are different ways of handling the two-parameter scale variations in this family of image descriptors. If we keep the local scale parameter formula_129 fixed and apply increasingly broadened versions of the window function by increasing the integration scale parameter formula_131 only, then we obtain a \"true formal scale space representation of the directional data computed at the given local scale\" formula_129. If we couple the local scale and integration scale by a \"relative integration scale\" formula_149, such that formula_150 then for any fixed value of formula_151, we obtain a reduced self-similar one-parameter variation, which is frequently used to simplify computational algorithms, for example in corner detection, interest point detection, texture analysis and image matching.\nBy varying the relative integration scale formula_149 in such a self-similar scale variation, we obtain another alternative way of parameterizing the multi-scale nature of directional data obtained by increasing the integration scale.\n\nA conceptually similar construction can be performed for discrete signals, with the convolution integral replaced by a convolution sum and with the continuous Gaussian kernel formula_153 replaced by the discrete Gaussian kernel formula_154:\nWhen quantizing the scale parameters formula_129 and formula_131 in an actual implementation, a finite geometric progression formula_158 is usually used, with \"i\" ranging from 0 to some maximum scale index \"m\". Thus, the discrete scale levels will bear certain similarities to image pyramid, although spatial subsampling may not necessarily be used in order to preserve more accurate data for subsequent processing stages.\n\nThe eigenvalues of the structure tensor play a significant role in many image processing algorithms, for problems like corner detection, interest point detection, and feature tracking. The structure tensor also plays a central role in the Lucas-Kanade optical flow algorithm, and in its extensions to estimate affine shape adaptation; where the magnitude of formula_77 is an indicator of the reliability of the computed result. The tensor has been used for scale space analysis, estimation of local surface orientation from monocular or binocular cues, non-linear fingerprint enhancement, diffusion-based image processing, and several other image processing problems. The structure tensor can be also applied in geology to filter seismic data.\n\nThe three-dimensional structure tensor has been used to analyze three-dimensional video data (viewed as a function of \"x\", \"y\", and time \"t\").\nIf one in this context aims at image descriptors that are \"invariant\" under Galilean transformations, to make it possible to compare image measurements that have been obtained under variations of a priori unknown image velocities formula_160\nit is, however, from a computational viewpoint more preferable to parameterize the components in the structure tensor/second-moment matrix formula_162 using the notion of \"Galilean diagonalization\"\nwhere formula_164 denotes a Galilean transformation of spacetime and formula_165 a two-dimensional rotation over the spatial domain,\ncompared to the abovementioned use of eigenvalues of a 3-D structure tensor, which corresponds to an eigenvalue decomposition and a (non-physical) three-dimensional rotation of spacetime\nTo obtain true Galilean invariance, however, also the shape of the spatio-temporal window function needs to be adapted, corresponding to the transfer of affine shape adaptation from spatial to spatio-temporal image data.\nIn combination with local spatio-temporal histogram descriptors,\nthese concepts together allow for Galilean invariant recognition of spatio-temporal events.\n\n\n"}
{"id": "33471233", "url": "https://en.wikipedia.org/wiki?curid=33471233", "title": "Subject reduction", "text": "Subject reduction\n\nIn type theory, a type system has the property of subject reduction (also subject evaluation, type preservation or simply preservation) if evaluation of expressions does not cause their type to change. Formally, if Γ ⊢ \"e\" : \"τ\" and \"e\" → \"e\" then Γ ⊢ \"e\" : \"τ\".\n\nTogether with progress, it is an important meta-theoretical property for establishing type soundness of a type system.\n\nThe opposite property, if Γ ⊢ \"e\" : \"τ\" and \"e\" → \"e\" then Γ ⊢ \"e\" : \"τ\", is called subject expansion. It often does not hold as evaluation can erase ill-typed sub-terms of an expression, resulting in a well-typed one.\n\n"}
{"id": "41287858", "url": "https://en.wikipedia.org/wiki?curid=41287858", "title": "The Real Estate Show", "text": "The Real Estate Show\n\nThe Real Estate Show was a Colab sponsored illegal occupation exhibition on the subject of landlord speculation in real estate held on New Year's Eve in January 1980 in a vacant city-owned building at 123 Delancey Street in the Lower East Side of Manhattan. The squatter action followed a year of long and frustrating campaigning to rent the property for an exhibition space from officials of the NYC Department of Housing Preservation and Development (HPD).\n\nOn New Year's Day the show was officially opened to the public. It was to be a two-week occupation/exhibit but was closed down by the police. On the morning of January 2, the Colab artists discovered the storefront padlocked from the inside, their work locked within. Phone calls revealed it to be the doing of HPD. \"The Real Estate Show\" had been open exactly one day.\n\nOn January 8, the artists, accompanied by art dealer Ronald Feldman and German conceptual artist Joseph Beuys assembled at the site to protest its closing in the company of reporters from the New York Times, Soho News, and the East Village Eye. There was a photograph taken of Beuys at the front door of \"The Real Estate Show\" standing with Feldman, Alan W. Moore, Joseph Nechvatal, Cid Collins and others taken that day.\n\nOn January 11 city workers swept into 123 Delancey, cleared out the exhibited work and trucked it to an uptown warehouse. It was not until a few days later that artists were granted entry into the warehouse to take their art back home.\n\nOn January 16 a deal was reached with the city that gave birth to ABC No Rio when, as a compromise, a city agency gave the artists control of nearby 156 Rivington Street.\n\nIn early 2014, there were four concurrent art exhibitions in New York City around \"The Real Estate Show\": at James Fuentes Gallery, ABC No Rio, the Lodge Gallery, and Cuchifritos Gallery/Essex Street Market.\n\nIn June 2017, Becky Howland & Matthias Mayer curated \"The Real Estate Show\" at Spor Klubu in Berlin, drawing from documentation of the original \"Real Estate Show\" (1980) from the Archive Collection of the extant project space ABC No Rio. Included in the show were Robert Cooney, Mitch Corber, Peter Fend, Coleen Fitzgibbon, Bobby G (Robert Goldman), Ilona Granet, Becky Howland, Christof Kohlhofer, Gregory Lehmann, Ann Messner, Peter Mönnig, Alan Moore, Joseph Nechvatal, Cara Perlman, Scott Pfaffman, Christy Rupp and Robin Winters. In conjunction with this show, another exhibition called \"The Real Estate Show Extended/Berlin: Group exhibition on the subject of Gentrification, Real Estate Speculation and Selling out the City\" was presented at Kunstpunkt Berlin. This show included many Berlin artists along with four original members of the \"Real Estate Show\" (1980): Becky Howland, Peter Mönnig, Alan Moore, and Joseph Nechvatal. Howland, Mönnig, Moore and Nechvatal also participated in a panel discussion on Real Estate and Art on June 3rd, 2017 that was moderated by Howard McCalebb of Dada Post, Berlin. \n\n\n"}
{"id": "51331209", "url": "https://en.wikipedia.org/wiki?curid=51331209", "title": "Traffic model", "text": "Traffic model\n\nA traffic model is a mathematical model of real-world traffic, usually, but not restricted to, road traffic. Traffic modeling draws heavily on theoretical foundations like network theory and certain theories from physics like the kinematic wave model. The interesting quantity being modeled and measured is the traffic flow, i.e. the throughput of mobile units (e.g. vehicles) per time and transportation medium capacity (e.g. road or lane width). Models can teach researchers and engineers how to ensure an optimal flow with a minimum number of traffic jams.\n\nTraffic models often are the basis of a traffic simulation.\n\n\n\n\n"}
{"id": "8158976", "url": "https://en.wikipedia.org/wiki?curid=8158976", "title": "Yan Huang Zisun", "text": "Yan Huang Zisun\n\nYan Huang Zisun () is a term that represents the Chinese people and refers to an ethnocultural identity based on a common ancestry associated with a mythological origin. It is noted that the term specifically refers to the Han ethnic group. \n\nThis term is connected to Yandi (炎帝) and Huangdi (黃帝), in which both figures are considered the legendary ancestors of the Huaxia people who themselves are ancestral to the Han people.\n\nTo this day, the Chinese still refer to themselves with this term.\n\nThe derivation of the term is mentioned as \"Yan Huang Shizhou\" (炎黃世胄) in the National Flag Anthem of the Republic of China.\n\nMa Ying-jeou, President of the Republic of China, has used this term to refer to all Chinese people.\n\n"}
