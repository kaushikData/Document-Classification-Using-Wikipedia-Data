{"id": "31460418", "url": "https://en.wikipedia.org/wiki?curid=31460418", "title": "AIMMS", "text": "AIMMS\n\nAIMMS is a prescriptive analytics software company with offices in the Netherlands, United States, China and Singapore. AIMMS has two main product offerings that provide modeling and optimization capabilities across a variety of industries. The AIMMS Prescriptive Analytics Platform is a tool for those with an Operations Research or Analytics background. It offers unlimited flexibility to develop optimization-based applications and deploy them to business users. AIMMS SC Navigator, launched in 2017, is built on the AIMMS Prescriptive Analytics Platform and provides configurable Apps for supply chain teams. SC Navigator provides supply chain analytics to individuals without a technical or analytics background so they can get the same benefits from sophisticated analytics without needing to code or model.\n\nAIMMS B.V. was founded in 1989 by noted mathematician Johannes Bisschop under the name of Paragon Decision Technology. His vision was to make optimization more approachable by building models rather than programming. In Bisschop’s view, modeling was able to build the bridge between the people who had problems and the people helping them solve those problems. \n\nAIMMS (an acronym for \"Advanced Interactive Multidimensional Modeling System\") began as a software system designed for modeling and solving large-scale optimization and scheduling-type problems. \n\nAIMMS is considered to be one of the five most important algebraic modeling languages and the creator (Johannes J. Bisschop) has been awarded with INFORMS Impact Prize for his work in this language.\n\nIn 2003, AIMMS was acquired by a small private equity firm who saw the potential value of mathematics to business. This led to the creation of a partnership program, further technical investment and the evolution of the platform. In 2011, the company launched AIMMS PRO, a way to deploy applications to end-users who don't have a technical background. This was quickly followed by the ability to publish and customize applications using a browser so that decision support applications are available on any device. These innovations led to rapid customer adoption and growth for the company. In 2017, AIMMS was recognized as a top B2B technology in the Netherlands. and was named one of the fastest growing companies in the Netherlands for the second consecutive year.\n\nIn 2017, the product management team did a listening tour with supply chain executives. This, along with the growing interest in embedded advanced analytics for supply chain management, generated the initial idea for the AIMMS SC Navigator Platform and the democratization of supply chain analytics. SC Navigator consists of ready-made applications that are easily configurable to put supply chain analytics in the hands of supply chain professionals. \n\nAIMMS SC Navigator launched in October, 2017 with three initial cloud-based Apps: Supply Chain Network Design, Sales & Operations Planning and Data Navigator. In 2018 two additional applications were rolled out - Center of Gravity and Product Lifecycle. Additional applications are rolling out every quarter.\n\nThe AIMMS Prescriptive Analytics Platform consists of an algebraic modeling language, an integrated development environment for both editing models and creating a graphical user interface around these models, and a graphical end-user environment.\nAIMMS is linked to multiple solvers through the AIMMS Open Solver Interface.\nSupported solvers include CPLEX, MOSEK, FICO Xpress, CBC, Conopt, MINOS, IPOPT, SNOPT, KNITRO and CP Optimizer.\n\nAIMMS features a mixture of declarative and imperative programming styles. Formulation of optimization models takes place through declarative language elements such as sets and indices, as well as scalar and multidimensional parameters, variables and constraints, which are common to all algebraic modeling languages, and allow for a concise description of most problems in the domain of mathematical optimization. Units of measurement are natively supported in the language, and compile- and runtime unit analysis may be employed to detect modeling errors.\n\nProcedures and control flow statements are available in AIMMS for \nTo support the re-use of common modeling components, AIMMS allows modelers to organize their model in user model libraries.\n\nAIMMS supports a wide range of mathematical optimization problem types:\nUncertainty can be taken into account in deterministic linear and mixed integer optimization models in AIMMS through the specification of additional attributes, such that stochastic or robust optimization techniques can be applied alongside the existing deterministic solution techniques.\n\nCustom hybrid and decomposition algorithms can be constructed using the GMP system library which makes available at the modeling level many of the basic building blocks used internally by the higher level solution methods present in AIMMS, matrix modification methods, as well as specialized steps for customizing solution algorithms for specific problem types.\n\nOptimization solutions created with AIMMS can be used either as a standalone desktop application or can be embedded as a software component in other applications.\n\nAIMMS Prescriptive Analytics Platform is used in a wide range of industries including retail, consumer products, healthcare, oil and chemicals, steel production and agribusiness.\n\nGE Grid uses AIMMS as the modeling and optimization engine of its energy market clearing software.\nTogether with GE Grid, AIMMS was part of the analytics team of Midwest ISO that won the Franz Edelman Award for Achievement in Operations Research and the Management Sciences of 2011 for successfully applying operations research in the Midwest ISO energy market. In 2012, TNT Express, an AIMMS customer won the Franz Edleman Award for modernizing its operations and reducing its carbon footprint. The AIMMS platform was also used by the Dutch Delta team to develop and implement a new method for calculating the most efficient levels of flood protection for the Netherlands and won the Edelman prize in 2013. \n\n\n"}
{"id": "13339494", "url": "https://en.wikipedia.org/wiki?curid=13339494", "title": "Appeal to advantage", "text": "Appeal to advantage\n\nAn appeal to advantage is a rhetorical device in which the speaker encourages his or her audience to perform some action by representing that action as being in the audience's best interest.\n\nAn appeal to advantage can also be a request from someone in a position of power to someone who is in a socially subordinate position; the request is specifically for the subordinate to perform an act contrary to the subordinate's wishes, such that the subordinate is forced to commit the act in order to satisfy a more significant need. The appeal is specifically most expedient or advantageous to the person in power, but is also presented as forwarding the subordinate's interests in some significant way.\n\n\n"}
{"id": "17699", "url": "https://en.wikipedia.org/wiki?curid=17699", "title": "Argument from ignorance", "text": "Argument from ignorance\n\nArgument from ignorance (from ), also known as appeal to ignorance (in which \"ignorance\" represents \"a lack of contrary evidence\") is a fallacy in informal logic. It asserts that a proposition is true because it has not yet been proven false or a proposition is false because it has not yet been proven true. This represents a type of false dichotomy in that it excludes a third option, which is that there may have been an insufficient investigation, and therefore there is insufficient information to prove the proposition be either true or false. Nor does it allow the admission that the choices may in fact not be two (true or false), but may be as many as four,\nIn debates, appeals to ignorance are sometimes used in an attempt to shift the burden of proof.\n\nAs described in Schreuder's \"Vision and Visual Perception\":\nTo reiterate, these arguments ignore the fact, and difficulty, that some true things may never be proved, and some false things may never be disproved with absolute certainty.\nThe phrase \"the absence of evidence is not the evidence of absence\" can be used as a shorthand rebuttal to the second form of the ignorance fallacy (i.e. \"P has never been absolutely proved and is therefore certainly false\"). Most often it is directed at any conclusion derived from \"null\" results in an experiment or from the non-detection of something. In other words, where one researcher may say their experiment suggests evidence of absence, another researcher might argue that the experiment failed to detect a phenomenon for other reasons.\n\nMuch confusion about arguments from ignorance can be caused when one side of a debate forgets that we often possess evidence of absence in practice. However, the confusion can be avoided by not partaking in a logical debate.\n\nThe ignorance fallacy is sometimes confused (or combined) with logically valid contrapositive arguments. Contrapositive arguments rightly utilize the transposition rule of inference in classical logic to conclude something like: To the extent that \"C implies E\" then \"Not-E must also imply Not-C\". In other words, if a cause \"always\" leads to an effect, then absence of the expected effect is evidence of absence of the cause. For example, if the causal proposition that \"If it's raining outside then the streets will be wet\" is assumed, then it can be assumed that \"if the streets are not wet then it is not raining outside\". The inference that it cannot be raining outside because the streets are not getting wet is exactly as true, or perhaps exactly as untrue, as the original proposition. The statements are logically equivalent.\n\nCarl Sagan explains in his book \"The Demon-Haunted World\":\nFor instance, absence of evidence that it rained (i.e. water is the evidence) may be considered positive evidence that it did not rain. Again, in science, such inferences are always made to some limited (sometimes extremely high) degree of probability and in this case absence of evidence is evidence of absence when the positive evidence should have been there but is not.\n\nArguments from ignorance can easily find their way into debates over the existence of God. It is a fallacy to draw conclusions based precisely on ignorance, since this does not satisfactorily address issues of philosophic burden of proof.\n\nContraposition is a logically valid rule of inference that allows the creation of a new proposition from the negation and reordering of an existing one. The method applies to any proposition of the type \"If A then B\" and says that negating all the variables and switching them back to front leads to a new proposition i.e. \"If Not-B then Not-A\" that is just as true as the original one and that the first implies the second and the second implies the first.\n\nTransposition is exactly the same thing as Contraposition, described in a different language.\n\n\"Absence of evidence,\" is the lack of – any kind of evidence – that may show, indicate, suggest, or be used to 1) infer, or 2) deduce the truthfulness of an asserted fact.\n\n\"Negative evidence\" is sometimes used as an alternative to \"absence of evidence\" and is often meant to be synonymous with it. On the other hand, the term may also refer to evidence with a \"negative\" value, or \"null\" result equivalent to \"evidence of absence\". It may even refer to positive evidence about something of an unpleasant nature.\n\nEvidence of absence is evidence of any kind that can be used to infer or deduce the non-existence or non-presence of something. For instance, if a doctor does not find any \"malignant cells\" in a patient this null result (finding nothing) is evidence of absence of cancer, even though the doctor has not actually detected anything per se. Such inductive reasoning is important to empiricism and science, but has well established limitations. The challenge thus becomes to try to identify when a researcher has received a null result (found nothing) because the thing does not exist (evidence of absence—objectively negative result), and when one simply lacks proper means of detection (absence of evidence—false negative).\n\"Null result\" is a term often used in science to indicate \"evidence of absence\". A search for water on the ground may yield a null result (the ground is dry); therefore, it probably did not rain.\n\nArguments from self-knowing take the form:\n\n\nIn practice these arguments are often fallacious and rely on the veracity of the supporting premise. For example, the argument that \"If I had just sat on a wild porcupine then I would know it; in fact I do not know it; therefore I did not just sit on a wild porcupine\" is probably not a fallacy and depends entirely on the veracity of the leading proposition that supports it. (See Contraposition and Transposition in the \"Related terms\" section in this article.)\n\n\"Absence of evidence\" is a condition in which no valid conclusion can be inferred from the \"mere\" absence of detection, normally due to doubt in the detection method. \"Evidence of absence\" is the successful variation: a conclusion that relies on specific knowledge in conjunction with negative detection to deduce the absence of something. An example of evidence of absence is checking your pockets for spare change and finding nothing, \"but\" being confident that the search would have found it if it was there.\n\nBy determining that a given experiment or method of detection is sensitive and reliable enough to detect the presence of X (when X is present) one can confidently exclude the possibility that X may be both undetected and present. This allows one to deduce that X cannot be present if a null result is received.\n\nThus there are only two possibilities, given a null result:\n\nTo the extent that option 2 can be eliminated, one can deduce that \"if X is not detected then X is not present\" and therefore the null result is evidence of absence.\n\n\n\n\n\nFrom \"Fallacies: classical and contemporary readings\" by Hans V. Hansen, Robert C. Pinto:\n\n\n\n"}
{"id": "30984301", "url": "https://en.wikipedia.org/wiki?curid=30984301", "title": "Autonoetic consciousness", "text": "Autonoetic consciousness\n\nAutonoetic consciousness is the human ability to mentally place ourselves in the past, in the future, or in counterfactual situations, and to thus be able to examine our own thoughts.\n\nOur sense of self affects our behavior, in the present, past and future. It relates to how we reflect on our own past behavior, how we feel about it, and this in turn determines if we do it again.\n\nIt is episodic memory that deals with self-awareness, memories of the self and inward thoughts that may be projected onto future actions of an individual. It was \"proposed by [Endel] Tulving for self-awareness, allowing the rememberer to reflect on the contents of episodic memory\". Moreover, autonoetic consciousness involves behaviors such as mental time travel, self-projection, and episodic future thinking, all of which have often been proposed as exclusively human capacities.\n\nAutonoetic consciousness is important in our formation of our \"self\" identity. What we have done in the past becomes a part of our \"self\" and the ability to reflect on this influences our behavior in the now.\n\nIn psychology, the self is often used for that set of attributes that a person attaches to himself or herself most firmly, the attributes that the person finds it difficult or impossible to imagine himself or herself without. Identity is also used to describe this. A person's gender is part of their identity but their profession, for example, may not be.\n\nIn philosophy, the self is the agent, the knower and the ultimate locus of personal identity. This self, the identity of which is at the bottom of every action, and involved in every bit of knowledge, is the self philosophers worry about. Nevertheless, care of the self is of utmost importance in the bios-logos relationship.\n\nA straightforward view of the self would be that the self is just the person, and that a person is a physical system. There are two problems with this view. First, the nature of freedom and consciousness has convinced many philosophers that there is a fundamentally non-physical aspect of persons. The second challenge stems from puzzling aspects of self-knowledge, as the knowledge we have of ourselves seems very unlike the knowledge we have of other objects in several ways.\n\nThe parietal cortex is strongly involved in autonoetic consciousness. Damage to areas of the parietal cortex can lead to different functioning errors, including changes in personality.\n\nThe parietal lobes can be divided into two functional regions. One involves sensation and perception, and the other is concerned with integrating sensory input, primarily with the visual system. The first function integrates sensory information to form a single perception. The second function constructs a spatial coordinate system to represent the world around us.\n\nIndividuals with damage to the parietal lobes often show striking deficits, such as abnormalities in body image and spatial relations.\n\nDamage to the left parietal lobe can result in what is called Gerstmann's syndrome which includes right-left confusion, difficulty with writing, and difficulty with mathematics. It can also produce disorders of language, and the inability to perceive objects normally.\n\nDamage to the right parietal lobe can result in neglecting part of the body or space, which can impair many self-care skills such as dressing and washing. Right side damage can also cause difficulty in making things, denial of deficits, and drawing ability.\n\nBi-lateral damage can cause Bálint's syndrome, a visual attention and motor syndrome. This is characterized by the inability to voluntarily control the gaze, inability to integrate components of a visual scene, and the inability to accurately reach for an object with visual guidance.\n\nLeft parietal-temporal lesions can affect verbal memory and the ability to recall strings of digits.\n\nThe right parietal-temporal lobe is concerned with non-verbal memory. Right parietal-temporal lesions can produce significant changes in personality.\n\nLesions in the right parietal lobe influence personality, and this could be because the parietal lobe has to do with our sense of self. Our sense of self is strongly reflected in our personality.\n\nSome common tests for parietal lobe function are: Kimura Box Test (apraxia) and the Two-Point Discrimination Test (somatosensory).\n\nDuring episodic retrieval, functional imaging studies consistently show differential activity in medial prefrontal and medial parietal cortices.\n\nWith positron emission tomography, it has been shown that the medial regions are functionally connected and interact with lateral regions that are activated according to the degree of self-reference.\n\nFor example, in one study, during retrieval of previous judgments of oneself, best friend, and the Danish Queen, activation increased in the left lateral temporal cortex and decreased in the right inferior parietal region with decreasing self-reference. The decrease in parietal cortex activation may then prove it is a nodal structure in self representation, functionally connected to both the right parietal and the medial prefrontal cortices. There was a decrease in the efficiency of retrieval of previous judgment of mental Self compared with retrieval of judgment of Other with transcranial magnetic stimulation at a latency of 160 ms, confirming the hypothesis that the medial parietal cortex in this network is essential for episodic memory retrieval with self-representation.\n\nThis network is strikingly similar to the network of the resting conscious state, suggesting that self-monitoring is a core function in resting consciousness.\n\nFor a coherent and meaningful life, conscious self-representation is mandatory. Autonoetic consciousness is thought to emerge by retrieval of memory of personally experienced events (episodic memory). Without the ability to reflect on our past experiences, we would be stuck in a state of constant awakening, without a past and therefore unable to prepare for the future.\n\nEpisodic memory is the memory we have for our past experiences, which influence our now, and our future. This is different from procedural memory, which is our memory for how to do things. Episodic memories influence our thinking about ourselves, good and bad.\n\nAutobiographical memories can be retrieved from either the first person perspective, in which individuals see the event through their own eyes, or from the third person perspective, in which individuals see themselves and the event from the perspective of an external observer.\n\nA growing body of research suggests that the visual perspective from which a memory is retrieved has important implications for a person's thoughts, feelings, and goals, and is integrally related to a host of self- evaluative processes.\n\nEvent-related potentials (ERPs) can measure autonoetic consciousness scientifically. Event-related brain potentials (ERPs) are a non-invasive method of measuring brain activity during cognitive processing. The transient electric potential shifts (so-called ERP components) are time-locked to the stimulus onset (e.g., the presentation of a word, a sound, or an image). Each component reflects brain activation associated with one or more mental operations.\n\nIn contrast to behavioral measures such as error rates and response times, ERPs are characterized by simultaneous multi-dimensional online measures of polarity (negative or positive potentials), amplitude, latency, and scalp distribution. Therefore, ERPs can be used to distinguish and identify psychological and neural sub-processes involved in complex cognitive, motor, or perceptual tasks.\n\nUnlike fMRI, they provide extremely high time resolution, in the range of one millisecond.\n\nThe methodological advantages of ERPs have resulted in an ever increasing number of ERP studies in cognitive neuroscience, cognitive psychology, psycholinguistics, neurolinguistics, neuropsychology, and neurology. ERPs have also been used to identify patients who seem to be \"brain-dead\" but in fact are not.\n\nThere is an event-related potential (ERP) experiment of human recognition memory that explored the relation between conscious awareness and electrophysiological activity of the brain. ERPs were recorded from healthy adults while they made \"remember\" and \"know\" recognition judgments about previously seen words, reflecting \"Autonoetic\" and \"Noetic\" awareness, respectively. The ERP effects differed between the two kinds of awareness while they were similar for \"true\" and \"false\" recognition.\n\nIn a study of real-time noninvasive recordings of the brain's electrical activity (event-related potentials, ERPs), there was a common neural \"signature\" that is associated with self-referential processing regardless of whether subjects are retrieving general knowledge (noetic awareness) or re-experiencing past episodes (autonoetic awareness).\n\nSocial anxiety disorder (SAD) is an example of how bad experiences can also lead to our behaviors. It demonstrates how our thoughts influence our feelings about ourselves and therefore our actions in society around us. It has to do with a person’s self-esteem, fear of failure, shame, fear of offending, and fear of strangers.\n\nCognitive models of social anxiety disorder believe the social self is a key psychological mechanism that maintains fear of negative evaluation in social and performance situations. Consequently, a distorted self-view is evident when recalling painful autobiographical social memories, as reflected in linguistic expression, negative self-beliefs, and emotion and avoidance.\n\nTo test this hypothesis, 42 adults diagnosed with SAD and 27 non-psychiatric healthy controls composed autobiographical narratives of distinct social anxiety related situations, generated negative self-beliefs, and provided emotion and avoidance ratings.\n\nAlthough narratives were matched for initial emotional intensity and present vividness, linguistic analyses demonstrated that, compared to the control group, the SAD group employed more self-referential, anxiety, and sensory words, and made fewer references to other people. Social anxiety symptom severity, however, was associated with greater self-referential NSB in SAD only.\n\nSAD reported greater current self-conscious emotions when recalling autobiographical social situations, and greater active avoidance of similar situations than did the control group. Autobiographical memory of social situations in SAD may influence current and future thinking, emotion, and behavioral avoidance.\n\n"}
{"id": "1462677", "url": "https://en.wikipedia.org/wiki?curid=1462677", "title": "Bertrand competition", "text": "Bertrand competition\n\nBertrand competition is a model of competition used in economics, named after Joseph Louis François Bertrand (1822–1900). It describes interactions among firms (sellers) that set prices and their customers (buyers) that choose quantities at the prices set. The model was formulated in 1883 by Bertrand in a review of Antoine Augustin Cournot's book \"Recherches sur les Principes Mathématiques de la Théorie des Richesses\" (1838) in which Cournot had put forward the Cournot model. Cournot argued that when firms choose quantities, the equilibrium outcome involves firms pricing above marginal cost and hence the competitive price. In his review, Bertrand argued that if firms chose prices rather than quantities, then the competitive outcome would occur with price equal to marginal cost. The model was not formalized by Bertrand: however, the idea was developed into a mathematical model by Francis Ysidro Edgeworth in 1889.\n\nThe model rests on very specific assumptions. There are at least two firms producing a homogeneous (undifferentiated) product and cannot cooperate in any way. Firms compete by setting prices simultaneously and consumers want to buy everything from a firm with a lower price (since the product is homogeneous and there are no consumer search costs). If two firms charge the same price, consumers' demand is split evenly between them. It is simplest to concentrate on the case of duopoly where there are just two firms, although the results hold for any number of firms greater than 1.\n\nA crucial assumption about the technology is that both firms have the same constant unit cost of production, so that marginal and average costs are the same and equal to the competitive price. This means that as long as the price it sets is above unit cost, the firm is willing to supply any amount that is demanded (it earns profit on each unit sold). If price is equal to unit cost, then it is indifferent to how much it sells, since it earns no profit. Obviously, the firm will never want to set a price below unit cost, but if it did it would not want to sell anything since it would lose money on each unit sold.\n\nWhy is the competitive price a Nash equilibrium in the Bertrand model? First, if both firms set the competitive price with price equal to marginal cost (unit cost), neither firm will earn any profits. However, if one firm sets price equal to marginal cost, then if the other firm raises its price above unit cost, then it will earn nothing, since all consumers will buy from the firm still setting the competitive price (recall that it is willing to meet unlimited demand at price equals unit cost even though it earns no profit). No other price is an equilibrium. If both firms set the same price above unit cost and share the market, then each firm has an incentive to undercut the other by an arbitrarily small amount and capture the whole market and almost double its profits. So there can be no equilibrium with both firms setting the same price above marginal cost. Also, there can be no equilibrium with firms setting different prices. The firms setting the higher price will earn nothing (the lower priced firm serves all of the customers). Hence the higher priced firm will want to lower its price to undercut the lower-priced firm. Hence the \"only\" equilibrium in the Bertrand model occurs when both firms set price equal to unit cost (the competitive price).\n\nNote that the Bertrand equilibrium is a \"weak\" Nash-equilibrium. The firms lose nothing by deviating from the competitive price: it is an equilibrium simply because each firm can earn no more than zero profits given that the other firm sets the competitive price and is willing to meet all demand at that price.\n\n\nFirm 1's optimum price depends on where it believes firm 2 will set its prices. Pricing just below the other firm will obtain full market demand (D), though this is not optimal if the other firm is pricing below marginal cost as that would entail negative profits. In general terms, firm 1's best response function is p’’(p), this gives firm 1 optimal price for each price set by firm 2.\n\nDiagram 1 shows firm 1’s reaction function p’’(p), with each firm's strategy on each axis. It shows that when P is less than marginal cost (firm 2 pricing below MC) firm 1 prices at marginal cost, p=MC. When firm 2 prices above MC but below monopoly prices, then firm 1 prices just below firm 2. When firm 2 prices above monopoly prices (P) firm 1 prices at monopoly level, p=p.\n\nBecause firm 2 has the same marginal cost as firm 1, its reaction function is symmetrical with respect to the 45 degree line. Diagram 2 shows both reaction functions.\n\nThe result of the firms' strategies is a Nash equilibrium, that is, a pair of strategies (prices in this case) where neither firm can increase profits by unilaterally changing price. This is given by the intersection of the reaction curves, Point N on the diagram. At this point p=p’’(p), and p=p’’(p). As you can see, point N on the diagram is where both firms are pricing at marginal cost.\n\nAnother way of thinking about it, a simpler way, is to imagine if both firms set equal prices above marginal cost, firms would get half the market at a higher than MC price. However, by lowering prices just slightly, a firm could gain the whole market, so both firms are tempted to lower prices as much as they can. It would be irrational to price below marginal cost, because the firm would make a loss. Therefore, both firms will lower prices until they reach the MC limit.\n\nIf one firm has lower average cost (a superior production technology), it will charge the highest price that is lower than the average cost of the other one (i.e. a price \"just\" below the lowest price the other firm can manage) and take all the business. This is known as \"limit pricing\"\n\nThe Bertrand model rests on some very extreme assumptions. For example, it assumes that consumers want to buy from the lowest priced firm. There are various reasons why this may not hold in many markets: non-price competition and product differentiation, transport and search costs. For example, would someone travel twice as far to save 1% on the price of their vegetables? The Bertrand model can be extended to include product or location differentiation but then the main result – that price is driven down to marginal cost – no longer holds. With search costs, there may be other equilibria apart from the competitive price – the monopoly price or even price dispersion may be equilibria as in the classic \"Bargains and Rip-offs\" model.\n\nThe model also ignores capacity constraints. If a single firm does not have the capacity to supply the whole market then the \"price equals marginal cost\" result may not hold. The analysis of this case was started by Francis Ysidro Edgeworth and has become known as the Bertrand–Edgeworth model. With capacity constraints, there may not exist any pure strategy Nash equilibrium, the so-called Edgeworth paradox. However, in general there will exist a mixed-strategy Nash equilibrium as shown by Huw Dixon\n\nThere is a big incentive to cooperate in the Bertrand model: colluding to charge the monopoly price and sharing the market each is the best that the firms could do in this set up. However not colluding and charging marginal cost, is the non-cooperative outcome and the only Nash equilibrium of this model. If we move from a one-shot game to a repeated game, then perhaps collusion can persist for some time or emerge.\n\nNeither model is necessarily \"better\" than the other. The accuracy of the predictions of each model will vary from industry to industry, depending on the closeness of each model to the industry situation. If capacity and output can be easily changed, Bertrand is generally a better model of duopoly competition. If output and capacity are difficult to adjust, then Cournot is generally a better model.\nUnder some conditions the Cournot model can be recast as a two-stage model, where in the first stage firms choose capacities, and in the second they compete in Bertrand fashion.\n\nBertrand predicts a duopoly is enough to push prices down to marginal cost level; a duopoly will result in an outcome exactly equivalent to what prevails under perfect competition.\n\n\n"}
{"id": "54087464", "url": "https://en.wikipedia.org/wiki?curid=54087464", "title": "Controversies about labeling terrorism", "text": "Controversies about labeling terrorism\n\nMany definitions of terrorism exist, but there is no consensus on a single, universal definition. For example, Kydd and Walter define terrorism as \"the use of violence against civilians by non-state actors to attain political goals.\" The US Department of State defines terrorism as \"premeditated, politically motivated violence perpetrated against noncombatant targets by subnational groups or clandestine agents.\" On the other hand, the FBI's Code of Federal Regulations defines terrorism as \"The unlawful use of force and violence against persons or property to intimidate or coerce a government, civilian population, or any segment thereof, in furtherance of political or social objectives. The lack of a universal definition of terrorism has led to controversies, making it sometimes difficult to label a certain attack as a terrorist incident. For example, Tarik Kafala, head of BBC Arabic insists journalists must stop using the term \"terrorist\", because it is a \"value-laden\" term. Furthermore, there have been controversies, such as Obama's refusal to use the phrase, \"Islamic terrorism\", with political implications.\n\nThere are several problems with the definition of terrorism.\n\n\"No universally accepted definition\"\n\nThere is no universally accepted definition of terrorism. In his paper, \"Defining Terrorism: Is One Man's Terrorist Another Man's Freedom Fighter?\", Boaz Ganor points out the urgency of an internationally accepted definition of terrorism due to the ambiguousness of the term. Even within the country of the United States, the US Department of State and the FBI have different definitions of terrorism, which can lead one institute to classify an event as terrorist while another does not.\n\n\"Vague wording\"\n\nAnother issue with the definition of terrorism is its vague wording. Navin Bapat, a professor of political science at the University of North Carolina at Chapel Hill and expert on terrorist campaigns, points out the subjective nature of words used in definitions of terrorism, such as \"non-combatant\" and \"political purposes\". The subjectivity of such words allows for dispute over whether a group is or is not terrorist, as questions such as the degree to which the attack was pre-meditated, the degree to which a subnational group was involved with the premeditation of the attack, or the definition of a \"non-combatant\" and what ideologies are considered \"political\" may arise.\n\n\"Examples\"\n\nSteve Biko was a South African anti-apartheid activist who believed that some violence against whites was inevitable while apartheid remained in place. He was a leading figure of the South African Students' Organization, and participated in founding the Black People's Convention. According to many definitions of terrorism, including the US Department of State and Kydd & Walter as well as the Terrorism Act of South Africa at the time, Steve Biko's activities qualified as terrorism. Steve Biko was arrested under the South African Terrorism Act, and died while in custody. Today, Steve Biko is appraised as a hero - <nowiki>\n"}
{"id": "31316540", "url": "https://en.wikipedia.org/wiki?curid=31316540", "title": "DataScene", "text": "DataScene\n\nDataScene is a scientific graphing, animation, data analysis, and real-time data monitoring software package. It was developed with the Common Language Infrastructure technology and the GDI+ graphics library. With the two Common Language Runtime engines - the .Net and Mono frameworks - DataScene runs on all major operating systems.\n\nWith DataScene, the user can plot 39 types 2D & 3D graphs (e.g., Area graph, Bar graph, Boxplot graph, Pie graph, Line graph, Histogram graph, Surface graph, Polar graph, Water Fall graph, etc.), manipulate, print, and export graphs to various formats (e.g., Bitmap, WMF/EMF, JPEG, PNG, GIF, TIFF, PostScript, and PDF), analyze data with different mathematical methods (fitting curves, calculating statics, FFT, etc.), create chart animations for presentations (e.g. with Powerpoint), classes, and web pages, and monitor and chart real-time data.\n\nDataScene was first released (version 1.0) in March 2009 for the Windows platform and the .Net 2.0 framework. Since version 2.0, DataScene has been ported to the Mono framework 2.6 and all Linux and Unix/X11 operating systems. The latest version of DataScene is version 3.0.\n\nCyberwit offers free licensing for the Express edition of DataScene.\n\n"}
{"id": "188731", "url": "https://en.wikipedia.org/wiki?curid=188731", "title": "Decomposition", "text": "Decomposition\n\nDecomposition is the process by which organic substances are broken down into a more simple organic matter. The process is a part of the nutrient cycle and is essential for recycling the finite matter that occupies physical space in the biosphere. Bodies of living organisms begin to decompose shortly after death. Animals, such as worms, also help decompose the organic materials. Organisms that do this are known as decomposers. Although no two organisms decompose in the same way, they all undergo the same sequential stages of decomposition. The science which studies decomposition is generally referred to as \"taphonomy\" from the Greek word \"taphos\", meaning tomb.\n\nOne can differentiate abiotic from biotic decomposition (biodegradation). The former means \"degradation of a substance by chemical or physical processes, e.g., hydrolysis. The latter means \"the metabolic breakdown of materials into simpler components by living organisms\", typically by microorganisms.\n\nDecomposition begins at the moment of death, caused by two factors: 1.) autolysis, the breaking down of tissues by the body's own internal chemicals and enzymes, and 2.) putrefaction, the breakdown of tissues by bacteria. These processes release compounds such as cadaverine and putrescine, that are the chief source of the unmistakably putrid odor of decaying animal tissue.\n\nPrime decomposers are bacteria or fungi, though larger scavengers also play an important role in decomposition if the body is accessible to insects, mites and other animals. The most important arthropods that are involved in the process include carrion beetles, mites, the flesh-flies (Sarcophagidae) and blow-flies (Calliphoridae), such as the green-bottle fly seen in the summer. In North America, the most important non-insect animals that are typically involved in the process include mammal and bird scavengers, such as coyotes, dogs, wolves, foxes, rats, crows and vultures. Some of these scavengers also remove and scatter bones, which they ingest at a later time. Aquatic and marine environments have break-down agents that include bacteria, fish, crustaceans, fly larvae and other carrion scavengers.\n\nFive general stages are used to describe the process of decomposition in vertebrate animals: fresh, bloat, active decay, advanced decay, and dry/remains. The general stages of decomposition are coupled with two stages of chemical decomposition: autolysis and putrefaction. These two stages contribute to the chemical process of decomposition, which breaks down the main components of the body. With death the microbiome of the living organism collapses and is followed by the necrobiome that undergoes predictable changes over time.\n\nAmong those animals that have a heart, the \"fresh\" stage begins immediately after the heart stops beating. From the moment of death, the body begins cooling or warming to match the temperature of the ambient environment, during a stage called rigor mortis. Shortly after death, within three to six hours, the muscular tissues become rigid and incapable of relaxing, during a stage called rigor mortis. Since blood is no longer being pumped through the body, gravity causes it to drain to the dependent portions of the body, creating an overall bluish-purple discolouration termed livor mortis or, more commonly, lividity.\n\nOnce the heart stops, the blood can no longer supply oxygen or remove carbon dioxide from the tissues. The resulting decrease in pH and other chemical changes causes cells to lose their structural integrity, bringing about the release of cellular enzymes capable of initiating the breakdown of surrounding cells and tissues. This process is known as autolysis.\n\nVisible changes caused by decomposition are limited during the fresh stage, although autolysis may cause blisters to appear at the surface of the skin.\n\nThe small amount of oxygen remaining in the body is quickly depleted by cellular metabolism and aerobic microbes naturally present in respiratory and gastrointestinal tracts, creating an ideal environment for the proliferation of anaerobic organisms. These multiply, consuming the body's carbohydrates, lipids, and proteins, to produce a variety of substances including propionic acid, lactic acid, methane, hydrogen sulfide, and ammonia. The process of microbial proliferation within a body is referred to as putrefaction and leads to the second stage of decomposition, known as bloat.\n\nBlowflies and flesh flies are the first carrion insects to arrive, and they seek a suitable oviposition site.\n\nThe bloat stage provides the first clear visual sign that microbial proliferation is underway. In this stage, anaerobic metabolism takes place, leading to the accumulation of gases, such as hydrogen sulfide, carbon dioxide, methane, and nitrogen. The accumulation of gases within the bodily cavity causes the distention of the abdomen and gives a cadaver its overall bloated appearance. The gases produced also cause natural liquids and liquefying tissues to become frothy. As the pressure of the gases within the body increases, fluids are forced to escape from natural orifices, such as the nose, mouth, and anus, and enter the surrounding environment. The buildup of pressure combined with the loss of integrity of the skin may also cause the body to rupture.\n\nIntestinal anaerobic bacteria transform haemoglobin into sulfhemoglobin and other colored pigments. The associated gases which accumulate within the body at this time aid in the transport of sulfhemoglobin throughout the body via the circulatory and lymphatic systems, giving the body an overall marbled appearance.\n\nIf insects have access, maggots hatch and begin to feed on the body's tissues. Maggot activity, typically confined to natural orifices, and masses under the skin, causes the skin to slip, and hair to detach from the skin. Maggot feeding, and the accumulation of gases within the body, eventually leads to post-mortem skin ruptures which will then further allow purging of gases and fluids into the surrounding environment. Ruptures in the skin allow oxygen to re-enter the body and provide more surface area for the development of fly larvae and the activity of aerobic microorganisms. The purging of gases and fluids results in the strong distinctive odors associated with decay.\n\nActive decay is characterized by the period of greatest mass loss. This loss occurs as a result of both the voracious feeding of maggots and the purging of decomposition fluids into the surrounding environment. The purged fluids accumulate around the body and create a cadaver decomposition island (CDI). Liquefaction of tissues and disintegration become apparent during this time and strong odors persist. The end of active decay is signaled by the migration of maggots away from the body to pupate.\n\nDecomposition is largely inhibited during advanced decay due to the loss of readily available cadaveric material. Insect activity is also reduced during this stage. When the carcass is located on soil, the area surrounding it will show evidence of vegetation death. The CDI surrounding the carcass will display an increase in soil carbon and nutrients, such as phosphorus, potassium, calcium, and magnesium; changes in pH; and a significant increase in soil nitrogen.\n\nDuring the dry/remains stage, the resurgence of plant growth around the CDI may occur and is a sign that the nutrients present in the surrounding soil have not yet returned to their normal levels. All that remains of the cadaver at this stage is dry skin, cartilage, and bones, which will become dry and bleached if exposed to the elements. If all soft tissue is removed from the cadaver, it is referred to as completely skeletonized, but if only portions of the bones are exposed, it is referred to as partially skeletonised.\n\nA dead body that has been exposed to the open elements, such as water and air, will decompose more quickly and attract much more insect activity than a body that is buried or confined in special protective gear or artifacts. This is due, in part, to the limited number of insects that can penetrate a coffin and the lower temperatures under soil.\n\nThe rate and manner of decomposition in an animal body is strongly affected by several factors. In roughly descending degrees of importance, they are:\n\nThe speed at which decomposition occurs varies greatly. Factors such as temperature, humidity, and the season of death all determine how fast a fresh body will skeletonize or mummify. A basic guide for the effect of environment on decomposition is given as Casper's Law (or Ratio): if all other factors are equal, then, when there is free access of air a body decomposes twice as fast than if immersed in water and eight times faster than if buried in earth. Ultimately, the rate of bacterial decomposition acting on the tissue will depend upon the temperature of the surroundings. Colder temperatures decrease the rate of decomposition while warmer temperatures increase it. A dry body will not decompose efficiently. Moisture helps the growth of microorganisms that decompose the organic matter, but too much moisture could lead to anaerobic conditions slowing down the decomposition process.\n\nThe most important variable is a body's accessibility to insects, particularly flies. On the surface in tropical areas, invertebrates alone can easily reduce a fully fleshed corpse to clean bones in under two weeks. The skeleton itself is not permanent; acids in soils can reduce it to unrecognizable components. This is one reason given for the lack of human remains found in the wreckage of the \"Titanic\", even in parts of the ship considered inaccessible to scavengers. Freshly skeletonized bone is often called \"green\" bone and has a characteristic greasy feel. Under certain conditions (normally cool, damp soil), bodies may undergo saponification and develop a waxy substance called adipocere, caused by the action of soil chemicals on the body's proteins and fats. The formation of adipocere slows decomposition by inhibiting the bacteria that cause putrefaction.\n\nIn extremely dry or cold conditions, the normal process of decomposition is halted – by either lack of moisture or temperature controls on bacterial and enzymatic action – causing the body to be preserved as a mummy. Frozen mummies commonly restart the decomposition process when thawed (see Ötzi the Iceman), whilst heat-desiccated mummies remain so unless exposed to moisture.\n\nThe bodies of newborns who never ingested food are an important exception to the normal process of decomposition. They lack the internal microbial flora that produce much of decomposition and quite commonly mummify if kept in even moderately dry conditions.\n\nAerobic decomposition takes place in the presence of oxygen. This is most common to occur in nature. Living organisms that use oxygen to survive feed on the body. Anaerobic decomposition takes place in the absence of oxygen. This could be place where the body is buried in organic material and oxygen can not reach it. This process of putrefaction has a bad odor accompanied by it due to the hydrogen sulfide and organic matter containing sulfur.\n\nEmbalming is the practice of delaying decomposition of human and animal remains. Embalming slows decomposition somewhat, but does not forestall it indefinitely. Embalmers typically pay great attention to parts of the body seen by mourners, such as the face and hands. The chemicals used in embalming repel most insects, and slow down bacterial putrefaction by either killing existing bacteria in or on the body themselves or by \"fixing\" cellular proteins, which means that they cannot act as a nutrient source for subsequent bacterial infections. In sufficiently dry environments, an embalmed body may end up mummified and it is not uncommon for bodies to remain preserved to a viewable extent after decades. Notable viewable embalmed bodies include those of:\n\nA body buried in a sufficiently dry environment may be well preserved for decades. This was observed in the case for murdered civil rights activist Medgar Evers, who was found to be almost perfectly preserved over 30 years after his death, permitting an accurate autopsy when the case of his murder was re-opened in the 1990s.\n\nBodies submerged in a peat bog may become naturally \"embalmed\", arresting decomposition and resulting in a preserved specimen known as a bog body. The time for an embalmed body to be reduced to a skeleton varies greatly. Even when a body is decomposed, embalming treatment can still be achieved (the arterial system decays more slowly) but would not restore a natural appearance without extensive reconstruction and cosmetic work, and is largely used to control the foul odors due to decomposition.\n\nAn animal can be preserved almost perfectly, for millions of years in a resin such as amber.\n\nThere are some examples where bodies have been inexplicably preserved (with no human intervention) for decades or centuries and appear almost the same as when they died. In some religious groups, this is known as incorruptibility. It is not known whether or for how long a body can stay free of decay without artificial preservation.\n\nVarious sciences study the decomposition of bodies under the general rubric of forensic science because the usual motive for such studies is to determine the time and cause of death for legal purposes:\n\nThe University of Tennessee Anthropological Research Facility (better known as the Body Farm) in Knoxville, Tennessee has a number of bodies laid out in various situations in a fenced-in plot near the medical center. Scientists at the Body Farm study how the human body decays in various circumstances to gain a better understanding of decomposition.\n\nDecomposition of plant matter occurs in many stages. It begins with leaching by water; the most easily lost and soluble carbon compounds are liberated in this process. Another early process is physical breakup or fragmentation of the plant material into smaller bits which have greater surface area for microbial colonization and attack. In smaller dead plants, this process is largely carried out by the soil invertebrate fauna, whereas in the larger plants, primarily parasitic life-forms such as insects and fungi play a major breakdown role and are not assisted by numerous detritivore species.\n\nFollowing this, the plant detritus (consisting of cellulose, hemicellulose, microbial products, and lignin) undergoes chemical alteration by microbes. Different types of compounds decompose at different rates. This is dependent on their chemical structure.\n\nFor instance, lignin is a component of wood, which is relatively resistant to decomposition and can in fact only be decomposed by certain fungi, such as the black-rot fungi. Wood decomposition is a complex process involving fungi which transport nutrients to the nutritionally scarce wood from outside environment. Because of this nutritional enrichment the fauna of saproxylic insects may develop and in turn affect dead wood, contributing to wood decomposition and nutrient cycling in the forest floor. Lignin is one such remaining product of decomposing plants with a very complex chemical structure causing the rate of microbial breakdown to slow. Warmth increases the speed of plant decay, by the same amount regardless of the composition of the plant\n\nIn most grassland ecosystems, natural damage from fire, insects that feed on decaying matter, termites, grazing mammals, and the physical movement of animals through the grass are the primary agents of breakdown and nutrient cycling, while bacteria and fungi play the main roles in further decomposition.\n\nThe chemical aspects of plant decomposition always involve the release of carbon dioxide. In fact, decomposition contributes over 90 percent of carbon dioxide released each year.\n\nThe decomposition of food, either plant or animal, called \"spoilage\" in this context, is an important field of study within food science. Food decomposition can be slowed down by conservation. The spoilage of meat occurs, if the meat is untreated, in a matter of hours or days and results in the meat becoming unappetizing, poisonous or infectious. Spoilage is caused by the practically unavoidable infection and subsequent decomposition of meat by bacteria and fungi, which are borne by the animal itself, by the people handling the meat, and by their implements. Meat can be kept edible for a much longer time – though not indefinitely – if proper hygiene is observed during production and processing, and if appropriate food safety, food preservation and food storage procedures are applied.\n\nSpoilage of food is attributed to contamination from microorganisms such as bacteria, molds, and yeasts, along with natural decay of the food. These decomposition bacteria reproduce at rapid rates under conditions of moisture and preferred temperatures. When the proper conditions are lacking the bacteria may form spores which lurk until suitable conditions arise to continue reproduction.\n\nThe rate of decomposition is governed by three sets of factors—the physical environment (temperature, moisture and soil properties), the quantity and quality of the dead material available to decomposers, and the nature of the microbial community itself.\n\nDecomposition rates are low under very wet or very dry conditions. Decomposition rates are highest in wet, moist conditions with adequate levels of oxygen. Wet soils tend to become deficient in oxygen (this is especially true in wetlands), which slows microbial growth. In dry soils, decomposition slows as well, but bacteria continue to grow (albeit at a slower rate) even after soils become too dry to support plant growth. When the rains return and soils become wet, the osmotic gradient between the bacterial cells and the soil water causes the cells to gain water quickly. Under these conditions, many bacterial cells burst, releasing a pulse of nutrients. Decomposition rates also tend to be slower in acidic soils. Soils which are rich in clay minerals tend to have lower decomposition rates, and thus, higher levels of organic matter. The smaller particles of clay result in a larger surface area that can hold water. The higher the water content of a soil, the lower the oxygen content and consequently, the lower the rate of decomposition. Clay minerals also bind particles of organic material to their surface, making them less accessible to microbes. Soil disturbance like tilling increases decomposition by increasing the amount of oxygen in the soil and by exposing new organic matter to soil microbes.\n\nThe quality and quantity of the material available to decomposers is another major factor that influences the rate of decomposition. Substances like sugars and amino acids decompose readily and are considered labile. Cellulose and hemicellulose, which are broken down more slowly, are \"moderately labile\". Compounds which are more resistant to decay, like lignin or cutin, are considered recalcitrant. Litter with a higher proportion of labile compounds decomposes much more rapidly than does litter with a higher proportion of recalcitrant material. Consequently, dead animals decompose more rapidly than dead leaves, which themselves decompose more rapidly than fallen branches. As organic material in the soil ages, its quality decreases. The more labile compounds decompose quickly, leaving an increasing proportion of recalcitrant material. Microbial cell walls also contain recalcitrant materials like chitin, and these also accumulate as the microbes die, further reducing the quality of older soil organic matter.\n\n\n"}
{"id": "16064919", "url": "https://en.wikipedia.org/wiki?curid=16064919", "title": "Donald M. Baer", "text": "Donald M. Baer\n\nDonald M. Baer was a psychologist who contributed to the science of applied behavior analysis and pioneered the development of behavior analysis at two separate institutions. Baer is best known for his contributions at the University of Kansas. Throughout his career, he published over two hundred articles, books, and chapters on various psychological issues. Some of his most noteworthy contributions include literature on behavior-analytic theory, experimental design, and early childhood interventions. Baer received numerous awards during his lifetime which acknowledged his innovation and dedication to his field of research.\n\nDonald M. Baer was born in St. Louis, Missouri on October 25, 1931. His family later moved to Chicago, Illinois. Baer attended the University of Chicago and received his doctoral degree in 1957 under the direction of Jacob L. Gewirtz (Poulson, 2002). After graduation, Baer began working with Sidney W. Bijou at the University of Washington. Baer was soon offered a faculty position at the University of Kansas where he remained until his death on April 28, 2002. He is survived by his wife Elsie Pinkston and his three daughters.\n\nCollectively, Baer and Bijou established the behavior analysis approach to child development at the University of Washington (Anonymous, 2002). From 1957 to 1965, Baer and Bijou conducted an array of research on the effects of reinforcement contingencies on children (Anonymous, 2002). An influential paper titled \"Effect of withdrawal of positive reinforcement on an extinguishing response in young children\" was written during this time (Baer, 1961). This research article established the effectiveness of \"withdrawal of positive reinforcement\" as a means of reducing behavior(Baer, 1961., p. 1).\n\nWhile at the University of Washington, Baer made another noteworthy research paper on the escape and avoidance behaviors of preschool children (Baer, 1960). Unfortunately, Baer and his research associates became involved in a \"typical Psychology civil war\" at the University of Washington (Baer, 1993., pp. 570). The war stemmed from the administration's \"intolerance of Skinnerian perspectives and the rejection of research using single-subject designs.\" (Horowitz, 2002., pp. 313). Seeking a university environment more welcoming of behavior analysis, Don Baer found his way to the newly formed Human Development and Family Life Department at the University of Kansas.\n\nThe establishment of the Human Development and Family Life Department (HDFL) at the University of Kansas is a somewhat turbulent chain of events. The following comes from Baer (1993), unless otherwise stated. Due to various professors retiring, the University of Kansas decided to dissolve its Home Economics Department in the early 1960s. As a result, over $2 million in funding remained unused. A recently acquired academic researcher, Frances Horowitz, pleaded to the Kansas administration to simply transform the Home Economics Department into the Department of Human Development and Family Life (HDFL). The Home Economics Department was already a well known childhood research center. The administration agreed and appointed Horowitz chair of the department. Horowitz assumed the task of recruiting researchers. Horowitz and Donald Baer had conversed at APA meetings for years and became great friends. At an APA meeting in Los Angeles, Horowitz asked Baer if he would be \"interested in a challenge\" (Baer, 1993. pp. 570). The challenge Horowitz was referring to was establishing a new doctoral program from the ground up. Don Baer remained at the department and began expanding the program to the highest degree.\n\nAt the University of Kansas, Baer began recruiting behavioral researchers for his newly formed department (Glover, pp. 146). Among those recruited, Baer began research with Montrose M. Wolf, Todd R. Risley, and James A. Sherman. The fruits of their research yielded the development of the discipline of applied behavior analysis (ABA) at the University of Kansas late in the 1960s. Applied behavior analysis concentrates on \"empirically-based interventions into problems of individual, social, and cultural importance\" (Anonymous, 2002, p. 1). This discipline provided a structure for working with developmentally delayed children and adults. According to Glover (1987), Baer established at Kansas \"one of the outstanding centers for research in applied behavior analysis\" (pp. 146).\n\nThe following comes from Colman (1994), unless otherwise stated. Shortly after the establishment of ABA program, the first journal, Journal of Applied Behavior Analysis, dedicated to the discipline was published in 1968. In the first issue, Baer, Wolf, and Risley (1968) published \"Some current dimensions of applied behavior analysis\". This groundbreaking article named seven definitive dimensions to the approach of behavior analysis. The seven dimensions state that \"work in the field should be applied, behavioral, analytic, technological, conceptually systematic, effective and capable of generalized effects.\" (pp. 406). Twenty years later, Baer et al. published an updated version, \"Some still current dimensions of applied behavior analysis\" (pp. 406). The seven dimensions had not changed, only elaborated on, throughout the years.\n\nThe following comes from Hains (1989), unless otherwise stated. Baer published hundreds of articles and books. He coauthored a paper with an associate, Ann H. Hains, in 1989 on experimental design entitled \"Interaction effects in multi-element designs: Inevitable, desirable, and ignorable\". In this paper, Baer addressed the single mindedness of the authors of single-subject research design books. He stated that these researchers only investigated the effectiveness of treatments in alteration and never examined whether the treatments interact. Baer proposed that single subject designs can and should be used to study not only separate effects but also interactions between the two conditions. Don Baer always seemed to be thinking ahead of his time.\n\nDeveloping new techniques and approaches for teaching developmentally delayed children was Baer's passion (Anonymous, 2002). He wrote many articles analyzing the current public education system and ways to improve their methods (Anonymous, 2002). The following comes from Warren et al. (1981), unless otherwise stated. One of his most read articles is titled \"Generalization and maintenance of question-asking by severely retarded individuals\". This article states the importance of establishing question asking strategies with developmentally delayed children.\n\nNormal children develop simple question-asking (e.g. \"What's that\") in early infancy (pp. 15). Severely retarded children usually have great difficulty developing this skill. Initial training for this strategy is costly and time consuming for school systems. Despite the vast amount of time spent with the children, they can easily lose this ability. Baer and colleagues urged school systems to establish maintenance programs to help developmentally delayed children to continue and improve on their question-asking strategy. By maintaining this ability with the children, they will not only improve question-asking but save the school system valuable time and money from remedial training.\n\nDonald M. Baer was an influential leader of the department of Applied Behavior Science at the University of Kansas. He advised over 100 doctoral students while at Kansas, in addition to countless other graduate students. He contributed to the Bureau of Child Research, now known as the Schiefelbusch Lifespan Institute (LSI). The program was successful, receiving years of funding from the National Institute of Mental Health. The Institute was also the first to obtain an award from the Society for the Advancement of Behavior Analysis (SABA) for its contributions to the field.\n\nBaer won the Edgar A. Doll Award for his assistance to those with developmental disorders. In 1987 he was awarded the Don Hake Award from Division 25 of the American Psychological Association. In 1997, he won the award for Distinguished Service to Behavior Analysis from SABA. Baer served as president of the Society for the Experimental Analysis of Behavior from 1983 to 1984. He served as an editor for the Journal of Applied Behavior Analysis (1970–1971). As well as serving as an associated editor of the American Journal of Mental Deficiency. Baer also spent much time traveling to other countries, including Australia, Japan, and Spain, while serving as a distinguished visiting professor.\n\nHe died April 28, 2002 of heart failure in Lawrence, Kansas. His ashes are buried at Pioneer Cemetery in Lawrence, Kansas.\n\nAuthor of Publication on Donald Baer: Kevin Phelan - University of North Carolina, Wilmington - 2008 - PSY 410 - History and Systems of Psychology - Final Paper\n\n"}
{"id": "440260", "url": "https://en.wikipedia.org/wiki?curid=440260", "title": "Enlightenment in Buddhism", "text": "Enlightenment in Buddhism\n\nThe English term enlightenment is the western translation of the abstract noun bodhi, (; Sanskrit: बोधि; Pali: \"bodhi\"), the knowledge or wisdom, or awakened intellect, of a Buddha. The verbal root \"budh-\" means \"to awaken,\" and its literal meaning is closer to \"awakening.\" Although its most common usage is in the context of Buddhism, the term \"buddhi\" is also used in other Indian philosophies and traditions. The term \"enlightenment\" was popularised in the Western world through the 19th century translations of Max Müller. It has the western connotation of a sudden insight into a transcendental truth or reality.\n\nThe term is also being used to translate several other Buddhist terms and concepts, which are used to denote insight (\"prajna\", \"kensho\" and \"satori\"); knowledge (\"vidhya\"); the \"blowing out\" (\"Nirvana\") of disturbing emotions and desires and the subsequent freedom or release (\"vimutti\"); and the attainment of Buddhahood, as exemplified by Gautama Buddha.\n\nWhat exactly constituted the Buddha's awakening is unknown. It may probably have involved the knowledge that liberation was attained by the combination of mindfulness and \"dhyāna\", applied to the understanding of the arising and ceasing of craving. The relation between \"dhyana\" and insight is a core problem in the study of Buddhism, and is one of the fundamentals of Buddhist practice.\n\nIn the western world the concept of (spiritual) enlightenment has taken on a romantic meaning. It has become synonymous with self-realization and the true self and false self, being regarded as a substantial essence being covered over by social conditioning.\n\n\"Bodhi\", Sanskrit बोधि, \"awakening,\" \"perfect knowledge,\" \"perfect knowledge or wisdom (by which a man becomes a बुद्ध [Buddha] or जिन [\"jina\", \"arahant\"; \"victorious,\" \"victor\"], the illuminated or enlightened intellect (of a Buddha or जिन).\"\n\nIt is an abstract noun, formed from the verbal root \"*budh-\", Sanskrit बुध, \"to awaken, to know,\" \"to wake , wake up , be awake,\" \"to recover consciousness (after a swoon),\" \"to observe , heed , attend to.\"\n\nIt corresponds to the verbs \"bujjhati\" (Pāli) and \"bodhati\", बोदति, \"become or be aware of, perceive, learn, know, understand, awake\"or \"budhyate\" (Sanskrit).\n\nThe feminine Sanskrit noun of \"*budh-\" is बुद्धि, \"buddhi\", \"prescience, intuition, perception, point of view.\"\n\nRobert S. Cohen notes that the majority of English books on Buddhism use the term \"enlightenment\" to translate the term \"bodhi\". The root \"budh\", from which both \"bodhi\" and \"Buddha\" are derived, means \"to wake up\" or \"to recover consciousness\". Cohen notes that \"bodhi\" is not the result of an \"illumination\", but of a path of realization, or coming to understanding. The term \"enlightenment\" is event-oriented, whereas the term \"awakening\" is process-oriented. The western use of the term \"enlighten\" has Christian roots, as in Calvin's \"It is God alone who enlightens our minds to perceive his truths\".\n\nEarly 19th century \"bodhi\" was translated as \"intelligence\". The term \"enlighten\" was first being used in 1835, in an English translation of a French article, while the first recorded use of the term 'enlightenment' is credited (by the Oxford English Dictionary) to the \"Journal of the Asiatic Society of Bengal\" (February, 1836). In 1857 \"The Times\" used the term \"the Enlightened\" for the Buddha in a short article, which was reprinted the following year by Max Müller. Thereafter, the use of the term subsided, but reappeared with the publication of Max Müller's \"Chips from a german Workshop\", which included a reprint from the \"Times\"-article. The book was translated in 1969 into German, using the term \"der Erleuchtete\". Max Müller was an essentialist, who believed in a natural religion, and saw religion as an inherent capacity of human beings. \"Enlightenment\" was a means to capture natural religious truths, as distinguished from mere mythology.\n\nBy the mid-1870s it had become commonplace to call the Buddha \"enlightened\", and by the end of the 1880s the terms \"enlightened\" and \"enlightenment\" dominated the English literature.\n\n\"Bodhi\" (Sanskrit, Pāli), from the verbal root \"budd\", \"to awaken\", \"to understand\", means literally \"to have woken up and understood\". According to Johannes Bronkhorst, Tillman Vetter, and K.R. Norman, \"bodhi\" was at first not specified. K.R. Norman:\nAccording to Norman, \"bodhi\" may basically have meant the knowledge that \"nibbana\" was attained, due to the practice of \"dhyana\". Originally only \"prajna\" may have been mentioned, and Tillman Vetter even concludes that originally dhyana itself was deemed liberating, with the stilling of pleasure of pain in the fourth jhana, not the gaining of some perfect wisdom or insight. Gombrich also argues that the emphasis on insight is a later development.\n\nIn Theravada Buddhism, \"bodhi\" refers to the realisation of the four stages of enlightenment and becoming an Arahant. In Theravada Buddhism, \"bodhi\" is equal to supreme insight, and the realisation of the four noble truths, which leads to deliverance. According to Nyanatiloka,\nThis equation of \"bodhi\" with the four noble truths is a later development, in response to developments within Indian religious thought, where \"liberating insight\" was deemed essential for liberation. The four noble truths as the liberating insight of the Buddha eventually were superseded by Pratītyasamutpāda, the twelvefold chain of causation, and still later by anatta, the emptiness of the self.\n\nIn Mahayana Buddhism, \"bodhi\" is equal to \"prajna\", insight into the Buddha-nature, sunyata and tathatā. This is equal to the realisation of the non-duality of absolute and relative.\n\nIn Theravada Buddhism \"pannā\" (Pali) means \"understanding\", \"wisdom\", \"insight\". \"Insight\" is equivalent to \"vipassana\"', insight into the three marks of existence, namely \"anicca\", \"dukkha\" and \"anatta\". Insight leads to the four stages of enlightenment and Nirvana.\n\nIn Mahayana Buddhism Prajna (Sanskrit) means \"insight\" or \"wisdom\", and entails insight into \"sunyata\". The attainment of this insight is often seen as the attainment of \"enlightenment\".\n\n\"Kensho\" and \"Satori\" are Japanese terms used in Zen traditions. \"Kensho\" means \"seeing into one's true nature.\" \"Ken\" means \"seeing\", \"sho\" means \"nature\", \"essence\", c.q Buddha-nature. \"Satori\" (Japanese) is often used interchangeably with kensho, but refers to the \"experience\" of kensho. The Rinzai tradition sees \"kensho\" as essential to the attainment of Buddhahood, but considers further practice essential to attain Buddhahood.\n\nEast-Asian (Chinese) Buddhism emphasizes insight into Buddha-nature. This term is derived from Indian tathagata-garbha thought, \"the womb of the thus-gone\" (the Buddha), the inherent potential of every sentient being to become a Buddha. This idea was integrated with the Yogacara-idea of the \"ālaya vijñāna\", and further developed in Chinese Buddhism, which integrated Indian Buddhism with native Chinese thought. Buddha-nature came to mean both the potential of awakening \"and\" the whole of reality, a dynamic interpenetration of absolute and relative. In this awakening it is realized that observer and observed are not distinct entities, but mutually co-dependent.\n\nThe term \"vidhya\" is being used in contrast to \"avidhya\", ignorance or the lack of knowledge, which binds us to samsara. The \"Mahasaccaka Sutta\" describes the three knowledges which the Buddha attained:\n\nAccording to Bronkhorst, the first two knowledges are later additions, while insight into the four truths represents a later development, in response to concurring religious traditions, in which \"liberating insight\" came to be stressed over the practice of \"dhyana\".\n\nVimutti, also called moksha, means \"freedom\", \"release\", \"deliverance\". Sometimes a distinction is being made between \"ceto-vimutti\", \"liberation of the mind\", and \"panna-vimutti\", \"liberation by understanding\". The Buddhist tradition recognises two kinds of \"ceto-vimutti\", one temporarily and one permanent, the last being equivalent to \"panna-vimutti\".\n\nYogacara uses the term \"āśraya parāvŗtti\", \"revolution of the basis\", \nNirvana is the \"blowing out\" of disturbing emotions, which is the same as liberation. The usage of the term \"enlightenment\" to translate \"nirvana\" was popularized in the 19th century, due, in part, to the efforts of Max Muller, who used the term consistently in his translations.\n\nThree types of buddha are recognized: \n\nSiddhartha Gautama, known as the Buddha, is said to have achieved full awakening, known as \"samyaksaṃbodhi\" (Sanskrit; Pāli: \"sammāsaṃbodhi\"), \"perfect Buddhahood\", or \"anuttarā-samyak-saṃbodhi\", \"highest perfect awakening\".\n\nThe term buddha has acquired somewhat different meanings in the various Buddhist traditions. An equivalent term for Buddha is Tathāgata, \"the thus-gone\". The way to Buddhahood is somewhat differently understood in the various buddhist traditions.\n\nIn the suttapitaka, the Buddhist canon as preserved in the Theravada tradition, a couple of texts can be found in which the Buddha's attainment of liberation forms part of the narrative.\n\nThe \"Ariyapariyesana Sutta\" (Majjhima Nikaya 26) describes how the Buddha was dissatisfied with the teachings of Alara Kalama and Uddaka Ramaputta, wandered further through Magadhan country, and then found \"an agreeable piece of ground\" which served for striving. The sutra then only says that he attained Nibbana.\n\nIn the \"Vanapattha Sutta\" (Majjhima Nikaya 17) the Buddha describes life in the jungle, and the attainment of awakening. The \"Mahasaccaka Sutta\" (Majjhima Nikaya 36) describes his ascetic practices, which he abandoned. There-after he remembered a spontaneous state of jhana, and set out for jhana-practice. Both suttras narrate how, after destroying the disturbances of the mind, and attaining concentration of the mind, he attained three knowledges (vidhya):\n\nInsight into the Four Noble Truths is here called awakening. The monk (\"bhikkhu\") has \n\nAwakening is also described as synonymous with Nirvana, the extinction of the passions whereby suffering is ended and no more rebirths take place. The insight arises that this liberation is certain:\nSchmithausen notes that the mention of the four noble truths as constituting \"liberating insight\", which is attained after mastering the Rupa Jhanas, is a later addition to texts such as Majjhima Nikaya 36. Bronkhorst notices that \nIt calls in question the reliability of these accounts, and the relation between \"dhyana\" and insight, which is a core problem in the study of early Buddhism. Originally the term \"prajna\" may have been used, which came to be replaced by the four truths in those texts where \"liberating insight\" was preceded by the four jhanas. Bronkhorst also notices that the conception of what exactly this \"liberating insight\" was developed throughout time. Whereas originally it may not have been specified, later on the four truths served as such, to be superseded by \"pratityasamutpada\", and still later, in the Hinayana schools, by the doctrine of the non-existence of a substantial self or person. And Schmithausen notices that still other descriptions of this \"liberating insight\" exist in the Buddhist canon: \nAn example of this substitution, and its consequences, is Majjhima Nikaya 36:42-43, which gives an account of the awakening of the Buddha.\n\nThe term bodhi acquired a variety of meanings and connotations during the development of Buddhist thoughts in the various schools.\n\nIn early Buddhism, \"bodhi\" carried a meaning synonymous to \"nirvana\", using only some different metaphors to describe the insight, which implied the extinction of \"lobha\" (greed), \"dosa\" (hate) and \"moha\" (delusion). \n\nIn Theravada Buddhism, bodhi and nirvana carry the same meaning, that of being freed from greed, hate and delusion. In Theravada Buddhism, \"bodhi\" refers to the realisation of the four stages of enlightenment and becoming an Arahant. In Theravada Buddhism, \"bodhi\" is equal to supreme insight, the realisation of the four noble truths, which leads to deliverance. Reaching full awakening is equivalent in meaning to reaching Nirvāṇa. Attaining Nirvāṇa is the ultimate goal of Theravada and other śrāvaka traditions. It involves the abandonment of the ten fetters and the cessation of dukkha or suffering. Full awakening is reached in four stages. According to Nyanatiloka,\nSince the 1980s, western Theravada-oriented teachers have started to question the primacy of insight. According to Thanissaro Bhikkhu, \"jhana\" and \"vipassana\" (insight) form an integrated practice. Polak and Arbel, following scholars like Vetter and Bronkhorst, argue that right effort, c.q. the four right efforts (sense restraint, preventing the arising of unwholesome states, and the generation of wholesome states), mindfulness, and \"dhyana\" form an integrated practice, in which \"dhyana\" is the actualisation of insight, leading to an awakened awareness which is \"non-reactive and lucid.\"\n\nIn Mahayana-thought, bodhi is the realisation of the inseparability of samsara and nirvana, and the unity of subject and object. It is similar to prajna, to realizing the Buddha-nature, realizing sunyata and realizing suchness. In time, the Buddha's awakening came to be understood as an immediate full awakening and liberation, instead of the insight into and certainty about the way to follow to reach enlightenment. However, in some Zen traditions this perfection came to be relativized again; according to one contemporary Zen master, \"Shakyamuni buddha and Bodhidharma are still practicing.\"\n\nMahayana discerns three forms of awakened beings:\n\nWithin the various Mahayana-schools exist various further explanations and interpretations. In Mahāyāna Buddhism the Bodhisattva is the ideal. The ultimate goal is not only of one's own liberation in Buddhahood, but the liberation of all living beings. But Mahayana Buddhism also developed a cosmology with a wide range of buddhas and bodhisattvas, who assist humans on their way to liberation.\n\nNichiren Buddhism regards Buddhahood as a state of perfect freedom, in which one is awakened to the eternal and ultimate truth that is the reality of all things. This supreme state of life is characterized by boundless wisdom and infinite compassion. The Lotus Sutra reveals that Buddhahood is a potential in the lives of all beings. \n\nIn the Tathagatagarbha and Buddha-nature doctrines bodhi becomes equivalent to the universal, natural and pure state of the mind:\nAccording to these doctrines bodhi is always there within one's mind, but requires the defilements to be removed. This vision is expounded in texts such as the Shurangama Sutra and the Uttaratantra.\n\nIn Shingon Buddhism, the state of Bodhi is also seen as naturally inherent in the mind. It is the mind's natural and pure state, where no distinction is being made between a perceiving subject and perceived objects. This is also the understanding of Bodhi found in Yogacara Buddhism.\n\nTo achieve this vision of non-duality, it is necessary to recognise one's own mind:\nDuring the development of Mahayana Buddhism the various strands of thought on Bodhi were continuously being elaborated. Attempts were made to harmonize the various terms. The Buddhist commentator Buddhaguhya treats various terms as synonyms:\nIn the western world the concept of \"enlightenment\" has taken on a romantic meaning. It has become synonymous with self-realization and the true self, being regarded as a substantial essence being covered over by social conditioning.\n\nThe use of the western word \"enlightenment\" is based on the supposed resemblance of \"bodhi\" with \"Aufklärung\", the independent use of reason to gain insight into the true nature of our world. In fact there are more resemblances with Romanticism than with the Enlightenment: the emphasis on feeling, on intuitive insight, on a true essence beyond the world of appearances.\n\nThe equivalent term \"awakening\" has also been used in a Christian context, namely the Great Awakenings, several periods of religious revival in American religious history. Historians and theologians identify three or four waves of increased religious enthusiasm occurring between the early 18th century and the late 19th century. Each of these \"Great Awakenings\" was characterized by widespread revivals led by evangelical Protestant ministers, a sharp increase of interest in religion, a profound sense of conviction and redemption on the part of those affected, an increase in evangelical church membership, and the formation of new religious movements and denominations.\n\nThe romantic idea of enlightenment as insight into a timeless, transcendent reality has been popularized especially by D.T. Suzuki. Further popularization was due to the writings of Heinrich Dumoulin. Dumoulin viewed metaphysics as the expression of a transcendent truth, which according to him was expressed by Mahayana Buddhism, but not by the pragmatic analysis of the oldest Buddhism, which emphasizes anatta. This romantic vision is also recognizable in the works of Ken Wilber.\n\nIn the oldest Buddhism this essentialism is not recognizable. According to critics it doesn't really contribute to a real insight into Buddhism:...most of them labour under the old cliché that the goal of Buddhist psychological analysis is to reveal the hidden mysteries in the human mind and thereby facilitate the development of a transcendental state of consciousness beyond the reach of linguistic expression.\n\nA common reference in western culture is the notion of \"enlightenment \"experience\"\". This notion can be traced back to William James, who used the term \"religious experience\" in his book, \"The Varieties of Religious Experience\". Wayne Proudfoot traces the roots of the notion of \"religious experience\" further back to the German theologian Friedrich Schleiermacher (1768-1834), who argued that religion is based on a feeling of the infinite. Schleiermacher used the notion of \"religious experience\" to defend religion against the growing scientific and secular critique.\n\nIt was popularised by the Transcendentalists, and exported to Asia via missionaries. Transcendentalism developed as a reaction against 18th Century rationalism, John Locke's philosophy of Sensualism, and the predestinationism of New England Calvinism. It is fundamentally a variety of diverse sources such as Hindu texts like the Vedas, the Upanishads and the Bhagavad Gita, various religions, and German idealism.\n\nIt was adopted by many scholars of religion, of which William James was the most influential.\n\nThe notion of \"experience\" has been criticised. Robert Sharf points out that \"experience\" is a typical western term, which has found its way into Asian religiosity via western influences.\n\nThe notion of \"experience\" introduces a false notion of duality between \"experiencer\" and \"experienced\", whereas the essence of kensho is the realisation of the \"non-duality\" of observer and observed. \"Pure experience\" does not exist; all experience is mediated by intellectual and cognitive activity. The specific teachings and practices of a specific tradition may even determine what \"experience\" someone has, which means that this \"experience\" is not the \"proof\" of the teaching, but a \"result\" of the teaching. A pure consciousness without concepts, reached by \"cleaning the doors of perception\" as per romantic poet William Blake, would, according to Mohr, be an overwhelming chaos of sensory input without coherence.\n\nSakyamuni's Buddhahood is celebrated on Bodhi Day. In Sri Lanka and Japan different days are used for this celebration.\n\nAccording to the Theravada tradition in Sri Lanka, Sakyamuni reached Buddhahood at the full moon in May. This is celebrated at Wesak Poya, the full moon in May, as Sambuddhatva jayanthi (also known as Sambuddha jayanthi).\n\nAccording to the Zen tradition, the Buddha reached his decisive insight on 8 December. This is celebrated in Zen monasteries with a very intensive eight-day session of \"Rōhatsu\".\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "994228", "url": "https://en.wikipedia.org/wiki?curid=994228", "title": "Environmental racism", "text": "Environmental racism\n\nEnvironmental racism is a term used to describe environmental injustice that occurs in practice and in policy within a racialized context. \n\nIn 1982, the term was coined by Benjamin Chavis, who was the then executive director of the United Church of Christ (UCC) Commission for Racial Justice, in response to the dumping of hazardous PCB waste in a town in Warren County, North Carolina. The UCC and US General Accounting Office (GAO) reports on this case in NC brought public attention to the strong association between locations of hazardous waste sites and poor minority neighborhoods. Early activists in environmental racism, Chavis and Robert Bullard pointed out institutionalized racism stemming from government and corporate policies that led to environmental racism. Practices like redlining, zoning, and colorblind adaptation planning, and factors that inhibit residents from preventing these practices include their low socioeconomic status, and lack of political representation and mobility that all contribute to environmental racism. \n\nEnvironmental racism can be explained through a number of ways and in most cases usually include four unique patterns. The first one includes exposure to hazardous waste. It can also be identified by determining how vulnerable a community is to issues like flooding. The accessibility of potable water can also help in measuring environmental justice. Finally, a discriminatory waste management program can also be considered a case of environmental injustice. Some social scientists have argued that some causes of environmental racism are intentional, for example, the dumping of hazardous waste in a minority community.  Apart from intentional reasons, this kind of racism can also be caused by structural and institutional elements. Chavis defined environmental racism in five categories. First, he termed it as racial discrimination in defining environmental policies. He also stated that this occurs when these regulations and laws are being enforced. He further stated that it is the deliberate targeting of communities of color as far as dumping of toxic waste is concerned. He also referred to this term as the official sanctioning of dangerous poisons and pollutants in the minority communities. Finally, he termed it as the history of exclusion of people of color from attaining leadership positions in the ecological organizations.  Other activists like Robert Bullard also had their own definition for the term claiming that it refers to any policy or directive that differentially harms people, groups, or even communities based on their color. \n\nIt is also important to understand the factors that can lead to environmental racism and social scientists have identified the four main ones. They include cheap land, the absence of political authority, lack of mobility, and lastly poverty. While corporations can obtain land cheaply because of economic reasons, they can also do that because a community lacks the power to fight such companies. It is also a known fact that minority communities lack political power and this has often worked to their disadvantage. Businesses know that they can do anything in such places without any meaningful resistance. It is worth noting that politicians and lawmakers are very selective on whom they represent, and these are some of the elements that have harmed these communities. Corporations also take advantage of the lack of mobility of these people and pay them minimum wages to limit their mobility even further.  It should be noted that the impoverished nature of these communities contributes to all the above factors because without money people cannot act either physically or politically.\n\nThe acknowledgement of environmental racism prompted the environmental justice social movement that began in the 1970s and 1980s in the United States. Although environmental racism has been historically tied to the environmental justice movement, throughout the years the term has dissociated more and more from the environmental justice movement. In response to cases of environmental racism, grassroots organizations and campaigns have brought more attention to environmental racism in policy making and emphasize the importance of having input from minorities in policy making. Although environmental racism was coined in the US, it also occurs on the international level. Examples include the exportation of hazardous wastes to poor countries in the Global South with lax environmental policies and safety practices (pollution havens). Marginalized communities that do not have the socioeconomic and political means to oppose large corporations are at risk to environmentally racist practices that are detrimental and sometimes fatal to humans.Economic statuses and political positions are crucial factors when looking at environmental problems because they determine where a person lives. People who do not have those privileges are usually the ones who suffer from environmental problems. \n\nEnvironmental racism crosses boundaries of countries and is addressed below in the context of environmental racism that has occurred outside of the US, where the term was coined.\n\nIn the United States, the first report to draw a relationship between race, income, and risk of exposure to pollutants was the Council of Environmental Quality's \"Annual Report to the President\" in 1971, in response to toxic waste dumping in an African American community in Warren County, NC. After protests in Warren County, North Carolina, the U.S. General Accounting Office (GAO) issued a report on the case in 1983, and the United Church of Christ (UCC) commissioned a report exploring the concept in 1987 drawing a connection between race and the placement of the hazardous waste facilities. Thus, the outcry in Warren County was an important event in spurring minority, grassroots involvement in the environmental justice movement by addressing cases of environmental racism. One activist, Benjamin Chavis, who at the time was the executive director of the Commission for Racial Justice of the United Church of Christ coined the term \"environmental racism\" 1982 in response to the case.\n\nFrom the groundbreaking reports on environmental racism in Warren County, NC, the accumulation of studies and reports on cases of environmental racism and injustices garnered increased public attention in the US, and eventually led to President Bill Clinton's 1994 Executive Order 12898. This was a historical step in addressing environmental injustice on a policy level, especially within a predominantly white-dominated environmentalism movement.\n\nEnvironmental racism was coined in the US but also exists on an international scale between countries in the Global North and Global South, and between different races and ethnicities on different continents. Corporations in the Global North often produce dangerous chemicals banned in the United States and export them to developing countries, or send waste materials to countries with less stringent environmental laws. While the US approach to environmental racism emphasizes race-based discrimination and structures policies to provide equal treatment, the European approach to environmental racism focuses on changing the social conditions that result in inequality. \n\nCost-benefit analysis (CBA) is a process that places a monetary value on costs and benefits to evaluate issues. Environmental CBA aims to provide policy solutions for intangible products such as clean air and water by measuring a consumer's willingness to pay for these goods. CBA contributes to environmental racism through the valuing of environmental resources based on their utility to society. The more someone is willing to pay for a resource such as clean water or air benefits society more than when people are not willing to pay for these goods. This creates a burden on poorer areas, however, by relocating toxic wastes and other environmentally hazardous goods through the justification that they are not willing (or able) to pay as much as a wealthier area for a clean environment. The placement of toxic wastes near poor people lowers the property value of already cheap land. Since the decrease in property value is less than that of a cleaner, wealthier area the monetary benefits to society are greater by dumping the toxic waste in a \"low-value\" area.\n\nChavis defined the term as \"racial discrimination in environmental policy making, the enforcement of regulations and laws, the deliberate targeting of communities of color for toxic waste facilities, the official sanctioning of the life-threatening presence of poisons and pollutants in our communities, and the history of excluding people of color from leadership of the ecology movements.\" Expanding the definition in \"The Legacy of American Apartheid and Environmental Racism,\" Robert Bullard describes the environmental racism \"refers to any policy, practice, or directive that differentially affects or disadvantages (whether intended or unintended) individuals, groups, or communities based on race or color.\"\n\nThere are four unique patterns that help identify and measure environmental injustice. Exposure to hazardous waste, how vulnerable a community is to flooding, how accessible potable water is, and discriminatory waste management processes.\n\nWhile some social scientists view the siting of hazardous facilities in minority communities as a demonstration of intentional racism, others view the causes of environmental racism as structural and institutional.\n\nThere are four factors leading to environmental racism: cheap land, lack of political power, lack of mobility, and finally poverty. First, cheap land is sought for economic reasons but cheap land can also be found due to lack of power in the community to resist the corporation. Second, the lack of political power carried by poor minority groups allow businesses to act with little to no resistance. Politicians and lawmakers are able to better understand communities that have the resources to make their beliefs known, unlike impoverished minority communities. Third, the lack of mobility provides a stream of workers who can be paid minimum wage and thus do not have the funds to leave the now hazardous community. Finally, the overall communities lack of money contributes to all the factors listed above and overall reduces the communities ability to act both physically and politically.\n\nIn the United States, correlation between the sites of hazardous waste facilities and low-income and minority communities was publicly addressed in the 1983 U.S. Government Accountability Office (GAO) report in response to the Warren County, NC protests. Minority communities do not have the financial means, resources, and political representation to oppose hazardous waste sites. They also may depend on the economic opportunities the site brings and are reluctant to oppose its location at the risk of their health. Additionally, controversial projects are less likely to be sited in non-minority areas that are expected to pursue collective action and succeed in opposing the siting of hazardous waste sites and sewage treatment facilities in their area. \n\nProcesses such as suburbanization, gentrification, and decentralization lead to patterns of environmental racism. For example, the process of suburbanization (or white flight) consists of non-minorities leaving industrial zones for safer, cleaner, and less expensive suburban locales. Meanwhile, minority communities are left in the inner cities and in close proximity to polluted industrial zones. In these areas, unemployment is high and businesses are less likely to invest in area improvement, creating poor economic conditions for residents and reinforcing a social formation that reproduces racial inequality. Furthermore, the poverty of property owners and residents in a municipality may be taken into consideration by hazardous waste facility developers since areas with depressed real estate values will cut expenses.\n\nAs a result of the placement of hazardous waste facilities, minority populations experience greater exposure to harmful chemicals and suffer from health outcomes that affect their ability at work and in schools. A comprehensive study of particulate emissions across the United States, published in 2018, found that Blacks were exposed to 54% more particulate matter emissions (soot) than the average American. Faber and Krieg found a correlation between higher air pollution exposure and low performance in schools and found that 92% of children at five Los Angeles public schools with the poorest air quality were of a minority background. School systems for communities heavily populated with minority families tend to provide “unequal educational opportunities” for students of color in comparison to school systems in predominantly white neighborhoods. Pollution consequently presents itself in these communities due to societal factors such as “underfunded schools, income inequality, and myriad egregious denials of institutional support” within the African American community.\nThe Indian Removal Act of 1830 and the Trail of Tears may be considered early examples of environmental racism in the United States. By 1850, all tribes east of the Mississippi had been removed to western lands, essentially confining them to \"lands that were too dry, remote, or barren to attract the attention of settlers and corporations\". Later, during World War II, military facilities were often located conterminous to reservations, leading to a situation in which \"a disproportionate number of the most dangerous military facilities are located near Native American lands\".\n\nMore recently, Native American lands have been used for waste disposal and illegal dumping by the US and multinational corporations. The International Tribunal of Indigenous People and Oppressed Nations, convened in 1992 to examine the history of criminal activity against indigenous groups in the United States, and published a Significant Bill of Particulars outlining grievances indigenous peoples had with the US. This included allegations that the US \"deliberately and systematically permitted, aided, and abetted, solicited and conspired to commit the dumping, transportation, and location of nuclear, toxic, medical, and otherwise hazardous waste materials on Native American territories in North America and has thus created a clear and present danger to the health, safety, and physical and mental well-being of Native American People\".\n\nAn ongoing issue for Native Americans activists is the Dakota Access Pipeline. The pipeline would start in North Dakota and travel to Illinois. Although it does not cross directly on a reservation, the pipeline is under scrutiny because it passes under a section of the Missouri river which is a key water source for Native American tribes including the Standing Rock Sioux Tribe. The pipeline also traverses a sacred burial ground for the Standing Rock Sioux. In 2017, Judge James Boasberg sided with the Standing Rock Sioux Tribe, citing the US Army Corps of Engineers failure to complete a study on the environmental impact of an oil spill in Lake Oahe.\n\nAltgeld Gardens is a 6,000 unit public housing community located in south Chicago that was built in 1945 on an abandoned landfill to accommodate returning African-American World War II veterans. Surrounded by 53 toxic facilities and 90% of the city's landfills, the Altgeld Gardens area became known as a \"toxic doughnut.\" In Altgeld Gardens, 90% of its population are African-American and 65% are below the poverty level. The known toxins and pollutants affecting the Altgeld Gardens area include mercury, ammonia gas, lead, dichlorodiphenyltrichloroethane (DDT), polychlorinated biphenyls (PCBs), polycyclic aromatic hydrocarbons (PAHs), heavy metals, and xylene.\n\nIn 1984, a study by Illinois Public Health Sector revealed excessive rates of prostate, bladder, and lung cancer. Additionally, as reported in the Organization for Economic Co-operation and Development's seminar on social and environment interface, medical records have indicated (1) high rates of children born with brain tumors, (2) high rates of fetuses that had to be aborted after tests revealed that the brains were developing outside the skull, and (3) higher rates of asthma, ringworm, and other ailments. Despite evidence of health problems, the residents of Altgeld Gardens have not been relocated to another public housing project.\n\nIn Chicago’s Latinx neighborhoods like Little Village, the array coal plants were contributors to respiratory diseases and other health complications during the early twenty-first century. In addition to air pollution, Little Village lacked safe outdoor recreational areas yet housed a County Jail that occupied 96 acres. Despite widespread displeasure among community members, the fact that Latinx regions were primarily populated by working class citizens caused the demand for environmental and community improvement to inevitably come with joint fear of gentrification among activists. Some advocates still fought for environmental improvements regardless of their fear, and when their requests began to come to fruition, like the eventual increase in local green spaces, many residents were left feeling out of place in their homes, which could be attributed to shifts in factors like local police presences, local racial diversity, and overall class of the townsfolk.\n\nRacism and environmental justice unified for the first time during the 1983 citizen opposition to a proposed PCB landfill in Warren County, North Carolina. Illegally, North Carolina state officials decided to bury soil contaminated with toxic polychlorinated biphenyls in Afton, a small town in Warren County. As a result, between June 1978 and August 1978, 30,000 gallons (114 m³) of polychlorinated biphenyls (PCB)-contaminated waste were illegally deposited along 210 miles of North Carolina roads. The U.S. Environmental Protection Agency (EPA) declared the PCBs a threat to public health and required the state to remove the polluted waste. In 1979, the North Carolina Department of Environment and Natural Resources and EPA Region 4 selected Warren County as the site to deposit the PCB-contaminated soil that was collected from the roadsides. Warren County is one of the six counties along the \"black belt\" of North Carolina. The counties residing in the \"black belt\" are significantly poorer than the rest of the state. In the early 1980s the residents in Warren County earned an average per capita income of $6,984 compared to $9,283 for the rest of the state. In 1980, the population of Warren County was 54.5% African-American. \n\nIn 1982, the local National Association for the Advancement of Colored People (NAACP) filed a lawsuit in the district courts to block the landfill. The residents lost the case in court. In September 1982, the outraged citizens of Warren County joined by civil rights groups, environmental leaders, and clergymen protested the first truckloads of PCB contaminated soil. During the protest, over 500 people were arrested and jailed. Despite protests and scientific evidence that the plan would cause drinking water contamination, the Warren County PCB Landfill was built and the toxic waste was placed in the landfill. After nearly two decades of suspected leaks, state and federal sources paid a contractor $18 million to detoxify the PCB contaminated soil in Warren County. Warren County is often cited as the first environmental justice case in the United States; however, this movement started years earlier in 1978 with the discovery of toxic waste in Love Canal, New York.\nOn November 19, 1984, the San Juanico disaster caused thousands of deaths and roughly a million injuries to poor surrounding neighborhoods. The disaster occurred at the PEMEX liquid propane gas plant in a densely populated area of Mexico City. The close proximity of illegally built houses that did not meet regulations worsened the effects of the explosion.\n\nIn 1989, the Louisiana Energy Services (LES), a British, German and American conglomerate, conducted a nationwide search to find the \"best\" site to build a privately owned uranium enrichment plant. The LES claimed to use an objective scientific method to select Louisiana as the \"best\" place to build the plant. In response to the selection, the communities of Homer, Forest Grove and Center Springs that are nearby the proposed site formed a group called Citizens against Nuclear Trash (CANT). With the help of the Sierra Club Legal Defense Fund (later changed to Earth Justice Legal Defense Fund), CANT sued LES for practicing environmental racism. Finally after 8 years, on May 1, 1997, a three-judge panel of the Nuclear Regulatory Commission's Atomic Safety and Licensing Board made their final initial decision. The panel found that racial bias did play a role in the selection process. In response to the victory, on May 11, 1997, the London Times declared, \"Louisiana Blacks Win Nuclear War.\" The courts decision was also upheld on appeal on April 4, 1998.\n\nSan Antonio's Kelly Air Force Base (KAFB) is one of the Air Force's major aircraft maintenance facilities and takes up 4000 acres of land, surrounded by residential neighborhoods of primarily Hispanic populations. KAFB maintains various parts of aircraft such as jet engines, and accessory components and even nuclear materials, generating as much as 282,000 tons of hazardous waste each year. Residents of the nearby communities have complained many times of unusual illnesses their children have experienced as well as respiratory illnesses and kidney disease. A 1997 survey done in the residential neighborhoods close to KAFB showed 91% of adults and 79% of children are suffering from conditions ranging from nose, ear, and throat issues to central nervous system disorders. Scientists released information in 1983 revealing that toxic waste had been dumped into an uncovered pit from 1960 to 1973. The waste in the pit contained various chemicals, such as PCB's and DDT, that contaminated groundwater.\n\nAt the time of Hurricane Katrina, 60.5% of New Orleans residents were African American. Pre-existing racial disparities in wealth within New Orleans worsened the outcome of Hurricane Katrina for minority populations. Institutionalized racial segregation of neighborhoods left minority members more likely to live in low-lying areas that were more vulnerable to flooding. Additionally, hurricane evacuation plans relied heavily on the use of cars and did not prepare for people who relied on public transportation. Because minority populations are less likely to own cars, some people had no choice but to stay behind, while white majority communities were able to escape. A report commissioned by the U.S. House of Representatives found that political leaders failed to consider the fact that \"100,000 city residents had no cars and relied on public transit\", and the city's failure to complete its mandatory evacuation led to hundreds of deaths. \n\nIn the months following the disaster, political, religious, and civil rights groups, celebrities, and New Orleans residents spoke out against what they believed was racism on the part of the United States government. After the hurricane, in a meeting held between the Congressional Black Caucus, the National Urban League, the Black Leadership Forum, the National Council of Negro Women, and the NAACP, Black leaders criticized the response of the federal government calling it \"slow and incomplete\" and discussed the role of race in this response. With rising sea levels, lack of mobility of non-white populations in coastal cities like New Orleans foreshadow future unequal impacts of climate change and natural disasters on minority communities.\n\nSince April 2014, residents of Flint, a city that is almost 57 percent black and notably impoverished, have been drinking and bathing in water that contains enough lead to meet the Environmental Protection Agency's definition of \"toxic waste\". Before 2014 when the city of Flint switched to their own river as means of water, Lake Huron provided the area with water. Researchers at Virginia Tech discovered in 2015 that the Flint River is 19 times more corrosive than Lake Huron. Lead contamination can engender multiple health conditions. A November 2015 class-action lawsuit describes how Michigan's Department of Environmental Quality (MDEQ) failed to treat the new water source with an anti-corrosive agent, thereby causing the water to become increasingly discolored. This was in violation of the Lead and Copper Rule and MDEQ did not correctly complete the Safe Drinking Water Act mandated lead assessments. Adding that agent (orthophosphate) would have cost $100 per day, according to CNN, and 90 percent of the problems with Flint's water would have been averted if it had been used.\n\nGenerally the consumption of Lead is considered among the environmental problems and some of the ways people can be exposed to it is from the corrosion of old pipes, the dust from lead-based paint and gasoline has metal dust which contains lead; but the amount of lead in gasoline has been reduced and this contributed a lot when it comes to Lead exposure.\n\nAfter an official investigation was conducted, Michigan's attorney general Bill Schuette initially filed charges against three government officials: two state officials of the Michigan Department of Environmental Quality, Michael Prysby and Stephen Busch, and a Flint city employee, Michael Glasglow, who was the city's water quality supervisor. They were brought up against felony charges such as \"misconduct, neglect of duty, and conspiracy to tamper with evidence.\" They were also charged with violating the Michigan Safe Water Drinking Act.\nChester, Pennsylvania, provides an example of \"social, political, and economic forces that shape the disproportionate distribution of environmental hazards in poor communities of color.\" Chester is located in Delaware County, an area with a population of 500,000 that, excluding Chester, is 91% white. Chester, however, is 65% African American, with the highest minority population and poverty rate in Delaware County, and recipient of a disproportionate amount of environmental risks and hazards. Chester has five large waste facilities including a trash incinerator, a medical waste incinerator, and a sewage treatment plant. These waste sites in Chester have a total permitted capacity of 2 million tons of waste per year while the rest of Delaware County has a capacity of merely 1,400 tons per year. One of the waste sites located in Chester is the Westinghouse incinerator, which burns all of the municipal waste from the entire county and surrounding states. These numerous waste facilities engender very significant health risks to the citizens of Chester, as the cancer rate in this area is 2.5 times higher than it is anywhere else in Pennsylvania. The mortality rate is 40% higher than the rest of Delaware county.\nDiamond, a small African American community, filed a lawsuit against Shell gas company after years of experiencing toxic emissions from the neighboring refinery. Shell offered to buy out the homes that the residents owned, however, the property value was so low that residents could not get new housing. Eventually after protesting and making the issue a public matter, Shell eventually agreed to relocate the residents (Lerner, 2005).\n\nNorth Carolina is home to 31 coal ash pits that store an expected 111 million tons of harmful waste created by coal-fired power plants. It is also home to many excrement pits, referred to indirectly as \"lagoons,\" that store roughly 10 billion pounds of wet waste created every year by swine, poultry, and dairy cattle in the state. North Carolina's mechanical hog tasks are firmly grouped in a couple of districts on the beach front plain that housed the most subjugated individuals preceding the Civil War. In the decades since, the area has held the state's densest populace of provincial African-American residents.\n\nWilmington, NC is usually one of the first cities hit by hurricanes off the Atlantic coast, and its environmental risks are increased by its proximity to hog farms, nuclear reactors, and coal-ash pits—one of which has already spilled over, due to Hurricane Florence in September 2018. Hog waste spills can be destructive to the individuals who live close to these pits and farms and a significant number of the neighbors are low-income ethnic minorities. African Americans have been battling for their justice in the port city. This can be traced to the Wilmington Rebellion of 1898, when whites stripped away black individuals' rights to cast a ballot and hold office through the power of force, in spite of the significant role African Americans play in building the greater part of the city's monuments. In 1971, racial strains over the absence of protection for African Americans in the threatening integration endeavors prompted a mob and resulted in the capture of several black activists who would later be known as the \"Wilmington Ten.\" One of those activists, Benjamin Chavis, would later turn into a significant figure in the environmental justice movement. Two studies of disease transmission analysts conducted at the University of North Carolina at Sanctuary Slope distributed a paper in 2014 titled: \"Industrial Hog Operations in North Carolina Excessively Effect African-Americans, Hispanics and Native Americans.\" They expressed, \"Flood of waste pits amid overwhelming precipitation occasions results in gigantic spills of animal waste into neighboring networks and conduits.\"\n\nExporting toxic wastes to countries in the Global South is one form of environmental racism that occurs on an international basis. In one alleged instance, the French aircraft carrier Clemenceau was prohibited from entering Alang, an Indian ship-breaking yard, due to a lack of clear documentation about its toxic contents. French President Jacques Chirac ultimately ordered the carrier, which contained tons of hazardous materials including asbestos and PCBs, to return to France.\n\nFrom the mid-1990s until about 2001, it is estimated that some 50 to 80 percent of the electronics collected for recycling in the western half of the United States was being exported for dismantling overseas, predominantly to China and Southeast Asia. This scrap processing is quite profitable and preferred due to an abundant workforce, cheap labour, and lax environmental laws.\n\nGuiyu, China is one of the largest recycling sites for e-waste, where heaps of discarded computer parts rise near the riverbanks and compounds, such as cadmium, copper, lead, PBDEs, contaminate the local water supply. Water samples taken by the Basel Action Network in 2001 from the Lianjiang River contained lead levels 190 times higher than WHO safety standards. Despite contaminated drinking water, residents continue to use contaminated water over expensive trucked-in supplies of drinking water. Nearly 80 percent of children in the e-waste hub of Guiyu, China, suffer from lead poisoning, according to recent reports. Before being used as the destination of electronic waste, most of Guiyu was composed of small farmers who made their living in the agriculture business. However, farming has been abandoned for more lucrative work in scrap electronics. \"According to the Western press and both Chinese university and NGO researchers, conditions in these workers' rural villages are so poor that even the primitive electronic scrap industry in Guiyu offers an improvement in income\".\n\nUnion Carbide Corporation, is the parent company of Union Carbide India Limited which outsources its production to an outside country. Located in Bhopal, India, Union Carbide India Limited primarily produced the chemical methyl isocyanate used for pesticide manufacture. On December 3, 1984, a cloud of methyl isocyanate leaked as a result of the toxic chemical mixing with water in the plant in Bhopal. Approximately 520,000 people were exposed to the toxic chemical immediately after the leak. Within the first 3 days after the leak an estimated 8,000 people living within the vicinity of the plant died from exposure to the methyl isocyanate. Some people survived the initial leak from the factory, but due to improper care and improper diagnoses many have died. As a consequence of improper diagnoses, treatment may have been ineffective and this was precipitated by Union Carbide refusing to release all the details regarding the leaked gases and lying about certain important information. The delay in supplying medical aid to the victims of the chemical leak made the situation for the survivors even worse. Many today are still experiencing the negative health impacts of the methyl isocyanate leak, such as lung fibrosis, impaired vision, tuberculosis, neurological disorders, and severe body pains.\n\nThe operations and maintenance of the factory in Bhopal contributed to the hazardous chemical leak. The storage of huge volumes of methyl isocyanate in a densely inhabited area, was in contravention with company policies strictly practiced in other plants. The company ignored protests that they were holding too much of the dangerous chemical for one plant and built large tanks to hold it in a crowded community. Methyl isocyanate must be stored at extremely low temperatures, but the company cut expenses to the air conditioning system leading to less than optimal conditions for the chemical. Additionally, Union Carbide India Limited never created disaster management plans for the surrounding community around the factory in the event of a leak or spill. State authorities were in the pocket of the company and therefore did not pay attention to company practices or implementation of the law. The company also cut down on preventative maintenance staff to save money. \n\nDue to their lack of environmental laws, emerging countries like Ecuador have been subjected to environmental pollution, sometimes causing health problems, loss of agriculture, and poverty. In 1993, 30,000 Ecuadorians, which included Cofan, Siona, Huaorani, and Quichua indigenous people, filed a lawsuit against Texaco oil company for the environmental damages caused by oil extraction activities in the Lago Agrio oil field. After handing control of the oil fields to an Ecuadorian oil company, Texaco did not properly dispose of its hazardous waste, causing great damages to the ecosystem and crippling communities.\n\nIn Nigeria, near the Niger Delta, cases of oil spills, burning of toxic waste, and urban air pollution are problems in more developed areas. In the early 1990s, Nigeria was among the 50 nations with the world's highest levels of carbon dioxide emissions, which totaled 96,500 kilotons, a per capita level of 0.84 metric tons. The UN reported in 2008 that carbon dioxide emissions in Nigeria totaled 95,194 kilotons.\n\nNumerous webpages were created in support of the Ogoni people, who are indigenous to Nigeria's oil-rich Delta region. Sites were used to protest the disastrous environmental and economic effects of Shell Oil drilling, to urge the boycotting of Shell Oil, and to denounce human rights abuses by the Nigerian government and by Shell. The use of the Internet in formulating an international appeal intensified dramatically after the Nigerian government's November 1995 execution of nine Ogoni activists, including Ken Saro-Wiwa, who was one of the founders of the nonviolent Movement for the Survival of the Ogoni People (MOSOP).\n\nEnvironmental Racism can be traced back around 500 years with the arrival of the Europeans and their displacement of Native Americans.  The Environmental Justice Movement, however, seems to be fairly recent having been rooted around the same time as the Civil Rights Movement.  The Civil Rights Movement influenced the mobilization of people by echoing the empowerment and concern associated with political action.  Here is where the civil rights agenda and the environmental agenda met. Despite this being the case, environmental organizations such as Sierra Club did distance themselves from cases such as the Warren County case likely because of their unwillingness to risk technical support when dealing with a very social issue.\n\nActivists have called for \"more participatory and citizen-centered conceptions of justice.\" The environmental justice (EJ) movement and climate justice (CJ) movement address environmental racism in bringing attention and enacting change so that marginalized populations are not disproportionately vulnerable to climate change and pollution. In the US, change must be made at the federal level, and enacted upon after being passed. This requires not only state and local agencies, but also the involvement of grassroots organizations. According to the United Nations Conference on Environment and Development, one possible solution is the precautionary principle, which states that \"where there are threats of serious or irreversible damage, lack of full scientific certainty shall not be used as a reason for postponing cost-effective measures to prevent environmental degradation.\" Under this principle, the initiator of the potentially hazardous activity is charged with demonstrating the activity's safety. Environmental justice activists also emphasize the need for waste reduction in general, which would act to reduce the overall burden.\n\nConcentrations of ethnic or racial minorities may also foster solidarity, lending support in spite of challenges and providing the concentration of social capital necessary for grassroots activism. Citizens who are tired of being subjected to the dangers of pollution in their communities have been confronting the power structures through organized protest, legal actions, marches, civil disobedience, and other activities. \n\nRacial minorities are often excluded from politics and urban planning (such as sea-level rise adaptation planning) so various perspectives of an issue are not included in policy making that may affect these excluded groups in the future. In general, political participation in African American communities is correlated with the reduction of health risks and mortality. Other strategies in battling against large companies include public hearings, the elections of supporters to state and local offices, meetings with company representatives, and other efforts to bring about public awareness and accountability. \n\nIn addressing this global issue, activists take to various social media platforms to both raise awareness and call to action.  The mobilization and communication between the intersectional grassroots movements where race and environmental imbalance meet has proven to be effective. The movement gained traction with the help of Twitter, Facebook, Instagram, and Snapchat among other platforms.  Celebrities such as Shailene Woodley, who advocated against the Keystone XL Pipeline, have shared their experiences including that of being arrested for protesting. Social media has allowed for a facilitated conversation between peers and the rest of the world when it comes to social justice issues not only online but in face-to-face interactions correspondingly. \n\nStudies have been important in drawing associations and public attention by exposing practices that cause marginalized communities to be more vulnerable to environmental health hazards. The US GAO study in response to the 1982 protests against the PCB landfill in Warren County was among the first groundbreaking studies that drew correlations between the racial and economic background of communities and the location of hazardous waste facilities. Their study, \"Siting of Hazardous Waste Landfills and Their Correlation with Racial and Economic Status of Surrounding Communities,\" revealed that \"three of the four commercial hazardous waste landfills in the Southeast United States were located in majority black communities.\" However, the study was limited in scope by only focusing on off-site hazardous waste landfills in the Southeastern United States. In response to this limitation the United Church of Christ Commission for Racial Justice, or CRJ, directed a comprehensive national study on demographic patterns associated with the location of hazardous waste sites. The CRJ national study conducted two examinations of areas surrounding commercial hazardous waste facilities and the location of uncontrolled toxic waste sites. The first study examined the association between race and socio-economic status and the location of commercial hazardous waste treatment, storage, and disposal facilities. After statistical analysis, the first study concluded that \"the percentage of community residents that belonged to a racial or ethnic group was a stronger predictor of the level of commercial hazardous waste activity than was household income, the value of the homes, the number of uncontrolled waste sites, or the estimated amount of hazardous wastes generated by industry\". The second study examined the presence of uncontrolled toxic waste sites in ethnic and racial minority communities, and found that 3 out of every 5 African and Hispanic Americans lived in communities with uncontrolled waste sites. Other studies like the 1987 \"Toxic Waste and Race in the United States\" by the Commission for Racial Justice found race to be the most influential variable in predicting where waste facilities were located. \n\nDeserting the Perpetrator - Victim Model of studying environmental justice issues, the Economic/Environmental Justice Model utilized a sharper lens to study the many complex factors, accompanied to race, that contributes to the act of environmental racism and injustice. Using this model the role of history and the overlapping of interest groups, stakeholders, and organizations are considered in case studies of environmental racism. For example, Lerner in \"Diamond: A struggle for Environmental Justice in Louisiana's Chemical Corridor\" not only revealed the role of race in the division of Diamond and Norco residents, but he also revealed the historical roles of the Shell Oil Company, the slave ancestry of Diamond residents, and of the history of white workers and families that were dependent upon the rewards of Shell. Involvement of outside organizations, such as the Bucket Brigade and Greenpeace, was also considered in the power that the Diamond community had when battling for environmental justice.\n\nIn wartimes, environmental racism can occur and are unearthed to the public through reports. Examining the Israeli-Palestine conflict, Friends of the Earth International's Environmental Nakba report brings attention to environmental racism that has occurred in the Gaza Strip. Some Israeli practices include cutting off three days of water supply to refugee Palestinians and destructing farms. \n\nBesides studies that point out cases of environmental racism, studies have also provided information on how to go about changing regulations and preventing environmental racism from happening. In a study by Daum, Stoler and Grant on e-waste management in Accra, Ghana, the importance of engaging with different fields and organizations such as recycling firms, communities, and scrap metal traders, to name a few are emphasized over adaptation strategies such as bans on burning and buy-back schemes that have not caused much effect on changing practices.\n\nStudies have also shown that since environmental laws have become prominent in the U.S. as well as Europe so companies have moved their waste towards the global south. The Third World has less of a focus on environmental concerns and therefore are susceptible to more discriminatory practices. This has not stopped activism however it has limited the effects activism has on political restrictions. As these activists push on there are still companies destroying land in these countries with harmful chemicals that are cheaper to use. \n\nManifestations of environmental racism predate the coining of such terminology. Before the 1970s, communities of color recognized this reality and organized against it. For example, the Black Panther Party organized survival programs that confronted the inequitable distribution of trash in predominantly black neighborhoods. Similarly, the Young Lords, a Puerto Rican revolutionary nationalist organization based in Chicago and New York City, protested pollution and toxic refuse present in their community via their Garbage Offensive program. These and other organizations also worked to confront the maldistribution of open spaces, toxic lead paint, and healthy food options. They also offered health programs to those affected by preventable, environmentally induced diseases such as tuberculosis.\nIn this way, these organizations serve as precursors to more pointed movements against environmental racism.\n\nMartin Luther King Jr. helped to bring light to the injustices done to many low-income neighborhoods and the working conditions of African-Americans. In the year before his assassination, Martin Luther King Jr. was in the midst of organizing a protest in Washington to create a bill to help the poor and homeless in the United States. After his assassination  and even with the push of the Southern Christian Leadership Conference (SCLC), this bill would never come to pass. Latino ranch laborers composed by Cesar Chavez battled for working environment rights, including insurance from harmful pesticides in the homestead fields of California's San Joaquin Valley. In 1967, African-American understudies rioted in the streets of Houston to battle a city trash dump in their locale which had killed two kids. In 1968, occupants of West Harlem, in New York City, battled unsuccessfully against the siting of a sewage treatment plant in their neighborhood. \n\nOne approach in activism is promoting the development and manufacturing of renewable energy sources and the integration of health, economic and preparedness issues into climate policies. However, despite President Bill Clinton’s executive order 12898, there remains differences between policy and action that advocacy groups continue to address.\n\nWith time environmental justice and civil rights movements fused together and as a result environmental justice organizations stood up for more ethnic groups and this increased diversity within the organization. The fusion is very logical since the people who suffer the most are minority groups. One of the main environmental problems that minority groups suffer from is uncontrolled toxic wastes. The factor that initiated environmental justice is the relationship between the geography of minority groups and hazardous waste landfills.\n\nWhen environmental racism became acknowledged in the US society, it stimulated the environmental justice social movement that gained wave throughout the 1970s and 1980s in the US. Historically, the term environmental racism has had ties with the environmental justice movement.  However, this has changed with time to the extent it is believed to lack any associations with the movement. Grassroots organizations and campaigns have sprung up in response to this environmental racism with these groups mainly demanding the inclusion of minorities when it comes to policy making involving the environment. It is also worth noting that this concept is international despite being coined in the US. A perfect example is when the United States exported its hazardous wastes to the poor nations in the Global South because they knew that these countries had lax environmental regulations and safety practices. Marginalized communities are usually at risk of environmental racism because they resource and means to oppose the large companies that dump these dangerous wastes.  As already stated, environmental racism is international, implying that it not only occurs in the United States. \n\nThe export of hazardous waste to third world countries is another growing concern. Between 1989 and 1994, an estimated 2,611 metric tons of hazardous waste was exported from Organization for Economic Cooperation and Development (OECD) countries to non-OECD countries. Two international agreements were passed in response to the growing exportation of hazardous waste into their borders. The Organization of African Unity (OAU) was concerned that the Basel Convention adopted in March 1989 did not include a total ban on the trans-boundary movement on hazardous waste. In response to their concerns, on January 30, 1991, the Pan-African Conference on Environmental and Sustainable Development adopted the Bamako Convention banning the import of all hazardous waste into Africa and limiting their movement within the continent. In September 1995, the G-77 nations helped amend the Basel Convention to ban the export of all hazardous waste from industrial countries (mainly OECD countries and Lichtenstein) to other countries.\n\nWith globalization and the increase in transnational agreements, introduce possibilities for cases of environmental racism. For example, the 1994 North American Free Trade Agreement (NAFTA) attracted US-owned factories to Mexico, where toxic waste was abandoned in the Colonia Chilpancingo community and was not cleaned up until activists called for the Mexican government to clean up the waste.\n\nIn the US, the environmental justice movement uses the Civil Rights Act (CRA) of 1964 to combat environmental racism in legal cases. For example, the CRA was used in the 1994 lawsuit against the Los Angeles County Metropolitan Transit Authority which failed to provide services for poor LA County residents. In Canada, progress is being made to address environmental racism (especially in Nova Scotia's Africville community) with the passing of Bill 111, An Act to Address Environmental Racism in the Nova Scotia Legislature. \n\nIn response to fatal diesel pollution in the air around ports in Los Angeles and Long Beach in 2006, the San Pedro Bay Ports Clean Air Action Plan, or CAAP, was passed. The action plan was created to reduce pollution caused by ports; specifically, it demanded a 45% decrease in pollution once the proposal was put into action. Another layer of the plan’s objective was to reduce negative environmental impacts from trucks with the initial plans for the Clean Truck Program (CTP), which intended to cut back on the use of shipping trucks from the docks and instead called for purer options like rail yards and warehouses that could hopefully improve the air quality. \n\n"}
{"id": "24903185", "url": "https://en.wikipedia.org/wiki?curid=24903185", "title": "Erin Sharma", "text": "Erin Sharma\n\nErin J. Sharma (née Donald, born April 24, 1976) is a former correction officer for the United States Federal Bureau of Prisons. She was sentenced to life in federal prison in 2009 for causing the beating death of an inmate at the maximum security unit of the Coleman Federal Correctional Complex near Coleman, Florida. Prosecutors said that after inmate Richard Delano grabbed her arm through a food slot and bruised it, she and another guard (later revealed to be her supervisor) arranged for him to be assigned to share a cell with a notoriously violent inmate, knowing Delano would be harmed.\n\nSharma was the daughter of a Army soldier, and moved around frequently during her childhood. She graduated high school a year early, and began working as a corrections officer in Washington in 1997. She married Rajesh \"Roger\" Sharma, a fellow corrections officer, in 1999. They moved together to Edgefield, South Carolina; where Erin got a job as a corrections officer at FCI Edgefield. When they gave birth to a daughter, they decided to move to Florida to be closer to Erin's mother.\n\nOn February 3, 2005, Richard Allen Delano, a convicted methamphetamine dealer, grabbed Sharma's arm through a food slot, bruising it. She then said to him, \"You're a dead man\". She received minor first aid treatment and was sent home for the rest of the day.\nOn March 1, Delano, who had a reputation as a snitch,\nwas transferred into a cell with John Javilo \"Animal\" McCullah, a convicted murderer who had assaulted all of his previous cellmates. Prior to the transfer, witnesses overheard Sharma encouraging McCullah to attack Delano, but to do so on a day when she was not at work.\n\nOn March 4, while Sharma was on a three-day vacation, Delano was beaten into a coma. He died 13 days later.\nSharma was charged with two felony counts of violating Delano's civil rights under color of law for conspiring to have him killed and with violating his Constitutional right not to be subjected to cruel and unusual punishment.\n\nAt the federal trial in Orlando, the prosecution showed that Sharma and an alleged corrections officer co-conspirator knew that McCullah, who had been placed on special single-cell hold status, was likely to assault Delano.\nIn his opening statement, federal prosecutor Douglas Kern told the jury that putting Delano into a cell with McCullah \"was like putting a sheep in a cage with a wolf\". Kern described McCullah as a \"hugely violent, racist gang member inmate\" who was a member of the Aryan Brotherhood prison gang. He described Sharma as the \"puppet master,\" and McCullah as her puppet.\n\nFBI Agent James Raby testified about three unrecorded interviews conducted with Sharma following the assault. Raby said Sharma had acknowledged knowing Delano's reputation as a snitch, knowing that McCullah was notoriously violent, and knowing that the shared cell arrangement would \"invariably\" lead to an assault. He said Sharma had expressed regret about the assault. Various witnesses described Sharma as a vengeful officer who played favorites and hated snitches.\n\nWhen her own turn to testify came, Sharma denied having made the statements that Raby testified to, and denied telling him that she felt \"haunted\" by the assault. She said her threats were clearly jokes, and that she \"treated those inmates almost like they were my kids.\"\n\nOn July 29, 2009, the jury found Sharma guilty of both charges.\n\nOn October 26, 2009, Senior U.S. District Judge Patricia C. Fawsett sentenced the 33-year-old Sharma to life in federal prison. Fawsett said Delano's death was \"an expectable result\" of the transfer, and that she \"took advantage of her superior position\" to gain revenge on Delano. Therefore, Delano's death amounted to second-degree murder. Fawsett also found that Sharma had perjured herself on the witness stand when she disputed Raby's testimony, saying that it ran counter to \"the overwhelming evidence\" of Sharma's involvement in the crime. The perjury finding qualified Sharma for a two-level obstruction of justice upward departure from the federal sentencing guidelines. Sharma's attorney said he would file an appeal. On August 24, 2010, the Eleventh Circuit of the US Court of Appeals affirmed Sharma's convictions and life sentence.\n\nThe location where Sharma is serving her sentence is something of a mystery. According to the Federal Bureau of Prisons website, Sharma is assigned to the Residential Reentry Management Field Office in Kansas City, Missouri; she was previously assigned to the RRM field office in Sacramento, California. However, RRMs are not prison facilities, but are merely an administrative designation for inmates assigned to home-confinement, \"halfway-houses\", or state and county correctional facilities. Since she is serving a life sentence, this designation almost certainly means that she was originally held in a California state prison before being transferred to a Missouri state prison. However, a search of the Missouri Department of Corrections inmate database does not reveal anyone being held under her name, and an earlier search of the California Department of Corrections database did not turn up anyone under her name either. This suggests that she is being held under an alias, which would not be unusual for a former correctional officer.\n\nFollowing the fatal assault, McCullah was temporarily transferred to the United States Penitentiary, Florence ADX, the federal supermax prison in Colorado. After this he was transferred to the Federal Correctional Institution, Victorville, a medium security facility in California. Currently he is serving his sentence in the Special Management Unit at the United States Penitentiary, Lewisburg Pennsylvania.\n\nDuring Sharma's trial, Raby testified that Sharma's supervisor, Michael Kennedy, proposed that Delano be transferred into a cell with McCullah, believing that McCullah would give Delano \"a good [butt] kicking and head-knocking\".\nOn November 2, 2009, the U.S. Justice Department announced that a grand jury had indicted Kennedy on one count of conspiring to violate Delano's federal civil rights and one count of violating his civil rights by arranging for another inmate to assault him.\n\nOn July 8, 2010, Kennedy was found guilty of both civil rights counts. The jury did not reach a unanimous finding that the violations resulted in Delano's death. Kennedy was sentenced to nine years in federal prison.\n"}
{"id": "3182668", "url": "https://en.wikipedia.org/wiki?curid=3182668", "title": "Existential crisis", "text": "Existential crisis\n\nAn existential crisis is a moment at which an individual questions if their life has meaning, purpose, or value. It may be commonly, but not necessarily, tied to depression or inevitably negative speculations on purpose in life (e.g., \"if one day I will be forgotten, what is the point of all of my work?\"). This issue of the meaning and purpose of human existence is a major focus of the philosophical tradition of existentialism.\n\nAn existential crisis may result from, be misdiagnosed as, or be combined with:\n\nAn existential crisis is often provoked by a significant event in the person's life—psychological trauma, marriage, separation, major loss, the death of a loved one, a life-threatening experience, a new love partner, psychoactive drug use, adult children leaving home, reaching a personally significant age (turning 16, turning 40, etc.), etc. Usually, it provokes the sufferer's introspection about personal mortality, thus revealing the psychological repression of said awareness.\n\nAn existential crisis may resemble anomie (a personal condition resulting from a lack of norms) or a midlife crisis. An existential crisis may stem from one's new perception of life and existence. Analogously, existentialism posits that a person \"can and does\" define the meaning and purpose of his or her life, and therefore must \"choose\" to resolve the crisis of existence.\n\nIn existentialist philosophy, the term 'existential crisis' specifically relates to the crisis of the individual when they realize that they must always define their own lives through the choices they make. The existential crisis occurs when one recognizes that even the decision to either refrain from action or withhold assent to a particular choice is, in itself, a choice. In other words, humankind is \"condemned\" to freedom.\n\nExistential crisis has often been negatively associated with depression.\n\nPeter Wessel Zapffe, a Norwegian philosopher and adherent of nihilism and antinatalism, asserted in his book, \"The Last Messiah\", four ways that he believed all self-conscious beings use in order to cope with their apprehension of indifference and absurdity in existence, comprising \"anchoring\", \"isolation\", \"distraction\", and \"sublimation\":\n\n\nIn the 19th century, Kierkegaard considered that angst and existential despair would appear when an inherited or borrowed world-view (often of a collective nature) proved unable to handle unexpected and extreme life-experiences. Nietzsche extended his views to suggest that the Death of God—the loss of collective faith in religion and traditional morality—created a more widespread existential crisis for the philosophically aware.\n\nExistential crisis has indeed been seen as the inevitable accompaniment of modernism (1890–1945). Whereas Durkheim saw individual crises as the byproduct of social pathology and a (partial) lack of collective norms, others have seen existentialism as arising more broadly from the modernist crisis of the loss of meaning throughout the modern world. Its twin answers were either a religion revivified by the experience of anomie (as with Martin Buber), or an individualistic existentialism based on facing directly the absurd contingency of human fate within a meaningless and alien universe, as with Sartre and Camus.\n\nIrvin Yalom, an emeritus professor of psychiatry at Stanford University has made fundamental contributions to the field of existential psychotherapy. Rollo May is another of the founders of this approach.\n\nFredric Jameson has suggested that postmodernism, with its saturation of social space by a visual consumer culture, has replaced the modernist angst of the traditional subject, and with it the existential crisis of old, by a new social pathology of flattened affect and a fragmented subject.\n\n\n"}
{"id": "43834296", "url": "https://en.wikipedia.org/wiki?curid=43834296", "title": "Extension of a topological group", "text": "Extension of a topological group\n\nIn mathematics, more specifically in topological groups, an extension of topological groups, or a topological extension, is a short exact sequence formula_1 where formula_2 and formula_3 are topological groups and formula_4 and formula_5 are continuous homomorphisms which are also open onto their images. Every extension of topological groups is therefore a group extension.\n\nWe say that the topological extensions \nand \nare equivalent (or congruent) if there exists a topological isomorphism formula_8 making commutative the diagram of Figure 1.\n\nWe say that the topological extension\n\nis a \"split extension\" (or splits) if it is equivalent to the trivial extension\n\nwhere formula_11 is the natural inclusion over the first factor and formula_12 is the natural projection over the second factor.\n\nIt is easy to prove that the topological extension formula_6 splits if and only if there is a continuous homomorphism formula_14 such that formula_15 is the identity map on formula_16\n\nNote that the topological extension formula_6 splits if and only if the subgroup formula_18 is a topological direct summand of formula_19\n\n\nAn extension of topological abelian groups will be a short exact sequence formula_1 where formula_2 and formula_3 are locally compact abelian groups and formula_4 and formula_5 are relatively open continuous homomorphisms.\n"}
{"id": "39902845", "url": "https://en.wikipedia.org/wiki?curid=39902845", "title": "Fairview (surveillance program)", "text": "Fairview (surveillance program)\n\nFairview is a secret program under which the National Security Agency cooperates with the American telecommunications company AT&T in order to collect phone, internet and e-mail data mainly of foreign countries' citizens at major cable landing stations and switching stations inside the United States. The FAIRVIEW program started in 1985, one year after the Bell breakup.\n\nThe \"key corporate partner\" for cooperation under the FAIRVIEW program was first identified on October 23, 2013 by \"The Washington Post\"—quoting NSA historian Matthew Aid—as\nAT&T.\n\nThis was confirmed by a joint report by \"Pro Publica\" and \"The New York Times\" from August 15, 2015, based upon NSA documents that describe the company as \"highly collaborative\" and praise the company's \"extreme willingness to help\".\n\nIn 2011, the FAIRVIEW program cost the NSA $188.9 million, which was twice as much as the costs for STORMBREW, which is the second-largest program.\n\nAccording to 2013 revelations by whistleblower Edward Snowden:\n\nThe NSA partners with a large US telecommunications company ... [which] partners with telecoms in the foreign countries, [which] then allow the US company access to those countries' telecommunications systems, and that access is then exploited to direct traffic to the NSA's repositories.\n\nAccording to the revelations, the NSA had collected 2.3 billion separate pieces of data from Brazilian users in January 2013 alone.\n\nSeveral weeks earlier, Snowden had revealed that the NSA was also harvesting the telephone metadata and text messages from over a billion subscribers in China; however, no precise program name was reported at the time.\nA slide about the FAIRVIEW program that was seen on Brazilian television in 2013 showed a map with markers all over the United States, but without a legend that explained what they stood for. From a similar map with the proper legend, that was published in August 2015, it became clear that in 2010 the NSA had access to the following AT&T facilities:\nExcept for the VoIP facilities, most of these access points are situated along the US borders.\n\nThe collection of data under the FAIRVIEW program takes place under different legal authorities: FISA, which requires individualized warrants from the FISA Court, section 702 FAA for when one end of the communications if foreign, and the Transit Authority for when both ends of a communications are foreign.\n\nUnder the FAIRVIEW program, AT&T also provided the NSA with domestic telephone metadata in bulk, which was authorized under section 215 of the USA PATRIOT Act. First this was from landline connections, but in 2011, AT&T also started handing over cell phone metadata: 1.1 billion a day.\n\n"}
{"id": "237901", "url": "https://en.wikipedia.org/wiki?curid=237901", "title": "Finland's language strife", "text": "Finland's language strife\n\nThe Language Strife () was a major conflict in mid-19th century Finland. Both the Swedish and Finnish languages were commonly used in Finland at the time, associated with descendants of Swedish colonisation and leading to class tensions among the speakers of the different languages. It became acute in the mid-19th century. The competition was considered to have officially ended when Finnish gained official language status in 1923 and became equal to the Swedish language.\n\nFinland had once been under Swedish rule (Sweden-Finland). Swedish (with some Latin) was the language of administration and education in the Swedish Realm. Swedish was therefore the most-used language of administration and higher education among the Finns. To gain higher education, one had to learn Swedish, and Finnish was considered by the upper classes to be a \"language of peasants\". Immigration of Swedish peasants to Finland's coastal regions also boosted the status of Swedish by sheer number of speakers. Although Mikael Agricola had started written Finnish with \"Abckiria\" in the 1500s, and a Finnish translation of the Civil Code of 1734 was published in 1759 (\"Ruotzin waldacunnan laki\"), it had no official status as a legal publication since the official language of administration was Swedish.\n\nAs a result of the Finnish War, Sweden ceded Finland to Russia in 1809. Finland became the autonomous Grand Duchy of Finland within the Russian Empire. Under Russian rule, the laws of the Sweden–Finland era remained largely unchanged, and Swedish continued to be used in administration.\n\nThe language strife became more acute in the second half of the 19th century. Johan Vilhelm Snellman, a Swede who wished to increase education in Finland, became a chief initiator of conflict in the 1850s due to his concern about the changing language use among the educated classes, many of whom were using Russian or Finnish. He wrote to Zachris Topelius in 1860: \"My view is this: Whether Russian or Finnish will win, only God knows. I dare not hope for anything. But that Swedish will lose - that I do know.\" Elias Lönnrot compiled the first Finnish-Swedish dictionary (\"Finsk-Svenskt lexikon\"), completing it in 1880.\n\nThe rise of Fennomanic Finnish nationalism in the 19th century eventually led to the revived predominance of Finnish use in the country. A significant contribution to the Finnish national awakening from the mid-19th century onward came from the members of the mostly Swedish-speaking upper classes deliberately choosing to promote Finnish culture and language. Snellman was himself was an ethnic Swede and was later ennobled. These Finnish Swedes, known as the Fennomans, Fennicized their family names, learned Finnish, and made a point of using Finnish both in public and at home. However, another group of the Swedish-speaking population, the Svecomans, did not wish to abandon Swedish and opposed the Fennoman ideology and Fennoman-inspired reforms.\n\nIn 1863 Alexander II (AsK 26/1863) ruled that Finnish had an official language status comparable to that of Swedish; it could thereafter be used in an official capacity in legal and state office matters. Within a generation, the Finnish language use gained predominance in the government and the society of Finland. \n\nDuring the Russification of Finland, Tsar Nicholas II attempted to change the official language to Russian (\"Language Manifesto of 1900\"), but Russification was halted by the general strike of 1905.\n\nAfter Finland gained independence in 1917, its relations with Sweden unexpectedly became strained in connection with the Finnish Civil War and the Åland crisis. These events aggravated the language dispute, and the controversy over Swedish and Finnish became a prominent feature of domestic politics during the 1920s and 1930s.\n\nIn the newly independent Finnish constitution of 1919, Finnish and Swedish were given equal status as national languages. The language strife thereafter centered on this and on the role of Swedish in universities, particularly regarding the number of professors who spoke and wrote in Swedish in their teaching. In the interwar period, the University of Helsinki was the scene of conflict between those who wanted to advance the use of Finnish and those who wished to maintain the use of Swedish. Geographer Väinö Tanner was one of the most vocal defenders of Swedish. A campaign initiated by the Swedish People's Party of Finland collected 153,914 signatures in defense of Swedish in a petition that was presented to the parliament and government in October 1934. The conflict at the university generated an international reaction when academics from Denmark, Sweden, Norway, and Iceland sent letters to the diplomatic representatives of Finland in their respective countries warning that diminishing the role of Swedish at the university would result in a weakening of Nordic unity.\n\nThe government issued a language decree on 1 January 1923 making Finnish and Swedish equal in status.\nDuring the resettlement of more than 420,000 Karelian refugees after the Winter War against the Soviet Union (1939–1940), the Swedish-speaking minority feared that the new Finnish-speaking settlers would change the linguistic balance of their neighborhoods. Since the late 20th century, there has been discussion of whether the policy of mandatory Swedish classes in schools should continue.\n\n\n"}
{"id": "16515997", "url": "https://en.wikipedia.org/wiki?curid=16515997", "title": "Forum Against Islamophobia and Racism", "text": "Forum Against Islamophobia and Racism\n\nThe Forum Against Islamophobia and Racism (FAIR) is a London-based Muslim advocacy and lobbying group which campaigns against discrimination in the form of Islamophobia and racism. It was established in 2001 as an independent charitable organization with the aim of monitoring media coverage of Islam and Muslims, and challenging examples of Islamophobia through dialogue with media organizations. Since its inception, it has produced numerous publications relating to Islamophobia in the United Kingdom. Formed in 2000, Navid Akhtar and Samar Mashadi have been directors of FAIR.\n\nAccording to the group's mission statement, it was set up \"for the purpose of raising awareness of and combating Islamophobia and racism, monitoring specific incidents of Islamophobia and racism, working towards eliminating religious and racial discrimination, campaigning and lobbying on issues relevant to Muslim and other multi-ethnic communities in Britain.\"\n\nFAIR was active in campaigning for passing of the Racial and Religious Hatred Act 2006 which outlawed the incitement towards religious hatred in Britain. They issued a memorandum in 2002 entitled \"The Religious Offences Bill: A Response\" to the House of Lords Select Committee in which they raised a number of publications by the far-right British National Party inciting hatred against Islam and the Muslim community in Britain, stating that this exposed a loophole in British law wherein some communities were protected from incitement to hatred whilst others were not. In 2003, FAIR reported 29 incidents of assault or attacks against Muslims for that year, though they believe many went unreported. This included attacks on Muslim women and people wearing traditional Muslim clothes, as well as vandalism of mosques and Islamic centers. Also reported were desecration of Muslim graves.\n\nIn 2004, the FAIR carried out a survey in association with the Muslim College and the Al-Khoei Foundation which found that since the September 11 attacks, 80 percent of Muslims surveyed reported being subjected to Islamophobia; 68 percent believed they had been treated and perceived differently; and 32 percent reported being discriminated against at airports in the United Kingdom. FAIR have also co-organised events and demonstrations alongside the Muslim Council of Britain (MCB) and the National Assembly of Black People.\n\n\n\n"}
{"id": "1297539", "url": "https://en.wikipedia.org/wiki?curid=1297539", "title": "Free particle", "text": "Free particle\n\nIn physics, a free particle is a particle that, in some sense, is not bound by an external force, or equivalently not in a region where its potential energy varies. In classical physics, this means the particle is present in a \"field-free\" space. In quantum mechanics, it means a region of uniform potential, usually set to zero in the region of interest since potential can be arbitrarily set to zero at any point (or surface in three dimensions) in space.\n\nThe classical free particle is characterized simply by a fixed velocity v. The momentum is given by\n\nand the kinetic energy (equal to total energy) by\n\nwhere \"m\" is the mass of the particle and v is the vector velocity of the particle.\n\nA free particle in non-relativistic quantum mechanics is described by the free Schrödinger equation:\n\nwhere ψ is the wavefunction of the particle at position r and time \"t\". The solution for a particle with momentum p or wave vector k, at angular frequency ω or energy \"E\", is given by the complex plane wave:\n\nwith amplitude \"A\". As for \"all\" quantum particles free \"or\" bound, the Heisenberg uncertainty principles\n\n(similarly for the \"y\" and \"z\" directions), and the De Broglie relations:\n\napply. Since the potential energy is (set to) zero, the total energy \"E\" is equal to the kinetic energy, which has the same form as in classical physics:\n\nThe integral of the probability density function\n\nwhere * denotes complex conjugate, over all space is the probability of finding the particle in all space, which must be unity if the particle exists:\n\nThis is the normalization condition for the wave function. The wavefunction is not normalizable for a plane wave, but is for a wavepacket.\nThe free particle wave function may be represented by a superposition of \"momentum\" eigenfunctions, with coefficients given by the Fourier transform of the initial wavefunction:\n\nwhere the integral is over all k-space and formula_11 (to ensure that the wave packet is a solution of the free particle Schrödinger equation). Here formula_12 is the value of the wave function at time 0 and formula_13 is the Fourier transform of formula_12. (The Fourier transform formula_15 is essentially the momentum wave function of the position wave function formula_16, but written as a function of formula_17 rather than formula_18.)\n\nThe expectation value of the momentum p for the complex plane wave is\n\nand for the general wave packet it is\n\nThe expectation value of the energy E is \n\nThe phase velocity is defined to be the speed at which a plane wave solution propagates, namely\n\nNote that formula_23 is \"not\" the speed of a classical particle with momentum formula_24; rather, it is half of the classical velocity.\n\nMeanwhile, suppose that the initial wave function formula_12 is a wave packet whose Fourier transform formula_13 is concentrated near a particular wave vector formula_17. Then the group velocity of the plane wave is defined as\n\nwhich agrees with the formula for the classical velocity of the particle. The group velocity is the (approximate) speed at which the whole wave packet propagates, while the phase velocity is the speed at which the individual peaks in the wave packet move. The figure illustrates this phenomenon, with the individual peaks within the wave packet propagating at half the speed of the overall packet.\n\nThe notion of group velocity is based on a linear approximation to the dispersion relation formula_29 near a particular value of formula_30. In this approximation, the amplitude of the wave packet moves at a velocity equal to the group velocity \"without changing shape\". This result is an approximation that fails to capture certain interesting aspects of the evolution a free quantum particle. Notably, the width of the wave packet, as measured by the uncertainty in the position, grows linearly in time for large times. This phenomenon is called the spread of the wave packet for a free particle.\n\nSpecifically, it is not difficult to compute an exact formula for the uncertainty formula_31 as a function of time, where formula_32 is the position operator. Working in one spatial dimension for simplicity, we have:\nwhere formula_12 is the time-zero wave function. The expression in parentheses in the second term on the right-hand side is the quantum covariance of formula_32 and formula_36.\n\nThus, for large positive times, the uncertainty in formula_32 grows linearly, with the coefficient of formula_38 equal to formula_39. If the momentum of the initial wave function formula_12 is highly localized, the wave packet will spread slowly and the group-velocity approximation will remain good for a long time. Intuitively, this result says that if the initial wave function has a very sharply defined momentum, then the particle has a sharply defined velocity and will (to good approximation) propagate at this velocity for a long time.\n\nThere are a number of equations describing relativistic particles: see relativistic wave equations.\n\n\n\n"}
{"id": "33330773", "url": "https://en.wikipedia.org/wiki?curid=33330773", "title": "Global Map", "text": "Global Map\n\nGlobal Map is a set of digital maps that accurately cover the whole globe to express the status of global environment. It is developed through the cooperation of National Geospatial Information Authorities (NGIAs) in the world. An initiative to develop Global Map under international cooperation, the Global Mapping Project, was advocated in 1992 by Ministry of Construction, Japan (MOC) at the time (the current Ministry of Land, Infrastructure, Transport and Tourism, Japan-MLIT).\n\nGlobal Map is digital geospatial information in 1 km resolution which satisfies the following conditions:\nThe global geospatial information developed as Global Map mainly consists of the following thematic layers:\n\nSince the United Nations Conference on the Human Environment in 1972, global environmental challenges have been recognized as an issue which is common to humankind. “The United Nations Conference on Environment and Development (the Earth Summit)” in Brazil in 1992 adopted “An action plan of humankind for sustainable development: Agenda 21.” Agenda 21 describes in many parts the importance of information for decision-making to appropriately cope with global environmental challenges. Especially geospatial information is regarded to be critical.\n\nIn response to the objectives of Agenda 21 and in recognition of the need for further contribution to the development of geospatial information, the MOC at the time (the current MLIT) advocated, in the same year, the Global Mapping Project, an international cooperation initiative to develop global geospatial information to understand the present status and changes of global environment. This concept was presented at the Fifth United Nations Regional Cartographic Conference for the Americas in New York in 1993. At the same time, resolution\ncalling for the promotion of the development of global geospatial data was adopted at this conference. Following this conference, a similar resolution\nwas adopted at Thirteenth United Nations Regional Cartographic Conference for Asia and the Pacific in Beijing in 1994.\nIn 1996, International Steering Committee for Global Mapping (ISCGM), which consists of heads or its equivalents of NGIAs, was established to promote the Global Mapping Project. Thus the mechanism for the international promotion was formed. The Geospatial Information Authority of Japan (GSI) has been serving as the secretariat of the ISCGM. In the following year, in 1997, which was five years after the Earth Summit, the nineteenth special session of the United Nations General Assembly (19th UNGASS) was held. At the paragraph 112 of the resolution adopted by the 19th UNGASS, importance of a supportive environment to enhance national capacities and capabilities for information collection and processing, especially in developing countries, to facilitate public access to information on global environmental issues was mentioned along with the description mentioning the significance of international cooperation, including global mapping, as a means to develop the supportive environment.\n\nAs a result of such movement, In 1998, a recommendatory letter to participate in the Global Mapping Project was sent from the United Nations to NGIAs of respective countries in the world.\n\nFurther, at the World Summit on Sustainable Development (Johannesburg Summit) held in 2002, global mapping was included in the Plan of Implementation.\n\n"}
{"id": "40532068", "url": "https://en.wikipedia.org/wiki?curid=40532068", "title": "Global peace system", "text": "Global peace system\n\nGlobal Peace System is a concept of global conflict resolution dependent on nonviolent processes to eradicate war. It relies upon a multi-strand approach to conflict resolution, incorporating broad social and political solutions. In contemporary peace and conflict studies, the concept of a global peace system has been evolving since the 1940s around the theory that there is a global infrastructure of peacebuilding and that there is a need for systems thinking in peacebuilding. The term \"global peace system\" was coined from the work of Robert Johansen, who explored the concept in 1978's \"Toward a Dependable Place\".\n\nAn early use of the term “peace system” occurred in Kirby Page’s book \"National Defense\" (1931). In a 1943 pamphlet and a 1966 book, political theorist David Mitrany stated that the prevention of wars required the creation of a peace system which he outlined. In 1978's \"Toward a Dependable Place\", Johansen coined the term \"global peace system\", arguing that, contrary to the advocated and perceived military security in the international system, the peace system provides greater justice, economic well-being and ecological security. In a global peace system, according to Johansen, \"conflict is resolved through nonviolent, political, social, and judicial processes. There are no expectations of war and no national military arsenals.\" Johansen called for the creation of such a system by a people’s movement.\n\nIn 1988, Robert A. Irwin stated that a peace system involves multiple war prevention layers: first, global reforms which reduce the causes of war involve non-threatening defense-policies as well as political, economic, ecological and cultural change; second, conflict resolution mechanisms on the local, regional, national and global level.\n\nIn 1991, Louise Diamond and John W. McDonald authored \"Multi-track Diplomacy: a Systems Approach to Peace\", wherein they identified nine specific tracks to produce a synergy in peacebuilding: public opinion and communication, government, professional conflict resolution, business, private citizens, activism, religion, funding, and research, training and education. In 1992, the pair founded the United States-based non-profit organization Institute for Multi-track Diplomacy with the mission of putting into practice their systems-based approach to peacebuilding.\n\nIn 2003's \"Instead of War: the Urgency and Promise of the Global Peace System\", Timothy McElwee emphasizes three major focal areas in the construction of a global peace system: (1) strengthening international norms and institutions against war; (2) eliminating the conditions that give rise to war and violence; (3) supporting and encouraging alternative means of international conflict transformation. Other sector-based approaches to peacebuilding link international development, humanitarian assistance, gender, the private sector, religion, environmental change, security, media, health and the rule of law.\n\nIn 2011, Kent Shifferd proposed that there is a good chance to outlaw war within the next 100 years. Examining the history of peace, Shifferd posits that there has been more peace than war in history and numerous peace movements were active even during war times. Referring to the global peace system, Shifferd states that most of what needs to be invented to end war has been invented. Rob Ricigliano in 2012 stated that \"while...difficult to ascribe specific causation to any one group, the positive trends … indicate that the international community, practitioners, and academics have learned important lessons about ending wars and building peace.”\n\nWriting in 2013, Douglas Fry noted that the creation of a global peace system involves synergistic elements such as a transformative vision that a peace-based global system is possible, the understanding of interdependence and cooperation, an added level of social identity including all human beings, the creation of effective and democratic procedures of international adjudication, and peace-supporting symbols and values.\n\nBeginning in 2003, peace ethologist Judith Hand approached the subject of a paradigm shift to a global peace system from biological, behavioral, anthropological, and gender perspectives.. Based on the biological principles of sexual dimorphism and parental investment theory and the political principle of gender parity in government (which she termed koinoniarchy) she argued that gender parity in governing is a necessary condition, not an option, for establishment and maintenance of a global peace system.\n\n\n"}
{"id": "1177323", "url": "https://en.wikipedia.org/wiki?curid=1177323", "title": "Grave robbery", "text": "Grave robbery\n\nGrave robbery, tomb robbing, or tomb raiding is the act of uncovering a grave, tomb or crypt to steal matter. It is usually perpetrated to take and profit from valuable artifacts or personal effects. A related act is body snatching, a term denoting the contested or unlawful taking of a body (seldom from a grave), which can be extended to the unlawful taking of organs alone. These acts carry two stigmas from the evolution of morality: selfishness and psychological trauma to which the behavioural immune system adds the stigma of disgust to those coming into contact with part-decayed, particularly human, bodies.\n\nGrave robbing has caused great difficulty to the study of archaeology, art history, and history. Countless precious grave sites and tombs have been robbed before scholars were able to examine them. In any way, the archaeological context and the historical and anthropological information are destroyed:\nGrave robbers who are not caught usually sell relatively modern items anonymously and artifacts on the black market. Those intercepted, in a public justice domain, are inclined to deny their guilt due to the three stigmas mentioned. Though some artifacts may make their way to museums or scholars, the majority end up in private collections.\n\nChinese jade burial suits were believed to be myths for many years until two were discovered in 1968; it is now believed that most jade burial suits were removed long ago by grave robbers.\n\nGrave robbing is still problematic in 21st century China. The increase in technology, such as night vision goggles, air breathing equipment, and metal detectors allows grave robbers to better find and rob ancient gravesites. There are institutions in which you can learn how to rob graves– “for about 200 yuan (about $30) a day. Land surveying skills are first taught, before progressing to probes and shovels, then finally explosives. After 10 days, adepts have the chance to assist an instructor in a real tomb robbery”.\n\nAncient Egyptian tombs are one of the most common examples of tomb or grave robbery. Most of the tombs in Egypt's Valley of the Kings were robbed within one hundred years of their sealing (including the tomb of the famous King Tutankhamen, which was raided at least twice before it was discovered in 1922). As most of the artifacts in these ancient burial sites have been discovered, it is through the conditions of the tombs and presumed articles that are missing in which historians and archaeologists are able to determine whether the tomb has been robbed. Egyptian pharaohs often kept records of the precious items in their tombs, so an inventory check is presumed for archaeologists. Oftentimes, warnings would be left by the Pharaohs in the tombs of calamities and curses that would be laid upon any who touched the treasure, or the bodies, which did little to deter grave robbers. There are many examples of grave robbing in the Ancient World outside of Egypt. \n\nThe Romans (Byzantium) also suffered decades of theft and destruction of tombs, crypts, and graves.\n\nIn Europe, graves are robbed on an accelerating and alarming scale. Many grave robbers works with metal detectors and some of the groups are organised criminals, feeding the black market with highly prized archaeological artifacts.\n\nMerovingian graves in France and Germany and Anglo-Saxon graves in England contain many metal grave goods, mostly of iron. Grave robbers often leave them, being only interested in gold and silver. Grave contexts, ceramics, iron weapons and skeletons are destroyed.\n\nIn Eastern Europe, including Southeast Europe and the European part of Russia, grave robbers target all kinds of historically important graves, from pre-historic tombs to World War II graves.\n\nModern grave robbing in North America also involves long-abandoned or forgotten private Antebellum Period to pre-Great Depression era grave sites. These sites are often desecrated by grave robbers in search of old, hence valuable, jewelry. Affected sites are typically in rural, forested areas where once-prominent, wealthy landowners and their families were interred. The remote and often undocumented locations of defunct private cemeteries make them particularly susceptible to grave robbery. The practice may be encouraged by default upon the discovery of a previously unknown family cemetery by a new landowner.\n\nOne historical incident occurred during the evening of November 7, 1876, when a group of counterfeiters tried to abscond with Abraham Lincoln's mortal remains from his grave in Springfield, Illinois, in order to secure the release of their imprisoned leader, counterfeit engraver Benjamin Boyd. However, a secret service agent was present and had notified the police beforehand, so the attempted grave robbers only succeeded in the dislodgment of the lid of his coffin. As a consequence, when reburied, additional security measures prevented further depredations against Lincoln's body \n\nGrave robbers often sold stolen Aztec or Mayan goods on the black market for an extremely high price. The buyers (museum curators, historians, etc.) did not often suffer the repercussions of being in possession of stolen goods and that the blame (and charges) are put upon the lower-class grave robbers. Today's antiquities trade has become a streamlining industry – and the speed these artifacts enter the market has grown exponentially. Laws have been enacted in these regions, but due to extreme poverty, these grave robbings continue to grow each year.\n\nAfrican Americans would often bury their dead in a potter's field; not having the access or money for a proper funeral. When buried in potter’s fields, the dead were not normally buried very deep. A grave robber could just wait in the distance until everyone left and dig up the body from its shallow grave.\n\nOnce the railroad was invented and tracks laid—the sale of African American slaves from the South for dissection began; being sent to medical schools in the northern part of the United States. One Professor of Anatomy in New England reported that, in the 1880s and 1890s, he entered into an arrangement in which he received, twice each semester, a shipment of 12 bodies of southern African Americans. “They came in barrels labeled turpentine and were shipped to a local hardware store that dealt in painting materials”.\n\nAfter the Emancipation Act, African American Union soldiers that died while serving in the military were dissected by white military surgeons.\n\nState laws in Mississippi and North Carolina were passed in the 19th century which allowed medical schools to use the remains of those at the bottom of society’s hierarchy—the unclaimed bodies of poor persons, residents of alms houses, and those buried in potter’s fields. The option to dissect Confederate soldiers was also not available, being that Mississippi and North Carolina legally released the bodies to the families. The North Carolina law also provided that the body of whites never be sent to an African American medical college (such as the Leonard Medical School). These African American Medical schools typically obtained unclaimed Black ‘‘potter’s field bodies’’.\n\nThe geography and placement of burial grounds became a deterrent within itself. This is because without the accessibility of the automobile (in the early 19th century), the transportation of bodies was difficult.\n\nA perfect example of this is Mount Auburn Cemetery, in Cambridge Massachusetts. It was the first rural cemetery inside the United States. The rural location of the cemetery created transportation issues. In addition, the terrain of and around the cemetery was formidable. Further, Henry Alexander Scammell Dearborn, the designer wanted to leave the natural terrain (including ponds and hills) within the cemetery. If someone wanted to rob a grave they would have to maneuver around these obstacles for over large stretches of land, while in the dark. Note that Mount Auburn Cemetery is over 175 acres. Other cemeteries, of the time, that were originally built away from populated areas for similar reasons, include: Mount Hope Cemetery in Bangor, Maine (1834); Laurel Hill Cemetery in Philadelphia, Pennsylvania (1836); Mount Pleasant Cemetery in Taunton, Massachusetts (1836); Mount Hope Cemetery in Rochester, New York (1838); Green-Wood Cemetery in Brooklyn, New York (1838); and, Green Mount Cemetery in Baltimore, Maryland (1838).\n\nA mortsafe or mort safe was an iron coffin or framework which helped to protect a grave by preventing the body from being dug up and taken away. Mortsafes were specific for the task of preventing bodies from being stolen for purposes of medical dissections. These deterrents, used commonly in Scotland, would be only available for the rich to protect their loved ones, since iron was so expensive. After the body would decompose to a certain extent, the Mortsafe would be removed. These were not a commodity that were sold and bought; rather, they were rented.\n\nA mort house or dead house was used to store bodies until decomposed enough to no longer be targets for grave robbing. Up to 31 recorded mort houses were scattered throughout Scotland and northern England. Usually these structures were built within or near cemeteries to make transportation easier. Prior to grave robbers, they were used to store dead bodies in the winter, being that the ground was too cold and in some cases impossible to dig into. An example is the Udny Mort House built in 1832, Aberdeenshire, north-east Scotland and still standing today.\n\nThe coffin collar was an iron collar often fixed to a piece of wood. It was fixed around the neck of a corpse and then bolted to the bottom of a coffin. Most common reports of these collars being used came from Scotland around the 1820s.\n\nHistorically mausoleums have been used as a sign of a family’s wealth and a symbol of gentry and nobility in many countries. In the mid and late 19th century in North America, more and more families began to buy mausoleums. The belief was that it would be easier for a Resurrectionist or grave robber to dig up a grave rather than to topple down iron or steel doors guarding the mausoleum. A flaw in the design of the mausoleum was the stained glass or other windows within. Almost every family between the 18th and 19th century had a religious affiliation. As such, many of these families (usually with a Christian affiliation) would put stained glass within the mausoleums. The grave robbers would then just have to smash the glass to break in and to retrieve the body. Making it even easier, around the 1830s families began to fear burying family members. The living relatives would stand guard inside the mausoleum and would sometimes get trapped—only to be discovered upon the death of the next family member [citation needed]. To remedy this, families would put a spare key somewhere within the mausoleum and create doors with two way locks. In short, grave robbers could break a window, recover the body, find the key, and walk straight out the front door of the mausoleum.\n\nOne of the most simplistic and low-tech methods to prevent grave robbing were to have an individual guard over the newly buried body. This was done until decomposition of the body was brought to a point where they would no longer be desirable for medical use. If families did not have enough money to hire an individual to watch over the grave for a select number of days, the family would delegate this duty amongst them and close friends. As grave robbing became a lucrative business in the 19th century, a bribe would convince some guards to look the other way.\n\nWithin the Great Pyramid of Giza (completed around 2560 BC), an Egyptian deterrent system was built to guard the tomb of Pharaoh Khufu. This system consists of blocks and grooves to protect the King’s Chamber from tomb robbers. Some experts believe that Pharaoh Khufu’s tomb has actually not been found because of the deterrent system; instead, what had been found by grave robbers were fake rooms.\n\n\n\n"}
{"id": "24976623", "url": "https://en.wikipedia.org/wiki?curid=24976623", "title": "Hydrogen pinch", "text": "Hydrogen pinch\n\nHydrogen pinch analysis (HPA) is a hydrogen management method that originates from the concept of heat pinch analysis. HPA is a systematic technique for reducing hydrogen consumption and hydrogen generation through integration of hydrogen-using activities or processes in the petrochemical industry, petroleum refineries hydrogen distribution networks and hydrogen purification.\n\nA mass analysis is done by representing the purity and flowrate for each stream from the hydrogen consumers (sinks), such as hydrotreaters, hydrocrackers, isomerization units and lubricant plants and the hydrogen producers (sources), such as hydrogen plants and naphtha reformers, streams from hydrogen purifiers, membrane reactors, pressure swing adsorption and continuous distillation and off-gas streams from low- or high-pressure separators. The source-demand diagram shows bottlenecks, surplus or shortages. The hydrogen pinch is the purity at which the hydrogen network has neither hydrogen surplus nor deficit.\n\nAfter the analysis REFOPT from the Centre for Process Integration at The University of Manchester is used as a tool for process integration with which the process is optimized. The methodology was also developed into commercial software by companies such as Linnhoff March and AspenTech. The Aspen product incorporated the work of Nick Hallale (formerly a lecturer at University of Manchester) and was the first method to consider multiple components, rather than a pseudo-binary mixture of hydrogen and methane.\n\nThe first assessment based on cost and value composite curves of hydrogen resources of a hydrogen network was proposed by Tower et al. (1996). Alves developed the hydrogen pinch analysis approach based on the concept of heat pinch analysis in 1999. Nick Hallale and Fang Liu extended this original work, adding pressure constraints and mathematical programming for optimisation. This was followed by developments at AspenTech, producing commercial software for industrial application.\n\n\nNick Hallale, Ian Moore, Dennis Vauk, \"Hydrogen optimization at minimal investment\", Petroleum Technology Quarterly (PTQ), Spring (2003)\n\n"}
{"id": "548173", "url": "https://en.wikipedia.org/wiki?curid=548173", "title": "Instability", "text": "Instability\n\nIn numerous fields of study, the component of instability within a system is generally characterized by some of the outputs or internal states growing without bounds. Not all systems that are not stable are unstable; systems can also be marginally stable or exhibit limit cycle behavior.\n\nIn structural engineering, a structure can become unstable when excessive load is applied. Beyond a certain threshold, structural deflections magnify stresses, which in turn increases deflections. This can take the form of buckling or crippling. The general field of study is called structural stability.\n\nAtmospheric instability is a major component of all weather systems on Earth.\n\nIn the theory of dynamical systems, a state variable in a system is said to be unstable if it evolves without bounds. A system itself is said to be unstable if at least one of its state variables is unstable.\n\nIn continuous time control theory, a system is unstable if any of the roots of its characteristic equation has real part greater than zero (or if zero is a repeated root). This is equivalent to any of the eigenvalues of the state matrix having either real part greater than zero, or, for the eigenvalues on the imaginary axis, the algebraic multiplicity being larger than the geometric multiplicity. The equivalent condition in discrete time is that at least one of the eigenvalues is greater than 1 in absolute value, or that two or more eigenvalues are equal and of unit absolute value.\n\n\nFluid instabilities occur in liquids, gases and plasmas, and are often characterized by the shape that form; they are studied in fluid dynamics and magnetohydrodynamics. Fluid instabilities include:\n\n\nPlasma instabilities can be divided into two general groups (1) hydrodynamic instabilities (2) kinetic instabilities. Plasma instabilities are also categorised into different modes – see this paragraph in plasma stability.\n\nGalaxies and star clusters can be unstable, if small perturbations in the gravitational potential cause changes in the density that reinforce the original perturbation. Such instabilities usually require that the motions of stars be highly correlated, so that the perturbation is not \"smeared out\" by random motions. After the instability has run its course, the system is typically \"hotter\" (the motions are more random) or rounder than before. Instabilities in stellar systems include:\n\n\nThe most common residual disability after any sprain in the body is instability. Mechanical instability includes insufficient stabilizing structures and mobility that exceed the physiological limits. Functional instability involves recurrent sprains or a feeling of giving way of the injured joint. Injuries cause proprioceptive deficits and impaired postural control in the joint. Individuals with muscular weakness, occult instability, and decreased postural control are more susceptible to injury than those with better postural control. Instability leads to an increase in postural sway, the measurement of the time and distance a subject spends away from an ideal center of pressure. The measurement of a subject’s postural sway can be calculated through testing center of pressure (CoP), which is defined as the vertical projection of center of mass on the ground. Investigators have theorized that if injuries to joints cause deafferentation, the interruption of sensory nerve fibers, and functional instability, then a subject’s postural sway should be altered. Joint stability can be enhanced by the use of an external support system, like a brace, to alter body mechanics. The mechanical support provided by a brace provides cutaneous afferent feedback in maintaining postural control and increasing stability.\n\n"}
{"id": "44525775", "url": "https://en.wikipedia.org/wiki?curid=44525775", "title": "Intention", "text": "Intention\n\nIntention is a mental state that represents a commitment to carrying out an action or actions in the future. Intention involves mental activities such as planning and forethought.\n\nFolk psychology explains human behavior on the basis of mental states, including beliefs, desires, and intentions. Mental mechanisms, including intention, explain behavior in that individuals are seen as actors who have desires and who attempt to achieve goals that are directed by beliefs. Thus, an intentional action is a function to accomplish a desired goal and is based on the belief that the course of action will satisfy a desire.\n\nThere is also a theoretical distinction between intentionality (intentional actions), and a mental state of intention for the future. Searle (1983) labeled these as intention-in-action and prior intention respectively. Prior intentions reflect forethought about intentions-in-action; prior intentions do not need to be carried out to be considered intentions. An unfulfilled intention is a prior intention that has no action associated with it.\n\nAstington (1993) outlined the connections between mental states (desires, beliefs, and intentions) and actions carried out by an individual in order to reach a goal; these connections are referred to as the Intentional Chain. The proposed connective chain is that desire causes intention, which causes action, which causes outcome. The Intentional Chain maps the linking of a desire to the satisfaction of a goal via the intermediary intention.\n\nPsychological research suggests that understanding intentions of others may be a prerequisite for a higher-level understanding of other people's minds or theory of mind. Theory of mind research attempts to map how children come to understand the mind as a representational device for the world. This research has focused on the development of knowledge that others have beliefs, desires, and intentions that are different from one's own. A basic ability to comprehend other people's intentions based on their actions is critical to the development of theory of mind.\n\nUnderstanding intention is thought to be pivotal in understanding social contexts in numerous ways. First, acquiring an understanding of intention is important for development in that it helps children conceptualize how people and animals differ from objects. Much of behavior is caused by intentions, and understanding intentions helps to interpret these behaviors. Second, intentions are integral to an understanding of morality. Children learn to assign praise or blame based on whether actions of others are intentional. Intention is also necessary to understand and predict the plans and future actions of others. Understanding the intentions and motives of others aids in the interpretation of communication, and the achievement of cooperative goals. Social, cognitive and developmental psychological research has focused on the question: How do young children develop the ability to understand other people's behaviors and intentions?\n\nFrom an early age, typically-developing children parse human actions in terms of goals, rather than in terms of movements in space, or muscle movements. Meltzoff (1995) conducted a study in which 18-month-olds were shown an unsuccessful act. For instance, children watched an adult accidentally under or over shoot a target, or attempt to perform an action but his hand slipped. The aim of the study was to determine whether the children were able to interpret the intention of the adult, regardless of the actual action performed. Young children have a tendency to imitate other people's actions. The outcome measure was what the child chose to re-enact—the actual event (literal motions), or the adult's goal, which was not accomplished. The results of the study suggested that 18-month-olds are able to infer unseen goals and intentions of others based on their actions. Infants who saw unsuccessful attempts at a target act and infants who saw the target act imitated the act at a higher rate than infants who saw neither the act nor an attempt. Similar paradigms were conducted with children 9 months old and 15 months old. Nine-month-olds did not respond to the unsuccessful attempt demonstrations; however, 15-month-olds acted similarly to the 18-month-olds. This suggests that between 9 months and 15 months of age the ability to infer intentions in other people develops.\n\nThe development of understanding intention has also been studied in toddlers. As mentioned previously, an intentional action is based on the belief that the course of action will satisfy a desire. In that case, what was intended can be interpreted as a function of an understanding for what was desired. When outcomes are achieved without the action of the individual directed at the goal, intention is not attributed to the actor; rather, the event is considered an accident. Research by Astington and colleagues (1993) found that 3-year-olds are skilled at matching goals to outcomes to infer intention. If another individual's goals match an outcome, 3-year-olds are able to conclude that the action was done “on purpose.” Conversely, when goals do not match outcomes, the children labeled the individual's actions as accidental. Children may come to distinguish between desire and intention when they learn to view the mind as a medium for representations of the world. Astington argues that initially desire is undifferentiated from intention in that both function as a goal state. Children then develop a more mature command of understanding other's intentions when they are able to represent an action as caused by a prior intention that is separate from desire.\n\nThus, research suggests that by the age of fifteen months, humans are capable of understanding intentional acts in others. The ability to distinguish between intention and desire develops in early childhood. Gestures and object-directed actions have also been studied in connexion with the development of the understanding of intention. The development of the ability to use gestures and object-directed actions in social situations has been studied from numerous perspectives, including the embodiment perspective and the social-cognitive perspective.\n\nGestures are often recognized as a tool indicative of higher social reasoning. In order to engage in or understand a gesture, an individual has to recognize it as an indicator of an object or event separate from the self or the actor. It is thought that pointing, especially declarative pointing (i.e. pointing intended to direct and share intention rather than request an object), reveals the understanding of others as attentional and intentional agents (e.g. Liszkowski, Carpenter, & Tomasello, 2007). This understanding is indicated by object-directed reactions to pointing (rather than focusing on the hand). Pointing is also thought to denote perspective-taking ability and understanding of intention, as the individual must be able to understand that the actor is attending to the object and, perhaps most importantly, that the actor is attempting to communicate information regarding the referent. The development of pointing is thought to reach a critical stage at around 9 to 12 months in normally developing children (e.g. Leung & Rheingold, 1981; Moll & Tomasello, 2007; Schaffer, 2005). Liszkowski and colleagues (2004) found that human children begin to point at around one year of age and do so with a multiple motives, including sharing attention and interest. Earlier pointing may be different in nature and is thought to develop from a learned association between reaching and adult responsiveness to the child's desire for a referent object.\n\nThus, it seems pointing may be more complex than a straightforward indicator of social understanding. Early pointing may not indicate an understanding of intention; rather it may indicate an association between the gesture and interesting objects or events. However, an understanding of intention may develop as the child develops a theory of mind and begins to use pointing to convey meaning about referents in the world.\n\nThe embodiment hypothesis holds that cognition arises out of an individual's physical interactions with the environment. In this way, environment and behavior are an integral part of cognition and what psychologists conceive of as ‘mental representations’ are indistinguishable from perception and action (e.g. Smith, 2005). The ontogenetic development of social cognition may be thought of as intertwined with the development pointing actions. According to this perspective, gestures are not just indicators of development but play a key role in how children come to develop advanced social cognition, including understanding of object-directed relations and human intention. In particular, engaging in physical actions oneself may provide insight into the structure of another's actions (eventually leading to a more nuanced understanding of another's mind).\n\nOne method of determining developmental relations between actions and an understanding of social nuances behind actions is to assess correlations between infants’ reactions to actions and the frequency with which infants produce actions. Children are generally able to produce actions around the same time they are considered to be capable of understanding the actions in others. For instance, Woodward and Guajardo (2002) found a correlation between children's ability to produce points (either during the experience or based on parental report of pointing at home) and their understanding of object-directed pointing (as evidenced by a preference for looking at a new object rather than a new hand path in a habituation paradigm) by 12 months. In addition, Brune and Woodward (2007) found that infants who produce object-directed points tended to have an understanding of pointing and infants who engaged in shared attention tended to have an understanding of eye gaze. Although the findings are correlational, they support the idea that actions may facilitate cognitive understanding. It is unclear whether self-produced pointing gestures causally influence an understanding of pointing as relational; however, there is experimental evidence which suggests that infants supported in a new action skill will subsequently develop an understanding of that action. For instance, infants allowed to grasp objects with Velcro mittens gained an understanding of object-directed grasping.\n\nA social-cultural perspective includes the notion that not just actions, but partaking in social interactions and cooperation (both observing and acting) are key to both ontogenetic social development and responsible for larger cultural institutions, symbol systems, and other human social abilities (e.g. Moll & Tomasello, 2007; Tomasello et al., 2005).\n\nThis social-cultural perspective is derived from the Vygotskian view that higher cognitive functions originate in relations between individuals. The strict version of this view is that these functions are social actions that have been internalized. Pointing, according to Vygotsky, starts out as an attempt to grab a desired object. Then, a transitional gesture develops in which the individual reaches toward the object when it is desired as a cue to another to retrieve it. This transitional gesture, says Vygotsky, is an important step toward language in that participation in these social interactions are internalized and become an understanding of the psychological functions of others. Thus, pointing is an example of the internalization process that occurs over a long series of developmental events. These gestures help children to gain an understanding of triadic interactions, as the gestures go from being simply about the objects to being specifically directed at people and conveying intention toward others.\n\nTomasello and colleagues proposed a social-cultural perspective for understanding human affinity for advanced social cognition (e.g. Moll & Tomasello, 2007; Tomasello et al., 2005). This view takes from Vygotsky's theory the idea that social interactions (such as pointing) are not just indicative of higher cognitive functions, such as understanding intention, but play an important role in shaping them. They argue that advanced cognitive abilities are derived from the tendency to cooperate and engage in collaborative activities (e.g. Moll & Tomasello, 2007; Tomasello et al., 2005).\n\nIt was originally suspected that such foundational cognitive skills leading to advanced social understanding lie in the human ability to understand another's intention. Humans seem to have an affinity for figuring out what others are perceiving, intending, desiring, believing, etc. For example, the use of symbols requires the ability to understand another's action and attention on an entity in the world. However, understanding intentions is unlikely to be a species-specific ability.\n\nTomasello and colleagues argue that it is possible to break down the advanced understanding of shared intentionality into two developmental pathways that eventually become intertwined:\n\n\nThis claim may be further investigated by examining the functional origins of pointing. It is possible that the pointing exhibited by other species is different in purpose and origin from the pointing said to be indicative of a developing psychological understanding. The former, referred to as imperative pointing, was originally described by Vygotsky (1978) as pointing which begins in an attempt to reach for a desired object. When another retrieves the desired object, the individual learns to associate the gesture (typically hand and all fingers extended outward) with a communicated intention to acquire the desired object. However, research suggests not all points develop in this way. A study by Bates, Camaioni and Volterra (1975) distinguished between imperative and declarative gestures. Imperative gestures were described as those directed at an adult in order to obtain an object, while declarative gestures were those simply intended to obtain adult attention. Both types of gestures are social in nature; however, declarative pointing is thought to be linked to more advanced social understanding. Declarative gestures may involve more complex social and cooperative skills, linked to the development of communication skills (e.g. Liszkowski et al., 2005). For instance, Camaioni and colleagues found that declarative pointing was related to an understanding of adult's intentions, while imperative gestures were not related.\n\nAccording to a social-cultural perspective, it is not the actions of pointing themselves, but the tendency to engage in cooperative actions (as indicated by elements such as shared intentionality and declarative pointing) that determines the advanced social-cognitive status of normally developing humans. These cooperative actions reveal an understanding of intention and may be for the sole purpose of interacting or cooperating rather than achieving an end. It may be that declarative pointing (typically exhibited by normally developing children but not children with autism), rather than imperative pointing, is indicative of the tendency to engage in the cooperative interactions believed to be important for developing advanced social-cognitive understanding. This fits in with Tomasello and colleagues’ conception that triadic social interactions in which child and adult engage in cooperative actions with shared intention are not only indicative of advanced social-cognitive ability but critical to the development of it. During these interactions, children gradually begin to conceptualize both first- and third-person perspectives, gaining a “bird’s eye view” of social interactions. Both the embodiment and social cultural perspectives share the principle that gestures are not just indicators of development, but play an important role in how children come to understand object-directed relations and human intention.\n\nResearch suggests that faces are pivotal in offering social cues necessary for children's cognitive, language, and social development. These cues may offer information on another's emotional state, focus of attention, and potential intentions (For a discussion see Mosconi, Mack, McCarthy, & Pelphrey, 2005).\n\nIntention may be ascribed to an individual based on where in space that individual is attending. Intention is understood not only through actions and the manipulation of objects, but by tracking eye movements. Research in this area is focused on how humans develop the understanding that eye gaze indicates that the observer may be psychologically connected to the referent.\n\nEven as infants, humans are able to follow the gaze of others. Further research has aimed to test whether infants are simply inclined to look in the direction of head movements, without any real understanding of another individual's psychological state. Brooks (1999) found that children do not direct attention simply toward the visual hemisphere of novel head movements; rather, children as young as 15 months attend to object-directed eye gaze, suggesting that children are attending to referents to which others attend, and are not simply gazing in a similar direction. These results support the idea that infants understand eye gaze as an indicator of another individual's psychological state, which is a basic component of understanding that others may have intentions separate from one's own.\n\nNeuroimaging research suggests that biological motion is processed differently from other types of motion. Biological motion is processed as a category in which individuals are able to infer intention. An evolutionary perspective of this phenomenon is that humans survived on the basis of being able to predict the internal mental states and potential future actions of others. Research on biological motion has found cells in the primate superior temporal polysensory area (STP) that respond specifically to biological motion. In addition, there are brain regions, including the superior temporal sulcus, that respond to biological but not non-biological motion. These findings suggest that humans may have a biologically-based affinity for spotting and interpreting purposeful, biological motions.\n\nIn one experiment, 18-month-olds observed either a human or a mechanical arm attempting to perform actions, but failing to achieve a goal. The children imitated the action to complete the intended goal when the arm was human, but not when it was mechanical. This suggests that from a young age, humans are able to infer intention specifically as a biological mechanism between motions and goals.\n\nHumans have a tendency to infer intention from motion, even in the absence of other distinguishing features (e.g. body shape, emotional expression). This was demonstrated in a study by Heider and Simmel; they had observers view videos of moving triangles, and found that participants tended to attribute intentions and even personality traits to the shapes based on their movements. The movement had to be animate, meaning self-propelled and non-linear.\n\nJohansson devised a way to study biological motion without interference from other characteristics of humans such as body shape, or emotional expression. He attached dots of light to actors' joints and recorded the movements in a dark environment, so that only the dots of light were visible. The Johansson figures, as they came to be known, have been used to demonstrate that individuals attribute mental states, such as desires and intentions to movements, that are otherwise disconnected from context.\n\nThe simulation hypothesis holds that in order to understand intention in others, individuals must observe an action, and then infer the actor's intentions by estimating what their own actions and intentions might be in the situation. Individuals connect their own actions to internal mental states through the experience of sensory information when movements are carried out; this sensory information is stored and connected to one's own intentions. Since internal mental states, such as intention, cannot be understood directly through observing movements, it is hypothesized that these internal states are inferred based on one's own stored representations of those movements.\n\nThis theory is supported by research on mirror neurons, or neural regions, including the premotor cortex, and parietal cortex, that activate both when individuals are engaging in an action, and when they are observing the actions of others. This suggests individuals may be simulating the motor movements via internal representations of their own motor movements. Thus, research indicates that humans are hard-wired to notice biological motion, infer intention, and use previous mental representations to predict future actions of others.\n\nAlthough human behavior is extremely complex and still remains unpredictable, psychologists are trying to understand the influential factors in the process of forming intentions and performing actions. The theories of Reasoned Action and Planned Behavior are comprehensive theories that specify a limited number of psychological variables that can influence behavior, namely (a) intention; (b) attitude toward the behavior; (c) subjective norm; (d) perceived behavioral control; and (e) behavioral, normative and control beliefs. In the theory of reasoned action, intention is influenced by people's attitude toward performing the behavior and the subjective norm. However, the level of perceived control is believed to be influential on people's behavioral intention along with their attitude and subjective norms, according to the theory of planned behavior. Not surprisingly, in most studies, intention is driven by attitudes to a greater extent than by subjective norms.\n\nThe predictive validity of the theory of Reasoned Action has been examined in numerous studies that have previously served as literature for at least three quantitative reviews. Ajzen and Fishbein (1973) reviewed 10 studies and reported a .63 average correlation for the prediction of behavior from intentions and a mean multiple correlation of .76 for the equation predicting intentions from both attitudes and norms. With similar objectives but larger samples, Sheppard et al.'s and van den Putte's meta-analyses estimated correlations of .53 and .62 for the prediction of behavior and multiple correlations of .66 and .68, respectively, for the prediction of intentions. All these studies have reflected the strong correlation that exists between people's attitudes, social norms and their intentions, as well as between their intention and the prediction of their behaviors. However, these correlations do not remain unchanged across all the conditions in people's life. Although people are likely to develop intentions to perform the action in question if they have a favorable attitude and perceive the behavior as controllable, then people's perception of control would be irrelevant to intentions when people have negative attitudes and perceive normative pressure not to perform certain actions. Research has also shown that people are more likely to perform an action if they have previously formed the corresponding intentions. Their intentions to perform the action appear to derive from attitudes, subjective norms, and perceived behavioral control. For instance, the reason you are motivated to have a few drinks after work is mostly determined by several factors. The very first one is your intention. Whether you have a positive attitude towards drinking as it can help you relieve stress and enjoy your time can greatly influence your attitude towards drinking after work. The next factor is the subjective norms around you. The level of intention to drink after work you are most likely to develop is influenced by whether significant people around you also hold favorable attitudes towards drinking and whether society tends to reward people who can drink. The last factor is the level of perceived behavioral control you have towards your intended behavior, more specifically how much confidence you have in controlling how much you will drink. If all of these factors tend to enhance your intention to have some drinks after work, you are more likely to do so. The longer you maintain the behavior of drinking after work, the stronger and more consistent your original intention will become. As a result, the greater the likelihood you will have some drinks in the future.\n\nHow people think about and verbally communicate their own intentions also impacts these intentions. For example, asking a question about prior behaviors using the imperfective aspect of language seems to be able to bring out stronger intentions to perform such a behavior in the future. According to the \"World Atlas of Language Structures\", \"Imperfective Aspects\" refers to a specific form of language structure used for reference to the present and the future but also for ongoing and habitual events in the past. For example, ‘He writes/is writing/wrote/was writing/will write letters.’ People are more likely to interpret the event as ongoing, and likely to resume the action in the future when it has been described with the imperfective verb aspect. Similarly, using present tense to describe an action as ongoing may strengthen intentions to perform the same action in the future. Previous research has showed that both information on past behavior and their attitude towards such behavior play crucial roles in predicting people's future behavioral tendency. Recent research done by Carrera and others concluded that verb tense may not have direct influence on intentions, however it could still affect the type of information used as a basis of behavioral intentions. When participants described a past episode using the present tense, they consistently used the more concrete past behavior as a basis for their intentions. In contrast, when participants described a past episode using the past tense, they consistently used the more abstract attitude as a basis for their intentions.\n\n\n"}
{"id": "3296107", "url": "https://en.wikipedia.org/wiki?curid=3296107", "title": "Jablonski diagram", "text": "Jablonski diagram\n\nIn molecular spectroscopy, a Jablonski diagram is a diagram that illustrates the electronic states of a molecule and the transitions between them. The states are arranged vertically by energy and grouped horizontally by spin multiplicity. Nonradiative transitions are indicated by squiggly arrows and radiative transitions by straight arrows. The vibrational ground states of each electronic state are indicated with thick lines, the higher vibrational states with thinner lines.\nThe diagram is named after the Polish physicist Aleksander Jabłoński.\n\nRadiative transitions involve the absorption, if the transition occurs to a higher energy level, or the emission, in the reverse case, of a photon. Nonradiative transitions arise through several different mechanisms, all differently labeled in the diagram. Relaxation of the excited state to its lowest vibrational level is called vibrational relaxation. This process involves the dissipation of energy from the molecule to its surroundings, and thus it cannot occur for isolated molecules. A second type of nonradiative transition is internal conversion (IC), which occurs when a vibrational state of an electronically excited state can couple to a vibrational state of a lower electronic state. A third type is intersystem crossing (ISC); this is a transition to a state with a different spin multiplicity. In molecules with large spin-orbit coupling, intersystem crossing is much more important than in molecules that exhibit only small spin-orbit coupling. ISC can be followed by phosphorescence.\n\n\n"}
{"id": "10577625", "url": "https://en.wikipedia.org/wiki?curid=10577625", "title": "James Stewart (lawyer)", "text": "James Stewart (lawyer)\n\nJames Stewart is a London-based family lawyer.\n\nThe son of a County Tyrone farmer, Stewart was educated at Coleraine Academical Institution in County Londonderry, and the University of Essex. He attended the College of Law, Chester.\n\nStewart qualified as a solicitor in 1990, was accredited by Resolution as a specialist family lawyer in 1999. A year later he was appointed partner and head of the family department at Reynolds Porter Chamberlain. In 2003 he was profiled in the \"On the Verge\" series of the Observer magazine . He became a partner at Manches, now Penningtons Manches LLP, in 2006, shortly after being elected as a Fellow of the International Academy of Matrimonial Lawyers (IAML).\n\nHe is ranked as a leading individual in the 2015 edition of Legal 500, Spears WMS Legal Index and in Chambers UK 2011.\n\nIn January 2009 Stewart was appointed Governor of the IAML and invited to become a member of the Times Law Panel, an advisory body of 100 of the country's most prominent barristers and solicitors. In January 2011 Stewart was named in the Lawyer Magazines ‘Hot 100’ listing.\n\nStewart has acted on a number celebrity and high-profile divorces. His clients have included Pricilla Waters (in her divorce against Roger Waters) and Julie Le Brocquy, producer of BAFTA award-winning \"Osama\". Stewart also successfully represented Glory Annen Clibbery in the landmark family/Human Rights Act 1998 case of Clibbery v Allan (2002), where he won at first instance and on appeal. He also acted for the successful claimant, Kerry Cox, in the widely publicised cohabitation case of Cox v Jones in 2004. His firm, Penningtons Manches, represented Guy Ritchie in his divorce with his wife Madonna.\n\nIn 2015 Stewart's book, Family Law: Jurisdiction Comparisons, was published by Sweet and Maxwell.\n\nStewart lives in Covent Garden, London.\n\n\n"}
{"id": "23574940", "url": "https://en.wikipedia.org/wiki?curid=23574940", "title": "Joy (given name)", "text": "Joy (given name)\n\nJoy is a common female given name meaning \"joy\", \"happiness\", \"joyful\". A common variant of the name is the Latin female given name Joyce (name).\n\n\n"}
{"id": "23635926", "url": "https://en.wikipedia.org/wiki?curid=23635926", "title": "Lesya", "text": "Lesya\n\nLesya, according to the Jain theory of karma, is the coloring of the soul on account of its association with the karmic matter. The colour of \"leśyā\" varies from person to person depending on the psychic states and mental activities behind an action. The coloring of the soul is explained through the analogy of crystal, that acquires the color of the matter associated with it. In the same way, the soul reflects the qualities of colour, taste, smell and touch of associated karmic matter, although it is usually the colour that is referred to when discussing the \"leśyās.\" Paul Dundas notes the key text expressing this Jain doctrine, explaining how the literary form of the text is helpful in dating and reconstructing the history of transmission.\n\nA full statement of the theory of \"lesya\" occurs in chapter 34 of the Uttaradhyayana, one of the fundamental \"sutras\" of the scriptural canon. Inspection of the metrical structure there, which consists of a cluster of old \"sloka\" verses amplified by twice as many verses in the \"arya metre\", makes clear that a great deal of ancient editorial care was taken to ensure that an original rudimentary description of this dimension of karma became fully cogent.\n\nThe ancient Jain text \"Uttarādhyayana-sūtra\" speaks of six main categories of \"leśyā\" represented by six colours – black (krishna), blue (neel), grey (kapot), red (tejo), yellow (padma) and white (shukla). \"Uttarādhyayana-sūtra\" describes the mental disposition of persons having black and white \"leśyās\":\n\nA man who acts on the impulse of the five sins, does not possess the three \"guptis\", has not ceased to injure the six (kinds of living beings), commits cruel acts, is wicked and violent, is afraid of no consequences, is mischievous and does not subdue his senses – a man of such habits develops the black \"leśyā\".\n\n— \"Uttarādhyayana-sūtra\", 34.21:22\nA man of the following qualities: envy, anger, want of self-control, ignorance, deceit, want of modesty, greed, hatred, wickedness, carelessness, love of enjoyment; a man who pursues pleasures and does not abstain from sinful undertakings, who is wicked and violent – a man of such habits develops the blue \"leśyā\".\n\n— \"Uttarādhyayana-sūtra\", 34.23:24\nA man who is dishonest in words and acts, who is base, not upright, a dissembler and deceiver 3, a heretic, a vile man, a talker of hurtful and sinful things, a thief, and full of jealousy – a man of such habits develops the grey \"leśyā\". \n\n— \"Uttarādhyayana-sūtra\", 34.25:26\nA man who is humble, steadfast, free from deceit and inquisitiveness, well disciplined, restrained, attentive to his study and duties, who loves the Law and keeps it, who is afraid of forbidden things and strives after the highest good–a man of such habits develops the red \"leśyā\". \n\n— \"Uttarādhyayana-sūtra\", 34.27:28\n\nA man who has but little anger, pride, deceit, and greed, whose mind is at ease, who controls himself, who is attentive to his study and duties, who speaks but little, is calm, and subdues his senses–a man of such habits develops the yellow \"leśyā\".\n\n— \"Uttarādhyayana-sūtra\", 34.29:30\nA man who abstains from constant thinking about his misery and about sinful deeds, but engages in meditation on the law and truth only, whose mind is at ease, who controls himself, who practises the \"samitis\" and \"guptis\", whether he be still subject to passion or free from passion, is calm, and subdues his senses–a man of such habits develops the white \"leśyā\".\n\n— \"Uttarādhyayana-sūtra\", 34.31:32 \nThe black, blue and grey are inauspicious \"leśyā\" due to which the soul takes birth in various-unhappy states of existence. The yellow, red and white are the auspicious \"leśyās\" that enable a soul to take birth in various happy states of existence. According to Jain texts, a person with black Leshya will go to hell. a person with blue lesya is reincarnated in plant life and person with grey lesya is reincarnated in animal life. On the other hand, persons having red lesya are reincarnated as humans, those with yellow lesya are reincarnated as celestial beings while those having white lesya are either reborn in highest heaven or having achieved purity attain liberation.\n\nThe Jain texts further describe the mental dispositions of a soul on account of \"leśyās\" with an example of the reactions of six persons who are travelers, on seeing a fruit-bearing tree. They see a tree laden with fruits and begin to think of getting those fruits: one of them suggests uprooting the entire tree and eating the fruits; the second one suggests cutting the trunk of the tree; the third one suggests cutting the branches only; the fourth one suggests cutting the twigs; the fifth one suggests plucking the fruits only; the sixth one suggests picking up only the fruits that have fallen down. The thoughts, words and bodily activities of each of these six travellers are different based on their mental dispositions and are respectively illustrative of the six \"leśyās\". The person with the black \"leśyā\", having evil disposition, thinks of uprooting the whole tree even though he wants to eat only one fruit. The person proposing to cut the tree trunk has blue \"leśyā\", the one suggesting cutting branches has grey \"leśyā\", the person suggesting cutting twigs has red lesya and the person thinking of simply plucking the fruits has yellow \"leśyā\". On the other hand, the person with the white \"leśyā\", having pure disposition, thinks only of picking up fruits fallen on the ground sparing the tree.\n"}
{"id": "42368736", "url": "https://en.wikipedia.org/wiki?curid=42368736", "title": "Man: Whence, How and Whither, a Record of Clairvoyant Investigation", "text": "Man: Whence, How and Whither, a Record of Clairvoyant Investigation\n\nMan: Whence, How and Whither, A Record of Clairvoyant Investigation, published in 1913, is a theosophical book compiled by the second president of the Theosophical Society (TS) - Adyar, Annie Besant, and by a TS member, Charles W. Leadbeater. The book is a study on early times on planetary chains, beginnings of early root races, early civilizations and empires, and past lives of men.\n\nAt the beginning of the book the authors wrote that \"metaphysicians, ancient and modern, declare that Past, Present, and Future are ever simultaneously existent in the divine Consciousness, and are only successive as they come into manifestation, \"i.e.\", under Time which is verily the succession of states of consciousness.\" Hence, when a mystic turn the soul away from earth and focuses his attention upon the spirit – the soul may approach the \"Memory of Nature\", the personification in the material world of the \"Thoughts of the Logos,\" the image, as it were, of \"His Mind.\" There resides the Past in ever-living records.\n\nThe authors say that \"they, having been taught the method of gaining touch, but being subject to the difficulties involved in their uncompleted evolution, have done their best to observe and transmit, but are fully conscious of the many weaknesses which mar their work. Occasional help has been given to them by the Elder Brethren.\"\n\nLeadbeater speculated that \"the total number of souls, or Monads, making up humanity was sixty thousand million, the majority being out of incarnation at any given time.\" Thus reincarnation had been one of the doctrines of the Theosophical Society almost from its beginnings. The principal characters (so-called \"star names\" or pseudonyms) are identified in the following table.\nLeadbeater wrote later that \"the two hundred and fifty characters to whom names have been assigned are supposed to be less than one tenth of the whole\". Professor Joscelyn Godwin said, \"The series of incarnations showed these pioneers going through their own evolution from subhumans to worldly and spiritual leaders of the race.\"\n\nIn the chapter VII the authors describe in detail a remarkable Lemurian:\n\"In Lemuria there was some domestication of animals; the egg-headed Lemurian was seen leading about a scaly monster, almost as unattractive as his master. Animals of all sorts were eaten raw – among some tribes human flesh was not despised – and creatures of the grade of our slugs, snails and worms, much larger than their degenerate descendants, were regarded with peculiar favor as toothsome morsels.\"\n\nIn the chapter IX the authors wrote about Atlantis that \"explosions of gas, floods and earthquakes\" razed Ruta and Daitya, the great islands, left from the cataclysm of 200,000 B.C., and only the island of Poseidonis remained, the last vestige of the once grand continent of the Atlantis. These islands were lost in 75,025 B.C., Poseidonis survived to 9,564 B.C., when it also was devoured by the ocean.\n\nThe authors assert in the chapter X of the book:\n\"The immense growth of wealth and of luxury gradually undermined the most splendid civilization that the world has yet seen. Knowledge was prostituted to individual gain, and control over the powers of nature was turned from service to oppression. Hence Atlantis fell, despite the glory of its achievements and the might of its Empires.\"\n\nBlavatsky wrote in \"The Secret Doctrine\" that the Moon \"plays the largest and most important part, as well in the formation of the Earth itself, as in the peopling thereof with human beings\". The \"Lunar Monads\" or Pitris, the progenitors of man, become in reality man himself. A religious studies scholar Isaac Lubelsky stated that Besant and Leadbeater write in the book the chain of the Moon \"was an evolutionary precursor of the chain of the planet Earth.\"\n\nIn the chapter III the authors describe in detail how Sirius, Alcyone, Herakles and Mizar achieved individualization and left the animal kingdom while living as lunar monkey-creatures. They were servants to a family of lunar men, the leaders of which are now the Masters M. and K.H. Leadbeater wrote that \"individualization from the animal kingdom usually takes place through association with the humanity of the period. Such examples of it as we occasionally see taking place round us at the present time will serve as instances for us.\" Some peculiar domestic animal, well treated by its human friends, is stimulated by its permanent contact with them up to the point where it breaks away from the group-soul to which it has previously belonged. The authors wrote:\n\"One night there is an alarm; the hut is surrounded by savages, supported by their domesticated animals, fierce and strong, resembling furry lizards and crocodiles. The faithful guardians spring up around their master's hut and fight desperately in its defence; Mars comes out and drives back the assailants, using some weapons they do not possess; but, while he drives them backward, a lizard-like creature darts behind him into the hut, and catching up the child Surya begins to carry him away. Sirius springs at him, bearing him down, and throws the child to Alcyone, who carries him back into the hut, while Sirius grapples with the lizard, and, after a desperate struggle, kills it, falling senseless, badly mangled, over its body. Meanwhile, a savage slips behind Mars and stabs at his back, but Herakles, with one leap, flings himself between his master and the weapon and receives the blow full on his breast, and falls, dying. The savages are now flying in all directions, and Mars, feeling the fall of some creature against his back, staggers, and, recovering himself, turns. He recognizes his faithful animal defender, bends over his dying servant, and places his head in his lap. The poor monkey lifts his eyes, full of intense devotion, to his master's face, and the act of service done, with passionate desire to save, calls down a stream of response from the Will aspect of the Monad in a fierce rush of power, and in the very moment of dying the monkey individualizes, and thus he dies – a man.\"\n\nThe theosophical concept of Evolution suppose spiritual development of the Mankind will attain the fullness of the stature of Buddha, Christ, etc. of the prize of the high calling of God in Christianity.\n\nThe authors the book proclaimed \"when the Human Kingdom is traversed, and man stands on the threshold of His superhuman life, a liberated Spirit\", seven paths open before Him for His choosing: He may enter into the blissful omniscience and omnipotence of Nirvana, with activities far beyond our knowing, to become, perchance, in some future world an Avatara, or godlike Incarnation; this is sometimes called, \"taking the Dharmakaya vesture\". He may enter on \"the Spiritual Period\" — a phrase including unknown meanings, among them probably that of \"taking the Sambhogakaya vesture\". He may become part of that treasure-house of spiritual forces on which the Agents of the Logos draw for Their work, \"taking the Nirmanakaya vesture\". He may remain a member of the Occult Hierarchy which rules and guards the world in which He has reached perfection. He may pass on to the next Chain, to aid in constructing its forms. He may enter the majestic Angel – Deva-Evolution. He may give Himself to the immediate service of the Logos, to be used by Him in any part of the Solar System, His Servant and Messenger, who lives but to carry out His will and do His work over the whole of the system which He controls.\n\nA religious studies scholar Alvin Kuhn wrote in his thesis that \"The Canadian Theosophist\", a magazine published at Toronto, announced a series of articles in which \"parallel passages from the writings of Mrs. Blavatsky and the \"Mahatma Letters\" on one side, and from the books of Mrs. Besant and Mr. Leadbeater, on the other (\"Man: whence, how and whither\" included), gave specific evidence bearing on the claims of perversion of the original theories by those whom they call Neo-Theosophists.\"\n\nJohn Prentice in an article \"Clairvoyant Research\" criticized the book of the authors. He proclaimed that the material for the Peruvian lives in \"Man: whence, how and whither\" (Ch. XI) had been lifted out of Garcilaso de la Vega's \"Royal Commentaries on the Yuccas of Peru\" (written in 1609 and published in English translation in 1638, 1869 and 1871).\n\nJoseph Fussell wrote about \"abnormal and preposterous claims\" published by Besant and Leadbeater.\n\nHelena Roerich stated that Leadbeater's books are a mix of \"false and ugly\" statements and some fragments of truth: one of the Great Masters supposedly called the book \"Man: Whence, How and Whither\" as \"a work which devoid of knowledge, honesty and beauty.\" She wrote: \"The book by Besant and Leadbeater is particularly awful, therein is described the lives supposedly the Great Masters and some their pupils, namely: Besant, Leadbeater... I've never seen anything equal to this tasteless blasphemy and falsehood.\"\n\nProfessor Olav Hammer stated that Leadbeater's claims about past lives \"were increasingly used to buttress power struggles.\" He wrote, \"Those who supported the controversial Leadbeater were recorded as having had important roles in the past, while his opponents were depicted as villains.\"\n\"How can Leadbeater's readers know that his clairvoyant results really are 'records' and 'information', rather than delusions or deliberate fabrications? His response is feeble, 'there is no assurance. The investigators themselves are certain <...> of the difference between observation and imagination'.\"\n\nAlice Bailey stated that the book published at Adyar by Besant and Leadbeater was \"psychic in his implications and impossible of verification\". In her \"Unfinished Autobiography\" she wrote that it proved to her the \"untrustworthiness\" of Leadbeater's writings:\n\"Books were being published at Adyar by Mr. Leadbeater that were psychic in their implications and impossible of verification, carrying a strong note of astralism. One of his major works, \"Man: Whence, How and Whither,\" was a book that proved to me the basic untrustworthiness of what he wrote. It is a book that outlines the future and the work of the Hierarchy of the future, and the curious and arresting thing to me was that the majority of the people slated to hold high office in the Hierarchy and in the future coming civilisation were all Mr. Leadbeater's personal friends. I knew some of these people—worthy, kind, and mediocre, none of them intellectual giants and most of them completely unimportant.\"\n\n\n\n\n\n"}
{"id": "14657979", "url": "https://en.wikipedia.org/wiki?curid=14657979", "title": "Moksha (Jainism)", "text": "Moksha (Jainism)\n\nSanskrit or Prakrit mokkha refers to the liberation or salvation of a soul from \"saṃsāra\", the cycle of birth and death. It is a blissful state of existence of a soul, attained after the destruction of all karmic bonds. A liberated soul is said to have attained its true and pristine nature of infinite bliss, infinite knowledge and infinite perception. Such a soul is called \"siddha\" and is revered in Jainism.\n\nIn Jainism, \"moksha\" is the highest and the noblest objective that a soul should strive to achieve. In fact, it is the only objective that a person should have; other objectives are contrary to the true nature of soul. With the right view, knowledge and efforts all souls can attain this state. That is why Jainism is also known as \"\" or the \"path to liberation\".\n\nAccording to the Sacred Jain Text, Tattvartha sutra:\n\nFrom the point of view of potentiality of , Jain texts bifurcates the souls in two categories–\"bhavya\" and \"abhavya\". \"Bhavya\" souls are those souls who have faith in and hence will make some efforts to achieve liberation. This potentiality or quality is called \"bhavyata\". However, \"bhavyata\" itself does not guarantee , as the soul needs to expend necessary efforts to attain it. On the other hand, \"abhavya\" souls are those souls who cannot attain liberation as they do not have faith in and hence never make any efforts to attain it.\n\nAccording to Jainism, the \"Ratnatraya\" or \"three Gems\", \"samyagdarśana\" (correct perception), \"samyagjñāna\" (right knowledge) and \"samyakchāritra\" (right conduct), together constitute the \"mokṣamarga\" or the path to liberation. According to Acharya KundaKunda's Samayasara:\n\n\"Samyak Darsana\" or rational perception is the rational faith in the true nature of every substance of the universe.\n\n\"Samyak Caritra\" or rational conduct is the natural conduct of a (soul) living being. It consists in following austerities, engaging in right activities and observance of vows, carefulness and controls.\nOnce a soul secures \"samyaktva\", is assured within a few lifetimes.\nThe fourteen stages on the path to liberation are called \"Gunasthāna\". These are:\nThose who pass the last stage are called \"siddha\" and become fully established in Right Faith, Right Knowledge and Right Conduct.\n\nIn that night in which the Venerable Ascetic Mahavira, died, freed from all pains, the eighteen confederate kings of Kasi and Kosala, the nine Mallakis and nine Licchavis, on the day of new moon, instituted an illuminations on the Poshadha, which was a fasting day; for they said: 'Since the light of intelligence is gone, let us make an illumination of material matter!'(128)\n\nA liberated soul dwell in \"Siddhashila\" with infinite faith, infinite knowledge, infinite perception, and infinite perfection. According to the Jain text, \"Puruşārthasiddhyupāya\":\n\n"}
{"id": "4840055", "url": "https://en.wikipedia.org/wiki?curid=4840055", "title": "Moldovan schools in Transnistria", "text": "Moldovan schools in Transnistria\n\nThe Moldovan schools in Transnistria became an issue of contention in 2004 in the context of the disputed status of Transnistria, a breakaway region of Moldova since 1990/1992.\n\nMoldovan schools were first established in Transnistria after the 1924 formation of the Moldovan Autonomous Soviet Socialist Republic, which was part of the Soviet Republic of Ukraine. In 1940 the former Moldovan Autonomous Republic was split, 8 districts were included in the Soviet Socialist Republic of Ukraine and 6 districts were joined with part of Basarabia in the Soviet Socialist Republic of Moldavia. In the Ukrainian part of the former Moldavian autonomy Moldovan schools were transformed into Russian-language schools, but in the 6 districts that remained part of the Soviet Socialist Republic of Moldova a Moldovan-language network of schools was kept.\n\nThe Moldovan 1989 language law, that introduced the Latin script as the official script of the Republic of Moldova, was boycotted by the Transnistrian authorities, and all Moldovan schools in Transnistria were ordered to keep the Cyrillic script. After the War of Transnistria ended in mid-1992, the local schools became regulated by the government of Transnistria. The schools that chose to use the Latin script back in 1989 came under pressure of the authorities and most were forced to return to the Cyrillic script. Only six Romanian language schools in Transnistria were allowed to keep the Latin script.\n\nAttempts to expand the number of schools which are using Latin script are met with heavy-handed repression. In 1996, the director of the only Moldovan school in Slobozia, who supported the wish of the parents to conduct education in Latin alphabet was fired and forced to leave the region. In 1999 a lecturer of Moldovan language of the Bender Pedagogical College has been dismissed for promoting the Latin script in the institution. The dismissal has been preceded by threats on the phone and an aggression in the building where she lived.\n\nIn September 1996, the Grigoriopol administration used Cossacks and police to stop the activity of Moldovan School. On 2 October 1996 three teachers were arrested and taken to Tiraspol. On 7 October 1996, as a result of a demarche by the President of the Republic of Moldova and the OSCE Mission, the teachers were released.\n\nAnother attempt to teach Romanian clandestinely in Grigoriopol, in a ‘PMR state-funded school’, failed in 2002. Teaching staff and parents were blatantly vilified in the local press as ‘enemies of the State’. One by one they were invited to ‘reconsider’, threatened with loss of employment and the corresponding entitlement to housing. Children (and teachers) were forced to write explanations as to why they used the Latin script and local officials routinely visited classes to check whether tuition was being ‘properly’ conducted. The parent-teacher association was abolished and its head, Mihai Speian, was arrested. The Moldovan school in Grigoriopol was forced to move in Doroțcaia, a village controlled by Chișinău, and children commute 10–15 km daily to attend the school.\n\nIn the summer of 2004, the Transnistrian authorities closed four of the six schools in the region that taught Moldovan language using the Latin script, known as Romanian. Some of the 3,400 enrolled children were affected by this measure and the teachers and parents who opposed the closures were temporarily arrested for up to six hours. During the crisis, the Moldovan government decided to create a blockade that would isolate the disputed region from the rest of the world. The blockade was ineffective because of a lack of cooperation from Ukraine, led at the time by Leonid Kuchma. Transnistria retaliated with a series of actions meant to destabilize the economic situation in Moldova, in particular, by cutting the power supply from the power plants that were built in Transnistria in Soviet times. As a result, this crisis generated power outages in parts of Moldova.\n\nA leading figure in the conflict was Elena Vasilievna Bomeshko, the Minister of Education for Transnistria. According to her, and official Transnistrian policy, the language is referred to as \"Romanian\" when it is taught in Latin script and referred to as \"Moldovan\" when Cyrillic script is used. Transnistria rejects accusations of anti-Romanian bias and defends its preference of Cyrillic for Moldovan as a way to maintain the original language, pointing to the fact as far back as the Middle Ages, Moldovan Bibles were always written in Cyrillic. While the Romanian language used the Cyrillic alphabet for centuries, it is no longer used in Romania. Cyrillic script is still used in some parts of Moldova, but only one newspaper (state-owned by Transnistrian authorities) prints a few hundred copies in Cyrillic.\n\nThe closed Romanian schools were reopened, after registering as private institutions with the Transnistrian authorities. Pressure from the European Union (a travel ban was introduced to 10 Transnistrian education officials) may have sped up the process., but they still have the status of \"private schools\" and consequently do not receive funding from the Transnistrian government.\n\nMany teachers and parents of students studying at Moldovan schools with Cyrillic script had contacted the Moldovan Helsinki Committee for Human Rights asking support to turn education in Romanian (Latin script), as the studies based on the Cyrillic script and Soviet curricula don't have any perspective and the children are unable to pursue higher studies anywhere. The OSCE High Commissioner on National Minorities has condemned the actions of Transnistrian authorities as a \"linguistic cleansing\".\n\nAn OSCE report from June 2005 states: “If they [Moldovan parents in Transnistria] enroll their children in one of this schools that offer a Moldovan curriculum using a Latin script, they risk being threatened by the regional security service, and seeing their jobs put in jeopardy. Sending their children in one of the 33 Transdniestrian schools they teach in their native language in Cyrillic is, however, hardly an appealing alternative, as the schools follow an out-dated curriculum and use textbooks from the Soviet period”. This is the reason why many Moldovans from Transnistria send their children in harassment-free Russian language schools.\n\nTransnistrian authorities don't recognize the diplomas issued by Moldovan schools using Latin script, making impossible for graduates of those schools to study on Transnistrian higher educational institutions.\n\nIn November 2006 the European Court of Human Rights accepted to examine the claims submitted by three Moldovan schools in Transnistria (from Tighina, Rîbnița and Grigoriopol) regarding the violation of their right to education and right to work in conditions of non-discrimination. The three schools concerned regard Russia and Moldova as responsible for violation of their rights. In June 2009, the Court conducted hearings on three similar cases: Caldare and 42 Others v. Moldova and Russia (no. 8252/05), Catan and 27 Others v. Moldova and Russia (no. 43370/04), Cervaschi and 98 Others v. Moldova and Russia (no. 18454/06). In 2010, the Court has decided the case to be partly admissible In 2012, the Court decided that the right to education of the applicants was violated by Russia, but not violated by Moldova.\n\nIn November 2006, Louis O'Neill, head of OSCE mission to Moldova, has urged local authorities in the Transnistrian city of Rîbnița to return a confiscated building to the Moldovan Latin-script school located in the city. The building was built by the Government from Chișinău and was almost finished in 2004, when Transnistrian police took it by force, during the school crisis.\n\n\n\n\n"}
{"id": "7734404", "url": "https://en.wikipedia.org/wiki?curid=7734404", "title": "Nuamthong Praiwan", "text": "Nuamthong Praiwan\n\nNuamthong Praiwan (; ) (1946–1 November 2006) was a Thai taxi driver who drove his taxi into a tank in protest after the military coup of 2006. He was later found hanging from a pedestrian footbridge. Officials found a suicide note and later ruled his death a suicide. His sacrifices were praised by several democracy activists.\n\nAt 6 am, Saturday 30 September 2006, Nuamthong drove his taxi, spray painted with the words \"[CDR is] destroying the country,\" and \"Sacrificing life\", into an M41 Walker Bulldog tank at Bangkok's Royal Plaza. Nuamthong was severely injured and taken to a police station nearby. \"I did it intentionally to protest the junta that has destroyed our country, and I painted all the words myself,\" noted Nuamthong to reporters from his bed at Vachira Hospital.\n\nNuamthong was charged with damaging state property. Authorities downplayed the incident, saying that Nuamthong was drunk and no messages were sprayed on the taxi. However, several newspapers reported on the legibility of the words painted on the car. Akkara Thiproj, deputy spokesperson for the military junta, expressed scepticism about Nuamthong's intentions in crashing into the tank, and claimed that \"Nobodies’ ideals are so great that they would sacrifice their lives for them.\"\nHe was released from the hospital on 12 October, and attended the 33rd anniversary of the 14 October 1973 democratic uprising.\n\nNuamgthong's body was found hanging, with a hood covering his face, from a footbridge on Vibhavadi Rangsit road near the headquarters of Thai Rath newspaper in Bangkok on 1 November 2006. Police ruled the death a suicide after forensic tests were stated to have shown no traces of a physical assault or struggle. Although Nuamthong's body was found with a suicide note, family members testified that he had given no hint of being depressed or about to commit suicide. Nuamthong wore a black T-shirt with a drawing of the Democracy Monument and a poem about the power of the masses. Newspaper clippings about his crash into the army tank were also found on his body. His wife, Boonchu, said he had given no farewells or done anything that would suggest he was planning to kill himself.\n\nHis wife noted, \"I couldn't be more sad losing the love of my life and the leader of my family. I didn't think he would be this brave, but I'm very proud of him for sacrificing for the country.\"\n\nFuneral rites were held at Wat Bua Kwan in Nonthaburi's Muang district. A score of senior police showed up at his funeral at Wat Bua Kwan in Nonthaburi to \"keep the peace\" while pro-democracy groups sent representatives and flower wreaths to honour his death.\n\nHis wife later attempted to transport his coffin to the 14 October 1973 Memorial on Ratchadamnoen road for bathing rituals. Police prevented her van from arriving at the Memorial.\n\nAbout 200 people attended the second night of Nuamthong's funeral at Wat Bua Kwan. Among them were military and police officers, politicians from the Thai Rak Thai Party and National Human Rights Commissioner Jaran Dithapichai. Sant Hathirat read a statement declaring Nuamthong a \"democracy martyr.\"\n\nHe was also praised by activist and former senator Prateep Ungsongtham Hata, who noted, \"Uncle Nuamthong has made the biggest sacrifice for democracy. I fought for democracy all my life but don't have the courage to do as much as he did.\"\n\nJunta deputy spokesperson Akara Tipparoj apologised for his earlier claim that nobody would hurt themselves for political ideals and claimed he planned to attend the funeral.\n\nThe Young People for Democracy Movement (YPD) condemned Akara's views as ignorant and noted that \"His remark is an insult, as Thailand has a long history of people dying to defend democracy from dictatorship.\"\n\nPrime Minister General Surayud Chulanont said he was saddened by the news but doubted there would be copycat suicide attempts.\n\nAfter his death, an interview Nuamthong had earlier given was partially aired by iTV. The broadcast came to an abrupt end after the Director of Army-owned Channel 5 called to give a warning. Additional troops were dispatched to keep order at the station.\n\n"}
{"id": "11241418", "url": "https://en.wikipedia.org/wiki?curid=11241418", "title": "Orphan gene", "text": "Orphan gene\n\nOrphan genes (also called ORFans, especially in microbial literature) are genes without detectable homologues in other lineages. Orphans are a subset of taxonomically-restricted genes (TRGs), which are unique to a specific taxonomic level (e.g. plant-specific). In contrast to non-orphan TRGs, orphans are usually considered unique to a very narrow taxon, generally a species.\n\nThe classic model of evolution is based on duplication, rearrangement, and mutation of genes with the idea of common descent. Orphan genes differ in that they are lineage-specific with no known history of shared duplication and rearrangement outside of their specific species or clade. Orphan genes may arise through a variety of mechanisms, such as horizontal gene transfer, duplication and rapid divergence, and de novo origination, and may act at different rates in insects, primates, and plants. Despite their relatively recent origin, orphan genes may encode functionally important proteins.\n\nOrphan genes were first discovered when the yeast genome-sequencing project began in 1996. Orphan genes accounted for an estimated 26% of the yeast genome, but it was believed that these genes could be classified with homologues when more genomes were sequenced. At the time, gene duplication was considered the only serious model of gene evolution and there were few sequenced genomes for comparison, so a lack of detectable homologues was thought to be most likely due to a lack of sequencing data and not due to a true lack of homology. However, orphan genes continued to persist as the quantity of sequenced genomes grew, eventually leading to the conclusion that orphan genes are ubiquitous to all genomes. Estimates of the percentage of genes which are orphans varies enormously between species and between studies; 10-30% is a commonly cited figure.\n\nThe study of orphan genes emerged largely after the turn of the century. In 2003, a study of \"Caenorhabditis briggsae\" and related species compared over 2000 genes. They proposed that these genes must be evolving too quickly to be detected and are consequently sites of very rapid evolution. In 2005, Wilson examined 122 bacterial species to try to examine whether the large number of orphan genes in many species was legitimate. The study found that it was legitimate and played a role in bacterial adaptation. The definition of taxonomically-restricted genes was introduced into the literature to make orphan genes seem less \"mysterious.\"\n\nIn 2008, a yeast protein of established functionality, BSC4, was found to have evolved de novo from non-coding sequences whose homology was still detectable in sister species.\n\nIn 2009, an orphan gene was discovered to regulate an internal biological network: the orphan gene, QQS, from \"Arabidopsis thaliana\" modifies plant composition. The QQS orphan protein interacts with a conserved transcription factor, these data explain the compositional changes (increased protein) that are induced when QQS is engineered into diverse species. In 2011, a comprehensive genome-wide study of the extent and evolutionary origins of orphan genes in plants was conducted in the model plant \"Arabidopsis thaliana\" \"\n\nGenes can be tentatively classified as orphans if no orthologous proteins can be found in nearby species.\n\nOne method used to estimate nucleotide or protein sequence similarity indicative of homology (i.e. similarity due to common origin) is the Basic Local Alignment Search Tool (BLAST). BLAST allows query sequences to be rapidly searched against large sequence databases. Simulations suggest that under certain conditions BLAST is suitable for detecting distant relatives of a gene. However, genes that are short and evolve rapidly can easily be missed by BLAST.\n\nThe systematic detection of homology to annotate orphan genes is called phylostratigraphy. Phylostratigraphy generates a phylogenetic tree in which the homology is calculated between all genes of a focal species and the genes of other species. The earliest common ancestor for a gene determines the age, or phylostratum, of the gene. The term \"orphan\" is sometimes used only for the youngest phylostratum containing only a single species, but when interpreted broadly as a taxonomically-restricted gene, it can refer to all but the oldest phylostratum, with the gene orphaned within a larger clade.\n\nOrphan genes arise from multiple sources, predominantly through de novo origination, duplication and rapid divergence, and horizontal gene transfer.\n\nNovel orphan genes continually arise de novo from non-coding sequences. These novel genes may be sufficiently beneficial to be swept to fixation by selection. Or, more likely, they will fade back into the non-genic background. This latter option is supported by research in Drosophila showing that young genes are more likely go extinct.\n\nDe novo genes were once thought to be a near impossibility due to the complex and potentially fragile intricacies of creating and maintaining functional polypeptides, but research from the past 10 years or so has found multiple examples of de novo genes, some of which are associated with important biological processes, particularly testes function in animals. De novo genes were also found in fungi and plants.\n\nFor young orphan genes, it is sometimes possible to find homologous non-coding DNA sequences in sister taxa, which is generally accepted as strong evidence of de novo origin. However, the contribution of de novo origination to taxonomically-restricted genes of older origin, particularly in relation to the traditional gene duplication theory of gene evolution, remains contested.\n\nThe duplication and divergence model for orphan genes involves a new gene being created from some duplication or divergence event and undergoing a period of rapid evolution where all detectable similarity to the originally duplicated gene is lost. While this explanation is consistent with current understandings of duplication mechanisms, the number of mutations needed to lose detectable similarity is large enough as to be a rare event, and the evolutionary mechanism by which a gene duplicate could be sequestered and diverge so rapidly remains unclear.\n\nAnother explanation for how orphan genes arise is through a duplication mechanism called horizontal gene transfer, where the original duplicated gene derives from a separate, unknown lineage. This explanation for the origin of orphan genes is especially relevant in bacteria and archaea, where horizontal gene transfer is common.\n\nOrphans genes tend to be very short (~6 times shorter than mature genes), and some are weakly expressed, tissue specific and simpler in codon usage and amino acid composition. Orphan genes tend to encode more intrinsically disordered proteins, although some structure has been found in one of the best characterized orphan genes. \nOf the tens of thousands of enzymes of primary or specialized metabolism that have been characterized to date, none are orphans, or even of restricted lineage; apparently, catalysis requires hundreds of millions of years of evolution.\n\nWhile the prevalence of orphan genes has been established, the evolutionary role of orphans, and its resulting importance, is still being debated. One theory is that many orphans have no evolutionary role; genomes contain non-functional open reading frames (ORFs) that create spurious polypeptide products not maintained by selection, meaning that they are unlikely to be conserved between species and would likely be detected as orphan genes. However, a variety of other studies have shown that at least some orphans are functionally important and may help explain the emergence of novel phenotypes.\n"}
{"id": "31992661", "url": "https://en.wikipedia.org/wiki?curid=31992661", "title": "Parataxical Integration", "text": "Parataxical Integration\n\nFirst used by Irish-American psychoanalytic psychiatrist Harry Stack Sullivan in the 1940s, Parataxical Integration (a combination of terms) refers to the mutual condition of parataxic distortions (another concept of Sullivan’s). Parataxical integration exists when two people, usually intimate with each other (i.e. parents and children, spouses, romantic partners, business associates), are reciprocally reactive to each other’s seductions, judgmental inaccuracies, hostile comments, and manipulations or other \"triggering\" behaviors. One says or does something causing the other to react, setting off a cyclical \"ping-pong\", \"tit-for-tat\", \"you-get-me-and-I-get-you-back\" oscillation of verbal and/or behavioral reactions.\n\nThe concept first appeared in Sullivan's \"The Interpersonal Theory of Psychiatry\", published in 1953. It was developed further by his protégé, Lorna Smith Benjamin, in her \"Interpersonal Diagnosis and Treatment of Personality Disorders\" (1996). Benjamin saw parataxical integration as typical in the interpersonal behavior of couples with unresolved autonomy (i.e. separation, boundary) and identity issues. Erik Erikson had himself described the unconscious, reciprocal reactivation (without using Sullivan’s terms) in his essay, “The Problem of Ego Identity,” and in \"Identity and Anxiety\", by Stein \"et al\". (1960).\n\nThough the term itself is not used in much of the professional peer-reviewed literature, the interpersonal manifestation to which it refers appears regularly in the case study literature of the \"family systems\" school of psychologists, including Don D. Jackson, Jay Haley, Gregory Bateson, Virginia Satir, and Salvador Minuchin. Parataxical integrations are also presented in similar studies reported by Ronald D. Laing, Aaron Esterson, and anthropologist Jules Henry, largely during the 1950s and 1960s. Harold Searles and Charles McCormack describe manifestations of parataxical integration in their works on borderline personality disorders in the 1980s and 2000s.\n\nPaul Watzlawick \"et al\". describes the concept in his book, \"Change\", noting, \"... the circularity of their interaction makes it undecidable ... whether a given action is the cause or effect of an action by the other party ... either party sees its actions as determined and provoked by the other's actions ...\".\n\nRodger Garrett also employs the concept in his millennial-era work on borderline personality disorder and family of origin etiology, typically using the term “reciprocal reactivity” along with it.\n\nReciprocal reactivity was studied by Gary Sperduto \"et al\". in the 1970s, and it is clear from the abstract of his paper (see below) that his definitional terminology equated to that of Sullivan.\n\nNumerous mass-market psychology authors, many writing about the topic of \"co-dependence,\" including Melody Beattie, Pia Mellody, Anne Wilson Schaef, and Barry & Janae Weinhold, describe the interpersonal manifestation without using Sullivan’s term \"per se\". Co-dependence expert Pia Mellody describes the behavioral manifestations of parataxical integration at length in an audio presentation available online.\n\n L. S.: \"Interpersonal Diagnosis and Treatment of Personality Disorders\", Second Edition, New York: Guilford Press, 1996.\n"}
{"id": "1281455", "url": "https://en.wikipedia.org/wiki?curid=1281455", "title": "Peak signal-to-noise ratio", "text": "Peak signal-to-noise ratio\n\nPeak signal-to-noise ratio, often abbreviated PSNR, is an engineering term for the ratio between the maximum possible power of a signal and the power of corrupting noise that affects the fidelity of its representation. Because many signals have a very wide dynamic range, PSNR is usually expressed in terms of the logarithmic decibel scale.\n\nPSNR is most easily defined via the mean squared error (\"MSE\"). Given a noise-free \"m\"×\"n\" monochrome image \"I\" and its noisy approximation \"K\", \"MSE\" is defined as:\n\nThe PSNR (in dB) is defined as:\n\nHere, \"MAX\" is the maximum possible pixel value of the image. When the pixels are represented using 8 bits per sample, this is 255. More generally, when samples are represented using linear PCM with \"B\" bits per sample, \"MAX\" is 2−1.\n\nFor color images with three RGB values per pixel, the definition of PSNR is the same except the MSE is the sum over all squared value differences divided by image size and by three. Alternately, for color images the image is converted to a different color space and PSNR is reported against each channel of that color space, e.g., YCbCr or HSL.\n\nPSNR is most commonly used to measure the quality of reconstruction of lossy compression codecs (e.g., for image compression). The signal in this case is the original data, and the noise is the error introduced by compression. When comparing compression codecs, PSNR is an \"approximation\" to human perception of reconstruction quality.\n\nTypical values for the PSNR in lossy image and video compression are between 30 and 50 dB, provided the bit depth is 8 bits, where higher is better. For 16-bit data typical values for the PSNR are between 60 and 80 dB. Acceptable values for wireless transmission quality loss are considered to be about 20 dB to 25 dB.\n\nIn the absence of noise, the two images \"I\" and \"K\" are identical, and thus the MSE is zero. In this case the PSNR is infinite (or undefined, see Division by zero).\n\nAlthough a higher PSNR generally indicates that the reconstruction is of higher quality, in some cases it may not. One has to be extremely careful with the range of validity of this metric; it is only conclusively valid when it is used to compare results from the same codec (or codec type) and same content.\n\nGenerally, PSNR has been shown to perform poorly compared to other quality metrics when it comes to estimating the quality of images and particularly videos as perceived by humans.\n\nPSNR-HVS is an extension of PSNR that incorporates properties of the human visual system such as contrast perception.\n\nPSNR-HVS-M improves on PSNR-HVS by additionally taking into account visual masking. In a 2007 study, it delivered better approximations of human visual quality judgements than PSNR and SSIM by large margin. It was also shown to have a distinct advantage over DCTune and PSNR-HVS.\n\n"}
{"id": "24401", "url": "https://en.wikipedia.org/wiki?curid=24401", "title": "Psychology of torture", "text": "Psychology of torture\n\nTorture is the use of physical or psychological pain to control the victim or fulfill some needs of the perpetrator. The psychology of torture refers to the psychological processes underlying all aspects of torture including the relationship between the perpetrator and the victim, the immediate and long-term effects, and the political and social institutions that influence its use.\n\nResearch during the past 60 years, starting with the Milgram experiment, suggests that under the right circumstances, and with the appropriate encouragement and setting, most people can be encouraged to actively torture others.\n\nJohn Conroy:\"When torture takes place, people believe they are on the high moral ground, that the nation is under threat and they are the front line protecting the nation, and people will be grateful for what they are doing.\"\n\nStages of the perpetrator's torture mentality include:\n\n\nExample:\nOne of the apparent ringleaders of the Abu Ghraib prison torture, Charles Graner Jr., exemplified the stages of dehumanization and disinhibition when he was reported to have said, \"The Christian in me says it's wrong, but the corrections officer in me says, 'I love to make a grown man piss himself.'\"\n\nAs P. Saliya Sumanatilake concludes:\n\n\"Whether it be for securing a justifiable or reprehensible end, torture cannot be effectuated without invoking and focusing one's diffused innate cruelty. Accordingly, it is the prevalence of this congenital trait of heinousness that renders every human being a potential torturer: hence, the existence of torture! Moreover, it is the natural occurrence of such nascent evil within each successive generation of human beings that serves to propagate torture!\"\n\nThe effects of torture on the victim and the perpetrator are likely to be influenced by many factors. Therefore, it is unlikely that providing diagnostic categories of symptoms and behavior will be applicable across countries with very different personal, political or religious beliefs and perspectives. There is always a question about applying diagnostic categories and descriptions of symptoms or behavior developed in Western societies to people from the developing countries with very different personal, political, or religious beliefs and perspectives. One of the most marked cultural differences may occur between individualist societies where realization of personal goals often takes priority over the needs of kin and societal expectations, and collectivist societies in which the needs of family and prescribed roles take precedence over personal preferences. Another evident difference is the belief in a subsequent life in which suffering in this life is rewarded, and this has emerged in some studies of torture survivors in South East Asia.\n\nTorture has profound and long-lasting physical and psychological effects. Torture is a form of collective suffering that is not limited to the victim. The victims' family members and friends are often also affected due to adjustment problems such as outbreaks of anger and violence directed towards family members. According to research, psychological and physical torture have similar mental effects. Often torture victims suffer from elevated rates of the following:\n\nNo diagnostic terminology encapsulates the deep distrust of others which many torture survivors have developed, nor the destruction of all that gave their lives meaning. Guilt and shame about humiliation during torture, and about the survivor's inability to withstand it, as well as guilt at surviving, are common problems which discourage disclosure. Additional stress may be added due to uncertainty about the future, any possibility of being sent back to the country in which the survivor was tortured, and the potential lack of close confidants or social support systems. In addition, the presence of social isolation, poverty, unemployment, institutional accommodation, and pain can all predict higher levels of emotional distress in victims who survive torture.\n\nThe development of the diagnosis of posttraumatic stress disorder (PTSD) for American veterans of the Vietnam War can be understood as a political act which labeled the collective distress of a defeated USA as individual psychopathology. Proponents of this view, point to the de-politicization of the distress of torture survivors by describing their distress, disturbance, and profound sense of injustice in psychiatric terms. These are not only conceptual issues, because they may influence treatment outcomes. Recovery is associated with reconstruction of social and cultural networks, economic supports, and respect for human rights.\n\nThe rich research on treatment of PTSDs in veterans has substantially informed treatment offered to torture survivors. It is more appropriate than extrapolation from work with civilian survivors of single events as individuals (assault, accidents) or as communities or groups (natural or man-made disasters). Some literature distinguishes between single-event trauma (type 1) and prolonged and repeated trauma, such as torture (type 2). There is no doubt that (disregarding concerns about the diagnosis) rates of PTSD are much higher in refugees than among people of a similar age in the countries where the refugees settle, and that, among refugees, rates of PTSD are even higher among those seeking asylum.\n\nThe argument that torture causes unique problems waxes and wanes, and is often associated with claims to particular expertise in treatment, and therefore claims on funding. Gurr et al. describe how torture targets the person as a whole – physically, emotionally, and socially – so that PTSD is an inadequate description of the magnitude and complexity of the effects of torture. When the diagnosis of PTSD is applied, some survivors of torture who have very severe symptoms related to trauma may still not reach the criteria for diagnosis. Categories such as 'complex trauma' have been proposed, and it may be that the next iterations of the diagnostic compendia may modify the criteria\n\nMany people who engage in torture have various psychological deviations and often they derive sadistic satisfaction. Torture may fulfill the emotional needs of perpetrators when they willingly engage in these activities. They lack empathy and their victims' agonized painful reactions, screaming and pleading give them a sense of authority and feelings of superiority.\n\nTorture can harm not only the victim but the perpetrators as well. After the fact, perpetrators will often experience failing mental health, PTSD, suicidal tendencies, substance dependency and a myriad of other mental defects associated with inducing physical or mental trauma upon their victims.\n\nThe perpetrators may experience flashbacks of torture, intense rage, suicidal and homicidal ideas, alienation, impulse deregulation, alterations in attention and consciousness, alterations in self-perception, alterations in relationships with others, inability to trust and inability to maintain long-term relationships, or even mere intimacy.\n\nFor physicians, it is useful to recognize that symptoms of post-traumatic stress can complicate presentation and treatment. Pain predicts greater severity of both PTSD symptoms and major depression, and intrusive memories and flashbacks can exacerbate existing pain. While under-recognition and under-treatment of torture victims is common, there are useful guidelines for evidence-based medical practice, although not specifically concerned with pain, and for evidence-based psychological practice.\n\nSome people die during torture; many survivors are too disabled and destitute to find their way to safety. A large element of chance, and, to a lesser extent, resources and resilience, enable a minority to arrive in developed countries. Nevertheless, they often present multiple and complex problems, which the clinician can find overwhelming. For all these reasons, an interdisciplinary approach to assessment and treatment is therefore recommended, guarding against either disregarding significant psychological distress as inevitable in torture survivors or discounting physical symptoms by attributing them to psychological origin.\n\nRehabilitation and reparation are part of the rights of the torture survivor under the United Nations Convention, yet far less attention is paid to health needs on a national or international basis than to legal and civil claims. Collaborative efforts involving survivors themselves are needed to better understand the usefulness and limitations of existing assessment instruments and treatment methods. Some studies exist, such as that by Elsass et al. who interviewed Tibetan Lamas on the quantification of suffering in scales used to evaluate intervention with Tibetan torture survivors.\nEducation of medical and other healthcare personnel needs to address issues concerning treatment of torture survivors, who will be seen in all possible settings but not necessarily recognized or treated adequately. Teaching on ethics is also important, since medical students can have tolerant views of torture, and the complicity of medical and healthcare staff in torture continues in many countries. Medical staff are often in a key position to try to prevent torture and to help those who have survived.\n\nIn addition to providing treatment for victims of torture, psychologists have the skills and knowledge to conduct research regarding interrogation methods and determine when the methods used become torture. The standards, policies, and procedures of each country's professional psychological association may influence the participation of psychologists in administering torture, researching torture methods, and evaluating the effectiveness of the results. Kenneth Pope (2011) used direct quotes to indicate the American Psychological Association believes psychologists have a key role in eliciting information from people since interrogations require an understanding of psychological processes. Each professional association sets the standards for ethics and expected professional behavior which may influence psychologist researchers who investigate interrogation or torture and clinical psychologists' participation in interrogations that use methods deemed to be consistent with torture.\n\nFor an example of policy that influences the use of torture by American psychologists, please see the American Psychological Association Council of Representatives policy released in 2015. For an example of an external review of whether psychologists adhered to the APA ethics and policy please see the Hoffman Report (2015).\n\nDue to differences in political power globally, professional psychological organizations in well-developed countries may have a greater influence on discovering and defining what constitutes torture. Psychological associations in less developed countries may choose to adopt the definitions, standards, and ethical positions regarding torture developed by the APA when they are unable to support research regarding torture themselves within their own culture. The professional associations in well developed countries, such as the APA are likely to have a strong influence in defining the psychology of torture globally.\n\nPeople within an organization may be influenced to participate in torturing people. The culture and procedures of an organization provide the foundation to allow professionals, such as physicians, to violate the medical code of ethics in a manner that appears to align and meet the necessary standards of their employment. Annas and Crosby (2015) reported that lawyers provided advanced confirmation that physicians who participated in the enhanced interrogation techniques used at CIA sites would be given immunity for their actions since they were deemed a necessary requirement to protect the country (see also; Milgram experiment ) . \n\nThe physicians assisted by providing medical evaluations to ensure victims were healthy enough to undergo torture, developed methods of torture, ensured victims would survive the torture, and assisted victims to heal following torture procedures. Working in a secret facility with policies and procedures that promoted an expectation that torture and enhanced interrogation practices were required to protect the nation and would not result in negative personal consequences resulted in a setting in which physicians were willing to ignore the Hippocratic oath.\n\nThe policies and procedures within the United States military have also been found to produce an environment in which torture and enhanced interrogation techniques were used. Although the military has an excellent process for recruiting and training interrogators who use non abusive techniques successfully, changes in funding resulted in fewer highly trained interrogators being available. As more interrogators were recruited after 9/11, they were not as rigorously assessed, trained, or mentored and did not demonstrate the same abilities as the previous generation of military interrogators. In addition, the military rank of interrogators is not sufficient to control the decisions made when interrogation is needed. Military interrogators may be ordered to perform techniques they know to be inappropriate and ineffective by higher ranking officers who have not been adequately educated about effective interrogation procedures. The combination of a change in recruitment, reduced education and mentorship, and relatively low rank result in opportunities for torture and abuse to be used during interrogations.\n\nFictional stories, movies, and television shows may influence the beliefs people have regarding the efficacy of torture as a means for rapidly obtaining life-saving information. People who believe torture is an effective interrogation method are more supportive of using torture and enhanced interrogation techniques than those who do not think it provides accurate information. In addition, the information obtained through torture is also perceived as more valuable by people who support using torture than the same information obtained through non-abusive means of interrogation. These findings suggest confirmation bias (perception is skewed toward what a person already believes) influences the support for torture and is influenced by many commercially available sources of fictional examples.\n\n\n\n"}
{"id": "8874869", "url": "https://en.wikipedia.org/wiki?curid=8874869", "title": "Recognition (sociology)", "text": "Recognition (sociology)\n\nRecognition in sociology is public acknowledgement of person's status or merits (achievements, virtues, service, etc.). In the field of psychology, it is understood that a person who seeks excessive recognition could themselves be exhibiting traits of a narcissistic personality disorder.\n\nWhen some person is recognized, he or she is accorded some special status, such as title or classification. \n\n"}
{"id": "1837058", "url": "https://en.wikipedia.org/wiki?curid=1837058", "title": "Relaxation technique", "text": "Relaxation technique\n\nA relaxation technique (also known as relaxation training) is any method, process, procedure, or activity that helps a person to relax; to attain a state of increased calmness; or otherwise reduce levels of pain, anxiety, stress or anger. Relaxation techniques are often employed as one element of a wider stress management program and can decrease muscle tension, lower the blood pressure and slow heart and breath rates, among other health benefits.\n\nPeople respond to stress in different ways, namely, by becoming overwhelmed, depressed or both. Yoga, QiGong, Taiji, and Pranayama that includes deep breathing tend to calm people who are overwhelmed by stress, while rhythmic exercise improves the mental and physical health of those who are depressed. People who encounter both symptoms simultaneously, feeling depressed in some ways and overexcited in others, may do best by walking or performing yoga techniques that are focused on strength.\n\nResearch has indicated that removing stress helps to increase a person's health.\n\nResearch released in the 1980s indicated stronger ties between stress and health and showed benefits from a wider range of relaxation techniques than had been previously known. This research received national media attention, including a \"New York Times\" article in 1986.\n\nPeople use relaxation techniques for the following reasons, among others:\n\nVarious techniques are used by individuals to improve their state of relaxation. Some of the methods are performed alone; some require the help of another person (often a trained professional); some involve movement, some focus on stillness; while other methods involve different elements.\n\nCertain relaxation techniques known as \"formal and passive relaxation exercises\" are generally performed while sitting or lying quietly, with minimal movement and involve \"a degree of withdrawal\". These include:\n\nMovement-based relaxation methods incorporate exercise such as walking, gardening, yoga, T'ai chi, Qigong, and more. Some forms of bodywork are helpful in promoting a state of increased relaxation. Examples include massage, acupuncture, the Feldenkrais Method, myotherapy, reflexology and self-regulation.\n\nSome relaxation methods can also be used during other activities, for example, autosuggestion and prayer. At least one study has suggested that listening to certain types of music, particularly new-age music and classical music, can increase feelings associated with relaxation, such as peacefulness and a sense of ease.\n\nA technique growing in popularity is flotation therapy, which is the use of a float tank in which a solution of Epsom salt is kept at skin temperature to provide effortless floating. Research in USA and Sweden has demonstrated a powerful and profound relaxation after twenty minutes. In some cases, floating may reduce pain and stress and has been shown to release endorphins.\n\nEven actions as simple as a walk in the park have been shown to aid feelings of relaxation, regardless of the initial reason for the visit.\n\n"}
{"id": "12856226", "url": "https://en.wikipedia.org/wiki?curid=12856226", "title": "Scalar expectancy", "text": "Scalar expectancy\n\nThe scalar timing or scalar expectancy theory (SET) is a model of the processes that govern behavior controlled by time. The model posits an internal clock, and particular memory and decision processes. SET is one of the most important models of animal timing behavior.\n\nJohn Gibbon originally proposed SET to explain the temporally controlled behavior of non-human subjects. He initially used the model to account for a pattern of behavior seen in animals that are being reinforced at fixed-intervals, for example every 2 minutes. An animal that is well trained on such a fixed-interval schedule pauses after each reinforcement and then suddenly starts responding about two-thirds of the way through the new interval. (See operant conditioning) The model explains how the animal's behavior is controlled by time in this manner. Gibbon and others later elaborated the model and applied it to a variety of other timing phenomena.\n\nSET assumes that the animal has a clock, a working memory, a reference memory, and a decision process. The clock contains a discrete pacemaker that generates pulses like the ticks a mechanical clock. A stimulus that signals the start of a timed interval closes a switch, allowing pulses to enter an accumulator. The resulting accumulation of pulses represents elapsed time, and this time value is continuously sent to a working memory. When reinforcement happens at the end of the timed interval, the time value is stored in a long-term reference memory. This time-to-reinforcement in reference memory represents the expected time to reinforcement.\n\nKey to the SET model is the decision process that controls timing behavior. While the animal is timing some interval it continually compares the current time (stored in working memory) to the expected time (stored in reference memory). Specifically, the animal continually samples from its memory of past times at which reinforcement occurred and compares this memory sample with the current time on its clock. When the two values are close to one another the animal responds; when they are far enough apart, the animal stops responding. To make this comparison, it computes the ratio of the two values; when the ratio is less than a certain value it responds, when the ratio is larger it does not respond.\n\nBy using a ratio of current time to expected time, rather than, for example, simply subtracting one from the other, SET accounts for a key observation about animal and human timing. That is, timing accuracy is relative to the size of the interval being timed. This is the \"scalar\" property that gives the model its name. For example, when timing a 10 sec interval an animal might be accurate to within 1 sec, whereas when timing a 100 sec interval the animal would be accurate to only about 10 sec. Thus time perception is like the perception of lights, sounds, and other sensory events, where accuracy also is relative to the size (brightness, loudness, etc.) of the percept being judged. (See Weber-Fechner law.)\n\nA number of alternative models of timing have appeared over the years. These include Killeen’s Behavioral Theory of timing (BeT) model and Machado’s learning-to-time (LeT) model.\n\nIn 1993, John Wearden claimed that human behavior exhibits appropriate scalar properties, as was indicated by experiments on internal production with concurrent chronometric counting. However, human timing behavior is undoubtedly more varied than animal timing behavior. A major factor responsible for this variability is attentional allocation. \n"}
{"id": "24226726", "url": "https://en.wikipedia.org/wiki?curid=24226726", "title": "Starlight Information Visualization System", "text": "Starlight Information Visualization System\n\nStarlight is a software product originally developed at Pacific Northwest National Laboratory and now by Future Point Systems. It is an advanced visual analysis environment. In addition to using information visualization to show the importance of individual pieces of data by showing how they relate to one another, it also contains a small suite of tools useful for collaboration and data sharing, as well as data conversion, processing, augmentation and loading.\n\nThe software, originally developed for the intelligence community, allows users to load data from XML files, databases, RSS feeds, web services, HTML files, Microsoft Word, PowerPoint, Excel, CSV, Adobe PDF, TXT files, etc. and analyze it with a variety of visualizations and tools. The system integrates structured, unstructured, geospatial, and multimedia data, offering comparisons of information at multiple levels of abstraction, simultaneously and in near real-time. In addition Starlight allows users to build their own named entity-extractors using a combination of algorithms, targeted normalization lists and regular expressions in the Starlight Data Engineer (SDE).\n\nAs an example, Starlight might be used to look for correlations in a database containing records about chemical spills. An analyst could begin by grouping records according to the cause of the spill to reveal general trends. Sorting the data a second time, they could apply different colors based on related details such as the company responsible, age of equipment or geographic location. Maps and photographs could be integrated into the display, making it even easier to recognize connections among multiple variables.\n\nStarlight has been deployed to both the Iraq and Afghanistan wars and used on a number of large-scale projects.\n\nPNNL began developing Starlight in the mid-90's, with funding from the Land Information Warfare Agency, a part of the Army Intelligence and Security Command and continued developed at the laboratory with funding from the NSA and the CIA. Starlight integrates visual representations of reports, radio transcripts, radar signals, maps and other information. The software system was recently honored with an R&D 100 Award for technical innovation.\n\nIn 2006 Future Point Systems, a Silicon Valley startup, acquired rights to jointly develop and distribute the Starlight product in cooperation with the Pacific Northwest National Laboratory.\n\nThe software is now also used outside of the military/intelligence communities in a number of commercial environments.\n\n\n"}
{"id": "21705001", "url": "https://en.wikipedia.org/wiki?curid=21705001", "title": "The God of the Machine", "text": "The God of the Machine\n\nThe God of the Machine is a book written by Isabel Paterson and published in 1943 in the United States. At the time of its release, it was considered a cornerstone to the philosophy of individualism. Her biographer, Stephen D. Cox, in 2004 described Paterson as the \"earliest progenitor of libertarianism as we know it today\".\n\nIsabel Paterson wrote a regular column for the \"New York Herald Tribune\", where she first articulated many of her beliefs, which reached their final form in \"The God of the Machine\". She also foreshadowed those ideas, especially free trade, in her historical novels during the 1920s and 1930s.\n\nPaterson opposed most parts of the economic program, known as the New Deal, which US president Franklin D. Roosevelt and the Congress put into effect during the 1930s, and she advocated less governmental involvement in social and fiscal matters. She also led a group of younger friends (many of whom were other employees of the \"Herald Tribune\") who shared her views. One member of that group was the young Ayn Rand.\n\nPaterson and Rand promoted each other's books, and they conducted an extensive exchange of letters, touching on religion and philosophy. That correspondence ended with a personal quarrel in 1948. Rand, an atheist, was critical of the attempts of Paterson, a deist, to link capitalism with religion, things that Rand considered incompatible.\n\n"}
{"id": "37813467", "url": "https://en.wikipedia.org/wiki?curid=37813467", "title": "The personal is political", "text": "The personal is political\n\nThe personal is political, also termed The private is political, is a political argument used as a rallying slogan of student movement and second-wave feminism from the late 1960s. It underscored the connections between personal experience and larger social and political structures. In the context of the feminist movement of the 1960s and 1970s, it was a challenge to the nuclear family and family values. The phrase has been repeatedly described as a defining characterization of second-wave feminism, radical feminism, women's studies, or feminism in general.\n\nThe phrase was popularized by the publication of a 1969 essay by feminist Carol Hanisch under the title \"The Personal is Political\" in 1970, but she disavows authorship of the phrase. According to Kerry Burch, Shulamith Firestone, Robin Morgan, and other feminists given credit for originating the phrase have also declined authorship. \"Instead,\" Burch writes, \"they cite millions of women in public and private conversations as the phrase's collective authors.\" Gloria Steinem has likened claiming authorship of the phrase to claiming authorship of \"World War II.\"\n\nThe phrase figured in women-of-color feminism, such as \"A Black Feminist Statement\" by the Combahee River Collective, Audre Lorde's essay \"The Master's Tools Will Never Dismantle the Master's House\", and the anthology \"\", edited by Gloria E. Anzaldúa and Cherríe Moraga. More broadly, as Kimberlé Williams Crenshaw observes: \"This process of recognizing as social and systemic what was formerly perceived as isolated and individual has also characterized the identity politics of African Americans, other people of color, and gays and lesbians, among others.\"\n\nCarol Hanisch, a member of New York Radical Women and a prominent figure in the Women's Liberation Movement, drafted an article defending the political importance of consciousness-raising groups in February 1969 in Gainesville, Florida. Originally addressed to the women's caucus of the Southern Conference Educational Fund, the paper was first given the title, \"Some Thoughts in Response to Dottie [Zellner]'s Thoughts on a Women's Liberation Movement\". Hanisch was then a New York City-based staffer of the Fund and was advocating for it to engage in dedicated organizing for women's liberation in the American South. Hanisch sought to rebut the idea that sex, appearance, abortion, childcare, and the division of household labor were merely personal issues without political importance. To confront these and other issues, she urged women to overcome self-blame, discuss their situations amongst each other, and organize collectively against male domination of society. Hanisch does not use the phrase \"the personal is political\" in the essay, but writes:\nThe essay was published under the title, \"The Personal Is Political\", in \"Notes from the Second Year: Women's Liberation\" in 1970. The essay's author believes that Shulamith Firestone and Anne Koedt, the book's editors, gave the essay its famous title. It has since been reprinted in \"Radical Feminism: A Documentary Reader\".\n\nWhile the connection between women's personal experience and their subordination as women is highlighted by this phrase, feminists have interpreted the nature of that connection and the desired form of political action that emerges from it in widely divergent ways.\n\n\nPaula Rust compiled a list of interpretations of the phrase within feminist movements including the following: \"The personal reflects the political status quo (with the implication that the personal should be examined to provide insight into the political); the personal serves the political status quo; one can make personal choices in response to or protest against the political status quo; ... one's personal choices reveal or reflect one's personal politics; one should make personal choices that are consistent with one's personal politics; personal life and personal politics are indistinguishable.\"\n\nWriting in 2006, Hanisch observed, \"Like most of the theory created by the Pro-Woman Line radical feminists, these ideas have been revised or ripped off or even stood on their head and used against their original, radical intent.\"\n"}
{"id": "4041766", "url": "https://en.wikipedia.org/wiki?curid=4041766", "title": "Transpartisan", "text": "Transpartisan\n\nTranspartisanship represents an emerging school of political thought which accepts the validity of truths across a range of political perspectives and seeks to synthesize them into an inclusive, pragmatic container beyond typical political dualities. It is distinct from bipartisanship, which aims to negotiate between “right” and “left,” resulting in a dualistic perspective, and nonpartisanship, which tends to avoid political affiliation altogether.\n\nTranspartisanship is a movement to support and advance a common ground - or \"new center\" - that already exists in U.S. politics, emerging periodically into public view in the form of \"unusual coalitions\" of progressives and conservatives around issues ranging from war and the military budget to corporate power and the surveillance state.\n\nThe movement builds on methods of facilitated dialogue, deliberation and conflict resolution.\n\nCurrent examples of transpartisan initiatives include Transpartisan Center, TheSolution.org, Reuniting America, Transpartisan Alliance, and Liberty Coalition.\n\n\nTranspartisan gatherings have resulted not only in surprisingly civil conversations noted by mainstream media but also in shifts from traditional ideological stances by some participants.\n\nA close relative of transpartisanship is Integral politics. A transpartisan approach to policy would necessarily include individual and collective, as well as subjective and objective, perspective. Furthermore, similar to Integral theory, transpartisanship places politics in a developmental context, viewing democracy and prosperity not as static attainments, but rather emergent properties along a continuum of developmental stages.\n\nIn 2016, Emmanuel Macron created a new French political party, En Marche. The party sought to transcend traditional political boundaries to be a transpartisan organisation.\n\nMacron has described the party as being a progressive party uniting the left and the right. Observers and political commentators have described the party as being both socially and economically liberal in ideology, Emmanuel Macron became the President of France. The party also won the National Assembly elections a month later, as candidates in the legislative elections included members of the Democratic Movement, as well as dissidents from the Socialist Party, The Republicans and minor parties. It won an absolute majority of seats in the National Assembly, securing 308 under its label and 42 for the MoDem.\n\n\n"}
{"id": "1034511", "url": "https://en.wikipedia.org/wiki?curid=1034511", "title": "Tuberculosis classification", "text": "Tuberculosis classification\n\nThe current clinical classification system for tuberculosis (TB) is based on the pathogenesis of the disease.\n\nHealth care providers should comply with local laws and regulations requiring the reporting of TB. All persons with class 3 or class 5 TB should be reported promptly to the local health department. See list of notifiable diseases.\n\nThe U.S. Citizenship and Immigration Services has an additional TB classification for immigrants and refugees developed by the Centers for Disease Control and Prevention (CDC). The \"B notification program\" is an important screening strategy to identify new arrivals who have a high risk for TB.\n"}
{"id": "34882466", "url": "https://en.wikipedia.org/wiki?curid=34882466", "title": "Turing pattern", "text": "Turing pattern\n\nThe concept of a Turing pattern (often referred to in the plural as Turing patterns) was introduced by the English mathematician Alan Turing in a 1952 paper entitled \"The Chemical Basis of Morphogenesis\". This foundational paper describes the way in which patterns in nature such as stripes and spots can arise naturally out of a homogeneous, uniform state.\n\nThe original theory, a reaction–diffusion theory of morphogenesis, has served as an important model in theoretical biology. Reaction–diffusion systems have attracted much interest as a prototype model for pattern formation. Patterns such as fronts, hexagons, spirals, stripes and dissipative solitons are found in various types of reaction-diffusion systems, despite large discrepancies, in the local reaction terms for example.\n\nAs well as in biological organisms, Turing patterns occur in other natural systems – for example, the wind patterns formed in sand. Although Turing's ideas on morphogenesis and Turing patterns remained dormant for many years, they are now inspirational for much research in mathematical biology.\n\n\n"}
{"id": "780852", "url": "https://en.wikipedia.org/wiki?curid=780852", "title": "Two Dogmas of Empiricism", "text": "Two Dogmas of Empiricism\n\n\"Two Dogmas of Empiricism\" is a paper by analytic philosopher Willard Van Orman Quine published in 1951. According to City University of New York professor of philosophy Peter Godfrey-Smith, this \"paper [is] sometimes regarded as the most important in all of twentieth-century philosophy\". The paper is an attack on two central aspects of the logical positivists' philosophy. One is the analytic–synthetic distinction between analytic truths and synthetic truths, explained by Quine as truths grounded only in meanings and independent of facts, and truths grounded in facts. The other is reductionism, the theory that each meaningful statement gets its meaning from some logical construction of terms that refers exclusively to immediate experience.\n\n\"Two Dogmas\" has six sections. The first four focus on analyticity, the last two on reductionism. There, Quine turns the focus to the logical positivists' theory of meaning. He also presents his own holistic theory of meaning.\n\nMost of Quine's argument against analyticity in the first four sections is focused on showing that different explanations of analyticity are circular. The main purpose is to show that no satisfactory explanation of analyticity has been given.\n\nQuine begins by making a distinction between two different classes of analytic statements. The first one is called logically true and has the form:\n\nA sentence with that form is true independent of the interpretation of \"man\" and \"married\", so long as the logical particles \"no\", \"un-\" and \"is\" have their ordinary English meaning.\n\nThe statements in the second class have the form:\n\nA statement with this form can be turned into a statement with form (1) by exchanging synonyms with synonyms, in this case \"bachelor\" with \"unmarried man\". It is the second class of statements that lack characterization according to Quine. The notion of the second form of analyticity leans on the notion of synonymy, which Quine believes is in as much need of clarification as analyticity. Most of Quine's following arguments are focused on showing how explanations of synonymy end up being dependent on the notions of analyticity, necessity, or even synonymy itself.\n\nHow do we reduce sentences from the second class to a sentence of the first class? Some might propose \"definitions\". \"No bachelor is married\" can be turned into \"No unmarried man is married\" because \"bachelor\" is defined as \"unmarried man\". But, Quine asks: how do we find out that \"bachelor\" is defined as \"unmarried man\"? Clearly, a dictionary would not solve the problem, as a dictionary is a report of already known synonyms, and thus is dependent on the notion of synonymy, which Quine holds as unexplained.\n\nA second suggestion Quine considers is an explanation of synonymy in terms of interchangeability. Two linguistic forms are (according to this view) synonymous if they are interchangeable in all contexts without changing the truth-value. But consider the following example:\n\nObviously \"bachelor\" and \"unmarried man\" are not interchangeable in that sentence. To exclude that example and some other obvious counterexamples, such as poetic quality, Quine introduces the notion of cognitive synonymy. But does interchangeability hold as an explanation of cognitive synonymy? Suppose we have a language without modal adverbs like \"necessarily\". Such a language would be extensional, in the way that two predicates which are true about the same objects are interchangeable again without altering the truth-value. Thus, there is no assurance that two terms that are interchangeable without the truth-value changing are interchangeable because of meaning, and not because of chance. For example, \"creature with a heart\" and \"creature with kidneys\" share extension.\n\nIn a language with the modal adverb \"necessarily\" the problem is solved, as \"salva veritate\" holds in the following case:\n\nwhile it does not hold for\n\nPresuming that 'creature with a heart' and 'creature with kidneys' have the same extension, they will be interchangeable \"salva veritate\". But this interchangeability rests upon both empirical features of the language itself and the degree to which extension is empirically found to be identical for the two concepts, and not upon the sought for principle of cognitive synonymy.\n\nIt seems that the only way to assert the synonymy is by supposing that the terms 'bachelor' and 'unmarried man' are synonymous and that the sentence \"All and only all bachelors are unmarried men\" is analytic. But for \"salva veritate\" to hold as a definition of something more than extensional agreement, i.e., cognitive synonymy, we need a notion of necessity and thus of analyticity.\n\nSo, from the above example, it can be seen that in order for us to distinguish between analytic and synthetic we must appeal to synonymy; at the same time, we should also understand synonymy with interchangeability \"salva veritate\". However, such a condition to understand synonymy is not enough so we not only argue that the terms should be interchangeable, but necessarily so. And to explain this logical necessity we must appeal to analyticity once again.\n\nAnalyticity would be acceptable if we allowed for the verification theory of meaning: an analytic statement would be one synonymous with a logical truth, which would be an extreme case of meaning where empirical verification is not needed, because it is \"confirmed no matter what\". \"So, if the verification theory can be accepted as an adequate account of statement synonymy, the notion of analyticity is saved after all.\"\n\nThe problem that naturally follows is how statements are to be verified. An empiricist would say that it can only be done using empirical evidence. So some form of reductionism - \"the belief that each meaningful statement is equivalent to some logical construct upon terms which refer to immediate experience\" - must be assumed in order for an empiricist to 'save' the notion of analyticity. Such reductionism, says Quine, presents just as intractable a problem as did analyticity.\n\nIn order to prove that all meaningful statements can be translated into a sense-datum language, a reductionist would surely have to confront \"the task of specifying a sense-datum language and showing how to translate the rest of significant discourse, statement by statement, into it.\" To illustrate the difficulty of doing so, Quine describes Rudolf Carnap's attempt in his book \"Der logische Aufbau der Welt\".\n\nQuine first observes that Carnap's starting point was not the strictest possible, as his \"sense-datum language\" included not only sense-events but also \"the notations of logic, up through higher set theory... Empiricists there are who would boggle at such prodigality.\" Nonetheless, says Quine, Carnap showed great ingenuity in defining sensory concepts \"which, but for his constructions, one would not have dreamed were definable on so slender a basis.\" However, even such admirable efforts left Carnap, by his own admission, far short of completing the whole project.\n\nFinally, Quine objects in principle to Carnap's proposed translation of statements like \"quality q is at point-instant x;y;z;t\" into his sense-datum language, because he does not define the connective \"is at\". Without statements of this kind, it is difficult to see, even in principle, how Carnap's project could have been completed.\n\nThe difficulty that Carnap encountered shows that reductionism is, at best, unproven and very difficult to prove. Until a reductionist can produce an acceptable proof, Quine maintains that reductionism is another \"metaphysical article of faith\".\n\nInstead of reductionism, Quine proposes that it is the whole field of science and not single statements that are verified. All scientific statements are interconnected. Logical laws give the relation between different statements, while they also are statements of the system. This makes talk about the empirical content of a single statement misleading. It also becomes impossible to draw a line between synthetic statements, which depend on experience, and analytic statements, that hold come what may. Any statement can be held as necessarily true according to Quine, if the right changes are made somewhere else in the system. In the same way, no statements are immune to revision.\n\nEven logical laws can be revised according to Quine. Quantum logic, introduced by Garrett Birkhoff and John von Neumann, abandons the law of distributivity from classical logic in order to reconcile some of the apparent inconsistencies of classical Boolean logic with the facts related to measurement and observation in quantum mechanics. Quine makes the case that the empirical study of physics has furnished apparently credible grounds for replacing classical logic by quantum logic, rather as Newtonian physics gave way to Einsteinian physics. The idea that logical laws are not immune to revision in the light of empirical evidence has provoked an intense debate (see \"Is Logic Empirical?\").\n\nAccording to Quine, there are two different results of his reasoning. The first is a blurring of the line between metaphysics and natural science. The common-sense theory about physical objects is epistemologically comparable to the gods of Homer. Quine is a physicalist, in the sense that he considers it a scientific error not to adopt a theory which makes reference to physical objects. However, like Gods of Homer, physical objects are posits, and there is no great epistemic difference in kind; the difference is rather that the theory of physical objects has turned out to be a more efficient theory. As Quine states in \"Two Dogmas\", \"The myth of physical objects is epistemologically superior to most in that it has proved more efficacious than other myths as a device for working a manageable structure into the flux of\nexperience\".\n\nThe second result is a move towards pragmatism. Since, Quine says, the function of science is to predict future experiences in the light of past ones, the only ground for choosing which explanations to believe is \"the degree to which they expedite our dealings with sense experiences.\" While pragmatic concerns are important for Carnap and other logical positivists when choosing a linguistic framework, their pragmatism \"leaves off at the imagined boundary between the analytic and the synthetic\". For Quine, every change in the system of science is, when rational, pragmatic.\n\nRudolf Carnap prepared a reply entitled \"Quine on Analyticity\", but this was not published until 1990. Addressing Quine's concern over the status of the sentence \"Everything green is extended\", Carnap wrote \"the difficulty here lies in the unclarity of the word 'green', namely in an indecision over whether one should use the word for something unextended, i.e., for a single space-time point. In daily life it is never so used, and one scarcely ever speaks of space-time points.\" Carnap then puts forward that an exact artificial language ought to clarify the problem by defining 'green' (or its synonym) as something that is either necessarily or contingently not applied to space-time points. He wrote that once that decision is made, the difficulty is resolved. Carnap also answers Quine's argument on the use of sets of formal sentences to explain analyticity by arguing that this method is an explication of a poorly understood notion.\n\nPaul Grice and P. F. Strawson criticized \"Two Dogmas\" in their (1956) article \"In Defense of a Dogma\". Among other things, they argue that Quine's skepticism about synonyms leads to a skepticism about meaning. If statements can have meanings, then it would make sense to ask \"What does it mean?\". If it makes sense to ask \"What does it mean?\", then synonymy can be defined as follows: Two sentences are synonymous if and only if the true answer of the question \"What does it mean?\" asked of one of them is the true answer to the same question asked of the other. They also draw the conclusion that discussion about correct or incorrect translations would be impossible given Quine's argument. Four years after Grice and Strawson published their paper, Quine's book \"Word and Object\" was released. In the book Quine presented his theory of indeterminacy of translation.\n\nIn \"'Two Dogmas' revisited\", Hilary Putnam argues that Quine is attacking two different notions. Analytic truth defined as a true statement derivable from a tautology by putting synonyms for synonyms is near Kant's account of analytic truth as a truth whose negation is a contradiction. Analytic truth defined as a truth confirmed no matter what however, is closer to one of the traditional accounts of \"a priori\". While the first four sections of Quine's paper concern analyticity, the last two concern apriority. Putnam considers the argument in the two last sections as independent of the first four, and at the same time as Putnam criticizes Quine, he also emphasizes his historical importance as the first top rank philosopher to both reject the notion of apriority and sketch a methodology without it.\n\nJerrold Katz, a onetime associate of Noam Chomsky's, countered the arguments of \"Two Dogmas\" directly by trying to define analyticity non-circularly on the syntactical features of sentences.\n\nIn his book \"Philosophical Analysis in the Twentieth Century, Volume 1 : The Dawn of Analysis\" Scott Soames (pp 360–361) has pointed out that Quine's circularity argument needs two of the logical positivists' central theses to be effective:\n\nIt is only when these two theses are accepted that Quine's argument holds. It is not a problem that the notion of necessity is presupposed by the notion of analyticity if necessity can be explained without analyticity. According to Soames, both theses were accepted by most philosophers when Quine published \"Two Dogmas\". Today however, Soames holds both statements to be antiquated.\n\n\n\n"}
{"id": "161881", "url": "https://en.wikipedia.org/wiki?curid=161881", "title": "Uniqueness type", "text": "Uniqueness type\n\nIn computing, a unique type guarantees that an object is used in a single-threaded way, with at most a single reference to it. If a value has a unique type, a function applied to it can be optimized to update the value in-place in the object code. Such in-place updates improve the efficiency of functional languages while maintaining referential transparency. Unique types can also be used to integrate functional and imperative programming.\n\nUniqueness typing is best explained using an example. Consider a function codice_1 that reads the next line of text from a given file:\n\nNow codice_2 reads the next line from the file using an OS-level system call which has the side effect of changing the current position in the file. But this violates referential transparency because calling it multiple times with the same argument will return different results each time as the current position in the file gets moved. This in turn makes codice_1 violate referential transparency because it calls codice_2.\n\nHowever, using uniqueness typing, we can construct a new version of codice_1 that is referentially transparent even though it's built on top of a function that's not referentially transparent:\n\nThe codice_6 declaration specifies that the type of codice_7 is unique; that is to say that codice_7 may never be referred to again by the caller of codice_9 after codice_9 returns, and this restriction is enforced by the type system. And since codice_9 does not return codice_7 itself but rather a new, different file object codice_13, this means that it's impossible for codice_9 to be called with codice_7 as an argument ever again, thus preserving referential transparency while allowing for side effects to occur.\n\nUniqueness types are implemented in the functional programming languages Clean, Mercury and Idris. They are sometimes used for doing I/O operations in functional languages in lieu of monads.\n\nA compiler extension has been developed for the Scala programming language which uses annotations to handle uniqueness in the context of message passing between actors.\n\nA unique type is very similar to a linear type, to the point that the terms are often used interchangeably, but there is in fact a distinction: actual linear typing allows a non-linear value to be typecast to a linear form, while still retaining multiple references to it. Uniqueness guarantees that a value has no other references to it, while linearity guarantees that no more references can be made to a value.\n\n\n\n\n"}
{"id": "32095860", "url": "https://en.wikipedia.org/wiki?curid=32095860", "title": "United4Iran", "text": "United4Iran\n\nUnited4Iran advocates for an end to human rights violations in Iran and supports the movement for genuine democratic reform in the country. United for Iran works to raise awareness about human rights abuses and mobilizes pressure on the Iranian government to uphold human rights principles outlined in the Universal Declaration of Human Rights. United for Iran undertakes programs and campaigns that aim to advance accountability for violations against Iranian citizens and increase the cost for human rights abuses.\nAmong its policy agenda, United for Iran calls on the Iranian government to:\n• Release political prisoners.\n• End all restrictions that prevent citizens from accessing information and the Internet freely.\n• Reform its laws and practices to adhere to international standards established in the Universal Declaration of Human Rights, including the promotion of gender equality; respect for freedom of assembly, freedom of expression, and freedom of religion; and ensuring freedom from torture, arbitrary detention, and extrajudicial execution.\n• Institute electoral and democratic reforms and hold genuine democratic elections open to international electoral observation.\n• Establish a moratorium on the death penalty until due process rights are guaranteed and with a view toward joining the global movement toward abolition.\nTo advance this agenda, United for Iran will pursue the following objectives: contribute to NGO coalition-building and collaboration among organizations that work on human rights promotion in Iran; encourage cooperation by the Iranian government with the international human rights system; and exert international pressure on the Iranian government to carry about the above human rights objectives.\nUnited for Iran is an independent non-profit organization formed in 2009 after the 2009–2010 Iranian election protests\". \n\nThe group organized the July 25th, 2009, Global Day of Action for Iran. With the support of major human rights organizations and several Nobel Peace Prize winners, including Desmund Tutu, Shirin Ebadi, and Jody Williams, the impetus for the Global Day of Action spread around the world, gathering momentum as a grassroots movement in over 110 cities. Since its first Global Day of Action, United4Iran has coordinated several more international days of action, including one for International Human Rights Day (2009), two to mark the anniversaries of Iran's disputed 2009 election (2010 and 2011), and one for Iran's Student Day (2010). \n\nUnited4Iran has partnered with several prominent human rights organizations, activists, and NGOs, including\n\n"}
{"id": "3171366", "url": "https://en.wikipedia.org/wiki?curid=3171366", "title": "Vira Nirvana Samvat", "text": "Vira Nirvana Samvat\n\nThe Vira Nirvana Samvat (era) is a calendar era beginning on 15 October 527 BCE. It commemorates the nirvana of Mahavira, the 24th Jain Tirthankara. This is one of the oldest system of chronological reckoning which is still used in India.\n\nThe earliest text to mention 527 BCE as the year of Mahavira's nirvana is Yati-Vrishabha's \"Tiloya-Pannatti\" (5th century CE). Subsequent works such as Jinasena's \"Harivamśa\" (783 CE) mention the Vira Nirvana era, and give the difference between it and the Shaka era (beginning in 78 CE) as 605 years and 5 months.\n\nOn 21 October 1974 the 2500th Nirvana Mahotsava was celebrated by the Jains throughout India. and overseas \n\nThe Jain year Vira Nirvana Samvat is obtained by adding 470 years to the Kartikadi Vikram samvat. For example, The Vira Nirvana Samvat 2544 started right after Diwali of October 20, 2017 on Vikram 2074, Kartika Krishna Amavasya (Chaitradi and Purnimanta). The new Chaitradiadi Vikram samvat (common in North India) starts seven months earier in Chaitra, thus during Chaitra-Kartika Krishna, the difference between Vikram and Vir Nivana samvat is 469 years.\n\nThe Jain business people traditionally started their accounting year from Diwali. The relationship between the Vir and Shaka era is given in Titthogali Painnaya and Dhavalaa by Acharya Virasena:\n\nThus the Nirvana occurred 605 years and 5 months before the Saka era.\n\nThe Jain calendar (Panchāng) is a lunisolar calendar, just like the traditional Vikarm or Saka calendars . The months based on the position of the Moon with respect to the Earth and it is adjusted by adding an extra month (adhika masa) once every three years, to coincide with the Sun to bring month in phase with the season. Its day or date which is known as Tithi, indicates the moon phase and the month indicates the approximate season of the solar year.\n\nThe lunisolar calendar has the following arrangement:\nA regular or normal year has 12 months; a leap year has 13 months.\nA regular or normal year has 353, 354, or 355 days; a leap year has 383, 384, or 385 days.\n\nThe average number of days in a month is 30 but the average number of days in a Lunisolar year is 354 and not 360 (12 months in a year) because it takes the Moon about 29.5 days (not 30 days) to complete the circle around the Earth. Hence one Tithi is eliminated in about duration of two months.\nThe Hebrew, Hindu lunar, Buddhist, and Tibetan calendars are all lunisolar, and so were the Japanese calendars until 1873 and the Chinese calendars until 1912.\n\nThe Islamic calendar is a pure Lunar Calendar because its date (Tithi) indicates the moon phase but its months are not in phase with the time of the solar year or the season. It does not adjust its calendar to coincide with the sun or the season. Hence no extra month is added every three years.\n\nThe Gregorian calendar (English CE) is a pure Solar Calendar and its date indicates the time of the solar season but not the moon phase.\n\n\n"}
{"id": "58969496", "url": "https://en.wikipedia.org/wiki?curid=58969496", "title": "Wardley map", "text": "Wardley map\n\nA Wardley map is a map of the structure of a business or service, mapping the components needed to serve the customer or user. Wardley maps are named after Simon Wardley who claims to have created them in 2005.\n\nEach component in a Wardley map is classified by the value it has to the customer or user and by the maturity of that component, ranging from custom-made to commodity. Components are drawn as nodes on a graph with value on the y-axis and commodity on the x-axis. A custom-made component with no direct value to the user would sit at the bottom-left of such a graph while a commodity component with high direct value to the user would sit at the top-right of such a graph. Components are connected on the graph with edges showing that they are linked.\n\nMuch of the theory of Wardley mapping is set out in a series of 12 blog posts and a dedicated Wiki called Wardleypedia. As use of the technique has broadened to new institutions and been used to map new things the application of the technique in practice has drifted from the original vision.\n\nImagine that we want to set up a new drone courier service. The user need is to receive packages quickly from our company. Our objective as a company is to meet this user need by delivering packages quickly to customers. This is a high-value, low-commodity component and is placed at the top-left of a Wardley map. If there were dozens of competing drone courier companies this component would move right on the Wardley map, indicating that the service is closer to being a commodity.\n\nOther components are mapped similarly. For example a drone operator needs to be aware of the weather conditions to determine the route a drone should take and the maximum weight it can carry. Weather information is of little value to the customer and can be bought from a wide range of weather data providers. It is thus placed at the bottom-right of the Wardley map.\nWardley maps are used within UK government, with particular interest within the Government Digital Service (GDS) for strategic planning and identifying the best targets for government digital service modernisation.\n\nThey have been used to map the existing and planned technology infrastructure and services for High-Speed Two (HS2).\n\nThe Atlas2 tool, by Leading Edge Forum, aims to assist with the creation of Wardley maps.\n\nThe Wardley mapping process for abstracting a business or service into components is often not intuitive to people who experience it for the first time. A particular challenge is the subjective placement of each component on a Wardley map, with different people and even the same people at different times likely to place a component in a significantly different position on the map. While there is still value in identifying these uncertainties and differences of opinion, detractors claim that the same valuable discussion could have been achieved without the Wardley mapping process being involved.\n\nProponents of Wardley mapping claim that much of the process's value lies in \"exposing assumptions. allowing challenge and creating consensus\" — but detractors worry that the process in fact lets people \"launder assumptions into facts, delegitimise challenge (and still create consensus)\".\n"}
