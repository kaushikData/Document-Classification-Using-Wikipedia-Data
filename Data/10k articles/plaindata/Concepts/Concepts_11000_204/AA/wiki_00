{"id": "31284283", "url": "https://en.wikipedia.org/wiki?curid=31284283", "title": "1996 Parliament House riot", "text": "1996 Parliament House riot\n\nThe 1996 Parliament House riot (also called the Canberra riot) involved a physical attack on Parliament House, Canberra, Australia, on 19 August 1996, when protesters broke away from the \"Cavalcade to Canberra\" rally organised by the Australian Council of Trade Unions (ACTU) and sought to force their way into the national Parliament of Australia, causing property damage and attacking police.\n\nThe Australian Council of Trade Unions called the \"cavalcade to Canberra\" rally to protest against the industrial relations reform agenda of the Liberal-National Coalition Howard Government. The protest began with senior Australian Trade Union officials including ACTU President Jennie George and Assistant Secretary Greg Combet, as well as senior members of the Australian Labor Party rallying demonstrators from a podium.\n\nAccording to then President of the Senate, Margaret Reid, the initially peaceful protest deteriorated into violent action when a new group of demonstrators arrived in the early afternoon and, joined by people from the main protest, attacked the entrance to Parliament. Around 90 personnel were injured —including lacerations, sprains, and head and eye injuries. Damage to the forecourt and foyer of Parliament was initially estimated at $75,000 and the Parliamentary shop was looted. Nine rioters were arrested and charged with a variety of offences.\n\nBefore the 1996 Australian Federal election, ACTU Secretary, Bill Kelty, had threatened 'industrial war' if a Coalition Government tried to challenge union power. Following their 1996 election victory, the Howard Government proposed to balance the Australian federal budget by means of extensive budget cuts, and introduced a draft Workplace Relations Amendment Bill which proposed to curtail 'compulsory unionism' and to introduce a more decentralised bargaining structure to the Australian work place. Both endeavours were opposed by Trade Unionists who organised a campaign of protest to culminate on the eve of the federal budget, 19 August 1996 with a protest rally on the lawns of the Federal Parliament, called the \"Cavalcade to Canberra\".\n\nOn 4 July 1996, the A.C.T. Trades and Labour Council advised the Speaker and President of the Parliament that a rally would be held at Parliament House on Monday, 19 August and requested permission to conduct a march around the Parliament building. The march was approved for between 1.35 p.m. and 2.25 p.m, provided adequated marshalling was organised and that the demonstration did not obstruct access to Parliament. Subsequent discussions were also conducted with the Construction, Forestry, Mining and Energy Union and the indigenous representatives and all agreed to these parameters for the demonstration. On the day of 19 August, The protest rally remained peaceful until about 12.20 p.m. at which point a separate group of demonstrators entered the parliamentary precinct. The following day, the President of the Australian Senate outlined to Parliament the events that followed thus:\n\nDuring the course of the riot, unionist Davie Thomason, of the CFMEU, took the podium at the official rally with a bloodied face and spoke while shaking a police riot shield, saying to cheers from other protesters:\n\nAddressing the Senate the following day, Senator Robert Hill, leader of the Government in the Senate, described the event as \"very sad day in the history of the Australian political process\", and his opposition counterpart Senator John Faulkner condemned the \"appalling violence\" on behalf of the opposition. Senator Cheryl Kernot for the Australian Democrats said she \"condemned\" the violence and \"I deplore the actions of those who, in my opinion, selfishly and deliberately chose to distract from discussion of the issues\". Senator Dee Margetts, speaking for the Greens Western Australia said that \"the Greens WA do not associate ourselves with the violent action\" but that \"there are obviously some in the Greens movement who have differing opinions about that\". In the House of Representatives, Opposition Leader Kim Beazley called the rioters \"lunatics\" and \"louts\" who had distracted from a \"peaceful and lawful\" protest, while Prime Minister John Howard described the violence as \"thuggery\" and said that the ACTU should accept some responsibility for the riot:\n\n"}
{"id": "41190411", "url": "https://en.wikipedia.org/wiki?curid=41190411", "title": "Amishi Jha", "text": "Amishi Jha\n\nAmishi Jha is an associate professor of psychology at the University of Miami whose research on attention, working memory, and mindfulness has investigated the neural bases of executive functioning and mental training using various cognitive neuroscience techniques. Past studies have focused on the method by which attention selects information as relevant or irrelevant and how working memory then allows that information to be manipulated. Jha's most recent research has examined mindfulness training with soldiers in the U.S. military with the Strong Projects as a tool in improving situational awareness and reducing stress measured through brainwave activity and cognitive behavioral tools. Her work has been featured in numerous publications, popular press articles, she has spoken at the World Economic Forum, the Aspen Institute, and the NY Academy of Sciences and has presented her research to the Dalai Lama, Mind and Life Institute, and Poptech.\n\nJha received her B.S. in Psychology from the University of Michigan in 1993, and her PhD in Psychology from the University of California-Davis in 1998. During her graduate school she worked with Michael Gazzaniga and Ron Mangun. From 1998 to 2001 she completed her post-doctoral training in Neuroimaging and Functional MRI (fMRI), at Duke University with Gregory McCarthy. From 2002 to 2010 she worked as an assistant professor at University of Pennsylvania at the Center for Cognitive Neuroscience and Department of Psychology. In 2010 she joined the University of Miami as an associate professor in the Department of Psychology. Since arriving at the University of Miami, she has co-founded the Mindfulness Research and Practice Initiative and serves as the Director of Contemplative Neuroscience for that effort.\n\nJha has examined the executive functioning mechanisms of the brain such as attention, and working memory, utilizing functional magnetic resonance imaging, event related potential, electroencephalography, signal detection and behavioral methods. Her lab has investigated mindfulness training as a route to strengthen attentional processes as well as reduce perceived stress, and improve mood. Jha published one of the first studies on how attention may be modified by mindfulness training, and the first study of how mindfulness may protect against degradation in working memory in high stress groups, specifically military cohorts. Her research has followed two parallel lines of questioning – one seeking to determine the neural mechanisms of executive functioning and the other developing and evaluating ways to bolster executive functioning through training.\n\nJha's research into mindfulness-based training seeks to determine the benefits of mindfulness training and how this training changes cognitive and affective processes at neural and behavioral levels. Her findings have consistently shown that mindfulness training improves numerous aspects of both cognitive and emotional health. As well, she has demonstrated that there may be a dose-response effect of mindfulness training, so that greater benefits are observed with greater time spent engaging in mindfulness exercises.\n\nExposure to significant and potentially traumatizing stress is pervasive for military personnel during times of war. Such stress can lead to a degradation of psychological health with potentially dire consequences for the individual (e.g. PTSD), mission (e.g. impaired judgment), and family (e.g. divorce), amongst others. In some of the first studies of mindfulness training in the military, Jha partnered with the Mind Fitness Training Institute and training developed and delivered by Dr. Elizabeth Stanley of Georgetown University. An 8-week mindfulness training course was offered to Marines preparing to deploy to Iraq. She found that the more time Marines spent engaging in mindfulness exercises outside of the classroom, the more they improved in their working memory and mood. Jha suggested that working memory capacity may be a core cognitive capacity necessary to preserve psychological resilience in high stress cohorts. These findings suggest that even in individuals facing extraordinary levels of stress, mindfulness training may help improve overall mental health and well-being.\n\nIn a recent study with teachers, Jha and collaborators looked at whether mindfulness training would be effective at improving the mental well-being of elementary and secondary school teachers. Following the completion of the 8-week mindfulness training course, teachers reported less occupational stress, less occupational burnout, as well as lower symptoms of both depression and anxiety, while increasing their working memory capacity. Importantly, teachers who received mindfulness training continued to improve on all four measures over the three-month period following the completion of the course. This finding suggests that not only does mindfulness training have an immediate positive impact on emotional health but that the effects persist, providing long-term improvements in several aspects of well-being.\n\nMany types of contemplative practices, including mindfulness, train individuals to learn to control their attention, critical to success in a variety of settings, including school. In a recent study, Jha and colleagues sought to determine whether, and if so, how, concentrative meditation training can affect attentional processing in school age children. The results indicated that even in children as young as 13, meditation training improved children's ability to achieve and maintain alertness. Additionally, training improved children's ability to resolve conflicts between divergent actions, which may improving their ability to complete tasks as instructed. These findings suggest that meditation training may improve academic performance.\n\nIn another study in children who are part of the juvenile justice system, Jha and NYU collaborators investigated if mindfulness training vs. an active comparison training might protect against degradation in attention that may accompany being incarcerated. While those in the active comparison group degraded in their attention over a several month interval of being incarcerated, those in the mindfulness group did not. Again, Jha suggests that attention may serve as a core cognitive capacity necessary for resilience. In addition, attention is necessary for effective decision making under stress and regulation of behavior. Greater access to these capacities with mindfulness training may help promote resilience and reduce recidivism in children who have entered the juvenile justice system.\n\nJha is an internationally recognized speaker, having spoken at the World Economic Forum, the Huffington Post’s first-ever conference on women, \"The Third Metric: Redefining Success Beyond Money and Power\", the Aspen Institute, and the New York Academy of Sciences. She covers many topics, from optimizing attention, building brain fitness, and mindfulness training, to protecting the brain from stress and aging. She has been interviewed on NPR-Morning Edition, July 26, 2005, CBS Miami, May 30, 2012, and the Huffington Post Live.\nHer work has been featured in press articles, the most recent being a \"Miami Herald\" article titled \"The Mind Tamer\" as well as in the NY Times, the Washington Times, the Miami Herald, the Washington Post, Prevention Magazine, GQ and Newsweek.\n\nFrom 2010 to 2012, Jha served as a research advisor for Goldie Hawn's Foundation.\n\nJha authored the cover story for the March 2013 issue of \"Scientific American Mind\" on mindfulness, and is a Mind and Life Institute Fellow. She is a member of the Cognitive Neuroscience Society, the Society for Neuroscience, and serves on the editorial boards of the Frontiers in Human Neuroscience, Frontiers in Cognitive Science, and .\n\nJha has won several awards for teaching and innovation in science, the most recent being a PopTech Science and Public Leadership Fellow in 2010. In 2007 she received the Charles Ludwig Teaching award and in 2009 the Dean's Innovation in Teaching Award, both from the University of Pennsylvania.\n\n"}
{"id": "26589743", "url": "https://en.wikipedia.org/wiki?curid=26589743", "title": "Angustha purusha", "text": "Angustha purusha\n\nThe Katha Upanishad (1.12-13) mentions aa \"puruṣa\" (man, being) of the size of the thumb (\"aṅgúṣṭha\") enshrined in the interiors of the heart (\"hridaye guhaayaam\" \"cave of the heart\") of a human being.\n\nCertain other Upanishads describe the purusha to be of a golden (hiranmaya) hue and the purursha as a golden being (hiranmaya purusha). The purusha is seated inside the cave of the human heart. As per Hindu religious beliefs, the heart is identified as the abode of divinity within the body. In Gita, Lord Krishna enunciated that he stays within heart of each human being.\n\nThis \"being\" dwelling inside the heart has been equated with the ‘jiva’ or the ‘self’ which carries the consciousness and a meta-physical body (prakriti), also termed as the \"karana sharira\". The \"karana sharira\" has most transcendental existence and accumulates the experiences of the physical, earthly life. This 'jiva' is a spark of divinity, and its place in the body is the heart region.(Heart means not the physical heart But the spiritual heart or the heart chakra.)\n\nAs per Sri Aurobindo, the angustha purusha is made up of elements of \"chaitya\" (\"chetna\"), \"i.e.\" the angustha purusha is a being existing at the psychic (Chetas) sphere. Sri Aurobindo mentions that behind the ordinary chakra of the human heart region, inside a deep cave \"nihite guhayam\" as described by the Upanishads, is the center where a divine spark is present. This thumb-sized purusha is the bird in the twin-bird analogy of the Rigveda and Sankhya (\"dvau suparna sayuja sakhaya\") which actually gets involved in the prakriti and suffers and/or enjoys the fruits of its Karma.\n\nAs per Upanishads, the \"hiranmaya angustha purusha\" embodies the karana sharira. This \"hiranmaya angustha purusha\" is the vehicle of transmigration of soul among births.\n\nThe visualizations regarding the size of the 'self' has been of great significance in the Upansihads. The Brhadaranyaka Upanishad in V. 5.6.1 and the Chandogya Upanishad in V.3.14.3 maintain the size of the self to be equal to a seed. The Shatapata Brahmana, V.10.6.3, mentions that the self:<br>\n...in the interior of my heart, is as tiny like a rice or barley or millet seed and golden like a smokeless light (or flame).\n\nThe \"angustha purusha\" is indestructible and resplendent and is contra-distinct from the purusha of the Purusha sukta. The \"angustha purusha\" is individualistic \"jivaatma\" while the primeval \"purursha\" of the purusha sukta is the collective \"paramatma\".\n"}
{"id": "2377491", "url": "https://en.wikipedia.org/wiki?curid=2377491", "title": "Big History", "text": "Big History\n\nBig History is an academic discipline which examines history from the Big Bang to the present. Big History resists specialization, and searches for universal patterns or trends. It examines long time frames using a multidisciplinary approach based on combining numerous disciplines from science and the humanities, and explores human existence in the context of this bigger picture. It integrates studies of the cosmos, Earth, life, and humanity using empirical evidence to explore cause-and-effect relations, and is taught at universities and secondary schools often using web-based interactive presentations.\n\nAccording to historian David Christian, who has been credited with coining the term \"Big History\", the intellectual movement is made of an \"unusual coalition of scholars\". Some historians have expressed skepticism towards \"scientific history\" and argue that the claims of Big History are unoriginal. Others support the scientific merit but point out that cosmology and natural history have been studied since the Renaissance, and that the new term, Big History, continues such work.\n\nBig History examines the past using numerous time scales, from the Big Bang to modernity, unlike conventional history courses which typically begin with the introduction of farming and civilization, or with the beginning of written records. It explores common themes and patterns. Courses generally do not focus on humans until more than halfway through, and, unlike conventional history courses, there is not much focus on kingdoms or civilizations or wars or national borders. If conventional history focuses on human civilization with humankind at the center, Big History focuses on the universe and shows how humankind fits within this framework and places human history in the wider context of the universe's history.\n\nUnlike conventional history, Big History tends to go rapidly through detailed historical eras such as the Renaissance or Ancient Egypt. It draws on the latest findings from biology, astronomy, geology, climatology, prehistory, archaeology, anthropology, evolutionary biology, chemistry, psychology, hydrology, geography, paleontology, ancient history, physics, economics, cosmology, natural history, and population and environmental studies as well as standard history. One teacher explained:\n\nBig History arose from a desire to go beyond the specialized and self-contained fields that emerged in the 20th century. It tries to grasp history as a whole, looking for common themes across multiple time scales in history. Conventional history typically begins with the invention of writing, and is limited to past events relating directly to the human race. Big Historians point out that this limits study to the past 5,000 years and neglects the much longer time when humans existed on Earth. Henry Kannberg sees Big History as being a product of the Information Age, a stage in history itself following speech, writing, and printing. Big History covers the formation of the universe, stars, and galaxies, and includes the beginning of life as well as the period of several hundred thousand years when humans were hunter-gatherers. It sees the transition to civilization as a gradual one, with many causes and effects, rather than an abrupt transformation from uncivilized static cavemen to dynamic civilized farmers. An account in \"The Boston Globe\" describes what it polemically asserts to be the conventional \"history\" view:\n\nBig History, in contrast to conventional history, has more of an interdisciplinary basis. Advocates sometimes view conventional history as \"microhistory\" or \"shallow history\", and note that three-quarters of historians specialize in understanding the last 250 years while ignoring the \"long march of human existence.\" However, one historian disputed that the discipline of history has overlooked the big view, and described the \"grand narrative\" of Big History as a \"cliché that gets thrown around a lot.\" One account suggested that conventional history had the \"sense of grinding the nuts into an ever finer powder.\" It emphasizes long-term trends and processes rather than history-making individuals or events. Historian Dipesh Chakrabarty of the University of Chicago suggested that Big History was less politicized than contemporary history because it enables people to \"take a step back.\" It uses more kinds of evidence than the standard historical written records, such as fossils, tools, household items, pictures, structures, ecological changes and genetic variations.\n\nCritics of Big History, including sociologist Frank Furedi, have deemed the discipline an \"anti-humanist turn of history.\" The Big History narrative has also been challenged for failing to engage with the methodology of the conventional history discipline. According to historian and educator Sam Wineburg of Stanford University, Big History eschews the interpretation of texts in favor of a purely scientific approach, thus becoming \"less history and more of a kind of evolutionary biology or quantum physics.\"\n\nProfessor David Christian argued that the recent past is only understandable in terms of the \"whole 14-billion-year span of time itself.\" Big History seeks to retell the \"human story\" in light of scientific advances by such methods as radiocarbon dating and genetic analysis. In some instances, it uses mathematical modeling to explore interactions between long-term trends in sociological systems, and it has led to the coining of the term cliodynamics by Peter Turchin of the University of Connecticut to describe how mathematical models might explain events such as the growth of empires, social discontent, and the collapse of nations. It explores the mix of individual action and social and environmental forces, according to one view. While conventional history might see an invention such as sharper spear points as being deliberately created by some smart humans, and then copied by other humans, a Big History perspective might see sharper spear points as accidental, and then natural evolutionary processes enabled their users to be better hunters, even if they did not understand why this was the case. It seeks to discover repeating patterns during the 13.8 billion years since the Big Bang. For example, one pattern is that \"chaos catalyzes creativity\", such as the asteroid impact wiping out the dinosaurs.\n\nBig History makes comparisons based on different time scales, or what David Christian calls \"the play of scales\", and notes similarities and differences between the human, geological, and cosmological scales. Christian believes such \"radical shifts in perspective\" will yield \"new insights into familiar historical problems, from the nature/nurture debate to environmental history to the fundamental nature of change itself.\" It shows how human existence has been changed by both human-made and natural factors: for example, according to natural processes which happened more than four billion years ago, iron emerged from the remains of an exploding star and, as a result, humans could use this hard metal to forge weapons for hunting and war. The discipline addresses such questions as \"How did we get here?,\" \"How do we decide what to believe?,\" \"How did Earth form?,\" and \"What is life?\" It offers a \"grand tour of all the major scientific paradigms.\" According to one view, it helps students to become scientifically literate quickly.\n\nCosmic evolution, the scientific study of universal change, is closely related to Big History (as are the allied subjects of the epic of evolution and astrobiology); some researchers regard cosmic evolution as broader than Big History since the latter mainly (and rightfully) examines the specific historical trek from Big Bang → Milky Way → Sun → Earth → humanity. Cosmic evolution, while fully addressing all complex systems (and not merely those that led to humans), which is also sometimes called cosmological history or universal history, has been taught and researched for decades, mostly by astronomers and astrophysicists. This Big-Bang-to-humankind scenario well preceded the subject that some historians began calling Big History in the 1990s. Cosmic evolution is an intellectual framework that offers a grand synthesis of the many varied changes in the assembly and composition of radiation, matter, and life throughout the history of the universe. While engaging the time-honored queries of who we are and whence we came, this interdisciplinary subject attempts to unify the sciences within the entirety of natural history—a single, inclusive scientific narrative of the origin and evolution of all material things over ~14 billion years, from the origin of the universe to the present day on Earth.\n\nThe roots of the idea of cosmic evolution extend back millennia. Ancient Greek philosophers of the fifth century BCE, most notably Heraclitus, are celebrated for their reasoned claims that all things change. Early modern speculation about cosmic evolution began more than a century ago, including the broad insights of Robert Chambers, Herbert Spencer, and Lawrence Henderson. Only in the mid-20th century was the cosmic-evolutionary scenario articulated as a research paradigm to include empirical studies of galaxies, stars, planets, and life—in short, an expansive agenda that combines physical, biological, and cultural evolution. Harlow Shapley widely articulated the idea of cosmic evolution (often calling it \"cosmography\") in public venues at mid-century, and NASA embraced it in the late 20th century as part of its more limited astrobiology program. Carl Sagan, Eric Chaisson, Hubert Reeves, Erich Jantsch, and Preston Cloud, among others, extensively championed cosmic evolution at roughly the same time around 1980. This extremely broad subject now continues to be richly formulated as both a technical research program and a scientific worldview for the 21st century.\n\nCosmic evolution can elicit controversy for several reasons: evolution of any kind inherently attracts detractors, especially among religious fundamentalists; cosmic evolution addresses universal and human origins, which often elevate emotions; it challenges age-old ideas about life's sense of place in the cosmos; it embraces change, which many people dislike or distrust; it welcomes a broad interpretation of the concept of evolution, replacing the idea of evolution exclusive to life, which some biologists prefer; it proposes a sweeping, interdisciplinary worldview based on rationality and empiricism, which, despite its experimental tests, some find intellectually arrogant.\n\nOne popular collection of scholarly materials on cosmic evolution is based on teaching and research that has been underway at Harvard University since the mid-1970s\n\nCosmic evolution is a quantitative subject, whereas big history typically is not; this is because cosmic evolution is practiced mostly by natural scientists, while big history by social scholars. These two subjects, closely allied and overlapping, benefit from each other; cosmic evolutionists tend to treat universal history linearly, thus humankind enters their story only at the most very recent times, whereas big historians tend to stress humanity and its many cultural achievements, granting human beings a larger part of their story. One can compare and contrast these different emphases by watching two short movies portraying the Big-Bang-to-humankind narrative, one animating time linearly, and the other capturing time (actually look-back time) logarithmically; in the former, humans enter this 14-minute movie in the last second, while in the latter we appear much earlier—yet both are correct.\n\nThese different treatments of time over ~14 billion years, each with different emphases on historical content, are further clarified by noting that some cosmic evolutionists divide the whole narrative into three phases and seven epochs:\nThis contrasts with the approach used by some big historians who divide the narrative into many more thresholds, as noted in the discussion at the end of this section below. Yet another telling of the Big-Bang-to-humankind story is one that emphasizes the earlier universe, particularly the growth of particles, galaxies, and large-scale cosmic structure, such as in physical cosmology.\n\nNotable among quantitative efforts to describe cosmic evolution are Eric Chaisson's research efforts to describe the concept of energy flow through open, thermodynamic systems, including galaxies, stars, planets, life, and society. The observed increase of energy rate density (energy/time/mass) among a whole host of complex systems is one useful way to explain the rise of complexity in an expanding universe that still obeys the cherished second law of thermodynamics and thus continues to accumulate net entropy. As such, ordered material systems—from buzzing bees and redwood trees to shining stars and thinking beings—are viewed as temporary, local islands of order in a vast, global sea of disorder. A recent review article, which is especially directed toward big historians, summarizes much of this empirical effort over the past decade.\n\nOne striking finding of such complexity studies is the apparently ranked order among all known material systems in the universe. Although the \"absolute\" energy in astronomical systems greatly exceeds that of humans, and although the mass densities of stars, planets, bodies, and brains are all comparable, the energy rate \"density\" for humans and modern human society are approximately a million times greater than for stars and galaxies. For example, the Sun emits a vast luminosity, 4x10 erg/s (equivalent to nearly a billion billion billion watt light bulb), but it also has a huge mass, 2x10 g; thus each second an amount of energy equaling only 2 ergs passes through each gram of this star. In contrast to any star, more energy flows through each gram of a plant's leaf during photosynthesis, and much more (nearly a million times) rushes through each gram of a human brain while thinking (~20W/1350g).\n\nCosmic evolution is more than a subjective, qualitative assertion of \"one damn thing after another\". This inclusive scientific worldview constitutes an objective, quantitative approach toward deciphering much of what comprises organized, material Nature. Its uniform, consistent philosophy of approach toward all complex systems demonstrates that the basic differences, both within and among many varied systems, are of degree, not of kind. And, in particular, it suggests that optimal ranges of energy rate density grant opportunities for the evolution of complexity; those systems able to adjust, adapt, or otherwise take advantage of such energy flows survive and prosper, while other systems adversely affected by too much or too little energy are non-randomly eliminated.\n\nFred Spier is foremost among those big historians who have found the concept of energy flows useful, suggesting that Big History is the rise and demise of complexity on all scales, from sub-microscopic particles to vast galaxy clusters, and not least many biological and cultural systems in between.\n\nDavid Christian, in an 18-minute TED talk, described some of the basics of the Big History course. Christian describes each stage in the progression towards greater complexity as a \"threshold moment\" when things become more complex, but they also become more fragile and mobile. Some of Christian's threshold stages are:\n\nChristian elaborated that more complex systems are more fragile, and that while collective learning is a powerful force to advance humanity in general, it is not clear that humans are in charge of it, and it is possible in his view for humans to destroy the biosphere with the powerful weapons that have been invented.\n\nIn the 2008 lecture series through \"The Teaching Company's Great Courses\" entitled \"Big History: The Big Bang, Life on Earth, and the Rise of Humanity\", Christian explains Big History in terms of eight thresholds of increasing complexity:\n\n\nA theme in Big History is what has been termed Goldilocks conditions or the Goldilocks principle, which describes how \"circumstances must be right for any type of complexity to form or continue to exist,\" as emphasized by Spier in his recent book. For humans, bodily temperatures can neither be too hot nor too cold; for life to form on a planet, it can neither have too much nor too little energy from sunlight. Stars require sufficient quantities of hydrogen, sufficiently packed together under tremendous gravity, to cause nuclear fusion.\n\nChristian suggests that the universe creates complexity when these Goldilocks conditions are met, that is, when things are not too hot or cold, not too fast or slow. For example, life began not in solids (molecules are stuck together, preventing the right kinds of associations) or gases (molecules move too fast to enable favorable associations) but in liquids such as water which permitted the right kinds of interactions at the right speeds.\n\nSomewhat in contrast, Chaisson has maintained for well more than a decade that \"organizational complexity is mostly governed by the \"optimum\" use of energy—not too little as to starve a system, yet not too much as to destroy it\" (italics in the original published paper). Neither maximum energy principles nor minimum entropy states are likely relevant, and appeals to \"Goldilocks principles\" (or other such fairy tales) are unnecessary to appreciate the emergence of complexity in Nature writ large.\n\nAdvances in particular sciences such as archaeology, gene mapping, and evolutionary ecology have enabled historians to gain new insights into the early origins of humans, despite the lack of written sources. One account suggested that proponents of Big History were trying to \"upend\" the conventional practice in historiography of relying on written records.\n\nBig History proponents suggest that humans have been affecting climate change throughout history, by such methods as slash-and-burn agriculture, although past modifications have been on a lesser scale than in recent years during the Industrial Revolution.\n\nA book by Daniel Lord Smail in 2008 suggested that history was a continuing process of humans learning to self-modify our mental states by using stimulants such as coffee and tobacco, as well as other means such as religious rites or romance novels. His view is that culture and biology are highly intertwined, such that cultural practices may cause human brains to be wired differently from those in different societies.\n\nBig History is more likely than conventional history to be taught with interactive \"video-heavy\" websites without textbooks, according to one account. The discipline has benefited from having new ways of presenting themes and concepts in new formats, often supplemented by Internet and computer technology. For example, the ChronoZoom project is a way to explore the 14 billion year history of the universe in an interactive website format. It was described in one account:\n\nIn 2012, the History channel showed the film \"History of the World in Two Hours\". It showed how dinosaurs effectively dominated mammals for 160 million years until an asteroid impact wiped them out. One report suggested the History channel had won a sponsorship from StanChart to develop a Big History program entitled \"Mankind\". In 2013 the History channel's new H2 network debuted the 10-part series \"Big History\", narrated by Bryan Cranston and featuring David Christian and an assortment of historians, scientists and related experts. Each episode centered on a major Big History topic such as salt, mountains, cold, flight, water, meteors and megastructures.\n\nWhile the emerging field of Big History in its present state is generally seen as having emerged in the past two decades beginning around 1990, there have been numerous precedents going back almost 150 years. In the mid-19th century, Alexander von Humboldt's book \"Cosmos\", and Robert Chambers' 1844 book \"Vestiges of the Natural History of Creation\" were seen as early precursors to the field. In a sense, Darwin's theory of evolution was, in itself, an attempt to explain a biological phenomenon by examining longer term cause-and-effect processes. In the first half of the 20th century, secular biologist Julian Huxley originated the term \"evolutionary humanism\", while around the same time the French Jesuit paleontologist Pierre Teilhard de Chardin examined links between cosmic evolution and a tendency towards complexification (including human consciousness), while envisaging compatibility between cosmology, evolution, and theology. In the mid and later 20th century, \"The Ascent of Man\" by Jacob Bronowski examined history from a multidisciplinary perspective. Later, Eric Chaisson explored the subject of cosmic evolution quantitatively in terms of energy rate density, and the astronomer Carl Sagan wrote \"Cosmos\". Thomas Berry, a cultural historian, and the academic Brian Swimme explored meaning behind myths and encouraged academics to explore themes beyond organized religion.\n\nThe field continued to evolve from interdisciplinary studies during the mid-20th century, stimulated in part by the Cold War and the Space Race. Some early efforts were courses in \"Cosmic Evolution\" at Harvard University in the United States, and \"Universal History\" in the Soviet Union. One account suggested that the notable Earthrise photo, taken during a lunar orbit by the spacecraft Apollo 8, which showed Earth as a small blue and white ball behind a stark and desolate lunar landscape, not only stimulated the environmental movement but also caused an upsurge of interdisciplinary interest. The French historian Fernand Braudel examined daily life with investigations of \"large-scale historical forces like geology and climate\". Physiologist Jared Diamond in his book \"Guns, Germs, and Steel\" examined the interplay between geography and human evolution; for example, he argued that the horizontal shape of the Eurasian continent enabled human civilizations to advance more quickly than the vertical north-south shape of the American continent, because it enabled greater competition and information-sharing among peoples of the relatively same climate.\n\nIn the 1970s, scholars in the United States including geologist Preston Cloud of the University of Minnesota, astronomer G. Siegfried Kutter at Evergreen State College in Washington state, and Harvard University astrophysicists George B. Field and Eric Chaisson started synthesizing knowledge to form a \"science-based history of everything\", although each of these scholars emphasized somewhat their own particular specializations in their courses and books. In 1980, the Austrian philosopher Erich Jantsch wrote \"The Self-Organizing Universe\" which viewed history in terms of what he called \"process structures\". There was an experimental course taught by John Mears at Southern Methodist University in Dallas, Texas, and more formal courses at the university level began to appear.\n\nIn 1991 Clive Ponting wrote \"A Green History of the World: The Environment and the Collapse of Great Civilizations\". His analysis did not begin with the Big Bang, but his chapter \"Foundations of History\" explored the influences of large-scale geological and astronomical forces over a broad time period.\n\nSometimes the terms \"Deep History\" and \"Big History\" are interchangeable, but sometimes \"Deep History\" simply refers to history going back several hundred thousand years or more without the other senses of being a movement within history itself.\n\nOne exponent is David Christian of Macquarie University in Sydney, Australia. He read widely in diverse fields in science, and believed that much was missing from the general study of history. His first university-level course was offered in 1989. He developed a college course beginning with the Big Bang to the present in which he collaborated with numerous colleagues from diverse fields in science and the humanities and the social sciences. This course eventually became a Teaching Company course entitled \"Big History: The Big Bang, Life on Earth, and the Rise of Humanity\", with 24 hours of lectures, which appeared in 2008.\n\nSince the 1990s, other universities began to offer similar courses. In 1994 at the University of Amsterdam and the Eindhoven University of Technology, college courses were offered. In 1996, Fred Spier wrote \"The Structure of Big History\". Spier looked at structured processes which he termed \"regimes\":\n\nChristian's course caught the attention of philanthropist Bill Gates, who discussed with him how to turn Big History into a high school-level course. Gates said about David Christian:\n\nBy 2002, a dozen college courses on Big History had sprung up around the world. Cynthia Stokes Brown initiated Big History at the Dominican University of California, and she wrote \"Big History: From the Big Bang to the Present.\" In 2010, Dominican University of California launched the world's first Big History program to be required of all first-year students, as part of the school's general education track. This program, directed by Mojgan Behmand, includes a one-semester survey of Big History, and an interdisciplinary second-semester course exploring the Big History metanarrative through the lens of a particular discipline or subject. A course description reads:\n\nThe Dominican faculty's approach is to synthesize the disparate threads of Big History thought, in order to teach the content, develop critical thinking and writing skills, and prepare students to wrestle with the philosophical implications of the Big History metanarrative. In 2015, University of California Press published \"Teaching Big History\", a comprehensive pedagogical guide for teaching Big History, edited by Richard B. Simon, Mojgan Behmand, and Thomas Burke, and written by the Dominican faculty.\nBarry Rodrigue, at the University of Southern Maine, established the first general education course and the first online version, which has drawn students from around the world. The University of Queensland in Australia offers an undergraduate course entitled \"Global History\", required for all history majors, which \"surveys how powerful forces and factors at work on large time-scales have shaped human history\". By 2011, 50 professors around the world have offered courses. In 2012, one report suggested that Big History was being practiced as a \"coherent form of research and teaching\" by hundreds of academics from different disciplines.\nThere are efforts to bring Big History to younger students. In 2008, Christian and his colleagues began developing a course for secondary school students. In 2011, a pilot high school course was taught to 3,000 kids in 50 high schools worldwide. In 2012, there were 87 schools, with 50 in the United States, teaching Big History, with the pilot program set to double in 2013 for students in the ninth and tenth grades, and even in one middle school. The subject is a STEM course at one high school.\n\nThere are initiatives to make Big History a required standard course for university students throughout the world. An education project founded by philanthropist Bill Gates from his personal funds was launched in Australia and the United States, to offer a free online version of the course to high school students.\n\nCurrently, Big History is a consolidated academic field that is giving rise to new views and epistemological approaches, especially in Latin America and the Caribbean, whose decolonial vision of history, economics and science has opened new questions. In this sense, the transdisciplinary and biomimetics research of Javier Collado represents an ecology of knowledge between scientific knowledge and the ancestral wisdom of native peoples, such as Indigenous peoples in Ecuador. This transdisciplinary vision integrates and unifies diverse epistemes that are in, between, and beyond the scientific disciplines, that is, it includes ancestral wisdom, spirituality, art, emotions, mystical experiences and other dimensions forgotten in the history of science, specially by the positivist approach. In approaching the Big History from the complexity sciences, the transdisciplinary methodology seeks to understand the interconnections of the human race with the different levels of reality that co-exist in nature and in the cosmos, and this includes mystical and spiritual experiences, very present in the rituals of shamanism with ayahuasca and other sacred plants. The common denominator of all indigenous and aboriginal ancestral worldviews is the spiritual and ecological conception that structures their social organizations, which are in harmony and respect with the different forms of life that exist on our planet. In the same way that Fritjof Capra carried out an analysis of the parallels between modern physics and Eastern mysticism, the teaching of the Big History in universities of Brazil, Ecuador, Colombia, and Argentina is nourished by the worldview of their ancestor to analyze the parallels between the scientific discoveries and the original knowledge of the native and indigenous peoples.\n\nThe International Big History Association (IBHA) was founded at the Coldigioco Geological Observatory in Coldigioco, Marche, Italy, on 20 August 2010. Its headquarters is located at Grand Valley State University in Allendale, Michigan, United States. Its inaugural gathering in 2012 was described as \"big news\" in a report in \"The Huffington Post\".\n\nAcademics involved with the concept include:\n\n"}
{"id": "51155612", "url": "https://en.wikipedia.org/wiki?curid=51155612", "title": "Charlotte L. Brown", "text": "Charlotte L. Brown\n\nCharlotte L. Brown (1839–?) was an American educator and civil rights activist who was one of the first to legally challenge racial segregation in the United States when she filed a lawsuit against a streetcar company in San Francisco in the 1860s after she was forcibly removed from a segregated streetcar.\n\nBrown was born in Maryland in 1839, the daughter of James E Brown, who was born enslaved, and Charlotte Brown, a free seamstress. The elder Charlotte Brown purchased her husband's freedom and in 1850 they were living as Free people of color in Baltimore, Maryland, with several children, including Charlotte. Some time between 1850 and 1860 they moved their family to San Francisco, which was booming as a result of the California Gold Rush, and became a part of that city's burgeoning black middle class. The black population of the city at that time was 1,176 people, or about 2 percent. In San Francisco, James E Brown ran a livery stable, was a partner in the black newspaper \"Mirror of the Times\", an antislavery crusader, and a member of the San Francisco Literary Society, a discussion and debate group for prominent African American men.\n\nAt 8PM on April 17, 1863, Charlotte Brown took a seat on a horse-drawn streetcar one block from her home on Filbert Street in San Francisco. She was on her way to see her doctor. The streetcar was owned by the Omnibus Railroad Company. When the streetcar conductor approached her and asked her to leave, Brown said she \"had a right to ride\" and had no intention of leaving the car.\n\nIn her courtroom testimony, she stated:\n\n\"The conductor went around and collected tickets and when he came to me I handed him my ticket and he refused to take it. It was one of the Omnibus railroad tickets, one that I had purchased of them previous to that time. He replied that colored persons were not allowed to ride. I told him I had been in the habit of riding ever since the cars had been running. I answered that I had a great ways to go and I was later than I ought to be.\"\n\nThe conductor, Thomas Dennison, asked her several times to leave, and each time she refused. Finally, when a white woman objected to her presence, he grabbed her by the arm and escorted her off the car.\n\nHer father James E Brown hired attorney W. C. Burnett, and Charlotte Brown brought a lawsuit against the Omnibus Railroad Company for $200. African Americans had just won the right to testify against whites that same year. The Omnibus Railroad argued that its conductor's action was justified because racial segregation protected white women and children who might be fearful or 'repulsed' by riding in the same car as African Americans. Brown won her case, presided over by Judge Cowles, but the jury only awarded her twenty-five dollars. The conductor, Dennison, was convicted in criminal court of assault and battery against Brown.\n\nBrown's civil case was tied up in appeals for the next two years. In one appeals trial, the jury awarded Brown only five cents, the price of the streetcar ticket. Meanwhile, just three days after the first trial, Brown was ejected from yet another streetcar and brought a second suit against Omnibus, this time for $3,000. Finally, in October, 1864, her case was tried in a higher court. In his judgment of October 5, 1864, Judge Orville C. Pratt of the 12th District Court upheld the earlier verdict in favor of Brown, ruling that excluding passengers from streetcars because of their race was illegal. He had no desire, he said in his ruling, to \"perpetuate a relic of barbarism\":\n\n\"It has been already quite too long tolerated by the dominant race to see with indifference the negro or mulatto treated as a brute, insulted, wronged, enslaved, made to wear a yoke, to tremble before white men, to serve him as a tool, to hold property and life at his will, to surrender to him his intellect and conscience, and to seal his lips and belie his thought through dread of the white man's power\", Judge Pratt stated.\n\nIn January, 1865, a jury awarded Brown $500. The Omnibus Railroad Company appealed the verdict but was refused another trial.\n\nAfter the first trial, the black-owned newspaper the \"Pacific Appeal\" noted that the verdict in her favor \"establishes the right, by law, of colored persons to ride in such conveyances\".\n\n\"While the law recognizes and gives us the right to ride in such conveyances,\" the editorial continued, \"there are a certain number of the employees of this Company, who, if a colored person attempts to cross the street while their car is passing, are seized with a sudden fit of Negrophobia, which is generally manifested by pulling their alarm bell violently, as if some danger was imminent, so afraid are they that some other of our respectable females might attempt to exercise the right that Miss Brown has just won. So long as we have law, justice and right on our side, we want no pity.\"\n\nJudge Pratt's 1864 ruling was derided in an editorial cartoon by a local white-owned newspaper that showed blacks and whites riding side by side. Using a racial epithet, it accused Pratt of being partial to African Americans, and questioned whether the light-skinned Brown was really black, or just filing suit for the monetary award. A white editor of \"The Sacramento Daily Union\", on the other hand, said of Pratt's decision, \"his argument is lucid, and his decision, we believe, chimes in with the sentiment of the people\".\n\nThe Charlotte Brown case paved the way for other cases brought by San Francisco African Americans like William Bowen and Mary Ellen Pleasant that challenged the \"whites-only\" practices of the privately owned streetcars. In 1893 streetcar segregation was officially outlawed on statewide streetcars by the California legislature.\n\nAfter Charlotte Brown won her case, Senator Charles Sumner invoked the case in Congress as setting an important precedent for racial equality when he argued in favor of integrating streetcars in the nation's capitol.\n\nIn 1867, Charlotte Brown opened a school for young children at 10 Scotland Street in San Francisco, offering \"all the branches of primary education\" as well as music and embroidery. In 1874, she married James Henry Riker, another prominent African American activist of San Francisco, who had worked as a live-in personal servant to William Chapman Ralston and was employed as a steward at the Palace Hotel during their marriage. Riker, along with Brown's father, was one of the organizers of the 1865 California State Convention of Colored Citizens. The society pages of the black newspaper \"The Elevator\" printed an announcement in 1877 of a surprise party for a fellow steward at the Palace Hotel that was given by Charlotte and James H Riker in their residence on 1018 Powell Street in San Francisco.\n\nLittle is known about Charlotte Brown Riker's life after that time.\n\nThe Charlotte Brown lawsuit was one of the first of several actions that were brought by black activists in both Southern and Northern cities throughout the United States to protest exclusion and segregation on public transportation in the 19th and 20th centuries. In 1854, Elizabeth Jennings successfully filed suit against the Third Avenue Railway Company in New York City after she had been ejected from a streetcar there because of her skin color. In Philadelphia in 1865, a \"Mrs. Derry\" won a civil suit and $50 in damages after a streetcar conductor threw her off the bus and kicked her when she and a group of women were returning from tending to Civil War soldiers.\nSojourner Truth, Ida B. Wells, and Frances Watkins Parker also lobbied actively for streetcar integration after being refused ridership on streetcars in Washington, D.C., Memphis, and Pennsylvania.\n\nThough streetcars in the north were largely de-segregated by the end of the 19th century, segregation on public transportation became the official policy in many cities of the South. In the 20th century, women like Irene Morgan and Mary Louise Smith continued to battle segregation on public transportation. In 1954, Rosa Parks challenged the practice in Montgomery, Alabama, launching a citywide bus boycott and helping to start the Civil Rights Movement of the 1950s and '60s.\n\nIn \"African American Women Confront the West\", scholars Quintard Taylor and Shirley Ann Wilson Moore analyze the Charlotte Brown case and other 19th-century streetcar desegregation lawsuits brought by black women as they relate historically to both race and gender. They noted that it was frequently women who filed these lawsuits because of prevailing ideas of public vs. private space, and white views of black gender roles:\n\n\"Activist black men were keenly aware of the power of black women to go, where they, as men, could not,\" they write. \"Relying on the wedge of gender, black women's lawsuits against streetcar companies ultimately assured the right to travel of blacks generally ... the declaration of law was a general one that applied not simply to Charlotte Brown or to Mary Pleasant, but to all blacks as well.\"\n\n\n"}
{"id": "14421128", "url": "https://en.wikipedia.org/wiki?curid=14421128", "title": "Conflict avoidance", "text": "Conflict avoidance\n\nConflict avoidance is a method of reacting to conflict, which attempts to avoid directly confronting the issue at hand. Methods of doing this can include changing the subject, putting off a discussion until later, or simply not bringing up the subject of contention. Conflict prevention can be used as a temporary measure to buy time or as permanent means of disposing of a matter. The latter may be indistinguishable from simple acquiescence to the other party, to the extent that the person avoiding the conflict subordinates their own wishes to the party with whom they have the conflict. However, conflict prevention can also take the form of withdrawing from the relationship. Thus, avoidance scenarios can be either win-lose, lose-lose or possibly even win-win, if terminating the relationship is the best method of solving the problem.\n\nThe term \"conflict avoidance\" is sometimes used to describe conflict prevention. Bacal criticizes this use of terms by asking, \n\nTurner and Weed classify concealment as one of the three main types of responses to conflict, describing concealers as those who take no risk and so say nothing, concealing their views and feelings. Concealers are further divided into three types; namely:\n\nThe Thomas-Kilmann grid views avoidance as a lose-lose proposition since it does not address the issue at hand. But other sources view avoidance as a useful means of disposing of very minor, non-recurring conflicts whose resolution would expend excessive amounts of time or resources.\n\nResearch in personality psychology has indicated that the personality trait of agreeableness--one of the five identifiable dimensions of personality--correlates with proclivity toward conflict avoidance.\n\nIn the workplace, managers sometimes avoid directly dealing with conflict among co-workers by simply separating them. In workplaces and other situations where continued contact with a person cannot be severed, workers may eschew confrontation as being too risky or uncomfortable, opting instead to avoid directly dealing with the situation by venting to others or engaging in passive aggressive methods of attack such as gossip. Unresolved conflict in the workplace has been linked to miscommunication resulting from confusion or refusal to cooperate, increased stress, reduced creative collaboration and team problem solving, and distrust. According to an East Bay Business Times article, some possible results of conflict-averse senior executives may include \n"}
{"id": "18726162", "url": "https://en.wikipedia.org/wiki?curid=18726162", "title": "Darwin's World", "text": "Darwin's World\n\nDarwin's World, created by Dominic Covey, is a post-apocalyptic role-playing game first published under the d20 Open Game License in 2001. Originally designed as a quick adaptation of the 3rd Edition Dungeons & Dragons rules, the game has since been greatly expanded and revised and now utilizes the d20 Modern rules. Several \"Darwin's World\" books and supplements have seen print, though most support for the game is still only available in PDF format. In 2010, RPGObjects began producing game books using Pinnacle's Savage Worlds system.\n\n\"Darwin's World\" was nominated for an Ennie award (Best d20 Game) in 2002.\n\nThe online e-zine, Post Apocalyptic Dispatch, continues to provide short articles that support game play for both gamemasters and players alike. The game is still heavily supported with new material coming out on a regular basis.\n\nUnlike many existing post-apocalyptic role-playing games, \"Darwin's World\" is often described as a darker and more \"realistic\" game system. In specific, its use of real-life deformities and genetic diseases to portray character defects, as well as issues like slavery, racism, and drug use give it a grittier quality than most post-apocalyptic RPGs, which often have a fantastic or \"comic book\" feel that requires a broader willingness to suspend disbelief.\n\nWhile the original \"Darwin's World\" books clearly put forth a timeline and history, the second edition (and most recent edition, v2.5) has a broader focus intending to provide rules, suggestions, and guidelines for any kind of post-apocalyptic campaign setting. The default setting remains a wasteland created by global nuclear/biological/chemical warfare, however (See Weapon of mass destruction). There is a strong focus on regions that once constituted the Midwest and Western United States; numerous modules published for the game are nominally set in \"desert\" or \"wasteland\" areas. More specific locations include the desert trade town of Tucumcari, the sunken city of Bakersfield and its strange subterranean monsters, the overgrown ruins of Los Angeles, a San Francisco overrun by anti-technology zealots, and a decadent society thriving in the futuristic domed cities of the Midwest. One supplement went into great detail exploring the fate of the Pacific Northwest.\n\nA wide range of supplements, sourcebooks, and modules are available for \"Darwin's World\".\n\n\nNumerous adventures written for \"Darwin's World\" either briefly feature, reference, or center around real-life places and events. These include:\n\n\n\n"}
{"id": "51190957", "url": "https://en.wikipedia.org/wiki?curid=51190957", "title": "Difficulty of engagement", "text": "Difficulty of engagement\n\nDifficulty of engagement is a notion in the \"Campbell Paradigm\", a model of behavior change with person-independent difficulty.\n\nDifficulty is considered a key predictor of behavior in psychology and is included in most recognized models of behavior change, such as the theory of planned behavior. Most of these models rely on people's perceptions and estimates of behavioral difficulty. That is, difficulty is considered to be subjective and person-dependent. Obviously, perceived difficulty varies by individual. \n\nA more objective measure of difficulty is desirable, e.g., for environmental or energy policy, because people may misperceive the difficulty of behaviors, possibly because affected by mood or current circumstances.\n\nThe \"Campbell Paradigm\" was proposed by Kaiser et al. as a model of behavior change with person-independent difficulty. The model treats the likelihood of individual behavior as a function of attitude and of the difficulty of engaging in this behavior. The more demanding these barriers are, the more favorable attitude towards a general goal, such as environment protection. The relation between difficulty of behaviors, attitudes and behaviors can be computed using a one-parameter logistic Rasch model and yield the proportion of persons that engage in a given behavior.\n\n"}
{"id": "25160524", "url": "https://en.wikipedia.org/wiki?curid=25160524", "title": "Digital print matrix", "text": "Digital print matrix\n\nA digital print matrix is the digital state from which a print art object can be instanced with original intent.\n\nThe traditional term \"print matrix\" is the physical surface from which an image is printed, woodblock, plate, stone or screen. Although these may in themselves be produced digitally they comprise a traditional (physical) matrix. A digital matrix however is a repository of material which, stored digitally, is combined by the artist’s hand and instanced with original intent; Philip George’s “fluid diary” providing an early example. Technically the digital matrix comprises stable digital storage mechanisms (which retain the data when switched off) rather than volatile random access memory. Conceptually as there is no need for this storage to be in the physical presence of the artist then online and remote storage (including the Internet) may form as a whole or in part the digital matrix.\n"}
{"id": "5682984", "url": "https://en.wikipedia.org/wiki?curid=5682984", "title": "Donkey punch", "text": "Donkey punch\n\nDonkey punch is a slang term for the sexual practice of inflicting blunt force trauma to the back of the head or lower back of the receiving partner during anal or vaginal sex as an attempt by the penetrating partner to induce involuntary tightening of internal or external anal sphincter muscles or vaginal passage of the receiving partner. According to Dr. Jeffrey Bahr of Medical College of Wisconsin, there is no reflex in humans that would cause such tensing in response to a blow on the head, although striking a partner on the back of the neck or head could cause severe, even lethal injury.\n\nSex columnist Dan Savage has discussed the alleged practice on several occasions. In 2004, Savage referred to the donkey punch as \"a sex act that exists only in the imaginations of adolescent boys,\" adding \"no one has ever attempted \"the Pirate,\" just as no one has ever performed a Hot Karl, delivered a Donkey Punch, or inserted an Icy Mike. They’re all fictions.\" Responding to an enquiry from Wikipedia editors, he again discussed the donkey punch urban legend in his \"Savage Love\" column in 2006. He wrote, \"attempting a Donkey Punch can lead to ... unpleasant outcomes,\" including \"injury, death or incarceration\"; he also pointed out that it \"doesn't even work.\" He quoted Jeffrey Bahr, a faculty member at the Medical College of Wisconsin,\n\nJordan Tate, commenting in \"The Contemporary Dictionary of Sexual Euphemisms\" (2007) on the \"almost purely theoretical nature\" of the term, claimed,\n\"The concise new Partridge dictionary of slang and unconventional English\" (2007) defines the term as follows:\n\nThe adult film star credited as the first known recipient of a donkey punch is Gia Paloma, who had the act performed on her by Alex Sanders in the 2004 film \"Gutter Mouths 30\".\n\n\"Donkey Punch\", a pornographic film premised around the act, was released by JM Productions in 2005. The film consists of four scenes in which the male actors engage in rough sex with their female co-stars, punching them repeatedly in the head and body throughout. In response to her experience on the set, performer Alex Divine allegedly stated \"\"Donkey Punch\" was the most brutal, depressing, scary scene that I have ever done,\" and commented that \"I actually stopped the scene while it was being filmed because I was in too much pain.\"\n\nThe viciousness of the film prompted Peter van Aarle of Cyberspace Adult Video Reviews to forego covering any further releases from JM, while Zack Parsons of Something Awful (which awarded \"Donkey Punch\" a score of -49, where -50 is the worst score possible) wrote that the film was \"one of the most morally repugnant pornographic movies I have seen\" and \"is the sort of movie that the government would cite when trying to arrest pornographers and outlaw pornography.\"\n\n\"Donkey punch\" was one of several slang terms used by Enron traders to refer to their price gouging methods. During investigations into the 2004 Enron scandal over manipulation of the electricity market in California, recordings of Enron traders were uncovered dating from 2000 and 2001. In the recordings, fraudulent accounting schemes were referred to using slang terms, including \"Donkey Punch.\" The 2007 report by the Federal Energy Regulatory Commission was unable to identify the meaning that Enron had attached to the term \"Donkey Punch.\" U.S. Senator Maria Cantwell, in a 2004 press release about the Enron hearings, identified the Donkey Punch as \"a crude pornographic term,\" one of many \"lewd acts\" that Enron employees used to describe their schemes. Cantwell asked the Federal Energy Regulatory Commission to take down the emails that were on its website due to the content.\n\nThe term received extensive coverage online after it was mistakenly given as an answer on the January 16, 2012 broadcast of the American game show \"Jeopardy!\". The prompt was \"A blow to the back of the neck is the punch named for this animal\"; the correct answer was rabbit punch, a dangerous boxing move. The first contestant answered with \"What is a donkey?\" The subsequent contestant gave the correct answer. A clip of the scene became a viral video.\n\n"}
{"id": "393404", "url": "https://en.wikipedia.org/wiki?curid=393404", "title": "Estates of the realm", "text": "Estates of the realm\n\nThe estates of the realm, or three estates, were the broad orders of social hierarchy used in Christendom (Christian Europe) from the medieval period to early modern Europe. Different systems for dividing society members into estates developed and evolved over time.\n\nThe best known system is the French \"Ancien Régime\" (Old Regime), a three-estate system used until the French Revolution (1789–1799). Monarchy was for the king and the queen and this system was made up of clergy (the First Estate), nobles (the Second Estate), and peasants and bourgeoisie (the Third Estate). In some regions, notably Scandinavia and Russia, burghers (the urban merchant class) and rural commoners were split into separate estates, creating a four-estate system with rural commoners ranking the lowest as the Fourth Estate. Furthermore, the non-landowning poor could be left outside the estates, leaving them without political rights. In England, a two-estate system evolved that combined nobility and clergy into one lordly estate with \"commons\" as the second estate. This system produced the two houses of parliament, the House of Commons and the House of Lords. In southern Germany, a three-estate system of nobility (princes and high clergy), \"ritters\" (knights), and burghers was used.\n\nToday the terms \"three estates\" and \"estates of the realm\" may sometimes be re-interpreted to refer to the modern separation of powers in government into the legislature, administration, and the judiciary. Additionally the term \"fourth estate\" usually refers to forces outside the established power structure (evoking medieval three-estate systems), most commonly in reference to the independent press or media. Historically, in Northern and Eastern Europe, the Fourth Estate meant rural commoners.\n\nDuring the Middle Ages, advancing to different social classes was uncommon and very difficult.\n\nThe medieval Church was an institution where social mobility was most likely up to a certain level (generally to that of vicar general or abbot/abbess for commoners). Typically, only nobility were appointed to the highest church positions (bishops, archbishops, heads of religious orders, etc.), although low nobility could aspire to the highest church positions. Since clergy could not marry, such mobility was theoretically limited to one generation. Nepotism was common in this period.\n\nAnother possible way to rise in social position was due to exceptional military or commercial success. Such families were rare and their rise to nobility required royal patronage at some point. However, because noble lines went extinct naturally, some ennoblements were necessary.\n\n\"Medieval political speculation is imbued to the marrow with the idea of a structure of society based upon distinct orders,\" Johan Huizinga observes. The virtually synonymous terms \"estate\" and \"order\" designated a great variety of social realities, not at all limited to a class, Huizinga concluded applying to every social function, every trade, every recognisable grouping.\n\nThis static view of society was predicated on inherited positions. Commoners were universally considered the lowest order. The higher estates' necessary dependency on the commoners' production, however, often further divided the otherwise equal common people into burghers (also known as bourgeoisie) of the realm's cities and towns, and the peasants and serfs of the realm's surrounding lands and villages. A person's estate and position within it were usually inherited from the father and his occupation, similar to a caste within that system. In many regions and realms there also existed population groups born outside these specifically defined resident estates.\n\nLegislative bodies or advisory bodies to a monarch were traditionally grouped along lines of these estates, with the monarch above all three estates. Meetings of the estates of the realm became early legislative and judicial parliaments. Monarchs often sought to legitimize their power by requiring oaths of fealty from the estates. Today, in most countries, the estates have lost all their legal privileges, and are mainly of historical interest. The nobility may be an exception, for instance due to legislation against false titles of nobility; similarly British government well maintains the distinction- witness its House of Lords, and the House of Commons.\n\nOne of the earliest political pamphlets to address these ideas was called \"What Is the Third Estate?\" (French: Qu'est-ce que le tiers-état?) It was written by Abbé Emmanuel Joseph Sieyès in January 1789, shortly before the start of the French Revolution.\n\nAfter the fall of the Western Roman Empire, numerous geographic and ethnic kingdoms developed among the endemic peoples of Europe, affecting their day-to-day secular lives; along with those, the growing influence of the Catholic Church and its Papacy affected the ethical, moral and religious lives and decisions of all. This led to mutual dependency between the secular and religious powers for guidance and protection, but over time and with the growing power of the kingdoms, competing secular realities increasingly diverged from religious idealism and Church decisions.\n\nThe new lords of the land identified themselves primarily as warriors, but because new technologies of warfare were expensive, and the fighting men required substantial material resources and considerable leisure to train, these needs had to be filled. The economic and political transformation of the countryside in the period were filled by a large growth in population, agricultural production, technological innovations and urban centers; movements of reform and renewal attempted to sharpen the distinction between clerical and lay status, and power, recognized by the Church also had their effect.\n\nIn his book \"The Three Orders: Feudal Society Imagined\", the French medievalist Georges Duby has shown that in the period 1023-1025 the first theorist who justified the division of European society into the three estates of the realm was Gerard of Florennes, the bishop of Cambrai.\n\nAs a result of the Investiture Controversy of the late 11th and early 12th centuries, the powerful office of Holy Roman Emperor lost much of its religious character and retained a more nominal universal preeminence over other rulers, though it varied. The struggle over investiture and the reform movement also legitimized all secular authorities, partly on the grounds of their obligation to enforce discipline.\n\nIn the 11th and 12th centuries thinkers argued that human society consisted of three orders: those who pray, those who fight, and those who labour. The structure of the first order, the clergy, was in place by 1200 and remained singly intact until the religious reformations of the 16th century. The very general category of those who labour (specifically, those who were not knightly warriors or nobles) diversified rapidly after the 11th century into the lively and energetic worlds of peasants, skilled artisans, merchants, financiers, lay professionals, and entrepreneurs, which together drove the European economy to its greatest achievements. The second order, those who fight, was the rank of the politically powerful, ambitious, and dangerous. Kings took pains to ensure that it did not resist their authority.\n\nBy the 12th century, most European political thinkers agreed that monarchy was the ideal form of governance. This was because it imitated on earth the model set by God for the universe; it was the form of government of the ancient Hebrews and the Christian Biblical basis, the later Roman Empire, and also the peoples who succeeded Rome after the 4th century.\n\nFrance under the \"Ancien Régime\" (before the French Revolution) divided society into three estates: the First Estate (clergy); the Second Estate (nobility); and the Third Estate (commoners). The king was considered part of no estate.\n\nThe First Estate comprised the entire clergy, traditionally divided into \"higher\" and \"lower\" clergy. Although there was no formal demarcation between the two categories, the upper clergy were, effectively, clerical nobility, from the families of the Second Estate. In the time of Louis XVI, every bishop in France was a nobleman, a situation that had not existed before the 18th century.\n\nAt the other extreme, the \"lower clergy\" (about equally divided between parish priests, monks, and nuns) constituted about 90 percent of the First Estate, which in 1789 numbered around 130,000 (about 0.5% of the population). \n\nThe Second Estate (Fr. \"deuxieme état\") was the French nobility and (technically, though not in common use) royalty, other than the monarch himself, who stood outside of the system of estates.\n\nThe Second Estate is traditionally divided into \"noblesse d'épée\" (\"nobility of the sword\"), and \"noblesse de robe\" (\"nobility of the robe\"), the magisterial class that administered royal justice and civil government.\n\nThe Second Estate constituted approximately 1.5% of France's population. Under the \"ancien régime\" (\"old rule/old government\"), the Second Estate were exempt from the \"corvée royale\" (forced labour on the roads) and from most other forms of taxation such as the \"gabelle\" (salt tax) and most important, the \"taille\" (the oldest form of direct taxation). This exemption from paying taxes led to their reluctance to reform.\n\nThe Third Estate comprised all of those who were not members of the above and can be divided into two groups, urban and rural, together making up 98% of France's population. The urban included wage-labourers. The rural included free peasants (who owned their own land) who could be prosperous and villeins (serfs, or peasants working on a noble's land). The free peasants paid disproportionately high taxes compared to the other Estates and were unhappy because they wanted more rights. In addition, the First and Second Estates relied on the labour of the Third, which made the latter's unequal status all the more glaring.\n\nThere were an estimated 27 million people in the Third Estate when the French Revolution started.\n\nThey had the hard life of physical labour and food shortages. Most were born within this group and died as a part of it, too. It was extremely rare for people of this ascribed status to make it out into another estate. Those who did so managed as a result of either being recognized for their extraordinary bravery in a battle or entering religious life. A few commoners were able to marry into the second estate, but this was a rare occurrence.\n\nThe first Estates General (not to be confused with a \"class of citizen\") was actually a general citizen assembly that was called by Philip IV in 1302.\n\nIn the period leading up to the Estates General of 1789, France was in the grip of an unmanageable public debt (nearly 3.56 million livres). In May 1776, finance minister Turgot was dismissed, after failing to enact reforms. The next year, Jacques Necker, a foreigner, was appointed Comptroller-General of Finance. He could not be made an official minister because he was a Protestant.), terrible inflation and widespread food scarcity (a huge famine in the winter of 1788-89). This led to widespread popular discontent and produced a group of Third Estate representatives (612 exactly) pressing a comparatively radical set of reforms, much of it in alignment with the goals of Finance Minister Jacques Necker, but very much against the wishes of Louis XVI's court and many of the hereditary nobles forming his Second Estate allies (at least allies against taking more taxes upon themselves and keeping the unequal taxation on the commoners).\n\nWhen he could not persuade them to rubber-stamp his 'ideal program', Louis XVI sought to dissolve the Estates-General, but the Third Estate held out for their right to representation. The lower clergy (and some nobles and upper clergy) eventually sided with the Third Estate, and the King was forced to yield. Thus, the Estate-General meeting was an invitation to revolution.\n\nBy June, when continued impasses led to further deterioration in relations, the Estates-General was reconstituted first as the National Assembly (June 17, 1789) seeking a solution for the realm independent of the King's management of the meetings of the Estates General which occasionally continued to meet. These self-organized meetings are today defined as the epoch event beginning the historical epoch (era) of the French Revolution, during which – after several more weeks of civil unrest – the body assumed a new status as a revolutionary legislature, the National Constituent Assembly (July 9, 1789).\n\nThis unitary body composed of the former representatives of the three estates stepping up to govern along with an emergency committee in the power vacuum existing after the Bourbon monarchy fled Paris. Among the Assembly was Maximilien de Robespierre, an influential member of the Jacobins who would years later become instrumental in the turbulent period of violence and political upheaval in France known as the Reign of Terror (5 September 1793 – 28 July 1794).\n\nWhilst the estates were never formulated in a way that prevented social mobility, the English (subsequently the British) parliament was long based along the classic estate lines being composed on the \"Lords Spiritual and Temporal, and Commons\". The tradition where the Lords Spiritual and Temporal sat separately from the Commons began during the reign of Edward III in the 14th century.\n\nNotwithstanding the House of Lords Act 1999, the British Parliament still recognises the existence of the three estates: the Commons in the House of Commons, the nobility (Lords Temporal) in the House of Lords, and the clergy in the form of the Church of England bishops also entitled to sit in the upper House as the Lords Spiritual.\n\nThe members of the Parliament of Scotland were collectively referred to as the Three Estates (Older Scots: Thre Estaitis), also known as the community of the realm, and until 1690 composed of:\n\nThe First Estate was overthrown during the Glorious Revolution and the accession of William III. The Second Estate was then split into two to retain the division into three.\n\nA \"Shire Commissioner\" was the closest equivalent of the English office of \"Member of Parliament\", namely a commoner or member of the lower nobility. Because the Parliament of Scotland was unicameral, all members sat in the same chamber, as opposed to the separate English House of Lords and House of Commons.\n\nThe Parliament also had University constituencies (see Ancient universities of Scotland). The system was also adopted by the Parliament of England when James VI ascended to the English throne. It was believed that the universities were affected by the decisions of Parliament and ought therefore to have representation in it. This continued in the Parliament of Great Britain after 1707 and the Parliament of the United Kingdom until 1950.\n\nThe Estates in Sweden (including Finland) and later also Russia's Grand Duchy of Finland were the two higher estates, nobility and clergy, and the two lower estates, burghers and land-owning peasants. Each were free men, and had specific rights and responsibilities, and the right to send representatives to the Riksdag of the Estates. The Riksdag, and later the Diet of Finland was tetracameral: at the Riksdag, each Estate voted as a single body. Since early 18th century, a bill needed the approval of at least three Estates to pass, and constitutional amendments required the approval of all Estates. Prior to the 18th century, the King had the right to cast a deciding vote if the Estates were split evenly.\n\nAfter Russia's conquest of Finland in 1809, the estates in Finland swore an oath to the Emperor in the Diet of Porvoo. A Finnish House of Nobility was codified in 1818 in accordance with the old Swedish law of 1723. However, after the Diet of Porvoo, the Diet of Finland was reconvened only in 1863. In the meantime, for a period of 54 years, the country was governed only administratively.\n\nThere was also a population outside the estates. Unlike in other areas, people had no \"default\" estate, and were not peasants unless they came from a land-owner's family. A summary of this division is:\n\nIn Sweden, the Riksdag of the Estates existed until it was replaced with a bicameral Riksdag in 1866, which gave political rights to anyone with a certain income or property. Nevertheless, many of the leading politicians of the 19th century continued to be drawn from the old estates, in that they were either noblemen themselves, or represented agricultural and urban interests. Ennoblements continued even after the estates had lost their political importance, with the last ennoblement of explorer Sven Hedin taking place in 1902; this practice was formally abolished with the adoption of the new Constitution January 1, 1975, while the status of the House of Nobility continued to be regulated in law until 2003.\n\nIn Finland, this legal division existed until 1906, still drawing on the Swedish constitution of 1772. However, at the start of the 20th century most of the population did not belong to any Estate and had no political representation. A particularly large class were the rent farmers, who did not own the land they cultivated but had to work in the land-owner's farm to pay their rent (unlike Russia, there were no slaves or serfs.) Furthermore, the industrial workers living in the city were not represented by the four-estate system.\n\nThe political system was reformed as a result of the Finnish general strike of 1905, with the last Diet instituting a new constitutional law to create the modern parliamentary system, ending the political privileges of the estates. The post-independence constitution of 1919 forbade ennoblement, and all tax privileges were abolished in 1920. The privileges of the estates were officially and finally abolished in 1995, although in legal practice, the privileges had long been unenforceable. As in Sweden, the nobility has not been officially abolished and records of nobility are still voluntarily maintained by the Finnish House of Nobility.\n\nIn Finland, it is still illegal and punishable by jail time (up to one year) to defraud into marriage by declaring a false name or estate (Rikoslaki 18 luku § 1/Strafflagen 18 kap. § 1).\n\nThe Low Countries, which until the late sixteenth century consisted of several counties, prince bishoprics, duchies etc. in the area that is now modern Belgium, Luxembourg and the Netherlands, had no States General until 1464, when Duke Philip of Burgundy assembled the first States General in Bruges. Later in the 15th and 16th centuries Brussels became the place where the States General assembled. On these occasions deputies from the States of the various provinces (as the counties, prince-bishoprics and duchies were called) asked for more liberties. For this reason, the States General were not assembled very often.\n\nAs a consequence of the Union of Utrecht in 1579 and the events that followed afterwards, the States General declared that they no longer obeyed King Philip II of Spain, who was also overlord of the Netherlands. After the reconquest of the southern Netherlands (roughly Belgium and Luxemburg), the States General of the Dutch Republic first assembled permanently in Middelburg, and in The Hague from 1585 onward. Without a king to rule the country, the States General became the sovereign power. It was the level of government where all things were dealt with that were of concern to all the seven provinces that became part of the Republic of the United Netherlands.\n\nDuring that time the States General were formed by representatives of the States (i.e. provincial parliaments) of the seven provinces. In each States (a plurale tantum) sat representatives of the nobility and the cities (the clergy were no longer represented; in Friesland the peasants were indirectly represented by the \"Grietmannen\").\n\nIn the Southern Netherlands, the last meetings of the States General loyal to the Habsburgs took place in the Estates General of 1600 and the Estates General of 1632.\n\nAs a government, the States General of the Dutch Republic were abolished in 1795. A new parliament was created, called \"Nationale Vergadering\" (National Assembly). It no longer consisted of representatives of the States, let alone the Estates: all men were considered equal under the 1798 Constitution. Eventually, the Netherlands became part of the French Empire under Napoleon (1810:\" La Hollande est reunie à l'Empire\").\n\nAfter regaining independence in November 1813, the name \"States General\" was resurrected for a legislature constituted in 1814 and elected by the States-Provincial. In 1815, when the Netherlands were united with Belgium and Luxemburg, the States General were divided into two chambers: the First Chamber and the Second Chamber. The members of the First Chamber were appointed for life by the King, while the members of the Second Chamber were elected by the members of the States Provincial. The States General resided in The Hague and Brussels in alternate years until 1830, when, as a result of the Belgian Revolution, The Hague became once again the sole residence of the States General, Brussels instead hosting the newly founded Belgian Parliament.\n\nFrom 1848 on, the Dutch Constitution provides that members of the Second Chamber be elected by the people (at first only by a limited portion of the male population; universal male and female suffrage exists since 1919), while the members of the First Chamber are chosen by the members of the States Provincial. As a result, the Second Chamber became the most important. The First Chamber is also called Senate. This however, is not a term used in the Constitution.\n\nOccasionally the First and Second Chamber meet in a \"Verenigde Vergadering\" (Joint Session), for instance on Prinsjesdag, the annual opening of the parliamentary year, and when a new king is inaugurated.\n\nThe Holy Roman Empire had the Imperial Diet (\"Reichstag\"). The clergy was represented by the independent prince-bishops, prince-archbishops and prince-abbots of the many monasteries. The nobility consisted of independent aristocratic rulers: secular prince-electors, kings, dukes, margraves, counts and others. Burghers consisted of representatives of the independent imperial cities. Many peoples whose territories within the Holy Roman Empire had been independent for centuries had no representatives in the Imperial Diet, and this included the Imperial Knights and independent villages. The power of the Imperial Diet was limited, despite efforts of centralization.\n\nLarge realms of the nobility or clergy had estates of their own that could wield great power in local affairs. Power struggles between ruler and estates were comparable to similar events in the history of the British and French parliaments.\n\nThe Swabian League, a significant regional power in its part of Germany during the 15th Century, also had its own kind of Estates, a governing Federal Council comprising three Colleges: those of Princes, Cities, and Knights.\n\nIn the late Russian Empire the estates were called \"sosloviyes\". The four major estates were: nobility (\"dvoryanstvo\"), clergy, rural dwellers, and urban dwellers, with a more detailed stratification therein. The division in estates was of mixed nature: traditional, occupational, as well as formal: for example, voting in Duma was carried out by estates. Russian Empire Census recorded the reported estate of a person.\n\nThe Parliament of Catalonia was first established in 1283 as the Catalan Courts (\"Corts Catalanes\"), according to American historian Thomas Bisson, and it has been considered by several historians as a model of medieval parliament. For instance, English historian of constitutionalism Charles Howard McIlwain wrote that the General Courts of Catalonia, during the 14th century, had a more defined organization and met more regularly than the parliaments of England or France.\n\nThe roots of the parliament institution in Catalonia are in the Sanctuary and Truce Assemblies (\"assemblees de pau i treva\") that started in the 11th century.\nThe members of the parliament of Catalonia were organized in the Three Estates (Catalan: \"Tres Estats\"):\nThe parliament institution was abolished in 1716, together with the rest of institutions of Catalonia, after the War of the Spanish Succession.\n\n\nLocation specific:\n\nGeneral:\n\n"}
{"id": "32755464", "url": "https://en.wikipedia.org/wiki?curid=32755464", "title": "Exponentially closed field", "text": "Exponentially closed field\n\nIn mathematics, an exponentially closed field is an ordered field of formula_1 which has an order preserving isomorphism formula_2 of the additive group of formula_1 onto the multiplicative group of positive elements of formula_1 such that \nformula_5 for some natural number formula_6.\n\nIsomorphism formula_2 is called an exponential function in formula_1.\n\n\n"}
{"id": "4285389", "url": "https://en.wikipedia.org/wiki?curid=4285389", "title": "Featural writing system", "text": "Featural writing system\n\nIn a featural writing system, the shapes of the symbols (such as letters) are not arbitrary but encode phonological features of the phonemes that they represent. The term featural was introduced by Geoffrey Sampson to describe the Korean alphabet and Pitman shorthand.\n\nJoe Martin introduced the term featural notation to describe writing systems that include symbols to represent individual features rather than phonemes. He asserts that \"alphabets have no symbols for anything smaller than a phoneme\".\n\nA \"featural\" script represents finer detail than an alphabet. Here symbols do not represent whole phonemes, but rather the elements (features) that make up the phonemes, such as voicing or its place of articulation. Theoretically, each feature could be written with a separate letter; and abjads or abugidas, or indeed syllabaries, could be featural, but the only prominent system of this sort is the Korean alphabet, also known as hangul or chosŏn'gŭl. In the Korean alphabet, the featural symbols are combined into alphabetic letters, and these letters are in turn joined into syllabic blocks, so that the system combines three levels of phonological representation.\n\nMany scholars, e.g. John DeFrancis, reject this class or at least labeling the Korean alphabet as such. The Korean script is a conscious script creation by literate experts, which Daniels calls a \"sophisticated grammatogeny\". These include stenographies and constructed scripts of hobbyists and fiction writers (such as Tengwar), many of which feature advanced graphic designs corresponding to phonologic properties. The basic unit of writing in these systems can map to anything from phonemes to words. It has been shown that even the Latin script has sub-character \"features\".\n\nThis is a small list of examples of featural writing systems by date of creation. The languages for which each system was developed are also shown.\n\n\n\n\nOther scripts may have limited featural elements. Many languages written in the Latin alphabet make use of diacritics, and those letters using diacritics are sometimes considered separate letters within the language's alphabet. The Polish alphabet, for example, indicates a palatal articulation of some consonants with an acute accent. The Turkish alphabet uses the presence of one or two dots above a vowel to indicate that it is a front vowel. The Japanese kana syllabaries indicate voiced consonants with marks known as dakuten. The International Phonetic Alphabet (IPA) also has some featural elements, for example in the hooks and tails that are characteristic of implosives, , and retroflex consonants, . The IPA diacritics are also featural. The Fraser alphabet used for Lisu rotates the letters for the tenuis consonants ꓑ , ꓔ , ꓝ , ꓚ , and ꓗ 180° to indicate aspiration.\n"}
{"id": "54102442", "url": "https://en.wikipedia.org/wiki?curid=54102442", "title": "Feminist businesses", "text": "Feminist businesses\n\nFeminist businesses are companies established by activists involved in the feminist movement. Examples include feminist bookstores, feminist credit unions, feminist presses, feminist mail-order catalogs, and feminist restaurants. These businesses flourished as part of the second and third-waves of feminism in the 1970s, 1980s, and 1990s. Feminist entrepreneurs established organizations such as the Feminist Economic Alliance to advance their cause. Feminist entrepreneurs sought three primary goals: to disseminate their ideology through their businesses, to create public spaces for women and feminists, and to create jobs for women so that they didn't have to depend on men financially. While they still exist today, the number of some feminist businesses, particularly women's bookstores, has declined precipitously since 2000.\n\nFeminist bookstores hold a part of the second-wave feminism movement inside their stores, with expansion of the bookstores beginning in the 1980s. In 1983 there were around 100 bookstores located in North America, which created over $400 million in sales annually. There are still 13 feminist-run bookstores in the world today. There is only one in Canada and the rest are spread throughout the United States, with the oldest bookstore, Antigone Books, in Tucson, Arizona. The 13 book stores host feminist events to support feminism as well as carry books about the topics of queer theories, animal rights, lesbian fiction, gay studies, and also information about different cultures.\n\nThe decline in feminist bookstores is due to the competition of E-Books, corporate chains, online stores, and the presence of Amazon. These competitors make independent bookstores face financial difficulties. Recent studies show that consumers still want both the hard copy and digital copy. Besides big corporations coming in and taking business, it was also hard for feminist bookstores to keep up with the competitions distribution, publishing, as well as the ability to sell books to a wider audience.\n\nNotable stores include Amazon Bookstore Cooperative and Silver Moon Bookshop.\n\nDuring the Thanksgiving of 1975, the founding women of the Feminist Economic Alliance (FEA) met in Detroit, Michigan at a conference to discuss the problems women faced with money. Two leading women for the alliance in 1975 were Susan Osborne and Linda Maslanko, both from New York. They were the spokeswomen for FEA and educated the public on what the alliance meant and what the future of FEA looked like after splitting into eight geographic regions. The Feminist Economic Alliance was created to aid new sister credit unions as well as allowing any women to become economically powerful, independent, or grow as an individual. This independence for women was going to be achieved by encouraging, assisting, and promoting the women of feminist credit unions and feminist enterprises. The main idea behind this new alliance was that older sister credit unions could help the new developing credit unions by sharing research, resources, and guidance in the process.\n\nIn the 1970s during the second wave feminist movement, women had the urge to fight unequal credit so they created their own non-profit, financial institutions so that men were no longer in control of their money. The co-manager of the union, Susan Osborne, was creating an environment for women to save money as well as help other women in need. By creating their own credit unions, women were able to avoid being discriminated based on their gender even though the Equal Credit Opportunity Act of 1974 banned credit unions from discriminating potential customers. The women being excluded from receiving loans despite the law in place, were divorced women, low-income women, women needing legal money or women on welfare. Establishing feminist credit unions meant that they would now be able to receive loans hassle free, save their money, and gain money management counseling. When receiving credit, women are viewed by their individual character rather than if they were married or single. A woman no longer has to be the co-signer, she can now be in control of her money. The unions run no differently than any other union, in fact, the feminist credit unions are governed by the same laws as the normal credit unions.\n\nIn 1982, the Detroit, Michigan branch, the last feminist credit union, was dissolved due to financial problems and also the reconstruction of the unions language change. The language was suggested to be changed to include both genders, not just female. The Michigan Credit Union League saw the feminist credit unions as bias towards men and suggestions of equal consideration for males in a female position were to be given resulting in it being dissovled.\n\nThe feminist mail-order magazine came from Great Britain around the 1970s and lasted until the 1990s. The collectives were notable for allowing women to take equal parts in the creation of the magazine in all areas including: copy typing, design, layout or interviewing. By allowing women the equal chance at learning, women were developing their creativity and gaining new skills. Women were allowed to fight back at the patriarchal system by voicing their opinions and allowing women who were excluded also have a platform. Excluded women during that time were black, lesbian, working class, or single mothers. Popular magazines at the time were Spare Rib, Scarlet Woman, Catcall, and Outwrite. The magazines were not afraid to comment about inequalities the women were facing or issues that needed to be addressed. Feminist activities were also talked about in the magazines creating networks, reformation, expressing opinions or attitudes relating to a certain topic. Mail-order magazines were a way for women to become educated on feminism and how to join the movement.\n\nSpare Rib ran from 1972–93 and was an active part in the Women's Liberation movement. The magazine covered issues regarding stereotypes women face as well as issues women face in the world and realistic solutions to the problems.\n\nIn April 1975, the first issue of Scarlet Woman was published by Sydney SW Collective. The magazine was created to be a socialist feminist magazine and included articles dealing with money, lesbians, health and more.\n\nIn 1972, Gloria Steinem created the first magazine that was specifically for women, created by an entire team of women in New York. Ms. Magazine was the first magazine to address domestic violence, speak about politics, or discuss topics men thought were unnatural for women, motivating the feminist movement. Gloria Steinem was also the co-founder of Woman's Action Alliance and National Women's Political Caucus. Brenda Feigen, co-founder of the Woman's Action Alliance with Steinem, was an attorney activist who helped Steinem brainstorm ideas for the magazine as well as held meetings in her apartment. Today, Ms. Magazine is published by Liberty Media for Women, LLC, owned by Feminist Majority Foundation, that is based in Arlington, Virginia and Los Angeles, California.\n\nThe earliest form of feminist restaurants took shape in suffrage restaurants, tea rooms, or lunch rooms. Food was sold at a low cost of five or ten cents and men were permitted to eat, in hopes of women persuading men to support a certain political cause. These restaurants suffered from conflicts dealing with the founders and donators. Alva Belmont, a wealthy socialite, was the founder of a suffrage restaurant that was known for strict rules and a fast pace. The ideas and motives behind these suffrage restaurants in the 1910s were the foundations for the feminist restaurants in the 1970s.\n\nIn April 1972, the first feminist restaurant, Mother Courage, was founded by Dolores Alexander in New York. Feminist restaurants are used more as a place to gather and socialize rather than eating. The restaurants were used to share ideas, literature, educate one another and to promote the feminist movement. Guest speakers, political speakers, poets, or musicians would come to the restaurants to promote issues or spread awareness. Coffee houses and cafes are also popular among the feminist movement. Restaurants offered same pay to every staff member, which was entirely women. The style was simple and supported the movements that were occurring during that time. They support other occupations by avoiding certain products such as lettuce and grapes for the farmers or boycotting orange juice for the anti-gay campaign. Feminist restaurants were also notable for treating women or lesbians with respect in a non-hostile environment. A women who is dining with a man will be given a wine sample as well as the check at the end of the meal. That was typical not the case in restaurants that were not catered to feminism.\n\nIn today's society, feminist businesses look different besides the few bookstores left in the world. There are over hundreds of companies created by women, that have a purpose besides making money such as changing our society, impacting employers and the consumers they reach. One famous company started by a woman that has been successful is Tory Burch. She created the store from nothing and has been able to create a multibillion-dollar business as well as a foundation called the Tory Burch Foundation in 2009, to help empower women and female entrepreneurs. Today feminist business are about empowering women in the shape of products sold, campaigns run, and businesses created.\n"}
{"id": "19291269", "url": "https://en.wikipedia.org/wiki?curid=19291269", "title": "Genealogy (philosophy)", "text": "Genealogy (philosophy)\n\nIn philosophy, genealogy is a historical technique in which one questions the commonly understood emergence of various philosophical and social beliefs by attempting to account for the scope, breadth or totality of discourse, thus extending the possibility of analysis, as opposed to the Marxist use of the term ideology to explain the totality of historical discourse within the time period in question by focusing on a singular or dominant discourse (ideology). Moreover, a genealogy often attempts to look beyond the discourse in question toward the conditions of their possibility (particularly in Foucault's genealogies). It has been developed as continuation of the works of Friedrich Nietzsche. For example, tracking the lineages of a concept such as 'globalization' can be called a 'genealogy' to the extent that the concept is located in its changing constitutive setting. This entails not just documenting its changing meaning (etymology) but the social basis of its changing meaning.\n\nNietzsche criticized \"the genealogists\" in \"On the Genealogy of Morals\" and proposed the use of a historic philosophy to critique modern morality by supposing that it developed into its current form through power relations. But scholars note that he emphasizes that, rather than being purely necessary developments of power relations, these developments are to be exposed as at least partially contingent, the upshot being that the present conception of morality could always have been constituted otherwise. Even though the philosophy of Nietzsche has been characterized as genealogy, he only uses the term in \"On the Genealogy of Morals.\" The later philosophy that has been influenced by Nietzsche, and which is commonly described as genealogy, shares several fundamental aspects of Nietzschean philosophical insight. Nietzschean historic philosophy has been described as \"a consideration of oppositional tactics\" that embraces, as opposed to forecloses, the conflict between philosophical and historical accounts.\n\nIn the late twentieth century, Michel Foucault expanded the concept of genealogy into a counter-history of the position of the subject which traces the development of people and societies through history. His genealogy of the subject accounts \"for the constitution of knowledges, discourses, domains of objects, and so on, without having to make reference to a subject which is either transcendental in relation to the field of events or runs in its empty sameness throughout the course of history.\"\n\nAs Foucault discussed in his essay \"Nietzsche, Genealogy, History\", Foucault's ideas of genealogy were greatly influenced by the work that Nietzsche had done on the development of morals through power. Foucault also describes genealogy as a particular investigation into those elements which \"we tend to feel [are] without history\". This would include things such as sexuality, and other elements of everyday life. Genealogy is not the search for origins, and is not the construction of a linear development. Instead it seeks to show the plural and sometimes contradictory past that reveals traces of the influence that power has had on truth.\n\nAs one of the important theories of Michel Foucault, genealogy deconstructs truth, arguing that truth is, more often than not, discovered by chance, backed up by the operation of Power/knowledge or the consideration of interest. Furthermore, all truths are questionable. Pointing out the unreliability of truth, which is often accused as \"having tendency of relativity and nihilism\", the theory flatly refuses the uniformity and regularity of history, emphasizing the irregularity and inconstancy of truth and toppling the notion that history progresses in a linear order.\n\nThe practice of genealogy is also closely linked to what Foucault called the \"archeological method:\"\n\nIn short, it seems that from the empirical observability for us of an ensemble to its historical acceptability, to the very period of time in which it is actually observable, the analysis goes by way of the knowledge-power nexus, supporting it, recouping it at the point where it is accepted, moving toward what makes it acceptable, of course, not in general, but only where it is accepted. This is what can be characterized as recouping it in its positivity. Here, then, is a type of procedure, which, unconcerned with legitimizing and consequently excluding the fundamental point of view of the law, runs through the cycle of positivity by proceeding from the fact of acceptance to the system of acceptability analyzed through the knowledge-power interplay. Let us say that this is, approximately, the archaeological level [of analysis].\n\n"}
{"id": "2414831", "url": "https://en.wikipedia.org/wiki?curid=2414831", "title": "Heart (symbol)", "text": "Heart (symbol)\n\nThe heart shape () is an ideograph used to express the idea of the \"heart\" in its metaphorical or symbolic sense as the center of emotion, including affection and love, especially romantic love.\n\nThe \"wounded heart\" indicating lovesickness came to be depicted as a heart symbol pierced with an arrow (Cupid's), or heart symbol \"broken\" in two or more pieces.\n\nGreek. In the 6th-5th century BC, the heart shape was used to represent the heart-shaped fruit of the plant Silphium, a plant possibly used as a contraceptive Many species in the parsley family have estrogenic properties, and some, such as wild carrot, were used to induce abortion. Silver coins from Cyrene of the 6–5th BC bear a similar design, sometimes accompanied by a silphium plant and is understood to represent its seed or fruit.\n\nIt is likely that the heart symbol was found in devotion to God. The symbol appears as one rights themselves up, after having prayed with one's head to the ground. The hands form the symbol as one is in the process of standing up. \n\nThe combination of the heart shape and its use within the heart metaphor developed at the end of the Middle Ages, although the shape has been used in many ancient epigraphy monuments and texts. With possible early examples or direct predecessors in the 13th to 14th century, the familiar symbol of the heart represented love developed in the 15th century, and became popular in Europe during the 16th. Before the 14th century, the heart shape was not associated with the meaning of the heart metaphor. The geometric shape itself is found in much earlier sources, but in such instances does not depict a heart, but typically foliage: in examples from antiquity fig leaves, and in medieval iconography and heraldry typically the leaves of ivy and of the water-lily. One possible early use in the 11th century could be found in the manuscript, Al-Maqamat written by Al Hariri of Basra. The manuscript includes an illustration of a farewell greeting between two men while astride their camels, with the heart shape seen prominently over their heads.\n\nThe first known depiction of a heart as a symbol of romantic love dates to the 1250s. It occurs in a miniature decorating a capital 'S' in a manuscript of the French \"Roman de la poire\" (National Library FR MS. 2086, plate 12). In the miniature a kneeling lover (or more precisely, an allegory of the lover's \"sweet gaze\" or \"douz regart\") offers his heart to a damsel. The heart here resembles a pine cone (held \"upside down\", the point facing upward), in accord with medieval anatomical descriptions. However, in this miniature what suggests a heart shape is only the result of a lover's finger superimposed on an object; the full shape outline of the object is partly hidden, and therefore unknown. Moreover, the French title of the manuscript that features the miniature translates into \"Novel Of The Pear\" in English. Thus the heart shaped object would be a pear; the conclusion that a pear represents a heart is dubious. Opinions therefore differ over this being the first depiction of a heart as symbol of romantic love. Giotto in his 1305 painting in the Scrovegni Chapel (Padua) shows an allegory of charity (caritas) handing her heart to Jesus Christ. This heart is also depicted in the pine cone shape based on anatomical descriptions of the day (still held \"upside down\"). Giotto's painting exerted considerable influence on later painters, and the motive of Caritas offering a heart is shown by Taddeo Gaddi in Santa Croce, by Andrea Pisano on the bronze door of the south porch of the Baptisterium in Florence (c. 1337), by Ambrogio Lorenzetti in the Palazzo Publico in Siena (c. 1340) and by Andrea da Firenze in Santa Maria Novella in Florence (c. 1365). The convention of showing the heart point upward switches in the late 14th century and becomes rare in the first half of the 15th century.\n\nThe \"scalloped\" shape of the now-familiar heart symbol, with a dent in its base, arises in the early 14th century, at first only lightly dented, as in the miniatures in Francesco Barberino's \"Documenti d'amore \"(before 1320). A slightly later example with a more pronounced dent is found in a manuscript from the Cistercian monastery in Brussels (MS 4459–70, fol 192v. Royal Library of Belgium). The convention of showing a dent at the base of the heart thus spread at about the same time as the convention of showing the heart with its point downward. The modern indented red heart has been used on playing cards since the late 15th century.\n\nVarious hypotheses attempted to connect the \"heart shape\" as it evolved in the Late Middle Ages with instances of the geometric shape in antiquity. Such theories are modern, proposed from the 1960s onward, and they remain speculative, as no continuity between the supposed ancient predecessors and the late medieval tradition can be shown. Specific suggestions include: the shape of the seed of the silphium plant, used in ancient times as an herbal contraceptive, and stylized depictions of features of the human female body, such as the female's breasts, buttocks, pubic mound, or spread vulva.\n\nHeart shapes can be seen on the Bible Jesus holds in the Empress Zoë mosaic in the Hagia Sophia in Istanbul, but a reference to the organ was probably not intended. It probably dates from 1239.\n\nLikewise, heart shapes can be seen on various stucco reliefs and wall panels excavated from the ruins of Ctesiphon, the Persian capital (circa 90 BC – 637 AD).\n\nThe Luther rose was the seal that was designed for Martin Luther at the behest of Prince John Frederick, in 1530, while Luther was staying at the Coburg Fortress during the Diet of Augsburg. Luther wrote an explanation of the symbol to Lazarus Spengler: \"a black cross in a heart, which retains its natural color, so that I myself would be reminded that faith in the Crucified saves us. 'For one who believes from the heart will be justified' (Romans 10:10).\"\n\nThe aorta remains visible, as a protrusion at the top centered between the two \"chambers\" indicated in the symbol, in some depictions of the Sacred Heart well into the 18th century, and is partly still shown today (although mostly obscured by elements such as a crown, flames, rays, or a cross) but the \"hearts\" suit did not have this element since the 15th century.\n\nThe heart symbol reached Japan with the Nanban trade of 1543 to 1614, as evidenced by an Edo period Samurai helmet (dated c. 1630), which includes both the rounded and indented forms of the heart symbol, representing the heart of Marishiten, goddess of archers.\nSince the 19th century, the symbol has often been used on Valentine's Day cards, candy boxes, and similar popular culture artifacts as a symbol of romantic love.\n\nThe use of the heart symbol as a logograph for the English verb \"to love\" derives from the use in \"I ♥ NY,\" introduced in 1977.\n\nHeart symbols were used to symbolize \"health\" or \"lives\" in video games; influentially so in \"The Legend of Zelda\" (1986). \"Super Mario Bros. 2\" (1987, 1988) had a \"life bar\" composed of hexagons, but in 1990s remakes of these games, the hexagons were replaced by heart shapes. Since the 1990s, the heart symbol has also been used as an ideogram indicating health outside of the video gaming context, e.g. its use by restaurants to indicate heart-healthy nutrient content claim (e.g. \"low in cholesterol\"). A copyrighted \"heart-check\" symbol to indicate heart-healthy food was introduced by the American Heart Association in 1995.\n\nThe earliest heart-shaped charges in heraldry appear in the 12th century; the hearts in the coat of arms of Denmark go back to the royal banner of the kings of Denmark, in turn based on a seal used as early as the 1190s. However, while the charges are clearly heart-shaped, they did not in origin depict hearts, or symbolize any idea related to love. Instead they are assumed to have depicted the leaves of the water-lily. Early heraldic heart-shaped charges depicting the leaves of water-lilies are found in various other designs related to territories close to rivers or a coastline (\"e.g.\" Flags of Frisia).\n\nInverted heart symbols have been used in heraldry as stylized testicles (\"coglioni\" in Italian) as in the canting arms of the Colleoni family of Milan.\n\nA seal attributed to William, Lord of Douglas (of 1333) shows a heart shape, identified as the heart of Robert the Bruce. The authenticity of this seal is \"very questionable\", i.e. it could possibly date to the late 14th or even the 15th century.\n\nHeraldic charges actually representing hearts became more common in the early modern period, with the Sacred Heart depicted in ecclesiastical heraldry, and hearts representing love appearing in bourgeois coats of arms. Hearts also later became popular elements in municipal coats of arms.\n\nThere has been some conjecture regarding the link between the traditional heart symbol and images of the fruit of Silphium, a (probably) extinct plant known to classical antiquity and belonging to the genus Ferula, used as a condiment and medicine, (the medicinal properties including contraceptive and abortifacient activity, linking the plant to sexuality and love). Silver coins from the ancient Libya of the 6th - 5th BC bear images strongly reminiscent of the heart symbol, sometimes accompanied by images of the Silphium plant. The related Ferula species Asafoetida - which was actually used as an inferior substitute for Silphium - is regarded as an aphrodisiac in Tibet and India, suggesting yet a third amatory association relating to Silphium.\n\nA common emoticon for the heart is <3. In Unicode several heart symbols are available:\n\nAnd from the Miscellaneous Symbols and Pictographs and Supplemental Symbols and Pictographs ranges associated with emoji:\n\nIn Code page 437, the original character set of the IBM PC, the value of 3 (hexadecimal 03) represents the heart symbol. This value is shared with the non-printing ETX control character, which overrides the glyph in many contexts.\n\nThe Unicode character of the letter \"ghan\" of the Georgian alphabet (ღ) has seen some use as a surrogate heart symbol in online communication.\n\nA number of parametrisations of approximately heart-shaped curves have been described.\nThe best-known of these is the cardioid, which is an epicycloid with one cusp; though as the cardioid lacks the point, it may be seen as a stylized water-lily leaf, a so-called seeblatt, rather than a heart. Other curves, such as the implicit curve (x+y−1)−xy=0, may produce better approximations of the heart shape.\n\n\n\n"}
{"id": "1602548", "url": "https://en.wikipedia.org/wiki?curid=1602548", "title": "Implied powers", "text": "Implied powers\n\nImplied powers, in the United States, are powers authorized by the Constitution that, while not stated, seem implied by powers that are expressly stated. When George Washington asked Alexander Hamilton to defend the constitutionality of the First Bank of the United States against the protests of Thomas Jefferson, James Madison, and Attorney General Edmund Randolph, Hamilton produced what has now become the classic statement for implied powers. Hamilton argued that the sovereign duties of a government implied the right to use means adequate to its ends. Although the United States government was sovereign only as to certain objects, it was impossible to define all the means it should use, because it was impossible for the founders to anticipate all future exigencies. Hamilton noted that the \"general welfare clause\" and the \"necessary and proper clause\" gave elasticity to the Constitution. Hamilton won the argument with Washington, who signed the bank bill into law.\n\nLater, directly borrowing from Hamilton, Chief Justice John Marshall invoked the implied powers of government in the United States Supreme Court case, \"McCulloch v. Maryland\". In 1816, the United States Congress passed legislation creating the Second Bank of the United States. The state of Maryland attempted to tax the bank. The state argued the United States Constitution did not explicitly grant Congress the power to establish banks. In 1819, the Court decided against the state of Maryland. Chief Justice Marshall argued that Congress had the right to establish the bank, as the Constitution grants to Congress certain implied powers beyond those explicitly stated.\n\nIn the case of the United States Government, implied powers are powers Congress exercises that the Constitution does not explicitly define, but are necessary and proper to execute the powers.\n\nImplied powers are those that can reasonably be assumed to flow from express powers, though not explicitly mentioned. The legitimacy of these powers is derived from the Taxing and Spending Clause, the Necessary and Proper Clause, and the Commerce Clause. \n"}
{"id": "19287542", "url": "https://en.wikipedia.org/wiki?curid=19287542", "title": "Interaction overview diagram", "text": "Interaction overview diagram\n\nInteraction Overview Diagram is one of the fourteen types of diagrams of the Unified Modeling Language (UML), which can picture a control flow with nodes that can contain interaction diagrams. \n\nThe interaction overview diagram is similar to the activity diagram, in that both visualize a sequence of activities. The difference is that, for an interaction overview, each individual activity is pictured as a frame which can contain a nested interaction diagram. This makes the interaction overview diagram useful to \"deconstruct a complex scenario that would otherwise require multiple if-then-else paths to be illustrated as a single sequence diagram\".\n\nThe other notation elements for interaction overview diagrams are the same as for activity diagrams. These include initial, final, decision, merge, fork and join nodes. The two new elements in the interaction overview diagrams are the \"interaction occurrences\" and \"interaction elements.\" \n\n"}
{"id": "2768467", "url": "https://en.wikipedia.org/wiki?curid=2768467", "title": "Interstimulus interval", "text": "Interstimulus interval\n\nThe interstimulus interval (often abbreviated as ISI) is the temporal interval between the offset of one stimulus to the onset of another. For instance, Max Wertheimer did experiments with two stationary, flashing lights that at some interstimulus intervals appeared to the subject as moving instead of stationary. In these experiments, the interstimulus interval is simply the time between the two flashes. The ISI plays a large role in the phi phenomenon (Wertheimer) since the illusion of motion is directly due to the length of the interval between stimuli. When the ISI is shorter, for example between two flashing lines alternating back and forth, we perceive the change in stimuli to be movement. Wertheimer discovered that the space between the two lines is filled in by our brains and that the faster the lines alternate, the more likely we are to perceive it as one line moving back and forth. When the stimuli move fast enough, this creates the illusion of a moving picture like a movie or cartoon. Phi phenomenon is very similar to beta movement.\n\nAs it applies to classical conditioning, the term interstimulus interval is used to represent the gap of time between the start of the conditioned stimulus and the start of the unconditioned stimulus. An example would be the case of Pavlov's dog, where the time between the unconditioned stimulus, the food, and the conditioned stimulus, the bell, is considered the ISI. More particularly, ISI is often used in eyeblink conditioning (a widely studied type of classical conditioning involving puffs of air blown into the subject's eyes) where the ISI can effect learning based on the size of the time gap. What is of interest in this particular type of classical conditioning is that when the subject is conditioned to blink after the conditioned stimulus (tone), the blink will take place within the time period between the tone and the air puff, making the subject's eyes close before the puff can reach the eyes, protecting them from the air.\n\nThe timing between the conditioned and unconditioned stimulus is important. There are two types of approaches for eye blink conditioning when it comes to timing between the stimuli. The first is called delay conditioning, which is when the conditioned stimulus (tone) starts, then continues until the unconditioned stimulus (air puff) is released after a delay, then they both suspend at the same time. The other is called trace conditioning, where the conditioned stimulus (tone) is shorter and stops before the unconditioned stimulus (air puff) begins, leaving a gap between the two stimuli. This type of conditioning forces the subject, in this particular example, a bunny, to remember to link the conditioned stimulus with the unconditioned stimulus.\n\nThe distinction between the two types of conditioning is of importance because the difference in the interstimulus interval (ISI) can have major effects on learning. For example, it has been shown that the length of the ISI, as well as the variability, changes habituation in subjects. When ISI is short and constant, habituation will happen more rapidly. The changes in the gap of time can be minuscule, from tens of milliseconds to several seconds long, and the effects it will have will still be important. Sensory and motor tasks are among the elements that can be enhanced or hindered based on timing, like speech processing, which can be influenced by \"the ability to discriminate the interval and duration of sounds.\"\n"}
{"id": "162321", "url": "https://en.wikipedia.org/wiki?curid=162321", "title": "Invariant mass", "text": "Invariant mass\n\nThe invariant mass, rest mass, intrinsic mass, proper mass, or in the case of bound systems simply mass, is the portion of the total mass of an object or system of objects that is independent of the overall motion of the system. More precisely, it is a characteristic of the system's total energy and momentum that is the same in all frames of reference related by Lorentz transformations. If a center of momentum frame exists for the system, then the invariant mass of a system is equal to its total mass in that \"rest frame\". In other reference frames, where the system's momentum is nonzero, the total mass (a.k.a. relativistic mass) of the system is greater than the invariant mass, but the invariant mass remains unchanged.\n\nDue to mass-energy equivalence, the rest energy of the system is simply the invariant mass times the speed of light squared. Similarly, the total energy of the system is its total (relativistic) mass times the speed of light squared.\n\nSystems whose four-momentum is a null vector (for example a single photon or many photons moving in exactly the same direction) have zero invariant mass, and are referred to as \"massless\". A physical object or particle moving faster than the speed of light would have space-like four-momenta (such as the hypothesized tachyon), and these do not appear to exist. Any time-like four-momentum possesses a reference frame where the momentum (3-dimensional) is zero, which is a center of momentum frame. In this case, invariant mass is positive and is referred to as the rest mass.\n\nIf objects within a system are in relative motion, then the invariant mass of the whole system will differ from the sum of the objects' rest masses. This is also equal to the total energy of the system divided by \"c\". See mass–energy equivalence for a discussion of definitions of mass. Since the mass of systems must be measured with a weight or mass scale in a center of momentum frame in which the entire system has zero momentum, such a scale always measures the system's invariant mass. For example, a scale would measure the kinetic energy of the molecules in a bottle of gas to be part of invariant mass of the bottle, and thus also its rest mass. The same is true for massless particles in such system, which add invariant mass and also rest mass to systems, according to their energy.\n\nFor an isolated \"massive\" system, the center of mass of the system moves in a straight line with a steady sub-luminal velocity (with a velocity depending on the reference frame used to view it). Thus, an observer can always be placed to move along with it. In this frame, which is the center of momentum frame, the total momentum is zero, and the system as a whole may be thought of as being \"at rest\" if it is a bound system (like a bottle of gas). In this frame, which exists under these assumptions, the invariant mass of the system is equal to the total system energy (in the zero-momentum frame) divided by . This total energy in the center of momentum frame, is the minimum energy which the system may be observed to have, when seen by various observers from various inertial frames.\n\nNote that for reasons above, such a rest frame does not exist for single photons, or rays of light moving in one direction. When two or more photons move in different directions, however, a center of mass frame (or \"rest frame\" if the system is bound) exists. Thus, the mass of a system of several photons moving in different directions is positive, which means that an invariant mass exists for this system even though it does not exist for each photon.\n\nBecause the invariant mass includes the mass of any kinetic and potential energies which remain in the center of momentum frame, the invariant mass of a system can be greater than sum of rest masses of its separate constituents. For example, rest mass and invariant mass are zero for individual photons even though they may add mass to the invariant mass of systems. For this reason, invariant mass is in general not an additive quantity (although there are a few rare situations where it may be, as is the case when massive particles in a system without potential or kinetic energy can be added to a total mass).\n\nConsider the simple case of two-body system, where object A is moving towards another object B which is initially at rest (in any particular frame of reference). The magnitude of invariant mass of this two-body system (see definition below) is different from the sum of rest mass (i.e. their respective mass when stationary). Even if we consider the same system from center-of-momentum frame, where net momentum is zero, the magnitude of the system's invariant mass is not equal to the sum of the rest masses of the particles within it.\n\nThe kinetic energy of such particles and the potential energy of the force fields increase the total energy above the sum of the particle rest masses, and both terms contribute to the invariant mass of the system. The sum of the particle kinetic energies as calculated by an observer is smallest in the center of momentum frame (again, called the \"rest frame\" if the system is bound).\n\nThey will often also interact through one or more of the fundamental forces, giving them a potential energy of interaction, possibly negative.\n\nFor an isolated \"massive\" system, the center of mass moves in a straight line with a steady sub-luminal velocity. Thus, an observer can always be placed to move along with it. In this frame, which is the center of momentum frame, the total momentum is zero, and the system as a whole may be thought of as being \"at rest\" if it is a bound system (like a bottle of gas). In this frame, which always exists, the invariant mass of the system is equal to the total system energy (in the zero-momentum frame) divided by .\n\nIn particle physics, the invariant mass is equal to the mass in the rest frame of the particle, and can be calculated by the particle's energy  and its momentum  as measured in \"any\" frame, by the energy–momentum relation:\n\nor in natural units where ,\n\nThis invariant mass is the same in all frames of reference (see also special relativity). This equation says that the invariant mass is the pseudo-Euclidean length of the four-vector , calculated using the relativistic version of the Pythagorean theorem which has a different sign for the space and time dimensions. This length is preserved under any Lorentz boost or rotation in four dimensions, just like the ordinary length of a vector is preserved under rotations. In quantum theory the invariant mass is a parameter in the relativistic Dirac equation for an elementary particle. The Dirac quantum operator corresponds to the particle four-momentum vector.\n\nSince the invariant mass is determined from quantities which are conserved during a decay, the invariant mass calculated using the energy and momentum of the decay products of a single particle is equal to the mass of the particle that decayed.\nThe mass of a system of particles can be calculated from the general formula:\n\nwhere\n\nThe term invariant mass is also used in inelastic scattering experiments. Given an inelastic reaction with total incoming energy larger than the total detected energy (i.e. not all outgoing particles are detected in the experiment), the invariant mass (also known as the \"missing mass\") of the reaction is defined as follows (in natural units):\n\nIf there is one dominant particle which was not detected during an experiment, a plot of the invariant mass will show a sharp peak at the mass of the missing particle.\n\nIn those cases when the momentum along one direction cannot be measured (i.e. in the case of a neutrino, whose presence is only inferred from the missing energy) the Transverse mass is used.\n\nIn a two-particle collision (or a two-particle decay) the square of the invariant mass (in natural units) is\n\nThe invariant mass of a system made of two massless particles whose momenta form an angle formula_8\nhas a convenient expression:\n\nIn particle collider experiments, one often defines the angular position of a particle in terms of an azimuthal angle formula_9 and pseudorapidity formula_10. Additionally the transverse momentum, formula_11, is usually measured. In this case if the particles are massless, or highly relativistic ( formula_12,) then the invariant mass becomes:\n\nThe rest energy formula_13 of a particle is defined as:\n\nwhere formula_15 is the speed of light in vacuum. In general, only differences in energy have physical significance.\n\nThe concept of rest energy follows from the special theory of relativity that leads to Einstein's famous conclusion about equivalence of energy and mass. See background for mass–energy equivalence.\n\nOn the other hand, the concept of the equivalent Dirac invariant rest mass may be defined in terms of the self energy corresponding to the product of a geometric matter current and a generalized potential as part of a single definition of mass in a geometric unified theory.\n\n\n"}
{"id": "28163870", "url": "https://en.wikipedia.org/wiki?curid=28163870", "title": "Jacques Rousseau (secular activist)", "text": "Jacques Rousseau (secular activist)\n\nJacques André Rousseau (born 3 December 1971) is a South African academic, secular activist and social commentator.\n\nRousseau was born in Cape Town in 1971. He attended Stellenberg High School and the University of Cape Town (UCT), where he obtained a BA (Hons) in Philosophy and a MA in English.\n\nRousseau lectures on critical thinking and ethics in the UCT Commerce Faculty's School of Management Studies. Since his appointment to the UCT academic staff in the 1990s, he has served on various UCT committees including the Senate, the Senate Executive Committee, the Faculty of Commerce Readmission Appeals Committee and the Ethics in Research Committee. He currently serves on the University Information and Communication Technology Committee and the University Student Discipline Tribunal, and is the chairperson of the Academic Freedom Committee. He was elected to both the UCT Senate and Council for the four-year term of office from 1 July 2012 to 30 June 2016.\n\nHe also participates in various research activities at UCT. In 2004 he became a member of the National Centre for the Study of Gambling, and from 2008 to 2012 he served as co-ordinator of its Academic Division at UCT which conducted research into gambling in South Africa on behalf of the South African Responsible Gambling Foundation. His current research relates to epistemic standards in science journalism, decision theory, business ethics and religious conflict.\n\n\nRousseau is an atheist, secularist, humanist, naturalist, materialist, freethinker, scientific sceptic and rationalist. In 2006 he established a local community of the Brights movement, which he describes as \"an international movement which aims to promote the civic understanding and acknowledgement of the naturalistic world-view, which is free of supernatural and mystical elements\". In 2009 he founded the Free Society Institute to promote secularity, scientific reasoning, a naturalistic worldview and freedom of speech.\n\nSince 2008 his blog \"Synapses\" has focussed on secular issues in South Africa, and he is on the editorial board of International Humanist News. He was included on a panel of \"top skeptic bloggers\" who presented a Blogging Skepticism workshop at The Amaz!ng Meeting 2013 conference in Las Vegas organised by the James Randi Educational Foundation.\n\nHe regularly contributes to public debates in the South African media. From March 2010 to June 2013, he was a regular contributor for the South African online newspaper the \"Daily Maverick\". He has been interviewed on South African television channels M-Net, eTV and eNCA, and has been quoted in various news publications such as \"The Independent\" in the UK and the \"Financial Mail\" in South Africa. He has also written op-eds for local newspapers including the \"Cape Times\", the \"Cape Argus\", \"The Star\", \"The Mercury\" and the \"Mail & Guardian\".\n"}
{"id": "909631", "url": "https://en.wikipedia.org/wiki?curid=909631", "title": "Jet set", "text": "Jet set\n\nIn journalism, jet set was a term for an international social group of wealthy people who travelled the world to participate in social activities unavailable to ordinary people. The term, which replaced \"café society\", came from the lifestyle of travelling from one stylish or exotic place to another via jet plane.\n\nThe term \"jet set\" is attributed to Igor Cassini, a reporter for the \"New York Journal-American\" who wrote under the pen name \"Cholly Knickerbocker\".\n\nAlthough jet passenger service in the 1950s was initially marketed primarily to the rich, its introduction eventually resulted in a substantial democratization of air travel. Hence though the term \"jet set\" is still in some use, its literal meaning of those who travel by jet is no longer relevant. It continues however to refer to those who have the independent wealth and time to travel frequently and widely for pleasure.\n\nBOAC inaugurated the world's first commercial scheduled jet service on 2 May 1952, using the de Havilland Comet, followed by the introduction of the Comet 4 in 1958 after a series of accidents in 1953-1954. The first successful service, from October 1958, was the typical \"jet set\" route, London–New York City. Pan Am followed suit with the Boeing 707, making its first scheduled flight between New York City and Paris on 26 October 1958.\n\nOther cities on the standard jet set routes were Las Vegas, London, Los Angeles, Athens, Madrid, Paris, Rome, Hong Kong and Tokyo. Jet set resorts, invariably with white sand and salt water, were circumspect by modern standards; Acapulco, Nassau with Huntington Hartford's new Paradise Island (opened in 1962) were taking the place of Bermuda. Meanwhile, Cannes, Capri, St. Tropez, Marbella, Portofino, and selected small towns on the French and Italian Riviera were on the jet set itinerary. Greek Islands such as Mykonos were included in the loop around 1974.\n\nThe original members of this elite, free-wheeling set were those \"socialites\" who were not shy about publicity and entertained in semi-public places like restaurants and in night clubs, where the \"paparazzi\" – a jet set phenomenon – photographed them. They were the first generation that might weekend in Paris or fly to Rome just for a party. The jet set was celebrated in popular culture, for example, Federico Fellini captured their lifestyle in \"La Dolce Vita\" (1960), and many record albums of the era promoted flying to foreign lands for honeymoons and getaways, such as Capitol Records \"Honeymoon in Rome\" (1956).\n\nA sign that \"jet set\" had lost its glamorous edge was \"Vogue Magazine\"'s coinage of the term \"the Beautiful People\" in the spring of 1962, an expression that initially referred to the circle that formed around President John F. Kennedy and First Lady Jacqueline Kennedy. Readers of the 15 February 1964 \"Vogue\" could learn \"What the beautiful people are doing to keep fit.\" The two phrases ran for a time in tandem; in 1970, Cleveland Amory could fear \"that the Beautiful People and the Jet Set are being threatened by current economics.\"\n\nA more serious economic threat was the 1973 oil crisis, which cast a pall over the idea of jetting about for pleasure. A sign that \"jet set\" had passed from urbane use was the 1974 country song \"(We're Not) The Jet Set\", in which George Jones and Tammy Wynette claim they are \"the old Chevrolet set,\" as opposed to leading a glamorous, \"jet-setting\" lifestyle. books and movies about or referencing to this class include \"Jacqueline Susann's Once Is Not Enough\" (1973 novel, 1975 film) and the 1986 teen film drama, \"Fire With Fire\", the latter movie consisting of jet set parents who sent their daughter to a convent school and were planning to send her to a Swiss finishing school after graduation from the convent's high school, further depriving her of normal adolescent rites of passage and family contact.\n\nThe flagging \"jet set\" gained its second wind with the introduction in 1976 of the supersonic Concorde. Scheduled flights began on 21 January 1976 on the London-Bahrain oil executive route and the distinctly jet-set Paris-Rio de Janeiro (via Dakar) route. From November 1977 the Concorde was flying between standard \"jet set\" destinations, London or Paris to New York City; passenger lists on initial flights were gossip-column material. The Concorde restored the term's cachet: \"From rock stars to royalty, the Concorde was the way to travel for the jet set,\" according to the Nova retrospective special \"Supersonic Dream\". However, the Concorde was doomed by its sonic boom, inability to achieve global fly-over rights because of the boom, its huge thirst for jet fuel, and a disastrous crash. The aircraft was retired in 2003. Meanwhile, the Boeing 747, densely packed with passengers, was the craft that took over and dominated air travel by reducing travel cost and increasing airport capacity, accelerating the social changes already brought about by the jet age.\n\nWhere English is a second language the term has seen continued usage - in the early 1980s, the Argentinian rock band Soda Stereo recorded a successful song \"¿Por qué no puedo ser del Jet Set?\" (\"Why can't I belong to the jet set?\"), and in 2000 the French comedy \"Jet Set\" made fun of this \"art de vivre\". The TV series Mad Men featured an episode titled \"Jet Set\".\n\n\nNotes\nBibliograophy\n"}
{"id": "285544", "url": "https://en.wikipedia.org/wiki?curid=285544", "title": "John Money", "text": "John Money\n\nJohn William Money (8 July 1921 – 7 July 2006) was a psychologist, sexologist and author, specializing in research into sexual identity and biology of gender. He was one of the first scientists to study the psychology of sexual fluidity and how the societal constructs of \"gender\" affect an individual. Recent academic studies have criticized Money's work in many respects, particularly in regards to his involvement with the sex-reassignment of David Reimer and his eventual suicide. Money's writing has been translated into many languages, and includes around 2,000 articles, books, chapters and reviews. He received around 65 honors, awards, and degrees in his lifetime.\n\nBorn in Morrinsville, New Zealand, to a family of English and Welsh descent, Money initially studied psychology at Victoria University of Wellington, graduating with a double master's degree in psychology and education in 1944. Money was a junior member of the psychology faculty at the University of Otago in Dunedin, but in 1947, at the age of 26, he emigrated to the United States to study at the Psychiatric Institute of the University of Pittsburgh. He left Pittsburgh and earned his PhD from Harvard University in 1952. He was married briefly in the 1950s but had no children.\n\nMoney proposed and developed several theories and related terminology, including gender identity, gender role, gender-identity/role, and lovemap. He coined the term \"paraphilia\" (appearing in the DSM-III) to replace \"perversions\" and introduced the term \"sexual orientation\" in place of \"sexual preference\", arguing that attraction is not necessarily a matter of free choice. \nMoney was a professor of pediatrics and medical psychology at Johns Hopkins University from 1951 until his death. He also established the Johns Hopkins Gender Identity Clinic in 1965 along with Claude Migeon who was the head of pediatric endocrinology at Johns Hopkins. The hospital began performing sexual reassignment surgery in 1966. At Johns Hopkins, Money was also involved with the Sexual Behaviors Unit, which ran studies on sex-reassignment surgery. He received the Magnus Hirschfeld Medal in 2002 from the German Society for Social-Scientific Sexuality Research.\n\nMoney was an early supporter of New Zealand's arts, both literary and visual. He was a noted friend and supporter of author Janet Frame. In 2002, as his Parkinson's disease worsened, Money donated a substantial portion of his art collection to the Eastern Southland Art Gallery in Gore, New Zealand. In 2003, the New Zealand Prime Minister, Helen Clark, opened the John Money wing at the Eastern Southland Gallery.\n\nMoney died 7 July 2006, one day before his 85th birthday, in Towson, Maryland, of complications from Parkinson's disease.\n\nMoney was the co-editor of a 1969 book \"Transsexualism and Sex Reassignment\", which helped bring more acceptance to sexual reassignment surgery and transgender individuals.\n\nMoney introduced numerous definitions related to gender in journal articles in the 1950s, many of them as a result of his studies of Hermaphroditism.\n\nMoney's definition of gender is based on his understanding of sex differences among human beings. According to Money, the fact that one sex produces ova and the other sex produces sperm is the irreducible criterion of sex difference. However, there are other \"sex-derivative differences\" that follow in the wake of this primary dichotomy.\n\nThese differences involve the way urine is expelled from the human body and other questions of sexual dimorphism. According to Money's theory, \"sex-adjunctive differences\" are typified by the smaller size of females and their problems in moving around while nursing infants. This then makes it more likely that the males do the roaming and hunting.\n\n\"Sex-arbitrary differences\" are those that are purely conventional: for example, color selection (baby blue for boys, pink for girls). Some of the latter differences apply to life activities, such as career opportunities for men versus women.\n\nFinally, Money created the now-common term \"gender role\" which he differentiated from the concept of the more traditional terminology \"sex role\". This grew out of his studies of hermaphrodites. According to Money, the genitalia and erotic sexual roles were now, by his definition, to be included under the more general term \"gender role\" including all the non-genital and non-erotic activities that are defined by the conventions of society to apply to males or to females.\n\nIn his studies of hermaphrodites, Money found that there are six variables that define sex. While in the average person all six would line up unequivocally as either all \"male\" or \"female\", in hermaphrodites any one or more than one of these could be inconsistent with the others, leading to various kinds of anomalies. In his seminal 1955 paper he defined these factors as:\n\nand added, \n\nHe then defined gender role as \n\nMoney made the concept of \"gender\" a broader, more inclusive concept than one of masculine/feminine. For him, gender included not only one's status as a man or a woman, but was also a matter of personal recognition, social assignment, or legal determination; not only on the basis of one's genitalia but also on the basis of somatic and behavioral criteria that go beyond genital differences.\n\nIn 1972, Money presented his theories in \"Man & Woman, Boy & Girl\", a college-level, mainstream textbook. The book featured David Reimer (see below) as an example of gender reassignment.\n\nIn this book (Oxford 1988: 116), Money develops a conception of 'bodymind,' as a way for scientists, in developing a science about sexuality, to move on from the platitudes of dichotomy between nature versus nurture, innate versus the acquired, biological versus the social, and psychological versus the physiological. He suggests that all of these capitalize on the ancient, pre-Platonic, pre-biblical conception of body versus the mind, and the physical versus the spiritual. In coining the term \"bodymind\", in this sense, Money wishes to move beyond these very ingrained principles of our folk or vernacular psychology.\n\nMoney also develops here (Oxford 1988: 114–119) a view of \"Concepts of Determinism,\" which, transcultural, transhistorical, and universal, all people have in common, sexologically or otherwise. These include pairbondage, troopbondage, abidance, ycleptance, foredoomance, with these coping strategies: adhibition (engagement), inhibition, explication.\n\nMoney suggests that the concept of threshold (Oxford 1988: 115) – the release or inhibition of sexual (or other) behavior – is most useful for sex research as a substitute for any concept of motivation. Moreover, it confers the distinct advantage of having continuity and unity to what would otherwise be a highly disparate and varied field of research. It also allows for the classification of sexual behavior. For Money, the concept of threshold has great value because of the wide spectrum to which it applies. \"It allows one to think developmentally or longitudinally, in terms of stages or experiences that are programmed serially, or hierarchically, or cybernetically (i.e. regulated by mutual feedback).\" (Oxford 1988: 116)\n\nDuring his professional life, Money was respected as an expert on sexual behavior, especially known for his views that gender was learned rather than innate. However, it was later revealed that his most famous case of David Reimer was fundamentally flawed. In 1966, a botched circumcision left eight-month-old Reimer without a penis. Money persuaded the baby's parents that sex reassignment surgery would be in Reimer's best interest. At the age of 22 months, Reimer underwent an orchidectomy, in which his testicles were surgically removed. He was reassigned to be raised as female and given the name Brenda. Money further recommended hormone treatment to which the parents agreed. Money then recommended a surgical procedure to create an artificial vagina, which the parents refused. Money published a number of papers reporting the reassignment as successful.\n\nFor several years, Money reported on Reimer's progress as the \"John/Joan case\", describing apparently successful female gender development and using this case to support the feasibility of sex reassignment and surgical reconstruction even in non-intersex cases. Notes by a former student at Money's laboratory state that, during the yearly follow-up visits, Reimer's parents routinely lied to staff about the success of the procedure. Reimer's twin brother, Brian, later developed schizophrenia.\n\nDavid Reimer's case came to international attention in 1997 when he told his story to Milton Diamond, an academic sexologist who persuaded Reimer to allow him to report the outcome in order to dissuade physicians from treating other infants similarly. Soon after, Reimer went public with his story, and John Colapinto published a widely disseminated and influential account in \"Rolling Stone\" magazine in December 1997.\n\nOn July 1, 2002, Brian was found dead from an overdose of antidepressants. On May 4, 2004, after suffering years of severe depression, financial instability, and marital troubles, David committed suicide by shooting himself in the head with a sawed-off shotgun at the age of 38. Reimer's parents have stated that Money's methodology was responsible for the deaths of both of their sons.\n\nMoney argued that media response to the exposé was due to right-wing media bias and \"the antifeminist movement\". He said his detractors believed \"masculinity and femininity are built into the genes so women should get back to the mattress and the kitchen\". However, intersex activists also criticized Money, stating that the unreported failure had led to the surgical reassignment of thousands of infants as a matter of policy. Privately, Money was mortified by the case, colleagues said, and as a rule did not discuss it. Money's own views also developed and changed over the years.\n\nJohn Money was critical in debates on chronophilias, especially pedophilia. He stated that both sexual researchers and the public do not make distinctions between affectional pedophilia and sadistic pedophilia. Money asserted that affectional pedophilia was about love and not sex.\n\nMoney held the view that affectional pedophilia is caused by a surplus of parental love that became erotic, and is not a behavioral disorder. Rather, he took the position that heterosexuality is another example of a societal and therefore superficial, ideological concept.\n\n\n"}
{"id": "1805981", "url": "https://en.wikipedia.org/wiki?curid=1805981", "title": "Just in sequence", "text": "Just in sequence\n\nJust in sequence (JIS) is an inventory strategy that matches just in time (JIT) and complete fit in sequence with variation of assembly line production. Components and parts arrive at a production line right in time as scheduled before they get assembled. Feedback from the manufacturing line is used to coordinate transportation to and from the process area. When implemented successfully, JIS improves a company's return on assets (ROA), without loss in flexibility, quality or overall efficiency. JIS is mainly implemented with automobile manufacturing.\n\nJIS is sometimes called in-line vehicle sequencing (ILVS).\n\nJust in Sequence (JIS) is just one specialised strategy to achieve Just In Time (JIT). The process concept of JIT sees buffers at the production line as waste in capital bound. The aim is to eliminate buffers as much as possible at expense of stability when disturbances arise. Just In Sequence is one of the most extreme applications of the concept, where components arrive Just In Time and sequenced for consumption.\n\nThe sequencing allows companies to eliminate supply buffers as soon as the quantity in component part buffers necessary is reduced to a minimum. If not sequencing according to scheduled variety of production, all required components must be stocked in buffers. For flexible production lines, such as a modern automotive assembly line, the variety is an option to produce directly on customer orders. As soon as the next order arrives at the work center, the scheduler distributes the supply orders inline with the production sequence of the final production line.\n\nHowever, with JIS the buffer quantities are displaced upward in material flow to the components suppliers. It is a misinterpretation of JIS to assume that all buffers will be eliminated. Hence just the cost for buffer inventory becomes re-allocated to the producers of the supplies. Sequencing eliminates buffers in the final assembly line by consolidating all similar components into distributed and sequenced buffers, which partly reside on the paths of transportation to final assembly. This strategy thus reduces the line-side inventory buffer. However, the effect is worse when the sequence does not get correctly scheduled upwards or when the transportation line gets congested.\n\nJust In Sequence processes are typically implemented only after the company has achieved a high degree of competency on Just In Time processes. The first step for the organization is to implement JIT processes to synchronize all manufacturing and material departments inside the plant and to collaborate with suppliers, customers, and sub-contractors to reduce inventory buffers to within a few hours. This process typically uncovers deep manufacturing and logistic issues that are not easy to overcome (see JIT Implementation for more details). The manufacturing company can only benefit from sequencing items once these problems have been resolved successfully and components are delivered Just In Time.\n\nSequencing can be implemented in a Just In Time supply operation at many levels, bringing ever-higher inventory reduction and financial benefits:\n\nJust In Sequence implementations introduce a number of new process requirements on top of Just In Time practices. A production sequence or final assembly sequence must be shared upwards to suppliers and sub-contractors. Feedback to customers must be organized according to the scheduled output to earn all positive financial effects. For these and other reasons, the actual production sequence must be \"broadcast\" out to all relevant parties once it is firm. This \"broadcast\" can be done over the phone, paper, email, or other automated IT system. UN/EDIFACT supports an EDI message standard called DELJIT as one standardized way to communicate this information.\n\nOnce the sequence is broadcast, each party must immediately take action to deliver sequenced parts in time. In many cases the turn-around time from broadcast to final assembly is less than 2 hours, with some components required in 30 minutes or less. With this time frame, there is little room for errors. In addition, quality inspection and poka-yoke must be implemented in the sequencing step to guarantee that the sequenced components match the assembly sequence perfectly. In many cases, suppliers must manage periodic sequence reversals, for example, when loading racks into a truck, since the first rack into the truck is the last one to come out. Employees and systems must also properly manage exceptional scenarios, such as re-processing damaged items after initial sequencing, skipping slots for scrapped items, etc. Just In Sequence implementations can only be successful if all of these processes are implemented correctly and all people involved understand what is at stake.\n\nIn many manufacturing operations, the actual production sequence cannot be planned ahead of time with enough certainty to enable sequencing. The main reason is that some manufacturing processes require re-work frequently so that a scheduled sequence becomes irrelevant. For example, painting operations in an automotive plant can have re-work levels of up to 20% (USA, Southern Europe).\n\nStephan M. Wagner and Victor Silveira-Camargos, 2009, \"Decision model for the application of just-in-sequence\", in: Decision Sciences Institute Proceedings of the 40th annual conference, New Orleans, USA.\n"}
{"id": "51388883", "url": "https://en.wikipedia.org/wiki?curid=51388883", "title": "Life spans of home appliances", "text": "Life spans of home appliances\n\nThis page lists the average life spans of home appliances (major and small).\n\n"}
{"id": "31328401", "url": "https://en.wikipedia.org/wiki?curid=31328401", "title": "Mary Agnes Chase", "text": "Mary Agnes Chase\n\nMary Agnes Meara Chase (April 29, 1869 – September 24, 1963) was an American botanist who worked at the U.S. Department of Agriculture and the Smithsonian Institution. She is \"considered one of the world's outstanding agrostologists\" and is known for her work on the study of grasses and for her work as a suffragist.\n\nChase was born in Iroquois County, Illinois and held no formal education beyond grammar school. Chase made significant contributions to the field of botany, authored over 70 scientific publications, and was conferred with an honorary doctorate in science from the University of Illinois. She specialized in the study of grasses and conducted extensive field work in North and South America. Her field books from 1897 to 1959 are archived in the Smithsonian Institution Archives.\n\nIn 1893, Mary had visited the Colombian Exposition in Chicago with her nephew, who was a botanist, and this had inspired her to study plants in Northern Illinois. In 1901, Chase became a botanical assistant at the Field Museum of Natural History under Charles Frederick Millspaugh, where her work was featured in two museum publications: \"Plantae Utowanae\" (1900) and \"Plantae Yucatanae\" (1904). Two years later, Chase joined the U.S. Department of Agriculture (USDA) as a botanical illustrator and eventually became a scientific assistant in systematic agrostology (1907), assistant botanist (1923), and associate botanist (1925), all under Albert Spear Hitchcock. Chase worked with Hitchcock for almost twenty years, collaborating closely and also publishing (\"The North American Species of Panicum\" [1910]).\n\nFollowing Hitchcock's death in 1936, Chase succeeded him to become senior botanist in charge of systematic agrostology and custodian of the Section of Grasses, Division of Plants at the Smithsonian's United States National Museum (USNM). Chase retired from the USDA in 1939 but continued her work as custodian of the USNM grass herbarium until her death in 1963.\n\nChase experienced discrimination based on her gender in the scientific field, for example, being excluded from expeditions to Panama in 1911 and 1912 because the expedition's benefactors feared the presence of women researchers would distract men. During World War I, Chase marched with Alice Paul and was jailed several times for her activities. In 1918, she was arrested at the Silent Sentinels rally picketing the White House; she refused bail and was held for 10 days, where she instigated a hunger-strike and was force-fed. The USDA accused her of \"conduct unbecoming a government employee,\" but Hitchcock helped her keep her job.\n\nChase was also an active member of the National Association for the Advancement of Colored People (NAACP).\n\n\n\n\n\n"}
{"id": "34833199", "url": "https://en.wikipedia.org/wiki?curid=34833199", "title": "Medical neutrality", "text": "Medical neutrality\n\nMedical Neutrality refers to a principle of noninterference with medical services in times of armed conflict and civil unrest: physicians must be allowed to care for the sick and wounded, and soldiers must receive care regardless of their political affiliations; all parties must refrain from attacking and misusing medical facilities, transport, and personnel. Concepts comprising the principles of medical neutrality derive from international human rights law, medical ethics and humanitarian law. Medical neutrality may be thought of as a kind of social contract that obligates societies to protect medical personnel in both times of war and peace, and obligates medical personnel to treat all individuals regardless of religion, race, ethnicity, or political affiliation. Violations of medical neutrality constitute crimes outlined in the Geneva Conventions.\n\nThe principle of medical neutrality has roots in many social traditions. \n\nMedical neutrality is violated when health care professionals, facilities, or patients come under attack, or when medical professionals are not allowed to provide treatment. Examples include:\n\nOn 3 October 2015, U.S. airstrikes killed 42 people and destroyed the MSF (Médecins Sans Frontières - Doctors Without Borders) trauma hospital in Kunduz, Afghanistan (See main article). Many patients in the hospital burned alive in their beds as a US AC-130 gunship made multiple passes firing upon the hospital from overhead. MSF's request for an independent inquiry was never honoured. The U.S. military investigated itself, eventually taking disciplinary action against a dozen servicemembers. No criminal prosecutions followed. \n\nThe Bahraini government’s crackdown on the Bahrain uprising in 2011 and 2012 included extensive violations of medical neutrality. An investigative report released by Physicians for Human Rights revealed that many doctors were attacked or incarcerated. Furthermore, Bahraini security forces have seized control of medical facilities, prevented patients from receiving treatment, misused ambulance services, and violently interrogated wounded patients. In September 2011, 20 medical workers in Bahrain were sentenced to up to 15 years in prison for treating protesters. These sentences were immediately condemned by United Nations secretary general Ban Ki-Moon and human rights groups such as Physicians for Human Rights. Apparently in response to international pressure, the Bahrain government ordered that the doctors be retried in civilian court, but the verdict has yet to be decided.\n\nDuring the Battle of Grozny in 1996 during the First Chechen War, several hospitals were attacked. Municipal Hospital No. 9 was invaded by Russian soldiers and approximately 500 civilians were taken hostage. The ICRC Hospital of Novye Atagi, which was created to symbolize medical neutrality in the war-torn area, was attacked and six members of the ICRC staff were killed.\n\nIn 2011, during political unrest, state security forces directly attacked protestors and field clinics, injuring and killing numerous people. A state security officer even dressed himself as a doctor and administered fatal shots to those injured in a field clinic outside of Tahrir Square. Medical supplies were confiscated by “military officers and field hospital tents were burned down during a Tahrir raid.”\n\nIn the Salvadoran Civil War, many field clinics were attacked by guerillas. Patients were commonly abducted from hospitals, and government forces greatly limited the movements of health workers. Medical transports were also attacked, in some cases resulting in the deaths of medical workers.\n\nIn June 2008, Iranian authorities detained Dr. Arash Alaei and Dr. Kamiar Alaei, two well-known Iranian physicians and leaders in the fight against HIV/AIDS. The physicians, who are brothers, were held in Tehran’s notorious Evin prison for over six months without being charged or tried. On December 31, 2008, a one-day, closed-door trial was held, in which the brothers were tried as conspirators working with an “enemy government” to overthrow the government of Iran. They were also tried at that time on unspecified other charges which neither they nor their lawyer were allowed to know, see the evidence of, or address. They were charged with attempting to overthrow the Iranian government under article 508 of Iran’s Islamic Penal Code. Kamiar was sentenced to three years in prison and Arash to six. The government of Iran used the brothers’ travels to international AIDS conferences as the basis for these claims.\n\nThe international community decried the sentences of the doctors, and Physicians for Human Rights launched a campaign for their release. In 2010, Dr. Kamiar Alaei was freed after serving two years in prison. Dr. Arash Alaei was released in August 2011 after more than three years of detention. Since their release, the doctors have reunited in the United States, where they continue their medical and advocacy work.\n\nDuring the 2011 Libyan Civil War, human rights groups documented violations of medical neutrality along with many other gross violations of human rights. Physicians for Human Rights conducted investigations within Libya in 2011, and found that the military had attacked and destroyed hospitals. Several eyewitnesses reported that Gaddafi forces attacked ambulances carrying injured combatants, despite the fact that the ambulances were marked with the emblematic Red Crescent. Medical personnel were kidnapped by Gaddafi’s forces, and military forces used people as human shields.\n\nDuring the Mozambican Civil War, the resistance group RENAMO was responsible for many violations of medical neutrality. Attacks on hospitals and health clinics were common. In one instance, RENAMO soldiers raided the town of Homoine, killing 442 civilians including hospitalized patients.\n\nCivil unrest and demonstrations began in Panama in June 1987. During the unrest, human rights groups such as Physicians for Human Rights documented a variety of human rights abuses and violations of medical neutrality. The military blocked access to hospitals and interfered with provision of medical care, took control of ambulance services for military purposes, and interrogated wounded patients. In addition, Panamanian physicians were kidnapped, beaten, and tortured for speaking out against government policies which prevented them from providing their patients with adequate care.\n\nSri Lanka’s lengthy civil war was marked by extensive human rights abuses. In 2009, the Sri Lankan air force violated the principle of medical neutrality when it destroyed the Ponnampalam Memorial Hospital in Puthukkudiyiruppu.\n\nThe Syrian civil war has been marked by widespread human rights abuses, including numerous violations of medical neutrality. Government forces have invaded, attacked, and misused hospitals and medical transports, preventing civilians from receiving health care. An estimated 250 doctors have been detained and tortured for treating wounded civilians. An investigation by Physicians for Human Rights revealed that these circumstances have led to the rise of an underground health network.\n\nA year after a bloodless military coup in Bangkok in February 1991, the new government responded to the pro-democracy movement opening fire on a May opposition rally, resulting in 52 deaths, hundreds of injured, and many disappearances. Physicians for Human Rights reported that health professionals were prevented from reaching the wounded and the police shot at ambulances.\n\n\nThe Medical Neutrality Protection Act of 2011, (H.R. 2643), is a bipartisan bill introduced by Representatives Jim McDermott (D-WA), and Walter B. Jones, Jr. (R-NC) that intends to make the protection of medical professionals and access to medical services a global policy priority for the US government.\n"}
{"id": "1098651", "url": "https://en.wikipedia.org/wiki?curid=1098651", "title": "Medium of instruction", "text": "Medium of instruction\n\nA medium of instruction (plural: usually mediums of instruction, but the archaic media of instruction is still used by some) is a language used in teaching. It may or may not be the official language of the country or territory. If the first language of students is different from the official language, it may be used as the medium of instruction for part or all of schooling. Bilingual or multilingual education may involve the use of more than one language of instruction. UNESCO considers that \"providing education in a child's mother tongue is indeed a critical issue\".\n\n\nEvery public school uses Brazilian Portuguese as the medium of instruction, but no law prohibits the use of other languages in private schools. Many schools use other European languages (mainly because of the country's European heritage) such as English, German, Italian or French. Public schools also have mandatory English and Spanish but only once or twice a week.\n\n\nEnglish is used, but in some schools, Spanish, French (in Louisiana), Hawaiian (in Hawaii), and local Native American/American Indian languages are used as well.\n\n\n\n\n\nPrior to the 2017 law \"On Education\" the mediums of instruction in pre-school education were Ukrainian, Russian, Hungarian, Romanian, Moldovan, Crimean Tatar, English, Polish and German; in general education, Ukrainian, Russian, Hungarian, Romanian, Moldovan, Crimean Tatar, Polish, Bulgarian and Slovak; in vocational training, Ukrainian and Russian; in higher education, Ukrainian, Russian, Hungarian, Romanian\n\n\n"}
{"id": "47078194", "url": "https://en.wikipedia.org/wiki?curid=47078194", "title": "Meninism", "text": "Meninism\n\nMeninism is term used which has been used to describe various groups, including the men's rights movement, and male feminists. The term is sometimes used sincerely to challenge social issues facing men, and sometimes satirically or semi-satirically as a word-play on feminism. Members of these groups are known as meninists.\n\nThe term meninism was used in the early 2000s to describe male feminists who opposed sexism and supported women's right for equality in society, politics and at work. By the next decade, the term was used on social media to make jokes which mocked and criticised radical feminism. In 2013 the BBC reported that the hashtag #MeninistTwitter was being used on Twitter, first to share jokes about feminism, but later to share more serious difficulties facing modern men. In 2015, Nolan Feeney of \"Time\" reported that those who used meninist hashtags \"generally fall in two camps: people who use the term to call out ways they believe they’ve been victimised by feminism, and people who make fun of the first group for not understanding what feminism means in the first place\". Antifeminism is also associated with terms like Meninism\n\nThe term has partially evolved into a movement promoting awareness of the issues which the movement perceives as affecting men, opposition to the perceived oppression men face in the 21st century, opposition to the way some perceive that men are victimised by modern-day feminism, and occasionally violence against men. Women also identify as meninists.\n\nAccording to Martin Daubney of \"The Telegraph\", some meninists have used the term to discuss serious issues affecting men, such as domestic violence against men; fathers' rights and divorce issues; and disproportionate male prison sentences, suicide rates, and rates of homelessness. According to Radhika Sanghani of \"The Telegraph\", the hashtag is most commonly used on Twitter to mock feminism, but has also been used as a way to draw attention to men's issues, similar to \"The Red Pill\" forum on Reddit. Sanghani says that the movement's reaction to feminism is based more on the label than feminism's views. Abigail James writing for \"Catholic Online\" said that while meninism raises legitimate issues which should be taken seriously, its heart is based on a misinterpretation of the meaning of feminism. Antifeminism is also associated with the Meninist movement.\n\nMintified, an India-based media website, started the #BlameOneNotAll hashtag to discourage generalisations of men. According to Victoria Richards of \"The Independent\", the campaign was part of the meninist movement. The campaign received backlash for shifting focus away from the victims of rape and implying that basic decency should be rewarded. In an opinion piece for \"The Economic Times\" Shephali Bhatt criticised the use of the movement and International Men's Day to sell deodorant, saying \"...for an initiative to become a social phenomena, it needs to be rooted in truth. And the truth is that [men are] a sufficiently privileged gender, comparatively.\"\n\nThe hashtags have also been used for T-shirts and similar, with self-portraits of people wearing the clothes widely shared on social media. Several outlets reported that the clothes and images were widely mocked, and often Photoshopped sarcastically.\n"}
{"id": "43254", "url": "https://en.wikipedia.org/wiki?curid=43254", "title": "Morality", "text": "Morality\n\nMorality (from ) is the differentiation of intentions, decisions and actions between those that are distinguished as proper and those that are improper. Morality can be a body of standards or principles derived from a code of conduct from a particular philosophy, religion or culture, or it can derive from a standard that a person believes should be universal. Morality may also be specifically synonymous with \"goodness\" or \"rightness\".\n\nMoral philosophy includes moral ontology, which is the origin of morals; and moral epistemology, which studies the knowledge of morals. Different systems of expressing morality have been proposed, including deontological ethical systems which adhere to a set of established rules, and normative ethical systems which consider the merits of actions themselves. An example of normative ethical philosophy is the Golden Rule, which states that: \"One should treat others as one would like others to treat oneself.\"\n\nImmorality is the active opposition to morality (i.e. opposition to that which is good or right), while amorality is variously defined as an unawareness of, indifference toward, or disbelief in any particular set of moral standards or principles.\n\nEthics (also known as moral philosophy) is the branch of philosophy which addresses questions of morality. The word \"ethics\" is \"commonly used interchangeably with 'morality,' and sometimes it is used more narrowly to mean the moral principles of a particular tradition, group, or individual.\" Likewise, certain types of ethical theories, especially deontological ethics, sometimes distinguish between ethics and morals: \"Although the morality of people and their ethics amounts to the same thing, there is a usage that restricts morality to systems such as that of Immanuel Kant, based on notions such as duty, obligation, and principles of conduct, reserving ethics for the more Aristotelian approach to practical reasoning, based on the notion of a virtue, and generally avoiding the separation of 'moral' considerations from other practical considerations.\"\n\nIn its descriptive sense, \"morality\" refers to personal or cultural values, codes of conduct or social mores from a society that provides these codes of conduct in which it applies and is accepted by an individual. It does not connote objective claims of right or wrong, but only refers to that which is considered right or wrong. Descriptive ethics is the branch of philosophy which studies morality in this sense.\n\nIn its normative sense, \"morality\" refers to whatever (if anything) is \"actually\" right or wrong, which may be independent of the values or mores held by any particular peoples or cultures. Normative ethics is the branch of philosophy which studies morality in this sense.\n\nPhilosophical theories on the nature and origins of morality (that is, theories of meta-ethics) are broadly divided into two classes:\n\nSome forms of non-cognitivism and ethical subjectivism, while considered anti-realist in the robust sense used here, are considered realist in the sense synonymous with moral universalism. For example, universal prescriptivism is a universalist form of non-cognitivism which claims that morality is derived from reasoning about implied imperatives, and divine command theory and ideal observer theory are universalist forms of ethical subjectivism which claim that morality is derived from the edicts of a god or the hypothetical decrees of a perfectly rational being, respectively.\n\nCelia Green made a distinction between tribal and territorial morality. She characterizes the latter as predominantly negative and proscriptive: it defines a person's territory, including his or her property and dependents, which is not to be damaged or interfered with. Apart from these proscriptions, territorial morality is permissive, allowing the individual whatever behaviour does not interfere with the territory of another. By contrast, tribal morality is prescriptive, imposing the norms of the collective on the individual. These norms will be arbitrary, culturally dependent and 'flexible', whereas territorial morality aims at rules which are universal and absolute, such as Kant's 'categorical imperative' and Geisler's graded absolutism. Green relates the development of territorial morality to the rise of the concept of private property, and the ascendancy of contract over status.\n\nSome observers hold that individuals apply distinct sets of moral rules to people depending on their membership of an \"in-group\" (the individual and those they believe to be of the same group) or an \"out-group\" (people not entitled to be treated according to the same rules). Some biologists, anthropologists and evolutionary psychologists believe this in-group/out-group discrimination has evolved because it enhances group survival. This belief has been confirmed by simple computational models of evolution. In simulations this discrimination can result in both unexpected cooperation towards the in-group and irrational hostility towards the out-group. Gary R. Johnson and V.S. Falger have argued that nationalism and patriotism are forms of this in-group/out-group boundary. Jonathan Haidt has noted that experimental observation indicating an in-group criterion provides one moral foundation substantially used by conservatives, but far less so by liberals.\n\nPeterson and Seligman approach the anthropological view looking across cultures, geo-cultural areas and across millennia. They conclude that certain virtues have prevailed in all cultures they examined. The major virtues they identified include \"wisdom / knowledge; courage; humanity; justice; temperance; and transcendence\". Each of these includes several divisions. For instance \"humanity\" includes \"love\", \"kindness\", and \"social intelligence\".\n\nFons Trompenaars, author of \"Did the Pedestrian Die?\", tested members of different cultures with various moral dilemmas. One of these was whether the driver of a car would have his friend, a passenger riding in the car, lie in order to protect the driver from the consequences of driving too fast and hitting a pedestrian. Trompenaars found that different cultures had quite different expectations, from none to definite.\n\nJohn Newton, author of \"Complete Conduct Principles for the 21st Century\" compared the Eastern and the Western cultures about morality. As stated in \"Complete Conduct Principles for the 21st Century\", \"One of the important objectives of this book is to blend harmoniously the fine souls regarding conduct in the Eastern and the Western cultures, to take the result as the source and then to create newer and better conduct principles to suit the human society of the new century, and to introduce a lot of Chinese fine conduct spirits to the Western world. It is hoped that this helps solve lots of problems the human society of the 21st century faces, including (but not limited to the Eastern and the Western cultures) what a single culture cannot.\"\n\nThe development of modern morality is a process closely tied to sociocultural evolution. Some evolutionary biologists, particularly sociobiologists, believe that morality is a product of evolutionary forces acting at an individual level and also at the group level through group selection (although to what degree this actually occurs is a controversial topic in evolutionary theory). Some sociobiologists contend that the set of behaviors that constitute morality evolved largely because they provided possible survival or reproductive benefits (i.e. increased evolutionary success). Humans consequently evolved \"pro-social\" emotions, such as feelings of empathy or guilt, in response to these moral behaviors.\n\nOn this understanding, moralities are sets of self-perpetuating and biologically-driven behaviors which encourage human cooperation. Biologists contend that all social animals, from ants to elephants, have modified their behaviors, by restraining immediate selfishness in order to improve their evolutionary fitness. Human morality, although sophisticated and complex relative to the moralities of other animals, is essentially a natural phenomenon that evolved to restrict excessive individualism that could undermine a group's cohesion and thereby reducing the individuals' fitness.\n\nOn this view, moral codes are ultimately founded on emotional instincts and intuitions that were selected for in the past because they aided survival and reproduction (inclusive fitness). Examples: the maternal bond is selected for because it improves the survival of offspring; the Westermarck effect, where close proximity during early years reduces mutual sexual attraction, underpins taboos against incest because it decreases the likelihood of genetically risky behaviour such as inbreeding.\n\nThe phenomenon of reciprocity in nature is seen by evolutionary biologists as one way to begin to understand human morality. Its function is typically to ensure a reliable supply of essential resources, especially for animals living in a habitat where food quantity or quality fluctuates unpredictably. For example, some vampire bats fail to feed on prey some nights while others manage to consume a surplus. Bats that did eat will then regurgitate part of their blood meal to save a conspecific from starvation. Since these animals live in close-knit groups over many years, an individual can count on other group members to return the favor on nights when it goes hungry (Wilkinson, 1984)\n\nMarc Bekoff and Jessica Pierce (2009) have argued that morality is a suite of behavioral capacities likely shared by all mammals living in complex social groups (e.g., wolves, coyotes, elephants, dolphins, rats, chimpanzees). They define morality as \"a suite of interrelated other-regarding behaviors that cultivate and regulate complex interactions within social groups.\" This suite of behaviors includes empathy, reciprocity, altruism, cooperation, and a sense of fairness. In related work, it has been convincingly demonstrated that chimpanzees show empathy for each other in a wide variety of contexts. They also possess the ability to engage in deception, and a level of social politics prototypical of our own tendencies for gossip and reputation management.\nChristopher Boehm (1982) has hypothesized that the incremental development of moral complexity throughout hominid evolution was due to the increasing need to avoid disputes and injuries in moving to open savanna and developing stone weapons. Other theories are that increasing complexity was simply a correlate of increasing group size and brain size, and in particular the development of theory of mind abilities.\n\nMoral cognition refers to cognitive processes that allow a person to act or decide in morally permissible ways. It consists of several domain-general cognitive processes, ranging from perception of a morally-salient stimuli to reasoning when faced with a moral dilemma. While it’s important to mention that there is not a single cognitive faculty dedicated exclusively to moral cognition, characterizing the contributions of domain-general processes to moral behavior is a critical scientific endeavor to understand how morality works and how it can be improved.\n\nCognitive psychologists and neuroscientists investigate the inputs to these cognitive processes and their interactions, as well as how these contribute to moral behavior by running controlled experiments. In these experiments putatively moral versus nonmoral stimuli are compared to each other, while controlling for other variables such as content or working memory load. Often, the differential neural response to specifically moral statements or scenes, are examined using functional neuroimaging experiments.\n\nCritically, the specific cognitive processes that are involved depend on the prototypical situation that a person encounters. For instance, while situations that require an active decision on a moral dilemma may require active reasoning, an immediate reaction to a shocking moral violation may involve quick, affect-laden processes. Nonetheless certain cognitive skills such as being able to attribute mental states—beliefs, intents, desires, emotions to oneself, and to others is a common feature of a broad range of prototypical situations. In line with this, a meta-analysis found overlapping activity between moral emotion and moral reasoning tasks, suggesting a shared neural network for both tasks. The results of this meta-analaysis, however, also demonstrated that the processing of moral input is affected by task demands.\n\nThe brain areas that are consistently involved when humans reason about moral issues have been investigated by a quantitative large-scale meta-analysis of the brain activity changes reported in the moral neuroscience literature., In fact, the neural network underlying moral decisions overlapped with the network pertaining to representing others' intentions (i.e., theory of mind) and the network pertaining to representing others' (vicariously experienced) emotional states (i.e., empathy). This supports the notion that moral reasoning is related to both seeing things from other persons' points of view and to grasping others' feelings. These results provide evidence that the neural network underlying moral decisions is probably domain-global (i.e., there might be no such things as a \"moral module\" in the human brain) and might be dissociable into cognitive and affective sub-systems.\n\nAn essential, shared component of moral judgment involves the capacity to detect morally salient content within a given social context. Recent research implicated the salience network in this initial detection of moral content. The salience network responds to behaviorally salient events and may be critical to modulate downstream default and frontal control network interactions in the service of complex moral reasoning and decision-making processes.\n\nThe explicit making of moral right and wrong judgments coincides with activation in the ventromedial prefrontal cortex (VMPC) while intuitive reactions to situations containing implicit moral issues activates the temporoparietal junction area.\n\nStimulation of the VMPC by transcranial magnetic stimulation, has been shown to inhibit the ability of human subjects to take into account intent when forming a moral judgment. According to this investigation, TMS did not disrupt participants' ability to make any moral\njudgment. On the contrary, moral judgments of intentional harms and non-harms were unaffected by TMS to either the RTPJ or the control site; presumably, however, people typically make moral judgments of intentional harms by considering not only the action's harmful outcome but the agent's intentions and beliefs. So why\nwere moral judgments of intentional harms not affected by TMS to the RTPJ? One possibility is that moral judgments typically reflect a weighted function of any morally relevant information that is available at the time. On the basis of this view, when information concerning the agent's belief is unavailable or degraded, the resulting moral judgment simply reflects a higher weighting of other\nmorally relevant factors (e.g., outcome). Alternatively, following TMS to the RTPJ, moral judgments might be made via an abnormal processing route that does not take belief into account. On either account, when belief information is degraded or unavailable, moral judgments are shifted toward other morally relevant factors (e.g., outcome). For intentional harms and non-harms, however, the outcome suggests the same moral judgment as the intention. Thus, the researchers suggest that TMS to the RTPJ disrupted the processing of negative beliefs for both intentional harms and attempted harms, but the current design allowed the investigators to detect this effect only in the case of attempted harms, in which the neutral outcomes did not afford harsh moral judgments on their own.\n\nSimilarly VMPC-impaired persons will judge an action purely on its outcome and are unable to take into account the intent of that action.\n\nMirror neurons are neurons in the brain that fire when another person is observed doing a certain action. The neurons fire in imitation of the action being observed, causing the same muscles to act minutely in the observer as are acting grossly in the person actually performing the action. Research on mirror neurons, since their discovery in 1996, suggests that they may have a role to play not only in action understanding, but also in emotion sharing empathy. Cognitive neuro-scientist Jean Decety thinks that the ability to recognize and vicariously experience what another individual is undergoing was a key step forward in the evolution of social behavior, and ultimately, morality. The inability to feel empathy is one of the defining characteristics of psychopathy, and this would appear to lend support to Decety's view.\n\nIn modern moral psychology, morality is considered to change through personal development. A number of psychologists have produced theories on the development of morals, usually going through stages of different morals. Lawrence Kohlberg, Jean Piaget, and Elliot Turiel have cognitive-developmental approaches to moral development; to these theorists morality forms in a series of constructive stages or domains. In the Ethics of care approach established by Carol Gilligan, moral development occurs in the context of caring, mutually responsive relationships which are based on interdependence, particularly in parenting but also in social relationships generally. Social psychologists such as Martin Hoffman and Jonathan Haidt emphasize social and emotional development based on biology, such as empathy. Moral identity theorists, such as William Damon and Mordechai Nisan, see moral commitment as arising from the development of a self-identity that is defined by moral purposes: this moral self-identity leads to a sense of responsibility to pursue such purposes. Of historical interest in psychology are the theories of psychoanalysts such as Sigmund Freud, who believe that moral development is the product of aspects of the super-ego as guilt-shame avoidance.\n\nBecause we are naturally prone to be empathic and moral, we have a sense of responsibility to pursue moral purposes, we still, at least occasionally, engage in immoral behavior. Such behaviors jeopardize our moral self-image; however, when we engage in immoral behaviors we still feel as though we are moral individuals. \"Moral self-licensing\" attempts to explain this phenomenon and proposes that self-image security increases our likelihood to engage in immoral behavior. When our moral self-image is threatened, we can gain confidence from our past moral behavior. The more confident we are, the less we will worry about our future behavior which actually increases the likelihood that we will engage in immoral behaviors. Monin and Miller (2001) examined the \"moral self-licensing\" effect and found that when participants established credentials as non-prejudiced persons, they were more willing to express politically incorrect opinions despite the fact that the audience was unaware of their credentials.\n\nAs an alternative to viewing morality as an individual trait, some sociologists as well as social- and discursive psychologists have taken upon themselves to study the \"in-vivo\" aspects of morality by examining how persons conduct themselves in social interaction.\n\nIf morality is the answer to the question 'how ought we to live' at the individual level, politics can be seen as addressing the same question at the social level, though the political sphere raises additional problems and challenges. It is therefore unsurprising that evidence has been found of a relationship between attitudes in morality and politics. Jonathan Haidt and Jesse Graham have studied the differences between liberals and conservatives, in this regard. Haidt found that Americans who identified as liberals tended to value care and fairness higher than loyalty, respect and purity. Self-identified conservative Americans valued care and fairness less and the remaining three values more. Both groups gave care the highest over-all weighting, but conservatives valued fairness the lowest, whereas liberals valued purity the lowest. Haidt also hypothesizes that the origin of this division in the United States can be traced to geo-historical factors, with conservatism strongest in closely knit, ethnically homogenous communities, in contrast to port-cities, where the cultural mix is greater, thus requiring more liberalism.\n\nGroup morality develops from shared concepts and beliefs and is often codified to regulate behavior within a culture or community. Various defined actions come to be called moral or immoral. Individuals who choose moral action are popularly held to possess \"moral fiber\", whereas those who indulge in immoral behavior may be labeled as socially degenerate. The continued existence of a group may depend on widespread conformity to codes of morality; an inability to adjust moral codes in response to new challenges is sometimes credited with the demise of a community (a positive example would be the function of Cistercian reform in reviving monasticism; a negative example would be the role of the Dowager Empress in the subjugation of China to European interests). Within nationalist movements, there has been some tendency to feel that a nation will not survive or prosper without acknowledging one common morality, regardless of its content.\nPolitical Morality is also relevant to the behavior internationally of national governments, and to the support they receive from their host population. Noam Chomsky states that\n\n... if we adopt the principle of universality : if an action is right (or wrong) for others, it is right (or wrong) for us. Those who do not rise to the minimal moral level of applying to themselves the standards they apply to others—more stringent ones, in fact—plainly cannot be taken seriously when they speak of appropriateness of response; or of right and wrong, good and evil.\n\nIn fact, one of the, maybe the most, elementary of moral principles is that of universality, that is, If something's right for me, it's right for you; if it's wrong for you, it's wrong for me. Any moral code that is even worth looking at has that at its core somehow.\n\nReligion and morality are not synonymous. Morality does not depend upon religion although for some this is \"an almost automatic assumption\". According to \"The Westminster Dictionary of Christian Ethics\", religion and morality \"are to be defined differently and have no definitional connections with each other. Conceptually and in principle, morality and a religious value system are two distinct kinds of value systems or action guides.\"\n\nWithin the wide range of moral traditions, religious value systems co-exist with contemporary secular frameworks such as consequentialism, freethought, humanism, utilitarianism, and others. There are many types of religious value systems. Modern monotheistic religions, such as Islam, Judaism, Christianity, and to a certain degree others such as Sikhism and Zoroastrianism, define right and wrong by the laws and rules set forth by their respective scriptures and as interpreted by religious leaders within the respective faith. Other religions spanning pantheistic to nontheistic tend to be less absolute. For example, within Buddhism, the intention of the individual and the circumstances should be accounted for to determine if an action is right or wrong. A further disparity between the values of religious traditions is pointed out by Barbara Stoler Miller, who states that, in Hinduism, \"practically, right and wrong are decided according to the categories of social rank, kinship, and stages of life. For modern Westerners, who have been raised on ideals of universality and egalitarianism, this relativity of values and obligations is the aspect of Hinduism most difficult to understand\".\n\nReligions provide different ways of dealing with moral dilemmas. For example, there is no absolute prohibition on killing in Hinduism, which recognizes that it \"may be inevitable and indeed necessary\" in certain circumstances. In monotheistic traditions, certain acts are viewed in more absolute terms, such as abortion or divorce. Religion is not always positively associated with morality. Philosopher David Hume stated that, \"the greatest crimes have been found, in many instances, to be compatible with a superstitious piety and devotion; Hence it is justly regarded as unsafe to draw any inference in favor of a man's morals, from the fervor or strictness of his religious exercises, even though he himself believe them sincere.\"\n\nReligious value systems can diverge from commonly-held contemporary moral positions, such as those on murder, mass atrocities, and slavery. For example, Simon Blackburn states that \"apologists for Hinduism defend or explain away its involvement with the caste system, and apologists for Islam defend or explain away its harsh penal code or its attitude to women and infidels\". In regard to Christianity, he states that the \"Bible can be read as giving us a carte blanche for harsh attitudes to children, the mentally handicapped, animals, the environment, the divorced, unbelievers, people with various sexual habits, and elderly women\", and notes morally suspect themes in the Bible's New Testament as well. Christian apologists address Blackburn's viewpoints and construe that Jewish laws in the Jewish Bible showed the evolution of moral standards towards protecting the vulnerable, imposing a death penalty on those pursuing slavery and treating slaves as persons and not property. Elizabeth Anderson holds that \"the Bible contains both good and evil teachings\", and it is \"morally inconsistent\". Humanists like Paul Kurtz believe that we can identify moral values across cultures, even if we do not appeal to a supernatural or universalist understanding of principles - values including integrity, trustworthiness, benevolence, and fairness. These values can be resources for finding common ground between believers and nonbelievers.\n\nA number of studies have been conducted on the empirics of morality in various countries, and the overall relationship between faith and crime is unclear. A 2001 review of studies on this topic found \"The existing evidence surrounding the effect of religion on crime is varied, contested, and inconclusive, and currently no persuasive answer exists as to the empirical relationship between religion and crime.\" Phil Zuckerman's 2008 book, \"Society without God\", based on studies conducted during a 14-month period in Scandinavia in 2005-2006, notes that Denmark and Sweden, \"which are probably the least religious countries in the world, and possibly in the history of the world\", enjoy \"among the lowest violent crime rates in the world [and] the lowest levels of corruption in the world\".\n\nDozens of studies have been conducted on this topic since the twentieth century. A 2005 study by Gregory S. Paul published in the \"Journal of Religion and Society\" stated that, \"In general, higher rates of belief in and worship of a creator correlate with higher rates of homicide, juvenile and early adult mortality, STD infection rates, teen pregnancy, and abortion in the prosperous democracies,\" and \"In all secular developing democracies a centuries long-term trend has seen homicide rates drop to historical lows\" with the exceptions being the United States (with a high religiosity level) and \"theistic\" Portugal. In a response, Gary Jensen builds on and refines Paul's study. His conclusion is that a \"complex relationship\" exists between religiosity and homicide \"with some dimensions of religiosity encouraging homicide and other dimensions discouraging it\". On April 26, 2012, the results of a study which tested their subjects' pro-social sentiments were published in the \"Social Psychological and Personality Science\" journal in which non-religious people had higher scores showing that they were more inclined to show generosity in random acts of kindness, such as lending their possessions and offering a seat on a crowded bus or train. Religious people also had lower scores when it came to seeing how much compassion motivated participants to be charitable in other ways, such as in giving money or food to a homeless person and to non-believers.\n\n\n"}
{"id": "1534578", "url": "https://en.wikipedia.org/wiki?curid=1534578", "title": "Motion perception", "text": "Motion perception\n\nMotion perception is the process of inferring the speed and direction of elements in a scene based on visual, vestibular and proprioceptive inputs. Although this process appears straightforward to most observers, it has proven to be a difficult problem from a computational perspective, and extraordinarily difficult to explain in terms of neural processing.\n\nMotion perception is studied by many disciplines, including psychology (i.e. visual perception), neurology, neurophysiology, engineering, and computer science.\n\nThe inability to perceive motion is called akinetopsia and it may be caused by a lesion to cortical area V5 in the extrastriate cortex. Neuropsychological studies of a patient who could not see motion, seeing the world in a series of static \"frames\" instead, suggested that visual area V5 in humans is homologous to motion processing area MT in primates.\n\nTwo or more stimuli that are switched on and off in alternation can produce two different motion percepts. The first, demonstrated in the figure to the right is \"Beta movement\", often used in billboard displays, in which an object is perceived as moving when, in fact, a series of stationary images is being presented. This is also termed \"apparent motion\" and is the basis of movies and television. However, at faster alternation rates, and if the distance between the stimuli is just right, an illusory \"object\" the same colour as the background is seen moving between the two stimuli and alternately occluding them. This is called the phi phenomenon and is sometimes described as an example of \"pure\" motion detection uncontaminated, as in Beta movement, by form cues. This description is, however, somewhat paradoxical as it is not possible to create such motion in the absence of figural percepts. \n\nThe phi phenomenon has been referred to as \"first-order\" motion perception. Werner E. Reichardt and Bernard Hassenstein have modelled it in terms of relatively simple \"motion sensors\" in the visual system, that have evolved to detect a change in luminance at one point on the retina and correlate it with a change in luminance at a neighbouring point on the retina after a short delay. Sensors that are proposed to work this way have been referred to as either \"Hassenstein-Reichardt detectors\" after the scientists Bernhard Hassenstein and Werner Reichardt, who first modelled them, motion-energy sensors, or Elaborated Reichardt Detectors. These sensors are described as detecting motion by spatio-temporal correlation and are considered by some to be plausible models for how the visual system may detect motion. (Although, again, the notion of a \"pure motion\" detector suffers from the problem that there is no \"pure motion\" stimulus, i.e. a stimulus lacking perceived figure/ground properties). There is still considerable debate regarding the accuracy of the model and exact nature of this proposed process. It is not clear how the model distinguishes between movements of the eyes and movements of objects in the visual field, both of which produce changes in luminance on points on the retina.\n\n\"Second-order\" motion has been defined as motion in which the moving contour is defined by contrast, texture, flicker or some other quality that does not result in an increase in luminance or motion energy in the Fourier spectrum of the stimulus. There is much evidence to suggest that early processing of first- and second-order motion is carried out by separate pathways. Second-order mechanisms have poorer temporal resolution and are low-pass in terms of the range of spatial frequencies to which they respond. (The notion that neural responses are attuned to frequency components of stimulation suffers from the lack of a functional rationale and has been generally criticized by G. Westheimer (2001) in an article called \"The Fourier Theory of Vision.\") Second-order motion produces a weaker motion aftereffect unless tested with dynamically flickering stimuli.\n\nThe motion direction of a contour is ambiguous, because the motion component parallel to the line cannot be inferred based on the visual input. This means that a variety of contours of different orientations moving at different speeds can cause identical responses in a motion sensitive neuron in the visual system.\n\nSee MIT example\n\nSome have speculated that, having extracted the hypothesized motion signals (first- or second-order) from the retinal image, the visual system must integrate those individual \"local\" motion signals at various parts of the visual field into a 2-dimensional or \"global\" representation of moving objects and surfaces. (It is not clear how this 2D representation is then converted into the perceived 3D percept) Further processing is required to detect coherent motion or \"global motion\" present in a scene.\n\nThe ability of a subject to detect coherent motion is commonly tested using motion coherence discrimination tasks. For these tasks, dynamic random-dot patterns (also called \"random dot kinematograms\") are used that consist in 'signal' dots moving in one direction and 'noise' dots moving in random directions. The sensitivity to motion coherence is assessed by measuring the ratio of 'signal' to 'noise' dots required to determine the coherent motion direction. The required ratio is called the \"motion coherence threshold\".\n\nAs in other aspects of vision, the observer's visual input is generally insufficient to determine the true nature of stimulus sources, in this case their velocity in the real world. In monocular vision for example, the visual input will be a 2D projection of a 3D scene. The motion cues present in the 2D projection will by default be insufficient to reconstruct the motion present in the 3D scene. Put differently, many 3D scenes will be compatible with a single 2D projection. The problem of motion estimation generalizes to binocular vision when we consider occlusion or motion perception at relatively large distances, where binocular disparity is a poor cue to depth. This fundamental difficulty is referred to as the inverse problem.\n\nNonetheless, some humans do perceive motion in depth. There are indications that the brain uses various cues, in particular temporal changes in disparity as well as monocular velocity ratios, for producing a sensation of motion in depth.\n\nDetection and discrimination of motion can be improved by training with long-term results. Participants trained to detect the movements of dots on a screen in only one direction become particularly good at detecting small movements in the directions around that in which they have been trained. This improvement was still present 10 weeks later. However perceptual learning is highly specific. For example, the participants show no improvement when tested around other motion directions, or for other sorts of stimuli.\n\nA cognitive map is a type of mental representation which serves an individual to acquire, code, store, recall, and decode information about the relative locations and attributes of phenomena in their spatial environment.\n\nMany species of mammals can keep track of spatial location even in the absence of visual, auditory, olfactory, or tactile cues, by integrating their movements—the ability to do this is referred to in the literature as path integration. A number of theoretical models have explored mechanisms by which path integration could be performed by neural networks. In most models, such as those of Samsonovich and McNaughton (1997) or Burak and Fiete (2009), the principal ingredients are (1) an internal representation of position, (2) internal representations of the speed and direction of movement, and (3) a mechanism for shifting the encoded position by the right amount when the animal moves. Because cells in the Medial Entorhinal Cortex (MEC) encode information about position (grid cells) and movement (head direction cells and conjunctive position-by-direction cells), this area is currently viewed as the most promising candidate for the place in the brain where path integration occurs.\n\nMotion sensing using vision is crucial for detecting a potential mate, prey, or predator, and thus it is found both in vertebrates and invertebrates vision throughout a wide variety of species, although it is not universally found in all species. In vertebrates, the process takes place in retina and more specifically in retinal ganglion cells, which are neurons that receive input from bipolar cells and amacrine cells on visual information and process output to higher regions of the brain including, thalamus, hypothalamus, and mesencephalon.\n\nThe study of directionally selective units began with a discovery of such cells in the cerebral cortex of cats by David Hubel and Torsten Wiesel in 1959. Following the initial report, an attempt to understand the mechanism of directionally selective cells was pursued by Horace B. Barlow and William R. Levick in 1965. Their in-depth experiments in rabbit's retina expanded the anatomical and physiological understanding of the vertebrate visual system and ignited the interest in the field. Numerous studies that followed thereafter have unveiled the mechanism of motion sensing in vision for the most part. Alexander Borst and Thomas Euler's 2011 review paper, \"Seeing Things in Motion: Models, Circuits and Mechanisms\". discusses certain important findings from the early discoveries to the recent work on the subject, coming to the conclusion of the current status of the knowledge.\n\nDirection selective (DS) cells in the retina are defined as neurons that respond differentially to the direction of a visual stimulus. According to Barlow and Levick (1965), the term is used to describe a group of neurons that \"gives a vigorous discharge of impulses when a stimulus object is moved through its receptive field in one direction.\" This direction in which a set of neurons respond most strongly to is their \"preferred direction\". In contrast, they do not respond at all to the opposite direction, \"null direction\". The preferred direction is not dependent on the stimulus—that is, regardless of the stimulus' size, shape, or color, the neurons respond when it is moving in their preferred direction, and do not respond if it is moving in the null direction. There are three known types of DS cells in the vertebrate retina of the mouse, ON/OFF DS ganglion cells, ON DS ganglion cells, and OFF DS ganglion cells. Each has a distinctive physiology and anatomy.\n\nON/OFF DS ganglion cells act as local motion detectors. They fire at the onset and offset of a stimulus (a light source). If a stimulus is moving in the direction of the cell's preference, it will fire at the leading and the trailing edge. Their firing pattern is time-dependent and is supported by the Reichardt-Hassenstain model, which detects spatiotemporal correlation between the two adjacent points. The detailed explanation of the Reichardt-Hassenstain model will be provided later in the section. \nThe anatomy of ON/OFF cells is such that the dendrites extend to two sublaminae of the inner plexiform layer and make synapses with bipolar and amacrine cells. They have four subtypes, each with own preference for direction.\n\nUnlike ON/OFF DS ganglion cells that respond both to the leading and the trailing edge of a stimulus, ON DS ganglion cells are responsive only to a leading edge. The dendrites of ON DS ganglion cells are monostratified and extend into the inner sublamina of the inner plexiform layer. They have three subtypes with different directional preferences.\n\nOFF DS ganglion cells act as a centripetal motion detector, and they respond only to the trailing edge of a stimulus. They are tuned to upward motion of a stimulus. The dendrites are asymmetrical and arbor in to the direction of their preference.\n\nThe first DS cells in invertebrates were found in flies in a brain structure called the lobula plate. The lobula plate is one of the three stacks of the neuropils in the fly's optic lobe. The \"tangential cells\" of the lobula plate composed of roughly about 50 neurons, and they arborize extensively in the neuropile. The tangential cells are known to be directionally selective with distinctive directional preference. One of which is Horizontally Sensitive (HS) cells, such as the H1 neuron, that depolarize most strongly in response to stimulus moving in a horizontal direction (preferred direction). On the other hand, they hyperpolarize when the direction of motion is opposite (null direction). Vertically Sensitive (VS) cells are another group of cells that are most sensitive to vertical motion. They depolarize when a stimulus is moving downward and hyperpolarize when it is moving upward. Both HS and VS cells respond with a fixed preferred direction and a null direction regardless of the color or contrast of the background or the stimulus.\n\nIt is now known that motion detection in vision is based on the Hassenstein-Reichardt detector model. This is a model used to detect correlation between the two adjacent points. It consists of two symmetrical subunits. Both subunits have a receptor that can be stimulated by an input (light in the case of visual system). In each subunit, when an input is received, a signal is sent to the other subunit. At the same time, the signal is delayed in time within the subunit, and after the temporal filter, is then multiplied by the signal received from the other subunit. Thus, within each subunit, the two brightness values, one received directly from its receptor with a time delay and the other received from the adjacent receptor, are multiplied. The multiplied values from the two subunits are then subtracted to produce an output. The direction of selectivity or preferred direction is determined by whether the difference is positive or negative. The direction which produces a positive outcome is the preferred direction.\n\nIn order to confirm that the Reichardt-Hassenstain model accurately describes the directional selectivity in the retina, the study was conducted using optical recordings of free cytosolic calcium levels after loading a fluorescent indicator dye into the fly tangential cells. The fly was presented uniformly moving gratings while the calcium concentration in the dendritic tips of the tangential cells was measured. The tangential cells showed modulations that matched the temporal frequency of the gratings, and the velocity of the moving gratings at which the neurons respond most strongly showed a close dependency on the pattern wavelength. This confirmed the accuracy of the model both at the cellular and the behavioral level.\n\nAlthough the details of the Hassenstein-Reichardt model have not been confirmed at an anatomical and physiological level, the site of subtraction in the model is now being localized to the tangential cells. When depolarizing current is injected into the tangential cell while presenting a visual stimulus, the response to the preferred direction of motion decreased, and the response to the null direction increased. The opposite was observed with hyperpolarizing current. The T4 and T5 cells, which have been selected as a strong candidate for providing input to the tangential cells, have four subtypes that each project into one of the four strata of the lobula plate that differ in the preferred orientation.\n\nOne of the early works on DS cells in vertebrates was done on the rabbit retina by H. Barlow and W. Levick in 1965. Their experimental methods include variations to the slit-experiments and recording of the action potentials in the rabbit retina. The basic set-up of the slit experiment was they presented a moving black-white grating through a slit of various widths to a rabbit and recorded the action potentials in the retina. This early study had a large impact on the study of DS cells by laying down the foundation for later studies. The study showed that DS ganglion cells derive their property from the basis of sequence-discriminating activity of subunits, and that this activity may be the result of inhibitory mechanism in response to the motion of image in the null direction. It also showed that the DS property of retinal ganglion cells is distributed over the entire receptive field, and not limited to specific zones. Direction selectivity is contained for two adjacent points in the receptive field separated by as small as 1/4°, but selectivity decreased with larger separations. They used this to support their hypothesis that discrimination of sequences gives rise to direction selectivity because normal movement would activate adjacent points in a succession.\n\nON/OFF DS ganglion cells can be divided into 4 subtypes differing in their directional preference, ventral, dorsal, nasal, or temporal. The cells of different subtypes also differ in their dendritic structure and synaptic targets in the brain. The neurons that were identified to prefer ventral motion were also found to have dendritic projections in the ventral direction. Also, the neurons that prefer nasal motion had asymmetric dendritic extensions in the nasal direction. Thus, a strong association between the structural and functional asymmetry in ventral and nasal direction was observed. With a distinct property and preference for each subtype, there was an expectation that they could be selectively labeled by molecular markers. The neurons that were preferentially responsive to vertical motion were indeed shown to be selectively expressed by a specific molecular marker. However, molecular markers for other three subtypes have not been yet found.\n\nThe direction selective (DS) ganglion cells receive inputs from bipolar cells and starburst amacrine cells. The DS ganglion cells respond to their preferred direction with a large excitatory postsynaptic potential followed by a small inhibitory response. On the other hand, they respond to their null direction with a simultaneous small excitatory postsynaptic potential and a large inhibitory postsynaptic potential. Starburst amacrine cells have been viewed as a strong candidate for direction selectivity in ganglion cells because they can release both GABA and Ach. Their dendrites branch out radiantly from a soma, and there is a significant dendritic overlap. Optical measurements of Ca concentration showed that they respond strongly to the centrifugal motion (the outward motion from the soma to the dendrites), while they don't respond well to the centripetal motion (the inward motion from the dendritic tips to the soma).\nWhen the starburst cells were ablated with toxins, direction selectivity was eliminated. Moreover, their release of neurotransmitters itself, specifically calcium ions, reflect direction selectivity, which may be presumably attributed to the synaptic pattern. The branching pattern is organized such that certain presynaptic input will have more influence on a given dendrite than others, creating a polarity in excitation and inhibition. Further evidence suggests that starburst cells release inhibitory neurotransmitters, GABA onto each other in a delayed and prolonged manner. This accounts for the temporal property of inhibition.\n\nIn addition to spatial offset due to GABAergic synapses, the important role of chloride transporters has started to be discussed. The popular hypothesis is that starburst amacrine cells differentially express chloride transporters along the dendrites. Given this assumption, some areas along the dendrite will have a positive chloride-ion equilibrium potential relative to the resting potential while others have a negative equilibrium potential. This means that GABA at one area will be depolarizing and at another area hyperpolarizing, accounting for the spatial offset present between excitation and inhibition.\n\nRecent research (published March 2011) relying on serial block-face electron microscopy (SBEM) has led to identification of the circuitry that influences directional selectivity. This new technique provides detailed images of calcium flow and anatomy of dendrites of both starburst amacrine (SAC) and DS ganglion cells. By comparing the preferred directions of ganglion cells with their synapses on SAC's, Briggman et al. provide evidence for a mechanism primarily based on inhibitory signals from SAC's based on an oversampled serial block-face scanning electron microscopy study of one sampled retina, that retinal ganglion cells may receive asymmetrical inhibitory inputs directly from starburst amacrine cells, and therefore computation of directional selectivity also occurs postsynaptically. Such postsynaptic models are unparsimonious, and so if any given starburst amacrine cells conveys motion information to retinal ganglion cells then any computing of 'local' direction selectivity postsynaptically by retinal ganglion cells is redundant and dysfunctional. An acetylcholine (ACh) transmission model of directionally selective starburst amacrine cells provides a robust topological underpinning of a motion sensing in the retina.\n\n\n\n"}
{"id": "1285895", "url": "https://en.wikipedia.org/wiki?curid=1285895", "title": "Murder of Kriss Donald", "text": "Murder of Kriss Donald\n\nKriss Donald (2 July 1988 – 15 March 2004) was a 15-year-old Scottish white male who was kidnapped and murdered in Glasgow in 2004 by a gang of men of Pakistani origin, some of whom fled to Pakistan after the crime. Daanish Zahid, Imran Shahid, Zeeshan Shahid and Mohammed Faisal Mustaq were later found guilty of racially motivated murder and sentenced to life imprisonment.\n\nThe case, which featured the first-ever conviction for racially motivated murder in Scotland, is cited in two newspaper articles as an example of the lack of attention the media and society give to white sufferers of racist attacks compared to that given to ethnic minorities. It is also suggested the crime demonstrates how society has been forced to redefine racism so as to no longer exclude white victims.\n\nOn 15 March 2004, Donald was abducted from Kenmure Street by five men associated with a local British Pakistani gang led by Imran Shahid. The kidnapping was ostensibly revenge for an attack on Shahid at a nightclub in Glasgow city centre the night before by a local white gang, and Donald was chosen as an example of a \"white boy from the McCulloch Street area\" despite having no involvement in the nightclub attack or in any gang activity.\n\nDonald was taken on a 200-mile journey to Dundee and back while his kidnappers made phone calls looking for a house to take him to. Having no success at this, they returned to Glasgow and took him to the Clyde Walkway, near Celtic Football Club's training ground. There, they held his arms (ascertained due to an absence of defensive wounds) and stabbed him multiple times. He sustained internal injuries to three arteries, one of his lungs, his liver and a kidney. He was doused in petrol and set on fire as he bled to death.\n\nThe issue of the killing quickly became politicised because of the racial element. After the murder there were reportedly 'racial tensions' in the area sufficient to lead to police intervention.\n\nInitially, two men were arrested in connection with the crime. One man, Daanish Zahid, was found guilty of Kriss Donald's murder on 18 November 2004 and is the first person to be convicted of racially motivated murder in Scotland.\n\nThree suspects were arrested in Pakistan in July 2005 and extradited to the UK in October 2005, following the intervention of Mohammed Sarwar, the MP for Glasgow Central.\n\nThe Pakistani police had to engage in a \"long struggle\" to capture two of the escapees. There is no extradition treaty between Pakistan and Britain, but the Pakistani authorities agreed to extradite the suspects. There were numerous diplomatic complications around the case, including apparent divergences between government activities and those of ambassadorial officials; government figures were at times alleged to be reluctant to pursue the case for diplomatic reasons.\n\nThe three extradited suspects, Imran Shahid, Zeeshan Shahid, and Mohammed Faisal Mushtaq, all in their late twenties, arrived in Scotland on 5 October 2005. They were charged with Donald's murder the following day. Their trial opened on 2 October 2006 in Scotland.\n\nOn 8 November 2006, the three men were found guilty of the racially motivated murder of Kriss Donald. All three had denied the charge, but a jury at the High Court in Edinburgh convicted them of abduction and murder.\n\nThe BBC has been criticised by some viewers because the case featured on national news only three times and the first trial was later largely confined to regional Scottish bulletins including the verdict itself. Although admitting that the BBC had \"got it wrong\", the organisation's Head of Newsgathering, Fran Unsworth, largely rejected the suggestion that Donald's race played a part in the lack of reportage, instead claiming it was mostly a product of \"Scottish blindness\". In preference to reporting the verdict the organisation found the time to report the opening of a new arts centre in Gateshead in its running order.\nThe BBC again faced criticisms for its failure to cover the second trial in its main bulletins, waiting until day 18 to mention the issue and Peter Horrocks of the BBC apologised for the organisation's further failings.\n\nPeter Fahy, spokesman of race issues for the Association of Chief Police Officers, noted that the media as a whole tended to under-report the racist murders of white people, stating \"it was a fact that it was harder to get the media interested where murder victims were young white men\".\n\nThe British National Party were accused by Scotland's First Minister and Labour Party MSP Jack McConnell among others of seeking to exploit the case for political advantage, and an open letter signed by MSPs, trades unionists, and community leaders, condemned the BNP's plans to stage a visit to Pollokshields. The group did hold a rally in the area, leading to accusations that it was fuelling racial tension.\n\nA March 2004 article in \"The Scotsman\" newspaper alleged a lack of response by authorities to concerns of rising racial tensions and that Strathclyde Police had felt pressured to abandon Operation Gather, an investigation into Asian gangs in the area, for fear of offending ethnic minorities. In a January 2005 interview with a Scottish newspaper, prominent Pakistani Glaswegian Bashir Maan claimed that \"fear and intimidation\" had allowed problems with Asian gangs in some parts of the city to go unchecked. The article also quoted a former senior Strathclyde police officer who criticised \"a culture of political correctness\" which had allowed gang crime to \"grow unfettered\".\n\nA BBC report suggests that another reason for inaction was lack of evidence, as locals were more prepared to make complaints than to give evidence in court. Some commentators have argued the murder was somewhat mischaracterised in the media, as well as expressing a doubt that significant ethnic tensions exist in Pollokshields, suggesting that \"gangland revenge\" may have played a part.\n\nGlasgow band Glasvegas wrote the song \"Flowers And Football Tops\" having been inspired by the tragedy and the likely effect it would have on the victim's parents. The band dedicated their 2008 Philip Hall Radar NME award win to Donald's memory.\n\nA memorial plaque was installed on a bench beside the River Clyde, near to where he was killed, in memory of Donald. In addition,a memorial plaque was placed on a public fence in Pollokshields close to the spot where he was kidnapped; in July 2018, friends and family gathered at the spot to remember him on what would have been his 30th birthday.\n\nThe murder led some people to examine their views of racism and its victims. Mark Easton cited the racist murders of Donald and also Ross Parker as demonstrating how society has been forced to redefine racism and discard the erroneous definition of \"prejudice plus power\"—a definition which only allowed ethnic minorities to be victims of hate crime. Yasmin Alibhai-Brown also cited the Donald case when highlighting the lack of concern for white victims of racist murders. She drew comparisons with high-profile ethnic minority victims, asking whether Donald's murderers were \"less evil than those who killed Stephen Lawrence\". Alibhai-Brown came to the conclusion that treating \"some victims as more worthy of condemnation than others is unforgivable—and a betrayal of anti-racism itself\".\n\n\n"}
{"id": "154616", "url": "https://en.wikipedia.org/wiki?curid=154616", "title": "Negative number", "text": "Negative number\n\nIn mathematics, a negative number is a real number that is less than zero. Negative numbers represent opposites. If positive represents a movement to the right, negative represents a movement to the left. If positive represents above sea level, then negative represents below sea level. If positive represents a deposit, negative represents a withdrawal. They are often used to represent the magnitude of a loss or deficiency. A debt that is owed may be thought of as a negative asset, a decrease in some quantity may be thought of as a negative increase. If a quantity may have either of two opposite senses, then one may choose to distinguish between those senses—perhaps arbitrarily—as \"positive\" and \"negative\". In the medical context of fighting a tumor, an expansion could be thought of as a negative shrinkage. Negative numbers are used to describe values on a scale that goes below zero, such as the Celsius and Fahrenheit scales for temperature. The laws of arithmetic for negative numbers ensure that the common sense idea of an opposite is reflected in arithmetic. For example, −(−3) = 3 because the opposite of an opposite is the original value.\n\nNegative numbers are usually written with a minus sign in front. For example, −3 represents a negative quantity with a magnitude of three, and is pronounced \"minus three\" or \"negative three\". To help tell the difference between a subtraction operation and a negative number, occasionally the negative sign is placed slightly higher than the minus sign (as a superscript). Conversely, a number that is greater than zero is called \"positive\"; zero is usually (but not always) thought of as neither positive nor negative. The positivity of a number may be emphasized by placing a plus sign before it, e.g. . In general, the negativity or positivity of a number is referred to as its sign.\n\nEvery real number other than zero is either positive or negative. The positive whole numbers are referred to as natural numbers, while the positive and negative whole numbers (together with zero) are referred to as integers.\n\nIn bookkeeping, amounts owed are often represented by red numbers, or a number in parentheses, as an alternative notation to represent negative numbers.\n\nNegative numbers appeared for the first time in history in the \"Nine Chapters on the Mathematical Art\", which in its present form dates from the period of the Chinese Han Dynasty (202 BC – AD 220), but may well contain much older material. Liu Hui (c. 3rd century) established rules for adding and subtracting negative numbers. By the 7th century, Indian mathematicians such as Brahmagupta were describing the use of negative numbers. Islamic mathematicians further developed the rules of subtracting and multiplying negative numbers and solved problems with negative coefficients. Western mathematicians accepted the idea of negative numbers by the 17th century. Prior to the concept of negative numbers, mathematicians such as Diophantus considered negative solutions to problems \"false\" and equations requiring negative solutions were described as absurd.\n\nNegative numbers can be thought of as resulting from the subtraction of a larger number from a smaller. For example, negative three is the result of subtracting three from zero:\nIn general, the subtraction of a larger number from a smaller yields a negative result, with the magnitude of the result being the difference between the two numbers. For example,\nsince .\n\nThe relationship between negative numbers, positive numbers, and zero is often expressed in the form of a number line:\n\nNumbers appearing farther to the right on this line are greater, while numbers appearing farther to the left are less. Thus zero appears in the middle, with the positive numbers to the right and the negative numbers to the left.\n\nNote that a negative number with greater magnitude is considered less. For example, even though (positive) is greater than (positive) , written\nnegative is considered to be less than negative :\n(Because, for example, if you have £-8, a debt of £8, you would have less after adding, say £10, to it than if you have £-5.) \nIt follows that any negative number is less than any positive number, so\n\nIn the context of negative numbers, a number that is greater than zero is referred to as positive. Thus every real number other than zero is either positive or negative, while zero itself is not considered to have a sign. Positive numbers are sometimes written with a plus sign in front, e.g. denotes a positive three.\n\nBecause zero is neither positive nor negative, the term nonnegative is sometimes used to refer to a number that is either positive or zero, while nonpositive is used to refer to a number that is either negative or zero. Zero is a neutral number.\n\n\n\n\n\nThe minus sign \"−\" signifies the operator for both the binary (two-operand) operation of subtraction (as in ) and the unary (one-operand) operation of negation (as in , or twice in ). A special case of unary negation occurs when it operates on a positive number, in which case the result is a negative number (as in ).\n\nThe ambiguity of the \"−\" symbol does not generally lead to ambiguity in arithmetical expressions, because the order of operations makes only one interpretation or the other possible for each \"−\". However, it can lead to confusion and be difficult for a person to understand an expression when operator symbols appear adjacent to one another. A solution can be to parenthesize the unary \"−\" along with its operand.\n\nFor example, the expression may be clearer if written (even though they mean exactly the same thing formally). The subtraction expression is a different expression that doesn't represent the same operations, but it evaluates to the same result.\n\nSometimes in elementary schools a number may be prefixed by a superscript minus sign or plus sign to explicitly distinguish negative and positive numbers as in\n\nAddition of two negative numbers is very similar to addition of two positive numbers. For example,\nThe idea is that two debts can be combined into a single debt of greater magnitude.\n\nWhen adding together a mixture of positive and negative numbers, one can think of the negative numbers as positive quantities being subtracted. For example:\nIn the first example, a credit of is combined with a debt of , which yields a total credit of . If the negative number has greater magnitude, then the result is negative:\nHere the credit is less than the debt, so the net result is a debt.\n\nAs discussed above, it is possible for the subtraction of two non-negative numbers to yield a negative answer:\nIn general, subtraction of a positive number yields the same result as the addition of a negative number of equal magnitude. Thus\nand\n\nOn the other hand, subtracting a negative number yields the same result as the addition a positive number of equal magnitude. (The idea is that \"losing\" a debt is the same thing as \"gaining\" a credit.) Thus\nand\n\nWhen multiplying numbers, the magnitude of the product is always just the product of the two magnitudes. The sign of the product is determined by the following rules:\nThus\nand\nThe reason behind the first example is simple: adding three 's together yields :\nThe reasoning behind the second example is more complicated. The idea again is that losing a debt is the same thing as gaining a credit. In this case, losing two debts of three each is the same as gaining a credit of six:\nThe convention that a product of two negative numbers is positive is also necessary for multiplication to follow the distributive law. In this case, we know that\nSince , the product must equal .\n\nThese rules lead to another (equivalent) rule—the sign of any product \"a\" × \"b\" depends on the sign of \"a\" as follows:\nThe justification for why the product of two negative numbers is a positive number can be observed in the analysis of complex numbers.\n\nThe sign rules for division are the same as for multiplication. For example,\nand\nIf dividend and divisor have the same sign, the result is always positive. Another method of dividing negative numbers is that if one of the numbers being divided is a negative, the answer will be negative.\n\nThe negative version of a positive number is referred to as its negation. For example, is the negation of the positive number . The sum of a number and its negation is equal to zero:\nThat is, the negation of a positive number is the additive inverse of the number.\n\nUsing algebra, we may write this principle as an algebraic identity:\nThis identity holds for any positive number . It can be made to hold for all real numbers by extending the definition of negation to include zero and negative numbers. Specifically:\nFor example, the negation of is . In general,\n\nThe absolute value of a number is the non-negative number with the same magnitude. For example, the absolute value of and the absolute value of are both equal to , and the absolute value of is .\n\nIn a similar manner to rational numbers, we can extend the natural numbers N to the integers Z by defining integers as an ordered pair of natural numbers (\"a\", \"b\"). We can extend addition and multiplication to these pairs with the following rules:\n\nWe define an equivalence relation ~ upon these pairs with the following rule:\nThis equivalence relation is compatible with the addition and multiplication defined above, and we may define Z to be the quotient set N²/~, i.e. we identify two pairs (\"a\", \"b\") and (\"c\", \"d\") if they are equivalent in the above sense. Note that Z, equipped with these operations of addition and multiplication, is a ring, and is in fact, the prototypical example of a ring.\n\nWe can also define a total order on Z by writing\n\nThis will lead to an \"additive zero\" of the form (\"a\", \"a\"), an \"additive inverse\" of (\"a\", \"b\") of the form (\"b\", \"a\"), a multiplicative unit of the form (\"a\" + 1, \"a\"), and a definition of subtraction\nThis construction is a special case of the Grothendieck construction.\n\nThe negative of a number is unique, as is shown by the following proof.\n\nLet \"x\" be a number and let \"y\" be its negative.\nSuppose \"y′\" is another negative of \"x\". By an axiom of the real number system\n\nAnd so, \"x\" + \"y′\" = \"x\" + \"y\". Using the law of cancellation for addition, it is seen that\n\"y′\" = \"y\". Thus \"y\" is equal to any other negative of \"x\". That is, \"y\" is the unique negative of \"x\".\n\nFor a long time, negative solutions to problems were considered \"false\". In Hellenistic Egypt, the Greek mathematician Diophantus in the 3rd century AD referred to an equation that was equivalent to 4\"x\" + 20 = 4 (which has a negative solution) in \"Arithmetica\", saying that the equation was absurd.\n\nNegative numbers appear for the first time in history in the \"Nine Chapters on the Mathematical Art\" (\"Jiu zhang suan-shu\"), which in its present form dates from the period of the Han Dynasty (202 BC – AD 220), but may well contain much older material. The mathematician Liu Hui (c. 3rd century) established rules for the addition and subtraction of negative numbers. The historian Jean-Claude Martzloff theorized that the importance of duality in Chinese natural philosophy made it easier for the Chinese to accept the idea of negative numbers. The Chinese were able to solve simultaneous equations involving negative numbers. The \"Nine Chapters\" used red counting rods to denote positive coefficients and black rods for negative. This system is the exact opposite of contemporary printing of positive and negative numbers in the fields of banking, accounting, and commerce, wherein red numbers denote negative values and black numbers signify positive values. Liu Hui writes:\n\nThe ancient Indian \"Bakhshali Manuscript\" carried out calculations with negative numbers, using \"+\" as a negative sign. The date of the manuscript is uncertain. LV Gurjar dates it no later than the 4th century, Hoernle dates it between the third and fourth centuries, Ayyangar and Pingree dates it to the 8th or 9th centuries, and George Gheverghese Joseph dates it to about AD 400 and no later than the early 7th century,\n\nDuring the 7th century AD, negative numbers were used in India to represent debts. The Indian mathematician Brahmagupta, in \"Brahma-Sphuta-Siddhanta\" (written c. AD 630), discussed the use of negative numbers to produce the general form quadratic formula that remains in use today. He also found negative solutions of quadratic equations and gave rules regarding operations involving negative numbers and zero, such as \"A debt cut off from nothingness becomes a credit; a credit cut off from nothingness becomes a debt. \" He called positive numbers \"fortunes,\" zero \"a cipher,\" and negative numbers \"debts.\"\n\nIn the 9th century, Islamic mathematicians were familiar with negative numbers from the works of Indian mathematicians, but the recognition and use of negative numbers during this period remained timid. Al-Khwarizmi in his \"Al-jabr wa'l-muqabala\" (from which we get the word \"algebra\") did not use negative numbers or negative coefficients. But within fifty years, Abu Kamil illustrated the rules of signs for expanding the multiplication formula_3, and al-Karaji wrote in his \"al-Fakhrī\" that \"negative quantities must be counted as terms\". In the 10th century, Abū al-Wafā' al-Būzjānī considered debts as negative numbers in \"A Book on What Is Necessary from the Science of Arithmetic for Scribes and Businessmen\".\n\nBy the 12th century, al-Karaji's successors were to state the general rules of signs and use them to solve polynomial divisions. As al-Samaw'al writes:\nthe product of a negative number — \"al-nāqiṣ\" — by a positive number — \"al-zāʾid\" — is negative, and by a negative number is positive. If we subtract a negative number from a higher negative number, the remainder is their negative difference. The difference remains positive if we subtract a negative number from a lower negative number. If we subtract a negative number from a positive number, the remainder is their positive sum. If we subtract a positive number from an empty power (\"martaba khāliyya\"), the remainder is the same negative, and if we subtract a negative number from an empty power, the remainder is the same positive number.\n\nIn the 12th century in India, Bhāskara II gave negative roots for quadratic equations but rejected them because they were inappropriate in the context of the problem. He stated that a negative value is \"in this case not to be taken, for it is inadequate; people do not approve of negative roots.\"\n\nEuropean mathematicians, for the most part, resisted the concept of negative numbers until the 17th century, although Fibonacci allowed negative solutions in financial problems where they could be interpreted as debits (chapter 13 of \"Liber Abaci\", AD 1202) and later as losses (in \"Flos\").\n\nIn the 15th century, Nicolas Chuquet, a Frenchman, used negative numbers as exponents but referred to them as “absurd numbers.” In his 1544 \"Arithmetica Integra\" Michael Stifel also dealt with negative numbers, also calling them \"numeri absurdi\".\n\nIn 1545, Gerolamo Cardano, in his \"Ars Magna\", provided the first satisfactory treatment of negative numbers in Europe. He did not allow negative numbers in his consideration of cubic equations, so he had to treat, for example, \"x\" + \"ax\" = \"b\" separately from \"x\" = \"ax\" + \"b\" (with \"a\",\"b\" > 0 in both cases). In all, Cardano was driven to the study of thirteen different types of cubic equations, each expressed purely in terms of positive numbers.\n\nIn A.D. 1759, Francis Maseres, an English mathematician, wrote that negative numbers \"darken the very whole doctrines of the equations and make dark of the things which are in their nature excessively obvious and simple\". He came to the conclusion that negative numbers were nonsensical.\n\nIn the 18th century it was common practice to ignore any negative results derived from equations, on the assumption that they were meaningless.\n\nGottfried Wilhelm Leibniz was the first mathematician to systematically employ negative numbers as part of a coherent mathematical system, the infinitesimal calculus. Calculus made negative numbers necessary and their dismissal as \"absurd numbers\" quickly faded.\n\n"}
{"id": "43756958", "url": "https://en.wikipedia.org/wiki?curid=43756958", "title": "Phenomenal concept strategy", "text": "Phenomenal concept strategy\n\nThe phenomenal concept strategy (PCS) is an approach within philosophy of mind to provide a physicalist response to anti-physicalist arguments like the explanatory gap and philosophical zombies. The name was coined by Daniel Stoljar. As David Chalmers put it, PCS \"locates the gap in the relationship between our \"concepts\" of physical processes and our \"concepts\" of consciousness, rather than in the relationship between physical processes and consciousness themselves.\" The idea is that if we can explain why we \"think\" there's an explanatory gap, this will defuse the motivation to question physicalism.\n\nPCS advocates typically subscribe to what Chalmers has called \"type-B materialism\", which holds that there is an epistemic but not ontological gap between physics and subjective experience. PCS maintains that our concepts are dualistic, but reality is monistic, in a similar way as \"heat\" and \"molecular motion\" are two different concepts that refer to the same property. However, phenomenal concepts are different from other concepts in that they incline us to see an epistemic gap. PCS suggests that physicalist explanations \"cannot feel satisfactory [...] since the concepts used in the physical explanation don't entail any applications of the phenomenal concepts in terms of which the explanandum is characterized.\"\n\nPCS would help physicalists answer the knowledge argument because upon seeing red, Mary would have new thoughts about phenomenal concepts, even though those thoughts would only re-express physical facts she already knew. Likewise, we can conceive of zombies even if they aren't possible because when we think about their functional/physical characteristics, we don't also conjure thoughts about phenomenal concepts.\n\nDavid Papineau coined the term \"antipathetic fallacy\" to refer to the way in which we fail to see phenomenal experience in brain processing. It's the opposite of the pathetic fallacy of seeing consciousness in non-minds.\n\nChalmers outlines several ways in which phenomenal concepts might be distinctive:\n\nThese are so-called \"type-demonstratives\" in which we point to \"one of \"those\"\". For instance:\n\nPeter Carruthers suggests that phenomenal concepts are \"purely\" recognitional, which means\n\nWe think about physical and phenomenal concepts in different ways.\n\nSeveral philosophers have suggested that phenomenal concepts denote brain states indexically, in a similar way as saying \"now\" picks out a particular time. Even given full knowledge of physics, additional indexical information is required to say where and when one is.\n\nSome contend that phenomenal states are part of the concepts that refer to them. For instance, Papineau suggests that phenomenal concepts are \"quotational\", like saying \"That state: ___.\"\n\nKatalin Balog defends a \"constitutional account\" of phenomenal concepts, in which \"token experiences serve as modes of presentation of the phenomenal properties they instantiate.\" For instance, the concept of pain is partly constituted by a token experience of pain. She claims this position helps resolve the explanatory gap because an \"a priori\" description alone doesn't suffice to express the concept; in addition, a direct experiential constitution is required. While it seems like physical/functional information about tells us all there is about it, we feel something more for phenomenality because we \"have a 'substantive' grasp of its nature.\"\n\nPapineau takes a similar position. He claims that normal physical identity statements (such as that heat is molecular kinetic energy) involve two descriptions, which we can associate in our minds. In contrast, we think about a phenomenal concept by either \"actually undergoing the experience\" or at least by imagining it, and this creates a \"what-it’s-likeness\" sensation. Then:\nPapineau compares the situation to the use–mention distinction: Phenomenal concepts directly use the experiences to which they refer, while physical descriptions merely mention them.\n\nDavid Chalmers presents \"A Master Argument\" against PCS. He defines C as the PCS thesis that\nAll three of these must hold for PCS to succeed. He defines P as all physical facts. Then he poses a dilemma:\nRegardless of which horn is true, C is invalidated.\n\nCarruthers and Veillet argue that Chalmers's argument commits a fallacy of equivocation between first-person and third-person phenomenal concepts, but the authors reformulate it to avoid that problem. They proceed to attack the revised argument by denying the premise that if zombies must have third-person phenomenal concepts, then phenomenal concepts can't account for the explanatory gap. In particular, they suggest that, pace Chalmers, people and zombies would have the same epistemic situation even though the \"contents\" of their situations would be different. For instance, a person's phenomenal concept would have content of a phenomenal state, while the \"schmenomenal\" concept of a zombie would have content about a \"schmenomenal\" state. A zombie \"is correct when he says that he is conscious, because he isn't saying that he has phenomenal states as we understand them. He is correct because he means that he has schmenomenal states, and he has them.\" So people and zombies can both have true beliefs justified in similar ways (same epistemic situation), even if those beliefs are \"about\" different things.\n\n\n"}
{"id": "403139", "url": "https://en.wikipedia.org/wiki?curid=403139", "title": "Point at infinity", "text": "Point at infinity\n\nIn geometry, a point at infinity or ideal point is an idealized limiting point at the \"end\" of each line.\n\nIn the case of an affine plane (including the Euclidean plane), there is one ideal point for each pencil of parallel lines of the plane. Adjoining these points produces a projective plane, in which no point can be distinguished, if we \"forget\" which points were added. This holds for a geometry over any field, and more generally over any division ring.\n\nIn the real case, a point at infinity completes a line into a topologically closed curve. In higher dimensions, all the points at infinity form a projective subspace of one dimension less than that of the whole projective space to which they belong. A point at infinity can also be added to the complex line (which may be thought of as the complex plane), thereby turning it into a closed surface known as the complex projective line, CP, also called the Riemann sphere (when complex numbers are mapped to each point).\n\nIn the case of a hyperbolic space, each line has two distinct ideal points. Here, the set of ideal points takes the form of a quadric.\n\nIn an affine or Euclidean space of higher dimension, the points at infinity are the points which are added to the space to get the projective completion. The set of the points at infinity is called, depending on the dimension of the space, the line at infinity, the plane at infinity or the hyperplane at infinity, in all cases a projective space of one less dimension.\n\nAs a projective space over a field is a smooth algebraic variety, the same is true for the set of points at infinity. Similarly, if the ground field is the real or the complex field, the set of points at infinity is a manifold.\n\nIn artistic drawing and technical perspective, the projection on the picture plane of the point at infinity of a class of parallel lines is called their vanishing point.\n\nIn hyperbolic geometry, points at infinity are typically named ideal points. Unlike Euclidean and elliptic geometries, each line has two points at infinity: given a line \"l\" and a point \"P\" not on \"l\", the right- and left-limiting parallels converge asymptotically to different points at infinity.\n\nAll points at infinity together form the Cayley absolute or boundary of a hyperbolic plane.\n\nThis construction can be generalized to topological spaces. Different compactifications may exist for a given space, but arbitrary topological space admits Alexandroff extension, also called the \"one-point compactification\" when the original space is not itself compact. Projective line (over arbitrary field) is the Alexandroff extension of the corresponding field. Thus the circle is the one-point compactification of the real line, and the sphere is the one-point compactification of the plane. Projective spaces P for  > 1 are not \"one-point\" compactifications of corresponding affine spaces for the reason mentioned above under , and completions of hyperbolic spaces with ideal points are also not one-point compactifications.\n\n"}
{"id": "17033211", "url": "https://en.wikipedia.org/wiki?curid=17033211", "title": "Quantitative models of the action potential", "text": "Quantitative models of the action potential\n\nIn neurophysiology, several mathematical models of the action potential have been developed, which fall into two basic types. The first type seeks to model the experimental data quantitatively, i.e., to reproduce the measurements of current and voltage exactly. The renowned Hodgkin–Huxley model of the axon from the \"Loligo\" squid exemplifies such models. Although qualitatively correct, the H-H model does not describe every type of excitable membrane accurately, since it considers only two ions (sodium and potassium), each with only one type of voltage-sensitive channel. However, other ions such as calcium may be important and there is a great diversity of channels for all ions. As an example, the cardiac action potential illustrates how differently shaped action potentials can be generated on membranes with voltage-sensitive calcium channels and different types of sodium/potassium channels. The second type of mathematical model is a simplification of the first type; the goal is not to reproduce the experimental data, but to understand qualitatively the role of action potentials in neural circuits. For such a purpose, detailed physiological models may be unnecessarily complicated and may obscure the \"forest for the trees\". The Fitzhugh-Nagumo model is typical of this class, which is often studied for its entrainment behavior. Entrainment is commonly observed in nature, for example in the synchronized lighting of fireflies, which is coordinated by a burst of action potentials; entrainment can also be observed in individual neurons. Both types of models may be used to understand the behavior of small biological neural networks, such as the central pattern generators responsible for some automatic reflex actions. Such networks can generate a complex temporal pattern of action potentials that is used to coordinate muscular contractions, such as those involved in breathing or fast swimming to escape a predator.\n\nIn 1952 Alan Lloyd Hodgkin and Andrew Huxley developed a set of equations to fit their experimental voltage-clamp data on the axonal membrane. The model assumes that the membrane capacitance \"C\" is constant; thus, the transmembrane voltage \"V\" changes with the total transmembrane current \"I\" according to the equation\n\nwhere \"I\", \"I\", and \"I\" are currents conveyed through the local sodium channels, potassium channels, and \"leakage\" channels (a catch-all), respectively. The initial term \"I\" represents the current arriving from external sources, such as excitatory postsynaptic potentials from the dendrites or a scientist's electrode.\n\nThe model further assumes that a given ion channel is either fully open or closed; if closed, its conductance is zero, whereas if open, its conductance is some constant value \"g\". Hence, the net current through an ion channel depends on two variables: the probability \"p\" of the channel being open, and the difference in voltage from that ion's equilibrium voltage, \"V\" − \"V\". For example, the current through the potassium channel may be written as\n\nwhich is equivalent to Ohm's law. By definition, no net current flows (\"I\" = 0) when the transmembrane voltage equals the equilibrium voltage of that ion (when \"V\" = \"E\").\n\nTo fit their data accurately, Hodgkin and Huxley assumed that each type of ion channel had multiple \"gates\", so that the channel was open only if all the gates were open and closed otherwise. They also assumed that the probability of a gate being open was independent of the other gates being open; this assumption was later validated for the inactivation gate. Hodgkin and Huxley modeled the voltage-sensitive potassium channel as having four gates; letting \"p\" denote the probability of a single such gate being open, the probability of the whole channel being open is the product of four such probabilities, i.e., \"p\" = \"n\". Similarly, the probability of the voltage-sensitive sodium channel was modeled to have three similar gates of probability \"m\" and a fourth gate, associated with inactivation, of probability \"h\"; thus, \"p\" = \"m\"\"h\". The probabilities for each gate are assumed to obey first-order kinetics\n\nwhere both the equilibrium value \"m\" and the relaxation time constant τ depend on the instantaneous voltage \"V\" across the membrane. If \"V\" changes on a time-scale more slowly than τ, the \"m\" probability will always roughly equal its equilibrium value \"m\"; however, if \"V\" changes more quickly, then \"m\" will lag behind \"m\". By fitting their voltage-clamp data, Hodgkin and Huxley were able to model how these equilibrium values and time constants varied with temperature and transmembrane voltage. The formulae are complex and depend exponentially on the voltage and temperature. For example, the time constant for sodium-channel activation probability \"h\" varies as 3 with the Celsius temperature θ, and with voltage \"V\" as\n\nIn summary, the Hodgkin–Huxley equations are complex, non-linear ordinary differential equations in four independent variables: the transmembrane voltage \"V\", and the probabilities \"m\", \"h\" and \"n\". No general solution of these equations has been discovered. A less ambitious but generally applicable method for studying such non-linear dynamical systems is to consider their behavior in the vicinity of a fixed point. This analysis shows that the Hodgkin–Huxley system undergoes a transition from stable quiescence to bursting oscillations as the stimulating current \"I\" is gradually increased; remarkably, the axon becomes stably quiescent again as the stimulating current is increased further still. A more general study of the types of qualitative behavior of axons predicted by the Hodgkin–Huxley equations has also been carried out.\n\nBecause of the complexity of the Hodgkin–Huxley equations, various simplifications have been developed that exhibit qualitatively similar behavior. The Fitzhugh-Nagumo model is a typical example of such a simplified system. Based on the tunnel diode, the FHN model has only two independent variables, but exhibits a similar stability behavior to the full Hodgkin–Huxley equations. The equations are\n\nwhere \"g(V)\" is a function of the voltage \"V\" that has a region of negative slope in the middle, flanked by one maximum and one minimum (Figure FHN). A much-studied simple case of the Fitzhugh-Nagumo model is the Bonhoeffer-van der Pol nerve model, which is described by the equations\n\nwhere the coefficient ε is assumed to be small. These equations can be combined into a second-order differential equation\n\nThis van der Pol equation has stimulated much research in the mathematics of nonlinear dynamical systems. Op-amp circuits that realize the FHN and van der Pol models of the action potential have been developed by Keener.\n\nA hybrid of the Hodgkin–Huxley and FitzHugh–Nagumo models was developed by Morris and Lecar in 1981, and applied to the muscle fiber of barnacles. True to the barnacle's physiology, the Morris–Lecar model replaces the voltage-gated sodium current of the Hodgkin–Huxley model with a voltage-dependent calcium current. There is no inactivation (no \"h\" variable) and the calcium current equilibrates instantaneously, so that again, there are only two time-dependent variables: the transmembrane voltage \"V\" and the potassium gate probability \"n\". The bursting, entrainment and other mathematical properties of this model have been studied in detail.\n\nThe simplest models of the action potential are the \"flush and fill\" models (also called \"integrate-and-fire\" models), in which the input signal is summed (the \"fill\" phase) until it reaches a threshold, firing a pulse and resetting the summation to zero (the \"flush\" phase). All of these models are capable of exhibiting entrainment, which is commonly observed in nervous systems.\n\nWhereas the above models simulate the transmembrane voltage and current at a single patch of membrane, other mathematical models pertain to the voltages and currents in the ionic solution surrounding the neuron. Such models are helpful in interpreting data from extracellular electrodes, which were common prior to the invention of the glass pipette electrode that allowed intracellular recording. The extracellular medium may be modeled as a normal isotropic ionic solution; in such solutions, the current follows the electric field lines, according to the continuum form of Ohm's Law\n\nwhere j and E are vectors representing the current density and electric field, respectively, and where σ is the conductivity. Thus, j can be found from E, which in turn may be found using Maxwell's equations. Maxwell's equations can be reduced to a relatively simple problem of electrostatics, since the ionic concentrations change too slowly (compared to the speed of light) for magnetic effects to be important. The electric potential φ(x) at any extracellular point x can be solved using Green's identities\n\nwhere the integration is over the complete surface of the membrane; formula_12 is a position on the membrane, σ and φ are the conductivity and potential just within the membrane, and σ and φ the corresponding values just outside the membrane. Thus, given these σ and φ values on the membrane, the extracellular potential φ(x) can be calculated for any position x; in turn, the electric field E and current density j can be calculated from this potential field.\n\n"}
{"id": "50283749", "url": "https://en.wikipedia.org/wiki?curid=50283749", "title": "Rangeland management", "text": "Rangeland management\n\nRangeland management (also range management, range science, or arid-land management) is a professional natural science that centers around the study of rangelands and the \"conservation and sustainable management [of Arid-Lands] for the benefit of current societies and future generations.\" Range management is defined by Holechek et al. as the \"manipulation of rangeland components to obtain optimum combination of goods and services for society on a sustained basis.\"\n\nThe earliest form of Rangeland Management is not formally deemed part of the natural science studied today, although its roots can be traced to nomadic grazing practices of the neolithic agricultural revolution when humans domesticated plants and animals under pressures from population growth and environmental change. Humans might even have altered the environment in times preceding the Neolithic through hunting of large-game, whereby large losses of grazing herbivores could have resulted in altered ecological states; meaning humans have been inadvertently managing land throughout prehistory.\n\nRangeland management was developed in the United States in response to rangeland deterioration and in some cases, denudation, due to overgrazing and other misuse of arid lands, as was described by Hardin’s 1968 \"Tragedy of the Commons\" and evidenced previously by the 20th century \"Dust Bowl\". Historically, the discipline focused on the manipulation of grazing and the proper use of rangeland vegetation for livestock.\n\nToday, range management's focus has been expanded to include the host of ecosystem services that rangelands provide to humans world-wide. Key management components seek to optimize such goods and services through the protection and enhancement of soils, riparian zones, watersheds, and vegetation complexes, sustainably improving outputs of consumable range products such as red meat, wildlife, water, wood, fiber, leather, energy resource extraction, and outdoor recreation, as well as maintaining a focus on the manipulation of grazing activities of large herbivores to maintain or improve animal and plant production.\n\nThe Society for Range Management is \"the professional society dedicated to supporting persons who work with rangelands and have a commitment to their sustainable use.\" The primary Rangeland Management publications include the \"Journal of Range Management\", \"Rangelands\", and \"Rangeland Ecology & Management\".\n\nPastoralism has become a contemporary anthropological and ecological study as it faces many threats including fragmentation of land, conversion of rangeland into urban development, lack of grazing movement, impending threats on global diversity, damage to species with large terrain, decreases in shared public goods, decreased biological movements, threats of a \"tragedy of enclosures\", limitation of key resources, reduced biomass and invasive plant species growth. Interest in contemporary pastoralist cultures like the Maasai has continued to increase, especially because the traditional syncreticly-adaptive ability of pastoralists could promise lessons in collaborative and adaptive management for contemporary pastoralist societies threatened by globalization as well as for contemporary non-pastoralist societies that are managing livestock on rangelands.\n\nIn the United States, the study of range science is commonly offered at land-grant universities including New Mexico State University, Colorado State University, Oregon State University, South Dakota State University, Texas A&M University, Texas Tech University, the University of Arizona, the University of Idaho, the University of Wyoming, Utah State University, and Montana State University. The Range Science curriculum is strongly tied to animal science, as well as plant ecology, soil science, wildlife management, climatology and anthropology. Courses in a typical Range Science curriculum may include ethology, range animal nutrition, plant physiology, plant ecology, plant identification, plant communities, microbiology, soil sciences, fire control, agricultural economics, wildlife ecology, ranch management, Socioeconomics, cartography, hydrology, Ecophysiology, and environmental policy. These courses are essential to entering a range science profession.\n\nStudents with degrees in range science are eligible for a host of technician-type careers working for the federal government under the Bureau of Land Management, the United States Fish and Wildlife Service, the Agricultural Research Service, the United States Environmental Protection Agency, the NRCS, or the US Forest Service as range conservationists, inventory technicians, range monitoring/animal science agents, field botanists, natural-resource technicians, vegetation/habitat monitors, GIS programming assistants, general range technicians, and as ecological assessors, as well as working in the private sector as range managers, ranch managers, producers, commercial consultants, mining and agricultural real estate agents, or as Range/ Ranch Consultants. Individuals who complete degrees at the M.S. or P.h.D. level, can seek academic careers as professors, extension specialists, research assistants, and adjunct staff, in addition to a number of professional research positions for government agencies such as the US Department of Agriculture and other state run departments.\n\n"}
{"id": "5233290", "url": "https://en.wikipedia.org/wiki?curid=5233290", "title": "ReadyBoost", "text": "ReadyBoost\n\nReadyBoost (codenamed EMD) is a disk caching software component developed by Microsoft for Windows Vista and included in later versions of the Windows operating system. ReadyBoost enables NAND memory mass storage devices, including CompactFlash, SD cards, and USB flash drives, to be used as a write cache between a hard drive and random access memory in an effort to increase computing performance. ReadyBoost relies on the SuperFetch technology and, like SuperFetch, adjusts its cache based on user activity. Other features, including ReadyDrive, are implemented in a manner similar to ReadyBoost.\n\nUsing ReadyBoost-capable flash memory (NAND memory devices) for caching allows Windows Vista and later to service random disk reads with better performance than without the cache. This caching applies to all disk content, not just the page file or system DLLs. Flash devices typically are slower than a mechanical hard disk for sequential I/O, so, to maximize performance, ReadyBoost includes logic that recognizes large, sequential read requests and has the hard disk service these requests.\n\nWhen a compatible device is plugged in, the Windows AutoPlay dialog offers an additional option to use the flash drive to speed up the system; an additional ReadyBoost tab is added to the drive's properties dialog where the amount of space to be used can be configured. The minimum cache size is 250 MB. In Vista or with FAT32 formatting of the drive, the maximum is 4 GB. In Windows 7 with NTFS or exFAT formatting, the maximum cache size is 32 GB per device. Windows Vista allows only one device to be used, while Windows 7 allows multiple caches, one per device, up to a total of 256 GB.\n\nReadyBoost compresses and encrypts all data that is placed on the flash device with AES-128; Microsoft has stated that a 2:1 compression ratio is typical, so a 4 GB cache would usually contain 8 GB of data.\n\nFor a device to be compatible and useful, it must conform to these requirements:\n\nOther considerations:\n\nReadyBoost is not available on Windows Server 2008.\n\nAccording to Jim Allchin, for future releases of Windows, ReadyBoost will be able to use spare RAM on other networked Windows PCs.\n\nA system with 512 MB of RAM (the minimum requirement for Windows Vista) can see significant gains from ReadyBoost. In one test case, adding 1 GB of ReadyBoost memory sped up an operation from 11.7 seconds to 2 seconds. However, increasing the physical memory (RAM) from 512 MB to 1 GB (without ReadyBoost) reduced it to 0.8 seconds. System performance with ReadyBoost can be monitored by Windows Performance Monitor. As the price of RAM decreased and more RAM was installed in computers, the mitigations provided by ReadyBoost to systems with insufficient memory decreased.\n\nThe core idea of ReadyBoost is that a flash memory (e.g. a USB flash drive or an SSD) have a much faster seek time than a typical magnetic hard disk (less than 1 ms), allowing it to satisfy requests faster than reading files from the hard disk. It also leverages the inherent advantage of two parallel sources from which to read data, whereas Windows 7 enables the use of up to eight flash drives at once, allowing up to nine parallel sources. USB 2.0 flash drives are slower for \"sequential\" reads and writes than modern desktop hard drives. Desktop hard drives can sustain anywhere from 2 to 10 times the transfer speed of USB 2.0 flash drives but are equal to or slower than USB 3.0 and Firewire (IEEE 1394) for sequential data. USB 2.0 and faster flash drives have faster \"random access\" times: typically around 1 ms, compared to 12 ms for mainstream desktop hard drives.\nOn laptop computers, the performance shifts more in favor of flash memory when laptop memory is more expensive than desktop memory; many laptops also have relatively slow 4200 rpm and 5400 rpm hard drives.\n\nIn versions of Vista prior to SP1, ReadyBoost failed to recognize its cache data upon resume from sleep, and restarted the caching process, making ReadyBoost ineffective on machines undergoing frequent sleep/wake cycles. This problem was fixed in Vista SP1.\n\nSince flash drives wear out after a finite (though very large) number of writes, ReadyBoost will eventually wear out the drive it uses. According to the Microsoft Windows Client Performance group, the drive should be able to operate for at least ten years.\n\n\n"}
{"id": "43473086", "url": "https://en.wikipedia.org/wiki?curid=43473086", "title": "Reformism", "text": "Reformism\n\nReformism is a political doctrine advocating the reform of an existing system or institution instead of its abolition and replacement. Within the socialist movement, reformism is the view that gradual changes through existing institutions can eventually lead to fundamental changes in a society’s political and economic systems. Reformism as a political tendency and hypothesis of social change grew out of opposition to revolutionary socialism, which contends that revolutionary upheaval is a necessary precondition for the structural changes necessary to transform a capitalist system to a qualitatively different socialist economic system.\n\nAs a doctrine, reformism is distinguished from the act of pragmatic reform: pragmatic reform aims to safeguard and permeate the status quo by preventing fundamental structural changes to it, whereas reformism posits that an accumulation of reforms can eventually lead to the emergence of entirely different economic and political systems than those of present-day capitalism and democracy.\n\nThere are two types of reformism. One has no intention of bringing about socialism or fundamental economic change to society and is used to oppose such structural changes. The other is based on the assumption that while reforms are not socialist in themselves, they can help rally supporters to the cause of revolution by popularizing the cause of socialism to the working class.\nThe debate on the ability for social democratic reformism to lead to a socialist transformation of society is over a century old.\n\nReformism is criticized for being paradoxical: it seeks to overcome the existing economic system of capitalism, but it tries to improve the conditions of capitalism thereby making it appear more tolerable to society. According to Rosa Luxemburg, under reformism, \"(capitalism) is not overthrown, but is on the contrary strengthened by the development of social reforms.\" In a similar vein, Stan Parker of the Socialist Party of Great Britain argues that reforms are a diversion of energy for socialists and are limited because they must adhere to the logic of capitalism.\n\nThe French social theorist Andre Gorz criticized reformism by advocating a third alternative to reformism and social revolution that he called \"non-reformist reforms\", specifically focused on structural changes to capitalism, as opposed to reforms for improve living conditions within capitalism or to prop it up through economic interventions.\n\nIn 1875 German Social Democratic Party (SPD) adopted a Gotha Program that proposed \"every lawful means\" on a way to a \"socialist society\" and was criticized by Karl Marx who considered the communist revolution a required step. One of the delegates to the SPD congress, Eduard Bernstein, expanded on the concept, proposing \"evolutionary socialism\". Bernstein was a leading social democrat in Germany. Reformism was quickly targeted by revolutionary socialists, with Rosa Luxemburg condemning Bernstein's \"Evolutionary Socialism\" in her 1900 essay \"Reform or Revolution?\". While Luxemburg died in the German Revolution, the reformists soon found themselves contending with the Bolsheviks and their satellite communist parties for the support of intellectuals and the working class.\n\nIn 1959, the Godesberg Program (signed at a party convention in the West German capital of Bad Godesberg) marked the shift of the Social Democratic Party of Germany (SPD) from a Marxist program espousing an end to capitalism to a reformist one focused on social reform.\n\nAfter Joseph Stalin consolidated power in the Soviet Union, the Comintern launched a campaign against the Reformist movement by denouncing them as \"social fascists\". According to \"The God that Failed\" by Arthur Koestler, a former member of the Communist Party of Germany, the largest communist party in Western Europe in the Interwar period, communists, aligned with the Soviet Union, continued to consider the \"social fascist\" Social Democratic Party of Germany to be the real enemy in Germany, even after the Nazi Party had gotten into power.\n\nIn modern times, reformists are seen as centre-left. Some social democratic parties, such as the Canadian New Democratic Party and the Social Democratic Party of Germany, are still considered to be reformist.\n\nThe term was applied to elements within the British Labour Party in the 1950s and subsequently, on the party's right. Anthony Crosland wrote \"The Future of Socialism\" (1956) as a personal manifesto arguing for a reformulation of the term. For Crosland, the relevance of nationalization (or public ownership) for socialists was much reduced as a consequence of contemporary full employment, Keynesian management of the economy and reduced capitalist exploitation. In 1960, after the third successive defeat of his party in the 1959 General Election Hugh Gaitskell attempted to reformulate the original wording of Clause IV in the party's constitution, but proved unsuccessful.\n\nSome of the younger followers of Gaitskell, principally Roy Jenkins, Bill Rodgers and Shirley Williams left the Labour Party in 1981 to found the Social Democratic Party, but the central objective of the Gaitskellites was eventually achieved by Tony Blair in his successful attempt to rewrite Clause IV in 1995.\n\nThe use of the term is distinguished from the gradualism associated with Fabianism (the ideology of the Fabian Society), which itself should not be seen as being in parallel with the revisionism associated with Bernstein and the Social Democratic Party of Germany, as originally the Fabians had explicitly rejected Marxism.\n\n\n"}
{"id": "487247", "url": "https://en.wikipedia.org/wiki?curid=487247", "title": "Reuse of bottles", "text": "Reuse of bottles\n\nA reusable bottle is a bottle that can be reused, as in the case as by the original bottler or by end-use consumers. Reusable bottles have grown in popularity by consumers for both environmental and health safety reasons. Reusable bottles are one example of reusable packaging.\n\nEarly glass bottles were often reused, such as for milk, water, beer, soft drinks, yogurt, and other uses. Mason jars, for example, were developed and reused for home canning purposes.\n\nWith returnable bottles, a retailer would often collect empty bottles or would accept empty bottles returned by customers. Bottles would be stored and returned to the bottler in reusable cases or crates. Some regions have a container deposit which is refunded after returning the bottle to the retailer. At the bottler, the bottles would be inspected for damage, cleaned, sanitized, and refilled.\n\nMore recently, many bottles have been designed for single-use. This often allows for thinner glass bottles and less expensive plastic bottles and aluminum beverage cans. On a cost basis, the decision has often been made for non-returnable bottles.\n\nThe reuse of containers is often thought of as being a step toward more sustainable packaging. Reuse sits high on the waste hierarchy. When a container is used multiple times, the material required per use or per filling cycle is reduced.\n\nMany potential factors are involved in environmental comparisons of returnable vs. non-returnable systems. Researchers have often used life cycle analysis methodologies to balance the many diverse considerations. Some comparisons show no clear winner but rather show a realistic view of a complex subject.\n\nArguments in favor of reusing bottles, or recycling them into other products, are compelling. It is estimated that in the U.S. alone, consumers use 1,500 plastic water bottles every single second. But only about 23% of PET plastic, which is the plastic used in disposable plastic water bottles, gets recycled. Thus, about 38 billion water bottles are thrown away annually, equating to roughly $1 billion worth of plastic. The average American spends $242 per year per person on disposable, single use plastic water bottles. The environmental and cost consequences associated with disposable plastic water bottles are a strong argument for reusing bottles.\n\nReusable drinking bottles for water, coffee, salad dressing, soup, baby formula, and other beverages have gained in popularity by consumers in recent years, undoubtedly due to the costs and environmental problems associated with single use plastic water bottles and those used for other beverages. Common materials used to make reusable drinking bottles include glass, aluminum, stainless steel, and plastic. Reusable bottles include both single and double wall insulated bottles. Some baby bottles have an inner bag or bladder that can be replaced after each use.\n\nReusable bottles can hold bacteria. Drinking from a reusable bottle can transfer bacteria from a person's mouth to the beverage it contains, which can contaminate both bottle and water. Contamination can cause bacterial or fungal growth in the liquid while it's stored. It is recommend that users clean reusable drinking bottles thoroughly before each used. Users should take care to wash the bottle cap as well after each use for proper sanitation.\n\nSome experts state that there's generally no harm in reusing your own drinking bottle, but the risk for ingesting harmful bacteria increases if you share your bottle with others. University of Nebraska Medical Center Microbiologist Pete Iwen, Ph.D., says, “If it’s my bottle, my germs, I probably would not be all that paranoid about reusing the bottle. The main issue occurs when sharing bottles. Microbes present in my mouth may be harmful to others.” \n\nPlastic drinking bottles often contain the chemical Bisphenol A (BPA), which is made from polycarbonate and which shares resin identification code 7 with other plastics. Another chemical found in plastic drinking bottles is phthalate. Both of these chemicals are controversial because they are known endocrine disruptors, which can interfere with the body's hormonal system.\n\nA study by the Harvard School of Public Health (HSPH) found that participants who drank from polycarbonate bottles – which is the plastic commonly used in disposable plastic water bottles, other plastic drinking bottles, and baby bottles – for just one week showed a two-thirds increase in their urine of the chemical BPA. Exposure to BPA has been shown to interfere with reproductive development in animals and has been linked with cardiovascular disease and diabetes in humans. The study is the first to show that drinking from polycarbonate bottles increased the level of urinary BPA, and thus suggests that drinking containers made with BPA release the chemical into the liquid they contain, which people then consume. The amount of BPA consumed as a result of drinking from plastic bottles was enough to increase the level of BPA excreted in the urine of the people who drank from those containers.\n\nOther studies have shown that even BPA-free plastic bottles leach harmful chemicals into the liquids they contain, making the argument that the safest reusable drinking bottles are those made with either glass or stainless steel. Glass is non-toxic and inert, so it does not leach into the liquid that it contains, and stainless steel is one of the most inert metals, therefore it has a lower likelihood of leaching chemicals into the beverages it contains.\n\nSeveral countries have banned the use of plastics containing BPA used for water and other food items. Leaching of phthalates from PVC (resin identification code 3) is also a concern, but PVC is no longer used for water bottles.\n\nAnother common material used to make reusable drinking bottles is aluminum. However, aluminum also has health safety concerns. A study by the U.S. National Library of Medicine, National Institutes of Health found that aluminum levels were over 20-times higher in the elderly than in middle aged people. The study cited a correlation between aluminum levels and “densities of senile plaques and neurofibrillary tangles.” Furthermore, it found that lowering the amount of brain aluminum by using chelation was shown to slow the progression of Alzheimer’s Disease. This is an argument in favor or lessening the amount of aluminum that the body is exposed to, hence making it a less favorable material for reusable drinking bottles.\n\nA university student's master's thesis incorrectly suggested that repeatedly rewashing plastic water bottles can lead to the leaking diethylhydroxylamine (DEHA) into the drinking water, and can be detrimental to human health. The results of this research were repeated by various sources and also became a chain email, later declared to be a hoax.\n\nThe American Cancer Society and Cancer Research UK have stated that DEHA is not present in plastic water bottles; even if it were, it is not a known carcinogen.\n\n\n"}
{"id": "20775995", "url": "https://en.wikipedia.org/wiki?curid=20775995", "title": "School Day of Non-violence and Peace", "text": "School Day of Non-violence and Peace\n\nThe School Day of Non-violence and Peace (or DENIP, acronym from Catalan-Balearic: \"Dia Escolar de la No-violència i la Pau\"), is an observance founded by the Spanish poet \"Llorenç Vidal Vidal\" in Majorca in 1964 as a starting point and support for a pacifying and non-violent education of a permanent character. Different as the first proposed by the UNESCO \"Armistice Day\" in 1948, the \"School Day of Non-violence and Peace\" (DENIP) is observed on January 30 or thereabouts every year, on the anniversary of the death of Mahatma Gandhi, in schools all over the world. In countries with a Southern Hemisphere school calendar, it can be observed on 30 March. Its basic and permanent message is: \"Universal love, non-violence and peace. Universal love is better than egoism, non-violence is better than violence, and peace is better than war\". In Navarra the slogan for the 2009 was \"above all, we are friends\" (Spanish:\"Por encima de todo, somos amigos\").\nDENIP and World Association of Early Childhood Educators (AMEI-WAECE) collaborate to celebrate this event in the schools of the latter worldwide.\nFormer Director-General of UNESCO, Federico Mayor Zaragoza, has been promoting during decades the School Day of Non-violence and Peace, saying in Global Education Magazine: \"We can not achieve a sustainable development without a culture of peace\". \n\n\n"}
{"id": "5529638", "url": "https://en.wikipedia.org/wiki?curid=5529638", "title": "Stress–energy–momentum pseudotensor", "text": "Stress–energy–momentum pseudotensor\n\nIn the theory of general relativity, a stress–energy–momentum pseudotensor, such as the Landau–Lifshitz pseudotensor, is an extension of the non-gravitational stress–energy tensor which incorporates the energy–momentum of gravity. It allows the energy–momentum of a system of gravitating matter to be defined. In particular it allows the total of matter plus the gravitating energy–momentum to form a conserved current within the framework of general relativity, so that the \"total\" energy–momentum crossing the hypersurface (3-dimensional boundary) of \"any\" compact space–time hypervolume (4-dimensional submanifold) vanishes.\n\nSome people (such as Erwin Schrödinger) have objected to this derivation on the grounds that pseudotensors are inappropriate objects in general relativity, but the conservation law only requires the use of the 4-divergence of a pseudotensor which is, in this case, a tensor (which also vanishes). Also, most pseudotensors are sections of jet bundles, which are now recognized as perfectly valid objects in GR.\n\nThe use of the Landau–Lifshitz pseudotensor, a stress–energy–momentum pseudotensor for combined matter (including photons and neutrinos) plus gravity, allows the energy–momentum conservation laws to be extended into general relativity. Subtraction of the matter stress–energy–momentum tensor from the combined pseudotensor results in the gravitational stress–energy–momentum pseudotensor.\n\nLandau and Lifshitz were led by four requirements in their search for a gravitational energy momentum pseudotensor, formula_1:\n\nLandau & Lifshitz showed that there is a unique construction that satisfies these requirements, namely\n\nwhere:\n\n\nExamining the 4 requirement conditions we can see that the first 3 are relatively easy to demonstrate:\n\nWhen the Landau–Lifshitz pseudotensor was formulated it was commonly assumed that the cosmological constant, formula_17, was zero. Nowadays we don't make that assumption, and the expression needs the addition of a formula_17 term, giving:\nThis is necessary for consistency with the Einstein field equations.\n\nLandau & Lifshitz also provide two equivalent but longer expressions for the Landau–Lifshitz pseudotensor:\n\nThis definition of energy–momentum is covariantly applicable not just under Lorentz transformations, but also under general coordinate transformations.\n\nThis pseudotensor was originally developed by Albert Einstein.\n\nPaul Dirac showed that the mixed Einstein pseudotensor \n\nsatisfies a conservation law\n\nClearly this pseudotensor for gravitational stress–energy is constructed exclusively from the metric tensor and its first derivatives. Consequently, it vanishes at any event when the coordinate system is chosen to make the first derivatives of the metric vanish because each term in the pseudotensor is quadratic in the first derivatives of the metric. However it is not symmetric, and is therefore not suitable as a basis for defining the angular momentum.\n\n\n"}
{"id": "14589960", "url": "https://en.wikipedia.org/wiki?curid=14589960", "title": "Sustainable product development", "text": "Sustainable product development\n\nSustainable product development (SPD) is a method for product development that incorporates a Framework for Strategic Sustainable Development (FSSD), also known as The Natural Step (TNS) framework. As the demand for products continues to increase around the world and environmental factors like climate change increasingly affect policies - and thus business - it becomes more and more of a competitive advantage for businesses to consider sustainability aspects early on in the product development process.\n\nSPD is not limited to the actual product development, but also the product design. Green design which is a part of SPD has two main goals: the prevention of waste and to minimize environmental impact. Environmental impact involves: deforestation, greenhouse gas emissions, and resource/material management, etc. The early stages of design tend to be the areas that effect the environment the worst, the extraction and refining.\n\n\n\n"}
{"id": "4140063", "url": "https://en.wikipedia.org/wiki?curid=4140063", "title": "Symbolic power", "text": "Symbolic power\n\nThe concept of symbolic power was first introduced by French sociologist Pierre Bourdieu to account for the tacit, almost unconscious modes of cultural/social domination occurring within the everyday social habits maintained over conscious subjects. Symbolic power accounts for discipline used against another to confirm that individual's placement in a social hierarchy, at times in individual relations but most basically through system institutions, in particular education.\n\nAlso referred to as \"soft power\", symbolic power includes actions that have discriminatory or injurious meaning or implications, such as gender dominance and racism. Symbolic power maintains its effect through the mis-recognition of power relations situated in the social matrix of a given field. While symbolic power requires a dominator, it also requires the dominated to accept their position in the exchange of social value that occurs between them.\n\nThe concept of symbolic power may be seen as grounded in Friedrich Engels' concept of false consciousness. To Engels, under capitalism, objects and social relationships themselves are embedded with societal value that is dependent upon the actors who engage in interactions themselves. Without the illusion of natural law governing such transactions of social and physical worth, the proletariat would be unwilling to consciously support social relations that counteract their own interests. Dominant actors in a society must consciously accept that such an ideological order exists for unequal social relationships to take place. Louis Althusser further developed it in his writing on what he called Ideological State Apparatuses, arguing that the latter's power is partly based on symbolic repression.\n\nThe concept of symbolic power was first introduced by Pierre Bourdieu in \"La Distinction\". Bourdieu suggested that cultural roles are more dominant than economic forces in determining how hierarchies of power are situated and reproduced across societies. Status and economic capital are both necessary to maintain dominance in a system, rather than just ownership over the means of production alone. The idea that one could possess symbolic capital in addition and set apart from financial capital played a critical role in Bourdieu's analysis of hierarchies of power.\n\nFor example, in the process of reciprocal gift exchange in the Kabyle society of Algeria, where there is asymmetry in wealth between the two parties the better endowed giver \"can impose a strict relation of hierarchy and debt upon the receiver.\" Symbolic power, therefore, is fundamentally the imposition of categories of thought and perception upon dominated social agents who, once they begin observing and evaluating the world in terms of those categories—and without necessarily being aware of the change in their perspective—then perceive the existing social order as just. This, in turn, perpetuates a social structure favored by and serving the interests of those agents who are already dominant. Symbolic power, if real, is in some senses much more powerful than physical violence in that it is embedded in the very modes of action and structures of cognition of individuals, and imposes the specter of legitimacy of the social order.\n\n"}
{"id": "767637", "url": "https://en.wikipedia.org/wiki?curid=767637", "title": "System F", "text": "System F\n\nSystem F, also known as the (Girard–Reynolds) polymorphic lambda calculus or the second-order lambda calculus, is a typed lambda calculus that differs from the simply typed lambda calculus by the introduction of a mechanism of universal quantification over types. System F thus formalizes the notion of parametric polymorphism in programming languages, and forms a theoretical basis for languages such as Haskell and ML. System F was discovered independently by logician Jean-Yves Girard (1972) and computer scientist John C. Reynolds (1974).\n\nWhereas simply typed lambda calculus has variables ranging over functions, and binders for them, System F additionally has variables ranging over \"types\", and binders for them. As an example, the fact that the identity function can have any type of the form A→ A would be formalized in System F as the judgment\n\nwhere formula_2 is a type variable. The upper-case formula_3 is traditionally used to denote type-level functions, as opposed to the lower-case formula_4 which is used for value-level functions. (The superscripted formula_2 means that the bound \"x\" is of type formula_2; the expression after the colon is the type of the lambda expression preceding it.)\n\nAs a term rewriting system, System F is strongly normalizing. However, type inference in System F (without explicit type annotations) is undecidable. Under the Curry–Howard isomorphism, System F corresponds to the fragment of second-order intuitionistic logic that uses only universal quantification. System F can be seen as part of the lambda cube, together with even more expressive typed lambda calculi, including those with dependent types.\n\nAccording to Girard, the \"F\" in \"System F\" was picked by chance.\n\nThe formula_7 type is defined as:\nformula_8, where formula_9 is a type variable. This means: formula_7 is the type of all functions which take as input a type α and two expressions of type α, and produce as output an expression of type α (note that we consider formula_11 to be right-associative.)\n\nThe following two definitions for the boolean values formula_12 and formula_13 are used, extending the definition of Church booleans:\n\nThen, with these two formula_21-terms, we can define some logic operators (which are of type formula_22):\n\nAs in Church encodings, there is no need for an IFTHENELSE function as one can just use raw formula_17-typed terms as decision functions. However, if one is requested:\nwill do.\nA \"predicate\" is a function which returns a formula_17-typed value. The most fundamental predicate is ISZERO which returns formula_12 if and only if its argument is the Church numeral 0:\n\nSystem F allows recursive constructions to be embedded in a natural manner, related to that in Martin-Löf's type theory. Abstract structures (S) are created using \"constructors\". These are functions typed as:\n\nRecursivity is manifested when formula_30 itself appears within one of the types formula_31. If you have formula_32 of these constructors, you can define the type of formula_30 as:\n\nFor instance, the natural numbers can be defined as an inductive datatype formula_35 with constructors\nThe System F type corresponding to this structure is\nformula_38. The terms of this type comprise a typed version of the Church numerals, the first few of which are:\n\nIf we reverse the order of the curried arguments (\"i.e.,\" formula_43), then the Church numeral for formula_44 is a function that takes a function \"f\" as argument and returns the formula_44 power of \"f\". That is to say, a Church numeral is a higher-order function – it takes a single-argument function \"f\", and returns another single-argument function.\n\nThe version of System F used in this article is as an explicitly typed, or Church-style, calculus. The typing information contained in λ-terms makes type-checking straightforward. Joe Wells (1994) settled an \"embarrassing open problem\" by proving that type checking is undecidable for a Curry-style variant of System F, that is, one that lacks explicit typing annotations.\n\nWells's result implies that type inference for System F is impossible.\nA restriction of System F known as \"Hindley–Milner\", or simply \"HM\", does have an easy type inference algorithm and is used for many statically typed functional programming languages such as Haskell 98 and ML. Over time, as the restrictions of HM-style type systems have become apparent, languages have steadily moved to more expressive logics for their type systems. As of 2008, GHC, a Haskell compiler, goes beyond HM, and now uses System F extended with non-syntactic type equality.\nF# is designed from scratch with System F in mind.\n\nWhile System F corresponds to the first axis of Barendregt's lambda cube, System F or the higher-order polymorphic lambda calculus combines the first axis (polymorphism) with the second axis (type operators); it is a different, more complex system.\n\nSystem F can be defined inductively on a family of systems, where induction is based on the kinds permitted in each system:\n\n\nIn the limit, we can define system formula_51 to be\n\n\nThat is, F is the system which allows functions from types to types where the argument (and result) may be of any order.\n\nNote that although F places no restrictions on the \"order\" of the arguments in these mappings, it does restrict the \"universe\" of the arguments for these mappings: they must be types rather than values. System F does not permit mappings from values to types (Dependent types), though it does permit mappings from values to values (formula_4 abstraction), mappings from types to values (formula_3 abstraction, sometimes written formula_55) and mappings from types to types (formula_4 abstraction at the level of types)\n\n\n\n\n"}
{"id": "30334805", "url": "https://en.wikipedia.org/wiki?curid=30334805", "title": "Systemic development", "text": "Systemic development\n\nSystemic development is a process of thinking about development which uses a systems thinking approach to create a solution to a social, environmental, and economic stress which the world is suffering from.\n\nThe core approach of systemic development is a process for thinking holistically while addressing complex issues and progressing towards a mutual goal with high participation rates. The process encompasses comprehension of current activities and future needs from a holistic perspective. For success, it is essential that the process moves from an integrated assessment to a sustainable assessment. The perspective must consider the many facets of the current and proposed development including the economic, social, environmental, political and ecological aspects. The idea behind a systemic development approach can be applied to many disciplines, similar to sustainable development. Systemic development is practice rather than sustainability, which is an end state.\n\nGeneral Systems Theory (GST) laid the foundation to systemic thinking. Ludwig Von Bertalanffy was known as the founder of the original principles of GST. Prior to 1968, when GST was introduced in Bertalanffy’s book, \"General System Theory: Foundations, Development, Applications\", the traditional approach to development used linear thinking or cause-and-effect thinking. GST was influenced by many different types of theories such as “Chaos theory, complexity theory, catastrophe theory, cybernetics, fuzzy set theory, and learning matrices” In 1990, Peter Senge, author of \"The Fifth Discipline\", wove systemic thinking approach into development.\n\nThe integration of ontology, methodology, epistemology, and axiology has formed an outline for GST. Together, these concepts and philosophies contribute to the theory of systemic development.\n\nOntology is the most basic fundamental tool. In a systemic development approach, the first step is to define the boundaries, thresholds, and stakeholders. Understanding what is, is crucial to understand the layers of complexity of the development needs.\n\nMethodology in systemic development must consider all variables, values, and sustainability principles, and aim to ensure that no elements have been neglected. It is important to ensure completeness, comprehensiveness, and transparency of the assessment. Mutual feedback and interactions between stakeholders should be modeled and assessed using carefully designed specific methodologies.\n\nEpistemology stresses how dialogue and communication by the stakeholders are the key tools to systemic development. This is when the verification and confirmation of the facets of the development process are discovered. Insight from each stakeholder is important to enhance and broaden the perspective of all involved. It is essential to take these viewpoints into consideration because they influence the process, the trends, the drivers of change, and the interactions between the parts. It is during this phase that learning is accomplished.\n\nAxiologology emphasizes the ethical and aesthetic responsibility during the development process. It is important because the solution manifested by the systemic development process needs to represent the values of the represented stakeholders.\n\nTo successfully achieve development through a systems approach, holistic thinking is necessary. A holistic approach to a system thinks about each variable, the space between the variables and what defines the variable. “It’s the sum of the interaction of its parts” In this process each individual must learn from each other to understand the whole system in a multidimensional way to find a solution. To think about development with a systemic lens, one needs to be able to see the whole instead of parts and understand the relationship between the parts, the way the parts move, what drives the behavior of the parts, what influences the flow or direction, and to understand why there are no more or no fewer parts. The many factors that make up the whole can be a complex system.\n\nIncluding many diverse stakeholders helps each individual to grow their own perspective, gain an understanding of others and to increase their creativity. Systemic involvement must strive for a transdisciplinary approach instead of a multidisciplinary or interdisciplinary approach to achieve successful development. Transdisciplinary allows for the integration of methodologies and epistemologies through collaboration of the different stakeholders. Including more perspectives in the loop will increase the chance of a successful solution.\n\nThe foundation of systemic development, systems thinking, when applied, creates knowledge which leads to expanded knowledge, which leads to success. Dialog is an essential tool for sharing knowledge and translating it into action. Communication can provide feedback and insight about “system, culture, practices, and artifacts and about the objectives and values of the project sponsor, client, and other stakeholders.” Learning happens during the dialog process as each stakeholder comes to observe other stakeholder’s values and ideas about what successful development would encompass.\n\n\"The Fifth Element\", by Peter Senge, emphasizes the importance of learning to improve lives using systems thinking. Individuals who acquire information, knowledge, and skills from other stakeholders and the environment tend to experience a change in their own lives and livelihood. They themselves then become the agents of change by sharing their knowledge with others.\n\nA feedback loop, closed loop, or systems complex model is a tool to help cope with complexity and understand the system as a whole. The tool will help to visualize the direction, velocity, delay time, long term and short term effects, and to help see the dynamic process. Feedback is necessary to learn about each other, including objectives and values of stakeholders and officials. A systemic approach to development is change oriented. The approach must encourage humans to communicate through interpersonal interactions, address the values of each stakeholder, and take part in developing and understanding the complex loop.\n\nSystemic development is based on the principle that one must understand the complexity of the whole system to develop a solution. This can be accomplished by learning different elements in the system and applying them to their existing knowledge. We must learn about the past suggested solutions, the balance of the system, influx of the system, the challenges within the system, the best timing for each element, learning about uncertainty of cause-and-effect, the best leverage points in the system, the system does not work unless every piece is functioning at its prime, and there is no one individual at fault if everyone is working together.\n\nThe goal of systemic development is to have community participants shift from being reactors to viewing themselves “as active participants in shaping their reality [to move] from reacting to the present to creating the future”\n\nThe non-linear processes must coincide, be fluid and strive to benefit all parties involved.\n\n"}
{"id": "5168910", "url": "https://en.wikipedia.org/wiki?curid=5168910", "title": "Third Party Non-violent Intervention", "text": "Third Party Non-violent Intervention\n\nThird Party Non-violent Intervention (sometimes called TPNI) refers to the practice of intervening from the outside in violent conflicts with the aim of reducing violence and allowing \"space\" for conflict resolution. Two common forms of intervention are as an intermediary in a negotiating capacity or, physically, by interposing one's body between two factions.\n\nTPNI work is sometimes categorized into four main areas:\n\nExamples of groups that espouse this practice are Christian Peacemaker Teams, International Solidarity Movement, Muslim Peacemaker Teams, Meta Peace Team, Peace Brigades International and Nonviolent Peaceforce.\n\n"}
{"id": "5475447", "url": "https://en.wikipedia.org/wiki?curid=5475447", "title": "True20", "text": "True20\n\nTrue20 is a role-playing game system designed by Steve Kenson and published by Green Ronin Publishing. The system was first published as a part of the \"Blue Rose\" RPG before being published as a standalone universal generic role-playing game, True20 Adventure Roleplaying.\n\nThe True20 system was originally used in Green Ronin's award-winning \"Blue Rose\", itself based on their multiple-award winning \"Mutants & Masterminds\" RPG. Later that year, Green Ronin released a PDF distillation of the \"Blue Rose\" rules, with an appendix of some modern-era rules, as a generic form of the game. This was followed by an expanded hardcover release in 2006. A revised softcover rulebook, combining the rules section of the \"True20 Adventure Roleplaying\" book with the \"True20 Companion\" was released April 25, 2008.\n\nThe original setting for the system was the Blue Rose in which the system first saw print. In its generic role-playing game, the original hardcover printing of the \"True20 Adventure Roleplaying\" book included four sample settings. These were chosen among publisher submitted setting with the winners announced in Dragon Magazine:\n\nThe Revised Edition has four different settings:\n\nA follow up volume, \"True20 Worlds of Adventure\" includes five additional settings:\n\nUtilizing the Open Gaming License, \"True20\" is derived from Wizards of the Coast's \"d20 System\". Differences from the parent game include the following:\n\nSince before its release, the True20 system has been open to users under the terms of the Open Gaming License. To use the True20 logo though required a separate license and license fee purchased from Green Ronin. Several companies have taken advantage of this to produce their own True20 titles. On April 12, 2008, Chris Pramas of Green Ronin Publishing announced a new, free licensing agreement with third-party publishers to produce True20 products. Details were posted on the company’s website and forums, and met with praise from publishers, freelancers and players alike.\n\n\n\n"}
{"id": "29776453", "url": "https://en.wikipedia.org/wiki?curid=29776453", "title": "Wai Hnin Pwint Thon", "text": "Wai Hnin Pwint Thon\n\nWai Hnin Pwint Thon (; born January 4, 1989) is a Burmese activist based in London. Wai Hnin was born in Rangoon, Burma (Myanmar) and is the daughter of Mya Aye (Burmese:မြအေး), one of the leaders of the 88 Generation student group in Burma.\n\nThroughout her life, Wai Hnin's father was imprisoned for political involvement, meeting him for the first time through bars. She left Burma in 2006 to study in London. Shortly after arriving in London, her father was arrested for taking part in events leading up to the \"Saffron Revolution\" in Burma and sentenced to 65.5 years in prison. Since that time Wai Hnin has sought to raise her father's detention and the plight of the more than 2,200 other political prisoners currently held in detention in Burma. Wai Hnin has worked for Burma Campaign UK and Amnesty International and has frequently appeared in British and international media. In 2010, after meeting Wai Hnin, former British Foreign Secretary, David Miliband, remarked that he was \"struck first by her bravery and strength in leaving her family for a new life to study here in Britain, and secondly by her determination to bring about change in Burma.\" She has been studying Global politics and International Relations in Birkbeck, University of London since 2013 and in 2016, she got a Bachelor Degree .\n\n"}
{"id": "523061", "url": "https://en.wikipedia.org/wiki?curid=523061", "title": "White feather", "text": "White feather\n\nThe white feather is a widely recognised symbol, although its meaning varies significantly between different countries and cultures. In the United Kingdom and the countries of the British Empire since the eighteenth century it has been used a symbol of cowardice, used by patriotic groups, including prominent members of the Suffragette movement and early feminists, in order to shame men into enlisting. However in some cases of pacifism and in the United States armed forces, it is used to signify extraordinary bravery and excellence in combat marksmanship.\n\nAs a symbol of cowardice, the white feather supposedly comes from cockfighting and the belief that a cockerel sporting a white feather in its tail is likely to be a poor fighter. Pure-breed gamecocks do not show white feathers, so its presence indicates that the cockerel is an inferior cross-breed.\n\nShame was exerted upon men in England and France who had not taken the cross at the time of the Third Crusade, \"A great many men sent each other wool and distaff, hinting that if anyone failed to join this military undertaking they were only fit for women's work\".\n\nIn August 1914, at the start of the First World War, Admiral Charles Fitzgerald founded the Order of the White Feather with support from the prominent author Mrs Humphrey Ward. The organization aimed to shame men into enlisting in the British army by persuading women to present them with a white feather if they were not wearing a uniform.\n\nThis was joined by prominent feminists and suffragettes of the time, such as Emmeline Pankhurst and her daughter Christabel. They, in addition to handing out the feathers, also lobbied to institute an involuntary universal draft, which included those who lacked votes due to being too young or not owning property. Although the draft would conscript both sexes, only males would be on the front lines.\n\nWhile the true effectiveness of the campaign is impossible to judge, it did spread throughout several other nations in the Empire. In Britain it started to cause problems for the government when public servants and men in essential occupations came under pressure to enlist. This prompted the Home Secretary, Reginald McKenna, to issue employees in state industries with lapel badges reading \"King and Country\" to indicate that they too were serving the war effort. Likewise, the Silver War Badge, given to service personnel who had been honourably discharged due to wounds or sickness, was first issued in September 1916 to prevent veterans from being challenged for not wearing uniform. Anecdotes from the period indicates that the campaign was not popular amongst soldiers - not least because soldiers who were home on leave could find themselves presented with the feathers.\n\nOne such was Private Ernest Atkins who was on leave from the Western Front. He was riding a tram when he was presented with a white feather by a girl sitting behind him. He smacked her across the face with his pay book saying: \"Certainly I'll take your feather back to the boys at Passchendaele. I'm in civvies because people think my uniform might be lousy, but if I had it on I wouldn't be half as lousy as you.\" \n\nPrivate Norman Demuth, who had been discharged from the British Army after being wounded in 1916, received numerous white feathers after returning from the Western Front, and decided that if the women that handed them out were going to be rude to him, he was going to be rude back. One of the last feathers he received was presented to him whilst he was travelling on a bus, by a lady who was sat opposite him. She handed over the feather and said, \"Here's a gift for a brave soldier.\" Demuth replied, \"Thank you very much - I wanted one of those.\" He then used the feather to clean out his pipe, handed it back to her and remarked, \"You know we didn't get these in the trenches.\" The other passengers subsequently became angry with the woman and started shouting at her, much to Demuth's amusement.\n\nThe supporters of the campaign were not easily put off. A woman who confronted a young man in a London park demanded to know why he was not in the army. \"Because I am a German\", he replied. He received a white feather anyway.\n\nPerhaps the most misplaced use of a white feather was when one was presented to Seaman George Samson who was on his way in civilian clothes to a public reception in his honour. Samson had been awarded the Victoria Cross for gallantry in the Gallipoli campaign.\n\nRoland Gwynne, later mayor of Eastbourne (1929–1931) and lover of suspected serial killer John Bodkin Adams, received a feather from a relative. This prompted him to enlist, and he subsequently received the Distinguished Service Order for bravery. The writer Compton Mackenzie, then a serving soldier, complained about the activities of the Order of the White Feather. He argued that these \"idiotic young women were using white feathers to get rid of boyfriends of whom they were tired\". The pacifist Fenner Brockway claimed that he received so many white feathers he had enough to make a fan.\n\nThe white feather campaign was briefly renewed during World War II.\n\nIn contrast, the white feather has been used by some pacifist organisations as a sign of harmlessness.\n\nIn the 1870s, the Māori prophet of passive resistance Te Whiti o Rongomai promoted the wearing of white feathers by his followers at Parihaka. They are still worn by the iwi associated with that area, and by Te Ati Awa in Wellington. They are known as \"te raukura\", which literally means the red feather, but metaphorically, the chiefly feather. They are usually three in number, interpreted as standing for \"glory to God, peace on earth, goodwill toward people\" (Luke 2:14). Albatross feathers are preferred but any white feathers will do. They are usually worn in the hair or on the lapel (but not from the ear).\n\nSome time after the war, pacifists found an alternative interpretation of the white feather as a symbol of peace. The apocryphal story goes that in 1775, Quakers in a Friends meeting house in Easton, New York were faced by a tribe of Indians on the war path. Rather than flee, the Quakers fell silent and waited. The Indian chief came into the meeting house and finding no weapons he declared the Quakers as friends. On leaving he took a white feather from his quiver and attached it to the door as a sign to leave the building unharmed.\n\nIn 1937 the Peace Pledge Union sold 500 white feather badges as symbols of peace.\n\nIn the United States, the white feather has also become a symbol of courage, persistence and superior combat marksmanship. Its most notable wearer was US Marine Corps Gunnery Sergeant Carlos Hathcock, who was awarded the Silver Star medal for bravery during the Vietnam War. Hathcock picked up a white feather on a mission and wore it in his hat to taunt the enemy. He was so feared by enemy troops that they put a price on his head. Its wear on combat headgear flaunts an insultingly easy target for enemy snipers.\n\nThe adventure novel \"The Four Feathers\" (1902) by A. E. W. Mason tells the story of Harry Faversham, an officer in the British Army, who decides to resign his commission the day before his regiment is dispatched to fight in Sudan (the 1882 First War of Sudan, leading to the fall of Khartoum). Harry's three fellow officers and his fiancée conclude that he is resigning in order to avoid fighting in the conflict, and each send him a white feather. Stung by the criticism, Harry sails to Sudan, disguises himself as an Arab, and looks for the opportunity to redeem his honour. He manages this by fighting a covert war on behalf of the British, saving the life of one of his colleagues in the process. On returning to England he asks each of his accusers to take back one of the feathers.\n\nThe 1907 P. G. Wodehouse novel \"The White Feather\" is a school story about apparent cowardice and the efforts a boy goes to in order to redeem himself by physical combat.\n\nIn the novel \"Birds of a Feather\" by Jacqueline Winspear, four young girls take it upon themselves to hand out feathers to young men not in uniform in an effort to shame them into enlisting on Britain's side in The Great War.\n\n\"The Man Who Stayed at Home\", a 1914 play by J. E. Harold Terry and Lechmere Worrall, was renamed \"The White Feather\" when staged in North America. The title character is a British secret agent who is falsely perceived to be a coward for his refusal to enlist as a soldier.\n\nIn \"The Camels are Coming\" (1932), the first-ever collection of Biggles stories, Biggles is handed a white feather by a young woman while on leave in civilian clothes, who is later taken aback to find that he is one of the Royal Flying Corps' leading pilots.\n\nIn Pat Barker's 1991 novel \"Regeneration\", the character Burns is given two white feathers during his home leave from Craiglockhart War Hospital.\n\nIn the 2015 graphic novel \"\", Christabel Pankhurst is depicted encouraging women to hand white feathers to every young man they see out of uniform. Persephone Wright, the protagonist of the story and heretofore a staunch supporter of Pankhurst's Votes for Women campaigns, rejects the idea on ethical grounds, saying \"a man who's been shamed into service isn't a volunteer at all\".\n\nThe Order of the White Feather was the inspiration for the Weddings Parties Anything song \"Scorn of the Women\", which concerns a man who is deemed medically unfit for service when he attempts to enlist, and is unjustly accused of cowardice.\n\nIn 1983, new wave band Kajagoogoo released their debut album called \"White Feathers\", whose opener was the title track, a light-hearted allegory for weak people, whereas the final track, \"Frayo\", had a political flavour, referencing cowardice as the cause for an unchanging war-torn world.\n\nIn 1985, progressive rock band Marillion released an anti-war song called White Feather as the ending track to their album Misplaced Childhood.\n\nThe novel \"The Four Feathers\" has been the basis of at least seven feature films, the most recent being \"The Four Feathers\" (2002), starring Heath Ledger. It was also parodied in the \"Dad's Army\" episode \"The Two and a Half Feathers\".\n\nIn the 1980 BBC TV series \"To Serve Them All My Days\", David Powlett-Jones, a shell-shocked Tommy, takes a position in a boys' school. Suspecting that fellow teacher Carter may be avoiding war duty, he muses, \"I'd give a good deal to know whether he's really got a gammy knee\", to which an acerbic colleague responds, \"I suppose we couldn't get some chubby cherub to give him the white feather\" as a means of accusing the suspected malingerer.\n\nIn the short-lived 2007 British period drama \"Lilies\", the brother of the protagonists is discharged from the military during World War I after his boat sinks and he is one of a handful of shell shocked survivors. Billy gets sent and given numerous white feathers for his perceived cowardice, and begins to hallucinate them choking him. This is a recurring theme throughout the series.\n\nIn the 2010 Australian film \"Beneath Hill 60\" about real-life soldier Oliver Woodward, Woodward, before enlisting, receives several feathers to which he jokingly states, \"Just a few more feathers, and I'll have a whole chicken.\"\n\nIn the first episode of the second season of \"Downton Abbey\" a pair of young women interrupt a benefit concert to hand out white feathers to the men who have not enlisted, before being ordered out by an angry Earl of Grantham.\n\n"}
