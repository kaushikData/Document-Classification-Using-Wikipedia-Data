{"id": "42182927", "url": "https://en.wikipedia.org/wiki?curid=42182927", "title": "Adesha", "text": "Adesha\n\nĀdesha or Ādeśa (Sanskrit: आदेश) means 'an order', 'command' or 'advice', 'instruction', 'precept', 'rule'\n\nThe word, Ādesha appears to be a semantically polyvalent compound representing two homonymous compounds of different origin and formation; it was surmised that \"ādeśa-\" in the sense of \"substitute\" owes its origin to a combination of \"ā-\" and \"deśa-\", whereas \"ādeśa-\" in the sense of \"advice\" belongs to the verb \"ā- diś-\" \"to point out, to teach\"; it is the combination of \"ā-\" meaning 'toward', and \"diś\" meaning 'to show or direct'.\n\n\"Nirvachanashastra\" is a heremeneutic device that is used for word-analysis and derivation. The proper understanding of the relation in \"nirvachana\" analysis between a noun and some activity, expressed by a finite verb or by mention of a verbal root came to the fore through the study of Yāska's Nirukta done in the light of a model of substitution according to which model \"adesha\" ('substitution') takes the \"sthana\" ('place') of the original \"sthanin\" ('place-holder') under the given circumstances. \"Sthana\" refers to \"artha\" ('meaning'), thus \"nirvachanas\" are merely place-holders in semantic space; one gets a different thought if one gets a different sentence to represent the same truth.\n\nBy using \"Ādeśa\" rather than \"vikāra\", and holding on to \"varna-samāmnāya\" sound system, Pāṇini was able to achieve economy and brevity in the statements of algebraic system. With regard to the rule, requiring the substitution of soft unaspirate consonants in the place of hard consonants, the \"ādeśa\" that takes place in place of a vowel is not \"sthānvit\". Panini uses \"ādeśa\" in the sense of 'substitution' and 'substitute'. \"Vadha\" is not an independent root in the Paninian system it is an adesa for the root \"han-\"\n\nThe Upanishads have given four Mahāvākyas or grand proclamations corresponding to four prescribed practices viz. \"Upadesha\", \"Adesha\", \"Abhyasa\" and \"Anubhava\"; accordingly the \"vakya\" – Tat Tvam Asi, appearing in the Chandogya Upanishad, is an \"Adesha Vakya\", a command statement; the \"shishya\" or 'disciple' listens to the \"Upadesha vakya\" and the \"Adhesha vakya\" with full faith and devotion which act is called Sravana ('concentration').\n\nIn the Sanskrit phrase - अथात आदेशः नेति नेति \"athāta ādeśah: neti neti\", meaning – 'now hence the teaching: not this, not this' of Brihadaranyaka UpanishadII.ii.6, \"Adesha\" means 'specific instruction' and not 'substitute'.\n\nIn the Taittiriya Upanishad(II.iii.1), in the very brief part reading - आदेश आत्मा, the word, \"Adesha\", refers to the \"brahmana\" portion as the self ('trunk')when the sage says that the self constituted by mind is also of a human shape the shape the human shape the mental body takes after the human shape of the vital body. Sankara explains that the word, \"Ādeśah\" here means the \"brahmana\" portion of the Vedas, since (in consonance with the etymological meaning of \"ādeśa\", command) the \"brahmana\" portion inculcates all that has to be enjoyed.\n\nChanakya in his Arthashastra, refers to the then prevalent bill of exchange called \"Ādesha\" which was an order to a third person to pay up a sum of money on behalf of the sender of that order; in those days merchant guilds performed the functions of banks. There was considerable use of these instruments, including promissory notes, and merchants in large towns gave letters of credit to one another.\n\nAurobindo explains that the intellectual \"Asura\" determines his actions by his reason or ideal, the emotional \"Asura\" by his feelings; but the Shuddha determines them by the higher inspiration proceeding from the divine experience in the Vijanana, that is what people often call the \"Adesha\".\n\nKeshab Chandra Sen of Brahmo Samaj (and founder of \"Brahmo Samaj of India\") influenced by Christian theology propounded the doctrine of \"Adesha\", according to which God inspires knowledge in some individuals whose word must therefore be considered infallible and true which doctrine not accepted by some members witnessed division in ranks and the formation of \"Sadharan Brahmo Samaj\".\n\nIn the Bhagavad Gita Sloka XVII.22, it is seen that the word अदेशकाले (ādeśkāle) with the dropping of a mātra i.e. ā to a (and thus providing negative connotation), refers to wrong time.\n"}
{"id": "3669631", "url": "https://en.wikipedia.org/wiki?curid=3669631", "title": "Affect (philosophy)", "text": "Affect (philosophy)\n\nAffect (from Latin \"affectus\" or \"adfectus\") is a concept, used in the philosophy of Baruch Spinoza and elaborated by Henri Bergson, Gilles Deleuze and Félix Guattari, that places emphasis on bodily or embodied experience. The concept of affect takes on a different meaning in psychology and other fields. \n\nFor Spinoza, as discussed in Parts Two and Three of his \"Ethics\", affects are states of mind and body that are related to (but not exactly synonymous with) feelings and emotions, of which he says there are three primary kinds: pleasure or joy (\"laetitia\"), pain or sorrow (\"tristitia\") and desire (\"cupiditas\") or appetite. Subsequent philosophical usage by Gilles Deleuze, Félix Guattari and their translator Brian Massumi, while derived explicitly from Spinoza, tends to distinguish more sharply than Spinoza does between affect and what are conventionally called emotions. Affects are difficult to grasp and conceptualize because, as Spinoza says, \"an affect or passion of the mind [\"animi pathema\"] is a confused idea\" which is only perceived by the increase or decrease it causes in the body's vital force. The term \"affect\" is central to what has become known as the \"affective turn\" in the humanities and social sciences.\n\nIn Baruch Spinoza's \"Ethics\", Part III Definition 3, the term \"affect\" (\"affectus\", traditionally translated as \"emotion\") is the modification or variation produced in a body (including the mind) by an interaction with another body \"which increases or diminishes the body's power of activity\" (potentia agendi):\nAffect is thus a special case of the more neutral term \"affection\" (\"affectio\"), which designates the form \"taken on\" by some thing, the mode, state or quality of a body's relation to the world or nature (or infinite \"substance\"). In Part III, \"Definitions of the Emotions/Affects\", Spinoza defines 48 different forms of affect, including love and hatred, hope and fear, envy and compassion. They are nearly all manifestations of the three basic affects of:\nIn Spinoza's view, since God's power of activity is infinite, any affection which increases the organism's power of activity leads to greater perfection. Affects are transitional states or modes in that they are vital forces by which the organism strives to act against other forces which act on it and continually resist it or hold it in check.\n\nHenri Bergson contends in \"Matter and Memory\" (1896) that we do not know our body only \"from without\" by perceptions, but also \"from within\" by affections (French: \"affections\").\n\nThe terms \"affect\" and \"affection\" came to prominence in Gilles Deleuze and Félix Guattari's \"A Thousand Plateaus\", the second volume of \"Capitalism and Schizophrenia\". In his notes on the terminology employed, the translator Brian Massumi gives the following definitions of the terms as used in the volume:\nAffects, according to Deleuze, are not simple affections, as they are independent from their subject.\nArtists create affects and percepts, \"blocks of space-time\", whereas science works with functions, according to Deleuze, and philosophy creates concepts.\n\nSince 2000, a number of authors in the social sciences and humanities have begun to explore affect theory as a way of understanding spheres of experience (including bodily experience) which fall outside of the dominant paradigm of representation (based on rhetoric and semiotics); this movement has been called the affective turn. Consequently, these approaches are interested in the widest possible variety of interactions and encounters, interactions and encounters that are not necessarily limited to human sensibility. The translator of Deleuze and Guattari's \"A Thousand Plateaus\", Canadian political philosopher Brian Massumi, has given influential definitions of affect (see above) and has written on the neglected importance of movement and sensation in cultural formations and our interaction with real and virtual worlds. Likewise, geographer Nigel Thrift has explored the role of affect in what he terms \"non-representational theory\". In 2010, \"The Affect Theory Reader\" was published by Melissa Gregg and Gregory J. Seigworth and has provided the first compendium of affect theory writings.\nResearchers such as Mog Stapleton, Daniel D. Hutto and Peter Carruthers have pointed to the need to investigate and to develop the notions of affect and emotion. They hold that these are important in the developing paradigm of embodiment in cognitive science, in consciousness studies and the philosophy of mind. This step will be necessary for cognitive science, Mog Stapleton maintains, to be \"properly embodied\" cognitive science.\n\n\n"}
{"id": "6395956", "url": "https://en.wikipedia.org/wiki?curid=6395956", "title": "Analytic–synthetic distinction", "text": "Analytic–synthetic distinction\n\nThe analytic–synthetic distinction (also called the analytic–synthetic dichotomy) is a semantic distinction, used primarily in philosophy to distinguish propositions (in particular, statements that are affirmative subject–predicate judgments) into two types: analytic propositions and synthetic propositions. Analytic propositions are true by virtue of their meaning, while synthetic propositions are true by how their meaning relates to the world. However, philosophers have used the terms in very different ways. Furthermore, philosophers have debated whether there is a legitimate distinction.\n\nThe philosopher Immanuel Kant uses the terms \"analytic\" and \"synthetic\" to divide propositions into two types. Kant introduces the analytic–synthetic distinction in the Introduction to his \"Critique of Pure Reason\" (1781/1998, A6–7/B10–11). There, he restricts his attention to statements that are affirmative subject-predicate judgments and defines \"analytic proposition\" and \"synthetic proposition\" as follows:\n\nExamples of analytic propositions, on Kant's definition, include:\n\nKant's own example is:\n\nEach of these statements is an affirmative subject-predicate judgment, and, in each, the predicate concept is \"contained\" within the subject concept. The concept \"bachelor\" contains the concept \"unmarried\"; the concept \"unmarried\" is part of the definition of the concept \"bachelor\". Likewise, for \"triangle\" and \"has three sides\", and so on.\n\nExamples of synthetic propositions, on Kant's definition, include:\n\nKant's own example is:\n\nAs with the previous examples classified as analytic propositions, each of these new statements is an affirmative subject–predicate judgment. However, in none of these cases does the subject concept contain the predicate concept. The concept \"bachelor\" does not contain the concept \"alone\"; \"alone\" is not a part of the \"definition\" of \"bachelor\". The same is true for \"creatures with hearts\" and \"have kidneys\"; even if every creature with a heart also has kidneys, the concept \"creature with a heart\" does not contain the concept \"has kidneys\".\n\nIn the Introduction to the \"Critique of Pure Reason\", Kant contrasts his distinction between analytic and synthetic propositions with another distinction, the distinction between \"a priori\" and \"a posteriori\" propositions. He defines these terms as follows:\n\nExamples of \"a priori\" propositions include:\n\nThe justification of these propositions does not depend upon experience: one need not consult experience to determine whether all bachelors are unmarried, nor whether . (Of course, as Kant would grant, experience is required to understand the concepts \"bachelor\", \"unmarried\", \"7\", \"+\" and so forth. However, the \"a priori\" / \"a posteriori\" distinction as employed here by Kant refers not to the \"origins\" of the concepts but to the \"justification\" of the propositions. Once we have the concepts, experience is no longer necessary.)\n\nExamples of \"a posteriori\" propositions include:\n\nBoth of these propositions are \"a posteriori\": any justification of them would require one's experience.\n\nThe analytic/synthetic distinction and the \"a priori\" / \"a posteriori\" distinction together yield four types of propositions:\n\nKant posits the third type as obviously self-contradictory. Ruling it out, he discusses only the remaining three types as components of his epistemological frameworkeach, for brevity's sake, becoming, respectively, \"analytic\", \"synthetic a priori\", and \"empirical\" or \"a posteriori\" propositions. This triad will account for all propositions possible.\n\nPart of Kant's argument in the Introduction to the \"Critique of Pure Reason\" involves arguing that there is no problem figuring out how knowledge of analytic propositions is possible. To know an analytic proposition, Kant argued, one need not consult experience. Instead, one needs merely to take the subject and \"extract from it, in accordance with the principle of contradiction, the required predicate\" (A7/B12). In analytic propositions, the predicate concept is contained in the subject concept. Thus, to know an analytic proposition is true, one need merely examine the concept of the subject. If one finds the predicate contained in the subject, the judgment is true.\n\nThus, for example, one need not consult experience to determine whether \"All bachelors are unmarried\" is true. One need merely examine the subject concept (\"bachelors\") and see if the predicate concept \"unmarried\" is contained in it. And in fact, it is: \"unmarried\" is part of the definition of \"bachelor\" and so is contained within it. Thus the proposition \"All bachelors are unmarried\" can be known to be true without consulting experience.\n\nIt follows from this, Kant argued, first: All analytic propositions are \"a priori\"; there are no \"a posteriori\" analytic propositions. It follows, second: There is no problem understanding how we can know analytic propositions; we can know them because we only need to consult our concepts in order to determine that they are true.\n\nAfter ruling out the possibility of analytic \"a posteriori\" propositions, and explaining how we can obtain knowledge of analytic \"a priori\" propositions, Kant also explains how we can obtain knowledge of synthetic \"a posteriori\" propositions. That leaves only the question of how knowledge of synthetic \"a priori\" propositions is possible. This question is exceedingly important, Kant maintains, because all important metaphysical knowledge is of synthetic \"a priori\" propositions. If it is impossible to determine which synthetic \"a priori\" propositions are true, he argues, then metaphysics as a discipline is impossible. The remainder of the \"Critique of Pure Reason\" is devoted to examining whether and how knowledge of synthetic \"a priori\" propositions is possible.\n\nOver a hundred years later, a group of philosophers took interest in Kant and his distinction between analytic and synthetic propositions: the logical positivists.\n\nPart of Kant's examination of the possibility of synthetic \"a priori\" knowledge involved the examination of mathematical propositions, such as\n\nKant maintained that mathematical propositions such as these are synthetic \"a priori\" propositions, and that we know them. That they are synthetic, he thought, is obvious: the concept \"equal to 12\" is not contained within the concept \"7 + 5\"; and the concept \"straight line\" is not contained within the concept \"the shortest distance between two points\". From this, Kant concluded that we have knowledge of synthetic \"a priori\" propositions.\n\nGottlob Frege's notion of analyticity included a number of logical properties and relations beyond containment: symmetry, transitivity, antonymy, or negation and so on. He had a strong emphasis on formality, in particular formal definition, and also emphasized the idea of substitution of synonymous terms. \"All bachelors are unmarried\" can be expanded out with the formal definition of bachelor as \"unmarried man\" to form \"All unmarried men are unmarried\", which is recognizable as tautologous and therefore analytic from its logical form: any statement of the form \"All \"X\" that are (\"F\" and \"G\") are \"F\"\". Using this particular expanded idea of analyticity, Frege concluded that Kant's examples of arithmetical truths are analytical \"a priori\" truths and \"not\" synthetic \"a priori\" truths.\n\nThe logical positivists agreed with Kant that we have knowledge of mathematical truths, and further that mathematical propositions are \"a priori\". However, they did not believe that any complex metaphysics, such as the type Kant supplied, are necessary to explain our knowledge of mathematical truths. Instead, the logical positivists maintained that our knowledge of judgments like \"all bachelors are unmarried\" and our knowledge of mathematics (and logic) are in the basic sense the same: all proceeded from our knowledge of the meanings of terms or the conventions of language.\n\nThus the logical positivists drew a new distinction, and, inheriting the terms from Kant, named it the \"analytic/synthetic distinction\". They provided many different definitions, such as the following:\n\nSynthetic propositions were then defined as:\n\nThese definitions applied to all propositions, regardless of whether they were of subject–predicate form. Thus, under these definitions, the proposition \"It is raining or it is not raining\" was classified as analytic, while for Kant it was analytic by virtue of its logical form. And the proposition \"\" was classified as analytic, while under Kant's definitions it was synthetic.\n\nTwo-dimensionalism is an approach to semantics in analytic philosophy. It is a theory of how to determine the sense and reference of a word and the truth-value of a sentence. It is intended to resolve a puzzle that has plagued philosophy for some time, namely: How is it possible to discover empirically that a necessary truth is true? Two-dimensionalism provides an analysis of the semantics of words and sentences that makes sense of this possibility. The theory was first developed by Robert Stalnaker, but it has been advocated by numerous philosophers since, including David Chalmers and Berit Brogaard.\n\nAny given sentence, for example, the words,\n\nis taken to express two distinct propositions, often referred to as a \"primary intension\" and a \"secondary intension\", which together compose its meaning.\n\nThe primary intension of a word or sentence is its sense, i.e., is the idea or method by which we find its referent. The primary intension of \"water\" might be a description, such as \"watery stuff\". The thing picked out by the primary intension of \"water\" could have been otherwise. For example, on some other world where the inhabitants take \"water\" to mean \"watery stuff\", but, where the chemical make-up of watery stuff is not HO, it is not the case that water is HO for that world.\n\nThe \"secondary intension\" of \"water\" is whatever thing \"water\" happens to pick out in \"this\" world, whatever that world happens to be. So if we assign \"water\" the primary intension \"watery stuff\" then the secondary intension of \"water\" is HO, since HO is \"watery stuff\" in this world. The secondary intension of \"water\" in our world is HO, which is HO in every world because unlike \"watery stuff\" it is impossible for HO to be other than HO. When considered according to its secondary intension, \"Water is HO\" is true in every world.\n\nIf two-dimensionalism is workable it solves some very important problems in the philosophy of language. Saul Kripke has argued that \"Water is HO\" is an example of the \"necessary a posteriori\", since we had to discover that water was HO, but given that it is true, it cannot be false. It would be absurd to claim that something that is water is not HO, for these are known to be \"identical\".\n\nRudolf Carnap was a strong proponent of the distinction between what he called \"internal questions\", questions entertained within a \"framework\" (like a mathematical theory), and \"external questions\", questions posed outside any framework – posed before the adoption of any framework. The \"internal\" questions could be of two types: \"logical\" (or analytic, or logically true) and \"factual\" (empirical, that is, matters of observation interpreted using terms from a framework). The \"external\" questions were also of two types: those that were confused pseudo-questions (\"one disguised in the form of a theoretical question\") and those that could be re-interpreted as practical, pragmatic questions about whether a framework under consideration was \"more or less expedient, fruitful, conducive to the aim for which the language is intended\". The adjective \"synthetic\" was not used by Carnap in his 1950 work \"Empiricism, Semantics, and Ontology\". Carnap did define a \"synthetic truth\" in his work \"Meaning and Necessity\": a sentence that is true, but not simply because \"the semantical rules of the system suffice for establishing its truth\".\n\nThe notion of a synthetic truth is of something that is true both because of what it means and because of the way the world is, whereas analytic truths are true in virtue of meaning alone. Thus, what Carnap calls internal \"factual\" statements (as opposed to internal \"logical\" statements) could be taken as being also synthetic truths because they require \"observations\", but some external statements also could be \"synthetic\" statements and Carnap would be doubtful about their status. The analytic–synthetic argument therefore is not identical with the internal–external distinction.\n\nIn 1951, Willard Van Orman Quine published the essay \"Two Dogmas of Empiricism\" in which he argued that the analytic–synthetic distinction is untenable. The argument at bottom is that there are no \"analytic\" truths, but all truths involve an empirical aspect. In the first paragraph, Quine takes the distinction to be the following:\n\nQuine's position denying the analytic-synthetic distinction is summarized as follows:\nTo summarize Quine's argument, the notion of an analytic proposition requires a notion of synonymy, but establishing synonymy inevitably leads to matters of fact – synthetic propositions. Thus, there is no non-circular (and so no tenable) way to ground the notion of analytic propositions.\n\nWhile Quine's rejection of the analytic–synthetic distinction is widely known, the precise argument for the rejection and its status is highly debated in contemporary philosophy. However, some (for example, Boghossian) argue that Quine's rejection of the distinction is still widely accepted among philosophers, even if for poor reasons.\n\nPaul Grice and P. F. Strawson criticized \"Two Dogmas\" in their 1956 article \"In Defense of a Dogma\". Among other things, they argue that Quine's skepticism about synonyms leads to a skepticism about meaning. If statements can have meanings, then it would make sense to ask \"What does it mean?\". If it makes sense to ask \"What does it mean?\", then synonymy can be defined as follows: Two sentences are synonymous if and only if the true answer of the question \"What does it mean?\" asked of one of them is the true answer to the same question asked of the other. They also draw the conclusion that discussion about correct or incorrect translations would be impossible given Quine's argument. Four years after Grice and Strawson published their paper, Quine's book \"Word and Object\" was released. In the book Quine presented his theory of indeterminacy of translation.\n\nIn \"Speech Acts\", John Searle argues that from the difficulties encountered in trying to explicate analyticity by appeal to specific criteria, it does not follow that the notion itself is void. Considering the way which we would test any proposed list of criteria, which is by comparing their extension to the set of analytic statements, it would follow that any explication of what analyticity means presupposes that we already have at our disposal a working notion of analyticity.\n\nIn \"'Two Dogmas' Revisited\", Hilary Putnam argues that Quine is attacking two different notions:\nAnalytic truth defined as a true statement derivable from a tautology by putting synonyms for synonyms is near Kant's account of analytic truth as a truth whose negation is a contradiction. Analytic truth defined as a truth confirmed no matter what, however, is closer to one of the traditional accounts of \"a priori\". While the first four sections of Quine's paper concern analyticity, the last two concern a priority. Putnam considers the argument in the two last sections as independent of the first four, and at the same time as Putnam criticizes Quine, he also emphasizes his historical importance as the first top rank philosopher to both reject the notion of a priority and sketch a methodology without it.\n\nJerrold Katz, a one-time associate of Noam Chomsky, countered the arguments of \"Two Dogmas\" directly by trying to define analyticity non-circularly on the syntactical features of sentences.\n\nIn \"Philosophical Analysis in the Twentieth Century, Volume 1 : The Dawn of Analysis\", Scott Soames has pointed out that Quine's circularity argument needs two of the logical positivists' central theses to be effective:\n\nIt is only when these two theses are accepted that Quine's argument holds. It is not a problem that the notion of necessity is presupposed by the notion of analyticity if necessity can be explained without analyticity. According to Soames, both theses were accepted by most philosophers when Quine published \"Two Dogmas\". Today, however, Soames holds both statements to be antiquated. He says: \"Very few philosophers today would accept either [of these assertions], both of which now seem decidedly antique.\"\n\nPhilosopher Leonard Peikoff, in his essay \"The Analytic-Synthetic Dichotomy\", expands upon Ayn Rand's analysis. He posits that:\n\nThe theory of the analytic-synthetic dichotomy presents men with the following choice: If your statement is proved, it says nothing about that which exists; if it is about existents, it cannot be proved. If it is demonstrated by logical argument, it represents a subjective convention; if it asserts a fact, logic cannot establish it. If you validate it by an appeal to the meanings of your \"concepts\", then it is cut off from reality; if you validate it by an appeal to your \"percepts\", then you cannot be certain of it.\n\nTo Peikoff, the critical question is: What is included in the meaning of a concept? He rejects the idea that some of the characteristics of a concept's referents are excluded from the concept. Applying Rand's theory that a concept is a \"mental integration\" of similar existents, treated as \"units\", he argues that concepts stand for and mean the actual existents, including all their characteristics, not just those used to pick out the referents or define the concept. He states,\n\nSince a concept is an integration of units, it has no content or meaning apart from its units. The meaning of a concept consists of the units — the existents — which it integrates, including all the characteristics of these units... The fact that certain characteristics are, at a given time, unknown to man, does not indicate that these characteristics are excluded from the entity — or from the concept.\n\nFurthermore, he argues that there is no valid distinction between \"necessary\" and \"contingent\" facts, and that all truths are learned and validated by the same process: the application of logic to perceptual data. Associated with the analytic-synthetic dichotomy are a cluster of other divisions that Objectivism also regards as false and artificial, such as logical truth vs. factual truth, logically possible vs. empirically possible, and a priori vs. the a posteriori.\n\n\n\n"}
{"id": "31555694", "url": "https://en.wikipedia.org/wiki?curid=31555694", "title": "Andries Tatane", "text": "Andries Tatane\n\nAndries Tatane (22 February 1978 – 13 April 2011) was a 33-year-old South African citizen who was shot and killed by police officers during a service delivery protest in Ficksburg. Seven police officers accused of his murder and assault were acquitted in the Ficksburg Regional Court in March 2013.\n\nAndries was a member of the ANC until 2008 when he left the ANC to join the breakaway party COPE. In the months before he died he left COPE to join the Meqheleng Concerned Citizens (MCC), an autonomous local community organisation. He was a mathematics teacher, a community activist, journalist, community newspaper publisher and possible independent candidate for the municipal elections due to take place in May 2011. He was married to Rose Tatane.\n\nOn 13 April 2011, Andries Tatane, together with 4,000 other protesters, took to the streets and marched to Setsoto Municipal Offices in Ficksburg, Free State, South Africa to protest against poor service delivery in the area. The protesters were met by police members who attempted to disperse the crowd with water cannons. While some arrested protesters were being bundled into police vans, Tatane tried to argue with the police and to block a water cannon vehicle, at which point he was grabbed around one arm by a police officer. Tatane was seen to pull his arm away from the officer who then started to beat him with a baton. Tatane appeared to move aggressively towards this officer. Four or five other police officers then pulled him away and began to kick and beat him with batons. During this time he was twice shot in the chest. Tatane collapsed shortly after and died on the scene 20 minutes later.\n\nWhile Tatane's death as a result of police action during a protest is by no means a unique event in South Africa, it had notably garnered greater nationwide attention than any previous such occurrence. One of the main reasons for this was the fact that the entire incident was filmed on T.V. cameras and later broadcast during the prime time evening news of the national broadcaster, the SABC.\n\nFollowing Tatane's death, there has been public outrage about the manner in which he died.\n\nTatane's death has been described as \"a watershed moment in public perceptions of state violence after apartheid\".\n\nAn investigation into his death was launched by the Independent Complaints Directorate's commission. The South African Police Service has also launched their own internal investigation into the matter. The death of Tatane has placed both the issue of rising anger over a lack of service delivery as well as police brutality in the media spotlight, with comparisons being drawn to the deaths of Hector Pieterson and Steve Biko at the hands of police during the height of apartheid.\n\nIt has also emerged that this was not an isolated incident and that the ICD has investigated 1,769 separate incidents of people dying in police custody or as a result of police action in 2010. National Police Commissioner Bheki Cele's statement in late August 2009 that police officers should be able to \"shoot to kill\" without worrying about the consequences will undoubtedly be brought to the forefront again.\n\nThe ANC's National Spokesperson, Jackson Mthembu, condemned the brutality, but also chastised the SABC for broadcasting the footage during the prime time news, citing the fact that it might have upset sensitive viewers, calling on the Independent Communications Authority of South Africa (ICASA) to investigate the SABC's editorial decision. Mthembu has been one of the most vocal supporters of planned legislation to introduce a Media Appeals Tribunal to govern the South African media; legislation which is currently held in abeyance.\n\nAfter visiting the Tatane family in Meqheleng, Ficksburg, on 19 April 2011, Anglican Archbishop of Cape Town, Dr Thabo Makgoba, directed attention to the lack of justice and delivery on promises which had preceded the incident and points out the irony of the protesters having been met with water cannons, \"attacked with the very thing they don’t have the pleasure of in their daily lives.\" He called on the Minister of Co-operative Governance and Traditional Affairs, Sicelo Shiceka, to \"visit and see the appalling conditions under which God’s people live\" and the Minister of Human Settlements, Tokyo Sexwale, to \"provide houses\". He added that \"Minister Nathi Mthethwa and President Zuma should publicly apologise for this embarrassing act of aggression by police.\"\n\nThe Archbishop has subsequently said: \"Let us affirm and call for a renaming of our police services back to 'safety and security' and not a police 'force', for this force seems to maim and kill rather than offer safety and security.\" \n\nThere was an election boycott in the area following Tatane's death. In May 2011 it was reported that the Meqheleng Concerned Citizens group was a credible structure with growing influence. However, by December 2011 it had degenerated to the point of being described as \"a toothless organisation led by calculating tenderpreneurs\".\n\nA number of police officers were charged with Tatane's murder but they were found not guilty on the grounds that Tatane's murderers could not be identified as they were wearing helmets. However, a British journalist was easily able to identify his killers from video footage.\n\nIn poetry, Andries Tatane's death is referenced in a work by Adam Haupt, entitled \"For Andries Tatane\". This poem has since been republished as the epilogue to a scholarly book titled \"Static: Race & Representation in Post-Apartheid Music, Media & Film\" \n"}
{"id": "11624402", "url": "https://en.wikipedia.org/wiki?curid=11624402", "title": "Antihaitianismo", "text": "Antihaitianismo\n\nAntihaitianismo (, , ), also called anti-Haitianism in some English sources, is prejudice or social discrimination against Haitians in the Dominican Republic.\n\nAntihaitianismo includes prejudice against, hatred of, or discrimination against Haitians and their language, culture, but not their race.\n\nHuman Rights Watch has stated in their reports that the differences between Haitians and Dominicans can be based on colonial times from linguistic, cultural, and racial differences. For instance, the Dominican Republic was governed by the Spanish, and thus acquired part of their culture from the Spanish, mixed with African and Native American. Haiti, on the other hand, was governed by the French, and its culture is a mixture of French, African and Native American. The majority of Haiti's population is descended almost entirely from African slaves, while Dominicans possess a multiracial mix of Spanish, African and to a lesser extent, Native American ancestry. It is evident that historical background is related between the two countries, however, there are major cultural divisions.\n\"Antihaitianismo\" can be traced back to a policy of racial segregation instituted by the Spaniards in the Captaincy General of Santo Domingo (present-day Dominican Republic). Prior to the arrival of Europeans, the island was split into absolutist chiefdoms, three where modern-day Santo Domingo now exists, and two where modern-day Haiti now exists (albeit also including some territory which is currently part of Santo Domingo). Carib people from islands further south were often at war with the Taíno people. Columbus reached the island in 1492 (slaves imported from Africa arrived from 1503 onwards—many natives were also soon enslaved), and within a few decades the Spanish controlled most of the island. During the 17th century, however, the French also began maneuvering for control, and in 1697 acquired the western portion (now part of Haiti—whereas the Spanish portion encompassed the modern Dominican Republic). During the 1790s and early 19th century, the French and Spanish battled back and forth across the island; by 1809 the Haitian Revolution had resulted in the overthrow of both French and Spanish control. The Spanish briefly retook the eastern portion that same year, but in 1821 lost control again in another rebellion. Shortly afterwards, Haitian forces again briefly controlled the entire island, from 1822 to 1844. In 1844 the secret revolutionary movement called \"La Trinitaria\" took place and the Dominican Republic declared its independence defeating the Haitian forces. After several tumultuous decades, the Spanish briefly acquired nominal control of the Dominican Republic in the 1860s, setting off another war. By the late 19th century, over three hundred years of European control was ended; the modern history of west Hispaniola (Haiti) and east Hispaniola (Dominican Republic) had begun.\n\n\"Antihaitianismo\" was strongly institutionalized during the regime of Rafael Leónidas Trujillo. Border disputes under Trujillo culminated in the order of a military intervention and to massacre Haitians accused of practicing vodou or witchery, practices that were against the religious beliefs of the time. Claims range \"from several hundred to 26,000\" or even \"recorded as having a death toll reaching 30,000\" in October 1937, an event subsequently named the Parsley Massacre. During later diplomacy, Trujillo agreed to pay hundreds of thousands in reparations, but somewhat less was actually delivered. Due to corrupt Haitian bureaucrats, exceedingly little reached the families.\n\nDominican intellectuals Manuel Arturo Peña Batlle, Joaquín Balaguer, Manuel de Jesús Troncoso de la Concha, among others, led the campaign.\n\nTrujillo's policies served to perpetuate antihaitianismo within the Dominican Republic. In the 1996 Dominican presidential election, Joaquín Balaguer (historical leader of the populist Right and former right-hand of dictator Trujillo) united in a \"National Patriotic Front\" with PLD candidate Leonel Fernández in order to prevent Peña Gómez from becoming President.\n\n\n"}
{"id": "10458370", "url": "https://en.wikipedia.org/wiki?curid=10458370", "title": "Beyond Terror", "text": "Beyond Terror\n\nBeyond Terror: The Truth About the Real Threats to Our World is a book by Chris Abbott, Paul Rogers and John Sloboda of Oxford Research Group, a UK-based think tank. It is a 120-page paperback published by the Rider imprint of Random House in April 2007.\n"}
{"id": "362652", "url": "https://en.wikipedia.org/wiki?curid=362652", "title": "Carjacking", "text": "Carjacking\n\nCarjacking is a robbery in which the item taken over is a motor vehicle.\n\nThe word is a portmanteau of car and hijacking. The term was coined by EJ Mitchell, an editor with \"The Detroit News\". \"The News\" first used the term in a 1991 report on the murder of Ruth Wahl, a 22-year-old Detroit drugstore cashier who was killed when she would not surrender her Suzuki Sidekick, and in an investigative report examining the rash of what Detroit Police call \"robbery armed unlawful driving away an automobile\" [in dispatch slang shortened to R.A.-YOU-Da] plaguing Detroit.\n\nA study published in the \"British Journal of Criminology\" in 2003 found that \"for all of the media attention it has received in the United States, Europe and elsewhere, carjacking remains an under-researched and poorly understood crime.\" The study authors conducted semi-structured interviews with 28 active carjackers in St. Louis, Missouri, and based on these interviews concluded that \"the decision to commit a carjacking stems most directly from a situated interaction between particular sorts of perceived opportunities and particular sorts of perceived needs and desires, this decision is activated, mediated, and shaped by participation in urban street culture.\"\n\nA study published in the \"Journal of Contemporary Ethnography\" in 2013 noted that \"carjacking requires offenders to neutralize victims who are inherently mobile and who can use their vehicles as both weapons and shields.\" The study noted that carjackers use fear to compel compliance from victims. \n\nA 2008 paper by the Australian Institute of Criminology conceptualized carjackings as falling into four types based on method and motive: organized and instrumental, organized and acquisitive, opportunistic and instrumental, and opportunistic and acquisitive. An example of an \"organized and instrumental\" carjacking is a planned carjacking with a weapon to use the vehicle for ram an ATM to steal cash. An example of an \"organized and acquisitive\" carjacking is a planned carjacking to sell the vehicle in a known market. An example of an \"opportunistic and instrumental\" carjacking is a carjacking without a weapon to sell \"vehicle/parts with no market in mind.\" An example of an \"opportunistic and acquisitive\" carjacking is a carjacking without a weapon to joyride.\n\nA 2017 qualitative study published in \"Justice Quarterly\" examined auto theft and carjacking in the context of \"sanction threats\" that promoted fear and influenced \"crime preferences\" among criminals, thereby redirecting (\"channeling\") criminal activity. The study showed that \"auto thieves are reluctant to embrace the violence of carjacking due to concerns over sanction threat severity they attributed to carjacking—both formal (higher sentences) and informal (victim resistance and retaliation). Meanwhile, the carjackers are reticent to enact auto theft because of the more uncertain and putatively greater risk of being surprised by victims, a fear that appears to overcome the enhanced long-term formal penalty of taking a vehicle by force.\"\n\nCommon carjacking ruses include: (1) bumping the victim's vehicle from behind, and taking the car when the victim gets out of the vehicle to assess damage and exchange information; (2) staging a fake car accident, sometimes with injuries, and stealing the vehicle of a passerby who stops to assist; (3) flashing lights or waving to get the victim's attention, indicating that there is a problem with the victim's car, and then taking the car once the victim pulls over; and (4) following a victim home, blocking the victim's car in a driveway or in front of a gate.\n\nPolice departments, security agencies, and auto insurers have published lists of strategies for preventing and responding to carjackings. Common recommendations include:\n\nCarjacking is a significant problem in South Africa, where it is called \"hijacking\". South Africa is thought to have the highest carjacking rate in the world. There were 16,000 reported carjackings in 1998. The figures dropped to 12,434 reported carjackings in 2005, and continued to drop until the 2011-12, when the number of carjackings was 9,475, a record low. Subsequently, however, carjackings increased as part of an overall increase in violent organized crime, which the Institute for Security Studies attributed to poor police leadership. There were 11,221 reported carjackings in 2014. More than half of all carjackings in South Africa occurred in Gauteng province, which includes Johannesburg and Pretoria. \n\nThe carjacking issue in South Africa was depicted in the film \"Tsotsi\", which won the Academy Award for Best Foreign Language Film in 2005. \n\nIn the late 1990s and early 2000s, several new, unconventional anti-carjacking systems designed to harm the attacker were developed and marketed in South Africa, where carjacking had become endemic. Among these was the now defunct Blaster, a small flame-thrower that could be mounted to the underside of a vehicle.\n\nIn 1992, Congress, in the aftermath of a spate of violent carjackings (including some in which the victims were murdered), passed the Federal Anti-Car Theft Act of 1992 (FACTA), the first federal carjacking law, making it a federal crime (punishable by 15 years to life imprisonment) to use a firearm to steal \"through force or violence or intimidation\" a motor vehicle that had been shipped through interstate commerce. The 1992 Act, codified at 18 U.S.C. § 2119, took effect on October 25, 1992. However, only a small number of federal prosecutions were imposed for carjacking the year after the act was enacted, in part because many federal carjacking cases were turned over to state prosecutions because they do not meet U.S. Department of Justice criteria. The Federal Death Penalty Act, part of the Violent Crime Control and Law Enforcement Act of 1994, an omnibus crime bill, made sixty new federal crimes punishable by the federal death penalty; among these were the killing of a victim in the commission of carjacking.\n\nThroughout 1993, articles about carjackings appeared at the rate of more than one a week in newspapers throughout the country. The November 29, 1992, killing of two Osceola County, Florida men by carjackers using a stolen 9 mm pistol resulted in the first federal prosecution of a fatal carjacking.\n\nAccording to the National Crime Victimization Survey (NCVS) conducted by the U.S. Department of Justice's Bureau of Justice Statistics, from 1993 to 2002, some 38,000 carjackings occurred annually. According to the survey, over this time period men were more often victims than women, blacks more than whites, and Hispanics more than non-Hispanics. 56% of carjackers were identified by victims as black, 21% white, 16% Asian or Native American, and 7% mixed race or unknown. Some 93% of carjackings occurred in urban areas. \n\nThere were multiple carjackers in 56% of incidents, and the carjacker or carjackers were identified as male in 93% of incidents. A weapon was used in 74% of carjacking victimization: firearms in 45%, knives in 11%, and other weapons in 18%. Victims were injured in about 32% of completed carjackings and about 17% of attempted carjackings. Serious injuries, such as gunshot or knife wounds, broken bones, or internal injuries occurred in about 9% of incidents. About 14 murders a year involved car theft, but not all of these were carjackings. There were multiple carjackers in 56% of incidents, and the carjacker or carjackers were identified as male in 93% of incidents. Some 68% of carjackings occurred at nighttime hours (6 p.m. to 6 a.m.). Some 98% of completed carjackings and 77% of attempted carjackings were reported to police. About 44% of carjacking incidents occurred in an open area (e.g., on the street or near public transportation) while 24% occurred in parking lots or garages or near commercial places (e.g., stores, gas stations, office buildings, restaurants/bars).\n\nAccording to the NCVS, from 1992 and 1996, about 49,000 completed or attempted nonfatal carjackings took place each year in the United States. The carjacking was successful in about half of incidents. Data on fatal carjackings are not available; \"about 27 homicides by strangers each year involved automobile theft,\" but not all of these were carjackings.\n\nCarjackings were common in Newark, New Jersey, in the 1990s, and a wave of carjackings took place again in 2010. There were 288 carjackings in the city in 2010 (a 70% increase from the previous year), and Essex County (which includes Newark) had 69 in December 2010 alone. The Associated Press reported that \"unlike previous carjackings, in which thieves would strip vehicles for parts or sell them in other states, the recent wave perplexed law enforcement officials because almost all appeared to be done by thrill-seeking young men who would steal the cars for a few hours, drive them around and then abandon them.\" After federal, state, and law enforcement agencies formed a task force, 42 suspects were charged, and carjackings dropped dramatically. However, national media attention on carjackings in Essex County returned in December 2013, when a Hoboken lawyer was murdered at The Mall at Short Hills in Millburn, New Jersey, while defending his wife from four assailants, who were all later convicted of the crime.\n\nThe major U.S. city with the highest rates of carjacking is Detroit. In 2008, Detroit had 1,231 carjackings, more than three a day. By 2013, that number had fallen to 701, but this was still the highest known number of carjackings for any major city in the country. The significant decrease in carjackings was credited to a coordinated effort by the Detroit Police Department, the FBI, and the local federal prosecutor's office. Serial carjackers were targeted for federal prosecutions and longer sentences, and in 2009 the Detroit Police Department centralized all carjacking investigations and developed a suspect profiling system. Through mid-November 2014, Detroit had 486 carjackings, down 31% from the year before, but this was still three times more than the carjackings experienced by New York City (which has ten times Detroit's population) in all of 2013. Even James Craig, chief of police of the Detroit Police Department, was the victim of an attempted carjacking while he was in his police cruiser.\n\nA 2017 study used \"Risk Terrain Modeling\" analysis to identify spatial indicators of carjacking risk in Detroit. The analysis identified six factors that \"were influential in the best fitting model: proximity to service stations; convenience/grocery/liquor stores; bus stops; residential and commercial demolitions; and areas with high concentrations of drug arrests and restaurants.\" The study found that certain locations in Detroit \"had an expected rate of carjacking that was 278 times higher than other locations.\"\n\nSome states have a specific carjacking statute. Other states do not have a specific carjacking law, and prosecute carjackers under the general robbery statute.\n\nThe law of some states, such as Louisiana, explicitly lists a killing in the course of defending oneself against forcible entry of an occupied motor vehicle as a justifiable homicide.\n\nCarjacking is an uncommon crime in Britain, making up about 1% of all vehicle thefts.\n\nAustralia does not specifically record the number of carjackings; such crimes are variously recorded as assault, robbery, motor vehicle theft, and some combination. However, a 2008 paper by the Australian Institute of Criminology, analyzing police and insurance records, suggested that fewer than 300 carjackings occur annually in Australia (about 0.5% of all theft incidents in the country). The paper noted that the low incidence of carjacking compared to the United States is attributable to the low overall of firearm-related crime overall in Australia and the fact that the \"broader socioeconomic picture of Australian society is one of relative good health in terms of wealth distribution and social cohesion\" providing little motivation for victimization that is \"both personal and violent.\" The paper notes that although carjacking was rare, isolated hot spots do arise occasionally, and that since the late 1990s, \"[Sydney]] has experienced a number of carjacking clusters ... each lasting around three to six months and occurring in different locations including the eastern suburbs, the inner city and the south-west.\"\n\n\n"}
{"id": "270992", "url": "https://en.wikipedia.org/wiki?curid=270992", "title": "Citizen's arrest", "text": "Citizen's arrest\n\nA citizen's arrest is an arrest made by a person who is not acting as a sworn law-enforcement official. In common law jurisdictions, the practice dates back to medieval England and the English common law, in which sheriffs encouraged ordinary citizens to help apprehend law breakers.\n\nDespite the practice's name, in most countries, the arresting person is usually designated as a \"person\" with arrest powers, who need not be a \"citizen\" of the country in which they are acting. For example, in the British jurisdiction of England and Wales, the power comes from section 24A(2) of the Police and Criminal Evidence Act 1984, called \"any person arrest\". This legislation states \"any person\" has these powers, and does not state that they need to be a British citizen.\n\nA person who makes a citizen's arrest could risk exposing him or herself to possible lawsuits or criminal charges – such as charges of false imprisonment, unlawful restraint, kidnapping, or wrongful arrest – if the wrong person is apprehended or a suspect's civil rights are violated. This is especially true when police forces are attempting to determine who an aggressor is. Private citizens don’t enjoy the same immunity from civil liability when making arrests on other private citizens as police officers.\n\nThe level of responsibility that a person performing a citizen's arrest may bear depends on the jurisdiction. For instance, in France and Germany, a person stopping a criminal from committing a crime, including crimes against belongings, is not criminally responsible as long as the means employed are in proportion to the threat. Note, however, that in both countries, this results from a different legal norm, \"aid to others in immediate danger\", which is concerned with prevention, not prosecution, of crimes.\n\nIn Australia, the power to arrest is granted by both federal and state legislation, however the exact power granted differs depending on jurisdiction. The power to arrest for a Federal offence is granted by s.3Z of the \"Crimes Act\" 1914. Under the Act, a person who is not a police constable may, without warrant, arrest another person if they believe on reasonable grounds that:\n\n\nA person who arrests another person must, as soon as practicable after the arrest, arrange for the other person, and any property found on the other person, to be delivered into the custody of a constable.\n\nIn the Australian state of Victoria, the power to arrest is granted in section 458 of the \"Crimes Act 1958\" (Vic). It states that a person may, without a warrant, arrest a person that they find committing an offence for one or more of the following reasons:\n\nA person may also arrest another person if they are instructed to do so by a member of the police force, or if they believe on reasonable grounds that the offender is escaping legal custody.\n\nSection 461 states that if an arrest is made under 458 of the Crimes Act, and is later proven to be false, then the arrest itself won't be considered unlawful if it was made on reasonable grounds. Section 462A allows any person the right to use force \"not disproportionate to the objective as he believes on reasonable grounds to be necessary to prevent the commission, continuance or completion of an indictable offence or to effect or assist in effecting the lawful arrest of a person committing or suspected of committing any offence\".\n\nIn the Australian state of New South Wales, the power to arrest is granted to anyone who is not a police officer by s.100 of the \"Law Enforcement (Powers and Responsibilities) Act 2002\" of New South Wales. Under the Act, a person may, without a warrant, arrest another person if:\n\nSection 231 of the Act allows the use of such force as is \"reasonably necessary to make the arrest or to prevent the escape of the person after arrest\". A person who arrests another person under s.100 must, as soon as is reasonably practicable, take the person, and any property found on the person, before a magistrate to be dealt with according to law. The magistrate will also decide whether or not the force applied in making the arrest was reasonable under the circumstances.\n\nAccording to the Law Society of New South Wales, the arresting person should:\n\nIn the Australian state of Queensland, the power to arrest is granted by s.546 of Schedule 1 to the \"Criminal Code Act 1899\" of Queensland. Under the Act, any person who finds another committing an offence may, without warrant, arrest the other person. The power to arrest in Queensland also allows for arrest on suspicion of an offence:\ns.260 of the Act also provides a power to arrest in preventing a breach of the peace:\nThe Criminal Law Consolidation Act 1935 (s271) grants arrest powers to a person in South Australia.\n\nUnder the Police Offences Act 1935 (Tasmania), section 55(3), any person may arrest any other person whom they find committing an offence, where they have reasonable grounds to believe that the conduct will create or may involve substantial injury to another person, serious danger of such injury, loss of property or serious injury to property. Section 55(5) states \"For the purposes of this section, a person is said to be 'found offending' if he does any act, or makes any omission, or conducts or behaves himself, and thereby causes a person who finds him reasonable grounds for believing that he has, in respect of such act, omission, or conduct, committed an offence against this Act.\" There are further provisions in section 301 of the Criminal Code Act 1924 (Tas) that appear to allow a sliding scale of force in executing an arrest.\n\nIt was only in 2004 that the Western Australian parliament repealed the provisions of the former section 47 of the Police Act 1892 which allowed any person to arrest without a warrant \"any reputed common prostitute, thief, loose, idle or disorderly person, who, within view of such person apprehending, shall offend against this Act, and shall forthwith deliver him to any constable or police officer of the place where he shall have been apprehended, to be taken and conveyed before a Justice, to be dealt with according to law …\" A private citizen would have found it rather difficult to interpret the terms \"loose\" or \"idle\" with any degree of legal certainty. WA now locates its citizen's arrest powers in section 25 of the Criminal Investigation Act (WA) 2006.\n\n\nGenerally speaking, as regards the law in Australia: where it is clear on the evidence that a private citizen, or security officer, in detaining a suspect, acted reasonably and the suspect unreasonably, then it is likely that the court will find in favour of the citizen or security officer and against the suspect if that suspect chooses, later, to sue the citizen for assault or false imprisonment. In other circumstances where, e.g., a property owner (or an agent) arrests a thief in a manner, and in circumstances, disproportionate to the likely harm to the victim, and in clear defiance of the rights of the suspect (for example, to be taken forthwith to a police station), then the court is very likely to find in favour of the suspect (guilty or otherwise). The courts may order compensation for such suspects in appropriate circumstances.\n\nIn Brazil, a Federal law allows any person to arrest a suspect criminal found in flagrante delicto or fleeing from the crime scene. The person has to, at his/her own judgment, have the physical power to keep the suspect detained, has to verbally explain what he/she is doing to the arrestee and has to call the police. Both have to wait for the arrival of the police. The person who makes a citizen's arrest has to sign the police forms as a witness and explain the facts. Typically it will lead to a time burden of at least two hours. If the facts cannot be verified the person who realizes the citizen's arrest might be sued by the arrestee.\n\nCanada's blanket arrest authorities for crimes or violations of federal statutes are found in the Criminal Code. In Canada, a criminal offence is any offence that is created by a federal statute—there are no provincial \"crimes\".\n\nCriminal offences are divided into three groups: indictable, dual procedure, and summary conviction. For the purposes of arrest, dual procedure offences are considered to be indictable.\n\nThe Criminal Code provisions related to citizen arrests were changed in 2012, by the Citizen's Arrest and Self-defence Act. As a consequence, it is now possible to make a citizen's arrest in Canada in circumstances where a \"reasonable\" amount of time has lapsed between the commission of a property-related offence and the arrest.\n\nThere are several arrest authorities found through the various provincial statutes. The most notable citizen's arrest authority in Ontario is found in the Trespass to Property Act, but there are others found in the Highway Traffic Act, the Liquor Licence Act, and many others.\n\nChinese criminal procedure law empowers \"any citizen\" to make citizen's arrest:\n\nIn Denmark, pursuant to § 755 (2) of the Administration of Justice Act, anyone may arrest a suspect found at or in the immediate vicinity of a crime scene if the criminal act is subject to public prosecution. The arrestee must as soon as possible be turned over to the police with information about the time of and reasons for the arrest.\n\nIn Finland, Coercive Measures Act 22.7.2011/806 gives a right to apprehend someone in the act of committing a crime \"(in flagrante delicto)\" or fleeing from the crime scene, if punishment for the crime might be imprisonment or the crime is petty assault, petty theft, petty embezzlement, petty unauthorized use, petty stealing of a motor vehicle for temporary use, petty damage to property or petty fraud. A person wanted by the police (arrest warrant) can be apprehended by anyone. After the apprehension, the detainee must be handed over to the police as soon as possible. If the criminal is resisting or tries to escape, the law gives a citizen the right to use an amount of force considered necessary, when considering the nature of the crime, the behavior of the apprehended and the situation as a whole.\n\nFrench law allows any person to arrest a person having been caught \"in flagrante delicto\" committing a crime punishable by a jail or prison term, and to conduct that person before the nearest officer of judiciary police – in modern practice, one would rather call the police in after performing the arrest.\n\nCitizen's arrests can be made under § 127 Ⅰ 1 StPO (code of penal procedures) if the arrestee is caught \"in flagrante delicto\" and either the identity of the person cannot be otherwise established immediately or he/she is suspected to try to flee. The person making the arrest is allowed to hold the arrestee solely for the purpose of turning him over to a proper legal authority such as the police. German law does not establish that the crime has to be serious, nor that the person making the arrest has to actually be a citizen of Germany.\n\nCitizen's arrest is known as the \"101 power\". Under the Criminal Procedure Ordinance (cap. 221 of the Laws of Hong Kong), section 101(2) provides that \"Any person may arrest without warrant any person whom he may reasonably suspect of being guilty of an arrestable offence\" using \"force as is reasonable and proportionate in the circumstances\" . Once an arrest is made, the suspect must be delivered to a police office as soon as possible for court proceedings. \"Arrestable offence\" is defined as any crimes that can be sentenced for more than 12 months of jail time.\n\nAccording to article 127, section 3 of \"Act XIX. of 1998 concerning Penal Procedure\", anyone may arrest a person caught committing a felony, but is obliged to hand the person over to the \"investigative authorities\" immediately; if this is not possible, the police must be informed.\n\nSection 43, of the Code of Criminal Procedure, 1973 states that:\nAccording to this section any private person may arrest or cause to be arrested\nThe term \"citizen's arrest\" is colloquially used for arrest, without an arrest warrant, made by someone other than a member of the Garda Síochána. Despite the colloquial name, non-Irish citizens have performed such arrests. The law of the Republic of Ireland, being derived from English law, inherited the common law power for private individuals to arrest for felony or breach of the peace. The Criminal Law Act 1997 abolished the common-law distinction between felonies and misdemeanours and instead distinguishes \"arrestable\" and \"non-arrestable\" offences; arrestable offences are those punishable by at least five years' imprisonment, and private individuals may arrest those \"in flagrante\", having committed, or about to commit an arrestable offence. \n\nSeveral other statutes which define offences likewise state \"any person may arrest\" someone committing the offence; relevant offences include \nmaking off without payment, \nhawking revenue stamps,\nand property damage — this last permits arrest for a past crime as well as one in progress. \nIn addition, the Criminal Law (Jurisdiction) Act 1976 schedules offences associated with the Troubles in Northern Ireland, and authorises anyone to arrest someone for committing or having committed such an offence, whether in the Republic of Ireland or Northern Ireland. The 1976 act, and a similar Westminster act giving reciprocal extraterritorial jurisdiction, obviated the need for extradition between the jurisdictions, which would have been more controversial.\n\nIf the arrester lacks reasonable cause to believe the arrested person to have committed the offence, the latter may sue the former for defamation, wrongful arrest, or trespass. For most offences, a private individual can only make such an arrest if the suspect would otherwise evade arrest by a Garda, and the arrester must surrender the suspect to Garda custody as soon as practicable. An exception is that stamp hawkers must be brought before the District Court. Citizen's arrests are rare; most often they are made by store detectives on shoplifting suspects.\n\nAn Israeli law allowing anyone to arrest a suspect whom they witnessed carrying out a felony was repealed in 1996 and a new law now allows the detention of a suspect by another person under certain conditions. Section 75 of the Criminal Procedure Law (Enforcement Powers – Arrest) of 1996 allows anyone to detain a person who is witnessed carrying out certain suspected crimes. The crimes include the following: a felony, theft, a crime of violence and a crime which has caused serious damage to property. A person using these detention powers may use reasonable force if their request is not met as long as they do not cause the suspect bruising. They must hand the suspect over to the police immediately and no later than three hours. Persons whose identity is known or who are not suspected of fleeing may not be detained. The law, which is relatively new, is used by both private individuals and private security but is problematic because it has not yet been interpreted by the courts. In early 2009, a magistrate's court in Jerusalem handed down a verdict convicting two private security officers of assault following a detention of a suspect who assaulted one of them. The court ruled that the guards were not allowed to detain the suspect who was seated in a taxi at the time and should have waited for the police to arrive.\n\nIn Japan, Section 213 of the Code of Criminal Procedure allows anyone (not only citizens) witnessing any crime in progress to make an arrest. This is called \"genkouhan\" (現行犯, meaning \"in flagrante delicto\"). Most criminals who attempt to flee, or refuse to identify themselves, can be held until police arrive. However, making a citizen's arrest to prevent petty crime (e.g. illegal assembly, accidental injury, accidental trespass, defamation of character, leaving a parking lot without paying) is false imprisonment per Section 220 of the Criminal Code.\n\nCriminal Procedure Law in Latvia gives a right to any person to apprehend someone in the act of committing a crime \"(in flagrante delicto)\" or fleeing from the crime scene, if punishment for the crime might be imprisonment. Also a person wanted by the police, for whom there is an arrest warrant, can be arrested by anyone at any time. A person stopping a criminal from committing a crime is not criminally responsible as long as the means employed are in proportion to the threat. The arrested person must be handed over to the police immediately.\n\n<nowiki>\n"}
{"id": "5988043", "url": "https://en.wikipedia.org/wiki?curid=5988043", "title": "Crime Classification Manual", "text": "Crime Classification Manual\n\nCrime Classification Manual: A Standard System for Investigating and Classifying Violent Crimes (1992) is a text on the classification of violent crimes by John E. Douglas, Ann W. Burgess, Allen G. Burgess and Robert K. Ressler. The publication is a result of a project by the Federal Bureau of Investigation's National Center for the Analysis of Violent Crime.\n\nA second edition of the book was published in 2006, and added 155 pages of new information and research.\n\n\n"}
{"id": "6937513", "url": "https://en.wikipedia.org/wiki?curid=6937513", "title": "Déformation professionnelle", "text": "Déformation professionnelle\n\nDéformation professionnelle () is a tendency to look at things from the point of view of one's own profession or special expertise, rather than from a broader or humane perspective. It is often translated as \"professional deformation\" or \"job conditioning\", though French \"déformation\" can also be translated as \"distortion\". The implication is that professional training, and its related socialization, often result in a distortion of the way one views the world. Nobel laureate Alexis Carrel observed, \"Every specialist, owing to a well-known professional bias, believes that he understands the entire human being, while in reality he only grasps a tiny part of him.\"\n\nAs a term in psychology, it was likely coined by the Belgian sociologist Daniel Warnotte or Russian-American sociologist Pitirim Sorokin. \n\n"}
{"id": "5750673", "url": "https://en.wikipedia.org/wiki?curid=5750673", "title": "Empathizing–systemizing theory", "text": "Empathizing–systemizing theory\n\nThe empathizing–systemizing (E–S) theory suggests that people may be classified on the basis of their scores along two dimensions: empathizing (E) and systemizing (S). It measures a person's strength of interest in empathy (the ability to identify and understand the thoughts and feelings of others and to respond to these with appropriate emotions) and a person's strength of interest in systems (in terms of the drive to analyse or construct them).\n\nAccording to the originator of the hypothesis, Simon Baron-Cohen, the E-S theory has been tested using the Empathy Quotient (EQ) and Systemizing Quotient (SQ), developed by him and colleagues, and generates five different 'brain types' depending on the presence or absence of discrepancies between their scores on E or S. E-S profiles show that the profile E>S is more common in females than in males, and the profile S>E is more common in males than in females. Baron-Cohen and associates say the E-S theory is a better predictor than gender of who chooses STEM subjects (Science, Technology, Engineering and Mathematics). The E-S theory has been extended into the 'Extreme Male Brain' (EMB) theory of autism and Asperger syndrome, which are associated in the E-S theory with below-average empathy and average or above-average systemizing.\n\nBaron-Cohen's studies and theory have been questioned on multiple grounds. The overrepresentation of engineers could depend on a socioeconomic status rather than E-S differences, and analyses of autism have \"not\" found that autism clustered preferentially around areas rich in IT industry.\n\nE-S theory was developed by psychologist Simon Baron-Cohen as a major reconceptualization of cognitive sex differences in the general population and in an effort to understand why the cognitive difficulties in autism appeared to lie in domains in which he says on average females outperformed males and why cognitive strengths in autism appeared to lie in domains in which on average males outperformed females. In the first chapter of his 2003 book \"The Essential Difference\", he compares with the bestseller \"Men Are from Mars, Women Are from Venus\", written by John Gray in 1992-3, and states: \"the view that men are from Mars and women Venus paints the differences between the two sexes as too extreme. The two sexes are different. but are not \"so\" different that we cannot understand each other.\"\n\nHe had previously proposed the mind-blindness theory in 1985, which argued that children with autism are delayed in their development of a theory of mind, that is, the ability to understand the thoughts and feelings of themselves or others. Baron-Cohen says a strength of this theory lies in its power to explain one of the core features of autism (the social and communication difficulties), but a limitation of the mindblindness theory is that it ignored the other main domain in autism (unusually narrow interests and highly repetitive behaviors, also called 'resistance to change or need for sameness'). To address this, Baron-Cohen put forward the E-S theory.\n\nSuch a distinction can be traced back to Robert Vischer in 1873, who postulated the as yet undescribed distinction between \"verstehen\" and \"einfühlung\". It has become common to describe normative masculine pedagogical traits in boys and autistic children from this framework where parents and caregivers often mistakenly consider autistic children to be lacking in empathy.\n\nAccording to Baron-Cohen, females on average score higher on measures of empathy and males on average score higher on measures of systemizing. This has been found using the child and adolescent versions of the Empathy Quotient (EQ) and the Systemizing Quotient (SQ), which are completed by parents about their child/adolescent, and on the self-report version of the EQ and SQ in adults.\n\nBaron-Cohen and associates say that similar sex differences on average have been found using performance tests of empathy such as facial emotion recognition tasks and on performance tests of systemizing such as measures of mechanical reasoning or 'intuitive physics'. He also argues that these sex differences are not only due to socialization.\n\nWhile experience and socialization contribute to the observed sex differences in empathy and systemizing, Baron-Cohen and colleagues suggest that biology also plays a role. A candidate biological factor influencing E and S is fetal testosterone (FT). FT levels are positively correlated with scores on the Systemizing Quotient and are negatively correlated with scores on the Empathy Quotient A new field of research has emerged to investigate the role of testosterone levels in autism. Correlational research demonstrated that elevated rates of testosterone were associated with higher rates of autistic traits, lower rates of eye contact, and higher rates of other medical conditions. Furthermore, experimental studies showed that altering testosterone levels influences the maze performance in rats, having implications for human studies. The fetal testosterone theories posit that the level of testosterone in the womb influences the development of sexually dimorphic brain structures, resulting in sex differences and autistic traits in individuals.\n\nBaron-Cohen presents several possible evolutionary psychology explanations for this sex difference. For example, he says that better empathizing may improve care of children, and that better empathy may also improve women's social network which may help in various ways with the caring of children. On the other hand, he says that systemizing may help males become good hunters and increase their social status by improving spatial navigation and the making and use of tools.\n\nBaron-Cohen's work in systemizing-empathizing led him to investigate whether higher levels of fetal testosterone explain the increased prevalence of autism spectrum disorders among males in his theory is known as the \"extreme male brain\" theory of autism. A review of his book \"The Essential Difference\" published in \"Nature\" in 2003 summarizes his proposals as: \"the male brain is programmed to systemize and the female brain to empathize ... Asperger's syndrome represents the extreme male brain\".\n\nBaron-Cohen and colleagues extended the E-S theory into the extreme male brain theory of autism, which hypothesizes that autism shows an extreme of the typical male profile. This theory divides people into five groups:\n\nBaron-Cohen says that tests of the E-S model show that twice as many females than males are Type E and twice as many males than females are Type S. 65% of people with autism spectrum conditions are Extreme Type S. The concept of the Extreme Type E brain has been proposed; however, little research has been conducted on this brain profile.\n\nApart from the research using EQ and SQ, several other similar tests also have found female and male differences and that people with autism or Asperger syndrome on average score similarly to but more extremely than the average male. For example, the brain differences model provides a broad overview of sex differences that are represented in individuals with autism, including brain structures and hormone levels.\n\nSome, but not all, studies have found that brain regions that differ in average size between males and females also differ similarly between people who have autism and those who do not have autism.\n\nBaron-Cohen's research on relatives of people with Asperger syndrome and autism found that their fathers and grandfathers are twice as likely to be engineers as the general population. Natural science students have more relatives with autism than humanities students. Another similar finding by Baron-Cohen in California has been referred to as the \"Silicon Valley phenomenon\", where a large portion of the population works in technical fields, and he says autism prevalence rates are ten times higher than the average of the US population. These data suggest that genetics and the environment play a role in autism prevalence, and children with technically minded parents are therefore more likely to be diagnosed with autism.\n\nBaron-Cohen's studies have been questioned. The overrepresentation of engineers could depend on a sampling bias, and a 2010 analysis of autism diagnoses in California did \"not\" find that autism clustered preferentially around areas rich in IT industry. Instead, it found that clusters tended to occur in areas where parents were older and educated to a higher level than were parents in surrounding areas.\n\nAnother possibility has been proposed that spins the perspective of the extreme male brain. Social theorists have been investigating the concept that females have protective factors against autism by having a more developed language repertoire and more empathy skills. Female children speak earlier and use language more than their male counterparts, and the lack of this skill translates into many symptoms of autism, offering another explanation for the discrepancy in prevalence.\n\nThe fetal testosterone theory hypothesizes that higher levels of testosterone in the amniotic fluid of mothers push brain development towards improved ability to see patterns and analyze complex systems while diminishing communication and empathy, emphasizing \"male\" traits over \"female\", or in E-S theory terminology, emphasizing \"systemizing\" over \"empathizing\". This theory states that fetal testosterone influences the development of certain structures in the brain, and that these changes relate to behavioral traits seen in those with autism. Males generally have higher levels of fetal testosterone contributing to their brain developing in that particular way. The extreme male brain theory (EMB), put forward by Baron-Cohen suggests that autistic brains show an exaggeration of the features associated with male brains. These are mainly size and connectivity with males generally having a larger brain with more white matter, leading to increased connectivity in each hemisphere. This is seen in an exaggerated form in the brains of those with ASD. Another feature of male brains is having a smaller corpus callosum in at least some regions leading to decreased inter-hemispheric connectivity. This is also seen in those with ASD. Individuals with ASD were found to have widespread interconnectivity abnormalities in specific brain regions. This could explain the different results on empathy tests between men and women as well as the deficiencies in empathy seen in ASD as empathy requires several brain regions to be activated which need information from many different areas of the brain. A further example of how brain structure can influence ASD is looking at cases where the corpus callosum does not fully develop (agenesis of corpus callosum). It was found that autism is commonly diagnosed in children where the corpus callosum does not fully develop (45% of children with agenesis of the corpus callosum). A further example of brain structures relating to ASD is that children with ASD tend to have a larger amygdala, this is another example of being an extreme version of the male brain which generally has a larger amygdala. These brain differences have all been shown to have an influence on social cognition and communication. High levels of fetal testosterone have also been shown to be related to behavior associated with autism, such as eye contact. Studies examining the relationship between prenatal testosterone levels and autistic traits found that high levels correlated with traits such as decreased eye contact. These were present in both sexes. This suggests that fetal testosterone (fT) is the cause of sex differences in the brain and that there is a link between fT levels and ASD. In general females with autism have a higher rate of medical conditions which are related to high androgen levels and both males and females with autism have higher than average androgen levels. Males have higher fT levels naturally meaning that there is less of a change required in the hormone levels to reach a point high enough to cause the developmental changes seen in autism. This is a possible cause for the male prevalence seen in autism.\n\nThe imprinted brain theory is a somewhat similar although not identical theory. It argues that autism and psychosis are contrasting disorders on a number of variables. This is argued to be due to imbalanced genomic imprinting. According to the imprinted brain theory there could be a mismatch and more severe problems when extreme genomic imprinting occurs in the opposite sex, which would explain why female autism (and male psychosis) is often particularly severe, which is a problem for the \"extreme male brain\" theory which predicts the opposite.\n\nEmpathy can be subdivided into two major components: \n\nStudies found that individuals with autism spectrum disorder (ASD) self-report lower levels of empathic concern, show less or absent comforting responses toward someone who is suffering, and report equal or higher levels of personal distress compared to controls. The combination of reduced empathic concern and increased personal distress may lead to the overall reduction of empathy in ASD.\n\nStudies also suggest that individuals with ASD may have impaired theory of mind, involving the ability to understand the perspectives of others. The terms \"cognitive empathy\" and \"theory of mind\" are often used synonymously, but due to a lack of studies comparing theory of mind with types of empathy, it is unclear whether these are equivalent. Notably, many reports on the empathic deficits of individuals with Asperger syndrome are actually based on impairments in theory of mind.\n\nBaron-Cohen argued that psychopathy is associated with intact cognitive empathy but reduced affective empathy while ASD is associated with both reduced cognitive and affective empathy.\n\nThe theory has been criticized on multiple grounds. Columnist at \"The Guardian\" Madeleine Bunting has summarized some of these aspects in the 2010 article \"The truth about sex difference is that if men are from Mars, so are women\". Some research in systemizing and empathizing in early life indicates that boys and girls develop in similar ways, casting considerable doubt on the theory of sex differences in these areas. A cognitive style that more naturally opposes empathizing is Machiavellianism, which emphasizes self-interest and which has been shown to be strongly correlated with competitiveness; evolutionary theory predicts that males will be more competitive than females. In contrast, research has generally shown a weak negative correlation between empathizing and systemizing.\n\nAnother criticism is that original EQ and SQ, which form most of the research basis behind the notions of empathizing and systemizing, both clearly measure more than one factor, and that sex differences exist on only some of the factors.\n\nAs a basis for his theory, Baron-Cohen cites a study done on newborn infants in which baby boys looked longer at an object and baby girls looked longer at a person. However, a review of studies done with very young children found no consistent differences between boys and girls.\nCritics say that because his work has focused on higher-functioning individuals with autism spectrum disorders, his work requires independent replication with broader samples. A \"Nature\" article published in 2011 says, \"Some critics are also rankled by Baron-Cohen's history of headline-grabbing theories—particularly one that autism is an 'extreme male' brain state. They worry that his theory about technically minded parents may be giving the public wrong ideas, including the impression that autism is linked to being a 'geek'.\"\n\n\"Time\" magazine said Baron-Cohen \"most dramatically wandered into fraught territory in 2003, when he published the book \"The Essential Difference\", which called autism a manifestation of an extreme 'male brain'--one that's 'predominantly hard-wired for understanding and building systems,' as opposed to a 'female brain,' one that's 'predominantly hard-wired for empathy'--and ended up on the wrong side of the debate on science and sex differences.\" A book review published in the journal \"Nature\", wrote: \"The idea that males are more interested in systemizing than females merits serious consideration ... It is unquestionably a novel and fascinating idea that seems likely to generate a rich empirical body of literature as its properties are tested. The second part of the theory—that females are more empathic than males—is more problematic.\"\n\nColleagues Isabelle Rapin and Helen Tager-Flusberg expressed reservations about the theory; Isabelle Rapin ... finds Dr. Baron-Cohen's theory \"provocative\" but adds that \"it does not account for some of the many neurological features of the disorder, like the motor symptoms [such as repetitive movements and clumsiness], the sleep problems or the seizures.\" Others worry that the term \"extreme male brain\" could be misinterpreted. Males are commonly associated with \"qualities such as aggression,\" says Helen Tager-Flusberg ... \"What's dangerous is that's the inference people will make: Oh, these are extreme males.\"\n\n\"Phenomenology and the Cognitive Sciences\" characterized \"The Essential Difference\" as \"very disappointing\" with a \"superficial notion of intelligence\", concluding that Baron-Cohen's major claims about mind-blindness and systemizing–empathizing are \"at best, dubious\". \"The Spectator\" says that \"The emphasis on the ultra-maleness approach is no doubt attributable to the fact that Baron-Cohen works mainly with higher functioning autism and Asperger's syndrome.\"\n\nIn her 2010 book \"Delusions of Gender\", Cordelia Fine used Baron-Cohen's views as an example of \"neurosexism\"; she also criticized some of the experimental work that Baron-Cohen claims supports his views as being methodologically flawed. He replied to her critique in \"The Guardian\" and in \"The Psychologist\" publication.\n\nIn her 2017 book \"\", Angela Saini has relaunched the critique that the studies from Baron-Cohen and his colleagues carry on Darwin's \"idea that man and woman...evolved to meet their roles of hunter and gatherer, respectively.\"\n\n\n"}
{"id": "55698833", "url": "https://en.wikipedia.org/wiki?curid=55698833", "title": "Envelope (category theory)", "text": "Envelope (category theory)\n\nIn :Category theory and related fields of mathematics, an envelope is a construction that generalizes the operations of \"exterior completion\", like completion of a locally convex space, or Stone–Čech compactification of a topological space. A dual construction is called refinement. \n\nSuppose formula_1 is a category, formula_2 an object in formula_1, and formula_4 and formula_5 two classes of morphisms in formula_1. The definition of an envelope of formula_2 in the class formula_4 with respect to the class formula_5 consists of two steps.\nNotations:\n\nformula_39\n\nIn a special case when formula_4 is a class of all morphisms whose ranges belong to a given class of objects formula_41 in formula_1 it is convenient to replace formula_4 with formula_41 in the notations (and in the terms):\nformula_45\n\nSimilarly, if formula_5 is a class of all morphisms whose ranges belong to a given class of objects formula_47 in formula_1 it is convenient to replace formula_5 with formula_47 in the notations (and in the terms):\nformula_51\n\nFor example, one can speak about an \"envelope of formula_2 in the class of objects formula_41 with respect to the class of objects formula_47\":\n\nformula_55\n\n\nEnvelopes appear as standard functors in various fields of mathematics. Apart from the examples given above, \n\n\n\nIn abstract harmonic analysis the notion of envelope plays a key role in the generalizations of the Pontryagin duality theory to the clases of non-commutative groups: the holomorphic, the smooth and the continuous envelopes of stereotype algebras (in the examples given above) lead respectively to the constructions of the holomorphic, the smooth and the continuous dualities in \"big geometric disciplines\" – complex geometry, differential geometry, and topology – for certain classes of (not necessarily commutative) topological groups considered in these disciplines (affine algebraic groups, and some classes of Lie groups and Moore groups).\n\n\n"}
{"id": "41144410", "url": "https://en.wikipedia.org/wiki?curid=41144410", "title": "Equivariant topology", "text": "Equivariant topology\n\nIn mathematics, equivariant topology is the study of topological spaces that possess certain symmetries. In studying topological spaces, one often considers continuous maps formula_1, and while equivariant topology also considers such maps, there is the additional constraint that each map \"respects symmetry\" in both its domain and target space.\n\nThe notion of symmetry is usually captured by considering a Group action of formula_2 on formula_3 and formula_4 and demanding that formula_5 is equivariant under this action, so that formula_6 for all formula_7, a property usually denoted by formula_8 . Heuristically speaking, standard topology views two spaces as equivalent \"up to deformation,\" but equivariant topology considers spaces equivalent up to deformation so long as it pays attention to any symmetry possessed by both spaces. A famous theorem of equivariant topology is the Borsuk–Ulam theorem , which asserts that every formula_9-equivariant map formula_10 necessarily vanishes.\n\nAn important construction used in Equivariant cohomology and other applications includes a naturally occurring group bundle ( see Principal bundle for details.)\n\nLet us first consider the case where formula_2 acts freely on formula_3. Then, given a formula_2-equivariant map formula_15, we obtain sections formula_16 given by formula_17,\n\nwhere formula_18 gets the diagonal action, formula_19 and the bundle is formula_20, with fiber formula_4 and projection given by formula_22. Often, the total space is written formula_23\n\nMore generally, the assignment formula_24 actually does not map to formula_25 generally. Since formula_5 is equivariant, if formula_27(the isotropy subgroup), then by equivariance, we have that formula_28, so in fact formula_5 will map to the collection of formula_30. In this case, one can replace the bundle by a homotopy quotient where formula_2 acts freely and is bundle homotopic to the induced bundle on formula_3 by formula_5.\n\nIn the same way that one can deduce the Ham sandwich theorem from the Borsuk-Ulam Theorem, one can find many applications of equivariant topology to problems of discrete geometry. This is accomplished by using the Configuration-Space Test-Map paradigm:\n\nGiven a geometric problem formula_34, we define the \"configuration space\", formula_3, which parametrizes all associated solutions to the problem (such as points, lines, or arcs.) Additionally, we consider a \"test space\" formula_36 and a map formula_37 where formula_38 is a solution to a problem if and only if formula_39. Finally, it is usual to consider natural symmetries in a discrete problem by some group formula_2 that acts on formula_3 and formula_42 so that formula_5 is equivariant under these actions. The problem is solved if we can show the nonexistence of an equivariant map formula_44.\n\nObstructions to the existence of such maps are often formulated algebraically from the topological data of formula_3 and formula_46. An archetypal example of such an obstruction can be derived having formula_42 a vector space and formula_48. In this case, a nonvanishing map would also induce a nonvanishing section formula_49 from the discussion above, so formula_50, the top Stiefel–Whitney class would need to vanish.\n\n\n"}
{"id": "26783732", "url": "https://en.wikipedia.org/wiki?curid=26783732", "title": "Everything Will Be OK (book)", "text": "Everything Will Be OK (book)\n\nEverything Will Be OK is the title of a hard-cover folio format book and full-length DVD published in 2005 in Italy. It is primarily a collection of videos, animations and print-specific artworks by artists working in and around the internet and is published by \"This is a magazine\" (about nothing). It is the fourth book from a series of experimental publications by Donnachie, Simionato & Son.\n\nAdrian Shaughnessy, chief-editor of \"Varoom Magazine\" (UK) said “This is not a book, it is an event. Like Helmut Newton’s book-with-table, or the repackaging of Andy Warhol’s \"Interview\" magazine in a suitcase-with-wheels, ...it is more like a portable multi-media exhibition than a book...Donnachie/Simionato have created a publishing phenomenon.”\n\nIncluded with each copy of the book is the DVD of the same title containing original time-based artworks and video. The book also contained a plastic devil's horn (corna) lucky charm bookmark, a sealed sugar-cube, and a pink latex balloon printed with the words \"Everything will be OK\".\n\nThe DVD contains a number of easter eggs viewable by moving left twice then pressing play when selecting Augustin Gimel's film title from the index. It also has a hidden sub-titles track which when viewed becomes a second layer of black silhouettes for the video \"Holes in Between\" by Donnachie/Simionato with original music by the avant-garde band Starfuckers. An archive of This is a magazine online editions from 2002 to 2004, including the Everything will be OK Powerpoint episode is accessible from the DVD-ROM section.\n\nAn associated net art called EverythingwillbeOK.com comprising a video-loop of an inflatable stick figure has been maintained since 2004.\n\nThe Everything will be OK project, including the original video DVD and hard-cover book is part of the KIOSK collection, Christoph Keller's touring art publications archive, now a permanent collection of the National Art Library in Berlin, and has been exhibited at the ICA (USA), the Witte de With Center for Contemporary Art Rotterdam (Rotterdam), Artists Space (NY), the Emily Carr Institute (Vancouver), and Mudam (Luxembourg), among others.\n\n"}
{"id": "262256", "url": "https://en.wikipedia.org/wiki?curid=262256", "title": "Gasification", "text": "Gasification\n\nGasification is a process that converts organic- or fossil fuel-based carbonaceous materials into carbon monoxide, hydrogen and carbon dioxide. This is achieved by reacting the material at high temperatures (>700 °C), without combustion, with a controlled amount of oxygen and/or steam. The resulting gas mixture is called syngas (from synthesis gas) or producer gas and is itself a fuel. The power derived from gasification and combustion of the resultant gas is considered to be a source of renewable energy if the gasified compounds were obtained from biomass.\n\nThe advantage of gasification is that using the syngas (synthesis gas H2/CO) is potentially more efficient than direct combustion of the original fuel because it can be combusted at higher temperatures or even in fuel cells, so that the thermodynamic upper limit to the efficiency defined by Carnot's rule is higher or (in case of fuel cells) not applicable. Syngas may be burned directly in gas engines, used to produce methanol and hydrogen, or converted via the Fischer–Tropsch process into synthetic fuel. Gasification can also begin with material which would otherwise have been disposed of such as biodegradable waste. In addition, the high-temperature process refines out corrosive ash elements such as chloride and potassium, allowing clean gas production from otherwise problematic fuels. Gasification of fossil fuels is currently widely used on industrial scales to generate electricity.\n\nThe process of producing energy using the gasification method has been in use for more than 180 years. In the early time coal and peat were used to power these plants. Initially developed to produce town gas for lighting and cooking in the 1800s, this was replaced by electricity and natural gas, it was also used in blast furnaces but the bigger role was played in the production of synthetic chemicals where it has been in use since the 1920s.\n\nDuring both world wars, especially the World War II, the need for fuel produced by gasification reemerged due to the shortage of petroleum. Wood gas generators, called Gasogene or Gazogène, were used to power motor vehicles in Europe. By 1945 there were trucks, buses and agricultural machines that were powered by gasification. It is estimated that there were close to 9,000,000 vehicles running on producer gas all over the world.\n\nIn a gasifier, the carbonaceous material undergoes several different processes:\n\nIn essence, a limited amount of oxygen or air is introduced into the reactor to allow some of the organic material to be \"burned\" to produce carbon dioxide and energy, which drives a second reaction that converts further organic material to hydrogen and additional carbon dioxide. Further reactions occur when the formed carbon monoxide and residual water from the organic material react to form methane and excess carbon dioxide (formula_4). This third reaction occurs more abundantly in reactors that increase the residence time of the reactive gases and organic materials, as well as heat and pressure. Catalysts are used in more sophisticated reactors to improve reaction rates, thus moving the system closer to the reaction equilibrium for a fixed residence time.\n\nSeveral types of gasifiers are currently available for commercial use: counter-current fixed bed, co-current fixed bed, fluidized bed, entrained flow, plasma, and free radical.\n\nA fixed bed of carbonaceous fuel (e.g. coal or biomass) through which the \"gasification agent\" (steam, oxygen and/or air) flows in counter-current configuration. The ash is either removed in the dry condition or as a slag. The slagging gasifiers have a lower ratio of steam to carbon, achieving temperatures higher than the ash fusion temperature. The nature of the gasifier means that the fuel must have high mechanical strength and must ideally be non-caking so that it will form a permeable bed, although recent developments have reduced these restrictions to some extent. The throughput for this type of gasifier is relatively low. Thermal efficiency is high as the temperatures in the gas exit are relatively low. However, this means that tar and methane production is significant at typical operation temperatures, so product gas must be extensively cleaned before use. The tar can be recycled to the reactor.\n\nIn the gasification of fine, undensified biomass such as rice hulls, it is necessary to blow air into the reactor by means of a fan. This creates very high gasification temperature, as high as 1000 C. Above the gasification zone, a bed of fine and hot char is formed, and as the gas is blow forced through this bed, most complex hydrocarbons are broken down into simple components of hydrogen and carbon monoxide.\n\nSimilar to the counter-current type, but the gasification agent gas flows in co-current configuration with the fuel (downwards, hence the name \"down draft gasifier\"). Heat needs to be added to the upper part of the bed, either by combusting small amounts of the fuel or from external heat sources. The produced gas leaves the gasifier at a high temperature, and most of this heat is often transferred to the gasification agent added in the top of the bed, resulting in an energy efficiency on level with the counter-current type. Since all tars must pass through a hot bed of char in this configuration, tar levels are much lower than the counter-current type.\n\nThe fuel is fluidized in oxygen and steam or air. The ash is removed dry or as heavy agglomerates that defluidize. The temperatures are relatively low in dry ash gasifiers, so the fuel must be highly reactive; low-grade coals are particularly suitable. The agglomerating gasifiers have slightly higher temperatures, and are suitable for higher rank coals. Fuel throughput is higher than for the fixed bed, but not as high as for the entrained flow gasifier. The conversion efficiency can be rather low due to elutriation of carbonaceous material. Recycle or subsequent combustion of solids can be used to increase conversion. Fluidized bed gasifiers are most useful for fuels that form highly corrosive ash that would damage the walls of slagging gasifiers. Biomass fuels generally contain high levels of corrosive ash.\n\nA dry pulverized solid, an atomized liquid fuel or a fuel slurry is gasified with oxygen (much less frequent: air) in co-current flow. The gasification reactions take place in a dense cloud of very fine particles. Most coals are suitable for this type of gasifier because of the high operating temperatures and because the coal particles are well separated from one another.\n\nThe high temperatures and pressures also mean that a higher throughput can be achieved, however thermal efficiency is somewhat lower as the gas must be cooled before it can be cleaned with existing technology. The high temperatures also mean that tar and methane are not present in the product gas; however the oxygen requirement is higher than for the other types of gasifiers. All entrained flow gasifiers remove the major part of the ash as a slag as the operating temperature is well above the ash fusion temperature.\n\nA smaller fraction of the ash is produced either as a very fine dry fly ash or as a black colored fly ash slurry. Some fuels, in particular certain types of biomasses, can form slag that is corrosive for ceramic inner walls that serve to protect the gasifier outer wall. However some entrained flow type of gasifiers do not possess a ceramic inner wall but have an inner water or steam cooled wall covered with partially solidified slag. These types of gasifiers do not suffer from corrosive slags.\n\nSome fuels have ashes with very high ash fusion temperatures. In this case mostly limestone is mixed with the fuel prior to gasification. Addition of a little limestone will usually suffice for the lowering the fusion temperatures. The fuel particles must be much smaller than for other types of gasifiers. This means the fuel must be pulverized, which requires somewhat more energy than for the other types of gasifiers. By far the most energy consumption related to entrained flow gasification is not the milling of the fuel but the production of oxygen used for the gasification.\n\nIn a plasma gasifier a high-voltage current is fed to a torch, creating a high-temperature arc. The inorganic residue is retrieved as a glass like substance.\n\nThere are a large number of different feedstock types for use in a gasifier, each with different characteristics, including size, shape, bulk density, moisture content, energy content, chemical composition, ash fusion characteristics, and homogeneity of all these properties. Coal and petroleum coke are used as primary feedstocks for many large gasification plants worldwide. Additionally, a variety of biomass and waste-derived feedstocks can be gasified, with wood pellets and chips, waste wood, plastics and aluminium, Municipal Solid Waste (MSW), Refuse-derived fuel (RDF), agricultural and industrial wastes, sewage sludge, switch grass, discarded seed corn, corn stover and other crop residues all being used.\n\nChemrec has developed a process for gasification of black liquor.\n\nWaste gasification has several advantages over incineration:\n\nA major challenge for waste gasification technologies is to reach an acceptable (positive) gross electric efficiency. The high efficiency of converting syngas to electric power is counteracted by significant power consumption in the waste preprocessing, the consumption of large amounts of pure oxygen (which is often used as gasification agent), and gas cleaning. Another challenge becoming apparent when implementing the processes in real life is to obtain long service intervals in the plants, so that it is not necessary to close down the plant every few months for cleaning the reactor.\n\nEnvironmental advocates have called gasification \"incineration in disguise\" and argue that the technology is still dangerous to air quality and public health. \"Since 2003 numerous proposals for waste treatment facilities hoping to use... gasification technologies failed to receive final approval to operate when the claims of project proponents did not withstand public and governmental scrutiny of key claims,\" according to the Global Alliance for Incinerator Alternatives. One facility which operated from 2009–2011 in Ottawa had 29 \"emissions incidents\" and 13 \"spills\" over those three years. It was also only able to operate roughly 25% of the time.\n\nSeveral waste gasification processes have been proposed, but few have yet been built and tested, and only a handful have been implemented as plants processing real waste, and most of the time in combination with fossil fuels.\n\nOne plant (in Chiba, Japan using the Thermoselect process) has been processing industrial waste with natural gas and purified oxygen since year 2000, but has not yet documented positive net energy production from the process.\n\nIn 2007 Ze-gen erected a waste gasification demonstration facility in New Bedford, Massachusetts. The facility was designed to demonstrate gasification of specific non-MSW waste streams using \"liquid metal gasification\". This facility came after widespread public opposition shelved plans for a similar plant in Attleboro, Massachusetts. Today Ze-gen appears to be defunct, and the company website was taken down in 2014.\n\nAlso in the US, in 2011 a plasma system delivered by PyroGenesis Canada Inc. was tested to gasify municipal solid waste, hazardous waste and biomedical waste at the Hurlburt Field Florida Special Operations Command Air Force base. The plant, which cost $7.4 million to construct, was closed and sold at a government liquidation auction in May 2013. The opening bid was $25. The winning bid was sealed.\n\nSyngas can be used for heat production and for generation of mechanical and electrical power. Like other gaseous fuels, producer gas gives greater control over power levels when compared to solid fuels, leading to more efficient and cleaner operation.\n\nSyngas can also be used for further processing to liquid fuels or chemicals.\n\nGasifiers offer a flexible option for thermal applications, as they can be retrofitted into existing gas fueled devices such as ovens, furnaces, boilers, etc., where syngas may replace fossil fuels. Heating values of syngas are generally around 4–10 MJ/m.\n\nCurrently Industrial-scale gasification is primarily used to produce electricity from fossil fuels such as coal, where the syngas is burned in a gas turbine. Gasification is also used industrially in the production of electricity, ammonia and liquid fuels (oil) using Integrated Gasification Combined Cycles (IGCC), with the possibility of producing methane and hydrogen for fuel cells. IGCC is also a more efficient method of CO capture as compared to conventional technologies. IGCC demonstration plants have been operating since the early 1970s and some of the plants constructed in the 1990s are now entering commercial service.\n\nIn small business and building applications, where the wood source is sustainable, 250–1000 kWe and new zero carbon biomass gasification plants have been installed in Europe that produce tar free syngas from wood and burn it in reciprocating engines connected to a generator with heat recovery. This type of plant is often referred to as a wood biomass CHP unit but is a plant with seven different processes: biomass processing, fuel delivery, gasification, gas cleaning, waste disposal, electricity generation and heat recovery.\n\nDiesel engines can be operated on dual fuel mode using producer gas. Diesel substitution of over 80% at high loads and 70–80% under normal load variations can easily be achieved. Spark ignition engines and solid oxide fuel cells can operate on 100% gasification gas. Mechanical energy from the engines may be used for e.g. driving water pumps for irrigation or for coupling with an alternator for electrical power generation.\n\nWhile small scale gasifiers have existed for well over 100 years, there have been few sources to obtain a ready to use machine. Small scale devices are typically DIY projects. However, currently in the United States, several companies offer gasifiers to operate small engines.\n\nIn principle, gasification can proceed from just about any organic material, including biomass and plastic waste. The resulting syngas can be combusted. Alternatively, if the syngas is clean enough, it may be used for power production in gas engines, gas turbines or even fuel cells, or converted efficiently to dimethyl ether (DME) by methanol dehydration, methane via the Sabatier reaction, or diesel-like synthetic fuel via the Fischer–Tropsch process. In many gasification processes most of the inorganic components of the input material, such as metals and minerals, are retained in the ash. In some gasification processes (slagging gasification) this ash has the form of a glassy solid with low leaching properties, but the net power production in slagging gasification is low (sometimes negative) and costs are higher.\n\nRegardless of the final fuel form, gasification itself and subsequent processing neither directly emits nor traps greenhouse gases such as carbon dioxide. Power consumption in the gasification and syngas conversion processes may be significant though, and may indirectly cause CO emissions; in slagging and plasma gasification, the electricity consumption may even exceed any power production from the syngas.\n\nCombustion of syngas or derived fuels emits exactly the same amount of carbon dioxide as would have been emitted from direct combustion of the initial fuel. Biomass gasification and combustion could play a significant role in a renewable energy economy, because biomass production removes the same amount of CO from the atmosphere as is emitted from gasification and combustion. While other biofuel technologies such as biogas and biodiesel are carbon neutral, gasification in principle may run on a wider variety of input materials and can be used to produce a wider variety of output fuels.\n\nThere are at present a few industrial scale biomass gasification plants. Since 2008 in Svenljunga, Sweden, a biomass gasification plant generates up to 14 MW, supplying industries and citizens of Svenljunga with process steam and district heating, respectively. The gasifier uses biomass fuels such as CCA or creosote impregnated waste wood and other kinds of recycled wood to produces syngas that is combusted on site. In 2011 a similar gasifier, using the same kinds of fuels, is being installed at Munkfors Energy's CHP plant. The CHP plant will generate 2 MW (electricity) and 8 MW (district heating).\n\nExamples of demonstration projects include:\n\n\n"}
{"id": "232627", "url": "https://en.wikipedia.org/wiki?curid=232627", "title": "Gender-blind", "text": "Gender-blind\n\nA gender-blind (or unisex) person is someone who adheres to not distinguishing people by gender. Gender blind people generally advocate gender neutrality in society, such as activities undertaken and services provided without regard to the gender of those who participate. Those who identify as pansexual may also refer to themselves as 'gender-blind': however, pansexuality emphasizes gender-blindness in sexuality.\n\n\"Unisex\" is an older term, and a misnomer meaning \"one sex\". It should not be confused with bisexuality.\n\"Gender-blind\" however goes against most tenets of heteronormativity by not looking at gender at all.\n\nIn 2006 the National Student Genderblind Campaign was created as a collaborative grassroots organization intended to educate college students, administrators, and others throughout the United States. The NSGC advocates for the implementation of gender-inclusive dorm room and bathroom options.\n\nThe use of mixed-gender hospital rooms has proved controversial in both the United Kingdom and Canada. Manitoba's Health Minister, Theresa Oswald, has campaigned actively against such rooms after receiving complaints from a Winnipeg patient, Ollie Ingram. Oswald said if humanity can \"put somebody on the moon\", it can find a way to honor gender requests without leading to delays for patients. At the same time, some medical ethicists have been critical of efforts to return to single-sex rooms. Jacob Appel, an advocate for mixed rooms in the United States, has written that opposition to gender-mixed rooms stems from \"old-fashioned prejudice\" and argues that: \"Because some people have been brought up to fear or dislike sharing a room with a person of the opposite sex, or blush at the prospect of catching a glimpse of an unwelcome body part when a robe slips open, we enshrine and perpetuate this prejudice in social policy.\" Great Britain has agreed to phase out such rooms by 2010.\n\nMuch as with similar approaches to dealing with racism and ethnicity, not recognising and taking account of participants' sex can be harmful. It posits that it functions in a post-sexism society where women are no longer treated differently than men on the basis of their sex. Meanwhile, gendered treatment prevails all over the world. Of a study of organisations which offered women-only services, 23% said that their reason was based on women's inequality and the desire to address that imbalance; 20% that women-only spaces promote female development and empowerment; 18% that they were providing a service not being met by unisex services and which focused on the specific needs of women.\n\nThe legal test of the \"reasonable person\" has been criticised for being genderblind to be applied in some areas of the law, particularly sexual harassment. Women are subjected to more normalised and endemic sexual harassment than men. On the grounds of this, the American case of \"Ellison v. Brady\" 924 F.2d 872 (1991), the court held that \"a sex-blind reasonable person standard tends to be male-based and tends to systematically ignore the experiences of women\". \n\nStudies indicate a broad support for single-sex service options to remain available. Of 1000 women polled by the Women's Resource Centre, 97% stated that women should have the option of accessing female-only services if they were victims of sexual assault. 57% indicated that they would choose a women-only gym over a mixed gym. Single-sex services can have a benefit in providing greater comfort and engaging participants who would otherwise not get involved.\n\n"}
{"id": "1785007", "url": "https://en.wikipedia.org/wiki?curid=1785007", "title": "Gerda Alexander", "text": "Gerda Alexander\n\nGerda Alexander (February 15, 1908 – February 21, 1994) was a Danish teacher who devised a method of self-development called Eutony. She was born in Wuppertal, Germany, but moved to Denmark in 1929.\n\nAlexander's parents were believers in eurythmy, passing on to her a similar interest in movement. Alexander as a young woman contracted rheumatic fever and endocarditis, suffering several crises. This inspired her to find ways to move that did not exacerbate her symptoms. Long periods of rest stimulated her to look within herself looking for a \"more economic\" and more spontaneous form of movement, starting with learning regulation of muscle tone.\n\nBy means of observation and reflection on her students, their own ailments and difficulties in mobility, and the investigation of the neuro-psychological bases of human movement, she molded her own method. She postulated that \"it is important, in treatment, not to give and do more than is necessary, so that the other can rely on himself. It is not that I am the great master who gives you help. Rather, I can introduce you to my work for your own self discovery.\"\n\nQuackwatch describes Alexander's invention, Eutony, as \"form of body-centered psychotherapy\" which \"posits 'blocked energy' and a collective unconscious\".\n"}
{"id": "38521219", "url": "https://en.wikipedia.org/wiki?curid=38521219", "title": "History of type theory", "text": "History of type theory\n\nThe type theory was initially created to avoid paradoxes in a variety of formal logics and rewrite systems. Later, type theory referred to a class of formal systems, some of which can serve as alternatives to naive set theory as a foundation for all mathematics.\n\nIt has been tied to formal mathematics since \"Principia Mathematica\" to today's proof assistants.\n\nIn a letter to Gottlob Frege (1902) Russell announced his discovery of the paradox in Frege's Begriffsschrift. Frege promptly responded, acknowledging the problem and proposing a solution in a technical discussion of \"levels\". To quote Frege:\n\nIncidentally, it seems to me that the expression \"a predicate is predicated of itself\" is not exact. A predicate is as a rule a first-level function, and this function requires an object as argument and cannot have itself as argument (subject). Therefore I would prefer to say \"a concept is predicated of its own extension\".\n\nHe goes about showing how this might work but seems to pull back from it. As a consequence of what has become known as Russell's paradox both Frege and Russell had to quickly amend works that they had at the printers. In an Appendix B that Russell tacked onto his \"The Principles of Mathematics\" (1903) one finds his \"tentative\" theory of types. The matter plagued Russell for about five years.\n\nWillard Quine presents a historical synopsis of the origin of the theory of types and the \"ramified\" theory of types: after considering abandoning the theory of types (1905), Russell proposed in turn three theories:\nQuine observes that Russell's introduction of the notion of \"apparent variable\" had the following result:\nthe distinction between 'all' and 'any': 'all' is expressed by the bound ('apparent') variable of universal quantification, which ranges over a type, and 'any' is expressed by the free ('real') variable which refers schematically to any unspecified thing irrespective of type.\n\nQuine dismisses this notion of \"bound variable\" as \"pointless apart from a certain aspect of the theory of types\".\n\nQuine explains the \"ramified\" theory as follows: \"It has been so called because the type of a function depends both on the types of its arguments and on the types of the apparent variables contained in it (or in its expression), in case these exceed the types of the arguments\". Stephen Kleene in his 1952 \"Introduction to Metamathematics\" describes the \"ramified\" theory of types this way:\n\nBut because the stipulations of the ramified theory would prove (to quote Quine) \"onerous\", Russell in his 1908 \"Mathematical logic as based on the theory of types\" also would propose his \"axiom of reducibility\". By 1910 Whitehead and Russell in their \"Principia Mathematica\" would further augment this axiom with the notion of a \" matrix \" — a fully extensional specification of a function. From its matrix a function could be derived by the process of \"generalization\" and vice versa, i.e. the two processes are reversible — (i) generalization from a matrix to a function (by using apparent variables) and (ii) the reverse process of reduction of type by courses-of-values substitution of arguments for the apparent variable. By this method impredicativity could be avoided.\n\nEventually Emil Post (1921) would lay waste to Russell's \"cumbersome\" Theory of Types with his \"truth functions\" and their truth tables. In his \"Introduction\" to his 1921 Post places the blame on Russell's notion of apparent variable: \"Whereas the complete theory [of Whitehead and Russell (1910, 1912, 1913)] requires for the enunciation of its propositions real and apparent variables, which represent both individuals and propositional functions of different kinds, and as a result necessitates the cumbersome theory of types, this subtheory uses only real variables, and these real variables represent but one kind of entity, which the authors have chosen to call elementary propositions\".\n\nAt about the same time Ludwig Wittgenstein made short work of the theory of types in his 1922 work Tractatus Logico-Philosophicus in which he points out the following in parts 3.331–3.333:\n\nWittgenstein proposed the truth-table method as well. In his 4.3 through 5.101, Wittgenstein adopts an unbounded Sheffer stroke as his fundamental logical entity and then lists all 16 functions of two variables (5.101).\n\nThe notion of matrix-as-truth-table appears as late as the 1940–1950s in the work of Tarski, e.g. his 1946 indexes \"Matrix, see: Truth table\"\n\nRussell in his 1920 \"Introduction to Mathematical Philosophy\" devotes an entire chapter to \"The axiom of Infinity and logical types\" wherein he states his concerns: \"Now the theory of types emphatically does not belong to the finished and certain part of our subject: much of this theory is still inchoate, confused, and obscure. But the need of \"some\" doctrine of types is less doubtful than the precise form the doctrine should take; and in connection with the axiom of infinity it is particularly easy to see the necessity of some such doctrine\".\n\nRussell abandons the axiom of reducibility: In the second edition of \"Principia Mathematica\" (1927) he acknowledges Wittgenstein's argument. At the outset of his Introduction he declares \"there can be no doubt ... that there is no need of the distinction between real and apparent variables...\". Now he fully embraces the matrix notion and declares \"A \"function can only appear in a matrix through its values\"\" (but demurs in a footnote: \"It takes the place (not quite adequately) of the axiom of reducibility\"). Furthermore, he introduces a new (abbreviated, generalized) notion of \"matrix\", that of a \"logical matrix . . . one that contains no constants. Thus \"p\"|\"q\" is a logical matrix\". Thus Russell has virtually abandoned the axiom of reducibility, but in his last paragraphs he states that from \"our present primitive propositions\" he cannot derive \"Dedekindian relations and well-ordered relations\" and observes that if there is a new axiom to replace the axiom of reducibility \"it remains to be discovered\".\n\nIn the 1920s, Leon Chwistek and Frank P. Ramsey noticed that, if one is willing to give up the vicious circle principle,\nthe hierarchy of levels of types in the \"ramified theory of types\" can be collapsed.\n\nThe resulting restricted logic is called the theory of simple types or, perhaps more commonly, simple type theory. Detailed formulations of simple type theory were published in the late 1920s and early 1930s\nby R. Carnap, F. Ramsey, W.V.O. Quine, and A. Tarski. In 1940 Alonzo Church (re)formulated it as simply typed lambda calculus. and examined by Gödel in 1944. A survey of these developments is found in Collins (2012).\n\nKurt Gödel in his 1944 \"Russell's mathematical logic\" gave the following definition of the \"theory of simple types\" in a footnote:\n\nHe concluded the (1) theory of simple types and (2) axiomatic set theory, \"permit the derivation of modern mathematics and at the same time avoid all known paradoxes\" (Gödel 1944:126); furthermore, the theory of simple types \"is the system of the first \"Principia\" [\"Principia Mathematica\"] in an appropriate interpretation. . . . [M]any symptoms show only too clearly, however, that the primitive concepts need further elucidation\" (Gödel 1944:126).\n\nThe Curry–Howard correspondence is the interpretation of proofs-as-programs and formulae-as-types. The idea starting in 1934 with Haskell Curry and finalized in 1969 with William Alvin Howard. It connected the \"computational component\" of many type theories to the derivations in logics.\n\nHoward showed that the typed lambda calculus corresponded to intuitionistic natural deduction. (That is, natural deduction without the Law of excluded middle.) The connection between types and logic lead to a lot of subsequent research to find new type theories for existing logics and new logics for existing type theories.\n\nNicolaas Govert de Bruijn created the type theory Automath as a mathematical foundation for the Automath system, which could verify the correctness of proofs. The system developed and added features over time as type theory developed.\n\nPer Martin-Löf found a type theory that corresponded to predicate logic by introducing dependent types, which became known as intuitionistic type theory or Martin-Löf type theory.\n\nMartin-Löf's theory uses inductive types to represent unbounded data structures, such as natural numbers.\n\nThe lambda cube was not a new type theory but a categorization of existing type theories. The eight corners of the cube included some existing theories with simply typed lambda calculus at the lowest corner and the calculus of constructions at the highest.\n\n\n"}
{"id": "27138739", "url": "https://en.wikipedia.org/wiki?curid=27138739", "title": "Humanity (virtue)", "text": "Humanity (virtue)\n\nHumanity is a virtue associated with basic ethics of altruism derived from the human condition.\n\nHumanity differs from mere justice in that there is a level of altruism towards individuals included in humanity more so than the fairness found in justice. That is, humanity, and the acts of love, altruism, and social intelligence are typically individual strengths while fairness is generally expanded to all. \nPeterson & Seligman in \"\" (2004) class humanity as one of six virtues that are consistent across all cultures.\n\nThe concept goes back to the development of \"humane\" or \"humanist\" philosophy during the Renaissance (with predecessors in 13th-century scholasticism stressing a concept of basic human dignity inspired by Aristotelianism) and the concept of humanitarianism in the early modern period, and resulted in modern notions such as \"human rights\".\n\nConfucius said that humanity, or “Ren”(仁), is a “love of people” stating “if you want to make a stand, help others make a stand.” That is, the Confucian theory of humanity exemplifies the golden rule. It is so central to Confucian thought that it appears 58 times in the Analects. Similar to the Christian process of seeking God, Confucius teachers seeking Ren to a point of seemingly divine mastery until you are equal to, or better than, your teacher. The Confucian concept of Ren encompasses both love and altruism.\n\nPlato and Aristotle both wrote extensively on the subject of virtues, though neither ever wrote on humanity as a virtue, despite highly valuing love and kindness, two of the strengths of humanity. Plato and Aristotle considered \"courage, justice, temperance\" and \"generosity, wit, friendliness, truthfulness, magnificence, and greatness of soul\" to be the sole virtues, respectively.\n\nHumanity is one of Thomas Aquinas' \"Seven Heavenly Virtues.\" Beyond that, humanity was so important in some positivist Christian cultures that it was to be capitalized like God. \nKindness, altruism and love are all mentioned in the bible. Proverbs 19:22 \"states the desire of a man is his kindness.\" On the topic of altruism, emphasis is placed on helping strangers (Hebrews 13:1) and the biblical adage \"it is better to give than to receive\" (Acts 20:35).\n\nLove has many different definitions ranging from a set of purely biological and chemical processes to a religious concept. As a character strength, love is a mutual feeling between two people characterized by attachment, comfort, and generally positive feelings. It can be broken down into 3 categories: love between a child and their parents, love for your friends, and romantic love. Having love as a strength is not about the capacity to love, as such, it is about being involved in a loving relationship.\n\nLove, in the psychological sense, is most often studied in terms of attachment to others. A degree of controversy surrounds defining and researching love in this way, as it takes away the “mystery of love.” Because love is mysterious, to an extent, it is most often studied in terms of attachment theory, because it can be studied in the way across ages. In infants, attachment is studied through the Strange Situation Test. Attachment to an individual, usually the mother, is determined by how distressed the infant becomes when the mother is taken out of the experimental setting. There are several models of adult attachment including the Adult Attachment Interviews (Kaplan & Main), Adult Attachment Prototypes (Hazan & Shaver) and more. Generally adult attachment models focus on the mental representation of the visible signs of attachment seen in infants.\n\nEvidence in support of the benefits of love are seen in the negative affect states that result from lacking love. Orphaned children have been targeted in studies about negative attributes resulting from lack of attachment. Smyke \"et al.\" found that children raised in an environment that didn’t allow children to become attached to their preferred caregivers experienced attachment disorders. Additionally, individuals who develop securely attached have a lower likelihood of depression, high-self esteem, and less likelihood of divorce.\n\nThe strength kindness encompasses most related terms that evoke feelings of altruism, generosity, helpfulness and a general desire to help people. That is, a disposition for helping humanity. The following statements are from the Values in Action (VIA) psychological assessment, aimed at determining people's strengths in kindness: others are just as important to me, giving is more important than receiving, I care for the ungrateful as well as the grateful. Kindness, as a part of humanity, is deeply rooted in philosophical and religious traditions, each having words for the altruistic love aspect of kindness, such as \"agape\" in Greek, \"chesed\" in Hebrew, and the Latin word \"philantropia\", the root of the word \"philanthropy.\" Kindness is so valued as a strength beyond religious and theoretical concepts that it is advocated through school community service programs and national programs like AmeriCorps. Additionally, while gender differences in kindness are statistically significant, they are minimal, and the methods of testing used may not always have construct validity.\n\nKindness is most often measured on a case by case measure and not usually as a trait. The Self-Report Altruism Scale and the Altruism Facet Scale for Agreeableness Measure of the Revised NEO Personality Inventory (NEO-PI-R) psychological assessment are often used to ask people how often they engage in altruistic behaviors and gauge their concern for others. The former, however, only asks about 20 specific altruistic acts, leaving out a wide range of altruistic behaviors.\n\nThere are numerous benefits from kindness and altruism on both sides of the action. For some, the motivation to be kind comes from a place of egoistic motivation, and thus the benefit to them is the positive affect state they receive from the action. Another study found that the process of being kind develops pro-social skills in children, which positively effects their attachments and relationships. Additionally, volunteerism in the elderly has shown to lead to decreased risk of early death, and mental health benefits. One thing to note is the difference between altruism as a trait and as an act.\n\nSocial intelligence is the most modern of the three strengths associated with humanity. The Character Strengths and Virtues (CSV) psychological assessment defines social intelligence as the ability to understand “relationships with other people, including the social relationships involved in intimacy and trust, persuasion, group membership, and political power.”\n\nIntelligence has many psychological definitions from Weschler’s intelligence to the various theories of multiple intelligence. The CSV divides intelligence into hot and cold, hot intelligence being those intelligences related to active emotional processes. (338) Individuals with high social intelligence are very self-aware, and effective organizers and leaders. Additionally, it combines elements of the other two hot intelligences, personal and emotional intelligence. Personal intelligence being the internal counterpart to social intelligence and emotional intelligence being the capacity to understand emotions. The CSV highlights three social intelligence measurement scales: Factor Based Social Intelligence Tasks, Psychological Mindedness Assessment Procedure, and Mayer-Salovey-Caruso Emotional intelligence Test.\n\nSocial Intelligence research is limited, however, there is much literature on the characteristics associated with social intelligence. Zaccaro \"et al.\" found social intelligence and perceptiveness to be integral to effective leadership; that is, good leaders are “social experts.” Emotional intelligence, too, plays a role in leadership. Another study found that emotional intelligence enables leaders to better understand their followers, thereby enhancing their ability to influence them.\n\nAlthough only a relatively new field of inquiry for psychological researchers, \"character strengths\" and \"virtues\" have been consistently measured in psychometric surveys and have been shown to be positively associated with psychological and subjective wellbeing. What is more, even among those who endorse a spiritual/theistic worldview, these salutary associations appear to be better explained by humanity/civility rather than endorsing a faith in a supernatural being.\n\n"}
{"id": "23936022", "url": "https://en.wikipedia.org/wiki?curid=23936022", "title": "Infrastructure (number theory)", "text": "Infrastructure (number theory)\n\nIn mathematics, an infrastructure is a group-like structure appearing in global fields.\n\nIn 1972, D. Shanks first discovered the infrastructure of a real quadratic number field and applied his baby-step giant-step algorithm to compute the regulator of such a field in formula_1 binary operations (for every formula_2), where formula_3 is the discriminant of the quadratic field; previous methods required formula_4 binary operations. Ten years later, H. W. Lenstra published a mathematical framework describing the infrastructure of a real quadratic number field in terms of \"circular groups\". It was also described by R. Schoof and H. C. Williams, and later extended by H. C. Williams, G. W. Dueck and B. K. Schmid to certain cubic number fields of unit rank one and by J. Buchmann and H. C. Williams to all number fields of unit rank one. In his habilitation thesis, J. Buchmann presented a baby-step giant-step algorithm to compute the regulator of a number field of \"arbitrary\" unit rank. The first description of infrastructures in number fields of arbitrary unit rank was given by R. Schoof using Arakelov divisors in 2008.\n\nThe infrastructure was also described for other global fields, namely for algebraic function fields over finite fields. This was done first by A. Stein and H. G. Zimmer in the case of real hyperelliptic function fields. It was extended to certain cubic function fields of unit rank one by R. Scheidler and A. Stein. In 1999, S. Paulus and H.-G. Rück related the infrastructure of a real quadratic function field to the divisor class group. This connection can be generalized to arbitrary function fields and, combining with R. Schoof's results, to all global fields.\n\nA one-dimensional (abstract) infrastructure formula_5 consists of a real number formula_6, a finite set formula_7 together with an injective map formula_8. The map formula_9 is often called the \"distance map\".\n\nBy interpreting formula_10 as a circle of circumference formula_11 and by identifying formula_12 with formula_13, one can see a one-dimensional infrastructure as a circle with a finite set of points on it.\n\nA baby step is a unary operation formula_14 on a one-dimensional infrastructure formula_5. Visualizing the infrastructure as a circle, a baby step assigns each point of formula_13 the next one. Formally, one can define this by assigning to formula_17 the real number formula_18; then, one can define formula_19.\n\nObserving that formula_10 is naturally an abelian group, one can consider the sum formula_21 for formula_22. In general, this is not an element of formula_13. But instead, one can take an element of formula_13 which lies \"nearby\". To formalize this concept, assume that there is a map formula_25; then, one can define formula_26 to obtain a binary operation formula_27, called the giant step operation. Note that this operation is in general \"not\" associative.\n\nThe main difficulty is how to choose the map formula_28. Assuming that one wants to have the condition formula_29, a range of possibilities remain. One possible choice is given as follows: for formula_30, define formula_31; then one can define formula_32. This choice, seeming somewhat arbitrary, appears in a natural way when one tries to obtain infrastructures from global fields. Other choices are possible as well, for example choosing an element formula_33 such that formula_34 is minimal (here, formula_34 is stands for formula_36, as formula_37 is of the form formula_38); one possible construction in the case of real quadratic hyperelliptic function fields is given by S. D. Galbraith, M. Harrison and D. J. Mireles Morales.\n\nD. Shanks observed the infrastructure in real quadratic number fields when he was looking at cycles of reduced binary quadratic forms. Note that there is a close relation between reducing binary quadratic forms and continued fraction expansion; one step in the continued fraction expansion of a certain quadratic irrationality gives a unary operation on the set of reduced forms, which cycles through all reduced forms in one equivalence class. Arranging all these reduced forms in a cycle, Shanks noticed that one can quickly jump to reduced forms further away from the beginning of the circle by composing two such forms and reducing the result. He called this binary operation on the set of reduced forms a giant step, and the operation to go to the next reduced form in the cycle a baby step.\n\nThe set formula_10 has a natural group operation and the giant step operation is defined in terms of it. Hence, it makes sense to compare the arithmetic in the infrastructure to the arithmetic in formula_10. It turns out that the group operation of formula_10 can be described using giant steps and baby steps, by representing elements of formula_10 by elements of formula_12 together with a relatively small real number; this has been first described by D. Hühnlein and S. Paulus and by M. J. Jacobson, Jr., R. Scheidler and H. C. Williams in the case of infrastructures obtained from real quadratic number fields. They used floating point numbers to represent the real numbers, and called these representations CRIAD-representations resp. formula_45-representations. More generally, one can define a similar concept for all one-dimensional infrastructures; these are sometimes called formula_46-representations.\n\nA set of formula_46-representations is a subset formula_48 of formula_49 such that the map formula_50 is a bijection and that formula_51 for every formula_17. If formula_25 is a reduction map, formula_54 is a set of formula_46-representations; conversely, if formula_48 is a set of formula_46-representations, one can obtain a reduction map by setting formula_58, where formula_59 is the projection on $X$. Hence, sets of formula_46-representations and reduction maps are in a one-to-one correspondence.\n\nUsing the bijection formula_61, one can pull over the group operation on formula_10 to formula_48, hence turning formula_48 into an abelian group formula_65 by formula_66, formula_67. In certain cases, this group operation can be explicitly described without using formula_68 and formula_9.\n\nIn case one uses the reduction map formula_70, one obtains formula_71. Given formula_72, one can consider formula_73 with formula_74 and formula_75; this is in general no element of formula_76, but one can reduce it as follows: one computes formula_77 and formula_78; in case the latter is not negative, one replaces formula_73 with formula_80 and continues. If the value was negative, one has that formula_81 and that formula_82, i.e. formula_83.\n"}
{"id": "1126641", "url": "https://en.wikipedia.org/wiki?curid=1126641", "title": "Invariant (physics)", "text": "Invariant (physics)\n\nIn mathematics and theoretical physics, an invariant is a property of a system which remains unchanged under some transformation.\n\nIn the current era, the immobility of Polaris (the North Star) under the diurnal motion of the celestial sphere is a classical illustration of physical invariance.\n\nAnother example of a physical invariant is the speed of light under a Lorentz transformation and time under a Galilean transformation. Such spacetime transformations represent shifts between the reference frames of different observers.\n\nBy Noether's theorem invariance of the action of a physical system under a continuous symmetry represents a fundamental conservation law. For example, invariance under translation leads to conservation of momentum, and invariance in time leads to conservation of energy.\n\nQuantities can be invariant under some common transformations but not under others. For example, the velocity of a particle is invariant when switching from rectangular coordinates to curvilinear coordinates, but is not invariant when transforming between frames of reference that are moving with respect to each other. Other quantities, like the speed of light, are always invariant.\n\nInvariants are important in modern theoretical physics, and many theories are expressed in terms of their symmetries and invariants. \n\nCovariance and contravariance generalize the mathematical properties of invariance in tensor mathematics, and are frequently used in electromagnetism, special relativity, and general relativity.\n\n"}
{"id": "772541", "url": "https://en.wikipedia.org/wiki?curid=772541", "title": "John Edward Jacob", "text": "John Edward Jacob\n\nJohn Edward Jacob (born 1934 in Trout, Louisiana) was a U.S. civil rights leader. He served as the president of the National Urban League between 1982 and 1994.\n\nJacob received his B.A. and M.A. degrees from Howard University and was a social worker in Baltimore before joining the Urban League. In 1965, he became director of education and youth incentives at the Washington, D.C. chapter. Later he served as president and executive director of the San Diego Urban League. In 1979 he became executive vice-president of the national office under Vernon Jordan, whom he succeeded as president.\n\nDuring his tenure as Urban League president, Jacob fought cutbacks in federal social programs and the weakening of civil rights enforcement under the Reagan Administration. In particular, he objected to the appointment of a conservative majority to the Civil Rights Commission that was hostile to vigorous protection of civil rights, as well as the Justice Department's prosecutions of other public agencies engaged in affirmative action.\n\nIn the early 1980s, Jacob helped develop a plan for urban recovery similar to the 1947 Marshall Plan initiated to assist European nations after World War II. Aid was sought from private sectors to facilitate entry-level job training programs, and Jacob proposed the League give direct assistance from its own resources to poverty-stricken minorities and whites, including housing and job placement. In addition, he recommended the federal government institute full employment through substantial public works and job training programs, and along with other civil rights groups, supported economic pressure in the corporate world to develop markets and jobs for minorities.\n\nThe son of a Baptist minister, Jacob was also an adherent of self-help. He promoted SAT tutoring, comprehensive teenage pregnancy prevention, and a male responsibility program for fatherhood, to address issues contributing to the cycle of poverty in the African-American community. Jacob added voter registration, education, and drug control to the League's agenda of priorities.\n\nIn contrast to Reagan, George H.W. Bush was initially receptive to Jacob's domestic Marshall Plan proposal, and Jacob welcomed dialogue with the new administration. But Bush's veto of the Civil Rights Act of 1990 soured the relationship. The early 1990s also saw new court decisions and conservative political pressure against affirmative action policies the Urban League supported.\n\n"}
{"id": "54036", "url": "https://en.wikipedia.org/wiki?curid=54036", "title": "Jus sanguinis", "text": "Jus sanguinis\n\nJus sanguinis () is a principle of nationality law by which citizenship is not determined by place of birth but by having one or both parents who are citizens of the state. Children at birth may automatically be citizens if their parents have state citizenship or national identities of ethnic, cultural, or other origins. Citizenship can also apply to children whose parents belong to a diaspora and were not themselves citizens of the state conferring citizenship. This principle contrasts with \"jus soli\" (Latin: \"right of soil\").\n\nAt the end of the 19th century, the French-German debate on nationality saw the French, such as Ernest Renan, oppose the German conception, exemplified by Johann Fichte, who believed in an \"objective nationality\", based on blood, race or language. Renan's republican conception, but perhaps also the presence of a German-speaking population in Alsace-Lorraine, explains France's early adoption of \"jus soli\".\n\nMany nations have a mixture of \"jus sanguinis\" and \"jus soli\", including the United States Canada, Israel, Greece, the Republic of Ireland, and recently Germany. Today French nationality law narrowly applies \"jus sanguinis,\" but it is still the most common means of passing on citizenship in many continental European nations.\n\nSome modern European states which arose out of the dissolved Austro-Hungarian or Ottoman empires have huge numbers of ethnic populations outside of their new 'national' boundaries, as do most of the former Soviet states. Such long-standing diasporas do not conform to codified 20th-century European rules of citizenship.\n\nIn many cases, \"jus sanguinis\" rights are mandated by international treaty, with citizenship definitions imposed by the international community. In other cases, minorities are subject to legal and extra-legal persecution and choose to immigrate to their ancestral home country. States offering \"jus sanguinis\" rights to ethnic citizens and their descendants include Italy, Greece, Turkey, Bulgaria, Lebanon, Armenia and Romania. Each is required by international treaty to extend those rights.\n\n\nMany countries provide citizenship on preferential terms to individuals with ethnic ties to these countries (so-called \"leges sanguinis\"):\n\n"}
{"id": "53913187", "url": "https://en.wikipedia.org/wiki?curid=53913187", "title": "Land change modeling", "text": "Land change modeling\n\nLand change models (LCMs) describe, project, and explain changes in and the dynamics of land use and land-cover. LCMs are a means of understanding ways that humans are changing the Earth's surface in the past, present, in forecasting land change into the future.\n\nLand change models are valuable in development policy, helping guide more appropriate decisions for resource management and the natural environment at a variety of scales ranging from a small piece of land to the entire spatial extent. Moreover, developments within land-cover, environmental and socio-economic data (as well as within technological infrastructures) have increased opportunities for land change modeling to help support and influence decisions that affect human-environment systems, as national and international attention increasingly focuses on issues of global climate change and sustainability.\n\nThese are key concepts and various other terminologies necessary to understand the topic of land change modeling. \n\nChanges in land systems have consequences for climate and environmental change on every scale. Therefore, decisions and policies in relation to land systems are very important for reacting these changes and working towards a more sustainable society and planet.\n\nLand change models are significant in their ability to help guide the land systems to positive societal and environmental outcomes at a time when attention to changes across land systems is increasing.\n\nA plethora of science and practitioner communities have been able to advance the amount and quality of data in land change modeling in the past few decades. That has influenced the development of methods and technologies in model land change. The multitudes of land change models that have been developed are significant in their ability to address land system change and useful in various science and practitioner communities.\n\nFor the science community, land change models are important in their ability to test theories and concepts of land change and its connections to human-environment relationships, as well as explore how these dynamics will change future land systems without real-world observation.\n\nLand change modeling is useful to explore spatial land systems, uses, and covers. Land change modeling can account for complexity within dynamics of land use and land cover by linking with climactic, ecological, biogeochemical, biogeophysical and socioeconomic models. Additionally, LCMs are able to produce spatially explicit outcomes according to the type and complexity within the land system dynamics within the spatial extent. Many biophysical and socioeconomic variables influence and produce a variety of outcomes in land change modeling.\n\nA notable property of all land change models is that they have some irreducible level of uncertainty in the model structure, parameter values, and/or input data. For instance, one uncertainty within land change models is a result from temporal non-stationarity that exists in land change processes, so the further into the future the model is applied, the more uncertain it is. Another uncertainty within land change models are data and parameter uncertainties within physical principles (i.e., surface typology), which leads to uncertainties in being able to understand and predict physical processes.\n\nFurthermore, land change model design are a product of both decision-making and physical processes. Human-induced impact on the socio-economic and ecological environment is important to take into account, as it is constantly changing land cover and sometimes model uncertainty. To avoid model uncertainty and interpret model outputs more accurately, a model diagnosis is used to understand more about the connections between land change models and the actual land system of the spatial extent. The overall importance of model diagnosis with model uncertainty issues is its ability to assess how interacting processes and the landscape are represented, as well as the uncertainty within the landscape and its processes.\n\nA machine-learning approach uses land-cover data from the past to try to assess how land will change in the future, and works best with large datasets. There are multiple types of machine-learning and statistical models - a study in western Mexico from 2011 found that results from two outwardly similar models were considerably different, as one used a neural network and the other used a simple weights-of-evidence model.\n\nA cellular land change model uses maps of suitability for various types of land use, and compares areas that are immediately adjacent to one another to project changes into the future. Variations in the scale of cells in a cellular model can have significant impacts on model outputs.\n\nEconomic models are built on principles of supply and demand. They use mathematical parameters in order to predict what land types will be desired and which will be discarded. These are frequently built for urban areas, such as a 2003 study of the highly dense Pearl River Delta in southern China.\n\nAgent-based models try to simulate the behavior of many individuals making independent choices, and then see how those choices affect the landscape as a whole. Agent-based modeling can be complex - for instance, a 2005 study combined an agent-based model with computer-based genetic programming to explore land change in the Yucatan peninsula of Mexico.\n\nMany models do not limit themselves to one of the approaches above - they may combine several in order to develop a fully comprehensive and accurate model.\n\nScientists use LCMs to build and test theories in land change modeling for a variety of human and environmental dynamics. Land change modeling has a variety of implementation opportunities in many science and practice disciplines, such as in decision-making, policy, and in real-world application in public and private domains. The science disciplines use LCMs to formalize and test land change theory, and the explore and experiment with different scenarios of land change modeling. The practical disciplines use LCMs to analyze current land change trends and explore future outcomes from policies or actions in order to set appropriate guidelines, limits and principles for policy and action. Research and practitioner communities may study land change to address topics related to land-climate interactions, water quantity and quality, food and fiber production, and urbanization, infrastructure, and the built environment.\n\nOne improvement for land change modeling can be made through better data and integration with available data and models. Improved observational data can influence modeling quality. Finer spatial and temporal resolution data that can integrate with socioeconomic and biogeophysical data can help land change modeling couple the socioeconomic and biogeological modeling types. Land change modelers should value data at finer scales. Fine data can give a better conceptual understanding of underlying constructs of the model and capture additional dimensions of land use. It is important to maintain the temporal and spatial continuity of data from airborne-based and survey-based observation through constellations of smaller satellite coverage, image processing algorithms, and other new data to link satellite-based land use information and land management information. It is also important to have better information on land change actors and their beliefs, preferences, and behaviors to improve the predictive ability of models and evaluate the consequences of alternative policies.\n\nOne important improvement for land change modeling can be made though better aligning model choices with model goals. It is important to choose the appropriate modeling approach based on the scientific and application contexts of the specific study of interest. For example, when someone needs to design a model with policy and policy actors in mind, they may choose an agent-based model. Here, structural economic or agent-based approaches are useful, but specific patterns and trends in land change as with many ecological systems may not be as useful. When one needs to grasp the early stages of problem identification, and thus needs to understand the scientific patterns and trend of land change, machine learning and cellular approaches are useful.\n\nLand Change Modeling should also better integrate positive and normative approaches to explanation and prediction based on evidence-based accounts of land systems. It should also integrate optimization approaches to explore the outcomes that are the most beneficial and the processes that might produce those outcomes.\n\nIt is important to better integrate data across scales. A models design is based on the dominant processes and data from a specific scale of application and spatial extent. Cross-scale dynamics and feedbacks between temporal and spatial scales influences the patterns and processes of the model. Process like tele-coupling, indirect land use change, and adaption to climate change at multiple scales requires better representation by cross-scale dynamics. Implementing these processes will require a better understanding of feedback mechanisms across scales.\n\nAs there is continuous reinvention of modeling environments, frameworks, and platforms, land change modeling can improve from better research infrastructure support. For example, model and software infrastructure development can help avoid duplication of initiatives by land change modeling community members, co-learn about land change modeling, and integrate models to evaluate impacts of land change. Better data infrastructure can provide more data resources to support compilation, curation, and comparison of heterogeneous data sources. Better community modeling and governance can advance decision-making and modeling capabilities within a community with specific and achievable goals. Community modeling and governance would provide a step towards reaching community agreement on specific goals to move modeling and data capabilities forward.\n\nA number of modern challenges in land change modeling can potentially be addressed through contemporary advances in cyberinfrastructure such as crowd-source, “mining” for distributed data, and improving high-performance computing. Because it is important for modelers to find more data to better construct, calibrate, and validate structural models, the ability to analyze large amount of data on individual behaviors is helpful. For example, modelers can find point-of-sales data on individual purchases by consumers and internet activities that reveal social networks. However, some issues of privacy and propriety for crowdsourcing improvements have not yet been resolved.\n\nThe land change modeling community can also benefit from Global Positioning System and Internet-enabled mobile device data distribution. Combining various structural-based data-collecting methods can improve the availability of microdata and the diversity of people that see the findings and outcomes of land change modeling projects. For example, citizen-contributed data supported the implementation of Ushahidi in Haiti after the 2010 earthquake, helping at least 4,000 disaster events. Universities, non-profit agencies, and volunteers are needed to collect information on events like this to make positive outcomes and improvements in land change modeling and land change modeling applications. Tools such as mobile devices are available to make it easier for participants to participate in collecting micro-data on agents. Google Maps uses cloud-based mapping technologies with datasets that are co-produced by the public and scientists. Examples in agriculture such as coffee farmers in Avaaj Otalo showed use of mobile phones for collecting information and as an interactive voice.\n\nCyberinfrastructure developments may also increase the ability of land change modeling to meet computational demands of various modeling approaches given increasing data volumes and certain expected model interactions. For example, improving the development of processors, data storage, network bandwidth, and coupling land change and environmental process models at high resolution.\n\nAn additional way to improve land change modeling is through improvement of model evaluation approaches. Improvement in sensitivity analysis is needed to gain a better understand of the variation in model output in response to model elements like input data, model parameters, initial conditions, boundary conditions, and model structure. Improvement in pattern validation can help land change modelers make comparisons between model outputs parameterized for some historic case, like maps, and observations for that case. Improvement in uncertainty sources is needed to improve forecasting of future states that are non-stationary in processes, input variables, and boundary conditions. One can explicitly recognize stationarity assumptions and explore data for evidence in non-stationarity to better acknowledge and understand model uncertainty to improve uncertainty sources. Improvement in structural validation can help improve acknowledgement and understanding of the processes in the model and the processes operating in the real world through a combination of qualitative and quantitative measures.\n\n"}
{"id": "1009651", "url": "https://en.wikipedia.org/wiki?curid=1009651", "title": "Male bonding", "text": "Male bonding\n\nIn ethology and social science, male bonding is the formation of close personal relationships, and patterns of friendship or cooperation between males.\n\nIn the context of human relationships, male bonding is used to describe friendship between men, or the way in which men befriend each other. The expression is sometimes used synonymously with the word camaraderie. The first widely noticed use of the term was in \"Men in Groups\" (1969; 2004) by anthropologist Lionel Tiger.\n\nMale bonding can take place in various locations such as gyms, locker rooms, sport fields or courts, fraternities, and barbershops. This can include playing musical instruments, video games, business ventures, creative endeavors, journeys, quests, sporting activities, fishing, hunting, camping, gambling, social drinking, smoking cigars, working with tools, or even just conversing.\n\n\n"}
{"id": "10700701", "url": "https://en.wikipedia.org/wiki?curid=10700701", "title": "Manufacturing supermarket", "text": "Manufacturing supermarket\n\nA manufacturing supermarket (or market location) is, for a factory process, what a retail supermarket is for the customer. The customers draw products from the 'shelves' as needed and this can be detected by the supplier who then initiates a replenishment of that item. It was the observation that this 'way of working' could be transferred from retail to manufacturing that is one of the cornerstones of the Toyota Production System (TPS).\n\nIn the 1950s Toyota sent teams to the United States to learn how they achieved mass-production. However, the Toyota Delegation first got inspiration for their production system at an American Supermarket (a Piggly Wiggly, to be precise). They saw the virtue in the supermarket only reordering and restocking goods once they’d been bought by customers.\n\nIn a supermarket (like the TPS) customers (processes) buy what they need when they need it. Since the system is self-service the sales effort (materials management) is reduced. The shelves are refilled as products are sold (parts withdrawn) on the assumption that what has sold will sell again which makes it easy to see how much has been used and to avoid overstocking. The most important feature of a supermarket system is that stocking is triggered by actual demand. In the TPS this signal triggers the 'pull' system of production.\n\nMarket locations are appropriate where there is a desire to communicate customer pull up the supply chain. The aim of the 'market' is to send single unit consumption signals back up the supply chain so that a demand leveling effect occurs. Just as in a supermarket it is possible for someone to decide to cater for a party of 300 from the supermarket so it is possible to decide to suddenly fill ten trucks and send massively distorting signals up those same pathways. Thus the 'market location' can be used as a sort of isolator between actual demand and how supply would like demand to be, an isolator between batch demand spikes and the up upstream supply process.\n\nFor example, if the market were positioned at the loading bay, then it will receive 'spikes' of demand whenever a truck comes in to be loaded. Since, in general, one knows in advance when trucks will arrive and what they will require to be loaded onto them, it is possible to spread that demand spike over a chosen period before the truck actually arrives. It is possible to do this by designating a location, say a marked floor area, to be the 'virtual' truck and moving items from the market to the 'virtual truck' smoothly over the chosen period prior to the load onto the actual truck commencing. Smoothly here means that for each item its 'loading' is evenly spread across the period. For regular shipments this period might start the moment the last shipment in that schedule departs the loading bay. This has four key impacts:\n\n\nThis logic can, obviously, be applied upstream of any batch process and not just deliveries to another plant. It is a workaround for the fact that the batch process hasn't been made to flow yet. It therefore has some costs but the benefits in terms of reducing the three wastes should outweigh these.\n\nToyota use this technique and demand it of their suppliers in order to generate focus on the supply issues it uncovers. They then demand the preparation of loads for more frequent 'virtual' trucks than will actually appear in order to raise this pressure (see Frequent deliveries).\n\nAt low stocking levels for some items the 'market location' can require Just in Sequence supply rather than Just in Time.\n"}
{"id": "24162217", "url": "https://en.wikipedia.org/wiki?curid=24162217", "title": "Masters of the Ancient Wisdom", "text": "Masters of the Ancient Wisdom\n\nThe Masters of the Ancient Wisdom are reputed to be enlightened beings originally identified by the Theosophists Helena Blavatsky, Henry S. Olcott, Alfred Percy Sinnett, and others. These Theosophists claimed to have met some of the so-called \"Masters\" during their lifetimes in different parts of the world. Sometimes they are referred to by Theosophists as \"Elder Brothers of the Human Race\", \"Adepts\", \"Mahatmas\", or simply as \"The Masters\".\n\nHelena Blavatsky was the first person to introduce the concept of the Masters to the West. At first she talked about them privately, but she stated that after a few years two of these adepts, Kuthumi (K.H.) and Morya (M.), agreed to maintain a correspondence with two British Theosophists – Alfred P. Sinnett and A. O. Hume. This communication took place from 1880 to 1885, and during those years the reputed existence and objectives of the Mahatmas became public. The original letters are currently kept in the British Library in London and have been published as the \"Mahatma Letters\".\n\nK. Paul Johnson suggests in his book \"The Masters Revealed: Madam Blavatsky and Myth of the Great White Brotherhood\" that the Masters that Madam Blavatsky claimed she had personally met are idealizations of certain people she had met during her lifetime.\n\n\n"}
{"id": "644554", "url": "https://en.wikipedia.org/wiki?curid=644554", "title": "Mother's Day Proclamation", "text": "Mother's Day Proclamation\n\nThe \"Appeal to womanhood throughout the world\" (later known as \"Mother's Day Proclamation\") by Julia Ward Howe was an appeal for women to unite for peace in the world. Written in 1870, Howe's \"Appeal to womanhood\" was a pacifist reaction to the carnage of the American Civil War and the Franco-Prussian War. The appeal was tied to Howe's feminist conviction that women had a responsibility to shape their societies at the political level.\n\nIn 1872 Howe asked for the celebration of a \"Mother's Day for Peace\" on 2 June of every year, but she was unsuccessful. The modern Mother's Day is an unrelated celebration and it was established by Anna Jarvis years later.\n\nToday, the appeal is included in the Unitarian Universalist hymnal \"Singing the Living Tradition\".\n"}
{"id": "209846", "url": "https://en.wikipedia.org/wiki?curid=209846", "title": "National Museum of Iraq", "text": "National Museum of Iraq\n\nThe National Museum of Iraq (Arabic: المتحف العراقي) is a museum located in Baghdad, Iraq. Also known as the Iraq Museum, it contains precious relics from the Mesopotamian, Babylonian and Persian civilization. It was looted during and after the 2003 Invasion of Iraq. Despite international efforts, only some of the stolen artifacts were returned. After being closed for many years while being refurbished, and rarely open for public viewing, the museum was officially reopened in February 2015.\n\nAfter World War I, archaeologists from Europe and the United States began several excavations throughout Iraq. In an effort to keep those findings from leaving Iraq, British traveller, intelligence agent, archaeologist, and author Gertrude Bell began collecting the artifacts in a government building in Baghdad in 1922. In 1926, the Iraqi government moved the collection to a new building and established the Baghdad Antiquities Museum, with Bell as its director. Bell died later that year; the new director was Sidney Smith.\n\nIn 1966, the collection was moved again, to a two-story, building in Baghdad's Al-Ṣāliḥiyyah neighborhood in the Al-Karkh district on the east side of the Tigris River. It is with this move that the name of the museum was changed to the National Museum of Iraq. It was originally known as the Baghdad Archaeological Museum.\n\nDue to the archaeological riches of Mesopotamia, its collections are considered to be among the most important in the world and has a fine record of scholarship and display. The British connection with the museum — and with Iraq — has resulted in exhibits always being displayed bilingually, in both English and Arabic. It contains important artifacts from the over 5,000-year-long history of Mesopotamia in 28 galleries and vaults.\n\nThe collections of the National Museum of Iraq include art and artifacts from ancient Sumerian, Babylonian, Akkadian and Assyrian civilizations. The museum also has galleries devoted to collections of both pre-Islamic and Islamic Arabian art and artifacts. Of its many noteworthy collections, the Nimrud gold collection—which features gold jewelry and figures of precious stone that date to the 9th century bce—and the collection of stone carvings and cuneiform tablets from Uruk are exceptional. The Uruk treasures date to between 3500 and 3000 BCE.\n\nIn the months preceding the 2003 Iraq war, starting in December and January, various antiquities experts, including representatives from the American Council for Cultural Policy asked the Pentagon and the UK government to ensure the museum's safety from both combat and looting. Although promises were not made, U.S. forces did avoid bombing the site.\n\nOn April 8, 2003 the last of the museum staff left the museum. Iraqi forces engaged U.S. forces from within the museum, as well as the nearby Special Republican Guard compound. Lt. Col. Eric Schwartz of the U.S. third Infantry Division stated that he was unable to enter the compound and secure it since they attempted to avoid returning fire at the building. Sniper positions, discarded ammunition, and 15 Iraqi Army uniforms were later discovered in the building. Iraqi forces had built a fortified wall along the western side of the compound, allowing concealed movement between the front and rear of the museum.\n\nThefts took place between April 10 and 12, when some staff returned to the building and fended off further attempts by looters to enter the museum until U.S. forces arrived on April 16. A special team headed by Marine Col. Matthew Bogdanos initiated an investigation on April 21. His investigation indicated that despite claims to the contrary, no U.S. forces had looted the building, and that there were three separate thefts by three distinct groups over the four days. While the staff instituted a storage plan to prevent theft and damage (also used during the Iran–Iraq War and the first Gulf War), many larger statues, steles, and friezes had been left in the public galleries, protected with foam and surrounded by sandbags. Forty pieces were stolen from these galleries, mostly the more valuable ones. Of these 13 had been recovered as of January 2005, including the three most valuable: the Sacred Vase of Warka (though broken in fourteen pieces, which was the original state it was found in when first excavated), the Mask of Warka, and the Bassetki Statue.\nAccording to museum officials, the looters concentrated on the heart of the exhibition: \"the Warka Vase, a Sumerian alabaster piece more than 5,000 years old; a bronze Uruk statue from the Akkadian period, also 5,000 years old, which weighs 660 pounds; and the headless statue of Entemena. The Harp of Ur was torn apart by looters who removed its gold inlay.\" Among the stolen artifacts is the bronze Bassetki Statue, a life-size statue of a young man, originally found in the village Basitke in the northern part of Iraq, an Akkadian Empire piece that goes back to 2300 B.C. and the stone statue of King Schalmanezer, from the eighth century B.C.\n\nIn addition, the museum's aboveground storage rooms were looted; the exterior steel doors showed no signs of forced entry. Approximately 3,100 excavation site pieces (jars, vessels, pottery shards, etc.) were stolen, of which over 3,000 have been recovered. The thefts did not appear to be discriminating; for example, an entire shelf of fakes was stolen, while an adjacent shelf of much greater value was undisturbed.\n\nThe third occurrence of theft was in the underground storage rooms, where evidence pointed to an inside job. The thieves attempted to steal the most easily transportable objects, which had been intentionally stored in the most remote location possible. Of the four rooms, the only portion disturbed was a single corner in the furthest room, where cabinets contained 100 small boxes containing cylinder seals, beads, and jewelry. Evidence indicated that the thieves possessed keys to the cabinets but dropped them in the dark. Instead, they stole 10,000 small objects that were lying in plastic boxes on the floor. Of them, nearly 2,500 have been recovered.\n\nOne of the most valuable artifacts looted was a headless stone statue of the Sumerian king Entemena of Lagash. The Entemena statue, \"estimated to be 4,400 years old, is the first significant artifact returned from the United States and by far the most important piece found outside Iraq. American officials declined to discuss how they recovered the statue.\" The statue of the king, located in the center of the museum's second-floor Sumerian Hall, weighs hundreds of pounds, making it the heaviest piece stolen from the museum - the looters \"probably rolled or slid it down marble stairs to remove it, smashing the steps and damaging other artifacts.\"\n\nThe U.S. Immigration and Customs Enforcement (ICE) announced the recovery of the statue of King Entemena of Lagash on July 25, 2006. The statue was returned to the Iraq government. It was recovered in the United States with the help of Hicham Aboutaam, an art dealer in New York.\n\nThe U.S. government was criticised for doing nothing to protect the museum after occupying Baghdad. Dr. Irving Finkel of the British Museum said the looting was \"entirely predictable and could easily have been stopped.\" Martin E. Sullivan, chairman of the U.S. President's Advisory Committee on Cultural Property, and U.S. State Department cultural advisers Gary Vikan and Richard S. Lanier resigned in protest at the failure of US forces to prevent the looting.\n\nThe extent of the looting of Iraq's National Museum has been disputed. Based on a miscommunication by the first crews on the scene, and the empty display cases in the main galleries that in most cases had held objects which museum curators had removed before the invasion, news organizations for weeks reported that as much as 170,000 catalogued lots (501,000 pieces) had been looted, when the true figure was around 15,000 items, including 5,000 extremely valuable cylinder seals.\n\nReacting to the incorrect initial reports that the museum was a total loss, French President Jacques Chirac on April 16, 2003, declared the incident \"a crime against humanity.\"\n\nLater in 2003, \"The Daily Telegraph\" (of London) reported:\n\nWhen asked why the U.S. military did not try to guard the museum in the days after the invasion succeeded, Gen. Richard Myers, chairman of the Joint Chiefs of Staff, said \"If you remember, when some of that looting was going on, people were being killed, people were being wounded ... It's as much as anything else a matter of priorities.\" Civil Affairs expert William Sumner, who was tasked with handling arts, monuments, and archives, explained that the postwar Civil Affairs planners \"didn't foresee the marines as going out and assigning marine units as security ... The issue of archaeological sites was considered a targeting problem,\" to be dealt with by those flying bombing missions. Secretary of Defense Donald Rumsfeld, speaking about the museum's looting, said \"stuff happens\" and \"to try to pass off the fact of that unfortunate activity to a deficit in the war plan strikes me as a stretch\", and described the period of looting in general as \"untidiness\". Secretary of State Colin Powell said, \"The United States understands its obligations and will be taking a leading role with respect to antiquities in general but this museum in particular.\"\n\nTwo weeks after the museum thefts, when major news outlets still were reporting most of the museum's artifacts were gone, Dr. Donny George Youkhanna, General Director Research Studies for the Board of Antiquities in Iraq, said of the looting, \"It's the crime of the century, because it affects the heritage of all mankind\". After the U.S. Marines set up headquarters in Baghdad's Palestine Hotel, Dr. Youkhanna said he went there to plead for troops to protect the remainder of the Museum collection, but no guards were sent for another three days. Whether or not this was due to continued fighting is unclear.\n\nA few days later, agents of the FBI were sent to Iraq to search for stolen Museum property. UNESCO organized an emergency meeting of antiquities experts on April 17, 2003 in Paris to deal with the aftermath of the looting and its effects on the global art and antiquities market.\n\nOn April 18, 2003, the Baghdad Museum Project was formed in the United States with a proposal to assure the National Museum of Iraq every possibility of the eventual safe return of its collection, even if that is to take hundreds of years. Rather than focus only on law enforcement and the current antiquities market, the group set its mission as being to (1) establish a comprehensive online catalog of all cultural artifacts in the museum's collection, (2) create a virtual Baghdad Museum that is accessible to the general public over the Internet, (3) build a 3D collaborative workspace within the virtual Baghdad Museum for design and fundraising purposes, and (4) establish a resource center within the virtual Baghdad Museum for community cultural development. Various ancient items believed looted from the museum have surfaced in Jordan, Lebanon, the United States, Switzerland, and Japan, and on eBay.\n\nOn May 7, 2003, U.S. officials announced that nearly 40,000 manuscripts and 700 artifacts belonging to the National Museum of Iraq in Baghdad were recovered by U.S. Customs agents working with museum experts in Iraq. Some looters had returned items after promises of rewards and amnesty, and many items previously reported missing had actually been hidden in secret storage vaults at the museum prior to the outbreak of war. On June 7, 2003, U.S. authorities announced that world-famous treasures of Nimrud were recovered from a secret vault in the Iraqi Central Bank. The artifacts included necklaces, plates, gold earrings, finger and toe rings, bowls and flasks. Officials said that of the 170,000 items initially believed missing, just 3,000 remained unaccounted for. And, of those, 47 were main exhibition artifacts. In November, 2003 Coalition officials reported a few dozen of the most important items remained missing from the museum's public galleries, along with another 10,000 other items—most of them tiny and some of them fragments. The ultimate figure is now estimated at around 15,000 and the tiny items include some of the most valuable artifacts on the antiquities markets.\n\nThe museum has been protected since its looting, but archaeological sites in Iraq were left almost entirely unprotected by coalition forces, and there has been massive looting, especially in the period just before the invasion (when Saddam Hussein pulled forces away from site protection) and between summer 2003 and the end of 2007. Estimates are that 400–600,000 artifacts have been plundered. Iraqi sculptor Mohammed Ghani Hikmat spearheaded efforts by the Iraqi artist community to recover artworks looted from the museum. Approximately 150 of Hikmat's pieces were stolen from the museum alone. Hikmat's group has recovered approximately 100 of the museum's works, as of September 2011.\n\nUnited States Marine Colonel, and Manhattan Assistant District Attorney Matthew Bogdanos led the search for these stolen artifacts for over five years from 2003. Up to the year 2006 approximately 10,000 artifacts were recovered through his efforts. Antiquities recovered include the Warka Vase and the Mask of Warka.\n\nAt various Iraq reconstruction conferences, the Baghdad Museum Project gave presentations to the reconstruction community advocating preservation of Iraq's cultural heritage in rebuilding projects. On August 27, 2006, Iraq's museum director Dr. Donny Youkhanna fled the country to Syria, claiming \"pressure to follow a radical Islamic agenda in the preservation of Iraqi antiquities made his position impossible.\". Youkhanna held the position of visiting professor in the anthropology department of Stony Brook State University of New York until his death in March 2011.\n\nOn June 9, 2009, the treasures of the National Museum went online for the first time as Italy inaugurated the Virtual Museum of Iraq. On November 24, 2009, Google announced that it would create a virtual copy of the museum's collections at its own expense, and make images of four millennia of archaeological treasures available online, free, by early 2010. It is unclear the extent by which Google's effort overlaps with Italy's previous initiative. Google's Street View service was used to image much of the museum's exhibit areas and, as of November 2011, these images are online.\n\nThe museum has opened its doors on only a dozen times since September 1980 during the Iran-Iraq War. Since the invasion, it has opened only rarely, opened on July 3, 2003 for several hours for a visit by journalists and Coalition Provisional Authority head J. Paul Bremer, as a signal that things were returning to normal. In December 2008, the museum was opened for a photo opportunity for Ahmad Chalabi, who returned a number of artifacts supposedly handed in to him by Iraqis. The latest opening occurred on February 23, 2009, at the behest of Iraqi prime minister Maliki, to demonstrate that things were returning to normal. Many archaeological officials protested against this opening, arguing that conditions were not yet safe enough to put the museum at risk; the museum's director was fired for airing her objections.\n\nIn a ceremony to mark the occasion, Qahtan Abbas, Iraq's tourism and antiquities minister, said that only 6,000 of the 15,000 items looted in 2003 had been returned. In a book published in 2009, it was estimated that 600,000 archaeological pieces were looted by Kurdish and Shia militias allied with the United States since 2003 In September 2011 Iraqi officials announced the renovated museum will permanently reopen in November, protected by new climate control and security systems. The United States and Italian governments have both contributed to the renovation effort.\n\nOn February 28, 2015 the museum was officially reopened by Iraqi Prime Minister Haider al-Abadi. The museum also has items taken from the Mosul Museum, as ISIS has taken it over.\n\nOn September 7, 2010, the Associated Press reported that 540 looted treasures were returned to Iraq.\n\n638 stolen artifacts were returned to the Iraqi National Museum after they were located in the office of Prime Minister Nouri al-Maliki.\n\nOn January 30, 2012, a 6,500-year-old Sumerian gold jar, the head of a Sumerian battle axe and a stone from an Assyrian palace were among 45 relics returned to Iraq by Germany. Up to 10,000 of the National Museum pieces are still missing, said Amira Eidan, general director of the museum at the time of the recovery.\n\n\n"}
{"id": "3737245", "url": "https://en.wikipedia.org/wiki?curid=3737245", "title": "Owen and Mzee", "text": "Owen and Mzee\n\nOwen and Mzee are a hippopotamus and an Aldabra giant tortoise, respectively, that became the subject of media attention after forming an unusual bond of friendship. They live in Haller Park, Bamburi, Kenya.\n\nOwen was separated from his herd as a juvenile following the December 2004 tsunami and was brought to the Haller Park rescue center. Having no other hippos to interact with, Owen immediately attempted to bond with Mzee (Swahili for old man), whose large domed shell and brown color resembled an adult hippo. Mzee was reluctant about Owen at first but grew to like him and got used to Owen around him. \n\nThe pair were featured in \"Owen and Mzee: The True Story of a Remarkable Friendship\", a 2006 book by Isabella and Craig Hatkoff, as well as the 2007 sequel \"Owen and Mzee: The Language of Friendship\".\n\nOnce it was determined that Owen had grown too large to safely interact with Mzee, a separate enclosure was built for Owen and a new (female) hippo named Cleo, with whom he bonded quickly. With Owen now twice Mzee's size and well on his way to being socialized to other hippos, the famous friends went their separate ways and Mzee was returned to his regular enclosure.\n\n\n"}
{"id": "34857983", "url": "https://en.wikipedia.org/wiki?curid=34857983", "title": "Periscopic", "text": "Periscopic\n\nPeriscopic is a Portland, Oregon based data visualization firm founded by Kim Rees and Dino Citraro in 2004. Its motto is \"Do good with data.\" They have worked with such clients as Google, GE, and the Bill & Melinda Gates Foundation. Rees is also an advisor to the U.S. Congressional Budget Office. The company won an APDU Data Viz Award in 2017 for their work on the US Patent and Trademark Office's website PatentsView, a project they worked on with the US Department of Agriculture, the American Institutes for Research’s Center for the Science of Science and Innovation Policy, the University of California at Berkeley, and Twin Arch Technologies.\n\n\n"}
{"id": "1266589", "url": "https://en.wikipedia.org/wiki?curid=1266589", "title": "Point particle", "text": "Point particle\n\nA point particle (ideal particle or point-like particle, often spelled pointlike particle) is an idealization of particles heavily used in physics. Its defining feature is that it lacks spatial extension: being zero-dimensional, it does not take up space. A point particle is an appropriate representation of any object whose size, shape, and structure is irrelevant in a given context. For example, from far enough away, any finite-size object will look like and behave as a point-like object.\n\nIn the theory of gravity, physicists often discuss a ', meaning a point particle with a nonzero mass and no other properties or structure. Likewise, in electromagnetism, physicists discuss a ', a point particle with a nonzero charge.\n\nSometimes, due to specific combinations of properties, extended objects behave as point-like even in their immediate vicinity. For example, spherical objects interacting in 3-dimensional space whose interactions are described by the inverse square law behave in such a way as if all their matter were concentrated in their centers of mass. In Newtonian gravitation and classical electromagnetism, for example, the respective fields outside a spherical object are identical to those of a point particle of equal charge/mass located at the center of the sphere.\n\nIn quantum mechanics, the concept of a point particle is complicated by the Heisenberg uncertainty principle, because even an elementary particle, with no internal structure, occupies a nonzero volume. For example, the atomic orbit of an electron in the hydrogen atom occupies a volume of ~10 m. There is nevertheless a distinction between elementary particles such as electrons or quarks, which have no known internal structure, versus composite particles such as protons, which do have internal structure: A proton is made of three quarks. Elementary particles are sometimes called \"point particles\", but this is in a different sense than discussed above.\n\nWhen a point particle has an additive property, such as mass or charge, concentrated at a single point in space, this can be represented by a Dirac delta function.\n\nPoint mass (pointlike mass) is the concept, for example in classical physics, of a physical object (typically matter) that has nonzero mass, and yet explicitly and specifically is (or is being thought of or modeled as) infinitesimal (infinitely small) in its volume or linear dimensions.\n\nA common use for point mass lies in the analysis of the gravitational fields. When analyzing the gravitational forces in a system, it becomes impossible to account for every unit of mass individually. However, a spherically symmetric body affects external objects gravitationally as if all of its mass were concentrated at its center.\n\nA point mass in probability and statistics does not refer to mass in the sense of physics, but rather refers to a finite nonzero probability that is concentrated at a point in the probability mass distribution, where there is a discontinuous segment in a probability density function. To calculate such a point mass, an integration is carried out over the entire range of the random variable, on the probability density of the continuous part. After equating this integral to 1, the point mass can be found by further calculation.\n\nA point charge is an idealized model of a particle which has an electric charge. A point charge is an electric charge at a mathematical point with no dimensions.\n\nThe fundamental equation of electrostatics is Coulomb's law, which describes the electric force between two point charges. The electric field associated with a classical point charge increases to infinity as the distance from the point charge decreases towards zero making energy (thus mass) of point charge infinite.\n\nEarnshaw's theorem states that a collection of point charges cannot be maintained in an equilibrium configuration solely by the electrostatic interaction of the charges.\n\nIn quantum mechanics, there is a distinction between an elementary particle (also called \"point particle\") and a composite particle. An elementary particle, such as an electron, quark, or photon, is a particle with no internal structure. Whereas a composite particle, such as a proton or neutron, has an internal structure (see figure).\nHowever, neither elementary nor composite particles are spatially localized, because of the Heisenberg uncertainty principle. The particle wavepacket always occupies a nonzero volume. For example, see atomic orbital: The electron is an elementary particle, but its quantum states form three-dimensional patterns.\n\nNevertheless, there is good reason that an elementary particle is often called a point particle. Even if an elementary particle has a delocalized wavepacket, the wavepacket can be represented as a quantum superposition of quantum states wherein the particle is exactly localized. Moreover, the \"interactions\" of the particle can be represented as a superposition of interactions of individual states which are localized. This is not true for a composite particle, which can never be represented as a superposition of exactly-localized quantum states. It is in this sense that physicists can discuss the intrinsic \"size\" of a particle: The size of its internal structure, not the size of its wavepacket. The \"size\" of an elementary particle, in this sense, is exactly zero.\n\nFor example, for the electron, experimental evidence shows that the size of an electron is less than 10 m. This is consistent with the expected value of exactly zero. (This should not be confused with the classical electron radius, which, despite the name, is unrelated to the actual size of an electron.)\n\n\n\n"}
{"id": "27671883", "url": "https://en.wikipedia.org/wiki?curid=27671883", "title": "Portfolio: An Intercontinental Quarterly", "text": "Portfolio: An Intercontinental Quarterly\n\nPortfolio: An Intercontinental Quarterly was a cross-disciplinary literary journal published between 1945 and 1947. It was edited by Caresse Crosby and published through her Black Sun Press. Only six issues were published, each totaling about 1000 copies. Each issue was a series of loose sheets contained in a folio, lavishly illustrated, and printed in limited numbers. Contributors included many avant-garde authors, architects, photographers, and illustrators who were prominent in their respective fields, including individuals like Albert Camus (who contributed \"Letter to a German Friend,\" his first appearance in an English-language publication), architect Luigi Moretti, artist Pablo Picasso, and photographer Henri Cartier-Bresson, along with emerging writers like Charles Bukowski. It introduced American readers to many authors who later became famous.\n\nCaresse Crosby, a long-time patron of the arts, originally published with her husband Harry Crosby through the Black Sun Press a number of emerging writers during the 1920s and 1930s in Paris. After his suicide in December 1929, she continued their work, though less actively. The press stopped active publishing when World War II intervened and Crosby left Paris for the United States.\n\nTowards the end of World War II, Harry T. Moore encouraged her to start a new cultural magazine, which she titled \"Portfolio\" with the desire to present to the public \"lively and varied examples of work by modern artists.\" Each issue contained prose, poetry, prints and plans. She hoped to initiate an \"exchange of thought between America and Europe\" and to carry forth \"a new expression of man's aspirations.\" She followed in the tradition of the original literary journal \"transition\", which her husband Harry Crosby had edited. She hoped that artists of all kinds could \"build a bridge of enduring fabric between the ivory tower and the arena\" and contribute to rebuilding the culture of Europe. The \"Portfolios\" included work by a variety of avant-garde writers, artists, photographers, and architects. The publication featured new work, translated works of foreign writers, re-published works of writers whose work had not been widely known, and both original images and reproductions of various artist's work.\n\nDuring World War II and for some time after, paper was in short supply. Caresse printed the magazine on a variety of different sizes, colors and types of paper stock printed by different printers, stuffed into a folio, though the size varied. Caresse printed 1,000 copies of each issue, and as she had done with earlier works published by the Black Sun Press, gave special treatment to 100 or so deluxe copies that featured original artwork by Henri Matisse, Romare Bearden, and others. Crosby worked from her offices at 1620 20th Street NW in Washington D.C. She lived around the corner at 2008 Q St. NW. She later moved the offices to 918 F Street, N.W.\n\nOnly six issues were published. Issues 1, 3, and 5 were printed in Washington, D.C. Issue 2 was printed in Paris less than seven months after the end of World War II. It featured primarily French writers and artists. The fourth issue was published in Rome and featured Italian writers and artists. The last issue, number 6, was published in Athens and focused on Greek authors and artists.\n\nCrosby intended to publish four issues per year, and initially offered subscriptions for US$10.00 per year. Single copies were available for $3.00 each. She originally intended to print only 1,300 copies of each issue, but in fact printed only 1000 copies of each issue.\n\nThe inaugural issue was published in Washington, D.C., in 1945, immediately after the end of World War II. It included 26 leaves in loose sheets, contained in a paper folio cover. Edited by Caresse Crosby. Harry T. Moore served as assistant editor. Editorial advisers were Henry Miller for prose, Selden Rodman for poetry, and Sam Rosenberg for photography.\n\nProse was contributed by Caresse Crosby, René Crevel, Henry Miller (\"The Stuff of Life\"), David Daiches, Jerome Weidman (\"Sam\"). Poetry contributors included Karl Shapiro who contributed three sonnets from the \"Place of Love\"; Kay Boyle, Louis Aragon, Ruth Herschberger, Demetrios Capetanakis (\"Emily Dickinson\") and Coleman Rosenberger, who wrote \"Manet in the Sale Mines at Merkers\" and also served as an editorial adviser.\n\nDrawings and illustrations were produced by Jean Helion, Romare Bearden, Henry T. Moore, Pietro Lazzari, and Lilian Swann Saarinen. Sam Rosenberg contributed both drawings and photography. Crosby included a piece penned by her late husband Harry Crosby (\"Anatomy of Flight\"), a photograph by Harry, and an unattributed portrait of Kay Boyle.\n\n\"Portfolio II\" was published in Paris near Christmas in 1945, less than seven months after the end of World War II, It contained 22 leaves in a folio. Henry Miller once again served as an editorial adviser. Contributors included Paul Éluard, Albert Camus, Jean-Paul Sartre, René Char, Francis Ponge, Kristians Tonny, Henri Matisse, Myron O'Higgins, Francis Ponge, Paul Grimault, Claude Roy, Robert Lannoy, Robert Lowell, Claude Morgan, Valdi Leduc, Weldon Kees, Mireille Sidoine, Tudal, Jerome Snyder, Louis Martin-Chauffier, Francis Gruber, Selden Rodman, and Harry T. Moore.\n\nArtists and illustrators included Dora Maar, Henri Cartier-Bresson, and reproductions of work by Picasso, Matisse, and Alberto Giacometti.\n\n\"Portfolio III\" was published in Washington during the spring 1946. This issue included Charles Bukowski's first separately published work. His first book was not published until 1960, nearly fifteen years later. The leaves or broadsides were in various sizes, loosely contained by yellow paper covers. The front was lettered in red. It contained 29 leaves enclosed in a folio, plus an unnumbered leaf of \"Book Reviews\" by Selden Rodman, along with a \"Cover Leaf\" and \"Foreleaf.\" Only 1000 copies were printed.\n\nOther prose contributions came from Kay Boyle, Harry Crosby, Jean-Paul Sartre, Henry Miller, Stephen Spender, García Lorca, David Daiches, Jean Genet, Kenneth Rexroth, and others. Illustrations were done by Hans Richter, Wifredo Lam, Pierre Tal-Coat, Dorothea Tanning, and others.\n\nThe issue was edited by Henry Miller, Romare Bearden, Sam Rosenberg. and Harry T. Moore.\n\n\"Portfolio IV\" was published during the summer of 1946 in Rome. It was devoted to Italian writers and artists, including Alberto Moravia and Elio Vittorini. It contained 28 loose leaves in a folio, including three essays on contemporary Italian fiction, painting, and cinema and a short section of poetry. Henry Miller edited \"in absentia.\"\n\nProse selections were included from Harry Crosby, Caresse Crosby, Bruno Zevi, and others. A series of leaves were devoted to artwork (including photography) by Giorgio Morandi (on color reproduction); Carlo Levi, Giorgio de Chirico, Giacomo Manzù, Corrado Cagli, Panayiotis Tetsis (on painting and sculpture); and Pier Luigi Nervi and Luigi Moretti (on architecture).\n\n\"Portfolio V\" was published in the spring of 1947 in Paris. The folio was and contained 19 loose leaves of text and 11 illustrations, along with a cover leaf and table of contents.\n\nIt featured prose and poetry written by Harry T. Moore, Harry Crosby, Selden Rodman, Edwin Becker, Charles Olson, Leo Tolstoy (\"The Law of Love and the Law of Violence\"), Anaïs Nin, Emanuel Carnevale, George Mann, and others. It also included reproductions by Max Ernst, Man Ray, Carmelo, Roberto Fasola, Modigliani, Scipione, Justin Locke, Mirko, Meraud Guevara, and others.\n\n\"Portfolio VI\" was published during the summer of 1947. It focused on writers and artists from Greece, where publisher Caresse Crosby had for some years tried to establish a world peace center. It was the largest \"Portfolio\" with 36 leaves of text and illustration. Writers included Yórgos Theotokás, Nicolas Calas, Cambas, D. Nicolareizis, and others. She published contributions from Nikos Hadjikyriakos-Ghikas, a sculptor, engraver, iconographer, writer and academic, and from painter and poet Nikos Engonopoulos. Illustrations were provided by Yannis Moralis, Kanellis, Kapralos, Diamantopoulos, and others.\n\nCrosby had already begun work on an issue focusing on Ireland and a \"Negro\" issue. When she did not attract more sponsors, she finally decided she could not absorb the risk of publishing further issues. She stopped publication with Issue VI. She had intended to publish four issues per year, and never achieved that goal.\n\nA complete set of all six issues of \"Portfolio\" in fine quality was valued in 2010 by fine book seller Sim Reed Limited at £ (about €( / .8331) round 0 or $( / .6844) round 0). A collection missing two leaves from one issue, in very good condition, was sold in 2008 by PBA Galleries for $ (about €( / 1.28030) round 0 or £( / 1.28030) round 0).\n\nHarry T. Moore, \"The Later Caresse Crosby: Her Answer Remained 'Yes',\" Southern Illinois University, Carbondale, IL, III, 2, 1977, p. 129.\n"}
{"id": "31565460", "url": "https://en.wikipedia.org/wiki?curid=31565460", "title": "Protection from Abuse (Scotland) Act 2001", "text": "Protection from Abuse (Scotland) Act 2001\n\nThe Protection from Abuse (Scotland) Act 2001 (asp 14) is an Act of the Scottish Parliament. It was passed on 4 October 2001, receiving Royal Assent on 6 November.\n\nIn January 2000, following the devolution of the Scottish Parliament the previous year, Justice Minister Jim Wallace announced proposals to reform family law in Scotland, which would include new legislation on domestic abuse. Plans to increase protection for victims of domestic violence were temporarily delayed but the Protection from Abuse (Scotland) bill was published in June 2001 and was subsequently passed by the parliament on 4 October. The act received Royal Assent on 6 November.\n\nThe act allows interdicts to be brought against abusers by their victims and gives the police the power to arrest and charge an individual who breaks such an interdict. By giving the police this new power, the act amends the Matrimonial Homes (Family Protection) (Scotland) Act 1981. The 2001 act was intended to extend protection to victims of domestic abuse by unmarried and same-sex partners, as well as those no longer in a relationship with their abuser.\n\n"}
{"id": "4353533", "url": "https://en.wikipedia.org/wiki?curid=4353533", "title": "Psychological torture", "text": "Psychological torture\n\nPsychological torture is a type of torture that relies primarily on psychological effects, and only secondarily on any physical harm inflicted. Although not all psychological torture involves the use of physical violence, there is a continuum between psychological torture and physical torture. The two are often used in conjunction with one another, and often overlap in practice, with the fear and pain induced by physical torture often resulting in long-term psychological effects, and many forms of psychological torture involving some form of pain or coercion.\n\nThe Convention against Torture and Other Cruel, Inhuman or Degrading Treatment or Punishment (commonly known as the United Nations Convention against Torture) is an international human rights treaty, under the review of the United Nations, that aims to prevent torture and other acts of cruel, inhuman, or degrading treatment or punishment around the world. The Convention requires states to take effective measures to prevent torture in any state under their jurisdiction, and forbids states to transport people to any country where there is reason to believe torture could occur.\n\nThe text of the Convention was adopted by the United Nations General Assembly on 10 December 1984 and, following ratification by the 20th state party, it came into force on 26 June 1987. 26 June is now recognized as the International Day in Support of Victims of Torture, in honor of the Convention. As of May 2015, the Convention has 158 state parties.\n\nThe Convention gave for the first time in history a definition of psychological torture:\n\nThe Optional Protocol to such Convention (OPCAT, 2006) is an important addition to the United Nations Convention. The Committee Against Torture (CAT) is a body of independent experts that monitors implementation of the Convention by State parties. All State parties are obliged under the Convention to submit regular reports to the CAT on how the rights are being implemented. Upon ratifying the Convention, States must submit a report within one year, after which they are obliged to report every four years. The Committee examines each report and addresses its concerns and recommendations to the State party in the form of \"concluding observations\". Under certain circumstances, the CAT may consider complaints or communications from individuals claiming that their rights under the Convention have been violated. The CAT usually meets in May and November each year in Geneva.\n\nMany forms of psychological torture methods attempt to destroy the subject's normal self-image by removing them from any kind of control over their environment, creating a state of learned helplessness, psychological regression and depersonalization. Other techniques include forced nudity and head shaving, sleep deprivation, hooding and other forms of sensory deprivation.\n\nA strictly fear-inducing method is the mock execution. Various threats operate on the same fear-inducing principle.\n\nAnother method is indirect torture, in which a victim is forced to witness the torture of another person, often a loved one. This preys on the victim's affection for and loyalty to a partner, relative, friend, comrade-in-arms etc, whose real pain induces vicarious suffering in the targeted psychological victim, who is thus loaded with guilt but spared physical harm that might affect his or her ability to comply.\n\nWhile psychological torture may not leave any lasting physical damage—indeed, this is often one of the motivations for using psychological rather than physical torture—it can result in similar levels of permanent mental damage to its victims.\n\nIt has been alleged that some psychological torture methods may have been devised by, or in conjunction with, doctors and psychologists.\n\nThe United States has been accused of making extensive use of psychological torture techniques at Guantanamo Bay and other sites subsequent to the 9/11 attacks. Many other countries have been accused of using psychological torture, including Iran. In 1976 the European Commission of Human Rights found the British government guilty of using psychological torture on IRA political detainees in Northern Ireland, while in 1978 the European Court of Human Rights found that the treatment of political internees constituted \"inhuman and degrading treatment\" rather than torture.\n\n"}
{"id": "3466490", "url": "https://en.wikipedia.org/wiki?curid=3466490", "title": "Ragpicker", "text": "Ragpicker\n\nA Rag-picker, or Chiffonnier, is term for someone who makes a living by rummaging through refuse in the streets to collect material for salvage. Scraps of cloth and paper could be turned into cardboard, broken glass could be melted down and reused, and even dead cats and dogs could be skinned to make clothes. \n\nThe rag-pickers in 19th and early 20th Century did not recycle the materials themselves; they would simply collect whatever they could find and turn it over to a \"master rag-picker\" (usually a former rag-picker) who would, in turn, sell it—generally by weight—to wealthy investors with the means to convert the materials into something more profitable. \n\nAlthough it was solely a job for the lowest of the working classes, rag-picking was considered an honest occupation, more on the level of street sweeper than of a beggar. In Paris, for instance, rag-pickers were regulated by law: their operations were restricted to certain times of night, and they were required to return any unusually valuable items to the owner or to the authorities. When Eugène Poubelle introduced the garbage can in 1884, he was criticized in the French newspapers for meddling with the rag-pickers' livelihoods. Modern sanitation and recycling programs ultimately caused the profession to decline, though it did not disappear entirely; rag and bone men were still operating in the 1970s. \n\nRag-picking is still widespread in Third World countries today, such as in Mumbai, India, where it offers the poorest in society around the rubbish and recycling areas a chance to earn a hand-to-mouth supply of money. In 2015, the Environment Minister of India declared a national award to recognise the service rendered by rag-pickers. The award, with a cash prize of Rs. 1.5 lakh, is for three best rag pickers and three associations involved in innovation of best practices. \n\nRagpicking has a positive impact on urban spaces with a weak waste management infrastructure. In India, the economic activity of ragpicking is worth about ₹3200 crore. India was also found to have a near-90% recycle rate for PET bottles, which could probably be attributed to ragpicking, given a lack of solid-waste management and under-developed waste collection and recycling culture in that country. \n\n"}
{"id": "25169288", "url": "https://en.wikipedia.org/wiki?curid=25169288", "title": "Reservoir modeling", "text": "Reservoir modeling\n\nIn the oil and gas industry, reservoir modeling involves the construction of a computer model of a petroleum reservoir, for the purposes of improving estimation of reserves and making decisions regarding the development of the field, predicting future production, placing additional wells, and evaluating alternative reservoir management scenarios. \n\nA reservoir model represents the physical space of the reservoir by an array of discrete cells, delineated by a grid which may be regular or irregular. The array of cells is usually three-dimensional, although 1D and 2D models are sometimes used. Values for attributes such as porosity, permeability and water saturation are associated with each cell. The value of each attribute is implicitly deemed to apply uniformly throughout the volume of the reservoir represented by the cell.\n\nReservoir models typically fall into two categories:\n\nSometimes a single \"shared earth model\" is used for both purposes. More commonly, a geological model is constructed at a relatively high (fine) resolution. A coarser grid for the reservoir simulation model is constructed, with perhaps two orders of magnitude fewer cells. Effective values of attributes for the simulation model are then derived from the geological model by an upscaling process. Alternatively, if no geological model exists, the attribute values for a simulation model may be determined by a process of sampling geological maps.\n\nUncertainty in the true values of the reservoir properties is sometimes investigated by constructing several different realizations of the sets of attribute values. The behaviour of the resulting simulation models can then indicate the associated level of economic uncertainty.\n\nThe phrase \"reservoir characterization\" is sometimes used to refer to reservoir modeling activities up to the point when a simulation model is ready to simulate the flow of fluids.\n\nCommercially available software is used in the construction, simulation and analysis of the reservoir models.\n\nThe processes required to construct reservoir models are described by the phrase \"Seismic to simulation\". The process is successful if the model accurately reflects the original well logs, seismic data and production history.\n\nReservoir models are constructed to gain a better understanding of the subsurface that leads to informed well placement, reserves estimation and production planning. Models are based on measurements taken in the field, including well logs, seismic surveys, and production history. \n\nSeismic to simulation enables the quantitative integration of all field data into an updateable reservoir model built by a team of geologists, geophysicists, and engineers. Key techniques used in the process include integrated petrophysics and rock physics to determine the range of lithotypes and rock properties, geostatistical inversion to determine a set of plausible seismic-derived rock property models at sufficient vertical resolution and heterogeneity for flow simulation, stratigraphic grid transfer to accurately move seismic-derived data to the geologic model, and flow simulation for model validation and ranking to determine the model that best fits all the data.\n\nThe first step in seismic to simulation is establishing a relationship between petrophysical key rock properties and elastic properties of the rock. This is required in order to find common ground between the well logs and seismic data.\n\nWell logs are measured in depth and provide high resolution vertical data, but no insight into the inter-well space. Seismic are measured in time and provide great lateral detail but is quite limited in its vertical resolution. When correlated, well logs and seismic can be used to create a fine-scale 3D model of the subsurface.\n\nInsight into the rock properties comes from a combination of basic geologic understanding and well-bore measurements. Based on an understanding of how the area was formed over time, geologists can predict the types of rock likely to be present and how rapidly they vary spatially. Well log and core measurements provide samples to verify and fine-tune that understanding. \n\nSeismic data is used by petrophysicists to identify the tops of various lithotypes and the distribution of rock properties in the inter-well space using seismic inversion attributes such as impedance. Seismic surveys measure acoustic impedance contrasts between rock layers. As different geologic structures are encountered, the sound wave reflects and refracts as a function of the impedance contrast between the layers. Acoustic impedance varies by rock type and can therefore be correlated to rock properties using rock physics relationships between the inversion attributes and petrophysical properties such as porosity, lithology, water saturation, and permeability.\n\nOnce well logs are properly conditioned and edited, a petrophysical rock model is generated that can be used to derive the effective elastic rock properties from fluid and mineral parameters as well as rock structure information. The model parameters are calibrated by comparison of the synthetic to the available elastic sonic logs. Calculations are performed following a number of rock physics algorithms including: Xu & White, Greenberg & Castagna, Gassmann, Gardner, modified upper and lower Hashin-Shtrikman, and Batzle & Wang. \n\nWhen the petrophysical rock model is complete, a statistical database is created to describe the rock types and their known properties such as porosity and permeability. Lithotypes are described, along with their distinct elastic properties.\n\nIn the next step of seismic to simulation, seismic inversion techniques combine well and seismic data to produce multiple equally plausible 3D models of the elastic properties of the reservoir. Seismic data is transformed to elastic property log(s) at every trace. Deterministic inversion techniques are used to provide a good overall view of the porosity over the field, and serve as a quality control check. To obtain greater detail needed for complex geology, additional stochastic inversion is then employed.\n\nGeostatistical inversion procedures detect and delineate thin reservoirs otherwise poorly defined. Markov chain Monte Carlo (MCMC) based geostatistical inversion addresses the vertical scaling problem by creating seismic derived rock properties with vertical sampling compatible to geologic models.\n\nAll field data is incorporated into the geostatistical inversion process through the use of probability distribution functions (PDFs). Each PDF describes a particular input data in geostatistical terms using histograms and variograms, which identify the odds of a given value at a specific place and the overall expected scale and texture based on geologic insight.\n\nOnce constructed, the PDFs are combined using Bayesian inference, resulting in a posterior PDF that conforms to everything that is known about the field. A weighting system is used within the algorithm, making the process more objective.\n\nFrom the posterior PDF, realizations are generated using a Markov chain Monte Carlo algorithm. These realizations are statistically fair and produce models of high detail, accuracy and realism. Rock properties like porosity can be cosimulated from the elastic properties determined by the geostatistical inversion. This process is iterated until a best fit model is identified.\n\nInversion parameters are tuned by running the inversion many times with and without well data. Without the well data, the inversions are running in blind-well mode. These blind-well mode inversions test the reliability of the constrained inversion and remove potential bias. \n\nThis statistical approach creates multiple, equi-probable models consistent with the seismic, wells, and geology. Geostatistical inversion simultaneously inverts for impedance and discrete properties types, and other petrophysical properties such as porosity can then be jointly cosimulated. \n\nThe output volumes are at a sample rate consistent with the reservoir model because making synthetics of finely sampled models is the same as from well logs. Inversion properties are consistent with well log properties because the histograms used to generate the output rock properties from the inversion are based on well log values for those rock properties.\n\nUncertainty is quantified by using random seeds to generate slightly differing realizations, particularly for areas of interest. This process improves the understanding of uncertainty and risk within the model.\n\nFollowing geostatistical inversion and in preparation for history matching and flow simulation, the static model is re-gridded and up-scaled. The transfer simultaneously converts time to depth for the various properties and transfers them in 3D from the seismic grid to a corner-point grid. The relative locations of properties are preserved, ensuring data points in the seismic grid arrive in the correct stratigraphic layer in the corner point grid.\n\nThe static model built from seismic is typically orthogonal but flow simulators expect corner point grids. The corner point grid consists of cubes that are usually much coarser in the horizontal direction and each corner of the cube is arbitrarily defined to follow the major features in the grid. Converting directly from orthogonal to corner point can cause problems such as creating discontinuity in fluid flow. \n\nAn intermediate stratigraphic grid ensures that important structures are not misrepresented in the transfer. The stratigraphic grid has the same number of cells as the orthogonal seismic grid, but the boundaries are defined by stratigraphic surfaces and the cells follow the stratigraphic organization. This is a stratigraphic representation of the seismic data using the seismic interpretation to define the layers. The stratigraphic grid model is then mapped to the corner point grid by adjusting the zones.\n\nUsing the porosity and permeability models and a saturation height function, initial saturation models are built. If volumetric calculations identify problems in the model, changes are made in the petrophysical model without causing the model to stray from the original input data. For example, sealing faults are added for greater compartmentalization.\n\nIn the last step of seismic to simulation, flow simulation continues the integration process by bringing in the production history. This provides a further validation of the static model against history. A representative set of the model realizations from the geostatistical inversion are history matched against production data. If the properties in the model are realistic, simulated well bottom hole pressure behavior should match historical (measured) well bottom hole pressure. Production flow rates and other engineering data should also match. \n\nBased on the quality of the match, some models are eliminated. After the initial history match process, dynamic well parameters are adjusted as needed for each of the remaining models to improve the match. The final model represents the best match to original field measurements and production data and is then used in drilling decisions and production planning.\n\n\n"}
{"id": "14905284", "url": "https://en.wikipedia.org/wiki?curid=14905284", "title": "Scottish Episcopalians Act 1711", "text": "Scottish Episcopalians Act 1711\n\nThe Scottish Episcopalians Act 1711 (10 Ann c 10) is an Act of the Parliament of Great Britain. Its purpose was \"to prevent the disturbing those of the Episcopal Communion in Scotland in the Exercise of their Religious Worship and in the Use of the Liturgy of the Church of England and for repealing the Act passed in the Parliament of Scotland intituled Act against irregular Baptisms and Marriages\".\n\nThis Act was partly in force in Great Britain at the end of 2010.\n\nThis section was repealed by Part II of Schedule 1 to the Promissory Oaths Act 1871.\n\nThis section was repealed by Part V of Schedule 1 to the Statute Law (Repeals) Act 1977.\n\nThis section was repealed by section 28(20 of, and Schedule 3 to, the Marriage (Scotland) Act 1977\n\nThis section was repealed by Part V of Schedule 1 to the Statute Law (Repeals) Act 1977.\n\n"}
{"id": "38028979", "url": "https://en.wikipedia.org/wiki?curid=38028979", "title": "Self-consciousness (Vedanta)", "text": "Self-consciousness (Vedanta)\n\nSelf-consciousness in the Upanishads is not the first-person indexical self-awareness or the self-awareness which is self-reference without identification, and also not the self-consciousness which as a kind of desire is satisfied by another self-consciousness. It is Self-realisation; the realisation of the Self consisting of consciousness that leads all else.\n\nThe word \"Self-consciousness\" in the Upanishads means the knowledge about the existence and nature of Brahman. It means the consciousness of our own real being, the primary reality. Self-consciousness means Self-knowledge, the knowledge of Prajna i.e. of Prana which is Brahman. Swami Parmeshwaranand explains that Existence is not existence if it does not mean Self-consciousness, Reality is not reality if it does not express throughout its structure the mark of Self-consciousness, the Ultimate category of existence. According to the Upanishads the Atman or Paramatman and Ishvara are unknowable; they are not merely objects of faith but the objects of mystical realisation. The Atman is unknowable in its essential nature; it is unknowable in its essential nature because it is the eternal subject who knows about everything including itself. The Atman is the knower and also the known.\n\nMetaphysicians regard the Self either to be distinct from the Absolute or entirely identical with the Absolute. They have given form to three schools of thought – a) the \"Dualistic school\", b) the \"Quasi-dualistic school\" and c) the \"Monistic school\", as the result of their varying mystical experiences. Prakrti and Atman, when treated as two separate and distinct aspects form the basis of the Dualism of the Shvetashvatara Upanishad. Quasi-dualism is reflected in the Vaishnavite-monotheism of Ramanuja and the absolute Monism, in the teachings of Adi Shankara.\n\nSelf-consciousness is the Fourth state of consciousness or \"Turiya\", the first three being \"Vaisvanara\", \"Taijasa\" and \"Prajna\". These are the four states of individual consciousness.\n\nThere are three distinct stages leading to Self-realisation. The First stage is in mystically apprehending the glory of the Self within us as though we were distinct from it. The Second stage is in identifying the “I-within” with the Self, that we are in essential nature entirely identical with the pure Self. The Third stage is in realising that the Atman is Brahman, that there is no difference between the Self and the Absolute. The Fourth stage is in realising \"I am the Absolute\" - \"Aham Brahman Asmi\". The Fifth stage is in realising that Brahman is the “All” that exists, as also that which does not exist.\n\nThe sublime state of self-consciousness is reached after the Seeker after Truth devoid of egoism and delusion, overcoming the flaws of attachment, firm in spirituality, free from lusts, released from dualities called pleasures and pains, the un-deluded repairs to the imperishable status, because for a knower of Brahman who has realised the Ultimate Truth, there is much profit from reservoirs when all around there is an inundation. Through Self-consciousness one gains the knowledge of Existence which is the knowledge of Sole Reality. It is not mere intellectual apprehension of Truth, it is the apprehension of the Oneness which has to be realised here in this very life. In the Bhagavad Gita XIV.20 Lord Krishna tells Arjuna that when the embodied being is able to transcend these three modes or \"gunas\" associated with the material body i.e. \"Sattva\", the mode of goodness, \"Rajas\", the mode of passion and \"Tamasa\", the mode of ignorance, he can become free from birth, death, old age and their distresses and can enjoy nectar even in this life. Self-consciousness is a positive experience. It is the direct realization of the immortal Brahman - \"he enters into My Being\" - Bhagavad Gita XIV.19 who is \"the ground of the imperishable Brahman, of immortality, of the eternal virtue and of unending immutable bliss\" - Bhagavad GitaXIV.27.\n"}
{"id": "187557", "url": "https://en.wikipedia.org/wiki?curid=187557", "title": "Smile", "text": "Smile\n\nA smile is a facial expression formed primarily by flexing the muscles at the sides of the mouth. It is believed it takes about 42 muscles to smile. Some smiles include a contraction of the muscles at the corner of the eyes, an action known as a \"Duchenne smile\". Smiles performed without the eye contraction may be perceived as insincere.\n\nAmong humans, smiling is an expression denoting pleasure, sociability, happiness, joy or amusement. It is distinct from a similar but usually involuntary expression of anxiety known as a grimace. Although cross-cultural studies have shown that smiling is a means of communication throughout the world, there are large differences among different cultures, with some using smiles to convey confusion or embarrassment.\n\nPrimatologist Signe Preuschoft traces the smile back over 30 million years of evolution to a \"fear grin\" stemming from monkeys and apes who often used barely clenched teeth to portray to predators that they were harmless, or to signal submission to more dominant group members. The smile may have evolved differently among species and especially among humans. Apart from Biology as an academic discipline that interprets the smile, those who study kinesics and psychology such as Freitas-Magalhaes view the smile as an affect display that can communicate feelings such as love, happiness, glee, pride, contempt, and embarrassment. Also, other types of primates can express this gesture as a symbol of happiness and fun.\n\nA smile seems to have a favorable influence upon others and makes one likable and more approachable. In the social context, smiling and laughter have different functions in the order of sequence in social situations:\n\nSmiling is a signaling system that evolved from a need to communicate information of many different forms. One of these is advertisement of sexual interest. Female smiles are appealing to heterosexual males, increasing physical attractiveness and enhancing sex appeal. However, recent research indicates a man's smile may or may not be most effective in attracting heterosexual women, and that facial expressions such as pride or even shame might be more effective. The researchers ignored the role of smiles in other sexual preferences.\n\nThe influence of smiling on others is not necessarily benign. It may take the form of positive reinforcement, possibly for an underhand manipulative and abusive purpose. See also superficial smile.\n\nWhile smiling is perceived as a positive emotion most of the time, there are many cultures that perceive smiling as a negative expression and consider it unwelcoming. Too much smiling can be viewed as a sign of shallowness or dishonesty. In some parts of Asia, people may smile when they are embarrassed or in emotional pain. Some people may smile at others to indicate a friendly greeting. A smile may be reserved for close friends and family members. Many people in the former Soviet Union area consider smiling at strangers in public to be unusual and even suspicious behavior. \n\nCheek dimples are visible indentations of the epidermis, caused by underlying flesh, which form on some people's cheeks, especially when they smile. Dimples are genetically inherited and are a dominant trait. A rarer form is the single dimple, which occurs on one side of the face only. Anatomically, dimples may be caused by variations in the structure of the facial muscle known as zygomaticus major. Specifically, the presence of a double or bifid zygomaticus major muscle may explain the formation of cheek dimples. \n\nThis bifid variation of the muscle originates as a single structure from the zygomatic bone. As it travels anteriorly, it then divides with a superior bundle that inserts in the typical position above the corner of the mouth. An inferior bundle inserts below the corner of the mouth.\n\nWhile conducting research on the physiology of facial expressions in the mid-19th century, French neurologist Guillaume Duchenne identified two distinct types of smiles. A Duchenne smile involves contraction of both the zygomatic major muscle (which raises the corners of the mouth) and the orbicularis oculi muscle (which raises the cheeks and forms crow's feet around the eyes). The Duchenne smile has been described as \"smizing\", as in \"smiling with the eyes\". An exaggerated Duchenne smile is associated with lying.\n\nA non-Duchenne smile involves only the zygomatic major muscle. \"Research with adults initially indicated that joy was indexed by generic smiling, any smiling involving the raising of the lip corners by the zygomatic major [...]. More recent research suggests that smiling in which the muscle around the eye contracts, raising the cheeks high (Duchenne smiling), is uniquely associated with positive emotion.\"\n\nThe Pan Am smile, also known as the \"Botox smile\", is the name given to a fake smile, in which only the zygomatic major muscle is voluntarily contracted to show politeness. It is named after the now defunct airline Pan American World Airways, whose flight attendants would always flash every passenger the same perfunctory smile. Botox was introduced for cosmetic use in 2002. Chronic use of Botox injections to deal with eye wrinkle can result in the paralysis of the small muscles around the eyes, preventing the appearance of a Duchenne smile.\n\nIn animals, the baring of teeth is often used as a threat or warning display—known as a snarl—or a sign of submission. For chimpanzees, it can also be a sign of fear. However, not all animal displays of teeth convey negative acts or emotions. For example, Barbary macaques demonstrate an open mouth display as a sign of playfulness, which likely has similar roots and purposes as the human smile.\n\n\n\n\n"}
{"id": "3328588", "url": "https://en.wikipedia.org/wiki?curid=3328588", "title": "Societal collapse", "text": "Societal collapse\n\nSocietal collapse is the fall of a complex human society. Such a disintegration may be relatively abrupt, as in the case of Maya civilization, or gradual, as in the case of the fall of the Western Roman Empire. \n\nThe subject of societal collapse is of interest in such fields as history, anthropology, sociology, political science, and, more recently, complex-systems science.\n\nCommon factors that may contribute to societal collapse are economical, environmental, social and cultural, and disruptions in one domain sometimes cascade into others. In some cases a natural disaster (e.g. tsunami, earthquake, massive fire or climate change) may precipitate a collapse. Other factors such as a Malthusian catastrophe, overpopulation or resource depletion might be the proximate cause of collapse. Significant inequity may combine with lack of loyalty to established political institutions and result in an oppressed lower class rising up and seizing power from a smaller wealthy elite in a revolution. The diversity of forms that societies evolve corresponds to diversity in their failures. Jared Diamond suggests that societies have also collapsed through deforestation, loss of soil fertility, restrictions of trade and/or rising endemic violence.\n\nThe decline of the Roman Empire is one of the events traditionally marking the end of Classical Antiquity and the beginning of the European Middle Ages. Throughout the 5th century, the Empire's territories in western Europe and northwestern Africa, including Italy, fell to various invading or indigenous peoples in what is sometimes called the Barbarian invasions, although the eastern half still survived with borders essentially intact for another two centuries (until the Arab expansion). This view of the collapse of the Roman Empire is challenged, however, by modern historians who see Rome as merely transforming from the Western Empire into barbarian kingdoms as the Western Emperors delegated themselves out of existence, and the East transforming into the Byzantine Empire, which only fell in 1453 AD.\n\nNorth Africa's populous and flourishing civilization collapsed after exhausting its resources in internal fighting and suffering devastation from the invasion of the Bedouin tribes of Banu Sulaym and Banu Hilal. Ibn Khaldun noted that the lands ravaged by Banu Hilal invaders had become completely arid desert.\n\nIn the brutal pillaging that followed Mongol invasions, the invaders decimated the populations of China, Russia, the Middle East, and Islamic Central Asia. Later Mongol leaders, such as Timur, though he himself became a Muslim, destroyed many cities, slaughtered thousands of people and did irreparable damage to the ancient irrigation systems of Mesopotamia. These invasions transformed a settled society to a nomadic one.\n\nEncounters between European explorers and populations in the rest of the world often introduced local epidemics of extraordinary virulence. Smallpox ravaged Mexico in the 1520s, killing 150,000 in Tenochtitlán alone, including the emperor, and Peru in the 1530s, aiding the European conquerors. Some believe that the death of up to 95% of the Native American population of the New World was caused by Old World diseases although new research suggests tuberculosis from seals and sea lions played a significant part. Live smallpox was also included in the ship inventories of the Australian first settlement, and a smallpox epidemic spread across the continent 3 years after European settlement.\n\nSocietal collapse of many indigenous cultures has occurred as a result of European imperialism in various parts of the globe, particularly in areas where European settler communities took possession of land once held by native peoples, in Latin America and North America, and in Australasia. The effects of this dispossession are still evident in many of the problems confronting indigenous cultures, including alcoholism, high rates of incarceration, suicide rates and fraternal violence.\n\nThe Greek historian Polybius, writing in \"The Histories\", largely blamed the decline of the Hellenistic world on low fertility rates: \n\nIn a speech to Roman nobles, Emperor Augustus commented on the low birthrates of the Roman elite:\n\nUpon the establishment of the Roman Empire, Augustus introduced legislation designed to increase the birthrate of the Roman nobility.\n\nThere are three main types of collapse:\n\nReversion/Simplification: A society's adaptive capacity may be reduced by either a rapid change in population or societal complexity, destabilizing its institutions and causing massive shifts in population and other social dynamics. In cases of collapse, civilizations tend to revert to less complex, less centralized socio-political forms using simpler technology. These are characteristics of a Dark Age. Examples of such societal collapse are: the Hittite Empire, the Mycenaean civilization, the Western Roman Empire, the Mauryan and Gupta Empires in India, the Mayas, the Angkor in Cambodia, the Han and Tang dynasties in China and the Mali Empire.\n\nIncorporation/Absorption: Alternately, a society may be gradually incorporated into a more dynamic, more complex inter-regional social structure. This happened in Ancient Egypt and Mesopotamia, the Levantine cultures, the Mughal and Delhi Sultanates in India, Song China, the Aztec culture in Mesoamerica, the Inca culture in South America, and the modern civilizations of China, Japan, and India, as well as many modern states in the Middle East and Africa.\n\nObliteration: Vast numbers of people in the society die, or the birth rate plunges to a level that causes a dramatic depopulation.\n\nOther changes that may accompany a collapse:\n\n\nIn the general study of cultural change and population dynamics, a whole system displays complex ecosystem changes. Organizational adaptability relates importantly to organizational diversity.\n\nSeveral key features of human societal collapse can be related to population dynamics\n\nThe coupled breakdown of economic, cultural and social institutions with ecological relationships is perhaps the most common feature of collapse. In his book \"\", Jared Diamond proposes five interconnected causes of collapse that may reinforce each other: non-sustainable exploitation of resources, climate changes, diminishing support from friendly societies, hostile neighbors, and inappropriate attitudes for change.\n\nJoseph Tainter theorizes that collapsed societies essentially exhausted their own designs, and were unable to adapt to natural diminishing returns for what they knew as their method of survival. It matches closely Toynbee's idea that \"they find problems they can't solve\".\n\nModern social critics commonly interpret things like sedentary social behavior as symptomatic of societal decay, and link what appears to be laziness with the depletion of important non-renewable resources. However, many primitive cultures also have high degrees of leisure, so if that is a cause in one place it may not be in another—leisure or apparent laziness is then not a sufficient cause.\n\nWhat produces modern sedentary life, unlike nomadic hunter-gatherers, is extraordinary modern economic productivity. Tainter argues that exceptional productivity is actually more the sign of hidden weakness, both because of a society's dependence on it, and its potential to undermine its own basis for success by not being self limiting as demonstrated in Western culture's ideal of perpetual growth.\n\nAs a population grows and technology makes it easier to exploit depleting resources, the environment's diminishing returns are hidden from view. Societal complexity is then potentially threatened if it develops beyond what is actually sustainable, and a disorderly reorganization were to follow. The scissors model of Malthusian collapse, where the population grows without limit and resources do not, is the idea of great opposing environmental forces cutting into each other.\n\nFor the modern world economy, for example, the growing conflict between food and fuel, depending on many of the same finite and diminishing resources, is visible in recent major commodity price shocks. It is one of the key relationships researchers, since the early studies of the Club of Rome, have been most concerned with.\n\nJared Diamond pursues these themes in his 2005 book \"\".\n\nRomanian American economist Nicholas Georgescu-Roegen, a progenitor in economics and the paradigm founder of ecological economics, has argued that the carrying capacity of Earth — that is, Earth's capacity to sustain human populations and consumption levels — is bound to decrease sometime in the future as Earth's finite stock of mineral resources is presently being extracted and put to use; and consequently, that the world economy as a whole is heading towards an inevitable future collapse, leading to the demise of human civilisation itself.\n\nGeorgescu-Roegen is basing his pessimistic prediction on the two following considerations:\nTaken together, the Industrial Revolution in Britain in the second half of the 18th century has unintentionally thrust man's economy into a long, never-to-return overshoot-and-collapse trajectory with regard to the Earth's mineral stock. The world economy will continue growing until its inevitable and final collapse in the future. From that point on, Georgescu-Roegen conjectures, ever deepening scarcities will aggravate social conflict throughout the globe and ultimately spell the end of mankind itself.\n\nGeorgescu-Roegen was the paradigm founder of ecological economics and is also considered the main intellectual figure influencing the degrowth movement. Consequently, much work in these fields is devoted to discussing the existential impossibility of allocating earth's finite stock of mineral resources evenly among an unknown number of present and future generations. This number of generations is likely to remain unknown to us, as there is no way — or only little way — of knowing in advance if or when mankind will ultimately face extinction. In effect, \"any\" conceivable intertemporal allocation of the stock will inevitably end up with universal economic decline at some future point. \n\nA related economic model is proposed by Thomas Homer-Dixon and by Charles Hall in relation to our declining productivity of energy extraction, or energy return on energy invested (EROEI). This measures the amount of surplus energy a society gets from using energy to obtain energy.\n\nThere would be no surplus if EROEI approaches 1:1. What Hall showed is that the real cutoff is well above that, estimated to be 3:1 to sustain the essential overhead energy costs of a modern society. Part of the mental equation is that the EROEI of our generally preferred energy source, petroleum, has fallen in the past century from 100:1 to the range of 10:1 with clear evidence that the natural depletion curves all are downward decay curves. An EROEI of more than ~3, then, is what appears necessary to provide the energy for societally important tasks, such as maintaining government, legal and financial institutions, a transportation infrastructure, manufacturing, building construction and maintenance and the life styles of the rich and poor that a society depends on.\n\nThe EROEI figure also affects the number of people needed for sustainable food production. In the pre-modern world, it was often the case that 80% of the population was employed in agriculture to feed a population of 100%, with a low energy budget. In modern times, the use of cheap fossil fuels with an exceedingly high EROEI enabled 100% of the population to be fed with only 4% of the population employed in agriculture. Diminishing EROEI making fuel more expensive relative to other things may require food to be produced using less energy, and so increases the number of people employed in food production again.\n\nAccording to Joseph Tainter (1990), too many scholars offer facile explanations of societal collapse by assuming one or more of the following three models in the face of collapse:\n\n\nTainter argues that these models, though superficially useful, cannot severally or jointly account for all instances of societal collapse. Often they are seen as interconnected occurrences that reinforce each other.\n\nFor example, the failure of Easter Island's leaders to remedy rapid ecological deterioration cannot be understood without reference to the other models above. The islanders, who erected large statues called \"moai\" as a form of religious reverence to their ancestors, used felled trees as rollers to transport them. Because the islanders firmly believed that their displays of reverence would lead to increased future prosperity, they had a deeply entrenched incentive to intensify \"moai\" production. Because Easter Island's geographic isolation made its resources hard to replenish and made the balance of its overall ecosystem very delicate (\"House of Cards\"), deforestation led to soil erosion and insufficient resources to build boats for fishing or tools for hunting. Competition for dwindling resources resulted in warfare and many casualties (an additional \"Runaway Train\" iteration). Together these events led to the collapse of the civilization, but no single factor above provides an adequate account.\n\nMainstream interpretations of the history of Easter Island also include the slave raiders who abducted a large proportion of the population and epidemics that killed most of the survivors (see Easter Island History#Destruction of society and population.) Again, no single point explains the collapse; only a complex and integrated view can do so.\n\nTainter's position is that social complexity is a recent and comparatively anomalous occurrence requiring constant support. He asserts that collapse is best understood by grasping four axioms. In his own words (p. 194):\n\n\nWith these facts in mind, collapse can simply be understood as a loss of the energy needed to maintain social complexity. Collapse is thus the sudden loss of social complexity, stratification, internal and external communication and exchange, and productivity.\n\nThe British historian Arnold J. Toynbee, in his 12-volume magnum opus \"A Study of History\" (1961), theorized that all civilizations pass through several distinct stages: genesis, growth, time of troubles, universal state, and disintegration. (Carroll Quigley would expand on and refine this theory in his \"The Evolution of Civilizations\".)\n\nToynbee argues that the breakdown of civilizations is not caused by loss of control over the environment, over the human environment, or attacks from outside. Rather, societies that develop great expertise in problem solving become incapable of solving new problems by overdeveloping their structures for solving old ones.\n\nThe fixation on the old methods of the \"Creative Minority\" leads it to eventually cease to be creative and degenerates into merely a \"dominant minority\" (that forces the majority to obey without meriting obedience), failing to recognize new ways of thinking. He argues that creative minorities deteriorate due to a worship of their \"former self\", by which they become prideful, and fail to adequately address the next challenge they face.\n\nHe argues that the ultimate sign a civilization has broken down is when the dominant minority forms a Universal State, which stifles political creativity. He states:\n\nHe argues that, as civilizations decay, they form an \"Internal Proletariat\" and an \"External Proletariat.\" The Internal proletariat is held in subjugation by the dominant minority inside the civilization, and grows bitter; the external proletariat exists outside the civilization in poverty and chaos, and grows envious. He argues that as civilizations decay, there is a \"schism in the body social\", whereby \"abandon\" and \"self-control\" together replace creativity, and \"truancy\" and \"martyrdom\" together replace discipleship by the creative minority.\n\nHe argues that in this environment, people resort to archaism (idealization of the past), futurism (idealization of the future), detachment (removal of oneself from the realities of a decaying world), and transcendence (meeting the challenges of the decaying civilization with new insight, as a Prophet). He argues that those who transcend during a period of social decay give birth to a new Church with new and stronger spiritual insights, around which a subsequent civilization may begin to form after the old has died.\n\nToynbee's use of the word 'church' refers to the collective spiritual bond of a common worship, or the same unity found in some kind of social order.\n\nThe great irony expressed by these and others like them is that civilizations that seem ideally designed to creatively solve problems, find themselves doing so self-destructively. \n\nResearchers, as yet, have very little ability to identify internal structures of large distributed systems like human societies, which is an important scientific problem. Genuine structural collapse seems, in many cases, the only plausible explanation supporting the idea that such structures exist. However, until they can be concretely identified, scientific inquiry appears limited to the construction of scientific narratives, using systems thinking for careful storytelling about systemic organization and change.\n\nHistory includes many examples of the appearance and disappearance of human societies with no obvious explanation. The abrupt dissolution of the Soviet Union in the course of a few months, without any external attack, according to Johan Galtung was due to growing structural contradictions brought on by geopolitical overreach, which could not be resolved within the existing socio-political systems.\n\nAlthough a societal collapse is generally an endpoint for the administration of a culture's social and economic life, societal collapse can also be seen as simply a change of administration within the same culture. Russian culture would seem to have outlived both the society of Imperial Russia and the society of the Soviet Union, for example. Frequently the societal collapse phenomenon is also a process of decentralization of authority after a 'classic' period of centralized social order, perhaps replaced by competing centers as the central authority weakens. Societal failure may also result in a degree of empowerment for the lower levels of a former climax society, who escape from the burden of onerous taxes and control by exploitative elites. For example, the black plague contributed to breaking the hold of European feudal society on its underclass in the 15th century.\n\n\n\n\n\nMalthusian and environmental collapse themes\n\n\nCultural and institutional collapse themes:\n\n\nSystems science:\n\n\n"}
{"id": "24919408", "url": "https://en.wikipedia.org/wiki?curid=24919408", "title": "Space segment", "text": "Space segment\n\nThe space segment of an artificial satellite system is one of its three operational components (the others being the user and ground segments). It comprises the satellite or satellite constellation and the uplink and downlink satellite links.\n\nThe overall design of the payload, satellite, ground segment, and end-to-end system is a complex task. Satellite communications payload design must be properly coupled with the capabilities and interaction with the spacecraft bus that provides power, stability and environmental support to the payload.\n\nGeostationary earth orbit (GEO) supports businesses in satellite television and radio broadcasting, as well as data and mobile communications. The medium earth orbit (MEO) and low earth orbit (LEO) configurations can also be used for various applications.\n\nA communications satellite is composed of a communications payload (repeater and antenna) and supporting spacecraft bus (including solar arrays and batteries, attitude and orbit control systems, structure and thermal control system), and is placed in orbit by a launch vehicle. A successful satellite operator needs the right orbital slot or constellation, and satellites that deliver effective power and bandwidth to desirable regions and markets (i.e., those with growing demand for satellite services).\n\nSatellite radio now serves nearly 5 million subscribers, and satellite mobile telephone and data operators offer connectivity throughout the globe. Broadband mobile terminals now provide improved access to the Internet for a range of applications, including videoconferencing.\n\n"}
{"id": "35328973", "url": "https://en.wikipedia.org/wiki?curid=35328973", "title": "Terrorism and social media", "text": "Terrorism and social media\n\nDue to the convenience, affordability, and broad reach of social media platforms such as YouTube, Facebook and Twitter, terrorist groups have increasingly used social media to further their goals and spread their message. Attempts have been made by various governments and agencies to thwart the use of social media by terrorist organizations.\n\nMany authors have proposed that media attention increases perceptions of risk of fear of terrorism and crime and relates to how much attention the person pays to the news. The relationship between terrorism and the media has long been noted. Terrorist organizations depend on the open media systems of democratic countries to further their message and goals.In order to garner publicity towards their cause, terrorist organizations resort to acts of violence and aggression that deliberately target civilians. This method has proven to be effective in gathering attention:\n\nIt cannot be denied that although terrorism has proved remarkably ineffective as the major weapon for taking down governments and capturing political power, it has been a remarkably successful means of publicizing a political cause and relaying the terrorist threat to a wider audience, particularly in the open and pluralistic countries of the West. When one says 'terrorism' in a democratic society, one also says 'media'.\n\nWhile a media organization may not support the goals of terrorist organizations, it is their job to report current events and issues. In the fiercely competitive media environment, when a terrorist attack occurs, media outlets scramble to cover the event. In doing so the media help to further the message of terrorist organizations:\n\nTo summarise briefly on the symbiotic nature of the relationship between terrorists and the media, the recent history of terrorism in many democratic countries vividly demonstrates that terrorists do thrive on the oxygen of publicity, and it is foolish to deny this. This does not mean that the established democratic media share the values of the terrorists. It does demonstrate, however, that the free media in an open society are particularly vulnerable to exploitation and manipulation by ruthless terrorist organisations.\n\nOne notable example of the relationship between terror groups and the media was the release of the Osama bin Laden audio and video recordings. These tapes were sent directly to mainstream Arabic television networks including Al-Jazeera.\n\nMedia can often be the source of discontent for terrorist groups. Irene Kahn says,New seeds of social discord and insecurity are sprouting between citizens and noncitizens. Racism and xenophobia are latent in all societies, but in some European countries they feature blatantly as some politicians exploit people's fears and prejudices for short-term electoral gains. Some aspects of the media have played into this strategy, dehumanizing and demonizing foreigners, foreign-born citizens, refugees, and asylum seekers. They are pointed out as a source of danger and become an easy target for hate speech and violence. Those who need their rights protected the most have become the ones most at risk of attacks.\nMost terrorist groups use social media as a means to bypass the traditional media and spread their propaganda.\n\nMichel Foucault's theory of surveillance, panopticism, describes a networks of power, where all parties are transfixed by the actions of the others in the network. This model can be transposed on the network of power that media-outlet consumers and producers enter. In a network of power that includes consumers and producers, both parties have fixed gazes' on each other. The consumers transfix their gazes' on the stories that media outlets produce. And, the needs of the consumers, which is in this case their need to be updated regularly, becomes the producers gaze. The producers or media outlets are in competition with other media outlets to supply their constituents with the most up-to-date information. This network of fixed gazes' is both \"privileged and imperative\" for the system to satisfy the status quo. This network is especially imperative when major events in the world occur, which is usually the case with terrorism. Consumers looks to media outlets to provide news on terrorism. If consumers believe terrorism is a threat to their safety, they want to be informed of the threats against them. Media outlets fulfill their viewers' needs, and portray terrorism as a threat because of the cycle that surveillance engenders. As terrorism flourishes as a prominent discourse of fear, consumers want information faster because they feel their safe being is in peril. The idea of total surveillance, as prescribed by Foucault, becomes a cycle where the disruption of power causes scrutiny by various players in system. If the media-outlets are not constantly looking for stories that fulfill consumer needs, then they are scrutinized. In addition to the surveillance aspect of news dissemination, therein is the notion that \"needs\" drive the network of power: both the media outlets and consumers have needs that are fulfilled by broadcasting the news. It is this idea expressed in the uses and gratifications theory. It stipulates that the active audience and the terrorist \"seek to satisfy their various needs\" through media transmission. While media outlets know the stories they show have astounding effects on the political and sociological perspective in society, the impetus on economic gains is of greater importance.\n\nIn a study by Gabriel Weimann from the University of Haifa, Weimann found that nearly 90% of organized terrorism on the internet takes place via social media. According to Weimann, terror groups use social media platforms like Twitter, Facebook, YouTube, and internet forums to spread their messages, recruit members and gather intelligence.\n\nTerror groups take to social media because social media tools are cheap and accessible, facilitate quick, broad dissemination of messages, and allow for unfettered communication with an audience without the filter or \"selectivity\" of mainstream news outlets. Also, social media platforms allow terror groups to engage with their networks. Whereas previously terror groups would release messages via intermediaries, social media platforms allow terror groups to release messages directly to their intended audience and converse with their audience in real time: Weimann also mentions in \"Theater of Terror\", that terrorists use the media to promote the theatrical like nature of the premeditated terror.\n\nHSMPress is using Twitter the way social media experts have always advised- not just broadcasting, but engaging in conversation. Spend some time following the account, and you realize that you're dealing with a real human being with real ideas—albeit boastful, hypocritical, violent ideas.\n\nAl-Qaeda has been noted as being one of the terror groups that uses social media the most extensively. Brian Jenkins, senior advisor for the Rand Corporation, commented on Al-Qaeda's dominant presence on the web:\n\nWhile almost all terrorist organizations have websites, al Qaeda is the first to fully exploit the internet. This reflects al Qaeda's unique characteristics. It regards itself as a global movement and therefore depends on a global communications network to reach its perceived constituents. It sees its mission as not simply creating terror among its foes but awakening the Muslim community. Its leaders view communications as 90 percent of the struggle. Despite the risks imposed by intense manhunts, its leaders communicate regularly with video and audio messages, which are posted on its websites and disseminated on the Internet. The number of websites devoted to the al Qaeda-inspired movement has grown from a handful to reportedly thousands, although many of these are ephemeral.\n\nKnown terrorist group the Islamic State of Iraq and the Levant, also translated to ISIS, uses the widespread of news over social media to their advantage when releasing threatening videos of beheadings. As of November 16, 2014, following the beheading of former U.S. Army Ranger Peter Kassig, there have now been five recorded executions of Westerners taken captive in Syria. James Foley, David Cawthorne Haines, Alan Henning, and Steven Sotloff are also among the men kidnapped and executed by ISIS. The videos of the brutal beheadings are both posted online by ISIS, where they can be viewed by anyone using their own discretion, and sent to government officials as threats. Posting the executions online allows the terrorist groups the power to manipulate and cause havoc among the population viewing them, and the videos have the ability to instill fear within the Western world. The videos are typically high production quality and generally show the entirety of the gruesome act, with the hostage speaking a few words before they are killed on camera.\n\nIn the case of U.S. aid worker Peter Kassig, his video did not show the actual beheading act and he did not speak any final words before the execution. His silence and the fact that the actual execution was not included in the video raised question about his video was different than the rest. In response to Kassig's beheading, his family expressed their wish that news media avoid doing what the group wants by refraining from publishing or distributing the video. By refusing to circulate the video of the beheading, it therefore loses the ability to manipulate Americans or further the cause of the terrorist group.\nThe Taliban has been active on Twitter since May 2011, and has more than 7,000 followers. Tweeting under the handle @alemarahweb, the Taliban tweets frequently, on some days nearly hourly. This account is currently suspended. \nIn December 2011, it was discovered that the Somalia-based terror cell Al-Shabab was using a Twitter account under the name @HSMPress. Since opening on December 7, 2011, the account has amassed tens of thousands of followers and tweets frequently.\n\nShortly after a series of coordinated Christmas bombings in Kono, Nigeria, in 2011, the Nigerian-based terror group Boko Haram released a video statement defending their actions to YouTube.\n\nAQAP and Islamic State (ISIS/ISIL/DAESH)\n\nIslamic State has emerged as one of the most potent users of social media. In many respects, Islamic State learned their propaganda craft from al Qaeda on the Arabian Peninsula (AQAP). However, IS quickly eclipsed its mentor, deploying a whole range of narratives, images and political proselytizing through various social media platforms. A study by Berger and Morgan estimated that at least 46,000 Twitter accounts were used by ISIS supporters between September and December 2014. However, as ISIS supporters regularly get suspended and then easily create new, duplicate accounts, counting ISIS Twitter accounts over a few months can overestimate the number of unique people represented by 20–30%.\n\nHowever, as the November 2015 attacks in Paris demonstrate, IS also uses old-fashioned methods of communication and propaganda. Lewis notes that the attacks in Paris represent the sort of 'propaganda in action' which was a method developed by the 19th century anarchists in Europe. The November 2015 IS attacks were perpetrated without prior warning, largely because the operatives met face-to-face and used other non-digital means of communication.\n\nSome U.S. government officials have urged social media companies to stop hosting content from terror groups. In particular, Joe Lieberman has been especially vocal in demanding that social media companies not permit terror groups to use their tools. In 2008, Lieberman and the United States Senate Committee on Homeland Security and Governmental Affairs issued a report titled \"Violent Islamist Extremism, the Internet, and the Homegrown Terrorist Threat\". The report stated that the internet is one of the \"primary drivers\" of the terrorist threat to the United States.\nIn response to the news that Al-Shabab was using Twitter, U.S. officials have called for the company to shut down the account. Twitter executives have not complied with these demands and have declined to comment on the case.\n\nIn January 2012, Twitter announced changes to their censorship policy, stating that they would now be censoring tweets in certain countries when the tweets risked breaking the local laws of that country. The reason behind the move was stated on their website as follows:\n\nAs we continue to grow internationally, we will enter countries that have different ideas about the contours of freedom of expression. Some differ so much from our ideas that we will not be able to exist there. Others are similar but, for historical or cultural reasons, restrict certain types of content, such as France or Germany, which ban pro-Nazi content.Until now, the only way we could take account of those countries' limits was to remove content globally. Starting today, we give ourselves the ability to reactively withhold content from users in a specific country — while keeping it available in the rest of the world. We have also built in a way to communicate transparently to users when content is withheld, and why.\n\nThe move drew criticism from many Twitter users who said the move was an affront to free speech. Many of the users threatened to quit tweeting if the policy was not rescinded, including Chinese artist and activist Ai Weiwei.\n\nIn December 2010, in response to growing demands that YouTube pull video content from terrorist groups from its servers, the company added a \"promotes terrorism\" option under the \"violent or repulsive content\" category that viewers can select to \"flag\" offensive content. By limiting the terrorists access to conventional mass media and censoring news coverage of terrorist acts and their perpertrators and also minimising the terrorists allowance to manipulate mass media, the mass fear impact that is usually created will decrease.\nWestern governments have been actively trying to surveil and censor IS social media sites. As Jeff Lewis explains, as quickly as platform managers close down accounts, IS and its supporters continually create new IDs which they then use to resurge back with new accounts and sites for propaganda. A case study of an al Shabaab account and a George Washington University white paper found that accounts that resurged did not regain the high number of followers they had had originally. However this picture is complicated as a May 2016 article in the Journal of Terrorism Research found that resurgent accounts acquire an average (median) of 43.8 followers per day, while regular jihadist accounts accrue only 8.37 followers on average per day.\n\nU.S. Rep. Ted Poe, R-Texas, has said that the U.S. Constitution does not apply to terrorists and that they have given up their rights to free speech. He cited a Supreme Court ruling that anyone providing \"material support\" to a terrorist organization is guilty of a crime, even if that support only involves speaking and association. He also cited terrorist speech as being like child pornography in that it does harm.\n\nOn December 6, 2011 the US Committee on Homeland Security's Subcommittee on Counterterrorism and Intelligence held a hearing entitled \"Jihadist Use of Social Media - How to Prevent Terrorism and Preserve Innovation.\"\n\nAt the hearing, members heard testimony from William McCants, an analyst for the Center for Naval Analyses, Aaron Weisburd, director of the Society for Internet Research, Brian Jenkins, senior advisor for the Rand Corporation and Evan Kohlmann, senior partner from Flashpoint Global Partners.\nMcCants stated that while terror groups were actively using social media platforms to further their goals, research did not support the notion that the social media strategies they adopted were proving effective:\n\nWe are talking about a relatively small number of people. Because the number of people is so small, it is difficult to say why some become active supporters of al-Qaeda and others do not. What we can say is that the vast majority of people who watch and read al-Qaeda propaganda will never act violently because of it. Put metaphorically, the material may be incendiary but nearly everyone is fireproof. Since that is the case, it is better to spend our resources putting out the fires and issuing warnings about the dangers of fire rather than trying to fireproof everyone or remove incendiary material.\nMcCants added that he did not believe that closing online user accounts would be effective in stopping radicalization and stated that closing online accounts could even disadvantage US security and intelligence forces: \nI do not put much stock in closing online user accounts that do not violate our laws. I also do not put much stock in intervening with well-meaning outreach programs or removing propaganda. There are too many downsides to these approaches. They are also unnecessary. The FBI and local law enforcement in the United States have done an excellent job in finding al-Qaeda supporters online and arresting them before they hurt anyone. They have gotten very good at following the smoke trails and putting out fires.\n\nMcCants stressed that not enough research has been conducted on this topic and he would be willing to change his opinion on the matter if there was empirical evidence that proved that social media has a major role in radicalizing youth.\n\nWeisburd stated that any organization that played a part in producing and distributing media for terrorist organizations were in fact supporting terrorism:\n\nI would argue that a service provider who knowingly assists in the distribution of terrorist media is also culpable. While it is in no one's interest to prosecute internet service providers, they must be made to realize that they can neither turn a blind eye to the use of their services by terrorist organizations, nor can they continue to put the onus of identifying and removing terrorist media on private citizens.\nWeisburd argued that social media lends an air of legitimacy to content produced by terror organizations and provides terrorist organizations an opportunity to brand their content: \"Branding in terrorist media is a sign of authenticity, and terrorist media is readily identifiable as such due to the presence of trademarks known to be associated with particular organizations.\" He concluded that the goal of intelligence and security forces should not be to drive all terrorist media offline, but rather to deprive terror groups from the branding power gleaned from social media. \nJenkins stated that the risks associated with al Qaeda's online campaign do not justify an attempt to impose controls on content distributors. Any attempted controls would be costly and would deprive the intelligence officials of a valuable source of information. Jenkins also stated that there was no evidence that attempts to control online content would be possible: \nEven China, which has devoted immense resources to controlling social media networks with far fewer concerns about freedom of speech, has been unable to block the micro blogs that flourish on the web. Faced with the shutdown of one site, jihadist communicators merely change names and move to another, dragging authorities into a frustrating game of Whac-a-mole and depriving them of intelligence while they look for the new site. Is this, then the best way to address the problem?\"\nKohlmann stated US government officials must do more to pressure social media groups like YouTube, Facebook and Twitter to remove content produced by terror groups: \nUnfortunately, current U.S. law gives few incentives for companies like YouTube for volunteering information on illicit activity, or even cooperating when requested by U.S. law enforcement. If such companies are to be trusted to self-police their own professed commitments to fighting hate speech, then they must be held to a public standard which reflects the importance of that not unsubstantial responsibility.\n\n"}
{"id": "23057360", "url": "https://en.wikipedia.org/wiki?curid=23057360", "title": "The Twenty-four Filial Exemplars", "text": "The Twenty-four Filial Exemplars\n\nThe Twenty-four Filial Exemplars, also translated as The Twenty-four Paragons of Filial Piety (), is a classic text of Confucian filial piety written by Guo Jujing (郭居敬) during the Yuan dynasty (1260–1368). The text was extremely influential in the medieval Far East and was used to teach Confucian moral values.\n\nThe text is generally attributed to Guo Jujing (郭居敬) but other sources suggested two other possible authors or editors: Guo Shouzheng (郭守正) and Guo Juye (郭居業).\n\nSome of the stories in \"The Twenty-four Filial Exemplars\" were taken from other texts such as the \"Xiaozi Zhuan\" (孝子傳), \"Yiwen Leiju\", \"Imperial Readings of the Taiping Era\" and \"In Search of the Supernatural\".\n\nThere were earlier precedents of \"The Twenty-four Filial Exemplars\". A Buddhist \"bianwen\" titled \"Ershisi Xiao Yazuowen\" (二十四孝押座文), which was among the manuscripts discovered in Dunhuang's Mogao Caves, is the oldest extant text related to \"The Twenty-four Filial Exemplars\". During the Southern Song dynasty, the artist Zhao Zigu (趙子固) drew a painting, \"Ershisi Xiao Shuhua Hebi\" (二十四孝書畫合璧), about \"The Twenty-four Filial Exemplars\". During the Yuan dynasty, the scholar Xie Yingfang (謝應芳) mentioned in \"Gui Chao Ji\" (龜巢集) that a certain Wang Dashan (王達善) once praised \"The Twenty-four Filial Exemplars\" and the \"Classic of Filial Piety\". During the Qing dynasty, Wu Zhengxiu (吳正修) mentioned in \"Ershisi Xiao Gu Ci\" (二十四孝鼓詞) that the Twenty-four Filial Exemplars were very well known.\n\nAfter the release of \"The Twenty-four Filial Exemplars\", revised editions of the text and other similar works were published. Some of these include: \"Riji Gushi Daquan Ershisi Xiao\" (日記故事大全二十四孝; \"Complete Diary Stories of the Twenty-four Filial Exemplars\"), \"Nü Ershisi Xiao\" (女二十四孝; \"Female Twenty-four Filial Exemplars\"), and \"Nan Nü Ershisi Xiao\" (男女二十四孝; \"Male and Female Twenty-four Filial Exemplars\").\n\nThe philologist Yang Bojun mentioned the development of \"The Twenty-four Filial Exemplars\" in \"Jingshu Qiantan\" (經書淺談). After the book was compiled by Guo Shouzheng during the Yuan dynasty, a new illustrated edition with drawings by Wang Kexiao (王克孝) was released, and this made the book even more popular. Towards the end of the Qing dynasty, Zhang Zhidong and others edited and expanded the book and released it as \"Bai Xiao Tu Shuo\" (百孝圖說; \"Illustrated Hundred Stories of Filial Piety\").\n\nThe concept of filial piety has played a strong role in Chinese culture since ancient times. There was also a tradition of filial mourning, in which a person had to temporarily put aside whatever he/she was doing when his/her parent(s) died and mourn for three years. There were sayings such as \"When a ruler wants a subject to die, the subject must die; when a father wants a son to die, the son must die\", and \"A loyal subject should be sought from a family with filial sons.\"\n\nHowever, in contemporary times, some stories in \"The Twenty-four Filial Exemplars\" are regarded as negative examples. These stories include the extreme example of Guo Ju deciding to kill his son so that he could free up his son's share of the family's food consumption to feed his mother. The negative examples also include stories in which the protagonist harms himself in the process of fulfilling filial piety, such as Wu Meng allowing mosquitoes to suck his blood in the hope that they would not bother his parents, and Wang Xiang lying naked on ice to thaw the ice so that he could catch fish for his mother.\n\nThere are some stories which are heavily criticised and even deemed contrary to Confucian principles. One example is the story of Cai Shun being rewarded by the Chimei rebels for his filial piety. The story paints the rebels in a positive light when they actually violated the Confucian virtue of loyalty to one's country. Another example is the story of Laolaizi behaving in a childish manner to amuse his parents. The modern writer Lu Xun said that Laolaizi's story is \"an insult to the ancients, and a bad influence on future generations\".\n\n"}
{"id": "4597530", "url": "https://en.wikipedia.org/wiki?curid=4597530", "title": "Torture chamber", "text": "Torture chamber\n\nA torture chamber is a room where torture is inflicted. The medieval torture chamber was windowless and often built underground, was lit by a few candles and was specifically designed to induce \"horror, dread and despair\" to anyone but those possessing a strong mind and \"nerves of steel\".\n\nHistorically, torture chambers were located in royal palaces, in castles of the nobility and even buildings belonging to the church. They featured secret trap-doors which could be activated to throw victims into dark dungeons where they remained and eventually died. The skeletal remains of people who disappeared were strewn on the floor of the hidden dungeons. Other times the dungeons under the trap-doors included pits of water where the victim was thrown to drown after a lengthy torture session in the chamber above.\n\nIn Peru, the torture chambers of the Inquisition were specifically constructed with thick walls so that the screams of the victims could not penetrate them and no sound could be heard from the outside. Other more sophisticated designs used principles of acoustics to muffle the screams of the tortured and included walls which recessed and protruded in such a fashion as to reflect the screams of the victims so that the sounds would not be carried to the exterior.\n\nThe mere presence of the torture chamber was used as a form of intimidation and coercion. The victims were first shown the chamber and if they confessed they would not be tortured inside it. Other times the torture chamber was used as the final destination in a series of prison cells where the victims would gradually be moved from one type of cell to another, under progressively worsening conditions of incarceration, and if they did not recant in the earlier stages they would finally reach the torture chamber. The final stage of actually going to the torture chamber itself, just prior to the initiation of torture, was euphemistically called the \"Question\".\n\nThroughout history torture chambers have been used in a multiplicity of ways starting from Roman times. Torture chamber use during the Middle Ages was frequent. Religious, social and political persecution led to the widespread use of torture during that time. Torture chambers were also used during the Spanish Inquisition and at the Tower of London.\n\nAnother example of a torture chamber, not known by many, is \"The Thieves Tower\" in the Alsace region of France. Once a tower used for torture, it is now a small museum displaying instruments used upon the prisoners to get them to confess crimes.\n\nIn Venice, the Palazzo Ducale had its own torture chamber, which was deemed to be of such importance that renovations started in 1507 so that the chamber walls could be kept strong and secure: \"considerata la grandissima importantia de j cameroti di la Camera del tormento che siano forti e securi\".\n\nAccording to the narrations of Ashokavadana, King Ashoka, prior to his conversion to Buddhism, was a fierce and sadistic ruler, known as \"Ashoka the Fierce\", who built a palatial torture chamber known as \"Ashoka's Hell\". The legend of the torture palace is detailed in the writings of the Ashokavadana.\n\nAccording to Ashokavadana, Ashoka asked Girika, who was the official executioner of his kingdom, to design an elaborate torture chamber disguised as a beautiful and \"enticing\" palace adorned with all kinds of decorations and full of amenities such as exclusive baths decorated with flowers, fruit trees and many ornaments. It was artfully designed to make people long to just look at it.\n\nAccording to legend, beneath the veneer of beauty deep inside the exclusive mansion, torture chambers were constructed which were full of the most sadistic and cruel instruments of torture including furnaces producing molten metal.\n\nAccording to the accounts contained in the Ashokavadana, Girika, the architect of the chamber, was inspired by descriptions of the five tortures of the Buddhist hell for the design of the torture chamber and of the torture methods he inflicted upon his victims. The torture chamber was so terrifying, that King Ashoka himself was thought to have visited hell so that he could perfect its evil design.\n\nAshoka made Girika promise that he would never allow anyone who entered the palace to exit alive, including Ashoka himself. In the \"Biographical Sutra of King Ashoka\" the palace is described by the sentence: 'King Ashoka constructed a hell'.\n\nSometime later a Buddhist monk by the name of Samudra happened to visit the palace and upon entering he was informed by Girika that he would be tortured to death, and was subsequently led into the torture chamber. His torturers however failed to injure him and he appeared able to neutralise their torture methods by performing miracles.\n\nAshoka converted to Buddhism when he witnessed Samudra's miracles inside the torture chamber. He also ordered Girika burnt alive and ordered the demolition of the torture palace. According to the Ashokavadana, \"the beautiful jail was then torn down and a guarantee of security was extended to all beings\".\n\nXuanzang in his writings mentions that in the 7th century AD he had visited the place where Ashoka's Hell once was. In India the palace is known as \"Ashoka's Hell\" and its location near Pataliputra became a popular destination for pilgrims. Faxian also reports visiting it and his account of the story of the palace differs slightly from that of Xuanzang's.\n\nAccording to Frederick Howard Wines in his book \"Punishment and Reformation: A Study Of The Penitentiary System\" there were three main types of coercion employed in the torture chamber: Coercion by the cord, by water and by fire. There were five stages of torture that could have been applied to the accused: he could have been threatened with torture, he could have been taken to the torture chamber and been shown the instruments, he could have been undressed as if in preparation to be tied to the instrument, without actually being tied, he could have been tied to the instrument of torture but not actually getting tortured and finally he could have been tied to the instrument and tortured.\n\nThe process of being tied and led to the torture rack inside the torture chamber was a form of intimidation and was called \"territio realis\" as opposed to \"territio verbalis oder lexis\" which was the verbal threat of torture being made at the judgment hall. \"Territio realis\" as well as the actual torture session were called \"examen rigorosum\".\n\nIn the book \"Crime and criminal justice in Europe and Canada\" it is mentioned that fear was a factor in the process of torture and that there was a form of torture known as \"La présentation de la question\" or simply the \"Question\", where the prisoner was led to the torture chamber and was shown the implements of torture. While at the chamber, sentence to full torture was pronounced but, immediately after, the prisoner was taken back to the prison cell, without actually having been tortured.\n\nThe torture chamber was specifically designed to evoke fear in the victims. It was usually built underground and only dimly lit. Inside the chamber waited the executioner, his face completely covered apart from two holes in the garment to enable him to peer through and wearing a black hood; his menacing appearance being described as \"most diabolical\" and \"satanic\".\n\nWhen during the Question, the view of the chamber, the torture implements and the executioner did not cause the victim to confess, a full-scale torture session was planned. To prepare for torture, the victim was stripped naked with hands tied. The penultimate step to torture included a repetition of the questions asked earlier of the victims. If the victims still proclaimed their innocence, full torture was initiated.\n\nThe most common instrument of torture was the \"strappado\", which was a simple rope and pulley system. With the pulley attached to ceiling of the chamber, the lifting rope was tied to the wrist of the victim, whose hands were tied behind their back. Subsequently, the victim was raised to the ceiling and then lowered using a jerking motion causing dislocation of the shoulder joints. To increase the suffering caused by the strappado, weights were attached to the feet of the victim.\n\nChurch doctrine protected human life so it was problematic if a victim were to die, especially before they confessed. In difficult cases, when a victim would not readily confess or was too weak to continue in an uninterrupted torture session, breaks were allowed between torture sessions because Inquisition regulations only allowed one torture session per victim. That way, a torture session could resume after a break to allow time for the victim to recover or reconsider their opposition to confessing, and it was considered to have been the continuation of the previous torture session and not a new one.\n\nBecause confession under torture was not acceptable, the victim had to sign a written confession after they had made their oral confession under torture. Typically, during confession, the inquisitors demanded that the prisoner implicate as many people as possible and not only themselves. If the prisoner resisted signing, the inquisitors could always resume the torture by claiming that they had just halted the session, just for the signing, but did not really put an end to it.\n\nThe method of construction of the torture chamber of the papal palace at Avignon, used during the Inquisition, has been described as ingenious. The construction of some of the torture chambers at Avignon was based on principles of Acoustics, specifically designed to muffle the screams and cries of the tortured. The walls of the torture chamber recessed and protruded in a complementary fashion to the walls on the opposite side so as to reflect the screams of the victims locally, ensuring that their shrieks would not be carried to the exterior. A chamber located above the main torture chamber had a dungeon with a hole near the middle of the floor through which, according to accounts, the tortured bodies of the prisoners were thrown into a cavity. The chamber where the victims were being burnt was of circular construction and resembled the furnace of a glass-house with a funnel-like chimney at the top.\n\nThere were secret staircases and hidden spaces which were used to overhear the discussions in the prison cells. The ceiling of the torture chamber was especially designed to muffle the cries of the victims. Inside the torture chamber, furnaces and grates were also present. Up to 1850 the chambers were shown to visitors after which time the ecclesiastical authorities of Avignon decided to shut them down. In a similar vein the torture chamber of the Spanish Inquisition in Lima, Peru had one metre thick walls so that the screams of the victims could not penetrate them.\n\nIn Nuremberg and Salzburg the torture chambers featured trapdoors on their floors. In Nuremberg the room underneath the main torture chamber featured torture machinery while in Salzburg, the room under the trapdoor, functioned like a waiting room for prisoners. When the time came the prisoner was pulled up and into the upper torture chamber. Other times, deep water pits could be found under the trapdoor, where the victims of the torture chamber could be thrown, after a torture session, to drown.\n\nThe torture chamber was the final destination in a progression of four cell types during incarceration at the Palace of the Inquisition. The palace contained the Judgement Hall, the offices of the employees, the private apartments of the Grand Inquisitor and the detention cells adjacent to the apartments. The detention cell gradations started with the \"cells of mercy\" reserved mainly for rich transgressors who upon bequeathing all their property to the Inquisition were normally let go after a time of detention in the cells.\n\nFor more difficult prisoners the next cell stage was the \"cell of penitence\". These were situated in small round towers of about 3 metres (ten feet) in diameter. They were painted white and included rudimentary furniture such as a stool and a bed. Very little light was allowed in. If the prisoner did not cooperate, the next step in the detention process was the dungeon. The dungeon had walls 1.5 metres (five feet) thick, double doors and was in complete darkness. No conversation of any type was allowed in the dungeon. The food allowance for prisoners was less than a penny a day including the profit of the warden while any human refuse was removed every four days. After a stay in the dungeon, uncooperative prisoners were moved to their final destination: the torture chamber.\n\nThe Palace of Inquisition was a torture chamber in Cartagena, Colombia, built under orders of Philip III, which served as headquarters for the Spanish Inquisition. It was used to torture Jews, and other non-Catholics. Approximately 800 individuals were put to death there.\n\nThe traditional torture users of modern times have been dictatorship governments e.g. the Nazis, Argentine military junta (at the Navy School of Mechanics), and the Chilean dictatorship led by Augusto Pinochet as well as other South American regimes. The isolation felt inside the Nazi torture chambers was so strong that author, and victim, K. Zetnik, during his testimony at the Eichmann trial in Jerusalem in 1961, has described them as \"another galaxy.\"\n\nIn Chile, during the Pinochet dictatorship, the use of converted locker-rooms and skyboxes as torture chambers has been reported. The \"Esmeralda\", a training ship of the Chilean Navy had also been used as a \"floating torture chamber\" during Pinochet's dictatorship. In 2011, protests erupted in Vancouver, Canada, upon a visit by the \"Esmeralda\". In Santiago, Chile, \"Villa Grimaldi\", once a cultural center, was used as a torture centre which included torture chambers. A tour of \"Villa Grimaldi\" has been described as a \"tour of barbarity\" featuring exhibits with descriptive signs such as \"Place of hangings\", \"Torture chamber\", \"Annex torture chamber\", \"Women's cells 1x1 meter\", \"The Grill: Electric Beds\" and others.\n\nMichelle Bachelet, who later became President of Chile, was tortured in a torture chamber during the Pinochet years. After the fall of Pinochet, the torture chamber victims and the relatives of the \"desaparecidos\" refused to strike a deal with Pinochet or with the politicians that followed him.\n\nUse of torture chambers was also reported in Europe during the Greek military junta years (1967–1974). Alexandros Panagoulis and Army Major Spyros Moustaklis are examples of persons tortured at the EAT/ESA (Greek Military Police) interrogation cell units.\n\nUnder his reign (16 July 1979 until 9 April 2003), Saddam Hussein reportedly tortured those whom he deemed as a threat. After the invasion of Iraq by U.S. forces, pictures of dead Iraqis, with their necks slashed, their eyes gouged out and their genitals blackened, were located in many torture chambers. Jail cells, with dried blood on the floor and rusted shackles bolted to the walls, lined the corridors.\n\nIn November 2004, U.S. Marines found a number of torture rooms in Fallujah by following trails of dried blood, or the smell of dead bodies. Some rooms were hidden behind fake walls, or concealed in basements. \nLibyans have entered abandoned torture chambers and found devices that have been used against opposition members in the past.\n\nSince the 1970s, the Chicago Police Department has tortured prisoners at Homan Square to extract forced confessions. In addition, the CIA tortures prisoners at Black Sites around the world \n\nAside from its dictionary definition the term has great cultural resonance, because it transforms an abstract concept (torture) into a real place (torture chamber), and is an integral part of pop culture. Related exhibits can also be found in torture museums and in exhibits in places such as Las Vegas and Niagara Falls, which attract millions of tourists each year.\n\nIn film the torture chamber is also known as the chamber of horrors, with the word \"horror\" implying \"torture\" as well as \"murder\", or a combination of both. Classic films focusing on the torture chamber include:\n\n\n\n\n\n\n"}
{"id": "1393619", "url": "https://en.wikipedia.org/wiki?curid=1393619", "title": "Two truths doctrine", "text": "Two truths doctrine\n\nThe Buddhist doctrine of the two truths () differentiates between two levels of \"satya\" (Sanskrit), meaning truth or \"really existing\" in the discourse of the Buddha: the \"conventional\" or \"provisional\" () truth, and the \"ultimate\" () truth.\n\nThe exact meaning varies between the various Buddhist schools and traditions. The best known interpretation is from the Madhyamaka school of Mahāyāna Buddhism, whose founder was Nagarjuna. For Nagarjuna, the two truths are \"epistemological truths\". The phenomenal world is accorded a provisional existence. The character of the phenomenal world is declared to be neither real nor unreal, but logically indeterminable. Ultimately, phenomena are empty (\"sunyata\") of an inherent self or essence, but exist depending on other phenomena (\"Pratītyasamutpāda\").\n\nIn Chinese Buddhism, the Madhyamaka position is accepted and the two truths refer to two \"ontological truths\". Reality exists of two levels, a relative level and an absolute level. Based on their understanding of the Mahayana Mahaparinirvana Sutra, the Chinese supposed that the teaching of the Buddha-nature was, as stated by that sutra, the final Buddhist teaching, and that there is an essential truth above sunyata and the two truths.\n\nThe doctrine is an attempt to show that it is neither proper nor strictly justifiable to regard any metaphysical system as absolutely valid. It doesn't lead to nihilism but strikes a middle course between excessive naivete and excessive scepticism.\n\n\"Satya\" is usually taken to mean \"truth\", but does also refer to mean \"a reality,\" \"a genuinely real existent.\" \"Satya\" (Sat-yá) is derived from \"Sat\" and \"ya\". \"Sat\" means being, reality, and is the present participle of the root \"as\" \"to be\" (PIE \"\"; cognate to English \"is\"). \"Ya\" and \"yam\" means \"advancing, supporting, hold up, sustain, one that moves\". As a composite word, \"Satya\" and \"Satyam\" imply that \"which supports, sustains and advances reality, being\"; it literally means, \"that which is true, actual, real, genuine, trustworthy, valid\".\n\nThe two truths doctrine states that there is:\n\nChandrakīrti suggests three possible meanings of :\n\nThe conventional truth may be interpreted as \"obscurative truth\" or \"that which obscures the true nature\" as a result. It is constituted by the appearances of mistaken awareness. Conventional truth would be the appearance that includes a duality of apprehender and apprehended, and objects perceived within that. Ultimate truths, are phenomena free from the duality of apprehender and apprehended.\n\nBuddha's teaching of () may be viewed as a path () of release from the anxieties of life. The first Noble Truth equates life-experiences with pain and suffering. Buddha's language was simple and colloquial. Naturally, various statements of Buddha at times appear contradictory to each other. Later Buddhist teachers were faced with the problem of resolving these contradictions. Nagarjuna and other teachers introduced an exegetical technique level-distinction between two levels of truth, the conventional and the ultimate.\n\nA similar method is reflected in the Brahmanical exegesis of the Vedic scriptuires which combine the ritualistic injunctions of the Brahmana and speculative philosophical questions of the Upanishads as one whole 'revealed' body of work thereby contrasting the with .\n\nWhile the concept of the two truths is associated with the Madhyamaka school, its history goes back to the oldest Buddhism.\n\nIn the Pali canon, the distinction is not made between a lower \"truth\" and a higher \"truth\", but rather between two kinds of expressions of the same truth, which must be interpreted differently. Thus a phrase or passage, or a whole sutta, might be classed as \"neyyattha\" or \"samuti\" or \"vohāra\", but it is not regarded at this stage as expressing or conveying a different level of \"truth\".\n\n\"Nītattha\" (Pāli; Sanskrit: \"nītārtha\"), \"of plain or clear meaning\" and \"neyyattha\" (Pāli; Sanskrit: \"neyartha\"), \"[a word or sentence] having a sense that can only be guessed\". These terms were used to identify texts or statements that either did or did not require additional interpretation. A \"nītattha\" text required no explanation, while a \"neyyattha\" one might mislead some people unless properly explained:\n' or ' (Pāli; Sanskrit: \"\", meaning \"common consent, general opinion, convention\", and \"paramattha\" (Pāli; Sanskrit: \"paramārtha\"), meaning \"ultimate\", are used to distinguish conventional or common-sense language, as used in metaphors or for the sake of convenience, from language used to express higher truths directly. The term \"vohāra\" (Pāli; Sanskrit: \"vyavahāra\", \"common practice, convention, custom\" is also used in more or less the same sense as \"samuti\".\n\nThe Theravādin commentators expanded on these categories and began applying them not only to expressions but to the truth then expressed:\nThe Prajñaptivāda school took up the distinction between the conventional and ultimate (\"paramārtha/\"), and extended the concept to metaphysical-phenomenological constituents (\"dharmas\"), distinguishing those that are real (\"tattva\") from those that are purely conceptual, i.e., ultimately nonexistent (\"prajnāpti\").\n\nThe distinction between the two truths (\"satyadvayavibhāga\") was fully developed by Nāgārjuna (c. 150 – c. 250 CE) of the Madhyamaka school. The Madhyamikas distinguish between \"loka-samvriti-satya\", \"world speech truth\" c.q. \"relative truth\" c.q. \"truth that keeps the ultimate truth concealed,\" and \"paramarthika satya\", ultimate truth.\n\n\"Loka-samvriti-satya\" can be further divided in \"tathya-samvrti\" or \"loka-samvrti\", and \"mithya-samvrti\" or \"aloka-samvrti\", \"true samvrti\" and \"false samvrti.\" \"Tathya-samvrti\" or \"true samvrti\" refers to \"things\" which concretely exist and can be perceived as such by the senses, while \"mithya-samvrti\" or \"false samvrti\" refers to false cognitions of \"things\" which do not exist as they are perceived.\n\nNagarjuna's \"Mūlamadhyamakakārikā\" provides a logical defense for the claim that all things are empty (sunyata) of an inherently-existing self-nature. Sunyata, however, is also shown to be \"empty,\" and Nagarjuna's assertion of \"the emptiness of emptiness\" prevents sunyata from constituting a higher or ultimate reality. Nagarjuna's view is that \"the ultimate truth is that there is no ultimate truth\". According to Siderits, Nagarjuna is a \"semantic anti-dualist\" who posits that there are only conventional truths. Jay L. Garfield explains:\nIn Nāgārjuna's \"Mūlamadhyamakakārikā\" the two truths doctrine is used to defend the identification of dependent origination (\"pratītyasamutpāda\") with emptiness (\"śūnyatā\"):\nIn Nagarjuna's own words:\nNāgārjuna based his statement of the two truths on the Kaccāyanagotta Sutta. In the Kaccāyanagotta Sutta, the Buddha, speaking to the monk Kaccayana Gotta on the topic of right view, describes the middle Way between nihilsm and eternalism:\nAccording to Chattopadhyaya, although Nagarjuna presents his understanding of the two truths as a clarification of the teachings of the Buddha, the two truths doctrine as such is not part of the earliest Buddhist tradition.\n\nThe Yogacara school of Buddhism distinguishes the Three Natures and the Trikaya. The Three Natures are:\n\nThe Lankavatara Sutra took an idealistic turn in apprehending reality. D. T. Suzuki writes the following:\nWhen Buddhism came to China from Gandhara (now Afghanistan) and India in the first/second century CE, it was initially adapted to the Chinese culture and understanding. Buddhism was exposed to Confucianist and Taoist influences. Neo-Taoist concepts were taken over in Chinese Buddhism. Concepts such as \"T’i -yung\" (Essence and Function) and \"Li-Shih\" (Noumenon and Phenomenon) were first taken over by Hua-yen Buddhism, which consequently influenced Chán deeply.\n\nThe two truths doctrine was another point of confusion. Chinese thinking took this to refer to two \"ontological truths\": reality exists of two levels, a relative level and an absolute level. Taoists at first misunderstood sunyata to be akin to the Taoist non-being. In Madhyamaka the two truths are two \"epistemological truths\": two different ways to look at reality. Based on their understanding of the Mahayana Mahaparinirvana Sutra the Chinese supposed that the teaching of the Buddha-nature was, as stated by that sutra, the final Buddhist teaching, and that there is an essential truth above sunyata and the two truths.\n\nThe Huayan school or Flower Garland is a tradition of Mahayana Buddhist philosophy that flourished in China during the Tang period. It is based on the Sanskrit Flower Garland Sutra (S. \"Avataṃsaka Sūtra\", C. \"Huayan Jing\") and on a lengthy Chinese interpretation of it, the Huayan Lun. The name Flower Garland is meant to suggest the crowning glory of profound understanding.\n\nThe most important philosophical contributions of the Huayan school were in the area of its metaphysics. It taught the doctrine of the mutual containment and interpenetration of all phenomena, as expressed in Indra's net. One thing contains all other existing things, and all existing things contain that one thing.\n\nDistinctive features of this approach to Buddhist philosophy include:\n\nHuayan teaches the Four Dharmadhatu, four ways to view reality:\n\nThe teachings of Zen are expressed by a set of polarities: Buddha-nature - sunyata, absolute-relative, sudden and gradual enlightenment.\n\nThe Prajnaparamita Sutras and Madhyamaka emphasized the non-duality of form and emptiness: form is emptiness, emptiness is form, as the Heart Sutra says.\nThe idea that the ultimate reality is present in the daily world of relative reality fitted into the Chinese culture which emphasized the mundane world and society. But this does not tell how the absolute is present in the relative world. This question is answered in such schemata as the Five Ranks of Tozan and the Oxherding Pictures.\n\nThe polarity of absolute and relative is also expressed as \"essence-function\". The absolute is essence, the relative is function. They can't be seen as separate realities, but interpenetrate each other. The distinction does not \"exclude any other frameworks such as \"neng-so\" or \"subject-object\" constructions\", though the two \"are completely different from each other in terms of their way of thinking\".\n\nIn Korean Buddhism, essence-function is also expressed as \"body\" and \"the body's functions\":\nA metaphor for essence-function is \"A lamp and its light\", a phrase from the \"Platform Sutra\", where Essence is lamp and Function is light.\n\nThe Nyingma tradition is the oldest of the four major schools of Tibetan Buddhism. It is founded on the first translations of Buddhist scriptures from Sanskrit into Tibetan, in the eighth century. Ju Mipham (1846–1912) in his commentary to the \"Madhyamālaṃkāra\" of Śāntarakṣita (725–788) says:\nThe following sentence from Mipham's exegesis of Śāntarakṣita's\" Madhyamālaṃkāra\" highlights the relationship between the absence of \"the four extremes\" (\"mtha'-bzhi\") and the nondual or \"indivisible two truths\" (\"bden-pa dbyer-med\"):\nDzogchen holds that the two truths are ultimately resolved into non-duality as a lived experience and are non-different.\n\nThe Jain philosopher Kundakunda was influenced by Nagarjuna to develop a Jain version of the theory of two truths. In texts such as Pravacanasāra\n(‘The Essence of the Doctrine’) and Samayasāra (‘The Essence of the Soul’), Kundakunda distinguishes between two perspectives of truth:\n\n\nFor Kundakunda, the mundane realm of truth is also the relative perspective of normal folk, where the workings of karma operate and where things emerge, last for a certain duration and perish. The ultimate perspective meanwhile, is that of the liberated jiva, which is \"blissful, energetic, perceptive, and omniscient\".\n\nAdvaita took over from the Madhyamika the idea of levels of reality. Usually two levels are being mentioned, but Shankara uses sublation as the criterion to postulate an ontological hierarchy of three levels.\n\n\nChattopadhyaya notes that the eighth-century Mīmāṃsā philosopher Kumārila Bhaṭṭa rejected the \"Two Truths Doctrine\" in his \"Shlokavartika\". Bhaṭṭa was highly influential with his defence of the Vedic rituals against medieval Buddhist rejections of these rituals. Some believe that his influence contributed to the decline of Buddhism in India since his lifetime coincides with the period in which Buddhism began to decline. According to Kumarila, the two truths doctrine is an idealist doctrine, which conceals the fact that \"the theory of the nothingness of the objective world\" is absurd:\nMcEvilley (2002) notes a correspondence between Greek Pyrrhonism and Madhyamika doctrines:\n\n"}
{"id": "279693", "url": "https://en.wikipedia.org/wiki?curid=279693", "title": "Type signature", "text": "Type signature\n\nIn computer science, a type signature or type annotation defines the inputs and outputs for a function, subroutine or method. A type signature includes the number of arguments, the types of arguments and the order of the arguments contained by a function. A type signature is typically used during overload resolution for choosing the correct definition of a function to be called among many overloaded forms.\n\nIn C and C++, the type signature is declared by what is commonly known as a function prototype. In C/C++, a function declaration reflects its use; for example, a function pointer that would be invoked as:\nhas the signature:\n\nIn Erlang, type signatures may be optionally declared, as:\nFor example:\nA type signature in the Haskell programming language is generally written in the following format:\nNotice that the type of the result can be regarded as everything past the first supplied argument. This is a consequence of currying, which is made possible by Haskell's support for first-class functions; this function requires two inputs where one argument supplied and the function is \"curried\" to produce a function for the argument not supplied. Thus calling , where , yields a new function that can be called to produce .\n\nThe actual type specifications can consist of an actual type, such as , or a general type variable that is used in parametric polymorphic functions, such as , or , or . So we can write something like:\nSince Haskell supports higher-order functions, functions can be passed as arguments. This is written as:\nThis function takes in a function with type signature and returns data of type out.\n\nIn the Java virtual machine, \"internal type signatures\" are used to identify methods and classes at the level of the virtual machine code.\n\nExample: The method is represented in bytecode as . The signature of codice_1 method looks like this:\n\nAnd in the disassembled bytecode, it takes the form of \n\nThe method signature for the codice_1 method contains three modifiers:\n\n\nA function signature consists of the function prototype. It specifies the general information about a function like the name, scope and parameters. Many programming languages use name mangling in order to pass along more semantic information from the compilers to the linkers. In addition to mangling, there is an excess of information in a function signature (stored internally to most compilers) which is not readily available, but may be accessed.\n\nUnderstanding the notion of a function signature is an important concept for all computer science studies.\n\nThe practice of multiple inheritance requires consideration of the function signatures to avoid unpredictable results.\n\nComputer science theory, and the concept of polymorphism in particular, make much use of the concept of function signature.\n\nIn the C programming language signature is roughly equivalent to its prototype definition.\n\nThe term \"signature\" may carry other meanings in computer science. For example:\n\nIn computer programming, especially object-oriented programming, a method is commonly identified by its unique method signature, which usually includes the method name, and the number, types and order of its parameters. A method signature is the smallest type of a method.\n\nIn C/C++, the method signature is the method name and the number and type of its parameters, but it is possible to have a last parameter that consists of an array of values:\nManipulation of these parameters can be done by using the routines in the standard library header .\n\nSimilar to the C syntax, C# sees as the method signature its name and the number and type of its parameters, where the last parameter may be an array of values:\n\nIn the Java programming language, a method signature is the method name and the number, type and order of its parameters. Return types and thrown exceptions are not considered to be a part of the method signature. \nFor example, the following two methods have distinct signatures:\nThe following three methods do have the same signatures and are considered the same, as only the return value differs. The name of the parameter is not part of the method signature and is ignored by the compiler for checking method uniqueness.\nIn the Objective-C programming language, method signatures for an object are declared in the interface header file. For example,\ndefines a method that returns a general object (an ) and takes one integer argument. Objective-C only requires a type in a signature to be explicit when the type is not ; this signature is equivalent:\n"}
{"id": "2614714", "url": "https://en.wikipedia.org/wiki?curid=2614714", "title": "Vertigo", "text": "Vertigo\n\nVertigo is a symptom where a person feels as if they or the objects around them are moving when they are not. Often it feels like a spinning or swaying movement. This may be associated with nausea, vomiting, sweating, or difficulties walking. It is typically worse when the head is moved. Vertigo is the most common type of dizziness.\nThe most common diseases that result in vertigo are benign paroxysmal positional vertigo (BPPV), Ménière's disease, and labyrinthitis. Less common causes include stroke, brain tumors, brain injury, multiple sclerosis, migraines, trauma, and uneven pressures between the middle ears. Physiologic vertigo may occur following being exposed to motion for a prolonged period such as when on a ship or simply following spinning with the eyes closed. Other causes may include toxin exposures such as to carbon monoxide, alcohol, or aspirin. Vertigo typically indicates a problem in a part of the vestibular system. Other causes of dizziness include presyncope, disequilibrium, and non-specific dizziness.\nBenign paroxysmal positional vertigo is more likely in someone who gets repeated episodes of vertigo with movement and is otherwise normal between these episodes. The episodes of vertigo should last less than one minute. The Dix-Hallpike test typically produces a period of rapid eye movements known as nystagmus in this condition. In Ménière's disease there is often ringing in the ears, hearing loss, and the attacks of vertigo last more than twenty minutes. In labyrinthitis the onset of vertigo is sudden and the nystagmus occurs without movement. In this condition vertigo can last for days. More severe causes should also be considered. This is especially true if other problems such as weakness, headache, double vision, or numbness occur.\nDizziness affects approximately 20–40% of people at some point in time, while about 7.5–10% have vertigo. About 5% have vertigo in a given year. It becomes more common with age and affects women two to three times more often than men. Vertigo accounts for about 2–3% of emergency department visits in the developed world.\n\nVertigo is classified into either peripheral or central depending on the location of the dysfunction of the vestibular pathway, although it can also be caused by psychological factors.\n\nVertigo can also be classified into objective, subjective, and pseudovertigo. Objective vertigo describes when the person has the sensation that stationary objects in the environment are moving. Subjective vertigo refers to when the person feels as if they are moving. The third type is known as pseudovertigo, an intensive sensation of rotation inside the person's head. While this classification appears in textbooks, it has little to do with the pathophysiology or treatment of vertigo.\n\nVertigo that is caused by problems with the inner ear or vestibular system, which is composed of the semicircular canals, the vestibule (utricle and saccule), and the vestibular nerve is called \"peripheral\", \"otologic\" or \"vestibular\" vertigo. The most common cause is benign paroxysmal positional vertigo (BPPV), which accounts for 32% of all peripheral vertigo. Other causes include Ménière's disease (12%), superior canal dehiscence syndrome, labyrinthitis, and visual vertigo. Any cause of inflammation such as common cold, influenza, and bacterial infections may cause transient vertigo if it involves the inner ear, as may chemical insults (e.g., aminoglycosides) or physical trauma (e.g., skull fractures). Motion sickness is sometimes classified as a cause of peripheral vertigo.\n\nPeople with peripheral vertigo typically present with mild to moderate imbalance, nausea, vomiting, hearing loss, tinnitus, fullness, and pain in the ear. In addition, lesions of the internal auditory canal may be associated with facial weakness on the same side. Due to a rapid compensation process, acute vertigo as a result of a peripheral lesion tends to improve in a short period of time (days to weeks).\n\nVertigo that arises from injury to the balance centers of the central nervous system (CNS), often from a lesion in the brainstem or cerebellum, is called \"central\" vertigo and is generally associated with less prominent movement illusion and nausea than vertigo of peripheral origin. Central vertigo may have accompanying neurologic deficits (such as slurred speech and double vision), and pathologic nystagmus (which is pure vertical/torsional). Central pathology can cause disequilibrium which is the sensation of being off balance. The balance disorder associated with central lesions causing vertigo is often so severe that many patients are unable to stand or walk.\n\nA number of conditions that involve the central nervous system may lead to vertigo including: lesions caused by infarctions or hemorrhage, tumors present in the cerebellopontine angle such as a vestibular schwannoma or cerebellar tumors, epilepsy, cervical spine disorders such as cervical spondylosis, degenerative ataxia disorders, migraine headaches, lateral medullary syndrome, Chiari malformation, multiple sclerosis, parkinsonism, as well as cerebral dysfunction. Central vertigo may not improve or may do so more slowly than vertigo caused by disturbance to peripheral structures.\n\nVertigo is a sensation of spinning while stationary. It is commonly associated with nausea or vomiting, unsteadiness (postural instability), falls, changes to a person's thoughts, and difficulties in walking. Recurrent episodes in those with vertigo are common and frequently impair the quality of life. Blurred vision, difficulty in speaking, a lowered level of consciousness, and hearing loss may also occur. The signs and symptoms of vertigo can present as a persistent (insidious) onset or an episodic (sudden) onset.\n\nPersistent onset vertigo is characterized by symptoms lasting for longer than one day and is caused by degenerative changes that affect balance as people age. Naturally, the nerve conduction slows with aging and a decreased vibratory sensation is common.\nAdditionally, there is a degeneration of the ampulla and otolith organs with an increase in age. Persistent onset is commonly paired with central vertigo signs and symptoms.\n\nThe characteristics of an episodic onset vertigo are indicated by symptoms lasting for a smaller, more memorable amount of time, typically lasting for only seconds to minutes. Typically, episodic vertigo is correlated with peripheral symptoms.\n\nThe neurochemistry of vertigo includes six primary neurotransmitters that have been identified between the three-neuron arc that drives the vestibulo-ocular reflex (VOR). Glutamate maintains the resting discharge of the central vestibular neurons and may modulate synaptic transmission in all three neurons of the VOR arc. Acetylcholine appears to function as an excitatory neurotransmitter in both the peripheral and central synapses. Gamma-Aminobutyric acid (GABA) is thought to be inhibitory for the commissures of the medial vestibular nucleus, the connections between the cerebellar Purkinje cells, and the lateral vestibular nucleus, and the vertical VOR.\n\nThree other neurotransmitters work centrally. Dopamine may accelerate vestibular compensation. Norepinephrine modulates the intensity of central reactions to vestibular stimulation and facilitates compensation. Histamine is present only centrally, but its role is unclear. Dopamine, histamine, serotonin, and acetylcholine are neurotransmitters thought to produce vomiting. It is known that centrally acting antihistamines modulate the symptoms of acute symptomatic vertigo.\n\nTests for vertigo often attempt to elicit nystagmus and to differentiate vertigo from other causes of dizziness such as presyncope, hyperventilation syndrome, disequilibrium, or psychiatric causes of lightheadedness. Tests of vestibular system (balance) function include electronystagmography (ENG), Dix-Hallpike maneuver, rotation tests, head-thrust test, caloric reflex test, and computerized dynamic posturography (CDP).\n\nThe HINTS test, which is a combination of three physical exam tests that may be performed by physicians at the bedside has been deemed helpful in differentiating between central and peripheral causes of vertigo. The HINTS test involves the horizontal head impulse test, observation of nystagmus on primary gaze, and the test of skew. CT scans or MRIs are sometimes used by physicians when diagnosing vertigo.\n\nTests of auditory system (hearing) function include pure tone audiometry, speech audiometry, acoustic reflex, electrocochleography (ECoG), otoacoustic emissions (OAE), and the auditory brainstem response test.\n\nA number of specific conditions can cause vertigo. In the elderly, however, the condition is often multifactorial.\n\nA recent history of underwater diving can indicate a possibility of barotrauma or decompression sickness involvement but does not exclude all other possibilities. The dive profile (which is frequently recorded by dive computer) can be useful to assess a probability for decompression sickness, which can be confirmed by therapeutic recompression.\n\nBenign paroxysmal positional vertigo (BPPV) is the most common vestibular disorder and occurs when loose calcium carbonate debris has broken off of the otoconial membrane and enters a semicircular canal thereby creating the sensation of motion. Patients with BPPV may experience brief periods of vertigo, usually under a minute, which occur with change in the position.\n\nThis is the most common cause of vertigo. It occurs in 0.6% of the population yearly with 10% having an attack during their lifetime. It is believed to be due to a mechanical malfunction of the inner ear. BPPV may be diagnosed with the Dix-Hallpike test and can be effectively treated with repositioning movements such as the Epley maneuver.\n\nMénière's disease is an inner ear disorder of unknown origin, but is thought to be caused by an increase in the amount of endolymphatic fluid present in the inner ear (endolymphatic hydrops). However, this idea has not been directly confirmed with histopathologic studies but electrophysiologic studies have been suggestive of this mechanism. Ménière's disease frequently presents with recurrent, spontaneous attacks of severe vertigo in combination with ringing in the ears (tinnitus), a feeling of pressure or fullness in the ear (aural fullness), severe nausea or vomiting, imbalance, and hearing loss. As the disease worsens, hearing loss will progress.\n\nLabyrinthitis presents with severe vertigo with associated nausea, vomiting, and generalized imbalance and is believed to be caused by a viral infection of the inner ear though several theories have been put forward and the cause remains uncertain. Individuals with vestibular neuritis do not typically have auditory symptoms but may experience a sensation of aural fullness or tinnitus. Persisting balance problems may remain in 30% of people affected.\n\nVestibular migraine is the association of vertigo and migraines and is one of the most common causes of recurrent, spontaneous episodes of vertigo. The cause of vestibular migraines is currently unclear; however, one hypothesized cause is that the stimulation of the trigeminal nerve leads to nystagmus in individuals suffering from migraines. \n\nOther suggested causes of vestibular migraines include the following: unilateral neuronal instability of the vestibular nerve, idiopathic asymmetric activation of the vestibular nuclei in the brainstem, and vasospasm of the blood vessels supplying the labyrinth or central vestibular pathways resulting in ischemia to these structures. Vestibular migraines are estimated to affect 1-3% of the general population and may affect 10% of migraine patients. Additionally, vestibular migraines tend to occur more often in women and rarely affect individuals after the sixth decade of life. \n\nMotion sickness is common and is related to vestibular migraine. It is nausea and vomiting in response to motion and is typically worse if the journey is on a winding road or involves lots of stops and starts, or if the person is reading in a moving car. It is caused by a mismatch between visual input and vestibular sensation. For example, the person is reading a book which is stationary in relation to the body but the vestibular system senses that the car, and thus the body, is moving.\n\nAlternobaric vertigo is caused by a pressure difference between the middle ear cavities, usually due to blockage or partial blockage of one eustachian tube, usually when flying or diving underwater. It is most pronounced when the diver is in the vertical position; the spinning is towards the ear with the higher pressure and tends to develop when the pressures differ by 60 cm of water or more.\n\nVertigo is recorded as a symptom of decompression sickness in 5.3% of cases by the US Navy as reported by Powell, 2008 It including isobaric decompression sickness.\n\nDecompression sickness can also be caused at a constant ambient pressure when switching between gas mixtures containing different proportions of inert gas. This is known as isobaric counterdiffusion, and presents a problem for very deep dives. For example, after using a very helium-rich trimix at the deepest part of the dive, a diver will switch to mixtures containing progressively less helium and more oxygen and nitrogen during the ascent. Nitrogen diffuses into tissues 2.65 times slower than helium, but is about 4.5 times more soluble. Switching between gas mixtures that have very different fractions of nitrogen and helium can result in \"fast\" tissues (those tissues that have a good blood supply) actually increasing their total inert gas loading. This is often found to provoke inner ear decompression sickness, as the ear seems particularly sensitive to this effect.\n\nA stroke (either ischemic or hemorrhagic) involving the posterior fossa is a cause of central vertigo. Risk factors for a stroke as a cause of vertigo include increasing age and known vascular risk factors. Presentation may more often involve headache or neck pain, additionally, those who have had multiple episodes of dizziness in the months leading up to presentation are suggestive of stroke with prodromal TIAs. The HINTS exam as well as imaging studies of the brain (CT, CT angiogram, and/or MRI) are helpful in diagnosis of posterior fossa stroke.\n\nDefinitive treatment depends on the underlying cause of vertigo. Ménière's disease patients have a variety of treatment options to consider when receiving treatment for vertigo and tinnitus including: a low-salt diet and intratympanic injections of the antibiotic gentamicin or surgical measures such as a shunt or ablation of the labyrinth in refractory cases.\nCommon drug treatment options for vertigo may include the following:\n\nAll cases of decompression sickness should be treated initially with 100% oxygen until hyperbaric oxygen therapy (100% oxygen delivered in a high-pressure chamber) can be provided. Several treatments may be necessary, and treatment will generally be repeated until either all symptoms resolve, or no further improvement is apparent.\n\nVertigo is from the Latin word \"vertō\" which means \"a whirling or spinning movement\".\n\n"}
{"id": "2363287", "url": "https://en.wikipedia.org/wiki?curid=2363287", "title": "Visual learning", "text": "Visual learning\n\nVisual learning is a style in which a learner utilizes graphs, charts, maps and diagrams. It is one of the three basic types of learning styles in the Fleming VAK/VARK model that also includes kinesthetic learning and auditory learning.\n\nA review study concluded that using graphic organizers improves student performance in the following areas:\n\n\n\n\n\nVarious areas of the brain work together in a multitude of ways in order to produce the images that we see with our eyes and that are encoded by our brains. The basis of this work takes place in the visual cortex of the brain. The visual cortex is located in the occipital lobe of the brain and harbors many other structures that aid in visual recognition, categorization, and learning. One of the first things the brain must do when acquiring new visual information is recognize the incoming material. Brain areas involved in recognition are the inferior temporal cortex, the superior parietal cortex, and the cerebellum. During tasks of recognition, there is increased activation in the left inferior temporal cortex and decreased activation in the right superior parietal cortex. Recognition is aided by neural plasticity, or the brain's ability to reshape itself based on new information. Next the brain must categorize the material. The three main areas that are used when categorizing new visual information are the orbitofrontal cortex and two dorsolateral prefrontal regions which begin the process of sorting new information into groups and further assimilating that information into things that you might already know. After recognizing and categorizing new material entered into the visual field, the brain is ready to begin the encoding process – the process which leads to learning. Multiple brain areas are involved in this process such as the frontal lobe, the right extrastriate cortex, the neocortex, and again, the neostriatum. One area in particular, the limbic-diencephalic region, is essential for transforming perceptions into memories. With the coming together of tasks of recognition, categorization and learning; schemas help make the process of encoding new information and relating it to things you already know much easier. One can remember visual images much better when they can apply it to an already known schema. Schemas actually provide enhancement of visual memory and learning.\n\nBetween the fetal stage and 18 months, a baby experiences rapid growth of a substance called gray matter. Gray matter is the darker tissue of the brain and spinal cord, consisting mainly of nerve cell bodies and branching dendrites. It is responsible for processing sensory information in the brain such as areas like the primary visual cortex. The primary visual cortex is located within the occipital lobe in the back of infant's brain and is responsible for processing visual information such as static or moving objects and pattern recognition.\n\nWithin the primary visual cortex, there are four pathways: the superior colliculus pathway (SC pathway), the middle temporal area pathway (MT pathway), the frontal eye fields pathway (FEF pathway), and the inhibitory pathway. Each pathway is crucial to the development of visual attention in the first few months of life. The SC pathway is responsible for the generation of eye movements toward simple stimuli. It receives information from the retina and the visual cortex and can direct behavior toward an object. The MT pathway is involved in the smooth tracking of objects and travels between the SC pathway and the primary visual cortex. In conjunction with the SC pathway and the MT pathway, the FEF pathway allows the infant to control eye movements as well as visual attention. It also plays a part in sensory processing in the infant. Lastly, the inhibitory pathway regulates the activity in the superior colliculus and, later, is responsible for obligatory attention in the infant. The maturation and functionality of these pathways depends on how well the infant can make distinctions as well as focus on stimuli.\n\nA study by Haith, Hazan, & Goodman in 1988 showed that babies, as young as 3.5 months, are able to create short-term expectations of situations they confront. Expectations in this study refer to the cognitive and perceptual ways in which an infant can forecast a future event. This was tested by showing the infant either a predictable pattern of slides or an irregular pattern of slides and tracking the infant's eye movements. \nA later study by Johnson, Posner, & Rothbart in 1991 showed that by 4 months old, infants can develop expectations, but was tested through anticipatory looks and disengagement with stimuli. For example, anticipatory looks exhibit the infant is able to predict the next part of a pattern which can then be applied to the real world scenario of breast-feeding. Infants are able to predict a mother's movements and expect feeding so they can latch onto the nipple for feeding. Expectations, anticipatory looks, and disengagement all show that infants can learn visually, even if it is only short term. David Roberts (2016) tested multimedia learning propositions, he found that using certain images dislocates pedagogically harmful excesses of text, reducing cognitive overloading and exploiting under-used visual processing capacities \n\nFrom the ages 3–8 visual learning improves and begins to take many different forms. At the toddler age of 3–5, children's bodily actions structure the visual learning environment. At this age, toddlers are using their newly developed sensory-motor skills quite often and fusing them with their improved vision to understand the world around them. This is seen by toddler's using their arms to bring objects of interest close to their sensors, such as their eyes and face, to explore the object further. The act of bringing objects close to their face affects their immediate view by placing their mental and visual attention on that object and just blocking the view of other objects that are around them and out of view. There is an emphasis placed on objects and things that are directly in front of them and thus proximal vision is the primary perspective of visual learning. This is different from how adults utilize visual learning. This difference in toddler vision and adult vision is attributable to their body sizes, and body movements such that their visual experiences are created by their body movement. An adults view is broad, due to their larger body size, with most objects in view because of the distance between them and objects. Adults tend to scan a room, and see everything rather than focusing on one object only.\n\nThe way a child integrates visual learning with motor experiences enhances their perceptual and cognitive development. For elementary school children, aged 4–11, intellect is positively related to their level of auditory-visual integrative proficiency. The most significant period for the development of auditory-visual integration occurs between ages 5–7. During this time, the child has mastered visual-kinesthetic integration, and the child's visual learning can be applied to formal learning focused towards books and reading, rather than physical objects, thus impacting their intellect. As reading scores increase, children are able to learn more, and their visual learning has developed to not only focus on physical objects in close proximity to them, but also to interpret words and such to acquire knowledge by reading.\n\nHere we categorize middle childhood as ages 9 to 14. By this stage in a child's normal development vision is sharp and learning processes are well underway. Most studies that have focused their efforts on visual learning have found that visual learning styles as opposed to traditional learning styles greatly improve the totality of a student's learning experience. First off, visual learning engages students and student engagement is one of the most important factors that motivated students to learn. Visuals increase student interest with the use of graphics animation, and video. Consequently, it has been found that student pay greater attention to lecture material when visuals are used. With increased attention to lesson material, many positive outcomes have been seen with the use of visual tactics in the classrooms of middle aged students. Students organize and process information more thoroughly when they learn visually which helps them to understand the information better. Students are more likely to remember information that is learned with a visual aid. When teachers used visual tactics to teach middle aged students they found that students had more positive attitudes about the material they were learning. Students also exemplified higher test performance, higher standard achievement scores, thinking on levels that require higher order thinking, and more engagement. One study also found that learning about emotional events, such as the Holocaust, with visual aids increase middle aged children's empathy.\n\nGray matter is responsible for generating nerve impulses that process brain information, and white matter is responsible for transmitting that brain information between lobes and out through the spinal cord. Nerve impulses are transmitted by myelin, a fatty material that grows around a cell. White matter has a myelin sheath (a collection of myelin) while gray matter doesn't, which efficiently allows neural impulses to move swiftly along the fiber. The myelin sheath isn't fully formed until around ages 24–26. This means that adolescents and young adults typically learn differently, and subsequently often utilize visual aids in order to help them better comprehend difficult subjects.\n\nLearning preferences can vary across a wide spectrum. Specifically within the realm of visual learning, they can vary between people who prefer being given learning instructions with text as opposed to those who prefer being given instructions with graphics. College students were tested in general factors like learning preference and spatial ability (being able to be proficient in creating, holding, and manipulating spatial representations). The study determined that college-age individuals report efficient learning styles and learning preferences for themselves individually. These personal assessments have proved accurate, meaning that self-ratings of factors such as spatial ability and learning preference can be effective measures of how well one learns visually.\n\nStudies have indicated that adolescents learn best through 10 various styles; reading, manipulative activity, teacher explanation, auditory stimulation, visual demonstration, visual stimulation (electronic), visual stimulation (just pictures), games, social interaction, and personal experience. According to the study, young adult males demonstrate a preference for learning through activities they are able to manipulate, and young adult females show a greater preference for learning through teacher explanation or direction, and through reading. This suggests that men are more visually stimulated, interested in information that they can have physical direct control over. Women, on the other hand, learn best through reading information and having it explained in an auditory fashion.\n\nAlthough learning styles have \"enormous popularity\", and both children and adults express personal preferences, there is no evidence that identifying a student's learning style produces better outcomes, and there is significant evidence that the widely touted \"meshing hypothesis\" (that a student will learn best if taught in a method deemed appropriate for that student's learning style) is invalid. Well-designed studies \"flatly contradict the popular meshing hypothesis\". Rather than targeting instruction to the \"right\" learning style, students appear to benefit most from mixed modality presentations, for instance using both auditory and visual techniques for all students.\n\nHowever, recent research by David Roberts at Loughborough university, appearing in peer-reviewed scholarly journals [note here], shows that images used with reduced visible text generates greater subject engagement and higher levels of active learning in large group lecture settings. The visual teaching and learning method, developed from the scholarly research of Professor Richard Mayer, was evaluated over 3 years and across 9 disciplines using control and test group methods complemented by focus groups. The visual pedagogy, and the research testing it, is hosted by the UK Higher Education Academy [note here]\n\n\n"}
{"id": "12965084", "url": "https://en.wikipedia.org/wiki?curid=12965084", "title": "Women and trousers", "text": "Women and trousers\n\nTrousers (British English) or pants (American English) first appear in recorded history among nomadic steppe-people in Western Europe. Archaeological evidence suggests that men and women alike wore pants in that cultural context. However, for much of modern history, the use of trousers has been restricted to men. In many regions, this norm was enforced not only by social custom but also by law. There are, however, many historical cases of women wearing trousers in defiance of these norms, for a variety of reasons, including comfort, freedom of movement, fashion, disguise (notably for runaway slaves), attempts to evade the gender pay gap, and attempts to establish an empowered public identity for women. Especially in the 20th and 21st centuries, the customs and laws restricting this manner of dress have relaxed dramatically, reflecting a growing acceptance and normalization of the practice. \n\nVarious US cities, in the 19th and 20th centuries, passed legislation barring women from wearing trousers. Representative among these was an 1863 law passed by San Francisco's Board of Supervisors criminalizing appearing in public in “a dress not belonging to his or her sex,” although similar laws existed in Columbus, Ohio (passed 1848); Chicago, Illinois (passed 1851); Houston, Texas (passed 1864); Orlando, Florida (passed 1907), and approximately two dozen other US cities. (Anti-crossdressing laws continued to pass well into the 20th century, with Detroit, Michigan and Miami, Florida passing laws as late as the 1950s, and Cincinnati, Ohio passing one in 1974.)\n\nAdditionally, existing laws such as anti-vagrancy statutes were pressed into service to ensure that women would dress in accord with the gender norms of the time. One such instance would be New York's anti-vagrancy statute of 1845, which stated that \"Every person who, having his face painted, discolored, covered or concealed, or being otherwise disguised, in a manner calculated to prevent him from being identified, shall appear in any road or public highway, or in any field, lot, wood or inclosure, may be pursued and arrested”. This law was used to prosecute women for cross-dressing, on the grounds that their dressing outside of gender norms constituted a \"disguise\". Boston used similar anti-vagrancy laws to arrest Emma Snodgrass and Harriet French in 1852. (Snodgrass would be arrested again in Cleveland in 1853, and French would be arrested again in New York in 1856.) French reportedly broke with convention in order to pursue job opportunities open only to men: she claimed to the \"New York Daily Times\" that she could “get more wages” dressed as a man. \n\nAnti-vagrancy laws were also used to arrest Jennie Westbrook in New York, in 1882 and 1883. Westbook's case was said at the time to have \"awakened deep interest\" among the public, as it was understood that she was attempting to \"escape from that bondage [to] which social laws have subjected the sex.\" Like Harriet French in Boston, West identified work opportunities as her reason for cross-dressing: \"Her excuse was that she could make $20 a week in her disguise, while as a 'saleslady' in a fashionable store the pay would be only one-third that amount.\"\n\nIn 1851, early women's rights advocate Elizabeth Smith Miller introduced Amelia Bloomer to a garment initially known as the \"Turkish dress,\" which featured a knee-length skirt over Turkish-style pantaloons Bloomer came to advocate and promote the dress, including instructions for making it, in \"The Lily\", a newspaper dedicated to the \"Emancipation of Woman from Intemperance, Injustice, Prejudice, and Bigotry.\" This inspired a craze for the dress, which came to be known as bloomers.\n\nElizabeth Cady Stanton, Susan B. Anthony, and Lucy Stone, other early advocates for women's rights, also adopted this style of dress in the 1850s, referring to it as the \"freedom dress\" .\n\nConcurrently, some female laborers, notably the pit brow women working at coal pits in the Wigan area, began wearing trousers beneath a short skirt as a practical component of their uniform. This attracted the attention of the public, and various photographers produced records of the women's unconventional manner of dress through the mid- to late 19th century. \n\nAnother woman who advocated publicly for dress reform was Mary Edwards Walker, the abolitionist and Civil War surgeon. Walker, who had worn bloomers while working at a military hospital, wrote in 1871 that women's dress should \"protect the person, and allow freedom of motion and circulation, and not make the wearer a slave to it\". Walker openly wore men's trousers, and was arrested several times for wearing male attire (her earliest arrest was 1866, in New York, and her final arrest was in 1913, in Chicago, at the age of 80).\n\nAn updated version of the bloomer, for athletic use, was introduced in the 1890s as women's bicycling came into fashion. As activities such as tennis, cycling, and horseback riding became more popular at the turn of the century, women turned to pants or knickerbockers to provide comfort and freedom of movement in these activities, and some laws made allowances for women's wearing of pants during these activities. Women increasingly wore trousers as leisurewear in the 1920s and 30s, and working women, including female pilots, often wore trousers. \n\nArrests for cross-dressing did not necessarily cease during this time. For instance, in 1919, labor leader Luisa Capetillo became the first woman in Puerto Rico to wear trousers in public. Capetillo was sent to jail for what was then considered to be a crime in Puerto Rico, although the judge later dropped the charges against her.\n\nActresses Marlene Dietrich and Katharine Hepburn were often photographed in trousers in the 1930s; Dietrich famously appearing in a black tuxedo and matching fedora at the 1932 premiere of \"The Sign of the Cross\". \n\nEleanor Roosevelt became the first First Lady to appear in pants at a formal function, presiding over the Easter Egg Roll in 1933 wearing riding pants, a consequence of not having time to change after an early morning ride. However, she seemed to embrace the unconventional circumstance, posing for a photo in the pants on the South Portico of the White House. \n\n\"Vogue\" featured its first spread of women wearing slacks in 1939.\n\nDuring World War II, women working in industrial work in war service wore their husbands' trousers, and in the post-war era trousers were still common casual wear for gardening, socialising, and other leisure pursuits.\n\nSimilarly, in Britain during the Second World War, because of the rationing of clothing, many women took to wearing their husbands' civilian clothes to work while their husbands were away in the armed forces. This was partly because they were seen as work garments, and partly to allow women to keep their clothing allowance for other uses. As the men's clothes wore out, replacements were needed, so that by the summer of 1944 it was reported that sales of women's trousers were five times more than in the previous year.\n\nIn 1959, the Government Code Section 12947.5 (part of the California Fair Employment and Housing Act, passed in California) expressly protected the right to wear pants (American English for trousers). Thus, the standard FEHA discrimination complaint form now includes an option for \"denied the right to wear pants.\"\n\nIn the 1960s, André Courrèges introduced jeans for women, leading to the era of designer jeans. And in 1966, Yves Saint Laurent introduced \"Le Smoking\", a woman’s tuxedo intended for formal occasions, famously photographed by Helmut Newton in a manner emphasizing the wearer's androgyny and suggesting lesbian overtones.\n\nIn 1969 Rep. Charlotte Reid (R-Ill.) became the first woman to wear trousers in the U.S. Congress.\n\nAlso in 1969, Barbra Streisand became the first woman to attend the Oscars in pants, accepting an award for her role in \"Funny Girl\" dressed in an outfit designed by Arnold Scaasi. \n\nIn 1972, Pat Nixon was the first American First Lady to model pants in a national magazine. However, First Ladies had been seen earlier wearing pants, including Lou Hoover (photographed privately wearing riding pants at the presidential retreat Camp Rapidan) and Jackie Kennedy (photographed wearing pants and a sweater on Cape Cod in 1960 and wearing palazzo pants in Italy in 1962). \n\nIn 1972, the Education Amendments of 1972 passed in the United States, which, as part of the Title IX non-discrimination provisions, declared that dresses could not be required of girls. Dress codes thus changed in public schools across the United States.\n\nIn the 1970s, trousers became quite fashionable for women. Jane Fonda, Diana Ross, Katharine Hepburn, Tatum O’Neal, and Diane Keaton all helped to popularize the wearing of pants, appearing at high-profile awards ceremonies dressed in suits or pants ensembles; Tatum O'Neal notably accepted an Oscar at age 10 while wearing a tuxedo.\n\nIn 1989 California state senator Rebecca Morgan became the first woman to wear trousers in a U.S. state senate.\n\nHillary Clinton was the first woman to wear trousers in an official U.S. First Lady portrait.\n\nWomen were not allowed to wear trousers on the U.S. Senate floor until 1993. In 1993, Senators Barbara Mikulski and Carol Moseley Braun wore trousers onto the floor in defiance of the rule, and female support staff followed soon after, with the rule being amended later that year by Senate Sergeant-at-Arms Martha Pope to allow women to wear trousers on the floor so long as they also wore a jacket.\n\nIn 1994, Malawi women became legally allowed to wear trousers under President Kamuzu Banda's rule., originally prohibited by a law introduced in 1965.\n\nSince 2004 the International Skating Union has allowed women to wear trousers instead of skirts in competition if they wish.\n\nIn 2013, a bylaw requiring women in Paris, France to ask permission from city authorities before \"dressing as men\", including wearing trousers (with exceptions for those \"holding a bicycle handlebar or the reins of a horse\") was declared officially revoked by France's Women's Rights Minister, Najat Vallaud-Belkacem. The bylaw was originally intended to prevent women from wearing the pantalons fashionable with Parisian rebels in the French Revolution.\n\nAlso in 2013, Turkey's parliament ended a ban on women lawmakers wearing trousers in its assembly.\n\nIn 2014, an Indian family court in Mumbai ruled that a husband objecting to his wife wearing a kurta and jeans and forcing her to wear a sari amounts to cruelty inflicted by the husband and can be a grounds for divorce. The wife was thus granted a divorce on the ground of cruelty as defined under section 27(1)(d) of Special Marriage Act, 1954.\n\nWomen were not allowed to wear trousers on the U.S. Senate floor until 1993. In 1993, Senators Barbara Mikulski and Carol Moseley Braun wore trousers onto the floor in defiance of the rule, and female support staff followed soon after, with the rule being amended later that year by Senate Sergeant-at-Arms Martha Pope to allow women to wear trousers on the floor so long as they also wore a jacket.\n\nIn 2012 the Royal Canadian Mounted Police began to allow women to wear trousers and boots with all their formal uniforms.\n\nUntil 2016 some female crew members on British Airways were required to wear British Airways’ standard \"ambassador\" uniform, which has not traditionally included trousers.\n\nIn 2017, The Church of Jesus Christ of Latter-day Saints announced that its female employees could wear \"professional pantsuits and dress slacks\" while at work; dresses and skirts had previously been required.\n\nMost UK schools allow female pupils to wear trousers, but many girls still wear skirts in primary and secondary schools, even where the choice of trousers is given. In the late 20th and early 21st century, many schools began changing their uniform rules to allow trousers for girls amidst opposition to skirts-only policies - the most publicised possibly being Jo Hale vs Whickam Comprehensive in 2000. Although commonly accepted that girls may wear trousers to school, no test case is known to have been brought before the courts, making the legal position uncertain on requiring skirts as part of girls' uniforms. The rule is still enforced in many schools, particularly independent and selective state schools. In fact, government guidelines expressly state the decision of allowing girls to wear trousers is with individual schools.\n\nThere are a number of religions that prohibit women from revealing their legs, requiring all women and often young girls not to wear trousers but a long dress. By contrast, a sizable majority of Sikhs often believe wearing trousers is preferred for Sikh women to maintain modesty.\n\nAlthough many contemporary Mennonites have no dress code, among traditional, conservative Mennonites, sometimes referred to as \"Old Order Mennonites\", long skirts or dresses covering most of the legs are required. They also wear dresses and skirts because they believe men and women should be distinguished from one another. \"Deuteronomy 22:5\nThe woman shall not wear that which pertaineth unto a man, neither shall a man put on a woman's garment: for all that do so are abomination unto the Lord thy God.\" (KJV) Conservative conferences usually demand that women wear a specific style of dress. This is usually in the style of the cape dress, with a double covering or \"cape\". Most non-conservative conferences allow for the wearing of trousers by women.\n\nPentecostal women typically wear skirts because of the Biblical commandment in the Old Testament that women must not wear men's clothing; this is mandatory in some oneness Pentecostal churches (at the individual pastor's discretion).\n\nMany Independent Fundamental Baptist churches, colleges and universities prohibit females from wearing trousers. For example, at Pensacola Christian College, female students may only wear trousers or shorts for \"recreational purposes\" only. They are also required to wear skirts or dresses until 5:00 PM on workdays. \n\nIn Orthodox Jewish belief, the space between a woman's legs is considered to be a private area, and therefore, must be covered by a garment. However, in other cultures wearing men's clothing is forbidden biblically under the prohibition of \"Lo Silbash\" (\"A woman shall not wear that which pertaineth unto a man\", ). In some Jewish communities, such as the Jews from Arab countries and Modern Orthodox communities, it is reasoned that trousers provide an extra form of modesty.\n\nOn 13 November 866, Pope Nicholas I wrote to King Boris I of Bulgaria: \"Whether you or your women wear or do not wear trousers neither impedes your salvation nor leads to any increase of your virtue\" (\"sive vos, sive feminae vestrae, sive deponatis, sive induatis femoralia, nec saluti officit, nec ad virtutum vestrarum proficit incrementum\" - Patrologia Latina, CXIX, 1002). Some members of the Society of Saint Pius X have spoken of the preference of women's wearing skirts rather than trousers. Cardinal Siri's letter has also been cited as justification for women wearing skirts and dresses. In addition, Saint Thomas Aquinas, the Church's principal theologian, also taught that \"outward apparel should be consistent with the estate of the person, according to the general custom. Hence it is in itself sinful for a woman to wear man's clothes, or vice versa; and it is expressly forbidden in the Law (Deuteronomy 22)...\"\n\nIn 2012 and 2013, some Mormon women participated in \"Wear Pants to Church Day\", in which they wore trousers to church instead of the customary dresses to encourage gender equality within The Church of Jesus Christ of Latter-day Saints. Over one thousand women participated in this in 2012. \n\nMany forms of dancing require females to wear skirts or dresses, either by convention or competition rules. In Scottish highland dancing for example, women don't wear trews, but instead either wear a skirt or dress including the Aboyne dress (for the national dances) or the kilt-based outfit for the Highland dances. However, tartan trews can be worn by women in the United States.\n\nIn Sudan, Article 152 of the Memorandum to the 1991 Penal Code prohibits the wearing of \"obscene outfits\" in public. This law has been used to arrest and prosecute women wearing trousers. Thirteen women including journalist Lubna al-Hussein were arrested in Khartoum in July 2009 for wearing trousers; ten of the women pleaded guilty and were flogged with ten lashes and fined 250 Sudanese pounds apiece. Lubna al-Hussein considers herself a good Muslim and asserts \"Islam does not say whether a woman can wear trousers or not. I'm not afraid of being flogged. It doesn't hurt. But it is insulting.\" She was eventually found guilty and fined the equivalent of $200 rather than being flogged.\n\nIn Rome in 1992, a 45-year-old driving instructor was accused of rape. When he picked up an 18-year-old girl for her first driving lesson, he allegedly raped her for an hour, then told her that if she was to tell anyone he would kill her. Later that night she told her parents and her parents agreed to help her press charges. While the alleged rapist was convicted and sentenced, the Italian Supreme Court overturned the conviction in 1998 because the victim wore tight jeans. It was argued that she must have necessarily have had to help her attacker remove her jeans, thus making the act consensual (\"because the victim wore very, very tight jeans, she had to help him remove them...and by removing the jeans...it was no longer rape but consensual sex\"). The Italian Supreme Court stated in its decision “it is a fact of common experience that it is nearly impossible to slip off tight jeans even partly without the active collaboration of the person who is wearing them.” This ruling sparked widespread feminist protest. The day after the decision, women in the Italian Parliament protested by wearing jeans and holding placards that read “Jeans: An Alibi for Rape.” As a sign of support, the California Senate and Assembly followed suit. Soon Patricia Giggans, Executive Director of the Los Angeles Commission on Assaults Against Women, (now Peace Over Violence) made Denim Day an annual event. As of 2011 at least 20 U.S. states officially recognize Denim Day in April. Wearing jeans on this day has become an international symbol of protest against erroneous and destructive attitudes about sexual assault. As of 2008 the Italian Supreme Court has overturned their findings, and there is no longer a \"denim\" defense to the charge of rape.\n\n"}
