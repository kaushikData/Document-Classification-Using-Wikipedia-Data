{"id": "3253311", "url": "https://en.wikipedia.org/wiki?curid=3253311", "title": "Causation (law)", "text": "Causation (law)\n\nCausation is the \"causal relationship between conduct and result\". In other words, causation provides a means of connecting conduct with a resulting effect, typically an injury. In criminal law, it is defined as the \"actus reus\" (an action) from which the specific injury or other effect arose and is combined with \"mens rea\" (a state of mind) to comprise the elements of guilt. Causation only applies where a result has been achieved and therefore is immaterial with regard to inchoate offenses.\n\nLegal systems more or less try to uphold the notions of fairness and justice. If a state is going to penalize a person or require that person pay compensation to another for losses incurred, liability is imposed according to the idea that those who injure others should take responsibility for their actions. Although some parts of any legal system will have qualities of strict liability, in which the \"mens rea\" is immaterial to the result and subsequent liability of the actor, most look to establish liability by showing that the defendant was the cause of the particular injury or loss.\n\nEven the youngest children quickly learn that, with varying degrees of probability, consequences flow from physical acts and omissions. The more predictable the outcome, the greater the likelihood that the actor caused the injury or loss intentionally. There are many ways in which the law might capture this simple rule of practical experience: that there is a natural flow to events, that a reasonable man in the same situation would have foreseen this consequence as likely to occur, that the loss flowed naturally from the breach of contractual duties or tortuous actions, etc. However it is phrased, the essence of the degree of fault attributed will lie in the fact that reasonable people try to avoid injuring others, so if harm was foreseeable, there should be liability to the extent that the extent of the harm actually resulting was foreseeable.\n\nCausation of an event alone is insufficient to create legal liability.\n\nSometimes causation is one part of a multi-stage test for legal liability. For example, for the defendant to be held liable for the tort of negligence, the defendant must have owed the plaintiff a duty of care, breached that duty, by so doing caused damage to the plaintiff, and that damage must not have been too remote. Causation is just one component of the tort.\n\nOn other occasions, causation is the only requirement for legal liability (other than the fact that the outcome is proscribed). For example, in the law of product liability, the courts have come to apply to principle of \"strict liability\" : the fact that the defendant's product caused the plaintiff harm is the only thing that matters. The defendant need not also have been negligent.\n\nOn still other occasions, causation is irrelevant to legal liability altogether. For example, under a contract of indemnity insurance, the insurer agrees to indemnify the victim for harm not caused by the insurer, but by other parties.\n\nBecause of the difficulty in establishing causation, it is one area of the law where the case law overlaps significantly with general doctrines of analytic philosophy to do with causation. The two subjects have long been intermingled.\n\nWhere establishing causation is required to establish legal liability, it usually involves a two-stage inquiry, firstly establishing 'factual' causation, then 'legal' causation.\n\n‘Factual’ causation must be established before inquiring into legal causation, perhaps by assessing if the defendant acted in the plaintiff’s loss.\n\nDetermining ‘legal’ causation often involves a question of public policy regarding the sort of situation in which, despite the outcome of the factual enquiry, the defendant might nevertheless be released from liability, or impose liability.\n\nThe usual method of establishing factual causation is the \"but-for test\". The but for test inquires ‘But for the defendant’s act, would the harm have occurred?’ A shoots and wounds B. We ask ‘But for A's act, would B have been wounded?’ The answer is ‘No.’ So we conclude that A caused the harm to B. The but for test is a test of necessity. It asks was it ‘necessary’ for the defendant’s act to have occurred for the harm to have occurred. In New South Wales, this requirement exists in s 5D of the \"Civil Liability Act 2002\" (NSW), reinforcing established common law principles.\n\nOne weakness in the but-for test arises in situations where each of several acts alone are sufficient to cause the harm. For example, if both A and B fire what would alone be fatal shots at C at approximately the same time, and C dies, it becomes impossible to say that but-for A's shot, or but-for B's shot alone, C would have died. Taking the but-for test literally in such a case would seem to make neither A nor B responsible for C's death.\n\nThe courts have generally accepted the but for test notwithstanding these weaknesses, qualifying it by saying that causation is to be understood “as the man in the street” would, or by supplementing it with “common sense”.\n\nThis dilemma was handled in the United States in \"State v. Tally\", , where the court ruled that: “The assistance given ... need not contribute to criminal result in the sense that but for it the result would not have ensued. It is quite sufficient if it facilitated a result that would have transpired without it.” Using this logic, A and B are liable in that no matter who was responsible for the fatal shot, the other \"facilitated\" the criminal act even though his shot was not necessary to deliver the fatal blow.\n\nHowever, legal scholars have attempted to make further inroads into what explains these difficult cases. Some scholars have proposed a test of sufficiency instead of a test of necessity. H. L. A. Hart and Tony Honoré, and later Richard Wright, have said that something is a cause if it is a ‘necessary element of a set of conditions jointly sufficient for the result’. This is known as the NESS test. In the case of the two hunters, the set of conditions required to bring about the result of the victim's injury would include a gunshot to the eye, the victim being in the right place at the right time, gravity, etc. In such a set, either of the hunters' shots would be a member, and hence a cause. This arguably gives us a more theoretically satisfying reason to conclude that something was a cause of something else than by appealing to notions of intuition or common sense.\n\nHart and Honore, in their famous work \"Causation in the Law\", also tackle the problem of 'too many causes'. For them, there are degrees of causal contribution. A member of the NESS set is a \"causally relevant condition\". This is elevated into a \"cause\" where it is a deliberate human intervention, or an abnormal act in the context. So, returning to our hunter example, hunter A's grandmother's birth is a causally relevant condition, but not a \"cause\". On the other hand, hunter A's gunshot, being a deliberate human intervention in the ordinary state of affairs, is elevated to the status of \"cause\". An intermediate position can be occupied by those who \"occasion\" harm, such as accomplices. Imagine an accomplice to a murder who drives the principal to the scene of the crime. Clearly the principal's act in committing the murder is a \"cause\" (on the but for or NESS test). So is the accomplice's act in driving the principal to the scene of the crime. However, the causal contribution is not of the same level (and, incidentally, this provides some basis for treating principals and accomplices differently under criminal law). Leon Green and Jane Stapleton are two scholars who take the opposite view. They consider that once something is a \"but for\" (Green) or NESS (Stapleton) condition, that ends the factual inquiry altogether, and anything further is a question of policy.\n\nNotwithstanding the fact that causation may be established in the above situations, the law often intervenes and says that it will nevertheless not hold the defendant liable because in the circumstances the defendant is not to be understood, in a legal sense, as having caused the loss. In the United States, this is known as the doctrine of proximate cause. The most important doctrine is that of \"novus actus interveniens\", which means a ‘new intervening act’ which may ‘cut the chain of causation’.\n\nThe but-for test is factual causation and often gives us the right answer to causal problems, but sometimes not. Two difficulties are immediately obvious. The first is that under the but-for test, almost anything is a cause. But for a tortfeasor's grandmother's birth, the relevant tortious conduct would not have occurred. But for the victim of a crime missing the bus, he or she would not have been at the site of the crime and hence the crime would not have occurred. Yet in these two cases, the grandmother's birth or the victim's missing the bus are not intuitively causes of the resulting harm. This often does not matter in the case where cause is only one element of liability, as the remote actor will most likely not have committed the other elements of the test. The legally liable cause is the one closest to or most proximate to the injury. This is known as the Proximate Cause rule. However, this situation can arise in strict liability situations.\n\nImagine the following. A critically injures B. As B is wheeled to an ambulance, she is struck by lightning. She would not have been struck if she had not been injured in the first place. Clearly then, A caused B's whole injury on the ‘but for’ or NESS test. However, at law, the intervention of a supervening event renders the defendant not liable for the injury caused by the lightning.\n\nThe effect of the principle may be stated simply:\n\nNote, however, that this does not apply if the Eggshell skull rule is used. For details, see article on the Eggshell Skull doctrine.\nactus interveniens\n\nWhen two or more negligent parties, where the consequence of their negligence joins together to cause damages, in a circumstance where either one of them alone would have caused it anyway, each is deemed to be an \"Independent Sufficient Cause,\" because each could be deemed a \"substantial factor,\" and both are held legally responsible for the damages. For example, where negligent firestarter A's fire joins with negligent firestarter B's fire to burn down House C, both A and B are held responsible. (e.g., \"Anderson v. Minneapolis, St: P. & S. St. R.R. Co\"., 146 Minn. 430, 179 N.W. 45 (1920).) This is an element of Legal Cause.\n\nThe other problem is that of overdetermination. Imagine two hunters, A and B, who each negligently fire a shot that takes out C's eye. Each shot on its own would have been sufficient to cause the damage. But for A's shot, would C's eye have been taken out? Yes. The same answer follows in relation to B's shot. But on the but-for test, this leads us to the counterintuitive position that neither shot caused the injury. However, courts have held that in order to prevent each of the defendants avoiding liability for lack of actual cause, it is necessary to hold both of them responsible, See \"Summers v. Tice\", 33 Cal.2d 80, 199 P.2d 1 (1948). This is known, simply, as the \"Summers v. Tice\" Rule.\n\nSuppose that two actors' negligent acts combine to produce one set of damages, where but for either of their negligent acts, no damage would have occurred at all. This is two negligences contributing to a single cause, as distinguished from two separate negligences contributing to two successive or separate causes. These are \"concurrent actual causes\". In such cases, courts have held both defendants liable for their negligent acts. Example: A leaves truck parked in the middle of the road at night with its lights off. B fails to notice it in time and plows into it, where it could have been avoided, except for want of negligence, causing damage to both vehicles. Both parties were negligent. (\"Hill v. Edmonds\", 26 A.D.2d 554, 270 N.Y.S.2d 1020 (1966).)\n\nLegal Causation is usually expressed as a question of 'foreseeability'. An actor is liable for the foreseeable, but not the unforeseeable, consequences of his or her act. For example, it is foreseeable that if I shoot someone on a beach and they are immobilized, they may drown in a rising tide rather than from the trauma of the gunshot wound or from loss of blood. However it is not (generally speaking) foreseeable that they will be struck by lightning and killed by that event.\n\nThis type of causal foreseeability is to be distinguished from foreseeability of extent or kind of injury, which is a question of remoteness of damage, not causation. For example, if I conduct welding work on a dock that lights an oil slick that destroys a ship a long way down the river, it would be hard to construe my negligence as anything other than causal of the ship's damage. There is no \"novus actus interveniens\". However, I may not be held liable if that damage is not of a type foreseeable as arising from my negligence. That is a question of public policy, and not one of causation.\n\nAn example of how foreseeability does not apply to the extent of an injury is the eggshell skull rule. If Neal punched Matt in the jaw, it is foreseeable that Matt will suffer a bodily injury that he will need to go to the hospital for. However, if his jaw is very weak, and his jaw comes completely off from my punch, then the doctor bills, which would have been about $5,000 for wiring his jaw shut had now become $100,000 for a full-blown jaw re-attachment. Neal would still be liable for the entire $100,000, even though $95,000 of those damages were not reasonably foreseeable.\n\nBecause causation in the law is a complex amalgam of fact and policy, other doctrines are also important, such as foreseeability and risk. Particularly in the United States, where the doctrine of 'proximate cause' effectively amalgamates the two-stage factual then legal causation inquiry favoured in the English system, one must always be alert to these considerations in assessing the postulated relationship between two events.\n\nSome aspects of the physical world are so inevitable that it is always reasonable to impute knowledge of their incidence. So if A abandons B on a beach, A must be taken to foresee that the tide comes in and goes out. But the mere fact that B subsequently drowns is not enough. A court would have to consider where the body was left and what level of injury A believed that B had suffered. If B was left in a position that any reasonable person would consider safe but a storm surge caused extensive flooding throughout the area, this might be a \"novus actus\". That B was further injured by an event within a foreseen class does not of itself require a court to hold that every incident falling within that class is a natural link in the chain. Only those causes that are reasonably foreseeable fit naturally into the chain. So if A had heard a weather forecast predicting a storm, the drowning will be a natural outcome. But if this was an event like a flash flood, an entirely unpredictable event, it will be a \"novus actus\".\n\nThe question of A's beliefs is no different. If A honestly believes that B is only slightly injured and so could move himself out of danger without difficulty, how fair is it to say that he \"ought\" to have foreseen? The test is what the reasonable person would have known and foreseen, given what A had done. It is the function of any court to evaluate behaviour. A defendant cannot evade responsibility through a form of wilful blindness. Fault lies not only in what a person actually believes, but also in failing to understand what the vast majority of other people would have understood. Hence, the test is hybrid, looking both at what the defendant actually knew and foresaw (i.e. subjective), and at what the reasonable person would have known (i.e. objective) and then combining the conclusions into a general evaluation of the degree of fault or blameworthiness.\n\nSimilarly, in the quantification of damages generally and/or the partitioning of damages between two or more defendants, the extent of the liability to compensate the plaintiff(s) will be determined by what was reasonably foreseeable. So if, for example, the plaintiff unexpectedly contributed to the extent of the loss suffered, that additional element would not be included in the damages award even though the plaintiff would not have had the opportunity to make this mistake had it not been for the defendant's breach. In cases involving the partitioning of damages between multiple defendants, each will be liable to the extent that their contribution foreseeably produced the loss.\n\nSometimes the reverse situation to a \"novus actus\" occurs, i.e. factual causation cannot be proved but the court nevertheless does want to hold the defendant liable. In \"Sindell v. Abbott Laboratories\", the plaintiff's mother consumed diethylstilbestrol as a miscarriage preventative. The medicine, later re-called from the market, caused the defendant to develop a malignant bladder tumor due to its negligent manufacture. However, there were many manufacturers of that drug in the market. The manufacturer of the particular medication that caused the injury could not be ascertained for certain. The court held that the defendant was liable in proportion to its market share. They departed from traditional notions of pure cause and adopted a ‘risk based’ approach to liability. The defendant was held liable because of the amount of risk it contributed to the occasioning of the harm. Note that a risk theory is not strictly a theory built on notions of cause at all, as, by definition, the person who caused the injury could not be ascertained for certain. However, it does show that legal notions of causation are a complex mixture of factual causes and ideas of public policy relating to the availability of legal remedies. In \"R v Miller\" , the House of Lords said that a person who puts a person in a dangerous position, in that case a fire, will be criminally liable if he does not adequately rectify the situation.\n\nTo be acceptable, any rule of law must be capable of being applied consistently, thus a definition of the criteria for this qualitative analysis must be supplied. Let us assume a purely factual analysis as a starting point. A injures B and leaves him lying in the road. C is a driver who fails to see B on the road and by running over him, contributes to the cause of his death. It would be possible to ask for a detailed medical evaluation at a post mortem to determine the initial degree of injury and the extent to which B's life was threatened, followed by a second set of injuries from the collision and their contribution. If the first incident merely damaged B's leg so that he could not move, it is tempting to assert that C's driving must have been the more substantial cause and so represents a \"novus actus\" breaking the chain. Equally, if B was bleeding to death and the only contribution that the driving made was to break B's arm, the driving is not a \"novus actus\" and does not break the chain. But this approach ignores the issue of A's foresight.\n\nRoads are, by their nature, used by vehicles and it is clearly foreseeable that a person left lying on the road is at risk of being further injured by an inattentive driver. Hence, if A leaves B on the road with knowledge of that risk and a foreseeable event occurs, A remains the more proximate cause. This leaves whether the test of foresight should be subjective, objective or hybrid (i.e. both subjective and objective). Obviously, there is no difficulty in holding A liable if A had actual knowledge of the likelihood that B would be further injured by a driver. The fault which caused the initial injury is compounded by the omission to move B to a safer place or call for assistance. But let us assume that A never averts the possibility of further injury. The issue is now the extent to which knowledge may be imputed objectively.\n\nA difficult issue that has arisen recently is the case where the defendant neither factually causes the harm, nor increases the risk of its occurrence. In \"Chester v Afshar\" , a doctor negligently failed to warn a patient of risks inherent in an operation, specifically cauda equina syndrome. The patient had the operation and a risk materialized causing injury. It was found that even if the patient had been warned, the patient would still have undergone the operation, simply at a different time. The risk of the injury would be the same at both times. Accordingly, the doctor neither caused the injury (because but for the failure to warn, the patient would still have gone ahead with the operation), nor increased the risk of its occurrence (because the risk was the same either way). Yet the House of Lords, embracing a more normative approach to causation, still held the doctor liable. Lawyers and philosophers continue to debate whether and how this changes the state of the law.\n\n\n"}
{"id": "47579630", "url": "https://en.wikipedia.org/wiki?curid=47579630", "title": "Cheerleader effect", "text": "Cheerleader effect\n\nThe cheerleader effect, also known as the group attractiveness effect, is the cognitive bias which causes people to think individuals are more attractive when they are in a group. The term was backed up by research by Drew Walker & Edward Vul (2013) and van Osch et al. (2015).\n\nThe phrase was coined by the character Barney Stinson in \"Not a Father's Day\", an episode of the television series \"How I Met Your Mother\", first aired in 2008. Barney points out to his friends a group of women that initially seem attractive, but who all seem to be very ugly when examined individually. This point is made again by Ted and Robin later in the episode, who note that some of Barney's friends also only seem attractive in a group.\n\nAcross five studies by Walker and Vul (2013), participants rated the attractiveness of male and female faces when shown in a group photo, and an individual photo, with the order of the photographs randomised. The people photographed got higher scores for their group photos.\n\nThis effect occurs with male-only, female-only and mixed gender groups, and both small and large groups. The effect occurs to the same extent with groups of four and 16 people. Participants in studies looked more at the attractive people than the unattractive people in the group. The effect does not occur because group photos give the impression that individuals have more social or emotional intelligence: this was shown to be the case by a study which used individual photos grouped together in a single image, rather than photos taken of people in a group.\n\nDrew Walker and Edward Vul proposed that this effect arises due to the interplay of three cognitive phenomena:\nWhen all three of these phenomena are taken together, the individual faces will seem more attractive in a group, as they appear more similar to the average group face, which is more attractive than members' individual faces.\n\nA 2015 study by van Osch et al. confirmed the results obtained by Walker and Vul.\n\nThe research team offered two different explanations for the group attractiveness effect:\nThey claim that selective attention fits better the gathered data.\n\nA 2015 replication of Walker and Vul's study failed to show any significant results for the group attractiveness effect. The research team hypothesized that this may be due to cultural differences, since the replication study was performed in Japan.\n\n"}
{"id": "10407767", "url": "https://en.wikipedia.org/wiki?curid=10407767", "title": "Cigarette lighter receptacle", "text": "Cigarette lighter receptacle\n\nThe cigarette lighter receptacle (also called a cigar lighter receptacle or cigar lighter socket) in an automobile was initially designed to power an electrically heated cigarette lighter, but became a de facto standard DC connector to supply electrical power for portable accessories used in or near an automobile. While the cigarette lighter receptacle is a common feature of automobiles and trucks, as a DC power connector it has the disadvantages of bulkiness, relatively low current rating, and poor contact reliability.\n\nExamples of devices that can be operated from a cigarette lighter receptacle include lights, fans, beverage heating devices, and small motorized tools such as air compressors for inflating tires. Many portable electronic devices such as music players or mobile telephones use a cigarette lighter receptacle to recharge their internal batteries or to directly operate from the vehicle electrical system. Adapters for electronic devices may change voltage to be compatible with the supplied device. Devices that require alternating-current mains electricity can be operated with a plug-in inverter.\n\nAutomobiles may provide several 12 V receptacles that are intended only to operate electrical accessories, and which cannot be used with a cigarette lighter. Car manufacturers may offer a cigarette lighter only as an optional extra-cost accessory. Usually, only one 12 V receptacle near the driver will be able to accommodate an actual cigarette lighter, with other receptacles designated as \"12 V auxiliary power outlets\" which are not physically able to power a lighter.\n\nThe electrical cigar-lighter was invented and patented in the early 1880s by the German inventor . In the 1890s, these tools were sold as electrical cigar lighters (\"Cigarrenanzünder\"), and later as \"Zigarrenanzünder\" in the major German warehouse catalogues. Probably in the 1920s they were renamed \"cigarette lighters\", as cigarettes overtook cigars in sales.\n\nIn 1921, the Morris was issued for a so-called \"wireless\" or \"cordless\" lighter with a removable element. The igniter was heated in the socket and then manually removed for use after an appropriate time interval.\n\nIn the United States, cigarette lighters started appearing as standard equipment in automobiles in 1925–1926.\n\nIn 1928, the Connecticut Automotive Specialty Company (Casco) in Bridgeport patented its version of an automotive cigarette lighter, which used a cord and reel. In the reel-type lighters, the igniter unit was connected with a source of current by a cable which was wound on a spring drum so that the igniter unit and cable could be withdrawn from the socket and be used for lighting a cigar or cigarette. As the removable plug was returned to the socket, the wires were reeled back into it. The circuit was closed either by pressing a button or removing the igniter from its socket.\n\nThe modern \"automatic\" removable automotive V-Coil lighter was developed by Casco in 1956, for which it received , issued in 1960.\n\nThe lighter is a metal or plastic cylinder containing a thin flat coil of nichrome metal strip, through which high current (~10 amperes) passes when the device is activated, usually by pushing it into the socket as though it were a push-button. When pushed in, the lighter is held against the force of a spring by a clip attached to a bi-metallic strip. The heating element glows orange hot in seconds, causing the bi-metallic strip to bend and release the mechanism, and the handle pops out, eliminating the need for the user to time the heating operation. If the lighter is then promptly removed from its socket, it can light a cigarette, cigar, or tinder.\n\nIn newer cars, the socket is equipped with a plastic cover without the lighter heating element. However, the socket has been repurposed and continues to be used to power consumer electronics in vehicles. Often, a vehicle may come with several outlets for convenience, some in the rear passenger area of the vehicle or even the cargo area, for such purposes as powering portable GPS devices, recharging mobile phones, or powering a tire inflator, a vacuum cleaner or a thermoelectric cooler. These outlets usually have a plastic cap tethered to them, and are usually labeled as being only for DC power, because they are not intended to withstand the heat produced by an electrical cigarette lighter.\n\nThe use of cigarette lighter receptacles to supply 12 volt DC power is a classic example of backward compatibility to a de facto standard. The connector falls far short of ideal, being physically large and awkward to use, while being less reliable than alternatives such as the Anderson Powerpole connector (which is often used by amateur radio enthusiasts in mobile operations). Nevertheless, cigarette lighter receptacles are in widespread use, and all but the lowest-cost cars, trucks, RVs, and even boats can be expected to have at least one such receptacle. Portable cigarette lighter receptacles attached to cables and alligator clips for connection directly to car batteries are available for temporary use. In newer vehicles, one or more USB connectors may also be provided, as a source of modest amounts of 5 volt DC power, but even in these situations a cigarette lighter receptacle is provided for 12 volt DC power, and for applications that require higher current.\n\nStandardized 12 volt DC automobile plugs are defined in the United States by UL Standard 2089 regarding vehicle battery adapters. This standard covers plugs and cord sets that insert into cigarette lighter receptacles. In Europe, 12 volt plugs and sockets are not specially regulated, and do not require approvals for the CE mark.\n\nThe male plug is sometimes used to feed power \"into\" a vehicle to recharge its battery. For instance, portable solar battery maintainers generally connect to a vehicle's battery in this manner. Trickle chargers also sometimes connect in this way, eliminating the need to leave a vehicle's hood open, as well as eliminating the possibility of reversed polarity. In some models, the cigarette lighter outlet is not powered when the ignition key is removed and charging is not possible.\n\nThe sockets and mating plugs are defined in the ANSI/SAE J563 specification. For the 12 volt systems, the \"contact point\", which is the center part of the plug when viewed end-on, carries the positive voltage, whereas the \"can\" part, which is the outer part of the connector, carries the negative voltage (which is the \"ground\" connection for most automobiles, which have a negative ground electrical system).\n\n12 volt auto connectors are made to comply with a standard by Underwriters Laboratories for safety. UL2089 was developed to cover the requirements for portable adapters rated 24 V DC or less that are intended to be supplied from the battery powered electrical system of a vehicle. Products covered by the standard include cord assemblies of a plug that mates with the standard cigarette receptacle found in automobiles.\n\n\n\n\nPlugs often include a pilot light LED indicator to indicate that electrical power is connected. Optionally, the plug may be equipped with an internal fuse for electrical safety, usually rated at 10 amps or less. In some designs, the tip of the plug may be unscrewed to reveal a cylindrical glass fuse; other variants may use a newer blade-type fuse inserted into the side or back of the plug.\n\nSince the cigarette lighter socket was originally designed only to heat a cigarette lighter, repurposing these sockets as generic power connectors can lead to many problems. In addition to the issues with partially-compatible physical dimensions, the plugs can vibrate out of the socket under normal driving conditions, owing to poor retention. Also, there have been reports of melted plug tips due to poor contact and heat generation from ohmic resistance. Non-vehicular use in stationary settings may avoid vibration problems when used as an alternative to 120 volt AC outlets, but low-quality connectors may still develop high resistance or intermittent contact.\n\nA second problem is that nominally \"twelve-volt\" power in cars fluctuates widely. The actual voltage will be approximately 12.5 volts when dormant (less in cold conditions), approximately 14.5 volts when the engine and the alternator/generator are operating (more when cold), and may briefly drop as low as 5–6 volts during engine start. When used, DC to DC converters will usually compensate for small fluctuations, but reliable power may not be available without an independent battery-powered uninterruptible power supply.\n\nRarely, more extreme cases of voltage fluctuation can occur when the car battery is disconnected while the engine is running, or when the car receives a jump start. When the battery is disconnected while the engine is running, a load dump transient can produce very high voltages as the built in voltage regulator has been controlling the alternator field current to charge the vehicle battery and although it will attempt to reduce the field current to keep the output voltage constant, the field winding is highly inductive and setting the current to its new value takes several hundred milliseconds, during which the alternator output voltage will exceed its intended value. The load dump transient may also ruin the diodes in the alternator by exceeding their breakdown voltage. A car receiving a jump start from a truck may be subject to a 24 V electrical system used in some vehicles. Also, a \"double battery jump-start\" is performed by some tow truck drivers in cold climates.\n\nEquipment intended to be powered by the receptacle needs to account for intermittent contact, and voltages outside the nominal , such as maximum voltage continuously, or maximum voltage of lasting , lasting , and lasting . An example of protection component ratings tolerance is to . Robust equipment must tolerate temperatures varying between , plus possible high humidity and condensation of water.\n\n"}
{"id": "2275212", "url": "https://en.wikipedia.org/wiki?curid=2275212", "title": "Closed-household economy", "text": "Closed-household economy\n\nA closed-household economy is a society's economic system in which goods are not traded. Instead, those goods are produced and consumed by the same households. In other words, a closed-household economy is an economy where households are closed to trading. This kind of economy is present, for example, in hunter-gatherer societies.\n\nThe production and consumption of goods is not separated as in a society with high division of labor.\n\nThe closed-household economy contrasts with a barter economy, in which goods are bartered (traded against each other), and a monetary economy, in which goods are traded for money.\n\nThe closed-household economy and the barter economy are together referred to as \"non-monetary economies\".\n"}
{"id": "355240", "url": "https://en.wikipedia.org/wiki?curid=355240", "title": "Cognitive model", "text": "Cognitive model\n\nA cognitive model is an approximation to animal cognitive processes (predominantly human) for the purposes of comprehension and prediction. Cognitive models can be developed within or without a cognitive architecture, though the two are not always easily distinguishable.\n\nIn contrast to cognitive architectures, cognitive models tend to be focused on a single cognitive phenomenon or process (e.g. list learning), how two or more processes interact (e.g. visual search and decision making), or to make behavioral predictions for a specific task or tool (e.g. how instituting a new software package will affect productivity). Cognitive architectures tend to be focused on the structural properties of the modeled system, and help constrain the development of cognitive models within the architecture. Likewise, model development helps to inform limitations and shortcomings of the architecture. Some of the most popular architectures for cognitive modeling include ACT-R, Clarion, and Soar.\n\nCognitive modeling historically developed within cognitive psychology/cognitive science (including human factors), and has received contributions from the fields of machine learning and artificial intelligence to name a few. There are many types of cognitive models, and they can range from box-and-arrow diagrams to a set of equations to software programs that interact with the same tools that humans use to complete tasks (e.g., computer mouse and keyboard).\n\nA number of key terms are used to describe the processes involved in the perception, storage, and production of speech. Typically, they are used by speech pathologists while treating a child patient. The input signal is the speech signal heard by the child, usually assumed to come from an adult speaker. The output signal is the utterance produced by the child. The unseen psychological events that occur between the arrival of an input signal and the production of speech are the focus of psycholinguistic models. Events that process the input signal are referred to as input processes, whereas events that process the production of speech are referred to as output processes. Some aspects of speech processing are thought to happen online—that is, they occur during the actual perception\nor production of speech and thus require a share of the attentional resources dedicated to the speech task. Other processes, thought to happen offline, take place as part of the child's background mental processing rather than during the time dedicated to the speech task.\nIn this sense, online processing is sometimes defined as occurring in real-time, whereas offline processing is said to be time-free (Hewlett, 1990). In box-and-arrow psycholinguistic models, each hypothesized level of representation or processing can be represented in a diagram by a “box,” and the relationships between them by “arrows,” hence the name. Sometimes (as in the models of Smith, 1973, and Menn, 1978, described later in this paper) the arrows represent processes additional to those shown in boxes. Such models make explicit the hypothesized information-\nprocessing activities carried out in a particular cognitive function (such as language), in a manner analogous to computer flowcharts that depict the processes and decisions carried out by a computer program. Box-and-arrow models differ widely in the number of unseen psychological processes they describe and thus in the number of boxes they contain. Some have only one or two boxes between the input and output signals (e.g., Menn, 1978; Smith, 1973), whereas others have multiple boxes representing complex relationships between a number of different information-processing events (e.g., Hewlett, 1990; Hewlett, Gibbon, & Cohen- McKenzie,1998; Stackhouse & Wells, 1997). The most important box, however, and the source of much ongoing debate, is that representing the underlying representation (or UR). In essence, an underlying representation captures information stored in a child's mind about a word he or she knows and uses. As the following description of several models will illustrate, the nature of this information and thus the type(s) of representation present in the child's knowledge base have captured the attention of researchers for some time. (Elise Baker et al. Psycholinguistic Models of Speech Development and Their Application to Clinical Practice. Journal of Speech, Language, and Hearing Research. June 2001. 44. p 685–702.)\n\nA computational model is a mathematical model in computational science that requires extensive computational resources to study the behavior of a complex system by computer simulation. The system under study is often a complex nonlinear system for which simple, intuitive analytical solutions are not readily available. Rather than deriving a mathematical analytical solution to the problem, experimentation with the model is done by changing the parameters of the system in the computer, and studying the differences in the outcome of the experiments. Theories of operation of the model can be derived/deduced from these computational experiments.\nExamples of common computational models are weather forecasting models, earth simulator models, flight simulator models, molecular protein folding models, and neural network models.\n. expressed in characters, usually nonnumeric, that require translation before they can be used\n\"subsymbolic\" if it is made by constituent entities that are not representations in their turn, e.g., pixels, sound images as perceived by the ear, signal samples; subsymbolic units in neural networks can be considered particular cases of this category\nHybrid computers are computers that exhibit features of analog computers and digital computers. The digital component normally serves as the controller and provides logical operations, while the analog component normally serves as a solver of differential equations. See more details at hybrid intelligent system.\nIn the traditional computational approach, representations are viewed as static structures of discrete symbols. Cognition takes place by transforming static symbol structures in discrete, sequential steps. Sensory information is transformed into symbolic inputs, which produce symbolic outputs that get transformed into motor outputs. The entire system operates in an ongoing cycle.\n\nWhat is missing from this traditional view is that human cognition happens continuously and in real time. Breaking down the processes into discrete time steps may not fully capture this behavior. An alternative approach is to define a system with (1) a state of the system at any given time, (2) a behavior, defined as the change over time in overall state, and (3) a state set or state space, representing the totality of overall states the system could be in. The system is distinguished by the fact that all of these states belong together. A change in any aspect of the system depends on other aspects of the system.\n\nA typical dynamical model is formalized by several differential equations that describe how the system's state changes over time. By doing so, the form of the space of possible trajectories and the internal and external forces that shape a specific trajectory that unfold over time, instead of the physical nature of the underlying mechanisms that manifest this dynamics, carry explanatory force. On this dynamical view, parametric inputs alter the system's intrinsic dynamics, rather than specifying an internal state that describes some external state of affairs.\n\nEarly work in the application of dynamical systems to cognition can be found in the model of Hopfield networks. These networks were proposed as a model for associative memory. They represent the neural level of memory, modeling systems of around 30 neurons which can be in either an on or off state. By letting the network learn on its own, structure and computational properties naturally arise. Unlike previous models, “memories” can be formed and recalled by inputting a small portion of the entire memory. Time ordering of memories can also be encoded. The behavior of the system is modeled with vectors which can change values, representing different states of the system. This early model was a major step toward a dynamical systems view of human cognition, though many details had yet to be added and more phenomena accounted for.\n\nBy taking into account the evolutionary development of the human nervous system and the similarity of the brain to other organs, Elman proposed that language and cognition should be treated as a dynamical system rather than a digital symbol processor. Neural networks of the type Elman implemented have come to be known as Elman networks. Instead of treating language as a collection of static lexical items and grammar rules that are learned and then used according to fixed rules, the dynamical systems view defines the lexicon as regions of state space within a dynamical system. Grammar is made up of attractors and repellers that constrain movement in the state space. This means that representations are sensitive to context, with mental representations viewed as trajectories through mental space instead of objects that are constructed and remain static. Elman networks were trained with simple sentences to represent grammar as a dynamical system. Once a basic grammar had been learned, the networks could then parse complex sentences by predicting which words would appear next according to the dynamical model.\n\nA classic developmental error has been investigated in the context of dynamical systems: The A-not-B error is proposed to be not a distinct error occurring at a specific age (8 to 10 months), but a feature of a dynamic learning process that is also present in older children. Children 2 years old were found to make an error similar to the A-not-B error when searching for toys hidden in a sandbox. After observing the toy being hidden in location A and repeatedly searching for it there, the 2-year-olds were shown a toy hidden in a new location B. When they looked for the toy, they searched in locations that were biased toward location A. This suggests that there is an ongoing representation of the toy's location that changes over time. The child's past behavior influences its model of locations of the sandbox, and so an account of behavior and learning must take into account how the system of the sandbox and the child's past actions is changing over time.\n\nOne proposed mechanism of a dynamical system comes from analysis of continuous-time recurrent neural networks (CTRNNs). By focusing on the output of the neural networks rather than their states and examining fully interconnected networks, three-neuron central pattern generator (CPG) can be used to represent systems such as leg movements during walking. This CPG contains three motor neurons to control the foot, backward swing, and forward swing effectors of the leg. Outputs of the network represent whether the foot is up or down and how much force is being applied to generate torque in the leg joint. One feature of this pattern is that neuron outputs are either off or on most of the time. Another feature is that the states are quasi-stable, meaning that they will eventually transition to other states. A simple pattern generator circuit like this is proposed to be a building block for a dynamical system. Sets of neurons that simultaneously transition from one quasi-stable state to another are defined as a dynamic module. These modules can in theory be combined to create larger circuits that comprise a complete dynamical system. However, the details of how this combination could occur are not fully worked out.\n\nModern formalizations of dynamical systems applied to the study of cognition vary. One such formalization, referred to as “behavioral dynamics”, treats the agent and the environment as a pair of coupled dynamical systems based on classical dynamical systems theory. In this formalization, the information from the environment informs the agent's behavior and the agent's actions modify the environment. In the specific case of perception-action cycles, the coupling of the environment and the agent is formalized by two functions. The first transforms the representation of the agents action into specific patterns of muscle activation that in turn produce forces in the environment. The second function transforms the information from the environment (i.e., patterns of stimulation at the agent's receptors that reflect the environment's current state) into a representation that is useful for controlling the agents actions. Other similar dynamical systems have been proposed (although not developed into a formal framework) in which the agent's nervous systems, the agent's body, and the environment are coupled together\n\nBehavioral dynamics have been applied to locomotive behavior. Modeling locomotion with behavioral dynamics demonstrates that adaptive behaviors could arise from the interactions of an agent and the environment. According to this framework, adaptive behaviors can be captured by two levels of analysis. At the first level of perception and action, an agent and an environment can be conceptualized as a pair of dynamical systems coupled together by the forces the agent applies to the environment and by the structured information provided by the environment. Thus, behavioral dynamics emerge from the agent-environment interaction. At the second level of time evolution, behavior can be expressed as a dynamical system represented as a vector field. In this vector field, attractors reflect stable behavioral solutions, where as bifurcations reflect changes in behavior. In contrast to previous work on central pattern generators, this framework suggests that stable behavioral patterns are an emergent, self-organizing property of the agent-environment system rather than determined by the structure of either the agent or the environment.\n\nIn an extension of classical dynamical systems theory, rather than coupling the environment's and the agent's dynamical systems to each other, an “open dynamical system” defines a “total system”, an “agent system”, and a mechanism to relate these two systems. The total system is a dynamical system that models an agent in an environment, whereas the agent system is a dynamical system that models an agent's intrinsic dynamics (i.e., the agent's dynamics in the absence of an environment). Importantly, the relation mechanism does not couple the two systems together, but rather continuously modifies the total system into the decoupled agent's total system. By distinguishing between total and agent systems, it is possible to investigate an agent's behavior when it is isolated from the environment and when it is embedded within an environment. This formalization can be seen as a generalization from the classical formalization, whereby the agent system can be viewed as the agent system in an open dynamical system, and the agent coupled to the environment and the environment can be viewed as the total system in an open dynamical system.\n\nIn the context of dynamical systems and embodied cognition, representations can be conceptualized as indicators or mediators. In the indicator view, internal states carry information about the existence of an object in the environment, where the state of a system during exposure to an object is the representation of that object. In the mediator view, internal states carry information about the environment which is used by the system in obtaining its goals. In this more complex account, the states of the system carries information that mediates between the information the agent takes in from the environment, and the force exerted on the environment by the agents behavior. The application of open dynamical systems have been discussed for four types of classical embodied cognition examples: \nThe interpretations of these examples rely on the following logic: (1) the total system captures embodiment; (2) one or more agent systems capture the intrinsic dynamics of individual agents; (3) the complete behavior of an agent can be understood as a change to the agent's intrinsic dynamics in relation to its situation in the environment; and (4) the paths of an open dynamical system can be interpreted as representational processes. These embodied cognition examples show the importance of studying the emergent dynamics of an agent-environment systems, as well as the intrinsic dynamics of agent systems. Rather than being at odds with traditional cognitive science approaches, dynamical systems are a natural extension of these methods and should be studied in parallel rather than in competition.\n\n\n"}
{"id": "2273689", "url": "https://en.wikipedia.org/wiki?curid=2273689", "title": "Commons", "text": "Commons\n\nThe commons is the cultural and natural resources accessible to all members of a society, including natural materials such as air, water, and a habitable earth. These resources are held in common, not owned privately. Commons can also be understood as natural resources that groups of people (communities, user groups) manage for individual and collective benefit. Characteristically, this involves a variety of informal norms and values (social practice) employed for a governance mechanism.\nCommons can be also defined as a social practice of governing a resource not by state or market but by a community of users that self-governs the resource through institutions that it creates .\nThe Digital Library of the Commons defines \"commons\" as \"a general term for shared resources in which each stakeholder has an equal interest\".\n\nThe term \"commons\" derives from the traditional English legal term for common land, which are also known as \"commons\", and was popularised in the modern sense as a shared resource term by the ecologist Garrett Hardin in an influential 1968 article called The Tragedy of the Commons. As Frank van Laerhoven and Elinor Ostrom have stated; \"Prior to the publication of Hardin's article on the tragedy of the commons (1968), titles containing the words 'the commons', 'common pool resources', or 'common property' were very rare in the academic literature.\"\n\nSome texts make a distinction in usage between common ownership of the commons and collective ownership among a group of colleagues, such as in a producers' cooperative. The precision of this distinction is not always maintained.\n\nThe use of \"commons\" for natural resources has its roots in European intellectual history, where it referred to shared agricultural fields, grazing lands and forests that were, over a period of several hundred years, enclosed, claimed as private property for private use. In European political texts, the common wealth was the totality of the material riches of the world, such as the air, the water, the soil and the seed, all nature's bounty regarded as the inheritance of humanity as a whole, to be shared together. In this context, one may go back further, to the Roman legal category \"res communis\", applied to things common to all to be used and enjoyed by everyone, as opposed to \"res publica\", applied to public property managed by the government.\n\nThe examples below illustrate types of environmental commons.\n\nOriginally in medieval England the common was an integral part of the manor, and was thus legally part of the estate in land owned by the lord of the manor, but over which certain classes of manorial tenants and others held certain rights. By extension, the term \"commons\" has come to be applied to other resources which a community has rights or access to. The older texts use the word \"common\" to denote any such right, but more modern usage is to refer to particular rights of common, and to reserve the name \"common\" for the land over which the rights are exercised. A person who has a right in, or over, common land jointly with another or others is called a \"commoner\".\n\nIn middle Europe, commons (relatively small-scale agriculture in, especially, southern Germany, Austria, and the alpine countries) were kept, in some parts, till the present. Some studies have compared the German and English dealings with the commons between late medieval times and the agrarian reforms of the 18th and 19th centuries. The UK was quite radical in doing away with and enclosing former commons, while southwestern Germany (and the alpine countries as e.g. Switzerland) had the most advanced commons structures, and were more inclined to keep them. The Lower Rhine region took an intermediate position. However, the UK and the former dominions have till today a large amount of Crown land which often is used for community or conservation purposes.\n\nBased on a research project by the Environmental and Cultural Conservation in Inner Asia (ECCIA) from 1992 to 1995, satellite images were used to compare the amount of land degradation due to livestock grazing in the regions of Mongolia, Russia, and China. In Mongolia, where shepherds were permitted to move collectively between seasonal grazing pastures, degradation remained relatively low at approximately 9%. Comparatively, Russia and China, which mandated state-owned pastures involving immobile settlements and in some cases privatization by household, had much higher degradation, at around 75% and 33% respectively. A collaborative effort on the part of Mongolians proved much more efficient in preserving grazing land.\n\nWidespread success of the Maine lobster industry is often attributed to the willingness of Maine's lobstermen to uphold and support lobster conservation rules. These rules include harbor territories not recognized by the state, informal trap limits, and laws imposed by the state of Maine (which are largely influenced by lobbying from lobster industry itself). Essentially, the lobstermen collaborate without much government intervention to sustain their common-pool resource.\n\nIn the late 1980s, Nepal chose to decentralize government control over forests. Community forest programs work by giving local areas a financial stake in nearby woodlands, and thereby increasing the incentive to protect them from overuse. Local institutions regulate harvesting and selling of timber and land, and must use any profit towards community development and preservation of the forests. In twenty years, locals have noticed a visible increase in the number of trees. Community forestry may also contribute to community development in rural areas – for instance school construction, irrigation and drinking water channel construction, and road construction. Community forestry has proven conducive to democratic practices at grass roots level.\n\n\"Acequia\" is a method of collective responsibility and management for irrigation systems in desert areas. In New Mexico, a community-run organization known as Acequia Associations supervises water in terms of diversion, distribution, utilization, and recycling, in order to reinforce agricultural traditions and preserve water as a common resource for future generations.\n\nToday, the commons are also understood within a cultural sphere. These commons include literature, music, arts, design, film, video, television, radio, information, software and sites of heritage. Wikipedia is an example of the production and maintenance of common goods by a contributor community in the form of encyclopedic knowledge that can be freely accessed by anyone without a central authority.\n\nTragedy of the commons in the Wiki-Commons is avoided by community control by individual authors within the Wikipedia community.\n\nThe information commons may help protect users of commons. Companies that pollute the environment release information about what they are doing. The Corporate Toxics Information Project and information like the Toxic 100, a list of the top 100 polluters, helps people know what these corporations are doing to the environment.\n\nMayo Fuster Morell proposed a definition of digital commons as \"information and knowledge resources that are collectively created and owned or shared between or among a community and that tend to be non-exclusive, that is, be (generally freely) available to third parties. Thus, they are oriented to favor use and reuse, rather than to exchange as a commodity. Additionally, the community of people building them can intervene in the governing of their interaction processes and of their shared resources.\"\n\nExamples of digital commons are Wikipedia, free software and open-source hardware projects.\n\nUrban commons present the opportunity for the citizens to gain power upon the management of the urban resources and reframe city-life costs based on their use value and maintenance costs, rather than the market-driven value. \n\nUrban commons situates citizens as key players rather than public authorities, private markets and technologies. David Harvey (2012) defines the distinction between public spaces and urban commons. Public spaces and goods in the city make a commons when part of the citizens take political action. Syntagma Square in Athens, Tahrir Square in Cairo, and the Plaza de Catalunya in Barcelona were public spaces that transformed to an urban commons as people protested there to support their political statements. Streets are public spaces that have often become an urban commons by social action and revolutionary protests.. Urban commons are operating in the cities in a complementary way with the state and the market. Some examples are community gardening, urban farms on the rooftops and cultural spaces. More recently participatory studies of commons and infrastructures under the conditions of the financial crisis emerge .\n\nIn 2007, Elinor Ostrom along with her colleague Charlotte Hess, did succeed in extending the commons debate to knowledge, approaching knowledge as a complex ecosystem that operates as a common – a shared resource that is subject to social dilemmas. The focus here was on the ready availability of digital forms of knowledge and associated possibilities to store, access and share it as a common. The connection between knowledge and commons may be made through identifying typical problems associated with natural resource commons, such as congestion, overharvesting, pollution and inequities, which also apply to knowledge. Then, effective alternatives (community-based, non-private, non-state), in line with those of natural commons (involving social rules, appropriate property rights and management structures), solutions are proposed. Thus, the commons metaphor is applied to social practice around knowledge. It is in this context that the present work proceeds, discussing the creation of depositories of knowledge through the organised, voluntary contributions of scholars (the research community, itself a social common), the problems that such knowledge commons might face (such as free-riding or disappearing assets), and the protection of knowledge commons from enclosure and commodification (in the form of intellectual property legislation, patenting, licensing and overpricing). At this point, it is important to note the nature of knowledge and its complex and multi-layered qualities of non-rivalry and non-excludability. Unlike natural commons – which are both rival and excludable (only one person can use any one item or portion at a time and in so doing they use it up, it is consumed) and characterised by scarcity (they can be replenished but there are limits to this, such that consumption/destruction may overtake production/creation) – knowledge commons are characterised by abundance (they are non-rival and non-excludable and thus, in principle, not scarce, so not impelling competition and compelling governance). This abundance of knowledge commons has been celebrated through alternative models of knowledge production, such as Commons Based Peer Production (CBPP), and embodied in the free software movement. The CBPP model showed the power of networked, open collaboration and non-material incentives to produce better quality products (mainly software).\n\nA commons failure theory, now called tragedy of the commons, originated in the 18th century. In 1833 William Forster Lloyd introduced the concept by a hypothetical example of herders overusing a shared parcel of land on which they are each entitled to let their cows graze, to the detriment of all users of the common land. The same concept has been called the \"tragedy of the fishers\", when over-fishing could cause stocks to plummet.\n\nIt has been said the dissolution of the traditional land commons played a watershed role in landscape development and cooperative land use patterns and property rights. However, as in the British Isles, such changes took place over several centuries as a result of land enclosure.\n\nEconomist Peter Barnes has proposed a 'sky trust' to fix this tragedic problem in worldwide generic commons. He claims that the sky belongs to all the people, and companies do not have a right to over pollute. It is a type of cap and dividend program. Ultimately the goal would be to make polluting excessively more expensive than cleaning what is being put into the atmosphere.\n\nWhile the original work on the tragedy of the commons concept suggested that all commons were doomed to failure, they remain important in the modern world. Work by later economists has found many examples of successful commons, and Elinor Ostrom won the Nobel prize for analysing situations where they operate successfully. For example, Ostrom found that grazing commons in the Swiss Alps have been run successfully for many hundreds of years by the farmers there.\n\nAllied to this is the \"comedy of the commons\" concept, where users of the commons are able to develop mechanisms to police their use to maintain, and possibly improve, the state of the commons. This term was coined in an essay by legal scholar, Carol M. Rose, in 1986.\n\nOther related concepts are the \"inverse commons\", \"cornucopia of the commons\", and \"triumph of the commons\". It is argued that some types of commons, such as open-source software, work better in the cornucopia of the commons; proponents say that, in those cases, \"the grass grows taller when it is grazed on\".\n\n\n\n"}
{"id": "48786651", "url": "https://en.wikipedia.org/wiki?curid=48786651", "title": "Communication-avoiding algorithms", "text": "Communication-avoiding algorithms\n\nCommunication-Avoiding Algorithms minimize movement of data within a memory hierarchy for improving its running-time and energy consumption. These minimize the total of two costs (in terms of time and energy): arithmetic and communication. Communication, in this context refers to moving data, either between levels of memory or between multiple processors over a network. It is much more expensive than arithmetic.\n\nConsider the following running-time model:\n⇒ Total running time = γ*(no. of FLOPs) + β*(no. of words)\n\nFrom the fact that β » γ as measured in time and energy, communication cost dominates computation cost. Technological trends indicate that the relative cost of communication is increasing on a variety of platforms, from cloud computing to supercomputers to mobile devices. The report also predicts that gap between DRAM access time and FLOPs will increase 100x over coming decade to balance power usage between processors and DRAM.\n\nEnergy consumption increases by orders of magnitude as we go higher in the memory hierarchy. United States president Barack Obama cited Communication-Avoiding Algorithms in the FY 2012 Department of Energy budget request to Congress: \"“New Algorithm Improves Performance and Accuracy on Extreme-Scale Computing Systems. On modern computer architectures, communication between processors takes longer than the performance of a floating point arithmetic operation by a given processor. ASCR researchers have developed a new method, derived from commonly used linear algebra methods, to minimize communications between processors and the memory hierarchy, by reformulating the communication patterns specified within the algorithm. This method has been implemented in the TRILINOS framework, a highly-regarded suite of software, which provides functionality for researchers around the world to solve large scale, complex multi-physics problems.”\"\n\nCommunication-Avoiding algorithms are designed with the following objectives:\n\nThe following simple example demonstrates how these are achieved.\n\nLet A, B and C be square matrices of order n x n. The following naive algorithm implements C = C + A * B:\n\nArithmetic cost (time-complexity): n² (2n-1) for sufficiently large n or O(n³).\n\nRewriting this algorithm with communication cost labelled at each step\n\nFast memory may be defined as the local processor memory (CPU cache) of size M and slow memory may be defined as the DRAM.\n\nCommunication cost (reads/writes): n³ + 3n² or O(n³)\n\nSince total running time = γ*O(n³) + β*O(n³) and β » γ the communication cost is dominant. The Blocked (Tiled) Matrix Multiplication algorithm reduces this dominant term.\n\nConsider A,B,C to be n/b-by-n/b matrices of b-by-b sub-blocks where b is called the block size; assume 3 b-by-b blocks fit in fast memory.\n\nCommunication cost: 2n³/b + 2n² reads/writes « 2n³ arithmetic cost\n\nMaking b as large possible:\n3b ≤ M \nWe achieve the following communication lowerbound:\n3n/M + 2n or Ω(no. of FLOPs / M )\n\nMost of the approaches investigated in the past to address this problem rely on scheduling or tuning techniques that aim at overlapping communication with computation. However, this approach can lead to an improvement of at most a factor of two. Ghosting is a different technique for reducing communication, in which a processor stores and computes redundantly data from neighboring processors for future computations. Cache-oblivious algorithms represent a different approach introduced in 1999 for Fast Fourier Transforms, and then extended to graph algorithms, dynamic programming, etc. They were also applied to several operations in linear algebra as dense LU and QR factorizations. The design of architecture specific algorithms is another approach that can be used for reducing the communication in parallel algorithms, and there are many examples in the literature of algorithms that are adapted to a given communication topology.\n"}
{"id": "650751", "url": "https://en.wikipedia.org/wiki?curid=650751", "title": "Complete Heyting algebra", "text": "Complete Heyting algebra\n\nIn mathematics, especially in order theory, a complete Heyting algebra is a Heyting algebra that is complete as a lattice. Complete Heyting algebras are the objects of three different categories; the category CHey, the category Loc of locales, and its opposite, the category Frm of frames. Although these three categories contain the same objects, they differ in their morphisms, and thus get distinct names. Only the morphisms of CHey are homomorphisms of complete Heyting algebras.\n\nLocales and frames form the foundation of pointless topology, which, instead of building on point-set topology, recasts the ideas of general topology in categorical terms, as statements on frames and locales.\n\nConsider a partially ordered set (\"P\", ≤) that is a complete lattice. Then \"P\" is a \"complete Heyting algebra\" if any of the following equivalent conditions hold:\n\nThe system of all open sets of a given topological space ordered by inclusion is a complete Heyting algebra.\n\nThe objects of the category CHey, the category Frm of frames and the category Loc of locales are the complete lattices satisfying the infinite distributive law. These categories differ in what constitutes a morphism.\n\nThe morphisms of Frm are (necessarily monotone) functions that preserve finite meets and arbitrary joins. Such functions are not homomorphisms of complete Heyting algebras. The definition of Heyting algebras crucially involves the existence of right adjoints to the binary meet operation, which together define an additional implication operation ⇒. Thus, a \"homomorphism of complete Heyting algebras\" is a morphism of frames that in addition preserves implication. The morphisms of Loc are opposite to those of Frm, and they are usually called maps (of locales).\n\nThe relation of locales and their maps to topological spaces and continuous functions may be seen as follows. Let\nbe any map. The power sets \"P\"(\"X\") and \"P\"(\"Y\") are complete Boolean algebras, and the map\nis a homomorphism of complete Boolean algebras. Suppose the spaces \"X\" and \"Y\" are topological spaces, endowed with the topology \"O\"(\"X\") and \"O\"(\"Y\") of open sets on \"X\" and \"Y\". Note that \"O\"(\"X\") and \"O\"(\"Y\") are subframes of \"P\"(\"X\") and \"P\"(\"Y\"). If \"ƒ\" is a continuous function, then\npreserves finite meets and arbitrary joins of these subframes. This shows that \"O\" is a functor from the category Top of topological spaces to the category Loc of locales, taking any continuous map\nto the map\nin Loc that is defined in Frm to be the inverse image frame homomorphism\nIt is common, given a map of locales\nin Loc, to write\nfor the frame homomorphism that defines it in Frm. Hence, using this notation, \"O\"(\"ƒ\") is defined by the equation \n\nConversely, any locale \"A\" has a topological space \"S\"(\"A\") that best approximates the locale, called its \"spectrum\". In addition, any map of locales\ndetermines a continuous map\nand this assignment is functorial: letting \"P\"(1) denote the locale that is obtained as the powerset of the terminal set the points of \"S\"(\"A\") are the maps\nin Loc, i.e., the frame homomorphisms\nFor each we define the set that consists of the points such that It is easy to verify that this defines a frame homomorphism whose image is therefore a topology on \"S\"(\"A\"). Then, if\nto each point we assign the point \"S\"(\"ƒ\")(\"q\") defined by letting \"S\"(\"ƒ\")(p)* be the composition of \"p\"* with \"ƒ\"*, hence obtaining a continuous map\n\nThis defines a functor formula_17 from Loc to Top, which is right adjoint to \"O\".\n\nAny locale that is isomorphic to the topology of its spectrum is called \"spatial\", and any topological space that is homeomorphic to the spectrum of its locale of open sets is called \"sober\". The adjunction between topological spaces and locales restricts to an equivalence of categories between sober spaces and spatial locales.\n\nAny function that preserves all joins (and hence any frame homomorphism) has a right adjoint, and, conversely, any function that preserves all meets has a left adjoint. Hence, the category Loc is isomorphic to the category whose objects are the frames and whose morphisms are the meet preserving functions whose left adjoints preserve finite meets. This is often regarded as a representation of Loc, but it should not be confused with Loc itself, whose morphisms are formally the same as frame homomorphisms in the opposite direction.\n\n\n\n\n"}
{"id": "56286411", "url": "https://en.wikipedia.org/wiki?curid=56286411", "title": "Concepts of magic per society", "text": "Concepts of magic per society\n\nThe ancient Mesopotamians believed that magic was the only viable defense against demons, ghosts, and evil sorcerers. To defend themselves against the spirits of those they had wronged, they would leave offerings known as \"kispu\" in the person's tomb in hope to appease them. If that did not work, they also sometimes took a figurine of the deceased and buried it in the ground, demanding for the gods to eradicate the spirit, or force it to leave the person alone.\n\nThe ancient Mesopotamians also used magic to protect themselves from evil sorcerers who might place curses on them. They had no distinction between \"light magic\" and \"black magic\" and a person defending him or herself from witchcraft would use exactly the same techniques as the person trying to curse someone. The only major difference was the fact that curses were enacted in secret; whereas a defense against sorcery was conducted in the open, in front of an audience if possible. One ritual to punish a sorcerer was known as Maqlû, or \"The Burning\". The person afflicted by the witchcraft would create an effigy of the sorcerer and put it on trial at night. Then, once the nature of the sorcerer’s crimes had been determined, the person would burn the effigy and thereby break the sorcerer’s power over him or her.\n\nThe ancient Mesopotamians also performed magical rituals to purify themselves of sins committed unknowingly. One such ritual was known as the Šurpu, or \"Burning\", in which the caster of the spell would transfer the guilt for all his or her misdeeds onto various objects such as a strip of dates, an onion, and a tuft of wool. He or she would then burn the objects and thereby purify him or herself of all sins that he or she might have unknowingly committed. A whole genre of love spells existed. Such spells, which usually invoked the aid of the goddess Ishtar, were believed to cause a person to fall in love with another person, restore love which had faded, or cause a male sexual partner to be able to sustain an erection when he had previously been unable. Other spells were used to reconcile a man with his patron deity or to reconcile a wife with a husband who had been neglecting her.\n\nThe ancient Mesopotamians had no distinction between \"rational science\" and magic. When a person became ill, doctors would proscribe both magical formulas to be recited as well as medicinal treatments. Most magical rituals were intended to be performed by an \"āšipu\", an expert in the magical arts. The profession was generally passed down from father to son and was held in extremely high regard and often served as advisors to kings and great leaders. An āšipu probably served not only as a magician, but also as a physician, a priest, a scribe, and a scholar. He would have likely owned a large library of clay tablets containing important religious texts and hymns.\n\nThe Sumerian god Enki, who was later syncretized with the East Semitic god Ea, was closely associated with magic and incantations; he was the patron god of the \"bārȗ\" and the \"ašipū\" and was widely regarded as the ultimate source of all arcane knowledge. The ancient Mesopotamians also believed in omens, which could come when solicited or unsolicited. Regardless of how they came, omens were always taken with the utmost seriousness. The Mesopotamians also invented astrology.\n\nMagic was an integral part of ancient Egyptian religion and culture. The ancient Egyptians would often wear magical amulets, known as \"meket\", for protection. The most common material for such amulets was a kind of ceramic known as faience, but amulets were also made of stone, metal, bone, and wood. Amulets depicted specific symbols. One of the most common protective symbols was the Eye of Horus, which represented the new eye given to Horus by the god Thoth as a replacement for his old eye, which had been destroyed during a battle with Horus’s uncle Seth. The most popular amulet was the scarab beetle, the emblem of the god Khepri. Pregnant women would wear amulets depicting Tauret, the goddess of childbirth, to protect against miscarriage. The god Bes, who had the head of a lion and the body of a dwarf, was believed to be the protector of children. After giving birth, a mother would remove her Tauret amulet and put on a new amulet representing Bes.\nLike the Mesopotamians, the ancient Egyptians had no distinction between magic and medicine. The Egyptians believed that diseases stemmed from supernatural origins and ancient Egyptian doctors would prescribe both magical and practical remedies to their patients. Doctors would interrogate their patients to find out what ailments the person was suffering from. The symptoms of the disease determined which deity the doctor needed to invoke in order to cure it. \nDoctors were extremely expensive, so, for most everyday purposes, the average Egyptian would have relied on individuals who were not professional doctors, but who possessed some form of medical training or knowledge. Among these individuals were folk healers and seers, who could set broken bones, aid mothers in giving birth, proscribe herbal remedies for common ailments, and interpret dreams. If a doctor or seer was unavailable, then everyday people would simply cast their spells on their own without assistance. Although most Egyptians were illiterate, it was likely commonplace for individuals to memorize spells and incantations for later use.\n\nThe main principle behind Egyptian magic seems to have been the notion that, if a person said something with enough conviction, the statement would automatically become true. The interior walls of the pyramid of Unas, the final pharaoh of the Egyptian Fifth Dynasty, are covered in hundreds of magical spells and inscriptions, running from floor to ceiling in vertical columns. These inscriptions are known as the \"Pyramid Texts\" and they contain spells needed by the pharaoh in order to survive in the Afterlife. The Pyramid Texts were strictly for royalty only; the spells were kept secret from commoners and were written only inside royal tombs. During the chaos and unrest of the First Intermediate Period, however, tomb robbers broke into the pyramids and saw the magical inscriptions. Commoners began learning the spells and, by the beginning of the Middle Kingdom, commoners began inscribing similar writings on the sides of their own coffins, hoping that doing so would ensure their own survival in the Afterlife. These writings are known as the \"Coffin Texts\". \n\nEventually, the Coffin Texts became so extensive that they no longer fit on the outside of a coffin. They began to be instead recorded on scrolls of papyrus, which would then be placed inside the coffin with the deceased’s own corpse. The writings on these scrolls are now known as \"The Book of the Dead\". There were hundreds of different versions of \"The Book of the Dead\", all of them containing different spells. Egyptologists have identified more than four hundred different spells belonging to \"The Book of the Dead\" collectively. Egyptologists have codified and classified these spells, assigning them specific numbers based on their content and purpose.\n\nAs \"The Book of the Dead\" became more popular, a whole industry of scribes arose for the sole purpose of copying manuscripts so that customers would be able to buy copies of the spells to be buried with them in their tombs. The quality of manuscripts was highly variable. Some editions were ninety feet long and contained beautiful, color illustrations to illuminate the text; others were short with no illustrations whatsoever. The scrolls were copied before they were bought, meaning that the name of the owner was unknown. As such, the scribes would leave the places for the person’s name blank and fill in the person's name after the scroll was purchased. Sometimes scribes would accidentally misread or miscopy what they were writing. Sometimes the spells would be abbreviated in order to avoid running out of space. Such mistakes could render the texts unintelligible. \n\nAfter a person died, his or her corpse would be mummified and wrapped in linen bandages in order to ensure that the deceased's body would survive for as long as possible because the Egyptians believed that a person's soul could only survive in the Afterlife for as long as his or her physical body survived here on earth. The last ceremony before a person's body was sealed away inside the tomb was known as the \"Opening of the Mouth\". In this ritual, the priests would touch various magical instruments to various parts of the deceased's body, thereby giving the deceased the ability to see, hear, taste, and smell in the Afterlife.\n\nBefore dead person was buried, his or her mummified corpse would be stuffed full of magic amulets and protective charms in order to ensure that he or she would be safe in the next world. The family would also place important grave goods inside the person’s tomb in order to ensure that he or she had everything he or she would need in the next life. Among these grave goods were small figurines made of faience or wood known as \"shabti\". The shabti were intended as slaves for the deceased. The ancient Egyptians believed that physical labor was just as necessary in the Afterlife as it was in the present one. As such, they believed that the deceased could cast a spell to animate these figurines so that he or she would be able to order them to perform tasks and chores in the Afterlife so that the deceased him or herself would not be forced to perform any labor.\n\nAncient Greek scholarship of the 20th century, almost certainly influenced by Christianising preconceptions of the meanings of magic and religion, and the wish to establish Greek culture as the foundation of Western rationality, developed a theory of ancient Greek magic as primitive and insignificant, and thereby essentially separate from Homeric, communal (\"polis\") religion. Since the last decade of the century, however, recognising the ubiquity and respectability of acts such as \"katadesmoi\" (\"binding spells\"), described as magic by modern and ancient observers alike, scholars have been compelled to abandon this viewpoint. The Greek word \"mageuo\" (\"practise magic\") itself derives from the word \"Magos\", originally simply the Greek name for a Persian tribe known for practising religion. Non-civic \"mystery cults\" have been similarly re-evaluated:\n\n\" Katadesmoi\" (Latin: \" defixiones)\"), curses inscribed on wax or lead tablets and buried underground, were frequently executed by all strata of Greek society, sometimes to protect the entire \"polis\". Communal curses carried out in public declined after the Greek classical period, but private curses remained common throughout antiquity. They were distinguished as magical by their individualistic, instrumental and sinister qualities. These qualities, and their perceived deviation from inherently mutable cultural constructs of normality, most clearly delineate ancient magic from the religious rituals of which they form a part.\n\nA large number of magical papyri, in Greek, Coptic, and Demotic, have been recovered and translated. They contain early instances of:\n\nThe practice of magic was banned in the late Roman world, and the \"Codex Theodosianus\" (438 AD) states: If any wizard therefore or person imbued with magical contamination who is called by custom of the people a magician...should be apprehended in my retinue, or in that of the Caesar, he shall not escape punishment and torture by the protection of his rank.\n\nArs Magica or magic is a major component and supporting contribution to the belief and practice of spiritual, and in many cases, physical healing throughout the Middle Ages. Emanating from many modern interpretations lies a trail of misconceptions about magic, one of the largest revolving around wickedness or the existence of nefarious beings who practice it. These misinterpretations stem from numerous acts or rituals that have been performed throughout antiquity, and due to their exoticism from the commoner's perspective, the rituals invoked uneasiness and an even stronger sense of dismissal.\n\nOne societal force in the Middle Ages more powerful than the singular commoner, the Christian Church, rejected magic as a whole because it was viewed as a means of tampering with the natural world in a supernatural manner associated with the biblical verses of Deuteronomy 18:9-12. Despite the many negative connotations which surround the term magic, there exist many elements that are seen in a divine or holy light.\n\nThe various yet sparse healers of the Middle Ages were among the few, if not the only, proponents of a positive impression of magic. One of the most famous healers of this time was Saint Hildegard of Bingen. Her healing abilities were so sought after that many individuals, healthy and ill alike, would travel great distances to be blessed by her.\n\nModern historians of medicine along with the people of the Middle Ages both possess no straightforward answer as to where her abilities derived from; however, many of these historians argue or speculate that they are related to mental visions of which recorded documents, such as her three volumes of visionary theology, depict. The volumes include: Scivias, (\"Know the Ways\"), Liber Vitae Meritorum, (\"Book of Life's Merits\"), and Liber Divinorum Operum (\"Book of Divine Works\").\n\nDiversified instruments or rituals used in medieval magic include, but are not limited to: various amulets, talismans, potions, as well as specific chants, dances, prayers. Along with these rituals are the adversely imbued notions of demonic participation which influence of them. The idea that magic was devised, taught, and worked by demons would have seemed reasonable to anyone who read the Greek magical papyri or the Sefer-ha-Razim and found that healing magic appeared alongside rituals for killing people, gaining wealth, or personal advantage, and coercing women into sexual submission. Archaeology is contributing to a fuller understanding of ritual practices performed in the home, on the body and in monastic and church settings.\n\nThe Islamic reaction towards magic did not condemn magic in general and distinguished between magic which can heal sickness and possession, and sorcery. Magic is therefore a \"special gift from God\", while the latter is achieved through help of Jinn and devils. Ibn al-Nadim hold, Exorcists gain their power by their obedience to God, while sorcerers please the devils by acts of disobidience and sacrifices and they in return do him a favor. According to Ibn Arabi Al-Ḥajjāj ibn Yusuf al-Shubarbuli was due to his piety able to walk on water. Based on the Quran, regarding Islamic legends of Solomon, magic was taught by devils to the humans. Solomon took the writings of the sorcerer away and hid them under his throne. After his death, Iblis, unable to get close to Solomons court, told the people, they will find a treasure under the throne and thus lead them to sorcery. Another account hold, sorcery came with the fallen angels Harut and Marut to mankind.\n\nRenaissance humanism saw a resurgence in hermeticism and Neo-Platonic varieties of ceremonial magic. The Renaissance, on the other hand, saw the rise of science, in such forms as the dethronement of the Ptolemaic theory of the universe, the distinction of astronomy from astrology, and of chemistry from alchemy.\n\nThere was great uncertainty in distinguishing practices of superstition, occultism, and perfectly sound scholarly knowledge or pious ritual. The intellectual and spiritual tensions erupted in the Early Modern witch craze, further reinforced by the turmoil of the Protestant Reformation, especially in Germany, England, and Scotland.\n\nSorcery is a legal concept in Papua New Guinea law, which differentiates between legal good magic, such as healing and fertility, and illegal black magic, held responsible for unexplained deaths.\n"}
{"id": "361449", "url": "https://en.wikipedia.org/wiki?curid=361449", "title": "Descent (mathematics)", "text": "Descent (mathematics)\n\nIn mathematics, the idea of descent extends the intuitive idea of 'gluing' in topology. Since the topologists' glue is the use of equivalence relations on topological spaces, the theory starts with some ideas on identification.\n\nThe case of the construction of vector bundles from data on a disjoint union of topological spaces is a straightforward place to start.\n\nSuppose \"X\" is a topological space covered by open sets \"X\". Let \"Y\" be the disjoint union of the \"X\", so that there is a natural mapping\n\nWe think of \"Y\" as 'above' \"X\", with the \"X\" projection 'down' onto \"X\". With this language, \"descent\" implies a vector bundle on \"Y \"(so, a bundle given on each \"X\"), and our concern is to 'glue' those bundles \"V\", to make a single bundle \"V\" on X. What we mean is that \"V\" should, when restricted to \"X\", give back \"V\", up to a bundle isomorphism.\n\nThe data needed is then this: on each overlap\n\nintersection of \"X\" and \"X\", we'll require mappings\n\nto use to identify \"V\" and \"V\" there, fiber by fiber. Further the \"f\" must satisfy conditions based on the reflexive, symmetric and transitive properties of an equivalence relation (gluing conditions). For example, the composition\n\nfor transitivity (and choosing apt notation). The \"f\" should be identity maps and hence symmetry becomes formula_5 (so that it is fiberwise an isomorphism).\n\nThese are indeed standard conditions in fiber bundle theory (see transition map). One important application to note is \"change of fiber\": if the \"f\" are all you need to make a bundle, then there are many ways to make an associated bundle. That is, we can take essentially same \"f\", acting on various fibers.\n\nAnother major point is the relation with the chain rule: the discussion of the way there of constructing tensor fields can be summed up as 'once you learn to descend the tangent bundle, for which transitivity is the Jacobian chain rule, the rest is just 'naturality of tensor constructions'.\n\nTo move closer towards the abstract theory we need to interpret the disjoint union of the\n\nnow as\n\nthe fiber product (here an equalizer) of two copies of the projection p. The bundles on the \"X\" that we must control are \"V\"′ and \"V\"\", the pullbacks to the fiber of \"V\" via the two different projection maps to \"X\".\n\nTherefore, by going to a more abstract level one can eliminate the combinatorial side (that is, leave out the indices) and get something that makes sense for \"p\" not of the special form of covering with which we began. This then allows a category theory approach: what remains to do is to re-express the gluing conditions.\n\nThe ideas were developed in the period 1955–1965 (which was roughly the time at which the requirements of algebraic topology were met but those of algebraic geometry were not). From the point of view of abstract category theory the work of comonads of Beck was a summation of those ideas; see Beck's monadicity theorem.\n\nThe difficulties of algebraic geometry with passage to the quotient are acute. The urgency (to put it that way) of the problem for the geometers accounts for the title of the 1959 Grothendieck seminar \"TDTE\" on \"theorems of descent and techniques of existence\" (see FGA) connecting the descent question with the representable functor question in algebraic geometry in general, and the moduli problem in particular.\n\nLet formula_8. Each sheaf \"F\" on \"X\" gives rise to a descent data:\nwhere formula_10 satisfies the cocycle condition:\n\nThe fully faithful descent says: formula_12 is fully faithful. The descent theory tells conditions for which there is a fully faithful descent.\n\n\n\nOther possible sources include:\n\n"}
{"id": "18181188", "url": "https://en.wikipedia.org/wiki?curid=18181188", "title": "Dualism (politics)", "text": "Dualism (politics)\n\nIn Dutch politics the term dualism is used to refer to the separation of powers between the cabinet and parliament. In this respect, the way the Dutch cabinets function is somewhat semi-presidential in its system of government, not parliamentary. Unlike the presidential system, the legislative branch consists of the cabinet together with the parliament and cabinets are formed on basis of a majority in parliament. Unlike the Westminster parliamentary system, cabinet ministers cannot be members of parliament. An important political issue is whether ministers and leaders of governing parliamentary parties should prepare important political decisions. According to the dualistic position, members of parliament of governing parties should function independently of their cabinet. The term monism is used to refer to a stance that important decisions should be prepared by the members of the governing coalition in order to promote political stability.\n\nIn the context of the history of the Austro-Hungarian Empire, \"dualism\" refers to the political doctrine of Austria's and Hungary's co-equality. The phrase \"during dualism\" () is used in Hungarian historiography as shorthand for \"during the dual monarchy.\"\n"}
{"id": "39865431", "url": "https://en.wikipedia.org/wiki?curid=39865431", "title": "Earning to give", "text": "Earning to give\n\nEarning to give involves deliberately pursuing a high-earning career for the purpose of donating a significant portion of earned income, typically because of a belief in effective altruism.\n\nAdvocates of earning to give sometimes suggest that maximizing the amount one can donate to charity is an important consideration for individuals when deciding what career to pursue, even if the individual has less intrinsic interest in high-earning careers.\n\nIn the 1996 book \"Living High and Letting Die\", the philosopher Peter Unger wrote that it was morally praiseworthy and perhaps even morally required for people in academia who could earn substantially greater salaries in the business world to leave academia, earn the greater salaries, and donate most of the extra money to charity. Moral philosopher Peter Singer has laid the foundations for effective altruism and earning to give in his 1971 essay \"\"Famine, Affluence and Morality\"\" and since advocated for donating considerable amounts of one's income to effective charitable organizations. Singer is a public proponent of effective altruism and endorsed earning to give in his 2013 TED talk. Associate Professor in Philosophy at Oxford University William MacAskill promoted earning to give as one possible high impact career in several news articles and in his 2015 book \"Doing Good Better: Effective Altruism and a Radical New Way to Make a Difference\". MacAskill is the co-founder and president of 80,000 Hours, a nonprofit which conducts research on careers with positive social impact and provides career advice. The organization recommends earning to give as a career path with a high impact potential for effective altruists.\n\nMany of the people who practice earning to give consider themselves to be part of the effective altruism community. Some donate up to 50% of their income, more than the 10% required for the basic Giving What We Can pledge. They may live frugally to donate more money. Financial careers are popular for those pursuing earning to give.\n\nDavid Brooks criticized the concept in his \"New York Times\" opinion column, arguing that, while altruists may start doing \"earning to give\" to realize their deepest commitments, their values may erode over time, becoming progressively less altruistic. In addition, Brooks objected to the view on which altruists should turn themselves \"into a machine for the redistribution of wealth.\" Peter Singer responded to these criticisms in his book \"The Most Good You Can Do\" by giving examples of people who have been earning to give for years without losing their altruistic motivation. William MacAskill also defended the practice against Brooks' criticisms in \"The Washington Post,\" arguing that even Friedrich Engels was earning to give to support the work of anti-capitalist Karl Marx financially. Dana Goldstein has also criticized earning to give, prompting a response from Reihan Salam.\n\nEarning to give has been discussed in a number of news and media outlets including \"BBC News\", \"Quartz\", the \"Washington Post\", the \"New York Times\", \"The Atlantic\", \"The Guardian\", and \"Aeon\" Magazine.\n"}
{"id": "38521194", "url": "https://en.wikipedia.org/wiki?curid=38521194", "title": "Entropic value at risk", "text": "Entropic value at risk\n\nIn financial mathematics and stochastic optimization, the concept of risk measure is used to quantify the risk involved in a random outcome or risk position. Many risk measures have hitherto been proposed, each having certain characteristics. The entropic value-at-risk (EVaR) is a coherent risk measure introduced by Ahmadi-Javid, which is an upper bound for the value at risk (VaR) and the conditional value-at-risk (CVaR), obtained from the Chernoff inequality. The EVaR can also be represented by using the concept of relative entropy. Because of its connection with the VaR and the relative entropy, this risk measure is called \"entropic value-at-risk\". The EVaR was developed to tackle some computational inefficiencies of the CVaR. Getting inspiration from the dual representation of the EVaR, Ahmadi-Javid developed a wide class of coherent risk measures, called g-entropic risk measures. Both the CVaR and the EVaR are members of this class.\n\nLet formula_1 be a probability space with formula_2 a set of all simple events, formula_3 a formula_4-algebra of subsets of formula_2 and formula_6 a probability measure on formula_3. Let formula_8 be a random variable and formula_9 be the set of all Borel measurable functions formula_10 whose moment-generating function formula_11 exists for all formula_12. The entropic value-at-risk (EVaR) of formula_13 with confidence level formula_14 is defined as follows:\n\nIn finance, the random variable formula_15 in the above equation, is used to model the \"losses\" of a portfolio.\n\nConsider the Chernoff inequality\n\nSolving the equation formula_16 for formula_17 results in \n\nBy considering the equation (), we see that \n\nwhich shows the relationship between the EVaR and the Chernoff inequality. It is worth noting that formula_20 is the \"entropic risk measure\" or \"exponential premium\", which is a concept used in finance and insurance, respectively.\n\nLet formula_21 be the set of all Borel measurable functions formula_22 whose moment-generating function formula_11 exists for all formula_24. The dual representation (or robust representation) of the EVaR is as follows:\n\nwhere formula_25 and formula_26 is a set of probability measures on formula_27 with formula_28. Note that \n\nis the relative entropy of formula_30 with respect to formula_31 also called the Kullback–Leibler divergence. The dual representation of the EVaR discloses the reason behind its naming.\n\n\n\n\nFor formula_52\n\nFor formula_53\n\nFigures 1 and 2 show the comparing of the VaR, CVaR and EVaR for formula_54 and formula_55.\n\nLet formula_56 be a risk measure. Consider the optimization problem\n\nwhere formula_57 is an formula_58-dimensional real decision vector, formula_59 is an formula_60-dimensional real random vector with a known probability distribution and the function formula_61 is a Borel measurable function for all values formula_62 If formula_63 then the optimization problem () turns into:\n\nLet formula_64 be the support of the random vector formula_65 If formula_66 is convex for all formula_67, then the objective function of the problem () is also convex. If formula_68 has the form\n\nand formula_69 are independent random variables in formula_70, then () becomes\n\nwhich is computationally tractable. But for this case, if one uses the CVaR in problem (), then the resulting problem becomes as follows:\n\nIt can be shown that by increasing the dimension of formula_71, problem () is computationally intractable even for simple cases. For example, assume that formula_69 are independent discrete random variables that take formula_73 distinct values. For fixed values of formula_74 and formula_75 the complexity of computing the objective function given in problem () is of order formula_76 while the computing time for the objective function of problem () is of order formula_77. For illustration, assume that formula_78 and the summation of two numbers takes formula_79 seconds. For computing the objective function of problem () one needs about formula_80 years, whereas the evaluation of objective function of problem () takes about formula_81 seconds. This shows that formulation with the EVaR outperforms the formulation with the CVaR (see for more details).\n\nDrawing inspiration from the dual representation of the EVaR given in (), one can define a wide class of information-theoretic coherent risk measures, which are introduced in. Let formula_82 be a convex proper function with formula_83 and formula_84 be a non-negative number. The formula_82-entropic risk measure with divergence level formula_84 is defined as\n\nwhere formula_87 in which formula_88 is the generalized relative entropy of formula_30 with respect to formula_6. A primal representation of the class of formula_82-entropic risk measures can be obtained as follows:\n\nwhere formula_92 is the conjugate of formula_82. By considering\n\nwith formula_94 and formula_95, the EVaR formula can be deduced. The CVaR is also a formula_82-entropic risk measure, which can be obtained from () by setting\n\nwith formula_97 and formula_98 (see for more details).\n\nFor more results on formula_82-entropic risk measures see.\n\n\n"}
{"id": "58573510", "url": "https://en.wikipedia.org/wiki?curid=58573510", "title": "Ernst Roets", "text": "Ernst Roets\n\nErnst Roets is a political activist, writer and filmmaker in South Africa. He is Deputy CEO of the civil rights organisation AfriForum and the CEO of the film production company Forum Films.\n\nRoets grew up in the agricultural town of Tzaneen where he matriculated in 2003 from the Merensky High School. He obtained his LLB degree in 2009 from the University of Pretoria (UP). During his student years he served in various leadership structures, including the Student Representative Council (SRC) and the Senate, at this university.\n\nDuring his student years Roets served as founding member and the first National Chairperson of Solidarity Youth. In 2008 Solidarity Youth changed its name and the organisation was converted to AfriForum Youth. In 2016, Roets obtained his LLM degree in Public Law with distinction from the UP. The title of his dissertation was \"’n Peiling van die middele kragtens die Suid-Afrikaanse Grondwet, ter afdwinging van die basiese regte van minderheidsgemeenskappe\".\n\nRoets was arrested in 2010 after he attempted to install a billboard with the words “Welcome to Pretoria” next to the N1 highway. This action was in protest against the Tshwane Metropolitan Municipality’s use of the word “Tshwane” to refer to the capital during the 2010 Fifa Soccer World Cup, while the city’s name is in fact Pretoria.\n\nIn 2011 Roets was appointed as Deputy CEO of AfriForum. He testified on behalf of AfriForum in 2011 in the case of AfriForum vs. Malema. On behalf of AfriForum he submitted a charge of hate speech against Julius Malema after Malema sang the song “Dubula I’Bhunu” (Shoot the Boer) at various political gatherings. The court found Malema guilty of hate speech.\n\nRoets addressed various conferences of the United Nations (UN), especially on farm murders and the state of minority rights in South Africa. In 2015, at the UN’s offices in Genève, he reacted sharply to an accusation by the South African government that those who are asking for the prioritisation of farm murders only did so because they were steering a racial agenda and wanted to bring back apartheid. In the same year Roets submitted a complaint of discrimination against the South African government with the UN’s Special Rapporteur on the situation of human rights defenders after the South African government politically interfered at the UN to prevent AfriForum from registering as a non-government organisation at the UN.\n\nSince 2012 Roets has led AfriForum's campaign to raise awareness about farm attacks.\n\nTainted Heroes is a South African documentary film that was produced by Forum Films under direction of Elrich Yssel, with Roets and Beatrice Pretorius as producers. The film is about the violent battle of the African National Congress (ANC) against apartheid in South Africa between 1976 and 1994. The documentary is based on the book \"People’s War: New Light on the Struggle for South Africa\" by Anthea Jeffery. \n\nMax du Preez described the film as “crude” and “reckless”, but afterwards admitted that he hasn’t seen the film yet. The journalist Pieter du Toit said that Roets played with fire. Even though the ANC did not want to comment on the contents of the film, the party characterised it as propaganda..\n\nRoets is the author of the book \"Kill the Boer\" that was published in June 2018. \"Kill the Boer\" is about farm attacks in South Africa. In the book Roets argues that the South African government, due to a number of reasons, is complicit in the attacks. He characterises the events as ethnic cleansing. Roets argues that in 2017 South African farm attacks took place at an unusual frequency\" and that the killings \"were unusually cruel.\"\n"}
{"id": "3582323", "url": "https://en.wikipedia.org/wiki?curid=3582323", "title": "Hardware architecture", "text": "Hardware architecture\n\nIn engineering, hardware architecture refers to the identification of a system's physical components and their interrelationships. This description, often called a hardware design model, allows hardware designers to understand how their components fit into a system architecture and provides to software component designers important information needed for software development and integration. Clear definition of a hardware architecture allows the various traditional engineering disciplines (e.g., electrical and mechanical engineering) to work more effectively together to develop and manufacture new machines, devices and components.\n\n\"Hardware\" is also an expression used within the computer engineering industry to explicitly distinguish the (electronic computer) hardware from the \"software\" that runs on it. But \"hardware,\" within the automation and software engineering disciplines, need not simply be a computer of some sort. A modern automobile runs vastly more \"software\" than the Apollo spacecraft. Also, modern aircraft cannot function without running tens of millions of computer instructions embedded and distributed throughout the aircraft and resident in both standard computer hardware and in specialized hardward components such as IC wired logic gates, analog and hybrid devices, and other digital components. The need to effectively model how separate physical components combine to form complex systems is important over a wide range of applications, including computers, personal digital assistants (PDAs), cell phones, surgical instrumentation, satellites, and submarines.\n\nHardware architecture is the representation of an engineered (or \"to be engineered\") electronic or electromechanical hardware system, and the process and discipline for effectively implementing the design(s) for such a system. It is generally part of a larger integrated system encompassing information, software, and device prototyping.\n\nIt is a \"representation\" because it is used to convey information about the related elements comprising a hardware system, the relationships among those elements, and the rules governing those relationships.\n\nIt is a \"process\" because a sequence of steps is prescribed to produce or change the architecture, and/or a design from that architecture, of a hardware system within a set of constraints.\n\nIt is a \"discipline\" because a body of knowledge is used to inform practitioners as to the most effective way to design the system within a set of constraints.\n\nA hardware architecture is primarily concerned with the internal electrical (and, more rarely, the mechanical) interfaces among the system's components or subsystems, and the interface between the system and its external environment, especially the devices operated by or the electronic displays viewed by a user. (This latter, special interface, is known as the computer human interface, \"AKA\" human computer interface, or HCI; formerly called the man-machine interface.) Integrated circuit (IC) designers are driving current technologies into innovative approaches for new products. Hence, multiple layers of active devices are being proposed as single chip, opening up opportunities for disruptive microelectronic, optoelectronic, and new microelectromechanical hardware implementation.\n\nPrior to the advent of digital computers, the electronics and other engineering disciplines used the terms system and hardware as they are still commonly used today. However, with the arrival of digital computers on the scene and the development of software engineering as a separate discipline, it was often necessary to distinguish among engineered \"hardware\" artifacts, \"software\" artifacts, and the combined artifacts. \n\nA programmable hardware artifact, or machine, that lacks its computer program is impotent; even as a software artifact, or program, is equally impotent unless it can be used to alter the sequential states of a suitable (hardware) machine. However, a hardware machine and its programming can be designed to perform an almost illimitable number of abstract and physical tasks. Within the computer and software engineering disciplines (and, often, other engineering disciplines, such as communications), then, the terms hardware, software, and system came to distinguish between the hardware that runs a computer program, the software, and the hardware device complete with its program.\n\nThe \"hardware\" engineer or architect deals (more or less) exclusively with the hardware device; the \"software\" engineer or architect deals (more or less) exclusively with the program; and the \"systems\" engineer or systems architect is responsible for seeing that the programming is capable of properly running within the hardware device, and that the system composed of the two entities is capable of properly interacting with its external environment, especially the user, and performing its intended function.\n\nA hardware architecture, then, is an abstract representation of an electronic or an electromechanical device capable of running a fixed or changeable program.\n\nA hardware architecture generally includes some form of analog, digital, or hybrid electronic computer, along with electronic and mechanical sensors and actuators. Hardware design may be viewed as a 'partitioning scheme,' or algorithm, which considers all of the system's present and foreseeable requirements and arranges the necessary hardware components into a workable set of cleanly bounded subsystems with no more parts than are required. That is, it is a partitioning scheme that is exclusive, inclusive, and exhaustive. A major purpose of the partitioning is to arrange the elements in the hardware subsystems so that there is a minimum of electrical connections and electronic communications needed among them. In both software and hardware, a good subsystem tends to be seen as a meaningful \"object.\" Moreover, a clear allocation of user requirements to the architecture (hardware and software) provides an effective basis for validation tests of the user's requirements in the as-built system.\n\n"}
{"id": "8155923", "url": "https://en.wikipedia.org/wiki?curid=8155923", "title": "IMETS", "text": "IMETS\n\nDeveloped by Northrop Grumman, the Integrated Meteorological System (AN/TMQ-40 IMETS) is the meteorological component of the Intelligence and Electronic Warfare (IEW) an element of the Army Battle Command System (ABCS). IMETS provides Army commanders at all echelons with an automated weather system to receive, process, and disseminate weather observations, forecasts, and weather and environmental effects decision aids to all Battlefield Operating Systems (BOS). IMETS is a mobile, tactical, automated weather data receiving, processing and dissemination system. The IMETS is an Army-furnished and maintained system operated by US Air Force battlefield weather team personnel. It uses US Air Force and Army developed software to provide a total weather system to support the Army. The Integrated Weather Effects Decision Aid (IWEDA) software, originally developed by the U.S. Army Research Laboratory (ARL) in 1992, has been fielded on IMETS since 1997 to provide tactical weather support to the U.S. Army.\n\nIMETS is a heavy Humvee mounted tactical system which provides automation and communications support to staff weather teams assigned to echelons from brigade through Echelons Above Corps (EAC). IMETS receives weather information from polar-orbiting civilian and defense meteorological satellites, Air Force Global Weather Center, artillery meteorological teams, remote sensors and civilian forecast centers. IMETS processes and collates forecasts, observations, and climatological data to produce timely and accurate weather products tailored to the specific Warfighter’s needs.\n\nIMETS provides automation and communications support to USAF Weather Teams assigned to Army G2/G3 sections at echelons Brigade through EAC. IMETS receives, processes, and collates forecasts, observations, and climatological data to produce weather forecasts and timely and accurate products to meet Commanders' requirements. IMETS produces displays and disseminates, over Army ABCS, weather forecasts and tactical decision aids that compare the impact of current, projected, or hypothesized weather conditions on friendly and enemy capabilities.\n\nThe IMETS currently fielded is in two configurations. They are the vehicle-mounted configuration (VMC), IMETS-Heavy, and the laptop version, the IMETS-Light. The IMETS-Light is the most common version; US Army uses IMETS-Light for aviation brigades and brigade combat teams. Both configurations have identical intelligence processing capabilities. The IMETS-Light has recently passed the Milestone C review and its production and fielding have official authorization. Both configurations of IMETS operate with ABCS Version 6.X-complaint software.\n\nIMETS training is accomplished at Staff Weather Officer (SWO) Course at Fort Huachuca, Arizona. SWO training is four weeks long, IMETS portion being half of that. This is somewhat ironic, as IMETS has not been used much since the wars in Afghanistan and Iraq began.\n\nThe following units use IMETS:\n\nAs of 2018, My Weather Impacts Decision Aid (MyWIDA), a decision-support software, was developed by the U.S. Army Research Laboratory (ARL). Designed to improve compatibility of technology with environmental factors through weather forecasting, MyWIDA is an updated version of the Integrated Weather Effects Decision Aid (IWEDA), which the Army fielded in 1995.\n\nMyWIDA uses weather forecast data to evaluate environmental impacts on military technology, aiding decision-makers in selecting appropriate technological tools under forecasted weather events. An example is surface winds greater than 25 knots, which prohibit launching of unmanned aerial vehicles (UAVs) in a military scenario. The user would be alerted of this physical limitation through the MyWIDA software.\n\nThe system includes red-amber-green ratings (unfavorable-marginal-favorable), which account for a combination of weather parameters that affect a system or technology. These ratings determine limits beyond which it is not feasible for the technology function due to safety considerations, decreased system effectiveness or violation of manufacturer's operating limits.\n\nMyWida was preceded by Integrated Weather Effects Decision Aid (IWEDA), an automated software decision aid developed by ARL in 1992. \n\nIWEDA was fielded on the Integrated Meteorological System (IMETS) in 1997 to provide tactical weather support to the U.S. IWEDA software was certified and accredited for the Army in 2006. As of 2011, the web-based MYWIDA was under development for eventual replacement of IWEDA.\n\nIWEDA was designed to address the adverse effects of the environment and climate (i.e. wind, precipitation, storms and temperature) on military operations and weapon systems. IWEDA produced a graphic display of weather impacts on 70 weapon systems, including 16 threat systems. Impacts were displayed graphically on a user interface, called the Weather Effects Matrix (WEM), which color coded the impacts on the system of interest (i.e. green “favorable,” amber “marginal,” and red “unfavorable”). Although intended for the Army, applications were also integrated into the Air Force and Navy systems.\n\nOver the years, observations have identified varying needs for IWEDA’s improvement. These included (1) a need to derive complete mission impact due to poor weather conditions, rather than simply presenting a “worst-case scenario” for specific weapon systems; and (2) a need to improve representation of IWEDA’s “stoplight” color scheme (green, yellow and red) by providing more color-coded values.\n\nThis article contains some information that originally came from GlobalSecurity.org, in the public domain from http://www.globalsecurity.org/space/systems/imets.htm\n\nThis article contains some information that originally came from Military Intelligence Bulletin, in the public domain from http://www.findarticles.com/p/articles/mi_m0IBS/is_4_28/ai_94538577\n"}
{"id": "42338264", "url": "https://en.wikipedia.org/wiki?curid=42338264", "title": "Integrated threat theory", "text": "Integrated threat theory\n\nIntegrated threat theory, also known as intergroup threat theory is a theory in psychology and sociology which attempts to describe the components of perceived threat that lead to prejudice between social groups. The theory applies to any social group that may feel threatened in some way, whether or not that social group is a majority or minority group in their society. This theory deals with perceived threat rather than actual threat. Perceived threat includes all of the threats that members of group believe they are experiencing, regardless of whether those threats actually exist. For example, people may feel their economic well-being is threatened by an outgroup stealing their jobs even if, in reality, the outgroup has no effect on their job opportunities. Still, their perception that their job security is under threat can increase their levels of prejudice against the outgroup. Thus, even false alarms about threat still have “real consequence” for prejudice between groups.\n\nIntegrated Threat Theory was first proposed by Walter G. Stephan and Cookie White Stephan (2000). The original theory had four components: realistic threats, symbolic threats, intergroup anxiety, and negative stereotypes. \n\nRealistic threats are threats that pose a danger to the in-group’s well-being. These can include threats to physical safety or health, threats to economic and political power, and threats to the existence of the group. This component was originally developed as a part of realistic conflict theory by Donald T. Campbell (1965). \n\nSymbolic threats arise where there is a perceived difference between the values and worldview of an ingroup and outgroup. The difference can make the ingroup feel that the outgroup poses a threat to their group morals, standards, beliefs, and attitudes. These threats are thus strongly tied to a group’s sense of identity. The category was derived from Gordon Allport’s discussion of the relationship between one’s values and one’s identity. He proposed that, since values are important to who we are, we will reject other groups that disagree with our values. It is also based on the research of Esses and colleagues (1993), who found that groups had more negative feelings towards an outgroup if that outgroup interfered with the in-group’s customs.\n\nIntergroup anxiety refers to the expectation that interacting with someone from a different group will be a negative experience. People with intergroup anxiety fear that they will feel uncomfortable, embarrassed, unsafe, or judged, either by members of the outgroup or by people of their own in-group. \nBefore creating the Integrated Threat Theory framework, Stephan & Stephan had been conducting research on intergroup anxiety. The concept of intergroup anxiety also draws from Aversive Racism Theory, which argues that subconscious negative feelings about Black Americans are an important part of racism against them.\n\nStereotypes are a strategy of simplifying a complex situation by relying on popular pre-set judgements. Integrated Threat Theory predicts that negative pre-set judgments about another group can lead to prejudice. \nThis component of ITT draws from research that found that belief in negatively-rated stereotypical traits was linked to higher levels of prejudice against the stereotyped group. Stephan & Stephan (2000) acknowledged that some research has not found links between prejudice and general stereotypes. Thus, it seems that, while general stereotypes assume some positive things about other groups, only the negative aspects of stereotypes are relevant to prejudice.\n\nIn 2002, Stephan and Renfro proposed an updated version of the theory which reduced the four components to two basic types: realistic and symbolic threats. The categories of negative stereotypes and intergroup anxiety were removed from the basic framework of the theory because they were found to be better understood as subtypes of threat. They can lead to either realistic or symbolic threats rather than standing as their own separate categories. For example, intergroup anxiety can be based on expectations of physical danger, a realistic threat, as well as on expectations of damage to one’s identity, a symbolic threat. \n\nSince ITT makes a causal claim that perceived threat causes prejudice, studies using an experimental design are necessary. Some researchers have taken on this task to experimentally manipulate types of realistic and perceived threat in order to examine if they cause prejudice. For example, Esses and colleagues (1998) and Esses and colleagues (2001) carried out research studies in which they manipulated the research participants’ understanding of economic threat posed by immigrants. Esses and colleagues (1998) had Canadian undergraduate student participants read one of two editorials that were written for the study. One editorial discussed a new group of immigrants with no mention of the job market while the other editorial discussed the same group and emphasized their success in finding jobs despite the scarcity of jobs in Canada. They then studied the effects of perception of economic threat, a type of realistic threat, on attitudes about immigrants and reported willingness to help immigrants. Results showed that participants that read the editorial that emphasized competition had less favorable attitudes towards immigrants and were less likely to approve of programs to empower immigrants Esses and colleagues (2001) carried out similar experiments with very similar editorials. Their results showed that participants that read articles that emphasized the tough job market had more negative attitudes towards the immigrants, were less supportive of their immigration into Canada, and were less supportive of programs to empower immigrants. The data from these research studies provide some support for the causal influence of realistic threat on prejudice against immigrants.\n\nThe causal influence of symbolic threat on prejudice was partially explored in a study by Branscombe & Wann (1994), who focused on perceived threat to in-group identity in particular. The participants, undergraduate females from the U.S., answered questionnaires about their levels of pride in their American identity at the beginning of the study. They then manipulated the participants’ perceived threat to in-group identity using video clips, which either showed an American or a Russian boxer beating the other in a match. After seeing one version of the video, participants completed a questionnaire that measured their desire to distance themselves from the outgroup, in this case, Russians. The results of this study showed that increased perception of threat to in-group identity raises a desire to distance oneself from the out-group. This provides some experimental evidence that perception of threat to in-group identity may causes greater prejudice towards out-groups. However, further experimental research is necessary in order to more firmly and widely establish the causal role of realistic and symbolic threats in prejudice. \n\nSeveral factors can lead to increased or decreased levels of group perceived threat.[such as?]\n\nThe updated ITT theory draws from the findings of contact hypothesis, which claims that it is important to have equality between groups. Power dynamics between two groups are shown to have an influence on how the groups relate to and perceive each other. High-power groups are more likely to influence and threaten other groups. Low-power groups are often vulnerable to the influence and threats of other groups. Thus, low-power groups tend to be on alert and perceive more threats than high power groups do. Corenblum & Stephan (2001) found, for example, that Native Canadians felt more threatened by White Canadians than White Canadians felt about them. However, when high-power groups do perceive threat from another group, they “will react more strongly” than low-power groups. This is likely because they have more to lose if the threat is real and have more resources that allow them to counter to such threats. Two groups of relatively equal power status can be especially sensitive to feeling threatened if they are in competition with each other for resources, such as jobs. \n\nStephan & Renfro (2016) predicted that, the more important group membership is to ingroup members’ sense of personal identity, the more likely those people will feel threatened by and uncomfortable when interacting with other groups. According to this prediction, people with strong in-group identification are likely to be more focused on differences between the groups, thus giving them more motivation to hold negative stereotypes of other groups so that they can believe that their group is the best. \n\nThere may be a link between the personal importance of group membership and the larger culture in which the groups live. Collectivistic cultures, for example, place a greater emphasis on the importance of group membership compared to individualistic cultures. Culture can also influence perceived threat between groups through the culture’s level of uncertainty avoidance. Hofstede & Bond (1984) define uncertainty avoidance as “the degree to which people feel threatened by ambiguous situations, and have created beliefs and institutions that try to avoid these.” Stephan & Renfro (2016) thus suggest that cultures which hold norms and laws as very important are likely to perceive threat from “unfamiliar groups.” Further research on these topics can better inform the role of culture in intergroup relationships.\n\nThe Integrated Threat Theory has been used in research on various social groups, including immigrants, Muslims, tourists, and more. \n\nMultiple studies on inter-group relations have focused on immigrants. For example, Ward and Masgoret (2006) built upon ITT in combination with the Instrumentive Model of Group Conflict to test a model of attitudes toward immigrants, using participants from New Zealand. These participants filled out questionnaires that measured Multicultural Ideology, Intergroup Anxiety, Contact with Immigrants, Perceived Intergroup Threat, and Attitudes toward Immigrants. The results supported the model, suggesting that increased contact with immigrants and multicultural ideology are related to lower levels of perceived threat from immigrants, which is in turn directly related to more positive attitudes towards immigrants. \n\nCroucher (2013) used the ITT framework to explore reasons that dominant groups in France, Germany, and Great Britain sometimes resist Muslim immigrants’ efforts to assimilate. The data was collected through questionnaires, which included measures for symbolic threats, realistic threats, stereotypes, perception of immigrants’ motivation to assimilate, and multigroup ethnic identity. The results supported the theory that the more that the dominant groups felt threatened by the immigrants, the less they thought that the immigrants wanted to assimilate into their country. \n\nSimilarly, Rohmann, Piontkowski, and van Randenborgh (2008) used the ITT framework to examine the relationship between perceived threat and a dominant group’s expectation of an immigrant group’s attitude about acculturation. Their research included two studies, one in which German participants were asked about their expectations of French and Turkish immigrants in Germany and another in which German participants were asked about their expectations of two fictitious groups, based on paragraph-long descriptions. Results from both studies suggest that levels of perceived threat are higher if dominant groups expect that an immigrant group has different attitudes about acculturation than the dominant group does.\n\nTausch, Hewstone, and Roy (2009) examined Muslim relations with Hindus in India. ITT was incorporated into their research in order to examine which factors are important in perceived threat between the minority Muslim and majority Hindu groups of India. Their data was collected through a survey given to both Muslim and Hindu students at the same university, which measured contact quantity, contact quality, perceived relative status of the two groups, realistic threats, symbolic threats, intergroup anxiety, preference for social distance, and in-group bias. Results showed that symbolic threat was important for Hindus’ levels of perceived threat while realistic threat was important for Muslims’ levels of perceived threat. \n\nGonzalez and colleagues (2008) carried out similar research in the Netherlands, examining the prejudice of Dutch youth, who are members of the majority, against the Muslim minority in the country. Their data was collected through a questionnaire given to high schoolers in different cities, which measured support for multicultural ideologies, frequency of contact with Muslims, in‐group identification, realistic economic threat, symbolic threats, stereotypes, and prejudicial attitudes towards Muslims. Results showed that prejudicial attitudes were related to higher perception of symbolic threats and more belief in stereotypes. \n\nUenal (2016) applied the ITT framework to better understand factors involved in the presence of Islamophobic conspiracy stereotypes in Europe. The data was collected through an online survey given to German university students which measured ambiguity intolerance, belief in a clash of civilizations, realistic threats, symbolic threats, and levels of education. Ambiguity intolerance was found to be related to increased conspiracy stereotypes through increased perceptions of symbolic threat. Belief in a clash of civilizations was found to be related to higher levels of realistic and symbolic threat and higher levels of belief in conspiracy stereotypes. Higher education levels showed the opposite trends, as it was related to lower levels of perceived threat and lower levels of belief in conspiracy stereotypes.\n\nTourism can bring different groups into contact and has thus been the subject of some research on inter-group relations using ITT. For example, Ward and Berno (2011) used ITT and contact hypothesis as theoretical backgrounds for predicting attitudes about tourism in Fiji and New Zealand. They collected data through surveys, which included measures of perceived impact of tourism, contact with tourists, the four aspects of the original ITT, and attitudes towards tourists. Following the expectations of ITT, the data showed that lower levels of perceived realistic threat, symbolic threat, and intergroup anxiety, and more positive stereotypes were useful predictors of positives attitudes about tourism. Monterubio (2016) applied ITT in studying negative attitudes towards spring break tourists in Cancun, Mexico. Data was collected through interviews with Cancun residents, which included questions about the social impact of spring break and attitudes towards spring breakers. Transcripts of these interviews were then analyzed for themes, including the four components of the original ITT. The results suggested that realistic threats and intergroup anxiety were relevant aspects of prejudice against spring break tourists, largely because of the influence of their behavior. \n\nStephan & Renfro (2016) updated ITT into the two-factor model and admitted that “ultimately, the model is circular.” The theory states that perceived threat leads to prejudice but the outcomes of that prejudice itself can also lead into increased perceived threat. \n\nAnxiety/Uncertainty Management Theory counters the way that ITT conceptualizes anxiety as harmful for relationships between social groups. Instead, it understands anxiety as helpful for leading to more effective communication between groups. \n"}
{"id": "10289903", "url": "https://en.wikipedia.org/wiki?curid=10289903", "title": "James A. Shapiro", "text": "James A. Shapiro\n\nJames Alan Shapiro (born May 18, 1943) is an American biologist, an expert in bacterial genetics and a professor in the Department of Biochemistry and Molecular Biology at the University of Chicago.\n\nShapiro obtained his Bachelor's degree in English from Harvard College in 1964. Then, inspired by a genetics course he had taken as a senior, he shifted from English to science, earned a doctorate in genetics from Corpus Christi College, Cambridge in 1968, and did postdoctoral research with Jon Beckwith at the Harvard Medical School. He was troubled by the potential genetic engineering applications of his research. He spent two years teaching genetics in Havana, Cuba, before returning to another postdoctorate with Harlyn Halvorson at Brandeis University. Since 1973, he has worked as a professor of microbiology at the University of Chicago, and has also been a visiting professor from time to time, including once as a Darwin Prize Visiting Professor at the University of Edinburgh in 1994.\n\nWhile working with Beckwith at Harvard, Shapiro was part of the first team to isolate a single gene from an organism. The gene they isolated was \"lacZ\", which codes for the β-galactosidase enzyme used by \"E. coli\" bacteria to digest the sugars in milk. Their technique involved transduction of two complementary copies of the gene into two different bacteriophages, then mixing the genetic material from the two phages, and finally using a nuclease to degrade the single-stranded phage genome, leaving only the double-stranded DNA formed by the two copies.\n\nIn a paper published in the \"Proceedings of the National Academy of Sciences\" in 1979, Shapiro was the first to propose replicative transposition as a mechanism for gene mobility. In this model, genes such as retrotransposons are copied from one DNA sequence to another via a process in which the two sequences combine to form an intermediate \"theta\" shape, sometimes called a \"Shapiro intermediate\".\n\nLater, Shapiro showed that bacteria cooperate in communities that exhibit complex behavior such as hunting, building protective structures, and spreading spores, and in which individual bacteria may sacrifice themselves for the benefit of the larger community. Based on this work, Shapiro believes that cooperative behavior is a fundamental organizing concept for biological activity at all levels of complexity.\n\nShapiro has also studied pattern formation in bacteria, an area where he feels that there are new mathematical principles to be discovered that also underlie the growth of crystals and the shape of cosmological structures. For instance, he found that the gut bacterium \"Proteus mirabilis\" forms complex terraced rings, an emergent property of simple rules that the bacterium uses to avoid neighboring cells.\n\nHe has proposed the term natural genetic engineering to account for how novelty is created in the course of biological evolution. It has been criticized by some.\n\nShapiro was elected to Phi Beta Kappa in 1963 and was a Marshall Scholar from 1964 to 1966. He won the Darwin Prize Visiting Professorship of the University of Edinburgh in 1993. In 1994, he was elected as a fellow of the American Association for the Advancement of Science for \"innovative and creative interpretations of bacterial genetics and growth, especially the action of mobile genetic elements and the formation of bacterial colonies.\" And in 2001, he was made an honorary officer of the Order of the British Empire for his service to the Marshall Scholarship program. In 2014 he was chosen to give the 3rd annual \"Nobel Prize Laureate - Robert G. Edwards\" lecture \n\nShapiro edited the books \"Mobile Genetic Elements\" (Academic Press, 1983) and, with Martin Dworkin, \"Bacteria as Multicellular Organisms\" (Oxford University Press, 1997). He is the author of \"Evolution: A View from the 21st Century\" (FT Press Science, 2011, ).\n\n"}
{"id": "5700051", "url": "https://en.wikipedia.org/wiki?curid=5700051", "title": "Letters of Gediminas", "text": "Letters of Gediminas\n\nThere are 6 surviving transcripts of letters of Gediminas written in 1323–1324 by Grand Duke Gediminas. These letters are one of the first surviving documents from the Grand Duchy of Lithuania. Since they were sent to the Western Europe, the pope, merchants, and craftspeople, they were written in Latin.\n\nThe first letter was written to Pope John XXII. Gediminas claimed that the Teutonic Knights did not act in the interest of the Catholic faith. Instead, they brutally devastated the land. The people were forced into resistance. Gediminas enumerated many crimes and damages done by the knights; for example, he claimed that his predecessor Vytenis sent a letter to the Franciscan monks asking for two brothers who could come to the Grand Duchy of Lithuania to look after a local church. When the Teutonic Knights learned about the letter, they sent their army and destroyed the church. In the last sentence Gediminas vaguely promised to accept Christianity and obey the pope.\n\nThe second letter was written on January 25, 1323 to the German cities of Lübeck, Sund, Bremen, Magdeburg, Cologne and other cities in the Holy Roman Empire. Gediminas explained that the Grand Duchy of Lithuania was very tolerant to the Christians, but remained pagan and did not accept Christianity only because of brutal Teutonic Knights. He told about the first letter sent to the pope and his intentions to baptize in the Catholic rite. Gediminas invited knights, squires, merchants, doctors, smiths, wheelwrights, cobblers, skinners, millers, and others to come to the Grand Duchy and practice their trade and faith without any restrictions. The peasants were promised tax exemption for ten years. The merchants were also exempt from any tariffs or taxes. This letter is best known because Vilnius, capital of Lithuania, was mentioned in written sources for the first time. Therefore, 1323 is considered to be the official founding year of Vilnius. Gediminas is considered to be the city's founder even though the city existed years before Gediminas' reign. Also, Vilnius is unambiguously mentioned as the capital city. His invitation notably included German Jews as well as Christians, and its issuance is closely linked to the establishment of the Jewish community in Lithuania.\n\nThe third letter addressed to Lübeck, Rostock, Sund, Greifswald, Stetin, Gotland cities was written on May 26, 1323. In essence it repeated the second letter. It asked for various craftspeople (the list of crafts was expanded) to come to Lithuania and practice their trade. It said that there were three churches in the duchy: two Franciscan (in Vilnius and in Navahradak) and one Dominican. Everyone was free to use them.\n\nThe fourth and the fifth letters were also written on May 26, 1323 and were addressed to the Franciscan and Dominican Orders. Gediminas, in anticipation of his baptism, invited priests and monks to come to the Grand Duchy of Lithuania. He also asked to spread the word to craftspeople that they were welcome in Lithuania. In the letter to the Dominicans, Gediminas mentioned that his seal was burned by the Teutonic Knights.\n\nThe last surviving letter was written on September 22, 1324 and was addressed to the bishop of Tartu, Erzel, ruler of Tallinn land, and Council of Riga. It reported that the Teutonic Knights violated a peace treaty signed earlier. The Knights attacked border regions, killed residents, and took all valuables. Many messengers were captured and killed. Gediminas asked for help enforcing the treaty.\n\n"}
{"id": "6620756", "url": "https://en.wikipedia.org/wiki?curid=6620756", "title": "Lycian alphabet", "text": "Lycian alphabet\n\nThe Lycian alphabet was used to write the Lycian language. It was an extension of the Greek alphabet, with half a dozen additional letters for sounds not found in Greek. It was largely similar to the Lydian and the Phrygian alphabets.\n\nThe Lycian alphabet contains letters for 29 sounds. Some sounds are represented by more than one symbol, which is considered one \"letter\". There are six vowel letters, one for each of the four oral vowels of Lycian, and separate letters for two of the four nasal vowels. Nine of the Lycian letters do not appear to derive from the Greek alphabet.\n\nIn some ways, Modern Greek resembles Lycian more in its orthography than it does Ancient Greek.\n\nThe Lycian alphabet was added to the Unicode Standard in April, 2008 with the release of version 5.1.\nIt is encoded in Plane 1 (Supplementary Multilingual Plane).\n\nThe Unicode block for Lycian is U+10280–U+1029F:\n\n\n"}
{"id": "657908", "url": "https://en.wikipedia.org/wiki?curid=657908", "title": "Manas-vijnana", "text": "Manas-vijnana\n\nManas-vijnana (Skt. \"'मानस-विज्ञान\"'; mānas-vijñāna; \"mind-knowledge\", compare \"man-tra\", jñāna) is the seventh of the eight consciousnesses as taught in Yogacara and Zen Buddhism, the higher consciousness or intuitive consciousness that on the one hand localizes experience through thinking and on the other hand universalizes experience through intuitive perception of the universal mind of alayavijnana. Manas-vijnana, also known as klista-manas-vijnana or simply manas, is not to be confused with \"manovijnana\" which is the sixth consciousness.\n\nAccording to Bhikkhu Bodhi, the post-canonical Pali commentary uses the three terms \"viññāa\", \"mano\" and \"citta\" as synonyms for the mind sense base (\"mana-ayatana\"); however, in the Sutta Pitaka, these three terms are generally contextualized differently:\n\nAccording to Daisetz Teitaro Suzuki (who uses the term \"Manas\" rather than \"Manas Vijnana\" for the seventh consciousness) the Lankavatara Sutra presents the Zen view of the Eight Consciousnesses rather than the Yogacara view. In his introduction to his translation of the Lankavatara Sutra he clarifies the distinction between the overlapping terms:\n\n"}
{"id": "48468085", "url": "https://en.wikipedia.org/wiki?curid=48468085", "title": "Marko Vuokola", "text": "Marko Vuokola\n\nMarko Vuokola (born 1967) is a Finnish conceptual artist. He lives and works in Helsinki, Finland.\n\nMarko Vuokola was born in Toijala, Finland as the youngest son of photographer Aimo Vuokola and Tuula Vuokola. He studied at the Department of Sculpture at the Finnish Academy of Fine Arts, Helsinki from 1987 and received a BA in 1991, followed by studies in Media Arts at AKI, De Academie voor Kunst en Industrie in Enschede, the Netherlands 1991-92. He graduated with an MA from the Finnish Academy of Fine Arts in 2001.\n\nVuokola works across a wide variety of media and materials, departing often from a conceptual exploration of issues relating to time and space. Many works, particularly in his collaborations in the OLO collective (together with Pasi Eerik Karjula), have a strongly installational and site-related character. Despite his training as a sculptor, much of his artistic oeuvre is based on photography, video or computer animation. He is often mentioned as a photographer, although it is more proper to describe his work as based on ideas just as much as on materials. His works often also explore the optical and other aspects of his artistic media, like the camera or the video projector.\n\nDuring the summer of 1995, Vuokola spent five weeks in remote Palsinoja in the Finnish Lapland digging for gold. The 4 grams of gold that he collected were stretched into a thin thread only 0,12 mm thick and approximately 7 m long. The work deals with time just as much as with the precious material, and also with the concepts of sculpture and visuality in art. The thread can hardly be seen when stretched across a wall, while it represents a considerable amount of physical work. The work was exhibited at Henry Moore institute, Leeds, in 1996 and later acquired by Kiasma, the Finnish national museum of contemporary art.\n\nIn 2001 Vuokola started up the series of works that he is most known for, The Seventh Wave. In these photographic diptychs, each panel depicts what might seem an identical motive. The difference between the images can be described in terms of time, as an unknown amount of time has passed between the two exposures. The difference is also a matter of space and material, as the physical and light circumstances have changed, even if these differences might be very small. The camera remains in the same position during these exposures. Many of these works have been shot outdoors, but this does not mean it is a series of landscape images, as several interiors and even seemingly abstract motives are included in the series. The Seventh Wave includes landscapes from Finland, park views from the Versailles of Louis XIV, the surroundings of Donald Judd’s Marfa in Texas, interiors and exteriors from Hong Kong, Great Barrier Island in New Zealand, seascapes from Vietnam, a car vendor’s display room and the artist’s home and neighbourhood. The title of the series comes from the popular belief, that on sea, the seventh wave tends to be the largest.\n\nStarting as an experiment with a projector displaying a video of the projector itself – very much in the spirit of Nam June Paik’s work – RGB-light (1996) became just as much a study of how projected light behaves when interacting with visitors. The projection itself is in black and white, but the light cast on the wall of the exhibition space is prismatically split into a variety of colours, depending on how the three additive primary colours (red, green and blue) of the RGB projector are affected by the movement of visitors in the space.\nA continued experimentation with the possibilities within these aspects of optics has resulted also in the prints RGB (1996-2012) and the computer animation RGB-information (2005).\n\nOriginating in experiments with images of streaming water (Tammerkoski, 2012), Vuokola started exploring the very fundaments of digital photography. In Flora (sharpen), a large diptych of a Norwegian waterfall from the same year, a small selection of pixels from the center of each of the two digital files were enlarged to form a new diptych of their own. The connection in this case to landscape imagery is essential, as the work approaches the basics of the construction of visuality as well as digital images.\n\nTogether with artist colleague Pasi Eerik Karjula Vuokola has formed the collective OLO since 1990. OLO is primarily working with site-engaged installations and commissions. The collaboration has resulted in over 50 works, most of them outside of Finland. The best known of these is the extensive public art work for Hietalahti Square in Helsinki (OLO No. 22, 2000).\nAnother long term collaboration although less formalised is the collective Forest Camp, which was formed in 1998. Forest Camp functions as a think tank for immaterial work and temporary public interventions.\n\nVuokola’s solo exhibitions include a major presentation at SKMU Sørlandets kunstmuseum in Norway in 2011, followed by a large intervention with his works installed together with the collection of Trondheim Kunstmuseum in 2012-13, for which a book was published documenting the installations. He has shown extensively with Galerie Anhava in Helsinki since 1992. He was in 1994 the youngest artist to be presented with a solo exhibition at the Museum of Contemporary Art in Ateneum, the Finnish National Gallery in Helsinki.\n\nAmong Vuokola's most important group exhibitions are Melbourne International Biennial 1999 (in OLO collaboration with Pasi Eerik Karjula; curator Juliana Engberg); \"Rock the Campo\" organised by Frame Finnish Funf for Art Exchange at the 49th Venice Biennale 2001; \"From Dust to Dusk\" at Charlottenborg in Copenhagen 2003 (curator Pontus Kyander); \"Gridlock: cities, structures, spaces\" at Govett-Brewster Art Gallery in New Plymouth, New Zealand (curator Simon Rees); \"Sense of the Real\" at Ars 06, Kiasma in Helsinki 2006 (with collective Forest Camp, curated by Tuula Karjalainen and Jari-Pekka Vanhala); \"Super Structures\" a public art project by a little blah blah in Ho Chi Minh City, Vietnam, 2008 (curators Sue Hajdu & Pontus Kyander); \"Entr'acte\" at Kukje Gallery in Seoul, South Korea 2009 (curator Pontus Kyander); \"East of the Sun and West of the Moon\" at Växjö Konsthall, Sweden 2012; and \"Memories can’t wait – Film without film\", 60th International Short Film Festival Oberhausen, Germany (curator Mika Taanila).\n\nVuokola is represented in many public collections in Finland, such as the Kiasma Museum of Contemporary Art, Helsinki City Art Museum, EMMA – Espoo Museum of Modern Art, The Collection of the Finnish State, The Finnish Museum of Photography in Helsinki, Tampere Museum of Contemporary Art, and Sara Hildén Art Museum in Tampere.\n\nHis works are included also in the Malmö Art Museum in Sweden, the collection of Public Art Agency Sweden, Stockholm, SKMU Sørlandets Kunstmuseum in Kristiansand, Norway and Trondheim Kunstmuseum, Norway.\n\nVuokola's works have only rarely surfaced on the secondary market, but when a mid-size (125 x 125 cm x 2) diptych from The Seventh Wave-series depicting the Versailles park in France was auctioned on Christie's in London in 2008, the price reached 15,625 GBP (23,375 USD), twice over estimate.\n"}
{"id": "17517168", "url": "https://en.wikipedia.org/wiki?curid=17517168", "title": "Mathematical principles of reinforcement", "text": "Mathematical principles of reinforcement\n\nThe mathematical principles of reinforcement (MPR) constitute of a set of mathematical equations set forth by Peter Killeen and his colleagues attempting to describe and predict the most fundamental aspects of behavior (Killeen & Sitomer, 2003). The three key principles of MPR, arousal, constraint, and coupling, describe how incentives motivate responding, how time constrains it, and how reinforcers become associated with specific responses, respectively. Mathematical models are provided for these basic principles in order to articulate the necessary detail of actual data.\n\nThe first basic principle of MPR is arousal. Arousal refers to the activation of behavior by the presentation of incentives. An increase in activity level following repeated presentations of incentives is a fundamental aspect of conditioning. Killeen, Hanson, and Osborne (1978) proposed that adjunctive (or schedule induced) behaviors are normally occurring parts of an organism's repertoire. Delivery of incentives increases the rate of adjunctive behaviors by generating a heightened level of general activity, or arousal, in organisms.\n\nKilleen & Hanson (1978) exposed pigeons to a single daily presentation of food in the experimental chamber and measured general activity for 15 minutes after a feeding. They showed that activity level increased slightly directly following a feeding and then decreased slowly over time. The rate of decay can be described by the following function:\n\nThe time course of the entire theoretical model of general activity is modeled by the following equation:\n\nTo better conceptualize this model, imagine how rate of responding would appear with each of these processes individually. In the absence of temporal inhibition or competing responses, arousal level would remain high and response rate would be depicted as an almost horizontal line with a very small negative slope. Directly following food presentation, temporal inhibition is at its maximum level. It decreases quickly as time elapses, and response rate would be expected to increase up to the level of arousal in a short time. Competing behaviors such as goal tracking or hopper inspection are at a minimum directly after food presentation. These behaviors increase as the interval elapses, so the measure of general activity would slowly decrease. Subtracting these two curves results in the predicted level of general activity.\n\nKilleen et al. (1978) then increased the frequency of feeding from daily to every fixed-time seconds. They showed that general activity level increased substantially from the level of daily presentation. Response rate asymptotes were highest for the highest rates of reinforcement. These experiments indicate that arousal level is proportional to rate of incitement, and the asymptotic level increases with repeated presentations of incentives. The increase in activity level with repeated presentation of incentives is called cumulation of arousal. The first principle of MPR states that arousal level is proportional to rate of reinforcement, formula_4, where:\n\n= arousal level\n\n= specific activation\n\n= rate of reinforcement\n\n(Killeen & Sitomer, 2003).\n\nAn obvious but often overlooked factor when analyzing response distributions is that responses are not instantaneous, but take some amount of time to emit (Killeen, 1994). These ceilings on response rate are often accounted for by competition from other responses, but less often for the fact that responses cannot always be emitted at the same rate at which they are elicited (Killeen & Sitomer, 2003). This limiting factor must be taken into account in order to correctly characterize what responding could be theoretically, and what it will be empirically.\n\nAn organism may receive impulses to respond at a certain rate. At low rates of reinforcement, the elicited rate and emitted rate will approximate each other. At high rates of reinforcement, however, this elicited rate is subdued by the amount of time it takes to emit a response. Response rate, formula_5, is typically measured as the number of responses occurring in an epoch divided by the duration of an epoch. The reciprocal of formula_5 gives the typical measure of the inter response (IRT), the average time from the start of one response to the start of another (Killeen & Sitomer, 2003). This is actually the cycle time rather than the time between responses. According to Killeen & Sitomer (2003), the IRT consists of two subintervals, the time required to emit a response, formula_7 plus the time between responses, formula_8. Therefore, response rate can be measured either by dividing the number of responses by the cycle time:\n\nor as the number of responses divided by the actual time between responses:\n\nThis instantaneous rate, formula_11 may be the best measure to use, as the nature of the operandum may change arbitrarily within an experiment (Killeen & Sitomer, 2003).\n\nKilleen, Hall, Reilly, and Kettle (2002) showed that if instantaneous rate of responding is proportional to rate of reinforcement, formula_12, then a fundamental equation for MPR results. Killeen & Sitomer (2003) showed that:\n\nif formula_13\n\nthen formula_14,\n\nand rearranging gives:\n\nformula_15\n\nWhile responses may be elicited at a rate proportional to formula_16, they can only be emitted at rate formula_5 due to constraint. The second principle of MPR states that the time required to emit a response constrains response rate (Killeen & Sitomer, 2003).\n\nCoupling is the final concept of MPR that ties all of the processes together and allows for specific predictions of behavior with different schedules of reinforcement. Coupling refers to the association between responses and reinforcers. The target response is the response of interest to the experimenter, but any response can become associated with a reinforcer. Contingencies of reinforcement refer to how a reinforcer is scheduled with respect to the target response (Killeen & Sitomer, 2003), and the specific schedules of reinforcement in effect determine how responses are coupled to the reinforcer. The third principle of MPR states that the degree of coupling between a response and reinforcer decreases with the distance between them (Killeen & Sitomer, 2003). Coupling coefficients, designated as formula_18, are given for the different schedules of reinforcement. When the coupling coefficients are inserted into the activation-constraint model, complete models of conditioning are derived:\n\nThis is the fundamental equation of MPR. The dot after the formula_20 is a placeholder for the specific contingencies of reinforcement under study (Killeen & Sitomer, 2003).\n\nThe rate of reinforcement for fixed-ratio schedules is easy to calculate, as reinforcement rate is directly proportional to response rate and inversely proportional to ratio requirement (Killeen, 1994). The schedule feedback function is therefore:\n\nSubstituting this function into the complete model gives the equation of motion for ratio schedules (Killeen & Sitomer, 2003). Killeen (1994, 2003) showed that the most recent response in a sequence of responses is weighted most heavily and given a weight of formula_22, leaving formula_23 for the remaining responses. The penultimate response receives formula_24, the third back receives formula_25. The formula_26th response back is given a weight of formula_27\n\nThe sum of this series is the coupling coefficient for fixed-ratio schedules:\n\nThe continuous approximation of this is:\n\nwhere formula_30 is the intrinsic rate of memory decay. Inserting the reinforcement rate and coupling coefficient into the activation-constraint model gives the predicted response rates for FR schedules:\n\nThis equation predicts low response rates at low ratio requirements due to the displacement of memory by consummatory behavior. However, these low rates are not always found. Coupling of responses may extend back beyond the preceding reinforcer, and an extra parameter, formula_32 is added to account for this. Killeen & Sitomer (2003) showed that the coupling coefficient for FR schedules then becomes:\n\nformula_32 is the number of responses preceding the prior reinforcer that contribute to response strength. formula_35 which ranges from 0 to 1 is then the degree of erasure of the target response from memory with the delivery of a reinforcer. (formula_36) If formula_37, erasure is complete and the simpler FR equation can be used.\n\nAccording to Killeen & Sitomer (2003), the duration of a response can affect the rate of memory decay. When response durations vary, either within or between organisms, then a more complete model is needed, and formula_22 is replaced with formula_39 yielding:\n\nIdealized variable-ratio schedules with a mean response requirement of formula_26 have a constant probability of formula_42 of a response ending in reinforcement (Bizo, Kettle, & Killeen, 2001). The last response ending in reinforcement must always occur and receives strengthening of formula_22. The penultimate response occurs with probability formula_44 and receives a strengthening of formula_24 . The sum of this process up to infinity is (Killeen 2001, Appendix):\n\nThe coupling coefficient for VR schedules ends up being:\n\nformula_47\n\nMultiplying by degree of erasure of memory gives:\n\nformula_48\n\nThe coupling coefficient can then be inserted into the activation-constraint model just as the coupling coefficient for FR schedules to yield predicted response rates under VR schedules:\n\nformula_49\n\nIn interval schedules, the schedule feedback function is\n\nformula_50\n\nwhere formula_51 is the minimum average time between reinforcers (Killeen, 1994). Coupling in interval schedules is weaker than ratio schedules, as interval schedules equally strengthen all responses preceding the target rather than just the target response. Only some proportion formula_52 of memory is strengthened. With a response requirement, the final, target response must receive strength of formula_22. All preceding responses, target or non-target, receive a strengthening of formula_23.\n\nFixed-time schedules are the simplest time dependent schedules in which organisms must simply wait t seconds for an incentive. Killeen (1994) reinterpreted temporal requirements as response requirements and integrated the contents of memory from one incentive to the next. This gives the contents of memory to be:\n\nN\n\nMN= lò e-lndn\n\n0\n\nThis is the degree of saturation in memory of all responses, both target and non-target, elicited in the context (Killeen, 1994). Solving this equation gives the coupling coefficient for fixed-time schedules:\n\nc=r(1-e-lbt)\n\nwhere formula_52 is the proportion of target responses in the response trajectory. Expanding into a power series gives the following approximation:\n\nc» rlbt\nThis equation predicts serious instability for non-contingent schedules of reinforcement.\n\nFixed-interval schedules are guaranteed a strengthening of a target response, b=w1, as reinforcement is contingent on this final, contiguous response (Killeen, 1994). This coupling is equivalent to the coupling on FR 1 schedules\n\nw1=b=1-e-l.\n\nThe remainder of coupling is due to the memory of preceding behavior. The coupling coefficient for FI schedules is:\n\nc= b +r(1- b -e-lbt).\n\nVariable-time schedules are similar to random ratio schedules in that there is a constant probability of reinforcement, but these reinforcers are set up in time rather than responses. The probability of no reinforcement occurring before some time t’ is an exponential function of that time with the time constant t being the average IRI of the schedule (Killeen, 1994). To derive the coupling coefficient, the probability of the schedule not having ended, weighted by the contents of memory, must be integrated.\nM= lò e-n’t/te-ln’ dn’\nIn this equation, t’=n’t, where t is a small unit of time. Killeen (1994) explains that the first exponential term is the reinforcement distribution, whereas the second term is the weighting of this distribution in memory. Solving this integral and multiplying by the coupling constant r, gives the extent to which memory is filled on VT schedules:\n\nc=rlbt\n\nThis is the same coupling coefficient as an FT schedule, except it is an exact solution for VT schedules rather than an approximation. Once again, the feedback function on these non-contingent schedules predicts serious instability in responding.\n\nAs with FI schedules, variable-interval schedules are guaranteed a target response coupling of b. Simply adding b to the VT equation gives:\nM= b+ lò e-n’t/te-ln’ dn’\nSolving the integral and multiplying by r gives the coupling coefficient for VI schedules:\n\nc= b+(1-b) rlbt\n\nThe coupling coefficients for all of the schedules are inserted into the activation-constraint model to yield the predicted, overall response rate. The third principle of MPR states that the coupling between a response and a reinforcer decreases with increased time between them (Killeen & Sitomer, 2003).\n\nMathematical principles of reinforcement describe how incentives fuel behavior, how time constrains it, and how contingencies direct it. It is a general theory of reinforcement that combines both contiguity and correlation as explanatory processes of behavior. Many responses preceding reinforcement may become correlated with the reinforcer, but the final response receives the greatest weight in memory. Specific models are provided for the three basic principles to articulate predicted response patterns in many different situations and under different schedules of reinforcement. Coupling coefficients for each reinforcement schedule are derived and inserted into the fundamental equation to yield overall predicted response rates.\n\n"}
{"id": "20518", "url": "https://en.wikipedia.org/wiki?curid=20518", "title": "Metaphor", "text": "Metaphor\n\nA metaphor is a figure of speech that, for rhetorical effect, directly refers to one thing by mentioning another. It may provide clarity or identify hidden similarities between two ideas. Antithesis, hyperbole, metonymy and simile are all types of metaphor. One of the most commonly cited examples of a metaphor in English literature is the \"All the world's a stage\" monologue from \"As You Like It\":\n\n<poem>All the world's a stage,\nAnd all the men and women merely players;\nThey have their exits and their entrances ...\nThis quotation expresses a metaphor because the world is not literally a stage. By asserting that the world is a stage, Shakespeare uses points of comparison between the world and a stage to convey an understanding about the mechanics of the world and the behavior of the people within it.\n\n\"The Philosophy of Rhetoric\" (1937) by rhetorician I. A. Richards describes a metaphor as having two parts: the tenor and the vehicle. The tenor is the subject to which attributes are ascribed. The vehicle is the object whose attributes are borrowed. In the previous example, \"the world\" is compared to a stage, describing it with the attributes of \"the stage\"; \"the world\" is the tenor, and \"a stage\" is the vehicle; \"men and women\" is the secondary tenor, and \"players\" is the secondary vehicle.\n\nOther writers employ the general terms ground and figure to denote the tenor and the vehicle. Cognitive linguistics uses the terms target and source, respectively.\n\nThe English \"metaphor\" derived from the 16th-century Old French word \"métaphore\", which comes from the Latin \"metaphora\", \"carrying over\", in turn from the Greek μεταφορά (\"metaphorá\"), \"transfer\", from μεταφέρω (\"metapherō\"), \"to carry over\", \"to transfer\" and that from μετά (\"meta\"), \"after, with, across\" + φέρω (\"pherō\"), \"to bear\", \"to carry\".\n\nMetaphors are most frequently compared with similes. It is said, for instance, that a metaphor is 'a condensed analogy' or 'analogical fusion' or that they 'operate in a similar fashion' or are 'based on the same mental process' or yet that 'the basic processes of analogy are at work in metaphor'. It is also pointed out that 'a border between metaphor and analogy is fuzzy' and 'the difference between them might be described (metaphorically) as the distance between things being compared'. A simile is a specific type of metaphor that uses the words \"like\" or \"as\" in comparing two objects. A metaphor asserts the objects in the comparison are identical on the point of comparison, while a simile merely asserts a similarity. For this reason a common-type metaphor is generally considered more forceful than a simile.\n\nThe metaphor category contains these specialized types:\n\nMetaphor, like other types of analogy, can be distinguished from metonymy as one of two fundamental modes of thought. Metaphor and analogy work by bringing together concepts from different conceptual domains, while metonymy uses one element from a given domain to refer to another closely related element. A metaphor creates new links between otherwise distinct conceptual domains, while a metonymy relies on the existing links within them.\n\nA dead metaphor is a metaphor in which the sense of a transferred image has become absent. The phrases \"to grasp a concept\" and \"to gather what you've understood\" use physical action as a metaphor for understanding. The audience does not need to visualize the action; dead metaphors normally go unnoticed. Some distinguish between a dead metaphor and a cliché. Others use \"dead metaphor\" to denote both.\n\nA mixed metaphor is a metaphor that leaps from one identification to a second inconsistent with the first, e.g.:\n\nThis form is often used as a parody of metaphor itself:\nAn extended metaphor, or conceit, sets up a principal subject with several subsidiary subjects or comparisons. In the above quote from \"As You Like It\", the world is first described as a stage and then the subsidiary subjects men and women are further described in the same context.\n\nAristotle writes in his work the \"Rhetoric\" that metaphors make learning pleasant: \"To learn easily is naturally pleasant to all people, and words signify something, so whatever words create knowledge in us are the pleasantest.\" When discussing Aristotle's \"Rhetoric\", Jan Garret stated \"metaphor most brings about learning; for when [Homer] calls old age \"stubble\", he creates understanding and knowledge through the genus, since both old age and stubble are [species of the genus of] things that have lost their bloom.\" Metaphors, according to Aristotle, have \"qualities of the exotic and the fascinating; but at the same time we recognize that strangers do not have the same rights as our fellow citizens\".\n\nOther rhetoricians have asserted the relevance of metaphor when used for a persuasive intent. Sonja K. Foss characterizes metaphors as \"nonliteral comparisons in which a word or phrase from one domain of experience is applied to another domain\".\nShe argues that since reality is mediated by the language we use to describe it, the metaphors we use shape the world and our interactions to it.\n\nThe term metaphor is used to describe more basic or general aspects of experience and cognition:\nMetaphors can be implied and extended throughout pieces of literature.\n\nSome theorists have suggested that metaphors are not merely stylistic, but that they are cognitively important as well. In \"Metaphors We Live By\", George Lakoff and Mark Johnson argue that metaphors are pervasive in everyday life, not just in language, but also in thought and action. A common definition of metaphor can be described as a comparison that shows how two things that are not alike in most ways are similar in another important way. They explain how a metaphor is simply understanding and experiencing one kind of thing in terms of another, called a \"conduit metaphor\". A speaker can put ideas or objects into containers, and then send them along a conduit to a listener who removes the object from the container to make meaning of it. Thus, communication is something that ideas go into, and the container is separate from the ideas themselves. Lakoff and Johnson give several examples of daily metaphors in use, including \"argument is war\" and \"time is money\". Metaphors are widely used in context to describe personal meaning. The authors suggest that communication can be viewed as a machine: \"Communication is not what one does with the machine, but is the machine itself.\"\n\nMetaphors can map experience between two nonlinguistic realms. Musicologist Leonard Meyer demonstrated how purely rhythmic and harmonic events can express human emotions. It is an open question whether synesthesia experiences are a sensory version of metaphor, the “source” domain being the presented stimulus, such as a musical tone, and the target domain, being the experience in another modality, such as color.\n\nArt theorist Robert Vischer argued that when we look at a painting, we \"feel ourselves into it\" by imagining our body in the posture of a nonhuman or inanimate object in the painting. For example, the painting \"The Lonely Tree\" by Caspar David Friedrich shows a tree with contorted, barren limbs. Looking at the painting, we imagine our limbs in a similarly contorted and barren shape, evoking a feeling of strain and distress. Nonlinguistic metaphors may be the foundation of our experience of visual and musical art, as well as dance and other art forms.\n\nIn historical onomasiology or in historical linguistics, a metaphor is defined as a semantic change based on a similarity in form or function between the original concept and the target concept named by a word.\n\nFor example, mouse: \"small, gray rodent with a long tail\" → \"small, gray, computer device with a long cord\".\n\nSome recent linguistic theories view all language in essence as metaphorical.\n\nFriedrich Nietzsche makes metaphor the conceptual center of his early theory of society in \"On Truth and Lies in the Non-Moral Sense\". Some sociologists have found his essay useful for thinking about metaphors used in society and for reflecting on their own use of metaphor. Sociologists of religion note the importance of metaphor in religious worldviews, and that it is impossible to think sociologically about religion without metaphor.\n\nAs a characteristic of speech and writing, metaphors can serve the poetic imagination. This allows Sylvia Plath, in her poem \"Cut\", to compare the blood issuing from her cut thumb to the running of a million soldiers, \"redcoats, every one\"; and enabling Robert Frost, in \"The Road Not Taken\", to compare a life to a journey.\n\nMetaphor can serve as a device for persuading an audience of the user's argument or thesis, the so-called rhetorical metaphor.\n\nCognitive linguists emphasize that metaphors serve to facilitate the understanding of one conceptual domain—typically an abstraction such as \"life\", \"theories\" or \"ideas\"—through expressions that relate to another, more familiar conceptual domain—typically more concrete, such as \"journey\", \"buildings\" or \"food\". For example: we \"devour\" a book of \"raw\" facts, try to \"digest\" them, \"stew\" over them, let them \"simmer on the back-burner\", \"regurgitate\" them in discussions, and \"cook\" up explanations, hoping they do not seem \"half-baked\".\n\nLakoff and Johnson greatly contributed to establishing the importance of conceptual metaphor as a framework for thinking in language, leading scholars to investigate the original ways in which writers used novel metaphors and question the fundamental frameworks of thinking in conceptual metaphors.\n\nFrom a sociological, cultural, or philosophical perspective, one asks to what extent ideologies maintain and impose conceptual patterns of thought by introducing, supporting, and adapting fundamental patterns of thinking metaphorically. To what extent does the ideology fashion and refashion the idea of the nation as a container with borders? How are enemies and outsiders represented? As diseases? As attackers? How are the metaphoric paths of fate, destiny, history, and progress represented? As the opening of an eternal monumental moment (German fascism)? Or as the path to communism (in Russian or Czech for example)?\n\nSome cognitive scholars have attempted to take on board the idea that different languages have evolved radically different concepts and conceptual metaphors, while others hold to the Sapir-Whorf hypothesis. German philologist Wilhelm von Humboldt contributed significantly to this debate on the relationship between culture, language, and linguistic communities. Humboldt remains, however, relatively unknown in English-speaking nations. Andrew Goatly, in \"Washing the Brain\", takes on board the dual problem of conceptual metaphor as a framework implicit in the language as a system and the way individuals and ideologies negotiate conceptual metaphors. Neural biological research suggests some metaphors are innate, as demonstrated by reduced metaphorical understanding in psychopathy.\n\nJames W. Underhill, in \"Creating Worldviews: Ideology, Metaphor & Language\" (Edinburgh UP), considers the way individual speech adopts and reinforces certain metaphoric paradigms. This involves a critique of both communist and fascist discourse. Underhill's studies are situated in Czech and German, which allows him to demonstrate the ways individuals are thinking both within and resisting the modes by which ideologies seek to appropriate key concepts such as \"the people\", \"the state\", \"history\", and \"struggle\".\n\nThough metaphors can be considered to be \"in\" language, Underhill's chapter on French, English and ethnolinguistics demonstrates that we cannot conceive of language or languages in anything other than metaphoric terms.\n\n\n\n"}
{"id": "9273782", "url": "https://en.wikipedia.org/wiki?curid=9273782", "title": "Monad (philosophy)", "text": "Monad (philosophy)\n\nMonad (from Greek μονάς \"monas\", \"singularity\" in turn from μόνος \"monos\", \"alone\"), refers in cosmogony to the Supreme Being, divinity, or the totality of all things. The concept was reportedly conceived by the Pythagoreans and may refer variously to a single source acting alone, or to an indivisible origin, or to both. The concept was later adopted by other philosophers, such as Leibniz, who referred to the monad as an elementary particle. It had a geometric counterpart, which was debated and discussed contemporaneously by the same groups of people.\n\nAccording to Hippolytus, the worldview was inspired by the Pythagoreans, who called the first thing that came into existence the \"monad\", which begat (bore) the dyad (from the Greek word for two), which begat the numbers, which begat the point, begetting lines or , etc. It meant divinity, the first being, or the totality of all beings, referring in cosmogony (creation theories) variously to source acting alone and/or an indivisible origin and equivalent comparators.\n\nPythagorean and Platonic philosophers like Plotinus and Porphyry condemned Gnosticism (see Neoplatonism and Gnosticism) for their treatment of the monad.\n\nFor the Pythagoreans, the generation of number series was related to objects of geometry as well as cosmogony. According to Diogenes Laertius, from the monad evolved the dyad; from it numbers; from numbers, points; then lines, two-dimensional entities, three-dimensional entities, bodies, culminating in the four elements earth, water, fire and air, from which the rest of our world is built up.\n\nThe term monad was later adopted from Greek philosophy by Giordano Bruno, Leibniz (Monadology), John Dee, and others.\n\n\n"}
{"id": "21233", "url": "https://en.wikipedia.org/wiki?curid=21233", "title": "Nirvana", "text": "Nirvana\n\nIn Indian religions, \"nirvana\" is synonymous with \"moksha\" and \"mukti\". All Indian religions assert it to be a state of perfect quietude, freedom, highest happiness as well as the liberation from or ending of \"samsara\", the repeating cycle of birth, life and death.\n\nHowever, Buddhist and non-Buddhist traditions describe these terms for liberation differently. In the Buddhist context, \"nirvana\" refers to realization of non-self and emptiness, marking the end of rebirth by stilling the \"fires\" that keep the process of rebirth going. In Hindu philosophy, it is the union of or the realization of the identity of Atman with Brahman, depending on the Hindu tradition. In Jainism, it is also the soteriological goal, it represents the release of a soul from karmic bondage and samsara.\n\nThe word \"nirvāṇa\", states Steven Collins, is from the verbal root \"vā\" \"blow\" in the form of past participle \"vāna\" \"blown\", prefixed with the preverb \"nis\" meaning \"out\". Hence the original meaning of the word is \"blown out, extinguished\". Sandhi changes the sounds: the \"v\" of \"vāna\" causes \"nis\" to become \"nir\", and then the \"r\" of \"nir\" causes retroflexion of the following \"n\": \"nis+vāna\" > \"nirvāṇa\".\n\nThe term \"nirvana\" in the soteriological sense of \"blown out, extinguished\" state of liberation does not appear in the Vedas nor in the Upanishads. According to Collins, \"the Buddhists seem to have been the first to call it \"nirvana\".\" However, the ideas of spiritual liberation using different terminology, with the concept of soul and Brahman, appears in Vedic texts and Upanishads, such as in verse 4.4.6 of the Brihadaranyaka Upanishad. This may have been deliberate use of words in early Buddhism, suggests Collins, since Atman and Brahman were described in Vedic texts and Upanishads with the imagery of fire, as something good, desirable and liberating.\n\n\"Nirvāṇa\" is a term found in the texts of all major Indian religions – Buddhism, Hinduism, Jainism and Sikhism. It refers to the profound peace of mind that is acquired with \"moksha\", liberation from samsara, or release from a state of suffering, after respective spiritual practice or sādhanā.\n\nThe idea of \"moksha\" is connected to the Vedic culture, where it conveyed a notion of \"amrtam\", \"immortality\", and also a notion of a \"timeless\", \"unborn\", or \"the still point of the turning world of time\". It was also its timeless structure, the whole underlying \"the spokes of the invariable but incessant wheel of time\". The hope for life after death started with notions of going to the worlds of the Fathers or Ancestors and/or the world of the Gods or Heaven.\n\nThe earliest Vedic texts incorporate the concept of life, followed by an afterlife in heaven and hell based on cumulative virtues (merit) or vices (demerit). However, the ancient Vedic Rishis challenged this idea of afterlife as simplistic, because people do not live an equally moral or immoral life. Between generally virtuous lives, some are more virtuous; while evil too has degrees, and either permanent heaven or permanent hell is disproportionate. The Vedic thinkers introduced the idea of an afterlife in heaven or hell in proportion to one's merit, and when this runs out, one returns and is reborn. The idea of rebirth following \"running out of merit\" appears in Buddhist texts as well. This idea appears in many ancient and medieval texts, as \"Saṃsāra\", or the endless cycle of life, death, rebirth and redeath, such as section 6:31 of the \"Mahabharata\" and verse 9.21 of the \"Bhagavad Gita\". The Saṃsara, the life after death, and what impacts rebirth came to be seen as dependent on karma.\n\nThe liberation from Saṃsāra \"d\"eveloped as an ultimate goal and soteriological value in the Indian culture, and called by different terms such as nirvana, moksha, mukti and kaivalya. This basic scheme underlies Hinduism, Jainism and Buddhism, where \"the ultimate aim is the timeless state of \"moksa\", or, as the Buddhists first seem to have called it, nirvana.\"\n\nAlthough the term occurs in the literatures of a number of ancient Indian traditions, the concept is most commonly associated with Buddhism. It was later adopted by other Indian religions, but with different meanings and description (\"Moksha\"), such as in the Hindu text \"Bhagavad Gita\" of the \"Mahabharata\".\n\nNirvana (\"nibbana\") literally means \"blowing out\" or \"quenching\". It is the most used as well as the earliest term to describe the soteriological goal in Buddhism: release from the cycle of rebirth (\"saṃsāra\"). Nirvana is part of the Third Truth on \"cessation of dukkha\" in the Four Noble Truths doctrine of Buddhism. It is the goal of the Noble Eightfold Path.\n\nThe Buddha is believed in the Buddhist scholastic tradition to have realized two types of nirvana, one at enlightenment, and another at his death. The first is called \"sopadhishesa-nirvana\" (nirvana with a remainder), the second \"parinirvana\" or \"anupadhishesa-nirvana\" (nirvana without remainder, or final nirvana).\n\nIn the Buddhist tradition, nirvana is described as the extinguishing of the \"fires\" that cause rebirths and associated suffering. The Buddhist texts identify these three \"three fires\" or \"three poisons\" as \"raga\" (greed, sensuality), \"dvesha\" (aversion, hate) and \"avidyā\" or \"moha\" (ignorance, delusion).\n\nThe state of nirvana is also described in Buddhism as cessation of all afflictions, cessation of all actions, cessation of rebirths and suffering that are a consequence of afflictions and actions. Liberation is described as identical to \"anatta\" (\"anatman\", non-self, lack of any self). In Buddhism, liberation is achieved when all things and beings are understood to be with no Self. Nirvana is also described as identical to achieving \"sunyata\" (emptiness), where there is no essence or fundamental nature in anything, and everything is empty.\n\nIn time, with the development of Buddhist doctrine, other interpretations were given, such as being an unconditioned state, a fire going out for lack of fuel, abandoning weaving (\"vana\") together of life after life, and the elimination of desire. However, Buddhist texts have asserted since ancient times that nirvana is more than \"destruction of desire\", it is \"the object of the knowledge\" of the Buddhist path.\n\nThe most ancient texts of Hinduism such as the Vedas and early Upanishads don't mention the soteriological term \"Nirvana\". This term is found in texts such as the Bhagavad Gita and the Nirvana Upanishad, likely composed in the post-Buddha era. The concept of Nirvana is described differently in Buddhist and Hindu literature. Hinduism has the concept of Atman – the soul, self – asserted to exist in every living being, while Buddhism asserts through its \"anatman\" doctrine that there is no Atman in any being. Nirvana in Buddhism is \"stilling mind, cessation of desires, and action\" unto emptiness, states Jeaneane Fowler, while nirvana in post-Buddhist Hindu texts is also \"stilling mind but not inaction\" and \"not emptiness\", rather it is the knowledge of true Self (Atman) and the acceptance of its universality and unity with metaphysical Brahman.\n\nThe ancient soteriological concept in Hinduism is moksha, described as the liberation from the cycle of birth and death through self-knowledge and the eternal connection of Atman (soul, self) and metaphysical Brahman. Moksha is derived from the root \"muc*\" () which means free, let go, release, liberate; Moksha means \"liberation, freedom, emancipation of the soul\". In the Vedas and early Upanishads, the word mucyate () appears, which means to be set free or release - such as of a horse from its harness.\n\nThe traditions within Hinduism state that there are multiple paths (\"marga\") to moksha: \"jnana-marga\", the path of knowledge; \"bhakti-marga\", the path of devotion; and \"karma-marga\", the path of action.\n\nThe term Brahma-nirvana appears in verses 2.72 and 5.24-26 of the Bhagavad Gita. It is the state of release or liberation; the union with the Brahman. According to Easwaran, it is an experience of blissful egolessness.\n\nAccording to Zaehner, Johnson and other scholars, \"nirvana\" in the Gita is a Buddhist term adopted by the Hindus. Zaehner states it was used in Hindu texts for the first time in the Bhagavad Gita, and that the idea therein in verse 2.71-72 to \"suppress one's desires and ego\" is also Buddhist. According to Johnson the term \"nirvana\" is borrowed from the Buddhists to confuse the Buddhists, by linking the Buddhist nirvana state to the pre-Buddhist Vedic tradition of metaphysical absolute called Brahman.\n\nAccording to Mahatma Gandhi, the Hindu and Buddhist understanding of \"nirvana\" are different because the nirvana of the Buddhists is shunyata, emptiness, but the nirvana of the Gita means peace and that is why it is described as brahma-nirvana (oneness with Brahman).\n\nThe terms \"moksa\" and \"nirvana\" are often used interchangeably in the Jain texts.\n\nUttaradhyana Sutra provides an account of Sudharman – also called Gautama, and one of the disciples of Mahavira – explaining the meaning of nirvana to Kesi, a disciple of Parshva.\n\nThe term \"Nirvana\" (also mentioned is \"parinirvana\") in the thirteenth or fourtheenth century Manichaean work \"The great song to Mani\" and \"The story of the Death of Mani\", referring to the \"realm of light\".\n\nThe concept of liberation as \"extinction of suffering\", along with the idea of \"sansara\" as the \"cycle of rebirth\" is also part of Sikhism. Nirvana appears in Sikh texts as the term \"Nirban\". However, the more common term is \"Mukti\" or \"Moksh\", a salvation concept wherein loving devotion to God is emphasized for liberation from endless cycle of rebirths.\n\n\n\n"}
{"id": "150159", "url": "https://en.wikipedia.org/wiki?curid=150159", "title": "Noether's theorem", "text": "Noether's theorem\n\nNoether's (first) theorem states that every differentiable symmetry of the action of a physical system has a corresponding conservation law. The theorem was proven by mathematician Emmy Noether in 1915 and published in 1918, although a special case was proven by E. Cosserat & F. Cosserat in 1909. The action of a physical system is the integral over time of a Lagrangian function (which may or may not be an integral over space of a Lagrangian density function), from which the system's behavior can be determined by the principle of least action. This theorem only applies to continuous symmetries over physical space.\n\nNoether's theorem is used in theoretical physics and the calculus of variations. A generalization of the formulations on constants of motion in Lagrangian and Hamiltonian mechanics (developed in 1788 and 1833, respectively), it does not apply to systems that cannot be modeled with a Lagrangian alone (e.g. systems with a Rayleigh dissipation function). In particular, dissipative systems with continuous symmetries need not have a corresponding conservation law.\n\nAs an illustration, if a physical system behaves the same regardless of how it is oriented in space, its Lagrangian is symmetric under continuous rotations: from this symmetry, Noether's theorem dictates that the angular momentum of the system be conserved, as a consequence of its laws of motion. The physical system itself need not be symmetric; a jagged asteroid tumbling in space conserves angular momentum despite its asymmetry. It is the laws of its motion that are symmetric.\n\nAs another example, if a physical process exhibits the same outcomes regardless of place or time, then its Lagrangian is symmetric under continuous translations in space and time respectively: by Noether's theorem, these symmetries account for the conservation laws of linear momentum and energy within this system, respectively.\n\nAs a final example, if the behavior of a physical system does not change upon spatial or temporal reflection, then its Lagrangian has reflection symmetry and time reversal symmetry respectively: Noether's theorem says that these symmetries result in the conservation laws of parity and entropy, respectively.\n\nNoether's theorem is important, both because of the insight it gives into conservation laws, and also as a practical calculational tool. It allows investigators to determine the conserved quantities (invariants) from the observed symmetries of a physical system. Conversely, it allows researchers to consider whole classes of hypothetical Lagrangians with given invariants, to describe a physical system. As an illustration, suppose that a physical theory is proposed which conserves a quantity \"X\". A researcher can calculate the types of Lagrangians that conserve \"X\" through a continuous symmetry. Due to Noether's theorem, the properties of these Lagrangians provide further criteria to understand the implications and judge the fitness of the new theory.\n\nThere are numerous versions of Noether's theorem, with varying degrees of generality. The original version applied only to ordinary differential equations (used for describing distinct particles) and not partial differential equations (used for describing fields). The original versions also assume that the Lagrangian depends only upon the first derivative, while later versions generalize the theorem to Lagrangians depending on the \"n\" derivative. There are natural quantum counterparts of this theorem, expressed in the Ward–Takahashi identities. Generalizations of Noether's theorem to superspaces also exist.\n\nAll fine technical points aside, Noether's theorem can be stated informally\n\nA more sophisticated version of the theorem involving fields states that:\n\nThe word \"symmetry\" in the above statement refers more precisely to the covariance of the form that a physical law takes with respect to a one-dimensional Lie group of transformations satisfying certain technical criteria. The conservation law of a physical quantity is usually expressed as a continuity equation.\n\nThe formal proof of the theorem utilizes the condition of invariance to derive an expression for a current associated with a conserved physical quantity. \nIn modern (since ca. 1980) terminology, the conserved quantity is called the \"Noether charge\", while the flow carrying that charge is called the \"Noether current\". The Noether current is defined up to a solenoidal (divergenceless) vector field.\n\nIn the context of gravitation, Felix Klein's statement of Noether's theorem for action \"I\" stipulates for the invariants: \nA conservation law states that some quantity \"X\" in the mathematical description of a system's evolution remains constant throughout its motion — it is an invariant. Mathematically, the rate of change of \"X\" (its derivative with respect to time) is zero,\n\nSuch quantities are said to be conserved; they are often called constants of motion (although motion \"per se\" need not be involved, just evolution in time). For example, if the energy of a system is conserved, its energy is invariant at all times, which imposes a constraint on the system's motion and may help in solving for it. Aside from insights that such constants of motion give into the nature of a system, they are a useful calculational tool; for example, an approximate solution can be corrected by finding the nearest state that satisfies the suitable conservation laws.\n\nThe earliest constants of motion discovered were momentum and energy, which were proposed in the 17th century by René Descartes and Gottfried Leibniz on the basis of collision experiments, and refined by subsequent researchers. Isaac Newton was the first to enunciate the conservation of momentum in its modern form, and showed that it was a consequence of Newton's third law. According to general relativity, the conservation laws of linear momentum, energy and angular momentum are only exactly true globally when expressed in terms of the sum of the stress–energy tensor (non-gravitational stress–energy) and the Landau–Lifshitz stress–energy–momentum pseudotensor (gravitational stress–energy). The local conservation of non-gravitational linear momentum and energy in a free-falling reference frame is expressed by the vanishing of the covariant divergence of the stress–energy tensor. Another important conserved quantity, discovered in studies of the celestial mechanics of astronomical bodies, is the Laplace–Runge–Lenz vector.\n\nIn the late 18th and early 19th centuries, physicists developed more systematic methods for discovering invariants. A major advance came in 1788 with the development of Lagrangian mechanics, which is related to the principle of least action. In this approach, the state of the system can be described by any type of generalized coordinates q; the laws of motion need not be expressed in a Cartesian coordinate system, as was customary in Newtonian mechanics. The action is defined as the time integral \"I\" of a function known as the Lagrangian \"L\"\n\nwhere the dot over q signifies the rate of change of the coordinates q,\n\nHamilton's principle states that the physical path q(\"t\")—the one actually taken by the system—is a path for which infinitesimal variations in that path cause no change in \"I\", at least up to first order. This principle results in the Euler–Lagrange equations,\n\nThus, if one of the coordinates, say \"q\", does not appear in the Lagrangian, the right-hand side of the equation is zero, and the left-hand side requires that\n\nwhere the momentum\n\nis conserved throughout the motion (on the physical path).\n\nThus, the absence of the ignorable coordinate \"q\" from the Lagrangian implies that the Lagrangian is unaffected by changes or transformations of \"q\"; the Lagrangian is invariant, and is said to exhibit a symmetry under such transformations. This is the seed idea generalized in Noether's theorem.\n\nSeveral alternative methods for finding conserved quantities were developed in the 19th century, especially by William Rowan Hamilton. For example, he developed a theory of canonical transformations which allowed changing coordinates so that some coordinates disappeared from the Lagrangian, as above, resulting in conserved canonical momenta. Another approach, and perhaps the most efficient for finding conserved quantities, is the Hamilton–Jacobi equation.\n\nThe essence of Noether's theorem is generalizing the ignorable coordinates outlined.\n\nOne can assume that the Lagrangian \"L\" defined above is invariant under small perturbations (warpings) of the time variable \"t\" and the generalized coordinates q. One may write\n\nwhere the perturbations \"δt\" and \"δ\"q are both small, but variable. For generality, assume there are (say) \"N\" such symmetry transformations of the action, i.e. transformations leaving the action unchanged; labelled by an index \"r\" = 1, 2, 3, …, \"N\".\n\nThen the resultant perturbation can be written as a linear sum of the individual types of perturbations,\nwhere \"ε\" are infinitesimal parameter coefficients corresponding to each: \nFor translations, Q is a constant with units of length; for rotations, it is an expression linear in the components of q, and the parameters make up an angle.\n\nUsing these definitions, Noether showed that the \"N\" quantities\n\n(which have the dimensions of [energy]·[time] + [momentum]·[length] = [action]) are conserved (constants of motion).\n\n\nFor illustration, consider a Lagrangian that does not depend on time, i.e., that is invariant (symmetric) under changes \"t\" → \"t\" + δ\"t\", without any change in the coordinates q. In this case, \"N\" = 1, \"T\" = 1 and Q = 0; the corresponding conserved quantity is the total energy \"H\"\n\n\nConsider a Lagrangian which does not depend on an (\"ignorable\", as above) coordinate \"q\"; so it is invariant (symmetric) under changes \"q\" → \"q\" + \"δq\". In that case, \"N\" = 1, \"T\" = 0, and \"Q\" = 1; the conserved quantity is the corresponding momentum \"p\"\n\nIn special and general relativity, these apparently separate conservation laws are aspects of a single conservation law, that of the stress–energy tensor, that is derived in the next section.\n\n\nThe conservation of the angular momentum L = r × p is analogous to its linear momentum counterpart. It is assumed that the symmetry of the Lagrangian is rotational, i.e., that the Lagrangian does not depend on the absolute orientation of the physical system in space. For concreteness, assume that the Lagrangian does not change under small rotations of an angle \"δθ\" about an axis n; such a rotation transforms the Cartesian coordinates by the equation\n\nSince time is not being transformed, \"T\"=0. Taking \"δθ\" as the \"ε\" parameter and the Cartesian coordinates r as the generalized coordinates q, the corresponding Q variables are given by\n\nThen Noether's theorem states that the following quantity is conserved,\n\nIn other words, the component of the angular momentum L along the n axis is conserved.\n\nIf n is arbitrary, i.e., if the system is insensitive to any rotation, then every component of L is conserved; in short, angular momentum is conserved.\n\nAlthough useful in its own right, the version of Noether's theorem just given is a special case of the general version derived in 1915. To give the flavor of the general theorem, a version of the Noether theorem for continuous fields in four-dimensional space–time is now given. Since field theory problems are more common in modern physics than mechanics problems, this field theory version is the most commonly used (or most often implemented) version of Noether's theorem.\n\nLet there be a set of differentiable fields formula_17 defined over all space and time; for example, the temperature formula_18 would be representative of such a field, being a number defined at every place and time. The principle of least action can be applied to such fields, but the action is now an integral over space and time\n\nA continuous transformation of the fields formula_17 can be written infinitesimally as\n\nwhere formula_22 is in general a function that may depend on both formula_23 and formula_17. The condition for formula_22 to generate a physical symmetry is that the action formula_26 is left invariant. This will certainly be true if the Lagrangian density formula_27 is left invariant, but it will also be true if the Lagrangian changes by a divergence,\n\nsince the integral of a divergence becomes a boundary term according to the divergence theorem. A system described by a given action might have multiple independent symmetries of this type, indexed by formula_29 so the most general symmetry transformation would be written as\n\nwith the consequence\n\nFor such systems, Noether's theorem states that there are formula_32 conserved current densities\n\nIn such cases, the conservation law is expressed in a four-dimensional way\n\nwhich expresses the idea that the amount of a conserved quantity within a sphere cannot change unless some of it flows out of the sphere. For example, electric charge is conserved; the amount of charge within a sphere cannot change unless some of the charge leaves the sphere.\n\nFor illustration, consider a physical system of fields that behaves the same under translations in time and space, as considered above; in other words, formula_37 is constant in its third argument. In that case, \"N\" = 4, one for each dimension of space and time. An infinitesimal translation in space, formula_38 (with formula_39 denoting the Kronecker delta), affects the fields as formula_40: that is, relabelling the coordinates is equivalent to leaving the coordinates in place while translating the field itself, which in turn is equivalent to transforming the field by replacing its value at each point formula_23 with the value at the point formula_42 \"behind\" it which would be mapped onto formula_23 by the infinitesimal displacement under consideration. Since this is infinitesimal, we may write this transformation as\n\nThe Lagrangian density transforms in the same way, formula_45, so\n\nand thus Noether's theorem corresponds to the conservation law for the stress–energy tensor \"T\", where we have used formula_47 in place of formula_35. To wit, by using the expression given earlier, and collecting the four conserved currents (one for each formula_47) into a tensor formula_50, Noether's theorem gives\n\nwith\n\nThe conservation of electric charge, by contrast, can be derived by considering \"Ψ\" linear in the fields \"φ\" rather than in the derivatives. In quantum mechanics, the probability amplitude \"ψ\"(x) of finding a particle at a point x is a complex field \"φ\", because it ascribes a complex number to every point in space and time. The probability amplitude itself is physically unmeasurable; only the probability \"p\" = |\"ψ\"| can be inferred from a set of measurements. Therefore, the system is invariant under transformations of the \"ψ\" field and its complex conjugate field \"ψ\" that leave |\"ψ\"| unchanged, such as\n\na complex rotation. In the limit when the phase \"θ\" becomes infinitesimally small, \"δθ\", it may be taken as the parameter \"ε\", while the \"Ψ\" are equal to \"iψ\" and −\"iψ\"*, respectively. A specific example is the Klein–Gordon equation, the relativistically correct version of the Schrödinger equation for spinless particles, which has the Lagrangian density\n\nIn this case, Noether's theorem states that the conserved (∂⋅\"j\" = 0) current equals\n\nwhich, when multiplied by the charge on that species of particle, equals the electric current density due to that type of particle. This \"gauge invariance\" was first noted by Hermann Weyl, and is one of the prototype gauge symmetries of physics.\n\nConsider the simplest case, a system with one independent variable, time. Suppose the dependent variables q are such that the action integral\n\nis invariant under brief infinitesimal variations in the dependent variables. In other words, they satisfy the Euler–Lagrange equations\n\nAnd suppose that the integral is invariant under a continuous symmetry. Mathematically such a symmetry is represented as a flow, φ, which acts on the variables as follows\n\nwhere \"ε\" is a real variable indicating the amount of flow, and \"T\" is a real constant (which could be zero) indicating how much the flow shifts time.\n\nThe action integral flows to\n\nwhich may be regarded as a function of \"ε\". Calculating the derivative at \"ε\"' = 0 and using Leibniz's rule, we get\n\nNotice that the Euler–Lagrange equations imply\n\nSubstituting this into the previous equation, one gets\n\nAgain using the Euler–Lagrange equations we get\n\nSubstituting this into the previous equation, one gets\n\nFrom which one can see that\n\nis a constant of the motion, i.e., it is a conserved quantity. Since φ[q, 0] = q, we get formula_71 and so the conserved quantity simplifies to\n\nTo avoid excessive complication of the formulas, this derivation assumed that the flow does not change as time passes. The same result can be obtained in the more general case.\n\nNoether's theorem may also be derived for tensor fields \"φ\" where the index \"A\" ranges over the various components of the various tensor fields. These field quantities are functions defined over a four-dimensional space whose points are labeled by coordinates \"x\" where the index \"μ\" ranges over time (\"μ\" = 0) and three spatial dimensions (\"μ\" = 1, 2, 3). These four coordinates are the independent variables; and the values of the fields at each event are the dependent variables. Under an infinitesimal transformation, the variation in the coordinates is written\n\nwhereas the transformation of the field variables is expressed as\n\nBy this definition, the field variations \"δφ\" result from two factors: intrinsic changes in the field themselves and changes in coordinates, since the transformed field \"α\" depends on the transformed coordinates ξ. To isolate the intrinsic changes, the field variation at a single point \"x\" may be defined\n\nIf the coordinates are changed, the boundary of the region of space–time over which the Lagrangian is being integrated also changes; the original boundary and its transformed version are denoted as Ω and Ω’, respectively.\n\nNoether's theorem begins with the assumption that a specific transformation of the coordinates and field variables does not change the action, which is defined as the integral of the Lagrangian density over the given region of spacetime. Expressed mathematically, this assumption may be written as\n\nwhere the comma subscript indicates a partial derivative with respect to the coordinate(s) that follows the comma, e.g.\n\nSince ξ is a dummy variable of integration, and since the change in the boundary Ω is infinitesimal by assumption, the two integrals may be combined using the four-dimensional version of the divergence theorem into the following form\n\nThe difference in Lagrangians can be written to first-order in the infinitesimal variations as\n\nHowever, because the variations are defined at the same point as described above, the variation and the derivative can be done in reverse order; they commute\n\nUsing the Euler–Lagrange field equations\n\nthe difference in Lagrangians can be written neatly as\n\nThus, the change in the action can be written as\n\nSince this holds for any region Ω, the integrand must be zero\n\nFor any combination of the various symmetry transformations, the perturbation can be written\n\nwhere formula_87 is the Lie derivative of φ in the \"X\" direction. When \"φ\" is a scalar or formula_88,\n\nThese equations imply that the field variation taken at one point equals\n\nDifferentiating the above divergence with respect to \"ε\" at \"ε\" = 0 and changing the sign yields the conservation law\n\nwhere the conserved current equals\n\nSuppose we have an \"n\"-dimensional oriented Riemannian manifold, \"M\" and a target manifold \"T\". Let formula_93 be the configuration space of smooth functions from \"M\" to \"T\". (More generally, we can have smooth sections of a fiber bundle over \"M\".)\n\nExamples of this \"M\" in physics include:\n\nNow suppose there is a functional\n\ncalled the action. (Note that it takes values into R, rather than C; this is for physical reasons, and doesn't really matter for this proof.)\n\nTo get to the usual version of Noether's theorem, we need additional restrictions on the action. We assume formula_96 is the integral over \"M\" of a function\n\ncalled the Lagrangian density, depending on φ, its derivative and the position. In other words, for φ in formula_93\n\nSuppose we are given boundary conditions, i.e., a specification of the value of φ at the boundary if \"M\" is compact, or some limit on φ as \"x\" approaches ∞. Then the subspace of formula_93 consisting of functions φ such that all functional derivatives of formula_26 at φ are zero, that is:\n\nand that φ satisfies the given boundary conditions, is the subspace of on shell solutions. (See principle of stationary action)\n\nNow, suppose we have an infinitesimal transformation on formula_93, generated by a functional derivation, \"Q\" such that\n\nfor all compact submanifolds \"N\" or in other words,\n\nfor all \"x\", where we set\n\nIf this holds on shell and off shell, we say \"Q\" generates an off-shell symmetry. If this only holds on shell, we say \"Q\" generates an on-shell symmetry. Then, we say \"Q\" is a generator of a one parameter symmetry Lie group.\n\nNow, for any \"N\", because of the Euler–Lagrange theorem, on shell (and only on-shell), we have\n\nSince this is true for any \"N\", we have\n\nBut this is the continuity equation for the current formula_109 defined by:\n\nwhich is called the Noether current associated with the symmetry. The continuity equation tells us that if we integrate this current over a space-like slice, we get a conserved quantity called the Noether charge (provided, of course, if \"M\" is noncompact, the currents fall off sufficiently fast at infinity).\n\nNoether's theorem is an on shell theorem: it relies on use of the equations of motion—the classical path. It reflects the relation between the boundary conditions and the variational principle. Assuming no boundary terms in the action, Noether's theorem implies that\n\nThe quantum analogs of Noether's theorem involving expectation values, e.g. ⟨∫\"d\"\"x\" ∂·\"J\"⟩ = 0, probing off shell quantities as well are the Ward–Takahashi identities.\n\nSuppose we have two symmetry derivations \"Q\" and \"Q\". Then, [\"Q\", \"Q\"] is also a symmetry derivation. Let's see this explicitly. Let's say\n\nand\n\nThen,\n\nwhere \"f\" = \"Q\"[\"f\"] − \"Q\"[\"f\"]. So,\n\nThis shows we can extend Noether's theorem to larger Lie algebras in a natural way.\n\nThis applies to \"any\" local symmetry derivation \"Q\" satisfying \"QS\" ≈ 0, and also to more general local functional differentiable actions, including ones where the Lagrangian depends on higher derivatives of the fields. Let \"ε\" be any arbitrary smooth function of the spacetime (or time) manifold such that the closure of its support is disjoint from the boundary. \"ε\" is a test function. Then, because of the variational principle (which does \"not\" apply to the boundary, by the way), the derivation distribution q generated by \"q\"[\"ε\"][Φ(\"x\")] = \"ε\"(\"x\")\"Q\"[Φ(\"x\")] satisfies \"q\"[\"ε\"][\"S\"] ≈ 0 for every \"ε\", or more compactly, \"q\"(\"x\")[\"S\"] ≈ 0 for all \"x\" not on the boundary (but remember that \"q\"(\"x\") is a shorthand for a derivation \"distribution\", not a derivation parametrized by \"x\" in general). This is the generalization of Noether's theorem.\n\nTo see how the generalization is related to the version given above, assume that the action is the spacetime integral of a Lagrangian that only depends on φ and its first derivatives. Also, assume\n\nThen,\n\nfor all \"ε\".\n\nMore generally, if the Lagrangian depends on higher derivatives, then\n\nLooking at the specific case of a Newtonian particle of mass \"m\", coordinate \"x\", moving under the influence of a potential \"V\", coordinatized by time \"t\". The action, \"S\", is:\n\nThe first term in the brackets is the kinetic energy of the particle, whilst the second is its potential energy. Consider the generator of time translations \"Q\" = \"d/dt\". In other words, formula_120. Note that \"x\" has an explicit dependence on time, whilst \"V\" does not; consequently:\n\nso we can set\n\nThen,\n\nThe right hand side is the energy, and Noether's theorem states that formula_124 (i.e. the principle of conservation of energy is a consequence of invariance under time translations.\n\nMore generally, if the Lagrangian does not depend explicitly on time, the quantity\n\n(called the Hamiltonian) is conserved.\n\nStill considering 1-dimensional time, let\n\ni.e. \"N\" Newtonian particles where the potential only depends pairwise upon the relative displacement.\n\nFor formula_127, let's consider the generator of Galilean transformations (i.e. a change in the frame of reference). In other words,\n\nNote that\n\nThis has the form of formula_130 so we can set\n\nThen,\n\nwhere formula_135 is the total momentum, \"M\" is the total mass and formula_136 is the center of mass. Noether's theorem states:\n\nBoth examples 1 and 2 are over a 1-dimensional manifold (time). An example involving spacetime is a conformal transformation of a massless real scalar field with a quartic potential in (3 + 1)-Minkowski spacetime.\n\nFor \"Q\", consider the generator of a spacetime rescaling. In other words,\n\nThe second term on the right hand side is due to the \"conformal weight\" of formula_17. Note that\n\nThis has the form of\n\n(where we have performed a change of dummy indices) so set\n\nThen\n\nNoether's theorem states that formula_145 (as one may explicitly check by substituting the Euler–Lagrange equations into the left hand side).\n\nNote that if one tries to find the Ward–Takahashi analog of this equation, one runs into a problem because of anomalies.\n\nApplication of Noether's theorem allows physicists to gain powerful insights into any general theory in physics, by just analyzing the various transformations that would make the form of the laws involved invariant. For example:\n\n\nIn quantum field theory, the analog to Noether's theorem, the Ward–Takahashi identity, yields further conservation laws, such as the conservation of electric charge from the invariance with respect to a change in the phase factor of the complex field of the charged particle and the associated gauge of the electric potential and vector potential.\n\nThe Noether charge is also used in calculating the entropy of stationary black holes.\n\n\n\n\n"}
{"id": "7786484", "url": "https://en.wikipedia.org/wiki?curid=7786484", "title": "Non-contact force", "text": "Non-contact force\n\nA Non Contact Force is a force which acts on an object without coming physically in contact with it. The most familiar example of a non-contact force is gravity, which confers weight. In contrast a contact force is a force applied to a body by another body that \"is\" in contact with it.\n\nAll four known fundamental interactions are non-contact forces:\n\n\n"}
{"id": "35480438", "url": "https://en.wikipedia.org/wiki?curid=35480438", "title": "Nudge theory", "text": "Nudge theory\n\nNudge is a concept in behavioral science, political theory and behavioral economics which proposes positive reinforcement and indirect suggestions as ways to influence the behavior and decision making of groups or individuals. Nudging contrasts with other ways to achieve compliance, such as education, legislation or enforcement. The concept has influenced British and American politicians. Several nudge units exist around the world at the national level (UK, Germany, Japan and others) as well as at the international level (e.g. OECD, World Bank, UN).\n\nThe first formulation of the term and associated principles was developed in cybernetics by James Wilk before 1995 and described by Brunel University academic D. J. Stewart as \"the art of the nudge\" (sometimes referred to as micronudges). It also drew on methodological influences from clinical psychotherapy tracing back to Gregory Bateson, including contributions from Milton Erickson, Watzlawick, Weakland and Fisch, and Bill O'Hanlon. In this variant, the nudge is a microtargetted design geared towards a specific group of people, irrespective of the scale of intended intervention.\n\nIn 2008, Richard Thaler and Cass Sunstein's book \"Nudge: Improving Decisions About Health, Wealth, and Happiness\" brought nudge theory to prominence. It also gained a following among US and UK politicians, in the private sector and in public health. The authors refer to influencing behaviour without coercion as libertarian paternalism and the influencers as choice architects. Thaler and Sunstein defined their concept as:\n\nIn this form, drawing on behavioral economics, the nudge is more generally applied to influence behaviour.\n\nOne of the most frequently cited examples of a nudge is the etching of the image of a housefly into the men's room urinals at Amsterdam's Schiphol Airport, which is intended to \"improve the aim\".\n\nA nudge makes it more likely that an individual will make a particular choice, or behave in a particular way, by altering the environment so that automatic cognitive processes are triggered to favour the desired outcome.\n\nAn individual’s behaviour is not always in alignment with their intentions (termed a value-action gap). It is common knowledge that humans are not fully rational beings; that is, people will often do something that is not in their own self interest, even when they are aware that their actions are not in their best interest. As an example, when hungry, dieters often under-estimate their ability to lose weight, and their intentions to eat healthy can be temporarily weakened until they are satiated.\n\nThaler and Sunstein describe two distinct systems for processing information as to why people sometimes act against their own self-interest: System 1 is fast, automatic, and highly susceptible to environmental influences; System 2 processing is slow, reflective, and takes into account explicit goals and intentions. When situations are overly complex or overwhelming for an individual’s cognitive capacity, or when an individual is faced with time-constraints or other pressures, System 1 processing takes over decision-making. System 1 processing relies on various judgmental heuristics to make decisions, resulting in faster decisions. Unfortunately, this can also lead to sub-optimal decisions. In fact, Thaler and Sunstein trace maladaptive behaviour to situations in which System 1 processing over-rides an individual’s explicit values and goals. It is well documented that habitual behaviour is resistant to change without a disruption to the environmental cues that trigger that behaviour.\n\nNudging techniques aim to use judgmental heuristics to our advantage. In other words, a nudge alters the environment so that when heuristic, or System 1, decision-making is used, the resulting choice will be the most positive or desired outcome. An example of such a nudge is switching the placement of junk food in a store, so that fruit and other healthy options are located next to the cash register, while junk food is relocated to another part of the store.\n\nNudges are small changes in environment that are easy and inexpensive to implement. Several different techniques exist for nudging, including defaults, social proof heuristics, and increasing the salience of the desired option.\n\nA default option is the option an individual automatically receives if he or she does nothing. People are more likely to choose a particular option if it is the default option. For example, Pichert & Katsikopoulos found that a greater number of consumers chose the renewable energy option for electricity when it was offered as the default option.\n\nA social proof heuristic refers to the tendency for individuals to look at the behavior of other people to help guide their own behavior. Studies have found some success in using social proof heuristics to nudge individuals to make healthier food choices.\n\nWhen an individual’s attention is drawn towards a particular option, that option will become more salient to the individual, and he or she will be more likely to choose to that option. As an example, in snack shops at train stations in the Netherlands, consumers purchased more fruit and healthy snack options when they were relocated next to the cash register.\n\nIn 2008, the United States appointed Sunstein, who helped develop the theory, as administrator of the Office of Information and Regulatory Affairs. \n\nNotable applications of nudge theory include the formation of the British Behavioural Insights Team in 2010. It is often called the \"Nudge Unit\", at the British Cabinet Office, headed by David Halpern. \n\nBoth Prime Minister David Cameron and President Barack Obama sought to employ nudge theory to advance domestic policy goals during their terms.\n\nIn Australia, the government of New South Wales established a Behavioural Insights community of practice.\n\nNudge theory has also been applied to business management and corporate culture, such as in relation to health, safety and environment (HSE) and human resources. Regarding its application to HSE, one of the primary goals of nudge is to achieve a \"zero accident culture\". \n\nLeading Silicon Valley companies are forerunners in applying nudge theory in corporate setting. These companies are using nudges in various forms to increase productivity and happiness of employees. Recently, further companies are gaining interest in using what is called \"nudge management\" to improve the productivity of their white-collar workers.\n\nBehavioral insights and nudges are currently used in many countries around the world.\n\nNudging has also been criticised. Tammy Boyce, from public health foundation The King's Fund, has said: \"We need to move away from short-term, politically motivated initiatives such as the 'nudging people' idea, which are not based on any good evidence and don't help people make long-term behaviour changes.\"\n\nCass Sunstein has responded to critiques at length in his \"The Ethics of Influence\" making the case in favor of nudging against charges that nudges diminish autonomy, threaten dignity, violate liberties, or reduce welfare. He further defended nudge theory in his \"Why Nudge?: The Politics of Libertarian Paternalism\" by arguing that choice architecture is inevitable and that some form of paternalism cannot be avoided. Ethicists have debated nudge theory rigorously. These charges have been made by various participants in the debate from Bovens to Goodwin. Wilkinson for example charges nudges for being manipulative, while others such as Yeung question their scientific credibility. \n\nPublic opinion on the ethicality of nudges has also been shown to be susceptible to “partisan nudge bias”. Research from David Tannenbaum, Craig R. Fox, and Todd Rogers (2017) found that adults and policymakers in the United States found behavioral policies to be more ethical when they aligned with their own political leanings. Conversely, people found these same mechanisms to be more unethical when they differed from their politics. The researchers also found that nudges are not inherently partisan: when evaluating behavioral policies absent of political cues, people across the political spectrum were alike in their assessments.\n\nSome, such as Hausman & Welch have inquired whether nudging should be permissible on grounds of (distributive) justice; Lepenies & Malecka have questioned whether nudges are compatible with the rule of law. Similarly, legal scholars have discussed the role of nudges and the law.\n\nBehavioral economists such as Bob Sugden have pointed out that the underlying normative benchmark of nudging is still homo economicus, despite the proponents' claim to the contrary. \n\nIt has been remarked that nudging is also a euphemism for psychological manipulation as practiced in social engineering.\n\nThere exists an anticipation and, simultaneously, implicit criticism of the nudge theory in works of Hungarian social psychologists who emphasize the active participation in the nudge of its target (Ferenc Merei), Laszlo Garai).\n"}
{"id": "55940", "url": "https://en.wikipedia.org/wiki?curid=55940", "title": "Nulla poena sine lege", "text": "Nulla poena sine lege\n\nNulla poena sine lege (Latin for \"no penalty without a law\") is a legal principle, requiring that one cannot be punished for doing something that is not prohibited by law. This principle is accepted and codified in modern democratic states as a basic requirement of the rule of law. It has been described as \"one of the most 'widely held value-judgement[s] in the entire history of human thought.\n\nIn modern European criminal law, e.g. of the Constitutional Court of Germany, the principle of \"nulla poena sine lege\" has been found to consist of four separate requirements:\n\n\n\n\n\nOne complexity is the lawmaking power of judges under common law. Even in civil law systems that do not admit judge-made law, it is not always clear when the function of interpretation of the criminal law ends and judicial lawmaking begins.\n\nIn English criminal law there are offences of common law origin. For example, murder is still a common law offence and lacks a statutory definition. The Homicide Act 1957 did not include a statutory definition of murder (or any other homicidal offense). Therefore, the definition of murder was the subject of no fewer than six appeals to the House of Lords within the next 40 years (\"Director of Public Prosecutions v. Smith\" [1961] A.C. 290; \"Hyam v. Director of Public Prosecutions\" [1975] A.C. 55; \"Regina v. Cunningham\" [1982] A.C. 566; \"Regina v. Moloney\" [1985] A.C. 905; \"Regina v. Hancock\" [1986] A.C. 455; \"Regina v. Woollin\" [1998] 4 A11 E.R. 103 (H.L.)).\n\nThe legal principle \"nulla poena sine lege\" as principle in natural law is due to the contention of scholars of the Scholasticism about the preconditions of a guilty conscience. In relation to the Ezekiel-commentary of Sophronius Eusebius Hieronymus, Thomas Aquinas and Francisco Suárez analysed the formal conditions of the punishment of conscience. Thomas located the conditions within the synderesis. For him it is a formal and active part of the human soul. Understanding of activity, which is in accordance with the human nature, is formal possible due to the synderesis. Hence the synderesis contains in the works of patristic authors a law which commands how the human as human has to act. In the individual case this law is contentual definite. For the scholastic this is shown in the action of the intellect. This action is named since Thomas \"conscientia\". A possible content of the \"conscientia\" is the punishment in concordance with the content of the synderesis, in case the human has had not act in concordance with the human nature. An example for the punishment is madness, which since antiquity is a punishment of conscience. The Oresteia is a famous example for this.\n\nAccording Suárez the punishment of conscience is the insight in an obligation to act in concordance with the human nature to undo a past misdeed. This insight obligates to impossible actions due to the fact that the misdeed is in the past and hence it is unchangeable. Therefore the \"conscientia\" obligates in concordance with the synderesis to do an impossible action. Hence the \"conscientia\" restricts conscientious persons by doing a limitation on their own will. For they are unable to think about any other action than to fulfil their obligation. Inasmuch the conscientia restricts the intellect the scholastic speak of it as a \"malum\" or \"malum metaphysicum\", because the limitation is related to a metaphysical quality of a human. The law is constituted by the human nature itself from what the \"malum metaphysicum\" is inflicted. Therefore the punishment of the conscience is executed because of a violation of natural law.\n\nThe question of jurisdiction may sometimes come to contradict this principle. For example, customary international law allows the prosecution of pirates by any country (applying universal jurisdiction), even if they did not commit crimes at the area that falls under this country's law. A similar principle has appeared in the recent decades with regard to crimes of genocide (see genocide as a crime under domestic law); and UN Security Council Resolution 1674 \"reaffirms the provisions of paragraphs 138 and 139 of the 2005 World Summit Outcome Document regarding the responsibility to protect populations from genocide, war crimes, ethnic cleansing and crimes against humanity\" even if the State in which the population is being assaulted does not recognise these assaults as a breach of domestic law. However, it seems that universal jurisdiction is not to be expanded substantially to other crimes, so as to satisfy \"Nulla poena sine lege\".\n\nSince the Nuremberg Trials, penal law is taken to include the prohibitions of international criminal law, in addition to those of domestic law. The normative content of nulla poena in international law is developed by Shahram Dana in \"Beyond Retroactivity to Realizing Justice: The Principle of Legality in International Criminal Law Sentencing\" published in 99 JOURNAL OF CRIMINAL LAW AND CRIMINOLOGY 857 (2009). Thus, prosecutions have been possible of such individuals as Nazi war criminals and officials of the German Democratic Republic responsible for the Berlin Wall, even though their deeds may have been allowed or even ordered by domestic law. Also, courts when dealing with such cases will tend to look to the letter of the law at the time, even in regimes where the law as it was written was generally disregarded in practice by its own authors.\n\nHowever, some legal scholars criticize this, because generally, in the legal systems of Continental Europe where the maxim was first developed, \"penal law\" was taken to mean statutory penal law, so as to create a guarantee to the individual, considered as a fundamental right, that he would not be prosecuted for an action or omission that was not considered a crime according to the statutes passed by the legislators in force at the time of the action or omission, and that only those penalties that were in place when the infringement took place would be applied. Also, even if one considers that certain actions are prohibited under general principles of international law, critics point out that a prohibition in a general principle does not amount to the establishment of a crime, and that the rules of international law also do not stipulate specific penalties for the violations.\n\nIn an attempt to address those criticisms, the statute of the recently established International Criminal Court provides for a system in which crimes and penalties are expressly set out in written law, that shall only be applied to future cases.\n\nThis principle is enshrined in several national constitutions, and a number of international instruments. See e.g. European Convention on Human Rights, article 7(1); Rome Statute of the International Criminal Court, articles 22 and 23.\n\n"}
{"id": "8756788", "url": "https://en.wikipedia.org/wiki?curid=8756788", "title": "One-pass algorithm", "text": "One-pass algorithm\n\nIn computing, a one-pass algorithm is a streaming algorithm which reads its input exactly once, in order, without unbounded buffering. A one-pass algorithm generally requires O(n) (see 'big O' notation) time and less than O(n) storage (typically O(1)), where n is the size of the input.\n\nBasically one-pass algorithm operates as follows:\n(1) the object descriptions are processed serially;\n(2) the first object becomes the cluster representative of the first cluster;\n(3) each subsequent object is matched against all cluster representatives existing at\nits processing time;\n(4) a given object is assigned to one cluster (or more if overlap is allowed) according\nto some condition on the matching function;\n(5) when an object is assigned to a cluster the representative for that cluster is\nrecomputed;\n(6) if an object fails a certain test it becomes the cluster representative of a new\ncluster (7) nothing happened\nGiven any list as an input:\n\nGiven a list of numbers:\n\nGiven a list of symbols from an alphabet of \"k\" symbols, given in advance.\n\nGiven any list as an input:\n\nGiven a list of numbers:\n"}
{"id": "1356771", "url": "https://en.wikipedia.org/wiki?curid=1356771", "title": "Outline of video games", "text": "Outline of video games\n\nThe following outline is provided as an overview of and topical guide to video games:\n\nVideo game – an electronic game that involves interaction with a user interface to generate visual feedback on a video device. The word \"video\" in \"video game\" traditionally referred to a raster display device, but following popularization of the term \"video game\", it now implies any type of display device.\n\nVideo game genres (list) – categories of video games based on their gameplay interaction and set of gameplay challenges, rather than visual or narrative differences.\n\nAction game – a video game genre that emphasizes physical challenges, including hand–eye coordination and reaction-time.\n\nAction-adventure game – a video game genre that combines elements of both the adventure game and the action game genres.\n\nAdventure game – a video game in which the player assumes the role of protagonist in an interactive story driven by exploration and puzzle-solving instead of physical challenge.\n\nRole-playing video game (RPG): a video game genre with origins in pen-and-paper role-playing games such as Dungeons & Dragons, using much of the same terminology, settings and game mechanics. The player in RPGs controls one character, or several adventuring party members, fulfilling one or many quests.\n\nSimulation video game – a diverse super-category of video games, generally designed to closely simulate aspects of a real or fictional reality.\n\nStrategy video game – a genre that emphasizes skillful thinking and planning to achieve victory. They emphasize strategic, tactical, and sometimes logistical challenges. Many games also offer economic challenges and exploration.\n\nVehicle simulation game – games in which the objective is to operate a manual or motor powered transport.\n\n\n\n\nGameplay\n\nLists of video games\n\nVideo game industry\n\n\n\n\n\n\n\nVideo game development – the software development process by which a video game is developed and video game developer is a software developer (a business or an individual) that creates video games.\n\nVideo game developer – a software developer (a business or an individual) that creates video games.\n\nIndependent video game development – the process of creating indie video games without the financial support of a video game publisher, usually designed by an individual or a small team.\n\n\nHistory of video games\n\n\n\nPrior to 1972<br>\n1970\n1971\n1972\n1973\n1974\n1975\n1976\n1977\n1978\n1979\n1980\n1981\n1982\n1983\n1984\n1985\n1986\n1987\n1988\n1989\n1990\n1991\n1992\n1993\n1994\n1995\n1996\n1997\n1998\n1999\n2000\n2001\n2002\n2003\n2004\n2005\n2006\n2007\n2008\n2009\n2010\n2011\n2012\n2013\n\n\n\n\n\n\n\n\n"}
{"id": "7502798", "url": "https://en.wikipedia.org/wiki?curid=7502798", "title": "Peace war game", "text": "Peace war game\n\nPeace war game is an iterated game originally played in academic groups and by computer simulation for years to study possible strategies of cooperation and aggression. As peace makers became richer over time it became clear that making war had greater costs than initially anticipated. The only strategy that acquired wealth more rapidly was a \"Genghis Khan\", a constant aggressor making war continually to gain resources. This led to the development of the \"provokable nice guy\" strategy, a peace-maker until attacked. Multiple players continue to gain wealth cooperating with each other while bleeding the constant aggressor. The Hanseatic League for trade and mutual defense appears to have originated from just such concerns about seaborne raiders.\n\nThe peace war game is a variation of the iterated prisoner's dilemma in which the decisions (Cooperate, Defect) are replaced by (Peace, War). Strategies remain the same with reciprocal altruism, \"Tit for Tat\", or \"provokable nice guy\" as the best deterministic one. This strategy is simply to make peace on the first iteration of the game; after that, the player does what his opponent did on the previous move. A slightly better strategy is \"Tit for Tat with forgiveness\". When the opponent makes war, on the next move, the player sometimes makes peace anyway, with a small probability. This allows an escape from wasting cycles of retribution, a motivation similar to the Rule of Ko in the game of Go. \"Tit for Tat with forgiveness\" is best when miscommunication is introduced, when one's move is incorrectly reported to the opponent. A typical payoff matrix for two players (A, B) of one iteration of this game is:\nHere a player's resources have a value of 2, half of which must be spent to wage war. In this case, there exists a Nash equilibrium, a mutually best response for a single iteration, here (War, War), by definition heedless of consequences in later iterations. \"Provokable nice guy's\" optimality depends on iterations. How many are necessary is likely tied to the payoff matrix and probabilities of choosing. A subgame perfect version of this strategy is \"Contrite Tit-for-Tat\" which is to make peace unless one is in \"good standing\" and one's opponent is not. Good (\"standing\" assumed) means to make peace with good opponents, make peace when bad, or make war when good and opponent is not.\n\n"}
{"id": "42782224", "url": "https://en.wikipedia.org/wiki?curid=42782224", "title": "People's Union for Democratic Rights", "text": "People's Union for Democratic Rights\n\nPeople's Union for Democratic Rights is an organisation based in Delhi which is committed to legally defend \"civil liberties and democratic rights\" of the people. The People's Union for Democratic Rights (PUDR) is an independent entity and is not affiliated to any political party or organisation.\n\nThe PUDR was initially formed as the Delhi's unit of the \"People's Union for Civil Liberties and Democratic Rights\" (PUCLDR) in 1977, but the PUCLDR discontinued its activities after the Janata Party's success in the elections, while the PUDR continued to work. The national forum (PUCLDR) was later revived in 1980, but in a \"new form\" and with a new name, PUCL, creating a dichotomous bisection between civil liberties and democratic rights. Later in February 1981, the PUDR opted to function as a separate organisation.\n\n\nThe PUDR has an executive committee that is constituted of 7 members that includes two secretaries and a treasurer. The executive committee is elected every year during the \"Annual General Body Meeting\" by the Secret ballot. Every member of the PUDR is a part of its \"General Body.\" The secretaries of the PUDR in 2013 were Asish Gupta and D. Manjit. Gautam Navlakha is also veteran activist of the PUDR, and has worked as the organisation's secretary as well.\n\nThe PUDR's membership is accorded only to the individuals and is not granted to organisations, based on the precondition that the member concur to the aims and objectives of the PUDR, and would \"work actively and support the programme\" of the PUDR. The applicants are required to get their membership application approved by the executive committee which thereafter has to be renewed annually. The membership fee per year is 10.\n\nThe PUDR raises funds for itself by \"income received from sale of its literature, membership fee, and donations received from members and sympathisers from time to time;\" and does not obtains money from the government, institutions, or political parties. It does not take any financial aid from any source in other countries as well. The members of the PUDR work as volunteers and are not given any payment for their time.\n\n"}
{"id": "7652097", "url": "https://en.wikipedia.org/wiki?curid=7652097", "title": "Perceptual paradox", "text": "Perceptual paradox\n\nA perceptual paradox illustrates the failure of a theoretical prediction. Theories of perception are supposed to help a researcher predict what will be perceived when senses are stimulated.\n\nA theory usually comprises a mathematical model (formula),\nrules for collecting physical measurements for input into the model,\nand rules for collecting physical measurements to which model outputs should map. When arbitrarily choosing valid input data, the model should reliably generate output data that is indistinguishable from that which is measured in the system being modeled.\n\nAlthough each theory may be useful for some limited predictions,\ntheories of vision, hearing, touch, smell, and taste are not typically\nreliable for comprehensive modeling of perception based on sensory inputs. A paradox illustrates where a theoretical prediction fails.\nSometimes, even in the absence of a predictive theory,\nthe characteristics of perception seem nonsensical.\n\nThis page lists some paradoxes and seemingly impossible properties of perception. When an animal is not named in connection with the discussion, human perception should be assumed since\nthe majority of perceptual research data applies to humans.\n\n\nA perceptual paradox, in its purest form is a statement\nillustrating the failure of a formula to predict\nwhat we perceive from what our senses transduce.\n\nA seemingly nonsensical characteristic is a statement of factual observation\nthat is sufficiently intractable that no theory has been proposed to account for it.\n\nOne branch of research into perception attempts to explain\nwhat we perceive by applying formulae to sensory inputs\nand expecting outputs similar to that which we perceive.\nFor example: what we measure with our eyes should be predicted\nby applying formulae to what we measure with instruments that imitate our eye.\n\nPast researchers have made formulae that predict\nsome, but not all, perceptual phenomena from their sensory origins.\nModern researchers continue to make formulae to overcome\nthe shortcomings of earlier formulae.\n\nSome formulae are carefully constructed to mimic\nactual structures and functions of sensory mechanisms.\nOther formulae are constructed by great leaps of faith\nabout similarity in mathematical curves.\n\nNo perceptual formulae have been raised to the status of \"natural law\"\nin the way that the laws of gravitation and electrical attraction have.\nSo, perceptual formulae continue to be an active area of development\nas scientists strive towards the great insight required of a law.\n\nSome Nobel laureates have paved the way with clear statements of good practice:\n\nIn the preface to his Histology\n\nSantiago Ramón y Cajal\nwrote that \"Practitioners will only be able to claim that a valid explanation of a histological observation has been provided if three questions can be answered satisfactorily: what is the functional role of the arrangement in the animal; what mechanisms underlie this function; and what sequence of chemical and mechanical events during evolution and development gave rise to these mechanisms?\"\n\nAllvar Gullstrand described the problems that arise\nwhen approaching the optics of the eye as if they were as predictable as camera optics.\n\nCharles Scott Sherrington, considered the brain to be\nthe \"crowning achievement of the reflex system\",\n(which can be interpreted as opening all aspects of perception to simple formulae\nexpressed over complex distributions).\n\n\n\nContrast Invariance\nBoundaries between brighter and darker areas\nappear to remain of constant relative contrast\nwhen the ratio of logarithms of the two intensities\nremains constant:\n\nBut the use of logarithms is forbidden\nfor values that can become zero such as formula_4,\nand division is forbidden\nby values that can become zero such as formula_5.\n\nNo published neuroanatomical model predicts the perception\nof contrast invariance.\n\n10 Decade Transduction\n\nLocal Contrast\n\nColor Constancy\nWhen observing objects in a scene, colors appears constant.\nAn apple looks red regardless of where it is viewed.\nIn bright direct sunshine, under a blue sky with the sun obscured,\nduring a colorful sunset, under a canopy of green leaves,\nand even under most man-made light sources,\nthe color of the apple remains unchanging.\n\nColor perception appears to be independent of light wavelength.\nEdwin Land demonstrated this by illuminating a room with\ntwo wavelengths of light of approximately 500 nm and 520 nm\n(both improperly called \"green\").\nThe room was perceived in full color,\nwith all colors appearing unattenuated,\nlike red, orange, yellow, blue, and purple,\ndespite the absence of photons other than two close to 510 nm.\nNote that formula_2 light misuses the terminology RGB\nsince color is a perception and\nthere are no such things as \"Red\", \"Green\", or \"Blue\" photons.\n\nJerome Lettvin wrote an article in the Scientific American\n\nillustrating the importance of boundaries and vertices\nin the perception of color.\n\nYet, no published formula predicts the perceived color of objects\nin a single image of arbitrary scene illumination.\n\nTransverse Chromatic Deaberration\nLight that goes through a simple lens such as found in an eye\nundergoes refraction, splitting colors.\nAn formula_2 point-source that is off-center to the eye\nprojects to a pattern where with color separation along a line radial to\nthe central axis of the eye.\nThe color separation can be many photoreceptors wide.\n\nYet, an formula_2 pixel on a television or computer screen appears white\neven when seen sidelong.\n\nNo published neuroanatomical model predicts the perception of\nthe eccentric white pixel.\n\nLongitudinal Chromatic Deaberration\nAs in Transverse Chromatic Deaberration,\ncolor splitting projects also projects the R, G, and B components\nof the formula_2 pixel to different focal lengths,\nresulting in a bulls-eye-like color distribution of light\neven at the center of vision.\n\nNo published neuroanatomical model predicts the perception of\nthe centered white pixel.\n\nSpherical Deaberration\nEyes have corneas and lenses that are imperfectly spherical.\nThis inhomogeneous shape results in a non-circular distribution of photons on the retina.\n\nNo published neuroanatomical model predicts the perception of\nthe non-circularly distributed white pixel.\n\nHyperacuity\nPeople report discrimination much finer than can be predicted\nby interpolating sense data between photosensors.\nHigh performing hyperacute vision in some people\nhas been measured to less than a tenth the radius of a single photoreceptor.\nAmong measures of hyperacuity are the vernier discrimination of two adjacent lines\nand the discrimination of two stars in a night sky.\n\nNo published neuroanatomical model predicts the discrimination of\nthe two white pixels closer together than a single photoreceptor.\n\nPupil Size Inversion\nWhen pupils are narrowed to around 1mm for reading fine print,\nthe size of the central \"Airy\" disk increases to a diameter of 10 photoreceptors.\nThe so-called \"blur\" is increased for reading.\nWhen pupils are widened for fight/flight response,\nthe size of the central \"Airy\" disk decreases to a diameter of about 1.5 photoreceptors.\nThe so-called \"blur\" is decreased in anticipation of large movements.\n\nNo published neuroanatomical model predicts that discrimination\nimproves when pupils are narrowed.\n\nPupil Shape Inversion\nEyes have pupils (apertures) that cause diffraction.\nA point-source of light is distributed on the retina.\nThe distribution for a perfectly circular aperture\nis known by the name \"Airy rings\".\n\nHuman pupils are rarely perfectly circular.\nCat pupils range from almost circular to a vertical slit.\nGoat pupils tend to be horizontal rectangular with rounded corners.\nGecko pupils range from circular, to a slit, to a series of pinholes.\nCuttlefish pupils have complex shapes.\n\nNo published neuroanatomical model predicts the perception of\nthe various pupil shape distributed white pixel.\n\nOne paradoxical perception concerning the sense of smell is the theory of one's own ability to smell. Smell is intrinsic to being alive, and is even shown to be a matter of genetics.\n"}
{"id": "10353408", "url": "https://en.wikipedia.org/wiki?curid=10353408", "title": "Prevalence of circumcision", "text": "Prevalence of circumcision\n\nThe prevalence of circumcision is the percentage of males in a given population who have been circumcised. The rates vary widely by country, from virtually 0% in Honduras, to 7% in Spain, to 20% in the United Kingdom, to 45% in South Africa, to 80% in the United States, to over 90% in many Muslim-majority countries. In 2007, the World Health Organization (WHO) estimated 33% of adult males worldwide (aged 15+) are circumcised, with about two-thirds of those being Muslims.\n\nMale circumcision is nearly universal in the Muslim world and in Israel due to the religious beliefs of the majority of Muslims and Jews; however, some non-Muslim groups living within Muslim-majority countries, such as Armenians and Assyrians, do not practise it. It is prevalent in some Muslim-majority countries in southeast Asia such as Indonesia and Malaysia; however, the WHO states that there is \"little non-religious circumcision in Asia, with the exceptions of the Republic of Korea and the Philippines\". In parts of Africa it is often practised as part of tribal or religious customs. The prevalence of circumcision is also high in the United States, although there has been a slight (~6%) decrease in routine neonatal circumcision in recent years.\n\nIn contrast, rates are much lower in most of Europe, parts of southern Africa, most of Asia, Oceania and Latin America, constituting South America, Central America, the Caribbean and Mexico.\n\nAustralia, Canada, Ireland, New Zealand and the United Kingdom are examples of countries that have seen a decline in male circumcision in recent decades, while there have been indications of increasing demand in southern Africa, partly for preventive reasons due to the HIV epidemic there.\n\nStudies indicate that about 62% of African males are circumcised. However, the rate varies widely between different regions, and among ethnic and religious groups, with Muslim North Africans practising it for religious reasons, central Africans as part of tribal rituals or local custom, (with some practising female genital mutilation as well) and some traditionally non-circumcising populations in the South recently adopting the practice due to measures by the World Health Organisation to prevent AIDS.\nWilliams, B.G. \"et al.\" commented that: \"Most of the currently available data on the prevalence of [male circumcision] are several decades old, while several of the recent studies were carried out as adjuncts to demographic and health surveys and were not designed to determine the prevalence of [male circumcision].\"\n\nBotswana, Rwanda, Swaziland, Zimbabwe.\n\nAngola, Burundi, Central African Republic, Chad, Congo (Rep), Lesotho, Malawi, Mozambique, Namibia, South Africa, Sudan, Tanzania, Uganda, Zambia.\n\nIt is estimated that 44.7% of males are circumcised in South Africa. One national study reported that 48.2% of black Africans were circumcised, with 32.1% of those traditionally circumcised and 13.4% circumcised for medical reasons.\n\nBenin, Burkin Faso, Cameroon, Congo (Dem Rep), Cote d’Ivoire, Djibouti, Equatorial Guinea, Eritrea, Ethiopia, Gabon, Gambia, Ghana, Guinea, Guinea-Bissau, Kenya, Liberia, Mali, Mauritania, Niger, Nigeria, Senegal, Sierra Leone, Somalia, Togo.\n\nArgentina, Belize, Bolivia, Brazil, Chile, Colombia, Costa Rica, Cuba, Dominican Republic, El Salvador, Ecuador, French Guiana, Guatemala, Guyana, Haiti, Honduras, Jamaica, Mexico, Nicaragua, Panama, Paraguay, Peru, Puerto Rico, The Bahamas, Trinidad and Tobago, Uruguay, Venezuela.\n\nThe overall prevalence of circumcision is reported to be 6.9% in Colombia, and 7.4% in Brazil (13% in Rio de Janeiro), with most of those being done due to medical issues later in life.\n\nThe prevalence of circumcision in Mexico is estimated to be 10% to 31%.\n\nCanada, United States.\n\nCircumcision in Canada followed the pattern that existed in other English speaking countries, picking up the practice during the 1900s to prevent masturbation and other perceived issues of the time, and then had its rate of circumcision decline due to new policy statements passed and due to coverage for the procedure being dropped, with a pattern of declining incidence of circumcision occurring from 1970 to 1979 after a new policy statement was released In 1975. The Canadian Paediatric Society offered an estimate of 48 percent for the prevalence of male circumcision in Canada in 1970 prior to this fall in prevalence. However, when conducting new studies to determine is prevalence in 1977-1978 There was a wide variation in the incidence of circumcision between different provinces and territories. For Example, Yukon reported a rate of 74.8 percent in while Newfoundland reported an incidence of 1.9 to 2.4 percent in 1977-78. The rate continued to drop, with the newborn circumcision rate in Ontario In 1994-95 dropping to 29.9%.\n\nA survey of Canadian maternity practices conducted in 2006/2007 and published in 2009 by the national public health agency found a newborn circumcision rate of 31.9%. Rates varied markedly across the country, from close to zero in Newfoundland and Labrador to 44.3% in Alberta. In 2015, the Canadian Paediatric Society used those statistics in determining their national circumcision rate, with that being the one which is currently used.\n\nThe Centers For Disease Control and Prevention (CDC) uses two data sources to track circumcision rates. The first is the National Health and Nutrition Examination Survey (NHANES), which records circumcisions performed at any time at any location. The second is the National Hospital Discharge Survey (NHDS), which does not record circumcisions performed outside the hospital setting or those performed at any age following discharge from the birth hospitalization.\n\nBased off NHANES data, the CDC estimated that 80.5% of American males aged 14 to 59 years old from 2005 to 2010 were circumcised. Among racial breakdown, 90.8% of non-Hispanic white males, 75.7% of non-Hispanic black, and 44% of Mexican American males from that same age group and time span were circumcised.\n\nBased off NHDS data, the CDC reported a national decline in circumcision rates of newborns, from 64.5% to 58.3%, during the 32-year period from 1972 to 2010. Trends varied regionally, with the Midwest mirroring the national trend; in the Northeast there was no discernible trend in the 32-year period; the South experienced an increase in circumcision rates from 1979 until 1998 and then a decline until 2010; finally, the West saw a decrease of 37% during the period in question, with the biggest drop happening in the 1980's, continuing with a slower decrease until 2010. The decline is in large part due to the growing Hispanic population. It is increasingly common due do increasing health care costs that health care providers require the mother and baby to leave the hospital within 24 hours after birth without complications. As a result, a large number of circumcisions are performed in out-patient clinics and are not recorded in the NHDS data.\n\nIn 2009, the Healthcare Cost and Utilization Project (HCUP) reported its findings regarding newborn circumcisions based on U.S. region and income bracket. Its data came from several states' hospital associations and health departments. The Western Region reported a rate of 24.6%, the North Central Region reported a rate of 76.2%, the Northeast Region reported a rate of 67%, and the Southern Region reported a rate of 55.7%. The combined newborn circumcision rate of all regions was 54.5%, which is similar but slightly lower than the NHDS data from 2010. There was also significant variation between rural and urban areas. Rural areas reported a rate of 66.9%, while urban areas reported a rate of 41.2%. The lowest income bracket reported a rate of 51.5%, while the highest income bracket reported a rate of 60.4%.\n\nMedicaid funding for infant circumcision used to be available in every state, but starting with California in 1982, 18 states (Arizona, California, Colorado, Florida, Idaho, Louisiana, Maine, Minnesota, Mississippi, Missouri, Montana, Nevada, North Carolina, North Dakota, Oregon, South Carolina, Utah, and Washington) had eliminated Medicaid coverage of routine (non-therapeutic) circumcision by July 2011. One study in the Midwest of the U.S. found that this had no effect on the newborn circumcision rate but it did affect the demand for circumcision at a later time. Another study, published in early 2009, found a difference in the neonatal male circumcision rate of 24% between states with and without Medicaid coverage. The study was controlled for other factors such as the percentage of Hispanic patients. Other studies have shown that the rise of immigrants from East Asia, Southeast Asian, South Asia and Hispanic South American countries are a large factor in why the rates continue to drop in the US.\n\nCircumcision was the second-most common procedure performed on patients under one year of age. There are various explanations for why the infant circumcision rate in the United States is different from comparable countries. Many parents’ decisions about circumcision are preconceived, which may contribute to the high rate of elective circumcision. Brown & Brown (1987) reported the most important factor is whether the father is circumcised.\n\nBhutan, Burma, China, Cambodia, India, Japan, Laos, Mongolia, Nepal, North Korea, Papua New Guinea, Sri Lanka, Taiwan, Thailand, Vietnam.\n\nThe overall prevalence of circumcision in Cambodia is reported to be 3.5%.\n\nThe overall prevalence of circumcision in China is reported to be 14%.\n\nIndonesia, Kazakhstan, Malaysia, Pakistan and South Korea.\n\nCircumcision is largely a modern-day phenomenon in South Korea. While the rate in the twentieth century has been nearing 80%, virtually no circumcision was performed just a century ago, as it was against Korea's long and strong tradition of preserving the body as a gift from parents. A 2001 study of 20-year-old South Korean men found that 78% were circumcised. At the time, the authors commented that \"South Korea has possibly the largest absolute number of teenage or adult circumcisions anywhere in the world. Because circumcision started through contact with the American military during the Korean War, South Korea has an unusual history of circumcision.\" According to a 2002 study, 86.3% of South Korean males aged 14–29 were circumcised. In 2012, it's the case of 75.8% of the same age group. Only after 1999 has some information against circumcision become available (at the time of the 2012 study, only 3% of Korean internet sites, using the most popular Korean search engine Naver, are against indiscriminate circumcision and 97% are for). The authors of the study speculate \"that the very existence of information about the history of Korean circumcision, its contrary nature relative to a longstanding tradition, its introduction by the US military, etc., has been extremely influential on the decision-making process regarding circumcision.\"\n\nAfghanistan, Azerbaijan, Bahrain, Brunei, Iran, Iraq, Israel, Jordan, Kuwait, Kyrgyzstan, Lebanon, Oman, Israel, West Bank, the Philippines, Qatar, Saudi Arabia, Syria, Tajikistan, Turkey, Turkmenistan, Uzbekistan, United Arab Emirates, Yemen.\n\nThe overall prevalence of circumcision (\"tuli\") in the Philippines is reported to be 92.5%. Most circumcisions in the Philippines are performed between the ages of 11 to 13.\n\nArmenia, Austria, Belarus, Belgium, Bulgaria, Croatia, Czech Republic, Cyprus, Denmark, Estonia, Finland, France, Georgia, Germany, Greece, Hungary, Iceland, Ireland, Italy, Latvia, Lithuania, Moldova, The Netherlands, Norway, Poland, Portugal, Romania, Russia, Slovakia, Spain, Serbia, Sweden, Switzerland, Ukraine, and the United Kingdom.\n\nA national survey on sexual attitudes in 2000 found that 15.8% of men or boys in the United Kingdom (ages 16–44) were circumcised by their parents' choosing. 11.7% of 16- to 19-year-olds, and 19.6% of 40- to 44-year-olds said they had been circumcised. Apart from black Caribbeans, men born overseas were more likely to be circumcised. Rickwood \"et al.\" reported that the proportion of English boys circumcised for medical reasons had fallen from 35% in the early 1930s to 6.5% by the mid-1980s. An estimated 3.8% of male children in the UK in 2000 were being circumcised by the age of 15. The researchers stated that too many boys, especially under the age of 5, were still being circumcised because of a misdiagnosis of phimosis. They called for a target to reduce the percentage to 2%.\n\nIn Finland, the overall prevalence of circumcision is 2-4%, according to a recent publication by the Finnish Health Ministry.\n\nIn Germany, the German Health Interview and Examination Survey for Children and Adolescents found that 10.9% of boys aged 0–17 had been circumcised.\n\nIn France, according to a telephone survey (TNS Sofres Institute, 2008), 14% of men are circumcised.\n\nThe overall prevalence of circumcision in Spain is reported to be 6.6%.\n\nIn 1986, 511 out of approximately 478,000 Danish boys aged 0–14 years were circumcised. This corresponds to a cumulative national circumcision rate of around 1.6% by the age of 15 years.\n\nIn Slovenia, a 1999-2001 national probability sample of the general population aged 18–49 years found that overall, 4.5% of Slovenian male citizens reported being circumcised. Prevalence strongly varied across religious groups, with 92.4% of Muslims being circumcised, 1.7% of Roman Catholics, 0% of other religious affiliations (Evangelic, Serbian Orthodox, other), and 7.1% of those with no religious affiliation.\n\nAlbania, Kosovo, Republic of Macedonia, and Bosnia and Herzegovina.\n\nIn Albania during the years 2008-09 the percentage of men age 15-49 who reported having been circumcised was 47.7%.\n\nAndorra, Croatia and Luxembourg are listed as unknown on the WHO prevalence map. Liechtenstein, Malta, Monaco, San Marino and Vatican City are unclear from the map.\n\nCircumcision reached its peak in Australia in the 1950s with a rate of more than 80%, but has steadily fallen to an estimated 26% in 2012. The rate of circumcision has dropped rapidly over the years. It is estimated that males aged roughly 80 percent of males 35 and under are intact, and remaining 20 percent are circumcised. Circumcision rates have declined drastically in recent years as young fathers are starting to have children of their own and leaving them intact.\n\nThe Australian Longitudinal Study of Health and Relationships is a computer assisted telephone interview of males aged 16–64 that uses a nationally representative population sample. In 2005 the interview found that the prevalence of circumcision in Australia was roughly 58%. Circumcision status was more common with males over 30 than males under 30, and more common with males who were born in Australia. 66% of males born in Australia were circumcised and less than 1/3 of males under 30 were circumcised. There has been a decline in the rate of infant circumcision in Australia. The Royal Australasian College of Physicians (RACP) estimated in 2010 that 10 to 20 percent of newborn boys are being circumcised, but the prevalence of male circumcision is much higher due to the presence of older circumcised males remaining in the population. Medicare Australia records show the number of males younger than six months that underwent circumcision dropped from 19,663 in 2007/08 to 6309 (4%) in 2016/17.\n\nAccording to the World Health Organisation, fewer than 20% of males are circumcised in New Zealand in 2007. In New Zealand routine circumcision for which there is no medical indication is uncommon and no longer publicly funded within the public hospital system. In a study of men born in 1972–1973 in Dunedin, 40.2% were circumcised. In a study of men born in 1977 in Christchurch, 26.1% were circumcised. A 1991 survey conducted in Waikato found that 7% of male infants were circumcised.\n\nCircumcision for cultural reasons is routine in Pacific Island countries.\n\n"}
{"id": "33158707", "url": "https://en.wikipedia.org/wiki?curid=33158707", "title": "Prototype 180", "text": "Prototype 180\n\nprototype 180 is an artwork by American conceptual artist Mary Ellen Carroll who lives and works in New York City and Houston. \"prototype 180\" is \"the centerpiece of Carroll's Innovation Territories, an initiative co-sponsored by the Rice University Building Institute. \"prototype 180\" is located at 6513 Sharpsview Drive, Houston.Houston was self-selected itself as the site of the artwork because it lacks an official land-use policy. \n\nThe artwork \"will make architecture performative.\" It is literally a ground-shifting exercise, because it structurally involves the rotation, back to front, of a single-family, ranch-style house and its surrounding land in the development of Sharpstown, a suburb of Houston, Texas.\n\nFollowing the rotation, the structure will be retrofitted and rehabilitated to become an occupied structure that will be become an institute for the study of considered urbanism. In planning for 10 years, \"prototype 180\" is described as \"reconsideration of monumentality that combines live performance, sculpture, architecture and technology.\" \n"}
{"id": "8542991", "url": "https://en.wikipedia.org/wiki?curid=8542991", "title": "Reflexive monism", "text": "Reflexive monism\n\nReflexive monism is a philosophical position developed by Max Velmans, in his books \"Understanding Consciousness\" (2000, 2009) and \"Toward a Deeper Understanding of Consciousness\" (2017), to address the problems of consciousness. It is a modern version of an ancient view that the basic stuff of the universe manifests itself both physically and as conscious experience (a dual-aspect theory in the traditions of Spinoza and Fechner). The argument is that the mind and, ultimately, the universe is psycho-physical.\n\nMonism is the view that the universe, at the deepest level of analysis, is composed of one fundamental kind of stuff. This is usually contrasted with substance dualism, the view found in the writings of Plato and Descartes that the universe is composed of two kinds of stuff, the physical and the stuff of soul, mind or consciousness.\n\nReflexive monism maintains that, in its evolution from some primal undifferentiated state, the universe differentiates into distinguishable physical entities, at least some of which have the potential for conscious experience, such as human beings. While remaining embedded within and dependent on the surrounding universe and composed of the same fundamental stuff, each human, equipped with perceptual and cognitive systems, has an individual perspective on, or view of, the rest of the universe and him or her self. In this sense, each human participates in a process whereby the universe differentiates into parts and becomes conscious of itself, making the process reflexive. Donald Price and James Barrell write that, according to reflexive monism, experience and matter are two complementary (first- and third-person viewable) sides of the same reality, and neither can be reduced to the other. That brain states are causes and correlates of consciousness, they write, does not mean that they are ontologically identical to it, and they develop the use of complementary first- and third-person perspectives into a non-reductive, empirical program for investigating the relationship of conscious experience to neuroscience.\nA similar combination of monism and reflexivity is found in later Vedic writings such as the Upanishads, as well as the Buddhist views of Chittamatra and Dzogchen.\n"}
{"id": "7715915", "url": "https://en.wikipedia.org/wiki?curid=7715915", "title": "Social cognitive theory", "text": "Social cognitive theory\n\nSocial cognitive theory (SCT), used in psychology, education, and communication, holds that portions of an individual's knowledge acquisition can be directly related to observing others within the context of social interactions, experiences, and outside media influences. This theory was advanced by Albert Bandura as an extension of his social learning theory. The theory states that when people observe a model performing a behavior and the consequences of that behavior, they remember the sequence of events and use this information to guide subsequent behaviors. Observing a model can also prompt the viewer to engage in behavior they already learned. In other words, people do not learn new behaviors solely by trying them and either succeeding or failing, but rather, the survival of humanity is dependent upon the replication of the actions of others. Depending on whether people are rewarded or punished for their behavior and the outcome of the behavior, the observer may choose to replicate behavior modeled. Media provides models for a vast array of people in many different environmental settings.\n\nThe conceptual roots for social cognitive theory come from Edwin B. Holt and Harold Chapman Brown's 1931 book theorizing that all animal action is based on fulfilling the psychological needs of \"feeling, emotion, and desire\". The most notable component of this theory is that it predicted a person cannot learn to imitate until they are imitated.\n\nIn 1941, Neal E. Miller and John Dollard presented their book with a revision of Holt's social learning and imitation theory. They argued four factors contribute to learning: drives, cues, responses, and rewards. One driver is social motivation, which includes imitativeness, the process of matching an act to an appropriate cue of where and when to perform the act. A behavior is imitated depending on whether the model receives a positive or negative response consequences. Miller and Dollard argued that if one were motivated to learn a particular behavior, then that particular behavior would be learned through clear observations. By imitating these observed actions the individual observer would solidify that learned action and would be rewarded with positive reinforcement.\n\nThe proposition of social learning was expanded upon and theorized by Canadian psychologist Albert Bandura. Bandura, along with his students and colleagues conducted a series of studies, known as the Bobo doll experiment, in 1961 and 1963 to find out why and when children display aggressive behaviors. These studies demonstrated the value of modeling for acquiring novel behaviors. These studies helped Bandura publish his seminal article and book in 1977 that expanded on the idea of how behavior is acquired, and thus built from Miller and Dollard's research. In Bandura's 1977 article, he claimed that Social Learning Theory shows a direct correlation between a person's perceived self-efficacy and behavioral change. Self-efficacy comes from four sources: \"performance accomplishments, vicarious experience, verbal persuasion, and physiological states\".\n\nIn 1986, Bandura published his second book, which expanded and renamed his original theory. He called the new theory \"social cognitive theory\". Bandura changed the name to emphasize the major role cognition plays in encoding and performing behaviors. In this book, Bandura argued that human behavior is caused by personal, behavioral, and environmental influences.\n\nIn 2001, Bandura brought SCT to mass communication in his journal article that stated the theory could be used to analyze how \"symbolic communication influences human thought, affect and action\". The theory shows how new behavior diffuses through society by psychosocial factors governing acquisition and adoption of the behavior.\n\nIn 2011, Bandura published a book chapter -- \"The Social and Policy Impact of Social Cognitive Theory\"—to extend SCT'S application in health promotion and urgent global issues, which provides insight into addressing global problems through a macro social lens, aiming at improving equality of individuals' lives under the umbrellas of SCT.\n\nSCT has been applied to many areas of human functioning such as career choice and organizational behavior as well as in understanding classroom motivation, learning, and achievement.\n\nSocial Cognitive Theory originated in psychology, but based on an unofficial November 2013 Google Scholar search, only 2 percent of articles published on SCT are in the pure psychology field. About 20 percent of articles are from Education and 16 percent from Business. The majority of publications using SCT, 56 percent, come from the field of Applied Health Psychology. The majority of current research in Health Psychology focuses on testing SCT in behavioral change campaigns as opposed to expanding on the theory. Campaign topics include: increasing fruit and vegetable intake, increasing physical activity, HIV education, and breastfeeding.\n\nBorn in 1925, Bandura is still influencing the world with expansions of SCT. His recent work, published May 2011, focuses on how SCT impacts areas of both health and population in relation to climate change. He proposes that these problems could be solved through television serial dramas that show models similar to viewers performing the desired behavior. On health, Bandura writes that currently there is little incentive for doctors to write prescriptions for healthy behavior, but he believes the cost of fixing health problems start to outweigh the benefits of being healthy. Bandura argues that we are on the cusp of moving from a disease model (focusing on people with problems) to a health model (focusing on people being healthy) and SCT is the theory that should be used to further a healthy society. Specifically on Population, Bandura states that population growth is a global crisis because of its correlation with depletion and degradation of our planet's resources. Bandura argues that SCT should be used to increase birth control use, reduce gender inequality through education, and to model environmental conservation to improve the state of the planet.\n\nSocial cognitive theory is a learning theory based on the idea that people learn by observing others. These learned behaviors can be central to one's personality. While social psychologists agree that the environment one grows up in contributes to behavior, the individual person (and therefore cognition) is just as important. People learn by observing others, with the environment, behavior, and cognition acting as primary factors that influence development in a reciprocal triadic relationship. Each behavior witnessed can change a person's way of thinking (cognition). Similarly, the environment one is raised in may influence later behaviors. For example, a caregiver's mindset (also cognition) determines the environment in which their children are raised.\n\nThe core concepts of this theory are explained by Bandura through a schematization of triadic reciprocal causation, The schema shows how the reproduction of an observed behavior is influenced by the interaction of the following three determinants: \n\nIt is important to note that learning can occur without a change in behavior. According to J.E. Ormrod's general principles of social learning, while a visible change in behavior is the most common proof of learning, it is not absolutely necessary. Social learning theorists believe that because people can learn through observation alone, their learning may not necessarily be shown in their performance. These are interdependent on each other and its influence can be directly linked with individual or group psychological behavior. According to Alex Stajkovic and Fred Luthans it is critically important to recognize that the relative influences exerted by one, two, or three interacting factors on motivated behavior will vary depending on different activities, different individuals and different circumstances.\n\nSocial cognitive theory is proposed in an agentic perspective, which suggests that, instead of being just shaped by environments or inner forces, individuals are self-developing, self-regulating, self-reflecting and proactive. Specifically, human agency operates within three modes:\n\nHuman agency has four core properties:\n\n\nEvolving over time, human beings are featured with advanced neural systems, which enable individuals to acquire knowledge and skills by both direct and symbolic terms. Four primary capabilities are addressed as important foundations of social cognitive theory: symbolizing capability, self-regulation capability, self-reflective capability, and vicarious capability.\n\n\nSocial cognitive theory revolves around the process of knowledge acquisition or learning directly correlated to the observation of models. The models can be those of an interpersonal imitation or media sources. Effective modeling teaches general rules and strategies for dealing with different situations.\n\nTo illustrate that people learn from watching others, Albert Bandura and his colleagues constructed a series of experiments using a Bobo doll. In the first experiment, children were exposed to either an aggressive or non-aggressive model of either the same sex or opposite sex as the child. There was also a control group. The aggressive models played with the Bobo doll in an aggressive manner, while the non-aggressive models played with other toys. They found that children who were exposed to the aggressive models performed more aggressive actions toward the Bobo doll afterward, and that boys were more likely to do so than girls.\n\nFollowing that study, Albert Bandura tested whether the same was true for models presented through media by constructing an experiment he called \"Bobo Doll Behavior: A Study of Aggression\". In this experiment Bandura exposed a group of children to a video featuring violent and aggressive actions. After the video he then placed the children in a room with a Bobo doll to see how they behaved with it. Through this experiment, Bandura discovered that children who had watched the violent video subjected the dolls to more aggressive and violent behavior, while children not exposed to the video did not. This experiment displays the social cognitive theory because it depicts how people reenact behaviors they see in the media. In this case, the children in this experiment reenacted the model of violence they directly learned from the video.\n\nObservations should include: \n\nModeling does not limit to only live demonstrations but also verbal and written behaviour can act as indirect forms of modeling. Modeling not only allows students to learn behaviour that they should repeat but also to inhibit certain behaviours. For instance, if a teacher glares at one student who is talking out of turn, other students may suppress this behavior to avoid a similar reaction. Teachers model both material objectives and underlying curriculum of virtuous living. Teachers should also be dedicated to the building of high self-efficacy levels in their students by recognizing their accomplishments.\n\nTo learn a particular behavior, people must understand what the potential outcome is if they repeat that behavior. The observer does not expect the actual rewards or punishments incurred by the model, but anticipates similar outcomes when imitating the behavior (called \"outcome expectancies\"), which is why modeling impacts cognition and behavior. These expectancies are heavily influenced by the environment that the observer grows up in; for example, the expected consequences for a DUI in the United States of America are a fine, with possible jail time, whereas the same charge in another country might lead to the infliction of the death penalty.\n\nFor example, in the case of a student, the instructions the teacher provides help students see what outcome a particular behaviour leads to. It is the duty of the teacher to teach a student that when a behaviour is successfully learned, the outcomes are meaningful and valuable to the students.\n\nSocial cognitive theory posits that learning most likely occurs if there is a close identification between the observer and the model and if the observer also has a good deal of self-efficacy. Self–efficacy is the extent to which an individual believes that they can master a particular skill. Self-efficacy beliefs function as an important set of proximal determinants of human motivation, affect, and action—which operate on action through motivational, cognitive, and affective intervening processes.\n\nAccording to Bandura, self-efficacy is \"the belief in one's capabilities to organize and execute the courses of action required to manage prospective situations\". Bandura and other researchers have found an individual's self-efficacy plays a major role in how goals, tasks, and challenges are approached. Individuals with high self-efficacy are more likely to believe they can master challenging problems and they can recover quickly from setbacks and disappointments. Individuals with low self-efficacy tend to be less confident and don't believe they can perform well, which leads them to avoid challenging tasks. Therefore, self-efficacy plays a central role in behavior performance. Observers who have high level of self-efficacy are more likely to adopt observational learning behaviors.\n\nSelf-efficacy can be developed or increased by: \n\nFor example, students become more effortful, active, pay attention, highly motivated and better learners when they perceive that they have mastered a particular task. It is the duty of the teacher to allow student to perceive in their efficacy by providing feedback to understand their level of proficiency. Teachers should ensure that the students have the knowledge and strategies they need to complete the tasks.\n\nSelf-efficacy has also been used to predict behavior in various health related situations such as weight loss, quitting smoking, and recovery from heart attack. In relation to exercise science, self-efficacy has produced some of the most consistent results revealing an increase in participation in exercise.\n\nIdentification allows the observer to feel a one-to-one similarity with the model, and can thus lead to a higher chance of the observer following through with the modeled action. People are more likely to follow behaviors modeled by someone with whom they can identify with. The more commonalities or emotional attachments perceived between the observer and the model, the more likely the observer learns and reenact the modeled behavior.\n\nSocial cognitive theory is often applied as a theoretical framework of studies pertained to media representation regarding race, gender, age and beyond. Social cognitive theory suggested heavily repeated images presented in mass media can be potentially processed and encoded by the viewers (Bandura, 2011). Media content analytic studies examine the substratum of media messages that viewers are exposed to, which could provide an opportunity to uncover the social values attached to these media representations. Although media contents studies cannot directly test the cognitive process, findings can offer an avenue to predict potential media effects from modeling certain contents, which provides evidence and guidelines for designing subsequent empirical work.\n\nSocial cognitive theory is pervasively employed in studies examining attitude or behavior changes triggered by the mass media. As Bandura suggested, people can learn how to perform behaviors through media modeling. SCT has been widely applied in media studies pertained to sports, health, education and beyond. For instance, Hardin and Greer in 2009 examined the gender-typing of sports within the theoretical framework of social cognitive theory, suggesting that sports media consumption and gender-role socialization significantly related with gender perception of sports in American college students.\n\nIn health communication, social cognitive theory has been applied in research related to smoking quit, HIV prevention, safe sex behaviors, and so on. For example, Martino, Collins, Kanouse, Elliott, and Berry in 2005 examined the relationship between the exposure to television’s sexual content and adolescents’ sexual behavior through the lens of social cognitive theory, confirming the significant relationship between the two variables among white and African American groups; however, no significant correlation was found between the two variables in the ethic group of Hispanics, indicating that peer norm could possibly serve as a mediator of the two examined variables.\n\nMiller's 2005 study found that choosing the proper gender, age, and ethnicity for models ensured the success of an AIDS campaign to inner city teenagers. This occurred because participants could identify with a recognizable peer, have a greater sense of self-efficacy, and then imitate the actions to learn the proper preventions and actions.\nA study by Azza Ahmed in 2009 looked to see if there would be an increase in breastfeeding by mothers of preterm infants when exposed to a breastfeeding educational program guided by SCT. Sixty mothers were randomly assigned to either participate in the program or they were given routine care. The program consisted of SCT strategies that touched on all three SCT determinants: personal – showing models performing breastfeeding correctly to improve self-efficacy, behavioral –weekly check-ins for three months reinforced participants' skills, environmental – mothers were given an observational checklist to make sure they successfully completed the behavior. The author found that mothers exposed to the program showed significant improvement in their breastfeeding skills, were more likely to exclusively breastfeed, and had fewer problems then the mothers who were not exposed to the educational program.\n\nSocial cognitive theory emphasizes a large difference between an individual's ability to be morally competent and morally performing. Moral competence involves having the ability to perform a moral behavior, whereas moral performance indicates actually following one's idea of moral behavior in a specific situation. Moral competencies include:\n\nAs far as an individual's development is concerned, moral competence is the growth of cognitive-sensory processes; simply put, being aware of what is considered right and wrong. By comparison, moral performance is influenced by the possible rewards and incentives to act a certain way. For example, a person's moral competence might tell them that stealing is wrong and frowned upon by society; however, if the reward for stealing is a substantial sum, their moral performance might indicate a different line of thought. Therein lies the core of social cognitive theory.\n\nFor the most part, social cognitive theory remains the same for various cultures. Since the concepts of moral behavior did not vary much between cultures (as crimes like murder, theft, and unwarranted violence are illegal in virtually every society), there is not much room for people to have different views on what is morally right or wrong. The main reason that social cognitive theory applies to all nations is because it does not say what is moral and immoral; it simply states that we can acknowledge these two concepts. Our actions in real-life scenarios are based on whether we believe the action is moral and whether the reward for violating our morals is significant enough, and nothing else.\n\nIn series TV programming, according to social cognitive theory, the awarded behaviors of liked characters are supposed to be followed by viewers, while punished behaviors are supposed to be avoided by media consumers. However, in most cases, protagonists in TV shows are less likely to experience the long-term suffering and negative consequences caused by their risky behaviors, which could potentially undermine the punishments conveyed by the media, leading to a modeling of the risky behaviors. Nabi and Clark conducted experiments about individual’s attitudes and intentions consuming various portrayals of one-night stand sex– unsafe and risky sexual behavior, finding that individuals who had not previously experience one night stand sex, consuming media portrayals of this behavior could significantly increase their expectations of having an one night stand sex in the future, although negative outcomes were represented in TV shows.\n\n\n\n"}
{"id": "41180743", "url": "https://en.wikipedia.org/wiki?curid=41180743", "title": "Sociocultural perspective", "text": "Sociocultural perspective\n\nThe sociocultural perspective is a theory used in fields such as psychology and is used to describe awareness of circumstances surrounding individuals and how their behaviors are affected specifically by their surrounding, social and cultural factors. According to Catherine A. Sanderson (2010) “Sociocultural perspective: A perspective describing people’s behavior and mental processes as shaped in part by their social and/or cultural contact, including race, gender, and nationality.” Sociocultural perspective theory is a broad yet significant aspect in our being. It applies to every sector of our daily lives. How we communicate, understand, relate and cope with one another is partially based on this theory. Our spiritual, mental, physical, emotional, physiological being are all influenced by factors studied by sociocultural perspective theory.\n\nVarious studies examine topics using the sociocultural perspective in order to account for variability from person to person and acknowledge that social and cultural differences affect these individuals. One example comes from the journal \"European Psychologist\": Investigating Motivation in Context: Developing Sociocultural Perspectives by Richard A. Walker, Kimberley Pressick-kilborn, Bert M. Lancaster, and Erica J. Sainsbury (2004). Recently, however, a renewed interest in the contextual nature of motivation has come about for several reasons. First, the relatively recent influence of the ideas of Vygotsky and his followers (John-Steiner & Mahn, 1996; Greeno & The Middle School Through Applications Project, 1998) in educational psychology has led writers in the field (Goodenow, 1992; Pintrich, 1994; Anderman & Anderman, 2000) to acknowledge the importance of context and to call for its greater recognition in educational psychology, and more particularly in motivational research. As both Goodenow (1992) and Hickey (1997) note, in sociocultural theories deriving from Vygotsky, human activities, events, and actions cannot be separated from the context in which they occur so that context becomes an important issue in sociocultural research. Second, researchers concerned with learning and cognition (e.g., Greeno et al., 1998) have come to see these processes also as being situated in particular contexts. While this view, with its emphasis on the distributed nature of learning and cognition, has origins in sociocultural theories\".\n\nThis theory or perspective is examined in \"The Modern Language Journal\" “A Sociocultural Perspective on Language Learning Strategies: The Role of Mediation” by Richard Donato and Dawn McCormick. According to Donato and McCormick (1994) “Sociocultural theory maintains that social interaction and cultural institutions, such as schools, classrooms, etc., have important roles to play in an individual’s cognitive growth and development.” “We believe that this perspective goes beyond current cognitive and social psychological conceptions of strategic language learning, both of which assume that language tasks and contexts are generalizable. The sociocultural perspective, on the other hand, views language learning tasks and contexts as situated activities that are continually under development (22) and that are influential upon individuals’ strategic orientations to classroom learning.”\n\nThe sociocultural perspective is also used here in order to assess use of mental health services for immigrants: “From a sociocultural perspective, this article reviews causes of mental health service under use among Chinese immigrants and discusses practice implications. Factors explaining service under use among Chinese immigrants are multifaceted, extending across individual, family, cultural and system domains. The first of these is cultural explanation of mental illness. Cultural beliefs, regarding the cause of mental disorders greatly affect service use. The perceived causes of mental illness include moral, religious, or cosmological, physiological, psychological, social and genetic factors”. From Canadian Social Work, “A Sociocultural Perspective of Mental Health Services Use by Chinese Immigrants” by Lin Fang, (2010).\n\nAccording to \"Asian American Journal of Psychology\", \"Coping with perceived racial and gender discrimination experiences among 11 Asian/Asian American female faculty at various Christian universities\" have been examined in this theory. After the study was conducted the results revealed that \"ten of the 11 women described experiences where they perceived being treated differently due to race and/or gender. Qualitative analyses of interview data revealed four themes related to coping: Proactive Coping, External Support, Personal Resources, and Spiritual Coping. The resulting themes are discussed in light of existing research, with an emphasis on the importance of understanding cultural and religious values to the study of coping\". Kim, C. L., Hall, M., Anderson, T. L., & Willingham, M. M. (2011). Coping with discrimination in academia: Asian-American and Christian perspectives. Asian American Journal of Psychology, 2(4), 291-305. doi:10.1037/a0025552\n\nAnother instance of the sociocultural perspective can be found in language learning literature: “By adopting a sociocultural perspective that highlights the critical role of the social context in cognitive and social development (Vygotsky, 1978), we propose that learners’ actions to facilitate or sometimes constrain their language learning cannot be fully understood without considering the situated contexts in which strategies emerge and develop, as well as the kinds of hierarchies within which studies from diverse backgrounds find themselves in U.S. classrooms (Bourdieu, 1991). From Theory Into Practice, “A Sociocultural Perspective on Second Language Learner Strategies: Focus on the Impact of Social Context.” by Eun-Young Jang and Robert T. Jimenez, 2011.\n\n-Kim, C. L., Hall, M., Anderson, T. L., & Willingham, M. M. (2011). Coping with discrimination in academia: Asian-American and Christian perspectives. Asian American Journal of Psychology, 2(4), 291-305. doi:10.1037/a0025552\n-Jarrett, C. (2008). Foundations of sand?. The Psychologist, 21(9), 756-759.\n-European Psychologist, Vol 9(4), Dec, 2004. Special Section: Motivation in Real-Life, Dynamic, and Interactive Learning Environments. pp. 245–256\n-\"Modern Language Journal\", Vol 78(4), Win, 1994. Special issue: Sociocultural theory and second language learning. pp. 453–464.\n-Canadian Social Work: “A Sociocultural Perspective of Mental Health Services Use by Chinese Immigrants” by Lin Fang, Autumn 2010, Vol. 12 Issue 1, p152-160, 9p\n-Theory Into Practice. A Sociocultural Perspective on Second Language Learner Strategies: Focus on the Impact of Social Context. Eun-Young Jang and Robert T. Jimenez 2011, Vol. 50 Issue 2, p141-148. 8p.\n\n"}
{"id": "535928", "url": "https://en.wikipedia.org/wiki?curid=535928", "title": "Spiritual evolution", "text": "Spiritual evolution\n\nSpiritual evolution is the philosophical, theological, esoteric or spiritual idea that nature and human beings and/or human culture evolve: either extending from an established cosmological pattern (ascent), or in accordance with certain pre-established potentials. The phrase \"spiritual evolution\" can occur in the context of \"higher evolution\", a term used to differentiate psychological, mental, or spiritual evolution from the \"lower\" or biological evolution of physical form.\n\nThe concept of spiritual evolution is also complemented by the idea of a creative impulse in human beings, known as epigenesis.\n\nWithin this broad definition, theories of spiritual evolution are very diverse. They may be cosmological (describing existence at large), personal (describing development of an individual), or both. They can be holistic (holding that higher realities emerge from and are not reducible to the lower), idealist (holding that reality is primarily mental or spiritual) or nondual (holding that there is no ultimate distinction between mental and physical reality). One can regard all of them as teleological to a greater or lesser degree.\n\nPhilosophers, scientists, and educators who have proposed theories of spiritual evolution include Schelling (1775-1854), Hegel (1770-1831), Carl Jung (1875-1961), Max Théon (1848-1927), Helena Petrovna Blavatsky (1831-1891), Henri Bergson (1859-1941), Rudolf Steiner (1861-1925), Sri Aurobindo (1872-1950), Jean Gebser (1905-1973), Pierre Teilhard de Chardin (1881-1955), Owen Barfield (1898-1997), Arthur M. Young (1905-1995), Edward Haskell (1906-1986), E. F. Schumacher (1911-1977), Erich Jantsch (1929-1980), Clare W. Graves (1914-1986), Alfred North Whitehead (1861-1947), Terence McKenna (1946-2000), and P. R. Sarkar (1921-1990). William Irwin Thompson (born 1938), Victor Skumin (born 1948), Ken Wilber (born 1949), and Brian Swimme (born 1950) work in this field.\n\nMircea Eliade has suggested that in many pre-modern cultures one finds the concept of the Fall and a \"nostalgia for paradise\". However, for those cultures that have a cyclic cosmology, the concept of a progressive deterioration of the universe (as in the Hesiodic, Hindu, and Lurianic cosmologies of a degradation from a Golden Age to an Iron Age or Kali Yuga) might be balanced by a corresponding ascent to more spiritual stages and a return to paradisical conditions. This is what one finds in Buddhist and especially Jain cosmologies.\n\nMany premodern cosmologies and esoteric systems of thought are based on an emanationist view of reality. If the Cyclic view is temporal, then emanation is a non-temporal precursor to the theory of spiritual evolution.\n\nAccording to this paradigm, Creation proceeds as an outpouring or even a transformation in the original Absolute or Godhead. The Supreme Light or Consciousness descends through a series of stages, gradations, worlds or hypostases, becoming progressively more material and embodied, before finally turning around to return to the One, retracing its steps through spiritual knowledge, contemplation and ascent.\n\nA supreme example of this form of thinking is the Neoplatonism of Plotinus and his successors. Other examples and interpretations might be found in the Hindu sect of Kashmir Shaivism and Tantra in general, Gnosticism, Sufism, and Kabbalah. The Hindu idea of the Chakras might also considered here as the \"microcosmic\" counterpart of macrocosmic involution and evolution. The Yogi raises the Kundalini or life force through and thus transcends each chakra in turn, until he reaches the crown chakra and liberation.\n\nAn early example of the doctrine of spiritual evolution is found in Samkhya, one of the six systems of Hindu philosophy, that goes back more than two and a half thousand years (although its present form dates to around the 4th or 5th century c.e.). Unlike most types of classic Hinduism, the traditional Samkhyan philosophy is atheistic and dualistic. Pure spirit (called \"purusha\") comes into proximity with \"prakriti\" (psychophysical nature), disturbing its equilibrium. As a result, the original root-prakriti (\"mulaprakriti\") undergoes a series of progressive transformations or unfoldings, in the form of successive essences called \"tattvas\". The most subtle tattwas emerge first, then progressively grosser ones, each in a particular order, and finally the elements and the organs of sense. The goal of evolution however is, paradoxically, the release of purusha and the return to the unmanifest condition. Hence everything is tending towards a goal of spiritual quiescence.\n\nThe concept of the great chain of being developed by Plato and Aristotle whose ideas were taken up and synthesised by Plotinus. Plotinus in turn heavily influenced Augustine's theology, and from there Aquinas and the Scholastics. The Great Chain of Being was an important theme in Renaissance and Elizabethan thought, had an under-acknowledged influence on the shaping of the ideas of the Enlightenment and played a large part in the worldview of 18th century Europe. And while essentially a static worldview, by the 18th and early 19th century it had been \"temporalized\" by the concept of the soul ascending or progressing spiritually through the successive rungs or stages, and thus growing or evolving closer to God. It also had at this time an impact on theories of biological evolution.\n\nE. F. Schumacher, author of \"Small is Beautiful\", has recently proposed a sort of simplified Great Chain of Being, based on the idea of four \"kingdoms\" (mineral, vegetable, animal, human). Schumacher rejects modernist and scientific themes, his approach recalling the universalist orientation of writers like Huston Smith, and likely contributing to Ken Wilber's \"holonomic\" hierarchy or \"Great Nest of Being\".\n\nThe concept of spiritual evolution has been taught in Buddhism. William Sturgis Bigelow - a physician and Buddhist - attempted to merge biology with spirituality. He accepted the existence of both material and spiritual realms, and many of his ideas were discussed in his book \"Buddhism and Immortality\" (1908). Bigelow used the concept of natural selection as a mechanism for evolution. According to the author, spiritual evolution involves an individual emerging from \"unconditioned consciousness\" and moving \"up the scale of evolution guided by natural selection\". Then the individual moves to a level of celestial experience, and finally is able to \"return to the unconditioned consciousness from which all things emerge\". Bigelow accepted both material and spiritual evolution and he also believed that Buddhism and science were compatible.\n\nAlbert Low a Zen master and author of \"The Origin of Human Nature: A Zen Buddhist Looks at Evolution\" (2008) opposes neo-Darwinism and the selfish gene theory as he claims they are materialistic. He also opposes creationism for being dogmatic and instead advocates spiritual evolution.\n\nIn Vajrayana, spiritual evolution is equivalent with the development of the three bodies of Trikaya.\n\nTheories of spiritual evolution are important in many Occult and Esoteric teachings, which emphasise the progression and development of the individual either after death (spiritualism) or through successive reincarnations (Theosophy, Hermeticism).\n\nSpiritualists reacted with uncertainty to the theories of evolution in the late 19th and early 20th century. Broadly speaking, the concept of evolution fit the spiritualist thought of the progressive development of humanity. At the same time, however, a belief in the animal origins of man threatened the foundation of the immortality of the spirit, for if man had not been created, it was scarcely plausible that he would be specially endowed with a spirit. This led to spiritualists embracing spiritual evolution.\n\nIn the 19th century, Anglo-American Spiritualist ideas emphasized the progression of the soul after death to higher states of existence, in contrast to Spiritism which admits to reincarnation.\n\nSpiritualism taught that after death, spirits progressed to new spheres of existence. According to this idea, evolution occurred in the spirit world “at a rate more rapid and under conditions more favorable to growth” than encountered on earth.\n\nThe biologist and spiritualist Alfred Russel Wallace (1823–1913) believed that qualitative novelties could arise through the process of spiritual evolution, in particular, the phenomena of life and mind. Wallace attributed these novelties to a supernatural agency. Later in his life, Wallace was an advocate of spiritualism and believed in an immaterial origin for the higher mental faculties of humans. He believed that evolution suggested the universe had a purpose, and that certain aspects of living organisms are not explainable in terms of purely materialistic processes. In a 1909 magazine article entitled \"The World of Life\", which he later expanded into a book of the same name Wallace argued in his 1911 book \"World of life\" for a spiritual approach to evolution and described evolution as “creative power, directive mind and ultimate purpose”. Wallace believed natural selection could not explain intelligence or morality in the human being so suggested that non-material spiritual forces accounted for these. Wallace believed the spiritual nature of man could not have come about by natural selection alone, the origins of the spiritual nature must originate “in the unseen universe of spirit”.\n\nRobert Broom in his book \"The Coming of Man: Was it Accident or Design?\" (1933) claimed that \"spiritual agencies\" had guided evolution as animals and plants were too complex to have arisen by chance. According to Broom there were at least two different kinds of spiritual forces, and psychics are capable of seeing them. Broom claimed there was a plan and purpose in evolution and that the origin of Homo sapiens is the ultimate purpose behind evolution. According to Broom \"Much of evolution looks as if it had been planned to result in man, and in other animals and plants to make the world a suitable place for him to dwell in.\n\nThe Anglo-American position recalls (and is presumably inspired by) 18th century concepts regarding the temporalization of The Great Chain of Being. Spiritual evolution, rather than being a physical (or physico-spiritual) process is based on the idea of realms or stages through which the soul or spirit passes in a non-temporal, qualitative way. This is still an important part of some spiritualist ideas today, and is similar to some mainline (as opposed to fundamentalist) Protestant Christian beliefs, according to which after death the person goes to \"summerland\" (see Spirit world)\n\nTheosophy presents a more sophisticated and complex cosmology than Spiritualism, although coming out of the same general milieu. H. P. Blavatsky developed a highly original cosmology, according to which the human race (both collectively and through the succession of individual reincarnation and spiritual evolution) passes through a number of Root Races, beginning with the huge ethereal and mindless Polarian or First Root Race, through the Lemurian (3rd), Atlantean (4th) and our present \"Aryan\" 5th Race. This will give rise to a future, Post-Aryan 6th Root Race of highly spiritual and enlightened beings that will arise in Baja California in the 28th century, and an even more sublime 7th Root Race, before ascending to totally superhuman and cosmic states of existence.\n\nBlavatsky's ideas were further developed by her successors, such as C.W. Leadbeater, Rudolf Steiner, Alice Bailey, Benjamin Creme, and Victor Skumin each of whom went into huge detail in constructing baroque cycles of rounds, races, and sub-races. Skumin elaborated on the theosophical conceptions of spiritual evolution, he proposed a definition and classification of Homo spiritalis (Latin: \"spiritual man\"), the sixth root race, consisting of eight sub-races (subspecies): HS0 Anabiosis spiritalis, HS1 Scientella spiritalis, HS2 Aurora spiritalis, HS3 Ascensus spiritalis, HS4 Vocatus spiritalis, HS5 Illuminatio spiritalis, НS6 Creatio spiritalis, and HS7 Servitus spiritalis.\n\nAlthough including elements of the science of her day as well as both eastern and western esoteric thought, Blavatsky rejected the Darwinian idea that man evolved from apes, and most subsequent esotericists followed this lead. Darwinism, with its explanation of evolution through material factors like natural selection and random mutation, does not sit well with many spiritual evolutionists, for whom evolution is initiated or guided by metaphysical principles or is tending towards a final spiritual or divine state. It is believed by Theosophists that humans are evolving \"spiritually\" through a series of esoteric initiations and in the future humans will become esoteric masters themselves as their souls gradually rise upward through the spiritual hierarchy over the course of eons as they reincarnate.\n\nDespite this, recent Theosophists and Anthroposophists have tried to incorporate the facts of geology and paleontology into their cosmology and spiritual evolution (in Anthroposophy Hermann Poppelbaum is a particularly creative thinker in this regard). Some have attempted to equate Lemuria with Gondwanaland, for example. Today all these ideas have little influence outside their specialised followings, but for a time Theosophical concepts were immensely influential. Theosophy-like teachings also continue today in a group of religions based on Theosophy called the Ascended Master Teachings.\n\nTheurgy has a clear relationship to Neoplatonism and Kabbalah and contains the concept of spiritual evolution and ultimately unification with God or the Godhead at its core. Theurgy is considered by many to be another term for high magic and is known to have influenced the members of the Hermetic Order of the Golden Dawn many of whom considered the order to be Theurgic in nature. Aleister Crowley also considered his Thelemic system of magical philosophy to be a Theurgic tradition as it emphasized the Great Work, which is essentially another form of spiritual evolution. The Great Work is believed to result in communication with one's personal angel or higher self.\n\nEpigenesis is the philosophical/theological/esoteric idea that since the mind was given to the human being, it is the original creative impulse, epigenesis, which has been the cause of all of mankind's development.\n\nAccording to spiritual evolution, humans build upon that which has already been created, but add new elements because of the activity of the spirit. Humans have the capacity, therefore, to become creative intelligences—creators. For a human to fulfill this promise, his training should allow for the exercise of originality, which distinguishes creation from imitation. When epigenesis becomes inactive, in the individual or even in a race, evolution ceases and degeneration commences.\n\nThis concept is based on the Rosicrucian view of the world as a training school, which posits that while mistakes are made in life, humans often learn more from mistakes than successes. Suffering is considered as merely the result of error, and the impact of suffering on the consciousness causes humans to be active along other lines which are found to be good, in harmony with nature. Humans are seen as spirits attending the school of life for the purpose of unfolding latent spiritual power, developing themselves from \"impotence\" to omnipotence (related also to development from innocence into virtue), reaching the stage of creative gods at the end of mankind's present evolution: \"Great Day of Manifestation\".\n\nSri Aurobindo and Pierre Teilhard de Chardin both describe a progression from inanimate matter to a future state of Divine consciousness. Teilhard de Chardin refers to this as the Omega Point, and Sri Aurobindo as the Supermind.\n\nTeilhard, who was a Jesuit Paleontologist who played an important role in the discovery of Peking Man, presented a teleological view of planetary and cosmic evolution, according to which the formation of atoms, molecules and inanimate matter is followed by the development of the biosphere and organic evolution, then the appearance of man and the noosphere as the total envelope of human thought. According to Teilhard evolution does not cease here but continues on to its culmination and unification in the Omega Point, which he identifies with Christ.\n\nMeher Baba has used the term involution to describe the inner journey of consciousness after transcending the physical or gross state up to the attainment of Self-consciousness, or merging with God. According to Meher Baba, the consciousness of the soul in duality first goes through the long process of evolution of form, then, upon reaching the human form, consciousness enters the process of \"reincarnation\", and finally reaches the process of \"involution\", which culminates in \"God-realization\".\n\nSurat Shabda Yoga esoteric cosmology depicts the whole of creation (the macrocosm) as being emanated and arranged in a spiritually differentiated hierarchy, often referred to as eggs, regions, or planes. Typically, eight spiritual levels are described above the physical plane, although names and subdivisions within these levels will vary to some extent by mission and Master. (One version of the creation from a Surat Shabda Yoga perspective is depicted at the Sant Ajaib Singh Ji Memorial Site in “The Grand Scheme of All Creation”.)\n\nThe constitution of the individual (the microcosm) is an exact replica of the macrocosm. Consequently, the microcosm consists of a number of bodies, each one suited to interact with its corresponding plane or region in the macrocosm. These bodies developed over the yugas through involution (emanating from higher planes to lower planes) and evolution (returning from lower planes to higher planes), including by karma and reincarnation in various states of consciousness.\n\nArthur M. Young and Edward Haskell have each independently incorporated the findings of science into a larger theory of spiritual evolution, and extended the traditional human, animal, vegetable, and mineral categories with kingdoms representing photons, atoms and molecules. Arthur M. Young goes further in considering the human state as a subset of a larger kingdom of \"Dominion\", of which the sixth stage is represented for example by Christ and Buddha, and the seventh (final) stage an even higher level of Enlightenment or God-realisation. Moreover, both Haskell and Young present profound accounts of evolution through these kingdoms in terms of cybernetic principles. A more \"mainstream\" scientific presentation of this same idea is provided by Erich Jantsch in his account of how self-organising systems evolve and develop as a series of \"symmetry breaks\" through the sequence of matter, life, and mind. Although abiding strictly by the understanding of science, Jantsch arranges the various elements of cosmic, planetary, biological, psychological, and human evolution in a single overall framework of emergent evolution that may or may not be considered teleological.\n\nNew Age thought is strongly syncretic. A common theme is the evolution or the transcendence of the human or collective planetary consciousness in a higher state or higher \"vibratory\" (a metaphor taken from G. I. Gurdjieff) level.\n\nDavid Spangler's communications speak of a \"New Heaven and a new Earth\", while Christopher Hills refers (perhaps influenced by Sri Aurobindo) to\nthe divinization of man.\n\n\"Jonathan Livingston Seagull\" narrated the idea of evolution in a fascinating fashion. James Redfield in his novel \"The Celestine Prophecy\" suggested that through experiencing a series of personal spiritual insights, humanity is becoming aware of the connection between our evolution and the Divine. More recently in his book \"God and the Evolving Universe: The Next Step in Personal Evolution\" (2002) co-written with Michael Murphy, he claims that humanity is on the verge of undergoing a change in consciousness.\n\nAn interpretation of social and psychological development that could also be considered a theory of spiritual evolution is spiral dynamics, based on the work of Clare W. Graves.\n\nMore recently the concept of spiritual evolution has been given a sort of respectability it has not had since the early 19th century through the work of the integral theorist Ken Wilber, in whose writings both the cosmological and the personal dimensions are described. In this integral philosophy (inspired in part by the works of Plotinus, Hegel, Sri Aurobindo, Eric Jantsch, and many others) reality is said to consist of several realms or stages, including more than one of the following: the physical, the vital, the psychic, (after the Greek \"psyche\", \"soul\"), the causal (referring to \"that which causes, or gives rise to, the manifest world\"), and the ultimate (or non-dual), through which the individual progressively evolves. Although this schema is derived in large part from Tibetan Buddhism, Wilber argues (and uses many tables of diagrams to show) that these same levels of being are common to all wisdom teachings. Described simplistically, Wilber sees humans developing through several stages, including magic, mythic, pluralistic, and holistic mentalities. But he also sees cultures as developing through these stages. And, much like Hegel, he sees this development of individuals and cultures \"as\" the evolution of existence itself. Wilber has also teamed up with Don Beck to integrate Spiral Dynamics into his own Integral philosophy, and vice versa. Spiral Dynamics posits a series of stages through which human's cultural development progresses – from a survival-based hunter-gatherer stage to a magical-tribal-agrarian stage to a city-building-invading stage to a mythic-religious-empire stage to a rational-scientific-capitalist stage to a green-holistic-inclusive stage and then ascending to a second tier where all the previous stages are contemplated and integrated and a third transpersonal tier where a spiritual unity or Omega point is eventually reached, which all the other stages are struggling to embody. He feels that individuals in each of the meme-plexes/stages can ascend to the peak of consciousness – these being the prophets, visionaries and leaders of any region/age.\n"}
{"id": "31236248", "url": "https://en.wikipedia.org/wiki?curid=31236248", "title": "Stationary orbit", "text": "Stationary orbit\n\nIn celestial mechanics, the term stationary orbit refers to an orbit around a planet or moon where the orbiting satellite or spacecraft remains orbiting over the same spot on the surface. From the ground, the satellite would appear to be standing still, hovering above the surface in the same spot, day after day.\n\nIn practice, this is accomplished by matching the rotation of the surface below, by reaching a particular altitude where the orbital speed almost matches the rotation below, in an equatorial orbit. As the speed decreases slowly, then an additional boost would be needed to increase the speed back to a matching speed, or a retro-rocket could be fired to slow the speed when too fast. \n\nThe stationary-orbit region of space is known as the Clarke Belt, named after British science fiction writer Arthur C. Clarke, who published the idea in \"Wireless World\" magazine in 1945. A stationary orbit is sometimes referred to as a \"fixed orbit\".\n\nAround the Earth, stationary satellites orbit at altitudes of approximately . Writing in 1945, the science-fiction author Arthur C. Clarke imagined communications satellites as travelling in stationary orbits, where those satellites would travel around the Earth at the same speed the globe is spinning, making them hover stationary over one spot on the Earth's surface.\n\nA satellite being propelled into place, into a stationary orbit, is first fired to a special equatorial orbit called a \"geostationary transfer orbit\" (GTO). Within this oval-shaped (elliptical) orbit, the satellite will alternately swing out to high and then back down to an altitude of only above the Earth (223 times closer). Then, at a planned time and place, an attached \"kick motor\" will push the satellite out to maintain an even, circular orbit at the 22,300-mile altitude.\n\nAn \"areostationary orbit\" or \"areosynchronous equatorial orbit\" (abbreviated \"AEO\") is a circular areo­synchronous orbit in the Martian equatorial plane about above the surface, any point on which revolves about Mars in the same direction and with the same period as the Martian surface. Areo­stationary orbit is a concept similar to Earth's geo­stationary orbit. The prefix \"areo-\" derives from Ares, the ancient Greek god of war and counterpart to the Roman god Mars, with whom the planet was identified. The modern Greek word for Mars is ().\n\n"}
{"id": "2556015", "url": "https://en.wikipedia.org/wiki?curid=2556015", "title": "Systematized Nomenclature of Medicine", "text": "Systematized Nomenclature of Medicine\n\nThe Systematized Nomenclature of Medicine (SNOMED) is a systematic, computer-processable collection of medical terms, in human and veterinary medicine, to provide codes, terms, synonyms and definitions which cover anatomy, diseases, findings, procedures, microorganisms, substances, etc. It allows a consistent way to index, store, retrieve, and aggregate medical data across specialties and sites of care. Although now international, SNOMED was started in the U.S. by the College of American Pathologists (CAP) in 1973 and revised into the 1990s. In 2002 CAP's SNOMED Reference Terminology (SNOMED RT) was merged with, and expanded by, the National Health Service's Clinical Terms Version 3 (previously known as the Read codes) to produce SNOMED CT.\n\nVersions of SNOMED released prior to 2001 were based on a multiaxial, hierarchical classification system. As in any such system, a disease may be located in a body organ (anatomy), which results in a code in a topography axis and may lead to morphological alterations represented by a morphology code.\n\nIn 2002 the first release of SNOMED CT adopted a completely different structure. A sub-type hierarchy, supported by defining relationships based on description logic, replaced the axes described in this article. Versions of SNOMED prior to SNOMED CT are planned to be formally deprecated from 2017. Therefore, readers interested in current information about SNOMED are directed to the article on SNOMED CT.\n\nSNOMED was designed as a comprehensive nomenclature of clinical medicine for the purpose of accurately storing and/or retrieving records of clinical care in human and veterinary medicine.\nThe metaphor used by Roger A. Côté, the first editorial chair, was that SNOMED would become the periodic table of elements of medicine because of its definitional organization beyond the hierarchical design. Indeed, diseases and procedures were ordered hierarchically and are further referenced back to more elementary terms (see Reference Ontology and Multi-Axial Design, below).\n\nSNOMED was originally conceived by Côté as an extension of the design of the Systematized Nomenclature of Pathology (SNOP) applicable for all medicine. SNOP was originally designed by Arnold Pratt to describe pathological specimens according to their morphology and anatomy (topography). The ambitious development of SNOMED required many more axes (see multi-axial design, below). SNOMED was jointly proposed for development to the College of American Pathologists by Côté and Pratt. The former was appointed as editorial chair of the Committee on Nomenclature and Classification of Diseases of the CAP and developed the SNOMED from 1973 to 1997. In 1998, Kent Spackman was appointed to chair this committee and spearheaded the transformation of the multi-axis systems into a highly computable form (See SNOMED CT): a directed acyclic graph anchored in formal representation logic. In 2007, the newly formed International Health Terminology Standards Development Organisation (IHTSDO) acquired all the Intellectual Property of SNOMED CT and all antecedent SNOMED versions.\n\nBrief timeline:\n\nSNOMED was designed from its inception with complex concepts defined in terms of simpler ones. For example, a disease can be defined in terms of its abnormal anatomy, abnormal functions and morphology. In some cases, the etiology of the disease is known and can be attributed to an infectious agent, a physical trauma or a chemical or pharmaceutical agent.\n\nThe current concept uses eleven (11) axes that comprise terms organised in hierarchical trees. The axes and some examples are provided below:\n\n\n\n\"For the Morphology axis, SNOMED has agreed to collaborate and use the same harmonized codes shared with International Classification of Diseases for Oncology. Additional examples on topology are provided on that page.\"\n\n\n\n\n\n\n\n\n"}
{"id": "50831308", "url": "https://en.wikipedia.org/wiki?curid=50831308", "title": "Theatrum Mundi", "text": "Theatrum Mundi\n\nTheatrum Mundi (or the Great Theater of the World) is a metaphorical concept developed throughout Western literature and thought, apparent in theories of the world such as Plato's Allegory of the Cave, and a popular idea in the Baroque Period among certain writers. This metaphysical explanation of the world portrays the world as a theater (apparent in Shakespeare's saying that \"all the world's a stage\") wherein people are characters and their actions form a drama, with God as the author, specifically for some Christian thinkers. This metaphor can take various forms, some more deterministic than others, and has also been formulated in different fashions, such as the world as chess game by the Persian philosopher Omar Khayyam. In each formulation of the \"theatrum mundi\", though, the world is a sum greater than its parts, where various roles are played by different actors.\n\nThe world as a stage was expressed among the ancient Greeks, and especially gained popularity among Stoic and Neoplantonist philosophers such as Plotinus in the late antiquity of the Roman Empire. In Neoplatonism, which went on to influence Christianity, the belief of the separate realm of the soul and its transcendence above the instability of worldly affairs influenced philosophers and later, important Christian figures like St. Augustine to view the world as a theatrical spectacle. \n\nThe relation of God to humanity and the world was expressed throughout the Middle Ages. John of Salisbury, a 12th-century theologian especially coined the term \"theatrum mundi\", characterized by commenting that saints \"despise the theater of this world from the heights of their virtue\". In several chapters of the third book of his \"Policraticus\", a moral encyclopedia, he meditates on the fact that \"the life of man on earth is a comedy, where each forgetting his own plays another's role\". The comedy takes place on the scene/in the world, while the \"auditorium\" is associated to the Christian paradise. Only a few sages, like some Stoic philosophers or the prophets like Abraham or John the Baptiste, are able to accept the role given by God. This acceptation allows to extract themselves from the \"theatrum mundi\", to adopt a celestial position in the \"auditorium\", and to watch and understand the roles played in the comedy.\n\nThe metaphor had intercourse with the actual theater, which could be conversely conceived as a world, or a microcosm. The idea continued to be expressed throughout the early modern period and became less of a strictly theological or philosophical metaphor and began to insert itself in various forms of literature and rhetorical expressions, and its use in certain times has been theorized to be a result of political and religious alienation, especially in the midst of tumultuous historical periods. It is possible that it gradually began to lose its religious connotations, and the \"theatrum mundi\" took on more of a secular, political aspect. But going back to Plato's emphasis in \"Laws\" of a protagonist embodying the ideal political subject by reflecting the ideal plane, this both political and theological formulation of \"theatrum mundi\"was eventually propagated by Tertullian to the point where the sociological idea of roles descends from it as well. Additionally, it was also cultivated by the transition towards new social forms; for instance, the trappings of the feudal monarchy in England was seen as \"empty\" and \"theatrical\" because they occurred in the social context of an ascendant capitalism. It was in England, moreover, where the metaphor was the most developed, although it was also refined in Spain.\n\nTransforming with the developments of philosophy and culture, the \"theatrum mundi\" became less popular in the 18th and 19th centuries, but more recently reconceptualizations have developed among Situationists and Jean Baudrillard, as well as Brecht, Beckett, and Artaud.\n\n\nQuiring, Bjorn. \"Revisions of the Theatrum Mundi Metaphor in Early Modern England\". De Gruyter, 2014.\n"}
{"id": "3101959", "url": "https://en.wikipedia.org/wiki?curid=3101959", "title": "Ticking time bomb scenario", "text": "Ticking time bomb scenario\n\nThe ticking time bomb scenario is a thought experiment that has been used in the ethics debate over whether torture can ever be justified. As a thought experiment, there is no need that the scenario be plausible; it need only serve to highlight ethical considerations. The scenario can be formulated as follows:\n\nSuppose that a person with knowledge of an imminent terrorist attack, that will kill many people, is in the hands of the authorities and that he will disclose the information needed to prevent the attack only if he is tortured. Should he be tortured?\n\nThe scenario can be better understood through the arguments of those who respond to it; the consequentialist argument is that nations, even those that legally disallow torture, can justify its use if they have a terrorist in custody who possesses critical knowledge, such as the location of a time bomb or a weapon of mass destruction that will soon explode and cause great loss of life. Opponents to the argument usually begin by exposing certain assumptions that tend to be hidden by initial presentations of the scenario and tend to obscure the true costs of permitting torture in \"real-life\" scenarios—e.g., the assumption that the person is in fact a terrorist, whereas in real life there usually remains uncertainty about whether the person is in fact a terrorist and that he has useful information—and rely on legal, philosophical/moral, and empirical grounds to reaffirm the need for the absolute prohibition of torture. There is also uncertainty about the effectiveness of torture, and much opposition to torture is based on the fact it is not effective rather than any moral issue.\n\nPhilosopher Jeremy Bentham has been regarded as the \"father\" of the ticking time bomb argument. He wrote in his 1804 essay \"Means of extraction for extraordinary occasions\":\n\nThe concept was popularized in the 1960s in the novel \"Les Centurions\" by Jean Lartéguy which is set during the Algerian war. The version in the novel has the following conditions:\nAccording to Darius Rejali, a professor of political science at Reed College, the possibility of sudden, massive destruction of innocent life provided French liberals with a more acceptable justification for committing torture.\n\nAlan Dershowitz, a prominent American defense attorney, surprised some observers by giving limited support to the idea that torture could be justified. He argued that human nature can lead to unregulated abuse \"off the books\". Therefore, it would be better if there were a regulated procedure through which an interrogator could request a \"torture warrant\" and that requiring a warrant would establish a paper trail of accountability. Torturers, and those who authorize torture, could be held to account for excesses. Dershowitz's suggested torture warrants, similar to search warrants, arrest warrants and phone tap warrants, would spell out the limits on the techniques that interrogators may use, and the extent to which they may abridge a suspect's rights.\n\nIn September 2002, when reviewing Alan Dershowitz's book, \"Why Terrorism Works: Understanding the Threat, Responding to the Challenge\", Richard Posner, legal scholar and judge of the United States Court of Appeals for the Seventh Circuit, wrote in \"The New Republic\", \"If torture is the only means of obtaining the information necessary to prevent the detonation of a nuclear bomb in Times Square, torture should be used--and will be used--to obtain the information... No one who doubts that this is the case should be in a position of responsibility.\"\n\nIn February 2010 Bruce Anderson wrote a column for \"The Independent\", arguing that the British government would have not just the right, but the duty, to torture if there was a ticking bomb, and that they should torture the relatives of suspects if they believed that doing so would yield information that would avert a terrorist attack: \"It came, in the form of a devilish intellectual challenge. 'Let's take your hypothesis a bit further. We have captured a terrorist, but he is a hardened character. We cannot be certain that he will crack in time. We have also captured his wife and children'. After much agonising, I have come to the conclusion that there is only one answer to Sydney Kentridge's question. Torture the wife and children.\"\n\nSome human rights organizations, professional and academic experts, and military and intelligence leaders have absolutely rejected the idea that torture is ever legal or acceptable, even in a so-called ticking bomb situation. They have expressed grave concern about the way the dramatic force and artificially simple moral answers the ticking bomb thought-experiment seems to offer, have manipulated and distorted the legal and moral perceptions, reasoning and judgment of both the general population and military and law enforcement officials. They reject the proposition, implicit or explicit, that certain acts of torture are justifiable, even desirable. They believe that simplistic responses to the scenario may lead well-intentioned societies down a slippery slope to legalized and systematic torture. They point out that no evidence of any real-life situation meeting all the criteria to constitute a pure ticking bomb scenario has ever been presented to the public, and that such a situation is highly unlikely.\n\nAs well, torture can be criticized as a poor vehicle for discovering truth, as people experiencing torture, once broken, are liable to make anything up in order to stop the pain and can become unable to tell the difference between fact and fiction under intense psychological pressure. Additionally, since the terrorist presumably knows that the timer is ticking, he has an excellent reason to lie and give false information under torture in order to misdirect his interrogators; merely giving a convincing answer which the investigators will waste time checking out makes it more likely that the bomb will go off, and of course once the bomb has gone off, not only has the terrorist won, but there is also no further point in torturing him, except perhaps as revenge.\n\nOthers point out that the ticking-bomb torture proponents adopt an extremely short-term view, which impoverishes their consequentialism. Using torture—or even declaring that one is prepared to accept its use—makes other groups of people much more likely to use torture themselves in the long run. The consequence is likely to be a long-term increase in violence. This long-term effect is so serious that the person making the torture decision cannot possibly (according to this argument) make a reasonable estimate of its results. Thus the decision-maker has no grounds for certainty that the value of the lives saved from the ticking bomb will outweigh the value of the lives lost because of the subsequent disorder. He or she cannot arrive at a successful accounting of consequences.\n\nThis anti-torture argument, in fact, works by positing that human knowledge has intrinsic limits. An analogous argument holds that human decision-makers are fundamentally prone in certain situations to believe that their judgment is better than it is, and that, to be ethical, they must pre-commit themselves to a particular course of action in those situations. Knowing that, under stress, they will never be able to accurately assess the likely success of torture in obtaining information needed to prevent an attack, humans thus pre-commit to not torture. In general, this family of arguments faults the \"ticking-bomb\" scenario for implicitly including an incorrect presumption that the decision-maker can know in advance the outcome of torture, either in the short run (likelihood that it will prevent an attack) or the long run (likelihood that it will not set off a general increase in human violence).\n\nJoe Navarro, one of the FBI’s top experts in questioning techniques, told The New Yorker:\n\nThe United Nations Convention against Torture and Other Cruel, Inhuman or Degrading Treatment or Punishment, which was adopted on December 10, 1984, and entered into force on June 26, 1987, explicitly states in Article 2.2 that:\n\nCritics of the thought experiment scenario maintain that it is essentially implausible, based on simultaneous presence of numerous unlikely factors. This is particularly acute in fictional exploration of the scenario.\n\nFor example, in perhaps the most common variants on the scenario, one must assume that torturers know, with a reasonable degree of certainty that some form of deadly attack is imminent, but lack a crucial component of that plan, such as its precise location. They must also have in their custody someone who they are reasonably certain has said information and would talk under torture or threat of torture. They must then be able to accurately distinguish between true and false information which the subject may supply under torture. They must then be able to use this information to form a plan of response which is effective at stopping the planned attack. All of this must occur within a limited time frame allowed by the \"ticking bomb\".\n\nWorks of fiction, such as the television series \"24\", often rely on ticking time bomb scenarios for dramatic effect. According to the Parents Television Council, given that each season represents a 24-hour period, Jack Bauer encounters someone who needs torturing to reveal a ticking bomb on average 12 times per day.\n\nMichael Chertoff, the Secretary of Homeland Security under the George W. Bush administration, declared that \"24\" \"reflects real life\", John Yoo, the former Justice Department lawyer who produced the torture memos cited Bauer in support while Supreme Court Justice Antonin Scalia went further, \"Jack Bauer saved Los Angeles... He saved hundreds of thousands of lives. Are you going to convict Jack Bauer?\". One of the shows' creators stated:\n\nThe show uses the same techniques that are used by the U.S. against terrorist suspects during the War on Terror. U.S. Army Brigadier General Patrick Finnegan, the dean of the United States Military Academy at West Point, and others, objected to the central theme of the show—that the letter of American law must be sacrificed for the country’s security—as it had an adverse effect on the training of actual American soldiers by advocating unethical and illegal behavior. As Finnegan said:\n\nHe continued,\n\nThe \"ticking time bomb scenario\" is subject of the drama \"The Dershowitz Protocol\" by Canadian author Robert Fothergill. In that play, the American government has established a protocol of \"intensified interrogation\" for terrorist suspects which requires participation of the FBI, CIA and the Department of Justice. The drama deals with the psychological pressure and the tense triangle of competences under the overriding importance that each participant has to negotiate the actions with his conscience.\n\n\n\n"}
{"id": "715886", "url": "https://en.wikipedia.org/wiki?curid=715886", "title": "Time-to-digital converter", "text": "Time-to-digital converter\n\nIn electronic instrumentation and signal processing, a time to digital converter (abbreviated TDC) is a device for recognizing events and providing a digital representation of the time they occurred. For example, a TDC might output the time of arrival for each incoming pulse. Some applications wish to measure the time interval between two events rather than some notion of an absolute time.\n\nIn electronics time-to-digital converters (TDCs) or time digitizers are devices commonly used to measure a time interval and convert it into digital (binary) output. In some cases interpolating TDCs are also called time counters (TCs).\n\nTDCs are used in many different applications, where the time interval between two signal pulses (start and stop pulse) should be determined. Measurement is started and stopped, when either the rising or the falling edge of a signal pulse crosses a set threshold. These requirements are fulfilled in many physical experiments, like time-of-flight and lifetime measurements in atomic and high energy physics, experiments that involve laser ranging and electronic research involving the testing of integrated circuits and high-speed data transfer.\n\nTDCs are used in applications where measurement events happen infrequently, such as high energy physics experiments, where the sheer number of data channels in most detectors ensures that each channel will be excited only infrequently by particles such as electrons, photons, and ions.\n\nIf the required time resolution is not high, then counters can be used to make the conversion.\n\nIn its simplest implementation, a TDC is simply a high-frequency counter that increments every clock cycle. The current contents of the counter represents the current time. When an event occurs, the counter's value is captured in an output register.\n\nIn that approach, the measurement is an integer number of clock cycles, so the measurement is quantized to a clock period. To get finer resolution, a faster clock is needed. The accuracy of the measurement depends upon the stability of the clock frequency.\n\nTypically a TDC uses a crystal oscillator reference frequency for good long term stability. High stability crystal oscillators are usually relative low frequency such as 10 MHz (or 100 ns resolution). To get better resolution, a phase-locked loop frequency multiplier can be used to generate a faster clock. One might, for example, multiply the crystal reference oscillator by 100 to get a clock rate of 1 GHz (1 ns resolution).\n\nHigh clock rates impose additional design constraints on the counter: if the clock period is short, it is difficult to update the count. Binary counters, for example, need a fast carry architecture because they essentially add one to the previous counter value. A solution is using a hybrid counter architecture. A Johnson counter, for example, is a fast non-binary counter. It can be used to count very quickly the low order count; a more conventional binary counter can be used to accumulate the high order count. The fast counter is sometime called a prescaler.\n\nThe speed of counters fabricated in CMOS-technology is limited by the capacitance between the gate and the channel and by the resistance of the channel and the signal traces. The product of both is the cut-off-frequency. Modern chip technology allows multiple metal layers and therefore coils with a large number of windings to be inserted into the chip.\nThis allows designers to peak the device for a specific frequency, which may lie above the cut-off-frequency of the original transistor.\n\nA peaked variant of the Johnson counter is the traveling-wave counter which also achieves sub-cycle resolution. Other methods to achieve sub-cycle resolution include analog-to-digital converters and vernier Johnson counters.\n\nIn most situations, the user does not want to just capture an arbitrary time that an event occurs, but wants to measure a time interval, the time between a start event and a stop event.\n\nThat can be done by measuring an arbitrary time both the start and stop events and subtracting. The measurement can be off by two counts.\n\nThe subtraction can be avoided if the counter is held at zero until the start event, counts during the interval, and then stops counting after the stop event.\n\nCoarse counters base on a reference clock with signals generated at a stable frequency formula_1. When the start signal is detected the counter starts counting clock signals and terminates counting after the stop signal is detected. The time interval formula_2 between start and stop is then\n\nwith formula_4, the number of counts and formula_5, the period of the reference clock.\n\nSince start, stop and clock signal are asynchronous, there is a uniform probability distribution of the start and stop signal-times between two subsequent clock pulses. This detuning of the start and stop signal from the clock pulses is called quantization error.\n\nFor a series of measurements on the same constant and asynchronous time interval one measures two different numbers of counted clock pulses formula_6 and formula_7 (see picture). These occur with probabilities\n\nwith formula_10 the fractional part of formula_11. The value for the time interval is then obtained by\n\nMeasuring a time interval using a coarse counter with the averaging method described above is relatively time consuming because of the many repetitions that are needed to determine the probabilities formula_13 and formula_14. In comparison to the other methods described later on, a coarse counter has a very limited resolution (1ns in case of a 1 GHz reference clock), but satisfies with its theoretically unlimited measuring range.\n\nIn contrast to the coarse counter in the previous section, fine measurement methods with much better accuracy but far smaller measuring range are presented here. Analogue methods like time interval stretching or double conversion as well as digital methods like tapped delay lines and the Vernier method are under examination. Though the analogue methods still obtain better accuracies, digital time interval measurement is often preferred due to its flexibility in integrated circuit technology and its robustness against external perturbations like temperature changes.\n\nThe counter implementation's accuracy is limited by the clock frequency. If time is measured by whole counts, then the resolution is limited to the clock period. For example, a 10 MHz clock has a resolution of 100 ns. To get resolution finer than a clock period, there are time interpolation circuits. These circuits measure the fraction of a clock period: that is, the time between a clock event and the event being measured. The interpolation circuits often require a significant amount of time to perform their function; consequently, the TDC needs a quiet interval before the next measurement.\n\nWhen counting is not feasible because the clock rate would be too high, analog methods can be used. Analog methods are often used to measure intervals that are between 10 and 200 ns. These methods often use a capacitor that is charged during the interval being measured. Initially, the capacitor is discharged to zero volts. When the start event occurs, the capacitor is charged with a constant current \"I\"; the constant current causes the voltage \"v\" on the capacitor to increase linearly with time. The rising voltage is called the fast ramp. When the stop event occurs, the charging current is stopped. The voltage on the capacitor \"v\" is directly proportional to the time interval \"T\" and can be measured with an analog-to-digital converter (ADC). The resolution of such a system is in the range of 1 to 10 ps.\n\nAlthough a separate ADC can be used, the ADC step is often integrated into the interpolator. A second constant current \"I\" is used to discharge the capacitor at a constant but much slower rate (the slow ramp). The slow ramp might be 1/1000 of the fast ramp. This discharge effectively \"stretches\" the time interval; it will take 1000 times as long for the capacitor to discharge to zero volts. The stretched interval can be measured with a counter. The measurement is similar to a dual-slope analog converter.\n\nThe dual-slope conversion can take a long time: a thousand or so clock ticks in the scheme described above. That limits how often a measurement can be made (dead time). Resolution of 1 ps with a 100 MHz (10 ns) clock requires a stretch ratio of 10,000 and implies a conversion time of 150 μs. To decrease the conversion time, the interpolator circuit can be used twice in a residual interpolator technique. The fast ramp is used initially as above to determine the time. The slow ramp is only at 1/100. The slow ramp will cross zero at some time during the clock period. When the ramp crosses zero, the fast ramp is turned on again to measure the crossing time (\"t\"). Consequently, the time can be determined to 1 part in 10,000.\n\nInterpolators are often used with a stable system clock. The start event is asynchronous, but the stop event is a following clock. For convenience, imagine that the fast ramp rises exactly 1 volt during a 100 ns clock period. Assume the start event occurs at 67.3 ns after a clock pulse; the fast ramp integrator is triggered and starts rising. The asynchronous start event is also routed through a synchronizer that takes at least two clock pulses. By the next clock pulse, the ramp has risen to .327 V. By the second clock pulse, the ramp has risen to 1.327 V and the synchronizer reports the start event has been seen. The fast ramp is stopped and the slow ramp starts. The synchronizer output can be used to capture system time from a counter. After 1327 clocks, the slow ramp returns to its starting point, and interpolator knows that the event occurred 132.7 ns before the synchronizer reported.\n\nThe interpolator is actually more involved because there are synchronizer issues and current switching is not instantaneous. Also, the interpolator must calibrate the height of the ramp to a clock period.\n\nThe vernier method is more involved. The method involves a triggerable oscillator and a coincidence circuit. At the event, the integer clock count is stored and the oscillator is started. The triggered oscillator has a slightly different frequency than the clock oscillator. For sake of argument, say the triggered oscillator has a period that is 1 ns faster than the clock. If the event happened 67 ns after the last clock, then the triggered oscillator transition will slide by −1 ns after each subsequent clock pulse. The triggered oscillator will be at 66 ns after the next clock, at 65 ns after the second clock, and so forth. A coincidence detector looks for when the triggered oscillator and the clock transition at the same time, and that indicates the fraction time that needs to be added.\n\nThe interpolator design is more involved. The triggerable clock must be calibrated to clock. It must also start quickly and cleanly.\n\nThe Vernier method is a digital version of the time stretching method. Two only slightly detuned oscillators (with frequencies formula_15 and formula_16) start their signals with the arrival of the start and the stop signal. As soon as the leading edges of the oscillator signals coincide the measurement ends and the number of periods of the oscillators (formula_6 and formula_7 respectively) lead to the original time interval formula_2:\n\nSince highly reliable oscillators with stable and accurate frequency are still quite a challenge one also realizes the vernier method via two tapped delay lines using two slightly different cell delay times formula_21. This setting is called differential delay line or vernier delay line.\n\nIn the example presented here the first delay line affiliated with the start signal contains cells of D-flip-flops with delay formula_22 which are initially set to transparent. During the transition of the start signal through one of those cells, the signal is delayed by formula_22 and the state of the flip-flop is sampled as transparent. The second delay line belonging to the stop signal is composed of a series of non-inverting buffers with delay formula_24. Propagating through its channel the stop signal latches the flip-flops of the start signal's delay line. As soon as the stop signal passes the start signal, the latter is stopped and all leftover flip-flops are sampled opaque. Analogous to the above case of the oscillators the wanted time interval formula_2 is then\n\nwith n the number of cells marked as transparent.\n\nIn general a tapped delay line contains a number of cells with well defined delay times formula_21. Propagating through this line the start signal is delayed. The state of the line is sampled at the time of the arrival of the stop signal.\nThis can be realized for example with a line of D-flip-flop cells with a delay time formula_21. The start signal propagates through this line of transparent flip-flops and is delayed by a certain number of them. The output of each flip-flop is sampled on the fly. The stop signal latches all flip-flops while propagating through its channel undelayed and the start signal cannot propagate further. Now the time interval between start and stop signal is proportional to the number of flip-flops that were sampled as transparent.\n\nCounters can measure long intervals but have limited resolution. Interpolators have high resolution but they cannot measure long intervals. A hybrid approach can achieve both long intervals and high resolution. The long interval can be measured with a counter. The counter information is supplemented with two time interpolators: one interpolator measures the (short) interval between the start event and a following clock event, and the second interpolator measure the interval between the stop event and a following clock event. The basic idea has some complications: the start and stop events are asynchronous, and one or both might happen close to a clock pulse. The counter and interpolators must agree on matching the start and end clock events. To accomplish that goal, synchronizers are used.\n\nThe common hybrid approach is the Nutt method. In this example the fine measurement circuit measures the time between start and stop pulse and the respective second nearest clock pulse of the coarse counter (\"T\", \"T\"), detected by the synchronizer (see figure). Thus the wanted time interval is\n\nwith \"n\" the number of counter clock pulses and \"T\" the period of the coarse counter.\n\nTime measurement has played a crucial role in the understanding of nature from the earliest times. Starting with sun, sand or water driven clocks we are able to use clocks today, based on the most precise caesium resonators.\n\nThe first direct predecessor of a TDC was invented in the year 1942 by Bruno Rossi for the measurement of muon lifetimes. It was designed as a time-to-amplitude-converter, constantly charging a capacitor during the measured time interval. The corresponding voltage is directly proportional to the time interval under examination.\n\nWhile the basic concepts (like Vernier methods (Pierre Vernier 1584-1638) and time stretching) of dividing time into measurable intervals are still up-to-date, the implementation changed a lot during the past 50 years. Starting with vacuum tubes and ferrite pot-core transformers those ideas are implemented in complementary metal-oxide-semiconductor (CMOS) design today.\n\nRegarding even the fine measuring methods presented, there are still errors one may wish remove or at least to consider. Non-linearities of the time-to-digital conversion for example can be identified by taking a large number of measurements of a poissonian distributed source (statistical code density test). Small deviations from the uniform distribution reveal the non-linearities.\nInconveniently the statistical code density method is quite sensitive to external temperature changes. Thus stabilizing delay or phase-locked loop (DLL or PLL) circuits are recommended.\n\nIn a similar way, offset errors (non-zero readouts at \"T\" = 0) can be removed.\n\nFor long time intervals, the error due to instabilities in the reference clock (jitter) plays a major role. Thus clocks of superior quality are needed for such TDCs.\n\nFurthermore, external noise sources can be eliminated in postprocessing by robust estimation methods.\n\nTDCs are currently built as stand-alone measuring devices in physical experiments or as system components like PCI cards. They can be made up of either discrete or integrated circuits.\n\nCircuit design changes with the purpose of the TDC, which can either be a very good solution for single-shot TDCs with long dead times or some trade-off between dead-time and resolution for multi-shot TDCs.\n\nThe time-to-digital converter measures the time between a start event and a stop event. There is also a digital-to-time converter or delay generator. The delay generator converts a number to a time delay. When the delay generator gets a start pulse at its input, then it outputs a stop pulse after the specified delay. The architectures for TDC and delay generators are similar. Both use counters for long, stable, delays. Both must consider the problem of clock quantization errors.\n\nFor example, the Tektronix 7D11 Digital Delay uses a counter architecture. A digital delay may be set from 100 ns to 1 s in 100 ns increments. An analog circuit provides an additional fine delay of 0 to 100 ns. A 5 MHz reference clock drives a phase-locked loop to produce a stable 500 MHz clock. It is this fast clock that is gated by the (fine-delayed) start event and determines the main quantization error. The fast clock is divided down to 10 MHz and fed to main counter. The instrument quantization error depends primarily on the 500 MHz clock (2 ns steps), but other errors also enter; the instrument is specified to have 2.2 ns of jitter. The recycle time is 575 ns.\n\nJust as a TDC may use interpolation to get finer than one clock period resolution, a delay generator may use similar techniques. The Hewlett-Packard 5359A High Resolution Time Synthesizer provides delays of 0 to 160 ms, has an accuracy of 1 ns, and achieves a typical jitter of 100 ps. The design uses a triggered phase-locked oscillator that runs at 200 MHz. Interpolation is done with a ramp, an 8-bit digital-to-analog converter, and a comparator. The resolution is about 45 ps.\nWhen the start pulse is received, then counts down and outputs a stop pulse. For low jitter the synchronous counter has to feed a zero flag from the most significant bit down to the least significant bit and then combine it with the output from the Johnson counter.\n\nA digital-to-analog converter (DAC) could be used to achieve sub-cycle resolution, but it is easier to either use vernier Johnson counters or traveling-wave Johnson counters.\n\nThe delay generator can be used for pulse width modulation, e.g. to drive a MOSFET to load a Pockels cell within 8 ns with a specific charge.\n\nThe output of a delay generator can gate a digital-to-analog converter and so pulses of a variable height can be generated. This allows matching to low levels needed by analog electronics, higher levels for ECL and even higher levels for TTL. If a series of DACs is gated in sequence, variable pulse shapes can be generated to account for any transfer function.\n\n\n"}
{"id": "38788309", "url": "https://en.wikipedia.org/wiki?curid=38788309", "title": "Vyapti", "text": "Vyapti\n\nVyapti, a Sanskrit expression, in Hindu philosophy refers to the state of pervasion. It is considered as the logical ground of inference which is one of the means to knowledge. No conclusion can be inferred without the knowledge of vyapti. Vyapti guarantees the truth of conclusion. It signifies the relation of invariable concomitance between \"\"hetu\" and \"sadhya\" and is of two kinds. Vyapti between terms of unequal extension is called \"asamavyavyapti\" or \"visamavyapti\", and vyapti between equal extensions is called \"samavyapti\".\n\nVyapti is a universal statement that expresses the \"niyata sahacharya\" or relation of constant concomitance between \"hetu\" or the middle term and \"sadhya\" or the major term and implies the \"sahacara\" i.e. the knowledge of invariable relation of causality or co-existence between \"sadhya\" and \"hetu\" in all the three instances of time, which is possible when the \"anupadhik sambandha\" i.e. relation of unconditionality between the two is known. It is defined as the unconditional and constant concomitant relation between \"vyapya\", the pervaded, and \"vyapaka\", the pervader. \n\nThe Charvaka school of Hindu philosophy while admitting the existence of the world and denying pre-existence rejects inference and testimony; they recognize perception as the only means to knowledge. They hold the view that the universal concomitance of the middle term with the major term can never be known since their agreement in presence and agreement in absence can never be known as also their invariable concomitance because there are no class-characters and universals. Vyapti can never be known because it does not exist. If inductive inference is proved by vyapti then these two cannot be mutually dependent.\n\nThe Nyaya school of Gautama speaks of five-membered inference or \"pararthanumana\". Knowledge of vyapti is considered by this school to be the cause of successful inference because inference depends upon the unconditional universal concomitance between the middle term and the major term, the middle term indicating the existence of the major term, and is to be found in the minor term or \"paksa\", the subject of inference. It is not possible to perceive all instances of the middle term and the major term nor can vyapti be known by internal perception. In order for the inference to be sound the major and the minor premises have to be true, the former should be secure because the latter’s truth is given by perception. They hold the view that vyapti is the unconditional uniform relation of the reason to the predicate and that a condition pervades the predicate. Faulty reasons such as inconclusive (\"savyabhicara\"), contradictory (\"viruddha\"), counterbalanced (\"prakaranasama\"), unproved (\"sadhyasama\"), and mistimed (\"atitkala\") or contradicted (\"badhita) hinder the production of a valid inference when they are known. Vyapti is known by the joint method of agreement in presence and agreement in absence based on repeated observation aided by favourable hypothetical reasoning. Doubt about vyapti and certainty of the absence of vyapti act as hindrances to inferential knowledge; the certainty about vyapti is the cause of inferential knowledge. \n\nJain philosophy recognizes inference (\"\"anumana\") as a valid means of knowledge. They consider induction (\"tarka\") to be the knowledge of the invariable concomitance (vyapti) of the middle term with the major term in the three periods of time, arising from the observation of their co-presence and co-absence, and vyapti to be of two kinds, \"anvayavyapti\" and \"vyatirekavyapti\". Wherever there is smoke, there is fire; this is \"anvayavyapti\". Wherever there is no fire, there is no smoke; this is \"vyatirekavyapti\". They hold the view that inference is based on vyapti which is derived from induction. \n\nThe followers of the Advaita Vedanta do not regard the knowledge of the existence of the probans, pervaded by the probandum, in the subject of inference as the cause of inference or the instrumental cause of inference. Vyapti is the co-existence of the probans and the probandum in all the strata of the probans and does not depend upon the agreement in absence between the probans and the probandum. Inference is \"anvayi\" and depends upon the agreement in presence between the probans and the probandum and is founded on their positive concomitance. They reject \"anvaya-vyatireki\" inference recognized by the Navya Nayaya. \n\nEven though most schools of Indian thought have proposed their own method of ascertaining vyapti, because they base the knowledge of universal propositions on the principle of causality and essential identity in order to know how cause and effect are universally related, the Buddhists adopt the method of \"pancakarani\". To the Vedantins vyapti is the result of an induction by simple enumeration. The Naiyayikas firstly look for the relation of agreement in presence between two things, and thereafter look for the uniform agreement in absence between them, then they look for contrary instances and finally eliminate all upadhi or conditions. They supplement the uncontradicted experience of the relation between two facts by \"tarka\" or indirect proof and by \"samanylakshana\"\n\nWith regard to the \"Ashta Siddhis\" that already exist in nature, the followers of Aurobindo agree that consciousness in itself is free to communicate between one mind and another without physical means consciously and voluntarily, and it does so through two siddhis, namely, \"Vyapti\" and \"Prakamya\"\". \"Vyapti\" is when feelings of others from outside are felt, and also when one sends own thoughts to others. \"Prakamya\" is when one looks mentally or physically at something and perceives what is in that thing or super-perceives via the senses \n"}
{"id": "4594861", "url": "https://en.wikipedia.org/wiki?curid=4594861", "title": "Water–cement ratio", "text": "Water–cement ratio\n\nThe water–cement ratio is the ratio of the weight of water to the weight of cement used in a concrete mix. A lower ratio leads to higher strength and durability, but may make the mix difficult to work with and form. Workability can be resolved with the use of plasticizers or super-plasticizers.\n\nOften, the ratio refers to the ratio of water to cement plus pozzolan ratio, w/(c+p). The pozzolan is typically a fly ash, or blast furnace slag. It can include a number of other materials, such as silica fume, rice husk ash or natural pozzolans. Pozzolans can be added to strengthen concrete.\n\nThe notion of water–cement ratio was first developed by Duff A. Abrams and published in 1918. Refer to concrete slump test.\nThe 1997 Uniform Building Code specifies a maximum of 0.5 ratio when concrete is exposed to freezing and thawing in a moist condition or to de-icing chemicals, and a maximum of 0.45 ratio for concrete in a severe or very severe sulfate condition.\n\nConcrete hardens as a result of the chemical reaction between cement and water (known as hydration, this produces heat and is called the heat of hydration). For every pound (or kilogram or any unit of weight) of cement, about 0.35 pounds (or 0.35 kg or corresponding unit) of water is needed to fully complete hydration reactions.\n\nHowever, a mix with a ratio of 0.35 may not mix thoroughly, and may not flow well enough to be placed. More water is therefore used than is technically necessary to react with cement. Water–cement ratios of 0.45 to 0.60 are more typically used. For higher-strength concrete, lower ratios are used, along with a plasticizer to increase flowability.\n\nToo much water will result in segregation of the sand and aggregate components from the cement paste. Also, water that is not consumed by the hydration reaction may leave concrete as it hardens, resulting in microscopic pores (bleeding) that will reduce final strength of concrete. A mix with too much water will experience more shrinkage as excess water leaves, resulting in internal cracks and visible fractures (particularly around inside corners), which again will reduce the final strength.\n"}
{"id": "1851887", "url": "https://en.wikipedia.org/wiki?curid=1851887", "title": "Weasel program", "text": "Weasel program\n\nThe weasel program or Dawkins' weasel is a thought experiment and a variety of computer simulations illustrating it. Their aim is to demonstrate that the process that drives evolutionary systems—random variation combined with non-random cumulative selection—is different from pure chance.\n\nThe thought experiment was formulated by Richard Dawkins, and the first simulation written by him; various other implementations of the program have been written by others.\n\nIn chapter 3 of his book \"The Blind Watchmaker\", Dawkins gave the following introduction to the program, referencing the well-known infinite monkey theorem:\n\nThe scenario is staged to produce a string of gibberish letters, assuming that the selection of each letter in a sequence of 28 characters will be random. The number of possible combinations in this random sequence is 27, or about 10, so the probability that the monkey will produce a given sequence is extremely low. Any particular sequence of 28 characters could be selected as a \"target\" phrase, all equally as improbable as Dawkins's chosen target, \"METHINKS IT IS LIKE A WEASEL\".\n\nA computer program could be written to carry out the actions of Dawkins's hypothetical monkey, continuously generating combinations of 26 letters and spaces at high speed. Even at the rate of millions of combinations per second, it is unlikely, even given the entire lifetime of the universe to run, that the program would ever produce the phrase \"METHINKS IT IS LIKE A WEASEL\".\n\nDawkins intends this example to illustrate a common misunderstanding of evolutionary change, i.e. that DNA sequences or organic compounds such as proteins are the result of atoms randomly combining to form more complex structures. In these types of computations, any sequence of amino acids in a protein will be extraordinarily improbable (this is known as Hoyle's fallacy). Rather, evolution proceeds by hill climbing, as in adaptive landscapes.\n\nDawkins then goes on to show that a process of \"cumulative\" selection can take far fewer steps to reach any given target. In Dawkins's words:\n\nBy repeating the procedure, a randomly generated sequence of 28 letters and spaces will be gradually changed each generation. The sequences progress through each generation:\n\nDawkins continues:\n\nThe program aims to demonstrate that the preservation of small changes in an evolving string of characters (or genes) can produce meaningful combinations in a relatively short time as long as there is some mechanism to select cumulative changes, whether it is a person identifying which traits are desirable (in the case of artificial selection) or a criterion of survival (\"fitness\") imposed by the environment (in the case of natural selection). Reproducing systems tend to preserve traits across generations, because the offspring inherit a copy of the parent's traits. It is the differences between offspring, the variations in copying, which become the basis for selection, allowing phrases closer to the target to survive, and the remaining variants to \"die.\"\n\nDawkins discusses the issue of the mechanism of selection with respect to his \"biomorphs\" program:\n\nRegarding the example's applicability to biological evolution, he is careful to point out that it has its limitations:\n\nIn \"The Blind Watchmaker,\" Dawkins goes on to provide a graphical model of gene selection involving entities he calls biomorphs. These are two-dimensional sets of line segments which bear relationships to each other, drawn under the control of \"genes\" that determine the appearance of the biomorph. By selecting entities from sequential generations of biomorphs, an experimenter can guide the evolution of the figures toward given shapes, such as \"airplane\" or \"octopus\" biomorphs.\n\nAs a simulation, the biomorphs are not much closer to the actual genetic behavior of biological organisms. Like the Weasel program, their development is shaped by an external factor, in this case the decisions of the experimenter who chooses which of many possible shapes will go forward into the following generation. They do however serve to illustrate the concept of \"genetic space,\" where each possible gene is treated as a dimension, and the actual genomes of living organisms make up a tiny fraction of all possible gene combinations, most of which will not produce a viable organism. As Dawkins puts it, \"however many ways there may be of being alive, it is certain that there are vastly more ways of being dead\". \n\nIn \"Climbing Mount Improbable\", Dawkins responded to the limitations of the Weasel program by describing programs, written by other parties, that modeled the evolution of the spider web. He suggested that these programs were more realistic models of the evolutionary process, since they had no predetermined goal other than coming up with a web that caught more flies through a \"trial and error\" process. Spiderwebs were seen as good topics for evolutionary modeling because they were simple examples of biosystems that were easily visualized; the modeling programs successfully generated a range of spider webs similar to those found in nature.\n\nAlthough Dawkins did not provide the source code for his program, a \"Weasel\" style algorithm could run as follows.\n\n\nFor these purposes, a \"character\" is any uppercase letter, or a space. The number of copies per generation, and the chance of mutation per letter are not specified in Dawkins's book; 100 copies and a 5% mutation rate are examples. Correct letters are not \"locked\". Each correct letter may become incorrect in subsequent generations. The terms of the program and the existence of the target phrase do however mean that such 'negative mutations' will quickly be 'corrected'.\n\n\n"}
{"id": "50111", "url": "https://en.wikipedia.org/wiki?curid=50111", "title": "Zebu", "text": "Zebu\n\nA zebu (; \"Bos primigenius indicus\" or \"Bos indicus\" or \"Bos taurus indicus\"), sometimes known as indicine cattle or humped cattle, is a species or subspecies of domestic cattle originating in South Asia. Zebu are characterised by a fatty hump on their shoulders, a large dewlap, and sometimes drooping ears. They are well adapted to withstanding high temperatures, and are farmed throughout the tropical countries, both as pure zebu and as hybrids with taurine cattle, the other main type of domestic cattle. Zebu are used as draught and riding animals, dairy cattle, and beef cattle, as well as for byproducts such as hides and dung for fuel and manure. Zebu, namely Miniature Zebu, are kept as companion animals. In 1999, researchers at Texas A&M University successfully cloned a zebu.\n\nThe scientific name of zebu cattle was originally \"Bos indicus\", but they are now more commonly classified within the species \"Bos taurus\" as \"B. t. indicus\", together with taurine cattle (\"B. t. taurus\") and the extinct ancestor of both of them, the aurochs (\"B. t. primigenius\"). Taurine (\"European\") cattle are descended from the Eurasian aurochs, while zebu are descended from the Indian aurochs. \"Zebu\" may be either singular or plural, but \"zebus\" is also an acceptable plural form. The Spanish name, \"cebu\" or \"cebú\", is also present in a few English works.\n\nZebu cattle are thought to be derived from Indian aurochs, sometimes regarded as a subspecies, \"B. p. namadicus.\" Wild Asian aurochs disappeared during the time of the Indus Valley Civilisation from its range in the Indus River basin and other parts of the South Asian region possibly due to interbreeding with domestic zebu and resultant fragmentation of wild populations due to loss of habitat.\n\nArchaeological evidence including pictures on pottery and rocks suggests that the species were present in Egypt around 2000 BC and were thought to be imported from the near east or south. \"Bos indicus\" is believed to have first appeared in sub-Saharan Africa between 700 and 1500 and was introduced to the Horn of Africa around 1000.\n\nSome 75 breeds of zebu are known, split about evenly between African breeds and Indian ones. The major zebu cattle breeds of the world include Gyr, Kankrej and Guzerat, Indo-Brazilian, Brahman,Cholistani, Nelore, Ongole, Sahiwal, Red Sindhi, Butana and Kenana, Boran, Baggara, Tharparkar, Kangayam, Southern Yellow, Kedah-Kelantan, and Local Indian Dairy (LID). Kedah-Kelantan and LID originated from Malaysia. Other breeds of zebu are quite local, like the Hariana of Haryana and eastern Punjab or the Rath of Alwar in eastern Rajasthan.\n\nThe African sanga cattle breeds originated from hybridization of zebu with indigenous African humpless cattle; they include the Afrikaner, Red Fulani, Ankole-Watusi, and many other breeds of central and southern Africa. Sanga cattle can be distinguished from pure zebu by having smaller humps located farther forward on the animals.\nZebu were imported to Africa over many hundreds of years, and interbred with taurine cattle there. Genetic analysis of African cattle has found higher concentrations of zebu genes all along the east coast of Africa, with especially pure cattle on the island of Madagascar, either implying that the method of dispersal was cattle transported by ship or alternatively, the zebu may have reached East Africa via the coastal route (Pakistan, Iran, Southern Arabian coast) much earlier and crossed over to Madagascar. Partial resistance to rinderpest led to another increase in the frequency of zebu in Africa.\n\nZebu, which can tolerate extreme heat, were imported into Brazil in the early 20th century and crossbred with Charolais cattle, a European taurine breed. The resulting breed, 63% Charolais and 37% zebu, is called the Canchim. It has a better meat quality than the zebu and better heat resistance than European cattle. The zebu breeds used were primarily Indo-Brazilian with some Nelore and Guzerat.\n\nMany breeds are complex mixtures of the zebu and various taurine types, and some also have yak, gaur, or banteng genes. Zebu are very common in much of Asia, including China, India, Nepal, and almost all countries in Southeast Asia. In Asia, taurine cattle are only found in the northern regions such as Japan, Korea, and Mongolia, possibly domesticated separately from the other taurine cattle originating from Europe and Africa). Other species of cattle domesticated in parts of Asia include yak, gaur, banteng, and water buffalo.\n\nHanwoo is a traditional Korean taurine–zebu hybrid breed.\n\nZebu have humps on the shoulders, large dewlaps, and droopy ears. Compared to taurine cattle, zebus are well adapted to the hot, dry environment of the tropics. Adaptations include resistance to drought and tolerance of intense heat and sunlight.\n\nZebu are generally mature enough to begin reproducing around 44 months old. This is based on the development of their bodies to withstand the strain of carrying and lactation. Early reproduction can place too much stress on the body and possibly shorten lifespans. Carrying time of the calf averages at 285 days, but varies depending on the age and nutrition of the mother. The sex of the calf may also affect the carrying time, as male calves are carried for a shorter period than females. Location, breed, body weight, and season affect the overall health of the animal and in return may also affect the carrying period.\n\nZebu are used as draught and riding animals, dairy cattle, as well as for byproducts such as hides, dung for fuel and manure, and bone for knife handles and the like. Zebu, mostly miniature zebu, are kept as pets. \n\n\"B. t. indicus\" cows commonly have low production of milk. They do not produce milk until maturation later in their lives and do not produce much, giving it solely to their calves. When \"B. t. indicus\" is crossed with \"B. t. taurus,\" production generally increases.\n\nGenetic analysis has shown that the αS1 gene is most likely to have originated in \"B. t. indicus\".\n"}
