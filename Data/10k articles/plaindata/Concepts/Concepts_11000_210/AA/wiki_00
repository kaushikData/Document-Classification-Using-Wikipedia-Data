{"id": "19111605", "url": "https://en.wikipedia.org/wiki?curid=19111605", "title": "1% rule (Internet culture)", "text": "1% rule (Internet culture)\n\nIn Internet culture, the 1% rule is a rule of thumb pertaining to participation in an internet community, stating that only 1% of the users of a website actively create new content, while the other 99% of the participants only lurk. Variants include the \"1–9–90 rule\" (sometimes \"90–9–1 principle\" or the \"89:10:1 ratio\"), which states that in a collaborative website such as a wiki, 90% of the participants of a community only view content, 9% of the participants edit content, and 1% of the participants actively create new content.\n\nSimilar rules are known in information science, such as the 80/20 rule known as the Pareto principle, that 20 percent of a group will produce 80 percent of the activity, however the activity may be defined.\n\nThe 1% rule states that the number of people who create content on the Internet represents approximately 1% of the people who view that content. For example, for every person who posts on a forum, generally about 99 other people view that forum but do not post. The term was coined by authors and bloggers Ben McConnell and Jackie Huba, although earlier references to the same concept did not use this name.\n\nThe terms \"lurk\" and \"lurking\", in reference to online activity, are used to refer to online observation without engaging others in the community, and were first used by veteran print journalist, P. Tomi Austin, circa 1990, when her presence was noticed by other users in chat rooms, who queried her reasons for not engaging in chat. There were repeated inquiries about her identity and her refusal to engage in chat. The etiquette was, apparently, to greet other users upon entry into the chat rooms/sites. At the time, (then in her 30s, surfing among users averaging in their teens and 20s) she was only identified as \"Bilbo\", she explained that she was a mature, but computer-literate, user and novice to chat, and preferred to \"lurk\", or was \"lurking\" to familiarize herself with the chat culture, etiquette, and the sites to which she had logged on. In some instances, she needed to explain her coinage of the term \"lurking\", as the term was new to the online community, but others quickly understood her meaning. To her knowledge, the terms had not been used prior to that period, and there appears to be no earlier dated reference to the coinage.\n\nA 2005 study of radical Jihadist forums found 87% of users had never posted on the forums, 13% had posted at least once, 5% had posted 50 or more times, and only 1% had posted 500 or more times. \n\nA 2014 peer-reviewed paper entitled \"The 1% Rule in Four Digital Health Social Networks: An Observational Study\" empirically examined the 1% rule in health oriented online forums. The paper concluded that the 1% rule was consistent across the four support groups, with a handful of \"Superusers\" generating the vast majority of content. A study later that year, from a separate group of researchers, replicated the 2014 van Mierlo study in an online forum for depression. Results indicated that the distribution frequency of the 1% rule fit followed Zipf's Law, which is a specific type of a power law. \n\nThe \"90–9–1\" version of this rule states that for websites where users can both create and edit content, 1% of people create content, 9% edit or modify that content, and 90% view the content without contributing.\n\nThe actual percentage is likely to vary depending upon the subject matter. For example, if a forum requires content submissions as a condition of entry, the percentage of people who participate will probably be significantly higher than one percent, but the content producers will still be a minority of users. This is validated in a study conducted by Michael Wu, who uses economics techniques to analyze the participation inequality across hundreds of communities segmented by industry, audience type, and community focus.\n\nThe 1% rule is often misunderstood to apply to the Internet in general, but it applies more specifically to any given Internet community. It is for this reason that one can see evidence for the 1% principle on many websites, but aggregated together one can see a different distribution. This latter distribution is still unknown and likely to shift, but various researchers and pundits have speculated on how to characterize the sum total of participation. Research in late 2012 suggested that only 23% of the population (rather than 90 percent) could properly be classified as lurkers, while 17% of the population could be classified as intense contributors of content. Several years prior, results were reported on a sample of students from Chicago where 60 percent of the sample created content in some form.\n\nA similar concept was introduced by Will Hill of AT&T Laboratories and later cited by Jakob Nielsen; this was the earliest known reference to the term \"participation inequality\" in an online context. The term regained public attention in 2006 when it was used in a strictly quantitative context within a blog entry on the topic of marketing.\n\n\n"}
{"id": "295850", "url": "https://en.wikipedia.org/wiki?curid=295850", "title": "All Species Foundation", "text": "All Species Foundation\n\nThe All Species Foundation aimed to catalog all species on Earth by 2025. It began in 2001 as a spinoff of the Long Now Foundation. \n\nThe Foundation started with a large grant from the Schlinger Foundation but because of the stock market crash of 2000, at least in part, it was unable to attract appreciable additional funding. \n\nThe All Species Foundation received some criticism over the goal of identifying literally \"all\" species on earth. The criticism was that in reality species often have indistinct boundaries so that it is often not possible to objectively decide when there is a single species or multiple species.\n\n\n"}
{"id": "22433972", "url": "https://en.wikipedia.org/wiki?curid=22433972", "title": "Antahkarana", "text": "Antahkarana\n\nIn Hindu philosophy, the antahkarana (Sanskrit: \"the inner cause\") refers to the totality of two levels of mind, namely the buddhi, the intellect or higher mind, and the manas, the middle levels of mind which (according to theosophy) exist as or include the mental body. Antahkarana has also been called the link between the middle and higher mind, the reincarnating part of the mind.\n\nIn Vedāntic literature, this (\"internal organ\") is organised into four parts:\n\nAnother description says that \"antahkarana\" refers to the entire psychological process, including mind and emotions, are composing the mind levels, as described above, which are mentioned as a unit that functions with all parts working together as a whole. Furthermore, when considering that mind levels are bodies, they are: \"manomayakosha\" - related to manas - the part of mind related to five senses, and also craving for new and pleasant sensations and emotions, while buddhi (intellect, intelligence, capacity to reason), is related to vijnanamayakosha - the body of consciousness, knowledge, intuition and experience.\n\nAntahkarana also refers to a symbol used in the Reiki healing method.\n\n"}
{"id": "31576655", "url": "https://en.wikipedia.org/wiki?curid=31576655", "title": "Complex equality", "text": "Complex equality\n\nComplex equality is a theory of justice outlined by Michael Walzer in his 1983 work \"Spheres of Justice\". The theory posits that inequalities in the several spheres of society should not invade one another. Walzer's definition of complex equality is: \"In formal terms, complex equality means that no citizen's standing in one sphere or with regard to one social good can be undercut by his standing in some other sphere, with regard to some other good.\"\n\n"}
{"id": "44006007", "url": "https://en.wikipedia.org/wiki?curid=44006007", "title": "Cosmology episode", "text": "Cosmology episode\n\nA cosmology episode is a sudden loss of meaning, followed eventually by a transformative pivot, which creates the conditions for revised meaning.\n\nIn the wake of the 1962 Cuban Missile Crisis, the Vietnam War, the 1977 Tenerife airport disaster, the 1984 Bhopal chemical disaster, and the relatively sudden insertion of personal computers into the workplace, organizational scholar Karl E. Weick coined the term \"cosmology episode,\" as follows, in 1985:\n\n\"Representations of events normally hang together sensibly within the set of assumptions that give them life and constitute a 'cosmos' rather than its opposite, a 'chaos.' Sudden losses of meaning that can occur when an event is represented electronically in an incomplete, cryptic form are what I call a 'cosmology episode.' Representations in the electronic world can become chaotic for at least two reasons: The data in these representations are flawed, and the people who manage those flawed data have limited processing capacity. These two problems interact in a potentially deadly vicious circle.\"\n\nThe concept of cosmology episodes evolved significantly between 1985 and 1993, when Weick published his now-classic reanalysis of Norman Maclean's study of the Mann Gulch wildland firefighting disaster in 1949. In the 1993 article, Weick positions cosmology episodes within a constructivist ontology, he links the term to a variety of similar concepts, and he provides a better-developed definition than he was able to provide in 1985.\n\nFirst, Weick makes it clear that cosmology episodes occur within a constructivist ontology of the world, rather than the more familiar objectivist and subjectivist ontologies:\n\n\"The basic idea of sensemaking is that reality is an ongoing accomplishment that emerges from efforts to create order and make retrospective sense of what occurs... Sensemaking emphasizes that people try to make things rationally accountable to themselves and others. Thus, in the words of Morgan, Frost, and Pondy (1983: 24), \"individuals are not seen as living in, and acting out their lives in relations \"to\", a wider reality, so much as creating and sustaining images of a wider reality, in part to rationalize what they are doing. They realize their reality by 'reading into' their situation patterns of significant meaning.\"\n\nSecond, Weick clarifies the key phrase \"sudden loss of meaning\" by linking it to related ideas described by other organizational scholars:\n\n\"Minimal organizations, such as we find in the crew at Mann Gulch, are susceptible to sudden losses of meaning, which have been variably described as fundamental surprises (Reason, 1990) or as events that are inconceivable (Lanir, 1989), hidden (Westrum, 1982), or incomprehensible (Perrow, 1984). Each of these labels points to the low probability that the event could occur, which is why it is meaningless. But these explanations say less about the astonishment of the perceiver, and even less about the perceiver's inability to rebuild some sense of what is happening.\"\n\nThird, Weick expands his 1985 definition -- \"sudden losses of meaning\"—to a more nuanced description:\n\n\"Cosmology refers to a branch of philosophy often subsumed under metaphysics that combines rational speculation and scientific evidence to understand the universe as a totality of phenomena. Cosmology is the ultimate macro perspective, directed at issues of time, space, change, and contingency as they relate to the origin and structure of the universe. Integrations of these issues, however, are not just the handiwork of philosophers. Others also must make their peace with these issues, as reflected in what they take for granted. People, including those who are smokejumpers, act as if events cohere in time and space and that change unfolds in an orderly manner. These everyday cosmologies are subject to disruption. And when they are severely disrupted, I call this a cosmology episode (Weick, 1985: 51-52). A cosmology episode occurs when people suddenly and deeply feel that the universe is no longer a rational, orderly system.\"\n\nBuilding on Weick's 1993 definition of cosmology episodes, Weick, his colleagues, and other researchers have advanced knowledge of best practices during cosmology episodes at four levels of analysis: (1) catastrophic cosmology episodes, such as the 9/11 terrorist attacks of 2001 and the Haitian earthquake of 2010; (2) disastrous cosmology episodes, such as the 2004 Indian Ocean tsunami and the 2005 Hurricane Katrina; (3) contextualized cosmology episodes occurring within specific domains, such as the 1976 Three Mile Island nuclear system incident and the 1970 Apollo 13 space system incident; and (4) everyday cosmology episodes, such as transition of leaders at the top of an organization and the introduction of new technologies throughout an organization.\n\nThe study of cosmology episodes is distinct from the study of high-reliability organizations because the concept directs explicit attention to the integral role of human spirituality during catastrophic events—as the threatened entity (what now is my role in the universe?), the source of inspiration/improvisation (what now am I able to do to respond to the event?), and the re-established entity (what now is my different role in the universe?). Consequently, the topic of cosmology episodes lends itself to study by researchers within two small academic organizations—the American Psychological Association's Division 36 (Psychology of Religion and Spirituality) and the Academy of Management's MSR Division (Management, Spirituality, and Religion).\n\nA 2016 article titled \"Cosmology Episodes: A Reconceptualization,\" Doug Orton and Kari O'Grady identified three taxonomies that they found helpful in studying 164 citations of the term \"cosmology episode\" on Google Scholar. One taxonomy was based on level of analysis (nation, community, organization, team, and individual). One taxonomy was based on five resilience processes (anticipating, sense-losing, improvising, sense-remaking, and renewing). The third taxonomy -- of interest here -- is level of intensity of the cosmology episode (catastrophe, disaster, crisis, ancillary, and metaphorical). Communications researchers tend to be more present in the study of community disasters; management researchers tend to be more present in the study of organizational crises. Importantly, though, it is possible for a community-level cosmology episode to be catastrophic (e.g. Hurrican Katrina), disastrous (e.g the Red River floods of 1997), a crisis (e.g. the 22 July 2005 Stockwell Subway Shooting), ancillary (e.g. an e coli epidemic for a food company), and metaphorical (e.g. the introduction of new technology in an Italian courtroom).\n"}
{"id": "4176386", "url": "https://en.wikipedia.org/wiki?curid=4176386", "title": "Cotillion", "text": "Cotillion\n\nThe cotillion (also cotillon or \"French country dance\") is a social dance, popular in 18th-century Europe and America. Originally for four couples in square formation, it was a courtly version of an \"English country dance\", the forerunner of the \"quadrille\" and, in the United States, the square dance. \n\nIt was for some fifty years regarded as an ideal finale to a ball but was eclipsed in the early 19th century by the \"quadrille\". It became so elaborate that it was sometimes presented as a concert dance performed by trained and rehearsed dancers. The later \"German\" cotillion included more couples as well as plays and games.\n\nThe name \"cotillion\" (French: \"petticoat\") appears to have been in use as a dance-name at the beginning of the 18th century but, though it was only ever identified as a sort of country dance, it is impossible to say of what it consisted at that early date. \n\nAs we first encounter it, it consists of a main \"figure\" that varied from dance to dance and was interspersed with \"changes\" – a number of different figures that broke out of the square formation, often decided spontaneously by the leading couple or by a caller or \"conductor\". Each of these was designed to fit a tune of eight or occasionally sixteen measures of 2/4 time. Participants exchanged partners within the formation network of the dance. \"Changes\" included the \"Great Ring\", a simple circle dance with which the dance often began, as well as smaller Ladies' and Gentlemen's rings, top and bottom and sides rings, and chains. Other changes included the \"allemande\", \"promenade\" and \"moulinet\". A complete dance composed of a prescribed order of these was called a \"set\". \n\nThe \"cotillion\" was introduced into England by 1766 and to America in about 1772. In England from that time onwards there are a large number of references stressing its universal popularity in the best and highest circles of society, and many teaching manuals were published to help recall the vast number of changes that were invented. There is a reference in Robert Burns's 1790 poem \"Tam o' Shanter\" to the \"cotillion brent-new frae France\" (brand new from France). \n\nDancing masters differed as to the exact way of doing these dances: some, recognising the affair as an English country dance, taught that the steps and jumps of these were appropriate, while others insisted upon French elegance, recommending the basic step of the gavotte or the minuet. In reality many participants simply walked through the figure and changes, seeing these as the dance and the exact steps as dispensable. On the other hand, some figures required high skill at social dancing and many performances took place at which the majority preferred to watch rather than dance.\n\nThe \"quadrille\" gained fame a few years later as a variety of cotillion that could be danced by only two couples. In London in 1786 Longman & Broderip's \"6th book of Twenty Four New Cotillions\" brings together for the first time the most characteristic dance-figures of the \"quadrille; Les Pantalons, L'Eté, La Belle Poule\" and \"La Pastorale\". However, while the \"cotillion\" kept all the dancers in almost perpetual motion, the quadrille often allowed rest to half of the participants while the other half danced.\n\nIn the 1790s the \"cotillion\" was falling from favour, but it re-emerged in a new style in the early years of the next century, with fewer and fewer changes, making it barely distinguishable from the newly-emerging \"quadrille\", which was introduced into English high society by Lady Jersey in 1816 and by 1820 had eclipsed the \"cotillion\", though it was recognisably a very similar dance, particularly as it also began to be danced by four couples. References to the English Cotillion dances persist here and there until the 1840s, but these were more games than fashionable dances, and were often danced to the waltz or the mazurka.\n\nIn the United States, however, the opposite was true: quadrilles were termed cotillions until the 1840s, when it was realised that all the distinctive figures of the earlier dance had been taken up into the newer. The German cotillion was introduced to New York society at a costume ball with a Louis XV theme given by Mr. William Colford Schermerhorn in the early winter of 1854. Here, too, waltzes, mazurkas, fun, games and boisterous behaviour at private parties took on a more important role, and only some figures of the earlier dances survived. Finally the term cotillion was used to refer to the ball itself and the cotillion and quadrille became the square dance.\n"}
{"id": "265752", "url": "https://en.wikipedia.org/wiki?curid=265752", "title": "Decision-making", "text": "Decision-making\n\nIn psychology, decision-making (also spelled decision making and decisionmaking) is regarded as the cognitive process resulting in the selection of a belief or a course of action among several alternative possibilities. Every decision-making process produces a final choice, which may or may not prompt action.\n\nDecision-making is the process of identifying and choosing alternatives based on the values, preferences and beliefs of the decision-maker.\n\nDecision-making can be regarded as a problem-solving activity terminated by a solution deemed to be optimal, or at least satisfactory. It is therefore a process which can be more or less rational or irrational and can be based on explicit or tacit knowledge and beliefs. Tacit knowledge can be obtained by experience or reflection, for instance. It might be something, that you are not able to put in words, as opposed to explicit knowledge. Tacit knowledge is often used to fill the gaps in complex decision making processes. Usually both of these types of knowledge, tacit and explicit, are used in decision-making process together. Explicit knowledge is less likely to result in major decisions than tacit knowledge, which means that the decision-making process usually relies on knowledge acquired through experience.\n\nHuman performance has been the subject of active research from several perspectives:\n\nA major part of decision-making involves the analysis of a finite set of alternatives described in terms of evaluative criteria. Then the task might be to rank these alternatives in terms of how attractive they are to the decision-maker(s) when all the criteria are considered simultaneously. Another task might be to find the best alternative or to determine the relative total priority of each alternative (for instance, if alternatives represent projects competing for funds) when all the criteria are considered simultaneously. Solving such problems is the focus of multiple-criteria decision analysis (MCDA). This area of decision-making, although very old, has attracted the interest of many researchers and practitioners and is still highly debated as there are many MCDA methods which may yield very different results when they are applied on exactly the same data. This leads to the formulation of a decision-making paradox.\n\nLogical decision-making is an important part of all science-based professions, where specialists apply their knowledge in a given area to make informed decisions. For example, medical decision-making often involves a diagnosis and the selection of appropriate treatment. But naturalistic decision-making research shows that in situations with higher time pressure, higher stakes, or increased ambiguities, experts may use intuitive decision-making rather than structured approaches. They may follow a recognition primed decision that fits their experience and arrive at a course of action without weighing alternatives.\n\nThe decision-maker's environment can play a part in the decision-making process. For example, environmental complexity is a factor that influences cognitive function. A complex environment is an environment with a large number of different possible states which come and go over time. Studies done at the University of Colorado have shown that more complex environments correlate with higher cognitive function, which means that a decision can be influenced by the location. One experiment measured complexity in a room by the number of small objects and appliances present; a simple room had less of those things. Cognitive function was greatly affected by the higher measure of environmental complexity making it easier to think about the situation and make a better decision.\n\nResearch about decision-making is also published under the label problem solving, in particular in European psychological research.\n\nIt is important to differentiate between problem analysis and decision-making. Traditionally, it is argued that problem analysis must be done first, so that the information gathered in that process may be used towards decision-making.\n\n\n\nAnalysis paralysis is the state of over-analyzing (or over-thinking) a situation so that a decision or action is never taken, in effect paralyzing the outcome.\n\nInformation overload is \"a gap between the volume of information and the tools we have to assimilate\" it. Information used in decision making is to reduce or eliminate uncertainty. Excessive information affects problem processing and tasking, which affects decision-making. Crystal C. Hall and colleagues described an \"illusion of knowledge\", which means that as individuals encounter too much knowledge it can interfere with their ability to make rational decisions.\n\nEvaluation and analysis of past decisions is complementary to decision-making. See also Mental accounting and Postmortem documentation.\n\nDecision-making techniques can be separated into two broad categories: group decision-making techniques and individual decision-making techniques. Individual decision-making techniques can also often be applied by a group.\n\n\n\nA variety of researchers have formulated similar prescriptive steps aimed at improving decision-making.\n\nIn the 1980s, psychologist Leon Mann and colleagues developed a decision-making process called GOFER, which they taught to adolescents, as summarized in the book \"Teaching Decision Making To Adolescents\". The process was based on extensive earlier research conducted with psychologist Irving Janis. GOFER is an acronym for five decision-making steps:\n\n\nIn 2008, Kristina Guo published the DECIDE model of decision-making, which has six parts:\n\nIn 2007, Pam Brown of Singleton Hospital in Swansea, Wales, divided the decision-making process into seven steps:\n\n\nIn 2009, professor John Pijanowski described how the Arkansas Program, an ethics curriculum at the University of Arkansas, used eight stages of moral decision-making based on the work of James Rest:\n\nAccording to B. Aubrey Fisher, there are four stages or phases that should be involved in all group decision-making:\n\nIt is said that establishing critical norms in a group improves the quality of decisions, while the majority of opinions (called consensus norms) do not.\n\nConflicts in socialization are divided in to functional and dysfunctional types. Functional conflicts are mostly the questioning the managers assumptions in their decision making and dysfunctional conflicts are like personal attacks and every action which decrease team effectiveness. Functional conflicts are the better ones to gain higher quality decision making caused by the increased team knowledge and shared understanding.\n\nIn economics, it is thought that if humans are rational and free to make their own decisions, then they would behave according to rational choice theory. Rational choice theory says that a person consistently makes choices that lead to the best situation for himself or herself, taking into account all available considerations including costs and benefits; the rationality of these considerations is from the point of view of the person himself, so a decision is not irrational just because someone else finds it questionable.\n\nIn reality, however, there are some factors that affect decision-making abilities and cause people to make irrational decisionsfor example, to make contradictory choices when faced with the same problem framed in two different ways (see also Allais paradox).\n\nOne of the most prominent theories of decision making is subjective expected utility (SEU) theory, which describes the rational behavior of the decision maker. The decision maker assesses different alternatives by their utilities and the subjective probability of occurrence.\n\nRational decision-making is often grounded on experience and theories exist that are able to put this approach on solid mathematical grounds so that subjectivity is reduced to a minimum, see e.g. scenario optimization.\n\nBiases usually affect decision-making processes. They appear more when decision task has time pressure, is done under high stress and/or task is highly complex. \n\nHere is a list of commonly debated biases in judgment and decision-making:\n\n\nIn groups, people generate decisions through active and complex processes. One method consists of three steps: initial preferences are expressed by members; the members of the group then gather and share information concerning those preferences; finally, the members combine their views and make a single choice about how to face the problem. Although these steps are relatively ordinary, judgements are often distorted by cognitive and motivational biases, include \"sins of commission\", \"sins of omission\", and \"sins of imprecision\".\n\nHerbert A. Simon coined the phrase \"bounded rationality\" to express the idea that human decision-making is limited by available information, available time and the mind's information-processing ability. Further psychological research has identified individual differences between two cognitive styles: \"maximizers\" try to make an optimal decision, whereas \"satisficers\" simply try to find a solution that is \"good enough\". Maximizers tend to take longer making decisions due to the need to maximize performance across all variables and make tradeoffs carefully; they also tend to more often regret their decisions (perhaps because they are more able than satisficers to recognise that a decision turned out to be sub-optimal).\n\nThe psychologist Daniel Kahneman, adopting terms originally proposed by the psychologists Keith Stanovich and Richard West, has theorized that a person's decision-making is the result of an interplay between two kinds of cognitive processes: an automatic intuitive system (called \"System 1\") and an effortful rational system (called \"System 2\"). System 1 is a bottom-up, fast, and implicit system of decision-making, while system 2 is a top-down, slow, and explicit system of decision-making. System 1 includes simple heuristics in judgment and decision-making such as the affect heuristic, the availability heuristic, the familiarity heuristic, and the representativeness heuristic.\n\nStyles and methods of decision-making were elaborated by Aron Katsenelinboigen, the founder of predispositioning theory. In his analysis on styles and methods, Katsenelinboigen referred to the game of chess, saying that \"chess does disclose various methods of operation, notably the creation of predisposition-methods which may be applicable to other, more complex systems.\"\n\nKatsenelinboigen states that apart from the methods (reactive and selective) and sub-methods (randomization, predispositioning, programming), there are two major styles: positional and combinational. Both styles are utilized in the game of chess. According to Katsenelinboigen, the two styles reflect two basic approaches to uncertainty: deterministic (combinational style) and indeterministic (positional style). Katsenelinboigen's definition of the two styles are the following.\n\nThe combinational style is characterized by:\n\nIn defining the combinational style in chess, Katsenelinboigen wrote: \"The combinational style features a clearly formulated limited objective, namely the capture of material (the main constituent element of a chess position). The objective is implemented via a well-defined, and in some cases, unique sequence of moves aimed at reaching the set goal. As a rule, this sequence leaves no options for the opponent. Finding a combinational objective allows the player to focus all his energies on efficient execution, that is, the player's analysis may be limited to the pieces directly partaking in the combination. This approach is the crux of the combination and the combinational style of play.\n\nThe positional style is distinguished by:\n\n\"Unlike the combinational player, the positional player is occupied, first and foremost, with the elaboration of the position that will allow him to develop in the unknown future. In playing the positional style, the player must evaluate relational and material parameters as independent variables. ... The positional style gives the player the opportunity to develop a position until it becomes pregnant with a combination. However, the combination is not the final goal of the positional playerit helps him to achieve the desirable, keeping in mind a predisposition for the future development. The pyrrhic victory is the best example of one's inability to think positionally.\"\n\nThe positional style serves to:\n\nAccording to Isabel Briggs Myers, a person's decision-making process depends to a significant degree on their cognitive style. Myers developed a set of four bi-polar dimensions, called the Myers-Briggs Type Indicator (MBTI). The terminal points on these dimensions are: \"thinking\" and \"feeling\"; \"extroversion\" and \"introversion\"; \"judgment\" and \"perception\"; and \"sensing\" and \"intuition\". She claimed that a person's decision-making style correlates well with how they score on these four dimensions. For example, someone who scored near the thinking, extroversion, sensing, and judgment ends of the dimensions would tend to have a logical, analytical, objective, critical, and empirical decision-making style. However, some psychologists say that the MBTI lacks reliability and validity and is poorly constructed.\n\nOther studies suggest that these national or cross-cultural differences in decision-making exist across entire societies. For example, Maris Martinsons has found that American, Japanese and Chinese business leaders each exhibit a distinctive national style of decision-making.\n\nDecision-making is a region of intense study in the fields of systems neuroscience, and cognitive neuroscience. Several brain structures, including the anterior cingulate cortex (ACC), orbitofrontal cortex and the overlapping ventromedial prefrontal cortex are believed to be involved in decision-making processes. A neuroimaging study found distinctive patterns of neural activation in these regions depending on whether decisions were made on the basis of perceived personal volition or following directions from someone else. Patients with damage to the ventromedial prefrontal cortex have difficulty making advantageous decisions.\n\nA common laboratory paradigm for studying neural decision-making is the two-alternative forced choice task (2AFC), in which a subject has to choose between two alternatives within a certain time. A study of a two-alternative forced choice task involving rhesus monkeys found that neurons in the parietal cortex not only represent the formation of a decision but also signal the degree of certainty (or \"confidence\") associated with the decision. Another recent study found that lesions to the ACC in the macaque resulted in impaired decision-making in the long run of reinforcement guided tasks suggesting that the ACC may be involved in evaluating past reinforcement information and guiding future action. A 2012 study found that rats and humans can optimally accumulate incoming sensory evidence, to make statistically optimal decisions.\n\nEmotion appears able to aid the decision-making process. Decision-making often occurs in the face of uncertainty about whether one's choices will lead to benefit or harm (see also Risk). The somatic marker hypothesis is a neurobiological theory of how decisions are made in the face of uncertain outcome. This theory holds that such decisions are aided by emotions, in the form of bodily states, that are elicited during the deliberation of future consequences and that mark different options for behavior as being advantageous or disadvantageous. This process involves an interplay between neural systems that elicit emotional/bodily states and neural systems that map these emotional/bodily states. A recent lesion mapping study of 152 patients with focal brain lesions conducted by Aron K. Barbey and colleagues provided evidence to help discover the neural mechanisms of emotional intelligence.\n\nDuring their adolescent years, teens are known for their high-risk behaviors and rash decisions. Recent research has shown that there are differences in cognitive processes between adolescents and adults during decision-making. Researchers have concluded that differences in decision-making are not due to a lack of logic or reasoning, but more due to the immaturity of psychosocial capacities that influence decision-making. Examples of their undeveloped capacities which influence decision-making would be impulse control, emotion regulation, delayed gratification and resistance to peer pressure. In the past, researchers have thought that adolescent behavior was simply due to incompetency regarding decision-making. Currently, researchers have concluded that adults and adolescents are both competent decision-makers, not just adults. However, adolescents' competent decision-making skills decrease when psychosocial capacities become present.\n\nRecent research has shown that risk-taking behaviors in adolescents may be the product of interactions between the socioemotional brain network and its cognitive-control network. The socioemotional part of the brain processes social and emotional stimuli and has been shown to be important in reward processing. The cognitive-control network assists in planning and self-regulation. Both of these sections of the brain change over the course of puberty. However, the socioemotional network changes quickly and abruptly, while the cognitive-control network changes more gradually. Because of this difference in change, the cognitive-control network, which usually regulates the socioemotional network, struggles to control the socioemotional network when psychosocial capacities are present.\n\nWhen adolescents are exposed to social and emotional stimuli, their socioemotional network is activated as well as areas of the brain involved in reward processing. Because teens often gain a sense of reward from risk-taking behaviors, their repetition becomes ever more probable due to the reward experienced. In this, the process mirrors addiction. Teens can become addicted to risky behavior because they are in a high state of arousal and are rewarded for it not only by their own internal functions but also by their peers around them.\n\nAdults are generally better able to control their risk-taking because their cognitive-control system has matured enough to the point where it can control the socioemotional network, even in the context of high arousal or when psychosocial capacities are present. Also, adults are less likely to find themselves in situations that push them to do risky things. For example, teens are more likely to be around peers who peer pressure them into doing things, while adults are not as exposed to this sort of social setting.\n\nA recent study suggests that adolescents have difficulties adequately adjusting beliefs in response to bad news (such as reading that smoking poses a greater risk to health than they thought), but do not differ from adults in their ability to alter beliefs in response to good news. This creates biased beliefs, which may lead to greater risk taking.\n"}
{"id": "48917069", "url": "https://en.wikipedia.org/wiki?curid=48917069", "title": "Edith Irby Jones", "text": "Edith Irby Jones\n\nEdith Irby Jones (born December 23, 1927) is an American physician who was the first African American to be accepted as a non-segregated student at the University of Arkansas Medical School and the first black student to attend racially mixed classes in the American South. She was the first African American to graduate from a southern medical school, first black intern in the state of Arkansas, and later first black intern at Baylor College of Medicine. Jones was the first woman president of the National Medical Association. She has been honored by many awards, including induction into both the University of Arkansas College of Medicine Hall of Fame and the inaugural group of women inducted into the Arkansas Women's Hall of Fame.\n\nEdith Mae Irby was born on December 23, 1927, near Conway in Faulkner County, Arkansas, to Mattie (née Buice) and Robert Irby. At the age of eight, she lost her father, an older sister died at 12 years of age from typhoid fever, and Irby herself suffered from rheumatic fever as a child. These were motivating factors in her desire to help those who were underserved and impoverished and which propelled her toward a career in medicine. Her mother relocated the family to Hot Springs, where Irby graduated from Langston Secondary School in 1944. After winning a scholarship, she studied chemistry, biology and physics at Knoxville College in Knoxville, Tennessee. Irby was well aware of the role she was playing and obligation she had for the black community. One of her teachers had helped her attain the scholarship, members of the local African American community collected change and the black press ran a campaign in the \"Arkansas State-Press\" which they donated to help with her tuition and living expenses. During her schooling, she secretly made trips with teams of workers from the NAACP to enroll members for the organization. She graduated with her BS from Knoxville College in 1948 and then completed a graduate course at Northwestern University in Evanston, Illinois to prepare for Medical School.\n\nUpon her graduation, Jones returned to Hot Springs and practiced medicine there for six years. When tension over the Little Rock Nine polarized Arkansas, and newspapers began to spotlight her again, in 1959, they moved to Houston, Texas, where she was accepted as the first black woman intern at the Baylor College of Medicine Affiliated Hospitals. The segregated staff at the hospital and limited patient rosters in Texas, caused her to finish her last three months of residency at Freedman's Hospital in Washington, D.C. In 1962, she founded her private practice in Houston's \"third ward\", part of the inner city of Houston, to help those who could not access care elsewhere. That same year, she became the chief of cardiology at St. Elizabeth’s Hospital in Houston. She also became an associate chief of medicine at Riverside General Hospital. In 1963, she accepted a post as a Clinical Assistant Professor at Baylor College of Medicine. Continuing her education, Jones completed graduate courses at the West Virginia College of Medicine in 1965 and the Cook County Graduate School of Medicine in Chicago in 1966. In 1969, she was honored by the Houston Chapter of \"Theta Sigma Phi\" professional women with the Matrix Award for Medicine.\n\nIn 1964, Jones was elected to serve as second vice president of the National Medical Association (NMA). In 1975, she became the first woman to chair the Council on Scientific Assembly for the NMA and then a decade later, she was elected as the first woman president of the organization. In 1986, \"Edith Irby Jones Day\" was proclaimed by the City of Houston and in 1988 she was named Internist of the Year by the American Society of Internal Medicine. She was one of the founders of Mercy Hospital in Houston and one of the 12 physician owners and developers of the Park Plaza Hospital. Jones also supervises residents at the University of Texas Health Science Center and is active on the board of Planned Parenthood and the Houston School Board.\n\nThroughout her career, Jones has received many awards and honors for both her professional and volunteer work, including Honorary Doctorates from Missouri Valley College (1988), Mary Holmes College (1989), Lindenwood College (1991), and Knoxville College (1992); Southeast Memorial Hospital renamed its ambulatory center in her honor (1998); recipient of the 2001 Oscar E. Edwards Memorial Award for Volunteerism and Community Service from the American College of Physicians; inducted into the University of Arkansas College of Medicine Hall of Fame (2004); US Congresswoman Sheila Jackson Lee nominated Jones as a Local Legend for the National Library of Medicine; induction into the inaugural class of women honored by the Arkansas Women's Hall of Fame in 2015; and received a commendation from the Texas House of Representatives for her service in 2015. Two international hospitals also are named in her honor: Dr. Edith Irby Jones Clinic in Vaudreuil, Haiti, which she helped found in 1991, and the Dr. Edith Irby Jones Emergency Clinic in Veracruz, Mexico.\n\n"}
{"id": "26091185", "url": "https://en.wikipedia.org/wiki?curid=26091185", "title": "Egyptian Organization for Human Rights", "text": "Egyptian Organization for Human Rights\n\nThe Egyptian Organization for Human Rights (EOHR), founded in April 1985 and with its headquarters in Cairo, Egypt, is a non-profit NGO and one of the longest-standing bodies for the defense of human rights in Egypt. It investigates, monitors, and reports on human rights violations and defends people's rights regardless the identity, gender or color of the victim. EOHR faces any human rights violations made either by governmental or non-governmental parties. It is registered with the United Nations and works with other human rights groups.\n\nThe Egyptian Organization for Human Rights (EOHR) was founded in 1985 by Saad Eddin Ibrahim and Hani Shukrallah. It was the first human rights organization in the country and remains one of the most professional non-governmental organizations (NGOs) in Egypt. Its headquarters is in Cairo, and has regional branches with a national membership of approximately 2,300 active volunteers in 17 provincial branches located across the country. EOHR is a non-profit NGO working within the framework of the principles established in the Universal Declaration of Human Rights and other international human rights instruments regardless of the identity or affiliation of the victim or violator .\n\nEOHR acts and reports on both governmental and non-governmental human rights violations. According to its reports issues annually the decade of the late 1980s through to the late 1990s was marked by an increase in torture in Egypt. During this period the organization's activists were regularly harassed and arrested making their work increasingly difficult. Despite these challenges the organization maintained a steady accounting of state and non-state violations of human rights and started a series of publications of human rights books to raise awareness of human rights issues in the country. An attempt to start a theater company to tour villages with plays improvised on human rights topics was scrapped after the organizer was arrested by state security forces. During the last decade of the Mubarak dictatorship the EOHR was represented, while maintaining its independence, on the state sponsored Supreme Council of Human Rights.\n\nEOHR is part of the wider international and Arab human rights movement. It cooperates with the United Nations human rights bodies, as well as with other international and regional human rights organizations. EOHR was registered at the Ministry of Social Affairs in 2003 under registration No. 5220/2003.\n\nEOHR was granted special consultative status with the United Nations Economic and Social Council in 2006. This consultative status enables EOHR to enjoy closer interaction with the United Nations by participating in the activities of the International Council for Human Rights, according to ECOSOC decision 31/1996. This decision aimed to reinforce the principles of the human rights stipulated in the Universal Declaration of Human Rights, the Vienna Declaration and all other international human rights documents. EOHR is also a member of five international organizations: the Arab Organization for Human Rights (AOHR), the World Organisation Against Torture (OMCT), International Federation for Human Rights (IFHR), the International Commission of Jurists (ICJ), and the International Freedom of Expression Exchange (IFEX).\n\n\nEOHR adopts peaceful methods to promote and defend human rights. It believes that the promotion of human rights is a common goal for the entire world community and it is determined to spare no peaceful effort in its struggle against human rights violations. His main actions are:\n\nThe EOHR accomplishes this by fact-finding missions. These include visits to prisons in order to collect testimonies, and to obtain document information about human rights violations. It also Issues urgent appeals, press releases, reports and publications on human rights violations. Furthermore, the EOHR tries to raise awareness on human rights issues among individuals and groups through publications, conferences, seminars and studies. In addition, this organization contacts Egyptian governmental and non-governmental bodies as well as international, regional and national organizations to seek co-operation, obtain information and take action to increase respect for human rights. Through the use of these methods the EOHR has adopted a peaceful method to promote and defend human rights in Egypt.\n\nThe General Assembly of fee-paying members holds supreme authority in EOHR. A General Assembly meeting is convened once every other year to review and evaluate the organization’s progress and to elect the Board of Trustees. The Board of Trustees is the policy-making body within the EOHR and is led by the Chairman. It is made up of fifteen elected members, and up to four other prominent figures in the field of human rights can be invited by the Board of Trustees to join.\n\nThe Executive Board is elected by the Board of Trustees and consists of the Secretary-General, the Treasurer and representatives of the main committees of the organization. The Executive Board, led by the Secretary-General, is responsible for the day-to-day activities of the EOHR and for any decisions made in the interim period between meetings of the Board of Trustees.\n\nEOHR has five substantive and inter-linked units, two projects and three auxiliary units:\n\nThe unit includes a number of lawyers who receive complaints on the human rights violations and do the required investigations in order to make sure that the complaint is serious. They receive individual allegations of human rights violations and investigate each case’s merits. Much of their work takes place in the field, on the ground, in prisons, in court, in detention centers, in homes, anywhere information can be found. The investigating lawyer writes an official report on each case. Then, depending on the nature and the merits of the case, the case may be removed from consideration or investigated further. Appeals and requests for information may be made to competent authorities and complainants may receive free legal counsel so that they can pursue a formal suit. All services made available to victims of human rights violations are absolutely free of charge.The unit does also the following:\nThe unit includes a number of lawyers located at the headquarters of EOHR and others in the governorates to do the following: \n\nThis unit conducts academic research on human rights issues, analyzes the raw material of human rights abuses supplied by the Field Work Unit and the WLAP. This desk is responsible for producing EOHR’s reports and is our routine link to the Arab media. The unit handles the following:\nThis unit coordinates EOHR’s partnerships with other human rights organizations throughout the non-Arab world. This desk also coordinates our endless battle to equip us with the financial resources necessary to meet our objectives.\nMoreover, it is responsible for translating all EOHR’s literature into English so that it can be shared with the English-speaking world. Thus, the unit includes a number of translators work on the following:\n\nThe Training Unit\n\nThe unit includes a number of certified specialized trainers doing the following:\n\nThe Women’s Legal Aid Project (WLAP)\n\nThis project manages human rights problems dealing with women’s issues. Such problems include marital and family issues and sexual discrimination. WLAP undertakes most of the work supporting campaigns that focus on women’s issues and coordinates an educational and training program aimed at increasing women’s legal literacy in Egypt’s poorer areas.\n\nThis project was established in 2001 as a joint project between EOHR and the American University in Cairo. This unit provides legal aid to those who seek refugee status in Egypt and it works within the framework of the United Nations High Commissioner for Refugees (UNHCR).\n\nAccounting and Financial Unit\n\nThe Unit handles the accounting works and prepares the annual financial report of EOHR.\n\nAdministrative and Secretarial Unit\n\nThe unit handles all the administrative and secretarial works, organizes meetings' appointments of the secretary general, prepares the published materials to be sent to those who are interested in the human rights' issues, via E-mail or fax.\n\nArchiving Unit\n\nThe unit handles paper and electronic archiving of the published materials of EOHR.\n\n\nThe EOHR also worked much on the political and constitutional reform issues:\n\nSince 1985, EOHR has issued many kinds of publications on the human rights:\n\n\n"}
{"id": "523130", "url": "https://en.wikipedia.org/wiki?curid=523130", "title": "El Segundo blue", "text": "El Segundo blue\n\nThe El Segundo blue (\"Euphilotes battoides allyni\") is a butterfly local to a small dune ecosystem in Southern California that used to be a community called Palisades del Rey, close to the Los Angeles International Airport (LAX). In 1976 it became a federally designated endangered species. The El Segundo Blue Butterfly Habitat Preserve next to LAX exists to protect the species. There are only three colonies of this tiny butterfly still in existence. The largest of these is on the grounds of LAX; a further colony exists on a site within the huge Chevron El Segundo oil refinery complex, and the smallest colony is an area of only a few square yards on a local beach. The butterfly lays its eggs on coast buckwheat, which the adults also use as a nectar source. Recently, some nearby beach cities such as Redondo Beach have replaced ice plant growth near the beaches with coast buckwheat, in order to provide the butterflies with more of their natural food source.\n\n"}
{"id": "1940754", "url": "https://en.wikipedia.org/wiki?curid=1940754", "title": "Feminist sociology", "text": "Feminist sociology\n\nFeminist sociology is a conflict theory and theoretical perspective which observes gender in its relation to power, both at the level of face-to-face interaction and reflexivity within a social structure at large. Focuses include sexual orientation, race, economic status, and nationality.\n\nAt the core of feminist sociology is the idea of the systemic oppression of women and the historical dominance of men within most societies: 'patriarchy'. Feminist thought has a rich history, however, which may be categorized into three 'waves'. The current, 'third wave', emphasizes the concepts of globalization, postcolonialism, post-structuralism and postmodernism. Contemporary feminist thought has frequently tended to do-away with all generalizations regarding sex and gender, closely linked with antihumanism, posthumanism, queer theory and the work of Michel Foucault.\n\nFeminist theory- Charlotte Perkins Gilman(1860-1935) was a women ahead of her time who work and research would help formalize \"feminist theory during the 1960s. Growing up she went against traditional holds that were placed on her by society by focusing on reading and learning concepts different than women who were taught to be a housewife. Her main focus was on gender inequality between men and women along with gender roles placed on my society. Where men go to work secure proper income for the family while women stay at home and tend to the family along with house hold chores. She \"emphasized how differential socialization leads to gender inequality\". Yet, she did agree that biologically there is different between those born with female and male parts. \n\nParts of her research involves a theoretical orientation of a multidimensional approach to gender and discusses more in depth in her book \"Women and Economics\". Due to gender roles she believes that women pretend to live a certain life to avoid achieving their full potential living the role of a housewife. This is an example of falling under false consciousness instead of \"true\" consciousness. Leading the belief that women are viewed as property towards their husbands because despite their own work they may do; economically women were still dependent on husbands to provide financial support to themselves and their family. She also stated that the traditional division of labor was not biologically driven , but instead pushed upon based structure of how society was established since before the 19th century. \n\nIn the end Gilman describes it as \"sociobiological\" tragedy because women are disregarded as being part of the ideology of \"survival of the fittest\". Instead females are thought to be soft and weak individuals that are only good for productive reasons. Females are depicted as emotional and frail human beings who are born to serve their husbands, children, and family without living for herself. Gilman's research was conducted during a time where women being a sociologist were unheard of; she lived during a time that women couldn't even vote. Her research helps create a ripple effect along with other female sociologists that help paved wave for feminism, feminist, and concepts to correlate with feminist theory.\n\nHeterosexism\n\nHeterosexism is a system of attitudes, bias, and discrimination in favour of male-female sexuality and relationships.\nHeterosexism describes a set of paradigms and institutionalized beliefs that systematically disadvantage anyone other than heterosexual, which can stem from personal beliefs, societal institutions and or a country's government. For example, heterosexual marriage has been or is the only lawful union between two people that was or is fully recognized and subsequently given full benefits in many countries. This has acted to greatly disadvantage people in same-sex relationships within society as compared with those in different-sex relationships.\n\nWomen who suffer from oppression due to race may find themselves in a double bind. The relationship between feminism and race was largely overlooked until the second wave of feminists produced greater literature on the topic of 'black feminism'.\n\nAnna Julia Cooper and Ida Bell Wells-Barnett are African American women who were instrumental in conducting much research and making valuable contributions in the field of black feminism. \"Cooper and Wells-Barnett both consciously drew on their lived experiences as African American women to develop a \"systematic consciousness of society and social relations.\" As such, these women foreshadow the development of a feminist sociological theory based in the interests of women of colour.\"\n\nAn instrumental contribution to the field was Kimberlé Crenshaw's seminal 1989 paper, \"Demarginalizing the Intersection of Race and Sex:A Black Feminist Critique of Antidiscrimination Doctrine, Feminist Theory and Antiracist Politics\" (Crenshaw 1989). In it she outlines the manner in which black women have been erased from feminist pedagogy. Black women must be understood as having multiple identities that intersect and reinforce one another, the two key experiences of being black and of being women. Furthermore, black women suffer on both racist and sexist fronts, marginalized not only by larger systems of oppression but by existing feminist discourse that disregards their intersectionality. Crenshaw's work is integral to understanding feminist sociology, as it advocated for black feminist thought and set the building blocks for future feminist sociologists such as Patricia Hill Collins. \n\nModern queer theory attempts to unmake the social and contextual elements reinforcing heteronormativity by challenging oppressive institutions on traditional binary distinctions between male and female, among its many other criticisms. In this regard, feminism and queer theory address the same ways social structures violently categorize and erase women and LGBTQIA+ from the social narrative. However, sociological feminism often reinforces the gender binary through the research process \"as the gendered subject is made the object of the study\" (McCann 2016, 229). In her recent work \"Epistemology of the Subject: Queer Theory's Challenge to Feminist Sociology\", McCann confronts the theoretical perspective and methodology of feminist sociology:\"[the subject] rarely reflects the fluid, unstable, and dynamic realities of bodies and experiences. To “settle” on a subject category, then, is to reinscribe a fixity that excludes some, often in violent ways (for example, those who are literally erased because their bodies do not conform to a discrete binary)\" (McCann 2016, 231-232). There can be a refashioning of the field, where extending boundaries to include queer theory would \"develop new and innovative theoretical approaches to research...[and] address inequality within society\" (McCann 2016, 237). \n\nDebates within ethnic relations, particularly regarding the opposing perspectives of assimilationism and multiculturalism, have led to the accusation that feminism is incompatible with multiculturalist policy. The remit of multiculturalism is to allow distinct cultures to reside in Western societies, or separate societies in general, and one possible consequence is that certain religious or traditional practices may negate Western feminist ideals. Central debates include the topics of arranged marriage and female genital mutilation. Others have argued that these debates stem from Western orientalism and general political reluctance to accept foreign migrants.\n\n\n</ref>\n"}
{"id": "39011827", "url": "https://en.wikipedia.org/wiki?curid=39011827", "title": "Holistic management (agriculture)", "text": "Holistic management (agriculture)\n\nHolistic management (from \"holos\", a Greek word meaning \"all\", \"whole\", \"entire\", \"total\") in agriculture is a systems thinking approach to managing resources that was originally developed by Allan Savory for reversing desertification. In 2010 the Africa Centre for Holistic Management in Zimbabwe, Operation Hope (a \"proof of concept\" project using holistic management) was named the winner of the 2010 Buckminster Fuller Challenge for \"recognizing initiatives which take a comprehensive, anticipatory, design approach to radically advance human well being and the health of our planet's ecosystems\".\n\nThe idea of holistic planned grazing began in the 1960s when Allan Savory, a wildlife biologist in his native Southern Rhodesia, set out to understand desertification. This can be seen in the context of the larger environmental movement. Heavily influenced by the work of André Voisin and the ineffectiveness of mainstream rangeland science of the time, Savory concluded that the spread of deserts, the loss of wildlife, and the human impoverishment that always resulted were related to the reduction of the natural herds of large grazers and even more, the change in behavior of those few remaining herds. Livestock could be substituted to provide important ecosystem services like nutrient cycling when mimicking those uniquely coevolved grasses and grazers. But managers had found that while rotational grazing systems can work for diverse management purposes, scientific experiments had demonstrated that they do not necessarily work for specific ecological purposes. An adaptive management plan was needed for the integration of the experiential with the experimental, as well as the social with the biophysical, to provide a more comprehensive framework for the management of rangeland systems. None of these sources of knowledge could be understood except in the context of the whole. Holistic management was developed to meet that need.\n\nIn many regions, pastoralism and communal land use are blamed for environmental degradation caused by overgrazing. After years of research and experience, Savory came to understand this assertion was often wrong, and that sometimes removing animals actually made it worse. This concept is a variation of the trophic cascade, where humans are seen as the top level predator and the cascade follows from there.\n\n\"I have been particularly fascinated, for example, by the work of a remarkable man called Allan Savory, in Zimbabwe and other semi arid areas, who has argued for years against the prevailing expert view that is the simple numbers of cattle that drive overgrazing and cause fertile land to become desert. On the contrary, as he has since shown so graphically, the land needs the presence of feeding animals and their droppings for the cycle to be complete, so that soils and grassland areas stay productive. Such that, if you take grazers off the land and lock them away in vast feedlots, the land dies. \"- His Royal Highness The Prince of Wales (Prince Charles) from a speech to the IUCN World Conservation Congress\nSavory developed a flexible management system designed to improve grazing systems. Holistic planned grazing is one of a number of newer grazing management systems that more closely simulate the behavior of natural herds of wildlife and have been shown to improve riparian habitats and water quality over systems that often led to land degradation, and be an effective tool to improve range condition for both livestock and wildlife. Holistic planned grazing is similar to rotational grazing but differs in that it more explicitly recognizes and provides a framework for adapting to the four basic ecosystem processes: the water cycle, the mineral cycle including the carbon cycle, energy flow, and community dynamics (the relationship between organisms in an ecosystem) as equal in importance to livestock production and social welfare. Thus the holistic context in the planning stage leads to different decisions in dealing with that complexity. Holistic management has been likened to \"a permaculture approach to rangeland management\".\n\nWhile originally developed as a tool for range land use and restoring desertified land, the holistic management system can be applied to other areas with multiple complex socioeconomic and environmental factors. One such example is Integrated Water Resources Management (IWRM), which promotes sector integration in development and management of water resources to ensure that water is allocated between different users in a fair way, maximizing economic and social welfare without compromising the sustainability of vital ecosystems. In essence, coordinated holistic water management takes into consideration all water users in nature and society. Another example is mine reclamation. A fourth use of Holistic management is in certain forms of no till crop production, intercropping, and permaculture. Holistic management has been acknowledged by The United States Department of Agriculture Natural Resources Conservation Service (USDA-NRCS).\n\nThe holistic management decision-making framework uses six key steps to guide the management of resources:\n\n\nHolistic management planned grazing has four key principles that take advantage of the symbiotic relationship between large herds of grazing animals, their predators and the grasslands that support them:\n\n\nClaims that holistic grazing can have a profound effect on climate change by making grazing cattle carbon negative have no basis in any peer-reviewed scientific literature and are based on literature only available on the Savory Institute website. \n\nRange scientists have not been able to experimentally confirm that intensive grazing systems similar to those at the center of holistic management show a benefit, and claim that managers' reports of success are anecdotal.\n\nAn assessment of multiple research studies, published by the United States Department of Agriculture, concluded that \"these results refute prior claims that animal trampling associated with high stocking rates or grazing pressures in rotational grazing systems enhance soil properties and promote hydrological function\". Similarly, a survey article by Briske et al. (the same author) that examined rotational grazing systems found \"few, if any, consistent benefits over continuous grazing.\" \n\nA paper by Richard Teague, a coauthor of the USDA paper, et al. pointed out that Briske had examined rotational systems in general and not Savory's holistic planned grazing process, developed in the 1960s when Savory recognized that a hundred years of rotational and other prescribed grazing systems had exacerbated desertification, even in the U.S. as stated in Savory’s TED talk. The paper contrasted the success reported by many ranchers practicing multi-paddock grazing with the general lack of evidence found by formal research.\n\nEarlier research that compared short duration grazing (SDG) and Savory Grazing Method (SGM) in southern Africa and found no evidence of range improvement, a slight economic improvement of a seven-unit intensive system with more animals but with individual weight loss. That study found no evidence for soil improvement, but instead that increased trampling had led to soil compaction.\n\nIn March 2013, the Savory Institute published a research portfolio with selected abstracts of papers, theses and reports supporting holistic management and responding to some of their critics. The same month Savory was a guest speaker with TED and gave a presentation titled \"How to Fight Desertification and Reverse Climate Change\". Responses were posted on several prominent blogs, including the blog \"The Wildlife News,\" by Ralph Maughan, who said \"The idea that we can almost like magic, green the desert and the degraded lands, by running even more livestock, albeit in a different fashion, sucking up greenhouse gases all the while, is a compelling and dangerous fantasy.\" RealClimate.org published a piece saying that Savory's claims that his technique can bring atmospheric carbon \"back to pre-industrial levels\" are \"simply not reasonable.\"\n\nA few scientists have focused much of their research to demonstrate the science behind how Holistic Planned Grazing (or Adaptive Multi-Paddock Grazing) does improve soil health and there is a growing body of research work which is reversing the claim that the only evidence for holistic management is anecdotal.\n\nSoil scientist Dr. Christine Jones from Australia with much of her research available on her website, wrote an article in which she discussed the research done by Dr. Mark Adams from the University of Sydney regarding the soil's ability to sequester methane.\n\nDr. Richard Teague from Texas A&M Agrilife has a number of different studies that he has undertaken focusing on GHG mitigation, as well as an article on the role of ruminants in reducing agriculture’s carbon that was co-authored with Dr. Rattan Lal of Ohio State University. Teague has also done specific research in Canada and Texas comparing the increased carbon sequestration on land where people are practicing Holistic Planned Grazing in comparison to their neighbors that are not. On average in the Texas study the holistically grazed land added 3 tons Carbon/ha/yr more than the heavy continuously grazed neighboring land.\n\nDr. Keith Weber from Idaho State University who shows how holistic planned grazing improves soil moisture retention on semi-arid rangelands. This water retention leads to more plant growth which increases soil carbon and thus carbon sequestration.\n\nSome qualitative studies show the improved profitability and sustainability of holistically managed operations and the efficacy of Holistic Management as a whole farm/ranch planning tool. In particular, Dr. Deborah Stinner, Dr. Benjamin Stinner of (Ohio State University at Wooster), and Ed Marsolf wrote an article about Holistic Management and how biodiversity is an organizing principle in agroecosystem management. Another was done by Charley Orchard as he surveyed ranchers in the Northern Rockies and the results they had achieved.\n\nMany farmers claim the system works well for them and they have even received awards.\n\n\n"}
{"id": "381399", "url": "https://en.wikipedia.org/wiki?curid=381399", "title": "Hydroelectricity", "text": "Hydroelectricity\n\nHydroelectricity is electricity produced from hydropower. In 2015, hydropower generated 16.6% of the world's total electricity and 70% of all renewable electricity, and was expected to increase about 3.1% each year for the next 25 years.\n\nHydropower is produced in 150 countries, with the Asia-Pacific region generating 33 percent of global hydropower in 2013. China is the largest hydroelectricity producer, with 920 TWh of production in 2013, representing 16.9 percent of domestic electricity use.\n\nThe cost of hydroelectricity is relatively low, making it a competitive source of renewable electricity. The hydro station consumes no water, unlike coal or gas plants. The average cost of electricity from a hydro station larger than 10 megawatts is 3 to 5 U.S. cents per kilowatt-hour. With a dam and reservoir it is also a flexible source of electricity since the amount produced by the station can be changed up or down very quickly to adapt to changing energy demands. Once a hydroelectric complex is constructed, the project produces no direct waste, and in many cases, has a considerably lower output level of greenhouse gases than fossil fuel powered energy plants.\n\nHydropower has been used since ancient times to grind flour and perform other tasks. In the mid-1770s, French engineer Bernard Forest de Bélidor published \"Architecture Hydraulique\" which described vertical- and horizontal-axis hydraulic machines. By the late 19th century, the electrical generator was developed and could now be coupled with hydraulics. The growing demand for the Industrial Revolution would drive development as well. In 1878 the world's first hydroelectric power scheme was developed at Cragside in Northumberland, England by William Armstrong. It was used to power a single arc lamp in his art gallery. The old Schoelkopf Power Station No. 1 near Niagara Falls in the U.S. side began to produce electricity in 1881. The first Edison hydroelectric power station, the Vulcan Street Plant, began operating September 30, 1882, in Appleton, Wisconsin, with an output of about 12.5 kilowatts. By 1886 there were 45 hydroelectric power stations in the U.S. and Canada. By 1889 there were 200 in the U.S. alone.\nAt the beginning of the 20th century, many small hydroelectric power stations were being constructed by commercial companies in mountains near metropolitan areas. Grenoble, France held the International Exhibition of Hydropower and Tourism with over one million visitors. By 1920 as 40% of the power produced in the United States was hydroelectric, the Federal Power Act was enacted into law. The Act created the Federal Power Commission to regulate hydroelectric power stations on federal land and water. As the power stations became larger, their associated dams developed additional purposes to include flood control, irrigation and navigation. Federal funding became necessary for large-scale development and federally owned corporations, such as the Tennessee Valley Authority (1933) and the Bonneville Power Administration (1937) were created. Additionally, the Bureau of Reclamation which had begun a series of western U.S. irrigation projects in the early 20th century was now constructing large hydroelectric projects such as the 1928 Hoover Dam. The U.S. Army Corps of Engineers was also involved in hydroelectric development, completing the Bonneville Dam in 1937 and being recognized by the Flood Control Act of 1936 as the premier federal flood control agency.\n\nHydroelectric power stations continued to become larger throughout the 20th century. Hydropower was referred to as \"white coal\" for its power and plenty. Hoover Dam's initial 1,345 MW power station was the world's largest hydroelectric power station in 1936; it was eclipsed by the 6809 MW Grand Coulee Dam in 1942. The Itaipu Dam opened in 1984 in South America as the largest, producing 14,000 MW but was surpassed in 2008 by the Three Gorges Dam in China at 22,500 MW. Hydroelectricity would eventually supply some countries, including Norway, Democratic Republic of the Congo, Paraguay and Brazil, with over 85% of their electricity. The United States currently has over 2,000 hydroelectric power stations that supply 6.4% of its total electrical production output, which is 49% of its renewable electricity.\n\nThe technical potential for the growth of hydropower around the world are, 71% Europe, 75% North America, 79% South America, 95% Africa, 95% Middle East, 82% Asia Pacific. \nThe political realities of new reservoirs in western countries, economic limitations in the third world and the lack of a transmission system in undeveloped areas, result in the possibility of developing 25% of the remaining potential before 2050, with the bulk of that being in the Asia Pacific area. A few countries are highly developed and have very little room for growth, Switzerland 12% and Mexico 20%.\n\nMost hydroelectric power comes from the potential energy of dammed water driving a water turbine and generator. The power extracted from the water depends on the volume and on the difference in height between the source and the water's outflow. This height difference is called the head. A large pipe (the \"penstock\") delivers water from the reservoir to the turbine.\n\nThis method produces electricity to supply high peak demands by moving water between reservoirs at different elevations. At times of low electrical demand, the excess generation capacity is used to pump water into the higher reservoir. When the demand becomes greater, water is released back into the lower reservoir through a turbine. Pumped-storage schemes currently provide the most commercially important means of large-scale grid energy storage and improve the daily capacity factor of the generation system. Pumped storage is not an energy source, and appears as a negative number in listings.\n\nRun-of-the-river hydroelectric stations are those with small or no reservoir capacity, so that only the water coming from upstream is available for generation at that moment, and any oversupply must pass unused. A constant supply of water from a lake or existing reservoir upstream is a significant advantage in choosing sites for run-of-the-river. In the United States, run of the river hydropower could potentially provide (about 13.7% of total use in 2011 if continuously available).\n\nA tidal power station makes use of the daily rise and fall of ocean water due to tides; such sources are highly predictable, and if conditions permit construction of reservoirs, can also be dispatchable to generate power during high demand periods. Less common types of hydro schemes use water's kinetic energy or undammed sources such as undershot water wheels. Tidal power is viable in a relatively small number of locations around the world. In Great Britain, there are eight sites that could be developed, which \nhave the potential to generate 20% of the electricity used in 2012.\n\nLarge-scale hydroelectric power stations are more commonly seen as the largest power producing facilities in the world, with some hydroelectric facilities capable of generating more than double the installed capacities of the current largest nuclear power stations.\n\nAlthough no official definition exists for the capacity range of large hydroelectric power stations, facilities from over a few hundred megawatts are generally considered large hydroelectric facilities.\n\nCurrently, only four facilities over () are in operation worldwide, see table below.\n\nSmall hydro is the development of hydroelectric power on a scale serving a small community or industrial plant. The definition of a small hydro project varies but a generating capacity of up to 10 megawatts (MW) is generally accepted as the upper limit of what can be termed small hydro. This may be stretched to 25 MW and 30 MW in Canada and the United States. Small-scale hydroelectricity production grew by 29% from 2005 to 2008, raising the total world small-hydro capacity to . Over 70% of this was in China (), followed by Japan (), the United States (), and India ().\n\nSmall hydro stations may be connected to conventional electrical distribution networks as a source of low-cost renewable energy. Alternatively, small hydro projects may be built in isolated areas that would be uneconomic to serve from a network, or in areas where there is no national electrical distribution network. Since small hydro projects usually have minimal reservoirs and civil construction work, they are seen as having a relatively low environmental impact compared to large hydro. This decreased environmental impact depends strongly on the balance between stream flow and power production.\n\nMicro hydro is a term used for hydroelectric power installations that typically produce up to of power. These installations can provide power to an isolated home or small community, or are sometimes connected to electric power networks. There are many of these installations around the world, particularly in developing nations as they can provide an economical source of energy without purchase of fuel. Micro hydro systems complement photovoltaic solar energy systems because in many areas, water flow, and thus available hydro power, is highest in the winter when solar energy is at a minimum.\n\nPico hydro is a term used for hydroelectric power generation of under . It is useful in small, remote communities that require only a small amount of electricity. For example, to power one or two fluorescent light bulbs and a TV or radio for a few homes. Even smaller turbines of 200-300W may power a single home in a developing country with a drop of only . A Pico-hydro setup is typically run-of-the-river, meaning that dams are not used, but rather pipes divert some of the flow, drop this down a gradient, and through the turbine before returning it to the stream.\n\nAn underground power station is generally used at large facilities and makes use of a large natural height difference between two waterways, such as a waterfall or mountain lake. An underground tunnel is constructed to take water from the high reservoir to the generating hall built in an underground cavern near the lowest point of the water tunnel and a horizontal tailrace taking water away to the lower outlet waterway.\n\nA simple formula for approximating electric power production at a hydroelectric station is: formula_1, where\n\nAnnual electric energy production depends on the available water supply. In some installations, the water flow rate can vary by a factor of 10:1 over the course of a year.\n\nHydropower is a flexible source of electricity since stations can be ramped up and down very quickly to adapt to changing energy demands. Hydro turbines have a start-up time of the order of a few minutes. It takes around 60 to 90 seconds to bring a unit from cold start-up to full load; this is much shorter than for gas turbines or steam plants. Power generation can also be decreased quickly when there is a surplus power generation. Hence the limited capacity of hydropower units is not generally used to produce base power except for vacating the flood pool or meeting downstream needs. Instead, it serves as backup for non-hydro generators.\n\nThe major advantage of conventional hydroelectric dams with reservoirs is their ability to store water at low cost for dispatch later as high value clean electricity. The average cost of electricity from a hydro station larger than 10 megawatts is 3 to 5 U.S. cents per kilowatt-hour. When used as peak power to meet demand, hydroelectricity has a higher value than base power and a much higher value compared to intermittent energy sources.\n\nHydroelectric stations have long economic lives, with some plants still in service after 50–100 years. Operating labor cost is also usually low, as plants are automated and have few personnel on site during normal operation.\n\nWhere a dam serves multiple purposes, a hydroelectric station may be added with relatively low construction cost, providing a useful revenue stream to offset the costs of dam operation. It has been calculated that the sale of electricity from the Three Gorges Dam will cover the construction costs after 5 to 8 years of full generation. Additionally, some data shows that in most countries large hydropower dams will be too costly and take too long to build to deliver a positive risk adjusted return, unless appropriate risk management measures are put in place.\n\nWhile many hydroelectric projects supply public electricity networks, some are created to serve specific industrial enterprises. Dedicated hydroelectric projects are often built to provide the substantial amounts of electricity needed for aluminium electrolytic plants, for example. The Grand Coulee Dam switched to support Alcoa aluminium in Bellingham, Washington, United States for American World War II airplanes before it was allowed to provide irrigation and power to citizens (in addition to aluminium power) after the war. In Suriname, the Brokopondo Reservoir was constructed to provide electricity for the Alcoa aluminium industry. New Zealand's Manapouri Power Station was constructed to supply electricity to the aluminium smelter at Tiwai Point.\n\nSince hydroelectric dams do not use fuel, power generation does not produce carbon dioxide. While carbon dioxide is initially produced during construction of the project, and some methane is given off annually by reservoirs, hydro in specific nordic cases, has the lowest lifecycle greenhouse gas emissions for power generation. Compared to fossil fuels generating an equivalent amount of electricity, hydro displaced three billion tonnes of CO2 emissions in 2011. One measurement of greenhouse gas related and other externality comparison between energy sources can be found in the ExternE project by the Paul Scherrer Institute and the University of Stuttgart which was funded by the European Commission. According to that study, hydroelectricity in Europe produces the least amount of greenhouse gases and externality of any energy source. Coming in second place was wind, third was nuclear energy, and fourth was solar photovoltaic. The low greenhouse gas impact of hydroelectricity is found especially in temperate climates. The above study was for local energy in Europe; presumably similar conditions prevail in North America and Northern Asia, which all see a regular, natural freeze/thaw cycle (with associated seasonal plant decay and regrowth). Greater greenhouse gas emission impacts are found in the tropical regions because the reservoirs of power stations in tropical regions produce a larger amount of methane than those in temperate areas.\n\nReservoirs created by hydroelectric schemes often provide facilities for water sports, and become tourist attractions themselves. In some countries, aquaculture in reservoirs is common. Multi-use dams installed for irrigation support agriculture with a relatively constant water supply. Large hydro dams can control floods, which would otherwise affect people living downstream of the project.\n\nLarge reservoirs associated with traditional hydroelectric power stations result in submersion of extensive areas upstream of the dams, sometimes destroying biologically rich and productive lowland and riverine valley forests, marshland and grasslands. Damming interrupts the flow of rivers and can harm local ecosystems, and building large dams and reservoirs often involves displacing people and wildlife. The loss of land is often exacerbated by habitat fragmentation of surrounding areas caused by the reservoir.\n\nHydroelectric projects can be disruptive to surrounding aquatic ecosystems both upstream and downstream of the plant site. Generation of hydroelectric power changes the downstream river environment. Water exiting a turbine usually contains very little suspended sediment, which can lead to scouring of river beds and loss of riverbanks. Since turbine gates are often opened intermittently, rapid or even daily fluctuations in river flow are observed.\n\nA 2011 study by the United States National Renewable Energy Laboratory concluded that hydroelectric plants in the U.S. consumed between 1,425 and 18,000 gallons of water per megawatt-hour (gal/MWh) of electricity generated, through evaporation losses in the reservoir. The median loss was 4,491 gal/MWh, which is higher than the loss for generation technologies that use cooling towers, including concentrating solar power (865 gal/MWh for CSP trough, 786 gal/MWh for CSP tower), coal (687 gal/MWh), nuclear (672 gal/MWh), and natural gas (198 gal/MWh). Where there are multiple uses of reservoirs such as water supply, recreation, and flood control, all reservoir evaporation is attributed to power production.\n\nWhen water flows it has the ability to transport particles heavier than itself downstream. This has a negative effect on dams and subsequently their power stations, particularly those on rivers or within catchment areas with high siltation. Siltation can fill a reservoir and reduce its capacity to control floods along with causing additional horizontal pressure on the upstream portion of the dam. Eventually, some reservoirs can become full of sediment and useless or over-top during a flood and fail.\n\nChanges in the amount of river flow will correlate with the amount of energy produced by a dam. Lower river flows will reduce the amount of live storage in a reservoir therefore reducing the amount of water that can be used for hydroelectricity. The result of diminished river flow can be power shortages in areas that depend heavily on hydroelectric power. The risk of flow shortage may increase as a result of climate change. One study from the Colorado River in the United States suggest that modest climate changes, such as an increase in temperature in 2 degree Celsius resulting in a 10% decline in precipitation, might reduce river run-off by up to 40%. Brazil in particular is vulnerable due to its heavy reliance on hydroelectricity, as increasing temperatures, lower water ﬂow and alterations in the rainfall regime, could reduce total energy production by 7% annually by the end of the century.\n\nLower positive impacts are found in the tropical regions, as it has been noted that the reservoirs of power plants in tropical regions produce substantial amounts of methane. This is due to plant material in flooded areas decaying in an anaerobic environment, and forming methane, a greenhouse gas. According to the World Commission on Dams report, where the reservoir is large compared to the generating capacity (less than 100 watts per square metre of surface area) and no clearing of the forests in the area was undertaken prior to impoundment of the reservoir, greenhouse gas emissions from the reservoir may be higher than those of a conventional oil-fired thermal generation plant.\n\nIn boreal reservoirs of Canada and Northern Europe, however, greenhouse gas emissions are typically only 2% to 8% of any kind of conventional fossil-fuel thermal generation. A new class of underwater logging operation that targets drowned forests can mitigate the effect of forest decay.\n\nAnother disadvantage of hydroelectric dams is the need to relocate the people living where the reservoirs are planned. In 2000, the World Commission on Dams estimated that dams had physically displaced 40-80 million people worldwide.\n\nBecause large conventional dammed-hydro facilities hold back large volumes of water, a failure due to poor construction, natural disasters or sabotage can be catastrophic to downriver settlements and infrastructure.\n\nDuring Typhoon Nina in 1975 Banqiao Dam failed in Southern China when more than a year's worth of rain fell within 24 hours. The resulting flood resulted in the deaths of 26,000 people, and another 145,000 from epidemics. Millions were left homeless.\n\nThe creation of a dam in a geologically inappropriate location may cause disasters such as 1963 disaster at Vajont Dam in Italy, where almost 2,000 people died.\n\nThe Malpasset Dam failure in Fréjus on the French Riviera (Côte d'Azur), southern France, collapsed on December 2, 1959, killing 423 people in the resulting flood.\n\nSmaller dams and micro hydro facilities create less risk, but can form continuing hazards even after being decommissioned. For example, the small earthen embankment Kelly Barnes Dam failed in 1977, twenty years after its power station was decommissioned; causing 39 deaths.\n\nHydroelectricity eliminates the flue gas emissions from fossil fuel combustion, including pollutants such as sulfur dioxide, nitric oxide, carbon monoxide, dust, and mercury in the coal. Hydroelectricity also avoids the hazards of coal mining and the indirect health effects of coal emissions.\n\nCompared to nuclear power, hydroelectricity construction requires altering large areas of the environment while a nuclear power station has a small footprint, and hydro-powerstation failures have caused tens of thousands of more deaths than any nuclear station failure. The creation of Garrison Dam, for example, required Native American land to create Lake Sakakawea, which has a shoreline of 1,320 miles, and caused the inhabitants to sell 94% of their arable land for $7.5 million in 1949.\n\nHowever, nuclear power is relatively inflexible; although nuclear power can reduce its output reasonably quickly. Since the cost of nuclear power is dominated by its high infrastructure costs, the cost per unit energy goes up significantly with low production. Because of this, nuclear power is mostly used for baseload. By way of contrast, hydroelectricity can supply peak power at much lower cost. Hydroelectricity is thus often used to complement nuclear or other sources for load following. Country examples were they are paired in a close to 50/50 share include the electric grid in Switzerland, the Electricity sector in Sweden and to a lesser extent, Ukraine and the Electricity sector in Finland.\n\nWind power goes through predictable variation by season, but is intermittent on a daily basis. Maximum wind generation has little relationship to peak daily electricity consumption, the wind may peak at night when power isn't needed or be still during the day when electrical demand is highest. Occasionally weather patterns can result in low wind for days or weeks at a time, a hydroelectric reservoir capable of storing weeks of output is useful to balance generation on the grid. Peak wind power can be offset by minimum hydropower and minimum wind can be offset with maximum hydropower. In this way the easily regulated character of hydroelectricity is used to compensate for the intermittent nature of wind power. Conversely, in some cases wind power can be used to spare water for later use in dry seasons.\n\nIn areas that do not have hydropower, pumped storage serves a similar role, but at a much higher cost and 20% lower efficiency. An example of this is Norway's trading with Sweden, Denmark, the Netherlands and possibly Germany or the UK in the future. Norway is 98% hydropower, while it's flatland neighbors are installing wind power.\n\nThe ranking of hydro-electric capacity is either by actual annual energy production or by installed capacity power rating. In 2015 hydropower generated 16.6% of the worlds total electricity and 70% of all renewable electricity.\nHydropower is produced in 150 countries, with the Asia-Pacific region generated 32 percent of global hydropower in 2010. China is the largest hydroelectricity producer, with 721 terawatt-hours of production in 2010, representing around 17 percent of domestic electricity use. Brazil, Canada, New Zealand, Norway, Paraguay, Austria, Switzerland, and Venezuela have a majority of the internal electric energy production from hydroelectric power. Paraguay produces 100% of its electricity from hydroelectric dams, and exports 90% of its production to Brazil and to Argentina. Norway produces 98–99% of its electricity from hydroelectric sources.\n\nA hydro-electric station rarely operates at its full power rating over a full year; the ratio between annual average power and installed capacity rating is the capacity factor. The installed capacity is the sum of all generator nameplate power ratings.\n\n\n"}
{"id": "579311", "url": "https://en.wikipedia.org/wiki?curid=579311", "title": "Image (mathematics)", "text": "Image (mathematics)\n\nIn mathematics, an image is the subset of a function's codomain which is the output of the function from a subset of its domain.\n\nEvaluating a function at each element of a subset \"X\" of the domain, produces a set called the \"image of X under or through\" the function. The inverse image or preimage of a particular subset \"S\" of the codomain of a function is the set of all elements of the domain that map to the members of \"S\".\n\nImage and inverse image may also be defined for general binary relations, not just functions.\n\nThe word \"image\" is used in three related ways. In these definitions, \"f\" : \"X\" → \"Y\" is a function from the set \"X\" to the set \"Y\".\n\nIf \"x\" is a member of \"X\", then \"f\"(\"x\") = \"y\" (the value of \"f\" when applied to \"x\") is the image of \"x\" under \"f\". \"y\" is alternatively known as the output of \"f\" for argument \"x\".\n\nThe image of a subset \"A\" ⊆ \"X\" under \"f\" is the subset \"f\"<nowiki>[</nowiki>\"A\"<nowiki>]</nowiki> ⊆ \"Y\" defined by (in set-builder notation):\n\nWhen there is no risk of confusion, \"f\"<nowiki>[</nowiki>\"A\"<nowiki>]</nowiki> is simply written as \"f\"(\"A\"). This convention is a common one; the intended meaning must be inferred from the context. This makes \"f\"[.] a function whose domain is the power set of \"X\" (the set of all subsets of \"X\"), and whose codomain is the power set of \"Y\". See Notation below.\n\nThe image \"f\"<nowiki>[</nowiki>\"X\"<nowiki>]</nowiki> of the entire domain \"X\" of \"f\" is called simply the image of \"f\".\n\nIf \"R\" is an arbitrary binary relation on \"X\"×\"Y\", the set { y∈\"Y\" | \"xRy\" for some \"x\"∈\"X\" } is called the image, or the range, of \"R\". Dually, the set { \"x\"∈\"X\" | \"xRy\" for some y∈\"Y\" } is called the domain of \"R\".\n\nLet \"f\" be a function from \"X\" to \"Y\". The preimage or inverse image of a set \"B\" ⊆ \"Y\" under \"f\" is the subset of \"X\" defined by\n\nThe inverse image of a singleton, denoted by \"f\"<nowiki>[</nowiki>{\"y\"}<nowiki>]</nowiki> or by \"f\"<nowiki>[</nowiki>\"y\"<nowiki>]</nowiki>, is also called the fiber over \"y\" or the level set of \"y\". The set of all the fibers over the elements of \"Y\" is a family of sets indexed by \"Y\".\n\nFor example, for the function \"f\"(\"x\") = \"x\", the inverse image of {4} would be {−2, 2}. Again, if there is no risk of confusion, we may denote \"f\"<nowiki>[</nowiki>\"B\"<nowiki>]</nowiki> by \"f\"(\"B\"), and think of \"f\" as a function from the power set of \"Y\" to the power set of \"X\". The notation \"f\" should not be confused with that for inverse function. The notation coincides with the usual one, though, for bijections, in the sense that the inverse image of \"B\" under \"f\" is the image of \"B\" under \"f\".\n\nThe traditional notations used in the previous section can be confusing. An alternative is to give explicit names for the image and preimage as functions between powersets:\n\n\n\n\n\nGiven a function \"f\" : \"X\" → \"Y\", for all subsets \"A\", \"A\", and \"A\" of \"X\" and all subsets \"B\", \"B\", and \"B\" of \"Y\" we have:\n\nThe results relating images and preimages to the (Boolean) algebra of intersection and union work for any collection of subsets, not just for pairs of subsets:\n(Here, \"S\" can be infinite, even uncountably infinite.)\n\nWith respect to the algebra of subsets, by the above we see that the inverse image function is a lattice homomorphism while the image function is only a semilattice homomorphism (it does not always preserve intersections).\n\n\n"}
{"id": "25174869", "url": "https://en.wikipedia.org/wiki?curid=25174869", "title": "Innovation competition", "text": "Innovation competition\n\nAn innovation competition is a method or process of the industrial process, product or business development. It is a form of social engineering, which focuses to the creation and elaboration of the best and sustainable ideas, coming from the best innovators.\n\nThere are few major works, like Terwiesch and Ulrich, who exclusively focus on innovation competitions. They argue, that while innovation is seen as a largely creative endeavor, it can also be rigorously managed by viewing and structuring the innovation process as a collection of “opportunities”. Profitable innovation comes not from increasing investments in R&D, but from systematically identifying more exceptional opportunities. Terwiesch and Ulrich show how to design and run innovation tournaments: pitting competing opportunities against one another, and then consistently filtering out the weakest ones until only those with the highest profit potential remain.\n\nThe aims and the design principles of the innovation competitions are noted in the literature as follows:\n\n\nIn September 2009, Netflix awarded 1 million US dollars to the winner of Netflix prize—the team who by 10% improved the accuracy of predictions about the extent that people enjoy a movie based on their past movie preferences.\n\nOn Nov. 16, 2010, General Electric will announce the winners of its multimillion-dollar challenge to find new, breakthrough ideas to create cleaner, more efficient and economically viable grid technologies, and to accelerate the adoption of smart grid technologies.\n\nSince 2010, the IXL Innovation Olympics has provided a platform to source breakthrough ideas, for CEOs from Fortune 100 Companies, using teams from MBA programs across the globe. As an example, IBM used this platform to source breakthrough ideas for increaesing accessibility to screens and devices for the aging population.<ref> September 6, 2016, Nicola Palmarini.\n\n\n"}
{"id": "4087426", "url": "https://en.wikipedia.org/wiki?curid=4087426", "title": "Instinctive drift", "text": "Instinctive drift\n\nInstinctive drift also known as instinctual drift is the tendency of an animal, of any species, to revert to unconscious and automatic behaviour that interferes with operant conditioning and the learned responses that come with it. B.F Skinner was an American psychologist who coined the term operant conditioning or instrumental conditioning, which is to learn by means of being granted either a reward or punisher for the action. It's through the association of the behaviour and consequence that follows that depicts if an animal will learn to practice the behaviour, or if the behaviour will become extinct. The concept of instinctive drift originated by the two former students of B.F Skinner's, Keller and Marian Breland when they taught raccoons to deposit money into a bank slot. The raccoons were initially successful at this activity but over time began to dip the coins in and out and rub them together rather than drop them in. In nature raccoons instinctively dip their food into water several times and rub it in order to wash it. Therefore, the adoption of this behaviour with coins is instinctual drift, and interferes with their training. Instinctive behaviour is innate, it isn't learned. it's usually automatic and unplanned, a natural reaction which often is preferred by the animal over learned and unnatural actions. This topic is relevant and discussed when it comes to the nature and nurture controversy. Many experimental studies have been conducted on this topic which proves its reliability. Natural behaviours are easier to learn and often easier expressed, even subliminally.\n\nThe term instinctive drift was coined by Keller and Marian Breland, former students of B.F. Skinner at the University of Minnesota (Marian in 1938, Keller in 1940). B.F. Skinner coined the term operant conditioning. The Breland's followed Skinners principles of control and decided to live off this principle through creation of \"Animal Behaviour Enterprises. They began the commercial training of animals instead of finishing their degrees. They began their business on a farm, but as multiple contracts were being created with various companies, the Breland shows ended up spreading nationwide. It was Keller and Marian Breland who discovered the meaning of and coined the term instinctive drift through their research. They were working with racoons and conducted an experiment which involved teaching raccoons to deposit money into a bank slot. The Brelands noticed that over time and as the reinforcement schedule was spaced out, the raccoons began to dip the coins in and out of the bank and rub them with their paws rather than depositing them. They concluded that this was an instinct that was interfering with the raccoons’ performance on the task. In nature, raccoons dip their food in water several times in order to wash it. This is an instinct which was seemingly triggered by the similar action sequence involved in retrieving and depositing coins into a bank. This instinctual drift was successfully avoided when they instead taught the raccoons to place a basketball into a basket. Because of the size of the ball and the different body position involved in this action, the raccoons did not experience instinctual drift (they did not dip the balls in and out of the basket.)\n\nA similar experiment was conducted on pigs. They were trained to insert wooden coins into a piggy bank. Over time, the pigs stopped depositing the coins and instead began to drop it in the dirt, push it down with their noses, drag it back out, and fling it into the air. This is a series of actions which are part of a behaviour known as rooting. It is an instinctual pattern of behaviour which pigs use to dig for food and to communicate. The pigs preferred rooting than performing their trained action (depositing the coin) and this is therefore an example of instinctual drift interfering with operant conditioning.\n\nThe nature vs. nurture controversy is a major topic discussed in psychology and pertains to animal training as well. Both sides of the nature vs. nurture debate have valid points and this controversy is one of the most debated in psychology. A common question asked today by many experts in various fields is if behaviour is due to life experiences or if it is predisposed in DNA. Today, partial credit is given to both sides and in many cases nature and nurture are given equal weight. With animal training it is often questioned if the training and shaping is the cause of a behaviour exhibited by an animal (nurture), or if the behaviour is actually innate to the species (nature). Instinctive drift centers around the nature of behaviour more so than learning being the sole cause of a behaviour. Species are obviously capable of learning behaviours, this is not denied in instinctive drift. Instinctive drift says that animals often revert to innate (nature) behaviours that can interfere with conditioned responses (nurture).\n\nInstinctive drift can be discussed in association with evolution. Evolution is commonly classified as change occurring over a period of time. Instinctive drift says that animals will behave in accordance with evolutionary contingencies, as opposed to operant contingencies of their specific training. Evolutionary roots of instinct exist. Evolution of traits and behaviours occur over time and it is by means of evolution and natural selection that adaptive traits and behaviours are passed on to the next generation and maladaptive traits are weaned out. It is the adaptive traits of species over time that is exhibited in instinctive drift and that species revert to that interferes with operant conditioning. Much knowledge on the topic of evolution and natural selection can be credited to Charles Darwin. Darwin developed and proposed the theory of evolution and it was through this knowledge that other subjects could be better understood, such as instinctive drift.\n"}
{"id": "23098762", "url": "https://en.wikipedia.org/wiki?curid=23098762", "title": "Jacobi coordinates", "text": "Jacobi coordinates\n\nIn the theory of many-particle systems, Jacobi coordinates often are used to simplify the mathematical formulation. These coordinates are particularly common in treating polyatomic molecules and chemical reactions, and in celestial mechanics. An algorithm for generating the Jacobi coordinates for \"N\" bodies may be based upon binary trees. In words, the algorithm is described as follows:\nLet \"m\" and \"m\" be the masses of two bodies that are replaced by a new body of virtual mass \"M\" = \"m\" + \"m\". The position coordinates x and x are replaced by their relative position r = x − x and by the vector to their center of mass R = (\"m\" \"q\" + \"m\"\"q\")/(\"m\" + \"m\"). The node in the binary tree corresponding to the virtual body has \"m\" as its right child and \"m\" as its left child. The order of children indicates the relative coordinate points from x to x. Repeat the above step for \"N\" − 1 bodies, that is, the \"N\" − 2 original bodies plus the new virtual body. \n\nFor the \"N\"-body problem the result is:\n\nwith\n\nThe vector formula_4 is the center of mass of all the bodies:\n\nThe result one is left with is thus a system of \"N\"-1 translationally invariant coordinates formula_5 and a center of mass coordinate formula_6, from iteratively reducing two-body systems within the many-body system.\n"}
{"id": "9978479", "url": "https://en.wikipedia.org/wiki?curid=9978479", "title": "Joseph L. Graves", "text": "Joseph L. Graves\n\nJoseph L. Graves, Jr. (born 1955), is an American Scientist and the Associate Dean for Research and Professor of Biological Studies at the Joint School for Nanoscience and Nanoengineering which is jointly administered by North Carolina Agricultural and Technical State University and UNC Greensboro. His past research has included an examination of the evolution of life history and physiological performance in Drosophila, a genus of small flies often called fruit flies. His current work includes the genomics of adaptation, as well as the response of bacteria to metallic/metallic oxide nanoparticles. A particular application of this research has been to the evolutionary theory of aging. Using his background in evolutionary biology, he has also written two books that address myths and theories of race in American society. Graves has made appearances in six documentary films on these general topics. He has been a Principal Investigator on grants from the National Institute of Health, National Science Foundation and the Arizona Disease Research Commission.\n\nGraves was born in 1955 and received his B.A. in biology from Oberlin College in 1977 and his PhD from Wayne State University in 1988. Before his appointment to North Carolina A & T State University, he held positions at the University of California, Irvine; at the West campus of Arizona State University, with a joint appointment in African American Studies at the main campus of Arizona State University in Tempe; and as University Core Director at Fairleigh Dickinson University.\n\nChildhood experiences shaped Graves' interest in race and racism: \"My parents were poor. They didn’t know how to read. I had to teach myself how to read,\" he says. \"The school system of my home was racially biased. When I was in kindergarten teachers wanted to declare me mentally retarded so that I could be placed in a special education curriculum. The regular curriculum had a tracking system,\" Graves continues. \"For no apparent reason, all the black kids ended up in the lower track.\" But, by graduation day, years later, Graves had risen to be among the highest ranked students at his high school. He accepted an academic scholarship to attend Oberlin College and graduated from there with a A.B. in Biology in 1977. His next two years were spent at the Institute for Tropical Disease at the University of Lowell (today called U. Mass. Lowell.) And thus, this allowed him to earn a National Science Foundation Graduate Fellowship to begin his PhD work at the University of Michigan in 1979. He completed his PhD in Evolutionary, Environmental, and Systematic Biology at Wayne State University in 1988. This work afforded him the prestigious President's Postdoctoral Fellowship at the University of California, Irvine from 1988-1990. He joined the faculty at the University of California, Irvine in 1990.\n\nWorking with Laurence D. Mueller, Graves found that population density is an important factor in determining both the immediate chances of survival and the course of natural selection for small organisms such as fruit flies. In \"Chance, Development, and Aging\", \"Human Biology\" December 2001, Graves wrote that the explanation of individual patterns of aging must take into account subtle mechanisms such as extensive chance variations in cell number and connections, in cell fates during differentiation, and in physiological patterns that arise during development. Graves has studied the tiny insects for more than a decade in pursuit of greater understanding of senescence, the process of aging.\n\nIn addition to the study of aging, Graves is interested in the history and philosophy of science as it relates to the biology of race and racism in western society. He has received a fair amount of attention from the press for his writings on this topic, especially his strong statements about the socially constructed nature of race. According to his profile on the University of North Carolina Minority Health Project website, he believes: there are still significant academic and popular views of race that are mired in the biological determinism of the 19th century and the application of proper scientific method and philosophy, along with quantitative genetics reveals the underlying racist ideology of these programs.\n\nIn addition to his research interests, Graves has also been an active participant in the struggle to protect and improve the teaching of science in the public schools.\nHe advocates discussing human biological variation and race in high school and college science curricula.\n\n\n\n"}
{"id": "49670559", "url": "https://en.wikipedia.org/wiki?curid=49670559", "title": "Judicial system in the United Arab Emirates", "text": "Judicial system in the United Arab Emirates\n\nThe legal system of the United Arab Emirates is based on the Constitution of the United Arab Emirates; this system is dual in nature as it has local and federal courts with a Supreme Court Based at Abu Dhabi.\n\nUnlike Britain and other countries where previous court judgments are regularly used as legal precedents, the UAE does not depend that much on precedents, although sometimes the judgments of higher courts can be applied by lower courts in similar cases.\n\nWhere legislative provisions do not cover a specific issue, the court will make a decision in accordance with Sharia law. Islamic jurisprudence is widely used in the construction and interpretation of the UAE law.\n\nPrior to the Union in 1971, the Trucial States were known as a protectorate of the British Empire established through a number of treaties. During that period, most disputes were handled by the rulers of the emirates, heads of local tribes, and unofficial judges following customary law. The primary source of law was Islam along with the unwritten social conventions or \"Urf\". Sharia judges specialized in family disputes whereas customary law judges handled criminal assaults and personal disputes. In the inland Bedouin communities, disputes usually related to livestock and water resources. Disputes in coastal cities were normally about trade and commercial relations, particularly related to the pearl trade.\n\nAfter the 1853 truce, disputes pertaining to pearl diving were handled directly by the British political resident in the Persian Gulf.\n\nThe United Arab Emirates’ Constitution, which came into effect on 2 December 1971, dedicates all its fifth Section for the Union legal system. Article 94 of the Constitution stipulates that “justice is the basis of authority”.\n\nThe United Arab Emirates is essentially a civil law jurisdiction that depends in the first place on Roman, French, Egyptian laws . However, the main source of the Emirati law is the Sharia law. International law is another source of Emirati law, in compliance with the UAE obligations signed in international conventions.\n\nBeing the highest judicial instance in the UAE does not mean that it applies to the seven Emirates, Dubai and Ras Al Khayma have their own local judicial system. Article 96 of the UAE Constitution reads as follows “The Supreme Court of the Union shall consist of a President and a number of Judges, not exceeding five in all, who shall be appointed by decree, issued by the President of the Union after approval by the Supreme Council ”.\n\nThe constitution also divides courts in the country into two types, federal courts and local courts. The president and the members of the Supreme Court can by no means be removed from their offices, except in the following cases: Death, resignation, completion of term or secondment, retirement, permanent disability that prevent a judge from undertaking their duties, disciplinary discharge and finally “appointment to other offices, with their agreement”. Any court has it jurisdiction, the judgments of the Emirati Supreme Court cover matters like Miscellaneous disputes raising between the Member Emirates, law constitutionality, constitution interpretation, interrogation of senior officials of the Union like Ministers, crimes threatening affecting the interests of the Union. Article 101 of the Emirati Constitution stipulates that “The judgements of the Supreme Court of the Union shall be final and binding upon all.”\n\nThe first President of the Union Supreme Court of the United Arab Emirates is H.E Judge Dr. Abdul Wahab Abdul (incumbent).\n\nAs their names suggests, Federal Courts are competent to consider all cases arising within the UAE territory, like disputes, domiciliary, transactions, etc. \"Furthermore, UAE courts have a kind of “emergency jurisdiction,” whereby a UAE court can determine preliminary applications for attachment of a defendant’s assets as well as urgent applications, even where the court would otherwise lack jurisdiction\" . Cases are commenced in Federal Courts when the complaint is filed with the Reconciliation and Settlement Committee. “The relevant reconciliation committee will attempt reconciliation, and refer complaints to the court only after resolution efforts have failed.”\n\nLocal courts (First Instance Court) in the UAE basically consider “cases, authentications and all urgent matters related to disputes among the people as well as the safeguard of their rights, security and safety. It also undertakes forcible judicial execution for execution deed stipulated by law, as well as executions by deputation or reference”.\n\nSharia Courts\n\nSharia courts focus on civil disputes in which the parties are Muslims. They depend on the juristic provisions provided for in the Quran the Hadith to render decisions. “The Sharia court may, at the federal level only (which, as mentioned earlier, excludes Dubai and Ras Al Khaimah), also hear appeals of certain criminal cases including rape, robbery, driving under the influence of alcohol and related crimes, which were originally tried in lower criminal courts.\"\n\nCivil Courts\n\nThe jurisdiction of civil courts is generic as they can consider urgent cases and petitions of all types, including petitions of attachments, orders of payment and complaints. A civil Court also \"deals with financial and corporeal rights and legal positions meant to protect by virtue of it, it does not includes commercial, realty, labor, personal status or endowment or inheritance cases\".\n\nThe DIFC Courts are an independent English language common law judiciary, with jurisdiction governing civil and commercial disputes nationally, regionally and worldwide. The Courts began operations in 2006.\n\nOriginally, the jurisdiction of the DIFC Courts was limited to the geographical area of the DIFC. On October 2011, the signing of Dubai Law No 16. allowed the DIFC Courts to hear any local or international cases and to resolve commercial disputes with the consent of all parties.\n\nEstablished in accordance with Abu Dhabi Law No (4) of 2013, ADGM Courts are broadly modelled on the English judicial system. The common law of England, including the principles and rules of Equity, apply and form part of the law of the Abu Dhabi Global Market. The Regulations for ADGM Courts were also drawn from Scots and Australian Federal law and have been tailored specifically to meet the requirements of ADGM Courts. The direct application of English law makes ADGM the first jurisdiction in the Middle East to adopt a similar approach to that of Singapore and Hong Kong.\n\nNeither DIFC nor ADGM courts have jurisdiction in criminal matters.\n\nDubai and Ras Al Khaimah have two independent legal systems. For instance, Dubai’s court system three stages: The Court of First Instance, the Court of Appeal, and the Court of Cassation. Generally speaking, Dubai and Ras Al Khaimah courts consider local disputes concerning property and domestic disputes. However, it can also enforce foreign judgments, arbitration awards, and awards from other tribunals such as the dispute resolution bodies within the Free Zones & Special Economic Zones. The legal systems of the two emirates are different from one another at several levels, but there is at least one important similarity which that of being independent from the federal system, and from the authority of the federal Supreme Court \n\nEstablished pursuant to Local Decree No.26 of 2013, the RDC has almost exclusive jurisdiction to settle disputes arising between landlords and tenants in Dubai, as per Dubai Law No.26 of 2007 Regulating Relationship between Landlords and Tenants in the Emirate of Dubai.\n\nThere are various Alternative Dispute Resolution (ADR) mechanisms available for individuals and business in the UAE.\n\nArbitration has long been recognized as a dispute resolution mechanism in the region, and one that has been quoted in Islamic religious texts. There has been numerous initiatives in the UAE to build up a strong presence in international commercial arbitration, including the codification of modern arbitration rules in federal and other laws.\n\nFirst created in 1994 as the \"Centre for Commercial Conciliation and Arbitration\", the current DIAC is non-profit institution located in the Dubai Chamber of Commerce & Industry (DCCI).\n\nA joint venture between the Dubai International Financial Centre (DIFC) and the London Court of International Arbitration (LCIA). The arbitration rules for the Center are closely modeled on the LCIA’s own rules with some modifications.\n\nIICRA is an international, independent, non-profit organization supporting the Islamic finance industry. The center settles all financial and commercial disputes that arise between financial or business institutions that choose to apply the provisions of Islamic law, Sharia principles, in resolving disputes that arise between these institutions and their clients or between them and third parties through reconciliation or arbitration.\n\nUnder the UAE Federal Labor Law, all unresolved employment disputes must be lodged first at the Ministry of Labour office where a settlement is negotiated between employers and workers. If the negotiations fail, either party may take up the matter at court. Since there are no trade unions in the UAE, collective disputes are handled by a special committee composed of the Minister of Labour, a Supreme Court Judge, and one expert. In Dubai, individuals can also refer to a similar service provided by the General Directorate of Residency and Foreigners Affairs as a first step before going to court. In 2015, a new department dedicated to resolving disputes between domestic helpers and sponsors has opened at the General Directorate of Residency and Foreigners Affairs (GDRFA) branch in Al Aweer. The department, a sub-branch of the Follow up on Illegals and Foreigners Sector, handles complaints from domestic helpers in Dubai. \n\nIn Dubai, the General Department of Human Rights at Dubai Police receives individual and collective complaints filed by workers against their employers. Complaints would be related to living conditions, wages, and security and safety of labour accommodations and this service is accessible online. \n\nAs to civil servants in Dubai, employment complaints can be filed at the office of the Ruler of Dubai, which will attempt to find an agreement between the two parties. If it fails, it will issue the employee a letter addressed to the Dubai courts regarding the complaint. In 2015, the Crown Prince of Dubai has approved the establishment of the Dubai government staff central grievances committee. Members of the committee include a representative from the General Secretariat of the Executive Council of the Emirate of Dubai, the higher legislation committee and the department of human resources. \n\nDisputes that fall under the jurisdiction of the Centre are not registered at the courts unless they are submitted first and a referral is issued. Claims are handled by experienced mediators under the supervision of a judge. Once a settlement is reached, the legally enforceable agreement will be signed by the parties and attested by a judge.\n\nThe Family Guidance Department forms an essential part of the family court. All divorce applications are handled first through the department where experienced mediators work to resolve the issues between couples and family members.\n\nThe Commercial Compliance and Consumer Protection Sector in Dubai Economy aims at raising the awareness and by commercial commitment, providing a safe and encouraging economic environment for businesses and investors, and increasing the awareness and trust of the consumer in the Emirate of Dubai. The organizational units in the Commercial Compliance and Consumer Protection Sector work to provide services to the consumers, businesses and the intellectual property owners. The organizational units also organize the practice of economic activity in the Emirate through inspection visits to the markets and raise the awareness of consumers and businesses through workshops and different awareness programs.\n\nThe Consumer Protection Department is responsible for protecting the rights of the consumers and educating them about their rights and responsibilities by providing consumer complaints services, organizing workshops and awareness programs aiming at enabling consumers to comply with the consumer laws and to follow the proper instructions that may preserve and guarantee the rights of the consumer and the business.\nConsumer Complaints Department:\nThe Consumer Complaints Department receives complaints, inquiries and remarks of the consumers against the commercial establishments in the Emirate of Dubai. Complaints received by the staff of Dubai Economy, and considered in accordance with the consumer protection law, and the procedures in force at Department of Economic Development.\n\nDubai Consumer Application – download it from Play Store or App Store \nAligning with directions of His Highness Sheikh Mohammed, bin Rashid Al Maktoum, Vice President and Prime Minister of the UAE, Ruler of Dubai, to provide government services with the best quality and time efficiency using the latest Artificial Intelligence systems. (SMART Protection), an artificial intelligence system that is the first of its kind in the world, has been developed to protect consumers and introduce them to their rights and obligations. The system uses the direct dialogue method to understand the details of the Complaint, process it, analyze the data and respond to the consumer with the applicable policies and laws in a simplified and immediate manner within a few minutes without human interaction by the specialists. At the end of the conversation, the consumer gets an enforceable letter to empower the consumers and enable them to communicate directly with the companies and obtain solutions. The system aims to increase consumers’ confidence in local markets by enabling them to solve consumer complaints in only 5 minutes and at high quality at any time, place and around the clock.\n\n• Call center 600545555 (solving problems in 4 working days) \n• Instagram :dubai_consumers\n• Twitter :dubai_consumers\n• Face Book dubai_consumers\n• We Chat :\n亲爱的消费者，微信号+971 55 954 0619请添加。\n謝謝你 歡迎來到迪拜\n\nMediation is one of the legal services provided by the Chamber. The value of mediation cases which were settled by the chamber during 2015 reached Dh18.3 million ($4.9 millions). The first mediation smart app in the Middle East was announced in 2015.\n\nThe Small Claims Tribunal (SCT) of the DIFC Courts, features a formal session of mediation as part of its procedure. Approximately 90% of applications before the SCT settle at the “consultation” phase which is a mandatory court–guided mediation session. Only if the parties are unable to reach a settlement will a judge of the DIFC Courts go on to hold a hearing and deliver a Court judgment. SCT proceedings are confidential and parties are not normally legally represented.\n\nSince early years following its independence, the UAE has tried to build its judicial framework. To achieve this goal, the UAE has worked on several levels. Training people who would be judges, lawyers, clerks and others was one very important level.\n\nThe Institute of Training and Judicial Studies (ITJS) based in Abu Dhabi was established in December 14, 1992. The Cabinet issued Resolution N 14 of 1992 that officially establishes this institute. In 2004, a federal law confirmed the federal status of the institute.\n\nInstitute of Training and Judicial Studies (ITJS) undertakes several missions:\n\nDubai Judicial Institute is a public institute established in 1996 by virtue of law No 1 of 1996, amended by law No 27 of 2009. This institute undertakes it’s the missions under the supervision the Cabinet of the Ruler of Dubai.\n\nAbu Dhabi Judicial Academy was established under Decision No (16) of 2007 issued by His Highness Sheikh Mansour Bin Zayed Al Nahyan, the Chairman of the Judicial Department, the Academy is an institution of higher education and specialized training that trains the Judicial Authority members on all branches of law and judicial majors.\n\nThe DIFC Dispute Resolution Authority Academy (Academy of Law) was established in 2015 as an independent entity to provide quality services to the UAE legal community. Its core functions include training and regulating lawyers, publishing, hosting events for the legal community, and providing free legal advice for people in need. The academy offers comprehensive training to UAE legal professionals from all backgrounds and levels, from experienced English language Common Law practitioners through to newly qualified lawyers and those currently operating in the Arabic speaking civil system. The training faculty includes senior practicing barristers from Dubai and London with extensive experience of training advocates in England and other jurisdictions.\n\nEstablished in 2005, the UAE Branch of CIArb is part of the network of CIArb branches across the world. It offers arbitration related training, education (both in Arabic and English), as well as networking events for legal professionals and arbitrators.\n\nThe Initiatives aim at enhancing the capacities of individuals engaged in the judicial field, and motivating pioneering in legal work. The project consists of a number of programs including the moot court competition, legal scientific research competition, legal scholarships program, program for legal and judicial conferences, and the Dubai International Financial Centre (DIFC) Courts Academy.\n"}
{"id": "1128511", "url": "https://en.wikipedia.org/wiki?curid=1128511", "title": "Kiss of peace", "text": "Kiss of peace\n\nThe kiss of peace is an ancient traditional Christian greeting, sometimes also called the \"holy kiss\", \"brother kiss\" (among men), or \"sister kiss\" (among women). Such greetings signify a wish and blessing that peace be with the recipient, and besides their spontaneous uses they have certain ritualized or formalized uses long established in liturgy. Many denominations use other forms of greeting (besides literal kisses) to serve equivalent purposes; they include handshakes, gestures, and hugs, any of which may be called a sign of peace. \n\nIt was the widespread custom in the ancient western Mediterranean for men to greet each other with a kiss. That was also the custom in ancient Judea and practiced also by Christians.\n\nHowever, the New Testament's references to a holy kiss (, \"en philemati hagio\") and kiss of love (\"en philemati agapēs\") transformed the character of the act beyond a greeting. Such a kiss is mentioned five times in the concluding section of letters in the New Testament:\n\n\nIt has been noted that these mentions of the holy kiss come at the end of these epistles. Since these epistles were addressed to Christian communities they would most probably have been read in the context of their communal worship. If the assemblies for worship already concluded in a celebration of the Eucharist the holy kiss would already have occurred in the position it would later occupy in most ancient Christian liturgical tradition (with the exception of the Roman Rite), namely after the proclamation of the Word and at the beginning of the celebration of the Eucharist.\n\nThe writings of the early church fathers speak of the holy kiss, which they call \"a sign of peace\", which was already part of the Eucharistic liturgy, occurring after the Lord's Prayer in the Roman Rite and the rites directly derived from it. St. Augustine, for example, speaks of it in one of his Easter Sermons: \n\nAugustine's Sermon 227 is just one of several early Christian primary sources, both textual and iconographic (i.e., in works of art) providing clear evidence that the \"kiss of peace\" as practiced in the Christian liturgy was customarily exchanged for the first several centuries, not mouth to cheek, but mouth to mouth (note that men were separated from women during the liturgy) for, as the primary sources also show, this is how early Christians believed Christ and his followers exchanged their own kiss. For example, In his \"Paschale carmen\" (ca. 425-50), Latin priest-poet Sedulius condemns Judas and his betrayal of Christ with a kiss thus, \"And leading that sacrilegious mob with its menacing swords and spikes, you press your mouth against his, and infuse your poison into his honey?\" The kiss of peace was known in Greek from an early date as \"eirḗnē\" () (\"peace\", which became \"pax\" in Latin and peace in English). The source of the peace greeting is probably from the common Hebrew greeting shalom; and the greeting \"Peace be with you\" is similarly a translation of the Hebrew shalom aleichem. In the Gospels, both greetings were used by Jesus - e.g. ; , . The Latin term translated as \"sign of peace\" is simply \"pax\" (\"peace\"), not \"signum pacis\" (\"sign of peace\") nor \"osculum pacis\" (\"kiss of peace\"). So the invitation by the deacon, or in his absence by the priest, \"Let us offer each other the sign of peace\", is in Latin: \"Offerte vobis pacem\" (\"Offer each other peace\" or \"Offer each other the peace\").\n\nFrom an early date, to guard against any abuse of this form of salutation, women and men were required to sit separately, and the kiss of peace was given only by women to women and by men to men.\n\nThe practice remains a part of the worship in traditional churches, including the Episcopal Church,Roman Catholic Church, Eastern Catholic Churches, Eastern Orthodox churches, Oriental Orthodox churches; some liturgical mainline Protestant denominations; and Spiritual Christian, where it is often called the kiss of peace, sign of peace, Holy kiss or simply peace or pax; It is practiced as a part of worship in many Anabaptist heritage groups including Old German Baptist Brethren, and Apostolic Christian.\n\nIn the Catholic Church, the term now used is not \"the kiss of peace\", but \"the sign of peace\" or \"the rite of peace\". The \"General Instruction of the Roman Missal\" states: \"There follows the Rite of Peace, by which the Church entreats peace and unity for herself and for the whole human family, and the faithful express to each other their ecclesial communion and mutual charity before communicating in the Sacrament.\" The priest says or sings: \"The peace of the Lord be with you always\", to which the people respond: \"And with your spirit.\" Then, as stated in the Roman Missal, \"if appropriate, the Deacon, or the Priest, adds: 'Let us offer each other the sign of peace.'\"\n\nIn the Roman Rite, it is placed after the Pater Noster and before the Fractio Panis. Even within the Catholic Church, there are liturgical rites (the Ambrosian Rite and the Mozarabic Rite) in which it is placed after the Liturgy of the Word, before the gifts for consecration are put on the altar. The latter placing is influenced by the recommendation in about seeking reconciliation with another before completing an offering at the altar. It was a practice in Rome itself at the time of Justin Martyr in the middle of the 2nd century. In the 3rd century the present placing was chosen not only in Rome but also in other parts of the West such as Roman Africa, where Saint Augustine understood it as related to the petition, \"Forgive us our trespasses as we forgive those who trespass against us\", in the Lord's Prayer and to the link between being in communion with the body of Christ understood as the Church and receiving communion with the body of Christ in the Eucharist.\n\nIn the Tridentine Mass form of the Roman Rite, the sign of peace is given at Solemn Masses alone and is exchanged only among the clergy (unless emperors, kings or princes were present, in which case they, too, received the greeting. If several members of a royal family were present, at least the sovereign received the greeting). It is given by extending both arms in a slight embrace with the words \"Pax tecum\" (Peace be with you), first by the priest celebrant to the deacon, who in turn gives it to the subdeacon, who gives the sign to any other clergy present in choir dress.\n\nIn the Roman-Rite revised in 1969, the sign of peace is used at most Masses but is not obligatory. It is exchanged between all present in no prescribed order, except that \"the Priest gives the sign of peace to a Deacon or minister\". The manner prescribed is as follows: \"It is appropriate that each one give the sign of peace only to those who are nearest and in a sober manner. The Priest may give the sign of peace to the ministers but always remains within the sanctuary, so as not to disturb the celebration. He does likewise if for a just reason he wishes to extend the sign of peace to some few of the faithful.\"\n\nThe following are considered abuses by the Sacred Congregation for Divine Worship and the Sacraments:\n\nThe gesture by which the sign of peace is exchanged is to be determined by the local episcopal conference. In some countries, such as the United States, the conference has laid down no rules, and the everyday handshake is generally used, while in other countries, such as India and Thailand, a bow is prescribed. A 2014 letter of the Congregation for Divine Worship and the Discipline of the Sacraments recommended that conferences choose gestures more appropriate than \"familiar and profane gestures of greeting\".\n\nIn the Eastern Orthodox Church's Divine Liturgy of St. John Chrysostom, the exchange of the peace occurs at the midpoint of the service, when the scripture readings have been completed and the Eucharistic prayers are yet to come. The priest announces, \"Let us love one another that with one accord we may confess--\" and the people conclude the sentence, \"Father, Son, and Holy Spirit, the Trinity, one in essence and undivided.\" At that point the Kiss of Peace is exchanged by clergy at the altar, and in some churches among the laity as well (the custom is being reintroduced, but is not universal). Immediately after the peace, the deacon cries \"The doors! The doors!\"; in ancient times, the catechumens and other non-members of the church would depart at this point, and the doors would be shut behind them. At that, worshippers then recite the Nicene Creed. In the Eastern Orthodox Liturgy, the Kiss of Peace is preparation for the Creed: \"Let us love one another that we may confess...the Trinity.\"\n\nIn the early centuries the kiss of peace was exchanged between the clergy: clergy kissing the bishop, lay men kissing laymen, and women kissing the women, according to the Apostolic Constitutions. Today the kiss of love is exchanged between concelebrating priests. Such has been the case for centuries. In a few Orthodox dioceses in the world in the last few decades, the kiss of peace between laymen has attempted to be reinstituted, usually as a handshake, hugging or cheek kissing.\n\nAnother example of an exchange of the peace is when, during the Divine Liturgy, the Priest declares to the people \"Peace be with all\", and their reply: \"And with your Spirit\". More examples of this practice may be found within Eastern Orthodoxy, but these are the most prominent examples.\n\nIn the Anglican church it is common practice at more formal services for the congregation to be invited to \"offer one another a sign of peace\". However, this is usually a handshake although married couples may kiss one another instead.\n\nDifferent Protestant and Reformed churches have readopted the holy kiss either metaphorically (in that members extend a pure, warm welcome that is referred to as a \"holy kiss\") or literally (in that members kiss one another). This practice is particularly important among many Anabaptist sects.\n\n\n"}
{"id": "53659730", "url": "https://en.wikipedia.org/wiki?curid=53659730", "title": "Lifting property", "text": "Lifting property\n\nIn mathematics, in particular in category theory, the lifting property is a property of a pair of morphisms in a category. It is used in homotopy theory within algebraic topology to define properties of morphisms starting from an explicitly given class of morphisms. It appears in a prominent way in the theory of model categories, an axiomatic framework for homotopy theory introduced by Daniel Quillen. It is also used in the definition of a factorization system, and of a weak factorization system, notions related to but less restrictive then the notion of a model category. Several elementary notions may also be expressed using the lifting property starting from a list of (counter)examples.\n\nA morphism \"i\" in a category has the \"left lifting property\" with respect to a morphism \"p\", and \"p\" also has the \"right lifting property\" with respect to \"i\", sometimes denoted formula_1 or formula_2, iff the following implication holds for each morphism \"f\" and \"g\" in the category:\n\nThis is sometimes also known as the morphism \"i\" being \"orthogonal to\" the morphism \"p\"; however, this can also refer to\nthe stronger property that whenever \"f\" and \"g\" are as above, the diagonal morphism \"h\" exists and is also required to be unique.\n\nFor a class \"C\" of morphisms in a category, its \"left orthogonal\" formula_9 or formula_10 with respect to the lifting property, respectively its \"right orthogonal\" formula_11 or formula_12, is the class of all morphisms which have the left, respectively right, lifting property with respect to each morphism in the class C. In notation,\n\nTaking the orthogonal of a class \"C\" is a simple way to define a class of morphisms excluding non-isomorphisms from \"C\", in a way which is useful in a diagram chasing computation.\n\nThus, in the category Sets of sets the right orthogonal formula_14 of the simplest non-surjection formula_15, is the class of surjections. The left and right orthogonals of formula_16, the simplest non-injection, are both precisely the class of injections, formula_17\n\nIt is clear that formula_18 and formula_19. The class formula_20 is always closed under retracts, pullbacks, (small) products (whenever they exist in the category) and composition of morphisms, and contains all isomorphisms of C. Meanwhile, formula_21 is closed under retracts, pushouts, (small) coproducts and \ntransfinite composition (filtered colimits) of morphisms (whenever they exist in the category), and also contains all isomorphisms.\n\nA number of notions can be defined by passing to the left or right orthogonal several times starting from a list of explicit examples, i.e. as formula_22, where formula_23 is a class consisting of several explicitly given morphisms. A useful intuition is to think that the property of left-lifting against a class \"C\" is a kind of negation\nof the property of being in \"C\", and that right-lifting is also a kind of negation. Hence the classes obtained from \"C\" by taking orthogonals an odd number of times, such as formula_24 etc., represent various kinds of negation of \"C\", so formula_24 each consists of morphisms which are far from having property formula_23.\n\nA map formula_27 has the \"path lifting property\" iff formula_28 where \nformula_29 is the inclusion of one end point of the closed interval into the interval formula_30.\n\nA map formula_31 has the homotopy lifting property iff formula_32 where formula_33 is the map formula_34.\n\nFibrations and cofibrations.\n\nIn Sets, \n\nIn the category \"R\"-mod of \"R\"-modules over a commutative ring \"R\",\n\nIn the category of groups, \n"}
{"id": "233050", "url": "https://en.wikipedia.org/wiki?curid=233050", "title": "Listening", "text": "Listening\n\nListening is giving one's attention to sound, for example speech, music or natural sounds. Listening involves complex affective, cognitive, and behavioral processes. Effective processes include the motivation to attend to others; cognitive processes include attending to, understanding, receiving, and interpreting content and relational messages; and behavioral processes include responding with verbal and nonverbal feedback.\n\nListening differs from obeying. A person who receives and understands information or an instruction, and then chooses not to comply with it or to agree to it, has listened to the speaker, even though the result is not what the speaker wanted. Listening is a term in which the listener listens to the one who produced the sound to be listened.\n\nSemiotician Roland Barthes characterized the distinction between listening and hearing as \"Hearing is a physiological phenomenon; listening is a psychological act.\" Hearing is always occurring, most of the time subconsciously. In contrast, listening is the interpretative action taken by the listener in order to understand and potentially make meaning out of the sound waves. Listening can be understood on three levels: alerting, deciphering, and an understanding of how the sound is produced and how the sound affects the listener.\n\nAlerting, the first level, is the detection of environmental sound cues. While discussing this level, Barthes mentions the idea of territory being demarcated by sounds. This is best explained using the example of one's home. One's home, for instance, has certain sounds associated with it that make it familiar and comfortable. An intrusion sound (e.g. a squeaking door or floorboard, a breaking window) alerts the dweller of the home to the potential danger.\n\nDeciphering, the second level, describes detecting patterns in and interpreting sounds. An example of this level is that of a child waiting for the sound of his mother's return home. In this scenario the child is waiting to pick up on sound cues (e.g. jingling keys, the turn of the doorknob, etc.) that will mark his mother's approach.\n\nUnderstanding, the third level of listening, means knowing how what one says will affect another. This sort of listening is important in psychoanalysis. Barthes states that the psychoanalyst must turn off their judgement while listening to the analysand in order to communicate with their patient's unconscious in an unbiased fashion.\n\nHowever, in contrast to the distinct levels of listening listed above, it must be understood that they all function within the same plane, and sometimes all at once. Specifically the second and third levels, which overlap vastly, can be intertwined in that obtaining, understanding and deriving meaning are part of the same process. In that the child, upon hearing the doorknob turn (obtaining), can almost automatically assume that someone is at the door (deriving meaning).\n\nAlong with speaking, reading, and writing, listening is one of the \"four skills\" of language learning. All language teaching approaches except for grammar-translation incorporate a listening component. Some teaching methods, such as Total Physical Response, involve students simply listening and responding.\n\nA distinction is often made between \"intensive listening\", in which learners attempt to listen with maximum accuracy to a relatively brief sequence of speech, and \"extensive listening\", in which learners listen to lengthy passages for general comprehension. While intensive listening may be more effective in terms of developing specific aspects of listening ability, extensive listening is more effective in building fluency and maintaining learner motivation.\n\n\n"}
{"id": "2299327", "url": "https://en.wikipedia.org/wiki?curid=2299327", "title": "Mayhem (crime)", "text": "Mayhem (crime)\n\nMayhem is a common law criminal offense consisting of the intentional maiming of another person.\n\nUnder the law of England and Wales and other common law jurisdictions, it originally consisted of the intentional and wanton removal of a body part that would handicap a person's ability to defend themself in combat. Under the strict common law definition, initially this required damage to an eye or a limb, while cutting off an ear or a nose was deemed not sufficiently disabling. Later the meaning of the crime expanded to encompass any mutilation, disfigurement, or crippling act done using any instrument.\n\nIn England and Wales, it has fallen into disuse. In 1992 the Law Commission recommended that it be abolished, and in 1998 the Home Office proposed to abolish it, in the course of codifying the law relating to offences against the person.\n\nThe most significant change in common-law mayhem doctrine came in 1697, when the King's Bench decided \"Fetter v. Beale\", 91 Eng. Rep. 1122. There, the plaintiff recovered in a battery action against a defendant. Shortly thereafter, \"part of his skull by reason of the said battery came out of his head\", and the plaintiff brought a subsequent action under mayhem. Though \"Fetter\" is also known as an early example of res judicata, it is most significant for expanding the ambit of mayhem to include \"loss of the skull\".\n\nIn modern times, the offense of mayhem has been superseded in many jurisdictions by statutorily defined offenses such as aggravated battery.\n\nModern statutes in the U.S. define mayhem as disabling or disfiguring, such as rendering useless a member of another person's arms or legs. The injury must be permanent, not just a temporary loss. Some courts will hold even a minor battery as mayhem if the injury is not minor. Mayhem in the U.S. is a felony.\n\nBoth the noun \"mayhem\" and the verb \"maim\" come from Old French via Anglo-Norman. The word is first attested in various Romance languages in the 13th century, but its ultimate origin is unclear. For one hypothesis about its origin, see the Wiktionary entry for this word.\n\nMayhem can describe a person going on a rampage. Popular misunderstanding of the common journalese expression \"rioting and mayhem\" caused the common usual modern use of \"mayhem\" to mean \"havoc and disorder\", often with humorous overtones.\n\n"}
{"id": "26530571", "url": "https://en.wikipedia.org/wiki?curid=26530571", "title": "Mental operations", "text": "Mental operations\n\nMental operations are operations that affect mental contents. Initially, operations of reasoning have been the object of logic alone. Pierre Janet was one of the first to use the concept in psychology. Mental operations have been investigated at a developmental level by Jean Piaget, and from a psychometric perspective by J. P. Guilford. There is also a cognitive approach to the subject, as well as a systems view of it.\n\nSince Antiquity, mental operations, more precisely, formal operations of reasoning have been the object of logic.\n\nIn 1903, Pierre Janet described two types of mental operations:\nJean Piaget differentiated a preoperational stage, and operational stages of cognitive development, on the basis of presence of mental operations as an adaptation tool.\n\nJ. P. Guilford's Structure of Intellect model described up to 180 different intellectual abilities organized along three dimensions—Operations, Content, and Products.\n\nAccording to \nmost logicians, the three primary mental operations are apprehension (understanding), judgement, and inference.\n\nApprehension is the mental operation by which an idea is formed in the mind. If you were to think of a sunset or a baseball, the action of forming that picture in your mind is apprehension. The verbal expression of apprehension is called a term.\n\nJudgment is the mental operation by which we predicate something of a subject. Were you to think, \"That sunset is beautiful\" or \"Baseball is the all-American sport\" is to make a judgment. The verbal expression of judgment is the statement (or proposition).\n\nInference (or reasoning) is the mental operation by which we draw conclusions from other information. If you were to think, \"I like to look at that sunset, because I enjoy beautiful things, and that sunset is beautiful\" you would be reasoning. The verbal expression of reasoning is the logical argument.\n\nJean Piaget identifies several mental operations of the concrete operational stage of cognitive development:\n\n\nPiaget also describes a formal operational stage, with formal operations of abstract thinking: hypothesizing, hypothesis testing, and deduction.\n\nAccording to J. P. Guilford's Structure of Intellect (SI) theory, an individual's performance on intelligence tests can be traced back to the underlying mental abilities or factors of intelligence. SI theory comprises multiple intellectual abilities organized along three dimensions—Operations, Content, and Products.\n\n\nSI includes six operations or general intellectual processes:\nCognition—The ability to understand, comprehend, discover, and become aware of information.\nMemory recording—The ability to encode information.\nMemory retention—The ability to recall information.\nDivergent production—The ability to generate multiple solutions to a problem; creativity.\nConvergent production—The ability to deduce a single solution to a problem; rule-following or problem-solving.\nEvaluation—The ability to judge whether or not information is accurate, consistent, or valid.\n\n\nSI includes five broad areas of information to which the human intellect applies the six operations:\nVisual—Information perceived through seeing.\nAuditory—Information perceived through hearing.\nKinesthetic -through actions\nSymbolic—Information perceived as symbols or signs that have no meaning by themselves; e.g., Arabic numerals or the letters of an alphabet.\nSemantic—Information perceived in words or sentences, whether oral, written, or silently in one's mind.\nBehavioral—Information perceived as acts of people.\n\n\nAs the name suggests, this dimension contains results of applying particular operations to specific contents. The SI model includes six products, in increasing complexity:\nUnits—Single items of knowledge.\nClasses—Sets of units sharing common attributes.\nRelations—Units linked as opposites or in associations, sequences, or analogies.\nSystems—Multiple relations interrelated to comprise structures or networks.\nTransformations—Changes, perspectives, conversions, or mutations to knowledge.\nImplications—Predictions, inferences, consequences, or anticipations of knowledge.\n\nTherefore, according to Guilford there are 6 x 5 x 6 = 180 intellectual abilities or factors. Each ability stands for a particular operation in a particular content area and results in a specific product, such as Comprehension of Figural Units or Evaluation of Semantic Implications.\n\nFollowing on the footsteps of Silvio Ceccato, Giulio Benedetti describes several types of mental operations:\n\nTaking into account all mental processes, the following types of mental operations have been described:\n\n"}
{"id": "29486162", "url": "https://en.wikipedia.org/wiki?curid=29486162", "title": "Mokshopaya", "text": "Mokshopaya\n\nThe Mokṣopāya or Mokṣopāyaśāstra is a Sanskrit philosophical text on salvation for non-ascetics (\"mokṣa-upāya\": 'means to liberation'), written on the Pradyumna hill in Śrīnagar in the 10th century AD. It has the form of a public sermon and claims human authorship and contains about 30,000 śloka's (making it longer than the \"Rāmāyaṇa\"). The main part of the text forms a dialogue between Vasiṣṭha and Rāma, interchanged with numerous short stories and anecdotes to illustrate the content. This text was later (11th to the 14th century AD) expanded and vedanticized, which resulted in the \"Yogavāsiṣṭha\".\n\nThe text of the \"Mokṣopāya\" shows that a unique philosophy has been created by the author. It taught a monism ('advaita') that is different from Advaita Vedanta. It makes use of other Darśanas in an inclusive way. The text teaches that the recognition that cognitive objects are non-existent, leads to ultimate detachment, which causes an attitude of \"dispassion and non-involvement with worldly things and matters\", though still fulfilling one's daily duties and activities. This liberation is available for everyone, no matter their sex, caste or education, as long as one uses reason and maintains an active life in this world. To reach this liberation, one has to go through three stages: rational thinking and discernment (\"vicāra\"), true understanding (\"jñāna\") and detachment (\"vairāgya\").\n\nIt is only by one's own effort (\"pauruṣa\") that one can be liberated from the bonds of existence. For one who knows the reality, \"fate\" (\"daiva\") does not mean anything, something like \"fate\" does not exist and has, accordingly, no consequences at all.\n\nThe Mokṣopāya Project supervised by professor Walter Slaje at the Martin Luther University of Halle-Wittenberg in Germany is currently working on a critical edition of the \"Mokṣopāya\". The project is embedded in the Centre for Research in the Historiography and Intellectual Culture of Kashmir (under the Patronage of the Academy of Sciences and Literature, Mainz). A commentary by Bhāskarakaṇṭha (\"Mokṣopāya-ṭīkā\"; late 17th century AD) and more than thirty manuscripts in Nāgarī, Śāradā, Grantha, and Telugu scripts are being used.\n\nThe critical edition of the complete Sanskrit text is expected to be finalized by the end of 2018. It will be accompanied by a German translation, a philological commentary and a dictionary of its Sanskrit vocabulary (all in German).\n\n\n\n\n\n\n"}
{"id": "38706734", "url": "https://en.wikipedia.org/wiki?curid=38706734", "title": "Myth of superabundance", "text": "Myth of superabundance\n\nThe myth of superabundance is the belief that earth has more than sufficient natural resources to satisfy humanity's needs, and that no matter how much of these resources humanity uses, the planet will continuously replenish the supply. Although the idea had existed previously among conservationists in the 19th century, it was not given a name until Stewart Udall's 1964 book \"The Quiet Crisis\".\n\nUdall describes the myth as the belief that there was \"so much land, so much water, so much timber, so many birds and beasts\" that man did not envision a time where the planet would not replenish what had been sowed. The myth of superabundance began to circulate during Thomas Jefferson's presidency at the beginning of the nineteenth century and persuaded many Americans to exploit natural resources as they pleased with no thought of long-term consequences. According to historian of the North American west George Colpitts, \"No theme became as integral to western promotion as natural abundance.\" Especially with respect to the west after 1890, promotional literature encouraged migration by invoking the idea that God had provided an abundant environment there such that no man or family would fail if they sought to farm or otherwise live off the land out west. Since at that time environmental science and the study of ecology barely allowed for the possibility of animal extinction and did not provide tools for measuring biomass or the limits of natural resources, many speculators, settlers, and other parties participated in unsustainable practices that led to various extinctions, the Dust Bowl phenomenon, and other environmental catastrophes.\n\nIn 1784, John Filson wrote \"The Discovery, Settlement And present State of Kentucke\", which included the chapter \"The Adventures of Colonel Daniel Boon\". This work represents one of the earliest instance of the myth of superabundance, acting as something of a promotional ad enticing settlers to Kentucky based on the abundance of resources to be found there.\n\nUdall describes many large-scale impacts on natural resources, terming them \"The Big Raid on resources\". The first was the need for lumber in a growing nation for fuel, housing and paper. Udall states that it was with this first big raid on the earth’s natural resources that the myth of superabundance began to show its fallacy. It was only towards the end of the nineteenth century that people were awakened to the empty hillsides and the vastness of blackened woods from the lumber industry. Petroleum followed, as it was widely believed that oil was constantly made inside the earth, and so, like everything else, was inexhaustible. Then came seal hunting, and by 1866 the seal population that originally numbered approximately five million was drastically cut in half. Many of the seals were shot in the water and never recovered, allowing for enormous waste. The Fur Seal Treaty which came about in 1911 saved the seals from becoming the first major marine species to become extinct thanks to the myth of superabundance.\n\nThe passenger pigeon was the largest wildlife species known to humanity in the early nineteenth century, when the bird's population was estimated at about five billion. By the early 20th century, due to overhunting and habitat destruction brought about by the timber industry, the species had become extinct, the last passenger pigeon having died in the Cincinnati Zoo. The passenger pigeon became extinct in under a century and was just one of the many victims of the myth of superabundance.\n\nLikewise, the American buffalo was threatened by the myth of superabundance. They were considered to be the largest and most valuable resource because just about every piece of them was usable. The big kill of the buffalo began at the end of the Civil War when armies wanted the animals killed in order to starve out the Plains Indians. Railroad men wanted them killed in order to supply heavier and profitable loads of hides. Buffalo were killed for their tongues and hides, and some hunters simply wanted them as trophies. Pleas of protection for the buffalo were ignored, nearly wiping out the species.\n\nThe Great Leap Forward in China in 1958 corresponded closely with the myth of superabundance; economic planners reduced the acreage space for planting wheat and grains, trying to force farmers and agricultural labourers into accepting new forms of industry. As a result, production of wheat and grain was slowed dangerously, and floods in the South and droughts in the North struck in 1959, leading China into the record-breaking Great Chinese Famine.\n\nGeorge Perkins Marsh, who wrote \"Man and Nature\" in 1864, rejected the idea that any resource could be exploited without any concerns for the future. Perkins was a witness to natural destruction; he saw that mistakes of the past were destroying the present prosperity. He believed that nature should be second nature to all and should not be used as an exploitation for economics and politics. He was, after all, \"forest born\". Man's role as a catalyst of change in the natural world intrigued him. He believed that progress was entirely possible and necessary, if only men used wisdom in the management of resources. He deflated, but did not destroy the myth of superabundance. He began the spin into doubt, which made way for John Muir in 1874. Muir, who had grown up surrounded by wilderness, believed that wildlife and nature could provide people with heightened sense abilities and experiences of awe that could be found nowhere else. Entering into civilization with a desire to see preservation of some of what he believed to be America’s most beautiful nature, he built upon steps that had been taken by Frederick Law Olmsted, a young landscape architect who designed Central Park in New York City. Olmsted had persuaded Congress to pass a bill preserving much of Yosemite Valley, which President Lincoln had then approved in 1864. In 1872 President Grant signed the Yellowstone Park bill, saving over two million acres of wildlife.\n\nMuir saw overgrazing destruction in Yosemite, in parts of it that were not under protection. It was a result of nearby sheepmen and their herds. In 1876, Muir wrote an article \"God’s First Temples – How Shall We Preserve Our Forests\", which he published in the newspaper, pleading for help with protection of the forests. At first he failed against the overriding ideal of the myth of superabundance, but he did inspire bills in the 1880s that sought to enlarge Yosemite’s reservation. Muir formed the Sierra Club, a group of mountaineers and conservationists like him who had responded to his many articles. The Sierra Club’s first big fight came as a counter-attack on lumbermen and stockmen who wanted to monopolize some of Yosemite County. Yosemite Valley, which was still owned by the state, was mismanaged and natural reserves like the meadows and Mirror Lake, which was dammed for irrigation, were still being destroyed even under supposed protection. In 1895, Muir and the Sierra Club began a battle that would span over ten years, fighting for natural management of Yosemite Valley. Theodore Roosevelt met with Muir in 1903 and was instantly fascinated with Muir’s passion for the wilderness. Roosevelt approved Muir’s argument for Yosemite Valley, and so the Sierra Club took their decade long campaign to Sacramento, where they finally won against California legislature in 1905. With Roosevelt on Muir’s side, Yosemite Valley finally became part of the Yosemite National Park and was allowed natural management.\n\nUdall asserts that the myth of superabundance, once exposed, was replaced in the 20th century by the \"myth of scientific supremacy\": the belief that science can eventually find a solution to any problem. This leads to behaviors which, while recognizing that resources are not infinite, still fail to properly preserve those resources, putting the problem off to future generations to solve through science. \"Present the repair bill to the next generation\" is their silent motto. George Perkins Marsh had said that conservation's greatest enemies were \"greed and shortsightedness\". Men reach a power trip thinking they can manipulate nature the way that they want.\n\nIn order for man to live harmoniously with nature, as Muir and Perkins and many others have fought for, Patsy Hallen in the article, \"The Art of Impurity\" says that an ethics development must occur in which respect for nature and our radical dependency on it can take place. Humans see themselves as superior to nature, and yet we are in a constant state of continuity with it. Hallen argues that humanity cannot afford such an irrational state of mind and ecological denial if it expects to prosper in the future.\n"}
{"id": "18254861", "url": "https://en.wikipedia.org/wiki?curid=18254861", "title": "Naturalization of intentionality", "text": "Naturalization of intentionality\n\nAccording to Franz Brentano, intentionality refers to the \"aboutness of mental states that cannot be a physical relation between a mental state and what is about (its object) because in a physical relation each of the relata must exist whereas the objects of mental states might not.\n\nSeveral problems arise for features of intentionality, which are unusual for materialistic relations. Representation is unique. When 'x represents y' is true, it is not the same as other relations between things, like when 'x is next to y' or when 'x caused y' or when 'x met y', etc. Representation is different because, for instance, when 'x represents y' is true, y need not exist. This isn't true when say 'x is the square root of y' or 'x caused y' or 'x is next to y'. Similarly, when 'x represents y' is true, 'x represents z' can still be false, even when y = z. Intentionality encompasses relations that are both physical and mental. In this case, \"Billy can love Santa and Jane can search for unicorns even if Santa does not exist and there are no unicorns.\"\n\nFranz Brentano, the nineteenth century philosopher, spoke of mental states as involving presentations of the objects of our thoughts. This idea encompasses his belief that one cannot desire something unless they actually have a representation of it in their minds.\n\nDennis Stampe was one of the first philosophers in modern times to suggest a theory of content according to which content is a matter of reliable causes.\n\nFred Dretskes book, \"Knowledge and the Flow of Information\" (1981), was a major influence on the development of informational theories, and although the theory developed there is not a teleological theory, Dretske (1986, 1988, 1991) later produced an informational version of teleosemantics. He begins with a concept of carrying information that he calls \"indicating\", explains that indicating is not equivalent to representing, and then suggests that a representation's content is what it has the function of indicating\n\nTeleosemantics, also known as biosemantics, is used to refer to the class of theories of mental content that use a teleological notion of function. Teleosemantics is best understood as a general strategy for underwriting the normative nature of content, rather than any particular theory. What all teleological theories have in common is the idea that semantic norms are ultimately derivable from functional norms.\n\nAttempts to naturalize semantics began in the late 1970s. Many attempts were and still are being made to bring natural-physical explanations to bear on minds and, specifically, to the question of how minds acquire content. This is an interesting question; it is no surprise that it takes center stage in the philosophy of mind. Indeed, it is certainly an interesting question how minds, thought by those in the natural camp to be \"natural physical objects\", could have developed intentional properties. In the mid-1980s, with the works of Ruth Millikan and David Papineau (Language, Thought, and Other Biological Categories and \"Representation and Explanation\", respectively) teleosemantics, a theory of mental content that attempts to address the question of content and intentionality of minds, was born.\n\nRuth Millikan is perhaps the most vocal supporter of the teleosemantic program. Millikan's view differs from other teleosemantic views in myriad ways, but perhaps its most unusual characteristic is its distinction between the mechanisms that produce mental representations from those that consume mental representations. There is a representational function as a whole, at a composite level; and there are two \"sub-functions\", the producer-function and the consumer-function. In terms that are easy to understand, let's take Millikan's own example of beavers splashing their tails. One beaver alerts other beavers to the presence of danger by splashing its tail on the surface of water. The splashing of the tail tells, or represents to, the other beavers that there is danger in the environment, and the other beavers dip into the water to avoid the danger. The splashing of the beaver's tail produces a representation; the other beavers consume the representation. The representation that the beavers consume guides their behavior in ways that relate to their survival.\n\nOf course, the foci of the teleosemantic program is internal representations, and not just representational states of affairs between two (or more) distinct, external entities. How does the picture of the producer and consumer beavers, for instance, play into a story about internal representations? Papineau and Macdonald describe Millikan's account of this well and loyally, saying \"The producing mechanisms will be the sensory and other cerebral mechanisms that give rise to cognitive representations.\" The consuming mechanisms are those that \"use these representations to direct behavior in pursuit of some biological end\". Here, we have a picture similar to the beaver example, but this picture portrays the two sub-functions, producer and consumer, operating within a more-obviously unified system, namely, the cognitive system. One sub-function produces mental representations while the other sub-function consumes them in order to reach some end, e.g., danger-avoidance or food-acquisition. The representations consumed by the consumer sub-function guide an organism's behavior toward some biological end, e.g., survival. This is a rather brief sketch of Millikan's overall portrait. Of course, more goes into her account of the relation between producer- and consumer-functions in order to arrive at a nuanced account of mental representation. But that is a matter of how. Details as to the how aside, much of Millikan's efforts are directed towards the why, viz., why it is that perceivers like us have mental representations—why representations are produced in the first place.\n\nThe theory of asymmetric dependence, from Fodor, who says that his theory \"distinguishes merely informational relations on the basis of their higher-order relations to each other: informational relations depend upon representational relations, but not vice versa. He gives an example of this theory when he says, \"if tokens of a mental state type are reliably caused by horses, cows-on-dark-nights, zebras-in-the-mist and Great Danes, then they carry information about horses, etc. If however, such tokens are caused by cows-on-dark-nights, etc. because they are caused by horses, but not vice versa, then they represent horses (or property horse).\n\n20th century American philosopher Willard Van Orman Quine believed that linguistic terms do not have distinct meanings that accompany them because there are no such entities as \"meanings\". In his books, \"Word and Object\" (1960) and \"Ontological Relativity\" (1968), Quine considers the methods available to a field linguist attempting to translate an unknown language in order to outline his thesis. His thesis, the indeterminacy of translation, notes that there are many different ways to distribute purpose and meanings among words. Whenever a theory of translation is made it is commonly based upon context. An argument over the correct translation of an unidentified term depends on the possibility that the native could have spoken a different sentence. The same problem of indeterminacy would appear in this argument once again since any hypothesis can be defended if one adopts enough compensatory hypotheses about other parts of the language. Quine uses as an example the word \"gavagai\" spoken by a native upon seeing a rabbit. One can go the simplest route and translate the word to \"Lo, a rabbit\", but other possible translations such as \"Lo, food\" or \"Let's go hunting\" are completely reasonable given what the linguist knows. Subsequent observations can rule out certain possibilities as well as questioning the natives. But this is only possible once the linguist has mastered much of the natives' grammar and vocabulary. This is a big problem because this can only be done on the basis of hypotheses derived from simpler, observation-connected bits of language, which admit multiple interpretations, as we have seen.\n\nDaniel C. Dennett's theory of mental content, the intentional stance, tries to view the behavior of things in terms of mental properties. According to Dennett: \n\"Here is how it works: first you decide to treat the object whose behavior is to be predicted as a rational agent; then you figure out what beliefs that agent ought to have, given its place in the world and its purpose. Then you figure out what desires it ought to have, on the same considerations, and finally you predict that this rational agent will act to further its goals in the light of its beliefs. A little practical reasoning from the chosen set of beliefs and desires will in most instances yield a decision about what the agent ought to do; that is what you predict the agent will do.\"\n\nDennett's thesis has three levels of abstraction:\n\nDennett states that the more concrete the level, the more accurate is in principle our predictions. Though if one chooses to view an object through a more abstract level, he will gain greater computational power by getting a better overall picture of the object and skipping over any extraneous details. Also, switching to a more abstract level has its risks as well as its benefits. If we applied the intentional stance to a thermometer that was heated to 500 °C, trying to understand it through its beliefs about how hot it is and its desire to keep the temperature just right, we would gain no useful information. The problem would not be understood until we dropped down to the physical stance to comprehend that it has been melted. Whether to take a particular stance should be decided by how successful that stance is when applied. Dennett argued that it is best to understand human beliefs and desires at the level of the intentional stance.\n\n\n"}
{"id": "41546610", "url": "https://en.wikipedia.org/wiki?curid=41546610", "title": "Nets within Nets", "text": "Nets within Nets\n\nNets within Nets is a modelling method belonging to the family of Petri nets.\nThis method is distinguished from other sorts of Petri nets by the possibility to provide their tokens with a proper structure, which is based on Petri net modelling again. Hence, a net can contain further net items, being able to move around and fire themselves.\n\nNets within nets are well suited for the modelling of distributed systems under the particular aspects of\n\nIn a lot of publications in relation to object-oriented design is given, in order to combine the ability of Petri nets in modelling distributed computing with the modelling of objects, being able to be created and to interact.\n\nStarting from the need of practical applications, by the mid of nineties, different formalisms have been created, which fit the description of „nets within nets“. Lomazova and Schnoebelen are listing\nsome of these approaches, namely by Sibertin-Blanc, Lakos, Moldt und Wienberg as extending Coloured Petri nets, aside the Object Nets of Valk.\nThe earliest use of such hierarchic net models appeared by Rüdiger Valk in Valk and Jessen, where the so-called task-flow nets are introduced in order to model task systems in operating systems. In these models tasks are modelled by a Petri net, which represents the precedences of tasks and their state of execution.\n\nThe most important differences in semantics is given by the execution of net tokens. On the one side net tokens can be references to net items, which case is called „reference semantics“. This kind of semantic is distinguished from value semantics, where net objects may exist in different places and different internal states. In value semantics different copies can be created to model concurrent execution. The corresponding join of such a split can be defined in different ways, as for instance by „distributed token semantics“ or „history process semantics“. In connection with mobile computing hybrid versions of reference and value semantics are of importance. In distributed token semantics the important calculus of place invariants for Petri nets remains valid.\n\nThe formalism of nets within nets would be of few importance without communication between net tokens. Like in object-oriented programming communication of net tokens is introduced via predefined interfaces which are dynamically bound.\n\nIn Figure 1 a Petri net is shown containing a token Petri net in place „a“. The token net can move around from place „a“ to place „b“ and back by firing of the transitions of the outer net. The channel inscriptions at the transitions behave like a call of a method, resulting in the synchronised firing of the calling transition in the outer net [e.g. labelled by x:forth()] and the called transition [e.g. labelled by :forth()] in the token net. The variable „x“ at an arrow is bound to the token net in the place connected with this arrow. The brackets may contain parameters to be passed. This example is so simple that reference and value semantics coincide.\n\nStandard Petri net properties like reachability, boundedness and liveness show a mixed picture. A paper of Köhler-Bußmeier gives a survey on decidability results for elementary object systems.\nTo reduce the complexity of the formalism subclasses have been defined by restricting the structure of the Petri nets, as for instance to state machines. Such restrictions still allow complex modelling of distributed and mobile systems, but have polynomial complexity in model checking.\n\n"}
{"id": "1806782", "url": "https://en.wikipedia.org/wiki?curid=1806782", "title": "New Soviet man", "text": "New Soviet man\n\nThe New Soviet man or New Soviet person ( \"novy sovetsky chelovek\"), as postulated by the ideologists of the Communist Party of the Soviet Union, was an archetype of a person with specific qualities that were said to be emerging as dominant among all citizens of the Soviet Union, irrespective of the country's cultural, ethnic, and linguistic diversity, creating a single Soviet people, Soviet nation.\n\nFrom the ideology's roots in the mid 19th and early 20th century, ideologists of Communism have postulated that within the new society of pure communism and the social conditions therein, a New Man and New Woman would develop with qualities reflecting surrounding circumstances of post-scarcity and unprecedented scientific development.\nFor example, Leon Trotsky wrote in 1924 in \"Literature and Revolution\" about the \"Communist man\", \"man of the future\":\n\nWilhelm Reich asked in 1933: \n\nWill the new socio-economic system reproduce itself in the structure of the people's character? If so, how? Will his traits be inherited by his children? Will he be a free, self-regulating personality? Will the elements of freedom incorporated into the structure of the personality make any authoritarian forms of government unnecessary?\n\nAuthor and philosopher Bernard Byhovsky, Ph.D. writes: \"The new man is endowed, first of all, with a new ethical outlook.\" \n\nThe Soviet man was to be selfless, learned, healthy, muscular, and enthusiastic in spreading the socialist Revolution. Adherence to Marxism-Leninism, and individual behavior consistent with that philosophy's prescriptions, were among the crucial traits expected of the New Soviet man, which required intellectualism and hard discipline. He was not driven by crude impulses of nature but by conscious self-mastery, a belief that required the rejection of both innate personality and the unconscious, which Soviet psychologists therefore rejected.\n\nHe treated public property with respect, as if it were his own. He also has lost any nationalist sentiments, being Soviet rather than Russian, Ukrainian, Belarusian, or any of the many other nationalities found in the USSR.\n\nHis work required exertion and austerity, to show the new man triumphing over his base instincts. Alexey Stakhanov's spurious record-breaking day in mining coal caused him to be set forth as the exemplar of the \"new man\" and the members of Stakhanovite movements tried to become Stakhanovites.\n\nThis could also be a new woman; \"Pravda\" described the Soviet woman as someone who had and could never have existed before. Female Stakhanovites were rarer than male, but a quarter of all trade-union women were designated as \"norm-breaking.\" For the Paris World Fair, Vera Mukhina depicted a momentual sculpture, \"Worker and Kolkhoz Woman\", dressed in work clothing, pressing forward with his hammer and her sickle crossed.\n\nAleksandr Zinovyev put forth the satirical argument that a new kind of person was indeed created by the Soviet system, but held that this new man - which they call \"Homo Sovieticus\" - was in many ways the opposite of the ideal of the New Soviet man.\n\nAmong the major traits of a new Soviet man was selfless collectivism. The selfless new man was willing to sacrifice his life for good causes.\n\nThis trait was glorified from the first Soviet days, as exemplified by lines from the poem \"Vladimir Ilyich Lenin\" by the Soviet poet Vladimir Mayakovsky:\n\n<poem>\nWho needs a \"1\"?\nThe voice of a \"1\"\nWho will hear it?\nA \"1\" is nonsense.\n</poem>\n\nFictional characters and presentations of contemporary celebrities embodying this model were prominent features of Soviet cultural life, especially at times when fostering the concept of the New Soviet man was given special priority by the government.\n\nPro-natalist policies encouraging women to have many children were justified by the selfishness inherent in limiting the next generation of \"new men.\"\n\nIn the 1920s and into the Stalinist era the concept of the \"New Soviet Woman\" served alongside that of the \"New Soviet Man.\" Her roles were vastly different than that of her male counterpart; she was burdened with a complex identity that changed with ideology shifts in the party doctrine toward more conservative notions of the role of the family and the mother in the Soviet system. The New Soviet Woman was a Superwoman who balanced competing responsibilities and took on the burden of multiple roles: Communist citizen, full-time worker, wife and mother.\n\nThe New Soviet Person was generally characterized as male. In Soviet propaganda centered on the New Soviet Person, it was standard for men to be depicted as the primary actors, either battling opponents of the Marxist revolution or rebuilding the world. Women, on the other hand, were often portrayed as \"backward,\" passive beneficiaries of the revolution rather than its securers. This was so not least because the proletarian movement was organized and fought by the working class, which by and large consisted of men. Thus, propaganda often equated male domination with proletariat domination. Although the party leadership claimed the sexes enjoyed equal status under the law, a significant accomplishment in itself, men remained the measure of worth.\n\nThis marginalization of women in the newly developing civil order made it difficult for women to find a place among the proletarian class for which the revolution was fought. Regulations during the New Economic Policy period on the extent to which women could work in dangerous conditions, how many hours they could work in a shift and the kinds of special care they received during maternity made many factory owners reluctant to hire women, despite the Commissariat of Labor's requirements that women to be given equal access to employment.\n\nThere were gains made in combating illiteracy and promoting education for women during the 1920s. Soviet policy encouraged working-class women to attend school and develop vocational skills. There existed opportunities for women to participate in politics, become party members and vie for elected and administrative positions. Access to the political sphere, however, was extremely limited.\n\nJoseph Stalin's policies on women were more conservative than that of his predecessor Lenin. Because he was concerned with a declining population rate, Stalin de-emphasized the Marxist feminist view of women in society, which, in his view, freed women from patriarchy and capitalism. In keeping with the party line, Stalin reasserted the importance of women in the workforce and female education, primarily literacy, although he began to emphasize the role of mother in a way that differed from more radical notions of the early 1920s.\n\nThe \"withering away\" of the family was no longer a goal of economic and political progress. The new party line was that the family, like the state, was to grow stronger with the full realization of socialism. Massive propaganda campaigns linked the joys of motherhood with the benefits of Soviet power. Soviet ideology began to argue that women's public roles were compatible with her roles as wife and mother. In fact, that the two reinforced one another and were both necessary for real womanhood.\n\nThe New Soviet Woman differed greatly from the conceptions of revolutionaries preceding the 1930s. Instead of being freed from domestic concerns, she was bound to them. Though she now filled the role of man's peer in the workplace, she was also obligated to devote herself to being his helpmate in the home. One of the primary roles of the New Soviet Woman was that of mother. This role became of great importance in the wake of population decline beginning in the 1920s. War and revolution had decimated the population. Legislation legalizing abortions and the increasing use of contraception—though still not that widespread—in the 1920s also contributed to the lower population numbers as women began to work more and give birth less.\n\nAs a means to combat that trend, propaganda placed a new emphasis on the female's role as the perpetuator of the Communist regime in their ability to produce the next class of healthy workers, a policy called pronatalism. Propaganda presented pronatalism, a means to encourage women to bear children, in different ways to urban working-class women and to rural peasant women. Propaganda designed for an urban audience linked healthy female sexuality with reproduction while medical information to peasant women positioned conception as the purpose of sexual intercourse.\n\nThe new ideal Soviet woman appealed to many Soviet women. Many had found Marxist feminism difficult to swallow and preferred their traditional female roles. Although these women drew satisfaction from their role as mothers, they appreciated the opportunity afforded them by the Communist ideology to dismantle the oppression that often went hand-in-hand with domestic life. With husbands that often beat, abused, and abandoned them and a society and government that looked down on them as intellectually and ideologically inferior prior to the Stalinist era, many women welcomed the ability to cast aside the stigma that came with their role as mother while retaining the status as an equal participant in society.\n\nDuring the 1920s and into the Stalinist era, Soviet policy forced women to curtail their professional aspirations in order to fulfill their dual role as worker and housewife. Competing requirements of family life limited female occupational mobility. Women managed the role strain experienced during the Stalinist era either by either a restriction of professional aspirations or by limiting family size. Despite pitfalls, unprecedented opportunities were available to lower-class women during this time. Women now had a voice in debates and the Zhenotdel, the women's section of the Central Committee from 1919–1930, made strides during its operation to increase political, social and economic agency of Soviet women.\n\n"}
{"id": "47040389", "url": "https://en.wikipedia.org/wiki?curid=47040389", "title": "Observatorio Venezolano de Prisiones", "text": "Observatorio Venezolano de Prisiones\n\nThe Observatorio Venezolano de Prisiones (OVP, in English Venezuelan Observatory of Prisons) is a Venezuelan NGO working to improve human rights for prisoners. Its offices are located in Caracas.\nSeveral media have quoted the Venezuelan Observatory of Prisons: New York Times and Associated Press.\n\nThe OVP was founded in 2002. Among its members there are political scientists, criminologists, sociologists, architects and experts in the prison system.\n\nThe organisation was very vocal when in 2014 when 34 inmates at the Uribana prison died during protests. This was in the wake of the 2013 Uribana prison riot in which over 60 prisoners died.\n\nThe IACHR expressed concerns after government official Diosdado Cabello informed on his TV program Hitting with the Hammer when several representatives of the OVP were arriving in Venezuela after an international tour. The IACH considered Cabello was threatening them by notifying what were private details of a trip.\n\nOfficial website\nFamiliares exigen respuestas por muertes en centro policial\nVenezuela, Accustomed to Tragedy, Is Shaken Again by Horrific Jail Fire\n"}
{"id": "57150908", "url": "https://en.wikipedia.org/wiki?curid=57150908", "title": "PETase", "text": "PETase\n\nPETases are an esterase class of enzymes that catalyze the hydrolysis of poly (ethylene terephthalate) PET plastic to monomeric mono-2-hydroxyethyl terephthalate (MHET). The idealized chemical reaction is (where n is the number of monomers in the polymer chain):\n\nTrace amount of the PET breaks down to bis(2-hydroxyethyl) terephthalate (BHET). PETases can also break down PEF-plastic (polyethylene-2,5-furandicarboxylate), which is a bioderived PET replacement. PETases can't catalyze the hydrolysis of aliphatic polyesters like polybutylene succinate or polylactic acid.\n\nThe first PETase enzyme was discovered in 2016 from \"Ideonella sakaiensis\" strain 201-F6 bacteria found from sludge samples collected close to a Japanese PET bottle recycling site. Scientists suggested that the PETase enzyme may have had past enzymatic activity associated with degradation of a waxy coating on plants. Normally the natural degradation of PET without PETase will take hundred of years. PET (polyethylene terephthalate-plastics)is a very common source of many plastic items used in the daily life. PETase can degrade PET in a way that is not harmful to the environment. Other types of PET degrading hydolases have been known before this discovery. These include hydrolases such as: lipases, esterases, and cutinases. Discoveries of polyester degrading enzymes date at least as far back as 1975 (α-chymotrypsin) and 1977 (lipase) for example. PET plastic was put into widespread use in the 1970s and it has been suggested that PETases in bacteria evolved only recently.\n\nPETase hydrolyses PET (polyethylene terephthalate) into soluble building blocks due to reaction with water which is a bioconversion of plastics. PET is a polymer composed of ester bond-linked terephthalate (TPA) and ethylene glycol (EG). A high molecular weight and other properties make PET a great utilizing plastic. By the hydrolysis reaction PET hydrolyzing enzymes decompose PET into building blocks which is helpful for the environment. During the hydrolyzing PET, the enzyme produces mono-(2-hydroxyethyl) terephthalic acid (MHET), TPA ,and bis-2(hydroxyethyl) TPA (BHET). The novel bacteria called \"Ideonella sakaieensis\" is isolating and it utilize PET as an energy and carbon source. The \"deonella sakaieensis\" sticks to the surface of PET and keep a cutinase enzyme that allow PET to degrade. That reaction allows to degrade PET, and make it less harmful for the environment. \n\nMHET is broken down in \"I. sakaiensis\" by the action of MHETase enzyme to terephthalic acid and ethylene glycol. These are environmentally harmless as they are broken down further to produce carbon dioxide and water. The \"I. sakaiensis\" bacterium adhere to the PET surface and release a unique enzyme, similar to cutinase, with the ability to degrade PET. These are environmentally harmless as they are broken down further to produce carbon dioxide and water.\n\nAs of April 2018, there were 13 known three-dimensional crystal structures of PETases: 6EQD, 6EQE, 6EQF, 6EQG, 6EQH, 6ANE, 5XJH, 5YNS, 5XFY, 5XFZ, 5XG0, 5XH2 and 5XH3. PETase exhibits shared qualities with both lipases and cutinases in that it possesses an α/β-hydrolase fold; although, the active-site cleft observed in PETase is more open than in cutinases. Scientists revolutionized the degradation rate of PET by PETase as a result of narrowing of the binding site through mutation of two active-site residues, although there are three comprising the active site. In the location of the active site, a catalytic triad is formed by the three residues Ser160, Asp206, and His237. \n\nIn 2018 scientists from the University of Portsmouth with the collaboration of the National Renewable Energy Laboratory of the United States Department of Energy developed a mutant of this PETase that degrades PET faster than the one in its natural state. In this study it was also shown that PETases can degrade polyethylene 2,5-furandicarboxylate (PEF).\n\nThere are approximately 69 PETase-like enzymes comprising a variety of diverse organisms, and there are two classifications of these enzymes including type I and type II. It is suggested that 57 enzymes fall into the type I category whereas the rest fall into the type II group, including the PETase enzyme found in the \"Ideonella sakaiensis.\" Within all 69 PETase-like enzymes, there exists the same three residues within the active site, suggesting that the catalytic mechanism is the same in all forms of PETase-like enzymes. \n\n"}
{"id": "56108", "url": "https://en.wikipedia.org/wiki?curid=56108", "title": "Penrose triangle", "text": "Penrose triangle\n\nThe Penrose triangle, also known as the Penrose tribar, or the impossible tribar, is a triangular impossible object. It was first created by the Swedish artist Oscar Reutersvärd in 1934. The psychiatrist Lionel Penrose and his mathematician son Roger Penrose independently devised and popularized it in the 1950s, describing it as \"impossibility in its purest form\". It is featured prominently in the works of artist M. C. Escher, whose earlier depictions of impossible objects partly inspired it.\n\nThe tribar appears to be a solid object, made of three straight beams of square cross-section which meet pairwise at right angles at the vertices of the triangle they form. The beams may be broken, forming cubes or cuboids.\nThis combination of properties cannot be realized by any three-dimensional object in ordinary Euclidean space. Such an object can exist in certain Euclidean 3-manifolds. There also exist three-dimensional solid shapes each of which, when viewed from a certain angle, appears the same as the 2-dimensional depiction of the Penrose triangle on this page (such as – for example – the adjacent image depicting a sculpture in Perth, Australia). The term \"Penrose triangle\" can refer to the 2-dimensional depiction or the impossible object itself.\n\nM.C. Escher's lithograph \"Waterfall\" (1961) depicts a watercourse that flows in a zigzag along the long sides of two elongated Penrose triangles, so that it ends up two stories higher than it began. The resulting waterfall, forming the short sides of both triangles, drives a water wheel. Escher helpfully points out that in order to keep the wheel turning some water must occasionally be added to compensate for evaporation.\n\nIf a line is traced around the Penrose triangle, a 4-loop Möbius strip is formed..\n\nAlthough the tribar is named one of the impossible objects, there exist many more that fit into the same category. Other impossible objects include the devil's fork, the dancing elephant, and impossible arch.\n\nWhile it is possible to construct analogies to the Penrose triangle with other shapes and regular polygons to create a Penrose polygon, the visual effect is not as striking, and as the sides increase, the object seems merely to be warped or twisted.\n\n\n"}
{"id": "51172574", "url": "https://en.wikipedia.org/wiki?curid=51172574", "title": "RepRap Snappy", "text": "RepRap Snappy\n\nThe RepRap Snappy is an open-source fused deposition modeling 3D printer, part of the RepRap project, it is the most self replicating 3D printer in the world.\n\nThe RepRap Snappy is designed to address the core goal of the RepRap project of creating a \"'general-purpose self-replicating manufacturing machine\"'. The RepRap Snappy is able to create 73% of its own parts by volume with a design that eliminates as many of the non 3D printed parts as possible including belts and bearings which are replaced with a rack and pinion system. The name Snappy comes from the use of snap fit connectors used on the small printed parts to construct larger pieces, this both cuts down on the use of non 3D printed parts and means a smaller build volume is needed on the machine producing the parts. The only non self replicating parts on the printer are the motors, electronics, a glass build plate and one 686 bearing, the 3D printed parts take around 150 hours to create. The RepRap Snappy received an honourable mention in the Uplift Prize Grand Personal Manufacturing Prize.\n\n"}
{"id": "2483516", "url": "https://en.wikipedia.org/wiki?curid=2483516", "title": "Resocialization", "text": "Resocialization\n\nResocialization is the process by which one's sense of social values, beliefs, and norms are re-engineered. This process is deliberately carried out in a variety of settings such as in many single parent households and military boot-camps, through an intense social process that may take place in a total institution. An important thing to note about socialization is that what can be learned, can be unlearned. This forms the basis of resocialization: to unlearn and relearn.\n\nResocialization can also be defined as a process wherein individuals, defined as inadequate according to the norms of a dominant institution, are subjected to a dynamic redistribution of those values, attitudes and abilities which would allow them to function according to the norms of said dominant institutions. This definition relates more to a jail sentence. If individuals exhibit deviance, society delivers the offenders to a total institution where they can be rehabilitated.\n\nResocialization varies in its severity. A mild resocialization might be involved in moving to a different country. One who does this may need to learn new social customs and norms such as language, eating habits, dress, and talking habits. A more drastic example of resocialization is joining a military or a cult, and the most severe example would be if one suffers from a loss of all memories and therefore would have to relearn society's norms over again.\n\nThe first stage of resocialization is the destruction of an individual's former beliefs and confidence.\n\nA total institution refers to an institution in which one is totally immersed and that controls all the day-to-day life. All activity will occur in a single place under a single authority. Examples of a total institution can include prisons, fraternity houses, and the military.\n\nThe goal of total institutions is resocialization which radically alters residents' personalities through deliberate manipulation of their environment. Resocialization is a two-part process. First, the institutional staff try to erode the residents' identities and independence.\n\nStrategies to erode identities include forcing individuals to surrender all personal possessions, get uniform haircuts and wear standardized clothing. Independence is eroded by subjecting residents to humiliating and degrading procedures. Examples are strip searches, fingerprinting and assigning serial numbers or code names to replace the residents' given names.\n\nThe second part of resocialization process involves the systematic attempt to build a different personality or self. This is generally done through a system of rewards and punishments. The privilege of being allowed to read a book, watch television or make a phone call can be a powerful motivator for conformity. Conformity occurs when individuals change their behaviour to fit in with the expectations of an authority figure or the expectations of the larger group.\n\nNo two people respond to resocialization programs in the same manner. While some residents are found to be \"rehabilitated\", others might become bitter and hostile. As well, over a long period of time, a strictly controlled environment can destroy a person's ability to make decisions and live independently. This is known as institutionalisation, a negative outcome of total institution that prevents an individual from ever functioning effectively in the outside world again. (Sproule, 154-155)\n\nResocialization is also evident in individuals who have never been \"socialized\" in the first place, or who have not been required to behave socially for an extended period of time. Examples include feral children (never socialized) or inmates who have been in solitary confinement.\n\nSocialization is a lifelong process. Adult socialization often includes learning new norms and values that are very different from those associated with the culture in which the person was raised. This process can be voluntary. Currently, joining a volunteer military qualifies as an example of voluntary resocialization. The norms and values associated with military life are different from those associated with civilian life. (Riehm, 2000)\n\nSociologist Erving Goffman studied resocialization in mental institutions. He characterized the mental institution as a total institution—one in which virtually every aspect of the inmates’ lives was controlled by the institution and calculated to serve the institution's goals. For example, the institution requires that patients comply with certain regulations, even when compliance is not necessarily in the best interest of the individual.\n\nOnce a person joins the military they enter a new social realm where they become socialized as a military member. Resocialization is defined as a “process wherein an individual, defined as inadequate according to the norms of a dominant institution(s), is subjected to a dynamic program of behavior intervention aimed at instilling and/or rejuvenating those values, attitudes, and abilities which would allow…[her] to function according to the norms of said dominant institution(s).” \n\nBoot camp serves as an example for understanding how military members are resocialized within the total institution of the military. According to Fox and Pease (2012) , the purpose of military training, like boot camp, is to “promote the willing and systematic subordination of one’s own individual desires and interests to those of one’s unit and, ultimately, country.\" To accomplish this, all aspects of military members’ lives exist within the same military institution, are controlled by the same “institutional authorities” (drill instructors) and are done to accomplish the goals of the total institution. The individual’s “civil[ian] identity, with its built-in restraints is eradicated, or at least undermined and set aside in favor of the warrior identity and its central focus upon killing.\" Military training prepares individuals for combat by promoting traditional ideas of masculinity, like training individuals to disregard their bodies’ natural reactions to run from fear, have pain or show emotions. Although resocialization through military training can create a sense of purpose in military members, it also has the likelihood to create mental and emotional distress when members are unable to achieve set standards and expectations.\n\nMilitary members, in part, find purpose and meaning through resocialization, because the institution provides access to symbolic and material resources, helping military members construct meaningful identities. Fox and Pease state, “like any social identity, military identity is always an achievement, something dependent upon conformity to others’ expectations and their acknowledgment. The centrality of performance testing in the military, and the need to ‘measure up,’ heightens this dependence. Although resocialization through military training can create a sense of purpose in military members, it also has the likelihood to create mental and emotional distress when members are unable to achieve set standards and expectations. \n\nIn the first couple of days, the most important aspect of basic training is the surrender of their identity. Recruits shed their clothes and hair which are the physical representation of their old identities. These processes happen very quickly and allows no time for recruits to think over the loss of their identity, this is so the recruits don’t have a chance to regret their decisions.\n\nThe drill sergeants then give the young men and women a romanticized view on what it is to be a soldier, and how manly it is. When the training starts it is physically demanding and gets harder every week. The recruits are constantly insulted and put down; this breaks down their pride and destroys their ability to resist the change they are undergoing. The drill sergeants put up a facade that tells their recruits that finishing out basic training sets them apart from all of the others who will fail. However, almost all recruits will succeed and graduate from basic training.\n\nThe training is also set up with roles. There are three younger drill sergeants closer to the recruits in age and one senior drill sergeant who becomes a father figure to the new recruits. And the company commander plays a god-like role, that the recruits look up to. The people in these roles will become role models and authority figures, but will also help create a sense of loyalty to the entire organization.\n\nRecruits are made to march in a formation where every person is moving the same way at the same time causes a sense of unity. It makes the recruits feel less like an individual and more like they are a part of a group. They sing in cadence to boost morale and to make the group feel important. The drill sergeants also feed the group small doses of triumphs which keep the soldiers proud and feeling accomplished.\n\nThe troop also undergoes group punishment which unifies the unit. Generally the similar hatred of something will bring everyone together. In this case, group punishment allows all the recruits to hate the drill sergeants and the punishment but find unity within their unit. They will encourage others to push themselves and create shared hardships.\n\nPrisons have two different types of resocialization. The first type is when the prisoner has to learn the new normal behaviors that apply to their new environment. The second type is if the prisoner has to partake in rehabilitation measures to help fix their deviant ways. When the individual violates the dominant society’s norms, the criminology system subjects them to a form of resocialization called criminal rehabilitation.\n\nRehabilitation aims to bring an inmate's real behavior closer to that of the majority of the individuals' behaviour that make up the dominant society. This 'ideal' societal behaviour is highly valued in many societies, mainly because it serves to protect and promote the well being of the majority of that society's members. In rehabilitation, the system will strip the criminal of his prior socialization of criminal behavior including the techniques of committing a crime and the specific motives, drives, rationalizations and attitudes. Criminal behavior is learned behavior and can therefore be unlearned.\n\nThe first step towards rehabilitation is the choice of Milieu. This is the type of interactions the deviant will have with the people around him while in custody. Usually this is determined after psychological and sociological screenings are performed on the criminal. The second step is Diagnosis. The diagnosis is a continual process influenced by feedback from the individual’s behavior. The next stage is treatment. Treatment is dependent on the diagnosis. Whether it is treating an addiction or redefining the values of a person, the treatment is what will resocialize the criminal back to societal norms.\n\n"}
{"id": "3224795", "url": "https://en.wikipedia.org/wiki?curid=3224795", "title": "Scientific modelling", "text": "Scientific modelling\n\nScientific modelling is a scientific activity, the aim of which is to make a particular part or feature of the world easier to understand, define, quantify, visualize, or simulate by referencing it to existing and usually commonly accepted knowledge. It requires selecting and identifying relevant aspects of a situation in the real world and then using different types of models for different aims, such as conceptual models to better understand, operational models to operationalize, mathematical models to quantify, and graphical models to visualize the subject. Modelling is an essential and inseparable part of many scientific disciplines, each of which have their own ideas about specific types of modelling.\n\nThere is also an increasing attention to scientific modelling in fields such as science education, philosophy of science, systems theory, and knowledge visualization. There is growing collection of methods, techniques and meta-theory about all kinds of specialized scientific modelling.\n\nA scientific model seeks to represent empirical objects, phenomena, and physical processes in a logical and objective way. All models are \"in simulacra\", that is, simplified reflections of reality that, despite being approximations, can be extremely useful. Building and disputing models is fundamental to the scientific enterprise. Complete and true representation may be impossible, but scientific debate often concerns which is the better model for a given task, e.g., which is the more accurate climate model for seasonal forecasting.\n\nAttempts to formalize the principles of the empirical sciences use an interpretation to model reality, in the same way logicians axiomatize the principles of logic. The aim of these attempts is to construct a formal system that will not produce theoretical consequences that are contrary to what is found in reality. Predictions or other statements drawn from such a formal system mirror or map the real world only insofar as these scientific models are true.\n\nFor the scientist, a model is also a way in which the human thought processes can be amplified. For instance, models that are rendered in software allow scientists to leverage computational power to simulate, visualize, manipulate and gain intuition about the entity, phenomenon, or process being represented. Such computer models are \"in silico\". Other types of scientific models are \"in vivo\" (living models, such as laboratory rats) and \"in vitro\" (in glassware, such as tissue culture).\n\nModels are typically used when it is either impossible or impractical to create experimental conditions in which scientists can directly measure outcomes. Direct measurement of outcomes under controlled conditions (see Scientific method) will always be more reliable than modelled estimates of outcomes.\n\nWithin modelling and simulation, a model is a task-driven, purposeful simplification and abstraction of a perception of reality, shaped by physical, legal, and cognitive constraints. It is task-driven, because a model is captured with a certain question or task in mind. Simplifications leave all the known and observed entities and their relation out that are not important for the task. Abstraction aggregates information that is important, but not needed in the same detail as the object of interest. Both activities, simplification and abstraction, are done purposefully. However, they are done based on a perception of reality. This perception is already a \"model\" in itself, as it comes with a physical constraint. There are also constraints on what we are able to legally observe with our current tools and methods, and cognitive constraints which limit what we are able to explain with our current theories. This model comprises the concepts, their behavior, and their relations in formal form and is often referred to as a conceptual model. In order to execute the model, it needs to be implemented as a computer simulation. This requires more choices, such as numerical approximations or the use of heuristics. Despite all these epistemological and computational constraints, simulation has been recognized as the third pillar of scientific methods: theory building, simulation, and experimentation.\n\nA simulation is the implementation of a model. A steady state simulation provides information about the system at a specific instant in time (usually at equilibrium, if such a state exists). A dynamic simulation provides information over time. A simulation brings a model to life and shows how a particular object or phenomenon will behave. Such a simulation can be useful for testing, analysis, or training in those cases where real-world systems or concepts can be represented by models.\n\nStructure is a fundamental and sometimes intangible notion covering the recognition, observation, nature, and stability of patterns and relationships of entities. From a child's verbal description of a snowflake, to the detailed scientific analysis of the properties of magnetic fields, the concept of structure is an essential foundation of nearly every mode of inquiry and discovery in science, philosophy, and art.\n\nA system is a set of interacting or interdependent entities, real or abstract, forming an integrated whole. In general, a system is a construct or collection of different elements that together can produce results not obtainable by the elements alone. The concept of an 'integrated whole' can also be stated in terms of a system embodying a set of relationships which are differentiated from relationships of the set to other elements, and from relationships between an element of the set and elements not a part of the relational regime. There are two types of system models: 1) discrete in which the variables change instantaneously at separate points in time and, 2) continuous where the state variables change continuously with respect to time.\n\nModelling is the process of generating a model as a conceptual representation of some phenomenon. Typically a model will deal with only some aspects of the phenomenon in question, and two models of the same phenomenon may be essentially different—that is to say, that the differences between them comprise more than just a simple renaming of components.\n\nSuch differences may be due to differing requirements of the model's end users, or to conceptual or aesthetic differences among the modellers and to contingent decisions made during the modelling process. Considerations that may influence the structure of a model might be the modeller's preference for a reduced ontology, preferences regarding statistical models versus deterministic models, discrete versus continuous time, etc. In any case, users of a model need to understand the assumptions made that are pertinent to its validity for a given use.\n\nBuilding a model requires abstraction. Assumptions are used in modelling in order to specify the domain of application of the model. For example, the special theory of relativity assumes an inertial frame of reference. This assumption was contextualized and further explained by the general theory of relativity. A model makes accurate predictions when its assumptions are valid, and might well not make accurate predictions when its assumptions do not hold. Such assumptions are often the point with which older theories are succeeded by new ones (the general theory of relativity works in non-inertial reference frames as well).\n\nThe term \"assumption\" is actually broader than its standard use, etymologically speaking. The Oxford English Dictionary (OED) and online Wiktionary indicate its Latin source as \"assumere\" (\"accept, to take to oneself, adopt, usurp\"), which is a conjunction of \"ad-\" (\"to, towards, at\") and \"sumere\" (to take). The root survives, with shifted meanings, in the Italian \"sumere\" and Spanish \"sumir\". In the OED, \"assume\" has the senses of (i) “investing oneself with (an attribute), ” (ii) “to undertake” (especially in Law), (iii) “to take to oneself in appearance only, to pretend to possess,” and (iv) “to suppose a thing to be.” Thus, \"assumption\" connotes other associations than the contemporary standard sense of “that which is assumed or taken for granted; a supposition, postulate,” and deserves a broader analysis in the philosophy of science.\n\nA model is evaluated first and foremost by its consistency to empirical data; any model inconsistent with reproducible observations must be modified or rejected. One way to modify the model is by restricting the domain over which it is credited with having high validity. A case in point is Newtonian physics, which is highly useful except for the very small, the very fast, and the very massive phenomena of the universe. However, a fit to empirical data alone is not sufficient for a model to be accepted as valid. Other factors important in evaluating a model include:\nPeople may attempt to quantify the evaluation of a model using a utility function.\n\nVisualization is any technique for creating images, diagrams, or animations to communicate a message. Visualization through visual imagery has been an effective way to communicate both abstract and concrete ideas since the dawn of man. Examples from history include cave paintings, Egyptian hieroglyphs, Greek geometry, and Leonardo da Vinci's revolutionary methods of technical drawing for engineering and scientific purposes.\n\nSpace mapping refers to a methodology that employs a \"quasi-global\" modeling formulation to link companion \"coarse\" (ideal or low-fidelity) with \"fine\" (practical or high-fidelity) models of different complexities. In engineering optimization, space mapping aligns (maps) a very fast coarse model with its related expensive-to-compute fine model so as to avoid direct expensive optimization of the fine model. The alignment process iteratively refines a \"mapped\" coarse model (surrogate model).\n\n\n\n\nOne application of scientific modelling is the field of modelling and simulation, generally referred to as \"M&S\". M&S has a spectrum of applications which range from concept development and analysis, through experimentation, measurement and verification, to disposal analysis. Projects and programs may use hundreds of different simulations, simulators and model analysis tools.\nThe figure shows how Modelling and Simulation is used as a central part of an integrated program in a Defence capability development process.\n\nModel–based learning in education, particularly in relation to learning science involves students creating models for scientific concepts in order to:\n\nDifferent types of model based learning techniques include:\n\nModel–making in education is an iterative exercise with students refining, developing and evaluating their models over time. This shifts learning from the rigidity and monotony of traditional curriculum to an exercise of students' creativity and curiosity. This approach utilizes the constructive strategy of social collaboration and learning scaffold theory. Model based learning includes cognitive reasoning skills where existing models can be improved upon by construction of newer models using the old models as a basis.\n\n\"Model–based learning entails determining target models and a learning pathway that provide realistic chances of understanding.\" Model making can also incorporate blended learning strategies by using web based tools and simulators, thereby allowing students to:\n\n\"A well-designed simulation simplifies a real world system while heightening awareness of the complexity of the system. Students can participate in the simplified system and learn how the real system operates without spending days, weeks or years it would take to undergo this experience in the real world.\" \n\nThe teacher's role in the overall teaching and learning process is primarily that of a facilitator and arranger of the learning experience. He or she would assign the students, a model making activity for a particular concept and provide relevant information or support for the activity. For virtual model making activities, the teacher can also provide information on the usage of the digital tool and render troubleshooting support in case of glitches while using the same. The teacher can also arrange the group discussion activity between the students and provide the platform necessary for students to share their observations and knowledge extracted from the model making activity.\n\nModel–based learning evaluation could include the use of rubrics that assess the ingenuity and creativity of the student in the model construction and also the overall classroom participation of the student vis-a-vis the knowledge constructed through the activity.\n\nIt is important, however, to give due consideration to the following for successful model–based learning to occur:\n\n\nNowadays there are some 40 magazines about scientific modelling which offer all kinds of international forums. Since the 1960s there is a strong growing number of books and magazines about specific forms of scientific modelling. There is also a lot of discussion about scientific modelling in the philosophy-of-science literature. A selection:\n\n"}
{"id": "6042463", "url": "https://en.wikipedia.org/wiki?curid=6042463", "title": "Secret law", "text": "Secret law\n\nSecret law refers to legal authorities that require compliance that are classified or otherwise withheld from the public. Such non-promulgated laws were common in the Soviet Union and Eastern Bloc countries. The term has been used in reference to some counterterrorist measures taken by the Bush Administration in the United States following the September 11, 2001 terrorist attacks. The Patriot Act has been referred to as having secret interpretations.\n\nSince about 2015 the branches of the United States federal government have accused one another of creating secret law. Journalists, scholars, and anti-secrecy activists have also made similar allegations. Scholarly analysis has shown that secret law is present in all three branches. One scholar, Professor Dakota Rudesill, recommends that the country affirmatively decide whether to tolerate secret law, and proposes principles for governing it, including: public law’s supremacy over secret law; no secret criminal law; public notification of creation of secret law; presumptive sunset and publication dates; and availability of all secret law to Congress.\n\nIn Britain many open places freely accessible by the public are actually privately owned public spaces (POPS). Information on ownership is considered confidential, and not provided either by the owners, or by local councils that have the information. As in any private property the owner may require visitors to abide by specified rules; but people freely accessing the place are not informed of the rules, which may nevertheless be enforced by security guards. Typical prohibitions which do not apply to genuinely public spaces include protesting, and photography.\n\nCouncils mostly refused to provide information on existing and planned pseudo-public spaces. They also refused to say how information could be obtained, or to provide information on private restrictions on exercising the other rights people have on genuinely public land. Councils were criticised for being under the influence of property developers and corporate owners. A member of the London Assembly said \"Being able to know what rules you are being governed by, and how to challenge them, is a fundamental part of democracy\".\n\nSecret laws and their negative effects are described in Franz Kafka's novel \"Der Prozess\" (\"The Trial\").\n\n"}
{"id": "29205627", "url": "https://en.wikipedia.org/wiki?curid=29205627", "title": "Segal space", "text": "Segal space\n\nIn mathematics, a Segal space is a simplicial space satisfying some pullback conditions, making it look like a homotopical version of a category. More precisely, a simplicial set, considered as a simplicial discrete space, satisfies the Segal conditions iff it is the nerve of a category. The condition for Segal spaces is a homotopical version of this. \n\nComplete Segal spaces were introduced by as models for (∞, 1)-categories.\n\n"}
{"id": "187337", "url": "https://en.wikipedia.org/wiki?curid=187337", "title": "State diagram", "text": "State diagram\n\nA state diagram is a type of diagram used in computer science and related fields to describe the behavior of systems. State diagrams require that the system described is composed of a finite number of states; sometimes, this is indeed the case, while at other times this is a reasonable abstraction. Many forms of state diagrams exist, which differ slightly and have different semantics.\n\nState diagrams are used to give an abstract description of the behavior of a system. This behavior is analyzed and represented as a series of events that can occur in one or more possible states. Hereby \"each diagram usually represents objects of a single class and track the different states of its objects through the system\".\n\nState diagrams can be used to graphically represent finite state machines. This was introduced by C.E. Shannon and W. Weaver in their 1949 book \"The Mathematical Theory of Communication\". Another source is Taylor Booth in his 1967 book \"Sequential Machines and Automata Theory\". Another possible representation is the State transition table.\n\nA classic form of state diagram for a finite state machine or finite automaton (FA) is a directed graph with the following elements (Q,Σ,Z,δ,q,F):\n\n\nThe output function ω represents the mapping of ordered pairs of input symbols and states onto output symbols, denoted mathematically as ω : Σ × Q→ Z.\n\n\nFor a deterministic finite automaton (DFA), nondeterministic finite automaton (NFA), generalized nondeterministic finite automaton (GNFA), or Moore machine, the input is denoted on each edge. For a Mealy machine, input and output are signified on each edge, separated with a slash \"/\": \"1/0\" denotes the state change upon encountering the symbol \"1\" causing the symbol \"0\" to be output. For a Moore machine the state's output is usually written inside the state's circle, also separated from the state's designator with a slash \"/\". There are also variants that combine these two notations.\n\nFor example, if a state has a number of outputs (e.g. \"a= motor counter-clockwise=1, b= caution light inactive=0\") the diagram should reflect this : e.g. \"q5/1,0\" designates state q5 with outputs a=1, b=0. This designator will be written inside the state's circle.\n\n\"S\" and \"S\" are states and \"S\" is an accepting state or a final state. Each edge is labeled with the input. This example shows an acceptor for strings over {0,1} that contain an even number of zeros.\n\n\"S\", \"S\", and \"S\" are states. Each edge is labeled with \"\"j\" / \"k\"\" where \"j\" is the input and \"k\" is the output.\n\nHarel statecharts are gaining widespread usage since a variant has become part of the Unified Modeling Language (UML). The diagram type allows the modeling of superstates, orthogonal regions, and activities as part of a state.\n\nClassic state diagrams require the creation of distinct nodes for every valid combination of parameters that define the state. This can lead to a very large number of nodes and transitions between nodes for all but the simplest of systems (state and transition explosion). This complexity reduces the readability of the state diagram. With Harel statecharts it is possible to model multiple cross-functional state diagrams within the statechart. Each of these cross-functional state machines can transition internally without affecting the other state machines in the statechart. The current state of each cross-functional state machine in the statechart defines the state of the system. The Harel statechart is equivalent to a state diagram but it improves the readability of the resulting diagram.\n\nThere are other sets of semantics available to represent state diagrams. For example, there are tools for modeling and designing logic for embedded controllers. These diagrams, like Harel's original state machines, support hierarchically nested states, orthogonal regions, state actions, and transition actions.\n\nNewcomers to the state machine formalism often confuse state diagrams with flowcharts. The figure below shows a comparison of a state diagram with a flowchart. A state machine (panel (a)) performs actions in response to explicit events. In contrast, the flowchart (panel (b)) does not need explicit events but rather transitions from node to node in its graph automatically upon completion of activities.<ref name=\"Practical UML Statecharts in C/C++, Second Edition: Event-Driven Programming for Embedded Systems\"></ref>\n\nNodes of flowcharts are edges in the induced graph of states.\nThe reason is that each node in a flowchart represents a program command.\nA program command is an action to be executed.\nSo it is not a state, but when applied to the program's state, it results in a transition to another state.\n\nIn more detail, the source code listing represents a program graph.\nExecuting the program graph (parsing and interpreting) results in a state graph.\nSo each program graph induces a state graph.\nConversion of the program graph to its associated state graph is called \"unfolding\" of the program graph.\n\nThe program graph is a sequence of commands.\nIf no variables exist, then the state consists only of the program counter, which keeps track of where in the program we are during execution (what is the next command to be applied).\n\nIn this case before executing a command the program counter is at some position (state before the command is executed).\nExecuting the command moves the program counter to the next command.\nSince the program counter is the whole state, it follows that executing the command changed the state.\nSo the command itself corresponds to a transition between the two states.\n\nNow consider the full case, when variables exist and are affected by the program commands being executed.\nThen between different program counter locations, not only does the program counter change, but variables might also change values, due to the commands executed.\nConsequently, even if we revisit some program command (e.g. in a loop), this doesn't imply the program is in the same state.\n\nIn the previous case, the program would be in the same state, because the whole state is just the program counter, so if the program counter points to the same position (next command) it suffices to specify that we are in the same state.\nHowever, if the state includes variables, then if those change value, we can be at the same program location with different variable values, meaning in a different state in the program's state space.\nThe term \"unfolding\" originates from this multiplication of locations when producing the state graph from the program graph.\n\nA representative example is a do loop incrementing some counter until it overflows and becomes 0 again.\nAlthough the do loop executes the same increment command iteratively, so the program graph executes a cycle, in its state space is not a cycle, but a line.\nThis results from the state being the program location (here cycling) combined with the counter value, which is strictly increasing (until the overflow), so different states are visited in sequence, until the overflow.\nAfter the overflow the counter becomes 0 again, so the initial state is revisited in the state space, closing a cycle in the state space (assuming the counter was initialized to 0).\n\nThe figure above attempts to show that reversal of roles by aligning the arcs of the state diagrams with the processing stages of the flowchart.\n\nYou can compare a flowchart to an assembly line in manufacturing because the flowchart describes the progression of some task from beginning to end (e.g., transforming source code input into object code output by a compiler). A state machine generally has no notion of such a progression. The door state machine shown at the top of this article, for example, is not in a more advanced stage when it is in the \"closed\" state, compared to being in the \"opened\" state; it simply reacts differently to the open/close events. A state in a state machine is an efficient way of specifying a particular behavior, rather than a stage of processing.\n\nAn interesting extension is to allow arcs to flow from any number of states to any number of states. This only makes sense if the system is allowed to be in multiple states at once, which implies that an individual state only describes a condition or other partial aspect of the overall, global state. The resulting formalism is known as a Petri net.\n\nAnother extension allows the integration of flowcharts within Harel statecharts. This extension supports the development of software that is both event driven and workflow driven.\n\n\n"}
{"id": "18546203", "url": "https://en.wikipedia.org/wiki?curid=18546203", "title": "Stimulus (psychology)", "text": "Stimulus (psychology)\n\nIn psychology, a stimulus is any object or event that elicits a sensory or behavioral response in an organism. \n\n\nIn the second half of the 19th century, the term \"stimulus\" was coined in psychophysics by defining the field as the \"scientific study of the relation between stimulus and sensation\". This may have led James J. Gibson to conclude that \"whatever could be controlled by an experimenter and applied to an observer could be thought of as a stimulus\" in early psychological studies with humans, while around the same time, the term stimulus described anything eliciting a reflex in animal research. \n\nThe concept \"stimulus\" was essential to behaviorism and the behavioral theory of B. F. Skinner in particular. Within such a framework several kinds of stimuli have been distinguished (see also classical conditioning):\n\nAn eliciting stimulus was defined as a stimulus that precedes a certain behavior and thus causes a response. A discriminative stimulus in contrast increases the probability of a response to occur, but does not necessarily elicit the response. A reinforcing stimulus usually denoted a stimulus delivered after the response has already occurred; in psychological experiments it was often delivered on purpose to reinforce the behavior. Emotional stimuli were regarded as not eliciting a response. Instead, they were thought to modify the strength or vigor with which a behavior is carried out.\n\n"}
{"id": "12302967", "url": "https://en.wikipedia.org/wiki?curid=12302967", "title": "SuperWrite", "text": "SuperWrite\n\nSuperWrite is an English shorthand system based largely on previous shorthand systems and largely intended for people who need to increase their writing speed without devoting months to learning more complicated systems. It is a writing system, as it uses cursive forms of the letters of the spoken alphabet to represent sounds. Aside from assigning special abbreviations to common words, the system functions largely by omitting short vowels from within words. Hence, SuperWrite could be considered an impure abjad. Its publishers claim it uses only the 26 letters of the longhand alphabet with no extra symbols, however, the capital letters used (C, O, S, T, U, X) have different functions from their lowercase forms, and the uncrossed t — which would be considered a mistake in longhand — has a different function from the crossed t, bringing the total number of symbols in SuperWrite to 33.\nSuperWrite was developed by A. James Lemaster, with assistance from Ellen Hankin and John Metz Baer.\n\nSee Lemaster, A. J., & Baer, J. (1999). SuperWrite (Vol. 1). Cincinnati: South-Western , and\n\nLemaster, A. J., & Baer, J. (1999). SuperWrite (Vol. 2). Cincinnati: South-Western \n\n"}
{"id": "1524635", "url": "https://en.wikipedia.org/wiki?curid=1524635", "title": "Supererogation", "text": "Supererogation\n\nSupererogation (Late Latin: \"supererogatio\" \"payment beyond what is due or asked\", from \"super\" \"beyond\" and \"erogare\" \"to pay out, expend\", itself from \"ex\" \"out\" and \"rogare\" \"to ask\") is the performance of more than is asked for; the action of doing more than duty requires. In ethics, an act is supererogatory if it is good but not morally required to be done. It refers to an act that is more than is necessary, when another course of action—involving less—would still be an acceptable action. It differs from a duty, which is an act wrong not to do, and from acts morally neutral. Supererogation may be considered as performing above and beyond a normative course of duty to further benefits and functionality.\n\nIn the theology of the Roman Catholic Church, \"works of supererogation\" (also called \"acts of supererogation\") are those performed beyond what God requires. For example, in 1 Corinthians 7, Paul the Apostle says that while everyone is free to marry, it is better to refrain from marriage and remain celibate to better serve God. The Roman Catholic Church holds that the counsels of perfection are supererogatory acts, which specific Christians may engage in above their moral duties. Similarly, it teaches that to determine how to act, one must engage in reasonable efforts to be sure of what the right actions are; after the reasonable action, the person is in a state of invincible ignorance and guiltless of wrongdoing, but to undertake more than reasonable actions to overcome ignorance is supererogatory, and praiseworthy.\n\nAccording to the classic teaching on indulgences, the works of supererogation performed by all the saints form a treasure with God, the \"treasury of merit,\" which the church can apply to exempt repentant sinners from the works of penitence that would otherwise be required of them to achieve full reconciliation with the church.\n\nMartin Luther's opposition of this teaching seeded the Protestant Reformation. The Church of England denied the doctrine of supererogation in the fourteenth of the Thirty-Nine Articles, which states that works of supererogation\ncannot be taught without arrogancy and impiety: for by them men do declare, that they not only render unto God as much as they are bound to, but that they do more for his sake, than of bounden duty is required: whereas Christ saith plainly, When ye have done all that are commanded to you, say, We are unprofitable servants\nLater Protestant movements followed suit, such as in the Methodist Articles of Religion.\n\nA Muslim must complete a minimum of the five daily prayers, each typically lasting an average of 5 to 10 minutes. Supererogatory prayers beyond these are known as nafl prayers, and praying them is considered to bear additional reward. There are also several other supererogatory acts in Islam, such as fasting outside of the month of Ramadhan, or giving sadaqah (charity, consisting of simple acts of kindness to financial assistance) that is not obligatory.\n\nWhether an act is supererogatory or obligatory can be debated. In many schools of thought, donating money to charity is supererogatory. In other schools of thought that regard some level of charitable donation to be duty (such as with the tithe in Judaism, zakat in Islam, and similar standards in many Christian sects), only exceeding a certain level of donation (e.g. going above the common 2.5%-of-capital-assets standard in zakat) would count as supererogatory.\n\nIn criminal law, it may be observed that state prohibitions on killing, stealing, and so on derive from the state's duty to protect one's own citizens. However, a nation state has no duty to protect the citizens of an adjacent nation from crime. To send a peacekeeping force into another country would be — in the view of the nation doing it — supererogatory.\n\nSome schools of ethics do not include supererogatory acts. In utilitarianism, an act can only be better because it would bring more good to a greater number, and in that case it becomes a duty, not a supererogatory act. The lack of a notion of supererogation in utilitarianism and related schools leads to the demandingness objection, arguing that these schools are too ethically demanding, requiring unreasonable acts.\n\n"}
{"id": "35004503", "url": "https://en.wikipedia.org/wiki?curid=35004503", "title": "Sustainable refurbishment", "text": "Sustainable refurbishment\n\nSustainable refurbishment describes working on existing buildings to improve their environmental performance using sustainable methods and materials.\n\nSustainable refurbishment is the equivalent of sustainable development which relates to new developments of cities, buildings or industries etc.\nSustainable refurbishment includes insulation and related measures to reduce the energy consumption of buildings, installation of renewable energy sources such as solar water heating and photovoltaics, measures to reduce water consumption, and changes to reduce overheating, improve ventilation and improve internal comfort. The process of sustainable refurbishment includes minimizing the waste of existing components, recycling and using environmentally friendly materials, and minimizing energy use, noise and waste during the refurbishment.\n\nThe importance of sustainable refurbishment is that the majority of buildings in use are not new and thus were constructed when energy standards were low or non-existent, and are otherwise incompatible with current standards or the expectations of users. Much of the existing building stock is likely to be in use for many years to come since demolition and replacement is often unacceptable owing to cost, social disruption or because the building is of architectural and/or historical interest. The solution is to refurbish or renovate such buildings to make them appropriate for current and future use and to satisfy current requirements and standards of energy use and comfort.\n\nSustainable refurbishment is not a new concept but is gaining recognition and importance owing to current concerns about high energy use leading to climate change, overheating in buildings, the need for healthy internal environments, waste and environmental damage associated with materials production. Many governments are beginning to realize the importance of sustainably renovating their existing building stock, rather than just raising standards for new buildings and developments, and are producing guidance and grants and other support and stimulation activities. Think-tanks, lobby groups and voluntary organizations continue to publicize and promote the need for and practice of sustainable refurbishment. Examples and demonstration projects abound in many countries.\n\nThe techniques of sustainable refurbishment have been developing over many years and though the principles are very similar to those used on new buildings, the practice and details appropriate for the wide range of situations found in old buildings has required development of specific solutions and guidance to optimize the process and avoid subsequent problems. Detailed technical guidance is widely available from government-sponsored sources.\n\n\n\nSeveral books on the subject have been published aimed at different audiences, for example:\n\nall available from http://www.routledge.com.\n\n"}
{"id": "48508959", "url": "https://en.wikipedia.org/wiki?curid=48508959", "title": "Thelma Dale Perkins", "text": "Thelma Dale Perkins\n\nThelma Dale Perkins (1915-2014) was an African-American activist. Her maternal uncle was Frederick Douglass Patterson. She joined the Alpha Kappa Alpha sorority, the Liberal Club, which \"advocated for the integration of African Americans\" into the greater society, the Southern Negro Youth Congress, and the American Youth Congress. As a member of the American Youth Congress she went to the White House for \"chats\" sponsored by First Lady Eleanor Roosevelt to discuss the issues facing young people. She graduated from Howard University in 1936. She worked for E. Franklin Frasier on a National Youth Administration Fellowship, and worked for the federal government, but as she recalled, \"I resigned from the government rather than sign a loyalty oath and accepted the job of National Secretary of the National Negro Congress in New York City.\" In 1945 she attended the founding meeting of the Women's International Democratic Federation, held in Paris.\n\nShe was friends with Paul Robeson and his wife Eslanda Robeson, and worked as managing editor for \nPaul's \"Freedom\" newspaper, and was involved in a campaign to get his passport restored. She wrote a tribute to Paul Robeson in the book \"Paul Robeson: The Great Forerunner\" (1998), by the editors of Freedomways. She was a manager of community relations for CIBA-GEIGY Corporation, where she initiated and developed the \"Exceptional Black Scientist\" series, which was nationally recognized. \n\nShe married Lawrence Rickman Perkins Jr., in 1957, and adopted two babies, Lawrence Dale Perkins and Patrice Dale Perkins.\n\nIn the Dale/Patterson Family papers in the Anacostia Community Museum Archives there is a small collection related to her, donated by her niece Dianne Dale. \n\n\"Radicalism at the Crossroads: African American Women Activists in the Cold War\", by Dayo Gore (2011) [about Thelma Dale Perkins and others]\n"}
{"id": "261002", "url": "https://en.wikipedia.org/wiki?curid=261002", "title": "Traffic sign", "text": "Traffic sign\n\nTraffic signs or road signs are signs erected at the side of or above roads to give instructions or provide information to road users. The earliest signs were simple wooden or stone milestones. Later, signs with directional arms were introduced, for example, the fingerposts in the United Kingdom and their wooden counterparts in Saxony.\n\nWith traffic volumes increasing since the 1930s, many countries have adopted pictorial signs or otherwise simplified and standardized their signs to overcome language barriers, and enhance traffic safety. Such pictorial signs use symbols (often silhouettes) in place of words and are usually based on international protocols. Such signs were first developed in Europe, and have been adopted by most countries to varying degrees.\n\nVarious international conventions have helped to achieve a degree of uniformity in Traffic Signing in various countries.\n\nTraffic signs can be grouped into several types. For example, Annexe 1 of the Vienna Convention on Road Signs and Signals (1968), which on 30 June 2004 had 52 signatory countries, defines eight categories of signs:\n\nIn the United States, Canada, Ireland, Australia, and New Zealand signs are categorized as follows:\n\nIn the United States, the categories, placement, and graphic standards for traffic signs and pavement markings are legally defined in the Federal Highway Administration's \"Manual on Uniform Traffic Control Devices\" as the standard.\n\nA rather informal distinction among the directional signs is the one between advance directional signs, interchange directional signs, and reassurance signs. Advance directional signs appear at a certain distance from the interchange, giving information for each direction. A number of countries do not give information for the road ahead (so-called \"pull-through\" signs), and only for the directions left and right. Advance directional signs enable drivers to take precautions for the exit (e.g., switch lanes, double check whether this is the correct exit, slow down).\nThey often do not appear on lesser roads, but are normally posted on expressways and motorways, as drivers would be missing exits without them. While each nation has its own system, the first approach sign for a motorway exit is mostly placed at least from the actual interchange. After that sign, one or two additional advance directional signs typically follow before the actual interchange itself.\n\nThe earliest road signs were milestones, giving distance or direction; for example, the Romans erected stone columns throughout their empire giving the distance to Rome. In the Middle Ages, multidirectional signs at intersections became common, giving directions to cities and towns.\n\nIn 1686, the first known Traffic Regulation Act in Europe is established by King Peter II of Portugal. This act foresees the placement of priority signs in the narrowest streets of Lisbon, stating which traffic should back up to give way. One of these signs still exists at Salvador street, in the neighborhood of Alfama.\n\nThe first modern road signs erected on a wide scale were designed for riders of high or \"ordinary\" bicycles in the late 1870s and early 1880s. These machines were fast, silent and their nature made them difficult to control, moreover their riders travelled considerable distances and often preferred to tour on unfamiliar roads. For such riders, cycling organizations began to erect signs that warned of potential hazards ahead (particularly steep hills), rather than merely giving distance or directions to places, thereby contributing the sign type that defines \"modern\" traffic signs.\n\nThe development of automobiles encouraged more complex signage systems using more than just text-based notices. One of the first modern-day road sign systems was devised by the Italian Touring Club in 1895. By 1900, a Congress of the International League of Touring Organizations in Paris was considering proposals for standardization of road signage. In 1903 the British government introduced four \"national\" signs based on shape, but the basic patterns of most traffic signs were set at the 1908 International Road Congress in Paris. In 1909, nine European governments agreed on the use of four pictorial symbols, indicating \"bump\", \"curve\", \"intersection\", and \"grade-level railroad crossing\". The intensive work on international road signs that took place between 1926 and 1949 eventually led to the development of the European road sign system. Both Britain and the United States developed their own road signage systems, both of which were adopted or modified by many other nations in their respective spheres of influence. The UK adopted a version of the European road signs in 1964 and, over past decades, North American signage began using some symbols and graphics mixed in with English.\n\nOver the years, change was gradual. Pre-industrial signs were stone or wood, but with the development of Darby's method of smelting iron using coke, painted cast iron became favoured in the late 18th and 19th centuries. Cast iron continued to be used until the mid-20th century, but it was gradually displaced by aluminium or other materials and processes, such as vitreous enamelled and/or pressed malleable iron, or (later) steel. Since 1945 most signs have been made from sheet aluminium with adhesive plastic coatings; these are normally retroreflective for nighttime and low-light visibility. Before the development of reflective plastics, reflectivity was provided by glass reflectors set into the lettering and symbols.\n\nNew generations of traffic signs based on electronic displays can also change their text (or, in some countries, symbols) to provide for \"intelligent control\" linked to automated traffic sensors or remote manual input. In over 20 countries, real-time Traffic Message Channel incident warnings are conveyed directly to vehicle navigation systems using inaudible signals carried via FM radio, 3G cellular data and satellite broadcasts. Finally, cars can pay tolls and trucks pass safety screening checks using video numberplate scanning, or RFID transponders in windshields linked to antennae over the road, in support of on-board signalling, toll collection, and travel time monitoring.\n\nYet another \"medium\" for transferring information ordinarily associated with visible signs is RIAS (Remote Infrared Audible Signage), e.g., \"talking signs\" for print-handicapped (including blind/low-vision/illiterate) people. These are infra-red transmitters serving the same purpose as the usual graphic signs when received by an appropriate device such as a hand-held receiver or one built into a cell phone.\n\nRoad signs in Mauritius are regulated by the Traffic Signs Regulations 1990. They are particularly modelled on the British road signs since Mauritius is a former British colony. Mauritius has left-hand traffic.\n\nRoad signs in Sierra Leone are standardized road signs closely follow those used in Italy with certain distinctions.\n\nDirection signs are: \n\nHong Kong's traffic signs follow the British road sign conventions and are bi-lingual in English and Chinese (English on top, and Traditional Chinese characters at the bottom).\n\nRoad signs in Iran mainly follow the Vienna Convention. Signs are in Persian and English.\n\nRoad signs in Israel mainly follow the Vienna Convention, but have some variants.\n\nRoad signs in Japan are either controlled by local police authorities under or by other road-controlling entities including Ministry of Land, Infrastructure, Transport and Tourism, local municipalities, NEXCO (companies controlling expressways), under . Most of the design of the road signs in Japan are similar to the signs on the Vienna Convention, except for some significant variances, such as stop sign with a red downward triangle. The main signs are categorized into four meaning types: \n\nRoad signs in Macau are inherited from pre-1998 reform Portuguese road signages.\n\nRoad signs in the Philippines are standardized in the \"Road Signs and Pavement Markings Manual\", published by the Department of Public Works and Highways. Philippine road signage practice closely follow those used in Europe, but with local adaptations and some minor influences from the US MUTCD and Australian road signs. However, some road signs may differ by locale, and mostly diverge from the national standard. For example, the Metropolitan Manila Development Authority (MMDA) has used pink and light blue in its signage for which it has been heavily criticised.\n\nRoad signs in the Philippines are classified as:\n\nRegulatory road signs – other than the stop and give way signs – are generally circular, with (for prohibitions) a black symbol on a white background within a red border, or (for mandatory instructions) a white symbol on a blue background. In some cases circular regulatory signs are placed on white rectangular panels together with text supplementing their meanings.\n\nMost warning signs display a black symbol on a white background within a red-bordered equilateral triangle. Since 2012, however, a more visibly distinctive design (taken from that used for school signs in the US) has been adopted for pedestrian-related signs: these consist of a fluorescent yellow-green pentagon with black border and symbol. Additional panels may be placed below signs to supplement their meanings.\n\nGuide signs are divided into directional signs, service area signs, route markers, and tourist-related signs, with influence from both American and Australian practice. Directional signs use a green background with white letters and arrows. Service area signs use a blue background with white letters, arrows, and symbols. Tourist-related signs use a brown background with white letters, arrows, and symbols. The route marker sign, excluding the AH26 route marker, is based on the Australian National Route marker, but reserved for future use.\n\nSigns on expressways mostly take elements from Australian motorway/freeway signs. Exit signs, wrong way signs and start/end of expressway signs are very similar to Australian freeway signage. Traffic instruction signs are textual signs used to supplement warning and regulatory signs.\nRoad signs in Saudi Arabia frequently show their text both in Arabic and English. Road signs also indicate which part of the road is for Muslims, and which part is for non-Muslims, for instance near Mecca.\n\nSingapore's traffic signs closely follow British road sign conventions, although the government has introduced some changes to them.\n\nRoad signs in Sri Lanka are standardized road signs closely follow those used in Europe with certain distinctions, and a number of changes have introduced road signs that suit as per local road and system. Sri Lankan government announced by a gazette that aimed to get a face-lift and introduction of over 100 new road traffic signs.\n\nSince the signing of the 1931 Geneva Convention concerning the Unification of Road Signals by a number of countries that the standardization of the traffic signs started in Europe. The 1931 Convention rules were developed in the 1949 Geneva Protocol on Road Signs and Signals.\n\nIn 1968, the European countries signed the Vienna Convention on Road Traffic treaty, with the aim of standardizing traffic regulations in participating countries in order to facilitate international road traffic and to increase road safety. Part of the treaty was the Vienna Convention on Road Signs and Signals, which defined the traffic signs and signals. As a result, in Western Europe the traffic signs are well standardized, although there are still some country-specific exceptions, mostly dating from the pre-1968 era.\n\nThe principle of the European traffic sign standard is that certain shapes and colours are to be used with consistent meanings:\n\nDirectional signs (\"guide signs\" in American parlance) have not been harmonized under the Convention, at least not on ordinary roads. As a result, there are substantial differences in directional signage throughout Europe. Differences apply to the choice of typeface, arrows and, most notably, colours. The convention does, however, specify that the type of directional signage used should, for each country, distinguish limited-access roads (\"motorways\") from ordinary, all-purpose roads.\n\nDirectional signage on motorways uses: \n\nDifferences are greater for non-motorways: \n\nThe black-on-white signposting of secondary roads distinguishes them from primary roads in Finland, France, Portugal, the Republic of Ireland, Switzerland, and the United Kingdom. In Germany, Hungary, Italy, Romania, and Sweden black-on-white indicates urban-only roads or urban destinations.\n\nThe signposting of road numbers also differs greatly, except that European route numbers, if displayed, are always indicated using white characters on a green rectangle. European route numbers are, however, not signed at all in the United Kingdom.\n\nThe Convention recommends that certain signs – such as \"STOP\", \"ZONE\", etc. – be in English; however, use of the local language is also permitted. If a language uses non-Latin characters, a Latin-script transliteration of the names of cities and other important places should also be given. Road signs in the Republic of Ireland are bilingual, using Irish and English. Wales similarly uses bilingual Welsh–English signs, while some parts of Scotland have bilingual Scottish Gaelic–English signs. Finland also uses bilingual signs, in Finnish and Swedish. Signs in Belgium are in French, Dutch, or German depending on the region. In the Brussels Capital Region, road signs are in both French and Dutch. Signs in Switzerland are in French, German, Italian, or Romansh depending on the canton.\n\nEuropean countries – with the notable exception of the United Kingdom, where distances and lengths are indicated in miles, yards, feet, and inches, and speed limits are expressed in miles per hour – use the metric system on road signs.\n\nFor countries driving on the left, the convention stipulates that the traffic signs should be mirror images of those used in countries driving on the right. This practice, however, is not systematically followed in the four European countries driving on the left, Cyprus, Malta, the Republic of Ireland, and the United Kingdom. The convention permits the use of two background colours for danger and prohibition signs: white or yellow. Most countries use white, with a few – such as Finland, Iceland, Poland, and Sweden – opting for yellow as this tends to improve the winter-time visibility of signs in areas where snow is prevalent. In some countries, such as France, white is the normal background colour for such signs, but yellow is used for temporary signage (as, for example, at road works).\n\nEuropean traffic signs have been designed with the principles of heraldry in mind; i.e., the sign must be clear and able to be resolved at a glance. Most traffic signs conform to heraldic tincture rules, and use symbols rather than written texts for better semiotic clarity.\n\nCroatian road signs follow the Vienna convention (SFR Yugoslavia was the original signatory for Croatia, which is now a contracting party itself). The most common signs are: \n\nIn the first years following Croatia's independence, its traffic signs were the same as in the rest of the former Yugoslavia. In the early 2000s, replacement of the yellow background of warning signs began, and new signs now use a white background.\n\nThe signage typeface is SNV, as with the other countries of the former Yugoslavia.\n\nRoad signs in Iceland mainly follow the Vienna Convention, but use a variant of the colour scheme and minor design changes similar to the signs in Sweden.\n\nUntil the partition of Ireland in 1922 and the independence of the Irish Free State (now the Republic of Ireland), British standards applied across the island. In 1926 road sign standards similar to those used in the UK at the time were adopted. Law requires that the signs be written in both Irish and English.\n\nIn 1956, road signs in the Republic were changed from the UK standard with the adoption of US-style \"diamond\" signs for many road hazard warnings (junctions, bends, railway crossings, traffic lights). Some domestic signs were also invented, such as the keep-left sign (a black curved arrow pointing to the upper-left, although some are similar to the European \"white arrow on blue disk\" signs), while some other signs are not widely adopted outside Ireland, such as the no-entry sign (a black arrow pointing ahead in a white circle with a red slashed circumference).\n\nDirectional signage is similar to current United Kingdom standards. The same colours are used for directional signs in Ireland as in the UK, and the UK Transport and Motorway fonts are used. Unlike Wales and Scotland, where Welsh and Gaelic place-names use the upright Transport face, Irish place-names are rendered in an italic face.\n\nIn January 2005 Ireland adopted metric speed limits. Around 35,000 existing signs were replaced and a further 23,000 new signs erected bearing the speed limit in kilometres per hour. To avoid confusion with the old signs, each speed limit sign now has \"km/h\" beneath the numerals. Also, since the adoption of signs based on the \"Warboys Committee\" standard in 1977, Irish directional signs have used the metric system; however, unlike with the later speed limit changeover, there was no effort made to change the existing signage, and many finger posts still remain on rural roads with distances in miles, although the numbers continue to decline as roads are improved.\n\nIn late 2007 Ireland began an extensive programme of sign and post replacement. Good examples are the M1 (Dublin–Dundalk) and the M50 (Dublin). While being mostly the same as the old signs, it is welcome as a lot of the signs were damaged/stained. About half of the new posts are now two medium posts with crosshatched metal posts in-between instead of one large pole to minimise the damage in case of a crash.\n\nRoad signs in Latvia largely adhere to Vienna Convention guidelines. In detailed design they closely resemble the signs used in Germany.\n\nRoad signs in the Netherlands follow the Vienna Convention. Directional signs (which have not been harmonized under the Convention) always use blue as the background colour. The destinations on the sign are printed in white. If the destination is not a town (but an area within town or some other kind of attraction), that destination will be printed in black on a separate white background within the otherwise blue sign.\n\nThe Netherlands always signposts European road numbers where applicable (i.e., on the advance directional signs, the interchange direction signs and on the reassurance signs). Dutch national road numbers are placed on a rectangle, with motorways being signposted in white on a red rectangle (as an A\"xx\") and primary roads in black on a yellow rectangle (as N\"xx\"). When a motorway changes to a primary road, its number remains the same, but the A is replaced by the N. So at a certain point the A2 becomes N2, and when it changes to a motorway again, it becomes A2 again.\n\nSigns intended for bike-riders always go on white signs with red or green letters.\n\nThe Dutch typeface, known as ANWB-Ee, is based on the US typeface. A new font, named ANWB-Uu (also known as Redesign), has been developed in 1997 and appears on many recent Dutch signs. On the motorways however the typeface remains the ANWB-Ee or a similar typeface. The language of the signs is typically Dutch, even though bilingual signs may be used, when the information is relevant for tourists.\n\nSigns in Norway mostly follow the Vienna Convention, except the polar bear warning sign, which is a white bear on a black background and a red border. \nThese are the directional signs:\n\nThe signs for road numbering are rectangular, and have this colour scheme:\n\nThe road signs in Sweden mostly follow the Vienna Convention with a few adaptations, however, allowed within the convention:\n\nThe signage typeface Tratex is used exclusively in Sweden and is available as freeware.\n\nEven though Switzerland is not a member of the EU, the road signs mostly follow the Vienna Convention with a few adaptations and exceptions. Road signs are categorized as follows:\n\n\nDistances and other measurements are displayed in metric units. Starting 2003, ASTRA-Frutiger is the typeface used to replace SNV, which is still used in several European countries.\n\nMajor exceptions from the norm are:\n\n\n\nSee also Road rules of Switzerland.\n\nTraffic signing in the UK conforms broadly to European norms, though a number of signs are unique to Britain and direction signs omit European route numbers. The current sign system, introduced on 1 January 1965, was developed in the late 1950s and early 1960s by the Anderson Committee, which established the motorway signing system, and by the Worboys Committee, which reformed signing for existing all-purpose roads. (For illustrations of most British road signs, see 'Know your traffic signs' on the GOV.UK website.)\n\nThe UK remains the only European Union member nation and the only Commonwealth country to use non-metric (Imperial) measurements for distance and speed, although \"authorised weight\" signs have been in metric tonnes since 1981 and there is currently a dual-unit (metric first) option for height and width restriction signage, intended for use on safety grounds. On motorways kilometre signs are visible at intervals of indicating the distance from the start of the motorway. (See Driver location sign).\n\nThree colour schemes exist for direction signs: \n\nTwo typefaces are specified for British road signs. Transport \"Medium\" or Transport \"Heavy\" are used for all text on fixed permanent signs and most temporary signage, depending on the colour of the sign and associated text colour; dark text on a white background is normally set in \"Heavy\" so that it stands out better. However route numbers on motorway signs use a taller limited character set typeface called \"Motorway\".\n\nSigns are generally bilingual in all parts of Wales (English/Welsh or Welsh/English), and similar signs are beginning to be seen in parts of the Scottish Highlands (English/Scottish Gaelic).\n\nAll signs and their associated regulations can be found in the Traffic Signs Regulations and General Directions, as updated by the TSRGD 2008, TSRGD 2011 and TSRGD 2016 and complemented by the various chapters of the \"Traffic Signs Manual\".\n\nThe Northern American, Australian, and New Zealand colors normally have these meanings:\n\nThe US \"Manual on Uniform Traffic Control Devices\" prescribes four other colors:\n\n\nRegulatory signs are also sometimes seen with white letters on red or black signs. In Quebec, blue is often used for public services such as rest areas; many black-on-yellow signs are red-on-white instead.\n\nMany US states and Canadian provinces now use fluorescent orange for construction signs.\n\nEvery state and province has different markers for its own highways, but use standard ones for all federal highways. Many special highways – such as the Queen Elizabeth Way, Trans-Canada Highway, and various auto trails in the U.S. – have used unique signs. Counties in the US sometimes use a pentagonal blue sign with yellow letters for numbered county roads, though the use is inconsistent even within states.\n\nDistances on traffic signs generally follow the measurement system in use locally: that is to say, the metric system in all countries of the world except Burma, Liberia, the United Kingdom, and the United States – although the metric system is used in the UK for all purposes other than the display of road distances and the defining of speed limits, and in the US the Federal Department of Transportation has developed (very rarely used) metric standards for all signs.\n\nWhere signs use a language, the recognized language/s of the area is normally used. Signs in most of the US, Canada, Australia, and New Zealand are in English. Quebec uses French, while New Brunswick and the Jacques-Cartier and Champlain bridges, in Montreal (as well as some parts in the West Island), use both English and French, and a number of other provinces and states, such as Ontario, Manitoba, and Vermont use bilingual French–English signs in certain localities. Puerto Rico (a US territory) and Mexico use Spanish. Within a few miles of the US–Mexico border, road signs are often in English and Spanish in places like San Diego, Yuma, and El Paso. Indigenous languages, mainly Nahuatl as well as some Mayan languages, have been used as well.\n\nThe typefaces predominantly used on signs in the US and Canada are the FHWA alphabet series (Series B through Series F and Series E Modified). Details of letter shape and spacing for these alphabet series are given in \"Standard Alphabets for Traffic Control Devices\", first published by the Bureau of Public Roads (BPR) in 1945 and subsequently updated by the Federal Highway Administration (FHWA). It is now part of Standard Highway Signs (SHS), the companion volume to the MUTCD which gives full design details for signfaces.\n\nInitially, all of the alphabet series consisted of uppercase letters and digits only, although lowercase extensions were provided for each alphabet series in a 2002 revision of SHS. Series B through Series F evolved from identically named alphabet series which were introduced in 1927.\n\nStraight-stroke letters in the 1927 series were substantially similar to their modern equivalents, but unrounded glyphs were used for letters such as B, C, D, etc., to permit more uniform fabrication of signs by illiterate painters. Various state highway departments and the federal BPR experimented with rounded versions of these letters in the following two decades.\n\nThe modern, rounded alphabet series were finally standardized in 1945 after rounded versions of some letters (with widths loosely appropriate for Series C or D) were specified as an option in the 1935 MUTCD and draft versions of the new typefaces had been used in 1942 for guide signs on the newly constructed Pentagon road network.\n\nThe mixed-case alphabet now called Series E Modified, which is the standard for destination legend on freeway guide signs, originally existed in two parts: an all-uppercase Series E Modified, which was essentially similar to Series E, except for a larger stroke width, and a lowercase-only alphabet. Both parts were developed by the California Division of Highways (now Caltrans) for use on freeways in 1948–1950.\n\nInitially, the Division used all-uppercase Series E Modified for button-reflectorized letters on ground-mounted signs and mixed-case legend (lowercase letters with Series D capitals) for externally illuminated overhead guide signs. Several Eastern turnpike authorities blended all-uppercase Series E Modified with the lowercase alphabet for destination legends on their guide signs.\n\nEventually, this combination was accepted for destination legend in the first manual for signing Interstate highways, which was published in 1958 by the American Association of State Highway Officials and adopted as the national standard by the BPR.\n\nThe US National Park Service uses NPS Rawlinson Roadway, a serif typeface, for guide signage; it typically appears on a brown background. Rawlinson has replaced Clarendon as the official NPS typeface, but some states still use Clarendon for recreational signage.\n\nGeorgia, in the past, used uppercase Series D with a custom lowercase alphabet on its freeway guide signs; the most distinctive feature of this typeface is the lack of a dot on lowercase \"i\" and \"j\". More recent installations appear to include the dots.\n\nThe Clearview typeface, developed by US researchers to provide improved legibility, is permitted for light legend on dark backgrounds under FHWA interim approval. Clearview has seen widespread use by state departments of transportation in Arkansas, Arizona, Illinois, Kentucky, Maryland, Michigan, Ohio, Pennsylvania, Texas, and Virginia. The Kansas Turnpike Authority has also introduced Clearview typeface to some of its newer guide signs along the Kansas Turnpike, but the state of Kansas continues to use the FHWA typefaces for signage on its non-tolled Interstates and freeways.\n\nIn Canada, the Ministry of Transportation for the Province of British Columbia specifies Clearview for use on its highway guide signs, and its usage has shown up in Ontario on the Don Valley Parkway and Gardiner Expressway in Toronto and on new 400-series highway installations in Hamilton, Halton and Niagara, as well as street signs in various parts of the province. The font is also being used on newer signs in Alberta, Manitoba, and Quebec.\n\nIt is common for local governments, airport authorities, and contractors to fabricate traffic signs using typefaces other than the FHWA series; Helvetica, Futura and Arial are common choices.\n\nNew Zealand road signs are generally influenced both by American and European practices.\n\nWarning signs are diamond-shaped with a yellow background for permanent warnings, and an orange background for temporary warnings. They are somewhat more pictorial than their American counterparts. This is also true for Canadian signage.\n\nRegulatory signs also follow European practice, with a white circle with a red border indicating prohibitive actions, and a blue circle indicating mandatory actions. White rectangular signs with a red border indicate lane usage directions. Information and direction signs are rectangular, with a green background indicating a state highway, a blue background for all other roads and all services (except in some, where directional signage is white), and a brown background for tourist attractions.\n\nBefore 1987, most road signs had black backgrounds – diamonds indicated warnings, and rectangles indicated regulatory actions (with the exception of the Give Way sign (an inverted trapezium), and Stop sign and speed limit signs (which were the same as today)). Information signs were yellow, and direction signage was green on motorways and black everywhere else.\n\nRoad signs in Caribbean, Central America, and South America vary from country to country. For the most part, conventions in signage tend to resemble United States signage conventions more so than European and Asian conventions. For example, warning signs are typically diamond-shaped and yellow rather than triangular and white. Some variations include the \"Parking\" and \"No Parking\" signs, which contain either a letter \"E\" or \"P\", depending on which word is used locally for \"Parking\" (Spanish \"estacionamiento\" or \"parqueo\", Portuguese \"estacionamento\"), as well as the Stop sign, which usually reads \"Pare\" or \"Alto\". Notable exceptions include speed limit signs, which follow the European conventions, and the \"No Entry\" sign, often replaced with a crossed upwards arrow.\n\nTraffic signs in Colombia are classified into three categories: \n\nWarning signs are very similar to warning signs in United States. They are yellow diamond-shaped with a black symbol (the yellow colour is changed to an orange colour in areas under construction). In certain cases, the yellow colour is shifted to fluorescent yellow (in the School area sign and Chevron sign).\n\nMandatory signs are similar to European signs. They are circular with a red border, a white background and a black symbol. Stop sign and Yield sign are as European, except the word \"Stop\" is changed for \"Pare\" and the Yield sign has no letters, it is a red triangle with white centre.\n\nInformation signs have many shapes and colours. Principally they are blue with white symbols and in many cases these signs have an information letter below the symbol.\n\nRoad signs in Suriname are particularly modelled on the Dutch road signs since Suriname is a former Dutch colony, although traffic drives on the left.\n\nCars are beginning to feature cameras with automatic traffic sign recognition, beginning with the Opel Insignia. It mainly recognizes speed limits and no-overtaking areas.\n\n\n\n\n\n\n\n"}
{"id": "17529396", "url": "https://en.wikipedia.org/wiki?curid=17529396", "title": "Transference neurosis", "text": "Transference neurosis\n\nTransference neurosis is a term that Sigmund Freud introduced in 1914 to describe a new form of the analysand's infantile neurosis that develops during the psychoanalytic process. Based on Dora's case history, Freud suggested that during therapy the creation of new symptoms stops, but new versions of the patient's fantasies and impulses are generated. He called these newer versions \"transferences\" and characterized them as the substitution of the analyst for a person from the patient's past. According to Freud's description: \"a whole series of psychological experiences are revived not as belonging to the past, but as applying to the person of the analyst at the present moment\". When transference neurosis develops, the relationship with the therapist becomes the most important one for the patient, who directs strong infantile feelings and conflicts towards the therapist, e.g. the patient may react as if the analyst is his/her father.\n\nTransference neurosis can be distinguished from other kinds of transference because: \n\nOnce transference neurosis has developed, it leads to a form of resistance, called \"transference resistance\". At this point, the analysis of the transference becomes difficult since new obstacles arise in therapy, e.g. the analysand may insist on fulfilling the infantile wishes that emerged in transference, or may refuse to acknowledge that the current experience is, in fact, a reproduction of a past experience. However, the successful resolution of transference neurosis through interpretation will lead to the lifting of repression and will enable the Ego to solve the infantile conflicts in new ways. Furthermore, it will allow the analysand to recognize that the current relationship with the analyst is based on repetition of childhood experiences, leading to the detachment of the patient from the analyst.\n\nThe replacement of the infantile neurosis by transference neurosis and its resolution through interpretation remains the main focus of the classical psychoanalytic therapy. In other types of therapy, either the transference neurosis does not develop at all, or it does not play a central role in the therapy process. Although it is more likely for transference neurosis to develop in psychoanalysis, where the sessions are more frequent, it may also appear during psychotherapy.\n\n"}
{"id": "48035043", "url": "https://en.wikipedia.org/wiki?curid=48035043", "title": "Ulises Carrión", "text": "Ulises Carrión\n\nUlises Carrión (1941, San Andres Tuxtla, Mexico - 1989, Amsterdam, The Netherlands), considered as \"perhaps Mexico’s most important conceptual artist\", is widely known for his decisive role in defining and conceptualising the artistic genre artists' book through his manifest \"The New Art of Making Books\" (1975). But his alertness and interest in new forms of art and innovative operations implicated that he was active in most of the artistic fields of his time. The activities cover artworks, theory and independent initiatives. This includes not only a great number of bookworks - as he named artists' books - and unique artworks, but also performances, alongside film, video, and sound works, as well as several edition, publishing, and curating projects, a couple of considerable public projects, and various significant works and initiatives within the international community of mail artists during its most creative period. Equally essential for his artistic career is his engagement in several artists' run spaces. All his artistic activities were reflected by him in highly elaborated theories.\n\nCarrión was born in San Andres Tuxtla, Veracruz, Mexico in 1941. After studying philosophy and literature at the National University of Mexico, he started his career as a successful and respected young writer. In 1964 he received a grant for further studies at the Sorbonne Paris, France. Short after he went for studies to the Goethe Institute, Achenmühle, Germany, and to Leeds, England, where he studied English language and literature at the University and graduated with a diploma. In 1972 he definitively settled in Amsterdam, an open and cosmopolitan city with a lot of artistic innovation and international exchanges. Here he became co-founder and member of the \"In-Out Center\" (1972-1975). In 1975, he founded \"Other Books and So\", the first space of its kind devoted to all kind of artists’ publications, which in 1979 became the \"Other Books And So Archive\". He was also the co-founder of the \"Vereniging van Videokunstenaars\", later \"Time Based Arts\" (1983-1993) in Amsterdam, NL. Ulises Carrión died in 1989 in Amsterdam.\n\nDue to a certain cult of the overlooked and underestimated(post-)1960s avant-garde, Ulises Carrión has undergone an extraordinary appreciation in a few years. Exhibitions of his works and a flood of references to his artistic strategies and theories in current art works, essays, conferences, as well as newly annotated, listed, translated, and edited works give proof of the widespread reception today. The widespread reception of his manifesto \"The New Art of Making Books\" made him the central reference for the definition of the concept of the artist book. Also his bookshop gallery \"Other Books and So\" became, despite its short duration, a mythologized paragon. In particular in Latin America, Ulises Carrión is received for some time as an important conceptual compatriot.\n\n\n\n"}
{"id": "936012", "url": "https://en.wikipedia.org/wiki?curid=936012", "title": "Union violence", "text": "Union violence\n\nUnion violence is violence committed by unions or union members during labor disputes. When union violence has occurred, it has frequently been in the context of industrial unrest. Violence has ranged from isolated acts by individuals to wider campaigns of organised violence aimed at furthering union goals within an industrial dispute.\n\nAnti-union violence has also occurred frequently in the context of industrial unrest, and has often involved the collusion of management and government authorities, private agencies, or citizens' groups in organising violence against unions and their members.\n\nProtests and verbal abuse are routinely aimed against union members or replacement workers who cross picket lines (\"blacklegs\" or \"scabs\") during industrial disputes. The inherent aim of a union is to create a labor monopoly so as to balance the monopsony a large employer enjoys as a purchaser of labor. Strikebreakers threaten that goal and undermine the union's bargaining position, and occasionally this erupts into violent confrontation, with violence committed either by, or against, strikers. Some who have sought to explain such violence observe, if labor disputes are accompanied by violence, it may be because labor has no legal redress. In 1894, some workers declared:\n...\"the right of employers to manage their own business to suit themselves,\" is fast coming to mean in effect nothing less than a right to manage the country to suit themselves.\n\nOccasionally, violent disputes occur between unions, when one union breaks another's strike.\n\nResearchers in industrial relations, criminology, and wider cultural studies have examined violence by workers or trade unions in the context of industrial disputes. US and Australian government reports have examined violence during industrial disputes.\n\n\n\n\n\nPeople\n\n"}
{"id": "32370", "url": "https://en.wikipedia.org/wiki?curid=32370", "title": "Vector space", "text": "Vector space\n\nA vector space (also called a linear space) is a collection of objects called vectors, which may be added together and multiplied (\"scaled\") by numbers, called \"scalars\". Scalars are often taken to be real numbers, but there are also vector spaces with scalar multiplication by complex numbers, rational numbers, or generally any field. The operations of vector addition and scalar multiplication must satisfy certain requirements, called \"axioms\", listed below.\n\nEuclidean vectors are an example of a vector space. They represent physical quantities such as forces: any two forces (of the same type) can be added to yield a third, and the multiplication of a force vector by a real multiplier is another force vector. In the same vein, but in a more geometric sense, vectors representing displacements in the plane or in three-dimensional space also form vector spaces. Vectors in vector spaces do not necessarily have to be arrow-like objects as they appear in the mentioned examples: vectors are regarded as abstract mathematical objects with particular properties, which in some cases can be visualized as arrows.\n\nVector spaces are the subject of linear algebra and are well characterized by their dimension, which, roughly speaking, specifies the number of independent directions in the space. Infinite-dimensional vector spaces arise naturally in mathematical analysis, as function spaces, whose vectors are functions. These vector spaces are generally endowed with additional structure, which may be a topology, allowing the consideration of issues of proximity and continuity. Among these topologies, those that are defined by a norm or inner product are more commonly used, as having a notion of distance between two vectors. This is particularly the case of Banach spaces and Hilbert spaces, which are fundamental in mathematical analysis.\n\nHistorically, the first ideas leading to vector spaces can be traced back as far as the 17th century's analytic geometry, matrices, systems of linear equations, and Euclidean vectors. The modern, more abstract treatment, first formulated by Giuseppe Peano in 1888, encompasses more general objects than Euclidean space, but much of the theory can be seen as an extension of classical geometric ideas like lines, planes and their higher-dimensional analogs.\n\nToday, vector spaces are applied throughout mathematics, science and engineering. They are the appropriate linear-algebraic notion to deal with systems of linear equations. They offer a framework for Fourier expansion, which is employed in image compression routines, and they provide an environment that can be used for solution techniques for partial differential equations. Furthermore, vector spaces furnish an abstract, coordinate-free way of dealing with geometrical and physical objects such as tensors. This in turn allows the examination of local properties of manifolds by linearization techniques. Vector spaces may be generalized in several ways, leading to more advanced notions in geometry and abstract algebra.\n\nThe concept of vector space will first be explained by describing two particular examples:\n\nThe first example of a vector space consists of arrows in a fixed plane, starting at one fixed point. This is used in physics to describe forces or velocities. Given any two such arrows, and , the parallelogram spanned by these two arrows contains one diagonal arrow that starts at the origin, too. This new arrow is called the \"sum\" of the two arrows and is denoted . In the special case of two arrows on the same line, their sum is the arrow on this line whose length is the sum or the difference of the lengths, depending on whether the arrows have the same direction. Another operation that can be done with arrows is scaling: given any positive real number , the arrow that has the same direction as , but is dilated or shrunk by multiplying its length by , is called \"multiplication\" of by . It is denoted . When is negative, is defined as the arrow pointing in the opposite direction, instead.\n\nThe following shows a few examples: if , the resulting vector has the same direction as , but is stretched to the double length of (right image below). Equivalently, is the sum . Moreover, has the opposite direction and the same length as (blue vector pointing down in the right image).\n\nA second key example of a vector space is provided by pairs of real numbers and . (The order of the components and is significant, so such a pair is also called an ordered pair.) Such a pair is written as . The sum of two such pairs and multiplication of a pair with a number is defined as follows:\nand \n\nThe first example above reduces to this one if the arrows are represented by the pair of Cartesian coordinates of their end points.\n\nIn this article, vectors are represented in boldface to distinguish them from scalars.\n\nA vector space over a field is a set  together with two operations that satisfy the eight axioms listed below.\n\n\nElements of are commonly called \"vectors\". Elements of  are commonly called \"scalars\".\n\nIn the two examples above, the field is the field of the real numbers and the set of the vectors consists of the planar arrows with fixed starting point and of pairs of real numbers, respectively.\n\nTo qualify as a vector space, the set  and the operations of addition and multiplication must adhere to a number of requirements called axioms. In the list below, let , and be arbitrary vectors in , and and scalars in .\n\nThese axioms generalize properties of the vectors introduced in the above examples. Indeed, the result of addition of two ordered pairs (as in the second example above) does not depend on the order of the summands:\nLikewise, in the geometric example of vectors as arrows, since the parallelogram defining the sum of the vectors is independent of the order of the vectors. All other axioms can be checked in a similar manner in both examples. Thus, by disregarding the concrete nature of the particular type of vectors, the definition incorporates these two and many more examples in one notion of vector space.\n\nSubtraction of two vectors and division by a (non-zero) scalar can be defined as\n\nWhen the scalar field is the real numbers , the vector space is called a \"real vector space\". When the scalar field is the complex numbers , the vector space is called a \"complex vector space\". These two cases are the ones used most often in engineering. The general definition of a vector space allows scalars to be elements of any fixed field . The notion is then known as an -\"vector spaces\" or a \"vector space over \". A field is, essentially, a set of numbers possessing addition, subtraction, multiplication and division operations. For example, rational numbers form a field.\n\nIn contrast to the intuition stemming from vectors in the plane and higher-dimensional cases, there is, in general vector spaces, no notion of nearness, angles or distances. To deal with such matters, particular types of vector spaces are introduced; see below.\n\nVector addition and scalar multiplication are operations, satisfying the closure property: and are in for all in , and , in . Some older sources mention these properties as separate axioms.\n\nIn the parlance of abstract algebra, the first four axioms are equivalent to requiring the set of vectors to be an abelian group under addition. The remaining axioms give this group an -module structure. In other words, there is a ring homomorphism from the field into the endomorphism ring of the group of vectors. Then scalar multiplication is defined as .\n\nThere are a number of direct consequences of the vector space axioms. Some of them derive from elementary group theory, applied to the additive group of vectors: for example, the zero vector of and the additive inverse of any vector are unique. Further properties follow by employing also the distributive law for the scalar multiplication, for example equals if and only if equals or equals .\n\nVector spaces stem from affine geometry via the introduction of coordinates in the plane or three-dimensional space. Around 1636, Descartes and Fermat founded analytic geometry by equating solutions to an equation of two variables with points on a plane curve. In 1804, to achieve geometric solutions without using coordinates, Bolzano introduced certain operations on points, lines and planes, which are predecessors of vectors. His work was then used in the conception of barycentric coordinates by Möbius in 1827. In 1828 C. V. Mourey suggested the existence of an algebra surpassing not only ordinary algebra but also two-dimensional algebra created by him searching a geometrical interpretation of complex numbers.\n\nThe definition of vectors was founded on Bellavitis' notion of the bipoint, an oriented segment of which one end is the origin and the other a target, then further elaborated with the presentation of complex numbers by Argand and Hamilton and the introduction of quaternions and biquaternions by the latter. They are elements in , , and ; their treatment as linear combinations can be traced back to Laguerre in 1867, who also defined systems of linear equations.\n\nIn 1857, Cayley introduced matrix notation, which allows for a harmonization and simplification of linear maps. Around the same time, Grassmann studied the barycentric calculus initiated by Möbius. He envisaged sets of abstract objects endowed with operations. In his work, the concepts of linear independence and dimension, as well as scalar products, are present. In fact, Grassmann's 1844 work extended a vector space of \"n\" dimensions to one of 2 dimensions by consideration of 2-vectors formula_1 and 3-vectors formula_2 called multivectors. This extension, called multilinear algebra, is governed by the rules of exterior algebra. Peano was the first to give the modern definition of vector spaces and linear maps in 1888.\n\nAn important development of vector spaces is due to the construction of function spaces by Lebesgue. This was later formalized by Banach and Hilbert, around 1920. At that time, algebra and the new field of functional analysis began to interact, notably with key concepts such as spaces of \"p\"-integrable functions and Hilbert spaces. Vector spaces, including infinite-dimensional ones, then became a firmly established notion, and many mathematical branches started making use of this concept.\n\nThe simplest example of a vector space over a field is the field itself, equipped with its standard addition and multiplication. More generally, a vector space can be composed of\n\"n\"-tuples (sequences of length ) of elements of , such as\nA vector space composed of all the -tuples of a field is known as a \"coordinate space\", usually denoted . The case is the above-mentioned simplest example, in which the field is also regarded as a vector space over itself. The case and was discussed in the introduction above.\n\nThe set of complex numbers , i.e., numbers that can be written in the form for real numbers and where is the imaginary unit, form a vector space over the reals with the usual addition and multiplication: and for real numbers , , , and . The various axioms of a vector space follow from the fact that the same rules hold for complex number arithmetic.\n\nIn fact, the example of complex numbers is essentially the same (i.e., it is \"isomorphic\") to the vector space of ordered pairs of real numbers mentioned above: if we think of the complex number as representing the ordered pair in the complex plane then we see that the rules for sum and scalar product correspond exactly to those in the earlier example.\n\nMore generally, field extensions provide another class of examples of vector spaces, particularly in algebra and algebraic number theory: a field containing a smaller field is an -vector space, by the given multiplication and addition operations of . For example, the complex numbers are a vector space over , and the field extension formula_3 is a vector space over . \n\nFunctions from any fixed set to a field also form vector spaces, by performing addition and scalar multiplication pointwise. That is, the sum of two functions and is the function given by\nand similarly for multiplication. Such function spaces occur in many geometric situations, when is the real line or an interval, or other subsets of . Many notions in topology and analysis, such as continuity, integrability or differentiability are well-behaved with respect to linearity: sums and scalar multiples of functions possessing such a property still have that property. Therefore, the set of such functions are vector spaces. They are studied in greater detail using the methods of functional analysis, see below. Algebraic constraints also yield vector spaces: the vector space is given by polynomial functions:\n\nSystems of homogeneous linear equations are closely tied to vector spaces. For example, the solutions of \nare given by triples with arbitrary , , and . They form a vector space: sums and scalar multiples of such triples still satisfy the same ratios of the three variables; thus they are solutions, too. Matrices can be used to condense multiple linear equations as above into one vector equation, namely\nwhere formula_4 is the matrix containing the coefficients of the given equations, is the vector , denotes the matrix product, and is the zero vector. In a similar vein, the solutions of homogeneous \"linear differential equations\" form vector spaces. For example,\nyields , where and are arbitrary constants, and is the natural exponential function.\n\n\"Bases\" allow one to represent vectors by a sequence of scalars called \"coordinates\" or \"components\". A basis is a (finite or infinite) set of vectors , for convenience often indexed by some index set , that spans the whole space and is linearly independent. \"Spanning the whole space\" means that any vector can be expressed as a finite sum (called a \"linear combination\") of the basis elements:\n\nwhere the are scalars, called the coordinates (or the components) of the vector with respect to the basis , and elements of . Linear independence means that the coordinates are uniquely determined for any vector in the vector space.\n\nFor example, the coordinate vectors , , to , form a basis of , called the standard basis, since any vector can be uniquely expressed as a linear combination of these vectors:\nThe corresponding coordinates , , , are just the Cartesian coordinates of the vector.\n\nEvery vector space has a basis. This follows from Zorn's lemma, an equivalent formulation of the Axiom of Choice. Given the other axioms of Zermelo–Fraenkel set theory, the existence of bases is equivalent to the axiom of choice. The ultrafilter lemma, which is weaker than the axiom of choice, implies that all bases of a given vector space have the same number of elements, or cardinality (cf. \"Dimension theorem for vector spaces\"). It is called the \"dimension\" of the vector space, denoted by dim \"V\". If the space is spanned by finitely many vectors, the above statements can be proven without such fundamental input from set theory.\n\nThe dimension of the coordinate space is , by the basis exhibited above. The dimension of the polynomial ring \"F\"[\"x\"] introduced above is countably infinite, a basis is given by , , , A fortiori, the dimension of more general function spaces, such as the space of functions on some (bounded or unbounded) interval, is infinite. Under suitable regularity assumptions on the coefficients involved, the dimension of the solution space of a homogeneous ordinary differential equation equals the degree of the equation. For example, the solution space for the above equation is generated by . These two functions are linearly independent over , so the dimension of this space is two, as is the degree of the equation.\n\nA field extension over the rationals can be thought of as a vector space over (by defining vector addition as field addition, defining scalar multiplication as field multiplication by elements of , and otherwise ignoring the field multiplication). The dimension (or degree) of the field extension over depends on . If satisfies some polynomial equation\nformula_5\nwith rational coefficients (in other words, if α is algebraic), the dimension is finite. More precisely, it equals the degree of the minimal polynomial having α as a root. For example, the complex numbers C are a two-dimensional real vector space, generated by 1 and the imaginary unit \"i\". The latter satisfies \"i\" + 1 = 0, an equation of degree two. Thus, C is a two-dimensional R-vector space (and, as any field, one-dimensional as a vector space over itself, C). If α is not algebraic, the dimension of Q(α) over Q is infinite. For instance, for α = π there is no such equation, in other words π is transcendental.\n\nThe relation of two vector spaces can be expressed by \"linear map\" or \"linear transformation\". They are functions that reflect the vector space structure—i.e., they preserve sums and scalar multiplication:\n\nAn \"isomorphism\" is a linear map such that there exists an inverse map , which is a map such that the two possible compositions and are identity maps. Equivalently, \"f\" is both one-to-one (injective) and onto (surjective). If there exists an isomorphism between \"V\" and \"W\", the two spaces are said to be \"isomorphic\"; they are then essentially identical as vector spaces, since all identities holding in \"V\" are, via \"f\", transported to similar ones in \"W\", and vice versa via \"g\".\nFor example, the \"arrows in the plane\" and \"ordered pairs of numbers\" vector spaces in the introduction are isomorphic: a planar arrow v departing at the origin of some (fixed) coordinate system can be expressed as an ordered pair by considering the \"x\"- and \"y\"-component of the arrow, as shown in the image at the right. Conversely, given a pair (\"x\", \"y\"), the arrow going by \"x\" to the right (or to the left, if \"x\" is negative), and \"y\" up (down, if \"y\" is negative) turns back the arrow v.\n\nLinear maps \"V\" → \"W\" between two vector spaces form a vector space Hom(\"V\", \"W\"), also denoted L(\"V\", \"W\"). The space of linear maps from \"V\" to \"F\" is called the \"dual vector space\", denoted \"V\". Via the injective natural map , any vector space can be embedded into its \"bidual\"; the map is an isomorphism if and only if the space is finite-dimensional.\n\nOnce a basis of is chosen, linear maps are completely determined by specifying the images of the basis vectors, because any element of \"V\" is expressed uniquely as a linear combination of them. If , a 1-to-1 correspondence between fixed bases of and gives rise to a linear map that maps any basis element of to the corresponding basis element of . It is an isomorphism, by its very definition. Therefore, two vector spaces are isomorphic if their dimensions agree and vice versa. Another way to express this is that any vector space is \"completely classified\" (up to isomorphism) by its dimension, a single number. In particular, any \"n\"-dimensional -vector space is isomorphic to . There is, however, no \"canonical\" or preferred isomorphism; actually an isomorphism is equivalent to the choice of a basis of , by mapping the standard basis of to , via . The freedom of choosing a convenient basis is particularly useful in the infinite-dimensional context; see below.\n\n\"Matrices\" are a useful notion to encode linear maps. They are written as a rectangular array of scalars as in the image at the right. Any \"m\"-by-\"n\" matrix \"A\" gives rise to a linear map from \"F\" to \"F\", by the following\nor, using the matrix multiplication of the matrix with the coordinate vector :\nMoreover, after choosing bases of and , \"any\" linear map is uniquely represented by a matrix via this assignment.\nThe determinant of a square matrix is a scalar that tells whether the associated map is an isomorphism or not: to be so it is sufficient and necessary that the determinant is nonzero. The linear transformation of corresponding to a real \"n\"-by-\"n\" matrix is orientation preserving if and only if its determinant is positive.\n\nEndomorphisms, linear maps , are particularly important since in this case vectors can be compared with their image under , . Any nonzero vector satisfying , where is a scalar, is called an \"eigenvector\" of with \"eigenvalue\" . Equivalently, is an element of the kernel of the difference (where Id is the identity map . If is finite-dimensional, this can be rephrased using determinants: having eigenvalue is equivalent to\nBy spelling out the definition of the determinant, the expression on the left hand side can be seen to be a polynomial function in , called the characteristic polynomial of . If the field is large enough to contain a zero of this polynomial (which automatically happens for algebraically closed, such as ) any linear map has at least one eigenvector. The vector space may or may not possess an eigenbasis, a basis consisting of eigenvectors. This phenomenon is governed by the Jordan canonical form of the map. The set of all eigenvectors corresponding to a particular eigenvalue of forms a vector space known as the \"eigenspace\" corresponding to the eigenvalue (and ) in question. To achieve the spectral theorem, the corresponding statement in the infinite-dimensional case, the machinery of functional analysis is needed, see below.\n\nIn addition to the above concrete examples, there are a number of standard linear algebraic constructions that yield vector spaces related to given ones. In addition to the definitions given below, they are also characterized by universal properties, which determine an object by specifying the linear maps from to any other vector space.\n\nA nonempty subset \"W\" of a vector space \"V\" that is closed under addition and scalar multiplication (and therefore contains the 0-vector of \"V\") is called a \"linear subspace\" of \"V\", or simply a \"subspace\" of \"V\", when the ambient space is unambiguously a vector space. Subspaces of \"V\" are vector spaces (over the same field) in their own right. The intersection of all subspaces containing a given set \"S\" of vectors is called its span, and it is the smallest subspace of \"V\" containing the set \"S\". Expressed in terms of elements, the span is the subspace consisting of all the linear combinations of elements of \"S\".\n\nA linear subspace of dimension 1 is a vector line. A linear subspace of dimension 2 is a vector plane. A linear subspace that contains all elements but one of a basis of the ambient space is a vector hyperplane. In a vector space of finite dimension , a vector hyperplane is thus a subspace of dimension .\n\nThe counterpart to subspaces are \"quotient vector spaces\". Given any subspace , the quotient space \"V\"/\"W\" (\"\"V\" modulo \"W\"\") is defined as follows: as a set, it consists of where v is an arbitrary vector in \"V\". The sum of two such elements and is and scalar multiplication is given by . The key point in this definition is that if and only if the difference of v and v lies in \"W\". This way, the quotient space \"forgets\" information that is contained in the subspace \"W\".\n\nThe kernel ker(\"f\") of a linear map consists of vectors v that are mapped to 0 in \"W\". Both kernel and image are subspaces of \"V\" and \"W\", respectively. The existence of kernels and images is part of the statement that the category of vector spaces (over a fixed field \"F\") is an abelian category, i.e. a corpus of mathematical objects and structure-preserving maps between them (a category) that behaves much like the category of abelian groups. Because of this, many statements such as the first isomorphism theorem (also called rank–nullity theorem in matrix-related terms)\nand the second and third isomorphism theorem can be formulated and proven in a way very similar to the corresponding statements for groups.\n\nAn important example is the kernel of a linear map for some fixed matrix \"A\", as above. The kernel of this map is the subspace of vectors x such that , which is precisely the set of solutions to the system of homogeneous linear equations belonging to \"A\". This concept also extends to linear differential equations\nIn the corresponding map\nthe derivatives of the function \"f\" appear linearly (as opposed to \"f\"′′(\"x\"), for example). Since differentiation is a linear procedure (i.e., and for a constant ) this assignment is linear, called a linear differential operator. In particular, the solutions to the differential equation form a vector space (over or ).\n\nThe \"direct product\" of vector spaces and the \"direct sum\" of vector spaces are two ways of combining an indexed family of vector spaces into a new vector space.\n\nThe \"direct product\" formula_10 of a family of vector spaces \"V\" consists of the set of all tuples (, which specify for each index \"i\" in some index set \"I\" an element v of \"V\". Addition and scalar multiplication is performed componentwise. A variant of this construction is the \"direct sum\" formula_11 (also called coproduct and denoted formula_12), where only tuples with finitely many nonzero vectors are allowed. If the index set \"I\" is finite, the two constructions agree, but in general they are different.\n\nThe \"tensor product\" , or simply , of two vector spaces \"V\" and \"W\" is one of the central notions of multilinear algebra which deals with extending notions such as linear maps to several variables. A map is called bilinear if \"g\" is linear in both variables v and w. That is to say, for fixed w the map is linear in the sense above and likewise for fixed v.\n\nThe tensor product is a particular vector space that is a \"universal\" recipient of bilinear maps \"g\", as follows. It is defined as the vector space consisting of finite (formal) sums of symbols called tensors\nsubject to the rules \n\nThese rules ensure that the map \"f\" from the to that maps a tuple to is bilinear. The universality states that given \"any\" vector space \"X\" and \"any\" bilinear map , there exists a unique map \"u\", shown in the diagram with a dotted arrow, whose composition with \"f\" equals \"g\": . This is called the universal property of the tensor product, an instance of the method—much used in advanced abstract algebra—to indirectly define objects by specifying maps from or to this object.\n\nFrom the point of view of linear algebra, vector spaces are completely understood insofar as any vector space is characterized, up to isomorphism, by its dimension. However, vector spaces \"per se\" do not offer a framework to deal with the question—crucial to analysis—whether a sequence of functions converges to another function. Likewise, linear algebra is not adapted to deal with infinite series, since the addition operation allows only finitely many terms to be added. Therefore, the needs of functional analysis require considering additional structures.\n\nA vector space may be given a partial order ≤, under which some vectors can be compared. For example, \"n\"-dimensional real space R can be ordered by comparing its vectors componentwise. Ordered vector spaces, for example Riesz spaces, are fundamental to Lebesgue integration, which relies on the ability to express a function as a difference of two positive functions\nwhere \"f\" denotes the positive part of \"f\" and \"f\" the negative part.\n\n\"Measuring\" vectors is done by specifying a norm, a datum which measures lengths of vectors, or by an inner product, which measures angles between vectors. Norms and inner products are denoted formula_13 and formula_14, respectively. The datum of an inner product entails that lengths of vectors can be defined too, by defining the associated norm formula_15. Vector spaces endowed with such data are known as \"normed vector spaces\" and \"inner product spaces\", respectively.\n\nCoordinate space \"F\" can be equipped with the standard dot product:\nIn R, this reflects the common notion of the angle between two vectors x and y, by the law of cosines:\nBecause of this, two vectors satisfying formula_18 are called orthogonal. An important variant of the standard dot product is used in Minkowski space: R endowed with the Lorentz product\nIn contrast to the standard dot product, it is not positive definite: formula_20 also takes negative values, for example for formula_21. Singling out the fourth coordinate—corresponding to time, as opposed to three space-dimensions—makes it useful for the mathematical treatment of special relativity.\n\nConvergence questions are treated by considering vector spaces \"V\" carrying a compatible topology, a structure that allows one to talk about elements being close to each other. Compatible here means that addition and scalar multiplication have to be continuous maps. Roughly, if x and y in \"V\", and \"a\" in \"F\" vary by a bounded amount, then so do and . To make sense of specifying the amount a scalar changes, the field \"F\" also has to carry a topology in this context; a common choice are the reals or the complex numbers.\n\nIn such \"topological vector spaces\" one can consider series of vectors. The infinite sum\ndenotes the limit of the corresponding finite partial sums of the sequence (\"f\") of elements of \"V\". For example, the \"f\" could be (real or complex) functions belonging to some function space \"V\", in which case the series is a function series. The mode of convergence of the series depends on the topology imposed on the function space. In such cases, pointwise convergence and uniform convergence are two prominent examples.\nA way to ensure the existence of limits of certain infinite series is to restrict attention to spaces where any Cauchy sequence has a limit; such a vector space is called complete. Roughly, a vector space is complete provided that it contains all necessary limits. For example, the vector space of polynomials on the unit interval [0,1], equipped with the topology of uniform convergence is not complete because any continuous function on [0,1] can be uniformly approximated by a sequence of polynomials, by the Weierstrass approximation theorem. In contrast, the space of \"all\" continuous functions on [0,1] with the same topology is complete. A norm gives rise to a topology by defining that a sequence of vectors v converges to v if and only if\nBanach and Hilbert spaces are complete topological vector spaces whose topologies are given, respectively, by a norm and an inner product. Their study—a key piece of functional analysis—focusses on infinite-dimensional vector spaces, since all norms on finite-dimensional topological vector spaces give rise to the same notion of convergence. The image at the right shows the equivalence of the 1-norm and ∞-norm on R: as the unit \"balls\" enclose each other, a sequence converges to zero in one norm if and only if it so does in the other norm. In the infinite-dimensional case, however, there will generally be inequivalent topologies, which makes the study of topological vector spaces richer than that of vector spaces without additional data.\n\nFrom a conceptual point of view, all notions related to topological vector spaces should match the topology. For example, instead of considering all linear maps (also called functionals) , maps between topological vector spaces are required to be continuous. In particular, the (topological) dual space consists of continuous functionals (or to ). The fundamental Hahn–Banach theorem is concerned with separating subspaces of appropriate topological vector spaces by continuous functionals.\n\n\"Banach spaces\", introduced by Stefan Banach, are complete normed vector spaces. \n\nA first example is the vector space formula_24 \nconsisting of infinite vectors with real entries \nformula_25\nwhose formula_26-norm \nformula_27\ngiven by \nThe topologies on the infinite-dimensional space \nformula_24 \nare inequivalent for different formula_26. \nE.g. the sequence of vectors \nformula_33, \ni.e. the first formula_34 components are formula_35, \nthe following ones are formula_36, converges to the zero vector for \nformula_37, \nbut does not for \nformula_38:\n\nMore generally than sequences of real numbers, functions \nformula_41\nare endowed with a norm that replaces the above sum by the Lebesgue integral\nThe space of integrable functions on a given domain \nformula_43 \n(for example an interval) satisfying \nformula_44,\nand equipped with this norm are called Lebesgue spaces, \ndenoted formula_45. \n\nThese spaces are complete. (If one uses the Riemann integral instead, the space is \"not\" complete, which may be seen as a justification for Lebesgue's integration theory.) \nConcretely this means that for any sequence of Lebesgue-integrable functions   \nformula_46  \nwith formula_47, \nsatisfying the condition\nthere exists a function formula_49 belonging to the vector space formula_45 such that\n\nImposing boundedness conditions not only on the function, but also on its derivatives leads to Sobolev spaces.\nComplete inner product spaces are known as \"Hilbert spaces\", in honor of David Hilbert.\nThe Hilbert space \"L\"(Ω), with inner product given by\nwhere formula_53 denotes the complex conjugate of \"g\"(\"x\"), is a key case.\n\nBy definition, in a Hilbert space any Cauchy sequence converges to a limit. Conversely, finding a sequence of functions \"f\" with desirable properties that approximates a given limit function, is equally crucial. Early analysis, in the guise of the Taylor approximation, established an approximation of differentiable functions \"f\" by polynomials. By the Stone–Weierstrass theorem, every continuous function on can be approximated as closely as desired by a polynomial. A similar approximation technique by trigonometric functions is commonly called Fourier expansion, and is much applied in engineering, see below. More generally, and more conceptually, the theorem yields a simple description of what \"basic functions\", or, in abstract Hilbert spaces, what basic vectors suffice to generate a Hilbert space \"H\", in the sense that the \"closure\" of their span (i.e., finite linear combinations and limits of those) is the whole space. Such a set of functions is called a \"basis\" of \"H\", its cardinality is known as the Hilbert space dimension. Not only does the theorem exhibit suitable basis functions as sufficient for approximation purposes, but together with the Gram–Schmidt process, it enables one to construct a basis of orthogonal vectors. Such orthogonal bases are the Hilbert space generalization of the coordinate axes in finite-dimensional Euclidean space.\n\nThe solutions to various differential equations can be interpreted in terms of Hilbert spaces. For example, a great many fields in physics and engineering lead to such equations and frequently solutions with particular physical properties are used as basis functions, often orthogonal. As an example from physics, the time-dependent Schrödinger equation in quantum mechanics describes the change of physical properties in time by means of a partial differential equation, whose solutions are called wavefunctions. Definite values for physical properties such as energy, or momentum, correspond to eigenvalues of a certain (linear) differential operator and the associated wavefunctions are called eigenstates. The spectral theorem decomposes a linear compact operator acting on functions in terms of these eigenfunctions and their eigenvalues.\nGeneral vector spaces do not possess a multiplication between vectors. A vector space equipped with an additional bilinear operator defining the multiplication of two vectors is an \"algebra over a field\". Many algebras stem from functions on some geometrical object: since functions with values in a given field can be multiplied pointwise, these entities form algebras. The Stone–Weierstrass theorem mentioned above, for example, relies on Banach algebras which are both Banach spaces and algebras.\n\nCommutative algebra makes great use of rings of polynomials in one or several variables, introduced above. Their multiplication is both commutative and associative. These rings and their quotients form the basis of algebraic geometry, because they are rings of functions of algebraic geometric objects.\n\nAnother crucial example are \"Lie algebras\", which are neither commutative nor associative, but the failure to be so is limited by the constraints ( denotes the product of and ):\nExamples include the vector space of \"n\"-by-\"n\" matrices, with , the commutator of two matrices, and , endowed with the cross product.\n\nThe tensor algebra T(\"V\") is a formal way of adding products to any vector space \"V\" to obtain an algebra. As a vector space, it is spanned by symbols, called simple tensors\nThe multiplication is given by concatenating such symbols, imposing the distributive law under addition, and requiring that scalar multiplication commute with the tensor product ⊗, much the same way as with the tensor product of two vector spaces introduced above. In general, there are no relations between and . Forcing two such elements to be equal leads to the symmetric algebra, whereas forcing yields the exterior algebra.\n\nWhen a field, is explicitly stated, a common term used is -algebra.\n\nVector spaces have many applications as they occur frequently in common circumstances, namely wherever functions with values in some field are involved. They provide a framework to deal with analytical and geometrical problems, or are used in the Fourier transform. This list is not exhaustive: many more applications exist, for example in optimization. The minimax theorem of game theory stating the existence of a unique payoff when all players play optimally can be formulated and proven using vector spaces methods. Representation theory fruitfully transfers the good understanding of linear algebra and vector spaces to other mathematical domains such as group theory.\n\nA \"distribution\" (or \"generalized function\") is a linear map assigning a number to each \"test\" function, typically a smooth function with compact support, in a continuous way: in the above terminology the space of distributions is the (continuous) dual of the test function space. The latter space is endowed with a topology that takes into account not only \"f\" itself, but also all its higher derivatives. A standard example is the result of integrating a test function \"f\" over some domain Ω:\nWhen the set consisting of a single point, this reduces to the Dirac distribution, denoted by δ, which associates to a test function \"f\" its value at the . Distributions are a powerful instrument to solve differential equations. Since all standard analytic notions such as derivatives are linear, they extend naturally to the space of distributions. Therefore, the equation in question can be transferred to a distribution space, which is bigger than the underlying function space, so that more flexible methods are available for solving the equation. For example, Green's functions and fundamental solutions are usually distributions rather than proper functions, and can then be used to find solutions of the equation with prescribed boundary conditions. The found solution can then in some cases be proven to be actually a true function, and a solution to the original equation (e.g., using the Lax–Milgram theorem, a consequence of the Riesz representation theorem).\n\nResolving a periodic function into a sum of trigonometric functions forms a \"Fourier series\", a technique much used in physics and engineering. The underlying vector space is usually the Hilbert space \"L\"(0, 2π), for which the functions sin \"mx\" and cos \"mx\" (\"m\" an integer) form an orthogonal basis. The Fourier expansion of an \"L\" function \"f\" is\n\nThe coefficients \"a\" and \"b\" are called Fourier coefficients of \"f\", and are calculated by the formulas\n\nIn physical terms the function is represented as a superposition of sine waves and the coefficients give information about the function's frequency spectrum. A complex-number form of Fourier series is also commonly used. The concrete formulae above are consequences of a more general mathematical duality called Pontryagin duality. Applied to the group R, it yields the classical Fourier transform; an application in physics are reciprocal lattices, where the underlying group is a finite-dimensional real vector space endowed with the additional datum of a lattice encoding positions of atoms in crystals.\n\nFourier series are used to solve boundary value problems in partial differential equations. In 1822, Fourier first used this technique to solve the heat equation. A discrete version of the Fourier series can be used in sampling applications where the function value is known only at a finite number of equally spaced points. In this case the Fourier series is finite and its value is equal to the sampled values at all points. The set of coefficients is known as the discrete Fourier transform (DFT) of the given sample sequence. The DFT is one of the key tools of digital signal processing, a field whose applications include radar, speech encoding, image compression. The JPEG image format is an application of the closely related discrete cosine transform.\n\nThe fast Fourier transform is an algorithm for rapidly computing the discrete Fourier transform. It is used not only for calculating the Fourier coefficients but, using the convolution theorem, also for computing the convolution of two finite sequences. They in turn are applied in digital filters and as a rapid multiplication algorithm for polynomials and large integers (Schönhage–Strassen algorithm).\n\nThe tangent plane to a surface at a point is naturally a vector space whose origin is identified with the point of contact. The tangent plane is the best linear approximation, or linearization, of a surface at a point. Even in a three-dimensional Euclidean space, there is typically no natural way to prescribe a basis of the tangent plane, and so it is conceived of as an abstract vector space rather than a real coordinate space. The \"tangent space\" is the generalization to higher-dimensional differentiable manifolds.\n\nRiemannian manifolds are manifolds whose tangent spaces are endowed with a suitable inner product. Derived therefrom, the Riemann curvature tensor encodes all curvatures of a manifold in one object, which finds applications in general relativity, for example, where the Einstein curvature tensor describes the matter and energy content of space-time. The tangent space of a Lie group can be given naturally the structure of a Lie algebra and can be used to classify compact Lie groups.\n\nA \"vector bundle\" is a family of vector spaces parametrized continuously by a topological space \"X\". More precisely, a vector bundle over \"X\" is a topological space \"E\" equipped with a continuous map \nsuch that for every \"x\" in \"X\", the fiber π(\"x\") is a vector space. The case dim is called a line bundle. For any vector space \"V\", the projection makes the product into a \"trivial\" vector bundle. Vector bundles over \"X\" are required to be locally a product of \"X\" and some (fixed) vector space \"V\": for every \"x\" in \"X\", there is a neighborhood \"U\" of \"x\" such that the restriction of π to π(\"U\") is isomorphic to the trivial bundle . Despite their locally trivial character, vector bundles may (depending on the shape of the underlying space \"X\") be \"twisted\" in the large (i.e., the bundle need not be (globally isomorphic to) the trivial bundle ). For example, the Möbius strip can be seen as a line bundle over the circle \"S\" (by identifying open intervals with the real line). It is, however, different from the cylinder , because the latter is orientable whereas the former is not.\n\nProperties of certain vector bundles provide information about the underlying topological space. For example, the tangent bundle consists of the collection of tangent spaces parametrized by the points of a differentiable manifold. The tangent bundle of the circle \"S\" is globally isomorphic to , since there is a global nonzero vector field on \"S\". In contrast, by the hairy ball theorem, there is no (tangent) vector field on the 2-sphere \"S\" which is everywhere nonzero. K-theory studies the isomorphism classes of all vector bundles over some topological space. In addition to deepening topological and geometrical insight, it has purely algebraic consequences, such as the classification of finite-dimensional real division algebras: R, C, the quaternions H and the octonions O.\n\nThe cotangent bundle of a differentiable manifold consists, at every point of the manifold, of the dual of the tangent space, the cotangent space. Sections of that bundle are known as differential one-forms.\n\n\"Modules\" are to rings what vector spaces are to fields: the same axioms, applied to a ring \"R\" instead of a field \"F\", yield modules. The theory of modules, compared to that of vector spaces, is complicated by the presence of ring elements that do not have multiplicative inverses. For example, modules need not have bases, as the Z-module (i.e., abelian group) Z/2Z shows; those modules that do (including all vector spaces) are known as free modules. Nevertheless, a vector space can be compactly defined as a module over a ring which is a field with the elements being called vectors. Some authors use the term \"vector space\" to mean modules over a division ring. The algebro-geometric interpretation of commutative rings via their spectrum allows the development of concepts such as locally free modules, the algebraic counterpart to vector bundles.\n\nRoughly, \"affine spaces\" are vector spaces whose origins are not specified. More precisely, an affine space is a set with a free transitive vector space action. In particular, a vector space is an affine space over itself, by the map\nIf \"W\" is a vector space, then an affine subspace is a subset of \"W\" obtained by translating a linear subspace \"V\" by a fixed vector ; this space is denoted by (it is a coset of \"V\" in \"W\") and consists of all vectors of the form for An important example is the space of solutions of a system of inhomogeneous linear equations\ngeneralizing the homogeneous case above. The space of solutions is the affine subspace where x is a particular solution of the equation, and \"V\" is the space of solutions of the homogeneous equation (the nullspace of \"A\").\n\nThe set of one-dimensional subspaces of a fixed finite-dimensional vector space \"V\" is known as \"projective space\"; it may be used to formalize the idea of parallel lines intersecting at infinity. Grassmannians and flag manifolds generalize this by parametrizing linear subspaces of fixed dimension \"k\" and flags of subspaces, respectively.\n\n\n\n\n\n\n\n\n"}
{"id": "225424", "url": "https://en.wikipedia.org/wiki?curid=225424", "title": "Zugzwang", "text": "Zugzwang\n\nZugzwang (German for \"compulsion to move\", ) is a situation found in chess and other games wherein one player is put at a disadvantage because they must make a move when they would prefer to pass and not move. The fact that the player is compelled to move means that their position will become significantly weaker. A player is said to be \"in zugzwang\" when any possible move will worsen their position.\n\nThe term is also used in combinatorial game theory, where it means that it directly changes the outcome of the game from a win to a loss, but the term is used less precisely in games such as chess. Putting the opponent in zugzwang is a common way to help the superior side win a game, and in some cases, it is necessary in order to make the win possible.\n\nThe term \"zugzwang\" was used in German chess literature in 1858 or earlier, and the first known use of the term in English was by World Champion Emanuel Lasker in 1905. The concept of zugzwang was known to players many centuries before the term was coined, appearing in an endgame study published in 1604 by Alessandro Salvio, one of the first writers on the game, and in shatranj studies dating back to the early 9th century, over 1000 years before the first known use of the term.\n\nPositions with zugzwang occur fairly often in chess endgames, especially in king and pawn endgames. According to John Nunn, positions of reciprocal zugzwang are surprisingly important in the analysis of endgames.\nThe word comes from German \"Zug\" 'move' + \"Zwang\" 'compulsion', so that \"Zugzwang\" means 'being forced to make a move'. Originally the term was used interchangeably with the term \"zugpflicht\" 'obligation to make a move' as a general game rule. Games like chess and checkers have \"zugzwang\" (or \"zugpflicht\"): a player \"must\" always make a move on his turn even if this is to their disadvantage. Tabletop war games or role playing games do not: a player can simply decide to \"wait\" or \"do nothing\" on their turn. Over time, the term became especially associated with chess.\n\nAccording to chess historian Edward Winter, the term had been in use in German chess circles in the 19th century.Pages 353–358 of the September 1858 Deutsche Schachzeitung had an unsigned article \"Zugzwang, Zugwahl und Privilegien\". F. Amelung employed the terms Zugzwang, Tempozwang and Tempozugzwang on pages 257–259 of the September 1896 issue of the same magazine. When a perceived example of zugzwang occurred in the third game of the 1896–97 world championship match between Steinitz and Lasker, after 34...Rg8, the Deutsche Schachzeitung (December 1896, page 368) reported that \"White has died of zugzwang\".\n\nThe earliest known use of the term \"zugzwang\" in English was on page 166 of the February 1905 issue of \"Lasker's Chess Magazine\". The term did not become common in English-language chess sources until the 1930s, after the publication of the English translation of Nimzowitsch's \"My System\" in 1929.\n\nThe concept of zugzwang, if not the term, must have been known to players for many centuries. Zugzwang is required to win the elementary (and common) king and rook versus king endgame, and the king and rook (or differently-named pieces with the same powers) have been chess pieces since the earliest versions of the game.\nOther than basic checkmates, the earliest published use of zugzwang may be in this study by Zairab Katai, which was published sometime between 813 and 833, discussing shatranj. After\nputs Black in zugzwang, since 3...Kc4 4.Kg3 Kd4 5.Re1 and White wins.\n\nThe concept of zugzwang is also seen in the 1585 endgame study by Giulio Cesare Polerio, published in 1604 by Alessandro Salvio, one of the earliest writers on the game. The only way for White to win is 1.Ra1 Kxa1 2.Kc2, placing Black in zugzwang. The only legal move is 2...g5, whereupon White promotes a pawn first and then checkmates with 3.hxg5 h4 4.g6 h3 5.g7 h2 6.g8=Q h1=Q 7.Qg7#\n\nJoseph Bertin refers to zugzwang in \"The Noble Game of Chess\" (1735), wherein he documents 19 rules about chess play. His 18th rule is: \"To play well the latter end of a game, you must calculate who has the move, on which the game always depends.\"\n\nFrançois-André Danican Philidor wrote in 1777 of the position illustrated that after White plays 36.Kc3, Black \"is obliged to move his rook from his king, which gives you an opportunity of taking his rook by a double check [sic], or making him mate\". Lasker explicitly cited a mirror image of this position (White: king on f3, queen on h4; Black: king on g1, rook on g2) as an example of zugzwang in \"Lasker's Manual of Chess\". The British master George Walker analyzed a similar position in the same endgame, giving a maneuver that resulted in the superior side reaching the initial position, but now with the inferior side on move and in zugzwang. Walker wrote of the superior side's decisive move: \"throwing the move upon Black, in the initial position, and thereby winning\".\n\nPaul Morphy is credited with composing the position illustrated \"while still a young boy\". After 1.Ra6, Black is in zugzwang and must allow mate on the next move with 1...bxa6 2.b7# or 1...B (moves) 2.Rxa7#.\nThere are three types of chess positions:\nThe great majority of positions are of the first type. In chess literature, most writers call positions of the second type \"zugzwang\", and the third type \"reciprocal zugzwang\" or \"mutual zugzwang\". Some writers call the second type a \"squeeze\" and the third type \"zugzwang\".\n\nNormally in chess, having tempo is desirable because the player who is to move has the advantage of being able to choose a move that improves their situation. Zugzwang typically occurs when \"the player to move cannot do anything without making an important concession\".\nZugzwang most often occurs in the endgame when the number of pieces, and so the number of possible moves, is reduced, and the exact move chosen is often critical. The first diagram shows the simplest possible example of zugzwang. If it is White's move, they must either stalemate Black with 1.Kc6 or abandon the pawn, allowing 1...Kxc7 with a draw. If it is Black's move, the only legal move is 1...Kb7, which allows White to win with 2.Kd7 followed by queening the pawn on the next move.\n\nThe second diagram is another simple example. Black, on move, must allow White to play Kc5 or Ke5, when White wins one or more pawns and can advance his own pawn toward promotion. White, on move, must retreat his king, when Black is out of danger. The squares d4 and d6 are \"corresponding squares\". Whenever the white king is on d4 with White to move, the black king must be on d6 to prevent the advance of the white king.\n\nIn many cases, the player having the move can put the other player in zugzwang by using \"triangulation\". This often occurs in king and pawn endgames. Pieces other than the king can also triangulate to achieve zugzwang, such as in the Philidor position. Zugzwang is a mainstay of chess compositions and occurs frequently in endgame studies.\n\nSome zugzwang positions occurred in the second game of the 1971 candidates match between Bobby Fischer and Mark Taimanov. In the position in the diagram, Black is in zugzwang because he would rather not move, but he must: a king move would lose the knight, while a knight move would allow the passed pawn to advance. The game continued:\nand Black is again in zugzwang. The game ended shortly (because the pawn will slip through and promote):\n\nIn the position on the right, White has just gotten his king to a6, where it attacks the black pawn on b6, tying down the black king to defend it. White now needs to get his bishop to f7 or e8 to attack the pawn on g6. Play continued:\nNow the bishop is able to make a tempo move. It is able to move while still attacking the pawn on g6, and preventing the black king from moving to c6.\nand Black is in zugzwang. Knights are unable to lose a tempo, so moving the knight would allow the bishop to capture the pawns. The black king must give way.\nand White has a won position. Either one of White's pawns will promote or the white king will attack and win the black kingside pawns and a kingside pawn will promote. Black resigned seven moves later. Andy Soltis says that this is \"perhaps Fischer's most famous endgame\".\n\nThis position from a 1988 game between Vitaly Tseshkovsky and Glenn Flear at Wijk aan Zee shows an instance of \"zugzwang\" where the obligation to move makes the defense more difficult, but it does not mean the loss of the game. A draw by agreement was reached eleven moves later.\n\nA special case of zugzwang is \"reciprocal zugzwang\" or \"mutual zugzwang\", which is a position such that whoever is to move is in zugzwang. Studying positions of reciprocal zugzwang is in the analysis of endgames. A position of mutual zugzwang is closely related to a game with a Conway value of zero in game theory.\n\nIn a position with reciprocal zugzwang, only the player to move is actually in zugzwang. However, the player who is not in zugzwang must play carefully because one inaccurate move can cause him to be put in zugzwang. That is in contrast to regular zugzwang, because the superior side usually has a waiting move to put the opponent in zugzwang.\n\nThe diagram on the right shows a position of reciprocal zugzwang. If Black is to move, 1... Kd7 is forced, which loses because White will move 2. Kb7, promote the pawn, and win. If White is to move the result is a draw as White must either stalemate Black with 1. Kc6 or allow Black to the pawn. Since each side would be in zugzwang if it were his move, it is a reciprocal zugzwang.\n\nAn extreme type of reciprocal zugzwang, called \"trébuchet\", is shown in the diagram. It is also called a \"full-point mutual zugzwang\" because it will result in a loss for the player in zugzwang, resulting a full point for his opponent. Whoever is to move in this position must abandon his own pawn, thus allowing his opponent to capture it and proceed to promote the remaining pawn, resulting in an easily winnable position.\nCorresponding squares are squares of mutual zugzwang. When there is only one pair of corresponding squares, they are called \"mined squares\". A player will fall into zugzwang if they move their king onto the square and his opponent is able to move onto the corresponding square. In the diagram on the right, if either king moves onto the square marked with the dot of the same color, it falls into zugzwang if the other king moves into the mined square near them.\n\nZugzwang usually works in favor of the stronger side, but sometimes it aids the defense. In this position based on a game between Zoltán Varga and Peter Acs, it saves the game for the defense:\nReciprocal zugzwang.\nReciprocal zugzwang again.\nReciprocal zugzwang again.\nThis position is a draw and the players agreed to a draw a few moves later.\n\nAlex Angos notes that, \"As the number of pieces on the board increases, the probability for \"zugzwang\" to occur decreases.\" As such, zugzwang is very rarely seen in the middlegame.\n\nThe game Fritz Sämisch versus Aron Nimzowitsch, Copenhagen 1923, is often called the \"Immortal Zugzwang Game\". According to Nimzowitsch, writing in the \"Wiener Schachzeitung\" in 1925, this term originated in \"Danish chess circles\". Some consider the final position to be an extremely rare instance of zugzwang occurring in the middlegame. It ended with White resigning in the position in the diagram.\n\nWhite has a few pawn moves which do not lose material, but eventually he will have to move one of his pieces. If he plays 1.Rc1 or Rd1, then 1...Re2 traps White's queen; 1.Kh2 fails to 1...R5f3, also trapping the queen, since White cannot play 2.Bxf3 because the bishop is pinned to the king; 1.g4 runs into 1...R5f3 2.Bxf3? Rh2 mate. Angos analyzes 1.a3 a5 2.axb4 axb4 3.h4 Kh8 (waiting) 4.b3 Kg8 and White has run out of waiting moves and must lose material. Best in this line is 5.Nc3!? bxc3 6.bxc3, which just leaves Black with a serious positional advantage and an extra pawn. Other moves lose material in more obvious ways.\n\nHowever, since Black would win even without the zugzwang, it is debatable whether the position is true zugzwang. Even if White could pass his move he would still lose, albeit more slowly, after 1...R5f3 2.Bxf3 Rxf3, trapping the queen and thus winning queen and bishop for two rooks. Wolfgang Heidenfeld thus considers it a misnomer to call this a true zugzwang position. See also Immortal Zugzwang Game: Objections to the sobriquet.\n\nThis game between Wilhelm Steinitz versus Emanuel Lasker in the 1896–97 World Chess Championship, is an early example of zugzwang in the middlegame. After Lasker's 34...Re8–g8!, Steinitz had no moves, and resigned. White's bishop cannot move because that would allow the crushing ...Rg2+. The queen cannot move without abandoning either its defense of the bishop on g5 or of the g2 square, where it is preventing ...Qg2#. White's move 35.f6 loses the bishop: 35...Rxg5 36. f7 Rg2+, forcing mate. The move 35.Kg1 allows 35...Qh1+ 36.Kf2 Qg2+ followed by capturing the bishop. The rook cannot leave the first , as that would allow 35...Qh1#. Rook moves along the first rank other than 35.Rg1 allow 35...Qxf5, when 36.Bxh4 is impossible because of 36...Rg2+; for example, 35.Rd1 Qxf5 36.d5 Bd7, winning. That leaves only 35.Rg1, when Black wins with 35...Rxg5! 36.Qxg5 (36.Rxg5? Qh1#) Qd6+ 37.Rg3 hxg3+ 38.Qxg3 Be8 39.h4 Qxg3+ 40.Kxg3 b5! 41.axb5 a4! and Black queens first. Colin Crouch calls the final position, \"An even more perfect middlegame zugzwang than ... Sämisch–Nimzowitsch ... in the final position Black has no direct threats, and no clear plan to improve the already excellent positioning of his pieces, and yet any move by White loses instantly\".\nSoltis writes that his \"candidate for the ideal zugzwang game\" is the following game , Podgaets–Dvoretsky, USSR 1974: 1. d4 c5 2. d5 e5 3. e4 d6 4. Nc3 Be7 5. Nf3 Bg4 6. h3 Bxf3 7. Qxf3 Bg5! 8. Bb5+ Kf8! Black exchanges off his , but does not allow White to do the same. 9. Bxg5 Qxg5 10. h4 Qe7 11. Be2 h5 12. a4 g6 13. g3 Kg7 14. 0-0 Nh6 15. Nd1 Nd7 16. Ne3 Rhf8 17. a5 f5 18. exf5 e4! 19. Qg2 Nxf5 20. Nxf5+ Rxf5 21. a6 b6 22. g4? hxg4 23. Bxg4 Rf4 24. Rae1 Ne5! 25. Rxe4 Rxe4 26. Qxe4 Qxh4 27. Bf3 Rf8!! 28. Bh1 28.Qxh4? Nxf3+ and 29...Nxh4 leaves Black a piece ahead. 28... Ng4 29. Qg2 (first diagram) Rf3!! 30. c4 Kh6!! (second diagram) Now all of White's piece moves allow checkmate or ...Rxf2 with a crushing attack (e.g. 31.Qxf3 Qh2#; 31.Rb1 Rxf2 32.Qxg4 Qh2#). That leaves only moves of White's b-pawn, which Black can ignore, e.g. 31.b3 Kg7 32.b4 Kh6 33.bxc5 bxc5 and White has run out of moves. 0–1\n\nIn this 1959 game between future World Champion Bobby Fischer and Héctor Rossetto, 33.Bb3! puts Black in zugzwang. If Black moves the king, White plays Rb8, winning a piece (...Rxc7 Rxf8); if Black moves the rook, 33...Ra8 or Re8, then 34.c8=Q+ and the black rook will be lost after 35.Qxa8, 35.Qxe8 or 35.Rxe7+ (depending on Black's move); if Black moves the knight, Be6 will win Black's rook. That leaves only pawn moves, and they quickly run out. The game concluded:\n\nJonathan Rowson coined the term \"Zugzwang Lite\" to describe a situation, sometimes arising in symmetrical opening variations, where White's \"extra move\" is a burden. He cites as an example of this phenomenon in Hodgson versus Arkell at Newcastle 2001. The position at left arose after 1. c4 c5 2. g3 g6 3. Bg2 Bg7 4. Nc3 Nc6 5. a3 a6 6. Rb1 Rb8 7. b4 cxb4 8. axb4 b5 9. cxb5 axb5 (see diagram). Here Rowson remarks, Both sides want to push their d-pawn and play Bf4/...Bf5, but White has to go first so Black gets to play ...d5 before White can play d4. This doesn't matter much, but it already points to the challenge that White faces here; his most natural continuations allow Black to play the moves he wants to. I would therefore say that White is in 'Zugzwang Lite' and that he remains in this state for several moves. The game continued 10. Nf3 d5 11. d4 Nf6 12. Bf4 Rb6 13. 0-0 Bf5 14. Rb3 0-0 15. Ne5 Ne4 16. h3 h5!? 17. Kh2. The position is still almost symmetrical, and White can find nothing useful to do with his extra move. Rowson whimsically suggests 17.h4!?, forcing Black to be the one to break the symmetry. 17... Re8! Rowson notes that this is a useful waiting move, covering e7, which needs protection in some lines, and possibly supporting an eventual ...e5 (as Black in fact played on his 22nd move). White cannot copy it, since after 18.Re1? Nxf2 Black would win a pawn. After 18. Be3?! Nxe5! 19. dxe5 Rc6! Black seized the initiative and went on to win in 14 more moves.\nAnother instance of \"Zugzwang Lite\" occurred in Lajos Portisch versus Mikhail Tal, Candidates Match 1965, again from the Symmetrical Variation of the English Opening, after 1. Nf3 c5 2. c4 Nc6 3. Nc3 Nf6 4. g3 g6 5. Bg2 Bg7 6. 0-0 0-0 7. d3 a6 8. a3 Rb8 9. Rb1 b5 10. cxb5 axb5 11. b4 cxb4 12. axb4 d6 13. Bd2 Bd7 (see diagram). Soltis wrote, \"It's ridiculous to think Black's position is better. But Mikhail Tal said it is easier to play. By moving second he gets to see White's move and then decide whether to match it.\" 14. Qc1 Here, Soltis wrote that Black could maintain equality by keeping the symmetry: 14...Qc8 15.Bh6 Bh3. Instead, he plays to prove that White's queen is misplaced by breaking the symmetry. 14... Rc8! 15. Bh6 Nd4! Threatening 15...Nxe2+. 16. Nxd4 Bxh6 17. Qxh6 Rxc3 18. Qd2 Qc7 19. Rfc1 Rc8 Although the pawn structure is still symmetrical, Black's control of the gives him the advantage. Black ultimately reached an endgame two pawns up, but White managed to hold a draw in 83 moves. See First-move advantage in chess#Symmetrical openings for more details.\n\nBibliography\n\n\n"}
