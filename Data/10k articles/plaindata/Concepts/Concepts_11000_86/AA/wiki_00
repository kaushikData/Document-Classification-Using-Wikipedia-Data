{"id": "18613326", "url": "https://en.wikipedia.org/wiki?curid=18613326", "title": "Abdel-Moniem El-Ganayni", "text": "Abdel-Moniem El-Ganayni\n\nDr. Abdel-Moniem ibn Ali El-Ganayni () is an Egyptian-born American nuclear physicist, former prison Imam, and an active member of the Pittsburgh Muslim community. In 2007 El-Ganayni's U.S. security clearance was revoked, and he subsequently lost his job as a senior scientist at Bettis Atomic Power Laboratory.\n\nEl-Ganayni was born in Minyat Samannoud, Egypt in 1950. His father, Sheikh Ali ibn Abdel-Moniem El-Ganayni (الشيخ علي بن عبدالمنعم الجنايني) was educated at the prestigious Al-Azhar University, where he completed his elementary, secondary, and high school studies. Sheikh Ali proceeded to the Al-Azhar Faculty of Islamic Law, where he completed the equivalent of a bachelor’s degree. He continued his studies to the graduate level, obtaining the equivalent of a Ph.D. in Islamic law (العالمية), and later a certification to practice law (إجازة القضاء الشرعي).\n\nIn 1973, Dr. Abdel-Moniem El-Ganayni obtained his Bachelor of Science in Physics from Ain Shams University in Cairo, Egypt. He went on to pursue a master's in Nuclear Physics and completed his degree in 1978.\n\nIn 1980 El-Ganayni moved to the United States to complete his Ph.D., obtaining a second master's degree in Nuclear Physics from the University of Pittsburgh in 1983. Between 1984 and 1989 El-Ganayni worked as a teaching assistant in the Department of Physics, and completed his Ph.D. in Atomic Physics in 1990. He became an American citizen in 1988.\n\nShortly after completing his graduate studies, El-Ganayni was hired by Westinghouse Electric Company, working in the Bettis Atomic Power Laboratory as a Senior Scientific Programmer, and later as a Senior Scientist. Throughout his employment at Bettis he received positive reviews, authored and coauthored a number of academic papers, and contributed to numerous projects.\n\nEl-Ganayni’s contributions were not limited to scientific endeavors; he is a founding member of the Islamic Center of Pittsburgh, served two terms as the center’s president, and helped to establish the mosque’s current location in Schenley Heights. For close to a decade, El-Ganayni worked and volunteered as an Imam at correctional facilities in both Ohio and Pennsylvania. Part of El-Ganayni’s prison outreach included the creation of the PA DOC Monitor, a website examining the condition, and necessary reform of the Department of Corrections.\n\nDr. El-Ganayni’s work at Bettis Atomic Power Laboratory required the possession of a Department of Energy (DOE) issued security clearance, enabling him to work with classified information. In October 2007 El-Ganayni’s security clearance was suspended, effectively barring him from any work at Bettis. DOE regulations stipulate that an individual may appeal a suspension, however El-Ganayni’s right to appeal was denied in May 2008 when Jeffery F. Kupfer, deputy secretary of the DOE, invoked national security, refusing to reveal the government’s allegations against Dr. El-Ganayni.\n\nThe suspension of El-Ganayni’s security clearance was prefaced by an interview with DOE security officials, and later the Federal Bureau of Investigation (FBI). In these interviews, questions were asked regarding El-Ganayni’s political beliefs, religious views, and his work as an Imam in the prison system. No questions related to his work or any potential breaches of security were asked in either interview.\n\nThe DOE officials instead focused on a seemingly innocent Islamic book entitled, “The Miracle of the Ant,” authored by Turkish Islamic publisher and author Harun Yahya; unbeknownst to El-Ganayni, the content of Yahya’s book was largely, if not completely duplicated from a Pulitzer Prize winning work entitled, “Journey to the Ants” published by Harvard University Press.\n\nDr. El-Ganayni distributed excerpts from book to prisoners (both Muslim and non-Muslim) while serving as an Imam at Forest State Correctional Institution. DOE officials expressed concern over a purely scientific description of ants’ biological defense mechanisms in a chapter entitled, “Defense and War Tactics” insinuating that El-Ganayni distributed this material with sinister intent, and subsequently questioned his allegiance to the United States of America. His supervisor at Forest, chaplaincy director Glenn McQuown reviewed the book and described it as “completely benign.”\n\nThe interviews also focused on El-Ganayni’s criticism of American foreign policy and the FBI’s mistreatment of Muslims after September 11, 2001. Specifically, El-Ganayni raised concern over an FBI raid of a Pittsburgh mosque during Friday prayers, where attendees were searched and forced to stand outside while being questioned. Revocation of El-Ganayni’s security clearance also may be related to his establishment of the PA DOC Monitor, a website and prison outreach program which critically examines the Pennsylvania Department of Corrections and its policies, insisting that drastic reform of the system is required for it to truly benefit prisoners and society at large.\n\nOn June 26, 2008 the American Civil Liberties Union (ACLU) with Schnader, Harrison, Segal, and Lewis as co-counsels filed a lawsuit on behalf of Dr. El-Ganayni alleging that the DOE revoked his clearance due to criticism of the FBI and U.S. foreign policy. The suit demands that the DOE reveal their allegations against El-Ganayni and restore due process, allowing him to contest any allegations made against him. A hearing date has not been set as of July 28, 2008.\n\nA motion for preliminary and permanent injunction against the U.S. Department of Energy and its acting director, Jeffrey Kupfer was filed on September 3, 2008 requesting the following:\n\n\nOn September 25, 2008 the DOE filed a motion to dismiss El-Ganayni's preliminary and permanent injunction. On October 14, 2008 Dr. El-Ganayni's attorneys filed a brief in opposition of the DOE's motion to dismiss.\n\nOn October 31, 2008 Judge Terrence F. McVerry stated that the court \"reluctantly concludes that it lacks subject-matter jurisdiction to adjudicate the claims made in Counts I and II of the Complaint\" effectively granting the DOE's Motion to Dismiss in relation to Counts I and II. However, the court requested supplemental briefing regarding Count III, specifically concerning the DOE's interpretation, application, and adherence to executive procedure. Briefs from both the DOE and El-Ganayni's defense regarding Count III were submitted on November 14, 2008.\n\nJudge McVerry ruled in favor of Dr. El-Ganayni and refused to dismiss Count III on November 20, 2008. In his court order McVerry stated that, \"[the] Defendants’ interpretation [of their own regulations] is not consistent with the actual text of the Executive Orders.\" This ruling was based on a DOE regulation stating that a clearance can only be revoked by a DOE agency head, not simply a Deputy Secretary. Subsequently, November 24, 2008, a day before a scheduled hearing to discuss pending motions, future direction of the litigation, and the possible scheduling of a preliminary injunction hearing, DOE secretary Samuel Bodman signed a revocation of El-Ganayni's clearance citing national security and the government requested a dismissal of the case. This dismissal was granted , however Dr. El-Ganayni appealed his case to the 3rd U.S. Circuit Court. Dr. El-Ganayni's brief for appeal of the dismissal was filed March 19, 2009 and a decision was made in the DOE's favor on January 11, 2010. In summation, the judges stated that the DOE is by extension part of the government's Executive Branch, and therefore the 3rd Circuit has no jurisdiction over the matter.\n\nOn November 26, 2008 El-Ganayni and his wife left to Egypt after residing in the United States for 28 years. El-Ganayni stated that he would rather return to his birthplace than live in America as a second-class citizen and that he \"feel[s] very sad that the American people have lost a good bit of their Constitution,\" quoting John Adams who said, \"once you lose your rights and liberties, it's very hard to get them back.\"\n\n"}
{"id": "23275444", "url": "https://en.wikipedia.org/wiki?curid=23275444", "title": "Ahmet Ögüt", "text": "Ahmet Ögüt\n\nAhmet Ögüt (born 1981 in Diyarbakir, Turkey) is a conceptual artist living and working in Amsterdam, Netherlands. He works with a broad range of media including video, photography, installation, drawing and printed media. In 2009 he represented Turkey in the Pavilion of Turkey at the 53rd International Art Exhibition of the Venice Biennial.\n\nAhmet Ögüt received his BA from the Fine Arts Faculty of Hacettepe University in 2003, and his MFA from the Art and Design Faculty of Yildiz Technical University in 2006. Ögüt has been a guest artist at the Rijksakademie van Beeldende Kunsten in Amsterdam in 2007-2008.\n\nSolo exhibitions since 2005 include 'Ahmet Ögüt', Mala Galerija / The Museum of Modern Art of Ljubljana, 2005; 'Ahmet Ögüt and Borga Kantürk', Platform Garanti Contemporary Art Center, Istanbul, 2006; 'Softly But Firmly', Galerija Miroslav Kraljevic, Zagreb, 2007; 'Across the Slope', Centre d'Art Santa Mònica, Barcelona, 2008; 'Mutual Issues, Inventive Acts', Kunsthalle Basel, 2008; 'Things we count', Künstlerhaus Bremen, 2009; 'Speculative Social Fantasies', Artspace, Sydney, 2010.\n\nGroup exhibitions include 'Lapses', 53rd Venice Biennial / The Pavilion of Turkey, Venice, 2009; 28th Biennial of Graphic Arts, Ljubljana, 2009; 'Take the Money and Run', De Appel, Amsterdam, 2009; 'The Generational', The New Museum of Contemporary Art, 2009; 7th SITE Santa Fe Biennial, 2008; 5th Berlin Biennial, 2008; 'Fuild Street', Kiasma Museum of Contemporary Art, 2008; 'Be[com]ing Dutch', Van Abbemuseum, 2008; 'Car Culture', Scottsdale Museum of Contemporary Art, 2008; 'Stalking with Stories', Apexart, 2007; 1st Contemporary Art Biennale of Thessaloniki, 2007; 'Normalization', Rooseum Center for Contemporary Art, Malmö, 2006; 9th Istanbul Biennial, 2005.\n\nToday in History (published by Book Works, London and Platform Garanti Contemporary Art Center, Istanbul, 2007) ;\nSoftly but Firmly (published by Galerija Miroslav Kraljevic, Zagreb, 2007) ;\nOn The Road to Other Lands (published by A Prior and 5th Berlin Biennial, 2008);\nInformal Incidents (published by art-ist contemporary art, Istanbul, 2008) \n\n"}
{"id": "1464947", "url": "https://en.wikipedia.org/wiki?curid=1464947", "title": "Alhambra Decree", "text": "Alhambra Decree\n\nThe Alhambra Decree (also known as the Edict of Expulsion; Spanish: \"Decreto de la Alhambra, Edicto de Granada\") was an edict issued on 31 March 1492, by the joint Catholic Monarchs of Spain (Isabella I of Castile and Ferdinand II of Aragon) ordering the expulsion of practicing Jews from the Kingdoms of Castile and Aragon and its territories and possessions by 31 July of that year. The primary purpose was to eliminate their influence on Spain's large \"converso\" population and ensure they did not revert to Judaism. Over half of Spain's Jews had converted as a result of the religious persecution and pogroms which occurred in 1391. Due to continuing attacks around 50,000 more had converted by 1415. A further number of those remaining chose to convert to avoid expulsion. As a result of the Alhambra decree and persecution in prior years, over 200,000 Jews converted to Catholicism and between 40,000 and 100,000 were expelled, an indeterminate number returning to Spain in the years following the expulsion.\n\nThe edict was formally and symbolically revoked on 16 December 1968, following the Second Vatican Council. This was a full century after Jews had been openly practicing their religion in Spain and synagogues were once more legal places of worship under Spain's Laws of Religious Freedom.\n\nIn 1924, the regime of Primo de Rivera granted Spanish citizenship to the entire Sephardic Jewish diaspora. In 2014, the government of Spain passed a law allowing dual citizenship to Jewish descendants who apply, to \"compensate for shameful events in the country's past.\" Thus, Sephardi Jews who can prove they are the descendants of those Jews expelled from Spain because of the Alhambra Decree can \"become Spaniards without leaving home or giving up their present nationality.\"\n\nBy the end of the 8th century, Muslim forces had conquered and settled most of the Iberian Peninsula. Under Islamic law, the Jews who had lived in the region since at least Roman times, were considered \"People of the Book,\" which was a protected status. Compared to the repressive policies of the Visigothic kingdom, who, starting in the sixth-century had enacted a series of anti-Jewish statutes which culminated in their forced conversion and enslavement, the tolerance of the Muslim Moorish rulers of \"al-Andalus\" allowed Jewish communities to thrive. Jewish merchants were able to trade freely across the Islamic world, which allowed them to flourish, and made Jewish enclaves in Muslim Iberian cities great centers of learning and commerce. This led to a flowering of Jewish culture, as Jewish scholars were able to gain favor in Muslim courts as skilled physicians, diplomats, translators, and poets. Although Jews never enjoyed equal status to Muslims, in some Taifas, such as Granada, Jewish men were appointed to very high offices, including Grand Vizier. However, Muslim forces did not control the entire peninsula.\n\nThe \"Reconquista\", or the gradual reconquest of Muslim Iberia by the Christian kingdoms in the North, was driven by a powerful religious motivation: to reclaim Iberia for Christendom following the Umayyad conquest of Hispania centuries before. By the 14th century, most of the Iberian Peninsula (present-day Spain and Portugal) had been reconquered by the Christian kingdoms of Castile, Aragon, León, Galicia, Navarre, and Portugal.\n\nDuring the Christian re-conquest, the Muslim kingdoms in Spain became less welcoming to the \"dhimmi\". In the late twelfth century, the Muslims in al-Andalus invited the fanatical Almohad dynasty from North Africa to push the Christians back to the North. After they gained control of the Iberian Peninsula, the Almohads offered the Sephardim a choice between expulsion, conversion, and death. Many Jewish people fled to other parts of the Muslim world, and also to the Christian kingdoms, which initially welcomed them. In Christian Spain, Jews functioned as courtiers, government officials, merchants, and moneylenders. Therefore, the Jewish community was both useful to the ruling classes and to an extent protected by them. The erosion of rights the Sephardim had enjoyed under the Muslims, in Christian territories was slow and in some areas barely perceptible.\n\nAs the Reconquista drew to a close, overt hostility against Jews in Christian Spain became more pronounced, finding expression in brutal episodes of violence and oppression. In the early fourteenth century, the Christian kings vied to prove their piety by allowing the clergy to subject the Jewish population to forced sermons and disputations. More deadly attacks came later in the century from mobs of angry Catholics, led by popular preachers, who would storm into the Jewish quarter, destroy synagogues, and break into houses, forcing the inhabitants to choose between conversion and death. Thousands of Jews sought to escape these attacks by converting to Christianity.These Jewish converts were commonly called \"conversos\", New Christians, or \"marranos\"; the latter two terms were used as insults. At first these conversions seemed an effective solution to the cultural conflict: many \"converso\" families met with social and commercial success. But eventually their success made these new Catholics unpopular with their neighbors, including some of the clergy of the Church and Spanish aristocrats competing with them for influence over the royal families. By the mid-fifteenth century, the demands of the Old Christians, that the Catholic Church and the monarchy differentiate them from the conversos, led to the first \"Limpieza de Sangre\" laws, which restricted opportunities for converts.\n\nThese suspicions on the part of Christians were only heightened by the fact that some of the coerced conversions were undoubtedly insincere. Some, but not all, \"conversos\" had understandably chosen to salvage their social and commercial positions or their lives by the only option open to them – baptism and embrace of Christianity – while privately adhering to their Jewish practice and faith. Recently converted families who continued to intermarry were especially viewed with suspicion. These secret practitioners are commonly referred to as crypto-Jews or \"marranos\".\n\nThe existence of crypto-Jews was a provocation for secular and ecclesiastical leaders who were already hostile toward Spain's Jewry. For their part, the Jewish community viewed conversos with compassion, because Jewish law held that conversion under threat of violence was not necessarily legitimate. Although the Catholic Church was also officially opposed to forced conversion, under ecclesiastical law all baptisms were lawful, and once baptized, converts were not allowed to rejoin their former religion. The uncertainty over the sincerity of Jewish converts added fuel to the fire of antisemitism in 15th century Spain.\n\nFrom the 13th to the 16th centuries European countries expelled the Jews from their territory on at least 15 occasions. Before the Spanish expulsion, the Jews were expelled from England in 1290 and multiple times from France between 1182 and 1354. The French case is typical of most expulsions, because whether they were local or national, the Jews were usually invited back a few years later. The Jews were also expelled from some of the German states. The Spanish expulsion was succeeded by at least five more expulsions, from other European countries. However, the expulsion of the Jews from Spain was both the largest of its kind, and officially the longest in Western European history.\n\nOver the four hundred year period the majority of these decrees were implemented, the meaning of the expulsions gradually changed. At first, the expulsions of the Jews, or lack thereof, were exercises of kingly prerogative. Jewish communities in Europe were often protected by and associated with European monarchs in the Middle Ages, because under the feudal system, the Jews were often the monarch’s only reliable tax base. Furthermore, the Jews had the reputation of moneylenders because they were the only group allowed to loan money at a profit under the prevailing interpretation of the Vulgate (Latin; official for Catholics) Bible, which forbid Christians from collecting interest. Therefore, the Jews were the creditors of the merchants, the aristocrats, and even the monarchs. Most expulsions were centered on this issue: the monarch would tax the Jewish community heavily, forcing them to call in loans, and then expel them. At the time of expulsion, the monarch would seize their remaining valuable assets, including the debts owed by the aristocrats and the merchants. Therefore, expulsion of the Jews from Spain was unique not only in scale, but in its ideological motivations.\n\nHostility towards the Jews in Spain was brought to a climax during the reign of the “Catholic Monarchs,” Ferdinand and Isabella. Their marriage in 1469, which formed a personal union of the crowns of Aragon and Castile, with coordinated policies between their distinct kingdoms, eventually led to the final unification of Spain.\n\nAlthough their initial policies towards the Jews were protective, Ferdinand and Isabella were disturbed by reports claiming that most Jewish converts to Christianity were insincere in their conversion. As mentioned above, some accusations that \"conversos\" continued to practice Judaism in secret (see Crypto-Judaism) were true, but the “Old” Christians exaggerated the scale of the problem. More serious were the accusations that the Jews were trying to draw conversos back into the Jewish fold. In 1478, Ferdinand and Isabella made a formal application to Rome to set up an Inquisition in Castile to investigate these and other suspicions. In 1487, King Ferdinand promoted the establishment of the Spanish Inquisition Tribunals in Castile. In Aragon, it had been first instituted in 13th century to combat the Albigense heresy. However, the focus of this new Inquisition was to find and punish \"conversos\" who were practicing Judaism in secret.\n\nThese issues came to a head during Ferdinand and Isabella’s final conquest of Granada. The independent Islamic Emirate of Granada had been a tributary state to Castile since 1238. Jews and conversos played an important role during this campaign, because they had the ability to raise money and acquire weapons through their extensive trade networks. This perceived increase in Jewish influence further infuriated the Old Christians and the hostile elements of the clergy. Finally, in 1491 in preparation for an imminent transition to Castilian territory, the Treaty of Granada was signed by Emir Muhammad XII and the Queen of Castile, protecting the religious freedom of the Muslims there. By 1492, Ferdinand and Isabella had won the Battle of Granada and completed the Catholic Reconquista of the Iberian Peninsula from Islamic forces. However, the Jewish population emerged from the campaign more hated by the populace and less useful to the monarchs.\n\nThe king and queen issued the Alhambra Decree less than three months after the surrender of Granada. Although Isabella was the force behind the decision, her husband Fernando did not oppose it, and both proved skillful at exploiting the hopes of the desperate Jewish population for monetary gain. That her confessor had just changed from the tolerant Hernando de Talavera to the very intolerant Francisco Jiménez de Cisneros suggests an increase in royal hostility towards the Jews. The most powerful proponent of the decision however, was Torquemada, the priest in charge of the Spanish Inquisition. The text of the decree accused the Jews of trying \"to subvert the holy Catholic faith” by attempting to \"draw faithful Christians away from their beliefs.\" These measures were not new in Europe.\n\nAfter the decree was passed, Spain's entire Jewish population was given only four months to either convert to Christianity or leave the country. The edict promised the Jews' royal \"protection and security\" for the effective three-month window before the deadline. They were permitted to take their belongings with them, excluding “gold or silver or minted money or other things prohibited by the laws of our kingdoms.\" In practice, however, the Jews had to sell anything they could not carry: their land, their houses, and their libraries, and converting their wealth to a more portable form proved difficult. The market in Spain was saturated with these goods, which meant the prices were artificially lowered for the months before the deadline. As a result, much of the wealth of the Jewish community remained in Spain. The punishment for any Jew who did not convert or leave by the deadline was summary execution.\n\nThe Sephardic Jews migrated to four major areas: North Africa, the Ottoman Empire, Portugal, and Italy. Some Spanish Jews who emigrated to avoid conversion dispersed throughout the region of North Africa known as the Maghreb. The Jewish scholars and physicians among previous Sephardic immigrants to this area had reinvigorated the Jewish communities in North Africa. However, in the 1490s, parts of the Mediterranean world, including Morocco were experiencing severe famine. As a result, a number of cities in Morocco refused to let the Spanish Jews in. This led to mass starvation among the refugees, and made the Jewish refugees vulnerable to the predation of slavers, although the regional ruler invalidated many of these sales within a few years. A good number of the Jews who had fled to North Africa returned to Spain and got converted. The Jews who stayed in North Africa often intermingled with the already existing Mizrahi Arabic or Berber speaking communities, becoming the ancestors of the Moroccan, Algerian, Tunisian and Libyan Jewish communities.\n\nMany Spanish Jews also fled to the Ottoman Empire, where they were given refuge. Sultan Bayezid II of the Ottoman Empire, learning about the expulsion of Jews from Spain, dispatched the Ottoman Navy to bring the Jews safely to Ottoman lands, mainly to the cities of Thessaloniki (currently in Greece) and İzmir (currently in Turkey). Many of these Jews also settled in other parts of the Balkans ruled by the Ottomans such as the areas that are now Bulgaria, Serbia and Bosnia. Concerning this incident, Bayezid II is alleged to have commented, \"those who say that Ferdinand and Isabella are wise are indeed fools; for he gives me, his enemy, his national treasure, the Jews.\"\n\nA majority of Sephardim migrated to Portugal, where they gained only a few years of respite from persecution. About 600 Jewish families were allowed to stay in Portugal following an exorbitant bribe, until the Portuguese king entered negotiations to marry the daughter of Ferdinand and Isabella. Caught between his desire for an alliance with Spain and his economic reliance on the Jews, Manuel I declared the Jewish community in Portugal (perhaps then some 10% of that country's population) Christians by royal decree unless they left the country. In return, he promised the Inquisition would not come to Portugal for 40 years. He then seized the Jews who tried to leave and had them forcibly baptized, after separating them from their children. It was years before the Jews who fled to Portugal were allowed to emigrate. When the ban was lifted, many of them fled to the Low Countries, or the Netherlands. \n\nThroughout history, scholars have given widely differing numbers of Jews expelled from Spain. However, the figure is likely to be below the 100,000 Jews who had not yet converted to Christianity by 1492, possibly as low as 40,000. Such figures exclude the significant number of Jews who returned to Spain due to the hostile reception they received in their countries of refuge, notably Fez (Morocco). The situation of returnees was legalized with the Ordinance of the 10 of November 1492 which established that civil and church authorities should be witness to baptism and, in the case that they were baptized before arrival, proof and witnesses of baptism were required. Furthermore, all property could be recovered by returnees at the same price at which it was sold. Similarly the Provision of the Royal Council of 24 of October 1493 set harsh sanctions for those who slandered these New Christians with insulting terms such as \"tornadizos\". After all, the Catholic monarchs were concerned with the souls of their subjects, and Catholic doctrine held that the persecution of converts would remove an important incentive for conversion. Returnees are documented as late as 1499.\n\nA majority of Spain's Jewish population had converted to Christianity during the waves of religious persecutions prior to the Decree—a total of 200,000 converts according to Joseph Pérez. The main objective of the expulsion of practicing Jews was ensuring the sincerity of the conversions of such a large convert population. Of the 100,000 Jews that remained true to their faith by 1492, an additional number chose to convert and join the converso community rather than face expulsion. Recent conversos were subject to additional suspicion by the Inquisition, which had been established to persecute religious heretics, but in Spain and Portugal was focused on finding crypto-Jews. Although Judaism was not considered a heresy, professing Christianity while engaging in Jewish practices was heretical. Additionally, \"Limpieza de Sangre\" statutes instituted legal discrimination against converso descendants, barring them from certain positions and forbidding them from immigrating to the Americas. For years, families with urban origins who had extensive trade connections, and people who were learned and multilingual were suspected of having Jewish ancestry. According to the prejudice of the time, a person with Jewish blood was untrustworthy and inferior. Such measures slowly faded away as converso identity was forgotten and this community merged into Spain's dominant Catholic culture. This process lasted until the eighteenth century, with a few exceptions, most notably the Chuetas of the island of Majorca, where discrimination lasted into early 20th Century.\n\nA Y chromosome DNA test conducted by the University of Leicester and the Pompeu Fabra University has indicated an average of nearly 20% for Spaniards having some direct patrilineal descent from populations from the Near East which colonized the region either in historical times, such as Jews and Phoenicians, or during earlier prehistoric Neolithic migrations. Between the 90,000 Jews who converted under the Visigoth persecutions, and the 100,000+ Jews who converted in the years leading up to expulsion, it is likely that many of these people have Jewish ancestry. On the other hand, genetic studies have dispelled local beliefs in the American South West that Spanish Americans are the descendants of conversos.\n\nThe Spanish government has actively pursued a policy of reconciliation with the descendants of its expelled Jews. In 1924, the regime of Primo de Rivera granted Spanish citizenship to the entire Sephardic Jewish diaspora. As stated above, the Alhambra decree was officially revoked in 1968, after the Second Vatican Council rejected the charge of deicide traditionally attributed to the Jews. In 1992, in a ceremony marking the 500th anniversary of the Edict of Expulsion, King Juan Carlos (wearing a yarmulke) prayed alongside Israeli president Chaim Herzog and members of the Jewish community in the Beth Yaacov Synagogue. The King said, \"Sefarad (the Hebrew name for Spain) isn't a nostalgic memory anymore; it is a place where it must not be said that Jews should simply 'feel' at home there, for indeed Hispano-Jews are at home in Spain...What matters is not accountability for what we may have done wrong or right, but the willingness to look to the future, and analyze the past in light of our future.\"\n\nFrom November 2012 Sephardi Jews have had the right to automatic Spanish nationality without the requirement of residence in Spain. Prior to November 2012, Sephardi Jews already had the right to obtain Spanish citizenship after a reduced residency period of two years (versus ten years for foreigners but natural from Philippines, Equatorial Guinea, Brazil and about other 20 American republics than also require 2 years.) While their citizenship is being processed, Sephardi Jews are entitled to the consular protection of the Kingdom of Spain. This makes Spain unique among European nations as the only nation that currently grants automatic citizenship to the descendants of Jews expelled during the European medieval evictions. Although these measures are popular in the Jewish community, they have also sparked some controversy. A minority of thinkers hold that these policies represent less the abnegation of prejudice as a shift to philo-semitism. As of November 2015, 4300 Sephardi Jews have benefited from this law and acquired Spanish citizenship, swearing allegiance to the Spanish Constitution. In 2013, the number of Jews in Spain was estimated to range between 40,000 and 50,000 people.\n\n\n\n"}
{"id": "34460605", "url": "https://en.wikipedia.org/wiki?curid=34460605", "title": "Andersen healthcare utilization model", "text": "Andersen healthcare utilization model\n\nThe Andersen Healthcare Utilization Model - is a conceptual model aimed at demonstrating the factors that lead to the use of health services. According to the model, usage of health services (including inpatient care, physician visits, dental care etc.) is determined by three dynamics: predisposing factors, enabling factors, and need. Predisposing factors can be characteristics such as race, age, and health beliefs. For instance, an individual who believes health services are an effective treatment for an ailment is more likely to seek care. Examples of enabling factors could be family support, access to health insurance, one's community etc. Need represents both perceived and actual need for health care services. The original model was developed by Ronald M. Andersen, a health services professor at UCLA, in 1968. The original model was expanded through numerous iterations and its most recent form models past the use of services to end at health outcomes and includes feedback loops.\n\nA major motivation for the development of the model was to offer measures of access. Andersen discusses four concepts within access that can be viewed through the conceptual framework. Potential access is the presence of enabling resources, allowing the individual to seek care if needed. Realized access is the actual use of care, shown as the outcome of interest in the earlier models. The Andersen framework also makes a distinction between equitable and inequitable access. Equitable access is driven by demographic characteristics and need whereas inequitable access is a result of social structure, health beliefs, and enabling resources.\n\nAndersen also introduces the concept of mutability of his factors. The idea here being that if a concept has a high degree of mutability (can be easily changed) perhaps policy would be justified in using its resources to do rather than a factor with low mutability. Characteristics that fall under demographics are quite difficult to change, however, enabling resources is assigned a high degree of mutability as the individual, community, or national policy can take steps to alter the level of enabling resources for an individual. For example, if the government decides to expand the Medicaid program an individual may experience an increase in enabling resources, which in turn may beget an increase in health services usage. The RAND Health Insurance Experiment (HIE) changed a highly mutable factor, out-of-pocket costs, which greatly changed individual rates of health services usage.\n\nThe initial behavior model was an attempt to study of why a family uses health services. However, due to the heterogeneity of family members the model focused on the individual rather than the family as the unit of analysis. Andersen also states that the model functions both to predict and explain use of health services.\n\nA second model was developed in the 1970s in conjunction with Aday and colleagues at the University of Chicago. This iteration includes systematic concepts of health care such as current policy, resources, and organization. The second generation model also extends the outcome of interest beyond utilization to consumer satisfaction.\n\nThe next generation of the model builds upon this idea by including health status (both perceived and evaluated) as outcomes alongside consumer satisfaction. Furthermore, this model include personal health practices as an antecedent to outcomes, acknowledging that it not solely use of health services that drives health and satisfaction. This model emphasizes a more public health approach of prevention, as advocated by Evans and Stoddart wherein personal health practices (i.e. smoking, diet, exercise) are included as a driving force towards health outcomes.\n\nThe 6th iteration of Andersen’s conceptual framework focuses on the individual as the unit of analysis and goes beyond health care utilization, adopting health outcomes as the endpoint of interest. This model is further differentiated from its predecessors by using a feedback loop to illustrate that health outcomes may affect aspects such as health beliefs, and need. It added genetic susceptibility as a predisposing determinant and quality of life as an outcome By using the framework’s relationships we can determine the directionality of the effect following a change in an individual’s characteristics or environment. For example, if one experiences an increase in need as a result of an infection, the Andersen model predicts this will lead to an increased use of services (all else equal). One potential change for a future iteration of this model is to add genetic information under predisposing characteristics. As genetic information becomes more readily available it seems likely this could impact health services usage, as well as health outcomes, beyond what is already accounted for in the current model.\n\nThe model has been criticized for not paying enough attention to culture and social interaction but Andersen argues this social structure is included in the \"predisposing characteristics\" component. Another criticism was the overemphasis of need and at the expense of health beliefs and social structure. However, Andersen argues need itself is a social construct. This is why need is split into perceived and evaluated. Where evaluated need represents a more measurable/objective need, perceived need is partly determined by health beliefs, such as whether or not people think their condition is serious enough to seek health services. Another limitation of the model is its emphasis on health care utilization or adopting health outcomes as a dichotomous factor, present or not present. Other help-seeking models also consider the type of help source, including informal sources. More recent work has taken help-seeking behaviors further, and more real-world, by including online and other non-face-to-face sources.\n"}
{"id": "35619802", "url": "https://en.wikipedia.org/wiki?curid=35619802", "title": "Body cathexis", "text": "Body cathexis\n\nBody cathexis is defined as the degree of satisfaction or dissatisfaction one feels towards various parts and aspects of his or her own body. This evaluative dimension of body image is dependent on a person’s investment of mental and emotional energy in body size, parts, shape, processes, and functions, and is integral to one’s sense of self-concept. First recognized by Jourard and Secord, body cathexis is assessed by examining correlations between measures of self-concept or esteem and bodily attitudes. An individual’s evaluation of his or her own body tends to drive various behaviors, including clothing choices and weight management, and the existence of a universal ideal for certain dimensions of body type is, in many cases, a source of anxiety and insecurity.\n\nWhile the body has been studied by psychologists from numerous different viewpoints, few recent reports of systematic empirical research into feelings about the body exist. However, body cathexis is of crucial importance to understanding personality, since feelings about the body closely correspond to feelings about the self and produce marked behavioral consequences. Due to the substantial amount of attention individuals devote to the grooming and concern for bodily appearance, body cathexis is believed to be intrinsically related to the self-concept, with high self-esteem and self-acceptance serving as preventative factors against body dissatisfaction.\n\nAmong females in particular, one’s general attitude about the body is a significant personality variable with mental health implications. Recent studies have examined the effects of bodily attitudes on consumer dissatisfaction or satisfaction with fit of apparel, disordered eating, and participation in weight loss programs.\n\nFirst introduced by Secord and Jourard in 1953, the Body Cathexis Scale provides an objective measure of one’s feelings towards various aspects of his or her body. Originally composed of 46 items, the questionnaire asks individuals to indicate the strength and direction of feeling he or she has about certain bodily parts and functions according to a five-point Likert scale ranging from 1, “strongly negative,” to 5, “strongly positive.”\n\nThe body cathexis items used in the scale include body characteristics such as “width of shoulders,” “facial complexion,” and “body build.” Individuals are to consider each of the items listed and assign a numerical score that best represents his or her feelings about the various body aspects. Total body cathexis (BC) is obtained by summing the ratings for each of the 46 characteristics and dividing by total number of items.\n\nBody image is one of the most significant components of an individual’s self-concept. One's perception of his or her body and the feelings associated with this perceived image greatly influence overall satisfaction with the self and can predict levels of self-esteem. The relationship between body image and the self-concept has been investigated extensively by Secord and Jourard, and as their research indicates, self-esteem scores and personal identification are highly correlated with body cathexis, acceptance, and overall satisfaction with physical body traits and functions.\n\nAmong the few empirical studies relevant to the relatedness of the body and the self is that of Schilder, who – through a series of self-report questionnaires – procured evidence suggesting that negative feelings, associations, and memories about the body can probe higher levels of dissatisfaction with the self. In approaching the problem of body cathexis appraisal, Secord and Jourard adapted Shilder’s methods to test their hypothesis that feelings about the body are correlated with overall feelings about the self. In their study, the researchers developed a method for assessing an individual’s feelings towards his or her bodily features in order to ascertain whether or not the variables attained through these methods are relevant to personality theory. Using scales of body cathexis and self cathexis, the Maslow Test of Psychological Security-Insecurity, and an anxiety-related body cathexis homonym test, Secord and Jourard concluded that the body and self tend to be cathected to the same degree. Consequently, as suggested by the results of both the body cathexis and self cathexis scales (which represent attitudes about conceptual aspects of the self in correlation to the body), low body cathexis is significantly associated with anxiety, insecurity, and negative perceptions of the self.\n\nLikewise, an individual’s perception of self-worth is a fluctuating attitude that can rise and fall with changing components of the physical self. This attitude, coined self-esteem, is an evaluative component of the psychological self that is partially dependent upon one’s satisfaction with physical appearance. As various studies indicate, changes in body composition, perceptions of physical attractiveness, and overall body condition provide inferential support for the claim that body image is related to an individual’s self-esteem and perceived worth.\n\nBecause body image and body weight are a high priority in western culture, men and women alike face gender-based societal pressures to achieve an ideal body image, which in turn influences feelings about the body and preoccupation with size, shape, weight, and appearance. According to Salusso-Deonier and Schwarzkopf, gender is a salient factor in body image development, and due to sex differences in the management and enhancement of appearance, females tend to exhibit more negative cathexis responses to the body and self as compared to men.\n\nAs their study suggests, body cathexis scores among females tend to decrease as body type varies from the ideal thin, while body cathexis scores among males show similar trends when one’s perceived body is fatter or thinner than the muscular ideal. However, though previous literature demonstrates that both men and women are concerned with weight and appearance, men generally have higher body cathexis than women. Cultural scripting, particularly male socialization that fosters exercise involvement and physical fitness, may be partially responsible for higher body cathexis scores among males, since participation in fitness activities tends to yield significant improvement in body satisfaction. Similarly, dysphoric body image experiences often mediated by specific contextual cues also promote sizable sex differences in body cathexis and the occurrence of negative body image emotions. Relative to men, women are more strongly invested in their looks and tend to report a more negative overall body image evaluation. As the findings of their study confirm, Muth and Cash suggest that the gender-differential societal and personal standards of body attractiveness place women at a higher predisposition for less favorable, more invested, and more distressing body image attitudes.\n\nFeelings about the body have marked behavioral consequences, and as both casual and clinical observations suggest, body cathexis and body perceptions play a significant role in garment fit satisfaction. While the physical dimensions and product design of apparel are partially responsible for consumers’ dissatisfaction with clothing fit, consumers are often quick to blame themselves and their body type when a product does not work. For females in particular, fashionable clothing is often designed to fit a specific body type; thus, when a clothing article does not fit properly, the consumer tends to blame the poor fit on their body and not the design of the apparel – a result of the individual’s low body cathexis.\n\nSimilarly, social stimuli may play a reciprocal role in anticipating behavior, contributing to the formation of negative feelings about the body. Since consumers often rely on social information and preconceived notions of the ideal body image when shaping their self-concept, apparel fit may contribute to body cathexis and overall feelings about the self. Therefore, dissatisfaction with the fit of garments can lead to lower body cathexis and negative attitudes regarding overall appearance, body dimensions, and weight.\n\nAlong with garment fit satisfaction, research also indicates that body image attitudes and emotions may be correlated with disordered eating behaviors. Body dissatisfaction, especially negative attitudes about weight, is a significant risk factor for the development of depressive symptoms and low self-esteem, as well as unhealthy weight control strategies, such as skipping meals, fasting, crash dieting, and self-induced purging. Low body cathexis and preoccupation can contribute to the development of eating disorders among female adolescents in particular, often a result of societal pressures and expectations to achieve the ideal, thin body type.\n"}
{"id": "46605765", "url": "https://en.wikipedia.org/wiki?curid=46605765", "title": "Bousfield class", "text": "Bousfield class\n\nIn algebraic topology, the Bousfield class of, say, a spectrum \"X\" is the set of all (say) spectra \"Y\" whose smash product with \"X\" is zero: formula_1. Two objects are Bousfield equivalent if their Bousfield classes are the same.\n\nThe notion applies to module spectra and in that case one usually qualifies a ring spectrum over which the smash product is taken.\n\n\n"}
{"id": "28754334", "url": "https://en.wikipedia.org/wiki?curid=28754334", "title": "Bowers v. Baystate Technologies, Inc.", "text": "Bowers v. Baystate Technologies, Inc.\n\nBowers v. Baystate Technologies, 320 F.3d 1317 (Fed. Cir. 2003), was a U.S. Court of Appeals Federal Circuit case involving Harold L. Bowers (doing business as HLB Technology) and Baystate Technologies over patent infringement, copyright infringement, and breach of contract. In the case, the court found that Baystate had breached their contract by reverse engineering Bower's program, something expressly prohibited by a shrink wrap license that Baystate entered into upon purchasing a copy of Bower's software. This case is notable for establishing that license agreements can preempt fair use rights as well as expand the rights of copyright holders beyond those codified in US federal law.\n\nBaystate Technologies, Inc (\"Baystate\") and HLB Technology (\"Bowers\") were competing companies which created add-ons that interacted with a computer aided design (CAD) program known as CADKEY.\n\nBowers was the patent holder of a system called Cadjet that simplified interfacing with CAD software, which he began to license commercially in 1989. Bower's initial software offering was later combined with a product called Geodraft that was produced by George W. Ford III (Ford) that inserted tolerances compliant with ANSI for features in a CAD design. Together, these products were marketed as Designer's Toolkit. The Designer's Toolkit was sold with a shrink-wrap license that prohibited reverse engineering.\n\nBaystate sold competing CADKEY tools including Draft-Pak version 1 and 2. According to the court filings, Baystate acquired a copy of Bowers' Designer's Toolkit, and three months later, Baystate released version 3 of Draft-Pak which substantially overlapped with the features offered by Designer's Toolkit.\n\nIn 1991, Baystate sued Bowers seeking declaratory judgement that 1) Baystate's products do not infringe on Bowers' patent 2) the patent is invalid, and 3) the patent is unenforceable. Bowers filed counterclaims for copyright infringement, patent infringement, and breach of contract, contending that Baystate had reverse engineered Designer's Toolkit. At court, expert testimonial revealed \"evidence of extensive and unusual similarities\" between Draft-Pak and Designer's Toolkit, supporting Bowers' claim that Baystate had reverse-engineered a copy of his software. The District Court of Massachusetts concluded that Bower's was entitled to damages, finding that the shrink wrap license tied to Bowers' software preempted any fair use case for reverse engineering as allowed by Copyright law. Baystate appealed the district courts decision.\n\nThe central question the Federal Court addressed in Bowers v. Baystate was whether a shrink-wrap license that forbids reverse engineering was preempted by federal Copyright law which expressly permits reverse engineering.\n\nThe majority opinion of the Federal Court upheld that parties can freely enter into license agreements that enforce stricter requirements than copyright and that such agreements are not preempted by copyright law. In their decision, the court cited a number of prior cases involving contractual constraints that extend copyright law: \n\n\nA dissenting opinion was entered by Judge Dyk agreeing with all decisions except the decision that a copyright law does not preempt a state contract.\n\nDyk formed his argument using patent and copyright law cases:\n\n\nCritics scrutinized the outcome of this case, arguing that it not only allows companies to use state contract law to expand copyright protections, but also creates non-negotiated license terms which are equivalent to patent like protection without the limiting conditions of patent law. Critics further argued this precedent is unrealistic for the software industry. Reverse engineering is not only considered necessary to keep up with \"feature wars\", which \"Bowers v. Baystate\" is an example of, but is essential for interoperability and security purposes.\n\n\n"}
{"id": "55418299", "url": "https://en.wikipedia.org/wiki?curid=55418299", "title": "Chinese Apartment Art", "text": "Chinese Apartment Art\n\nChinese Apartment Art is the avante-gard art that was produced underground during the 1970s - 1990s in China. The word for apartment is \"gongyu\" in Chinese. The literal meaning of apartment is “government-owned residential complex” or “public house.” Chinese artists created private work space within public residential complexes out of sight of authorities. The pieces tend to be smaller as to fit on the confines of the living spaces.\n"}
{"id": "44103965", "url": "https://en.wikipedia.org/wiki?curid=44103965", "title": "Classification chart", "text": "Classification chart\n\nClassification chart or classification tree is a synopsis of the classification scheme, designed to illustrate the structure of any particular field.\n\nClassification is the process in which ideas and objects are recognized, differentiated, and understood, and classification charts are intended to help create and eventually visualized the outcome.\n\nAccording to Brinton \"in a classification chart the facts, data etc. are arranged so that the place of each in relation to all others is readily seen. Quantities need not be given, although a quantitative analysis adds to the value of a classification chart.\"\n\nKarsten (1923) explained, that \"in all chart-making, the material to be shown must be accurately compiled before it can be charted. For an understanding of the classification chart, we must delve somewhat into the mysteries of the classification and indexing. The art of classification calls into play the power of visualizing a 'whole' together with its 'parts'.\"\n\nEarly examples of classification chart are:\nEarly classification chart are often visualized in a tree structure. Modern charts can also be presented in table or as an infographic.\n\nThe term \"classification chart\" came into use in the 20th century. In his 1939 \"Graphic presentation.\" (first edition 1919) Willard Cope Brinton was one of the first to devoted a whole chapter on classification charts.\n\n"}
{"id": "22665441", "url": "https://en.wikipedia.org/wiki?curid=22665441", "title": "Completely regular semigroup", "text": "Completely regular semigroup\n\nIn mathematics, a completely regular semigroup is a semigroup in which every element is in some subgroup of the semigroup. The class of completely regular semigroups forms an important subclass of the class of regular semigroups, the class of inverse semigroups being another such subclass. A H Clifford was the first to publish a major paper on completely regular semigroups though he used the terminology \"semigroups admitting relative inverses\" to refer to such semigroups. The name \"completely regular semigroup\" stems from Lyapin's book on semigroups. In the Russian literature, completely regular semigroups are often called \"Clifford semigroups\".\nIn the English literature, the name \"Clifford semigroup\" is used synonymously to \"inverse Clifford semigroup\", and refers to a completely regular inverse semigroup.\nIn a completely regular semigroup, each Green \"H\"-class is a group and the semigroup is the union of these groups. Hence completely regular semigroups are also referred to as \"unions of groups\". Epigroups generalize this notion and their class includes all completely regular semigroups.\n\n\"While there is an abundance of natural examples of inverse semigroups, for completely regular semigroups the examples (beyond completely simple semigroups) are mostly artificially constructed: the minimum ideal of a\nfinite semigroup is completely simple, and the various relatively free completely regular semigroups are the other more or less natural examples.\"\n"}
{"id": "17065840", "url": "https://en.wikipedia.org/wiki?curid=17065840", "title": "Consciousness Industry", "text": "Consciousness Industry\n\nThe Consciousness Industry is a term coined by author and theorist Hans Magnus Enzensberger, which identifies the mechanisms through which the human mind is reproduced as a social product. Foremost among these mechanisms are the institutions of mass media and education. According to Enzensberger, the mind industry does not produce anything specific; rather, its main business is to perpetuate the existing order of man's domination over man.\n\nHans Haacke elaborates on the consciousness industry as it applies to the arts in a wider system of production, distribution, and consumption. Haacke specifically implicates museums as manufacturers of aesthetic perception that fail to acknowledge their intellectual, political, and moral authority: \"rather than sponsoring intelligent, critical awareness, museums thus tend to foster appeasement.\"\n\n\n\nMuseums: Managers of Consciousness \n"}
{"id": "1074618", "url": "https://en.wikipedia.org/wiki?curid=1074618", "title": "Contestant", "text": "Contestant\n\nA contestant is someone who takes part in a competition, usually a professional competition or a game show on television. The participants competing against each other have to go through rounds. The winners may have to compete in later stages or rounds until there is just one winner.\n\nGame show contestants are usually members of the general public who are selected via some sort of qualification system, such as a general knowledge or IQ test (an example of this is \"Jeopardy!\", in which contestants must pass a 50-question test) who then appear on the televised show. Game show careers are usually short-lived, perhaps lasting only one day. A very small minority go on to achieve national fame, such as Joyce Brothers and Ken Jennings in the United States and Charles Ingram in the United Kingdom.\n\nSome game shows deliberately target celebrity contestants, such as \"Match Game\" and \"Blankety Blank\".\n\nThere are links between game show contestants and other games and hobbies, such as Scrabble players who take part in word-based game shows like \"Countdown\" and \"BrainTeaser\".\n\nReality TV contestants are also selected from the general public, but again celebrity versions do exist. Shows like \"Big Brother\" select contestants from the general public by examining video logs that the contestants send in to the show. A small number, perhaps 10 or 15, are selected to live in a confined house separated from the outside world.\n\nIn general, reality TV contestants are set some task to do or achieve in a selected place, such as an isolated house or desert island, and are filmed for long periods of the day. In the case of \"Big Brother\", they are filmed up to 24 hours a day.\n"}
{"id": "6459406", "url": "https://en.wikipedia.org/wiki?curid=6459406", "title": "Cooperative breeding", "text": "Cooperative breeding\n\nCooperative breeding is a social system characterized by alloparental care: offspring receive care not only from their parents, but also from additional group members, often called helpers. Cooperative breeding encompasses a wide variety of group structures, from a breeding pair with helpers that are offspring from a previous season, to groups with multiple breeding males and females (polygynandry) and helpers that are the adult offspring of some but not all of the breeders in the group, to groups in which helpers sometimes achieve co-breeding status by producing their own offspring as part of the group's brood. Cooperative breeding occurs across taxonomic groups including birds, mammals, fish, and insects.\n\nCosts for helpers include a fitness reduction, increased territory defense, offspring guarding and an increased cost of growth. Benefits for helpers include a reduced chance of predation, increased foraging time, territory inheritance, increased environmental conditions and an inclusive fitness. Inclusive fitness is the sum of all direct and indirect fitness, where direct fitness is defined as the amount of fitness gained through producing offspring. Indirect fitness is defined as the amount of fitness gained through aiding related individuals offspring, that is relatives are able to indirectly pass on their genes through increasing the fitness of related offspring. This is also called kin selection.\n\nFor the breeding pair, costs include increased mate guarding and suppression of subordinate mating. Breeders receive benefits as reductions in offspring care and territory maintenance. Their primary benefit is an increased reproductive rate and survival.\n\nCooperative breeding causes the reproductive success of all sexually mature adults to be skewed towards one mating pair. This means the reproductive fitness of the group is held within a select few breeding members and helpers have little to no reproductive fitness. With this system, breeders gain an increased reproductive fitness, while helpers gain an increased inclusive fitness.\n\nMany hypotheses have been presented to explain the evolution of cooperative breeding. The concept behind cooperative breeding is the forfeiting of an individual's reproductive fitness to aid the reproductive success of others. This concept is hard to understand and the evolution of cooperative breeding is important, but difficult to explain. Most hypotheses aim to determine the reason helpers selectively reduce their fitness and take on an alloparental role.\n\nKin selection is the evolutionary strategy of aiding the reproductive success of related organisms, even at a cost to the own individual's direct fitness. Hamilton's rule (rB−C>0) explains that kin selection will exist if the genetic relatedness (r) of the aided recipient to the aiding individual, times the benefit to the aid recipient (B) is greater than the cost to the aiding individual(C). For example, the chestnut-crowned babbler (\"Pomatostomus ruficeps\") has been found to have high rates of kin selection. Helpers are predominantly found aiding closely related broods over nonrelated broods. Additional species such as \"Neolamprologus pulcher\" have shown that kin selection is a dominant driving force for cooperative breeding.\n\nGroup augmentation presents a second hypothesis towards the evolution of cooperative breeding. This hypothesis suggests that increasing the size of the group through the addition of helpers aids in individual survival and may increase the helper's future breeding success. Group augmentation is favored if the grouping provides passive benefits for helpers in addition to inclusive fitness. By group augmenting, each individual member reduces their chances of becoming a victim of predation. Additionally, an increase in members reduces each helper's duration as a sentinel (standing upon a high surface to survey for predators) or babysitting (guarding the offspring and den). The reduction in these guarding behaviors enables helpers to forage for longer periods.\n\nLukas et al. proposed an evolutionary model for cooperative breeding, which linked the coevolution of polytocy, production of multiple offspring, and monotocy, production of single offspring, with the evolution of cooperative breeding. The model is based on the evolution of larger litters forcing the need for helpers to maintain the high reproductive costs, thus leading to cooperative breeding. Lukas et al. suggests polytocy may have encouraged the evolution of cooperative breeding. Their proposed model suggests the transition from monotocy to polytocy is favorable. Additionally, they found the transition from polytocy without cooperative breeding to polytocy with cooperative breeding is highly favorable. This suggests cooperative breeding evolved from noncooperative breeding monotocy to cooperative breeding polytocy.\n\nToday, there is growing support for the theory that cooperative breeding evolved by means of some form of mutualism or reciprocity. Mutualism is a form of symbiosis that is beneficial to both involved organisms. Mutualism has many forms and can occur when the benefits are immediate or deferred, when individuals exchange beneficial behaviors in turn, or when a group of individuals contribute to a common good, where it may be advantageous for all group members to help raise young. When a group raises young together, it may be advantageous because it maintains or increases the size of the group. The greatest amount of research has been invested in reciprocal exchanges of beneficial behavior through the iterated prisoner's dilemma. In this model, two partners can either cooperate and exchange beneficial behavior or they can defect and refuse to help the other individual.\n\nEnvironmental conditions govern whether offspring disperse from their natal group or remain as helpers. Food or territory availability can encourage individuals to disperse and establish new breeding territories, but unfavorable conditions promote offspring to remain at the natal territory and become helpers to obtain an inclusive fitness. Additionally, remaining at the natal territory enables offspring to possibly inherit the breeding role and/or territory of their parents.\n\nA final factor influencing cooperative breeding is sexual dispersal. Sexual dispersal is the movement of one sex, male or female, from the natal territory to establish new breeding grounds. This is highly regulated by the reproductive costs in producing a male versus a female offspring. Maternal investment within female offspring may be considerably higher than male offspring for one species, or vice versa for another. During unfavorable conditions the cheaper sex will be produced at higher ratios.\n\nA second factor affecting the sexual dispersal is the difference in ability of each sex to establish a new breeding territory. Carrion crow (\"Corvus corone\") were found to produce more female offspring in favorable environmental conditions. Female \"Corvus corone\" have been found to establish successful breeding territories at a higher rate than males. Male \"Corvus corone\" were produced at a higher rate under unfavorable conditions. Males were found to remain at the natal territory and become helpers. Thus, if environmental conditions favor the dispersal of a specific sex it is considered the dispersal sex. If environmental conditions are unfavorable females may produce the philopatric sex, therefore generating more helpers and increasing the occurrence of cooperative breeding.\n\nBreeder costs consist of prenatal care, postnatal care and maintenance of breeding status. Prenatal care is the amount of maternal investment during fetus gestation and postnatal care is the investment following birth. Examples of prenatal care are fetal, placentae, uterus and mammary tissue development. Postnatal examples are lactation, food provisions and guarding behavior.\n\nDominant males and females exhibit suppressive behaviors towards subordinates to maintain their breeding status. These suppressive acts are dependent upon the sex ratio of helpers. Therefore, the costs will be altered depending upon the helpers. For example, if there are more male helpers as compared to females, then the dominant male will suppress subordinate males and experience a higher cost. The opposite is true for females. Breeders will even suppress subordinates from mating with other subordinates.\n\nThe cost to helpers varies depending upon presence or absence of related offspring. The presence of offspring has been found to increase the helper’s cost by the helper contributing to guard behaviors. Guarding behaviors, such as babysitting, can cause individuals to experience weight loss on an exponential scale depending upon the duration of the activity. Other activities, such as sentinel behavior and bipedal surveillance, cause helpers to have reduced foraging intervals inhibiting their weight gains. The reduced foraging behavior and increased weight loss reduces their chance to breed successfully, but increase their inclusive fitness by increasing the survival of related offspring.\n\nHelpers contribute depending upon the cost. The act of helping requires an allocation of energy towards actually performing the behavior. Prolonged allocation of energy may greatly impact a helper’s growth. In banded mongoose (\"Mungos mungo\") juvenile male helpers contribute far less than females. This is due to a difference in the age of sexual maturity. Female banded mongooses reach sexual maturity at one year of age, but males reach sexual maturity at two years of age. The difference in age causes the prolonged energy allocation to be detrimental to a specific sex.\n\nMale juvenile \"Mungos mungo\" may reduce helping behaviors until sexual maturity is reached. Similarly, if there is a lack of food due to environmental conditions, such as reduced rainfall, the degree of helper input may be reduced greatly within juveniles. Adults may maintain their full activity because they are sexually mature.\n\nAdditionally, the costs of being a helper can be more detrimental to one sex. For example, territorial defense costs are generally male dependent and lactation is female dependent. Meerkats (\"Suricata suricatta\") have exhibited male territory defense strategies, where male helpers will fend off intruding males to prevent such intruders from mating with subordinates or dominant females. Additionally, subordinate female pregnant helpers are sometimes exiled from the group by a dominant female. This eviction causes the subordinate female to have an abortion, which frees up resources such as lactation and energy that can be used to help the dominant female and her pups.\n\nRarely, a female helper or breeder will defend the territory while males are present. This suggests specific helping costs, such as territory defense, is rooted to one sex.\n\nCooperative breeding reduces the costs of many maternal investments for breeding members. Helpers aid the breeding females with provisioning, lactation stress, guarding of offspring and prenatal investment. Increasing the number of helpers enables a breeding female or male to maintain a healthier physique, higher fitness, increased lifespan and brood size.\n\nFemale helpers can aid in lactation, but all helpers, male or female, can aid in food provisioning. Helper food provisioning reduces the need for the dominant breeding pair to return to the den, thus allowing them to forage for longer periods. The dominant female and male will adjust their care input, or food provisioning, depending on the degree of activity of the helpers.\n\nThe presence of helpers allows the breeding female to reduce her prenatal investment in the offspring, which may lead to altricial births; altricial is the production of young which are dependent upon adult aid to survive. This enables the breeding female to retain energy to be used within a new breeding attempt. Overall, the addition of helpers to a breeding pair encourages multiple reproductions per year, and increases the rate of successful reproduction.\n\nMale breeders can benefit directly from reproducing with subordinate females and aiding in raising the young. This allows the male to obtain a “repayment investment” within these subordinate offspring. These offspring have a higher chance to become helpers once sexual maturity is reached. Thus, paying into their care will increase the dominant male’s overall fitness in the future. This act ensures the dominant male subordinate helpers for future reproduction.\n\nHelpers primarily benefit from an inclusive fitness. Helpers maintain an inclusive fitness while aiding related breeders and offspring. This type of kinship may lead to inheritance of quality foraging and breeding territories, which will increase the future fitness of helpers. Additional, helpers experience an increased chance of being helped if they were once a helper.\n\nFinally, helpers benefit from group interactions, such as huddling for thermodynamic benefits. These interactions provide necessary elements to survive. They may also benefit from the increased group interaction on the level of cognitive concern for one another increasing their overall life span and survival.\n\nApproximately eight percent of bird species are known to regularly engage in cooperative breeding, mainly among the Coraciiformes, Piciformes, basal Passeri and Sylvioidea. Only a small fraction of these, for instance the Australian mudnesters, Australo-Papuan babblers and ground hornbills, are however absolutely \"obligately\" cooperative and cannot fledge young without helpers.\n\nThe benefits of cooperative breeding in birds have been well-documented. One example is the azure-winged magpie (\"Cyanopica cyanus\"), in which studies found that the offspring’s cell-mediated immune response was positively correlated with increase in the number of helpers at the nest. Studies on cooperative breeding in birds have also shown that high levels of cooperative breeding are strongly associated with low annual adult mortality and small clutch sizes, though it remains unclear whether cooperative breeding is a cause or consequence. It was originally suggested that cooperative breeding developed among bird species with low mortality rates as a consequence of “overcrowding” and thus fewer opportunities to claim territory and breed. However, many observers today believe cooperative breeding arose because of the need for helpers to rear young in the extremely infertile and unpredictable environments of Australia and sub-Saharan Africa under the rare favourable conditions.\n\nMeerkats become reproductively active at one year of age and can have up to four litters per year. However, usually it is the alpha pair that reserves the right to mate and will usually kill any young that is not their own. While the alpha female is away from the group, females that have never reproduced lactate and hunt in order to feed the pups, as well as watch, protect, and defend them from predators. Although it was previously thought that a meerkat’s contribution to a pup’s diet depended on the degree of relatedness, it has been found that helpers vary in the number of food items they give to pups. This variation in food offering is due to variation in foraging success, sex, and age. Research has additionally found that the level of help is not correlated to the kinship of the litters they are rearing.\n\nCooperative breeding has been described in several canid species including red wolves, Arctic foxes and Ethiopian wolves.\n\nCooperative breeding entails one or more individuals, usually females, acting as \"helpers\" to one or a few dominant female breeders, usually helpers' kin. This sociosexual system is rare in primates, so far demonstrated among Neotropical callitricids, including marmosets and tamarins. Cooperative breeding requires \"repression\" of helpers' reproduction, by pheromones emitted by a breeder, by coercion, or by self-restraint. Sarah Blaffer Hrdy believes that cooperative breeding is an ancestral trait in humans, a controversial proposition.\n"}
{"id": "1344164", "url": "https://en.wikipedia.org/wiki?curid=1344164", "title": "Data flow diagram", "text": "Data flow diagram\n\nA data flow diagram (DFD) is a graphical representation of the \"flow\" of data through an information system, modelling its \"process\" aspects. A DFD is often used as a preliminary step to create an overview of the system without going into great detail, which can later be elaborated. DFDs can also be used for the visualization of data processing (structured design).\n\nA DFD shows what kind of information will be input to and output from the system, how the data will advance through the system, and where the data will be stored. It does not show information about process timing or whether processes will operate in sequence or in parallel, unlike a traditional structured flowchart which focuses on control flow, or a UML activity workflow diagram, which presents both control and data flows as a unified model.\n\nIn the 1970s, Larry Constantine, the original developer of structured design, proposed data flow diagrams as a practical technique based on Martin and Estrin's \"Data Flow Graph\" model of computation. \n\nData flow diagrams (DFD) quickly became a popular way to visualize the major steps and data involved in software system processes. DFDs were usually used to show data flow in a computer system, although they could in theory be applied to business process modeling. DFDs were useful to document the major data flows or to explore a new high-level design in terms of data flow.\n\nData flow diagrams are also known as bubble charts. DFD is a designing tool used in the top-down approach to Systems Design. This context-level DFD is next \"exploded\", to produce a Level 1 DFD that shows some of the detail of the system being modeled. The Level 1 DFD shows how the system is divided into sub-systems (processes), each of which deals with one or more of the data flows to or from an external agent, and which together provide all of the functionality of the system as a whole. It also identifies internal data stores that must be present in order for the system to do its job, and shows the flow of data between the various parts of the system.\n\nData flow diagrams are one of the three essential perspectives of the structured-systems analysis and design method SSADM. The sponsor of a project and the end users will need to be briefed and consulted throughout all stages of a system's evolution. With a data flow diagram, users are able to visualize how the system will operate, what the system will accomplish, and how the system will be implemented. The old system's data flow diagrams can be drawn up and compared with the new system's data flow diagrams to draw comparisons to implement a more efficient system. Data flow diagrams can be used to provide the end user with a physical idea of where the data they input ultimately has an effect upon the structure of the whole system from order to dispatch to report. How any system is developed can be determined through a data flow diagram model.\n\nIn the course of developing a set of \"levelled\" data flow diagrams the analyst/designer is forced to address how the system may be decomposed into component sub-systems, and to identify the transaction data in the data model.\n\nData flow diagrams can be used in both the Analysis and Design phases of the SDLC.\n\nThere are different notations to draw data flow diagrams (Yourdon & Coad and Gane & Sarson), defining different visual representations for processes, data stores, data flow, and external entities.\n\nA logical DFD captures the data flows that are necessary for a system to operate. It describes the processes that are undertaken, the data required and produced by each process, and the stores needed to hold the data. On the other hand, a physical DFD shows how the system is actually implemented, either at the moment (Current Physical DFD), or how the designer intends it to be in the future (Required Physical DFD). Thus, a Physical DFD may be used to describe the set of data items that appear on each piece of paper that move around an office, and the fact that a particular set of pieces of paper are stored together in a filing cabinet. It is quite possible that a Physical DFD will include references to data that are duplicated, or redundant, and that the data stores, if implemented as a set of database tables, would constitute an un-normalised (or de-normalised) relational database. In contrast, a Logical DFD attempts to capture the data flow aspects of a system in a form that has neither redundancy nor duplication.\n\n\n"}
{"id": "47547340", "url": "https://en.wikipedia.org/wiki?curid=47547340", "title": "Debreu theorems", "text": "Debreu theorems\n\nIn economics, the Debreu theorems are several statements about the representation of a preference ordering by a real-valued function. The theorems were proved by Gerard Debreu during the 1950s.\n\nSuppose we interrogate a person and ask him questions of the form \"Do you prefer A or B?\" (when A,B can be options, actions to take, states of the world, consumption bundles, etc.). We write down all the answers. Then, we want to represent the preferences of that person by a numeric \"utility function\", such that the utility of option A is larger than option B if and only if the agent prefers A to B.\n\nThe Debreu theorems come to answer the following basic question: what conditions on the preference relation of the agent guarantee that we can find such representative utility function?\n\nThe 1954 Theorems say, roughly, that every preference relation which is complete, transitive and continuous, can be represented by a continuous ordinal utility function.\n\nThe theorems are usually applied to spaces of finite commodities. However, they are applicable in a much more general setting. These are the general assumptions:\n\nEach one of the following conditions guarantees the existence of a real-valued continuous function that represents the preference relation formula_1:\n\n1. The set of equivalence classes of the relation formula_14 (defined by: formula_15 iff formula_16 and formula_17) are a countable set.\n\n2. There is a countable subset of X, formula_18, such that for every pair of non-equivalent elements formula_19, there is an element formula_20 that separates them (formula_21).\n\n3. X is separable and connected.\n\n4. X is second countable. This means that there is a countable set S of open sets, such that every open set in X is the union of sets of the class S.\n\nThe proof for the fourth result had a gap which Debreu later corrected.\n\nA. Let formula_22 with the standard topology (the Euclidean topology). Define the following preference relation: formula_23 iff formula_24. It is continuous because for every formula_25, the sets formula_26 and formula_27 are closed half-planes. Condition 1 is violated because the set of equivalence classes is uncountable. However, condition 2 is satisfied with Z as the set of pairs with rational coordinates. Condition 3 is also satisifed since X is separable and connected. Hence, there exists a continuous function which represents formula_1. An example of such function is formula_29.\n\nB. Let formula_22 with the standard topology as above. The lexicographic preferences relation is not continuous in that topology. For example, formula_31, but in every ball around (5,1) there are points with formula_32 and these points are inferior to formula_33. Indeed, this relation cannot be represented by a continuous real-valued function.\n\nDiamond applied Debreu's theorem to the space formula_34, the set of all bounded real-valued sequences with the topology induced by the supremum metric (see L-infinity). X represents the set of all utility streams with infinite horizon.\n\nIn addition to the requirement that formula_1 be total, transitive and continuous, He added a \"sensitivity\" requirement:\n\nUnder these requirements, every stream formula_36 is equivalent to a constant-utility stream, and every two constant-utility streams are separable by a constant-utility stream with a rational utility, so condition #2 of Debreu is satisfied, and the preference relation can be represented by a real-valued function.\n\nThe existence result is valid even when the topology of X is changed to the topology induced by the discounted metric: formula_43\n\nTheorem 3 of 1960 says, roughly, that if the commodity space contains 3 or more components, and every subset of the components is preferentially-independent of the other components, then the preference relation can be represented by an \"additive\" value function.\n\nThese are the general assumptions:\n\nThe function formula_47 is called \"additive\" if it can be written as a sum of \"n\" ordinal utility functions on the \"n\" factors:\nwhere the formula_51 are constants.\n\nGiven a set of indices formula_52, the set of commodities formula_53 is called \"preferentially independent\" if the preference relation formula_1 induced on formula_53, given constant quantities of the other commodities formula_56, does not depend on these constant quantities.\n\nIf formula_47 is additive, then obviously all subsets of commodities are preferentially-independent.\n\nIf all subsets of commodities are preferentially-independent AND at least three commodities are essential (meaning that their quantities have an influence on the preference relation formula_1), then formula_47 is additive.\n\nMoreover, in that case formula_47 is unique up to an increasing \"linear\" transformation.\n\nFor an intuitive constructive proof, see Ordinal utility - Additivity with three or more goods.\n\nTheorem 1 of 1960 deals with preferences on lotteries. It can be seen as an improvement to the von Neumann–Morgenstern utility theorem of 1947. The earlier theorem assumes that agents have preferences on lotteries with arbitrary probabilities. Debreu's theorem weakens this assumption and assumes only that agents have preferences on equal-chance lotteries (i.e., they can only answer questions of the form: \"Do you prefer A over an equal-chance lottery between B and C?\").\n\nFormally, there is a set formula_61 of sure choices. The set of lotteries is formula_62. Debreu's theorem states that if:\nThen there exists a cardinal utility function \"u\" that represents the preference relation on the set of lotteries, i.e.:\n\nTheorem 2 of 1960 deals with agents whose preferences are represented by frequency-of-choice. When they can choose between \"A\" and \"B\", they choose \"A\" with frequency formula_72 and \"B\" with frequency formula_73. The value formula_72 can be interpreted as measuring \"how much\" the agent prefers \"A\" over \"B\".\n\nDebreu's theorem states that if the agent's function \"p\" satisfies the following conditions:\nThen there exists a cardinal utility function \"u\" that represents \"p\", i.e:\n\n"}
{"id": "1120434", "url": "https://en.wikipedia.org/wiki?curid=1120434", "title": "Digital illustration", "text": "Digital illustration\n\nComputer illustration or digital illustration is the use of digital tools to produce images under the direct manipulation of the artist, usually through a pointing device such as a tablet or a mouse. It is distinguished from computer-\"generated\" art, which is produced by a computer using mathematical models created by the artist. It is also distinct from digital manipulation of photographs, in that it is an original construction \"from scratch\". (Photographic elements may be incorporated into such works, but they are not necessarily the primary basis\n\nMice are not very precise for drawing, so a graphics tablet is an important tool for a digital illustrator, because it allows the user to make a mark easily in any direction, in a way that reflects the natural or \"lively\" line made by the human hand. In addition to flexibility of movement, an industry-standard digital drawing tablet has a pressure-sensitive surface, allowing the illustrator to make marks that vary from faint to bold, and from thin to broad. These variations mimic traditional wet and dry media. Drawing on a digital drawing tablet starts to feel natural after about a week of practice. A hybrid graphics tablet/screen might be helpful, since the artist can see more accurately where to place strokes in the image, but the hardware is currently much more expensive.\n\nThere are two main types of tools used for digital illustration: bitmapped (also known as \"raster\") and vector applications. Bitmap applications are commonly called \"painting\" programs, while vector applications are called \"drawing\" programs. These terms reflect the difference in look-and-feel between the images created in each type of program.\nWith a bitmap application, the content is stored digitally in fixed rows and columns of pixels, which can be created in separate layers for more easily isolating and manipulating different parts of the image. A bitmap image contains information about each pixel's hue (color), luminance (brightness), and saturation (intensity of color). When the pointing device moves over an area of the image, new colors and values are applied to the underlying pixels. Painting tools allow the easy creation of \"fuzzy\" imagery, including effects such as glows and soft shadows, and textures such as fur, velvet, stone and skin, and are heavily used in photo-retouching.\n\nWith vector-based tools, the content is stored digitally as resolution-independent mathematical formulae describing lines (open paths), shapes (closed paths), and color fills, strokes or gradients. Vector paths are constructed of anchor points and path segments by using the pointing device to click and drag. Many vector graphics are readily available for download from online databases which can then be edited and incorporated into larger projects. Drawing tools typically create precise lines, shapes and patterns with well-defined edges and are superb for working with complex constructions such as maps and typography.\nDigital illustrations may include both raster and vector graphics in the same work. A bitmap image file may be saved in a format which embeds a layer of vector information, and vector image file may include imported bitmap images.\n\n"}
{"id": "17228639", "url": "https://en.wikipedia.org/wiki?curid=17228639", "title": "Disabilities (Jewish)", "text": "Disabilities (Jewish)\n\nJewish Disabilities were legal restrictions, limitations and obligations placed on European Jews in the Middle Ages, somewhat analogous to those imposed on Jews in the Muslim world. In Europe, the disabilities imposed on Jews included provisions requiring Jews to wear specific and identifying clothing such as the Jewish hat and the yellow badge, paying special taxes, swearing special oaths, living certain neighbourhoods, and forbidding Jews to enter certain trades (for example selling new clothes in medieval Sweden). Disabilities also included special taxes levied on Jews, exclusion from public life, restraints on the performance of religious ceremonies, and linguistic censorship. Some countries went even further and outright expelled Jews, for example England in 1290 (Jews were readmitted in 1655) and Spain in 1492 (readmitted in 1868).\n\nThe disabilities began to be lifted with Jewish emancipation in the late 18th and the 19th century. In 1791, Revolutionary France was the first country to abolish disabilities altogether, followed by Prussia in 1848. Emancipation of the Jews in the United Kingdom was achieved in 1858 after an almost 30-year struggle championed by Isaac Lyon Goldsmid with the ability of Jews to sit in parliament with the passing of the Jews Relief Act 1858. The newly united German Empire in 1871 abolished Jewish disabilities in Germany.\n\nThe first Jewish settlers in North America arrived in the Dutch colony of New Amsterdam in 1654. They were forbidden to hold public office, open a retail shop, or establish a synagogue. When the colony was seized by the British in 1664 Jewish rights remained unchanged, but by 1671 Asser Levy was the first Jew to serve on a jury in North America.\n\nIn the Russian Empire Jewish disabilities were completely abolished after the Russian Revolution in 1917, in which Jews had a prominent role. Soviet Russia had the largest population of Jews in the whole of Europe. However, extralegal antisemitic feelings and policies remained.\n\n"}
{"id": "154421", "url": "https://en.wikipedia.org/wiki?curid=154421", "title": "DragonQuest", "text": "DragonQuest\n\nDragonQuest is a fantasy role-playing game originally published by Simulations Publications (SPI) in 1980. Where first generation fantasy role-playing games such as \"Dungeons & Dragons\" restricted players to particular character classes, \"DragonQuest\" was one of the first games to utilize a system that emphasized skills, allowing more individual customization and a wider range of options.\n\nCharacter generation is much more involved than \"D&D\", with the player using 10 sided die to determine everything from the character's race to handedness to the number of points they have to distribute amongst the primary characteristics (Strength, Agility, Manual Dexterity, Magical Aptitude, Endurance, and Willpower) which determine the character's strengths and weaknesses. Being able to control the value of these attributes allows for greater flexibility in character generation. For example, players seeking a powerful magic user can divert points to Magical Aptitude and possibly Willpower. Those seeking pure fighters can invest their points in Strength, Agility and Manual Dexterity. By carefully balancing these numbers, fighter/mages, thief/assassins, and other combinations can be devised.\n\nThe magic system in \"DragonQuest\" features distinct Magical Colleges, each with its own group of spells and rituals. Player characters who enter magical study are assumed to have apprenticed with a mage of their particular Magic College, and have learned all the basic spells and a ritual or two from their former Master. Player characters cannot change Magic Colleges in the context of game play, and so are all essentially specialists in a college of magic. Some of the Magic Colleges include: Earth Magic, Air Magic, Fire Magic, Water Magic, Illusions, Ensorcelments, Greater & Lesser Summoning, and Necromantic Conjurations. The revised second edition and third edition (released by TSR) added some colleges and removed others.\n\nPlayers expend fatigue points to cast spells, and must roll percentile dice to succeed. Many of the more powerful spells have a very low chance of success, and may \"backfire\" with random results (many quite unpleasant). By expending experience points, a mage may improve their ability to cast specific spells by gaining rank in them. There are also advanced spells which can be obtained from more powerful mages in one's Magic College. This advanced knowledge may require a substantial cash payment or some kind of quest to obtain, however. Certain spells require expensive or rare elements to work properly, while the majority are merely spoken.\n\nSpecific rules exist for player Adepts to invest objects to create magic items (such as rings, amulets, weapons, etc.) for later use by themselves or other party members. Such items are also found on occasion during an expedition or exploration.\n\nAny player character may choose to learn various skills in \"DragonQuest\". Vocations such as Ranger, Thief, Assassin, Merchant, Courtesan, Navigator, Healer, Military Scientist, Mechanician, Beastmaster, Troubador may be acquired by expending the necessary experience points. Certain other skills, such as Stealth, Horseback Riding, reading and/or writing a Language can also be practiced and improved. Characters are not limited to any particular set of skills, and a Halfling Assassin who speaks perfect Elvish is technically possible.\n\nWeapons are learned in much the same manner as Vocations. The limitation is that weapons have various maximum ranks (levels) which can be achieved, while other skills usually top out at rank 10. Magic spells gain improved chances of success and better strength when rank is gained in them also, and this is done on a spell-by-spell basis (e.g. character is a fire mage and has improved his skill to rank 6 in Fireball, but rank 0 in many other spells of his magic college which he considers less important).\n\nCharacters are required to spend many weeks training \"after\" an experience-generating adventure in order to increase skill levels. Weapons training typically requires the aid of a person of greater skill than the player, and hiring a weapon-master can be expensive as well. Often, a group will return to town laden with gold, only to realize that between the several months spent training (room & board) and the cost of experts to assist them with training, most of the money is already spent! It is also possible that a character may actually \"be\" the expert in his/her local area, and thus might have to travel some distance to receive instruction from a person of greater ability.\n\n\"DQ\" uses a hex grid and miniatures for combat. Unlike other systems, where the miniatures are merely placeholders, \"DQ\" requires that characters know their facing, as attacks from the flanks and rear are more effective than frontal assaults. Combat takes place in 5-second \"pulses\" and characters may only move short distances while actively engaged in a meleè.\n\nEach character has a strike chance % based on (mostly) their manual dexterity and the base chance of the weapon used to attack. Additional factors, such as running into an attack or achieving surprise—as in an ambush—modify this base chance. The defender's defensive % is subtracted from this number, and percentile dice rolled to see if a hit is achieved. When a hit is delivered, the attacker rolls a d10, adds the weapon's attack bonus, and subtracts the target's armor rating. In some cases, such as a target with plate armor, few weapons can do much damage directly. Only certain special hits can damage the target severely. But with time, even the most heavily armored Knight can usually be worn-down.\n\nUnlike other systems, which use \"hit points\" to tally damage, \"DQ\" has a two-tiered system of fatigue and endurance. Normally a weapon does fatigue damage only, but an especially lucky hit may immediately cause \"endurance\" damage or even a \"grievous injury\", which allows the attacker to roll again on a table of nasty hits to the eyes, guts, etc. Once a character has lost all of his fatigue, he begins taking endurance damage instead. This is bad, since endurance damage requires magical intervention or extended bed rest to be recovered. Fatigue can be recovered by simply relaxing and getting a hot meal and a good night's sleep. Endurance damage may also increase susceptibility to infection, at the discretion of the referee.\n\nAnother \"DQ\" feature is a three-tiered combat range system: Ranged, Melee, and Close Combat. Ranged Combat typically involves bows, slings, and thrown knives, while Melee is swords, spears, maces and most other weapons. Close Combat in \"DQ\" is wrestling on the ground with knives, fists, rocks, etc. \"DQ\" allows a party of adventurers to be surrounded and ultimately overwhelmed by large numbers of peasants, who rather than attacking singly and being cut to ribbons, will instead seek to surround and leap into Close Combat to subdue and pin down Player Characters. Some weapons, such as daggers, can be used at all ranges, but most cannot and are useless when the character is being shot with bows or engaged in Close Combat.\n\nDragonQuest combat falls midway in complexity between \"D&D\" and systems such as \"Runequest\" or \"HârnMaster\". It can take several hours to resolve battles. Tactics, choice of weapon, and use of spells are keys to victory. The uniqueness to \"DQ\" is that novice characters and mighty adventurers have nearly the same ability to absorb damage—i.e. they can both be killed fairly easily (unlike \"D&D\" in which high-level characters can take remarkable amounts of damage without dying). This requires parties to have a balance of fighting and magic skills, since a party cannot be centered on a single nigh-invulnerable figure (a \"Conan the Barbarian\" type).\n\nAn experience points system is utilized that enables characters to increase their skill levels in spells and vocations, making their characters more formidable. Experience is used to 'purchase' new and improved abilities, rather than conferring a blanket increase in character skills as in \"D&D\".\n\nAs characters grow in skills and proficiency, the cost to raise to higher skill levels increases greatly, but the amount of base experience points awarded at the successful completion of an adventure increases as well. In addition, the Dungeon Master may award characters bonus experience points for valiant, clever or outstanding performance during gameplay.\n\n\"DragonQuest\" appeared in three editions: a first edition published in 1980, a second edition published by SPI in conjunction with Bantam Books in 1982, and a third edition published by TSR in 1989 after they acquired SPI. As of 2016, the game belongs to Wizards of the Coast, a wholly owned subsidiary of Hasbro, through acquisition of TSR.\n\nThe original working title for \"DragonQuest\" was \"Dragonslayer\" (Ares Magazine #2), but this had to be changed to avoid a conflict with Walt Disney Pictures' movie, \"Dragonslayer\". SPI later (in 1981) published a board game titled \"Dragonslayer\", a tie-in with the movie.\n\nThe \"DragonQuest\" trademark prevented the \"Dragon Quest\" video game series from being published in North America under that title; the video game was retitled \"Dragon Warrior\" when it debuted in that market in 1990, and subsequent sequels also used this name until 2003. In 2003, Square Enix registered the \"Dragon Quest\" trademark in the US. This trademark abandonment by Wizards of the Coast indicates that they have no interest in future \"DragonQuest\" publications.\n\nThe first SPI edition came in a cardboard box containing three separate, softcover books, \"Character Generation,\" \"Magic\" and a \"Monster/Skills\" manual. The second edition came as an all-in-one softcover version published by SPI. The third edition published by TSR came in yet another all-in-one softcover version with different cover graphics.\n\nThe differences between editions lies mainly in the addition and subtraction of some magical colleges as well as some modifications to combat mechanics.\n\nThere were supplemental materials released during the game's publication. There are several pre-written gaming adventures made for the DragonQuest system. A rolling screen for game masters was offered at one point, bearing helpful charts and tables necessary during gameplay. A large map of the fictional Kingdom of Alusia was released as well, which provided game masters a variety of habitats, environments and islands on which to place the action.\n\nDespite the game's relative obscurity and continued neglect by its copyright holders, the game continues to be played by DragonQuest devotees. Over the years, DQ groups throughout the world have devised and playtested additional secondary skills (such as swimming and hunting), magic colleges, races (rules for half-elves, half-orcs, etc.), and monsters for the game since it ceased being actively published. An unofficial player's association exists online.\n\nForrest Johnson reviewed \"DragonQuest\" in \"The Space Gamer\" No. 31. Johnson commented that \"Despite its faults, it still presents a pleasing contrast to the sloppiness of TFT, the illogic of D&D, the incoherence of C&S. It borrows good ideas liberally from the older systems, and offers noteworthy innovations of its own. [...] At [the price], \"DragonQuest\" is a terrific buy.\"\n\n\"DragonQuest\" was awarded the H.G. Wells Award for \"Best Roleplaying Rules of 1980\".\n\n"}
{"id": "1912367", "url": "https://en.wikipedia.org/wiki?curid=1912367", "title": "Electromagnetic tensor", "text": "Electromagnetic tensor\n\nIn electromagnetism, the electromagnetic tensor or electromagnetic field tensor (sometimes called the field strength tensor, Faraday tensor or Maxwell bivector) is a mathematical object that describes the electromagnetic field in spacetime. The field tensor was first used after the four-dimensional tensor formulation of special relativity was introduced by Hermann Minkowski. The tensor allows related physical laws to be written very concisely.\n\nThe electromagnetic tensor, conventionally labelled \"F\", is defined as the exterior derivative of the electromagnetic four-potential, \"A\", a differential 1-form:\n\nTherefore, \"F\" is a differential 2-form—that is, an antisymmetric rank-2 tensor field—on Minkowski space. In component form,\n\nwhere formula_3 is the four-gradient and formula_4 is the four-potential.\n\nSI units for Maxwell's equations and the particle physicist's sign convention for the signature of Minkowski space , will be used throughout this article.\n\nThe electric and magnetic fields can be obtained from the components of the electromagnetic tensor. The relationship is simplest in Cartesian coordinates:\n\nwhere \"c\" is the speed of light, and\nwhere formula_7 is the Levi-Civita symbol. Note that this gives the fields in a particular reference frame; if the reference frame is changed, the components of the electromagnetic tensor will transform covariantly, and the fields in the new frame will be given by the new components.\n\nIn contravariant matrix form,\n\nThe covariant form is given by index lowering,\n\nThe Faraday tensor's Hodge dual is \n\nFrom now on in this article, when the electric or magnetic fields are mentioned, a Cartesian coordinate system is assumed, and the electric and magnetic fields are with respect to the coordinate system's reference frame, as in the equations above.\n\nThe matrix form of the field tensor yields the following properties:\n\nThis tensor simplifies and reduces Maxwell's equations as four vector calculus equations into two tensor field equations. In electrostatics and electrodynamics, Gauss's law and Ampère's circuital law are respectively:\n\nand reduce to the inhomogeneous Maxwell equation:\n\nwhere\n\nis the four-current. In magnetostatics and magnetodynamics, Gauss's law for magnetism and Maxwell–Faraday equation are respectively:\n\nwhich reduce to Bianchi identity:\n\nor using the index notation with square brackets for the antisymmetric part of the tensor:\n\nThe field tensor derives its name from the fact that the electromagnetic field is found to obey the tensor transformation law, this general property of (non-gravitational) physical laws being recognised after the advent of special relativity. This theory stipulated that all the (non-gravitational) laws of physics should take the same form in all coordinate systems – this led to the introduction of tensors. The tensor formalism also leads to a mathematically simpler presentation of physical laws.\n\nThe inhomogeneous Maxwell equation leads to the continuity equation:\n\nimplying conservation of charge.\n\nMaxwell's laws above can be generalised to curved spacetime by simply replacing partial derivatives with covariant derivatives:\n\nwhere the semi-colon notation represents a covariant derivative, as opposed to a partial derivative. These equations are sometimes referred to as the curved space Maxwell equations. Again, the second equation implies charge conservation (in curved spacetime):\n\nClassical electromagnetism and Maxwell's equations can be derived from the action:\n\nwhere\n\nThis means the Lagrangian density is\n\nThe two middle terms in the parentheses are the same, as are the two outer terms, so the Lagrangian density is\n\nSubstituting this into the Euler–Lagrange equation of motion for a field:\n\nSo the Euler–Lagrange equation becomes:\n\nThe quantity in parentheses above is just the field tensor, so this finally simplifies to\n\nThat equation is another way of writing the two inhomogeneous Maxwell's equations (namely, Gauss's law and Ampère's circuital law) using the substitutions:\n\nwhere \"i, j, k\" take the values 1, 2, and 3.\n\nThe Lagrangian of quantum electrodynamics extends beyond the classical Lagrangian established in relativity to incorporate the creation and annihilation of photons (and electrons):\n\nwhere the first part in the right hand side, containing the Dirac spinor formula_30, represents the Dirac field. In quantum field theory it is used as the template for the gauge field strength tensor. By being employed in addition to the local interaction Lagrangian it reprises its usual role in QED.\n\n\n"}
{"id": "292420", "url": "https://en.wikipedia.org/wiki?curid=292420", "title": "Emission spectrum", "text": "Emission spectrum\n\nThe emission spectrum of a chemical element or chemical compound is the spectrum of frequencies of electromagnetic radiation emitted due to an atom or molecule making a transition from a high energy state to a lower energy state. The photon energy of the emitted photon is equal to the energy difference between the two states. There are many possible electron transitions for each atom, and each transition has a specific energy difference. This collection of different transitions, leading to different radiated wavelengths, make up an emission spectrum. Each element's emission spectrum is unique. Therefore, spectroscopy can be used to identify the elements in matter of unknown composition. Similarly, the emission spectra of molecules can be used in chemical analysis of substances.\n\nIn physics, emission is the process by which a higher energy quantum mechanical state of a particle becomes converted to a lower one through the emission of a photon, resulting in the production of light. The frequency of light emitted is a function of the energy of the transition. Since energy must be conserved, the energy difference between the two states equals the energy carried off by the photon. The energy states of the transitions can lead to emissions over a very large range of frequencies. For example, visible light is emitted by the coupling of electronic states in atoms and molecules (then the phenomenon is called fluorescence or phosphorescence). On the other hand, nuclear shell transitions can emit high energy gamma rays, while nuclear spin transitions emit low energy radio waves.\n\nThe emittance of an object quantifies how much light is emitted by it. This may be related to other properties of the object through the Stefan–Boltzmann law.\nFor most substances, the amount of emission varies with the temperature and the spectroscopic composition of the object, leading to the appearance of color temperature and emission lines. Precise measurements at many wavelengths allow the identification of a substance via emission spectroscopy.\n\nEmission of radiation is typically described using semi-classical quantum mechanics: the particle's energy levels and spacings are determined from quantum mechanics, and light is treated as an oscillating electric field that can drive a transition if it is in resonance with the system's natural frequency. The quantum mechanics problem is treated using time-dependent perturbation theory and leads to the general result known as Fermi's golden rule. The description has been superseded by quantum electrodynamics, although the semi-classical version continues to be more useful in most practical computations.\n\nWhen the electrons in the atom are excited, for example by being heated, the additional energy pushes the electrons to higher energy orbitals. When the electrons fall back down and leave the excited state, energy is re-emitted in the form of a photon. The wavelength (or equivalently, frequency) of the photon is determined by the difference in energy between the two states. These emitted photons form the element's spectrum.\n\nThe fact that only certain colors appear in an element's atomic emission spectrum means that only certain frequencies of light are emitted. Each of these frequencies are related to energy by the formula:\nwhere formula_2 is the energy of the photon, formula_3 is its frequency, and formula_4 is Planck's constant.\nThis concludes that only photons with specific energies are emitted by the atom. The principle of the atomic emission spectrum explains the varied colors in neon signs, as well as chemical flame test results (described below).\n\nThe frequencies of light that an atom can emit are dependent on states the electrons can be in. When excited, an electron moves to a higher energy level or orbital. When the electron falls back to its ground level the light is emitted.\nThe above picture shows the visible light emission spectrum for hydrogen. If only a single atom of hydrogen were present, then only a single wavelength would be observed at a given instant. Several of the possible emissions are observed because the sample contains many hydrogen atoms that are in different initial energy states and reach different final energy states. These different combinations lead to simultaneous emissions at different wavelengths.\n\nAs well as the electronic transitions discussed above, the energy of a molecule can also change via rotational, vibrational, and vibronic (combined vibrational and electronic) transitions. These energy transitions often lead to closely spaced groups of many different spectral lines, known as spectral bands. Unresolved band spectra may appear as a spectral continuum.\n\nLight consists of electromagnetic radiation of different wavelengths. Therefore, when the elements or their compounds are heated either on a flame or by an electric arc they emit energy in the form of light. Analysis of this light, with the help of a spectroscope gives us a discontinuous spectrum. A spectroscope or a spectrometer is an instrument which is used for separating the components of light, which have different wavelengths. The spectrum appears in a series of lines called the line spectrum. This line spectrum is called an atomic spectrum when it originates from an atom in elemental form. Each element has a different atomic spectrum. The production of line spectra by the atoms of an element indicate that an atom can radiate only a certain amount of energy. This leads to the conclusion that bound electrons cannot have just any amount of energy but only a certain amount of energy.\n\nThe emission spectrum can be used to determine the composition of a material, since it is different for each element of the periodic table. One example is astronomical spectroscopy: identifying the composition of stars by analysing the received light.\nThe emission spectrum characteristics of some elements are plainly visible to the naked eye when these elements are heated. For example, when platinum wire is dipped into a strontium nitrate solution and then inserted into a flame, the strontium atoms emit a red color. Similarly, when copper is inserted into a flame, the flame becomes green. These definite characteristics allow elements to be identified by their atomic emission spectrum. Not all emitted lights are perceptible to the naked eye, as the spectrum also includes ultraviolet rays and infrared lighting.\nAn emission is formed when an excited gas is viewed directly through a spectroscope.\nEmission spectroscopy is a spectroscopic technique which examines the wavelengths of photons emitted by atoms or molecules during their transition from an excited state to a lower energy state. Each element emits a characteristic set of discrete wavelengths according to its electronic structure, and by observing these wavelengths the elemental composition of the sample can be determined. Emission spectroscopy developed in the late 19th century and efforts in theoretical explanation of atomic emission spectra eventually led to quantum mechanics.\n\nThere are many ways in which atoms can be brought to an excited state. Interaction with electromagnetic radiation is used in fluorescence spectroscopy, protons or other heavier particles in Particle-Induced X-ray Emission and electrons or X-ray photons in Energy-dispersive X-ray spectroscopy or X-ray fluorescence. The simplest method is to heat the sample to a high temperature, after which the excitations are produced by collisions between the sample atoms. This method is used in flame emission spectroscopy, and it was also the method used by Anders Jonas Ångström when he discovered the phenomenon of discrete emission lines in the 1850s.\n\nAlthough the emission lines are caused by a transition between quantized energy states and may at first look very sharp, they do have a finite width, i.e. they are composed of more than one wavelength of light. This spectral line broadening has many different causes.\n\nEmission spectroscopy is often referred to as optical emission spectroscopy because of the light nature of what is being emitted.\n\nEmission lines from hot gases were first discovered by Ångström, and the technique was further developed by David Alter, Gustav Kirchhoff and Robert Bunsen.\n\nSee the history of spectroscopy for details.\n\nThe solution containing the relevant substance to be analysed is drawn into the burner and dispersed into the flame as a fine spray. The solvent evaporates first, leaving finely divided solid particles which move to the hottest region of the flame where gaseous atoms and ions are produced. Here electrons are excited as described above. It is common for a monochromator to be used to allow for easy detection.\n\nOn a simple level, flame emission spectroscopy can be observed using just a flame and samples of metal salts. This method of qualitative analysis is called a flame test. For example, sodium salts placed in the flame will glow yellow from sodium ions, while strontium (used in road flares) ions color it red. Copper wire will create a blue colored flame, however in the presence of chloride gives green (molecular contribution by CuCl). \n\nEmission coefficient is a coefficient in the power output per unit time of an electromagnetic source, a calculated value in physics. The emission coefficient of a gas varies with the wavelength of the light. It has units of mssr. It is also used as a measure of environmental emissions (by mass) per MWh of electricity generated, see: Emission factor.\n\nIn Thomson scattering a charged particle emits radiation under incident light. The particle may be an ordinary atomic electron, so emission coefficients have practical applications.\n\nIf X d\"V\" dΩ dλ is the energy scattered by a volume element d\"V\" into solid angle dΩ between wavelengths λ and λ+dλ per unit time then the Emission coefficient is X.\n\nThe values of X in Thomson scattering can be predicted from incident flux, the density of the charged particles and their Thomson differential cross section (area/solid angle).\n\nA warm body emitting photons has a monochromatic emission coefficient relating to its temperature and total power radiation. This is sometimes called the second Einstein coefficient, and can be deduced from quantum mechanical theory.\n\n\n"}
{"id": "795780", "url": "https://en.wikipedia.org/wiki?curid=795780", "title": "Ethos", "text": "Ethos\n\nEthos ( or ) is a Greek word meaning \"character\" that is used to describe the guiding beliefs or ideals that characterize a community, nation, or ideology. The Greeks also used this word to refer to the power of music to influence emotions, behaviours, and even morals. Early Greek stories of Orpheus exhibit this idea in a compelling way. The word's use in rhetoric is closely based on the Greek terminology used by Aristotle in his concept of the three artistic proofs or modes of persuasion.\n\n\"Ethos\" (, ; \"plurals:\" \"ethe\", ; \"ethea\", ) is a Greek word originally meaning \"accustomed place\" (as in \"the habitats of horses\", \"Iliad\" 6.511, 15.268), \"custom, habit\", equivalent to Latin \"mores\".\n\n\"Ethos\" forms the root of \"ethikos\" (), meaning \"moral, showing moral character\". As an adjective in the neuter plural form \"ta ethika\" (), used for the study of morals, it is the origin of the modern English word \"ethics\".\n\nIn modern usage, ethos denotes the disposition, character, or fundamental values particular to a specific person, people, corporation, culture, or movement. For example, the poet and critic T. S. Eliot wrote in 1940 that \"the general ethos of the people they have to govern determines the behaviour of politicians\". Similarly the historian Orlando Figes wrote in 1996 that in Soviet Russia of the 1920s \"the ethos of the Communist party dominated every aspect of public life\".\nEthos may change in response to new ideas or forces. For example, according to the Jewish historian Afrie Krampf, ideas of economic modernization which were imported into Palestine in the 1930s brought about \"the abandonment of the agrarian ethos and the reception of...the ethos of rapid development\".\n\nIn rhetoric, ethos is one of the three artistic proofs (\"pistis\", πίστις) or modes of persuasion (other principles being \"logos\" and \"pathos\") discussed by Aristotle in 'Rhetoric' as a component of argument. Speakers must establish ethos from the start. This can involve \"moral competence\" only; Aristotle however broadens the concept to include expertise and knowledge. Ethos is limited, in his view, by what the speaker says. Others however contend that a speaker's ethos extends to and is shaped by the overall moral character and history of the speaker—that is, what people think of his or her character before the speech has even begun (cf Isocrates).\n\nAccording to Aristotle, there are three categories of ethos:\n\n\nIn a sense, ethos does not belong to the speaker but to the audience. Thus, it is the audience that determines whether a speaker is a high- or a low-ethos speaker. Violations of ethos include:\n\n\nCompletely dismissing an argument based on any of the above violations of ethos is an informal fallacy (Appeal to motive). The argument may indeed be suspect; but is not, in itself, invalid.\n\nFor Aristotle, a speaker's ethos was a rhetorical strategy employed by an orator whose purpose was to \"inspire trust in his audience\" (\"Rhetorica\" 1380). \"Ethos\" was therefore achieved through the orator's \"good sense, good moral character, and goodwill\", and central to Aristotelian virtue ethics was the notion that this \"good moral character\" was increased in virtuous degree by habit (\"Rhetorica\" 1380). Aristotle links virtue, habituation, and \"ethos\" most succinctly in Book II of \"Nichomachean Ethics\": \"Virtue, then, being of two kinds, intellectual and moral, intellectual virtue in the main owes both its birth and its growth to teaching [...] while moral virtue comes about as a result of habit, whence also its name \"ethike\" is one that is formed by a slight variation from the word \"ethos\" (habit)\" (952). Discussing women and rhetoric, scholar Karlyn Kohrs Campbell notes that entering the public sphere was considered an act of moral transgression for females of the nineteenth century: \"Women who formed moral reform and abolitionist societies, and who made speeches, held conventions, and published newspapers, entered the public sphere and thereby lost their claims to purity and piety\" (13). Crafting an ethos within such restrictive moral codes, therefore, meant adhering to membership of what Nancy Fraser and Michael Warner have theorized as counterpublics. While Warner contends that members of counterpublics are afforded little opportunity to join the dominant public and therefore exert true agency, Nancy Fraser has problematized Habermas's conception of the public sphere as a dominant \"social totality\" by theorizing \"subaltern counterpublics\", which function as alternative publics that represent \"parallel discursive arenas where members of subordinated social groups invent and circulate counterdiscourses, which in turn permit them to formulate oppositional interpretations of their identities, interests, and needs\" (67).\n\nThough feminist rhetorical theorists have begun to offer more nuanced ways to conceive of ethos, they remain cognizant of how these classical associations have shaped and still do shape women's use of the rhetorical tool. Johanna Schmertz draws on Aristotelian ethos to reinterpret the term alongside feminist theories of subjectivity, writing that, \"Instead of following a tradition that, it seems to me, reads ethos somewhat in the manner of an Aristotelian quality proper to the speaker's identity, a quality capable of being deployed as needed to fit a rhetorical situation, I will ask how ethos may be dislodged from identity and read in such a way as to multiply the positions from which women may speak\" (83). Rhetorical scholar and professor Kate Ronald's claim that \"ethos is the appeal residing in the tension between the speaker's private and public self\", (39) also presents a more postmodern view of ethos that links credibility and identity. Similarly, Nedra Reynolds and Susan Jarratt echo this view of ethos as a fluid and dynamic set of identifications, arguing that \"these split selves are guises, but they are not distortions or lies in the philosopher's sense. Rather they are 'deceptions' in the sophistic sense: recognitions of the ways one is positioned multiply differently\" (56).\n\nRhetorical scholar Michael Halloran has argued that the classical understanding of ethos \"emphasizes the conventional rather than the idiosyncratic, the public rather than the private\" (60). Commenting further on the classical etymology and understanding of \"ethos\", Halloran illuminates the interdependence between \"ethos\" and cultural context by arguing that \"To have ethos is to manifest the virtues most valued by the culture to and for which one speaks\" (60). While scholars do not all agree on the dominant sphere in which ethos may be crafted, some agree that ethos is formed through the negotiation between private experience and the public, rhetorical act of self-expression. Karen Burke LeFevre's argument in \"Invention as Social Act\" situates this negotiation between the private and the public, writing that ethos \"appears in that socially created space, in the 'between', the point of intersection between speaker or writer and listener or reader\" (45-46).\n\nAccording to Nedra Reynolds, \"ethos, like postmodern subjectivity, shifts and changes over time, across texts, and around competing spaces\" (336). However, Reynolds additionally discusses how one might clarify the meaning of ethos within rhetoric as expressing inherently communal roots. This stands in direct opposition to what she describes as the claim \"that ethos can be faked or 'manipulated'\" because individuals would be formed by the values of their culture and not the other way around (336). Rhetorical scholar John Oddo also suggests that ethos is negotiated across a community, and not simply a manifestation of the self (47). In the era of mass-mediated communication, Oddo contends, one's ethos is often created by journalists and dispersed over multiple news texts. With this in mind, Oddo coins the term intertextual ethos, the notion that a public figure's \"ethos is constituted within and across a range of mass media voices\" (48).\n\nIn \"Black Women Writers and the Trouble with Ethos\", scholar Coretta Pittman notes that race has been generally absent from theories of ethos construction, and that this concept is troubling for black women. Pittman writes, \"Unfortunately, in the history of race relations in America, black Americans' ethos ranks low among other racial and ethnic groups in the United States. More often than not, their moral characters have been associated with a criminalized and sexualized ethos in visual and print culture\" (43).\n\nThe ways in which characters were constructed is important when considering ethos, or character, in Greek tragedy. Augustus Taber Murray explains that the depiction of a character was limited by the circumstances under which Greek tragedies were presented. These include the single unchanging scene, necessary use of the chorus, small number of characters limiting interaction, large outdoor theatres, and the use of masks, which all influenced characters to be more formal and simple. Murray also declares that the inherent characteristics of Greek tragedies are important in the makeup of the characters. One of these is the fact that tragedy characters were nearly always mythical characters. This limited the character, as well as the plot, to the already well-known myth from which the material of the play was taken. The other characteristic is the relatively short length of most Greek plays. This limited the scope of the play and characterization, so that the characters were defined by one overriding motivation toward a certain objective from the beginning of the play.\n\nHowever, Murray clarifies that strict constancy is not always the rule in Greek tragedy characters. To support this, he points out the example of Antigone who, even though she strongly defies Creon in the beginning of the play, begins to doubt her cause and plead for mercy as she is led to her execution.\n\nSeveral other aspects of the character element in ancient Greek tragedy are worth noting. One of these, which C. Garton discusses, is the fact that either because of contradictory action or incomplete description, the character cannot be viewed as an individual, or the reader is left confused about the character. One method of reconciling this would be to consider these characters to be flat, or type-caste, instead of round. This would mean that most of the information about the character centers around one main quality or viewpoint. Comparable to the flat character option, the reader could also view the character as a symbol. Examples of this might be the Eumenides as vengeance, or Clytemnestra as symbolizing ancestral curse. Yet another means of looking at character, according to Tycho von Wilamowitz and Howald, is the idea that characterisation is not important. This idea is maintained by the theory that the play is meant to affect the viewer or reader scene by scene, with attention being only focused on the section at hand. This point of view also holds that the different figures in a play are only characterised by the situation surrounding them, and only enough so that their actions can be understood.\n\nGaret makes three more observations about character in Greek tragedy. The first is an abundant variety of types of characters in Greek tragedy. His second observation is that the reader or viewer's need for characters to display a unified identity that is similar to human nature is usually fulfilled. Thirdly, characters in tragedies include incongruities and idiosyncrasies.\n\nAnother aspect stated by Garet is that tragedy plays are composed of language, character, and action, and the interactions of these three components; these are fused together throughout the play. He explains that action normally determines the major means of characterisation. Another principle he states is the importance of these three components' effect on each other; the important repercussion of this being character's impact on action.\n\nAugustus Taber Murray also examines the importance and degree of interaction between plot and character. He does this by discussing Aristotle's statements about plot and character in his Poetics: that plot can exist without character, but character cannot exist without plot, and so character is secondary to plot. Murray maintains that Aristotle did not mean that complicated plot should hold the highest place in a tragedy play. This is because the plot was, more often than not, simple and therefore not a major point of tragic interest. Murray conjectures that people today do not accept Aristotle's statement about character and plot because to modern people, the most memorable things about tragedy plays are often the characters. However, Murray does concede that Aristotle is correct in that \"[t]here can be no portrayal of character [...] without at least a skeleton outline of plot\".\n\nEthos, or character, also appears in the visual art of famous or mythological ancient Greek events in murals, on pottery, and sculpture, referred to generally as pictorial narrative. Aristotle even praised the ancient Greek painter Polygnotos because his paintings included characterization. The way in which the subject and his actions are portrayed in visual art can convey the subject's ethical character and through this the work's overall theme, just as effectively as poetry or drama can. This characterisation portrayed men as they ought to be, which is the same as Aristotle's idea of what ethos or character should be in tragedy. (Stansbury-O'Donnell, p. 178) Professor Mark D. Stansbury-O'Donnell states that pictorial narratives often had ethos as its focus, and was therefore concerned with showing the character's moral choices. (Stansbury-O'Donnell, p. 175) David Castriota, agreeing with Stansbury-O'Donnell's statement, says that the main way Aristotle considered poetry and visual arts to be on equal levels was in character representation and its effect on action. However, Castriota also maintains about Aristotle's opinion that \"his interest has to do with the influence that such ethical representation may exert upon the public\". Castriota also explains that according to Aristotle, \"[t]he activity of these artists is to be judged worthy and useful above all because exposure of their work is beneficial to the polis\". Accordingly, this was the reason for the representation of character, or ethos, in public paintings and sculptures. In order to portray the character's choice, the pictorial narrative often shows an earlier scene than when the action was committed. Stansbury-O'Donnell gives an example of this in the form of a picture by the ancient Greek artist Exekia which shows the Greek hero Ajax planting his sword in the ground in preparation to commit suicide, instead of the actual suicide scene. (Stansbury-O'Donnell, p. 177.) Additionally, Castriota explains that ancient Greek art expresses the idea that character was the major factor influencing the outcome of the Greeks' conflicts against their enemies. Because of this, \"ethos was the essential variable in the equation or analogy between myth and actuality\".\n\n\n\n"}
{"id": "595273", "url": "https://en.wikipedia.org/wiki?curid=595273", "title": "Fact-checking", "text": "Fact-checking\n\nFact-checking is the act of checking factual assertions in non-fictional text in order to determine the veracity and correctness of the factual statements in the text. This may be done either before (\"ante hoc\") or after (\"post hoc\") the text has been published or otherwise disseminated.\n\nAnte hoc fact-checking (fact-checking before dissemination) aims to remove errors and allow text to proceed to dissemination (or to rejection if it fails confirmations or other criteria). Post hoc fact-checking is most often followed by a written report of inaccuracies, sometimes with a visual metric from the checking organization (e.g., Pinocchios from \"The Washington Post\" \"Fact Checker,\" or \"TRUTH-O-METER\" ratings from PolitiFact). Several organizations are devoted to \"post hoc\" fact-checking, such as FactCheck.org and PolitiFact.\n\nResearch on the impact of fact-checking is relatively recent but the existing research suggests that fact-checking does indeed correct misperceptions among citizens, as well as discourage politicians from spreading misinformation.\n\nOne study finds that fact-checkers PolitiFact, FactCheck.org, and \"Washington Post's\" Fact Checker overwhelmingly agree on their evaluations of claims.\n\nHowever, a study by Morgan Marietta, David C. Barker and Todd Bowser found \"substantial differences in the questions asked and the answers offered.\" They concluded that this limited the \"usefulness of fact-checking for citizens trying to decide which version of disputed realities to believe.\"\n\nA paper by Chloe Lim, Ph.D. student at Stanford University, finds little overlap in the statements that fact-checkers check. Out of 1065 fact-checks by PolitiFact and 240 fact-checks by \"The Washington Post\"s Fact-Checker, there were only 70 statements that both fact-checkers checked. The study found that the fact-checkers gave consistent ratings for 56 out of 70 statements, which means that one out every five times, the two fact-checkers disagree on the accuracy of statements.\n\nStudies of \"post hoc\" fact-checking have made clear that such efforts often result in changes in the behavior, in general, of both the speaker (making them more careful in their pronouncements) and of the listener or reader (making them more discerning with regard to the factual accuracy of content); observations include the propensities of audiences to be completely unswayed by corrections to errors regarding the most divisive subjects, or the tendency to be more greatly persuaded by corrections of negative reporting (e.g., \"attack ads\"), and to see minds changed only when the individual in error was someone reasonably like-minded to begin with.\n\nA 2015 study found evidence a \"backfire effect\" (correcting false information may make partisan individuals cling more strongly to their views): \"Corrective information adapted from the Centers for Disease Control and Prevention (CDC) website significantly reduced belief in the myth that the flu vaccine can give you the flu as well as concerns about its safety. However, the correction also significantly reduced intent to vaccinate among respondents with high levels of concern about vaccine side effects--a response that was not observed among those with low levels of concern.\" A 2017 study attempted to replicate the findings of the 2015 study but failed to do so.\n\nA 2016 study found little evidence for the \"backfire effect\": \"By and large, citizens heed factual information, even when such information challenges their partisan and ideological commitments.\" A study of Donald Trump supporters during the 2016 race similarly found little evidence for the backfire effect: \"When respondents read a news article about Mr. Trump's speech that included F.B.I. statistics indicating that crime had \"fallen dramatically and consistently over time,\" their misperceptions about crime declined compared with those who saw a version of the article that omitted corrective information (though misperceptions persisted among a sizable minority).\" A 2018 study found no evidence of a backfire effect.\n\nStudies have shown that fact-checking can affect citizens' belief in the accuracy of claims made in political advertisement. A paper by a group of Paris School of Economics and Sciences Po economists found that falsehoods by Marine Le Pen during the 2017 French presidential election campaign (i) successfully persuaded voters, (ii) lost their persuasiveness when fact-checked, and (iii) did not reduce voters' political support for Le Pen when her claims were fact-checked. A 2017 study in the \"Journal of Politics\" found that \"individuals consistently update political beliefs in the appropriate direction, even on facts that have clear implications for political party reputations, though they do so cautiously and with some bias... Interestingly, those who identify with one of the political parties are no more biased or cautious than pure independents in their learning, conditional on initial beliefs.\"\n\nA study by Yale University cognitive scientists Gordon Pennycook and David G. Rand found that Facebook tags of fake articles \"did significantly reduce their perceived accuracy relative to a control without tags, but only modestly\". A Dartmouth study led by Brendan Nyhan found that Facebook tags had a greater impact than the Yale study found. A \"disputed\" tag on a false headline reduced the number of respondents who considered the headline accurate from 29% to 19%, whereas a \"rated false\" tag pushed the number down to 16%. The Yale study found evidence of a backfire effect among Trump supporters younger than 26 years whereby the presence of both untagged and tagged fake articles made the untagged fake articles appear more accurate. In response to research which questioned the effectiveness of the Facebook \"disputed\" tags, Facebook decided to drop the tags in December 2017 and would instead put articles which fact-checked a fake news story next to the fake news story link whenever it is shared on Facebook.\n\nBased on the findings of a 2017 study in the journal \"Psychological Science,\" the most effective ways to reduce misinformation through corrections is by:\nA forthcoming study in the \"Journal of Experimental Political Science\" found \"strong evidence that citizens are willing to accept corrections to fake news, regardless of their ideology and the content of the fake stories.\"\n\nA paper by Andrew Guess (of Princeton University), Brendan Nyhan (Dartmouth College) and Jason Reifler (University of Exeter) found that consumers of fake news tended to have less favorable views of fact-checking, in particular Trump supporters. The paper found that fake news consumers rarely encountered fact-checks: \"only about half of the Americans who visited a fake news website during the study period also saw any fact-check from one of the dedicated fact-checking website (14.0%).\"\n\nA 2018 study found that Republicans were more likely to correct their false information on voter fraud if the correction came from Breitbart News rather than a non-partisan neutral source such as PolitiFact.\n\nA 2015 experimental study found that fact-checking can encourage politicians to not spread misinformation. The study found that it might help improve political discourse by increasing the reputational costs or risks of spreading misinformation for political elites. The researchers sent, \"a series of letters about the risks to their reputation and electoral security if they were caught making questionable statements. The legislators who were sent these letters were substantially less likely to receive a negative fact-checking rating or to have their accuracy questioned publicly, suggesting that fact-checking can reduce inaccuracy when it poses a salient threat.\"\n\nOne experimental study found that fact-checking during debates affected viewers' assessment of the candidates' debate performance and \"greater willingness to vote for a candidate when the fact-check indicates that the candidate is being honest.\"\n\nA study of Trump supporters during the 2016 presidential campaign found that while fact-checks of false claims made by Trump reduced his supporters' belief in the false claims in question, the corrections did not alter their attitudes towards Trump.\n\nPolitical fact-checking is sometimes criticized as being opinion journalism. In September 2016, a Rasmussen Reports national telephone and online survey found that \"just 29% of all Likely U.S. Voters trust media fact-checking of candidates' comments. Sixty-two percent (62%) believe instead that news organizations skew the facts to help candidates they support.\"\n\nThe Reporters' Lab at Duke University maintains a database of fact-checking organizations that is managed by Mark Stencel and Bill Adair. The database tracks more than 100 non-partisan organizations around the world. The Lab's inclusion criteria is based on whether the organization\n\n\n\n\n\n\n\n\nAmong the benefits of printing only checked copy is that it averts serious, sometimes costly, problems, e.g. lawsuits and discreditation. Fact-checkers are primarily useful in catching accidental mistakes; they are not guaranteed safeguards against those who wish to commit journalistic frauds\n\nThe possible societal benefit of honing the fundamental skill of fact-checking has been noted in a round table discussion by Moshe Benovitz, who observes that \"modern students use their wireless worlds to augment skepticism and to reject dogma,\" but goes on to argue that this has positive implications for values development. He argues:\nHe closes, noting that this constitutes \"new opportunities for students to contribute to the discussion like never before, inserting technology positively into academic settings\" (rather than it being seen as purely as agent of distraction).\n\nOne journalistic controversy is that of admitted and disgraced reporter and plagiarist Stephen Glass, who began his journalism career as a fact-checker. The fact-checkers at \"The New Republic\" and other weeklies for which he worked never flagged the numerous fictions in Glass's reporting. Michael Kelly, who edited some of Glass's concocted stories, blamed himself, rather than the fact-checkers, saying: \"Any fact-checking system is built on trust ... If a reporter is willing to fake notes, it defeats the system. Anyway, the real vetting system is not fact-checking but the editor.\" \n\n\nThe following is a list of individuals for whom it has been reported, reliably, that they have played such a fact-checking role at some point in their careers, often as a stepping point to other journalistic endeavors, or to an independent writing career:\n\n\n"}
{"id": "47680368", "url": "https://en.wikipedia.org/wiki?curid=47680368", "title": "Faultless disagreement", "text": "Faultless disagreement\n\nA faultless disagreement is a disagreement when Party A states that \"P\" is true, while Party B states that \"non-P\" is true, and both parties are not at fault. Disagreements of this kind may arise in areas of evaluative discourse, such as aesthetics, justification of beliefs or moral values, etc. A representative example is John says Mary prettier that Anne, while Bob claims vice versa. Furthermore, in the case of a faultless disagreement, it is possible that if any party gives up their claim, there will be no improvement in the position of any of them.\n\nWithin the framework of formal logic it is impossible that both \"P\" and \"not-P\" are true, and it was attempted to justify faultless disagreements within the framework of relativism of the Truth, Max Kölbel and Sven Rosenkranz present arguments to the point that genuine faultless disagreements are impossible\n"}
{"id": "20757836", "url": "https://en.wikipedia.org/wiki?curid=20757836", "title": "Groupthink", "text": "Groupthink\n\nGroupthink is a psychological phenomenon that occurs within a group of people in which the desire for harmony or conformity in the group results in an irrational or dysfunctional decision-making outcome. Group members try to minimize conflict and reach a consensus decision without critical evaluation of alternative viewpoints by actively suppressing dissenting viewpoints, and by isolating themselves from outside influences.\n\nGroupthink requires individuals to avoid raising controversial issues or alternative solutions, and there is loss of individual creativity, uniqueness and independent thinking. The dysfunctional group dynamics of the \"ingroup\" produces an \"illusion of invulnerability\" (an inflated certainty that the right decision has been made). Thus the \"ingroup\" significantly overrates its own abilities in decision-making and significantly underrates the abilities of its opponents (the \"outgroup\"). Furthermore, groupthink can produce dehumanizing actions against the \"outgroup\".\n\nAntecedent factors such as group cohesiveness, faulty group structure, and situational context (e.g., community panic) play into the likelihood of whether or not groupthink will impact the decision-making process.\n\nGroupthink is a construct of social psychology but has an extensive reach and influences literature in the fields of communication studies, political science, management, and organizational theory, as well as important aspects of deviant religious cult behaviour.\n\nGroupthink is sometimes stated to occur (more broadly) within natural groups within the community, for example to explain the lifelong different mindsets of those with differing political views (such as \"conservatism\" and \"liberalism\" in the U.S. political context ) or the purported benefits of team work vs. work conducted in solitude. However, this conformity of viewpoints within a group does not mainly involve deliberate group decision-making, and might be better explained by the collective confirmation bias of the individual members of the group.\n\nMost of the initial research on groupthink was conducted by Irving Janis, a research psychologist from Yale University. Janis published an influential book in 1972, which was revised in 1982. Janis used the Bay of Pigs disaster (the failed invasion of Castro's Cuba in 1961) and the Japanese attack on Pearl Harbor in 1941 as his two prime case studies. Later studies have evaluated and reformulated his groupthink model.\n\nWilliam H. Whyte Jr. derived the term from George Orwell's \"Nineteen Eighty-Four\", and popularized it in 1952 in \"Fortune\" magazine:\nIrving Janis pioneered the initial research on the groupthink theory. He does not cite Whyte, but coined the term by analogy with \"doublethink\" and similar terms that were part of the newspeak vocabulary in the novel \"Nineteen Eighty-Four\" by George Orwell. He initially defined groupthink as follows:\n\nHe went on to write:\n\nJanis set the foundation for the study of groupthink starting with his research in the American Soldier Project where he studied the effect of extreme stress on group cohesiveness. After this study he remained interested in the ways in which people make decisions under external threats. This interest led Janis to study a number of \"disasters\" in American foreign policy, such as failure to anticipate the Japanese attack on Pearl Harbor (1941); the Bay of Pigs Invasion fiasco (1961); and the prosecution of the Vietnam War (1964–67) by President Lyndon Johnson. He concluded that in each of these cases, the decisions occurred largely because of groupthink, which prevented contradictory views from being expressed and subsequently evaluated.\n\nAfter the publication of Janis' book \"Victims of Groupthink\" in 1972, and a revised edition with the title \"Groupthink: Psychological Studies of Policy Decisions and Fiascoes\" in 1982, the concept of groupthink was used to explain many other faulty decisions in history. These events included Nazi Germany's decision to invade the Soviet Union in 1941, the Watergate scandal and others. Despite the popularity of the concept of groupthink, fewer than two dozen studies addressed the phenomenon itself following the publication of \"Victims of Groupthink\", between the years 1972 and 1998. This is surprising considering how many fields of interests it spans, which include political science, communications, organizational studies, social psychology, management, strategy, counseling, and marketing. One can most likely explain this lack of follow-up in that group research is difficult to conduct, groupthink has many independent and dependent variables, and it is unclear \"how to translate [groupthink's] theoretical concepts into observable and quantitative constructs.\"\n\nNevertheless, outside research psychology and sociology, wider culture has come to detect groupthink (somewhat fuzzily defined) in observable situations, for example:\n\n\n\n\nTo make groupthink testable, Irving Janis devised eight symptoms indicative of groupthink.\n\nType I: Overestimations of the group — its power and morality\n\nType II: Closed-mindedness\n\nType III: Pressures toward uniformity\n\nJanis prescribed three antecedent conditions to groupthink.\n\n1. High group cohesiveness\n\nJanis emphasized that cohesiveness is the main factor that leads to groupthink. Groups that lack cohesiveness can of course make bad decisions, but they do not experience groupthink. In a cohesive group, members avoid speaking out against decisions, avoid arguing with others, and work towards maintaining friendly relationships in the group. If cohesiveness gets to such a high level where there are no longer disagreements between members, then the group is ripe for groupthink. \n\n2. Structural faults\n\nCohesion is necessary for groupthink, but it becomes even more likely when the group is organized in ways that disrupt the communication of information, and when the group engages in carelessness while making decisions. \n\n3. Situational context:\n\n\nAlthough it is possible for a situation to contain all three of these factors, all three are not always present even when groupthink is occurring. Janis considered a high degree of cohesiveness to be the most important antecedent to producing groupthink and always present when groupthink was occurring; however, he believed high cohesiveness would not always produce groupthink. A very cohesive group abides to all group norms; whether or not groupthink arises is dependent on what the group norms are. If the group encourages individual dissent and alternative strategies to problem solving, it is likely that groupthink will be avoided even in a highly cohesive group. This means that high cohesion will lead to groupthink only if one or both of the other antecedents is present, situational context being slightly more likely than structural faults to produce groupthink.\n\nAs observed by Aldag & Fuller (1993), the groupthink phenomenon seems to rest on a set of unstated and generally restrictive assumptions:\n\nIt has been thought that groups with the strong ability to work together will be able to solve dilemmas in a quicker and more efficient fashion than an individual. Groups have a greater amount of resources which lead them to be able to store and retrieve information more readily and come up with more alternative solutions to a problem. There was a recognized downside to group problem solving in that it takes groups more time to come to a decision and requires that people make compromises with each other. However, it was not until the research of Janis appeared that anyone really considered that a highly cohesive group could impair the group's ability to generate quality decisions. Tight-knit groups may appear to make decisions better because they can come to a consensus quickly and at a low energy cost; however, over time this process of decision-making may decrease the members' ability to think critically. It is, therefore, considered by many to be important to combat the effects of groupthink.\n\nAccording to Janis, decision-making groups are not necessarily destined to groupthink. He devised ways of preventing groupthink:\n\n\nBy following these guidelines, groupthink can be avoided. After the Bay of Pigs invasion fiasco, President John F. Kennedy sought to avoid groupthink during the Cuban Missile Crisis using \"vigilant appraisal.\" During meetings, he invited outside experts to share their viewpoints, and allowed group members to question them carefully. He also encouraged group members to discuss possible solutions with trusted members within their separate departments, and he even divided the group up into various sub-groups, to partially break the group cohesion. Kennedy was deliberately absent from the meetings, so as to avoid pressing his own opinion.\n\nTesting groupthink in a laboratory is difficult because synthetic settings remove groups from real social situations, which ultimately changes the variables conducive or inhibitive to groupthink. Because of its subjective nature, researchers have struggled to measure groupthink as a complete phenomenon, instead frequently opting to measure its particular factors. These factors range from causal to effectual and focus on group and situational aspects.\n\nPark (1990) found that \"only 16 empirical studies have been published on groupthink,\" and concluded that they \"resulted in only partial support of his [Janis's] hypotheses.\" Park concludes, \"despite Janis' claim that group cohesiveness is the major necessary antecedent factor, no research has shown a significant main effect of cohesiveness on groupthink.\" Park also concludes that research on the interaction between group cohesiveness and leadership style does not support Janis' claim that cohesion and leadership style interact to produce groupthink symptoms. Park presents a summary of the results of the studies analyzed. According to Park, a study by Huseman and Drive (1979) indicates groupthink occurs in both small and large decision-making groups within businesses. This results partly from group isolation within the business. Manz and Sims (1982) conducted a study showing that autonomous work groups are susceptible to groupthink symptoms in the same manner as decisions making groups within businesses. Fodor and Smith (1982) produced a study revealing that group leaders with high power motivation create atmospheres more susceptible to groupthink. Leaders with high power motivation possess characteristics similar to leaders with a \"closed\" leadership style—an unwillingness to respect dissenting opinion. The same study indicates that level of group cohesiveness is insignificant in predicting groupthink occurrence. Park summarizes a study performed by Callaway, Marriott, and Esser (1985) in which groups with highly dominant members \"made higher quality decisions, exhibited lowered state of anxiety, took more time to reach a decision, and made more statements of disagreement/agreement.\" Overall, groups with highly dominant members expressed characteristics inhibitory to groupthink. If highly dominant members are considered equivalent to leaders with high power motivation, the results of Callaway, Marriott, and Esser contradict the results of Fodor and Smith. A study by Leana (1985) indicates the interaction between level of group cohesion and leadership style is completely insignificant in predicting groupthink. This finding refutes Janis' claim that the factors of cohesion and leadership style interact to produce groupthink. Park summarizes a study by McCauley (1989) in which structural conditions of the group were found to predict groupthink while situational conditions did not. The structural conditions included group insulation, group homogeneity, and promotional leadership. The situational conditions included group cohesion. These findings refute Janis' claim about group cohesiveness predicting groupthink.\n\nOverall, studies on groupthink have largely focused on the factors (antecedents) that predict groupthink. Groupthink occurrence is often measured by number of ideas/solutions generated within a group, but there is no uniform, concrete standard by which researchers can objectively conclude groupthink occurs. The studies of groupthink and groupthink antecedents reveal a mixed body of results. Some studies indicate group cohesion and leadership style to be powerfully predictive of groupthink, while other studies indicate the insignificance of these factors. Group homogeneity and group insulation are generally supported as factors predictive of groupthink.\n\nGroupthink can have a strong hold on political decisions and military operations, which may result in enormous wastage of human and material resources. Highly qualified and experienced politicians and military commanders sometimes make very poor decisions when in a suboptimal group setting. Scholars such as Janis and Raven attribute political and military fiascoes, such as the Bay of Pigs Invasion, the Vietnam War, and the Watergate scandal, to the effect of groupthink. More recently, Dina Badie argued that groupthink was largely responsible for the shift in the U.S. administration's view on Saddam Hussein that eventually led to the 2003 invasion of Iraq by the United States. After the September 11 attacks, \"stress, promotional leadership, and intergroup conflict\" were all factors that gave rise to the occurrence of groupthink. Political case studies of groupthink serve to illustrate the impact that the occurrence of groupthink can have in today's political scene.\n\nThe United States Bay of Pigs Invasion of April 1961 was the primary case study that Janis used to formulate his theory of groupthink. The invasion plan was initiated by the Eisenhower administration, but when the Kennedy administration took over, it \"uncritically accepted\" the plan of the Central Intelligence Agency (CIA). When some people, such as Arthur M. Schlesinger Jr. and Senator J. William Fulbright, attempted to present their objections to the plan, the Kennedy team as a whole ignored these objections and kept believing in the morality of their plan. Eventually Schlesinger minimized his own doubts, performing self-censorship. The Kennedy team stereotyped Fidel Castro and the Cubans by failing to question the CIA about its many false assumptions, including the ineffectiveness of Castro's air force, the weakness of Castro's army, and the inability of Castro to quell internal uprisings.\n\nJanis argued the fiasco that ensued could have been prevented if the Kennedy administration had followed the methods to preventing groupthink adopted during the Cuban Missile Crisis, which took place just one year later in October 1962. In the latter crisis, essentially the same political leaders were involved in decision-making, but this time they learned from their previous mistake of seriously under-rating their opponents.\n\nThe attack on Pearl Harbor on December 7, 1941, is a prime example of groupthink. A number of factors such as shared illusions and rationalizations contributed to the lack of precaution taken by U.S. Navy officers based in Hawaii. The United States had intercepted Japanese messages and they discovered that Japan was arming itself for an offensive attack \"somewhere\" in the Pacific Ocean. Washington took action by warning officers stationed at Pearl Harbor, but their warning was not taken seriously. They assumed that the Empire of Japan was taking measures in the event that their embassies and consulates in enemy territories were usurped.\n\nThe U.S. Navy and Army in Pearl Harbor also shared rationalizations about why an attack was unlikely. Some of them included:\n\n\nIn the weeks and months preceding the United States presidential election, 2016, there was near-unanimity among news media outlets and polling organizations that Hillary Clinton's election was extremely likely. For example, on November 7, the day before the election, \"The New York Times\" opined that Clinton then had \"a consistent and clear advantage in states worth at least 270 electoral votes.\" The Times estimated the probability of a Clinton win at 84%. Also on November 7, Reuters estimated the probability of Clinton defeating Donald Trump in the election at 90%,<ref name=\"Reuters/Ipsos\"></ref> and \"The Huffington Post\" put Clinton's odds of winning at 98.2% based on \"9.8 million simulations.\"\n\nThe disconnect between the election results and the pre-election estimates, both from news media outlets and from pollsters, may have been due to two factors: failure of imagination, in that few news and polling professionals could accept the idea of such an unconventional candidate as Trump becoming president; and polling error, in that a significant number of Trump supporters contacted by pollsters may have lied to or misled the pollsters out of fear of social ostracism, or those that were willing to express support for Trump were under-sampled by surveys.\n\nIn the corporate world, ineffective and suboptimal group decision-making can negatively affect the health of a company and cause a considerable amount of monetary loss.\n\nAaron Hermann and Hussain Rammal illustrate the detrimental role of groupthink in the collapse of Swissair, a Swiss airline company that was thought to be so financially stable that it earned the title the \"Flying Bank.\" The authors argue that, among other factors, Swissair carried two symptoms of groupthink: the belief that the group is invulnerable and the belief in the morality of the group. In addition, before the fiasco, the size of the company board was reduced, subsequently eliminating industrial expertise. This may have further increased the likelihood of groupthink. With the board members lacking expertise in the field and having somewhat similar background, norms, and values, the pressure to conform may have become more prominent. This phenomenon is called group homogeneity, which is an antecedent to groupthink. Together, these conditions may have contributed to the poor decision-making process that eventually led to Swissair's collapse.\n\nAnother example of groupthink from the corporate world is illustrated in the United Kingdom-based companies Marks & Spencer and British Airways. The negative impact of groupthink took place during the 1990s as both companies released globalization expansion strategies. Researcher Jack Eaton's content analysis of media press releases revealed that all eight symptoms of groupthink were present during this period. The most predominant symptom of groupthink was the illusion of invulnerability as both companies underestimated potential failure due to years of profitability and success during challenging markets. Up until the consequence of groupthink erupted they were considered blue chips and darlings of the London Stock Exchange. During 1998–1999 the price of Marks & Spencer shares fell from 590 to less than 300 and that of British Airways from 740 to 300. Both companies had already featured prominently in the UK press and media for more positive reasons to do with national pride in their undoubted sector-wide performance.\n\nRecent literature of groupthink attempts to study the application of this concept beyond the framework of business and politics. One particularly relevant and popular arena in which groupthink is rarely studied is sports. The lack of literature in this area prompted Charles Koerber and Christopher Neck to begin a case-study investigation that examined the effect of groupthink on the decision of the Major League Umpires Association (MLUA) to stage a mass resignation in 1999. The decision was a failed attempt to gain a stronger negotiating stance against Major League Baseball. Koerber and Neck suggest that three groupthink symptoms can be found in the decision-making process of the MLUA. First, the umpires overestimated the power that they had over the baseball league and the strength of their group's resolve. The union also exhibited some degree of closed-mindedness with the notion that MLB is the enemy. Lastly, there was the presence of self-censorship; some umpires who disagreed with the decision to resign failed to voice their dissent. These factors, along with other decision-making defects, led to a decision that was suboptimal and ineffective.\n\nResearcher Robert Baron (2005) contends that the connection between certain antecedents which Janis believed necessary has not been demonstrated by the current collective body of research on groupthink. He believes that Janis' antecedents for groupthink are incorrect, and argues that not only are they \"not necessary to provoke the symptoms of groupthink, but that they often will not even amplify such symptoms\". As an alternative to Janis' model, Baron proposed a ubiquity model of groupthink. This model provides a revised set of antecedents for groupthink, including social identification, salient norms, and low self-efficacy.\n\nAldag and Fuller (1993) argue that the groupthink concept was based on a \"small and relatively restricted sample\" that became too broadly generalized. Furthermore, the concept is too rigidly staged and deterministic. Empirical support for it has also not been consistent. The authors compare groupthink model to findings presented by Maslow and Piaget; they argue that, in each case, the model incites great interest and further research that, subsequently, invalidate the original concept. Aldag and Fuller thus suggest a new model called the general group problem-solving (GGPS) model, which integrates new findings from groupthink literature and alters aspects of groupthink itself. The primary difference between the GGPS model and groupthink is that the former is more value neutral and more political.\n\nOther scholars attempt to assess the merit of groupthink by reexamining case studies that Janis had originally used to buttress his model. Roderick Kramer (1998) believed that, because scholars today have a more sophisticated set of ideas about the general decision-making process and because new and relevant information about the fiascos have surfaced over the years, a reexamination of the case studies is appropriate and necessary. He argues that new evidence does not support Janis' view that groupthink was largely responsible for President Kennedy's and President Johnson's decisions in the Bay of Pigs Invasion and U.S. escalated military involvement in the Vietnam War, respectively. Both presidents sought the advice of experts outside of their political groups more than Janis suggested. Kramer also argues that the presidents were the final decision-makers of the fiascos; while determining which course of action to take, they relied more heavily on their own construals of the situations than on any group-consenting decision presented to them. Kramer concludes that Janis' explanation of the two military issues is flawed and that groupthink has much less influence on group decision-making than is popularly believed to be.\n\nWhyte (1998) suggests that collective efficacy plays a large role in groupthink because it causes groups to become less vigilant and to favor risks, two particular factors that characterize groups affected by groupthink. McCauley recasts aspects of groupthink's preconditions by arguing that the level of attractiveness of group members is the most prominent factor in causing poor decision-making. The results of Turner's and Pratkanis' (1991) study on social identity maintenance perspective and groupthink conclude that groupthink can be viewed as a \"collective effort directed at warding off potentially negative views of the group.\" Together, the contributions of these scholars have brought about new understandings of groupthink that help reformulate Janis' original model.\n\nAccording to a new theory many of the basic characteristics of groupthink – e.g., strong cohesion, indulgent atmosphere, and exclusive ethos – are the result of a special kind of mnemonic encoding (Tsoukalas, 2007). Members of tightly knit groups have a tendency to represent significant aspects of their community as episodic memories and this has a predictable influence on their group behavior and collective ideology.\n\n\n\n"}
{"id": "5566534", "url": "https://en.wikipedia.org/wiki?curid=5566534", "title": "Johnny Bright incident", "text": "Johnny Bright incident\n\nThe Johnny Bright incident was a violent on-field assault against African-American player Johnny Bright by a white opposing player during an American college football game held on October 20, 1951 in Stillwater, Oklahoma. The game was significant in itself as it marked the first time that an African-American athlete with a national profile and of critical importance to the success of his team, the Drake Bulldogs, had played against Oklahoma A&M College (now Oklahoma State University) at Oklahoma A&M's Lewis Field. Bright's injury also highlighted the racial tensions of the times and assumed notoriety when it was captured in what was later to become both a widely disseminated and eventually Pulitzer Prize-winning photo sequence.\n\nJohnny Bright's participation as a halfback/quarterback in the collegiate football game between the Drake Bulldogs and Oklahoma A&M Aggies on October 20, 1951 at Lewis Field was controversial even before it began. Bright had been the first African American football player to play at Lewis Field two years prior (without incident). In 1951, Bright was a pre-season Heisman Trophy candidate and led the nation in total offense. Bright had never played for a losing team in his college career. Coming into the contest, Drake carried a five-game winning streak, owing much to Bright's rushing and passing abilities.\n\nIt was an open secret that Oklahoma A&M players were targeting Bright. Both Oklahoma A&M's student newspaper, \"The Daily O'Collegian\", and the local newspaper, \"The News Press\", reported that Bright was a marked man, and several A&M students were openly claiming that Bright \"would not be around at the end of the game.\" Although Oklahoma A&M had integrated in 1949, the Jim Crow spirit was still very much alive on campus.\n\nDuring the first seven minutes of the game, Bright was knocked unconscious three times by blows from Oklahoma A&M defensive tackle Wilbanks Smith. While Smith's final elbow blow broke Bright's jaw, he was still able to complete a 61-yard touchdown pass to Drake halfback Jim Pilkington a few plays later. Soon afterward, the injury forced him to leave the game. Bright finished the game with less than 100 yards, the first time in his three-year collegiate career. Oklahoma A&M eventually won 27–14.\n\nBob Spiegel, a reporter with the \"Des Moines Register\", interviewed several spectators after the game, eventually publishing a report on the incident in the October 30, 1951 issue of the newspaper. According to Spiegel's report, several of the Oklahoma A&M students he interviewed overheard an Oklahoma A&M coach repeatedly say \"Get that nigger\" whenever the A&M practice squad ran Drake plays against the Oklahoma A&M starting defense prior to the October 20 game. Spiegel also recounted the experiences of a businessman and his wife, who were seated behind a group of Oklahoma A&M practice squad players. At the beginning of the game, one of the players turned around said, \"We're gonna get that nigger.\" After the first blow to Bright was delivered by Smith, the same player again turned around and told the businessman, \"See that knot on my jaw? That same guy [Smith] gave me that the very same way in practice.\"\n\nA six photograph sequence of the incident captured by \"Des Moines Register\" cameramen John Robinson and Don Ultang clearly showed Smith's jaw-breaking blow was thrown well after Bright had handed the ball off to Drake fullback Gene Macomber, and was well behind the play. Robinson and Ultang had actually set up a camera focusing on Bright before the game after the rumors of him being targeted became too loud to ignore. They rushed the film to Des Moines as soon as Bright was knocked out of the game. Ultang said years later that they were very lucky that the incident took place when it did; they'd only planned to stay through the first quarter so they could have enough time to develop the pictures before the deadline. The sequence won Robinson and Ultang the 1952 Pulitzer Prize for Photography, and eventually made the cover of \"Life Magazine\".\n\nOklahoma A&M's president, Oliver Willham, denied anything happened even after evidence of the incident was published nationwide. This began a cover-up that would last over half a century; during that time, whenever the story was discussed, the standard response from A&M/OSU was \"no comment.\" The determination to gloss over the affair was so strong that when Robert B. Kamm succeeded Willham in 1966, he knew that he could not even discuss the matter even though he had been Drake's dean of men at the time of the incident.\n\nWhen it became apparent that neither Oklahoma A&M nor the Missouri Valley Conference, to which both Drake and Oklahoma A&M belonged, would take any disciplinary action against Smith, Drake withdrew from the MVC in protest. The Bulldogs would not return to the MVC until 1956 for non-football sports, and would not return for football until 1971. Fellow member Bradley University pulled out of the league in solidarity with Drake and did not return for non-football sports until 1955; its football team never played another down in the MVC (Bradley dropped football in 1970). The incident eventually provoked changes in NCAA football rules regarding illegal blocking, and mandated the use of more protective helmets with face guards.\n\nBright's broken jaw limited his effectiveness for the remainder of his senior season at Drake, but he earned 70 percent of the yards Drake gained and scored 70 percent of the Bulldogs' points, despite missing the better part of the final three games of the season. Bright finished fifth in the balloting for the 1951 Heisman Trophy, and played in the post-season East–West Shrine Game and the Hula Bowl.\n\nFollowing his 1952 graduation from Drake, Bright went on to enjoy a 12-year professional football career in the Canadian Football League, retiring in 1964 as the CFL's all-time leading rusher, and was inducted into the Canadian Football Hall of Fame in 1970.\n\nRecalling the incident without apparent bitterness in a 1980 \"Des Moines Register\" interview three years before his death, Bright commented: \"There's no way it couldn't have been racially motivated.\" Bright went on to add: \"What I like about the whole deal now, and what I'm smug enough to say, is that getting a broken jaw has somehow made college athletics better. It made the NCAA take a hard look and clean up some things that were bad.\"\n\nWhen asked about Smith, whom he had not seen since the incident, Bright said he felt \"null and void\" about Smith, but added: \"The thing has been a great influence on my life. My total philosophy of life now is that, whatever a person's bias and limitation, they deserve respect. Everyone's entitled to their own beliefs.\"\n\nWilbanks Smith received over 1000 letters regarding the incident. Most of the mail was hate mail or death threats, but some was congratulatory and thankful. Smith maintained that he was not racist, the hit was \"not a racial incident,\" and that he had landed \"the same hit\" on a white player earlier in the game. He never apologized for the incident, but said in 2012 that he was glad the incident had helped to integrate college football, saying \"It took me a long time before I could smile about it. But now I can. I think it was a tool [Civil Rights'] organizations used, and it was very effective.\"\n\nOn September 28, 2005, Oklahoma State University President David J. Schmidly wrote a letter to Drake President David Maxwell formally apologizing for the incident. The apology came 22 years after Bright's death. Schmidly, reiterating a conversation earlier in the month over the phone, called the team's behavior that day \"an ugly mark on Oklahoma State University and college football.\"\n\n\n"}
{"id": "3094198", "url": "https://en.wikipedia.org/wiki?curid=3094198", "title": "Last man", "text": "Last man\n\nThe last man () is a term used by the philosopher Friedrich Nietzsche in \"Thus Spoke Zarathustra\" to describe the antithesis of his imagined superior being, the \"Übermensch\", whose imminent appearance is heralded by Zarathustra. The last man is tired of life, takes no risks, and seeks only comfort and security. \n\nThe last man's primary appearance is in \"Zarathustra's Prologue.\" According to Nietzsche, the last man is the goal that modern society and Western civilization have apparently set for themselves. After having unsuccessfully attempted to get the populace to accept the \"Übermensch\" as the goal of society, Zarathustra confronts them with a goal so disgusting that he assumes that it will revolt them. Zarathustra fails in this attempt, and instead of repelling and manipulating the populace into pursuing the goal of the Übermensch, the populace take Zarathustra literally and choose the \"disgusting\" goal of becoming the last men. This decision leaves Zarathustra disheartened and disappointed.\n\nThe lives of the last men are pacifist and comfortable. There is no longer a distinction between ruler and ruled, strong over weak or supreme over the mediocre. Social conflict and challenges are minimized. Every individual lives equally and in \"superficial\" harmony. There are no original or flourishing social trends and ideas. Individuality and creativity are suppressed. \n\nNietzsche warned that the society of the last man could be too barren and decadent to support the growth of healthy human life or great individuals. The last man is only possible by mankind having bred an apathetic person or ethnic group who are unable to dream, who are unwilling to take risks, and simply earn their living and keep warm. The society of the last man is antithetical to Nietzsche's theoretical will to power, the main driving force and ambition behind human nature, according to Nietzsche, as well as all other life in the universe. \n\nThe last man, Nietzsche predicted, would be one response to the problem of nihilism. But the full implications of the death of God had yet to unfold: \"The event itself is far too great, too distant, too remote from the multitude's capacity for comprehension even for the tidings of it to be thought of as having arrived as yet.\"\n\n"}
{"id": "31548988", "url": "https://en.wikipedia.org/wiki?curid=31548988", "title": "LogMAR chart", "text": "LogMAR chart\n\nA LogMAR chart comprises rows of letters and is used by ophthalmologists, optometrists and vision scientists to estimate visual acuity. This chart was developed at the National Vision Research Institute of Australia in 1976, and is designed to enable a more accurate estimate of acuity than do other charts (e.g., the Snellen chart). For this reason, the LogMAR chart is recommended, particularly in a research setting.\n\nWhen using the LogMAR chart, visual acuity is scored with reference to the Logarithm of the Minimum Angle of Resolution, as the chart's name suggests. An observer who can resolve details as small as 1 minute of visual angle scores LogMAR 0, since the base-10 logarithm of 1 is 0; an observer who can resolve details as small as 2 minutes of visual angle (i.e., reduced acuity) scores LogMAR 0.3, since the base-10 logarithm of 2 is near-approximately 0.3; and so on.\n\nThe chart was designed by Ian Bailey and Jan E Lovie-Kitchin at the National Vision Research Institute of Australia. They described their motivation for designing the LogMAR chart as follows: \"We have designed a series of near vision charts in which the typeface, size progression, size range, number of words per row and spacings were chosen in an endeavour to achieve a standardization of the test task.\"\n\nThe Snellen chart, which dates back to 1862, is also commonly used to estimate visual acuity. A Snellen score of 6/6 (20/20), indicating that an observer can resolve details as small as 1 minute of visual angle, corresponds to a LogMAR of 0 (since the base-10 logarithm of 1 is 0); a Snellen score of 6/12 (20/40), indicating an observer can resolve details as small 2 minutes of visual angle, corresponds to a LogMAR of 0.3 (since the base-10 logarithm of 2 is near-approximately 0.3), and so on.\n\nEach letter has a score value of 0.02 log units. Since there are 5 letters per line, the total score for a line on the LogMAR chart represents a change of 0.1 log units. The formula used in calculating the score is:\n\nThe LogMAR chart is designed to enable more accurate estimates of acuity as compared to other acuity charts (e.g., the Snellen chart). Each line of the LogMAR chart comprises the same number of test letters (effectively standardizing the test across letter size); the Sloan font is used (Sloan letters are approximately equally legible one from another); letter size from line to line varies logarithmically, as does the spacing between lines (making the chart easy to use at nonstandard viewing distances). Furthermore, the final LogMAR score is based on the total of all letters read.\n\nThe World Health Organization established criteria for low vision using the LogMAR scale. Low vision is defined as a best-corrected visual acuity worse than 0.5 LogMAR but equal or better than 1.3 LogMAR in the better eye. Blindness is defined as a best-corrected visual acuity worse than 1.3 LogMAR.\n\n"}
{"id": "742665", "url": "https://en.wikipedia.org/wiki?curid=742665", "title": "Mutatis mutandis", "text": "Mutatis mutandis\n\nMutatis mutandis is a Medieval Latin phrase meaning \"the necessary changes having been made\" or \"once the necessary changes have been made\". It remains unnaturalized in English and is therefore usually italicized in writing. It is used in many countries to acknowledge that a comparison being made requires certain obvious alterations, which are left unstated. It is not to be confused with the similar \"ceteris paribus\", which excludes any changes other than those explicitly mentioned. \"Mutatis mutandis\" is increasingly replaced by non-Latin equivalents, such as without loss of generality, but is still used in law, economics, mathematics, linguistics, and philosophy. In particular, in logic, it is encountered when discussing counterfactuals, as a shorthand for all the initial and derived changes which have been previously discussed.\n\nThe phrase '—now sometimes written ' to show vowel length—does not appear in surviving classical literature. It is Medieval Latin, first attested in British sources in 1272.\n\nBoth words are participles of the Latin verb ' (\"to move; to change; to exchange\"). ' is its perfect passive participle (\"changed; having been changed\"). \"\" is its gerundive, which functions both as a future passive participle (\"to be changed; going to be changed\") and as a verbal adjective or noun expressing necessity (\"needing to be changed; things needing to be changed\"). The phrase is an ablative absolute, using the ablative case to show that the clause is a necessary condition for the rest of the sentence.\n\n\"Mutatis mutandis\" was first borrowed into English in the 16th century, but continues to be italicized as a foreign phrase. Although many similar adverbial phrases are treated as part of the sentence, \"mutatis mutandis\" is usually set apart by commas or in some other fashion. The nearest English equivalent to an ablative absolute is the nominative absolute, so that a literal translation will either use the nominative case (\"those things which need to be changed having been changed\") or a preposition (\"with the things needing to be changed having been changed\"). More often, the idea is expressed more tersely (\"with the necessary changes\") or using subordinating conjunctions and a dependent clause (\"once the necessary adjustments are made\").\n\nThe legal use of the term is somewhat specialized. As glossed by Shira Scheindlin, judge for the Southern District of New York, for a 1998 case: \"This Latin phrase simply means that the necessary changes in details, such as names and places, will be made but everything else will remain the same.\" In the wake of the Plain English movements, some countries attempted to replace their law codes' legal Latin with English equivalents.\n\n\nThe phrase appears in other European languages as well. A passage of Marcel Proust's \"Remembrance of Things Past\" includes \"...j'ai le fils d'un de mes amis qui, mutatis mutandis, est comme vous...\" (\"A friend of mine has a son whose case, \"mutatis mutandis\", is very much like yours.\") The German Ministry of Justice, similar to the Plain English advocates above, now eschews its use. Their official English translation of the Civil Code now reads:\n\n\"Section 27 (Appointment of and management by the board). ...(3) The management by the board is governed by the provisions on mandate in sections 664 to 670 \"with the necessary modifications.\"\"\n\n"}
{"id": "3778432", "url": "https://en.wikipedia.org/wiki?curid=3778432", "title": "Negative repetition", "text": "Negative repetition\n\nA negative repetition (negative rep) is the repetition of a technique in weight lifting in which the lifter performs the eccentric phase of a lift. Instead of pressing the weight up slowly, in proper form, a spotter generally aids in the concentric, or lifting, portion of the repetition while the lifter slowly performs the eccentric phase for 3–6 seconds. Negative reps are used to improve both muscular strength and power in subjects, this is commonly known as hypertrophy training.\n\nDue to its mechanical properties, this form of training can be used for both healthy individuals and individuals who are in rehabilitation. Studies have shown that negative repetitions or \"eccentric phase training\" combines a high amount of force on the muscle with a lower energy cost than normal concentric training, which requires 4–5 times the amount of energy. This justifies why this type of training is more beneficial and less of a risk to subjects rehabilitating or with a limited exercise capacity.\n\nEccentric training is often associated with the terms \"muscles soreness\" and \"muscle damage\". In 1902, Theodore Hough discovered and developed the term DOMS (delayed onset muscle soreness), after he found that exercises containing negative repetitions caused athletes to have sore muscles. Hough believed this was causing a rupture within the muscle; when he looked further into the subject, he found that when performing eccentric exercise that exhibited soreness, the muscle \"quickly adapts and becomes accustomed to the increase in applied stress\". The result of this was that the muscles' soreness not only decreased, but the muscular damage did too.\n\nIt has been proven that eccentric resistance training improves the functional mobility of older adults. Studies have shown that eccentric training of the lower body, in particular the , are essential in preventing falls in older adults and helping them maintain their independence. A study conducted focusing on eccentric training for the age group 65–87 years of age showed that, over 12 weeks, they had strengthened their knee extensors by up to 26%. With this evidence, it is reasonable to suggest that negative repetitions can help improve the health of older adults.\n\nStudies have shown that eccentric training may be successful in the treatment of certain tendonitis. Studies have shown that the use of eccentric training for twelve weeks may be an alternative to therapy for people suffering from Patellar Tendinopathy (Jumper's Knee). Eccentric training has also been proven successful in the treatment of chronic Achilles tendonitis, using a twelve-week eccentric calf muscle program various studies have shown the ability for people to return to normal pre-tendonitis levels. The reasoning behind the benefits of eccentric training for tendinopathy is still unclear.\n\n"}
{"id": "662624", "url": "https://en.wikipedia.org/wiki?curid=662624", "title": "Noncommutative topology", "text": "Noncommutative topology\n\nIn mathematics, noncommutative topology is a term used for the relationship between topological and C*-algebraic concepts. The term has its origins in the Gelfand-Naimark theorem, which implies the duality of the category of locally compact Hausdorff spaces and the category of commutative C*-algebras. Noncommutative topology is related to analytic noncommutative geometry.\n\nThe premise behind noncommutative topology is that a noncommutative C*-algebra can be treated like the algebra of complex-valued continuous functions on a 'noncommutative space' which does not exist classically. Several topological properties can be formulated as properties for the C*-algebras without making reference to commutativity or the underlying space, and so have an immediate generalization.\nAmong these are:\n\nIndividual elements of a commutative C*-algebra correspond with continuous functions. And so certain types of functions can correspond to certain properties of a C*-algebra. For example, self-adjoint elements of a commutative C*-algebra correspond to real-valued continuous functions. Also, projections (i.e. self-adjoint idempotents) correspond to indicator functions of clopen sets.\n\nCategorical constructions lead to some examples. For example, the coproduct is the disjoint union and thus corresponds to the direct sum of algebras, which is the product of C*-algebras. Similarly, Product topology corresponds to the coproduct of C*-algebras, the tensor product of algebras. In a more specialized setting,\ncompactifications of topologies correspond to unitizations of algebras. So the one-point compactification corresponds to the minimal unitization of C*-algebras, the Stone-Čech compactification corresponds to the multiplier algebra, and corona sets corresponds with corona algebras.\n\nThere are certain examples of properties where multiple generalizations are possible and it is not clear which is preferable. For example, probability measures can correspond either to states or tracial states. Since all states are vacuously \ntracial states in the commutative case, so it is not clear whether the tracial condition is necessary to be a useful generalization.\n\nOne of the major examples of this idea is the generalization of topological K-theory to noncommutative C*-algebras in the form of operator K-theory.\n\nA further development in this is a bivariant version of K-theory called KK-theory, which has a composition product \n\nformula_1\n\nof which the ring structure in ordinary K-theory is a special case. The product gives the structure of a category to KK. It has been related to correspondences of algebraic varieties.\n"}
{"id": "10125582", "url": "https://en.wikipedia.org/wiki?curid=10125582", "title": "Nuclear peace", "text": "Nuclear peace\n\nNuclear peace is a theory of international relations that argues that under some circumstances nuclear weapons can induce stability and decrease the chances of crisis escalation. In particular, nuclear weapons are said to have induced stability during the Cold War, when both the US and the USSR possessed mutual second strike retaliation capability, eliminating the possibility of nuclear victory for either side. Proponents of nuclear peace argue that controlled nuclear proliferation may be beneficial for inducing stability. Critics of nuclear peace argue that nuclear proliferation not only increases the chance of interstate nuclear conflict, but increases the chances of nuclear material falling into the hands of violent non-state groups who are free from the threat of nuclear retaliation.\n\nThe major debate on this issue has been between Kenneth Waltz, the founder of neorealist theory in international relations, and Scott Sagan, a leading proponent of organizational theories in international politics. Waltz generally argues that \"more may be better,\" contending that new nuclear states will use their acquired nuclear capabilities to deter threats and preserve peace. Sagan argues that \"more will be worse\", since new nuclear states often lack adequate organizational controls over their new weapons, which makes for a high risk of either deliberate or accidental nuclear war, or theft of nuclear material by terrorists to perpetrate nuclear terrorism.\n\nA nuclear peace results when the costs of war are unacceptably high for both sides. In a two-sided conflict where both sides have mutual second strike capability, defense becomes impossible so it is the very prospect of fighting the war, rather than the possibility of losing it, that induces restraint.\n\nIn a condition of mutually assured destruction, there are civilian \"hostages\" on both sides, which facilitates cooperation by acting as an informal mechanism of contract enforcement between states. There are economic equivalents of such informal mechanisms used to effect credible commitment; for example, corporations use \"hostages\" (in the form of initial setup costs that act as collateral) to deter subsidiaries and franchisees from cheating.\n\nNuclear weapons may also lessen a state's reliance on allies for security, thus preventing allies from dragging each other into wars (a phenomenon known as chain ganging, frequently said to be a major cause of World War I).\n\nSince the death of civilians is an essential part of mutually assured destruction, one normative consequence of nuclear weapons is that war loses its historical function as a symbol of glory and measure of national strength.\n\nAs a method of preventing a destabilizing arms race, the concept of minimal deterrence represents one way of solving the security dilemma and avoiding an arms race.\n\nA study published in the \"Journal of Conflict Resolution\" in 2009 quantitatively evaluated the nuclear peace hypothesis and found support for the existence of the stability-instability paradox. The study determined that while nuclear weapons promote strategic stability and prevent largescale wars, they simultaneously allow for more lower intensity conflicts. When a nuclear monopoly exists between two states, and one state has nuclear weapons and its opponent does not, there is a greater chance of war. In contrast, when there is mutual nuclear weapon ownership with both states possessing nuclear weapons, the odds of war drop precipitously.\n\nCritics argue that war can occur even under conditions of mutually assured destruction:\n\nActors are not always rational, as bureaucratic procedure and internal intrigue may cause subrational outcomes. Related to and reinforcing that point is that there is always an element of uncertainty. One cannot always control emotions, subordinates, and equipment, especially when one has limited information and is faced with high stakes and fast timetables. There are unintended consequences, unwanted escalation, irrationality, misperception, and the security dilemma.\n\nAnother reason is that deterrence has an inherent instability. As Kenneth Boulding said: \"If deterrence were really stable… it would cease to deter.\" If decisionmakers were perfectly rational, they would never order the largescale use of nuclear weapons, and the credibility of the nuclear threat would be low.\n\nHowever, that apparent perfect rationality criticism is countered and so not inconsistent with current deterrence policy. In \"Essentials of Post-Cold War Deterrence\", the authors detail an explicit advocation of ambiguity regarding \"what is permitted\" for other nations and its endorsement of \"irrationality\" or, more precisely, the perception thereof as an important tool in deterrence and foreign policy. The document claims that the capacity of the United States, in exercising deterrence, would be hurt by portraying US leaders as fully rational and cool-headed: \n\nSome commentators critical of the concept of nuclear peace further make the argument that nonstate actors and rogue states could supply nuclear weapons to terrorist organizations and thereby undermine conventional deterrence and therefore nuclear peace, especially with the existence of international terrorist networks seeking access to nuclear sources.\n\nHowever Robert Gallucci, president of the John D. and Catherine T. MacArthur Foundation, argues that although traditional deterrence is not an effective approach toward terrorist groups bent on causing a nuclear catastrophe, \"the United States should instead consider a policy of expanded deterrence, which focuses not solely on the would-be nuclear terrorists but on those states that may deliberately transfer or inadvertently lead nuclear weapons and materials to them. By threatening retaliation against those states, the United States may be able to deter that which it cannot physically prevent.\"\n\nGraham Allison makes a similar case and argues that the key to expanded deterrence is coming up with ways of tracing nuclear material to the country that forged the fissile material: \"After a nuclear bomb detonates, nuclear forensic cops would collect debris samples and send them to a laboratory for radiological analysis. By identifying unique attributes of the fissile material, including its impurities and contaminants, one could trace the path back to its origin.\" The process is analogous to identifying a criminal by fingerprints: \"The goal would be twofold: first, to deter leaders of nuclear states from selling weapons to terrorists by holding them accountable for any use of their own weapons; second, to give leaders every incentive to tightly secure their nuclear weapons and materials.\"\n\n"}
{"id": "40941916", "url": "https://en.wikipedia.org/wiki?curid=40941916", "title": "Palatini identity", "text": "Palatini identity\n\nIn general relativity and tensor calculus, the Palatini identity is:\n\nwhere formula_2 denotes the variation of Christoffel symbols and formula_3 indicates covariant differentiation.\n\nA proof can be found in the entry Einstein–Hilbert action.\n\nThe \"same\" identity holds for the Lie derivative formula_4. In fact, one has:\n\nwhere formula_6 denotes any vector field on the spacetime manifold formula_7.\n\n\n"}
{"id": "50262184", "url": "https://en.wikipedia.org/wiki?curid=50262184", "title": "Prague Declaration of European Conservatives and Reformists", "text": "Prague Declaration of European Conservatives and Reformists\n\nThe Prague Declaration gathers the Principles of the European Conservatives and Reformists Group (ECR), decided and pronounced in March 2009.\n\nThe preamble brings up the need to reform the European Union (EU) according to the principles of both liberal and conservative ideas and values. The core elements are the following ones:\n\n- Free enterprise, free and fair trade and competition, minimal regulation, lower taxation, and small government as the ultimate catalysts for individual freedom and personal and national prosperity.\n\n- Freedom of the individual, more personal responsibility and greater democratic accountability.\n\n- Sustainable, clean energy supply with an emphasis on energy security.\n\n- The importance of the family as the bedrock of society.\n\n- The sovereign integrity of the nation state, opposition to EU federalism and a renewed respect for true subsidiarity.\n\n- The overriding value of the transatlantic security relationship in a revitalised NATO, and support for young democracies across Europe.\n\n- Effectively controlled immigration and an end to abuse of asylum procedures.\n\n- Efficient and modern public services and sensitivity to the needs of both rural and urban communities.\n\n- An end to waste and excessive bureaucracy and a commitment to greater transparency and probity in the EU institutions and use of EU funds.\n\n- Respect and equitable treatment for all EU countries, new and old, large and small.\n\n"}
{"id": "7378305", "url": "https://en.wikipedia.org/wiki?curid=7378305", "title": "Radioqualia", "text": "Radioqualia\n\nr a d i o q u a l i a is an art collaboration by New Zealanders, Adam Hyde and Honor Harger, founded in 1998 in Australia. Since 1999 they have been based in several different countries including the Netherlands, the UK and Latvia.\n\nr a d i o q u a l i a create broadcasts, installations, performances and online artworks. Their principal interest is how broadcasting technologies can be used to create new artistic forms, and how sound art can be used to illuminate abstract ideas.\n\nKey works include: The Frequency Clock (1998 - 2003), Free Radio Linux (2002 – 2004), Radio Astronomy (2004 -> now)\n\nThey have exhibited at museums, galleries and festivals, including: NTT ICC, Tokyo; New Museum, New York; Gallery 9, Walker Art Center, USA; Sónar, Barcelona; Ars Electronica festivals, Linz, Austria; Experimental Art Foundation, Australia; Maison Europeenne de la Photographie, Paris; and the Physics Room, New Zealand.\n\nIn August 2004 they were joint winners of a UNESCO Digital Art Prize (second place) for Radio Astronomy. \n\nByrne C (2005), \"net.radio and streaming audio\", paper for Looking Glass lectures at OKNO, Norway, Saturday 18 June 2005, published online: \nViewed 6 March 2007\n\nFlor M (2002), \"Hear me out: Free Radio Linux broadcasts the Linux sources on air and online\", Published on Netartcommons, March 2002:\n\nViewed 5 May 2007\n\nLudovico A (2005), \"Streaming Culture, Radioqualia interview\", Neural 23 magazine, Italy. \nReference: \nViewed 25 April 2007\n\nPerron J (2003), \"radioqualia\", la foundation Daniel Langlois website biography, Published online: \nViewed 20 March 2007\n\nTribe M (2006), \"r a d i o q u a l i a - Free Radio Linux\", Brown University Wiki, Published online: \nViewed 25 April 2007\n\nTribe M and Jana R, \"New Media Art\", Taschen, 2006. .\n\n\n"}
{"id": "3540587", "url": "https://en.wikipedia.org/wiki?curid=3540587", "title": "Robert Ayres (scientist)", "text": "Robert Ayres (scientist)\n\nRobert Underwood Ayres (born June 29, 1932) is an American-born physicist and economist. His career has focused on the application of physical ideas, especially the laws of thermodynamics, to economics; a long-standing pioneering interest in material flows and transformations (industrial ecology or industrial metabolism)—a concept which he originated. His most recent work challenges the widely held economic theory of growth.\n\nTrained as a physicist at the University of Chicago, University of Maryland, and King's College London (PhD in Mathematical Physics), Ayres has dedicated his professional life to advancing the environment, technology and resource end of the sustainability agenda. His major research interests include technological change, environmental economics, \"industrial metabolism\" and \"eco-restructuring\". He has worked at the Hudson Institute (1962–67), Resources for the Future Inc (1968) and International Research and Technology Corp (1969–76). From 1979 until 1992 he was Professor of Engineering and Public Policy at Carnegie Mellon University, Pittsburgh, Pennsylvania, except for two years (and six summers) on leave at the International Institute for Applied Systems Analysis (IIASA) in Laxenburg Austria. In 1992 he moved to the international business school INSEAD in Fontainebleau, France as Sandoz (later Novartis) Professor of Environment and Management. Since his formal retirement in 2000 he has been Jubilee Visiting Professor (2000–2001) and king Karl Gustav XVI professor of environmental science (2004–2005) at Chalmers Institute of Technology Gothenburg (Sweden). He is currently an Institute Scholar at IIASA.\n\nHe remains an active researcher. He has written or co-authored 20 books, edited or coedited another dozen books, written or co-authored more than 200 journal articles and book chapters not to mention many unpublished reports, on subjects ranging from environmental effects of nuclear war to theoretical economics. But most of his life-work is interdisciplinary. He was a pioneer of a new field, sometimes called Industrial Metabolism or Industrial Ecology. He has contributed to futures studies, technological forecasting, transportation and energy studies, material flow studies (`dematerialization'), environmental technology, environmental economics, thermodynamics and economics, and the theory of economic growth.\n\nHere taken from one of his recent papers are two paragraphs that provide a flavor of his recent work:\nMainstream economics today is based to a large extent on bad ideas. Economic concepts, from foundational issues like markets, supply and demand and “free trade”, to money and finance, lack any systematic awareness of the physical process of production or the implications of the Laws of Thermodynamics for those processes. A corollary, almost worthy of being a separate bad idea on its own, is that energy doesn’t matter (much) because the cost share of energy in the economy is so small that it can be ignored e.g. {Denison, 1984 #6184}. The so-called “production functions” used by all schools of economic thought that build growth models omit any necessary role for energy, as if output could be produced by labor and capital alone—or as if energy is merely a form of man-made capital that can be produced (as opposed to extracted) by labor and capital.\n\nThe essential truth missing from economic education today is that energy is the stuff of the universe, that all matter is also a form of energy, and that the economic system is essentially a system for extracting, processing and transforming energy as resources into energy embodied in products and services. This is a thermodynamic process, as the Rumanian economist Georgescu-Roegen said half a century ago (Georgescu-Roegen 1971). The economic process is subject to both the first law of thermodynamics (conservation of mass/energy; nothing can be created or destroyed) and the second law of thermodynamics (increasing entropy; all transformation processes are irreversible). The “first law” implies that the notion of “consumption” as applied to products is misleading: material transformation processes unavoidably generate large quantities of material wastes or residuals {Ayres, 1969 #284;{Ayres, 1989 #424}. Some of those wastes are merely inconvenient but others are harmful or toxic. The second law says that energy becomes less useful (exergy is destroyed) by every action.\nThere is much more to be said along these lines. Key publications reflecting these (and some other) important ideas are given in the bibliography below.\n\n\n"}
{"id": "2013797", "url": "https://en.wikipedia.org/wiki?curid=2013797", "title": "Rooi gevaar", "text": "Rooi gevaar\n\nRooi gevaar () is an Afrikaans phrase, sometimes translated into English as \"Communist danger\". The term gained popularity in South Africa during the Cold War and was associated with the perceived threat of international communism to religious, economic, and political freedom on the Southern African subcontinent. This pretext was used to justify the banning of the South African Communist Party (SACP) and its sister organisation, the African National Congress (ANC), which were regarded as leading anti-apartheid movements. Alternatively, the phrase rooi komplot () was also used. \n\nThe term diminished in use after the collapse of the Soviet Union in 1991.\n"}
{"id": "12268728", "url": "https://en.wikipedia.org/wiki?curid=12268728", "title": "Scourge of Worlds: A Dungeons &amp; Dragons Adventure", "text": "Scourge of Worlds: A Dungeons &amp; Dragons Adventure\n\nA Scourge of Worlds: A Dungeons & Dragons Adventure is an animated film or interactive adventure. In each scene, it allows the user a choice, and different endings or different paths to the same ending will be displayed depending upon that choice.\n\nIt was released by Rhino Theatrical in June of 2003.\n\nIt continued the tradition of 'Choose Your Own Adventure' novels, using a D&D theme and storyline.\n\nIt was directed by Dan Krech.\n\n"}
{"id": "1268954", "url": "https://en.wikipedia.org/wiki?curid=1268954", "title": "Self-abasement", "text": "Self-abasement\n\nSelf-abasement is voluntary self-punishment or humiliation in order to atone for some real or imagined wrongdoing.\n\nSelf-abasement might have a religious aspect for those seeking humility before God, perhaps in the context of monastic or cenobitic lifestyle.\n\nIt also has a sexual and fetish aspect for those people who enjoy erotic humiliation and other related BDSM practices. \n\nExamples of self-abasement practices include self-flagellation, bondage, torture, public humiliation (including online humiliation).\n\nIn psychology, self-abasement is associated with shame (rather than guilt) and involves the reduction of the subject's self-esteem. The notion of self-abasement can be said to be based in Freudian psychoanalysis. Fear may also result in self-abasement.\n\n"}
{"id": "32873274", "url": "https://en.wikipedia.org/wiki?curid=32873274", "title": "Social protection floor", "text": "Social protection floor\n\nThe Social Protection Floor (SPF), is the first level of social protection in a national social protection system. It is a basic set of social rights derived from human right treaties, including access to essential services (such as health, education, housing, water and sanitation, and others, as defined nationally) and social transfers, in cash or in kind, to guarantee income security, food security, adequate nutrition and access to essential services.\n\nAs a result of the extreme inequality, social security schemes have been developed and implemented, through private and public initiatives, since the 1970s in Europe and subsequently in other parts of the world. However, the problem of poverty persists. According to the World Bank, over a billion people, or roughly one in six, live in extreme poverty and 2.8 billion people live in poverty, meaning on less than 1USD and less than 2USD a day, respectively.\n\nTo remedy this situation and promote socio-economic development, the United Nations Chief Executives Board for Coordination (UNCEB) coined the concept of the SPF. This framework aims to place governments as the central responsible actor for the promotion of four essential and universal guarantees, which would set the ground for a more comprehensive social protection system.\n\nThe Social Protection Floor is a socio-economic development policy concept and a crisis management tool. It promotes a solid foundation for economic growth, provides a societal insurance against perpetuating poverty and mitigates the effects of economic shocks and crisis.\n\nThe Universal Declaration of Human Rights, the International Covenant on Economic, Social and Cultural Rights and the International Labour Organization Conventions are international instruments that have recognized these essential social rights and have been used as the legal basis to support the Social Protection Floor.\n\nFounded on a rights-based approach, the Social Protection Floor encourages countries to aim towards a universal standard of social protection coverage. Since the context of each country differs in terms of institutional capacity, political ideologies, financial resources, economic structure and cultural values, each floor is defined by individual countries.\n\nBuilding from past social protection programs, the SPF promotes a more coordinated design and implementation of social and labour policies in order to guarantee a country-defined basic set of social rights, services and facilities that every person should enjoy, which could be granted through:\n\nTo insure continuity and sustainability, these strategies build on existing social protection mechanisms and include a mix of contributory and non-contributory, targeted and universal, public and private instruments – depending on the social, economic and political context. Countries are also encouraged to develop higher levels of social protection in line with their needs, preferences, and financial capacities.\n\nA global coalition of United Nations agencies, international NGOs, development banks, bilateral organization and other development partners, the Social Protection Floor - Initiative Coalition (SPF-I Coalition), has been created to support countries with the establishment, expansion and edification of their national social protection floors.\n\nThe ultimate objective of the Social Protection Floor approach is to build a solid basis that would allow higher levels of protection, than simply the ground floor level. As economies grow and fiscal space is created, social protection systems can and should move up the Social Protection Floor \"staircase\", extending the scope, level and quality of benefits and services provided.\n\nWithin this strategy, the International Labour Organization (ILO) has suggested a two-dimensional approach to develop the SPF including : \n\nThis concept takes into consideration the national constraints of countries, while promoting a basic universal level of social protection to all. The gradual introduction and implementation of the social protection guarantees is essential for the development and sustainability of the system. In this way, countries do not lose sight of the overall objective of achieving comprehensive social protection for all; and they are able to assess and be constantly aware of the opportunity cost in terms of other guarantees that are put on hold when taking a decision on priorities. Formulating a package of guarantees as a floor should thus lead to rational cost/benefit-based policy decisions.\n\nEnsuring a Social Protection Floor for the entire world population represents a considerable challenge, but experiences from countries all over the world and calculations by various UN agencies, including UNICEF, show that a basic floor of social transfers is globally affordable at virtually any stage of economic development.\n\nIn 2008, the International Labour Organization published a cost-estimation study of 12 low-income countries in Africa and Asia that showed that the initial gross annual cost of a hypothetical basic social security package – that excluded access to essential health case because it is already to some extent financed – was projected to lie between 2.2 and 5.7 per cent of GDP in 2010.\n\nThere are many ways to attain affordable SFP coverage in middle- or low-income countries. While some countries seek to extend social insurance and combine it with social assistance, others subsidize social insurance coverage for the poor to enable them to enjoy participation in the general schemes, and still others seek to establish tax-financed universal schemes or conditional social transfer schemes. Each approach has its advantages and its limitations, depending on national values, past experience and institutional frameworks.\n\nThe core challenge for financing the basic social security guarantees remains that of securing the necessary fiscal space. For this reason, tax reforms may be necessary to increase fiscal resources, but also to enhance transparency, effectiveness and efficiency in tax collection.\n\nIn addition, measures may be required to maximize the administrative capacity to deliver benefits efficiently and inform the population about programmes. Evidence shows that good governance of social policies and programme is essential for efficient service delivery, monitoring, evaluation and financial management.\n\nThe Social Protection Floor Initiative (SPF-I) is a joint UN effort to build a global coalition of United Nations agencies, international NGOs, development banks, bilateral organisations and other development partners that are committed to collaborating at national, regional and global levels to support countries committed to building national social protection floors for their population.\n\nThe SPF-I was launched in April 2009 as one of the nine UN Chief Executives Board’s crisis initiatives – responding to repeated demands from member states for better coordinated technical, logistical and financial assistance from UN system agencies in times of crisis. The activities of the initiative are open for participation to all organisations that want to support the cause of strengthening social protection for all in need. Organizations that are involved include: ILO, World Health Organization, FAO, International Monetary Fund, OHCHR, UN Regional Commissions, UNAIDS, UN-DESA, UNDP, UNESCO, FAO, UNFPA, UN-HABITAT, UNHCR, UNICEF, UNODC, UNRWA, World Food Program, WMO, World Bank, ADB, BMZ, DFID, Helpage International, Save the Children, ICSW, GIZ, ESN, Ministry of Foreign Affairs Finland, French International Health and Social Protection Agency, GIPS, and others.\n\nPulling away from traditional top-down implementation, SPFs are country-led and developed based on the existing framework of county-specific social protection systems, institutional and administrative structures, economic constraints, fiscal space, political dynamics and social policy needs, objectives and priorities. The Coalition members of the SPF-I play a consultative role and among other activities: \n\n\n"}
{"id": "38218032", "url": "https://en.wikipedia.org/wiki?curid=38218032", "title": "Space mapping", "text": "Space mapping\n\nThe space mapping methodology for modeling and design optimization of engineering systems was first discovered by John Bandler in 1993. It uses relevant existing knowledge to speed up model generation and design optimization of a system. The knowledge is updated with new validation information from the system when available.\n\nThe space mapping methodology employs a \"quasi-global\" formulation that intelligently links companion \"coarse\" (ideal or low-fidelity) and \"fine\" (practical or high-fidelity) models of different complexities. In engineering design, space mapping aligns a very fast coarse model with the expensive-to-compute fine model so as to avoid direct expensive optimization of the fine model. The alignment can be done either off-line (model enhancement) or on-the-fly with surrogate updates (e.g., aggressive space mapping).\nAt the core of the process is a pair of models: one very accurate but too expensive to use directly with a conventional optimization routine, and one significantly less expensive and, accordingly, less accurate. The latter (fast model) is usually referred to as the \"coarse\" model (coarse space). The former (slow model) is usually referred to as the \"fine\" model. A validation space (\"reality\") represents the fine model, for example, a high-fidelity physics model. The optimization space, where conventional optimization is carried out, incorporates the coarse model (or surrogate model), for example, the low-fidelity physics or \"knowledge\" model. In a space-mapping design optimization phase, there is a prediction or \"execution\" step, where the results of an optimized \"mapped coarse model\" (updated surrogate) are assigned to the fine model for validation. After the validation process, if the design specifications are not satisfied, relevant data is transferred to the optimization space (\"feedback\"), where the mapping-augmented coarse model or surrogate is updated (enhanced, realigned with the fine model) through an iterative optimization process termed \"parameter extraction\". The mapping formulation itself incorporates \"intuition\", part of the engineer's so-called \"feel\" for a problem. In particular, the Aggressive Space Mapping (ASM) process displays key characteristics of cognition (an expert's approach to a problem), and is often illustrated in simple cognitive terms.\n\nFollowing John Bandler's concept in 1993, algorithms have utilized Broyden updates (aggressive space mapping), trust regions, and artificial neural networks. New developments include implicit space mapping, in which we allow preassigned parameters not used in the optimization process to change in the coarse model, and output space mapping, where a transformation is applied to the response of the model. A paper reviews the state of the art after the first ten years of development and implementation. Tuning space mapping utilizes a so-called tuning model—constructed invasively from the fine model—as well as a calibration process that translates the adjustment of the optimized tuning model parameters into relevant updates of the design variables. The space mapping concept has been extended to neural-based space mapping for large-signal statistical modeling of nonlinear microwave devices.\n\nA 2016 state-of-the-art review is devoted to aggressive space mapping. It spans two decades of development and engineering applications.\n\nThe space mapping methodology can also be used to solve inverse problems. Proven techniques include the Linear Inverse Space Mapping (LISM) algorithm, as well as the Space Mapping with Inverse Difference (SM-ID) method.\n\nSpace mapping optimization belongs to the class of surrogate-based optimization methods, that is to say, optimization methods that rely on a surrogate model.\n\nThe space mapping technique has been applied in a variety of disciplines including microwave and electromagnetic design, civil and mechanical applications, aerospace engineering, and biomedical research. Some examples:\n\n\nVarious simulators can be involved in a space mapping optimization and modeling processes.\n\nThree international workshops have focused significantly on the art, the science and the technology of space mapping.\n\n\nThere is a wide spectrum of terminology associated with space mapping: ideal model, coarse model, coarse space, fine model, companion model, cheap model, expensive model, surrogate model, low fidelity (resolution) model, high fidelity (resolution) model, empirical model, simplified physics model, physics-based model, quasi-global model, physically expressive model, device under test, electromagnetics-based model, simulation model, computational model, tuning model, calibration model, surrogate model, surrogate update, mapped coarse model, surrogate optimization, parameter extraction, target response, optimization space, validation space, neuro-space mapping, implicit space mapping, output space mapping, port tuning, predistortion (of design specifications), manifold mapping, defect correction, model management, multi-fidelity models, variable fidelity/variable complexity, multigrid method, coarse grid, fine grid, surrogate-driven, simulation-driven, model-driven, feature-based modeling.\n"}
{"id": "4587233", "url": "https://en.wikipedia.org/wiki?curid=4587233", "title": "Subjective validation", "text": "Subjective validation\n\nSubjective validation, sometimes called personal validation effect, is a cognitive bias by which a person will consider a statement or another piece of information to be correct if it has any personal meaning or significance to them. In other words, a person whose opinion is affected by subjective validation will perceive two unrelated events (i.e., a coincidence) to be related because their personal belief demands that they be related. Closely related to the Forer effect, subjective validation is an important element in cold reading. It is considered to be the main reason behind most reports of paranormal phenomena. According to Bob Carroll, psychologist Ray Hyman is considered to be the foremost expert on subjective validation and cold reading.\n\nThe term \"subjective validation\" first appeared in the 1980 book \"The Psychology of the Psychic\" by David F. Marks and Richard Kammann.\n\n"}
{"id": "623398", "url": "https://en.wikipedia.org/wiki?curid=623398", "title": "Sublanguage", "text": "Sublanguage\n\nA sublanguage is a subset of a language. Sublanguages occur in natural language, computer language, and relational databases.\n\nIn Informatics, natural language processing, and machine translation, a sublanguage is the language of a restricted domain, particularly a technical domain. In mathematical terms, \"a subset of the sentences of a language forms a sublanguage of that language if it is closed under some operations of the language: e.g., if when two members of a subset are operated on, as by \"and\" or \"because\", the resultant is also a member of that subset\" (Z. S. Harris \"Language and Information\", Columbia U. Press, 1988, p. 34).\n\nThe term sublanguage has also sometimes been used to denote a computer language that is a subset of another language. A sublanguage may be restricted syntactically (it accepts a subgrammar of the original language), and/or semantically (the set of possible outcomes for any given program is a subset of the possible outcomes in the original language).\n\nFor instance, ALGOL 68S was a subset of ALGOL 68 designed to make it possible to write a single-pass compiler for this \"sublanguage\".\n\nSQL (Structured Query Language) statements are classified in various ways, which can be grouped into sublanguages, commonly: a data query language (DQL), a data definition language (DDL), a data control language (DCL), and a data manipulation language (DML).\n\nIn relational database theory, the term \"sublanguage\", first used for this purpose by E. F. Codd in 1970, refers to a computer language used to define or manipulate the structure and contents of a relational database management system (RDBMS). Typical sublanguages associated with modern RDBMS's are QBE (Query by Example) and SQL (Structured Query Language). In 1985, Codd encapsulated his thinking in twelve rules which every database must satisfy in order to be truly relational. The fifth rule is known as the \"Comprehensive data sublanguage rule\", and states:\n\n"}
{"id": "1186932", "url": "https://en.wikipedia.org/wiki?curid=1186932", "title": "Swiss-system tournament", "text": "Swiss-system tournament\n\nA Swiss-system tournament is a non-eliminating tournament format which features a set number of rounds of competition, but considerably fewer than in a round-robin tournament. In a Swiss tournament, each competitor (team or individual) does not play every other. Competitors meet one-to-one in each round and are paired using a set of rules designed to ensure that each competitor plays opponents with a similar running score, but not the same opponent more than once. The winner is the competitor with the highest aggregate points earned in all rounds. All competitors play in each round unless there is an odd number of players.\n\nA Swiss system is used for competitions where the number of entrants is considered too large for a full round-robin to be feasible, and eliminating any competitors before the end of the tournament is undesirable. Round-robin pairings are suitable for a small number of competitors and rounds, as most or all players will play each other; the underlying assumption is that the player who has played all possible opponents and ends with the highest score must be the winner. Knockout, or elimination, pairings rapidly reduces the number of competitors, but may not necessarily result in the best possible competitor winning, as good competitors might have a bad day or eliminate and exhaust each other if they meet in early rounds. Swiss systems intend to provide a clear winner with a limited number of rounds and a potentially unlimited number of opponents. A Swiss system draw should result in a clear winner, without having to play all opponents as in round robin, and without a single bad result terminating participation.\n\nThe first tournament of this type was a chess tournament in Zurich in 1895, hence the name \"Swiss system\".\nSwiss systems are commonly used in chess, Go, bridge, \"Scrabble\", backgammon, and many other games.\n\nThe first round is either drawn at random or seeded according to some prior order, such as rating (in chess) or recent performance. All participants then proceed to the next round in which winners are pitted against winners, losers are pitted against losers, and so on. In subsequent rounds, each competitor faces an opponent with the same, or almost the same, cumulative score. No player is paired up with the same opponent twice. In chess the pairing rules try to ensure that each player plays an equal number of games with white and black, alternate colors in each round being the most preferable, and a particular effort is made not to assign a player the same color three times in a row.\n\nDuring all but the first round, competitors are paired based on approximately how well (or poorly) they have performed so far. In the first round, competitors are paired either randomly or according to some pattern that has been found to serve a given game or sport well. If it is desired for top-ranked participants to meet in the last rounds, the pattern must start them in different brackets, just the same as is done in seeding of pre-ranked players for a single elimination tournament. In subsequent rounds, competitors are sorted according to their cumulative scores and are assigned opponents with the same or similar score up to that point. Some adjustments may be made to assure that no two players ever oppose each other twice, or to even out advantages a player may have due to chance.\n\nThe detailed pairing rules are different in different variations of Swiss system. They may be quite complicated, so to make the task easier, quicker and more accurate, the tournament organizer often uses a computer program to do the pairing.\n\nIn chess, a specific pairing rule, called \"Dutch system\" by FIDE, is often implied when the term \"Swiss\" is used. The Monrad system for pairing is commonly used in chess in Denmark and Norway, as well as in other sports worldwide. These two systems are outlined below.\n\nThe players are divided into groups, based on their score. Within each group with the same score, players are ranked, based on rating or some other criteria. Subject to the other pairing rules, the top half is then paired with the bottom half. For instance, if there are eight players in a score group, number 1 is paired with number 5, number 2 is paired with number 6 and so on. Modifications are then made to prevent competitors from meeting each other twice, and to balance colors (in chess). Players are sorted by score groups, ranked and top half paired to bottom half. It is suggested that for smaller fields score groups are not a suitable approach.\n\nThe players are first ranked based on their score, then on their starting number (which can be random or based on seeding). Then #1 meets #2, #3 meets #4 etc., with modifications made to ensure that other rules are adhered to. Players are sorted by score (not score groups) and original rank, then each player paired to the next opponent, typically excluding repeats. This is more suitable for smaller numbers of competitors.\n\nThe Monrad system used in chess in Denmark is quite simple, with players initially ranked at random, and pairings modified only to avoid players meeting each other twice. The Norwegian system has an optional seeding system for the first round pairings, and within a score group, the pairing algorithm endeavours to give players alternating colors.\n\nThis pairing system is widely used in Scrabble and it's known as the King Of The Hill format. It's considered to be distinct from the Swiss pairing system.\n\nThere is a fixed number of rounds. After the last round, players are ranked by their score. If this is tied then a tie-break score, such as the sum of all their opponents' scores (Buchholz chess rating), can be used: see Tie-breaking in Swiss system tournaments.\n\nAssuming no drawn games, determining a clear winner (and, incidentally, a clear loser) would require the same number of rounds as a knockout tournament, that is the binary logarithm of the number of players rounded up. Thus three rounds can handle eight players, four rounds can handle sixteen players and so on. If fewer than this minimum number of rounds are played, it can happen that two or more players finish the tournament with a perfect score, having won all their games but never faced each other.\n\nCompared to a knockout tournament, a Swiss system has the advantage of not eliminating anyone: so a player who enters the tournament knows that they can play in all the rounds, regardless of how well they do. The only exception is that one player is left over when there is an odd number of players. The player left over receives a bye: they do not play that round but are usually awarded the same number of points as for winning a game (e.g. 1 point for a chess tournament). The player is reintroduced in the next round and will not receive another bye.\n\nAnother advantage compared to knockout tournaments is that the final ranking gives some indication of the relative strengths of all contestants, not just the winner of the tournament. As an example, the losing finalist in a knockout tournament may not be the second best contestant; that might have been any of the contestants defeated by the eventual tournament winner in earlier rounds.\n\nIn a Swiss system tournament, sometimes a player has such a great lead that by the last round he is assured of winning the tournament even if he loses the last game. This has some disadvantages. First, a Swiss-system tournament does not always end with the exciting climax of a knockout final. Second, while the outcome of the final game has no bearing on first place, the first place player can decide who wins second or third prize. In the 1995 All-Stars tournament in Scrabble, tournament directors made the decision to pair David Gibson, who had by then clinched first place, with the highest ranked player who could not win a prize so that second and third ranked players could compete between themselves for the final placements. The Gibson Rule is optional in Scrabble tournaments as players at smaller tournaments may still have an incentive to win their last game to improve their overall rating. Players may also be Gibsonized if they have clinched a spot in the next round, and can be paired with the highest ranked player who cannot possibly qualify for the next round. \n\nA disadvantage compared to an all-play-all tournament is that, while the players finishing near the top are typically those with the best performances, and those finishing near the bottom are those with the worst performances, the players in the middle tend to be jumbled with little meaningful order. For example, at the 2010 European Chess Championship, players scoring 5½/11 had performance ratings ranging from to 2189 to 2559; such a difference suggests that the stronger-performing player would score more than 90% against the weaker-performing one. One player with a 2441 performance rating scored two and a half points better than one performing at 2518.\n\nIn Swiss system tournaments, the later rounds have a much greater bearing on the final results than the earlier rounds. In fact, it can even be an advantage to have a poor start to a Swiss system tournament because the player is then more likely to be paired against weaker opposition. Chess players colloquially refer to this as a “Swiss Gambit”.\n\nThe system is used for selection to the English national pool team. Sixty-four players start the tournament and after six rounds, the top player will qualify as they will be unbeaten. The remaining seven places are decided after a series of round robins and play-offs.\n\nCompared with a round-robin tournament, a Swiss can handle many players without requiring an impractical number of rounds. An elimination tournament is better suited to a situation in which only a limited number of games may be played at once, e.g. in tennis. In a Swiss system, all players can be playing at the same time.\n\nThe method of accelerated pairings also known as accelerated Swiss is used in some large tournaments with more than the optimal number of players for the number of rounds. This method pairs top players more quickly than the standard method in the opening rounds and has the effect of reducing the number of players with perfect scores more rapidly (by approximately a factor of 2 after two rounds).\n\nFor the first two rounds, players who started in the top half have one point added to their score for pairing purposes only. Then the first two rounds are paired normally, taking this \"added score\" into account. In effect, in the first round the top quarter plays the second quarter and the third quarter plays the fourth quarter. Most of the players in the first and third quarters should win the first round. Assuming this is approximately the case, in effect for the second round the top eighth plays the second eighth, the second quarter plays the third quarter and the seventh eighth plays the bottom eighth. That is, in the second round, winners in the top half play each other, losers in the bottom half play each other, and losers in the top half play winners in the bottom half (for the most part). After two rounds, about ⅛ of the players will have a perfect score, instead of ¼. After the second round, the standard pairing method is used (without the added point for the players who started in the top half).\n\nAs a comparison between the standard Swiss system and the accelerated pairings, consider a tournament with eight players, ranked #1 through #8. Assume that the higher-ranked player always wins.\n\nStandard Swiss system\n\nAfter two rounds, the standings are:<br>\n1: 2-0<br>\n2: 2-0<br>\n3: 1-1<br>\n4: 1-1<br>\n5: 1-1<br>\n6: 1-1<br>\n7: 0-2<br>\n8: 0-2\n\nAccelerated pairings\n\nAfter two rounds, the standings are:<br>\n1: 2-0 <br>\n2: 1-1 <br>\n3: 1-1 <br>\n4: 1-1 <br>\n5: 1-1 <br>\n6: 1-1 <br>\n7: 1-1 <br>\n8: 0-2\n\nAccelerated pairings do not \"guarantee\" that fewer players will have a perfect score. In round 2, if #5 and #6 score upset wins against #3 and #4, and there is a decisive result between #1 and #2, there will be three players with a perfect 2-0 score.\n\nThe Danish system works in principle like a Monrad system, only without the restriction that no players can meet for a second time, so it is always #1 vs. #2, #3 vs. #4 etc.\n\nBridge team tournaments, if not played as \"Round Robin\", usually start with the Swiss system to make sure that the same teams would not play against each other frequently, but in the last one or two rounds there is a switch to the Danish system, especially to allow the first two ranked teams to battle against each other for the victory, even if they have met before during the tournament.\n\nIn a few tournaments which run over a long period of time, such as a tournament with one round every week for three months, the Grand Prix system can be used. A player's final score is based on his best results (e.g. best ten results out of the twelve rounds). Players are not required to play in every round, they may enter or drop out of the tournament at any time. Indeed, they may decide to play only one game if they wish to, although if a player wants to get a prize they need to play more rounds to accumulate points. The tournament therefore includes players who want to go for a prize and play several rounds as well as players who only want to play an off game. \n\nA variant known as the McMahon system tournament is the established way in which European Go tournaments are run. This differs mainly in that players have a skill ranking prior to the start of the tournament which determines their initial pairing in contrast to the basic Swiss-system approach where all players start at the same skill ranking. The McMahon system reduces the probability of a very strong team meeting a very weak team in the initial rounds. It is named for Lee E. McMahon (1931–1989) of Bell Labs.\n\nA tournament system in Italy. It is similar to the Swiss System, but doesn't split players based on their score. Before pairing any round, players are listed for decreasing score / decreasing rating, and the opponent of the first player in the list is the player following him by a number of positions equal to the number of remaining rounds, and so on for the other players. As consequence of this, the difference in rating between opponents at the first round is not so big (as for the accelerated systems), and ideally the \"big match\" between the first and the second one should occur at the last round, no matter how many players and rounds are in the tournament.\n\nInternational Student Badminton Tournaments depend on the Swiss ladder system to ensure its players get as many challenging matches as possible over the course of the badminton tournament. The tournaments are meant to promote both the sport and the social aspect of the game, hence its results are not connected to external rankings. Beforehand, players can enroll in three or four categories designed to separate national, regional and recreational players. Players of different clubs are coupled to form doubles and mixed doubles. The starting positions on each ladder (singles, doubles and mixed doubles) are random. Unlike in official matches a 1-1 draw is possible and games are usually not extended after 21 is reached in order to maximise the number of played matches.\n\nThe Swiss-system has been used in Hardcourt Bike Polo. Currently Podium is the most frequently used for seeding a two-day tournament. The site allows you to track games and can be viewed from mobile devices at the tournaments or across the world.\n\nThe Swiss system is used in some bridge tournament events, usually team matches where a team consists of four to six players (two to three pairs). In each round, one team plays against another team for several hands, with the North/South pair(s) of one team playing against the East/West pair(s) of the other team. The same hands are played at each table, and the results at the two (or more) tables are compared using the International Match Point (IMP) scoring system. The difference between the total IMPs scored by the two teams in that round is converted by a formula to Victory Points (VPs), with typically 20 VPs shared between the two teams, depending on the IMPs difference. In the first round, teams are usually paired randomly however pairings can be based on other criteria. In subsequent rounds, the teams are ranked in order of the number of VPs they have accumulated in previous rounds, and the top team plays the second team, the third team plays the fourth team, etc., subject to the proviso that teams do not play each other twice. In the last one or two rounds there may be a switch to the Danish system to make sure that each team plays the final match according to its actual ranking, even if this results in some teams playing against an opponent a second time.\n\nIn chess, each player is pitted against another player who has done as well (or as poorly) as he or she has done. The first round is either drawn at random or seeded according to rating. Players who win receive a point, those who draw receive half a point and losers receive no points. Win, lose or draw, all players proceed to the next round where winners are pitted against winners, losers are pitted against losers and so on. In subsequent rounds, players face opponents with the same (or almost the same) score. No player is paired up against the same opponent twice, however. In chess, the rules also try to ensure that each player plays an equal number of games with white and black. Alternating colors in each round is the most preferable and the same color is never repeated three times in a row. Players with the same score are ideally ranked according to rating. Then the top half is paired with the bottom half. For instance, if there are eight players in a score group, number 1 is paired with number 5, number 2 is paired with number 6 and so on. Modifications are then made to balance colors and prevent players from meeting each other twice. The first national event in the United States to use the Swiss system was in Corpus Christi, Texas in 1945; and the first Chess Olympiad using it was held in Haifa in 1976.\n\nIn chess, the terms Swiss and Monrad are both used, and denote systems with different pairing algorithms. The Monrad pairing system is commonly used in Denmark and Norway, while most of the rest of the world uses one of the Swiss systems defined by FIDE. In most other sports, only one format is used, and is known either as Monrad or Swiss.\n\nCurling uses a variation called the Schenkel system.\n\nLike a Swiss tournament, the Schenkel ensures that after the first round teams will play against teams with similar levels of success so far. That means that after the first round the pairs for the second round would be first-ranked team against the second, third against fourth, and so on.\n\nIn a true Swiss-style tournament all teams play in one group. However, in a curling arena there are a limited number of curling sheets available at any one time. Therefore, the teams are usually divided into groups, and the groups are rearranged after a round or two.\n\nThe criteria used for ranking are, in order:\n\nBritish Parliamentary Style debate competitions have four rather than two teams in each debate. The preliminary round for many such competitions, including the World Universities Debating Championship, ranks teams by a modified form of Swiss tournament, usually called a tab. \"Tab\" also denotes to the software used for scheduling of rounds and tabulation of results. Teams are ranked from first to fourth in each debate and awarded from three down to zero points. Teams with similar points totals are grouped off for each successive round. Just as chess Swiss tournaments are arranged to ensure players have a balance of playing with black pieces and white pieces, so too debate tournaments attempt to provide teams with a balance of places in the speaking order (i.e. Opening Government, Opening Opposition, Closing Government, and Closing Opposition). With four competitors rather than two, significantly greater compromise is required to balance the ideal requirements of, on the one hand, a team not meeting the same opponent twice and, on the other hand, a team having a balanced mix of places in the running order.\n\nMind Sports South Africa, the national body for eSports in South Africa, uses a Swiss system for all its tournaments. For its Swiss implementations, players receive three points for a win and only one for a draw and no player can play against another player more than once. There is the further provision that no player may play against another player from the same club in the first round as long as no one club has 40% of the entrants.\n\nOn June 1, 2016 ESL announced that it would be using the Swiss system for its ESL One Cologne 2016 offline qualifier tournament. ELEAGUE also adopted this system for the ELEAGUE Major 2017 qualifier and the ELEAGUE Major itself. The format was also used for the \"Dota 2\" Kiev Major tournament in 2017. Overwatch Open Division also makes use of the Swiss system, as well as the Hearthstone Global Games tournament.\n\nRelatively few Go tournaments use the Swiss system. Most amateur Go tournaments, at least in Europe and America, now use the McMahon system instead. Swiss-system tournaments must start with very unequal matches in the early rounds—\"slaughter pairing\" is the name of one initial pattern used—if the Swiss pairing rules applied subsequently are to allow the top players to meet in the latest rounds. The McMahon system is designed to give all players games against similarly skilled players all along, and to produce final standings that more accurately reflect the true current skill levels of players.\n\nThe DCI, the tournament sanctioning body for the card game \"\", uses a Swiss system for most tournaments. Unlike with other Swiss implementations, players receive three points for a win and only one for a draw. After sufficient rounds to mathematically ensure that players with a record of one loss or better will be ranked in the top eight players, typically the top eight players advance to a single-elimination stage, with several statistics used as tie-breakers. The minimum number of players to top 8 are 16 or more, and top 4 with 8 players or more, and top 2 (if necessary) if they are 4 or more players.\n\nTournaments in the Pokémon Trading Card Game and Video Game Championships use a combination of the Swiss system and single-elimination. The tournament begins as a Swiss-system tournament. At the end of the Swiss rounds, the top players advance to a single-elimination tournament (also known as the Top Cut). In previous years, the Top Cut would include between 12.5 and 25 percent of the original number of participants (e.g. if there were 64 to 127 players, there would be a Top 16).\n\nAs of the 2013-2014 season, Swiss rounds in City, State, Regional, National, and World Championships are played best-of-three, with a 50-minute plus three-turn time limit. Ties were introduced into the Swiss round portion of the tournaments in the 2013-2014 season for the first time since 2002-2003. A win is worth 3 match points, a tie is worth 1 match point, and a loss is worth 0 match points.\n\nTop Cut rounds are played best-of-three, with a 75-minute plus three-turn time limit.\n\nAlso, the Regional and National Championships were contested as two-day Swiss tournaments, where only the top 32 players from Day One would continue in another Swiss tournament in Day Two, prior to a Top 8.\n\nLeague Challenge and Pre-Release tournaments are played solely as a Swiss system. Local tournaments may or may not have a Top Cut.\n\nThe tiebreakers are in the order of Opponents' Win Percentage, Opponents' Opponents' Win Percentage, Head to Head, and Standing of Last Opponent. The fourth tiebreaker will always result in the tie being broken.\n\nIn some Scrabble tournaments, a system known variously as \"modified Swiss\", \"Portland Swiss\", \"Fontes Swiss\" or \"speed pairing\" is used, whereby first players are placed in groups of four, and play three rounds of round-robin play, and subsequently are paired as in Swiss pairing, but using the standings as of the second to last round, rather than the last round. This has the advantage of allowing the tournament directors to already know who plays whom by the time given players are finished with a round, rather than making the players wait until all players have finished playing a given round before being able to start the time-consuming pairing process.\n\nCommonly used in Australia, and now in many other countries, is a system known as \"Australian Draw\". Whereby each round is paired using a normal #1 plays #2, #3 plays #4, etc. except that repeat pairings within a selected range of previous games is forbidden. Often, for shorter tournaments the selected range will be since the very first round of the tournament, thus never having a repeat pairing for the entire tournament. For longer tournaments it is also common to have the first N rounds use the Australian Draw system, and followed by one or more \"King Of the Hill\" rounds. \"King Of the Hill\" is a strict #1 plays #2, #3 plays #4, etc. with no regard to previous pairings, thus unlimited repeat pairings are allowed.\n\nAlthough labelled as 'Austrialian Version of Swiss Pairing' it is more akin to the King Of The Hill pairing system, than to the Swiss - Dutch system. As in chess, when the term Swiss Pairing is used, it's usually a reference to the Swiss Dutch System.\n\nWindmill Windup, a three-day yearly Ultimate Frisbee tournament held in Amsterdam, was the first event in ultimate to introduce the Swiss draw system into the sport in 2005. In later years, many other tournaments started using this format, like Belgium's G-spot, Wisconsin Swiss and many others.\nFor each round, teams earn victory points based on the score difference of their win (or loss). In this way, also a team clearly losing a game is encouraged to fight for every point in order to get more victory points. After each round, teams are ranked according to their victory points. Ties are broken by considering the sum of the current victory points of their opponents. In the next round, neighboring teams in the ranking play each other. In case they have played each other in a previous round, adjustments to the rankings are made. After five rounds of Swiss draw, the top 8 teams play three playoff rounds to determine the final placement of the teams. All other teams continue with the Swiss draw in the remaining rounds.\n\nThe International Wargames Federation, the international body for wargames, uses a Swiss system for all its tournaments. For its Swiss implementations, players receive three points for a win and only one for a draw and no player can play against another player more than once. There is the further proviso that no player may play against another player from the same country in the first round as long as no one country has 40% of the entrants. For national championships such rule is amended to read that no player can play against a player from the same club in the first round as long as no one club has 40% of the entrants.\n\nKonami Digital Entertainment of the United States uses proprietary software for their sanctioned and official tournaments. Konami Tournament Software (KTS) is what is supplied to the Tournament Organizers to run each tournament. The software utilized the Swiss system similarly to Magic: The Gathering--3 points for a win, 1 for a draw, 0 for a loss. \n\nKonami's official tournament policy dictates how many rounds are played based on the number of participants. After the set number of rounds of Swiss are complete, there is generally a cut to advance in the tournament. This is then played as single-elimination until a winner is declared.\n\n\n"}
{"id": "38526066", "url": "https://en.wikipedia.org/wiki?curid=38526066", "title": "Systems medicine", "text": "Systems medicine\n\nSystems medicine is an interdisciplinary field of study that looks at the systems of the human body as part of an integrated whole, incorporating biochemical, physiological, and environment interactions. Systems medicine draws on systems science and systems biology, and considers complex interactions within the human body in light of a patient's genomics, behavior and environment.\n\nThe earliest uses of the term \"systems medicine\" appeared in 1992, in an article on systems medicine and pharmacology by B.J. Zeng and in a paper on systems biomedicine by T. Kamada.\n\nAn important topic in systems medicine and systems biomedicine is the development of computational models that describe disease progression and the effect of therapeutic interventions. \n\nMore recent approaches include the redefinition of disease phenotypes based on common mechanisms rather than symptoms. These provide then therapeutic targets including network pharmacology and drug repurposing. Since 2018, there is a dedicated scientific journal, Systems Medicine, published by Marie-Ann Liebert and with Jan Baumbach and Harald Schmidt as co-editors in chief. \n"}
{"id": "20899373", "url": "https://en.wikipedia.org/wiki?curid=20899373", "title": "The Endless Forest", "text": "The Endless Forest\n\nThe Endless Forest is a multiplayer online game for Microsoft Windows by Belgian studio \"Tale of Tales\". In the game, the player is a deer in a peaceful forest without goals or the ability to chat. Pictograms above registered deer's heads represent their names. Players communicate with one another through sounds and body language.\n\nDuring \"abiogenesis\", every deer runs around, rubs against trees, or sits next to sleeping deer to cast Forest Magic on one another or party under a night sky filled with floral fireworks in a spectacle, created in realtime, by authors whenever the mood hits them. \"The Endless Forest\" can be run as a screensaver or an application.\n\nA notable aspect of \"The Endless Forest\" is the community involvement. The Tale of Tales website has a forum and a community site where many players post their ideas and feedback for the game, which can affect the game's development.\n\nThe creators of \"The Endless Forest\" strived to create an artistic game, a moving painting. Dubbed by its developers as a \"multiplayer online game and social screensaver\", \"The Endless Forest\" is not a mainstream MMORPG – it is highly unlike other MMORPGs The most notable difference between \"The Endless Forest\" and other MMORPGs is the lack of violence and human communication (talking) in-game..\n\nIf the player is registered, he or she will begin the game as a fawn. As a fawn, Forest Magic wears off quickly. In a little over a month, the deer grows up and becomes a stag. Now Forest Magic stays on as long as the deer would like it to.\n\nCommon activities in the Forest are exploring, using the Forest Magic and Forest Actions, sleeping, and swimming. There are also social activities that deer commonly engage in, such as dancing, pool parties, hide and seek, and tag. Deer can also be seen jumping over rocks and logs, or running in herds.\n\nAbiogenesis is a spectacle created by the realtime authors of the game. ABIOGENESIS knows no limits – rocks and cages fall from the sky, disco balls spin, lightning bolts flash, the sky changes color, and rainbows pop up out of nowhere. It is a system that allows Endless Forest authors Auriea Harvey & Michaël Samyn to play god. At any moment they can make it rain, make flowers grow and beasts fly. ABIOGENESIS happens either randomly, whenever they please, or in an organised event, during a live performance. It has also been known that a giant stag will come and visit during an ABIOGENESIS performance...\n\nForest Magic is the main way to change the appearance of one's avatar, but the player cannot change his or her own look (The exceptions being some antlers, and the pelt acquired at the Crying Idol.) Other deer must cast spells on them in order to change their look, and it is up to the player to communicate and ask others to change their appearance for them. Only adult deer can keep the spells cast on them, however, registered deer will have the magic cast on them saved for the next time they log on. The magic wears off on all fawns after a short period of time. Swimming in the pond causes a deer to lose all of the Forest Magic that has been cast on them.\n\nThere are three antler customizations that the player can get by themselves:\n\nThere are two pelts that the player can receive by themselves:\n\nThere is one animal that the player can shapeshift themselves into:\n\nThere are four additional factors that can affect a player's appearance:\n\nBecause \"The Endless Forest\" has no chat box, or any human means of communication, players must resort to an action strip along the bottom of the game. This strip includes many buttons. Deer display the actions provided in order to communicate with each other. These actions can be divided up into three types; basic actions, expressive actions, and forest magic actions. The basic actions are stand/lie down, bleat, listen, jump, and rub a tree. For expression actions there are two other buttons, a blue button (emotions) and an orange button (motions). The blue button includes emotes such as quivering in fear, cocking the head in confusion, bowing in thanks/greeting/respect, or hunching over in sadness. The orange button includes actions such as dancing, rearing up on the hind legs, and shaking the head yes/no. Forest magic actions include casting, obtaining, and discarding magic. When a deer gains the ability to cast a spell on another deer, a button pops up. Each form of magic has its own button that will pop up when the spell is obtained; pelt, antlers, mask, and shapeshifted. For example, if a deer gains the ability to change another deer's pelt color, a gold button with a deer imprint on it pops up in the action strip. If they press this button, they cast the spell. There is also a worship button, which can only be used when near the Twin Gods Statue (see Forest Magic) and a deer shaped button where you can remove the spells the avatar is wearing.\n\n\"The Endless Forest\" consists of seven main areas: \"First Forest\", \"Birch Forest\", the \"Ruin\", the \"Pond\", the \"Old Oak\", the \"Playground\" and the \"Twin Gods Statue\".\n\nThe \"First Forest\" was the first part of the forest to be created, as well as the only section originally released. Its flora consists of poppies, hyacinths, and ferns, while the fauna consists of doves, butterflies, fireflies, and squirrels. Butterflies are seen floating over the hyacinths in the daytime, while fireflies replace them when it becomes nighttime.\n\nThe \"Birch Forest\" (double the size of the First Forest) was introduced in Phase 3. It added more land to roam and explore, as well as new visuals and sounds. The Birch Forest consists of birch trees and sycamores, blue berries and little birds. With the addition of the Birch Forest, little birds can now also rest upon the deers antlers if the player is very quiet. The grass in the Birch Forest is much taller compared to that of the rest of the forest.\n\nThe \"Ruin\" was created as part of an exhibition in Phase 1. Exhibition curator Michel Dewilde invited Tale of Tales to contribute to the fourth edition of the yearly Ename Actueel group art exhibition (September 1, 2005 to October 30, 2005) organized by the Provincial Archeological Museum in the small town of Ename in Belgium. When it was first created, its main focus was on Tardis, a time-and-space-travel device featured in the \"Doctor Who\" television series.\n\nAfter that, the base of the ruin was introduced, resembling the archeological site of a gothic Saint Salvator church. During the exhibition visitors could steer a deer through the forest like the online players can. But when they stopped playing and the deer went to sleep, the soul of this visitor escaped and started floating through the forest. If the player found a soul, a new button would appear in the action strip which could be used to convert this pagan soul. If the players did a good enough job by the end of the day, a grave would be erected near the ruin. If the players failed and the majority of the remaining souls were still unchristened, a pagan idol would show up. Every day of the exhibition, a grave or an idol was added. The size depended on the number of visitors of that day. Now that the exhibition and Tardis is over, the ruins stand exactly as they have been created, left to be seen by the current forest players. Today nothing remains of the once large stone building but its foundations.\n\nThe \"Pond\" was released in Phase 1.5. It was the premiere location for the first ever ABIOGENESIS. Weeping willows, frogs, lily pads, the occasional flower, and some brightly coloured koi fish make up the pond location.\n\nFalling into the pond will cause any deer to lose all of its magic. Without careful footing, all of the players hard earned spells could be lost to the water.\nThe pond splits off into a small river, over which a small bridge has been built. One of the secrets of the forest is the idea of how the bridge came to being.\n\nFurther down the river is the Crying Idol, named for the waterfalls flowing from the statue's eyes. Little red poppies grow around the idol. Players will also get a treat if they end up passing into the idol. Running through the idol will cause the deer to turn red. This red pelt will prohibit other deer from casting more magic onto anyone who wears it, but can easily be 'sneezed' off when they want to get spells again.\n\nThe \"Old Oak\" is a large, hollowed-out tree where deer come to rest, relax and rejuvenate. It is a comfortable place. Often when it is raining in the Forest, deer can be seen sleeping in the warm Old Oak. The Old Oak's most notable trait is the low humming sound emitted when the player travels close enough to it.\n\nThe \"Playground\" was released in Phase 3, and was given its name by the players on the community forum. The Playground consists of three large boulders and many other rocks. Deer have the ability to walk, climb and jump over the rocks. If a deer sits quietly enough, little birds will perch on the deers antlers, but they will fly away upon player induced movement of the deer.\n\nSometimes candles appear on all three boulders, allowing the deer to place candles on their antlers.\n\nThere is a set of three suitcase-shaped rocks arranged in a triangle formation nicknamed \"The Deermuda Triangle\". The area's name was generated completely by two players, Griffinsong and Tae, and is not considered an official area of the forest. Some players claim that mysterious things happen in this area, while others have spent time there and come out without any harm done.\n\nThe Deermuda has become widely accepted by players. Game creator Michaël Samyn commented, \"We think that's great. It is exactly how we see storytelling in games: we plant some seeds but the players grow the flowers.\"\n\nThe \"Twin Gods Statue\" is central to all ABIOGENESIS performances. During an ABIOGENESIS the two halves of the statue separate and follow the creators' avatars though the forest. A player can activate the ABIOGENESIS camera, which focuses on the Twin Gods statue, by pressing the keys CTRL + W. In addition, if a player uses the 'pray' action at the Twin Gods statue the deer avatar receives a temporary devout pelt. Players will often leave their deer avatars sleeping next to the Twin Gods statue.\n\nDe Drinkplaats (The Watering Hole) is a magical place where deer gather to drink from an endlessly running source. When the player enters the ring of mushrooms, all pelts and masks are shed and the player becomes one with nature. The more players gather, the more forest animals come to witness the idyllic scene. Drinking from the water turns the deer into one of these animals, of varying sizes. Two new animals were introduced for De Drinkplaats: a white rabbit and a black crow.\n\nDe Drinkplaats was added to The Endless Forest in response to a commission by Christophe De Jaeger for the Fantastic Illusions exhibition. Fantastic Illusions is a show of work by contemporary Chinese and Belgian artists that refers to the romantic desire to step into a picture and become immersed in it (\"Die Sehnsucht, im Bild zu sein\"). The exhibition ran in the Shanghai MoCA from September to October 2009, where visitors played The Endless Forest on two computers installed especially for the exhibit. The installation opened in the Kortrijk Broelmuseum in November 2009. The Broelmuseum has a fine collection of 16th-century paintings by the Flemish painter Roelandt Savery, and one of Savery's paintings, entitled \"De Drinkplaats\" (The Watering Hole), served as the direct inspiration for this addition to the Forest. The actual shape of the fountain was taken from a real-life watering hole in the garden of the museum.\n\nMany objects in the forest are clearly sculpted from stone. Examples include the idol of the man at the source of the river, The Ruins, the graves, the cage beside the ruins, the bridge and the forest god statues. Who or what created these artifacts is a mystery. The game creators have yet to respond to queries into the origin of these items, or to the possibility of creatures in addition to the deer living out of sight.\n\nThe game's prototype was originally commissioned by the Musee d'Art Moderne Grand-Duc Jean, Luxembourg, in 2003. Phase One was first released on September 12, 2005. This Phase included the Ruins and the ability to hop. On February 7, 2006, the streaming beta and beta screensaver version 1.5 was released, with the special addition of ABIOGENESIS. On February 13, an update of the beta version, version 1.5 beta 12, was released. It was in this version that the Pond was first introduced into the forest. It was then that the first series of ABIOGENESIS occurred on the February 15, 16 and 18 during the ARTEFACT festival in Leuven.\n\n2 months later, Phase Two was launched on April 26, 2006 with greatly improved graphics and sound. With the release of Phase Two came the use of Forest Magic, the ability to change the avatar's appearance, a handful of emotes to express oneself, a giant tree known as the Old Oak, and an update to the ABIOGENESIS system. On September 29, 2006, version 2.1 was released and of the most important features of the game, the Fawn, was added. The Fawn was created for Pixel Me, a traveling art exhibition for young teenagers in Belgium. The show ran from October 2006 to November 2008, stopping in cultural centers across the country. On January 16, 2007 the artistic game project reached 10,000 registered players and had been downloaded over 64,000 times since its first release in September 2005. During this time \"The Endless Forest\" was being supported by Musée d’Art Moderne Grand-Duc Jean in Luxemburg, Vlaams Audiovisueel Fonds and Design Vlaanderen. On June 21, 2007 the players said goodbye to Phase 2 with the last ABIOGENESIS of the phase. It was during this Summer Solstice ABIOGENESIS that \"The Endless Forest\" experienced a record breaking attendance, over 80 deer at one time, much more than the little server could handle.\n\nThe Phase 3 beta was released in late August 2007, when the game jumped to over 100,000 downloads and 17,000 registered players, the number of active players per month exceeding 1,500. On September 24, 2007, Phase 3 was officially released for download. In this version, new players began playing as fawns for a little over a month, which after that they grew into full grown stags. It was also in this phase that two new areas were created; The Birch Forest and The Playground, consisting of a bunch of large boulders. Phase 3 was also the first to be created with Quest3D.\nOn October 24, 2007, \"The Endless Forest\" crossed 20,000 registered players and over 100,000 downloads.\nVersion 3.1 was released for Halloween, which consisted of 3 new sets (the Day of the Dead Set, the Crying Mask Set, and the Zombie Deer Set) so players could dress up for the spooky holiday. Version 3.2 was released in honour of Mardi Gras, where 8 new sets, designed by players and textured by students at the Howest college in Kortrijk, Belgium were added to the game. Since January 16, 2012, the latest Version 3.41 is available. \nOn March 9, 2008, \"The Endless Forest\" reached 25,000 registered players and was downloaded almost 200,000 times\nDespite the game's growing popularity, the funding was becoming a problem. The game's funding was denied by Flemish Audiovisual Fund again, it had been denied in Phase 3 as well, leaving Tale of Tales with no possible way of applying for funding in Belgium. Development of \"The Endless Forest\" was at a standstill while Tale of Tales worked on their new game, \"The Path\"—which itself has been partly supported by the Flemish Ministry of Culture, and was released on March 18, 2009.\n\nAuriea Harvey and Michaël Samyn have general plans for what \"The Endless Forest\" is to become. They plan to continue the development of \"The Endless Forest\" in a way that suits themselves and their players. All plans go through the players to get their feedback and reaction. The game is based hugely on community involvement.\n\nThe general plan states:\n\n\"There's two main facets to the interaction design of \"The Endless Forest\". On the one hand, it is sort of a magical deer-RPG or simulation game and on the other a stage for virtual performances (kind of an extreme form of Dungeon Mastering).\"\n\nYou are a stag\n\n\"Every player starts the game as a young male deer. An older female (your mother) will guide you through the beginning of the game. As you keep playing, you will grow and your antlers will take on a unique shape.\"\n\"There will be magical spells to acquire in the forest. A peculiarity of these is that you cannot cast them on yourself. Only on other deer. These spells can make the game a lot more interesting, so it will be important to find someone who can cast them on you.\"\n\"Your deer avatar will live for a full year. If you want to continue playing with all the abilities you've gathered, you will need to procreate. There are plenty of attractive hinds in the forest. Perhaps you can persuade them. But better make sure you chase away your competitors with those awesome antlers of yours.\"\n\nWe are god\n\n\"Every once in a while we, Auriea Harvey & Michaël Samyn, the authors and directors of this project, will intervene with one thing or another. Sometimes this may be an elaborate intrusion or addition that impacts life in the forest dramatically. At other times, small things will be added. This way, the forest will remain a living universe where you never know what to expect.\"\n\"There will also be live events. Often things that are tied to real-life performances or exhibitions. Usually a remainder of these occasions will be left in the forest, but it will be most interesting to witness and participate in the event when it happens.\"\n\nSince its first release, \"The Endless Forest\" has received much positive feedback from the public. \"The Endless Forest\" has been invited to exhibitions such as Le Cube Festival, Game/Play, Bradford Animation Festival, IETM Autumn Plenary Meeting, VELOCITY festival, Play Cultures, Els límits de la natura (\"The Limits of Nature\") at Centre d'Art La Panera, Gameworld at LABoral Centro de Arte y Creación Industriale, Night Garden at Mediamatic, Mediaterra, Pixel Me, Edge Conditions at the San Jose Museum of Art, and more. In addition, \"The Endless Forest\" has been featured in many magazines, such as GEE magazine, Germany, as well as the Pittsburgh Post-Gazette. Game also has been featured in TV programs, such as \"Bez Vinta\" on Gameland TV. Other remarks center upon the game's unfinished nature, as well as hope for what may lie in store for the future.\n\n1 in 5 players discovered the game through fanart posted on deviantART.\n\n\"The Endless Forest\" has received negative feedback relating to its lack of traits considered essential to MMO games, and to video games in general.\n\n\nOther sites\n"}
{"id": "26766870", "url": "https://en.wikipedia.org/wiki?curid=26766870", "title": "Torture trade", "text": "Torture trade\n\nIn 2001, Amnesty International released the report \"Stopping the Torture Trade.\" The term torture trade refers to the manufacture, marketing, and export of tools commonly used for torture, like restraints and high-voltage electro-shock weapons.\n\nHigh-voltage electro-shock weapons were first developed in the US in the 1990s. They include electro-shock batons, stun guns, stun shields, dart-firing stun guns, and stun belts. From 1997 to 2000, US companies earned over $13 million exporting stun guns, electro-shock batons and optical sighting devices to Eastern Europe and the Middle East. More than 150 companies worldwide are involved in the manufacturing or marketing of torture devices, almost half of which are in the US.\n\nThe biggest electro-shock manufacturers are located in the US, mainland China, Taiwan and South Korea. Companies that produce electro-shock weapons, restraints and sprays say their products are nonlethal if used by security officials with proper training. Nonetheless, Amnesty International has documented cases of companies selling stun belts to countries who Amnesty International suspects of committing human rights abuses, like China and Saudi Arabia, without providing training.\n\nThe following table includes some of the countries identified by Amnesty International from 1998-2000 as engaged in the manufacture, distribution, supply, or brokerage of stun weapons and restraints.\n\nOne type of electro-shock weapon is the remote-controlled stun belt. Stun belts send 50,000 volt shocks through the victim using electrodes placed near the kidneys. The shock causes incapacitation and severe pain.\n\nElecto-shock weapons are one of the most common tools of torture. Electro-shock weapons are appealing because they leave no mark, although the physical and psychological effects are crippling. Shocks are often applied to sensitive areas like the soles of feet or genitals. Effects include severe pain, loss of muscle control, nausea, convulsions, fainting, and involuntary defecation and urination. Internationally, electro-shock torture is used on children, pregnant women, and other vulnerable populations.\n\nAmnesty International has asked companies worldwide to stop the manufacture, marketing, and trade of electro-shock and restraint devices; governments to ban the trade of torture devices; and individuals to write local government representatives and companies asking them to take these steps. The Amnesty International campaign focuses on the trade of restraints, pepper sprays and electroshock weapons.\n\nIn the European Union, Regulation No. 1236/2005, in effect since 2006, prohibits trade in goods which have no practical use other than torture, and requires licences for the export of goods which could have a use in torture as well as other legitimate uses. Critics say the regulation contains too many loopholes to be effective. Commission Implementing Regulation (EU) No 775/2014 lists prohibited and controlled goods. A proposal for amendments to Council Regulation (EC) No 1236/2005 was put forward by the European Commission on 14 January 2014 and approved by the European Parliament on 30 June 2016. The new regulation will ban the brokering of equipment which is subject to a ban and the supply of technical assistance regarding the supply of banned goods.\n\nThe US has also made regulatory changes to limit torture trade. The Department of Commerce created a separate export commodity code for electro-shock devices to make it easier to track them. All companies are now required to have export licenses, although there are still many loopholes. US companies can use drop shipping or paying an intermediary country with loose regulations to export banned goods to the importing country. In 1997, one US company was caught exporting electro-shock guns and pepper spray without a license by mislabeling them as “Fountain pens, Keychains, Child Sound device, [and] Electrical voltage units.”\n"}
{"id": "47402451", "url": "https://en.wikipedia.org/wiki?curid=47402451", "title": "Uthapuram", "text": "Uthapuram\n\nUthapuram is a village in Madurai district, Tamil Nadu, India. It is known for a wall which segregated Dalits from the village for two decades.\n\nUthapuram is located in Peraiyur taluk of Madurai district. It is 41 kilometers away from the district capital Madurai. As of census 2011, it has population of 5149 of which scheduled caste consists 2172. It comes under the Usilampatti assembly constituency and Theni parliamentary constituency.\n\nThe village have two major communities, one is dominant caste Hindu Pillai another one is scheduled caste Dalit Pallar community. The caste violence occurred occasionally between these communities. The violence happened in 1948, 1964 and 1989. After 1989 violence caste Hindus constructed a 30 meter long wall to segregate Dalits from the village. The wall later described as \"wall of untouchability\". The Dalits were not allowed to enter the streets of caste Hindus. Following this, Tamil Nadu Untouchability Eradication Front (TNUEF) of Communist Party of India (Marxist) and Dalit organizations protested against the construction of the wall. The caste Hindus argued that the wall was built on the private land. In May 2008, Madurai district administration demolished the wall and allowed the Dalits into the village. After demolition, violence occurred between the communities. The caste Hindus threatened that they would leave the village. To control the situation police opened fire in that one person died. The cases were filed against the incident. In 2012, Madras High Court ordered compensation to the people who suffered during the violence.\n\nThe district administration initiated the peace agreement between the communities. Therefore, the Dalits were allowed to enter local Muthalamman-Mariamman temple who prevented since 1989.\n\n"}
{"id": "8188259", "url": "https://en.wikipedia.org/wiki?curid=8188259", "title": "Vis medicatrix naturae", "text": "Vis medicatrix naturae\n\nVis medicatrix naturae (literally \"the healing power of nature\", and also known as \"natura medica\") is the Latin rendering of the Greek \"Νόσων φύσεις ἰητροί\" (\"Nature is the physician(s) of diseases\"), a phrase attributed to Hippocrates. While the phrase is not actually attested in his corpus, it nevertheless sums up one of the guiding principles of Hippocratic medicine, which is that organisms left alone can often heal themselves (\"cf.\" the Hippocratic \"primum non nocere\").\n\nHippocrates believed that an organism is not passive to injuries or disease, but rebalances itself to counteract them. The state of illness, therefore, is not a malady but an effort of the body to overcome a disturbed equilibrium. It is this capacity of organisms to correct imbalances that distinguishes them from non-living matter. \n\nFrom this follows the medical approach that “nature is the best physician” or “nature is the healer of disease”. To do this Hippocrates considered a doctor’s chief aim was to help this natural tendency of the body by observing its action, removing obstacles to its action, and thus allow an organism to recover its own health. This underlies such Hippocratic practices as blood letting in which a perceived excess of a humor is removed, and thus was taken to help the rebalancing of the body's humors.\n\nAfter Hippocrates, the idea of \"vis medicatrix naturae\" continued to play a key role in medicine. In the early Renaissance, the physician and early scientist Paracelsus had the idea of “inherent balsam”. Thomas Sydenham, in the 18th century considered fever as a healing force of nature. \nIn the nineteenth-century, \"vis medicatrix naturae\" came to be interpreted as vitalism, and in this form it came to underlie the philosophical framework of homeopathy, chiropractic, hydropathy, osteopathy and naturopathy. As Bynum notes, \"Search the Internet for vis medicatrix naturae and you will find yourself in the land of what we now politely call 'alternative' or 'complementary' medicine\".\n\nWalter Cannon's notion of homeostasis also has its origins in \"vis medicatrix naturae\". \"All that I have done thus far in reviewing the various protective and stabilizing devices of the body is to present a modern interpretation of the natural vis medicatrix.\". In this, Cannon stands in contrast to Claude Bernard (the father of modern physiology), and his earlier idea of milieu interieur that he proposed to replace vitalistic ideas about the body. However, both the notions of homeostasis and milieu interieur are ones concerned with how the body's physiology regulates itself through multiple mechanical equilibrium adjustment feedbacks rather than nonmechanistic life forces.\n\nMore recently, evolutionary medicine has identified many medical symptoms such as fever, inflammation, sickness behavior, and morning sickness as evolved adaptations that function as \"darwinian medicatrix naturae\" due to their selection as means to protect, heal, or restore the injured, infected or physiologically disrupted body.\n\n"}
