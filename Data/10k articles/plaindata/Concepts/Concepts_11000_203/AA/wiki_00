{"id": "11522707", "url": "https://en.wikipedia.org/wiki?curid=11522707", "title": "2007 tuberculosis scare", "text": "2007 tuberculosis scare\n\nThe 2007 tuberculosis scare occurred when Atlanta personal-injury lawyer Andrew \"Drew\" Speaker flew from Atlanta, Georgia to Paris, France and on to Greece and then Italy before returning on a flight from Prague, Czech Republic to Montreal, Canada, where he crossed over the border and back into the United States while infected with multi-drug-resistant tuberculosis. The Centers for Disease Control and Prevention believed at the time that Speaker was suffering from extensively drug-resistant tuberculosis (XDR-TB). The incident sparked a debate in Congress on the failure of federal customs agents to stop him. Upon Speaker's return to the United States, the CDC placed him under involuntary isolation (similar to quarantine) using a provision of the Public Health Service Act. With this action, Speaker became the first individual subjected to a CDC isolation order since 1963.\n\nIn January 2007, Speaker suffered a fall and went to the doctor, concerned that he had bruised a rib. Doctors X-rayed his chest and found an abnormality that required further testing. Andrew Speaker was suspected of having TB when a positive PPD test came back on March 2, 2007. His third CT scan was done on March 3 and a bronchoscopy was done on March 8. After 18 days of incubation the isolate was sent to CDC for confirmation of his susceptibility results that were done by the Georgia Department of Human Resources (DHR).\n\nOn March 28, 2007 his doctors and the health department believed the TB strain Speaker had was a resistant one and communicated this to the CDC. On May 1 the apparent MDR TB infection was discussed with the CDC lab by his doctors and they discussed discontinuing the treatment he was on at that time. On May 9 the suspicion of MDR TB was confirmed. A meeting was held with Fulton County Health Officials, his doctors, his fiancée and his father and father-in-law on May 10, 2007. At this time he was told that he was not contagious and not a threat to anyone but that he would need to go to Denver for treatment. It would take a few weeks to arrange this. He was advised, or according to some accounts strongly recommended, not to travel.\n\nOn May 12, 2007, Speaker flew from the U.S. to Paris. On May 14, he flew on to Athens and, two days later, flew to the Aegean holiday island of Santorini for his wedding (Santorini's Mayor Angelos Roussos stated that Speaker lacked the necessary paperwork for the civil ceremony.). Speaker then flew to Rome for his honeymoon.\n\nDoctors say that only after Speaker left the United States did they realize he likely had XDR-TB. Speaker says that he was informed of MDR TB before leaving the country, and that while officials preferred him not to fly, they said that he was not a threat and was not required to wear a mask. Once Speaker was in Europe, however, test results showed his strain of tuberculosis was even rarer than originally thought, leading public health officials to try to persuade Speaker to turn himself in to Italian health authorities. The CDC informed him that there were no options for the CDC to get him home, and that he would have to arrange private transportation. Speaker instead flew by commercial jet to Prague and then on to Montréal. Both Speaker and his new wife claimed that, had they been offered transport, they would have accepted it and would have waited in Rome. Speaker has also said that the CDC told him they were going to send officials to put him in Italian quarantine for up to two years, and that he was not told special transportation was arranged.\n\nOnce in Montréal, Speaker rented a car and drove across the Canada–United States border. A Customs and Border Protection Officer failed to detain him at the frontier, disregarding a warning after he had passed Speaker's passport through the Treasury Enforcement Communications System (TECS) to hold the traveler, wear a protective mask when dealing with him, and call health authorities because he \"did not look sick\".\n\nAccording to the CDC, Speaker flew on the following flights:\n\nOn May 31, 2007, Speaker was moved from Grady Memorial Hospital in Atlanta to the National Jewish Medical and Research Center in Denver, Colorado, for further treatment.\n\nIt was reported that Speaker's father-in-law, Robert C. Cooksey, works for the Centers for Disease Control and Prevention and is a microbiologist who has conducted research on tuberculosis, according to his CDC biography posted on the agency's Web site.\n\nWearing a medical mask, Speaker was interviewed by Diane Sawyer on the June 1 edition of the American talk show \"Good Morning America\" on ABC and apologized to all passengers, explaining that he had not intended to endanger them.\n\nAccording to an interview on \"Larry King Live\", Speaker said that he had not been told that there was any risk of transmitting the disease to others, nor did the May 10 letter recommending against his travel state this, which Speaker in any case had not received before leaving May 14. His wife, with whom he lived for five months without precautions, remained uninfected.\n\nOn July 4, 2007 the National Jewish Medical and Research Center announced, and the CDC confirmed, that Mr. Speaker's earlier diagnosis was incorrect and that he instead had multi-drug-resistant tuberculosis (MDR-TB), a more treatable form of tuberculosis.\n\nBefore a Congressional hearing, Speaker and his father played audio recordings from CDC and Fulton County health officials which say he was not a danger to others. He asked such questions on five recordings repeatedly and was given the same answers even after stating on two recordings that he was going out of the country and the CDC later admitted they were aware and waited until he had already left before taking further actions.\n\nSpeaker was in New York when the CDC served him with an isolation order but CDC director Julie Gerberding stated that the government was legally constrained prior to that order. The federal statute granting quarantine authority allows isolation or quarantine but only for individuals coming into the country from a foreign country or territory.\n\nGeorgia TB law may have required Speaker to be confined for two weeks and only allowed travel for medical appointments. A court confinement order can isolate a patient only after the infected patient ignores medical advice. This method can be overridden by a declaration of public health emergency by the governor of Georgia.\n\nIn 2007, seven Canadians and two Czechs sued Speaker in Montreal Superior Court; eight of the plaintiffs were on the same flight as Andrew Speaker and one was related to one of the passengers.\n\n\n"}
{"id": "3348155", "url": "https://en.wikipedia.org/wiki?curid=3348155", "title": "Amir Ali Ghassemi", "text": "Amir Ali Ghassemi\n\nAmirali Ghasemi (Persian: امیر علی قاسمی) is an Iranian independent curator, media artist, and graphic designer. He is the founder and director of Parkingallery, an art space in Tehran that has established itself as an accessible platform for young Iranian contemporary artists. He has worked at both art production and curating with the intention of showing aspects of Contemporary art in Iran that do not fall into the trope of what he calls, ‘’Chador art’’ or stereotypes of life in Iran packaged for foreign consumption.\n\nAmirali Ghasemi was born in Tehran into a cultural environment. His grandparents published \"Arash\", a political and cultural commentary magazine in the 1960s, while his parents work in journalism and social communication. Amirali studied graphic design at Islamic Azad University Central Tehran Branch receiving a BFA in 2004 and pursued further studies at the \"Institute for Art in Context\" at the Universität der Künste in Berlin.\n\nAmirali Ghasemi's best known work is his 2006 piece, \"Tehran Remixed : Party series\", Ghassemi expressed through a series of photographs of a side of Tehran life different from what he felt were prejudicial images often produced in international media on a mass scale. The series is an \"intimate, real-life\" depiction of party culture and social life in private lives of Iranian youth. Ghasemi masked faces so that Iran Reformists could not use his images as an example of how Iranian society was progressing and at the same time prevented the conservatives from using his work to clamp down on what they perceived to be un-Islamic activities.\n\nIn 1998, Ghassemi converted his parents' garage into an art studio which over the years transformed to \"a workshop space, then a gallery, with turns as a catwalk, lecture hall, and screening space\" and finally Parkingallery that quickly became an \"energetic hub\" in Tehran's art scene. In 2002, an online presence was established to showcase the works of young Iranian contemporary artists. In 2014 the physical space was closed but the gallery has continued to exist as an online resource. In the summer of 2014, Ghasemi co-founded \"New Media Society\", a project space, library and video archive in downtown Tehran.\n\nIn 2008, Amirali Ghasemi along with Serhat Köksal organized Urban Jealousy, the 1st International Roaming Biennial of Tehran an itinerant exhibition that drew considerable attention. The same year, Ghasemi was invited to organize an exhibition of contemporary Iranian art at \"Culturcentrum Bruges\" in Belgium. Faced with such an important curatorial project Ghasemi found it necessary to challenge clichés and bypass the stereotypical exhibition of Iranian art created and intended for export. \"Iran & Co\" presented 11 emerging artists in a performance/exhibition project. The premise of the project by Ghasemi’s team was to create a company, \"Iran & Co\" that operated as a business that ordered Iranian art from abroad and had it produced there in the manner that artists from wealthier countries have regularly worked with craftsmen or technicians from underdeveloped countries. Of course, it was the other way round in Bruges, Antwerp and London where \"Iran & Co\" presented technicians and artists from economically strong countries who created and produced work for Iranian artists. In this manner, \"Iran & Co\" departed from the so-called “Iran Boom” that since, 2005-2006 led to hundreds of exhibitions and publications dealing with art from Iran and the Middle East.\n\nDewilde, Michel (2011) “Looking beyond the horizon, Iran & co, 24 October-2 November 2010” in \"Art Tomorrow\". Issue 3. Tehran: Publisher Nazar<br>\nIssa, Rose and Bhabha, Homi Eds. (2009) \"Iranian Photography Now\". Berlin: Hatje Cantz<br>\nKarimi, Pamela (Spring 2010) “When global art meanders on a magic carpet: A Conversation on Tehran’s Roaming Biennial“ in \"The Arab Studies Journal\". Washington, D.C.: Vol.18, No.1. p. 294<br>\nKhatib, Lina (2012) \"Image Politics in the Middle East: The Role of the Visual in Political Struggle\". London: I.B.Tauris<br>\nOrlotti, Marianna (April 15, 2015) “Iran & Co. an Iranian Curator Diaries”. \"Curate Archive\"<br>\nZanganeh, L.A. (2006) \"My Sister, Guard Your Veil; My Brother, Guard Your Eyes: Uncensored Iranian Voices\". Boston: Beacon Press, p. 107\n\n"}
{"id": "28365380", "url": "https://en.wikipedia.org/wiki?curid=28365380", "title": "Bicultural identity", "text": "Bicultural identity\n\nBicultural identity is the condition of being oneself regarding the combination of two cultures. The term can also be defined as biculturalism, which is the presence of two different cultures in the same country or region. As a general term, culture involves the behaviors and belief characteristics of a particular social, ethnic, or age group. Within culture, we have cultural effects, which are the shared behaviors and customs we learn from the institutions around us. An example of a cultural effect would be how an individual’s personality is strongly influenced by the biological and social norms he is exposed to. Another cultural effect would be that in some societies it would be more acceptable to dress or act in a certain way.\n\nIn regards to bicultural identity, an individual may face conflict assimilating into both cultures or finding a balance between both. An individual may face challenges assimilating into the whole, collective culture. Similarly, an individual may face difficulty balancing their identity within themselves due to the influence of both of their cultures. Being an individual with identity plurality can be hard mentally and emotionally. The different levels of biculturalism can be defined though the way people are able to simultaneously manage their two selves. The more they alternate between them, the more cognitive complexity they face, since they avoid cultural duality and do not practice handling both cultures at the same time. It is through identity integration that they will be able to solve the problem and alleviate the tolls that come with identity plurality Bicultural identity also may have positive effects on the individual, in terms of the additional knowledge they acquire from belonging to more than one culture. Furthermore, with the growing number of racial minorities in American society, individuals that identify with more than one culture may have more linguistic ability.\n\nCulture affects the personality of an individual because the individual may react in a way that is reflective of the knowledge one acquires from one or more culture(s). Problems may arise when ideals in one culture are not connected to another culture, which may cause generalizations about personality. Personality is shaped by both cultures and thus generalizations should not be made based on one single culture. One’s culture also influences one’s hormonal changes, one’s interaction with violence and one’s family values. For example, Hispanic American culture often requires older children to take care and/or help raise younger siblings, while mainstream American culture interprets parents as the sole caregivers. Another example of this difference would be religious preference and or practice. Cultures other than the American culture may often identify more with certain religions and are often more in tune with their religious beliefs.\n\nOne construct to measure bicultural identity is the Bicultural Identity Integration (BII) construct. It is a relatively new construct and was proposed in 2002 by Benet-Martínez, Leu, Lee & Morris. The BII looks at how the bicultural individual perceives his bicultural identities and whether they are compatible or oppositional. It also seeks to identify the big five aspects of an individual’s personality, including aspects such as sociability, activity and emotionality. The BII seeks to find whether an individual has a cultural distance or conflict within one’s cultures, which in turn helps indicate how biculturally competent we are.\n\nLow BII bicultural individuals have difficulties in incorporating both cultures into a cohesive identity and tend to see both cultures as highly dissimilar. Bicultural individuals with high BII on the other hand, see their identities as complementary and themselves as part of a “third” culture, which integrates elements from both their cultures.\nAccording to Margaret Mead, a cultural anthropologist, individuals respond in a more stable fashion when their cultural contexts are understood. \nResearchers wanted to examine how these differences could relate to other factors and the results are insightful. BII is significantly associated with the psychological and social adjustments of the bicultural. Low BII bicultural individuals are found to have inferior bilingual proficiency, experience more anxiety, depression and are more neurotic and less open than bicultural individuals with high BII.\nMore importantly, low BII bicultural individuals are not chameleon-like. They resist the frame switching and are more likely to respond in ways inconsistent with the cultural cues. In other words, when low BII Chinese-Americans are presented with American cues, unlike high BII bicultural individuals, they would not behave like Americans but instead, more like a Chinese.\nHowever, the identity struggle for bicultural individuals can be made less arduous. It is important to note that like other personality traits, BII is malleable to contextual factors. BII can be increased by asking bicultural individuals to recall positive cross-cultural exchanges or like in another study, make high-level construals. These findings can be useful in for example, helping immigrants to cope with their new environment.\n\nLanguage is an essential aspect of any culture. Individuals are able to maintain key aspects of their culture by maintaining their culture’s language. Language is important because it is an oral form of how people interact with other people within a society. Language reinforces the ties among the people who speak the same language, and thus encourages cultural bonding. Thus, by preserving the language within both of one’s cultures, one can maintain one’s integration within each culture. However, this can result in a difficulty in integrating one’s cultures if each has a distinct, different language as it can prevent outsiders from understanding that particular culture.\n\nThe concept of cultural frame switching (CFS) or double consciousness made popular by W.E.B Du Bois addresses how an individual switches between cultural frames or systems in response to their environment. The presence of culture-specific peers can elicit culture-specific values. CFS can be used to describe the switching of different language use depending on the context. Thus, CFS can be connected to cultural accommodation, which is seen when bilinguals respond to situations with the language that applies best to the situation present. \nIt is evident that language can have an effect on an individual’s thinking process; this is because the language itself primes the individual’s cultural values, attitudes and memory which in turn affects behavior. Thus, language has a powerful effect on the way in which an individual responds to change.\n\nAfrican American culture is also known as black culture in the United States and the identity of African American culture is rooted in the historical experience of the African American people. It is rooted in Africa, and is a blend of sub-Saharan African and Sahelean cultures. Due to aspects of African American culture that were accentuated by the slavery period, African American culture is dynamic. \nWithin the African American culture, race or physical differences led to mass murder, and violence against racial groups. These occurrences may affect an individual’s perception of their African American culture. \nIn America, Black and White differences are the most significant groupings largely because of American history. The US was founded on the principle of “all men are equal” and yet slavery existed. This is what resulted in the American Dilemma. Thus, due to historical reasons, and because they are often stereotyped, African Americans have difficulty assimilating with their culture and American culture.\n\nIndividuals having origins within the Far East, Southeast Asia or the Indian subcontinent are referred to as Asian under the U.S. Census Bureau. Asian American complete 4.8% of the U.S. population alone. Asian Americans have had the highest educational attainment level and median household income of any racial demographic in the country and attain the highest median personal income overall, . Thus, Asian American Culture is often depicted as the most similar culture to American Culture. \nAsian Americans often communicate non-verbally and/or indirectly, and often are not as bold or upfront as other cultures in terms of their communication. The Asian American way of life is much more group-oriented or holistic and thus the way in which they interpret the world is systematically different from American Culture in terms of thought process and lifestyle. This may make it difficult for Asian Americans to assimilate easily into American culture.\n\nHispanic and Latino Americans have origins in the countries of Latin America and the Iberian Peninsula consisting of Spain and Portugal. Hispanic Americans are very racially diverse. Hispanics constitute 16.5% of the total United States population. \nHispanic Americans often are very religiously oriented and focus on family values and the importance of intergenerational connections. This may cause difficulty in integration with American culture, as the Hispanic community often emphasizes the importance of helping one’s family and advancing as a family rather than simply individual success, which is more prominent within American Culture. Similarly, Hispanics may have difficulty associating with American Culture because of the language culture, as most Hispanics can speak Spanish. The ability to speak Spanish is valued greatly within Hispanic culture, as it is greatly used during social gatherings and amongst extended family. The Spanish language is a significant part of Hispanic culture, and because of the vast amount of racial differences within Hispanic Americans, the way in which Spanish is spoken within the different racial groups is often different. This makes it not only difficult to assimilate into American culture but to often assimilate with the different races in Hispanic America.\n\nImmigrants particularly find it difficult to assimilate both their cultural contexts. Immigrants need to reconcile both their current host cultures and their culture of origin, which is where they grew up. Immigrants culturally evolve through a process of adaptation and assimilation. Immigrants are usually influenced by more dominant values that they have learned in their native cultures. Immigrants encounter a major upheaval by moving far away from home and sometimes may never find themselves connected to either culture. Immigrants face many stresses, which can raise their risk for substance abuse and other psychological stressors. Developing a bicultural identity is involves blending two cultures together and learning to be competent within their two cultures. Immigrants and children of these individuals may be more at risk for victimization, poverty, and the need for assistance from the government. Immigrant parents for example may struggle to find a balance in their new lives and may be so busy keeping up with the demands that may be less involved in the community and in turn less involved with their child’s education.\n\nWith immigrants, language barriers may also bring hardship in terms of communication with natives of their less dominant culture. Immigrants may not adapt fully because of the language barriers holding them back from even simple conversation. Acculturation is the process in which a bicultural individual or immigrant adopts the social norms of the mainstream society. The cultural gap between immigrant parents and their children may widen due to acculturation because younger generations find it easier to adapt to the new culture. Family relations may be strained due to this issue. Children of immigrant parents may enjoy more mainstream culture, but may also want to stick to their families’ roots in order to please their caregivers. Immigrants and bicultural families do have more positive roles as well. They have strong commitments to family and have a dream for a better life. This in turns gives families a sense of purpose and connection and makes the family unit stronger. Native customs such as holidays and religious affiliations may also support the family unit and promote unity all around.\n\nIndividuals with bicultural identity face issues around stereotype threat. Others may be perceived negatively, or their judgments may in turn alter the way that one behaves in certain situations. For example, with standardized testing, African American students in low-income areas often do worse on a given test due to the expectations for them to do worse. Stereotype threat is so powerful that it may extend on to different areas of life, such as the workplace. It is a multidimensional concept that may affect an individual on many levels. Stereotype threat makes it harder for individuals to integrate successfully with their peers if they feel judged or feel pressures to exceed in certain ways especially if their dual cultural roles may be in conflict with one another. Thiese scenarios are contingent on an individual's success with acculturation strategies.\n\nA bicultural individual's integration into a workplace also depends on the cultural makeup of his or her team. A team can be categorized as culturally homogenous, culturally diverse, or possessing a cultural faultline. A bicultural is more likely to integrate with a team, possessing the skills to form a cultural attachment with homogenous or heterogenous teams by traversing cultural barriers.\n\nCaregivers also face a dilemma with their children who have bicultural identities; they want to instill pride in their children, but also must prepare their children for prejudice without making them feel inferior to other cultural groups. For example, African-American parents must socialize their children in such a manner where they will be prepared to face discrimination in society, but they also must preserve their culture in such a way that makes them feel prideful. This dilemma that parents face makes it harder for individuals to feel comfortable within social groups and may minimize the different cultures that individuals surround themselves with. Some individuals can develop a more multicultural outlook and feel confident being around many kinds of people, whereas others may have an issue with this and may stick to their own cultural group.\n\nAcademics within individuals with bicultural identity may also be aversely affected in terms of stereotype threat. An individual may lose motivation in a scholastic setting due to the negative expectations placed on them. Attitudes may change within academics if a student feels as though he cannot do well due to societal constraints on his particular culture. Although this may discourage some, specific tests have been made in order to integrate culture within standardized testing.\n\nA system created by Jane Mercer, assumes that test results cannot be distanced from the culture and it focuses on comparisons among people within particular culture groups rather than between culture groups. This system has been applied to intelligence and ability examinations in order to combat the concern of disadvantaged minorities doing poorly due to their incapacity to do as well as their counterparts.\n\n\n"}
{"id": "4116", "url": "https://en.wikipedia.org/wiki?curid=4116", "title": "Big Bang", "text": "Big Bang\n\nThe Big Bang theory (also described as the Everywhere Stretch) is the prevailing cosmological model for the observable universe from the earliest known periods through its subsequent large-scale evolution. The model describes how the universe expanded from a very high-density and high-temperature state, and offers a comprehensive explanation for a broad range of phenomena, including the abundance of light elements, the cosmic microwave background (CMB), large scale structure and Hubble's law (the farther away galaxies are, the faster they are moving away from Earth). If the observed conditions are extrapolated backwards in time using the known laws of physics, the prediction is that just before a period of very high density there was a singularity which is typically associated with the Big Bang. Physicists are undecided whether this means the universe began from a singularity, or that current knowledge is insufficient to describe the universe at that time. Detailed measurements of the expansion rate of the universe place the Big Bang at around 13.8 billion years ago, which is thus considered the age of the universe. After its initial expansion, the universe cooled sufficiently to allow the formation of subatomic particles, and later simple atoms. Giant clouds of these primordial elements (mostly hydrogen, with some helium and lithium) later coalesced through gravity, eventually forming early stars and galaxies, the descendants of which are visible today. Astronomers also observe the gravitational effects of dark matter surrounding galaxies. Though most of the mass in the universe seems to be in the form of dark matter, Big Bang theory and various observations seem to indicate that it is not made out of conventional baryonic matter (protons, neutrons, and electrons) but it is unclear exactly what it \"is\" made out of.\n\nSince Georges Lemaître first noted in 1927 that an expanding universe could be traced back in time to an originating single point, scientists have built on his idea of cosmic expansion. The scientific community was once divided between supporters of two different theories, the Big Bang and the Steady State theory, but a wide range of empirical evidence has strongly favored the Big Bang which is now universally accepted. In 1929, from analysis of galactic redshifts, Edwin Hubble concluded that galaxies are drifting apart; this is important observational evidence consistent with the hypothesis of an expanding universe. In 1964, the cosmic microwave background radiation was discovered, which was crucial evidence in favor of the Big Bang model, since that theory predicted the existence of background radiation throughout the universe before it was discovered. More recently, measurements of the redshifts of supernovae indicate that the expansion of the universe is accelerating, an observation attributed to dark energy's existence. The known physical laws of nature can be used to calculate the characteristics of the universe in detail back in time to an initial state of extreme density and temperature.\n\nThe Belgian astronomer and Catholic priest Georges Lemaître proposed on theoretical grounds that the universe is expanding, which was observationally confirmed soon afterwards by Edwin Hubble. In 1927 in the \"Annales de la Société Scientifique de Bruxelles\" (\"Annals of the Scientific Society of Brussels\") under the title \"Un Univers homogène de masse constante et de rayon croissant rendant compte de la vitesse radiale des nébuleuses extragalactiques\" (\"A homogeneous Universe of constant mass and growing radius accounting for the radial velocity of extragalactic nebulae\"), he presented his new idea that the universe is expanding and provided the first observational estimation of what is known as the Hubble constant. What later will be known as the \"Big Bang theory\" of the origin of the universe, he called his \"hypothesis of the primeval atom\" or the \"Cosmic Egg\".\n\nAmerican astronomer Edwin Hubble observed that the distances to faraway galaxies were strongly correlated with their redshifts. This was interpreted to mean that all distant galaxies and clusters are receding away from our vantage point with an apparent velocity proportional to their distance: that is, the farther they are, the faster they move away from us, regardless of direction. Assuming the Copernican principle (that the Earth is not the center of the universe), the only remaining interpretation is that all observable regions of the universe are receding from all others. Since we know that the distance between galaxies increases today, it must mean that in the past galaxies were closer together. The continuous expansion of the universe implies that the universe was denser and hotter in the past.\n\nLarge particle accelerators can replicate the conditions that prevailed after the early moments of the universe, resulting in confirmation and refinement of the details of the Big Bang model. However, these accelerators can only probe so far into high energy regimes. Consequently, the state of the universe in the earliest instants of the Big Bang expansion is still poorly understood and an area of open investigation and speculation.\n\nThe first subatomic particles to be formed included protons, neutrons, and electrons. Though simple atomic nuclei formed within the first three minutes after the Big Bang, thousands of years passed before the first electrically neutral atoms formed. The majority of atoms produced by the Big Bang were hydrogen, along with helium and traces of lithium. Giant clouds of these primordial elements later coalesced through gravity to form stars and galaxies, and the heavier elements were synthesized either within stars or during supernovae.\n\nThe Big Bang theory offers a comprehensive explanation for a broad range of observed phenomena, including the abundance of light elements, the CMB, large scale structure, and Hubble's Law. The framework for the Big Bang model relies on Albert Einstein's theory of general relativity and on simplifying assumptions such as homogeneity and isotropy of space. The governing equations were formulated by Alexander Friedmann, and similar solutions were worked on by Willem de Sitter. Since then, astrophysicists have incorporated observational and theoretical additions into the Big Bang model, and its parametrization as the Lambda-CDM model serves as the framework for current investigations of theoretical cosmology. The Lambda-CDM model is the current \"standard model\" of Big Bang cosmology, consensus is that it is the simplest model that can account for the various measurements and observations relevant to cosmology.\n\nExtrapolation of the expansion of the universe backwards in time using general relativity yields an infinite density and temperature at a finite time in the past. This singularity indicates that general relativity is not an adequate description of the laws of physics in this regime. Models based on general relativity alone can not extrapolate toward the singularity beyond the end of the Planck epoch.\n\nThis primordial singularity is itself sometimes called \"the Big Bang\", but the term can also refer to a more generic early hot, dense phase of the universe. In either case, \"the Big Bang\" as an event is also colloquially referred to as the \"birth\" of our universe since it represents the point in history where the universe can be verified to have entered into a regime where the laws of physics as we understand them (specifically general relativity and the standard model of particle physics) work. Based on measurements of the expansion using Type Ia supernovae and measurements of temperature fluctuations in the cosmic microwave background, the time that has passed since that event — otherwise known as the \"age of the universe\" — is 13.799 ± 0.021 billion years. The agreement of independent measurements of this age supports the ΛCDM model that describes in detail the characteristics of the universe.\n\nDespite being extremely dense at this time—far denser than is usually required to form a black hole—the universe did not re-collapse into a black hole. This may be explained by considering that commonly-used calculations and limits for gravitational collapse are usually based upon objects of relatively constant size, such as stars, and do not apply to rapidly expanding space such as the Big Bang.\n\nThe earliest phases of the Big Bang are subject to much speculation. In the most common models the universe was filled homogeneously and isotropically with a very high energy density and huge temperatures and pressures and was very rapidly expanding and cooling. Approximately 10 seconds into the expansion, a phase transition caused a cosmic inflation, during which the universe grew exponentially during which time density fluctuations that occurred because of the uncertainty principle were amplified into the seeds that would later form the large-scale structure of the universe. After inflation stopped, reheating occurred until the universe obtained the temperatures required for the production of a quark–gluon plasma as well as all other elementary particles. Temperatures were so high that the random motions of particles were at relativistic speeds, and particle–antiparticle pairs of all kinds were being continuously created and destroyed in collisions. At some point, an unknown reaction called baryogenesis violated the conservation of baryon number, leading to a very small excess of quarks and leptons over antiquarks and antileptons—of the order of one part in 30 million. This resulted in the predominance of matter over antimatter in the present universe.\n\nThe universe continued to decrease in density and fall in temperature, hence the typical energy of each particle was decreasing. Symmetry breaking phase transitions put the fundamental forces of physics and the parameters of elementary particles into their present form. After about 10 seconds, the picture becomes less speculative, since particle energies drop to values that can be attained in particle accelerators. At about 10 seconds, quarks and gluons combined to form baryons such as protons and neutrons. The small excess of quarks over antiquarks led to a small excess of baryons over antibaryons. The temperature was now no longer high enough to create new proton–antiproton pairs (similarly for neutrons–antineutrons), so a mass annihilation immediately followed, leaving just one in 10 of the original protons and neutrons, and none of their antiparticles. A similar process happened at about 1 second for electrons and positrons. After these annihilations, the remaining protons, neutrons and electrons were no longer moving relativistically and the energy density of the universe was dominated by photons (with a minor contribution from neutrinos).\n\nA few minutes into the expansion, when the temperature was about a billion (one thousand million) kelvin and the density was about that of air, neutrons combined with protons to form the universe's deuterium and helium nuclei in a process called Big Bang nucleosynthesis. Most protons remained uncombined as hydrogen nuclei.\n\nAs the universe cooled, the rest mass energy density of matter came to gravitationally dominate that of the photon radiation. After about 379,000 years, the electrons and nuclei combined into atoms (mostly hydrogen); hence the radiation decoupled from matter and continued through space largely unimpeded. This relic radiation is known as the cosmic microwave background radiation. The chemistry of life may have begun shortly after the Big Bang, 13.8 billion years ago, during a habitable epoch when the universe was only 10–17 million years old.\n\nOver a long period of time, the slightly denser regions of the nearly uniformly distributed matter gravitationally attracted nearby matter and thus grew even denser, forming gas clouds, stars, galaxies, and the other astronomical structures observable today. The details of this process depend on the amount and type of matter in the universe. The four possible types of matter are known as cold dark matter, warm dark matter, hot dark matter, and baryonic matter. The best measurements available, from Wilkinson Microwave Anisotropy Probe (WMAP), show that the data is well-fit by a Lambda-CDM model in which dark matter is assumed to be cold (warm dark matter is ruled out by early reionization), and is estimated to make up about 23% of the matter/energy of the universe, while baryonic matter makes up about 4.6%. In an \"extended model\" which includes hot dark matter in the form of neutrinos, then if the \"physical baryon density\" formula_1 is estimated at about 0.023 (this is different from the 'baryon density' formula_2 expressed as a fraction of the total matter/energy density, which as noted above is about 0.046), and the corresponding cold dark matter density formula_3 is about 0.11, the corresponding neutrino density formula_4 is estimated to be less than 0.0062.\n\nIndependent lines of evidence from Type Ia supernovae and the CMB imply that the universe today is dominated by a mysterious form of energy known as dark energy, which apparently permeates all of space. The observations suggest 73% of the total energy density of today's universe is in this form. When the universe was very young, it was likely infused with dark energy, but with less space and everything closer together, gravity predominated, and it was slowly braking the expansion. But eventually, after numerous billion years of expansion, the growing abundance of dark energy caused the expansion of the universe to slowly begin to accelerate.\n\nDark energy in its simplest formulation takes the form of the cosmological constant term in Einstein's field equations of general relativity, but its composition and mechanism are unknown and, more generally, the details of its equation of state and relationship with the Standard Model of particle physics continue to be investigated both through observation and theoretically.\n\nAll of this cosmic evolution after the inflationary epoch can be rigorously described and modeled by the ΛCDM model of cosmology, which uses the independent frameworks of quantum mechanics and Einstein's General Relativity. There is no well-supported model describing the action prior to 10 seconds or so. Apparently a new unified theory of quantum gravitation is needed to break this barrier. Understanding this earliest of eras in the history of the universe is currently one of the greatest unsolved problems in physics.\n\nThe Big Bang theory depends on two major assumptions: the universality of physical laws and the cosmological principle. The cosmological principle states that on large scales the universe is homogeneous and isotropic.\n\nThese ideas were initially taken as postulates, but today there are efforts to test each of them. For example, the first assumption has been tested by observations showing that largest possible deviation of the fine structure constant over much of the age of the universe is of order 10. Also, general relativity has passed stringent tests on the scale of the Solar System and binary stars.\n\nIf the large-scale universe appears isotropic as viewed from Earth, the cosmological principle can be derived from the simpler Copernican principle, which states that there is no preferred (or special) observer or vantage point. To this end, the cosmological principle has been confirmed to a level of 10 via observations of the CMB. The universe has been measured to be homogeneous on the largest scales at the 10% level.\n\nGeneral relativity describes spacetime by a metric, which determines the distances that separate nearby points. The points, which can be galaxies, stars, or other objects, are themselves specified using a coordinate chart or \"grid\" that is laid down over all spacetime. The cosmological principle implies that the metric should be homogeneous and isotropic on large scales, which uniquely singles out the Friedmann–Lemaître–Robertson–Walker metric (FLRW metric). \nThis metric contains a scale factor, which describes how the size of the universe changes with time. This enables a convenient choice of a coordinate system to be made, called comoving coordinates. In this coordinate system, the grid expands along with the universe, and objects that are moving only because of the expansion of the universe, remain at fixed points on the grid. While their \"coordinate\" distance (comoving distance) remains constant, the \"physical\" distance between two such co-moving points expands proportionally with the scale factor of the universe.\n\nThe Big Bang is not an explosion of matter moving outward to fill an empty universe. Instead, space itself expands with time everywhere and increases the physical distance between two comoving points. In other words, the Big Bang is not an explosion \"in space\", but rather an expansion \"of space\". Because the FLRW metric assumes a uniform distribution of mass and energy, it applies to our universe only on large scales—local concentrations of matter such as our galaxy are gravitationally bound and as such do not experience the large-scale expansion of space.\n\nAn important feature of the Big Bang spacetime is the presence of particle horizons. Since the universe has a finite age, and light travels at a finite speed, there may be events in the past whose light has not had time to reach us. This places a limit or a \"past horizon\" on the most distant objects that can be observed. Conversely, because space is expanding, and more distant objects are receding ever more quickly, light emitted by us today may never \"catch up\" to very distant objects. This defines a \"future horizon\", which limits the events in the future that we will be able to influence. The presence of either type of horizon depends on the details of the FLRW model that describes our universe.\n\nOur understanding of the universe back to very early times suggests that there is a past horizon, though in practice our view is also limited by the opacity of the universe at early times. So our view cannot extend further backward in time, though the horizon recedes in space. If the expansion of the universe continues to accelerate, there is a future horizon as well.\n\nEnglish astronomer Fred Hoyle is credited with coining the term \"Big Bang\" during a 1949 BBC radio broadcast, saying: \"These theories were based on the hypothesis that all the matter in the universe was created in one big bang at a particular time in the remote past.\"\n\nIt is popularly reported that Hoyle, who favored an alternative \"steady state\" cosmological model, intended this to be pejorative, but Hoyle explicitly denied this and said it was just a striking image meant to highlight the difference between the two models.\n\nThe Big Bang theory developed from observations of the structure of the universe and from theoretical considerations. In 1912 Vesto Slipher measured the first Doppler shift of a \"spiral nebula\" (spiral nebula is the obsolete term for spiral galaxies), and soon discovered that almost all such nebulae were receding from Earth. He did not grasp the cosmological implications of this fact, and indeed at the time it was highly controversial whether or not these nebulae were \"island universes\" outside our Milky Way. Ten years later, Alexander Friedmann, a Russian cosmologist and mathematician, derived the Friedmann equations from Albert Einstein's equations of general relativity, showing that the universe might be expanding in contrast to the static universe model advocated by Einstein at that time. In 1924 Edwin Hubble's measurement of the great distance to the nearest spiral nebulae showed that these systems were indeed other galaxies. Independently deriving Friedmann's equations in 1927, Georges Lemaître, a Belgian physicist, proposed that the inferred recession of the nebulae was due to the expansion of the universe.\n\nIn 1931 Lemaître went further and suggested that the evident expansion of the universe, if projected back in time, meant that the further in the past the smaller the universe was, until at some finite time in the past all the mass of the universe was concentrated into a single point, a \"primeval atom\" where and when the fabric of time and space came into existence.\n\nStarting in 1924, Hubble painstakingly developed a series of distance indicators, the forerunner of the cosmic distance ladder, using the Hooker telescope at Mount Wilson Observatory. This allowed him to estimate distances to galaxies whose redshifts had already been measured, mostly by Slipher. In 1929 Hubble discovered a correlation between distance and recession velocity—now known as Hubble's law. Lemaître had already shown that this was expected, given the cosmological principle.\n\nIn the 1920s and 1930s almost every major cosmologist preferred an eternal steady state universe, and several complained that the beginning of time implied by the Big Bang imported religious concepts into physics; this objection was later repeated by supporters of the steady state theory. This perception was enhanced by the fact that the originator of the Big Bang theory, Georges Lemaître, was a Roman Catholic priest. Arthur Eddington agreed with Aristotle that the universe did not have a beginning in time, \"viz\"., that matter is eternal. A beginning in time was \"repugnant\" to him. Lemaître, however, thought thatIf the world has begun with a single quantum, the notions of space and time would altogether fail to have any meaning at the beginning; they would only begin to have a sensible meaning when the original quantum had been divided into a sufficient number of quanta. If this suggestion is correct, the beginning of the world happened a little before the beginning of space and time.\n\nDuring the 1930s other ideas were proposed as non-standard cosmologies to explain Hubble's observations, including the Milne model, the oscillatory universe (originally suggested by Friedmann, but advocated by Albert Einstein and Richard Tolman) and Fritz Zwicky's tired light hypothesis.\n\nAfter World War II, two distinct possibilities emerged. One was Fred Hoyle's steady state model, whereby new matter would be created as the universe seemed to expand. In this model the universe is roughly the same at any point in time. The other was Lemaître's Big Bang theory, advocated and developed by George Gamow, who introduced big bang nucleosynthesis (BBN) and whose associates, Ralph Alpher and Robert Herman, predicted the CMB. Ironically, it was Hoyle who coined the phrase that came to be applied to Lemaître's theory, referring to it as \"this \"big bang\" idea\" during a BBC Radio broadcast in March 1949. For a while, support was split between these two theories. Eventually, the observational evidence, most notably from radio source counts, began to favor Big Bang over Steady State. The discovery and confirmation of the CMB in 1964 secured the Big Bang as the best theory of the origin and evolution of the universe. Much of the current work in cosmology includes understanding how galaxies form in the context of the Big Bang, understanding the physics of the universe at earlier and earlier times, and reconciling observations with the basic theory.\n\nIn 1968 and 1970 Roger Penrose, Stephen Hawking, and George F. R. Ellis published papers where they showed that mathematical singularities were an inevitable initial condition of general relativistic models of the Big Bang. Then, from the 1970s to the 1990s, cosmologists worked on characterizing the features of the Big Bang universe and resolving outstanding problems. In 1981, Alan Guth made a breakthrough in theoretical work on resolving certain outstanding theoretical problems in the Big Bang theory with the introduction of an epoch of rapid expansion in the early universe he called \"inflation\". Meanwhile, during these decades, two questions in observational cosmology that generated much discussion and disagreement were over the precise values of the Hubble Constant and the matter-density of the universe (before the discovery of dark energy, thought to be the key predictor for the eventual fate of the universe).\n\nIn the mid-1990s, observations of certain globular clusters appeared to indicate that they were about 15 billion years old, which conflicted with most then-current estimates of the age of the universe (and indeed with the age measured today). This issue was later resolved when new computer simulations, which included the effects of mass loss due to stellar winds, indicated a much younger age for globular clusters. While there still remain some questions as to how accurately the ages of the clusters are measured, globular clusters are of interest to cosmology as some of the oldest objects in the universe.\n\nSignificant progress in Big Bang cosmology has been made since the late 1990s as a result of advances in telescope technology as well as the analysis of data from satellites such as COBE, the Hubble Space Telescope and WMAP. Cosmologists now have fairly precise and accurate measurements of many of the parameters of the Big Bang model, and have made the unexpected discovery that the expansion of the universe appears to be accelerating.\n\nThe earliest and most direct observational evidence of the validity of the theory are the expansion of the universe according to Hubble's law (as indicated by the redshifts of galaxies), discovery and measurement of the cosmic microwave background and the relative abundances of light elements produced by Big Bang nucleosynthesis. More recent evidence includes observations of galaxy formation and evolution, and the distribution of large-scale cosmic structures, These are sometimes called the \"four pillars\" of the Big Bang theory.\n\nPrecise modern models of the Big Bang appeal to various exotic physical phenomena that have not been observed in terrestrial laboratory experiments or incorporated into the Standard Model of particle physics. Of these features, dark matter is currently subjected to the most active laboratory investigations. Remaining issues include the cuspy halo problem and the dwarf galaxy problem of cold dark matter. Dark energy is also an area of intense interest for scientists, but it is not clear whether direct detection of dark energy will be possible. Inflation and baryogenesis remain more speculative features of current Big Bang models. Viable, quantitative explanations for such phenomena are still being sought. These are currently unsolved problems in physics.\n\nObservations of distant galaxies and quasars show that these objects are redshifted—the light emitted from them has been shifted to longer wavelengths. This can be seen by taking a frequency spectrum of an object and matching the spectroscopic pattern of emission lines or absorption lines corresponding to atoms of the chemical elements interacting with the light. These redshifts are uniformly isotropic, distributed evenly among the observed objects in all directions. If the redshift is interpreted as a Doppler shift, the recessional velocity of the object can be calculated. For some galaxies, it is possible to estimate distances via the cosmic distance ladder. When the recessional velocities are plotted against these distances, a linear relationship known as Hubble's law is observed:\nformula_5\nwhere\n\nHubble's law has two possible explanations. Either we are at the center of an explosion of galaxies—which is untenable given the Copernican principle—or the universe is uniformly expanding everywhere. This universal expansion was predicted from general relativity by Alexander Friedmann in 1922 and Georges Lemaître in 1927, well before Hubble made his 1929 analysis and observations, and it remains the cornerstone of the Big Bang theory as developed by Friedmann, Lemaître, Robertson, and Walker.\n\nThe theory requires the relation formula_9 to hold at all times, where formula_7 is the comoving distance, \"v\" is the recessional velocity, and formula_6, formula_12, and formula_7 vary as the universe expands (hence we write formula_8 to denote the present-day Hubble \"constant\"). For distances much smaller than the size of the observable universe, the Hubble redshift can be thought of as the Doppler shift corresponding to the recession velocity formula_6. However, the redshift is not a true Doppler shift, but rather the result of the expansion of the universe between the time the light was emitted and the time that it was detected.\n\nThat space is undergoing metric expansion is shown by direct observational evidence of the Cosmological principle and the Copernican principle, which together with Hubble's law have no other explanation. Astronomical redshifts are extremely isotropic and homogeneous, supporting the Cosmological principle that the universe looks the same in all directions, along with much other evidence. If the redshifts were the result of an explosion from a center distant from us, they would not be so similar in different directions.\n\nMeasurements of the effects of the cosmic microwave background radiation on the dynamics of distant astrophysical systems in 2000 proved the Copernican principle, that, on a cosmological scale, the Earth is not in a central position. Radiation from the Big Bang was demonstrably warmer at earlier times throughout the universe. Uniform cooling of the CMB over billions of years is explainable only if the universe is experiencing a metric expansion, and excludes the possibility that we are near the unique center of an explosion.\n\nIn 1964 Arno Penzias and Robert Wilson serendipitously discovered the cosmic background radiation, an omnidirectional signal in the microwave band. Their discovery provided substantial confirmation of the big-bang predictions by Alpher, Herman and Gamow around 1950. Through the 1970s the radiation was found to be approximately consistent with a black body spectrum in all directions; this spectrum has been redshifted by the expansion of the universe, and today corresponds to approximately 2.725 K. This tipped the balance of evidence in favor of the Big Bang model, and Penzias and Wilson were awarded a Nobel Prize in 1978.\n\nThe \"surface of last scattering\" corresponding to emission of the CMB occurs shortly after \"recombination\", the epoch when neutral hydrogen becomes stable. Prior to this, the universe comprised a hot dense photon-baryon plasma sea where photons were quickly scattered from free charged particles. Peaking at around , the mean free path for a photon becomes long enough to reach the present day and the universe becomes transparent.\nIn 1989, NASA launched the Cosmic Background Explorer satellite (COBE), which made two major advances: in 1990, high-precision spectrum measurements showed that the CMB frequency spectrum is an almost perfect blackbody with no deviations at a level of 1 part in 10, and measured a residual temperature of 2.726 K (more recent measurements have revised this figure down slightly to 2.7255 K); then in 1992, further COBE measurements discovered tiny fluctuations (anisotropies) in the CMB temperature across the sky, at a level of about one part in 10. John C. Mather and George Smoot were awarded the 2006 Nobel Prize in Physics for their leadership in these results.\n\nDuring the following decade, CMB anisotropies were further investigated by a large number of ground-based and balloon experiments. In 2000–2001 several experiments, most notably BOOMERanG, found the shape of the universe to be spatially almost flat by measuring the typical angular size (the size on the sky) of the anisotropies.\n\nIn early 2003, the first results of the Wilkinson Microwave Anisotropy Probe (WMAP) were released, yielding what were at the time the most accurate values for some of the cosmological parameters. The results disproved several specific cosmic inflation models, but are consistent with the inflation theory in general. The Planck space probe was launched in May 2009. Other ground and balloon based cosmic microwave background experiments are ongoing.\n\nUsing the Big Bang model it is possible to calculate the concentration of helium-4, helium-3, deuterium, and lithium-7 in the universe as ratios to the amount of ordinary hydrogen. The relative abundances depend on a single parameter, the ratio of photons to baryons. This value can be calculated independently from the detailed structure of CMB fluctuations. The ratios predicted (by mass, not by number) are about 0.25 for <chem>^4He/H</chem>, about 10 for <chem>^2H/H</chem>, about 10 for <chem>^3He/H</chem> and about 10 for <chem>^7Li/H</chem>.\n\nThe measured abundances all agree at least roughly with those predicted from a single value of the baryon-to-photon ratio. The agreement is excellent for deuterium, close but formally discrepant for <chem>^4He</chem>, and off by a factor of two for <chem>^7Li</chem>; in the latter two cases there are substantial systematic uncertainties. Nonetheless, the general consistency with abundances predicted by Big Bang nucleosynthesis is strong evidence for the Big Bang, as the theory is the only known explanation for the relative abundances of light elements, and it is virtually impossible to \"tune\" the Big Bang to produce much more or less than 20–30% helium. Indeed, there is no obvious reason outside of the Big Bang that, for example, the young universe (i.e., before star formation, as determined by studying matter supposedly free of stellar nucleosynthesis products) should have more helium than deuterium or more deuterium than <chem>^3He</chem>, and in constant ratios, too.\n\nDetailed observations of the morphology and distribution of galaxies and quasars are in agreement with the current state of the Big Bang theory. A combination of observations and theory suggest that the first quasars and galaxies formed about a billion years after the Big Bang, and since then, larger structures have been forming, such as galaxy clusters and superclusters.\n\nPopulations of stars have been aging and evolving, so that distant galaxies (which are observed as they were in the early universe) appear very different from nearby galaxies (observed in a more recent state). Moreover, galaxies that formed relatively recently, appear markedly different from galaxies formed at similar distances but shortly after the Big Bang. These observations are strong arguments against the steady-state model. Observations of star formation, galaxy and quasar distributions and larger structures, agree well with Big Bang simulations of the formation of structure in the universe, and are helping to complete details of the theory.\n\nIn 2011, astronomers found what they believe to be pristine clouds of primordial gas by analyzing absorption lines in the spectra of distant quasars. Before this discovery, all other astronomical objects have been observed to contain heavy elements that are formed in stars. These two clouds of gas contain no elements heavier than hydrogen and deuterium. Since the clouds of gas have no heavy elements, they likely formed in the first few minutes after the Big Bang, during Big Bang nucleosynthesis.\n\nThe age of the universe as estimated from the Hubble expansion and the CMB is now in good agreement with other estimates using the ages of the oldest stars, both as measured by applying the theory of stellar evolution to globular clusters and through radiometric dating of individual Population II stars.\n\nThe prediction that the CMB temperature was higher in the past has been experimentally supported by observations of very low temperature absorption lines in gas clouds at high redshift. This prediction also implies that the amplitude of the Sunyaev–Zel'dovich effect in clusters of galaxies does not depend directly on redshift. Observations have found this to be roughly true, but this effect depends on cluster properties that do change with cosmic time, making precise measurements difficult.\n\nFuture gravitational waves observatories might be able to detect primordial gravitational waves, relics of the early universe, up to less than a second after the Big Bang.\n\nAs with any theory, a number of mysteries and problems have arisen as a result of the development of the Big Bang theory. Some of these mysteries and problems have been resolved while others are still outstanding. Proposed solutions to some of the problems in the Big Bang model have revealed new mysteries of their own. For example, the horizon problem, the magnetic monopole problem, and the flatness problem are most commonly resolved with inflationary theory, but the details of the inflationary universe are still left unresolved and many, including some founders of the theory, say it has been disproven. What follows are a list of the mysterious aspects of the Big Bang theory still under intense investigation by cosmologists and astrophysicists.\n\nIt is not yet understood why the universe has more matter than antimatter. It is generally assumed that when the universe was young and very hot it was in statistical equilibrium and contained equal numbers of baryons and antibaryons. However, observations suggest that the universe, including its most distant parts, is made almost entirely of matter. A process called baryogenesis was hypothesized to account for the asymmetry. For baryogenesis to occur, the Sakharov conditions must be satisfied. These require that baryon number is not conserved, that C-symmetry and CP-symmetry are violated and that the universe depart from thermodynamic equilibrium. All these conditions occur in the Standard Model, but the effects are not strong enough to explain the present baryon asymmetry.\n\nMeasurements of the redshift–magnitude relation for type Ia supernovae indicate that the expansion of the universe has been accelerating since the universe was about half its present age. To explain this acceleration, general relativity requires that much of the energy in the universe consists of a component with large negative pressure, dubbed \"dark energy\".\n\nDark energy, though speculative, solves numerous problems. Measurements of the cosmic microwave background indicate that the universe is very nearly spatially flat, and therefore according to general relativity the universe must have almost exactly the critical density of mass/energy. But the mass density of the universe can be measured from its gravitational clustering, and is found to have only about 30% of the critical density. Since theory suggests that dark energy does not cluster in the usual way it is the best explanation for the \"missing\" energy density. Dark energy also helps to explain two geometrical measures of the overall curvature of the universe, one using the frequency of gravitational lenses, and the other using the characteristic pattern of the large-scale structure as a cosmic ruler.\n\nNegative pressure is believed to be a property of vacuum energy, but the exact nature and existence of dark energy remains one of the great mysteries of the Big Bang. Results from the WMAP team in 2008 are in accordance with a universe that consists of 73% dark energy, 23% dark matter, 4.6% regular matter and less than 1% neutrinos. According to theory, the energy density in matter decreases with the expansion of the universe, but the dark energy density remains constant (or nearly so) as the universe expands. Therefore, matter made up a larger fraction of the total energy of the universe in the past than it does today, but its fractional contribution will fall in the far future as dark energy becomes even more dominant.\n\nThe dark energy component of the universe has been explained by theorists using a variety of competing theories including Einstein's cosmological constant but also extending to more exotic forms of quintessence or other modified gravity schemes. A cosmological constant problem, sometimes called the \"most embarrassing problem in physics\", results from the apparent discrepancy between the measured energy density of dark energy, and the one naively predicted from Planck units.\n\nDuring the 1970s and the 1980s, various observations showed that there is not sufficient visible matter in the universe to account for the apparent strength of gravitational forces within and between galaxies. This led to the idea that up to 90% of the matter in the universe is dark matter that does not emit light or interact with normal baryonic matter. In addition, the assumption that the universe is mostly normal matter led to predictions that were strongly inconsistent with observations. In particular, the universe today is far more lumpy and contains far less deuterium than can be accounted for without dark matter. While dark matter has always been controversial, it is inferred by various observations: the anisotropies in the CMB, galaxy cluster velocity dispersions, large-scale structure distributions, gravitational lensing studies, and X-ray measurements of galaxy clusters.\n\nIndirect evidence for dark matter comes from its gravitational influence on other matter, as no dark matter particles have been observed in laboratories. Many particle physics candidates for dark matter have been proposed, and several projects to detect them directly are underway.\n\nAdditionally, there are outstanding problems associated with the currently favored cold dark matter model which include the dwarf galaxy problem and the cuspy halo problem. Alternative theories have been proposed that do not require a large amount of undetected matter, but instead modify the laws of gravity established by Newton and Einstein; yet no alternative theory has been as successful as the cold dark matter proposal in explaining all extant observations.\n\nThe horizon problem results from the premise that information cannot travel faster than light. In a universe of finite age this sets a limit—the particle horizon—on the separation of any two regions of space that are in causal contact. The observed isotropy of the CMB is problematic in this regard: if the universe had been dominated by radiation or matter at all times up to the epoch of last scattering, the particle horizon at that time would correspond to about 2 degrees on the sky. There would then be no mechanism to cause wider regions to have the same temperature.\n\nA resolution to this apparent inconsistency is offered by inflationary theory in which a homogeneous and isotropic scalar energy field dominates the universe at some very early period (before baryogenesis). During inflation, the universe undergoes exponential expansion, and the particle horizon expands much more rapidly than previously assumed, so that regions presently on opposite sides of the observable universe are well inside each other's particle horizon. The observed isotropy of the CMB then follows from the fact that this larger region was in causal contact before the beginning of inflation.\n\nHeisenberg's uncertainty principle predicts that during the inflationary phase there would be quantum thermal fluctuations, which would be magnified to cosmic scale. These fluctuations serve as the seeds of all current structure in the universe. Inflation predicts that the primordial fluctuations are nearly scale invariant and Gaussian, which has been accurately confirmed by measurements of the CMB.\n\nIf inflation occurred, exponential expansion would push large regions of space well beyond our observable horizon.\n\nA related issue to the classic horizon problem arises because in most standard cosmological inflation models, inflation ceases well before electroweak symmetry breaking occurs, so inflation should not be able to prevent large-scale discontinuities in the electroweak vacuum since distant parts of the observable universe were causally separate when the electroweak epoch ended.\n\nThe magnetic monopole objection was raised in the late 1970s. Grand unified theories predicted topological defects in space that would manifest as magnetic monopoles. These objects would be produced efficiently in the hot early universe, resulting in a density much higher than is consistent with observations, given that no monopoles have been found. This problem is also resolved by cosmic inflation, which removes all point defects from the observable universe, in the same way that it drives the geometry to flatness.\n\nThe flatness problem (also known as the oldness problem) is an observational problem associated with a Friedmann–Lemaître–Robertson–Walker metric (FLRW). The universe may have positive, negative, or zero spatial curvature depending on its total energy density. Curvature is negative if its density is less than the critical density; positive if greater; and zero at the critical density, in which case space is said to be \"flat\".\n\nThe problem is that any small departure from the critical density grows with time, and yet the universe today remains very close to flat. Given that a natural timescale for departure from flatness might be the Planck time, 10 seconds, the fact that the universe has reached neither a heat death nor a Big Crunch after billions of years requires an explanation. For instance, even at the relatively late age of a few minutes (the time of nucleosynthesis), the density of the universe must have been within one part in 10 of its critical value, or it would not exist as it does today.\n\nPhysics may conclude that time did not exist before 'Big Bang', but 'started' with the Big Bang and hence there might be no 'beginning', 'before' or potentially 'cause' and instead always existed. Quantum fluctuations, or other laws of physics that may have existed at the start of the Big Bang could then create the conditions for matter to occur.\n\nBefore observations of dark energy, cosmologists considered two scenarios for the future of the universe. If the mass density of the universe were greater than the critical density, then the universe would reach a maximum size and then begin to collapse. It would become denser and hotter again, ending with a state similar to that in which it started—a Big Crunch.\n\nAlternatively, if the density in the universe were equal to or below the critical density, the expansion would slow down but never stop. Star formation would cease with the consumption of interstellar gas in each galaxy; stars would burn out, leaving white dwarfs, neutron stars, and black holes. Very gradually, collisions between these would result in mass accumulating into larger and larger black holes. The average temperature of the universe would asymptotically approach absolute zero—a Big Freeze. Moreover, if the proton were unstable, then baryonic matter would disappear, leaving only radiation and black holes. Eventually, black holes would evaporate by emitting Hawking radiation. The entropy of the universe would increase to the point where no organized form of energy could be extracted from it, a scenario known as heat death.\n\nModern observations of accelerating expansion imply that more and more of the currently visible universe will pass beyond our event horizon and out of contact with us. The eventual result is not known. The ΛCDM model of the universe contains dark energy in the form of a cosmological constant. This theory suggests that only gravitationally bound systems, such as galaxies, will remain together, and they too will be subject to heat death as the universe expands and cools. Other explanations of dark energy, called phantom energy theories, suggest that ultimately galaxy clusters, stars, planets, atoms, nuclei, and matter itself will be torn apart by the ever-increasing expansion in a so-called Big Rip.\n\nThe following is a partial list of misconceptions about the Big Bang model:\n\n\"The Big Bang as the origin of the universe:\" One of the common misconceptions about the Big Bang model is the belief that it was the origin of the universe. However, the Big Bang model does not comment about how the universe came into being. Current conception of the Big Bang model assumes the existence of energy, time, and space, and does not comment about their origin or the cause of the dense and high temperature initial state of the universe.\n\n\"The Big Bang was \"small\"\": It is misleading to visualize the Big Bang by comparing its size to everyday objects. When the size of the universe at Big Bang is described, it refers to the size of the observable universe, and not the entire universe.\n\n\"Hubble's law violates the special theory of relativity\": Hubble's law predicts that galaxies that are beyond Hubble Distance recede faster than the speed of light. However, special relativity does not apply beyond motion through space. Hubble's law describes velocity that results from expansion \"of\" space, rather than \"through\" space.\n\n\"Doppler redshift vs cosmological red-shift\": Astronomers often refer to the cosmological red-shift as a normal Doppler shift, which is a misconception. Although similar, the cosmological red-shift is not identical to the Doppler redshift. The Doppler redshift is based on special relativity, which does not consider the expansion of space. On the contrary, the cosmological red-shift is based on general relativity, in which the expansion of space is considered. Although they may appear identical for nearby galaxies, it may cause confusion if the behavior of distant galaxies is understood through the Doppler redshift.\n\nWhile the Big Bang model is well established in cosmology, it is likely to be refined. The Big Bang theory, built upon the equations of classical general relativity, indicates a singularity at the origin of cosmic time; this infinite energy density is regarded as impossible in physics. Still, it is known that the equations are not applicable before the time when the universe cooled down to the Planck temperature, and this conclusion depends on various assumptions, of which some could never be experimentally verified. \"(Also see Planck epoch.)\"\n\nOne proposed refinement to avoid this would-be singularity is to develop a correct treatment of quantum gravity.\n\nIt is not known what could have preceded the hot dense state of the early universe or how and why it originated, though speculation abounds in the field of cosmogony.\n\nSome proposals, each of which entails untested hypotheses, are:\n\nProposals in the last two categories see the Big Bang as an event in either a much larger and older universe or in a multiverse.\n\nAs a description of the origin of the universe, the Big Bang has significant bearing on religion and philosophy. As a result, it has become one of the liveliest areas in the discourse between science and religion. Some believe the Big Bang implies a creator, and some see its mention in their holy books, while others argue that Big Bang cosmology makes the notion of a creator superfluous.\n\n\n\n\n"}
{"id": "13291712", "url": "https://en.wikipedia.org/wiki?curid=13291712", "title": "Cartesianism", "text": "Cartesianism\n\nCartesianism is the philosophical and scientific system of René Descartes and its subsequent development by other seventeenth century thinkers, most notably Nicolas Malebranche and Baruch Spinoza. Descartes is often regarded as the first thinker to emphasize the use of reason to develop the natural sciences. For him, the philosophy was a thinking system that embodied all knowledge, and expressed it in this way:\n\nCartesians view the mind as being wholly separate from the corporeal body. Sensation and the perception of reality are thought to be the source of untruth and illusions, with the only reliable truths to be had in the existence of a metaphysical mind. Such a mind can perhaps interact with a physical body, but it does not exist in the body, nor even in the same physical plane as the body. The question of how mind and body interact would be a persistent difficulty for Descartes and his followers, with different Cartesians providing different answers.\n\nDescartes held that all existence consists in three distinct substances, each with its own essence:\n\nDescartes brought the question of how reliable knowledge may be obtained (epistemology) to the fore of philosophical enquiry. Many consider this to be Descartes' most lasting influence on the history of philosophy.\n\nCartesianism is a form of rationalism because it holds that scientific knowledge can be derived \"a priori\" from 'innate ideas' through deductive reasoning. Thus Cartesianism is opposed to both Aristotelianism and empiricism, with their emphasis on sensory experience as the source of all knowledge of the world.\n\nFor Descartes, the faculty of deductive reason is supplied by God and may therefore be trusted because God would not deceive us.\n\nIn the Netherlands, where Descartes had lived for a long time, Cartesianism was a doctrine popular mainly among university professors and lecturers. In Germany the influence of this doctrine was not relevant and followers of Cartesianism in the German-speaking border regions between these countries (e.g., the iatromathematician Yvo Gaukes from East Frisia) frequently chose to publish their works in the Netherlands. In France, it was very popular, and gained influence also among Jansenists such as Antoine Arnauld, though there also, as in Italy, it became opposed by the Church. In Italy, the doctrine failed to make inroads, probably since Descartes' works were placed on the \"Index Librorum Prohibitorum\" in 1663.\n\nIn England, because of religious and other reasons, Cartesianism was not widely accepted. Though Henry More was initially attracted to the doctrine, his own changing attitudes toward Descartes mirrored those of the country: \"quick acceptance, serious examination with accumulating ambivalence, final rejection.\"\n\n\n\n"}
{"id": "11109606", "url": "https://en.wikipedia.org/wiki?curid=11109606", "title": "Chronemics", "text": "Chronemics\n\nChronemics is the study of the role of time in communication. It is one of several subcategories of the study of nonverbal communication. Other prominent subcategories include haptics (touch), kinesics (body movement), vocalics (paralanguage), and proxemics (the use of space).\n\nThomas J. Bruneau of Radford University coined the term \"chronemics\" in the late 1970s to help define the function of time in human interaction:\n\nChronemics can be briefly and generally defined as the study of human tempo as it related to human communication. More specifically, chronemics involves the study of both subjective and objective human tempos as they influence and are interdependent with human behavior. Further, chronemics involves the study of human communication as it relates to interdependent and integrated levels of time-experiencing. Previously, these interdependent and integrated levels have been outlined and discussed as: biological time; psychological time; social time; and cultural time. A number of classification systems exist in the literature of time. However, such systems are not applied to human interaction directly.\n\nChronemics can be defined as \"the interrelated observations and theories of man's use of time\" – the way in which one perceives and values time, structures time, and reacts to time frames communication. Time perception plays a large role in the nonverbal communication process. Time perceptions include punctuality, willingness to wait, and interactions. The use of time can affect lifestyle, daily agendas, speed of speech, movements, and how long people are willing to listen.\n\nTime can be used as an indicator of status. For example, in most companies the boss can interrupt progress to hold an impromptu meeting in the middle of the work day, yet the average worker would have to make an appointment to see the boss. The way in which different cultures perceive time can influence communication as well.\n\nCultures are sometimes considered monochronic or polychronic.\n\nA monochronic time system means that things are done one at a time and time is segmented into precise, small units. Under this system, time is scheduled, arranged and managed.\n\nThe United States considers itself a monochronic society. This perception came about during the Industrial Revolution, when \"factory life required the labor force to be on hand and in place at an appointed hour\" (Guerrero, DeVito & Hecht, 1999, p. 238). Many Americans like to think that to them, time is a precious resource not to be wasted or taken lightly. \"We buy time, save time, spend time and make time. Our time can be broken down into years, months, days, hours, minutes, seconds and even milliseconds. We use time to structure both our daily lives and events that we are planning for the future. We have schedules that we must follow: appointments that we must go to at a certain time, classes that start and end at certain times, work schedules that start and end at certain times, and even our favorite TV shows, that start and end at a certain time.\"\n\nAs communication scholar Edward T. Hall wrote regarding the American's viewpoint of time in the business world, \"the schedule is sacred.\" Hall says that for monochronic cultures, such as the American culture, \"time is tangible\" and viewed as a commodity where \"time is money\" or \"time is wasted.\" The result of this perspective is that monochronic cultures, place a paramount value on schedules, tasks and \"getting the job done.\" These cultures are committed to regimented schedules and may view those who do not subscribe to the same perception of time as disrespectful, inefficient or unreliable.\n\nA polychronic time system is a system where several things can be done at once, and wider view of time is exhibited and time is perceived in large fluid sections. Examples of polychronic behaviors include: typing while answering telephones or taking notes while sitting participating in meetings. Polychronicity is in contrast to those who prefer monochronicity (doing one thing at a time).\n\nPolychronic cultures are much less focused on the preciseness of accounting for each and every moment. As Raymond Cohen notes, polychronic cultures are more focused on tradition and relationships rather than on tasks—a clear difference from their monochronic counterparts. Cohen notes that \"Traditional societies have all the time in the world. The arbitrary divisions of the clock face have little saliency in cultures grounded in the cycle of the seasons, the invariant pattern of rural life, community life, and the calendar of religious festivities\" (Cohen, 1997, p. 34).\n\nPolychronic culture is more focused on relationships, rather than watching the clock. Polychronic societies have no problem being \"late\" for an appointment if they are deeply focused on some work or in a meeting that ran past schedule, because the concept of time is fluid and can easily expand or contract as need be. As a result, polychronic cultures have a much less formal perception of time. They are not ruled by precise calendars and schedules. Rather, \"cultures that use the polychronic time system often schedule multiple appointments simultaneously so keeping on schedule is an impossibility.\"\n\nResearchers have developed the following questionnaires to measure polychronicity:\n\n\nConflicting attitudes between the monochronic and polychronic perceptions of time can interfere with cross-cultural relations, and similar challenges can occur within an otherwise assimilated culture. One example in the United States is the Hawaiian culture, which employs two time systems: Haole time and Hawaiian time.\n\n\"When you hear someone say, 'See you at two o'clock haole time,' they mean they will just that. Haole time is when the person will meet when they say they will meet. But if you were to hear someone say, 'I'll be there at two o'clock Hawaiian time,' then something different is implied. Hawaiian time is very lax and it basically means 'when you get there.'\" —Nick Lewis\n\nThe way an individual perceives time and the role time plays in their lives is a learned perspective. As discussed by Alexander Gonzalez and Phillip Zimbardo, \"every child learns a time perspective that is appropriate to the values and needs of his society\" (Guerrero, DeVito & Hecht, 1999, p. 227).\n\nThere are four basic psychological time orientations:\n\nEach orientation affects the structure, content, and urgency of communication (Burgoon, 1989). The past orientation has a hard time developing the notion of elapsed time and these individuals often confuse present and past happenings as all in the same. People oriented with time-line cognitivity are often detail oriented and think of everything in linear terms. These individuals also often have difficulty with comprehending multiple events at the same time. Individuals with a present orientation are mostly characterized as pleasure seekers who live for the moment and have a very low risk aversion. Those individuals who operate with future orientation are often thought of as being highly goal oriented and focused on the broad picture.\n\nThe use of time as a communicative channel can be a powerful, yet subtle, force in face-to-face \"interactions\". Some of the more recognizable types of interaction that use time are:\n\n\nTime orientation has also revealed insights into how people react to advertising. Martin, Gnoth and Strong (2009) found that future-oriented consumers react most favorably to ads that feature a product to be released in the distant future and that highlight primary product attributes. In contrast, present-oriented consumers prefer near-future ads that highlight secondary product attributes. Consumer attitudes were mediated by the perceived usefulness of the attribute information.\n\nJust as monochronic and polychronic cultures have different time perspectives, understanding the time orientation of a culture is critical to becoming better able to successfully handle diplomatic situations. Americans think they have, a future orientation. Hall indicates that for Americans \"tomorrow is more important\" and that they \"are oriented almost entirely toward the future\" (Cohen, 2004, p. 35). The future-focused orientation attributes to at least some of the concern that Americans have with \"addressing immediate issues and moving on to new challenges\" (Cohen, 2004, p. 35).\n\nOn the other hand, many polychronic cultures have a past-orientation toward time.\n\nThese time perspectives are the seeds for communication clashes in diplomatic situations. Trade negotiators have observed that \"American negotiators are generally more anxious for agreement because \"they are always in a hurry\" and basically \"problem solving oriented.\" In other words, they place a high value on resolving an issue quickly calling to mind the American catchphrase \"some solution is better than no solution\" (Cohen, 2004, p. 114). Similar observations have been made of Japanese-American relations. Noting the difference in time perceptions between the two countries, former ambassador to Tokyo, Mike Mansfield commented \"We're too fast, they're too slow\" (Cohen, 2004, p. 118).\n\nDifferent perceptions of time across cultures can influence global communication. When writing about time perspective, Gonzalez and Zimbardo comment that \"There is no more powerful, pervasive influence on how individuals think and cultures interact than our different perspectives on time—the way we learn how we mentally partition time into past, present and future.\" (Guerrero, DeVito & Hecht, 1999, p. 227)\n\nDepending upon where an individual is from, their perception of time might be that \"the clock rules the day\" or that \"we'll get there when we get there.\"\nImproving prospects for success in the global community requires understanding cultural differences, traditions and communication styles.\n\nThe monochronic-oriented approach to negotiations is direct, linear and rooted in the characteristics that illustrate low context tendencies. The low context culture approaches diplomacy in a lawyerly, dispassionate fashion with a clear idea of acceptable outcomes and a plan for reaching them. Draft arguments would be prepared elaborating positions. A monochronic culture, more concerned with time, deadlines and schedules, tends to grow impatient and want to rush to \"close the deal.\"\n\nMore polychronic-oriented cultures come to diplomatic situations with no particular importance placed on time. Chronemics is one of the channels of nonverbal communication preferred by a High context Polychronic negotiator over verbal communication.The polychronic approach to negotiations will emphasis building trust between participants, forming coalitions and finding consensus. High context Polychronic negotiators might be charged with emotion toward a subject thereby obscuring an otherwise obvious solution.\n\nTime has a definite relationship to power. Though power most often refers to the ability to influence people (Guerrero, DeVito & Hecht, 1999, p. 314), power is also related to dominance and status (Guerrero, DeVito & Hecht, 1999, p. 315).\n\nFor example, in the workplace, those in a leadership or management position treat time and – by virtue of position – have their time treated differently from those who are of a lower stature position. Anderson and Bowman have identified three specific examples of how chronemics and power converge in the workplace – waiting time, talk time and work time.\n\nResearchers Insel and Lindgren (Guerrero, DeVito & Hecht, 1999, p. 325) write that the act of making an individual of a lower stature wait is a sign of dominance. They note that one who \"is in the position to cause another to wait has power over him. To be kept waiting is to imply that one's time is less valuable than that of the one who imposes the wait.\"\n\nThere is a direct correlation between the power of an individual in an organization and conversation. This includes both length of conversation, turn-taking and who initiates and ends a conversation. Extensive research indicates that those with more power in an organization will speak more often and for a greater length of time. Meetings between superiors and subordinates provide an opportunity to illustrate this concept. A superior – regardless of whether or not they are running the actual meeting – lead discussions, ask questions and have the ability to speak for longer periods of time without interruption. Likewise, research shows that turn-taking is also influenced by power. Social psychologist Nancy Henley notes that \"Subordinates are expected to yield to superiors and there is a cultural expectation that a subordinate will not interrupt a superior\" (Guerrero, DeVito & Hecht, 1999, p. 326). The length of response follows the same pattern. While the superior can speak for as long as they want, the responses of the subordinate are shorter in length. Albert Mehrabian noted that deviation from this pattern led to negative perceptions of the subordinate by the superior. Beginning and ending a communication interaction in the workplace is also controlled by the higher-status individual in an organization. The time and duration of the conversation are dictated by the higher-status individual.\n\nThe time of high status individuals is perceived as valuable, and they control their own time. On the other hand, a subordinate with less power has their time controlled by a higher status individual and are in less control of their time – making them likely to report their time to a higher authority. Such practices are more associated with those in non-supervisory roles or in blue collar rather than white collar professions. Instead, as power and status in an organization increases, the flexibility of the work schedule also increases. For instance, while administrative professionals might keep a 9 to 5 work schedule, their superiors may keep less structured hours. This does not mean that the superior works less. They may work longer, but the structure of their work environment is not strictly dictated by the traditional work day. Instead, as Koehler and their associates note \"individuals who spend more time, especially spare time, to meetings, to committees, and to developing contacts, are more likely to be influential decision makers\" (Guerrero, DeVito & Hecht, 1999, p. 327).\n\nA specific example of the way power is expressed through work time is scheduling. As Yakura and others have noted in research shared by Ballard and Seibold, \"scheduling reflects the extent to which the sequencing and duration of plans activities and events are formalized\" (Ballard and Seibold, p. 6). Higher-status individuals have very precise and formal schedules – indicating that their stature requires that they have specific blocks of time for specific meetings, projects and appointments. Lower status individuals however, may have less formalized schedules. Finally, the schedule and appointment calendar of the higher status individual will take precedence in determining where, when and the importance of a specific event or appointment.\n\nDeveloped by Judee Burgoon, expectancy violations theory (EVT) sees communication as the exchange of information which is high in relational content and can be used to violate the expectations of another which will be perceived as either positively or negatively depending on the liking between the two people.\n\nWhen our expectations are violated, we will respond in specific ways. If an act is unexpected and is assigned favorable interpretation, and it is evaluated positively, it will produce more favorable outcomes than an expected act with the same interpretation and evaluation.\n\nThe interpersonal adaptation theory (IAT), founded by Judee Burgoon, states that adaptation in interaction is responsive to the needs, expectations, and desires of communicators and affects how communicators position themselves in relation to one another and adapt to one another's communication. For example, they may match each other's behavior, synchronize the timing of behavior, or behave in dissimilar ways. It is also important to note that individuals bring to interactions certain requirements that reflect basic human needs, expectations about behavior based on social norms, and desires for interaction based on goals and personal preferences (Burgoon, Stern & Dillman, 1995).\n\n\n\n\n"}
{"id": "4642057", "url": "https://en.wikipedia.org/wiki?curid=4642057", "title": "Cognitive style", "text": "Cognitive style\n\nCognitive style or \"thinking style\" is a concept used in cognitive psychology to describe the way individuals think, perceive and remember information. Cognitive style differs from cognitive ability (or level), the latter being measured by aptitude tests or so-called intelligence tests. There is controversy over the exact meaning of the term \"cognitive style\" and whether it is a single or multiple dimension of human personality. However it remains a key concept in the areas of education and management. If a pupil has a cognitive style that is similar to that of his/her teacher, the chances are improved that the pupil will have a more positive learning experience. Likewise, team members with similar cognitive styles likely feel more positive about their participation with the team. While matching cognitive styles may make participants feel more comfortable when working with one another, this alone cannot guarantee the success of the outcome.\n\nA popular multi-dimensional instrument for the measure of cognitive style is the Myers–Briggs Type Indicator.\n\nRiding (1991) developed a two-dimensional cognitive style instrument, his Cognitive Style Analysis (CSA), which is a compiled computer-presented test that measures individuals' position on two orthogonal dimensions – Wholist-Analytic (W-A) and Verbal-Imagery (V-I). The W-A dimension reflects how individuals organise and structure information. Individuals described as Analytics will deconstruct information into its component parts, whereas individuals described as Wholists will retain a global or overall view of information. The V-I dimension describes individuals' mode of information representation in memory during thinking – Verbalisers represent information in words or verbal associations, and Imagers represent information in mental pictures. The CSA test is broken down into three sub-tests, all of which are based on a comparison between response times to different types of stimulus items. Some scholars argue that this instrument, being at least in part reliant on the ability of the respondent to answer at speed, really measures a mix of cognitive style and cognitive ability (Kirton, 2003). This is said to contribute to the unreliability of this instrument.\n\nThe field dependence-independence model, invented by Herman Witkin, identifies an individual's perceptive behaviour while distinguishing object figures from the content field in which they are set. Two similar instruments to do this were produced, the Embedded Figures Test (EFT) and the Group Embedded Figures Test (GEFT) (1971). In both cases, the content field is a distracting or confusing background. These instruments are designed to distinguish field-independent from field-dependent cognitive types; a rating which is claimed to be value-neutral. Field-independent people tend to be more autonomous when it comes to the development of restructuring skills; that is, those skills required during technical tasks with which the individual is not necessarily familiar. They are, however, less autonomous in the development of interpersonal skills. The EFT and GEFT continue to enjoy support and usage in research and practice. However, they, too, are criticised by scholars as containing an element of ability and so may not measure cognitive style alone.\n\nLiam Hudson (Carey, 1991) identified two cognitive styles: convergent thinkers, good at accumulating material from a variety of sources relevant to a problem's solution, and divergent thinkers who proceed more creatively and subjectively in their approach to problem-solving. Hudson's Converger-diverger construct attempts to measure the processing rather than the acquisition of information by an individual. It aims to differentiate convergent from divergent thinkers; the former being persons who think rationally and logically while the latter tend to be more flexible and to base reasoning more on heuristic evidence.\n\nIn contrast, cognitive complexity theories as proposed by James Bieri (1961) attempt to identify individuals who are more complex in their approach to problem-solving against those who are simpler. The instruments used to measure this concept of \"cognitive style\" are either Driver's Decision Style Exercise (DDSE) (Carey, 1991) or the Complexity Self-Test Description Instrument, which are somewhat ad hoc and so are little used at present.\n\nGordon Pask (Carey, 1991) extended these notions in a discussion of strategies and styles of learning. In this, he classifies learning strategies as either holist or serialist. When confronted with an unfamiliar type of problem, holists gather information randomly within a framework, while serialists approach problem-solving step-wise, proceeding from the known to the unknown.\n\nRobert Ornstein's Hemispherical lateralisation concept (Carey, 1991), commonly called left-brain/right-brain theory, posits that the left hemisphere of the brain controls logical and analytical operations while the right hemisphere controls holistic, intuitive and pictorial activities. Cognitive style is thus claimed to be a single dimension on a scale from extreme left-brain to extreme right-brain types, depending on which associated behaviour dominates in the individual, and by how much.\n\nTaggart's (1988) \"Whole-brain human information processing theory\" classifies the brain as having six divisions, three per hemisphere, which in a sense is a refined model of the hemispherical lateralisation theory discussed above.\n\nThe Allinson-Hayes (1996) Cognitive Style Index (CSI) has features of Ornstein's left-brain/right-brain theory. Recent evidence suggests that it may be the most widely used measure of cognitive style in academic research in the fields of management and education (Cools, Armstrong and Verbrigghe, 2014; Evans, Cools and Charlesworth, 2010). The CSI contains 38 items, each rated using a 3-point scale (true; uncertain; false). Certain scholars have questioned its construct validity on the grounds of theoretical and methodological approaches associated with its development. Allinson and Hayes (2012), however, have refuted these claims on the basis of other independent studies of its psychometric properties. Research has indicated both gender and cultural differences in CSI scores. While this may complicate some management and educational applications, previous investigations have suggested it is entirely plausible that cognitive style is related to these social factors.\n\nAnother popular model of cognitive style was devised by Michael Kirton (1976, 2003). His model, called Adaption-Innovation theory, claims that an individual's preferred approach to problem solving, can be placed on a continuum ranging from high adaptation to high innovation. He suggests that some human beings, called adaptors tend to prefer the adaptive approach to problem-solving, while others (innovators), of course, prefer the reverse. Adaptors use what is given to solve problems by time-honoured techniques. Alternatively, innovators look beyond what is given to solve problems with the aid of innovative technologies. Kirton suggests that while adaptors prefer to do well within a given paradigm, innovators would rather do differently, thereby striving to transcend existing paradigms.\n\nKirton also invented an instrument to measure cognitive style (at least in accordance with this model) known as the Kirton Adaption-innovation Inventory (KAI). This requires the respondent to rate themselves against thirty-two personality traits. A drawback of all the other efforts to measure cognitive style discussed above is their failure to separate out cognitive style and cognitive level. As the items on the KAI are expressed in clear and simple language, cognitive level plays no significant role. Scores on the A-I continuum are normally distributed between the extreme cognitive styles of high innovation and high adaptation.\n\nAnother important concept associated with A-I theory is that of bridging in teams. Kirton (2003) defines bridging as \"reaching out to people in the team and helping them be part of it so that they may contribute even if their contribution is outside the mainstream\". Bridging is thus a task and a role, which has to be learnt. It is not a cognitive style. Bridging is also not leading, although the skilled leader may make use of persons they recognise as good bridgers to maintain group cohesion. Group cohesion means, to keep the group aware of the importance of its members working well together. Kirton (2003) suggests that it is easier for a person to learn and assume a bridging role if their cognitive style is an intermediate one. If person B assumes a bridging role which assists persons A and C to work well together in a team, then B's KAI score is recommended to be between those of A and C. Of course, it is only recommended that B's score lies between the scores of A and C, not that B's score lies near the KAI mean. All of A, B and C could be high-scoring innovators or, for that matter, high-scoring adaptors.\n\n\n"}
{"id": "244167", "url": "https://en.wikipedia.org/wiki?curid=244167", "title": "Communal reinforcement", "text": "Communal reinforcement\n\nCommunal reinforcement is a social phenomenon in which a concept or idea is repeatedly asserted in a community, regardless of whether sufficient empirical evidence has been presented to support it. Over time, the concept or idea is reinforced to become a strong belief in many people's minds, and may be regarded by the members of the community as fact. Often, the concept or idea may be further reinforced by publications in the mass media, books, or other means of communication. The phrase \"millions of people can't all be wrong\" is indicative of the common tendency to accept a communally reinforced idea without question, which often aids in the widespread acceptance of factoids. A very similar term to this term is community-reinforcement, which is a behavioral method to stop drug addiction.\n\nThe community-reinforcement approach (CRA) is an alcoholism treatment approach that aims to achieve abstinence by eliminating positive reinforcement for drinking and enhancing positive reinforcement for sobriety. CRA integrates several treatment components, including building the client's motivation to quit drinking, helping the client initiate sobriety, analyzing the client's drinking pattern, increasing positive reinforcement, learning new coping behaviors, and involving significant others in the recovery process. These components can be adjusted to the individual client's needs to achieve optimal treatment outcome. In addition, treatment outcome can be influenced by factors such as therapist style and initial treatment intensity. Several studies have provided evidence for CRA's effectiveness in achieving abstinence. Furthermore, CRA has been successfully integrated with a variety of other treatment approaches, such as family therapy and motivational interviewing, and has been tested in the treatment of other drug abuse.\n\nIn Chris E. Stout's book \"The Psychology of Terrorism: Theoretical Understandings and Perspective\", Stout explains how community reinforcement is present in the psychotic state of terrorists. \"The individual would feel less charged, validated, courageous, sanctified, and zealous, and would feel exposed as an individual.\" It is believed that the group mentality of a terrorist organization solidifies the mission of the group through communal reinforcement. Members are more likely to stay dedicated and follow through with the event of terror if they receive support from fellow terrorist members. An individual might abandon the mission in terror, but with the reinforcement of his peers, a member is more likely to stay involved.\n"}
{"id": "984784", "url": "https://en.wikipedia.org/wiki?curid=984784", "title": "Contrition", "text": "Contrition\n\nIn Christian theology, contrition or contriteness (from the Latin \"contritus\" 'ground to pieces', i.e. crushed by guilt) is repentance for sins one has committed. The remorseful person is said to be \"contrite\".\n\nA central concept in much of Christianity, contrition is regarded as the first step, through Christ, towards reconciliation with God. It consists of repentance for all one's sins, a desire for God over sin, and faith in Christ's redemption on the cross and its sufficiency for salvation (see regeneration and \"ordo salutis\"). It is widely referred to throughout the Bible, e.g. Ezekiel 33:11, Psalms 6:7ff, Psalm 51:1–12, Luke 13:5, Luke 18:9–13, and the well-known parable of the prodigal son (Luke 15:11–32).\n\nThe Council of Trent defined contrition as \"sorrow of soul, and a hatred of sin committed, with a firm purpose of not sinning in the future\". It is also known as \"animi cruciatus\" (affliction of spirit) and \"compunctio cordis\" (repentance of heart).\n\nThe word \"contrition\" implies a breaking of something that has become hardened. St. Thomas Aquinas in his Commentary on the Master of the Sentences thus explains its peculiar use: \"Since it is requisite for the remission of sin that a man cast away entirely the liking for sin which implies a sort of continuity and solidity in his mind, the act which obtains forgiveness is termed by a figure of speech 'contrition'.\" This sorrow of soul is not merely speculative sorrow for wrong done, remorse of conscience, or a resolve to amend; it is a real pain and bitterness of soul together with a hatred and horror for sin committed; and this hatred for sin leads to the resolve to sin no more. The early Christian writers in speaking of the nature of contrition sometimes insist on the feeling of sorrow, sometimes on the detestation of the wrong committed. Augustine includes both when writing: \"Compunctus corde non solet dici nisi stimulus peccatorum in dolore pœnitendi\".\n\nNearly all the medieval theologians hold that contrition is based principally on the detestation of sin. This detestation presupposes a knowledge of the heinousness of sin, and this knowledge begets sorrow and pain of soul. \"A sin is committed by the consent, so it is blotted out by the dissent of the rational will; hence contrition is essentially sorrow. But it should be noted that sorrow has a twofold signification—dissent of the will and the consequent feeling; the former is of the essence of contrition, the latter is its effect.\"\n\nThe formal doctrine of the Church, announced through the Council of Trent, declares that contrition has always been necessary to obtain pardon of one's sins. Contrition is the first and indispensable condition for pardon. While it is possible for one to receive pardon where confession is impossible, there is no case where sin can be pardoned without contrition.\n\nAccording to the \"Catholic Encyclopedia\", Catholic writers have always insisted that such necessity arises (a) from the very nature of repentance as well as (b) from the positive command of God. From the very nature of repentance, they point out that the sentence of Christ in Luke 13:5, is final: \"Except you repent\", etc., and from the Fathers they cite passages such as the following from Cyprian, \"De Lapsis\", no. 32: \"Do penance in full, give proof of the sorrow that comes from a grieving and lamenting soul ... they who do away with repentance for sin, close the door to satisfaction.\" Scholastic doctors laid down the satisfaction' principle, \"No one can begin a new life who does not repent him of the old\" (Bonaventure, In Lib. Sent. IV, dist. xvi, Pt. II, art. 1, Q. ii, also ex professo, ibid., Pt. I, art. I, Q. iii), and when asked the reason why, they point out the absolute incongruity of turning to God and clinging to sin, which is hostile to God's law. The Council of Trent, mindful of the tradition of the ages, defined (Sess. XlV. ch. iv de Contritione) that \"contrition has always been necessary for obtaining forgiveness of sin\". The positive command of God is also clear in the premises. The Baptist sounded the note of preparation for the coming of the Messiah: \"Make straight his paths\"; and, as a consequence \"they went out to him and were baptized confessing their sins\". The first preaching of Jesus is described in the words: \"Do penance, for the kingdom of heaven is at hand\"; and the Apostles, in their first sermons to the people, warn them to \"do penance and be baptized for the remission of their sins\" (Acts 2:38). The Fathers followed up with like exhortation (Clement in P.G., I, 341; Hermas iii P.G., II, 894; Tertullian in P.L., II).\n\nIf detestation of sin arises from the love of God, who has been grievously offended, then contrition is termed \"perfect\"; if it arise from any other motive, such as loss of heaven, fear of hell, or the heinousness of guilt, then it is termed \"imperfect contrition\", or attrition.\n\nPerfect contrition (also called contrition of charity), is a repentance for sin that is motivated by faith and the love of God. It contrasts with imperfect contrition, which arises from a less pure motive, such as common decency, or fear of Hell. The two types of contrition are distinguished by a person's motive for repentance, rather than the intensity of ones feelings or emotions. It is possible for perfect and imperfect contrition to be experienced simultaneously.\n\nIn perfect contrition its motive is founded on God's own goodness and not merely his goodness to the sinner or to humanity. There is no way of knowing with an absolute certainty if one has made an act of perfect contrition, but all that is required is the standard of all human action, moral certainty. If one says an act of contrition truthfully, intending it, then one would likely have moral certainty.\n\nPerfect contrition removes the guilt and eternal punishment due to mortal sin, even before the sinner has received absolution in the sacrament of penance, provided that the person has a firm resolution to have recourse to sacramental confession as soon as possible. An example of this theological precept is demonstrated in the \"Code of Canon Law\" in canon 916, which states: \"A person who is conscious of grave sin is not to celebrate Mass or receive the body of the Lord without previous sacramental confession unless there is a grave reason and there is no opportunity to confess; in this case the person is to remember the obligation to make an act of perfect contrition which includes the resolution of confessing as soon as possible.\"\n\nIn the case of imminent death, in which sacramental confession may not be possible, the firm resolution to go to sacramental confession, as soon as possible if a person survives, also removes the guilt and eternal punishment due to mortal sin.\n\nAccording to Psalm 111: 10, \"The fear of the Lord is the beginning of wisdom.\" In Philippians 2:12, Paul exhorts Christians to work out \"our salvation in fear and trembling\". In contrast to perfect contrition, imperfect contrition (also known as \"attrition\") is a desire not to sin for a reason other than love of God. While attrition does not produce justification, attrition \"does dispose\" the soul to receive grace in the Catholic Sacrament of Reconciliation.\n\nThe Council of Trent (1545–1563) held that while imperfect contrition is motivated by reasons such as \"the consideration of the turpitude of sin or from the fear of Hell and punishment\", it also is a gift from God. \"If any man assert that attrition ... is not a true and a profitable sorrow; that it does not prepare the soul for grace, but that it makes a man a hypocrite, yea, even a greater sinner, let him be Anathema.\"\n\nThe question has also been asked apropos of attrition when one receives a sacrament in mortal sin, of which sin he is not then aware, will attrition with the sacrament suffice unto justification? The answer is generally given in the affirmative.\n\nScriptural support for attrition can be found in Proverbs 13:13, Proverbs 14:26–27, Proverbs 19:23, Matthew 10:28, and Philippians 2:12.\n\nIn accord with Catholic tradition, contrition, whether perfect or imperfect, must be interior, supernatural, universal, and sovereign.\n\nContrition must be real and sincere sorrow of heart.\n\nIn accordance with Catholic teaching contrition ought to be prompted by God's grace and aroused by motives which spring from faith, as opposed to merely natural motives, such as loss of honour, fortune, and the like (Chemnitz, Exam. Concil. Trid., Pt. II, De Poenit.). In the Old Testament it is God who gives a \"new heart\" and who puts a \"new spirit)\" into the children of Israel (Ezech. 36:25–29); and for a clean heart the Psalmist prays in the Miserere (Ps. 1, 11 sqq.). Peter told those to whom he preached in the first days after Pentecost that God the Father had raised up Christ \"to give repentance to Israel\" (Acts, v, 30 sq.). Paul, in advising Timothy, insists on dealing gently and kindly with those who resist the truth, \"if peradventure God may give them full repentance\" (2 Timothy, 2:24–25). In the days of the Pelagian heresy Augustine insisted on the supernaturalness of contrition, when he writes, \"That we turn away from God is our doing, and this is the bad will; but to turn back to God we are unable unless He arouse and help us, and this is the good will.\" Some of the Scholastic doctors, notably Scotus, Cajetan, and after them Suarez (De Poenit., Disp. iii, sect. vi), asked speculatively whether man if left to himself could elicit a true act of contrition, but no theologian ever taught that makes for forgiveness of sin in the present economy of God could be inspired by merely natural motives. On the contrary, all the doctors have insisted on the absolute necessity of grace for contrition that disposes to forgiveness (Bonaventure, In Lib. Sent. IV, dist. xiv, Part I, art. II, Q. iii; also dist. xvii, Part I, art. I, Q. iii; cf. Thomas, In Lib. Sent. IV). In keeping with this teaching of the Scriptures and the doctors, the Council of Trent defined; \"If anyone say that without the inspiration of the Holy Spirit and without His aid a man can repent in the way that is necessary for obtaining the grace of justification, let him be anathema.\"\n\nTrue contrition must extend to, at the very least, all mortal sins committed, and not just a select convenient few. This doctrine is intimately bound up with the Catholic teaching concerning grace and repentance. There is no forgiveness without sorrow of soul, and forgiveness is always accompanied by God's grace; grace cannot coexist with sin; and, as a consequence, one sin cannot be forgiven while another remains for which there is no repentance.\n\nThe prophet Joel urged men to turn to God with their whole heart (Joel 2:12–19). and Christ tells the doctor of the law that we must love God with our whole mind, our whole strength (Luke 10:27). Ezekiel insists that a man must \"turn from his evil ways\" if he wish to live (Ezekiel 33:11).\n\nThe Scholastics inquired into this question when they asked whether or not there must be a special act of contrition for every serious sin, and whether, in order to be forgiven, one must remember at the moment all grievous transgressions. To both questions they answered in the negative, judging that an act of sorrow which implicitly included all his sins would be sufficient.\n\nAccording to Mark 8:35–37, Jesus admonished his disciples: \"For whosoever will save his life, shall lose it: and whosoever shall lose his life for my sake and the gospel, shall save it. For what shall it profit a man, if he gain the whole world, and suffer the loss of his soul? Or what shall a man give in exchange for his soul?\" Contrition for sin must take precedence over temporal concerns. When the envoys of the Empress Eudoxia threatened John Chrysostom, he responded, \"Go tell the princess that Chrysostom fears only one thing, and that is sin.\"\n\nContrition is not only a moral virtue, but the Council of Trent defined that it is a \"part\", nay more, quasi materia, in the Sacrament of Penance. \"The (quasi) matter of this sacrament consists of the acts of the penitent himself, namely, contrition, confession, and satisfaction. These, inasmuch as they are by God's institution required in the penitent for the integrity of the sacrament and for the full and perfect remission of sin, are for this reason called parts of penance. \"In consequence of this decree of Trent theologians teach that sorrow for sin must be in some sense sacramental. La Croix went so far as to say that sorrow must be aroused with a view of going to confession, but this seems to be asking too much; most theologians think with Schieler-Heuser (Theory and Practice of Confession, p. 113) that it is sufficient if the sorrow coexist in any way with the confession and is referred to it. Hence the precept of the Roman Ritual, \"After the confessor has heard the confession he should try by earnest exhortation to move the penitent to contrition\" (Schieler-Heuser, op. cit., p. 111 sqq.).\n\nRegarding that contrition which has for its motive the love of God, the Council of Trent declares: \"The Council further teaches that, though contrition may sometimes be made perfect by charity and may reconcile men to God before the actual reception of this sacrament, still the reconciliation is not to be ascribed to the contrition apart from the desire for the sacrament which it includes.\" The following proposition (no. 32) taken from Baius was condemned by Gregory XIII: \"That charity which is the fullness of the law is not always conjoined with forgiveness of sins.\" Perfect contrition, with the desire of receiving the Sacrament of Penance, restores the sinner to grace at once. This is certainly the teaching of the Scholastic doctors (Peter Lombard in P.L., CXCII, 885; St. Thomas, In Lib. Sent. IV, ibid.; St. Bonaventure, In Lib. Sent. IV, ibid.). This doctrine they derived from Holy Writ. Scripture certainly ascribes to charity and the love of God the power to take away sin: \"He that loveth me shall be loved by My Father\"; \"Many sins are forgiven her because she hath loved much\" (Luke 7:36-50).\n\nSince the act of perfect contrition implies necessarily this same love of God, theologians have ascribed to perfect contrition what Scripture teaches belongs to charity. Nor is this strange, for in the Old Covenant there was some way of recovering God's grace once man had sinned. God wills not the death of the wicked, but that the wicked turn from his way and live (Ezech. 33:11). This total turning to God corresponds to our idea of perfect contrition; and if under the Old Law love sufficed for the pardon of the sinner, surely the coming of Christ and the institution of the Sacrament of Penance cannot be supposed to have increased the difficulty of obtaining forgiveness. That the earlier Fathers taught the efficacy of sorrow for the remission of sins is very clear (Clement in P.G., I, 341 sqq.; and Hermas in P.G., II, 894 sqq.; Chrysostom in P.G., XLIX, 285 sqq.) and this is particularly noticeable in all the commentaries on Luke, vii, 47.\n\nThe Venerable Bede writes (P.L., XCII, 425): \"What is love but fire; what is sin but rust? Hence it is said, many sins are forgiven her because she hath loved much, as though to say, she hath burned away entirely the rust of sin, because she is inflamed with the fire of love.\" Theologians have inquired with much learning as to the kind of love that justifies with the Sacrament of Penance. All are agreed that pure, or disinterested, love (amor benevolentiæ, amor amicitiæ) suffices; when there is question of interested, or selfish, love (amor concupiscentia) theologians hold that purely selfish love is not sufficient. When on furthermore asks what must be the formal motive in perfect love, there seems to be no real unanimity among the doctors. Some say that where there is perfect love God is loved for His great goodness alone; other, basing their contention on Scripture, think that the love of gratitude (amor gratitudinis) is quite sufficient, because God's benevolence and love towards men are intimately united, nay, inseparable from His Divine perfections (Hurter, \"Theol. Dog.\", Thesis ccxlv, Scholion iii, no 3; Schieler-Heuser, op. cit., pp. 77 sq.).\n\nIn the very nature of things the sinner must repent before he can be reconciled with God (Sess. XIV, ch. iv, de Contritione, Fuit quovis tempore, etc.). Therefore, he who has fallen into grievous sin must either make an act of perfect contrition or supplement the imperfect contrition by receiving the Sacrament of Penance; otherwise reconciliation with God is impossible. This obligation urges under pain of sin when there is danger of death. In danger of death, therefore, if a priest be not at hand to administer the sacrament, the sinner must make an effort to elicit an act of perfect contrition. The obligation of perfect contrition is also urgent whensoever one has to exercise some act for which a state of grace is necessary and the Sacrament of Penance is not accessible. Theologians have questions how long a man may remain in the state of sin, without making an effort to elicit an act of perfect contrition. They seem agreed that such neglect must have extended over considerable time, but what constitutes a considerable time they find it hard to determine (Schieler-Hauser, op. cit., pp. 83 sqq.). Probably the rule of St. Alphonsus Liguori will aid the solution: \"The duty of making an act of contrition is urgent when one is obliged to make an act of love\" (Sabetti, \"Theologia Moralis: de necess. contritionis\", no. 731; Ballerine, \"Opus Morale: de contritione\").\n\nThe \"Augsburg Confession\", the primary confession of faith of the Lutheran Church, divides repentance into two parts: \"One is contrition, that is, terrors smiting the conscience through the knowledge of sin; the other is faith, which is born of the Gospel, or of absolution, and believes that for Christ's sake, sins are forgiven, comforts the conscience, and delivers it from terrors.\"\n\nPuritan preacher Thomas Hooker defined contrition as: \"...nothing else, namely, when the sinner when a sinner by the sight of sin and vileness of it, and the punishment due to the same, is made sensible of sin, and is made to hate it, and hath his heart separated from the same...\"\n\nAnglo-Catholic rector of St. Mark's Church in Philadelphia, Alfred Garnett Mortimer, pointed out that \"feelings\" are not an adequate gauge of contrition. The signs of true contrition are a readiness to confess, a readiness to amend one's life and avoid temptation, and a readiness to forgive those others.\n\n\n\n"}
{"id": "24231314", "url": "https://en.wikipedia.org/wiki?curid=24231314", "title": "Crocodile dilemma", "text": "Crocodile dilemma\n\nThe crocodile paradox, also known as crocodile sophism, is a paradox in logic in the same family of paradoxes as the liar paradox. The premise states that a crocodile, who has stolen a child, promises the father/mother that their child will be returned if and only if they correctly predict what the crocodile will do next.\n\nThe transaction is logically smooth but unpredictable if the parent guesses that the child will be returned, but a dilemma arises for the crocodile if the parent guesses that the child will not be returned. In the case that the crocodile decides to keep the child, he violates his terms: the parent's prediction has been validated, and the child should be returned. However, in the case that the crocodile decides to give back the child, he still violates his terms, even if this decision is based on the previous result: the parent's prediction has been falsified, and the child should not be returned. The question of what the crocodile should do is therefore paradoxical, and there is no justifiable solution.\n\nThe crocodile dilemma serves to expose some of the logical problems presented by metaknowledge. In this regard, it is similar in construction to the unexpected hanging paradox, which Richard Montague (1960) used to demonstrate that the following assumptions about knowledge are inconsistent when tested in combination:\nAncient Greek sources were the first to discuss the crocodile dilemma.\n\n"}
{"id": "15399756", "url": "https://en.wikipedia.org/wiki?curid=15399756", "title": "Declaration of incompatibility", "text": "Declaration of incompatibility\n\nA declaration of incompatibility is a declaration issued by a United Kingdom judge that a statute is incompatible with the European Convention of Human Rights under the Human Rights Act 1998 section 4. This is a central part of UK constitutional law. Very few declarations of incompatibility have been issued, in comparison to the number of challenges.\nSection 3(1) of the Human Rights Act 1998 reads as follows: \"So far as it is possible to do so, primary legislation and subordinate legislation must be read and given effect in a way which is compatible with the Convention rights\". Where the court determines a piece of legislation is inconsistent with the Convention rights, the court can issue a declaration of incompatibility under section 4 of the Human Rights Act 1998. However, the declaration of incompatibility is often seen as a last resort as the judiciary will attempt to interpret primary legislation as being compatible. Such a declaration will only be issued if such a reading is not possible. \n\nOnce the court has issued a declaration of incompatibility, the law remains the same until Parliament removes the incompatibility. The courts must still apply the legislation as it is and the parties to the actual case are unaffected by the declaration. Hence, the declaration has no actual legal effect and the parties neither gain nor lose by it. A declaration of incompatibility is only the start of a remedy to a Human Rights Act 1998 claim. Section 8 of the Act enables the court to make any further remedy it sees fit.\n\nIn England and Wales, the High Court, Court of Appeal, Supreme Court, Judicial Committee of the Privy Council, and the Courts Martial Appeal Court can issue declarations of incompatibility. In Scotland, in addition to the Supreme Court, the Court of Session and the High Court of Justiciary are also able to issue declarations of incompatibility. In Northern Ireland, the Northern Irish High Court or Court of Appeals can issue a statement of incompatibility for Acts of the Northern Irish Assembly \n\nBy section 10 of the Human Rights Act 1998, a \"fast track\" option of a remedial order (a type of statutory instrument) can be used by the ministers to amend non-compliant legislation which has been declared incompatible (except if it is a measure of the Church of England). As of 2016 this option has been used twice: in 2001 for the Mental Health Act 1983, and in 2009 for the Sexual Offences Act 2003.\n\nThere had been 20 declarations of incompatibility by April 2013, with 8 having been overturned on appeal. By July 2016 there were two more declarations finalised, and four more subject to appeal.\n\nThe following cases involved declarations of incompatibility that were overturned on appeal:\n\nThe following cases involved the court finding that a statute was incompatible but not making a formal declaration of incompatibility:\n\n"}
{"id": "2870451", "url": "https://en.wikipedia.org/wiki?curid=2870451", "title": "Declaration of the Rights of the Child", "text": "Declaration of the Rights of the Child\n\nThe Declaration of the Rights of the Child, sometimes known as the Geneva Declaration of the Rights of the Child, is an international document promoting child rights, drafted by Eglantyne Jebb and adopted by the League of Nations in 1924, and adopted in an extended form by the United Nations in 1959.\n\nThe text of the document, as published by the International Save the Children Union in Geneva on 23 February 1923, is as follows:\n\nThis text was endorsed by the League of Nations General Assembly on 26 November 1924 as the World Child Welfare Charter, and was the first human rights document approved by an inter-governmental institution. It was reaffirmed by the League in 1934. Heads of State and Government pledged to incorporate its principles in domestic legislation. In France, it was ordered to be displayed in every school.\n\nThe original document, in the archives of the city of Geneva, carries the signatures of various international delegates, including Jebb, Janusz Korczak, and Gustave Ador, a former President of the Swiss Confederation.\n\nAfter considering a number of options, including that of drafting an entirely new declaration, the United Nations resolved in 1946 to adopt the document, in a much expanded version, as its own statement of children's rights. Many different governments were involved in the drafting process. A slightly expanded version, with seven points in place of five, was adopted in 1948. Then on 20 November 1959 the United Nations General Assembly adopted a Declaration of the Rights of the Child, based on the structure and contents of the 1924 original, with ten principles. An accompanying resolution, proposed by the delegation of Afghanistan, called on governments to recognise these rights, strive for their acceptance, and publicise the document as widely as possible. This date has been adopted as the Universal Children's Day.\n\nThis Declaration was followed in 1989 by the Convention on the Rights of the Child, adopted by the UN General Assembly, \nadopted and opened for signature, ratification and accession by General Assembly resolution 44/25 of 20 November 1989; entry into force 2 September 1990, in accordance with article 49.\n\n\n"}
{"id": "7964", "url": "https://en.wikipedia.org/wiki?curid=7964", "title": "Definition", "text": "Definition\n\nA definition is a statement of the meaning of a term (a word, phrase, or other set of symbols). Definitions can be classified into two large categories, intensional definitions (which try to give the essence of a term) and extensional definitions (which proceed by listing the objects that a term describes). Another important category of definitions is the class of ostensive definitions, which convey the meaning of a term by pointing out examples. A term may have many different senses and multiple meanings, and thus require multiple definitions.\n\nIn mathematics, a definition is used to give a precise meaning to a new term, instead of describing a pre-existing term. Definitions and axioms are the basis on which all of modern mathematics is constructed.\n\nIn modern usage, a \"definition\" is something, typically expressed in words, that attaches a meaning to a word or group of words. The word or group of words that is to be defined is called the \"definiendum\", and the word, group of words, or action that defines it is called the \"definiens\". In the definition \"An elephant is a large gray animal native to Asia and Africa\", the word \"elephant\" is the \"definiendum\", and everything after the word \"is\" is the \"definiens\".\n\nThe \"definiens\" is not \"the meaning\" of the word defined, but is instead something that \"conveys the same meaning\" as that word.\n\nThere are many sub-types of definitions, often specific to a given field of knowledge or study. These include, among many others, lexical definitions, or the common dictionary definitions of words already in a language; demonstrative definitions, which define something by pointing to an example of it (\"\"This,\" [said while pointing to a large grey animal], \"is an Asian elephant.\"\"); and precising definitions, which reduce the vagueness of a word, typically in some special sense (\"'Large', among female Asian elephants, is any individual weighing over 5,500 pounds.\").\n\nAn \"intensional definition\", also called a \"connotative\" definition, specifies the necessary and sufficient conditions for a thing being a member of a specific set. Any definition that attempts to set out the essence of something, such as that by genus and differentia, is an intensional definition.\n\nAn \"extensional definition\", also called a \"denotative\" definition, of a concept or term specifies its \"extension\". It is a list naming every object that is a member of a specific set.\n\nThus, the \"seven deadly sins\" can be defined \"intensionally\" as those singled out by Pope Gregory I as particularly destructive of the life of grace and charity within a person, thus creating the threat of eternal damnation. An \"extensional\" definition would be the list of wrath, greed, sloth, pride, lust, envy, and gluttony. In contrast, while an intensional definition of \"Prime Minister\" might be \"the most senior minister of a cabinet in the executive branch of government in a parliamentary system\", an extensional definition is not possible since it is not known who future prime ministers will be.\n\nA genus–differentia definition is a type of intensional definition that takes a large category (the genus) and narrows it down to a smaller category by a distinguishing characteristic (i.e. the differentia).\n\nMore formally, a genus-differentia definition consists of:\n\nFor example, consider the following genus-differentia definitions:\n\nThose definitions can be expressed as a genus (\"a plane figure\") and two differentiae (\"that has three straight bounding sides\" and \"that has four straight bounding sides\", respectively).\n\nIt is possible to have two different genus-differentia definitions that describe the same term, especially when the term describes the overlap of two large categories. For instance, both of these genus-differentia definitions of \"square\" are equally acceptable:\n\nThus, a \"square\" is a member of both the genus \"rectangle\" and the genus \"rhombus\".\n\nOne important form of the extensional definition is \"ostensive definition\". This gives the meaning of a term by pointing, in the case of an individual, to the thing itself, or in the case of a class, to examples of the right kind. So one can explain who \"Alice\" (an individual) is by pointing her out to another; or what a \"rabbit\" (a class) is by pointing at several and expecting another to understand. The process of ostensive definition itself was critically appraised by Ludwig Wittgenstein.\n\nAn \"enumerative definition\" of a concept or term is an \"extensional definition\" that gives an explicit and exhaustive listing of all the objects that fall under the concept or term in question. Enumerative definitions are only possible for finite sets and only practical for relatively small sets.\n\n\"Divisio\" and \"partitio\" are classical terms for definitions. A \"partitio\" is simply an intensional definition. A \"divisio\" is not an extensional definition, but an exhaustive list of subsets of a set, in the sense that every member of the \"divided\" set is a member of one of the subsets. An extreme form of \"divisio\" lists all sets whose only member is a member of the \"divided\" set. The difference between this and an extensional definition is that extensional definitions list \"members\", and not subsets.\n\nIn classical thought, a definition was taken to be a statement of the essence of a thing. Aristotle had it that an object's essential attributes form its \"essential nature\", and that a definition of the object must include these essential attributes.\n\nThe idea that a definition should state the essence of a thing led to the distinction between \"nominal\" and \"real\" essence, originating with Aristotle. In a passage from the Posterior Analytics, he says that the meaning of a made-up name can be known (he gives the example \"goat stag\"), without knowing what he calls the \"essential nature\" of the thing that the name would denote, if there were such a thing. This led medieval logicians to distinguish between what they called the \"quid nominis\" or \"whatness of the name\", and the underlying nature common to all the things it names, which they called the \"quid rei\" or \"whatness of the thing\". (Early modern philosophers like Locke used the corresponding English terms \"nominal essence\" and \"real essence\"). The name \"hobbit\", for example, is perfectly meaningful. It has a \"quid nominis\". But one could not know the real nature of hobbits, and so the real nature or \"quid rei\" of hobbits cannot be known. By contrast, the name \"man\" denotes real things (men) that have a certain \"quid rei\". The meaning of a name is distinct from the nature that thing must have in order that the name apply to it.\n\nThis leads to a corresponding distinction between \"nominal\" and \"real\" definitions. A nominal definition is the definition explaining what a word means, i.e. which says what the \"nominal essence\" is, and is definition in the classical sense as given above. A real definition, by contrast, is one expressing the real nature or \"quid rei\" of the thing.\n\nThis preoccupation with essence dissipated in much of modern philosophy. Analytic philosophy in particular is critical of attempts to elucidate the essence of a thing. Russell described essence as \"a hopelessly muddle-headed notion\".\n\nMore recently Kripke's formalisation of possible world semantics in modal logic led to a new approach to essentialism. Insofar as the essential properties of a thing are \"necessary\" to it, they are those things it possesses in all possible worlds. Kripke refers to names used in this way as rigid designators.\n\nA definition may also be classified as an operational definition or theoretical definition.\n\nA homonym is, in the strict sense, one of a group of words that share the same spelling and pronunciation but have different meanings. Thus homonyms are simultaneously homographs (words that share the same spelling, regardless of their pronunciation) \"and\" homophones (words that share the same pronunciation, regardless of their spelling). The state of being a homonym is called homonymy. Examples of homonyms are the pair \"stalk\" (part of a plant) and \"stalk\" (follow/harass a person) and the pair \"left\" (past tense of leave) and \"left\" (opposite of right). A distinction is sometimes made between \"true\" homonyms, which are unrelated in origin, such as \"skate\" (glide on ice) and \"skate\" (the fish), and polysemous homonyms, or polysemes, which have a shared origin, such as \"mouth\" (of a river) and \"mouth\" (of an animal).\n\nPolysemy is the capacity for a sign (such as a word, phrase, or symbol) to have multiple meanings (that is, multiple semes or sememes and thus multiple senses), usually related by contiguity of meaning within a semantic field. It is thus usually regarded as distinct from homonymy, in which the multiple meanings of a word may be unconnected or unrelated.\n\nIn mathematics, definitions are generally not used to describe existing terms, but to give meaning to a new term. The meaning of a mathematical statement changes if definitions change. The precise meaning of a term given by a mathematical definition is often different than the English definition of the word used, which can lead to confusion for students who do not pay close attention to the definitions given.\n\nAuthors have used different terms to classify definitions used in formal languages like mathematics. Norman Swartz classifies a definition as \"stipulative\" if it is intended to guide a specific discussion. A stipulative definition might be considered a temporary, working definition, and can only be disproved by showing a logical contradiction. In contrast, a \"descriptive\" definition can be shown to be \"right\" or \"wrong\" with reference to general usage.\n\nSwartz defines a \"precising definition\" as one that extends the descriptive dictionary definition (lexical definition) for a specific purpose by including additional criteria. A precising definition narrows the set of things that meet the definition.\n\nC.L. Stevenson has identified \"persuasive definition\" as a form of stipulative definition which purports to state the \"true\" or \"commonly accepted\" meaning of a term, while in reality stipulating an altered use (perhaps as an argument for some specific belief). Stevenson has also noted that some definitions are \"legal\" or \"coercive\" – their object is to create or alter rights, duties, or crimes.\n\nA recursive definition, sometimes also called an \"inductive\" definition, is one that defines a word in terms of itself, so to speak, albeit in a useful way. Normally this consists of three steps:\n\nFor instance, we could define a natural number as follows (after Peano): \n\nSo \"0\" will have exactly one successor, which for convenience can be called \"1\". In turn, \"1\" will have exactly one successor, which could be called \"2\", and so on. Notice that the second condition in the definition itself refers to natural numbers, and hence involves self-reference. Although this sort of definition involves a form of circularity, it is not vicious, and the definition has been quite successful.\n\nIn the same way, we can define ancestor as follows:\nOr simply: an ancestor is a parent or a parent of an ancestor.\n\nIn medical dictionaries, definitions should to the greatest extent possible be:\n\nCertain rules have traditionally been given for definitions (in particular, genus-differentia definitions).\n\nGiven that a natural language such as English contains, at any given time, a finite number of words, any comprehensive list of definitions must either be circular or rely upon primitive notions. If every term of every \"definiens\" must itself be defined, \"where at last should we stop?\" A dictionary, for instance, insofar as it is a comprehensive list of lexical definitions, must resort to circularity.\n\nMany philosophers have chosen instead to leave some terms undefined. The scholastic philosophers claimed that the highest genera (the so-called ten \"generalissima\") cannot be defined, since a higher genus cannot be assigned under which they may fall. Thus being, unity and similar concepts cannot be defined. Locke supposes in \"An Essay Concerning Human Understanding\" that the names of simple concepts do not admit of any definition. More recently Bertrand Russell sought to develop a formal language based on logical atoms. Other philosophers, notably Wittgenstein, rejected the need for any undefined simples. Wittgenstein pointed out in his \"Philosophical Investigations\" that what counts as a \"simple\" in one circumstance might not do so in another. He rejected the very idea that every explanation of the meaning of a term needed itself to be explained: \"As though an explanation hung in the air unless supported by another one\", claiming instead that explanation of a term is only needed to avoid misunderstanding.\n\nLocke and Mill also argued that individuals cannot be defined. Names are learned by connecting an idea with a sound, so that speaker and hearer have the same idea when the same word is used. This is not possible when no one else is acquainted with the particular thing that has \"fallen under our notice\". Russell offered his theory of descriptions in part as a way of defining a proper name, the definition being given by a definite description that \"picks out\" exactly one individual. Saul Kripke pointed to difficulties with this approach, especially in relation to modality, in his book \"Naming and Necessity\".\n\nThere is a presumption in the classic example of a definition that the \"definiens\" can be stated. Wittgenstein argued that for some terms this is not the case. The examples he used include \"game\", \"number\" and \"family\". In such cases, he argued, there is no fixed boundary that can be used to provide a definition. Rather, the items are grouped together because of a family resemblance. For terms such as these it is not possible and indeed not necessary to state a definition; rather, one simply comes to understand the \"use\" of the term.\n\n\n\n"}
{"id": "2955590", "url": "https://en.wikipedia.org/wiki?curid=2955590", "title": "Dignitas (Roman concept)", "text": "Dignitas (Roman concept)\n\nDignitas is a Latin word referring to a unique, intangible, and culturally subjective social concept in the ancient Roman mindset. The word does not have a direct translation in English. Some interpretations include \"dignity\", which is a \"derivation\" from \"dignitas\", and \"prestige\" or \"charisma\".\n\nWith respect to ancient Rome, \"dignitas\" was regarded as the sum of the personal clout and influence that a male citizen acquired throughout his life. When weighing the \"dignitas\" of a particular individual, factors such as personal reputation, moral standing, and ethical worth had to be considered, along with the man's entitlement to respect and proper treatment.\n\nAuthors who had used \"dignitas\" extensively in their writings and oratories include Cicero, Julius Caesar, Tacitus, and Livy. The most prolific user was Cicero, who initially related it to the established term \"auctoritas\" (authority). These two words were highly associated, with the latter defined as the expression of a man's \"dignitas\".\n\nThe cultivation of \"dignitas\" in ancient Rome was extremely personal. Men of all classes, most particularly noblemen of consular families, were highly protective and zealous of this asset. This is because every man who took on a higher political office during the Roman Republic considered \"dignitas\" as comprising much more than just his dignity. It referred to his \"good name\" (his past and present reputation, achievement, standing, and honor). Its importance within the hierarchical classes of Roman society meant many historical figures would kill, commit suicide (e.g. Mark Antony), or enter exile in order to preserve their \"dignitas\".\n\nThe personal significance of one's \"dignitas\" had encouraged several conflicts in ancient Rome. Florus claimed that the stubbornness of Cato the Younger had driven Pompeius Magnus to prepare defenses in order to build up his \"dignitas\". Cicero wrote that Caesar valued his status so greatly that he did not want anyone to be his equal in \"dignitas\". Aulus Hirtius had written that Marcus Claudius Marcellus, who was one of the instigators of Caesar’s recall from Gaul, had attempted to build all of his own reputation on his success on turning people’s feelings against Caesar. Whether the exact term was used much during these times is unknown; however, the concept of \"dignitas\" was certainly influential and worth fighting for.\n\nOver the course of ancient Roman history, \"dignitas\" had never taken on all of the aforementioned descriptions simultaneously. The term took on different meanings over time, adjusting for the gradually changing viewpoints of society, politicians, and the various authors.\n\nYears after Caesar's death, his heir Augustus rejected the contemporary meaning of \"dignitas\". Augustus found the related term \"auctoritas\" to be a suitable alternative.\n\nIn 46 BC, Cicero cited the ambiguous nature of the concept of \"dignitas\". He wrote, \"And so I have, if loyal feeling for the state and winning good men's approval of those loyal feelings is all that \"dignitas\" amounts to; but if in \"dignitas\" you include the power of translating those loyal feelings into action or of defending them with complete freedom, then \"ne vestigium quidem ullum est reliquum nobis dignitatis\" [not even a trace is left to us of our dignity].\"\n\nWhen paired with the term \"otium\", the word \"dignitas\" took on a different meaning. Cicero did not consider himself worthy of having \"dignitas\" alone because he felt that—by turning his back on the Roman public—he had neglected the duty of one whose life had normally exemplified the concept. He then altered the definition to mean \"[lifetime] impact,\" to better describe his unique status. By this time, Cicero's political life had ended, and he labeled his past political influence as his \"dignitas\", and his present standing as \"otium\".\n\n\n"}
{"id": "4251950", "url": "https://en.wikipedia.org/wiki?curid=4251950", "title": "Disjunctive sequence", "text": "Disjunctive sequence\n\nA disjunctive sequence is an infinite sequence (over a finite alphabet of characters) in which every finite string appears as a substring. For instance, the binary Champernowne sequence\n\nformed by concatenating all binary strings in shortlex order, clearly contains all the binary strings and so is disjunctive. (The spaces above are not significant and are present solely to make clear the boundaries between strings). The complexity function of a disjunctive sequence \"S\" over an alphabet of size \"k\" is \"p\"(\"n\") = \"k\".\n\nAny normal sequence (a sequence in which each string of equal length appears with equal frequency) is disjunctive, but the converse is not true. For example, letting 0\"\" denote the string of length \"n\" consisting of all 0s, consider the sequence\n\nobtained by splicing exponentially long strings of 0s into the shortlex ordering of all binary strings. Most of this sequence consists of long runs of 0s, and so it is not normal, but it is still disjunctive.\n\nA disjunctive sequence is recurrent but never uniformly recurrent/almost periodic.\n\nThe following result can be used to generate a variety of disjunctive sequences:\n\nTwo simple cases illustrate this result:\nare disjunctive on the respective digit sets.\n\nAnother result that provides a variety of disjunctive sequences is as follows:\n\nE.g., using base-ten expressions, the sequences\nare disjunctive on {0,1,2,3,4,5,6,7,8,9}.\n\nA rich number or disjunctive number is a real number whose expansion with respect to some base \"b\" is a disjunctive sequence over the alphabet {0...,\"b\"−1}. Every normal number in base \"b\" is disjunctive but not conversely. The real number \"x\" is rich in base \"b\" if and only if the set { \"x b\" mod 1} is dense in the unit interval.\n\n"}
{"id": "1813344", "url": "https://en.wikipedia.org/wiki?curid=1813344", "title": "Eastern elk", "text": "Eastern elk\n\nThe eastern elk (\"Cervus canadensis canadensis\") was a subspecies or distinct population of elk that inhabited the northern and eastern United States, and southern Canada. The last eastern elk was shot in Pennsylvania on September 1, 1877. The subspecies was declared extinct by the United States Fish and Wildlife Service in 1880. Another subspecies of elk, the Merriam's elk, also became extinct at roughly the same time.\n\nAs of 2017, the IUCN has reclassified all North American elk subspecies aside from the tule and Roosevelt elk as \"C. c. canadensis\". If this is accurate, this means that the subspecies has returned to the eastern US as the Rocky Mountain elk reintroduced to the region since the 20th century.\n\nThe eastern elk was larger than its western cousins. A full-grown bull could weigh up to 1000 pounds, stand 50-60 inches tall at the shoulder, and carry a rack of antlers six feet in length.\n\nBy the late 15th century, elk were the most widespread in the New World and could be found throughout most of North America. Eastern elk inhabited the vast forests of eastern Canada and the eastern United States as far west as the Mississippi River. As people continued to settle in the region over the next few centuries, elk populations decreased due to over-hunting and the loss of their dense woodland habitat. Naturalist John James Audubon reportedly mentioned that by 1851 a few elk could still be found in the Allegheny Mountains but that they were virtually gone from the remainder of their range. By the end of the 19th century the Eastern elk was completely extinct. What little is known about this race of elk has been gleaned from remains and historical references. Mitochondrial DNA studies in 2004 indicate that \"Cervus canadensis\" are a species distinct from European red deer.\n\nNot long after the last elk was killed in Pennsylvania, federal officials, worried about mushrooming elk herds in and around Yellowstone National Park, offered the animals to anyone willing to take them. The recently formed Pennsylvania Game Commission took Yellowstone officials up on their offer, and launched a program to reintroduce elk to Pennsylvania. Starting in 1913 and ending in 1926, the Commission released 177 elk in 10 counties, including 50 animals from Yellowstone. Currently, Pennsylvania's elk herd numbers more than 800 and their range covers approximately 800 square miles.\n\nIn 1990, feasibility studies were conducted to determine if wild, free-ranging elk still had a place in some of their former eastern haunts. Once this was complete, healthy source herds of Rocky Mountain elk from Arizona, Kansas, New Mexico, North Dakota, Oregon, Utah and Alberta’s Elk Island National Park were used to introduce elk back into the former eastern elk range.\n\nSuccessful elk populations have now been introduced in Arkansas (1991), Wisconsin (1995), Ontario (2001), Kentucky, Tennessee and Great Smoky Mountains National Park in 2002, the area known as Northern Michigan, the Missouri Ozarks (2011), and in 2012 Virginia. In late 2016, elk were reintroduced into southern West Virginia. In addition, feasibility studies have also been completed in Illinois and New York (although these have not yet resulted in any elk restorations).\n\nThere may be more remaining of the eastern elk than old skeletons. In 1905, 18 elk were introduced to Fiordland National Park in New Zealand—a gift from Theodore Roosevelt. The elk were survivors of an original shipment of 20, half of which came from Yellowstone National Park and half from an Indian game reserve in Brookfield, Massachusetts, owned by H.E. Richardson. The latter are believed to be eastern elk captured in northern Minnesota by Native Americans. The possible eastern elk bloodline might explain some unusual characteristics he has seen in New Zealand elk, such as \"bifurcated\" antlers in which the dagger, or fourth point, forks at the tip.\n\nHowever, the likelihood of a pure bloodline is very low. Even though the animal population had successfully adapted to the harsh terrain, several factors likely contributed to a dilution of the pure gene pool. To wit, removal of protection in 1935; the crossbreeding with red deer that spread into the area; the gazetting of the Fiordland region as a national park in 1952; and the resulting status of the elk and all introduced game species being relegated to that of noxious animals, or pests, by the government agencies of the time has seen the wild herd go into decline. Today, that herd is but a shadow of its former self, being comprised now only of crossbreeds of varying degree that have defied the efforts of\ngovernment agencies to exterminate or remove them from Fiordland.\n\nEastern elk could have also hung on in the extensive forests of Ontario. While evidence is sketchy, numerous people reported seeing a band of elk near Sault Ste. Marie in the early 1980s. These elk could be of eastern origin—and could still exist in the wilds of Ontario.\n\n\n"}
{"id": "939470", "url": "https://en.wikipedia.org/wiki?curid=939470", "title": "Extended producer responsibility", "text": "Extended producer responsibility\n\nIn the field of waste management, extended producer responsibility (EPR) is a strategy designed to promote the integration of environmental costs associated with goods throughout their life cycles into the market price of the products. Extended producer responsibility legislation is a driving force behind the adoption of remanufacturing initiatives as it\n\"focuses on the end-of-use treatment of consumer products and has the primary aim to increase the amount and degree of product recovery and to minimize the environmental impact of waste materials\"\n\nThe concept was first formally introduced in Sweden by Thomas Lindhqvist in a 1990 report to the Swedish Ministry of the Environment. In subsequent reports prepared for the Ministry, the following definition emerged: \"[EPR] is an environmental protection strategy to reach an environmental objective of a decreased total environmental impact of a product, by making the manufacturer of the product responsible for the entire life-cycle of the product and especially for the take-back, recycling and final disposal.\n\nEPR uses financial incentives to encourage manufacturers to design environmentally friendly products by holding producers responsible for the costs of managing their products at end of life. This policy approach, which differs from product stewardship, which shares responsibility across the chain of custody of a product, attempts to relieve local governments of the costs of managing certain priority products by requiring manufacturers internalize the cost of recycling within the product price. EPR is based upon the principle that because producers (usually brand owners) have the greatest control over product design and marketing and these same companies have the greatest ability and responsibility to reduce toxicity and waste.\n\nEPR may take the form of a reuse, buy-back, or recycling program. The producer may also choose to delegate this responsibility to a third party, a so-called \"producer responsibility organization\" (PRO), which is paid by the producer for used-product management. In this way, EPR shifts the responsibility for waste management from government to private industry, obliging producers, importers and/or sellers to internalise waste management costs in their product prices and ensuring the safe handling of their products.\n\nA good example for producer responsibility organizations are the member organizations of PRO EUROPE. PRO EUROPE s.p.r.l. (Packaging Recovery Organisation Europe), founded in 1995, is the umbrella organization for European packaging and packaging waste recovery and recycling schemes. Product stewardship organizations like PRO EUROPE are intended to relieve industrial companies and commercial enterprises of their individual obligation to take back used products through the operation of an organization which fulfills these obligations on a nationwide basis on behalf of their member companies. The aim is to ensure the recovery and recycling of packaging waste in the most economically efficient and ecologically sound manner. In many countries, this is done through the Green Dot trademark of which PRO EUROPE is the general licensor. The Green Dot has evolved into a proven concept in many countries as implementation of Producer Responsibility. In twenty-five nations companies are now using the Green Dot as the financing symbol for the organization of recovery, sorting and recycling of sales packaging.\n\nIn response to all of the growing problem of excessive waste, several countries adopted waste management policies in which manufacturers are responsible for taking back their products from end users at the end of the products' useful life, or partially financing a collection and recycling infrastructure. These policies were adopted due to the lack of collection infrastructure for certain products that contain hazardous materials, or due to the high costs to local governments of providing such collection services. The primary goals of these take-back laws therefore are to partner with the private sector to ensure that all wastes are managed in a way that protects public health and the environment.\nThe goals of take-back laws are to\nTake-back programs help promote these goals by creating incentives for companies to redesign their products to minimize waste management costs, by designing their products to contain safer materials (so they do not need to be managed separately) or designing products that are easier to recycle and reuse (so recycling becomes more profitable). The earliest take-back activity began in Europe, where government-sponsored take-back initiatives arose from concerns about scarce landfill space and potentially hazardous substances in component parts. The European Union adopted a directive on Waste Electrical and Electronic Equipment (WEEE). The purpose of this directive is to prevent the production of waste electronics and also to encourage reuse and recycling of such waste. The directive requires the Member States to encourage design and production methods that take into account the future dismantling and recovery of their products. These take-back programs have been now adopted in nearly every OECD country. In the United States, most of these policies have been implemented at the state level, due to the political impasse at the federal level.\n\nRecycling, banning, and taxation fails to adequately reduce the pollution caused by plastic bags. An alternative to these policies would be to increase extended producer responsibility. In the US, under the Clinton presidency, the President's Council on Sustainable Development suggested EPR in order to target different participants in the cycle of a product's life. This can, however, make the product more expensive since the cost must be taken into consideration before being put on the market, which is why it is not widely used in the United States currently. Instead, there is banning or taxation of plastic bags, which puts the responsibility on the consumers. In the United States, EPR has not successfully been made mandatory, instead being voluntary. What has been recommended is a comprehensive program which combines taxation, producer responsibility, and recycling to combat pollution.\n\nMany governments and companies have adopted extended producer responsibility to help address the growing problem of e-waste — used electronics contain materials that cannot be safely thrown away with regular household trash. In 2007, according to the Environmental Protection Agency, people threw away 2.5 million tons of cell phones, TVs, computers, and printers. Many governments have partnered with corporations in creating the necessary collection and recycling infrastructure. Some argue that local and manufacturer-supported extended producer responsibility laws give manufacturers greater responsibility for the reuse, recycling, and disposal of their own products.\n\nThe kinds of chemicals found in e-waste that are particularly dangerous to human health and the environment are lead, mercury, brominated flame-retardants, and cadmium. Lead is found in the screens of phones, TVs and computer monitors and can damage kidneys, nerves, blood, bones, reproductive organs, and muscles. Mercury is found in the bulbs in flat screen TVs, laptop screens, and fluorescent bulbs, and can cause damage to the kidneys and the nervous system. Brominated flame-retardants found in cables and plastic cases can cause cancer, disruption of liver function, and nerve damage. Cadmium is found in rechargeable batteries and can cause kidney damage and cancer. Poorer countries are dumping grounds for the United States' e-waste as many governments accept money for disposing this waste on their lands. This causes increased health risks for people in these countries, especially ones who work or live close to these dumps.\n\nIn the United States, 25 states have implemented laws that require the recycling of electronic waste. Of those, 23 have incorporated some form of extended producer responsibility into their laws. According to analysis done by the Product Stewardship Institute, some states have not enacted EPR laws because of a lack of recycling infrastructure and funds for proper e-waste disposal. In contrast, according to a study of EPR legislation done by the Electronics TakeBack Coalition, states that have seen success in their e-waste recycling programs have done so because they have developed a convenient e-waste infrastructure or the state governments have instituted goals for manufacturers to meet. Essentially, these EPR programs have included some driver for increased collection of e-waste, and that is why these states have seen a greater impact on proper e-waste disposal than others. \n\nAdvocates for EPR also argue that including \"high expectations for performance\" into the laws, and ensuring that those are only minimum requirements, contribute to making the laws successful. In this way, manufacturers can be incentivized to collect more and dispose of e-waste more properly. Finally, the larger the scope of products that can be collected, the more e-waste will be disposed of properly.\n\nSimilar laws have been passed in other parts of the world as well. The European Union has taken steps to combat the issue of electronic waste management. They have restricted the use of harmful substances in member countries and have made it illegal to export waste. \n\nThe Chinese laws regarding e-waste are similar to the ones in the EU, but they focus on banning the import of e-waste. This has proven to be difficult, however, because illegal smuggling of waste still occurs in the country. In order to dispose of e-waste in China today, a license is required and plants are held responsible for treating pollution.\n\nWhen producers either face a financial or physical burden of recycling their electronics after use, they may be incentivized to design more sustainable, less toxic, and easily recyclable electronics. Using fewer materials and designing products to last longer can directly reduce producers' end-of-life costs. Thus, extended producer responsibility is often cited as one way to fight planned obsolescence, because it financially encourages manufacturers to design for recycling and make products last longer.\n\nSome people have concerns about extended producer responsibility programs for complex electronics that can be difficult to safely recycle, such as lithium-ion polymer batteries. Others worry that such laws could increase the cost of electronics because producers would add recycling costs into the initial price tag. When companies are required to transport their products to a recycling facility, it can be expensive if the product contains hazardous materials and does not have a scrap value, such as with CRT televisions, which can contain up to five pounds of lead. Organizations and researchers against EPR claim that the mandate would slow innovation and impede technological progress. \n\nOther critics are concerned that manufacturers may use takeback programs to take secondhand electronics off the reuse market, by shredding rather than reusing or repairing goods that come in for recycling. Another argument against EPR is that EPR policies are not accelerating environmentally-friendly designs because \"manufacturers are already starting to moving toward reduced material-use per unit of output, reduced energy use in making and delivering each product, and improved environmental performance.\"\n\nThe Reason Foundation argues that EPR is not clear in the way fees are established for the particular recycling processes. Fees are set in place to help incentivize recycling, but this may deter the use of manufacturing with better materials for the different electronic products. There are not set fees for certain materials, so confusion occurs when companies do not know what design features to include in their devices.\n\nEPR has been implemented in many forms, which may be classified into three major approaches:\n\nIt is perhaps because of the tendency of economic policy in market-driven economies not to interfere with consumers' preferences that the producer-centric representation is the dominant form of viewing the environmental impacts of industrial production: in statistics on energy, emissions, water, etc., impacts are almost always presented as attributes of industries (\"on-site\" or \"direct\" allocation) rather than as attributes of the supply chains of products for consumers. On a smaller scale, most existing schemes for corporate sustainability reporting include only impacts that arise out of operations controlled by the reporting company, and not supply-chain impacts According to this world view, \"upstream and downstream [environmental] impacts are ... allocated to their immediate producers. The institutional setting and the different actors' spheres of influence are not reflected\".\n\nOn the other hand, a number of studies have highlighted that final consumption and affluence, especially in the industrialised world, are the main drivers for the level and growth of environmental pressure. Even though these studies provide a clear incentive for complementing producer-focused environmental policy with some consideration for consumption-related aspects, demand-side measures to environmental problems are rarely exploited.\n\nThe nexus created by the different views on impacts caused by industrial production is exemplified by several contributions to the discussion about producer or consumer responsibility for greenhouse gas emissions. Emissions data are reported to the IPCC as contributions of producing industries located in a particular country rather than as embodiments in products consumed by a particular population, irrespective of productive origin. However, especially for open economies, taking into account the greenhouse gases embodied in internationally traded commodities can have a considerable influence on national greenhouse gas balance sheets. Assuming consumer responsibility, exports have to be subtracted from, and imports added to national greenhouse gas inventories. In Denmark, for example, Munksgaard and Pedersen (2001) report that a significant amount of power and other energy-intensive commodities are traded across Danish borders, and that between 1966 and 1994 the Danish foreign trade balance in terms of CO developed from a 7 Mt deficit to a 7 Mt surplus, compared to total emissions of approximately 60 Mt. In particular, electricity traded between Norway, Sweden and Denmark is subject to large annual fluctuations due to varying rainfall in Norway and Sweden. In wet years Denmark imports hydro-electricity whereas electricity from coal-fired power plants is exported in dry years. The official Danish emissions inventory includes a correction for electricity trade and thus applies the consumer responsibility principle.\n\nSimilarly, at the company level, \"when adopting the concept of eco-efficiency and the scope of an environmental management system stated in for example ISO 14001, it is insufficient to merely report on the carbon dioxide emissions limited to the judicial borders of the company\". 7 \"Companies must recognise their wider responsibility and manage the entire life-cycle of their products ... Insisting on high environmental standards from suppliers and ensuring that raw materials are extracted or produced in an environmentally conscious way provides a start.\" A life-cycle perspective is also taken in EPR frameworks: \"Producers of products should bear a significant degree of responsibility (physical and/or financial) not only for the environmental impacts of their products downstream from the treatment and disposal of their product, but also for their upstream activities inherent in the selection of materials and in the design of products.\" \"The major impetus for EPR came from northern European countries in the late 1980s and early 1990s, as they were facing severe landfill shortages. [... As a result,] EPR is generally applied to post-consumer wastes which place increasing physical and financial demands on municipal waste management.\"\n\nEPR has rarely been consistently quantified. Moreover, applying conventional life cycle assessment, and assigning environmental impacts to producers and consumers can lead to double-counting. Using input-output analysis, researchers have attempted for decades to account for both producers and consumers in an economy in a consistent way. Gallego and Lenzen demonstrate and discuss a method of consistently delineating producers' supply chains, into mutually exclusive and collectively exhaustive responsibilities to be shared by all agents in an economy. Their method is an approach to allocating responsibility across agents in a fully inter-connected circular system. Upstream and downstream environmental impacts are shared between all agents of a supply chain – producers and consumers.\n\nAuto Recycling Nederland (ARN) is a producer responsibility organisation (PRO) that organises vehicle recycling in the Netherlands. An advanced recycling fee is charged to those who purchase a new vehicle and is used to fund the recycling of it at the end of its useful life. The PRO was set up to satisfy the European Union's End of Life Vehicles Directive.\n\nThe Swiss Association for Information, Communication and Organisational Technology (SWICO), an ICT industry organisation, became a PRO to address the problem of electronic waste.\n\nThe Canada-Wide Action Plan for Extended Producer Responsibility (CAP-EPR) was adopted in Canada in 2009 under the guidance of the Canadian Council of Ministers of the Environment. The CAP-EPR followed years of waste and recycling efforts in Canada that remained largely ineffective as the diversion rates from landfills and incineration persisted. Despite three decades worth of recycling efforts, Canada fell short of many other G8 and OECD countries. Since the CAP-EPR’s 2009 inception, most provinces have enforced legislation or restrictions on a wider range of products and materials under EPR programs. “Nine out of ten provinces have [since implemented] EPR programs or [put] requirements in place… As a result of these new programs or requirements and expansion of existing ones, almost half of the product categories for Phase 1 are now covered by legislated EPR programs or requirements across Canada.”\n\nIn Germany, since the adoption of EPR, \"between 1991 and 1998, the per capita consumption of packaging was reduced from 94.7 kg to 82 kg, resulting in a reduction of 13.4%\". Furthermore, due to Germany's influence in EPR, the \"European Commission developed one waste directive\" for all of member states (Hanisch 2000). One major goal was to have all member states recycle \"25% of all packaging material\" and have accomplished the goal.\n\nIn the United States, EPR is gaining popularity \"with 40 such laws enacted since 2008. In 2010 alone, 38 such EPR bills were introduced in state legislatures across the United States, and 12 were signed into law.\" However, these laws are only at the state level as there are no federal laws for EPR. So far, \"only a handful of states have imposed five to six EPR laws as well as 32 states having at least one EPR law\".\n\n\n\n"}
{"id": "39265626", "url": "https://en.wikipedia.org/wiki?curid=39265626", "title": "Feminist stripper", "text": "Feminist stripper\n\nA feminist stripper is a professional exotic dancer who does not conform to the stereotypes associated with exotic dancers, and instead identifies positively with the identity of stripper. Feminist strippers are sex-positive feminists who view their profession as a choice and a career field. Feminist strippers interact with their profession in a positive manner and view it as a female centric form of power. They receive this sense of power by asserting their autonomy and by making informed decisions in regard to the regulation of their bodies.\n\nThe autonomy used by feminist strippers is seen through their ability to choose who they perform for, when they will perform, and how long it will last. They chose which types of performances they will perform. According to Meyer's \"since one must exercise control over one's life to be autonomous, autonomy is something that a person accomplishes, not something that happens to persons. But freedom is precisely a combination of self-creation and what happens to you”. There is liberation in the free expression of their sexuality. Becoming a stripper can give a \"curious kind of control over those who watched\". Roberta Perkin's work with sex workers [location] showed that sex workers \"exerted a significant amount of control over their working lives, felt empowered by their work and most were not arrested or subjected to violence.\" \n\nFeminist strippers find benefits from their work, including but not limited to, the ability to create their own hours, and work on a pay scale that is a direct reflection of their skills and talents. Feminist strippers receive a sense of empowerment from performing for an audience on stage. Feminist strippers exercise their freedom of sexuality, that is their freedom to choose how to use their bodies in interaction with their environment, through their performances, and this freedom contributes to a feeling of power and control. The amount of pay received can place strippers in an economic position of superiority over women in other occupations.\n\nBeing a stripper subjects these women to criticism from many sides. They are commonly perceived negatively by society and by working in their field are subjected to surveillance, arrest, detention, forced venereal disease testing, extortion, violence and rejection from family and friends. The type of work and the place of work can hamper or make impossible negotiation of workplace benefits such as sick leave, disability leave, or pension plans.\n\nOccupational health issues remain a concern in a stripper's work. Stripping is a physically demanding job, and strippers later in life can experience physical pain and injury.\n\nStrippers face the hardship of defending their work as legitimate work, and defending their justification of using their bodies for money.\n\nSafety is another point of concern and caution in a stripper's work place. Bouncers and other employees can ensure that the workers are kept out of harm's way at all times. However, unwanted touching, derogatory language, and sexist comments can all occur without protection from bouncers. This possibly unsafe and often uncomfortable environment contributes to a state of constant awareness within strip clubs.\n\nThe many different employees within strip clubs all contribute to a club's social structure and ultimately hierarchy. The job itself is overseen and controlled by the other workers within the club. \"Absolute control by male club managers and bosses... tends to reproduce in the crudest possible way the structure of patriarchal power and female dependence\" (buying power).. Strippers must negotiate with men within a stigmatized, male-controlled profession. Because they are generally paid in cash, strippers must negotiate with and share their earnings with booking agents, and pay fines to club owners for infractions such as showing up late or skipping a gig.\n\nThe mainstream media has perpetuated the virgin/whore dichotomy to a point of a selfless identity for the everyday woman whose career is stripping. The movies \"Striptease\" and \"Showgirls\" have brought to life a few sides of the stripper that is now seen as typical. She is either portrayed as a home-wrecking, unintelligent, thief, or she is a mother doing what needs to be done to put food on the table for her child. These mainstream stereotypes fail to portray the woman who takes pleasure in her job and the woman who chooses to have this job. The idea of the good stripper and the bad stripper are perpetuated though mainstream media. A good stripper hates her job and the bad stripper enjoys it.\n\n\"As an illustration, consider the situation of a young dancer who decides to undergo breast augmentation to increase her income per evening. After the surgery, she receives more attention, compliments, and money. Consequently, our hypothetical dancer feels more confident and powerful approaching men and soliciting dances, supporting the analysis of the sex radical feminists in her individual life. Simultaneously, from a radical feminist perspective (structural level), she is participating in and perpetuating an institutionalized beauty myth that reinforces women's subordinate status by rendering their value dependent on the approval of men.\"\n\nThere are many debates surrounding the justification of viewing stripping as a feminist action. Many feminists find stripping to violate human rights and dignity, saying that stripping, and sexual exploitation will be the \"end of feminism.\" This criticism is supported by societal views of women who engage in sexual acts for money, and the social stigmas associated with sex acts. Social stigmas surrounding stripping such as gang affiliation, drug use, STDs, and prostitution contribute to the criticism of women who become exotic dancers.\n\nFeminists and non feminists alike can view stripping as exploitation of the female body, \"the act of stripping overvalues physical perfection and abstracts the sexual qualities of the female body, it places voyeuristic men in the position of judges and arbiters of the female face.\" However, feminist strippers find the use of their body as a potential tool, to be sexually, psychologically, and emotionally empowering. Feminist strippers argue that they receive positive reinforcement while performing, and this reinforcement perpetuates their self empowerment. A main argument of feminism is the exploitation of the female body by the dominant group, i.e. males, is a main concern that perpetuates the domination of females because of their body. Many find problems with self-identified feminist strippers, because of this argument.\n\n\n"}
{"id": "35171726", "url": "https://en.wikipedia.org/wiki?curid=35171726", "title": "Field effect (semiconductor)", "text": "Field effect (semiconductor)\n\nIn physics, the field effect refers to the modulation of the electrical conductivity of a material by the application of an external electric field.\n\nIn a metal, the electron density that responds to applied fields is so large that an external electric field can penetrate only a very short distance into the material. However, in a semiconductor the lower density of electrons (and possibly holes) that can respond to an applied field is sufficiently small that the field can penetrate quite far into the material. This field penetration alters the conductivity of the semiconductor near its surface, and is called the \"field effect\". The field effect underlies the operation of the Schottky diode and of field-effect transistors, notably the MOSFET, the JFET and the MESFET. \n\nThe change in surface conductance occurs because the applied field alters the energy levels available to electrons to considerable depths from the surface, and that in turn changes the occupancy of the energy levels in the surface region. A typical treatment of such effects is based upon a \"band-bending diagram\" showing the positions in energy of the \"band edges\" as a function of depth into the material. \n\nAn example band-bending diagram is shown in the figure. For convenience, energy is expressed in eV and voltage is expressed in volts, avoiding the need for a factor \"q\" for the elementary charge. In the figure, a two-layer structure is shown, consisting of an insulator as left-hand layer and a semiconductor as right-hand layer. An example of such a structure is the \"MOS capacitor\", a two-terminal structure made up of a metal \"gate\" contact, a semiconductor \"body\" (such as silicon) with a body contact, and an intervening insulating layer (such as silicon dioxide, hence the designation \"O\"). The left panels show the lowest energy level of the conduction band and the highest energy level of the valence band. These levels are \"bent\" by the application of a positive voltage \"V\". By convention, the energy of electrons is shown, so a positive voltage penetrating the surface \"lowers\" the conduction edge. A dashed line depicts the occupancy situation: below this Fermi level the states are more likely to be occupied, the conduction band moves closer to the Fermi level, indicating more electrons are in the conducting band near the insulator.\n\nThe example in the figure shows the Fermi level in the bulk material beyond the range of the applied field as lying close to the valence band edge. This position for the occupancy level is arranged by introducing impurities into the semiconductor. In this case the impurities are so-called \"acceptors\" which soak up electrons from the valence band becoming negatively charged, immobile ions embedded in the semiconductor material. The removed electrons are drawn from the valence band levels, leaving vacancies or \"holes\" in the valence band. Charge neutrality prevails in the field-free region because a negative acceptor ion creates a positive deficiency in the host material: a hole is the absence of an electron, it behaves like a positive charge. Where no field is present, neutrality is achieved because the negative acceptor ions exactly balance the positive holes.\n\nNext the band bending is described. A positive charge is placed on the left face of the insulator (for example using a metal \"gate\" electrode). In the insulator there are no charges so the electric field is constant, leading to a linear change of voltage in this material. As a result, the insulator conduction and valence bands are therefore straight lines in the figure, separated by the large insulator energy gap. \n\nIn the semiconductor at the smaller voltage shown in the top panel, the positive charge placed on the left face of the insulator lowers the energy of the valence band edge. Consequently, these states are fully occupied out to a so-called \"depletion depth\" where the bulk occupancy reestablishes itself because the field cannot penetrate further. Because the valence band levels near the surface are fully occupied due to the lowering of these levels, only the immobile negative acceptor-ion charges are present near the surface, which becomes an electrically insulating region without holes (the \"depletion layer\"). Thus, field penetration is arrested when the exposed negative acceptor ion charge balances the positive charge placed on the insulator surface: the depletion layer adjusts its depth enough to make the net negative acceptor ion charge balance the positive charge on the gate. \n\nThe conduction band edge also is lowered, increasing electron occupancy of these states, but at low voltages this increase is not significant. At larger applied voltages, however, as in the bottom panel, the conduction band edge is lowered sufficiently to cause significant population of these levels in a narrow surface layer, called an \"inversion\" layer because the electrons are opposite in polarity to the holes originally populating the semiconductor. This onset of electron charge in the inversion layer becomes very significant at an applied \"threshold\" voltage, and once the applied voltage exceeds this value charge neutrality is achieved almost entirely by addition of electrons to the inversion layer rather than by an increase in acceptor ion charge by expansion of the depletion layer. Further field penetration into the semiconductor is arrested at this point, as the electron density increases exponentially with band-bending beyond the threshold voltage, effectively \"pinning\" the depletion layer depth at its value at threshold voltages.\n"}
{"id": "58527", "url": "https://en.wikipedia.org/wiki?curid=58527", "title": "Finitely generated abelian group", "text": "Finitely generated abelian group\n\nIn abstract algebra, an abelian group is called finitely generated if there exist finitely many elements \"x\", ..., \"x\" in \"G\" such that every \"x\" in \"G\" can be written in the form\nwith integers \"n\", ..., \"n\". In this case, we say that the set is a \"generating set\" of \"G\" or that \"x\", ..., \"x\" \"generate\" \"G\".\n\nEvery finite abelian group is finitely generated. The finitely generated abelian groups can be completely classified.\n\n\nThere are no other examples (up to isomorphism). In particular, the group formula_4 of rational numbers is not finitely generated: if formula_5 are rational numbers, pick a natural number formula_6 coprime to all the denominators; then formula_7 cannot be generated by formula_5. The group formula_9 of non-zero rational numbers is also not finitely generated. The groups of real numbers under addition formula_10 and non-zero real numbers under multiplication formula_11 are also not finitely generated.\n\nThe fundamental theorem of finitely generated abelian groups can be stated two ways, generalizing the two forms of the fundamental theorem of \"finite\" abelian groups. The theorem, in both forms, in turn generalizes to the structure theorem for finitely generated modules over a principal ideal domain, which in turn admits further generalizations.\n\nThe primary decomposition formulation states that every finitely generated abelian group \"G\" is isomorphic to a direct sum of primary cyclic groups and infinite cyclic groups. A primary cyclic group is one whose order is a power of a prime. That is, every finitely generated abelian group is isomorphic to a group of the form\nwhere \"n\" ≥ 0 is the \"rank\", and the numbers \"q\", ..., \"q\" are powers of (not necessarily distinct) prime numbers. In particular, \"G\" is finite if and only if \"n\" = 0. The values of \"n\", \"q\", ..., \"q\" are (up to rearranging the indices) uniquely determined by \"G\".\n\nWe can also write any finitely generated abelian group \"G\" as a direct sum of the form\nwhere \"k\" divides \"k\", which divides \"k\" and so on up to \"k\". Again, the rank \"n\" and the \"invariant factors\" \"k\", ..., \"k\" are uniquely determined by \"G\" (here with a unique order). The rank and the sequence of invariant factors determine the group up to isomorphism.\n\nThese statements are equivalent as a result of the Chinese remainder theorem, which implies that formula_14 if and only if \"j\" and \"k\" are coprime.\n\nThe history and credit for the fundamental theorem is complicated by the fact that it was proven when group theory was not well-established, and thus early forms, while essentially the modern result and proof, are often stated for a specific case. Briefly, an early form of the finite case was proven in , the finite case was proven in , and stated in group-theoretic terms in . The finitely \"presented\" case is solved by Smith normal form, and hence frequently credited to , though the finitely \"generated\" case is sometimes instead credited to ; details follow.\n\nGroup theorist László Fuchs states:\nThe fundamental theorem for \"finite\" abelian groups was proven by Leopold Kronecker in , using a group-theoretic proof, though without stating it in group-theoretic terms; a modern presentation of Kronecker's proof is given in , 5.2.2 Kronecker's Theorem, 176–177. This generalized an earlier result of Carl Friedrich Gauss from \"Disquisitiones Arithmeticae\" (1801), which classified quadratic forms; Kronecker cited this result of Gauss's. The theorem was stated and proved in the language of groups by Ferdinand Georg Frobenius and Ludwig Stickelberger in 1878. Another group-theoretic formulation was given by Kronecker's student Eugen Netto in 1882.\n\nThe fundamental theorem for \"finitely presented\" abelian groups was proven by Henry John Stephen Smith in , as integer matrices correspond to finite presentations of abelian groups (this generalizes to finitely presented modules over a principal ideal domain), and Smith normal form corresponds to classifying finitely presented abelian groups. There is the additional technicality of showing that a finitely \"presented\" abelian group is in fact finitely \"generated\", so Smith's classification is not a complete proof for finitely generated abelian groups.\n\nThe fundamental theorem for \"finitely generated\" abelian groups was proven by Henri Poincaré in , using a matrix proof (which generalizes to principal ideal domains). This was done in the context of computing the\nhomology of a complex, specifically the Betti number and torsion coefficients of a dimension of the complex, where the Betti number corresponds to the rank of the free part, and the torsion coefficients correspond to the torsion part.\n\nKronecker's proof was generalized to \"finitely generated\" abelian groups by Emmy Noether in .\n\nStated differently the fundamental theorem says that a finitely generated abelian group is the direct sum of a free abelian group of finite rank and a finite abelian group, each of those being unique up to isomorphism. The finite abelian group is just the torsion subgroup of \"G\". The rank of \"G\" is defined as the rank of the torsion-free part of \"G\"; this is just the number \"n\" in the above formulas.\n\nA corollary to the fundamental theorem is that every finitely generated torsion-free abelian group is free abelian. The finitely generated condition is essential here: formula_15 is torsion-free but not free abelian.\n\nEvery subgroup and factor group of a finitely generated abelian group is again finitely generated abelian. The finitely generated abelian groups, together with the group homomorphisms, form an abelian category which is a Serre subcategory of the category of abelian groups.\n\nNote that not every abelian group of finite rank is finitely generated; the rank 1 group formula_15 is one counterexample, and the rank-0 group given by a direct sum of countably infinitely many copies of formula_17 is another one.\n\n\n"}
{"id": "39756603", "url": "https://en.wikipedia.org/wiki?curid=39756603", "title": "Generalized filtering", "text": "Generalized filtering\n\nGeneralized filtering is a generic Bayesian filtering scheme for nonlinear state-space models. It is based on a variational principle of least action, formulated in generalized coordinates. Note that the concept of \"generalized coordinates\" as used here differs from the concept of generalized coordinates of motion as used in (multibody) dynamical systems analysis. Generalized filtering furnishes posterior densities over hidden states (and parameters) generating observed data using a generalized gradient descent on variational free energy, under the Laplace assumption. Unlike classical (e.g., Kalman-Bucy or particle) filtering, generalized filtering eschews Markovian assumptions about random fluctuations. Furthermore, it operates online, assimilating data to approximate the posterior density over unknown quantities, without the need for a backward pass. Special cases include variational filtering, dynamic expectation maximization and generalized predictive coding.\n\nDefinition: Generalized filtering rests on the tuple formula_1:\nHere ~ denotes a variable in generalized coordinates of motion: formula_11\n\nThe objective is to approximate the posterior density over hidden and control states, given sensor states and a generative model – and estimate the (path integral of) model evidence formula_12 to compare different models. This generally involves an intractable marginalization over hidden states, so model evidence (or marginal likelihood) is replaced with a variational free energy bound. Given the following definitions:\n\nDenote the Shannon entropy of the density formula_15 by formula_16. We can then write the variational free energy in two ways:\n\nThe second equality shows that minimizing variational free energy (i) minimizes the Kullback-Leibler divergence between the variational and true posterior density and (ii) renders the variational free energy (a bound approximation to) the negative log evidence (because the divergence can never be less than zero). Under the Laplace assumption formula_18 the variational density is Gaussian and the precision that minimizes free energy is formula_19. This means that free-energy can be expressed in terms of the variational mean (omitting constants):\n\nThe variational means that minimize the (path integral) of free energy can now be recovered by solving the generalized filter:\n\nwhere formula_22 is a block matrix derivative operator of identify matrices such that formula_23\n\nGeneralized filtering is based on the following lemma: \"The self-consistent solution to\" formula_24 \"satisfies the variational principle of stationary action, where action is the path integral of variational free energy\"\n\nProof: self-consistency requires the motion of the mean to be the mean of the motion and (by the fundamental lemma of variational calculus)\n\nPut simply, small perturbations to the path of the mean do not change variational free energy and it has the least action of all possible (local) paths.\n\nRemarks: Heuristically, generalized filtering performs a gradient descent on variational free energy in a moving frame of reference: formula_27, where the frame itself minimizes variational free energy. For a related example in statistical physics, see Kerr and Graham who use ensemble dynamics in generalized coordinates to provide a generalized phase-space version of Langevin and associated Fokker-Planck equations.\n\nIn practice, generalized filtering uses local linearization over intervals formula_28 to recover discrete updates\n\nThis updates the means of hidden variables at each interval (usually the interval between observations).\n\nUsually, the generative density or model is specified in terms of a nonlinear input-state-output model with continuous nonlinear functions:\n\nThe corresponding generalized model (under local linearity assumptions) obtains the from the chain rule\n\nGaussian assumptions about the random fluctuations formula_32 then prescribe the likelihood and empirical priors on the motion of hidden states\n\nThe covariances formula_34 factorize into a covariance among variables and correlations formula_35 among generalized fluctuations that encodes their autocorrelation:\n\nHere, formula_37 is the second derivative of the autocorrelation function evaluated at zero. This is a ubiquitous measure of roughness in the theory of stochastic processes. Crucially, the precision (inverse variance) of high order derivatives fall to zero fairly quickly, which means it is only necessary to model relatively low order generalized motion (usually between two and eight) for any given or parameterized autocorrelation function.\n\nWhen time series are observed as a discrete sequence of formula_38 observations, the implicit sampling is treated as part of the generative process, where (using Taylor's theorem)\n\nIn principle, the entire sequence could be used to estimate hidden variables at each point in time. However, the precision of samples in the past and future falls quickly and can be ignored. This allows the scheme to assimilate data online, using local observations around each time point (typically between two and eight).\n\nFor any slowly varying model parameters of the equations of motion formula_40 or precision formula_41 generalized filtering takes the following form (where formula_42 corresponds to the variational mean of the parameters)\n\nHere, the solution formula_44 minimizes variational free energy, when the motion of the mean is small. This can be seen by noting formula_45. It is straightforward to show that this solution corresponds to a classical Newton update.\n\nClassical filtering under Markovian or Wiener assumptions is equivalent to assuming the precision of the motion of random fluctuations is zero. In this limiting case, one only has to consider the states and their first derivative formula_46. This means generalized filtering takes the form of a Kalman-Bucy filter, with prediction and correction terms:\n\nSubstituting this first-order filtering into the discrete update scheme above gives the equivalent of (extended) Kalman filtering.\n\nParticle filtering is a sampling-based scheme that relaxes assumptions about the form of the variational or approximate posterior density. The corresponding generalized filtering scheme is called variational filtering. In variational filtering, an ensemble of particles diffuse over the free energy landscape in a frame of reference that moves with the expected (generalized) motion of the ensemble. This provides a relatively simple scheme that eschews Gaussian (unimodal) assumptions. Unlike particle filtering it does not require proposal densities—or the elimination or creation of particles.\n\nVariational Bayes rests on a mean field partition of the variational density:\n\nThis partition induces a variational update or step for each marginal density—that is usually solved analytically using conjugate priors. In generalized filtering, this leads to dynamic expectation maximisation. that comprises a D-step that optimizes the sufficient statistics of unknown states, an E-step for parameters and an M-step for precisions.\n\nGeneralized filtering is usually used to invert hierarchical models of the following form\n\nThe ensuing generalized gradient descent on free energy can then be expressed compactly in terms of prediction errors, where (omitting high order terms):\n\nHere, formula_51 is the precision of random fluctuations at the \"i\"-th level. This is known as generalized predictive coding [11], with linear predictive coding as a special case.\n\nGeneralized filtering has been primarily applied to biological timeseries—in particular functional magnetic resonance imaging and electrophysiological data. This is usually in the context of dynamic causal modelling to make inferences about the underlying architectures of (neuronal) systems generating data. It is also used to simulate inference in terms of generalized (hierarchical) predictive coding in the brain.\n\n\n"}
{"id": "1181974", "url": "https://en.wikipedia.org/wiki?curid=1181974", "title": "Home invasion", "text": "Home invasion\n\nIn some parts of the United States and some other English speaking countries home invasion is an illegal and usually forceful entry to an occupied, private dwelling with intent to commit a violent crime against the occupants, such as robbery, assault, rape, murder, or kidnapping.\n\nIn some jurisdictions, there is a defined crime of home invasion; in others, there is no crime defined as home invasion, but events that accompany the invasion are charged as crimes. Where home invasion is defined, the definition and punishments vary by jurisdiction. It is not a legally defined federal offense throughout the United States, but is in several states, such as Georgia, Michigan, Connecticut, Illinois, Florida, Louisiana, and in Las Vegas, Nevada. Home invasion laws also have been introduced in the South Carolina General Assembly and in the State of Maryland. On March 15, 2011, a bill making home invasion deaths a capital crime in New Hampshire passed the New Hampshire House without debate. Home invasion as such is not defined as a crime in most countries other than the US, with offenders being charged according to the actual crimes committed once inside the building, such as armed robbery, rape or murder. In English law, offenders who commit burglary while carrying a weapon can be convicted of the offence of aggravated burglary even if the weapon is not actually brandished or used.\n\nHome invasion differs from burglary in that its perpetrators have a violent intent apart from the unlawful entry itself, specific or general, much the same way as aggravated robbery—personally taking from someone by force—is differentiated from mere larceny (theft alone).\n\nFew statistics are available on the crime of home invasion as such, because it is not defined as a crime in its own right in most jurisdictions. Statistics about home invasion found on the Internet are often false or misleading. Persons arrested for what the police or media may refer to as \"home invasion\" are actually charged with crimes such as robbery, kidnapping, homicide, rape, or assault.\n\nThe first published use of the term \"home invasion\" recorded in the \"Oxford English Dictionary\" is an article in \"The Washington Post\" on 1 February 1912, with an article in the \"Los Angeles Times\" on 18 March 1925 clearly indicating the modern meaning.\n\n\"Home-invasion robberies\" were highlighted in June 1995, when the term appeared in the cover story of \"The FBI Law Enforcement Bulletin\" in an article written by Police Chief James T. Hurley of the Ft. Lauderdale, Florida, area, later republished on bNet, the online blog posted by Harvard Business School. Hurley posited that, at the time, the crime could be considered an alternative to bank or convenience store robberies, which were becoming more difficult to carry out due to technological advances in security. In the same article Hurley recommended educating the public about home invasion. Before the term \"home invasion\" came in use, the term \"hot burglary\" was often used in the literature. Early references also use \"burglary of occupied homes\" and \"burglar striking an occupied residence\".\n\nConnecticut Congressman Chris Murphy proposed in 2008 making home invasion a federal crime in the United States.\n\nThe \"Chauffeurs de la Drome\" (The Heaters of Drôme) were a gang of four men who carried out a series of attacks on remote dwellings in the Department of Drôme in south-west France between 1905 and 1908. They became notorious for roasting the feet of householders against the fireplace, to torture them into revealing the hiding places of valuables. Responsible for as many as 18 murders, three of the gang were guillotined on September 22, 1909. The fourth died on the penal colony at Devil's Island.\n\nOne well-known home invasion is the November 15, 1959, quadruple murder of the Clutter family by Richard \"Dick\" Hickock and Perry Edward Smith during a home-invasion robbery in rural Holcomb, Kansas. The murders were detailed in Truman Capote's \"nonfiction novel\" \"In Cold Blood\". However, the perpetrators were convicted of murder, not home invasion.\n\nMore recently, two paroled criminals were each charged with three counts of capital murder during a home invasion into the Petit family home in Cheshire, Connecticut, on July 23, 2007. During the invasion, the mother died of asphyxiation due to strangulation and the two daughters died of smoke inhalation after the suspects set the house on fire. The men were charged with first-degree sexual assault, murder of a kidnapped person, and murder of two or more people at the same time. The state attorney sought the death penalty against the suspects. The first defendant, Steven Hayes, was found guilty of 16 of 17 counts including capital murder on October 5, 2010, and on November 8, 2010, was sentenced to death. His co-defendant, Joshua Komisarjevsky, was convicted of all 17 counts against him in October 2011, and was also sentenced to death. Both men later had their sentences commuted to life without parole when Connecticut abolished the death penalty in 2015.\n\nAnother home invasion occurred on November 26, 2007, when Washington Redskins star Sean Taylor was murdered during an overnight home invasion of his suburban Miami home. Four defendants were charged with this crime.\n\nMany U.S. states (particularly those that endorse the Castle Doctrine) include defending oneself against forcible entry of one's home as part of their definition of justifiable homicide without any obligation to retreat.\n"}
{"id": "7949372", "url": "https://en.wikipedia.org/wiki?curid=7949372", "title": "Humanitarian principles", "text": "Humanitarian principles\n\nThere are a number of meanings for the term humanitarian. Here humanitarian pertains to the practice of saving lives and alleviating suffering. It is usually related to emergency response (also called humanitarian response) whether in the case of a natural disaster or a man-made disaster such as war or other armed conflict. Humanitarian principles govern the way humanitarian response is carried out.\n\nThe principle of humanity means that all humankind shall be treated humanely and equally in all circumstances by saving lives and alleviating suffering, while ensuring respect for the individual. It is the fundamental principle of humanitarian response.\n\nThe Code of Conduct for the International Red Cross and Red Crescent Movement and NGOs in Disaster Relief (RC/NGO Code) introduces the concept of the humanitarian imperative which expands the principle of humanity to include the right to receive and to give humanitarian assistance. It states the obligation of the international community \"to provide humanitarian assistance wherever it is needed.\"\n\nProvision of humanitarian assistance must be impartial and not based on nationality, race, religion, or political point of view. It must be based on need alone.\n\nFor most non-governmental humanitarian agencies (NGHAs), the principle of impartiality is unambiguous even if it is sometimes difficult to apply, especially in rapidly changing situations. However, it is no longer clear which organizations can claim to be humanitarian. For example, companies like PADCO, a USAID subcontractor, is sometimes seen as a humanitarian NGO. However, for the UN agencies, particularly where the UN is involved in peace keeping activities as the result of a Security Council resolution, it is not clear if the UN is in position to act in an impartial manner if one of the parties is in violation of terms of the UN Charter.\n\nHumanitarian agencies must formulate and implement their own policies independently of government policies or actions.\n\nProblems may arise because most NGHAs rely in varying degrees on government donors. Thus for some organizations it is difficult to maintain independence from their donors and not be confused in the field with governments who may be involved in the hostilities. The ICRC, has set the example for maintaining its independence (and neutrality) by raising its funds from governments through the use of separate annual appeals for headquarters costs and field operations.\n\nThe core principles are defining characteristics, the necessary conditions for humanitarian response. Organizations such as military forces and for-profit companies may deliver assistance to communities affected by disaster in order to save lives and alleviate suffering, but they are not considered by the humanitarian sector as humanitarian agencies as their response is not based on the core principles.\n\nIn addition to the core principles, there are other principles that govern humanitarian response for specific types of humanitarian agencies such as UN agencies, the Red Cross and Red Crescent Movement, and NGOs.\n\nThe International Red Cross and Red Crescent Movement follows, in addition to the above core principles, the principle of neutrality. For the Red Cross, neutrality means not to take sides in hostilities or engage at any time in controversies of a political, racial, religious or ideological nature.\n\nThe principle of neutrality was specifically addressed to the Red Cross Movement to prevent it from not only taking sides in a conflict, but not to \"engage at any time in controversies of a political, racial, religious or ideological nature.\" The principle of neutrality was left out of the Red Cross/NGO code because some of the NGHAs, while committed to giving impartial assistance, were not ready to forgo their lobbying on justice issues related to political and ideological questions.\n\nUnited Nations General Assembly Resolution 46/182 lists the principle of neutrality, alongside the principles of humanity and impartiality in its annex as a guide to the provision of humanitarian assistance. The resolution is designed to strengthen human response of the UN system, and it clearly applies to the UN agencies.\n\nNeutrality can also apply to humanitarian actions of a state. \"Neutrality remains closely linked with the definition which introduced the concept into international law to designate the status of a State which decided to stand apart from an armed conflict. Consequently, its applications under positive law still depend on the criteria of abstention and impartiality which have characterized neutrality from the outset.\"\n\nThe application of the word neutrality to humanitarian aid delivered by UN agencies or even governments can be confusing. GA Resolution 46/182 proclaims the principle of neutrality, yet as an inter-governmental political organization, the UN is often engaged in controversies of a political nature. According to this interpretation, the UN agency or a government can provide neutral humanitarian aid as long as it does it impartially, based upon need alone.\n\nToday, the word neutrality is widely used within the humanitarian community, usually to mean the provision of humanitarian aid in an impartial and independent manner, based on need alone. Few international NGOs have curtailed work on justice or human rights issues because of their commitment to neutrality.\n\nThe provision of aid must not exploit the vulnerability of victims and be used to further political or religious creeds. All of the major non-governmental humanitarian agencies (NGHAs) by signing up to the RC/NGO Code of Conduct have committed themselves not to use humanitarian response to further political or religious creeds.\n\nAll of the above principles are important requirements for effective field operations. They are based on widespread field experience of agencies engaged in humanitarian response. In conflict situations, their breach may drastically affect the ability of agencies to respond to the needs of the victims.\n\nIf a warring party believes, for example, that an agency is favoring the other side, or that it is an agent of the enemy, access to the victims may be blocked and the lives of humanitarian workers may be put in danger. If one of the parties perceives that an agency is trying to spread another religious faith, there may be a hostile reaction to their activities.\n\nThe core principles, found in the Red Cross/NGO Code of Conduct and in GA Resolution 46/182 are derived from the Fundamental Principles of the Red Cross, particularly principles I (humanity), II (impartiality), III (neutrality—in the case of the UN), and IV (independence).\n\nAccountability has been defined as: \"the processes through which an organisation makes a \ncommitment to respond to and balance the needs of stakeholders in its decision making processes and activities, and delivers against this commitment.\" Humanitarian Accountability Partnership International adds: \"Accountability is about using power responsibly.\"\n\nArticle 9 of the Code of Conduct for the International Red Cross and Red Crescent Movement and NGOs in Disaster Relief states:\n\"We hold ourselves accountable to both those we seek to assist and those from whom we accept resources;\" and thus identifies the two major stake holders: donors and beneficiaries. However, traditionally humanitarian agencies have tended to practice mainly \"upward accountability\", i.e. to their donors.\n\nThe experience of many humanitarian agencies during the Rwandan Genocide, led to a number of initiatives designed to improve humanitarian assistance and accountability, particularly with respect to the beneficiaries. Examples include the Sphere Project, ALNAP, Compas, the People In Aid Code of Good Practice, and the Humanitarian Accountability Partnership International, which runs a \"global quality insurance scheme for humanitarian agencies.\"\n\nThe RC/NGO Code also lists a number of more aspirational principles which are derived from experience with development assistance.\n\nThe Sphere Project Humanitarian Charter uses the language of human rights to remind that the right to life which is proclaimed in both the Universal Declaration of Human Rights and the International Convention on Civil and Political Rights is related to human dignity.\n\nHumanitarian principles are mainly focused on the behavior of organizations. However a humane response implies that humanitarian workers are not to take advantage of the vulnerabilities of those affected by war and violence. Agencies have the responsibility for developing rules of staff conduct which prevent abuse of the beneficiaries.\n\nOne of the most problematic areas is related to the issue of sexual exploitation and abuse of beneficiaries by humanitarian workers. In an emergency where victims have lost everything, women and girls are particularly vulnerable to sexual abuse.\n\nA number of reports which identified the sexual exploitation of refugees in west Africa prodded the humanitarian community to work together in examining the problem and to take measures to prevent abuses. In July 2002, the UN's Interagency Standing Committee (IASC) adopted a plan of action which stated: Sexual exploitation and abuse by humanitarian workers constitute acts of gross misconduct and are therefore grounds for termination of employment. The plan explicitly prohibited the \"Exchange of money, employment, goods, or services for sex, including sexual favours or other forms of humiliating, degrading or exploitative behaviour.\" The major NGHAs as well the UN agencies engaged in humanitarian response committed themselves to setting up internal structures to prevent sexual exploitation and abuse of beneficiaries.\n\nSubstantial efforts have been made in the humanitarian sector to monitor compliance with humanitarian principles. Such efforts include The People In Aid Code of Good Practice, an internationally recognised management tool that helps humanitarian and development organisations enhance the quality of their human resources management. The NGO, Humanitarian Accountability Partnership International, is also working to make humanitarian organizations more accountable, especially to the beneficiaries.\n\nStructures internal to the Red Cross Movement monitor compliance to the Fundamental Principles of the Red Cross.\n\nThe RC/NGO Code is self-enforcing. The SCHR carries out peer reviews among its members which look in part at the issue of compliance with principles set out in the RC/NGO Code\n\n"}
{"id": "41627505", "url": "https://en.wikipedia.org/wiki?curid=41627505", "title": "Imago Universi", "text": "Imago Universi\n\nAndreas Cellarius, German mathematician and cartographer (1596–1665), conceived an Atlas of the Universe, published in 1660, under the title of \"Harmonia Macrocosmica\". Numerous illustrations of the solar system appear in this atlas by different authors known at that time. Referring to Ptolemy, Cellarius called the representation of this Ptolemaic conception of heaven as \"Imago universi secundum Ptolaeum\"\n\n\"Imago\" is a word in Latin which means\" 'image\"' or even \"representation\". Therefore, the title expresses the \"Picture of the Universe according to Ptolemy.\" The Latin expression was used in the Middle Ages to express the representation and size of the known world at that time.\n\n\"Imago Universi\" is also the title, in Latin, of a cosmographic treatise, written in 2013 by the Spanish scientist Gabriel Barceló.\n\nAfter analyzing the history of cosmology, the treatise delves into the prevailing scientific lack of explanation of the rotation of the heavenly bodies in the laws of dynamic behaviour of the sidereal system. The author proposes the application of the Theory of Dynamic Interactions (TID) to astrophysics, in particular, the dynamics of stellar systems and galaxies. This theory allows new comprehension of the dynamics of nature and understands the dynamic equilibrium of the universe, always subjected to rotational accelerations, but repetitive and persistent. The author also highlights that the orbiting always coincides with the intrinsic rotation of celestial bodies. Paradox incorporating the book, noting that this had not been found to date.\n\n\n1. Einstein, Albert: The Origins of the General Theory of Relativity, lecture given at the George A. Foundation Gibson, University of Glasgow, 20 June 1933. Published by Jackson, Wylie and co, Glasgow, 1933.\n\n"}
{"id": "3358506", "url": "https://en.wikipedia.org/wiki?curid=3358506", "title": "Insight phenomenology", "text": "Insight phenomenology\n\nInsight is a sudden understanding of a problem or a strategy that aids in solving a problem. Usually, this involves conceptualizing the problem in a completely new way. Although insights may appear to be sudden, they are actually the result of prior thought and effort. While insight can be involved in solving well-structured problems, it is more often associated with ill-structured problems. \n\n\nWhen people solve, or attempt to solve an insight puzzle, they experience a common phenomenology, that is, a set of behavioural properties that accompany problem-solving activity (for a useful edited review of insight problems and their phenomenology, see Sternberg & Davidson, 1995). Other kinds of puzzle, such as the Tower of Hanoi, an example of a transformation problem, tend not to yield these phenomena. The phenomena may include:\n\n\nMax van Mannen proposed the so-called insight cultivators to obtain thematic insights when studying a phenomenon or phenomenological topic or event. This framework holds that insights can be obtained from philosophic, humanities, and human sciences sources. The idea is that the works of artists, scholars, and philosophers help us gain understanding about our own lived experiences. There is the view that this process can yield phenomenological anecdotes that can trigger an understanding that is beyond or more effective than what we could grasp intellectually because of the creative insights and understanding of a phenomenon.\n\nInsight cultivators can also lead to innovative or unique insights because they allow an evaluation of previous literature and experiences that reveal what has worked, what needs improvement, or what is wrong. The insights gleaned can allow us to identify a new way of looking at a phenomenon.\n\n\n"}
{"id": "56486267", "url": "https://en.wikipedia.org/wiki?curid=56486267", "title": "Jeanne Henriquez", "text": "Jeanne Henriquez\n\nJeanne Henriquez (born 1946) is an Afro-Curaçaoan educator, historian and activist. After teaching for over two decades, Henriquez, who had published articles and videos to balance the history and impact of colonialism in Curaçao became the director of the Center for the Protection of Women. She worked to alleviate domestic violence and provide educational and employment training for low-income women. She has worked with the National Archives and Museum Tula to develop materials to reclaim the history of Afro-Curaçaoans and the African diaspora throughout the Caribbean. She was awarded the Cross of Merit from the Government of Curaçao for her activism for women and the Afro-Curaçaoan communities.\n\nJeanne Dionise Henriquez was born in 1946 in the neighborhood of Otrobanda in Willemstad, Curaçao, which at the time was a part of the Netherlands Antilles, to Carmita Nicolasia (née Hernandez) and Plinio Miguel Henriquez. Her father was an analyst for the laboratory at Shell Oil Company and her mother was a dollmaker, whose designs celebrated the culture of Afro-Curaçaoans. Henriquez was the oldest daughter, having younger sisters Renee and Sarah. After completion of her secondary education, she began studying to be a teacher in 1962 in Curaçao. In 1967, she moved to Breda in the Netherlands, graduating from normal school in 1968. Continuing her schooling, Henriquez enrolled at the State University of Utrecht, studying socio-economic history. She obtained a master's degree and completed a PhD in 1976 evaluating the social and economic impact of the Atlantic slave trade on the history of Curaçao.\n\nAfter completing her degree, Henriquez returned to Curaçao in 1976 and taught history at the Peter Stuyvesant College, now known as the Kolegio Alejandro Paula, Between 1979 and 1980, she left the school to work as the temporary head of the Central Historical Archives of the Netherlands Antilles. She returned to the high school in 1980 and in 1984, when she was appointed as dean. Simultaneously between 1984 and 1989, Henriquez taught at the Teacher’s Training School in Willemstad. Influenced by Joceline Clemencia, she wrote several articles during this time on the relationship of the island to the Kingdom of the Netherlands and worked with women living in poverty. She also had a son in 1982, Ray Asim Henriquez, and chose to raise him as a single mother. Henriquez worked on the editorial staff of the Historical Archives of the Netherlands Antilles’ bi-annual journal \"Lanternu\" from 1984 to 1987. In the 1980s, she worked as the co-editor for the script on a series of fifteen videos, which aimed to present a balanced assessment on the colonial and post-colonial history of Curaçao without the Eurocentric view upon which previous textbooks had focused. After the series was presented, the transcripts were published as books.\n\nHenriquez was the head of Peter Stuyvesant College until 1988, when she moved to Washington, D. C. to pursue her master's degree. Taking her son with her to the United States, she enrolled in the women's studies program at George Washington University. While she was in school, she worked in a youth program providing educational materials and information on HIV/AIDS for young adults and then through 1991, worked as an intern in a rape crisis center in Washington. Graduating with a master of arts degree in women's studies in 1991, she returned to Curaçao as the director of the Center for the Promotion of Women (Papiamento: Sentro pa Desaroyo di Hende Muhé, SDHM), where she worked until 1998. At SDHM, Henriquez primarily worked with low-income women, providing training and materials written in Papiamento, the \"lingua franca\" of the Netherlands Antilles, on the prevention and crisis intervention for domestic violence. She also provided counseling on education and job opportunities.\n\nDuring her tenure at SDHM, Henriquez published several works focused on women and their lives. Included were testimonies of single mothers in Curaçao (1990); a poem \"Appeal from a Mother\" (1992); video scripts regarding the lives of working women (1994); and biographies of six female prisoners (1996). In 1998, she left SDHM and headed the public relations for the National Archives of Curaçao. In that capacity, she staged an exhibit on one of Curaçao's pioneering feminists, Adèle Rigaud, in 1999 and expanded the program to then cover other influential feminists. In 2002, she led a cooperative conference with UNESCO on the African diaspora. The conference evaluated new methods to study slavery and its impact for the Caribbean.\n\nHenriquez retired in 2004 and worked as an independent historical researcher and consultant for the study of Curaçaoan emancipation for two years, volunteering at organizations to combat violence against women. Between 2006 and 2010, Henriquez served at a board member of the Network for Women's Health in Latin America and the Caribbean () and simultaneously from 2009 worked as president of Dedima Foundation, an organization which protects the human rights of Curaçao’s women and children. Between 2005 and 2013, she served as project manager and coordinator of Museum Tula, a former plantation, which bears the name of Tula, who led the Curaçao Slave Revolt of 1795. The museum strives to exhibit materials focused on the socio-economic development of not only Afro-Curaçaoans but the broader Caribbean region. She worked on projects to collect oral history and published a manual on methodologies of oral history in 2013. That same year, she led a project to reconstruct a slave dwelling for the Museum of Afro-Curaçaoan Heritage and assisted with the establishment of the Slavery Heritage Knowledge Centre located at the museum.\n\nHenriquez received the Foundation Cultrual Bando Bou Award in 2013 for her work at Museum Tula and was the recipient of the Government of Curaçao’s Cross of Merit in 2015.\n\n"}
{"id": "3164321", "url": "https://en.wikipedia.org/wiki?curid=3164321", "title": "Kaal", "text": "Kaal\n\nKaal or ( ; kaalam or kaala) is a word used in Sanskrit to mean \"time\". The Tamil word \"kaalam\" refers to duration (an interval) in time. It is also the name of a deity, in which sense it is not always distinguishable from \"\", meaning \"black\". It is often used as one of the various names or forms of Yama. Kaal/Kaala is also referred to the concept of Spacetime. In the Yogic System the concept of SpaceTime was referred to as one word rather than two separate concepts of Space (darkness) and Time.\n\nMonier-Williams's widely used Sanskrit-English dictionary lists two distinct words with the form \"\".\n\n\nAccording to Monier-Williams, ' 2 is from the verbal root ' \"to calculate\", while the root of \"\" 1 is uncertain, though possibly the same.\n\nAs applied to gods and goddesses in works such as the ' ' and the \"Skanda\" ', ' 1 and ' 2 are not readily distinguishable. , a Hindu translator of the ' ', renders the feminine compound ' (where \"\" means \"night\") as \"dark night of periodic dissolution\".\n\nAs Time personified, destroying all things, Kala is a god of death sometimes identified with Yama.\n\nKala appears as an impersonal deity within the Mahabharata, the Ramayana, and the Bhagavata Purana. In the Mahabharata, Krishna, one of the main characters, reveals his identity as Time personified. He states to Arjuna that both sides on the battlefield of the Kurukshetra War have already been annihilated. At the end of the epic, the entire Yadu dynasty (Krishna's family) is similarly annihilated. The story ends with Yudhishthira, the last of the Pandava brothers, entering Heaven in his human form, thereby closing the link. In Heaven, Yudi sees everyone within the story, both people whom he hated, and people whom he loved, and is happy to see them all. He then sees their transcendent cosmic forms, Krishna as Vishnu, Draupadi as uma, and realizes that the participants in the play were merely gods in human form, engaging in pastimes and working out their karma. Yudi then abandons his bitterness and spends the rest of eternity in Heaven, it is a happy ending.\n\nKala appears in the Uttara Kanda of the Ramayana, as the messenger of Death (Yama). At the end of the story, Time, in the form of inevitability or necessity, informs Rama that his reign on Earth is now over. By a trick or dilemma, he forces the death of Lakshmana, and informs Rama that he must return to the realm of the gods. Lakshmana willingly passes away with Rama's blessing and Rama returns to Heaven.\n\nTime appears in the Bhagavata Purana as the force that is responsible for the imperceptible and inevitable change in the entire creation. According to the Purana, all created things are illusory, and thereby subject to creation and annihilation, this imperceptible and inconceivable impermanence is said to be due to the march of Time. Similarly, Time is considered to be the unmanifest aspect of God that remains after the destruction of the entire world at the end of a lifespan of Brahma.\n\nIn the Chaitanya Bhagavata, a Gaudiya Vaishnavist text and biography of Chaitanya Mahaprabhu, it is said that the fire that emerges from the mouth of Sankarshana at the End of Time is the \"Kālānala\", or \"fire of Time\". One of the names of Sankarshana is \"k\"ā\"l\"ā\"gni\", also \"fire of Time\".\n\nThe Vishnu Purana also states that Time (kala) is one of the four primary forms of Vishnu, the others being matter (Pradhana), visible substance (vyakta), and Spirit (Purusha).\n\nAt Bhagavad Gita 11.32, Krishna takes on the form of \"kāla\", the destroyer, announcing to Arjuna that all the warriors on both sides will be killed, apart from the Pandavas:\nकालो ऽस्मि लोकक्षयकृत् प्रवृद्धो लोकान् समाहर्तुम् इह प्रवृत्तः ।\n\nThis verse means: \"Time (kāla) I am, the great destroyer of the worlds, and I have come here to destroy all people.\" This phrase is famous for being quoted by J. Robert Oppenheimer as he reflected on the Manhattan Project's explosion of the first nuclear bomb in 1945.\n\nIn Javanese mythology, Batara Kala is the god of destruction. It is a very huge mighty and powerful god depicted as giant, born of the sperm of Shiva, the kings of gods.\n\nIn Borobudur, the gate to the stairs is adorned with a giant head, making the gate look like the open mouth of the giant. Many other gates in Javanese traditional buildings have this kind of ornament. Perhaps the most detailed Kala Face in Java is on the south side of Candi Kalasan.\n\n"}
{"id": "44844160", "url": "https://en.wikipedia.org/wiki?curid=44844160", "title": "Kalpana (imagination)", "text": "Kalpana (imagination)\n\nKalpanā (Sanskrit: कल्पना) is derived from the root - \"kalpanama\" (कल्पनम्) + ना, and means – 'fixing', 'settlement', 'making', 'performing', 'doing', 'forming', 'arranging', 'decorating', 'ornamenting', 'forgery', 'a contrivance', 'device'. and also means – 'assuming anything to be real', 'fictional'.\n\nSuresvaracharya in his \"Taittirīyavārttika\" (commentary on Śankāra's work on the \"Taittirīya Upanişad\") (II.297) has used the term \"kalpanā\" to mean – 'inferior conception'. Vishnu Purana (VI.vii.90) and Naradiya Purana (lxvii.70) define \"kalpanā\" as a two-termed relation which is a distinction between the contemplation and the object-to-be-contemplated.\n\nBadarayana has used the word \"kalpanā\" only once in his composition, Brahma Sutras, but while translating Sri Govinda Bhāshya of Baladeva Vidyabhushana, a commentary on Vedānta sutras, this word has been translated by Srisa Chandra Vasu to mean – 'the creative power of thought, formation, creation (and not imagination) ', which meaning is in the context of explaining Pradhana purported to have been referred to by the word - \"ajā\" (birthless entity) occurring in the Shvetashvatara Upanishad (IV.5). Badarayana states:-\n\nRoer in his translation of the commentary of Shankara on Brihadaranyaka Upanishad has translated the word \"kalpanā\" as 'fictitious view', and \" upadhi \", as 'fictitious attribute'. Shankara in his Brahma Sutra Bhāsya has interpreted this \"sutra\" as follows:-\n\nexplaining that the word \"ajā\" neither indicates the form of a she-goat nor has it been used in the derivative sense of that which is unborn; what is said by the Shvetashvatara Upanishad is as an instruction about the material source of all things – moving and immobile, using a form of imagery (kalpanā) - the analogy to a she-goat.\n\nDignāga in his \"Pramāna-samuccya\", tells us that amongst \" pratyaksha \" ('perception') that has the particular for the object and \" anumāna \" ('inference') that has only the universal cognisance, the former ('perception') is free from \"kalpanā\" or 'conceptual construction'. Katha Upanishad tells us that virtual objects exist only during \"kalpanā-kāla\" i.e. during the period of imagination, owing to \" avidyā \". And, according to Patanjali, \"kalpanā\" ('fancy') is more subjective than illusion and hallucination.\n\nMan is able to think because he has a perceiving and arranging \" manas \" ('mind') which self-illuminated gives him \" chetnā \" ('consciousness') and the faculties of \" pratyaksha \" ('perception'), \" chintā \" ('thought'), \"kalpanā\" ('imagination'), \" prayatna \" ('volition') and \" chaitanya \" ('higher sentience and intelligence'). The Vedic thinkers held the view that the universe is merely an idea, a \"kalpanā\" ('phantasm') or projection of the mind of the creator; even the experience of birth and death by the Jiva is a \"kalpanā\" ('hallucination') created by ignorance. Mental \"kalpanā\" is false superimposition on account of ignorance. However, the \" siddha \", exclusively intent on attaining \" yoga \" with own self, and self-reliant, gains powers arising spontaneously as devoid of any ruse or ploy (\"kalpanā\").\n"}
{"id": "39021153", "url": "https://en.wikipedia.org/wiki?curid=39021153", "title": "List of rationalists", "text": "List of rationalists\n\nIn philosophy and in its current sense, rationalism is a line of thought that appeals to reason or the intellect as a primary or fundamental source of knowledge or justification\". It is typically contrasted with empiricism, which appeals to sensory experience as a primary or fundamental source of knowledge or justification. Rationalism should not be confused with rationality, nor with rationalization.\n\nThe following is a list of rationalists, that is, people who theorize about rationalism as a line of thought within the area of Philosophy.\n\n\n\n\n"}
{"id": "52756461", "url": "https://en.wikipedia.org/wiki?curid=52756461", "title": "Mary Morris Knibb", "text": "Mary Morris Knibb\n\nMary Morris Knibb, MBE (28 February 1886 – 21 September 1964) was a Jamaican teacher, social reformer and philanthropist. She founded the Morris Knibb Preparatory School and donated a building which is used as the headquarters of the Moravian Church in Jamaica as well as land for construction of a community center. Morris Knibb was a women's rights activist and the first elected councilwomen in Jamaica. She was the first woman to vie for a seat in the House when Universal Suffrage was granted to all Jamaicans.\n\nMary Lenora Morris \"Nora\" was born on 28 February 1886 in Carmel, Westmoreland Parish, Jamaica. In 1893, Morris began teaching as an assistant teacher at the Moravian Day School, in the customary pupil-teacher system of recruitment. Throughout the Caribbean prior to the 1950s, the most promising primary students, began working as assistant teachers to offset the cost of their further education. In some cases, they became full teachers upon passing an examination and in others were sent after their pupil-teacher contractual period to normal schools for additional training. Morris, followed the latter path and attended the Shortwood Teachers' College. While attending the college, Morris became one of the founders of the Alumni Students' Association.\n\nBetween 1907 and 1917, Morris taught at the St. George Girls' School and then taught for almost two years at the Central Branch School. She then became the headmistress of the Wesley School, where she remained until 1928. In 1931, the now married Morris, opened her own school, the Mary Morris-Knibb Preparatory School in Kingston, at 3 Hector Street, Saint Andrew Parish, which she had inherited a few months previously upon the death of Frances Morris. Catering to the middle-class, Morris-Knibb was known to provide an excellent education and stern discipline. Students were required to study geography, history, Latin, math, reading, spelling, and writing, earning the school the reputation as \"one of the leading preparatory schools in the nation\".\n\nIn 1936 or 1937, she co-founded, along with Amy Bailey, Eulalie Domingo and Edith Dalton James, the Jamaica Women’s Liberal Club (LC). The organization was mostly made up of teachers and their goal was to agitate for women's inclusion in government service, including such posts as serving on the school board and civil service. Most of the women were black and middle-class women who wanted to advance the position of women in society on both socio-economic and political levels. One of the social protections for which Morris Knibb advocated was marriage. In part because of morality concerns, but in part because common-law arrangements did not protect children adequately, she favored even holding mass weddings so that the costs of the ceremony would be reduced and participation greater. When black women, like Morris-Knibb wanted to participate in the Child Welfare Association of Jamaica, they were told they were not welcome. The upperclass women of the Child Welfare Association suggested that the black women set up an auxiliary for women of their \"shade\". In response, Morris-Knibb joined Amy Bailey, May Farquharson, Dr. Jai Lal Varma, and Dr. Pengelley and others in founding the Save the Children Fund in 1938.\n\nMorris Knibb was one of the leaders in the fight for Jamaican suffrage and as it had been a long-practiced method for women to gain a foothold and show their readiness to vote, she was in favor of women running for local offices. In 1939, the LC organized women and ran a campaign with Morris Knibb as their candidate for parish council. She won the seat for the Kingston/Saint Andrew Parish seat on the council, becoming the first woman to serve as an elected official in Jamaica. Her work on the council focused on education and social services. She advocated for creation of after school programs, night schools, and trade education, utilizing existing school and government buildings for the purpose. She donated property located at 15 Byrnes Street to the Lower St. Andrews Citizen's Association to facilitate creation of a community centre and was active in programs to care for the poor and the aged.\n\nWhen universal franchise was granted to Jamaicans in 1944, Morris Knibb immediately opened a campaign to run for a seat in the House, becoming the first woman to contest a general election in Jamaica. Though she didn't win, she was undaunted, becoming one of the first women sworn in as Justice of the Peace in 1945 and continuing to serve as a councilwoman through the early 1950s. In 1953, she was honored with the Order of the British Empire for her years of social service. Throughout the 1950s, she continued her work with the Moravian Church and served several terms as the vice-chair of the school board.\nMorris Knibb died on 21 September 1964 in Woodford Park, Saint Andrew Parish, Jamaica. She donated the building which is used as the headquarters of the Moravian Church, 3 Hector Street, Kingston, Jamaica, to the organization. Generations of students have been educated at the Morris Knibb Preparatory School, including many prominent Jamaicans. In 1984, the school was relocated, from its original location next to the Moravian Church, to Molynes Road in St. Andrew Parish, teaching kindergarten to grade 6. In 2004, as part of the Moravian Church's 250th anniversary, the church worked to have the Postal Corporation of Jamaica issue commemorative stamps of important leaders in their organization. Morris Knibb was one of three honorees recognized in the commemorative stamp series.\n\n"}
{"id": "4503179", "url": "https://en.wikipedia.org/wiki?curid=4503179", "title": "Megaversal system", "text": "Megaversal system\n\nThe Megaversal system, sometimes known as the Palladium system, is a set of mechanics specifically employed in most role-playing games published by Palladium Books, with the exception of \"Recon\". It uses dice for roll-under percentile skill checks, roll-high combat checks and saving throws, and determination of damage (i.e. Mega Damage is to M.D.C. what \"damage\" is to S.D.C. ) sustained in melee encounters by which a character's Hit Points, Structural Damage Capacity (S.D.C.), or Mega-Damage Capacity (M.D.C.) is reduced accordingly.\n\nShannon Appelcline, in his book \"Designers & Dragons\", states that the Megaversal system was a revamp of Palladium's \"AD&D\"-derived game system: \"It was one part highly traditional – with its character classes, experience points and levels – and one part arcane – with its abbreviations like OCCs, RCCs, PCCs, PPE, SDC and MDC.\"\n\nCertain aspects of character creation vary across series. Depending upon the game, players may or may not need to select a race; for instance, it is assumed that characters in \"Ninjas & Superspies\" are human, while in \"Palladium Fantasy\" they very often are not. Nonetheless, all games share the same eight randomly generated attributes:\n\nFor humans, most of these statistics are determined by a roll of three six-sided dice, whereas other species' attributes are determined more or less depending on how they compare to the baseline human standard.\n\nOther statistics that may be used are:\n\nThe characters' race and attributes – not to mention the game itself – impact their selection of character classes:\n\nDepending upon the game, skills can come either from the character's O.C.C. and a related list, or from the character's educational or occupational background. Games set on modern Earth tend to favor the second; all others favor the first. O.C.C.s tend to be more specific than character classes in other games, with a wide range of O.C.C.s in a given profession, such as six or seven specialized mecha pilot classes in \"Rifts\" rather than a single \"Robot/Power Armor Pilot\" class.\n\nPalladium's alignments are described in detailed terms, outlining how a character will act in a certain situation: whether they will lie, how much force they will use against innocents, how they view the law, and so forth. The alignments are organized into three broad categories: Good, Selfish, and Evil. The seven core alignments are:\n\nAn eighth overall (and third Good) alignment, \"Taoist\", was introduced for \"Mystic China\", but has not seen use outside of that game.\n\nPalladium founder and lead designer Kevin Siembieda has a noted distaste for \"neutral\" alignments (as used in Dungeons & Dragons). This is stated in most core rulebooks in the alignment section, and stems from the idea that a truly neutral character would not do anything particularly interesting, like fight or go on an adventure.\n\nEach game has its own variations to make the system better suit its genre. \"After the Bomb\", \"Splicers\", \"Heroes Unlimited\" (for mutant animals only), and \"Teenage Mutant Ninja Turtles & Other Strangeness\" use \"Biological Energy\" (BIO-E) points to purchase mutations. \"Palladium Fantasy\" assumes that non-human characters will be routinely played, so most races will use normal O.C.C.s instead of R.C.C.s.\n\nSome games that feature advanced technology in science fiction settings like \"Rifts\", \"Robotech\", and \"Splicers\" use a special category of damage capacity called \"Mega-Damage\" (M.D.C.); the exception is \"Mechanoids\". M.D.C. is 100 times more powerful than normal damage (i.e., 1 M.D.C. = 100–199 S.D.C.). Normal weapons cannot damage a Mega-Damage structure at all unless they are capable of inflicting 100 S.D.C. or more in a single shot or burst of ammunition; the archetypal example of Mega-Damage is a tank, which can only be effectively destroyed through the use of powerful weapons designed to overcome its armor, while easily resisting small-arms fire. Many updates to the system in various campaign settings have also added Perception as a statistic. Prior to this it was entirely up to the GM if the player noticed anything.\n"}
{"id": "43349586", "url": "https://en.wikipedia.org/wiki?curid=43349586", "title": "National Centre for Knowledge on Men's Violence against Women", "text": "National Centre for Knowledge on Men's Violence against Women\n\nThe National Centre for Knowledge on Men's Violence against Women (, abbreviated NCK) is a Swedish knowledge and resource centre at Uppsala University, founded in 2006. NCK is working on behalf of the Government of Sweden to raise awareness at the national level of men's violence against women, and develops new methods for the treatment of abused women. NCK has a mandate to research, educate, compile and spread information. In 2008, its operations expanded to include violence and oppression in the name of honour, and violence in same-sex relationships.\n\nThe centre is part of Uppsala University, organised directly under the rector, and managed by Professor Gun Heimer.\n\nSince 2007, the centre have been operating a Government funded national helpline for women who have been subjected to threats and violence (). The telephone counseling is open 24-hour, toll-free, and the call does not appear on the telephone bill. Staff at the helpline provide support, practical advice, information about the healthcare system and how to report crimes. The staff consist of social workers, midwives and nurses with at least five years of professional experience; receiving on average 60 calls a day.\n\n\n"}
{"id": "52618454", "url": "https://en.wikipedia.org/wiki?curid=52618454", "title": "National Privacy Commission (Philippines)", "text": "National Privacy Commission (Philippines)\n\nThe National Privacy Commission, or NPC, is an independent body created under Republic Act No. 10173 or the Data Privacy Act of 2012, mandated to administer and implement the provisions of the Act, and to monitor and ensure compliance of the country with international standards set for data protection. It is attached to the Philippines' Department of Information and Communications Technology (DICT) for purposes of policy coordination, but remains independent in the performance of its functions. The Commission safeguards the fundamental human right of every individual to privacy, particularly Information privacy while ensuring free flow of information for innovation, growth, and national development.\n\nIn order to fulfil its mandate, the Commission is vested with a broad range of powers, from receiving complaints and instituting investigations on matters affecting personal data protection to compelling entities to abide by its orders in matters affecting data privacy. It also represents the Philippine Government internationally on data protection related issues. The Commission formulates and implements policies relating to the protection of personal data, including the relevant circulars and advisory guidelines, to assist organisations in understanding and complying with the Data Privacy Act. The Commission also reviews organizational actions in relation to data protection rules and issue decisions or directions for compliance where necessary. It is mandated to work with relevant sector regulators in exercising its functions.\n\nBeyond regulating data protection issues, the NPC also undertakes public and sector-specific educational and outreach activities to help organizations adopt good data protection practices and to help individuals to better understand how they may protect their own personal data from misuse.\n\nThe Data Privacy Act of 2012 is the first law in the Philippines which acknowledges the rights of Individuals over their Personal Data and Enforcing the responsibilities of entities who process them.\n\nThe initial definition was offered first in Republic Act 8792, Section 32 better known as the eCommerce Act of the Philippines and was formally introduced by the Department of Trade and Industry (DTI) on its Department Administrative Order #08 - Defining Guidelines for the Protection of Personal Data in Information Private Sector. Along with the Anti-Cybercrime Bill (now RA 10175), The first draft of the law started in 2001 under the Legal and Regulatory Committee of the former Information Technology and eCommerce Council (ITECC) which is the forerunner of the Commission on Information and Communication Technology (CICT). It was headed by former Secretary Virgilio \"Ver\" Peña and the Committee was chaired by Atty. Claro Parlade. It was an initiative of the Information Security and Privacy Sub-Committee chaired by Albert Dela Cruz who was the President of PHCERT together with then Anti-Computer Crime and Fraud Division Chief, Atty. Elfren Meneses of the NBI. The administrative and operational functions was provided by the Presidential Management Staff (PMS) acting as the CICT secretariat.\n\nWith rising concerns by the Information Technology and Business Process Association of the Philippines (IBPAP) of an absence of a Data Privacy Law, Philippine Congress passed Senate Bill No. 2965 and House Bill No. 4115 on June 6, 2012. President Benigno S. Aquino III signed Republic Act No. 10173 or the Data Privacy Act of 2012 on August 15, 2012. The law was influenced by the Data Protection Directive and the APEC Privacy Framework.\n\nPresident Aquino appointed on March 7, 2016 Raymund Liboro as inaugural head of the commission with Damian Domingo O. Mapa and Ivy D. Patdu as inaugural deputy privacy commissioners. With fixed terms of office, they continued with their roles during the administration of President Rodrigo Duterte.\n\nAfter consultation with various private organizations, civil societies and a series of public hearings in Manila, Cebu and Davao, the Implementing Rules and Regulations of the Data Privacy Act was signed on August 24, 2016. It took effect on September 9, 2016.\n\nIn May 2016, the Commission formally investigated the Commission on Elections for the Commission on Elections data breach one of the largest security breach in government held personal data. On February 21, 2017, NPC announced that the Commission on Elections was being investigated for another security breach due to alleged theft of a computer containing personal data of voters.\n\nThe NPC also began coordinating with different sectors on privacy and data protection.\nIn 2016, the National Privacy Commission was accepted as a member in the International Conference of Data Protection and Privacy Commissioners and the Asia Pacific Privacy Authorities.\n\n\n"}
{"id": "48648507", "url": "https://en.wikipedia.org/wiki?curid=48648507", "title": "Networked individualism", "text": "Networked individualism\n\nNetworked individualism represents the shift of the classical model of social arrangements formed around hierarchical bureaucracies or social groups that are tightly-knit, like households and work groups, to connected \"individuals\", using the means provided by the evolution of Information and communications technology. Although the turn to networked individualism started before the advent of the internet, it has been fostered by the development of social media networks.\n\nThe term was coined by Barry Wellman in 2000, and first published by Manuel Castells and Barry Wellman in 2001. It was elaborated on by Lee Rainie and Barry Wellman in their 2012 book \"Networked: The New Social Operating System\" (MIT Press).\n\nThe networked individuals are members of diverse groups in which they seek different things; for instance, the same set of individuals could be in a group used to seek emotional support while another group might be used to get good addresses in a city. Those groups can be dispersed around the globe, and the combination of those networks make for a highly individualized, and well-networked, person.\n\nThis new world of networked individualism is oriented around looser, more fragmented networks that provide on-demand succor. Such networks had already formed before the coming of the internet. In incorporating the internet and mobile phones into their lives, people have changed the ways they interact with each other. They have become increasingly networked as individuals, rather than embedded in groups. In the world of networked individuals, it is the person who is the focus; more than the family, the work unit, the neighborhood, and the social group.\n\nSocial relationships are changing, and technology is a driving force in many of these changes. There are some fears that the digital technologies are killing society, but studies by the Pew Internet Project show that these technologies are not isolated — or isolating — systems. They are being incorporated into people’s social lives much like their predecessors were.\n\nBarry Wellman questioned the future of networked individualism after the event of the September 11 attacks in his short essay,\"The Rise (and Possible Fall) of Networked Individualism\".\n"}
{"id": "47439608", "url": "https://en.wikipedia.org/wiki?curid=47439608", "title": "Pavlok", "text": "Pavlok\n\nPavlok is a wearable device that uses operant conditioning through haptic feedback to modify behavior. Users are said to be able to break bad habits by pairing the behavior with up to 150 volts of \"zaptic feedback\", and can establish new routines by pairing the behavior with vibration.\n\nPavlok is a Behavioral Technology Group product, created by Maneesh Sethi, an author and television personality. Sethi was previously known for his writing as Editor-in-chief of \"Hack The System\", a series of articles and guides on establishing a passive income and being productive.\n\nIn 2012, Sethi was featured on NY Daily News, CNET, Huffington Post, and other news outlets for hiring a woman off of Craigslist to slap him across the face whenever he was distracted from his work. Impressed with the observable effects of operant conditioning, Sethi founded Behavioral Technology Group in July of the following year.\n\nPavlok has two main hardware components: the wristband and the removable module.\n\nPavlok's wristband is one size fits all, and is made with silicone. As of July 2015, Pavlok offers colored wristbands for separate purchase in blue, red, pink, and gray.\n\nThe module is made to fit inside the wristband, and is constructed to prevent accidental disassembling. Inside of the module is a Bluetooth Low Energy (BLE) unit and a rechargeable battery. Pavlok synchronizes data with the user's cellphone via BLE. The battery is specified to typically last around 3-6 days on a single charge and is charged via a micro USB port. The zapping voltage is adjustable from 150 to 450 Volt.\n\nPavlok currently offers a private beta app for iOS, acting as a remote control for the device via Bluetooth. The full app (still in development) includes a five-day audio course in aversion conditioning, with custom sessions tailored specifically for smoking, nail-biting, and overeating.\n\nPavlok is built with an open API in order to encourage users to synchronize their data with third-party health and fitness apps. Interaction with original and third-party applications is incorporated with Pavlok's unique payment system, referred to as \"Volts\".\n\nIn 2014, Pavlok established its own digital currency. Users can earn Volts for their activity on the Pavlok application, such as completing an audio course or reaching a goal. Volts are an internet-based medium of exchange, and can be redeemed for additional in-app courses and modules. Pavlok uses this payment system to leverage behavioral economics and improve its capacity to modify behavior, as financial incentives have been shown to enhance success in smoking cessation, grade performance, and other endeavors.\n\nIn May 2016, Pavlok was featured on ABC's Shark Tank Season 7 Finale. Sethi asked the sharks for $500,000 for 3.14% equity, valuing Pavlok at nearly $16 million. Kevin O'Leary offered Maneesh $500,000 for 3.14% of Pavlok by way of a 2 year balloon loan at 7.5%. Maneesh Sethi, founder of Pavlok, declined the offer. \n\nBehavioral Technology Group is included on Entrepreneur Magazine's 2015 list of \"100 Brilliant Companies to Watch\".\n\nIn July 2015, Pavlok was named a finalist in the MassChallenge accelerator program.\n\nPavlok was a winner in the 2015 Shopify Build a Business Competition for having the most sales in a year within the Electronics & Gadgets category.\n"}
{"id": "27498238", "url": "https://en.wikipedia.org/wiki?curid=27498238", "title": "Pirate Parties International", "text": "Pirate Parties International\n\nPirate Parties International (PPI) is a not-for-profit international non-governmental organisation with its headquarters in Brussels, Belgium. Formed in 2010, it serves as a worldwide organisation for Pirate Parties, currently representing members from 42 countries. The Pirate Parties are political incarnations of the freedom of expression movement, trying to achieve their goals by the means of the established political system rather than through activism. In 2017 PPI had been granted special consultative status to the United Nations Economic and Social Council.\n\nThe PPI statutes give its purposes as:\nto help establish, to support and promote, and to maintain communication and co-operation between pirate parties around the world.\n\nThe PPI advocate on the international level for the promotion of the goals its Members share such as protection of human rights and fundamental freedoms in the digital age, consumer and authors rights oriented reform of copyright and related rights, support of information privacy, transparency and free access to information.\n\nThe name \"Pirates\" itself is a reappropriation of the title that was given to internet users by the representatives of the music and film industry, and does not refer to any illegal activity.\n\nThe first Pirate party was the Swedish Piratpartiet, founded on 1 January 2006.\nOther parties and groups were formed in Austria, Canada, Denmark, Finland, Germany, Ireland, the Netherlands, Poland, and Spain. In 2007, representatives of these parties met in Vienna, Austria to form an alliance and plan for the 2009 European Parliament elections. Further conferences were held in 2008 in Berlin and Uppsala, the latter leading to the \"Uppsala Declaration\" of a basic platform for the elections.\n\nIn September 2008, Andrew Norton (United States) was appointed as coordinator of the PPI collective. In August 2009 he stepped down and passed the function of coordinator over to the \"coreteam\" led by Patrick Mächler and Samir Allioui.\n\nIn 2009, the original Pirate Party won 7.1% of the vote\nin Sweden's European Parliament elections and won two of Sweden's twenty MEP seats, inspired by a surge in membership following the trial and conviction of three members of the ideologically aligned Pirate Bay a year earlier.\n\nOn 18 April 2010, the Pirate Parties International was formally founded in Brussels at the PPI Conference from April 16 to 18.\n\nAt the 2009 conference of Pirate Parties International in Uppsala (Sweden), European Pirate parties agreed on a common declaration of the parties' goals for the upcoming election of the European Parliament.\nCentral issues of the declaration are:\n\n\nAt 2012 conference of Pirate Parties International in Prague (Czech Republic), European Pirate parties agreed to run in the elections to the European Parliament in the year 2014 with a common program as well as establish a European political party. The declaration has been followed by conferences in Potsdam and Barcelona to work on the structure of the legal body to come and the statutes for it.\n\nIn February 2015, Pirate Party Australia resigned from PPI due to serious disagreement with the direction and management of the organisation. In the same month, Pirate Party UK also resigned and in March the Belgian Pirate Party suspended its membership within PPI.\n\nOn 20 April 2015, the Pirate Party of Iceland voted overwhelmingly to leave PPI. A member of the executive, Arnaldur Sigurðarson, reported a 96.56% vote in favour of leaving, adding: “PPI has been pretty much useless when it comes to its objectives which should be to encourage international cooperation between Pirate Parties.”\n\nIn May 2015, the Pirate Party of Sweden resolved with a significant majority to leave PPI, cancelling their observer status.\n\nIn July 2016, the Pirate Party of Canada officially withdrew from Pirate Parties International citing ongoing troubles with the organization as well as a failure to adequately provide any accomplishments over its history.\n\nThe PPI is governed by a board, formerly led by two co-chairs, and since Warsaw conference of 2015 by a chair and a vice-chair. Policy, governance, and applications for membership are the responsibility of the PPI General Assembly which must convene at least once per year. By the current rules, board members are elected for a two-year term, half of the board being elected every year.\n\nSee Pirate Party and List of Pirate Parties for an overview of all Pirate Parties around the world.\n\n"}
{"id": "40050529", "url": "https://en.wikipedia.org/wiki?curid=40050529", "title": "Resilience (engineering and construction)", "text": "Resilience (engineering and construction)\n\nIn the fields of engineering and construction, resilience is an objective of design, maintenance and restoration for buildings and infrastructure, as well as communities. It is the ability to absorb or avoid damage without suffering complete failure. A more comprehensive definition is that it is the ability to respond, absorb, and adapt to, as well as recover in a disruptive event. A resilient structure/system/community is expected to be able to resist to an extreme event with minimal damages and functionality disruptions during the event; after the event, it should be able to rapidly recovery its functionality similar to or even better than the pre-event level. \n\nThe concept of resilience originated from ecology and then gradually applied to other fields. It is related to that of vulnerability. Both terms are specific to the event perturbation, meaning that a system/infrastructure/community may be more vulnerable or less resilient to one event than another one. However, they are not the same. One obvious difference is that vulnerability focuses on the evaluation of system susceptibility in the pre-event phase; resilience emphasizes the dynamic features in the pre-event, during-event, and post-event phases. \n\nResilience is a multi-facet property, covering four dimensions: technical, organization, social and economic . Therefore, using one metric may not be representative to describe and quantify resilience. In engineering, resilience is characterized by four Rs: robustness, redundancy, resourcefulness, and rapidity. Current research studies have developed various ways to quantify resilience from multiple aspects, such as functionality- and socioeconomic- related aspects. \n\nThe first influential quantitative resilience metric based on the functionality recovery curve was proposed by Bruneau et al. , where resilience is quantified as the resilience loss as follows. \n\nformula_1 \n\nwhere formula_2 is the functionality at time formula_3; formula_4 is the time when the event strikes; formula_5 is the time when the functionality full recovers. \n\nThe resilience loss is a metric of a only positive value. It has the advantage of being easily generalized to different structures, infrastructures, and communities. This definition assumes that the functionality is 100% pre-event and will eventually be recovered to a full functionality of 100%. This may not be true in practice. A system may be partially functional when a hurricane strikes and may not be fully recovered due to uneconomic cost-benefit ratio. \n\nResilience index is a normalized metric between 0 and 1, computed from the functionality recovery curve . \n\nformula_6 \n\nwhere formula_2 is the functionality at time formula_3; formula_4 is the time when the event strikes; formula_10 is the time horizon of interest.\n\nSocioeconomic resilience metrics fall into two categories: system-based and capital-based. System-based socioeconomic resilience metrics focus on quantifying the post-event business continuity and operability, whereas capital-based resilience metric measure resilience from the capital of individuals and communities . \n\n"}
{"id": "22191063", "url": "https://en.wikipedia.org/wiki?curid=22191063", "title": "Self-competition", "text": "Self-competition\n\nIn business, self-competition is competition by a company with itself for customers. This can include one product or retail location competing with another. While self-competition is often undesirable for the business, it can be beneficial to the customer, because, like normal competition, the result is lower prices and better products.\n\nAny company which provides multiple products may suffer from product self-competition. Similar products are more likely to have this issue. For example, a bakery which offers raspberry muffins and then adds blackberry muffins will likely see a decline in sales of the original product, although the total sales of both products will likely be higher than the original single product. If total sales do not increase, then this will have a negative impact on the business. However, even if total sales do increase slightly, this may still lower profits, as producing two products increases costs over a single product. Therefore, only a large increase in total sales would justify the addition of the new product.\n\nIn order to limit self-competition, new products should ideally be significantly different from existing products. In the bakery example, bran muffins would create less competition with raspberry muffins. Adding whole loaves of bread to the product mix would create even less competition.\n\nA related issue involves two retail locations for the same company that are situated close to one another. The first location will likely see a decline in business when the new location is added. Just how far away two locations must be to avoid this effect depends on the type of business. For newspaper stands, they could be relatively close, as customers are often on foot and unlikely to walk more than block or two. For amusement parks, the distances must be much larger, as people are willing to drive long distances to spend the day at such an attraction. In the case of ski resorts, people are even willing to fly long distances. Furthermore, franchisers, like McDonald's, do not actually allow for their products to be placed within a certain mile radius of an already established franchise.\n\nSometimes, there will be two separate instances of a retail business less than a half mile apart. For instance, Subway has become a densely populated fast food franchise, and the opening of Wal-Mart stores has resulted in internal competition with separate Subway restaurants nearby since Subway has been integrated into Wal-Mart stores.\n\nSometimes, brief internal competition can be a consequence of having clearance items in a store's inventory, in which prices are decreased to encourage customers to clear out the clearance items. Other tactics can involve delaying introduction of some versions of certain products since it can also save some companies money.\n\nSelf-competition is a common side-effect of mergers and acquisitions, as the new combined business often has similar products and nearby retail locations. The success of the business often depends on their ability to eliminate similar products and redundant retail locations. Ideally, the most profitable products and locations should be kept, regardless of the source company. In some cases, the best attributes of each product may be retained. For example, one company may offer a superior food product, but the other may have better packaging, perhaps a resealable bag.\n\nWhile any company which offers more than a single product can suffer from the effects of self-competition, the larger a company becomes, in terms of market share, the more it becomes an issue. In the case of General Motors, they were eventually forced to drop their entire Oldsmobile line, as it was largely redundant with Buick, and to a smaller extent Chevrolet, though Pontiac which also later got axed as well had a sporty flare to it which made it look less redundant.\n\n"}
{"id": "314902", "url": "https://en.wikipedia.org/wiki?curid=314902", "title": "Skein relation", "text": "Skein relation\n\nSkein relations are a mathematical tool used to study knots. A central question in the mathematical theory of knots is whether two knot diagrams represent the same knot. One way to answer the question is using knot polynomials, which are invariants of the knot. If two diagrams have different polynomials, they represent different knots. In general, the converse does not hold.\n\nSkein relations are often used to give a simple definition of knot polynomials. A skein relation gives a linear relation between the values of a knot polynomial on a collection of three links which differ from each other only in a small region. For some knot polynomials, such as the Conway, Alexander, and Jones polynomials, the relevant skein relations are sufficient to calculate the polynomial recursively. For others, such as the HOMFLYPT polynomial, more complicated algorithms are necessary.\n\nA skein relationship requires three link diagrams that are identical except at one crossing. The three diagrams must exhibit the three possibilities that could occur for the two line segments at that crossing, one of the lines could pass \"under,\" the same line could be \"over\" or the two lines might not cross at all. Link diagrams must be considered because a single skein change can alter a diagram from representing a knot to one representing a link and vice versa. Depending on the knot polynomial in question, the links (or tangles) appearing in a skein relation may be oriented or unoriented.\nThe three diagrams are labelled as follows. Turn the three link diagram so the directions at the crossing in question are both roughly northward. One diagram will have northwest over northeast, it is labelled \"L\". Another will have northeast over northwest, it's \"L\". The remaining diagram is lacking that crossing and is labelled \"L\".\n\nIt is also sensible to think in a generative sense, by taking an existing link diagram and \"patching\" it to make the other two—just so long as the patches are applied with compatible directions.\n\nTo recursively define a knot (link) polynomial, a function \"F\" is fixed and for any triple of diagrams and their polynomials labelled as above,\nor more pedantically\n\nMore formally, a skein relation can be thought of as defining the kernel of a quotient map from the planar algebra of tangles. Such a map corresponds to a knot polynomial if all closed diagrams are taken to some (polynomial) multiple of the image of the empty diagram.\n\nSometime in the early 1960s, Conway showed how to compute the Alexander polynomial using skein relations. As it is recursive, it is not quite so direct as Alexander's original matrix method; on the other hand, parts of the work done for one knot will apply to others. In particular, the network of diagrams is the same for all skein-related polynomials.\n\nLet function \"P\" from link diagrams to Laurent series in formula_4 be\nsuch that formula_5 and a triple of skein-relation diagrams formula_6 satisfies the equation\nThen \"P\" maps a knot to one of its Alexander polynomials.\n\nIn this example, we calculate the Alexander polynomial of the cinquefoil knot (), the alternating knot with five crossings in its minimal diagram. At each stage we exhibit a relationship involving a more complex link and two simpler diagrams. Note that the more complex link is on the right in each step below except the last. For convenience, let \"A\" = \"x\"−x.\n\nTo begin, we create two new diagrams by patching one of the cinquefoil's crossings (highlighted in yellow) so\n\nThe first diagram is actually a trefoil; the second diagram is two unknots with four crossings. Patching the latter\n\ngives, again, a trefoil, and two unknots with \"two\" crossings (the Hopf link ). Patching the trefoil\n\ngives the unknot and, again, the Hopf link. Patching the Hopf link\n\ngives a link with 0 crossings (unlink) and an unknot. The unlink takes a bit of sneakiness:\n\nWe now have enough relations to compute the polynomials of all the links we've encountered, and can use the above equations in reverse order to work up to the cinquefoil knot itself. The calculation is described in the table below, where ? denotes the unknown quantity we are solving for in each relation:\nThus the Alexander polynomial for a cinquefoil is P(x) = x -x +1 -x +x.\n\nSome useful formulas for \"A\" = \"x\"−x:\n\nA set of high dimensional knot version of skein relations is discovered. Relations between local moves on high dimensional knots and the Alexander polynomial of high dimensional knots are found.One of them is a new type. It is A(K+)-A(K-)=(t+1)A(K0). Note that the coefficient in the right side is t+1, not t-1.\n\n"}
{"id": "240727", "url": "https://en.wikipedia.org/wiki?curid=240727", "title": "Sphere of influence", "text": "Sphere of influence\n\nIn the field of international relations, a sphere of influence (SOI) is a spatial region or concept division over which a state or organization has a level of cultural, economic, military, or political exclusivity, accommodating to the interests of powers outside the borders of the state that controls it.\n\nWhile there may be a formal alliance or other treaty obligations between the influenced and influencer, such formal arrangements are not necessary and the influence can often be more of an example of soft power. Similarly, a formal alliance does not necessarily mean that one country lies within another's sphere of influence. High levels of exclusivity have historically been associated with higher levels of conflict.\n\nIn more extreme cases, a country within the \"sphere of influence\" of another may become a subsidiary of that state and serve in effect as a satellite state or de facto colony. The system of spheres of influence by which powerful nations intervene in the affairs of others continues to the present. It is often analyzed in terms of superpowers, great powers, and/or middle powers.\n\nSometimes portions of a single country can fall into two distinct spheres of influence. In the colonial era the buffer states of Iran and Thailand, lying between the empires of Britain/Russia and Britain/France respectively, were divided between the spheres of influence of the imperial powers. Likewise, after World War II, Germany was divided into four occupation zones, which later consolidated into West Germany and East Germany, the former a member of NATO and the latter a member of the Warsaw Pact.\n\nThe term is also used to describe non-political situations, e.g., a shopping mall is said to have a sphere of influence which designates the geographical area where it dominates the retail trade.\n\nMany areas of the world are considered to have inherited culture from a previous sphere of influence, that while perhaps today halted, continues to share the same culture. Examples include the Anglosphere, Arab World, Eurosphere, Francophonie, Françafrique, Germanosphere, Indosphere, Latin Europe/Latin America, Lusophonie, Turkosphere, Chinese cultural sphere, Slavisphere, Hispanophone, Malay World, as well as many others.\n\nAn example of spheres of influence was China in the late 19th and early 20th Century, when Britain, France, Germany, and Russia (later replaced by Japan) had de facto control over large swaths of territory. These were taken by means of military attacks or threats to force Chinese authorities to sign unequal treaties and very long term \"leases\". \nIn December 1897 German Kaiser Wilhelm II declared his intent to seize territory in China, precipitating the scramble to demarcate zones of influence in China. The German government acquired, in Shandong province, exclusive control over developmental loans, mining, and railway ownership, while Russia gained, in addition to the previous tax exemption for trade in Mongolia and Xinjiang, economic powers similar to Germany's over Fengtian, Jilin and Heilongjiang provinces. France gained a sphere over Yunnan, Guangxi and Guangdong provinces, Japan over Fujian province, and the British Empire over the whole Yangtze River Valley and Tibet. Only Italy's request for Zhejiang province was declined by the Chinese government. These do not include the lease and concession territories where the foreign powers had full authority.\n\nIn 1902, Winston Churchill gave a speech regarding the division of China by the great powers, where he declared that \"we shall have to take the Chinese in hand and regulate them\", \"I believe in the ultimate partition of China\" and \"the Aryan stock is bound to triumph\".\n\nThe Russian government militarily occupied their zone, imposed their law and schools, seized mining and logging privileges, settled their citizens, and even established their municipal administration on several cities, the latter without Chinese consent.\n\nThe powers (and the United States) might have their own courts, post offices, commercial institutions, railroads, and gunboats in what was on paper Chinese territory. However, the foreign powers and their control in some cases could have been exaggerated; the local government persistently restricted further encroachment. The system ended after the Second World War.\nIn the Anglo-Russian Convention of 1907, Britain and Russia partitioned Persia (Iran) into spheres of influence, with the Russians gaining recognition for influence over most of northern Iran, and Britain establishing a zone in the Southeast.\n\nFor Siam (Thailand), Britain and France signed an agreement in 1904 whereby the British recognised a French sphere of influence to the east of the River Menam's (Chao Phraya River) basin; in turn, the French recognised British influence over the territory to the west of the Menam basin and west of the Gulf of Thailand. Both parties disclaimed any idea of annexing Siamese territory.\n\nAlexander Hamilton, first U.S. Secretary of the Treasury, aimed for the United States to establish a sphere of influence in North America. Hamilton, writing in the Federalist Papers, harboured ambitions for the U.S. to rise to world power status and gain the strength to expel European powers from the Americas, taking on the mantle of regional dominance among American nations, although most of the New World were European colonies during that period.\n\nThis doctrine was formalised under President James Monroe, who asserted that the New World was to be established as a Sphere of influence, removed from European encroachment. As the U.S. emerged as a world power, few nations dared to trespass on this sphere.\n\nAs of 2018, Secretary of State Rex Tillerson continued to refer to the Monroe doctrine to tout the United States as the region's preferred trade partner over other nations such as China.\n\nFor another example, during the height of its existence in World War II, the Japanese Empire had quite a large sphere of influence. The Japanese government directly governed events in Korea, Vietnam, Taiwan, and parts of Mainland China. The \"Greater East Asia Co-Prosperity Sphere\" could thus be quite easily drawn on a map of the Pacific Ocean as a large \"bubble\" surrounding the islands of Japan and the Asian and Pacific nations it controlled.\n\nAccording to a secret protocol attached to the Molotov-Ribbentrop pact of 1939 (revealed only after Germany's defeat in 1945), Northern and Eastern Europe were divided into Nazi and Soviet spheres of influence. In the North, Finland, Estonia, and Latvia were assigned to the Soviet sphere. Poland was to be partitioned in the event of its \"political rearrangement\"—the areas east of the Narev, Vistula, and San Rivers going to the Soviet Union while Germany would occupy the west. Lithuania, adjacent to East Prussia, would be in the German sphere of influence, although a second secret protocol agreed in September 1939 assigned Lithuania to the USSR. Another clause of the treaty stipulated that Bessarabia, then part of Romania, would join the Moldovan ASSR and become the Moldovan SSR under the control of Moscow. The Soviet invasion of Bukovina on 28 June 1940 violated the Molotov-Ribbentrop Pact, as it went beyond the Soviet sphere of influence as agreed with the Axis. The USSR continued to deny the existence of the Pact's protocols until after the dissolution of the USSR when the Russian government fully acknowledged the existence and authenticity of the secret protocols.\n\nFrom 1941 and the German attack on the Soviet Union, the Allied Coalition operated on the unwritten assumption that the Western Powers and the Soviet Union had each its own sphere of influence. The presumption of the US-British and Soviet unrestricted rights in their respective spheres started causing difficulties as the Nazi-controlled territory shrank and the allied powers successively liberated other states. The wartime spheres lacked a practical definition and it had never been determined if a dominant allied power was entitled to unilateral decisions only in the area of military activity, or could also force its will regarding political, social and economic future of other states. This overly informal system backfired during the late stages of the war and afterwards, when it turned out that the Soviets and the Western Allies had very different ideas concerning the administration and future development of the liberated regions and of Germany itself.\n\nDuring the Cold War the Baltic states, Central Europe, some countries in Eastern Europe, Cuba, Laos, Vietnam, North Korea, and, until the Sino-Soviet split, the People's Republic of China, among other countries at various times, were said to lie under the Soviet sphere of influence. Western Europe, Oceania, Japan, and South Korea, among other places, were often said to lie under the sphere of influence of the United States. However, the level of control exerted in these spheres varied and was not absolute. For instance, France and Great Britain were able to act independently to invade (with Israel) the Suez Canal (they were later forced to withdraw by joint U.S. and Soviet pressure). Later, France was also able to withdraw from the military arm of the North Atlantic Treaty Organisation (NATO). Cuba often took positions that put it at odds with its Soviet ally, including momentary alliances with the People's Republic of China, economic reorganizations, and providing support for insurgencies in Africa and the Americas without prior approval from the Soviet Union. \n\nWith the end of the Cold War, the Eastern Bloc fell apart, effectively ending the Soviet sphere of influence. Then in 1991, the Soviet Union collapsed, replaced by the Russian Federation and several other ex-Soviet Republics who became independent states.\n\nAfter the fall of the Soviet Union, the countries of Eastern Europe, the Caucasus, and Central Asia that became independent were often portrayed as part of the Russian Federation's \"sphere of influence\". According to Ulrich Speck, writing for \"Carnegie Europe\", \"After the breakup of the Soviet Union, the West's focus was on Russia. Western nations implicitly treated the post-Soviet countries (besides the Baltic states) as Russia's sphere of influence.\"\n\nIn 1997, NATO and Russia signed the \"Founding Act on Mutual Relations, Cooperation and Security\", stating the \"aim of creating in Europe a common space of security and stability, without dividing lines or spheres of influence limiting the sovereignty of any state.\"\n\nIn 2009, Russia asserted that the European Union desires a sphere of influence and that the Eastern Partnership is \"an attempt to extend\" it. In March 2009, Sweden's foreign minister Carl Bildt stated that \"The Eastern Partnership is not about spheres of influence. The difference is that these countries themselves opted to join\".\n\nFollowing the 2008 Russo-Georgian War, Václav Havel and other former central and eastern European leaders signed an open letter stating that Russia had \"violated the core principles of the Helsinki Final Act, the Charter of Paris... -all in the name of defending a sphere of influence on its borders.\" In April 2014, NATO stated that \"Contrary to [the Founding Act], Russia now appears to be attempting to recreate a sphere of influence by seizing a part of Ukraine, maintaining large numbers of forces on its borders, and demanding, as Russian Foreign Minister Sergei Lavrov recently stated, that “Ukraine cannot be part of any bloc.”\" Criticising Russia in November 2014, German Chancellor Angela Merkel said that \"old thinking about spheres of influence, which runs roughshod over international law\" put the \"entire European peace order into question\". In January 2017, British Prime Minister Theresa May said, \"We should not jeopardise the freedoms that President Reagan and Mrs Thatcher brought to Eastern Europe by accepting President Putin's claim that it is now in his sphere of influence.\"\n\nIn corporate terms, the sphere of influence of a business, organization or group can show its power and influence in the decisions of other businesses/organizations/groups. Influence shows in several ways, such as in size, frequency of visits, etc. In most cases, a company described as \"bigger\" has a larger sphere of influence.\n\nFor example, the software company Microsoft has a large sphere of influence in the market of operating systems; any entity wishing to sell a software product may weigh up compatibility with Microsoft's products as part of a marketing plan.\n\nIn another example, retailers wishing to make the most profits must ensure they open their stores in the correct location. This is also true for shopping centers that, to reap the most profits, must be able to attract customers to their vicinity.\n\nThere is no defined scale measuring such spheres of influence. However, one can evaluate the spheres of influence of two shopping centers by seeing how far people are prepared to travel to each shopping center, how much time they spend in its vicinity, how often they visit, the order of goods available, etc.\n\nFor historical and current examples of significant battles over spheres of influence see:\n\n\n"}
{"id": "25598803", "url": "https://en.wikipedia.org/wiki?curid=25598803", "title": "Surya Majapahit", "text": "Surya Majapahit\n\nSurya Majapahit (The Sun of Majapahit) is the emblem commonly found in ruins dated from the Majapahit era. The emblem commonly took the form of an eight-pointed sun ray with the rounded part in the center depicting Hindu deities. The emblem might have taken the form of a cosmological diagram haloed by typical \"Surya Majapahit\" sun rays, or a simple circle with typical sun rays. Because of the popularity of this sun emblem during the Majapahit era, it is suggested that the sun emblem served as the imperial symbol or emblem of the Majapahit empire.\n\nThe most common depiction of Surya Majapahit consists of the images of nine deities and eight sun rays.\nThe round center of the sun depicting nine Hindu gods called \"Dewata Nawa Sanga\". The major gods in the center is arranged in eight cardinal points around one god in the center. The arrangements are:\n\nThe minor deities located at the outer rim of the sun, symbolized by eight shining sun rays:\n\nThe emblem is rendered in many forms; sometimes it took the form of the circle of deities and sun rays, or just a simple eight-pointed sun ray such as the emblematic Surya Majapahit set into the ceiling of Candi Penataran. The deities in the sun arranged as cosmological diagram in the form of a mandala. Another variation of Surya Majapahit is the eight pointed sun rays with the god of sun Surya in the center riding celestial horse or chariot.\nThe carving of Surya Majapahit usually can be found on the center ceiling of the Garbhagriha (inner sanctum) of the temple such as Bangkal, Sawentar, and Jawi temple. Surya Majapahit also can be found on the Stella, carving of halo or aura at the back of the statue's head. The carving of Surya Majapahit also commonly found in gravestone dating from Majapahit era, such as the Troloyo cemetery in Trowulan.\n\n\n"}
{"id": "13721661", "url": "https://en.wikipedia.org/wiki?curid=13721661", "title": "Taxation of the Jews in Europe", "text": "Taxation of the Jews in Europe\n\nTaxation of the Jews in Europe refers to taxes imposed specifically on Jewish people in Europe, in addition to the taxes levied on the general population. Special taxation imposed on the Jews by the state or ruler of the territory in which they were living has played an important part in Jewish history. The abolition of special taxes on the Jews followed their admission to civil rights in France and elsewhere in Europe at the end of the 18th and beginning of the 19th centuries.\n\nThe Fiscus Judaicus (Latin: \"Jewish tax\") or \"Temple Tax\" was a tax collecting agency instituted to collect the tax imposed on Jews in the Roman Empire after the destruction of the Temple of Jerusalem in 70 CE in favor of the temple of Jupiter Capitolinus in Rome.\n\nThe tax was initially imposed by Roman Emperor Vespasian as one of the measures against Jews as a result of the First Roman-Jewish War of 66–73 CE. Vespasian imposed the tax in the aftermath of the Jewish revolt (Josephus \"BJ\" 7. 218; Dio Cassius 65.7.2). The tax was imposed on all Jews throughout the empire, not just on those who took part in the revolt against Rome. The tax was imposed after the destruction of the Second Temple in 70 CE in place of the levy (or Tithe) payable by Jews towards the upkeep of the Temple. The amount levied was two denarii, equivalent to the one-half of a shekel that observant Jews had previously paid for the upkeep of the Temple of Jerusalem (). The tax was to go instead to the Temple of Capitoline Jupiter, the major center of ancient Roman religion. The \"fiscus Iudaicus\" was a humiliation for the Jews. In Rome, a special procurator known as \"procurator ad capitularia Iudaeorum\" was responsible for the collection of the tax. Only those who had abandoned Judaism were exempt from paying it.\n\nThe Jewish populations of Europe were politically insecure and could be easily exploited for the levying of heavy taxes in exchange for official protection. The high interest rates charged by Jews became an illimitable source of tax revenue and Jewish wealth was technically easy to gauge because Jews typically stored their assets in the form of cash or promissory notes. \n\nThe Imperial Tax Register of 1241 was the first register to include taxes on the Jews. The total of the taxes on the Jews listed in the Register amounted to 857 silver marks; the total contribution of all the cities together amounted to 4.290 silver marks. These local taxes served wholly or in part to finance town-building. Not all of the contributions reached the central administration. In contrast, it is clear from the Register that the payments made by the Jews reached the Exchequer in their entirety. The taxes on the Jews were first described as the ’’Jewish Tax’’ in 1330.\n\nThe Opferfennig (originally Guldenpfennig) tax was introduced in 1342 by Emperor Louis IV the Bavarian, who ordered all Jews above the age of 12 and possessing 20 gulden to pay one gulden annually for protection. This taxation was 1 florin for every Jew and included assets worth more than 20 florins. Widows were not exempted. King Wenceslas removed the taxable minimum but an exemption for Jews dependent on alms was made later by Sigsmund, who himself levied heavy taxation. Sigsmund taxed a third of the value of Jewish properties. It was represented as a coronation tax as part of Sigsmund's attempts to ascend the throne. Isenmann disagrees and believes it was an innovation sprouted by Archchamberlain Konrad von Weinsberg. By 1433-4 the tax collectors were collecting tax worth a half of Jewish properties. \n\nEmperor Charles IV later ordered the income of the Opferfennig tax to be delivered to the archbishop of Triers. This tax was at some places replaced by an overall communal tax.\n\nThe \"Leibzoll\" or \"Judengeleit\" was a special toll which Jews had to pay in most of the European states in the Middle Ages and up to the beginning of the nineteenth century.\n\nTolerance tax (Toleranzgebührer) was a tax that was levied against Jews of Hungary, then part of the Austrian Empire, beginning in 1747.\n\nThe tax was based on the German statute that a Jew was obliged to pay a certain tax to be \"tolerated\".\n\nIn 1571 a contract was drafted with regard to the status of the Jews in Koło, in which the city's Christians have undertaken to provide protection to the Jews, in return for which the Jews were required to pay a special annual municipal tax.\n\nIn 1729 the Jewish community was required to pay 150 gold coins as an annual poll tax, and in 1738 this sum was increased to 300 gold coins.\n\nIn 1775 the Polish parliament imposed a special duty on books written in Hebrew and Yiddish, requiring each book to be stamped by the municipality. Despite heavy penalties imposed on owners of unstamped books, many books were concealed and unstamped.\n\nThe Russian Kosher tax, known as the \"korobka\", was a tax paid only by Jews for each animal slaughtered in accordance with the kashrut rules and for each pound of this meat sold. It was part of the Russian Jewish \"basket tax\" or \"box tax\". Though it was used to refer to a tax on meat or slaughtering, the word \"korobka\" (Russian: коробка) actually means \"box\" in Russian. The tax came to be called that because Jews paying had to deposit a coin in a box at the kosher slaughterer.\n\nAccording to Herman Rosenthal and Jacob Goodale Lipman, the tax was \"the most burdensome and annoying of the special taxes imposed upon the Jews of Russia by the government\". The burden of taxes, and the \"korobka\" in particular, was one of the factors which drove many Jews to abandon the towns and settle in villages or on noblemen estates.\n\nBetween 1777 and 1784, the Jews of Horodenka, a region on the southeast corner of Galicia, paid a number of special taxes, including the \"protection and tolerance tax\", and the \"property and occupation tax\". In 1784, the property and occupation tax was replaced with the kosher meat tax.\n\nIn 1741, Moldavian prince Grigore Ghica confirmed the obligation of each Jew to pay the crupca, an indirect tax on kosher meat similar to the Russian \"korobka\".\n\nDuring the period 1641–1842, the Jews of Altona (then a town close to Hamburg) paid specifically Jewish taxes as well as the same taxes as other residents of Altona. The tax burden on the members of the Jewish community was twice as heavy as that on the other residents.\n\nIn 1640 the Danish King Christian IV acquired part of the County of Pinneberg including Altona. Altona was subsequently granted the rights and status of a town on 23 August 1664.\n\nThe community was founded principally by Portuguese merchants and was known as the Portuguese-Jewish Community, although many of its members were of Spanish-Jewish descent. \n\nThese Sephardic Jews, who initially pretended to be persecuted Catholics, first came to Hamburg at the end of the 16th century. They were mostly Portuguese- or Spanish-speaking merchants. In 1621, when the armistice between Spain and the Netherlands came to an end, many of the Portuguese Jews moved to Hamburg. They were made welcome, even after the actual situation had become clear; however they were not permitted to establish a cemetery within the city walls. Thanks to the linguistic skills of the Sephardim and their contacts among their co-religionaries they controlled a large sector of the German market in provisions. The Sephardim differed culturally and socially from the Jews who came to Altona, and subsequently also to Hamburg, from the east. These were Yiddish Language speaking Ashkenazim.\nThe permanent settlement of the Ashkenazim was opposed by both the Senate of Hamburg (town council) and the citizens, supported by the Sephardim, who did not wish to see the establishment of a second community in Hamburg. As servants, referred to as tudescos, the Ashkenazi Jews were, de facto, under the protection of the Sephardic Jews.\n\nOn 1 August 1641 the Danish king had formally granted the Ashkenazi Jews the privilege of having in Altona, as hitherto, a cemetery and a synagogue, thus providing the basis for the existence of a Jewish community. Subsequently, the Danish kings promised the Jews personal security, the freedom to practice trade and religious liberties. Some of the Jews living in Hamburg therefore tried to secure the legal protection of the Danish crown in case of any attempt to expel them. In Altona the conditions of residence were favorable, in Hamburg the conditions for trading. These were the reasons for the genesis of the Altona community of Ashkenazi Jews from Hamburg and Altona. Thanks to immigration from the east, Altona became a center of research and scholarship in Jewish teaching, attracting hundreds of students. The officially recognized Jewish court of justice had a reputation as one of the most distinguished in the whole Jewish world. The Jews did not acquire these privileges freely, but in return for the payment of taxes.\n\nFrom 1584 to 1639, as in the Middle Ages, the Jews of Altona paid taxes specific to the Jews, but no further taxes. Each Jewish family was required to pay 6 Reichstaler per year. Under Danish rule this changed: the Jews continued to pay the specifically Jewish taxes plus the same taxes as all other residents. From 1641 every Jewish family was required to pay 5 Reichstaler in Jewish taxes; in the year when Altona became a town the contribution rose to 6 Reichstaler. With the ordinance of 1641 the Danish king had permitted the Jews shechita. This privilege too was not cost-free. For the years 1667–1669 we have records of taxes paid by Jewish butchers. According to these the rates were 1 Mark and 8 Schillinge for an ox, 4 Schillinge for a calf and 2 Schillinge for a lamb. These taxes were twice as high as those paid by Christian butchers. From 1681 Individual taxes on the Jews (6 Reichstaler plus the payments made by the Jewish butchers), were replaced by lump-sum payments by the Jewish community. \n\nFrom the year 1712 onwards it is possible to calculate the amount of the lump-sum payments made by the Jews. During the period 1712–1818 this amounted to 6 Reichstaler for each Jewish family; 6 Reichstaler was the level that had already been set in 1584. Assuming that a Jewish family consisted of approximately 6 persons, 6 Reichstaler corresponded to 1 Reichstaler for each individual Jew. On top of this 1 Reichstaler, also paid by the other residents, had to be paid. The tax burden on the members of the Jewish community was twice as heavy as that on the other residents. The influence of this on business practice constituted an obstacle to the granting of civil rights. In the year 1818 the Jewish Elders declared to the community of Altona that they could not, ’’on the one hand, levy specifically Jewish taxes on the members of their community and, on the other hand, encourage our co-religionaries, especially the younger ones, to pursue useful activity. In short: so to improve our condition that we might not seem unworthy to acquire civil rights. The extension – against our wishes - of the taxes on the Jews would be incompatible with the granting of civil rights.’’ This marks the beginning of the struggle of the Jewish community for emancipation. The Jewish community secured the abolition of taxes on the Jews in the year 1842.\n\n\n\n"}
{"id": "16872509", "url": "https://en.wikipedia.org/wiki?curid=16872509", "title": "The Corner House (organisation)", "text": "The Corner House (organisation)\n\nThe Corner House is a not for profit company limited by guarantee founded in 1997 in the United Kingdom. According to its website, it aims \"to support democratic & community movements for environmental & social justice\"\n\nIn 2007, The Corner House and the Campaign Against Arms Trade sought a judicial review of the UK Government's discontinuation of an investigation by the Serious Fraud Office into the Al Yamamah arms deal. The High Court initially declared the Serious Fraud Office's discontinuation unlawful in April 2008, but this decision was unanimously reversed by the House of Lords in July of the same year.\n\nThe principle had already been established in \"Khalid\" that where the Crown had no choice, a prosecution could be dropped in the face of an extreme threat. The question was therefore simply whether the Director of the Serious Fraud Office had done 'all that could reasonably be done had been done to resist the threat'. In April 2008 the High Court of Justice ruled that the Serious Fraud Office had acted unlawfully in dropping the investigation for failing to do so. \n\nOn 30 July of the same year, this decision was unanimously reversed by the House of Lords. They overruled both the factual and principled basis of the High Court's decision. As a matter of principle, they ruled the courts could only declare the decision unlawful if it was outside of the Director's power, not merely unreasonable. Factually, they held that his decision was in any case reasonable.\n\nJudgements:\n\nCorner House (High Court): \"R. (on the application of Corner House Research) v Director of the Serious Fraud Office\" [2008] EWHC 714 (Admin)\n\nCorner House (House of Lords): \"R (On the Application of Corner House Research and Others) v Director of the Serious Fraud Office\" [2008] UKHL 60\n\n"}
{"id": "5316594", "url": "https://en.wikipedia.org/wiki?curid=5316594", "title": "The Imp of the Perverse", "text": "The Imp of the Perverse\n\nThe Imp of the Perverse is a metaphor for the urge to do exactly the wrong thing in a given situation for the sole reason that it is possible for wrong to be done. The impulse is compared to an imp (a small demon) which leads an otherwise decent person into mischief.\n\nThe phrase has a long history in literature, and was popularized (and perhaps coined) by Edgar Allan Poe in his short story, \"The Imp of the Perverse\".\n\nPoe explores this impulse through several of his fictional characters, the narrators in \"The Black Cat\" and in \"The Tell-Tale Heart\".\n\n\n\n"}
{"id": "1170065", "url": "https://en.wikipedia.org/wiki?curid=1170065", "title": "Underdog", "text": "Underdog\n\nAn underdog is a person or group in a competition, usually in sports and creative works, who is popularly expected to lose. The party, team, or individual expected to win is called the favorite or top dog. In the case where an underdog wins, the outcome is an upset. An \"underdog bet\" is a bet on the underdog or outsider for which the odds are generally higher.\n\nThe first recorded uses of the term occurred in the second half of the 18th century; its first meaning was \"the beaten dog in a fight\".\n\nIn British and American culture, underdogs are highly regarded. This harkens to core Judeo-Christian parables such as the story of David and Goliath and also ancient British legends such as Robin Hood and King Arthur, and reflects the ideal behind the American dream, where someone who is poor and/or weak can use hard work to achieve victory. Underdogs are most valorized in sporting culture, both in real events, such as the Miracle on Ice, and in popular culture depictions of sports, where the trope is omnipresent. The idea is so common that even when teams are evenly matched, spectators and commentators are drawn to establishing one side as the underdog. Historian David M. Potter explained that underdogs are appealing to Americans not because they simply beat the odds, but overcome an injustice that explains those odds - such as the game being unfairly rigged due to privilege and power.\n\nIn a story, the Fool is often an underdog if they are the main character. Their apparent ineptitude leads to people underestimating their true abilities, and they are able to win either through luck or hidden wisdom against a more powerful, \"establishment\" villain. An example in film is The Tramp portrayed by Charlie Chaplin.\n\n"}
{"id": "20640342", "url": "https://en.wikipedia.org/wiki?curid=20640342", "title": "Weiquan movement", "text": "Weiquan movement\n\nThe Weiquan movement is a non-centralized group of lawyers, legal experts, and intellectuals in China who seek to protect and defend the civil rights of the citizenry through litigation and legal activism. The movement, which began in the early 2000s, has organized demonstrations, sought reform via the legal system and media, defended victims of human rights abuses, and written appeal letters, despite opposition from Communist Party authorities. Among the issues adopted by Weiquan lawyers are property and housing rights, protection for AIDS victims, environmental damage, religious freedom, freedom of speech and the press, and defending the rights of other lawyers facing disbarment or imprisonment.\n\nIndividuals involved in the Weiquan movement have met with occasionally harsh reprisals from Chinese officials, including disbarment, detention, harassment, and, in extreme instances, torture. Authorities have also responded to the movement with the launch of an education campaign on the \"socialist concept of rule of law,\" which reasserts the role of the Communist Party and the primacy of political considerations in the legal profession, and with the Three Supremes, which entrenches the supremacy of the Communist Party in the judicial process.\n\nSince the legal reforms of the late 1970s and 1980s, the Chinese Communist Party (CCP) has moved to embrace the language of the rule of law and establish a modern court system. In the process, it has enacted thousands of new laws and regulations, and begun training more legal professionals. The concept of \"rule of law\" was enshrined in the constitution, and the CCP embarked on campaigns to publicize the idea that citizens have protection under the law. At the same time, however, a fundamental contradiction exists in the implementation of rule of law wherein the CCP insists that its authority supersedes that of the law; the constitution enshrines rule of law, but also emphasizes the principle of the \"leadership of the Communist Party.\" The judiciary is not independent, and is therefore subject to politicization and control by the Communist Party. This has produced a system that is often described as \"rule by law,\" rather than rule of law.\n\nBecause judicial decisions are subject to the sometimes arbitrary assessments of the CCP, citizens who attempt to make use of the legal system to pursue grievances find that, if their cause is determined to have the potential to undermine the authority of the Communist Party, they may be suppressed. Defendants who find themselves facing criminal charges, such as for conducting activism or for their religious beliefs, often have few means of pursuing an effective defense.\n\nThe Weiquan movement coalesced in the early 2000s in response to these inherent contradictions and the arbitrary exercise of legal authority in China, though its roots could be traced to the consumer protection movement that began in the 1990s. The movement is informal, and can be understood as including lawyers and legal activists who advocate for civil rights and defend the interests of citizens against corporations, government or Communist Party organs. Fu Hualing and Richard Cullen note that Weiquan lawyers \"are generally always on the side of the weaker party: (migrant) workers v. employers in labor disputes; peasants in cases involving taxation, persons contesting environmental pollution, land appropriation, and village committee elections; journalists facing government censorship; defendants subject to criminal prosecution; and ordinary citizens who are discriminated against by government policies and actions.\"\n\nThe emergence of the Weiquan movement was made possible by a confluence of factors, including a market for their services, and an emerging rights consciousness. It was also facilitated by the 1996 \"Lawyers Law,\" which changed the definition of lawyers from \"state legal workers\" to professionals holding a legal certificate who perform legal services. The law effectively delinked lawyers from the state, and gave lawyers greater (though still limited) autonomy within the profession.\n\nWeiquan lawyers tend to be especially critical of the lack of judicial independence in China. Rather than challenging particular laws, they frame their work as being in keeping with Chinese laws, and describe their activities as a means of defending and upholding the Constitution against abuses. As such, Weiquan lawyering has been described as a form of Rightful resistance.\n\nWeiquan Lawyers (), or \"rights protection\" lawyers, refer to a small but influential movement of lawyers, legal practitioners, scholars and activists who help Chinese citizens to assert their constitutional, civil rights and/or public interest through litigation and legal activism. In the context of a rising number of lawyers in China, the proportion of Weiquan lawyers is very small Weiquan Lawyers face considerable personal, financial and professional risks.\n\nNotable Weiquan Lawyers include He Weifang, Xu Zhiyong, Teng Biao, Guo Feixiong and Chen Guangcheng, Gao Zhisheng, Zheng Enchong, and Li Heping. Many barefoot lawyers are peasants who teach themselves enough law to file civil complaints, engage in litigation, and educate fellow citizens about their rights. Some Weiquan lawyers are the pragmatists and some are more radical.\n\nAlthough freedom of speech is enshrined in Article 35 of the Constitution of the People's Republic of China, Chinese authorities enforce restrictions on political and religious expression. Such restrictions are sometimes in accordance with Article 105 of the criminal code, which contains vague and broadly defined provisions against \"inciting subversion of state power\". Weiquan lawyers, along with international human rights organizations, have argued that the provisions against subversion are inconsistent both with China's own constitution and with international human rights standards, particularly in light of the lack of transparency and clear guidelines used in applying the laws.\n\nSeveral Weiquan lawyers have been involved in litigation and other forms of advocacy to defend the rights to free expression for individuals charged with the crime of subversion. Notable cases include that of Liu Xiaobo, a prominent Beijing intellectual sentenced to 11 years in prison for inciting subversion in December 2009. Chengdu activist Tan Zuoren was sentenced to five years for inciting subversion for publishing writings on the 1989 Tiananmen Square Massacre, advocating for the families of the 2008 Sichuan earthquake victims, and accepting interviews from the Falun Gong-affiliated Sound of Hope radio. His lawyers were reportedly barred from entering the courtroom. In October 2009, intellectual Guo Quan was sentenced to 10 years in prison for publishing \"reactionary\" articles online.\nWeiquan lawyers have also challenged the application of state secret laws, which are sometimes used to prosecute individuals who disseminate information on politically sensitive issues. In November 2009, for instance, lawyers were involved in arguing for Huang Qi, a Sichuan activist who had advocated online for the parents of Sichuan earthquake victims. Huang was sentenced to three years in prison for possession of state secrets.\n\nThe Chinese Constitution enshrines rule of law, but simultaneously emphasizes the principle of the \"leadership of the Communist Party.\" The legal profession itself is subordinate to the authority of the Communist Party; the Ministry of Justice, not the bar associations, is responsible for issuing and renewing lawyers' licenses. Weiquan lawyers have argued that this structure precludes the emergence of genuine rule of law, and in some cases have advocated for reforms to advance judicial independence and the protection of legal professionals.\n\nIn late August 2008, a collection of several dozen Beijing lawyers signed a petition stating that the Beijing Bar Association leaders should be elected by the organization's members, rather than being appointed. The petition letter stated that selection process in place for the Association's directors is inconsistent with official guidelines and the Chinese constitution, and should be replaced with a democratic voting process. The Beijing Bar Association responded to the campaign by asserting that \"Any individual who uses text messages, the web or other media to privately promote and disseminate the concept of direct elections, express controversial opinions, thereby spreading rumors within the Beijing Bar Association, confuse and poison people's minds, and convince people of circumstances that do not exist regarding the so-called 'Call For Direct Elections For the Beijing Bar Association' is illegal.\" The following year, the Beijing Bureau of Justice refused to renew the licenses of 53 Beijing Weiquan lawyers, all of whom had signed the petition for elections to the Bar Association.\n\nUnder Chinese property law, there is no privately held land; \"urban land\" is owned by the state, which grants land rights for a set number of years. Rural, or \"collectively owned land,\" is leased by the state for periods of 30 years, and is theoretically reserved for agricultural purposes, housing and services for farmers.\n\nForced evictions are forbidden under International Covenant on Economic, Social and Cultural Rights, which China has ratified. Under China's constitution and other property laws, expropriation of urban land is permitted only if it is for the purpose of supporting the \"public interest,\" and those being evicted are supposed to receive compensation, resettlement, and protection of one's living conditions. The \"public interest\" is not defined, however, and abuses are common in the expropriation process, with many citizens complaining of receiving little or no compensation.\n\nForced evictions with little or no compensation occur frequently in both urban and rural China, with even fewer legal protections for rural citizens. Collectively owned rural land may be \"reallocated\" at the discretion of authorities, and in many regions local governments collude with private developers to reclassify rural land as urban land, which can then be sold. from the mid-1990s to mid-2000s, an estimated 40 million Chinese peasants were affected by land requisitions. Citizens who resist or protest the evictions have reportedly been subjected to harassment, beatings, or detention, and land-related grievances occasionally escalate into large-scale protests or riots.\n\nSeveral Weiquan lawyers have advocated for the rights of individual citizens whose land and homes were taken with inadequate compensation, including Shanghai lawyer Zheng Enchong. Ni Yulan, a Beijing lawyer, was herself left homeless by forced eviction, and became an outspoken advocate for victims before being sentenced to two years in prison.\n\nIn 2007, a 54-year-old farmer in Heilongjiang Yang Chunlin published numerous articles on human rights and land rights, and helped to organise a petition entitled: \"We want human rights, not the Olympics.\" The petition reportedly collected over ten thousand signatures. Yang was put to trial, and sentenced to five years in prison, where he has allegedly been tortured. Li Fangping was hired to defend him, but was denied access to his client.\n\nSeveral Weiquan lawyers, including Teng Biao, Jiang Tianyong, and Li Fangping, offered legal aid to Tibetans in the wake of the March 2008 Tibetan protests. The protests resulted in the imprisonment of at least 670 Tibetans, and the execution of at least four individuals. Chinese government sources asserted that the unrest and violence in Tibet had been masterminded by the Dalai Lama and executed by his followers for the purpose of fomenting unrest and disrupting the 2008 Summer Olympics in Beijing. The Open Constitution Initiative (OCI), operated by several Weiquan lawyers and intellectuals, issued a paper in May 2009 challenging the official narrative, and suggesting that the protests were instead a response to economic inequities, Han Chinese migration, and religious sentiments. The OCI recommended that Chinese authorities better respect and protect the rights and interests of the Tibetan people, including religious freedom, and pursue the reduction of economic inequality and official corruption.\n\nTibetan Filmmaker Dhondup Wangcheng was sentenced to six years in prison for making a documentary on human rights in Tibet in the run-up to the Beijing Olympics. Two lawyers who sought to represent him, Chang Boyang and Li Dunyong, faced threats and harassment for their advocacy.\n\nIn July 2010, a group of Chinese activists including Teng Biao co-signed a letter to the Chinese leadership to protest the 15-year prison sentence that had been meted out to Uighur journalist Halaite Niyaze. Niyaze was not permitted to have a lawyer at his trial, where he was charged with \"endangering state security.\" According to reports, Niyaze was being charged because he had criticized the Chinese government in an interview with a Hong Kong news agency for not doing enough to prevent the July 2009 Ürümqi riots.\n\nFalun Gong, a spiritual qigong discipline that once claimed tens of million adherents in China, was banned in July 1999 under the leadership of the Communist Party, and a campaign was launched to suppress the group. In an attempt to have Falun Gong adherents renounce their belief in the practice, they are subject to state-sanctioned, systematic violence in custody, sometimes resulting in death. Some sources indicate hundreds of thousands may have been detained in reeducation-through-labor camps for practicing Falun Gong and/or resisting persecution.\n\nIn November 1999, the Supreme People's Court offered a judicial interpretation of article 300 of the criminal code, stating that Falun Gong should be regarded as a \"\"xie jiao\",\" or cult. Large numbers were subsequently sentenced to long prison terms, often under article 300, in what are typically very short trials without the presence of a lawyer. In 2009 alone, the Falun Dafa Information Center reported that several hundred Falun Gong adherents have been sentenced to prison terms of up to 18 years. Human rights groups, including Amnesty International, note that the application of the law to persecute Falun Gong adherents contravenes both China's own constitution and international standards. Several Weiquan lawyers have argued similarly while defending Falun Gong adherents who face criminal or administrative sentencing for their beliefs. Lawyers who have defended Falun Gong include Guo Guoting, Zhang Kai and Li Chunfu, Wang Yonghang,Tang Jitian and Liu Wei, among others.\n\nIn addition to litigation work, Weiquan lawyers like Gao Zhisheng have also advocated publicly and in the media for human rights for Falun Gong. In 2004 and 2005, Gao wrote a series of letters to China's top leadership detailing accounts of torture and sexual abuse against Falun Gong practitioners, and calling for an end to the persecution of the group. In response, Gao lost his legal license, was put under house arrest, detained, and was reportedly tortured.\n\nSome Weiquan lawyers have advocated for the rights of HIV/AIDS victims who contracted the virus as a result of state-sponsored blood drives. In the 1990s, government officials in central China, and especially in Henan, encouraged rural citizens to sell blood plasma in order to supplement their incomes. Gross mismanagement of the process resulted in hundreds of thousands of individuals being infected with HIV. According to activists, victims have not been compensated, and no government officials were held accountable. Authorities continue to suppress information about the epidemic, which is particularly sensitive in light of the involvement of Li Changchun, the Communist Party Propaganda head and formerly Party chief in Henan.\n\nHu Jia is arguably the most well known advocate for HIV/AIDS victims, having served as the executive director of the Beijing Aizhixing Institute of Health Education and as one of the founders of the non-governmental organization Loving Source.\n\nChen Guangcheng, a blind self-taught Weiquan lawyer, rose to prominence for defending victims of China's one-child policy. First implemented in 1979, the one-child policy mandates that couples may only have one child, though there are exceptions for some rural citizens, ethnic minorities, and couples who were themselves only children. Though Chinese laws condemn harsh enforcement measures, Chinese authorities and family planning staff have been accused of carrying out coercive, late-term forced abortions, sterilization, incarceration and torture to enforce the policy. In 2005, Chen Guangcheng filed a class action case against family planning officials in Linyi, Shandong, who were accused of subjecting thousands of women to sterilization or forced abortions.\n\nChina's constitution guarantees freedom of religion, yet also provides a caveat specifying that only \"normal\" religious activities are permitted. In practice, religious freedom is granted only within the strictly prescribed parameters of the five officially sanctioned \"patriotic\" religious associations of Buddhism, Taoism, Islam, Protestantism and Catholicism. Groups falling outside the state-administered religions, including \"underground\" or \"house church\" Christians, are subject to varying degrees of repression and persecution.\n\nAlthough there are no definitive figures on the number of underground Christians in China, some estimates have put their total number in excess of 70 million. At least 40 Catholic bishops operate independently of official sanction, and some are under surveillance, house arrest, detention, or have disappeared. Several leaders and members of underground Protestant churches have also been detained and sentenced to reeducation through labor or prison terms. Violent raids and demolitions have been carried out on underground churches, sometimes resulting in injury to congregants inside. Chinese officials have labelled several underground Protestant churches as a \"xie jiao\" (translated literally as \"evil religion\"), or cult, thus providing a pretext for harsher punishment of members.\n\nSeveral prominent Weiquan lawyers themselves identify with the underground Protestant movement, and have sought to defend church members and leaders facing imprisonment. These include Zhang Kai, Li Heping, and Gao Zhisheng. Former house church leader Bob Fu's US-based organization \"ChinaAid\" has sponsored legal cases, and provided \"rule-of-law training\" and legal help for distressed clients in China.\n\nA number of specific events have attracted the help and attention of Weiquan activists. In the March 2008 earthquake in Sichuan province, shoddy school construction resulted in the collapse of several schools full of students. A number of Weiquan lawyers, including Tan Zuoren, were involved in advocating for the rights of parents, and in investigating allegations that corrupt officials were responsible for the poor construction. Parents and lawyers met with reprisals from Chinese officials for their activism.\n\nLater the same year, it was revealed that large quantities of infant formula had been tainted with melamine, causing 300,000 infants to fall ill and resulting in several deaths. A group of parents of the victims were reportedly detained for attempting to draw media attention to their plight. Dozens of lawyers—particularly from the provinces of Hebei, Henan and Shandong—offered pro-bono legal services to victims, but their efforts were obstructed by authorities.\n\nIndividual human rights cases, such as the Deng Yujiao incident and the death of Qian Yunhui, have also drawn help from rights defenders such as Wu Gan.\n\nIn 2003, a group of legal scholars, including Teng Biao and Xu Zhiyong, formed the Open Constitution Initiative () to advocate for greater rule of law. The organization was involved in the Sun Zhigang case, and has advocated for petitioners, labor rights, freedom of expression, HIV/AIDS victims, Tibetans, land rights, and protection of public health, among other issues.\n\nIn response to the emergence of the Weiquan movement, which often makes use of the official language about \"rule of law\" to justify its work, in April 2006 a political campaign was launched to solidify the Communist Party's leadership over judicial work, combat the idea of greater independence for judges and lawyers, and educate people and judicial authorities about the \"socialist concept of rule of law.\" The campaign was announced by Luo Gan, then the head of the Party Central Committee's Political and Legislative Affairs Committee. Luo urged that in order to protect political stability, \"forceful measures\" be adopted \"against those who, under the pretext of rights-protection (weiquan), carry out sabotage.\" The launch of the campaign coincided with a crackdown on Weiquan lawyers.\n\nShortly after the campaign's launch, Party Committees provided instruction to judges reminding them of the political goals that their work must uphold. According to one document issued to judges in Zhejiang and quoted by Human Rights Watch, \"Recently, some judges have started to believe that to be a judge you just have to strictly apply the law in a case. In fact, this kind of concept is erroneous [...] all the legal formulations have a clear political background and direction [...] We must stamp out the kind of narrow viewpoint that thinks that you can also do court work by having judicial independence.\"\n\nDuring a December 2007 conference on political-legal work, CCP general secretary Hu Jintao articulated the theory of the \"Three Supremes,\" which emphasized again that legal work should regard as supreme the concerns and interests of the Communist Party. In March 2008, Wang Shengjun was confirmed as the new head of the Supreme People's Court. Wang, who has no formal legal training himself, abandoned the efforts of his predecessors to improve judicial competence, training, and autonomy, and instead placed primary importance on the ideological implications of the \"Three Supremes\" theory and upholding the leadership of the Communist Party.\n\nIn 2010, China's Ministry of Justice issued two new regulations intended to \"strengthen the supervision and management of lawyers and law firms\". According to the Associated Press, the new regulations would serve to \"allow authorities to punish lawyers ... for actions such as talking to the media or even causing 'traffic troubles.'\"\n\nIn March 2012, China's Ministry of Justice issued a new directive requiring lawyers first obtaining their license or renewing an existing license to swear an oath of loyalty to the Communist Party. According to the Ministry's website, a section of oath includes the following: “I swear to faithfully fulfill the sacred mission of legal workers in socialism with Chinese characteristics. I swear my loyalty to the motherland, to the people, to uphold the leadership of the Communist Party of China and the socialist system, and to protect the dignity of the Constitution and laws.\" \n\nWeiquan lawyers have faced various challenges to their work from the Chinese government, including disbarment or suspension, violence, threats, surveillance, arbitrary detention, and prosecution. This is particularly true for lawyers who take up politically sensitive cases. Reports of harassment, intimidation, and violence against Weiquan lawyers increased in 2006 following the launch of the campaign to promote the \"socialist concept of the rule of law.\" Authorities have refused to renew the licenses of several dozen Weiquan lawyers, and several have effectively been banned for life from the legal profession. In 2009, for instance, at least 17 Weiquan lawyers were not permitted to renew their legal licenses after taking on politically sensitive cases. Several Weiquan lawyers have themselves been sentenced to prison in response to their activism. A selection of notable instances of suppression are listed here:\n\n\nAlthough there is relatively little awareness of the Weiquan phenomenon as a movement outside of China, Western governments and human rights organizations have consistently expressed concern over the treatment of individual Weiquan lawyers in China, some of whom have faced disbarment, imprisonment, prolonged disappearance, sentencing and alleged torture for their work in promoting civil rights and speaking out against one-party rule. In October 2010, a bipartisan group of 29 members of the U.S. House of Representatives pressed President Obama to raise the cases of Liu Xiaobo and Gao Zhisheng with the Chinese leadership, writing of Gao Zhizheng's prolonged detention: \"If lawyers are hauled away for the \"crime\" of defending their clients, then even the pretense of rule of law in China has failed.\" The U.S. State Department claims to have raised the cases of these two individuals with their Chinese counterparts.\n\nIn 2008, Hu Jia was awarded the Sakharov Prize by the European Parliament recognizing his human rights advocacy. The same year, Hu and Gao Zhisheng received nominations for the Nobel Peace Prize, and were considered to be favorites for the award. Two years later, seven members of the U.S. House of Representatives nominated imprisoned lawyers Gao Zhisheng and Chen Guangcheng, along with fellow dissident Liu Xiaobo for the prize. The letter noted that these individuals have sought to \"raise the Chinese people's awareness of their dignity and rights, and to call their government to govern within its constitution, its laws, and the international human rights agreements it has signed,\" and thereby made a significant contribution to peace. The Nobel Prize Committee awarded the honor to Liu in absentia in December 2010.\n\n\n"}
{"id": "41083749", "url": "https://en.wikipedia.org/wiki?curid=41083749", "title": "Wholistic reference", "text": "Wholistic reference\n\nWholistic reference is reference to the whole—with respect to the context. In its strongest, unqualified form, the principle of wholistic reference is the proposition that each and every proposition, regardless how limited the referents of its non-logical or content terms, refers to the whole of its universe of discourse. According to this principle every proposition of number theory, even an equational proposition such as 5 + 7 = 12, refers not only to the individual numbers that it happens to mention but to the whole universe of numbers. The relation verb ‘refers’ is being used in its broad sense (loosely “is about”) and not as a synonym for ‘names’ in the sense of “is a name of”.\n\nGeorge Boole (1815–1864) introduced this principle into modern logic: Even though he changed from a monistic fixed-universe framework in his 1840s writings to a pluralistic multiple-universe framework in 1854, he never wavered in his frank avowal of the principle of wholistic reference. Indeed, he took it as an essential accompaniment to his theory of concept formation and proposition formation. For Boole, the essential first step in the process of conceiving of a proposition preliminary to making a judgement of its truth or falsity – or even using it in a deduction, however hypothetically – was to conceive of the universe of discourse. See Boole 1854/2003, xxi, 27, 42, 43. One statement of his principle is in the sentence immediately following his definition of universe of discourse, which is his first use of the expression 'universe of discourse' and probably the first in the history of the English language. See the next section.\n\nSimilar views, perhaps not similarly motivated, are found in later logicians, including Gottlob Frege (1848–1925). Some recent formulations of standard one-sorted first-order logic seem to be in accord with a form of it, if they do not actually imply the principle itself.\n"}
{"id": "244817", "url": "https://en.wikipedia.org/wiki?curid=244817", "title": "Yukio Tsuda (professor)", "text": "Yukio Tsuda (professor)\n\nTsuda was born in Kanagawa, Japan in 1950. He majored in English and graduated from Yokohama National University in 1973. He received his M.A. in TEFL (Teaching English as a Foreign Language) in 1978 and his Ph.D. in Speech Communication in 1985 from Southern Illinois University at Carbondale.\n\nTsuda held several faculty positions in his academic career. He was Associate Professor in the Faculty of Economics at Nagasaki University from 1986 to 1988. He then moved to Nagoya University where he was Associate Professor in the Institute of Languages and Cultures from 1988 to 1993 and Professor in the Department of International Communication and in the Graduate School of International Development from 1993 to 2001.\n\nTsuda had been Professor in the Doctoral Program in Modern Cultures and Public Policies in the Graduate School of Humanities and Social Sciences at the University of Tsukuba since 2001. He retired from the University of Tsukuba and founded the Institute of Peace Linguistics in Ibaraki Prefecture in 2014. He is currently Professor in the Department of English at Matsuyama University in Ehime Prefecture.\n\nTsuda was Visiting Professor at the International Research Center for Japanese Studies in Kyoto in 1996 and at the College of San Mateo in California in 2007. He was also Braj Kachru Fellow in the \"Internationalization Forum\" program in 1996 and Visiting Fellow in 1999 at the East-West Center in Honolulu, Hawaii.\n\nTsuda's academic interests include language policy, cross-cultural psychoanalysis, and international and intercultural communication. Among his publications are \"Language Inequality and Distortion in Intercultural Communication: A Critical Theory Approach\" (John Benjamins, 1986), \"Language, Education, and Intercultural Communication\" (Nagasaki University, 1988), \"Eigo Shihai-no Kouzou\" [\"The Structure of the Dominance of English\"] (Daisan Shokan, 1990), \"Shinryaku-suru Eigo, Hangeki-suru Nihongo\" [\"The Invading English, The Counter-Attacking Japanese\"] (PHP Institute, 1996), and \"Eigo Shihai-to Kotoba-no Byoudou\" [\"The Hegemony of English and Linguistic Equality\"] (Keio University Press, 2006).\n\nTsuda is well known as a critic of the hegemony of English and as an advocate of linguistic and cultural pluralism. He believes that the domination of English is tantamount to linguicism and linguicide, and that addressing the problem of linguistic hegemony is crucial to the development of human and cultural security. In an article in \"The San Matean\", a San Mateo Community College newspaper, dated March 19, 2007, Tsuda was quoted to say: \"It is more important to be students learning other languages than being a teacher only teaching one.\" He proposed the \"ecology of language\" paradigm as opposed to the \"diffusion of English\" paradigm.\n\nTsuda urges international and intercultural communication scholars to recognize the hegemony of English as a subject of academic inquiry in the fields especially in the English-speaking countries. He also suggests that English-language teaching professionals incorporate the ecology of language paradigm into the contents and methods of teaching as well as teacher education. Finally, he insists that both native speakers and non-native speakers of English learn the philosophy of the ecology of language so that they will become more sensitive to the ethical aspects of international and intercultural communication.\n\n\n"}
{"id": "732725", "url": "https://en.wikipedia.org/wiki?curid=732725", "title": "Zhu Yu (artist)", "text": "Zhu Yu (artist)\n\nZhu Yu ( 朱昱 b. 1970) is a performance artist living in Beijing, China. Zhu graduated from the Affiliated High School of the China Central Academy of Fine Arts in 1991. His work deals with subjects of contemporary art.\n\nZhu Yu is often termed the most controversial and criticized artist in China. Zhu graduated from the Affiliated High School of the Central Academy of Fine Arts in 1991. His contemporary performance art raises questions about moral agendas, and draws an audience through its shock value. His artwork often encompasses the human body. He is categorized by some critics as an artist of the “cadaver school,” which consists of artists who tend to use human body parts in their work. Yu's most famous piece of conceptual art, titled \"Eating People,\" was performed at a Shanghai arts festival in 2000. It consisted of him cooking and eating what is alleged to be a human fetus. The picture, circulated on the internet via e-mail in 2001, provoked investigations by both the FBI and Scotland Yard. It was intended as \"shock art\". Snopes and other urban legend sites have said the \"fetus\" used by Zhu Yu was most likely constructed from a duck's body and a doll head. Other images from another art exhibit were falsely circulated along with Zhu Yu's photos and claimed to be evidence of fetus soup. The piece's cannibalistic theme caused a stir in Britain when Yu's work was featured on a Channel 4 documentary exploring Chinese Contemporary Art in 2003. In response to the public reaction, Mr. Yu stated, \"No religion forbids cannibalism. Nor can I find any law which prevents us from eating people. I took advantage of the space between morality and the law and based my work on it\". Yu has claimed that he used an actual fetus which was stolen from a medical school. He was prosecuted for his deeds.\n\nImages from the piece have also been used in anti-Chinese propaganda, disseminated by e-mail with a short text attached explaining the images show China's \"hottest food\" and that dead fetuses can be bought for 10-12,000 Yen (approximately US$100 - US$120). Recipients are encouraged to forward the mail, and the explanatory text is written in both English and Korean script. The Turkistan Islamic Party claimed that \"Muslim children in Turkistan\" were eaten by the Chinese, showing the faked pictures by Zhu Yu and photos of fake fetuses from an art exhibit.\n\nZhu Yu has been involved in many group exhibitions including Post-Sense Sensibility- Alien Bodies & Delusion in Beijing (January 1999), and The Third Guangzhou Triennial in Guangzhou (September 2008), which involved 181 artists from 40 countries.\nMost notable is his work at the Fuck Off Exhibit curated by Ai Weiwei and Feng Boyi in Shanghai, 2000. This controversial exhibit hosted 48 contemporary avant-garde artists. This is where his most controversial piece of performance art “Eating People” appeared. Among his other solo exhibitions are Plaything (Long March Space, Beijing 2010) and Leftover (Xin Beijing Art Gallery, Beijing, 2007).\n\nThe Foundation of All Epistemology: This work appeared in the 1998 group exhibition \"It’s All Right\" in Shanghai. For this piece, Zhu Yu cut and boiled five human brains which were purchased from a local hospital. He placed them in neatly labeled jars that he then signed with his own name. Zhu put these jars of brains up for sale in a market that sponsored the exhibit. He ended up selling 15 bottles each for the price of 98 yuans.\n\nEating People: In his performance art piece Eating People, Zhu photographs himself cooking and eating a human fetus that he divided into five parts. Zhu says that “I herewith announce my intention and my aim to eat people as a protest against mankind’s moral idea that he/she cannot eat people.” In further response to Zhu’s bold performance, The Ministry of Culture cited a menace to social order and the spiritual health of the Chinese people, and banned exhibitions involving culture, animal abuse, corpses, and overt violence and sexuality. However, this piece did not even appear at the exhibit; The night before the exhibition, Ai Weiwei collaborated with Zhu and the photographs were removed from the gallery. This piece was thought particularly controversial, and organizers did not want to risk government censorship for the rest of the exhibit. The response to this work stemmed from its appearance on the internet shortly after. \"Eating People\" appeared in Malaysia's \"Perdan Weekly\" without caption and generated the question of whether eating babies was accepted in Asia on various myth-debunking websites.\n\nPocket Theology: Appearing in the 1999 group exhibition \"Post-Sense Sensibility- Alien Bodies & Delusion\" in Beijing, curated by Wu Meichun and Qiu Zhije. A long coiling rope was gripped by a severed, decomposing arm which was suspended by a meat hook. This display was held in a small room in the basement that was being rented by a group of Chinese artists who organize the exhibition. Viewers were forced to walk over the rope which filled the entire space.\n\nSkin Graft: This performance art installation appeared in the 2000 exhibition \"Infatuation with Injury\" organized by Li Xianting. In the exhibit, Zhu uses his own flesh as a canvas. Photos were shown of trunk of a quartered pig lying on a hospital bed. Zhu grafted a piece of his own skin onto a section of damaged skin from the pig. Two photos of this process appeared in the exhibit; one that showed the surgical process, and another which featured the artist sewing his own skin onto the pig carcass. Zhu stood by the exhibit and lifted his shirt to show the scar which stood as evidence of the procedure.\n\nLeftover: This series was exhibited by the Xin Beijing Art Gallery at the China International Gallery Exposition. Zhu photographed plates that held bits of leftover food, and then painted those images on canvas with oil. Eight paintings appeared at the Xin Beijing Art Gallery.\n\nZhu Yu’s most recent works follow his ideas with the Leftover exhibit, in which he paints highly detailed portraits of mundane objects. His series “Stain,” features a bird’s-eye view of teacups that contain the dregs of tea leaves. The next series, “Pebble,” appeared at Zhu’s solo exhibition \"Play Thing\", at the Long March Space in Beijing, 2010. This is another series of highly detailed, realistic paintings that show individual pebbles, each featuring a slightly different hue or shape. This work implies that all life can be reduced to a pebble, a simple object from which much meaning can be derived.\n\n\"Long March Space- Beijing@Sh Contemporary 2011\": September 2011, Shanghai Exhibition Center, Shanghai\n\n\"“Top Events” 3rd Session- Poster Exhibition\": September–October 2011, TOP Contemporary Art Center, Shanghai\n\n\"Long March Space@Art Beijing 2011 Contemporary Art Fair\": April–May 2011, National Agriculture Exhibition Center, Beijing\n\n\"Long March Space@ShContemporary 2010 Shanghai Art Fait International\": September 2010, Shanghai Exhibition Center, Shanghai\n\n\"Discoveries\": Re-Value@ShContemporary 2010 Shanghai Art Fair International: September 2010, Shanghai Exhibition Center, Shanghai\n\n\"Great Performance\": August- October 2010, Pace Beijing, Beijing\n\n\"Play Thing\"(Solo): April–May 2010, Long March Project, Beijing\n\n\"Jungle\": A Close-up Focus on Chinese Contemporary Art Trends: March–May 2010, Platform Chinga Contemporary Art Institute Space A, Beijing\n\n\"Contemporary Art Exhibition in Songjiang\": September 2009, Shanghai Songjiang Creative Studio, Shanghai\n\n\"Blackboard\": May–June 2009, ShanghART Gallery, Shanghai\n\n\"Xin Beijing Art Gallery@ShContemporary 08\": September 2008, Shanghai Exhibition Center\n\n\"“Insomnia” Photographs Exhibition\": September 2008, BizArt Center, Shanghai\n\n\"The Third Guanzhou Triennial\": September–November 2008, Guangdong Museum of Art, Guangzhou\n\n\"Portraying Food\": June–July 2008, Walsh Gallery, Chicago\n\n\"Illegal Construction II\": March–May 2008, Long March Project, Beijing\n\n\"Retrospective Exhibition I\": January 2008, Xin Beijing Art Gallery, Beijing\n\n\"Exit/Entrance\": September–October 2007, Xin Beijing Art Gallery, Beijing\n\n\"Xin Beijing Art Gallery@ShContemporary 07\": September 2007, Shanghai Exhibition Center, Shanghai\n\n\"Leftover\" (Solo): August 2007, Xin Beijing Art Gallery, Beijing\n\n\"NONO\": April–June 2007, Long March Project, Beijing\n\n\"It’s All Right\": December 2006, BizArt Center, Shanghai\n\n\"One Project Composed of 100 Projects\" (Solo): May 2006, BizArt Center, Shanghai\n\n\"Conspire\": November–January 2005/2006, TS1 Gallery, Beijing\n\n\"Internal Injuries Part 1\": July–September 2005, Primo Marella Gallery, Beijing\n\n\"Dial 62721232\": 2004, BizArt Center, Beijing\n\n\"Nasty\": October 2003, BizArt Center, Beijing\n\n\"Mushroom, Or Utopia\": November–December 2002, The Bund Museum, Shanghai\n\n\"Fan Mingzhen and Fan Mingzhen\": November 2002, BizArt Center, Beijing\n\n\"Fuck Off\": November 2000, Shanghai Eastlink Gallery, Shanghai\n\n\"It’s All Right\": January 1998-December 2006, BizArt Center, Beijing\n\n\n"}
