{"id": "22565743", "url": "https://en.wikipedia.org/wiki?curid=22565743", "title": "1998 California Proposition 227", "text": "1998 California Proposition 227\n\nProposition 227 was a California ballot proposition passed on the June 2, 1998, ballot. Proposition 227 was repealed by Proposition 58 on November 8, 2016.\n\nAccording to Ballotpedia, \"Proposition 227 changed the way that \"Limited English Proficient\" (LEP) students are taught in California. Specifically, it\n\nThe bill's intention was to educate Limited English proficiency students in a rapid, one-year program. It was sponsored by Ron Unz, the runner-up candidate in the 1994 Republican gubernatorial primary. The proposition was controversial because of its close proximity to heated political issues including race, immigration, and poverty. The methods of education enacted by the proposition reflected the electorate's support of assimilation over multiculturalism. It passed with a margin of 61% to 39%.\n\nOn September 28, 2014, the state legislature passed, and Governor Jerry Brown signed, Senate Bill 1174, which added Proposition 58 to the November 2016 ballot. Proposition 58, which repealed most of Proposition 227, passed by a wide margin. \n\nThe number of bilingual credentials fell after Proposition 227's passage. A California Department of Education spokesperson anticipated a shortage of bilingual teachers after the passage of Proposition 58.\n\n"}
{"id": "1043494", "url": "https://en.wikipedia.org/wiki?curid=1043494", "title": "Academic honor code", "text": "Academic honor code\n\nAn academic honor code or honor system is a set of rules or ethical principles governing an academic community based on ideals that define what constitutes honorable behaviour within that community. The use of an honor code depends on the notion that people (at least within the community) can be trusted to act honorably. Those who are in violation of the honor code can be subject to various sanctions, including expulsion from the institution. Honor codes are used to deter academic dishonesty and should be taken seriously at all times.\n\nThe documented history of an academic honor code dates back to 1736 at The College of William and Mary and is the oldest honor code in the United States. Still a tradition, students administer the honor pledge to each incoming student and educate faculty and administration on the relevance of the Code and its application to students' lives at the College. Students administer the Code through six Honor Councils and the Council of Chairs. The College of William and Mary founded the Phi Beta Kappa academic honor society in 1776 and was the first school of higher education in the United States to install an honor code of conduct for students. It is based upon the premise that honor is one's most cherished attribute.\n\nIn a community devoted to learning, a foundation of honor among individuals must exist if that community is to thrive with respect and harmony among its members. An honor system is an ideal mechanism to ensure such a state of affairs. With it, students and faculty are afforded a freedom that otherwise may not be available. With that freedom comes each individual's responsibility to conduct oneself in such a way that the spirit of mutual trust which sustains the system is not compromised.\n\nThe Honor Code applies to alleged acts of lying, stealing or cheating that adversely affect the College community committed by a student on campus or elsewhere.\n\nPresently, some of the most notable and most stringent honor codes exist at the United States Military Academy (see Cadet Honor Code), Virginia Military Institute, the United States Air Force Academy, the United States Merchant Marine Academy, and the United States Coast Guard Academy. The United States Naval Academy has an \"Honor Concept,\" which is similar in scope to the honor codes at the other academies.\n\nThe military academy honor codes not only govern the cadets' and midshipmen's lives at the academies but also are deemed essential to the development of military officers who are worthy of the public trust. As such, the codes are not limited merely to academic situations or to conduct on campus; cadets and midshipmen are expected to live by the codes' ethical standards at all times. The codes are as old as the academies themselves and simply state that cadets and midshipmen do not lie, cheat or steal, nor tolerate those who do. A single-sanction Honor Code, in which any offense results in expulsion regardless of severity, exists at Virginia Military Institute, which features a \"drum out\" ceremony which is still carried out upon a cadet's dismissal. Outside of the military, Washington and Lee University and the University of Virginia also have single sanction codes.\n\nAt three of the service academies and at Virginia Military Institute, anyone who learns of an honor code violation is required to report it. Failure to do so is considered \"toleration,\" which itself violates the code. That also holds true at schools with combined cadet and traditional student programs, such as Norwich University, Texas A&M, and The Citadel, whose honor codes specifically provide that all students, both cadets and civilians, do not \"tolerate those who do.\" It is notable that the three Senior Military Colleges have two honor codes, one for cadets and one for civilians, whether on-campus or through distance online programs, etc. The Honor Concept of the Brigade of Midshipmen at the United States Naval Academy allows the observer of an honor violation to confront the accused without formally reporting. It was found that it was more constructive at developing the honor of midshipmen. A non-toleration clause, on the other hand, is believed to make enemies of classmates. Additionally, it is thought that one's true honor, if other than utmost, was not able to be formally remediated when hidden from public view. Under the academies' honor codes, violators can face severe punishment, up to being forwarded for expulsion by the Secretary of the Army, Navy, or Air Force.\n\nStringent honor codes, however, are not limited to military institutions. The all-male Hampden–Sydney College is reputed for an honor code system on a par with military systems, which extends to all student activities both on and off campus (off-campus violations can be prosecuted), and also like the military system, it considers tolerance of a violation itself a violation. Like the Naval Academy, however, those who witness a violation are encouraged to confront the violator and convince them to turn themselves in before resorting to reporting the violation. Another school with a very strict honor code is Brigham Young University. The university not only mandates honest behavior but also incorporates various aspects of Mormon religious law: drinking, smoking, drug use, and premarital sex are all banned. Also, the code includes standards for dress and grooming. Men must be clean-shaven, and men and women cannot wear short shorts or other revealing clothing.\n\nMany military academies have strict Honor systems, such as the following:\n\nIn addition, the following colleges with a Corps of Cadets maintain an honor code for both the cadets and civilian students:\nNorwich University\nTexas A&M University\nVirginia Tech \nThe Citadel\n\nThere are also traditional liberal arts and technical universities that maintain Honor systems:\n\nThere are differences between the honor codes of different universities. \n\n\n"}
{"id": "1957", "url": "https://en.wikipedia.org/wiki?curid=1957", "title": "Alexander Technique", "text": "Alexander Technique\n\nThe Alexander Technique, named after its creator Frederick Matthias Alexander, is an educational process that was created to retrain habitual patterns of movement and posture. Alexander believed that poor habits in posture and movement damaged spatial self-awareness as well as health, and that movement efficiency could support overall physical well-being. He saw the technique as a mental training technique as well.\n\nAlexander began developing his technique's principles in the 1890s in an attempt to address voice loss during public speaking. He credited his method with allowing him to pursue his passion for reciting in Shakespearean theater.\n\nSome proponents of the Alexander Technique say that it addresses a variety of health conditions related to cumulative physical behaviors but there is little evidence to support these claims. As of 2015 there was evidence suggesting the Alexander Technique may be helpful for long-term back pain, long-term neck pain, and may help people cope with Parkinson's disease.\n\nFrederick Matthias Alexander (1869–1955) was a Shakespearean orator from Tasmania, who developed voice loss during his unamplified performances. After doctors found no physical cause, Alexander reasoned that he was inadvertently damaging himself while speaking. He observed himself in multiple mirrors and saw that he was contracting his posture in preparation for any speech. He hypothesized that a habitual conditioned pattern (of pulling his head backwards and downwards) needlessly was disrupting the normal working of his total postural, breathing, and vocal processes.\n\nWith experimentation, Alexander developed the ability to stop the unnecessary and habitual contracting in his neck, displacement of his head, and shortening of his stature. As he became practised at speaking without these interferences, he found that his problem with recurrent voice loss was resolved. While on a recital tour in New Zealand (1895), he came to believe in the wider significance of improved carriage for overall physical functioning although evidence from his own publications appears to indicate it happened less systematically and over a long period of time.\n\nThe American philosopher and educator John Dewey became impressed with the Alexander Technique after his headaches, neck pains, blurred vision, and stress symptoms largely improved during the time he used Alexander's advice to change his posture. In 1923, Dewey wrote the introduction to Alexander's \"Constructive Conscious Control of the Individual\".\n\nAldous Huxley had transformative lessons with Alexander, and continued doing so with other teachers after moving to the US. He rated Alexander's work highly enough to base the character of the doctor who saves the protagonist in 'Eyeless in Gaza' (an experimental form of autobiographical work) on F.M. Alexander, putting many of his phrases into the character's mouth. Huxley's work 'The Art of Seeing' also discusses his views on the technique.\n\nSir Stafford Cripps, George Bernard Shaw, Henry Irving and other stage grandees, Lord Lytton and other eminent people of the era also wrote positive appreciations of his work after taking lessons with Alexander.\n\nSince Alexander's work in the field came at the start of the 20th century, his ideas influenced many originators in the field of mind-body improvement. Fritz Perls, who originated Gestalt therapy, credited Alexander as an inspiration for his psychological work. The Mitzvah Technique was influenced by the Alexander Technique; as was the Feldenkrais Method - who expanded on the one exercise in Alexander Technique called \"The Whispered Ah.\"\n\nAlexander's approach emphasizes awareness strategies applied to conducting oneself while in action, (which could be now called 'mindful' action, though in his four books he did not use that term.)\n\nActions such as sitting, squatting, lunging or walking are often selected by the teacher. Other actions may be selected by the student that is tailored to their interests or work activities: hobbies, computer use, lifting, driving or artistic performance or practice, sports, speech or horseback riding. Alexander teachers often use themselves as examples. They demonstrate, explain, and analyze a student's moment-to-moment responses as well as using mirrors, video feedback or classmate observations. Guided modelling with a highly skilled light hand contact is the primary tool for detecting and guiding the student into a more coordinated state in movement and at rest during in-person lessons. Suggestions for improvements are often student-specific, as everyone starts out with slightly different habits.\n\nExercise as a teaching tool is deliberately omitted because of a common mistaken assumption that there exists a \"correct\" position. There are only two specific procedures that are practiced by the student; the first is lying semi-supine. Resting in this way uses \"mechanical advantage\" as a means of redirecting long-term and short-term accumulated muscular tension into a more integrated and balanced state. This position is sometimes referred to as \"constructive rest\", or \"the balanced resting state\". It's also a specific time to practice Alexander's principle of conscious \"directing\" without \"doing\". The second exercise is the \"Whispered Ah\", which is used to co-ordinate freer breathing and vocal production.\n\nFreedom, efficiency and patience are the prescribed values. Proscribed are unnecessary effort, self-limiting habits as well as mistaken perceptual conclusions about the nature of training and experimentation. Students are led to change their largely automatic routines that are interpreted by the teacher to currently or cumulatively be physically limiting, inefficient, or not in keeping with best \"use\" of themselves as a whole. The Alexander teacher provides verbal coaching while monitoring, guiding and preventing unnecessary habits at their source with a specialized hands-on assistance.\n\nThis specialized hands-on skill also allows Alexander teachers to bring about a balanced working of the student's supportive musculature as it relates to gravity's downward pull from moment to moment. Often, students require a great deal of hands-on work in order to first gain an experience of a fully poised relation to gravity and themselves. The hands-on skill requires Alexander teachers to maintain in themselves from moment-to-moment their own improved psycho-physical co-ordination that the teacher is communicating to the student.\n\nAlexander developed terminology to describe his methods, outlined in his four books that explain the experience of learning and substituting new improvements.\n\n\n\n\n\n\"Directing\" serves to counteract the common backward and downward pull and shortening in stature that can be detected at the beginning of every movement - particularly addressing a startle pattern of \"fight, flight or freeze.\" A mere thought, as a projection of intention, shapes preparatory movement below the level of sensing it. Alexander used these words for reshaping these subliminal preparations: \"The neck to be free, the head to go forward and up, the back to lengthen and widen.\" Some teachers have shortened this to a suggestion of, \"Freer?\" Negative directions (that use Alexander's other preventive principle of \"inhibition\") have also been found to be effective, because negative directions leave the positive response open-ended.\nWhichever is used, all \"Directing\" is suggestively thought, (rather than willfully accomplished.) This is because the neuro-muscular responses to \"Directing\" often occur underneath one's ability to perceive how they are actually carried out neuro-physiologically and neuro-cognitively. As freedom of expression or movement is the objective, the most appropriate responses cannot be anticipated or expected, only observed and chosen in the moment. \nTeacher trainees gradually learn to include a constant attending to their lengthening in stature in every movement. It becomes a basis for initiating and continuing every action, every response to stimuli or while remaining constructively at rest.\n\n\nThe Alexander Technique is used and taught by classically trained vocal coaches and musicians in schools and private lessons. Its advocates state that it allows for a balanced use of all aspects of the vocal tract by consciously increasing air-flow, allowing improved vocal skill and tone. The method is said by actors to reduce stage fright and to increase spontaneity.\n\nThe Alexander Technique is a frequent component in acting training, because it can assist the actor in being more natural in performance.\n\nAccording to Alexander Technique instructor Michael J. Gelb, people tend to study the Alexander Technique for reasons of personal development.\n\nThe Alexander Technique is most commonly taught privately in a series of 10 to 40 private lessons which may last from 30 minutes to an hour. Students are often performers, such as actors, dancers, musicians, athletes and public speakers, people who work on computers, or those who are in frequent pain for other reasons. Instructors observe their students, then show them how to move with better poise and less strain. Sessions include chair work - often in front of a mirror, during which the instructor and the student will stand, sit and lie down, moving efficiently while maintaining a comfortable relationship between the head, neck and spine, and table work or physical manipulation.\n\nTo qualify as a teacher of Alexander Technique, instructors are required to complete 1,600 hours, spanning three years, of supervised teacher training. The result must be satisfactory to qualified peers to gain membership in professional societies.\n\nSome advocates for the Alexander technique have claimed it can help people with many kinds of health conditions.\n\nOne example is a claim that it can benefit people with asthma. A 2012 Cochrane systematic review found that there is no conclusive evidence that the Alexander technique is effective for treating asthma, and randomized clinical trials are needed in order to assess the effectiveness of this type of treatment approach.\n\nA review by Aetna last updated in 2016 stated: \"Aetna considers the following alternative medicine interventions experimental and investigational, because there is inadequate evidence in the peer-reviewed published medical literature of their effectiveness.\" Included is Alexander technique in that list.\n\nA review published in 2015 and conducted for the Australia Department of Health in order to determine what services the Australian government should pay for, reviewed clinical trials published to date and found that: \"Overall, the evidence was limited by the small number of participants in the intervention arms, wide confidence intervals or a lack of replication of results.\" It concluded that: \"The Alexander technique may improve short-term pain and disability in people with low back pain, but the longer-term effects remain uncertain. For all other clinical conditions, the effectiveness of Alexander technique was deemed to be uncertain, due to insufficient evidence.\" It also noted that: \"Evidence for the safety of\nAlexander technique was lacking, with most trials not reporting on this outcome. Subsequently in 2017 the Australian government named the Alexander Technique as a practice that would not qualify for insurance subsidy, saying this step would \"ensure taxpayer funds are expended appropriately and not directed to therapies lacking evidence\".\n\nA review of evidence for Alexander Technique for various health conditions provided by UK NHS Choices last updated in July 2015 found that:\nThere's evidence suggesting the Alexander technique can help people with:\n\n\"NHS Choices\" also states that \"some research has also suggested the Alexander technique may improve general long-term pain, stammering and balance skills in elderly people to help them avoid falls. But the evidence in these areas is limited and more studies are needed. There's currently little evidence to suggest the Alexander technique can help improve other health conditions, including asthma, headaches, osteoarthritis, difficulty sleeping (insomnia) and stress.\"\n\nA review published in \"BMC Complementary and Alternative Medicine\" in 2014 focused on \"the evidence for the effectiveness of AT sessions on musicians' performance, anxiety, respiratory function and posture\" concluded that: \"Evidence from RCTs and CTs suggests that AT sessions may improve performance anxiety in musicians. Effects on music performance, respiratory function and posture yet remain inconclusive.\"\n\nA review published in the \"International Journal of Clinical Practice\" in 2012 found: \"Strong evidence exists for the effectiveness of Alexander Technique lessons for chronic back pain and moderate evidence in Parkinson’s-associated disability. Preliminary evidence suggests that Alexander Technique lessons may lead to improvements in balance skills in the elderly, in general chronic pain, posture, respiratory function and stuttering, but there is insufficient evidence to support recommendations in these areas.\"\n\n\n"}
{"id": "5145147", "url": "https://en.wikipedia.org/wiki?curid=5145147", "title": "Atmospheric sounding", "text": "Atmospheric sounding\n\nAn atmospheric sounding is a measurement of vertical distribution of physical properties of the atmospheric column such as pressure, temperature, wind speed and wind direction (thus deriving wind shear), liquid water content, ozone concentration, pollution, and other properties. Such measurements are performed in a variety of ways including remote sensing and in situ observations.\n\nThe most common in situ sounding is a radiosonde, which usually is a weather balloon, but can also be a rocketsonde.\n\nRemote sensing soundings generally use passive infrared and microwave radiometers:\n\nSensors that measure atmospheric constituents directly, such as thermometers, barometers, and humidity sensors, can be sent aloft on balloons, rockets or dropsondes. They can also be carried on the outer hulls of ships and aircraft or even mounted on towers. In this case, all that is needed to capture the measurements are storage devices and/or transponders.\n\nThe more challenging case involves sensors, primarily satellite-mounted, such as radiometers, optical sensors, radar, lidar and ceilometer as well as sodar since these cannot measure the quantity of interest, such as temperature, pressure, humidity etc., directly. By understanding emission and absorption processes, we can figure out what the instrument is looking at between the layers of atmosphere.\nWhile this type of instrument can also be operated from ground stations or vehicles—optical methods can also be used inside in situ instruments—satellite instruments are particularly important because of their extensive, regular coverage. The AMSU instruments on three NOAA and two EUMETSAT satellites, for instance, can sample the entire globe at better than one degree resolution in less than a day.\n\nWe can distinguish between two broad classes of sensor: \"active\", such as radar, that have their own source, and \"passive\" that only detect what is already there. There can be a variety of sources for a passive instrument, including scattered radiation, light emitted directly from the sun, moon or stars—both more appropriate in the visual or ultra-violet range—as well light emitted from warm objects, which is more appropriate in the microwave and infrared.\n\nA limb sounder looks at the edge of the atmosphere where it is visible above the Earth. It does this in one of two ways: either it tracks the sun, moon, a star, or another transmitting satellite through the limb as the source gets occultated behind the Earth, or it looks towards empty space, collecting radiation that is scattered from one of these sources. In contrast, a nadir-looking atmospheric sounder looks down through the atmosphere at the surface. The SCIAMACHY instrument operates in all three of these modes.\n\nThe following applies mainly to passive sensors, but has some applicability to active sensors.\n\nTypically, there is a vector of values of the quantity to be retrieved, formula_1, called the state vector and a vector of measurements, formula_2. The state vector could be temperatures, ozone number densities, humidities etc. The measurement vector is typically counts, radiances or brightness temperatures from a radiometer or similar detector but could include any other quantity germain to the problem.\nThe forward model maps the state vector to the measurement vector:\n\nUsually the mapping, formula_4, is known from physical first principles, but this may not always be the case. Instead, it may only be known empirically, by matching actual measurements with actual states. Satellite and many other remote sensing instruments do not measure the relevant physical properties, that is the state, but rather the amount of radiation emitted in a particular direction, at a particular frequency. It is usually easy to go from the state space to the measurement space—for instance with Beer's law or radiative transfer—but not the other way around, therefore we need some method of inverting formula_4 or of finding the inverse model, formula_6.\n\nIf the problem is linear we can use some type of matrix inverse method—often the problem is ill-posed or unstable so we will need to regularize it: good, simple methods include the normal equation or singular value decomposition. If the problem is weakly nonlinear, an iterative method such Newton-Raphson may be appropriate.\n\nSometimes the physics is too complicated to model accurately or the forward model too slow to be used effectively in the inverse method. In this case, statistical or machine learning methods such as linear regression, neural networks, statistical classification, kernel estimation, etc. can be used to form an inverse model based on a collection of ordered pairs of samples mapping the state space to the measurement space, that is, formula_7. These can be generated either from models—e.g. state vectors from dynamical models and measurement vectors from radiative transfer or similar forward models—or from direct, empirical measurement. Other times when a statistical method might be more appropriate include highly nonlinear problems.\n\n\n\n\n"}
{"id": "2339273", "url": "https://en.wikipedia.org/wiki?curid=2339273", "title": "Best available technology", "text": "Best available technology\n\nIn pollution abatement, the best available technology (or BAT) is the technology approved for limiting pollutant discharges with regard to an abatement strategy. Similar terms are \"best available techniques\", \"best practicable means\" or \"best practicable environmental option\". BAT is a moving target on practices, since developing societal values and advancing techniques may change what is currently regarded as \"reasonably achievable\", \"best practicable\" and \"best available\".\n\nA literal understanding will connect it with a \"spare no expense\" doctrine which prescribes the acquisition of the best state of the art technology available, without regard for traditional cost-benefit analysis. In practical use, the cost aspect is also taken into account. [See also discussions on the topic of the precautionary principle which, along with considerations of \"best available technologies\" and \"cost-benefit analyses\", is also involved in discussions leading to formulation of environmental policies and regulations (or opposition to same).]\n\n\"Best practicable means\" was used for the first time in UK national primary legislation in section 5 of the Salmon Fishery Act 1861 and another early use was found in the Alkali Act Amendment Act 1874, but before that appeared in the Leeds Act of 1848.\n\nThe \"BAT\" concept was first time used in the 1992 OSPAR Convention for the protection of the marine environment of the North-East Atlantic for all types of industrial installations (for instance, chemical plants).\n\nSome doctrine deem it already acquired the status of customary law.\n\nIn the United States, BAT or similar terminology is used in the Clean Air Act and Clean Water Act.\n\n\"Best available techniques not entailing excessive costs (BATNEEC)\", sometimes referred to as \"best available technology\", was introduced in 1984 with Directive 84/360/EEC and applied to air pollution emissions from large industrial installations.\n\nIn 1996, Directive 84/360/EEC was superseded by the Integrated pollution prevention and control directive (IPPC), 96/61/EC, which applied the framework concept of \"Best Available Techniques\" (BAT) to the integrated control of pollution to the three media air, water and soil. The concept is also part of the directive's recast in 2008 (2008/1/EC) and its successor directive, the Industrial Emissions Directive 2010/75/EU published in 2010.\n\nAccording to article 15(2) of the Industrial Emissions Directive, emission limit values and the equivalent parameters and technical measures in permits shall be based on the best available techniques, without prescribing the use of any technique or specific technology.\n\nThe directive includes a definition of best available techniques in article 3(10):\n\n\"best available techniques\" means the most effective and advanced stage in the development of activities and their methods of operation which indicates the practical suitability of particular techniques for providing the basis for emission limit values and other permit conditions designed to prevent and, where that is not practicable, to reduce emissions and the impact on the environment as a whole:\n\nBAT for a given industrial sector are described in BAT reference documents (BREFs) as defined in article 3(11) of the Industrial Emissions Directive. BREFs are the result of an exchange of information between European Union Member States, the industries concerned, non-governmental organisations promoting environmental protection and the European Commission pursuant to article 13 of the directive. This exchange of information is often called the Sevilla process because it is steered by the Institute for Prospective Technological Studies of the European Commissions' Joint Research Centre, which is based in Seville. The process is described in detail in Commission Implementing Decision 2012/119/EU. The most important chapter of the BREFs, the BAT conclusions, are published as implementing decisions of the European Commission in the Official Journal of the European Union. According to article 14(3) of the Industrial Emissions Directive, the BAT conclusions shall be the reference for setting permit conditions of large industrial installations.\n\nThe Clean Air Act requires that certain facilities employ Best Available Control Technology to control emissions.\n\nThe Clean Water Act (CWA) requires issuance of national industrial wastewater discharge regulations (called \"effluent guidelines\"), which are based on BAT and several related standards.\n\nIn the development of the effluent standards, the BAT concept is a \"model\" technology rather than a specific regulatory requirement. The U.S. Environmental Protection Agency (EPA) identifies a particular model technology for an industry, and then writes a regulatory performance standard based on the model. The performance standard is typically expressed as a numeric effluent limit measured at the discharge point. The industrial facility may use any technology that meets the performance standard.\n\nA related CWA provision for cooling water intake structures requires standards based on \"best technology available.\"\n\nThe concept of BAT is also used in a number of international conventions such as the Minamata Convention on Mercury, the Stockholm Convention on Persistent Organic Pollutants, or the OSPAR Convention for the protection of the marine environment of the North-East Atlantic.\n\n\n"}
{"id": "19972507", "url": "https://en.wikipedia.org/wiki?curid=19972507", "title": "Blažej Baláž", "text": "Blažej Baláž\n\nBlažej Baláž (born 29 October 1958 in Nevoľné, Slovakia, former Czechoslovakia) is a contemporary Slovak artist. His practise as an artist is usually associated with political art, environmental, activist, mail-art and neo-conceptualism. After 1988 he began working with text as art, neo-conceptual and post-conceptual texts (intext, outtext).\n\nBaláž studied at the Academy of Fine Arts, Bratislava and his Magister of Fine Arts degree received in 1983. Baláž has been the Head of Department of Fine Arts Education at the University of Trnava since 1999. He was a founder member of the artists group East of Eden (1998). Since 1979, he has been married to the artist Maria Balazova.\nHe lives and works in Trnava.\n\nHe has worked in the areas of political, environmental, activist and neo-conceptual art. His practice also comprises media painting, works on paper, performances, drawing, object, mail art and printmaking.\n\nHe has shown work internationally in exhibitions including the Drawing 1990, Provo, USA (1990), Vth International Drawing Triennale, Wrocław (1992), 12th International Biennale of Small Sculpture, Murska Sobota (1995), International Biennale of Graphic Arts, Ljubljana (1989, 1995, 1999), Object / Object. Prague, Czech Museum of Fine Arts (2001), Intertext / From the conceptual to postconceptual text, Ján Koniarek Gallery, Trnava (2009), Formats of Transformation 89 – 09 / Seven views on the new Czech and Slovak identity, House of Art, Brno (2009).\n\nSolo exhibitions (retrospective): Geld macht Kunst, Ján Koniarek Gallery, Trnava (2003) , Blažej Baláž Texts 1988/2007, The East Slovak Gallery, Košice (2007), Museum of Fine Arts, Žilina (2007), SUCHARATOLEST House of Art, Bratislava (2009), WARTEZEIT, Slowakisches Institut Wien (2009), Post-Geo-Text (with Mária Balážová), Slowakisches Institut Berlin (2011).\n\nHis works are held in the public collections of Slovak National Gallery, Bratislava, National Gallery in Prague (CZ), Muzeum Archidiecezjalne, Katowice (PL), Fries Museum, Leeuwarden (NL), Jan Koniarek Gallery, Trnava , East Slovakian Gallery Košice, City Gallery , Bratislava, Central Slovakian Gallery, Banská Bystrica .\n\nBaláž's works brings the new possibilities at the field of post-conceptual and neo-conceptual text. The exhibition is divided in three segments, all of them are connected by the specific and authentic search of the possibilities in the conceptual Macaronic language.\nThe first is created from the neo-conceptual horizontal/vertical texts, a part of Treptomachia, (WARTEZEIT, 2006, WARTERAUM, 2006 and How to domesticate the English language, 2006). The internal orientation of artworks, the search of art itself, the search of language and text brings the cracking of syntactic/semantic wholeness of graphems. The metatexts of initial prototext are distinguished by the colors (oil on canvas, 300 x 200 cm).\nIn this regard Baláž's eff ort culminates by the diptych TREPTOMACHIA.EN-A / How to domesticate the English language and TREPTOMACHIA.DE-B / How to domesticate the German language (2006). Both are depicted on big-size canvas, within which the only dominating word with partial messages is replaced with newly defined optical language with tens of words. The colour decomposition of the grapheme creates a symptomatic background for language overlapping and the contamination, which creates the supreme moment of the author's Babylonian crossroads.\nAnother, the second segment, black and white acryl-paintings enriches the former one, the artworks are created on principle of the structuralist analysis of text. It is a variant, which is called and named by author and curator the intext. By the segmentation of text the author discovers several lexis. These have a different lexical origin, five West-European, five East-European, and „dead“ language of Medieval Esperanto – Latin. Despite the cold artificiality of works, this segment has also the hidden sociocritical context, which is oscillated between phenomenons West and East, art and politics.\nThe third segment is the most open and subjective. These horizontal texts are named by its author as the simultaneous texts. It is a different alternative in the post-conceptual Makaronian text, the melange is created by harmony of the initial and derivative text. Its ironic and attacking character replaces a cold aspect of the first two segments. Vajanský's Suchá ratolesť (an ironic parallel of failing elits) is floundered by streams, the subject is named by the author as east rat in the age of 20-years of wandering a la democrazy, with a more complicated ambivalent reading. The fragility of ground – paper [four artworks, width 10 m] raises the openness of statement, ODRBMADEMOKRATICKY, 2008 [screw me over democratically], is predominative.\n\n\n\n\n\n\n\n\n"}
{"id": "25350393", "url": "https://en.wikipedia.org/wiki?curid=25350393", "title": "Busybody", "text": "Busybody\n\nA busybody, do-gooder, meddler, or marplot is someone who meddles in the affairs of others.\n\nAn early study of the type was made by the ancient Greek philosopher Theophrastus in his typology, \"Characters\", \"In the proffered services of the busybody there is much of the affectation of kind-heartedness, and little efficient aid.\"\n\nSusanna Centlivre wrote a successful play, \"The Busie Body\", which was first performed in 1709 and has been revived repeatedly since. It is a farce in which Marplot interferes in the romantic affairs of his friends and, despite being well-meaning, frustrates them. The characterisation of Marplot as a busybody whose \"chief pleasure is knowing everybody's business\" was so popular that they appeared as the title character in a sequel, \"Marplot\". The name is a pun — mar / plot — and passed into the language as an eponym or personification of this type.\n\nIn English law, the doctrine of \"locus standi\" requires that a plaintiff have some connection with the matter being contested. In two cases in 1957 and 1996, Lord Denning ruled that \"The court will not listen to a busybody who is interfering in things which do not concern him...\" Similarly, there is a long-standing rule that a person must have an insurable interest in a property or person that they wish to insure.\n\n"}
{"id": "844785", "url": "https://en.wikipedia.org/wiki?curid=844785", "title": "Car donation", "text": "Car donation\n\nCar donation is the practice of giving away no-longer-wanted automobiles or other vehicles to charitable organizations. In the United States, these donations can provide a tax benefit.\n\nSome critics have claimed that car donations are essentially a tax shelter. However, non-profit organizations in the US have come to rely increasingly upon the revenue from car donations. This type of donation has become increasingly widespread; in 2000, 733,000 U.S. taxpayers reduced their taxes by $654 million.\n\nAlthough advertised as an easy way to dispose of an old car, donors need to fulfill certain post-donation requirements to qualify for the tax deduction, such as obtaining a written acknowledgment of the car's subsequent sale by the charity, and itemizing tax returns instead of taking the standard deduction.\n\nFor vehicles valued at less than $500, the deduction amount comes from the donor's own estimate of the car's value, even if the charity receives less money from its sale. Deductions greater than $500 are limited to the proceeds of selling the vehicle, usually at auction. The U.S Internal Revenue Service advises that starting in 2005:\n\nCar donation schemes in the UK are slightly different from those operating in the United States and only established themselves as a valued source of income for UK charities in January 2010, led by Giveacar – a non-profit organisation. Operating as a non-profit organisation allows charities to avoid the large overheads created by profit-making car donation companies. In addition, whereas car donation in the US has been incentivised through tax breaks, in the UK there are no such tax benefits to donating your car.\n\nVehicle donations in America are operated in a wide variety of plans, ranging from highly organized and professional-grade not-for-profit, national, or local charities to scrap yards, haulers, tow-truck companies and salvagers who establish programs that may support a charity. According to Charity Navigator, the guidance of the rating agencies concerning car donation programs, where the charity receives a flat fee for the use of their name by a third party, versus program management by a third party,there are some questionable companies who contract to use a nonprofit's agencies name and logo to raise funds and then just give them a flat fee unrelated to income or performance. This is frowned upon by rating agencies and the government. However, a percentage return program is viewed positively if the nonprofit receives more than 50% of the generated income. Programs that engage a third party, but with a \"cost maximum cap\" involved, such as with national charities like the Society of Saint Vincent de Paul, performance in this area is exemplary, with 70-80% on average being returned to the nonprofit.\n\nMany charities run donation programs which accept car donations, such as Goodwill, Salvation Army and even the American Cancer Society. Many charities will use your car donation directly to transport volunteers and supplies to areas that need help. Some even have their own car lots which sell the donated cars but many have their donations processed through auto auction companies. Many processing companies also collect and sell donated cars and distribute the money to a charity the donor indicates. The processing company typically takes a percentage of the sale value of the car, but these programs allow charities without their own facilities or staff dedicated to fund raising to benefit from vehicle donation programs.\n\nIdeally, donors should also investigate how much money from the sale of the car goes to the auction processor and how much actually benefits the charity's programs, as opposed to its administrative overhead.\n"}
{"id": "3340637", "url": "https://en.wikipedia.org/wiki?curid=3340637", "title": "Clinamen", "text": "Clinamen\n\nClinamen (; plural \"clinamina\", derived from \"clīnāre\", to incline) is the Latin name Lucretius gave to the unpredictable swerve of atoms, in order to defend the atomistic doctrine of Epicurus. In modern English it has come more generally to mean an inclination or a bias.\n\nAccording to Lucretius, the unpredictable swerve occurs \"at no fixed place or time\":\nWhen atoms move straight down through the void by their own weight, they deflect a bit in space at a quite uncertain time and in uncertain places, just enough that you could say that their motion has changed. But if they were not in the habit of swerving, they would all fall straight down through the depths of the void, like drops of rain, and no collision would occur, nor would any blow be produced among the atoms. In that case, nature would never have produced anything.\n\nThis swerving, according to Lucretius, provides the \"free will which living things throughout the world have\". Lucretius never gives the primary cause of the deflections. \n\nIn modern English clinamen is defined as an inclination or a bias. It implies that one is inclined or biased towards introducing a plausible but unprovable \"clinamen\" when a specific mechanism cannot be found to refute a credible argument against one's hypothesis or theory. The OED gives its first recorded use in English by Jonathan Swift in his 1704 \"Tale of a Tub\" ix.166, satirizing the atomistic theory of Epicurus:\nEpicurus modestly hoped that one time or other, a certain fortuitous concourse of all men's opinions—after perpetual justlings, the sharp with the smooth, the light and the heavy, the round and the square—would, by certain clinamina, unite in the notions of atoms and void, as these did in the originals of all things.\n\nThe term has been taken up by Harold Bloom to describe the inclinations of writers to \"swerve\" from the influence of their predecessors; it is the first of his \"Ratios of Revision\" as described in \"The Anxiety of Influence\".\n\nIn \"Difference and Repetition\", Gilles Deleuze employs the term in his description of \"multiplicities\". In addition, other French writers such as Simone de Beauvoir, Jacques Lacan, Jacques Derrida, Jean-Luc Nancy, Alain Badiou, and Michel Serres have made extensive use of the word 'clinamen' in their writings, albeit with very different meanings.\n\nLucretius' concept is central to the book \"\", written by Stephen Greenblatt.\n\n\"Clinamen\" is defined by Alfred Jarry in Chapter 33 of his \"Exploits and Opinions of Dr. Faustroll, Pataphysician\". The notion later figured in the imaginary science of the Jarry-inspired College of Pataphysics and the experimental literature of OuLiPo.\n\n"}
{"id": "10904266", "url": "https://en.wikipedia.org/wiki?curid=10904266", "title": "Coinduction", "text": "Coinduction\n\nIn computer science, coinduction is a technique for defining and proving properties of systems of concurrent interacting objects.\n\nCoinduction is the mathematical dual to structural induction. Coinductively defined types are known as codata and are typically infinite data structures, such as streams.\n\nAs a definition or specification, coinduction describes how an object may be \"observed\", \"broken down\" or \"destructed\" into simpler objects. As a proof technique, it may be used to show that an equation is satisfied by all possible implementations of such a specification.\n\nTo generate and manipulate codata, one typically uses corecursive functions, in conjunction with lazy evaluation. Informally, rather than defining a function by pattern-matching on each of the inductive constructors, one defines each of the \"destructors\" or \"observers\" over the function result.\n\nIn programming, co-logic programming (co-LP for brevity) \"is a natural generalization of logic programming and coinductive logic programming, which in turn generalizes other extensions of logic programming, such as infinite trees, lazy predicates, and concurrent communicating predicates. Co-LP has applications to rational trees, verifying infinitary properties, lazy evaluation, concurrent logic programming, model checking, bisimilarity proofs, etc.\" Experimental implementations of co-LP are available from The University of Texas at Dallas and in Logtalk (for examples see ) and SWI-Prolog.\n\n\n"}
{"id": "21427618", "url": "https://en.wikipedia.org/wiki?curid=21427618", "title": "Conservation communities", "text": "Conservation communities\n\nA conservation community is a group of individuals and families living in a community who are committed to saving large parcels of land from ecological degradation. This land can be forested land, agricultural land, ranch land, or any other type of land that needs protecting from high-impact development.\n\nConservation communities are models of sustainable community development, a new approach to development which provides alternatives to conventional forms of development. They are adaptable to the needs of different regions and they use small-scale residential development to fund conservation, eliminating the need to depend on funding from private donors or governments. This land development model is important to the environmental movements towards sustainable development, Green homebuilding, local food security, and responsible management of natural resources.\n\nSome of the tools used to create conservation communities are conservation covenants, ecoforestry covenants, and other forms of covenant registered to the title of the land. Covenants are a legal contract used to protect the integrity of land. Covenants help protect the ecological health of watersheds (which are damaged by increased development), maintain long-term access to natural resources and associated value-added opportunities, protect native plant and animal species, and prevent climate change impacts of intense development practices. Conservation communities are developed under a standard of performance that not only minimizes their ecological footprint on the Earth but also ensures that the development improves (or at the very least does not diminish) the existing ecological system’s performance.\n\nThe planning basis for the residential community focuses around maintaining and enhancing the ecological integrity of the land. The community, therefore, sits on the least sensitive part of the land from an ecological point of view, and is often built using low-impact infrastructure such as LEED-targeted housing.\n\nConservation communities can also offer the opportunity for other eco-compatible uses such as sustainable resource extraction, value-added manufacturing opportunities, organic horticulture, live/work enterprises, ecotourism, recreational and ecological educational opportunities.\n\nConservation communities can create sustainable employment opportunities for those living in the communities as well as the surrounding region. Ecotourism promotes environmental protection and support for the well-being of local community members by bringing visitors into the conservation community for educational and recreational purposes.\n\nConservation communities can be designed to strengthen the connections between neighbors and communities. Kim Davis, of the Vancouver Sun newspaper notes that \"while hardship, economic or otherwise, is not something most people desire, let alone seek, adversity does have a way of inspiring collaboration, creativity and innovation.\"\n"}
{"id": "219364", "url": "https://en.wikipedia.org/wiki?curid=219364", "title": "Constitutional convention (political custom)", "text": "Constitutional convention (political custom)\n\nA constitutional convention is an informal and uncodified procedural agreement that is followed by the institutions of a state. In some states, notably those Commonwealth of Nations states that follow the Westminster system and whose political systems derive from British constitutional law, most government functions are guided by constitutional convention rather than by a formal written constitution.\nIn these states, actual distribution of power may be markedly different from those the formal constitutional documents describe. In particular, the formal constitution often confers wide discretionary powers on the head of state that, in practice, are used only on the advice of the head of government.\n\nSome constitutional conventions operate separate from or alongside written constitutions, such as in Canada since the country was formed with the enactment of the Constitution Act, 1867. Others, notably the United Kingdom which lack a single overarching constitutional document, unwritten conventions are still of vital importance in understanding how the state functions. In most states, however, many old conventions have been replaced or superseded by laws (called codification).\n\nThe term was first used by British legal scholar A. V. Dicey in his 1883 book, \"Introduction to the Study of the Law of the Constitution\". Dicey wrote that in Britain, the actions of political actors and institutions are governed by two parallel and complementary sets of rules:\n\nA century later, Canadian scholar Peter Hogg wrote,\n\nConstitutional conventions arise when the exercise of a certain type of power, which is not prohibited by law, arouses such opposition that it becomes impossible, on future occasions, to engage in further exercises of this power. For example, the constitutional convention that the Prime Minister of the United Kingdom cannot remain in office without the support of a majority of votes the House of Commons is derived from an unsuccessful attempt by the ministry of Robert Peel to govern without the support of a majority in the House, in 1834–1835.\n\nConstitutional conventions are not, and cannot be, enforced by courts of law. The primary reason for this, according to the Supreme Court of Canada in its 1981 Patriation Reference, is that, \"They are generally in conflict with the legal rules which they postulate and the courts may be bound to enforce the legal rules.\" More precisely, the conventions make certain acts, which would be permissible under a straightforward reading of the law, impermissible in practice. The court ruled that this conflict between convention and law means that no convention, no matter how well-established or universally accepted, can \"crystallize\" into law, unless the relevant parliament or legislature enacts a law or constitutional amendment codifying for a convention at which must specify request and consensus' for enactment. This principle is regarded as authoritative in a number of other jurisdictions, including the UK.\n\nSome conventions evolve or change over time. For example, before 1918 the British Cabinet requested a parliamentary dissolution from the monarch, with the Prime Minister conveying the request. Since 1918, Prime Ministers request dissolutions on their own initiative, and need not consult members of the Cabinet (although, at the very least, it would be unusual for the Cabinet not to be aware of the Prime Minister's intention).\n\nHowever, conventions are rarely ever broken. Unless there is general agreement on the breach, the person who breaches a convention is often heavily criticised, on occasions leading to a loss of respect or popular support. It is often said that \"conventions are not worth the paper they are written on\", i.e., they are unenforceable in law because they are not written down.\n\n\nNo convention is absolute; all but one (the second) of the above conventions were disregarded in the leadup to or during the constitutional crisis of 1975.\n\nIgnoring constitutional conventions does not always result in a crisis. After the Tasmanian state election, 2010, the Governor of Tasmania rejected the advice of his Premier to appoint the leader of the opposition as Premier because he felt the advice was tendered in bad faith. The Premier went on to form a new government.\n\n\n\n\n\n\n\n\nThere is a convention that the Prime Minister of New Zealand should not ask for an early election unless he or she is unable to maintain confidence and supply. By the 1950s, it had also become a convention that elections should be held on the last Saturday of November, or the closest date to this range as possible. There are several times when these conventions have been broken and an election has been held several months earlier:\n\nBecause of the 1814 written constitution's pivotal role in providing independence and establishing democracy in the 19th century, the Norwegian parliament has been very reluctant to change it. Few of the developments in the political system that have been taking place since then have been codified as amendments. This reluctance has been labelled constitutional conservatism. The two most important examples of constitutional conventions in the Norwegian political system are parliamentarism and the declining power of the King.\n\nMuch of Spain's political framework is codified in the Spanish Constitution of 1978, which formalizes the relationship between an independent constitutional monarchy, the government, and the legislature. However, the constitution invests the monarch as the \"arbitrator and moderator of the institutions\" of government.\n\n\nThe following constitutional conventions are part of the political culture of Switzerland. They hold true at the federal level and mostly so at the cantonal and communal level. Mostly, they aim to reconcile the democratic principle of majority rule with the need to achieve consensus in a nation that is much more heterogeneous in many respects than other nation-states.\n\nWhile the United Kingdom does not have a written constitution that is a single document, the collection of legal instruments that have developed into a body of law known as constitutional law has existed for hundreds of years.\n\nAs part of this uncodified British constitution, constitutional conventions play a key role. They are rules that are observed by the various constituted parts though they are not written in any document having legal authority; there are often underlying enforcing principles that are themselves not formal and codified. Nonetheless it is very unlikely that there would be a departure of such conventions without good reason, even if an underlying enforcing principle has been overtaken by history, as these conventions also acquire the force of custom. Examples include:\n\n\n\n\n"}
{"id": "3360851", "url": "https://en.wikipedia.org/wiki?curid=3360851", "title": "Countercontrol", "text": "Countercontrol\n\nCountercontrol is a term used by Dr. B.F. Skinner in 1953 as a functional class in the analysis of social behavior.\n\nCounter control can embedded itself in both passive and active behavior. An individual may not respond to the demanding interventionist or may completely withdraw from the situation passively. The foundation for countercontrol is that human behavior is both a function of the environment and a source of control over it. Counter control originates from the essential behavior-analytic position which states that behavior is always caused or controlled.\n\nControl is fundamental in conceptual, experimental and applied behavior analysis, as it is fundamental in all experimental science. To study functional relations in behavior and environment, one must manipulate (control) environmental variables to study their effect in behavior. Countercontrol can be defined as human operant behavior as a response to social aversive control. The individual that is exposed to aversive control may try to oppose controlling attempts through the process of negative reinforcement, such as by escaping, attacking, or passively resisting.\n\nCountercontrol is a way in which individuals regain behavioral freedom when faced with aversive controlling attempts of others.\n\nThere are two types of countercontrol:\n\n\nCountercontrol is mostly avoidance or escape behavior, thus, this behavior class is only unique insofar as the behaver is (a) confronted with some form of aversive interpersonal or social controlling stimulation and (b) responds to oppose control rather than to reinforce it by \"giving in”.\n\nFrom the principle of consequential causality or selection by consequences, responses occur and are then met by environmental consequences that control the situations of similar responses in future events. Skinner (1953, 1968, 1971, 1972, 1974, 1978) found that countercontrol was important in understanding human behavior because of the prevalence of aversive control in human relations. According to Michael (1982, 1993) Countercontrol can occur at two levels. At one level, countercontrolling behavior results in avoidance or escape from a short-term problem along with non-reinforcement or counter measure from the controlee. For example, a teacher threatens a student with detention and in response the student threatens the teacher with a serious allegation and in turn the teacher withdraws. At another level, individuals may avoid or escape specific short-term consequences contingent on the result that occurs but also avoid or escape long standing aversion contingencies in which those consequences participate.\n\nThere are multiple components of behaviour changes which may occur in countercontrol such being nontargeted responses which fits well with increasing emphasis on the ecological-systems considerations in behaviour analysis.\n\nCountercontrol is also present when adults use modeling to influence the behavior of children. For instance, the adult exhibits appropriate behavior and directs the children to reproduce the same. However, resistance will often occur in the form of disobedience, negativism, opposition, and uncooperativeness thus reducing the probability of the child replicating the desired skill or behavior demonstrated by the adult.\n\nA series of responses by the child may result by this resistance:\n\n\nCountercontrol can have an effect on intervention strategies by minimizing instructional time while increasing program expense. \nMary B.; Robert, Suppa J.; Sharon, Schoen F.; Roberts, Suzanne R (1984). \"Countercontrol: An issue in Intervention\". \"Rase\". 5:38-40. \n\nThe key to reducing the occurrence of countercontrol is to work hard at reducing the degree to which we try to control students' actions.\n\nCountercontrol is mentioned in \"About Behaviorism\". It is also mentioned in Skinner's \"Technology of Teacher\".\n"}
{"id": "2044733", "url": "https://en.wikipedia.org/wiki?curid=2044733", "title": "Duty to rescue", "text": "Duty to rescue\n\nA duty to rescue is a concept in tort law that arises in a number of cases, describing a circumstance in which a party can be held liable for failing to come to the rescue of another party in who could face potential injury or death without being rescued. In common law systems, it is rarely formalized in statutes which would bring the penalty of law down upon those who fail to rescue. This does not necessarily obviate a moral duty to rescue: though law is binding and carries government-authorized sanctions and awarded civil penalties, there are also separate ethical arguments for a duty to rescue that may prevail even where law does not punish failure to rescue.\n\nIn the common law of most English-speaking countries, there is no general duty to come to the rescue of another. Generally, a person cannot be held liable for doing nothing while another person is in peril. However, such a duty may arise in two situations:\n\n\nWhere a duty to rescue arises, the rescuer must generally act with reasonable care, and can be held liable for injuries caused by a reckless rescue attempt. However, many states have limited or removed liability from rescuers in such circumstances, particularly where the rescuer is an emergency worker. Furthermore, the rescuers need not endanger themselves in conducting the rescue.\n\nMany civil law systems, which are common in Continental Europe, Latin America and much of Africa, impose a far more extensive duty to rescue. The duty is usually limited to doing what is “reasonable”. In particular, a helper does not have to substantially endanger themselves.\n\nThis can mean that anyone who finds someone in need of medical help must take all reasonable steps to seek medical care and render best-effort first aid. Commonly, the situation arises on an event of a traffic accident: other drivers and passers-by must take an action to help the injured without regard to possible personal reasons not to help (e.g. having no time, being in a hurry) or ascertain that help has been requested from officials. In practice however, almost all cases of compulsory rescue simply require the rescuer to alert the relevant entity (police, fire brigade, ambulance) with a phone call.\n\nIn some countries, there exists a legal requirement for citizens to assist people in distress, unless doing so would put themselves or others in harm's way. Citizens are often required to, at minimum, call the local emergency number, unless doing so would be harmful, in which case the authorities should be contacted when the harmful situation has been removed. , there were such laws in several countries, including Albania, Andorra, Argentina, Austria, Belgium, Brazil, Bulgaria, Croatia, Czech Republic, Denmark, Estonia, Finland, France, Germany, Greece, Hungary, Iceland, Israel, Italy, the Netherlands, Norway, Poland, Portugal, Russia, Serbia, Spain, Switzerland and Tunisia.\n\nArgentina has legislation on \"abandonment of persons\", Articles 106-108 of the Argentine Penal Code, which includes the provision in Article 106 that \"a person who endangers the life or health of another, either by putting a person in jeopardy or ... will be imprisoned for between 2 and 6 years\" [emphasis added].\n\nIn Quebec, which makes use of civil law, there is a general duty to rescue in its Charter of Rights: \"Every human being whose life is in peril has a right to assistance...Every person must come to the aid of anyone whose life is in peril, either personally or calling for aid, by giving him the necessary and immediate physical assistance, unless it involves danger to himself or a third person, or he has another valid reason.\" Criminal law in Canada is under the exclusive jurisdiction of the federal government, so failure to comply with an article of the Charter in Quebec does not constitute a criminal offence except if by doing so a party also violates the Criminal Code.\n\nOther provinces follow common law.\n\nIn Canadian air law, it is mandatory to make oneself and one's aircraft available to aid search-and-rescue efforts if the aircraft is in the immediate area and a distress signal is received. \n\nUnder the Danish penal code, all persons must provide aid to the best of their ability to any person who appears to be lifeless or in mortal danger (§ 253), must alert authorities or take similar steps to prevent impending disasters that could cause loss of life (§ 185), must comply with all reasonable requests of assistance by a public authority when a person's life, health or well-being is at stake (§ 142), and must, if they learn of a planned crime against the state, human life or well-being, or significant public goods, do everything in their power to prevent or mitigate the crime, including but not limited to reporting it to authorities (§ 141), in all cases provided that acting would not incur particular danger or personal sacrifice.\nViolations are punishable by up to three months (§ 142), two years (§ 185 and § 253) or three years (§ 141) in prison.\n\nAnyone who fails to render assistance to a person in danger will be found liable before French Courts (civil and criminal liability). The penalty for this offence in criminal courts is imprisonment and a fine (under article 223–6 of the Criminal Code) while in civil courts judges will order payment of pecuniary compensation to the victims.\n\nThe photographers at the scene of the fatal car collision of Diana, Princess of Wales, were investigated for violation of the French law of \"non-assistance à personne en danger\" (failing to provide assistance to a person in danger), which can be punished by up to 5 years of imprisonment and a fine of up to €75,000.\n\nIn Germany, \"unterlassene Hilfeleistung\" (failure to provide assistance) is a crime under section 323(c) of the German Criminal Code: any citizen is obligated to provide assistance in case of an accident or general danger if necessary, and is normally immune from prosecution if assistance given in good faith and following the reasonable person's (aka ordinary prudent person's) understanding of required measures turns out to be harmful. Also, the rescuer or responder may not be held liable if the action they should take in order to help is unacceptable for them and they are unable to act (for example when unable to act at the sight of blood). In Germany, knowledge of basic emergency measures and first aid and CPR certification is a prerequisite for the granting of a driving license.\n\nIn Greece, a citizen is required by law to provide help to anyone who asks for it in case of a tragedy or public danger, as long as providing help does not endanger him or her personally. According to article 288 of the criminal code, not providing help in those cases can impose a prison sentence of up to 6 months.\n\nIn 1998, Israel enacted the “Stand-not-idly-by-thy-neighbor’s-blood Law”, taking its name from Leviticus 19:16. It requires one to render assistance whenever one is in the presence of a person who, due to some sudden occurrence, is in severe and immediate danger to life, limb or health, provided that one can do so without placing oneself or a third party in danger. Notifying the authorities (e.g. the police or fire department, as relevant) or calling on others who can render assistance for aid is considered “rendering assistance” under the law. A person obliged to render assistance who fails to do so can be fined.\n\nIn Russia, Article 125 of the criminal code prohibits knowingly abandoning people who are in life- or health-threatening situations when said people can't help themselves. However it binds only those who are either legally obligated to care for said people or who themselves have put said people into life or health threatening situation. The maximum penalty is 1 year in prison.\n\nIn Serbia, a citizen is required by law to provide help to anyone in need (after for example a major car accident) as long as providing help does not endanger him or her personally. Serbian criminal code Articles 126 and 127 state that should one abandon a helpless person and/or not provide aid to a person in need, one could receive a prison sentence of up to one year. If the person dies of injuries due to no aid having been provided by the bystander, a sentence up to 8 years in prison can be imposed.\n\nIn Spain, a citizen is required by law to provide or seek help to anyone in need as long as providing help does not endanger him or her personally. Not doing so is a criminal offence under Article 195 of the Spanish Criminal Code\n\nLegal requirements for a duty to rescue do not pertain in all nations, states, or localities. However, a moral or ethical duty to rescue may exist even where there is no legal duty to rescue. There are a number of potential justifications for such a duty.\n\nOne sort of justification is general and applies regardless of role-related relationships (doctor to patient; firefighter to citizen, etc.). Under this general justification, persons have a duty to rescue other persons in distress by virtue of their common humanity, regardless of the specific skills of the rescuer or the nature of the victim's distress.\n\nThese would justify cases of rescue and in fact make such rescue a duty even between strangers. They explain why philosopher Peter Singer suggests that if one saw a child drowning and could intervene to save him, they should do so, if the cost is moderate to themselves. Damage to their clothing or shoes or how late it might make them for a meeting would be insufficient excuse to avoid assistance. Singer goes on to say that one should also attempt to rescue distant strangers, not just nearby children, because globalization has made it possible to do so. Such general arguments for a duty to rescue also explain why after the 2010 Haiti earthquake, Haitians were digging family members, friends, and strangers out of the rubble with their bare hands and carrying injured persons to whatever medical care was available. They also explain why, while covering that same earthquake, journalist and physician Sanjay Gupta and a number of other MD-journalists began acting as physicians to treat injuries rather than remaining uninvolved in their journalistic roles. Similarly, they justify journalist Anderson Cooper's attempt to shepherd an injured young boy away from some \"toughs\" nearby in the aftermath of the Haiti earthquake.\n\nSpecific arguments for such a duty to rescue include, but are not limited to:\n\nThere are also ethical justifications for role-specific or skill-specific duties of rescue such as those described above under the discussion of U.S. Common Law. Generally, these justifications are rooted in the idea that the \"best\" rescues, the most effective rescues, are done by those with special skills. Such persons, when available to rescue, are thus even more required to do so ethically than regular persons who might simply make things worse (for a utilitarian, rescue by a skilled professional in a relevant field would maximize the good even better than rescue by a regular stranger). This particular ethical argument makes sense when considering the ability firefighters to get both themselves and victims safely out of a burning building, or of health care personnel such as physicians, nurses, physician's assistants, and EMTs to provide medical rescue.\n\nThese are some of the ethical justifications for a duty to rescue, and they may hold true for both regular citizens and skilled professionals even in the absence of legal requirements to render aid.\n\nIn an 1898 case, Buch v. Amory Mfg. Co., 69 N.H. 257, 44 A. 809, 1897 N.H. LEXIS 49 (N.H. 1898), the New Hampshire Supreme Court unanimously held that after an eight-year-old boy negligently placed his hand in the defendant's machinery, the boy had no right to be rescued by the defendant. Beyond that, the trespassing boy could be held liable for damages to the defendant's machine.\n\nIn the 1907 case People v. Beardsley, Beardsley's mistress, Blanche Burns, passed out after overdosing on morphine. Rather than seek medical attention, Beardsley instead had a friend hide her in the basement, and Burns died a few hours later. Beardsley was tried and convicted of manslaughter for his negligence. However, his conviction was reversed by the Supreme Court of Michigan saying that Beardsley had no legal obligation to her.\n\nIn 2016, a 83-year-old man collapsed in a bank lobby in Essen and later died. Several customers stepped over him without providing assistance. With the help of security camera footage, these customers were identified and sentenced to fines of several thousand euros each for failing to provide assistance. A customer who phoned emergency services was not indicted, as he was considered to have provided sufficient assistance.\n\n"}
{"id": "6224656", "url": "https://en.wikipedia.org/wiki?curid=6224656", "title": "Edmund Montgomery", "text": "Edmund Montgomery\n\nEdmund Duncan Montgomery (March 19, 1835 – April 17, 1911) was a Scottish-American philosopher, scientist and physician. He was the husband of German-American sculptor Elisabet Ney\n\nMontgomery was born on the 19th of March, 1835, in Edinburgh, Scotland. His parentage is unknown, but the Elisabet Ney Museum relates the possibility that he was the son of Isabella Davidson (or Montgomery) and a prominent Scottish jurist, Duncan McNeill, 1st Baron Colonsay. He and his mother lived in Paris and Frankfurt, supplemented by a trust fund for him.\n\nBy the time he entered his teens, he began to be interested in the philosophical works of Arthur Schopenhauer. While still living in Frankfurt and only 13 years old, he participated in the Revolutions of 1848 in the German states.\n\nIn 1852, Montgomery majored in medicine at the University of Heidelberg, where he did lab work under Robert Bunsen and came under the influence of Christian Kapp, Ludwig Andreas Feuerbach and Jacob Moleschott. He later attended lectures by Johannes Peter Müller at the University of Berlin during his 1855–1856 enrollment. While studying in Bonn 1856–1857, he attended influential lectures of Hermann von Helmholtz.\n\nMontgomery received his MD degree from the University of Würzburg on February 18, 1858.\n\nMontgomery interned at Prague and Vienna. He served his residency at the German Hospital, Dalston (London) and Bermondsay Dispensary. While doing biological research, he became Curator of the St Thomas' Hospital and Demonstrator of Morbid Anatomy. He was elected to the Royal College of Physicians of London in 1862.\n\nAfter being diagnosed with tuberculosis in 1863, Montgomery left the Royal College of Physicians of London and established medical practices on the resort island of Madeira (1863–1865), in Menton (1866) on the French Riviera, and also in Rome (1867) and Munich (1868), while continuing to do his research. A life annuity allowed him to retire from medical practice in 1869, and devote the rest of his life to philosophy, scientific research, and writing.\n\nMontgomery's work in medicine included the study of cell theory.\n\nMontgomery's study of philosophy complemented his work as a physical scientist. He saw life as the ability of certain chemical compounds to resist damage. He commented on conceptions of knowledge and self in over sixty journal articles and five books. He was an advocate of humanitarianism and a \"religion of life,\" focusing on the idea that man must not ignore the potential of his own yet-to-be-completed evolutionary process.\n\nThe two most significant papers written by Montgomery were his \"Refutation of Kant From the Standpoint of the Empirical\" (1870) and \"The Revelation of Present Experience \"(1910).\n\nIn the former, Montgomery convincingly refutes Immanuel Kant's a priori, the lynchpin of Kant's system, and in the latter, he insists that all knowledge (no exceptions) is based on the evidence provided by the senses.\n\nMontgomery was an advocate of non-Darwinian evolution and organicism. He was also cited as a defender of vitalism. He authored \"The Vitality and Organization of Protoplasm\" (1904) and \"Philosophical Problems in the Light of Vital Organization\". He has been described as a pioneer of organicism.\n\nWhile a student at the University of Heidelberg in 1853, Montgomery began a courtship with sculptor Elisabet Ney, who was visiting friends in the city. They were married at the British consulate in Madeira on November 7, 1863.\n\nMontgomery was diagnosed with tuberculosis in 1863. By 1870, the Franco-Prussian War had begun. In the autumn of that year, Ney became pregnant with their first child. Montgomery received a letter from his friend, Baron Carl Vicco Otto Friedrich Constantin von Stralendorff of Mecklenburg-Schwerin, who had moved to Thomasville, Georgia with his new wife, Margaret Elizabeth Russell of Boston, Massachusetts, declaring the location \"Earth's paradise.\" On January 14, 1871, Ney and Montgomery, accompanied by their housekeeper, Cenci, emigrated to Georgia, to a colony promoted as a resort for consumptives. Their first son, Arthur (1871–1873) was born there, and second son, Lorne (1872–1913), was born in Red Wing, Minnesota, during one of their travels. Baron and Baroness von Stralendorff returned to Wismar, Germany, where he died on July 1, 1872.\n\nIn 1873, Ney traveled alone to Texas. With the help of German Consul Julius Runge in Galveston, Ney was shown Liendo Plantation near Hempstead in Waller County. On March 4, 1873, Montgomery and the rest of the family arrived, and he purchased it. While he tended to his research, Ney ran it for the next twenty years. Their son, Arthur, died of diphtheria in 1873.\n\nMontgomery was involved in an advisory capacity in the founding of Prairie View A&M, originally called Prairie View Normal School.\nMontgomery became a naturalized United States citizen, and thereafter became active in local politics and events. He served two terms as Waller County Road Commissioner and oversaw the building of an iron bridge across the Brazos River. In 1903, he was elected president of the Texas Academy of Science.\n\nMontgomery died on April 17, 1911 after suffering a number of strokes and is buried next to Ney at Liendo.\n\n\n\n"}
{"id": "1695035", "url": "https://en.wikipedia.org/wiki?curid=1695035", "title": "Feature integration theory", "text": "Feature integration theory\n\nFeature integration theory is a theory of attention developed in 1980 by Anne Treisman and Garry Gelade that suggests that when perceiving a stimulus, features are \"registered early, automatically, and in parallel, while objects are identified separately\" and at a later stage in processing. The theory has been one of the most influential psychological models of human visual attention. \n\nAccording to Treisman, the first stage of the feature integration theory is the preattentive stage. During this stage, different parts of the brain automatically gather information about basic features (colors, shape, movement) that are found in the visual field. The idea that features are automatically separated appears to be counterintuitive; however, we are not aware of this process because it occurs early in perceptual processing, before we become conscious of the object.\n\nThe second stage of the feature integration theory is the focused attention stage, where the individual features of an object combine in order to perceive the whole object. In order to combine the individual features of an object, attention is required and selection of that object occurs within a \"master map\" of locations. The master map of locations contains all of the locations in which features have been detected, with each location in the master map having access to the multiple feature maps. When attention is focused at a particular location on the map, the features currently in that position are attended to and are stored in \"object files\". If the object is familiar, associations are made between the object and prior knowledge, which results in identification of that object. In support of this stage, researchers often refer to patients suffering from Balint's syndrome. Due to damage in the parietal lobe, these people are unable to focus attention on individual objects. Given a stimulus that requires combining features, people suffering from Balint's syndrome are unable to focus attention long enough to combine the features, providing support for this stage of the theory. \n\nTreisman distinguishes between two kinds of visual search tasks, \"feature search\" and \"conjunction search\". Feature searches can be performed fast and pre-attentively for targets defined by only one feature, such as color, shape, perceived direction of lighting, movement, or orientation. Features should \"pop out\" during search and should be able to form illusory conjunctions. Conversely, conjunction searches occur with the combination of two or more features and are identified serially. Conjunction search is much slower than feature search and requires conscious attention and effort. In multiple experiments, some referenced in this article, Treisman concluded that color, orientation, and intensity are features for which feature searches may be performed.\n\nAs a reaction to the feature integration theory, Wolfe (1994) proposed the Guided Search Model 2.0. According to this model, attention is directed to an object or location through a preattentive process. The preattentive process, as Wolfe explains, directs attention in both a bottom-up and top-down way. Information acquired through both bottom-up and top-down processing is ranked according to priority. The priority ranking \"guides\" visual search and makes the search more efficient. Whether the Guided Search Model 2.0 or the feature integration theory are \"correct\" theories of visual search is still a hotly debated topic.\n\nIn order to test the notion that attention plays a vital role in visual perception, Treisman and Schmidt (1982) designed an experiment to show that features may exist independently of one another early in processing. Participants were shown a picture involving four objects hidden by two black numbers. The display was flashed for one-fifth of a second followed by a random-dot masking field that appeared on screen to eliminate \"any residual perception that might remain after the stimuli were turned off\". Participants were to report the black numbers they saw at each location where the shapes had previously been. The results of this experiment verified Treisman and Schmidt's hypothesis. In 18% of trials, participants reported seeing shapes \"made up of a combination of features from two different stimuli\", even when the stimuli had great differences; this is often referred to as an illusory conjunction. Specifically, illusory conjunctions occur in various situations. For example, you may identify a passing person wearing a red shirt and yellow hat and very quickly transform him or her into one wearing a yellow shirt and red hat. The feature integration theory provides explanation for illusory conjunctions; because features exist independently of one another during early processing and are not associated with a specific object, they can easily be incorrectly combined both in laboratory settings, as well as in real life situations. \n\nAs previously mentioned, Balint's syndrome patients have provided support for the feature integration theory. Particularly, Research participant R.M., a Bálint's syndrome sufferer who was unable to focus attention on individual objects, experiences illusory conjunctions when presented with simple stimuli such as a \"blue O\" or a \"red T.\" In 23% of trials, even when able to view the stimulus for as long as 10 seconds, R.M. reported seeing a \"red O\" or a \"blue T\". This finding is in accordance with feature integration theory's prediction of how one with a lack of focused attention would erroneously combine features.\n\nIf people use their prior knowledge or experience to perceive an object, they are less likely to make mistakes, or illusory conjunctions. In order to explain this phenomenon, Treisman and Souther (1986) conducted an experiment in which they presented three shapes to participants where illusory conjunctions could exist. Surprisingly, when she told participants that they were being shown a carrot, lake, and tire (in place of the orange triangle, blue oval, and black circle, respectively), illusory conjunctions did not exist. Treisman maintained that prior-knowledge played an important role in proper perception. Normally, bottom-up processing is used for identifying novel objects; but, once we recall prior knowledge, top-down processing is used. This explains why people are good at identifying familiar objects rather than unfamiliar.\n\nWhen identifying letters while reading, not only are their shapes picked up but also other features like their colors and surrounding elements. Individual letters are processed serially when spatially conjoined with another letter. The locations of each feature of a letter are not known in advance, even while the letter is in front of the reader. Since the location of the letter's features and/or the location of the letter is unknown, feature interchanges can occur if one is not attentively focused. This is known as lateral masking, which in this case, refers to a difficulty in separating a letter from the background.\n\n\n\n"}
{"id": "51626243", "url": "https://en.wikipedia.org/wiki?curid=51626243", "title": "Ferdinand Lee Barnett (Chicago)", "text": "Ferdinand Lee Barnett (Chicago)\n\nFerdinand Lee Barnett (February 18, 1852 – March 11, 1936) was an African-American journalist, lawyer, and civil rights activist in Chicago, Illinois in the late Reconstruction era and after. He was a founding editor of \"The Chicago Conservator\" monthly in 1878. He was the third black person to be admitted to the Illinois bar and became a successful lawyer.\n\nIn 1895, he married Ida B. Wells, a journalist and anti-lynching activist. In 1896, he became Illinois' first black assistant state's attorney. He was active in anti-lynching and civil rights and was called \"one of the foremost citizens Chicago has ever had\" by the \"Chicago Defender\".\n\nFerdinand Lee Barnett was born in Nashville, Tennessee in 1852. His mother was a freewoman, Martha Brooks, born about 1825. His father, also named Ferdinand Lee Barnett, was born in Nashville about 1810 and worked as a blacksmith. He purchased his family's freedom the year Ferdinand was born. They lived in Nashville until about 1859, when they left the United States and moved to Windsor, Ontario, across the Detroit River from Detroit, Michigan. They wanted to get beyond the reach of the Fugitive Slave Act of 1850, which had created incentives for slave catchers to kidnap free blacks and sell them into slavery.\n\nThe Barnett family returned to the US in 1869 after the American Civil War and the end of slavery, settling in Chicago, Illinois. Ferdinand was educated in Chicago schools, first attending the old Jones school at Clark and Harrison. He entered Central High School, graduating in 1874. After high school he taught in the southern United States for two years before returning to Chicago to attend Union College of Law, later a part of Northwestern Law School. Barnett graduated from law school and was admitted to the Illinois bar in 1878. He was the third black person to pass the Illinois bar, following Lloyd G. Wheeler and Richard A. Dawson.\n\nNumbered among his cousins were Ferdinand L. Barnett and his brother Alfred S. Barnett. They were also journalists and lived in Omaha, Nebraska, and Des Moines, Iowa, respectively.\n\nBarnett's father died in early February 1898. His mother died November 11, 1908.\n\nIn December 1877, Barnett, along with co-editors Abram T. Hall, Jr. and James E. Henderson, organized the semi-monthly newspaper, the \"Conservator\", the first edition appearing on January 1, 1878. Also among the editors and stakeholders was Iowan Alexander Clark. He moved to Chicago in 1884, where he served as an editor on the paper. (Clark was appointed as US ambassador to Liberia in 1891.)\n\nThe \"Conservator\" was a radical journal that focused on justice and equal rights, and Barnett was soon recognized as a local black leader. He was selected as a delegate to the May 1879 National Conference of Colored Men in Nashville, where he gave a noted speech calling for unity and education. He was a delegate to the 1884 Inter-State Conference of Colored Men in Pittsburgh, and the national convention of the Timothy Thomas Fortune led Afro-American League in Chicago in 1890 where he was named secretary.\n\nBarnett started practicing law around 1883. His prominence grew quickly and in 1888 he was considered for a Republican nomination for Cook County Commissioner. In 1892, he started a law partnership with S. Laing Williams. The pair split over Williams' affiliation with Booker T. Washington, whom Barnett frequently opposed.\n\nIn 1892, three men, including a friend of Ida B. Wells, were lynched by a white mob while in police custody in Memphis, Tennessee, in an event known as the Peoples Grocery lynching. The act sparked a national outcry and Barnett took part in meetings in Chicago called to organize reaction. At a meeting of one thousand people at Bethel A. M. E. Church, Reverend W. Gaines' call for the crowd to sing the then \"de facto\" national anthem, \"My Country, 'Tis of Thee\", but the call was refused, one member of the audience declaring, \"I don't want to sing that song until this country is what it claims to be, 'sweet land of liberty'\". Gaines substituted the Civil War-era song about the abolitionist martyr, \"John Brown's Body\". Barnett closed the meeting appealing for calm and a careful response, but also expressing great frustration and concern that the violence against blacks may one day lead to reprisals.\n\nThe event inspired Wells, who began to research and speak out against lynchings. In 1893, Wells sued the \"Memphis Commercial\" for libel when the journal attacked Wells over her reports on the racism and injustice of lynching. It had been claimed that lynching, while not legal, was a natural result of the need for revenge of a community against perpetrators of violent crime and did not single out blacks. Wells' work showed the falseness of this narrative. Wells asked lawyer and activist Albion W. Tourgee to represent her on the case, but Tourgee refused, having largely retired from law (with the exception of his ongoing support of the case which would become Plessy v Fergusson). Tourgee recommended Wells contact Barnett, and Barnett agreed to take the case. This may have been Barnett's introduction to Wells, whom he would marry two years later, and Barnett was taken with Wells. However, Barnett came to agree with the advice of Tourgee that the case could not be won, as a black woman would never win such a case heard by a white, male jury, and the case was dropped.\n\nIn 1893, Barnett coauthored a pamphlet entitled \"The Reason Why the Colored American is not in the World's Columbian Exposition – The Afro-American's Contribution to Columbian Literature\". The exposition, held in Chicago, refused to include an African American exhibit. The pamphlet was another early example of Barnett's personal and professional relationship with Wells. The pamphlet was published by Barnett, Wells, abolitionist Frederick Douglass, and educator Irvine Garland Penn. The exhibition included a number of exhibits put on by individuals and approved by white organizers of the fair, including exhibits by the sculptor Edmonia Lewis, a painting exhibit by scientist George Washington Carver, and a statistical exhibit by John Imogen Howard. It also included blacks in white exhibits, such as Nancy Green's portrayal of the character, \"Aunt Jemima\" for the R. T. Davis Milling Company.\n\nBarnett's first wife was Molly Henrietta Graham Barnett, who was the first black woman to graduate from the University of Michigan. Molly and Ferdinand had two children, Ferdinand L. and Albert Graham Barnett. Molly died in 1888 of heart disease. The younger Ferdinand Barnett served as Eighth Regiment supply sergeant in World War I. Albert Barnett became the city editor of the \"Defender\" in Chicago.\n\nWells remained in Chicago after the Columbian Exposition. In June 1895, she and Barnett married. The couple had four children, Charles Aked (1896), Herman Kohlsaat (1897), Ida B. (1901) and Alfreda M. Barnett (1904). Charles was named for the English anti-lynching activist, Charles Aked, and Herman was named for the owner of the \"Chicago Inter Ocean\", Herman Kohlsaat, who supported the \"Conservator\". Barnett's attraction to Wells included his recognition of the mutual support for each other's careers that the relationship would bring. Shortly before their marriage, Wells purchased Barnett's stake in the \"Conservator\" and became the paper's manager and co-editor, while Barnett focused on his legal career. Reverend Richard DeBaptiste, founder of Olivet Baptist Church, was co-editor after Barnett.\n\nBarnett was an active Republican, and his support for the party put him in line for public office. In 1896, he was put in charge of the bureau of information and education for blacks by the Republican National Committee.\n\nAlso in 1896, Barnett became the first black assistant state's attorney in Illinois, appointed by the state's attorney Charles S. Deneen upon the recommendation of the Cook County Commissioner Edward H. Wright. As assistant state's attorney, Barnett worked in the juvenile court, in antitrust cases, and in habeas corpus and extradition proceedings. He frequently appeared before the Illinois Supreme Court and had a good record. In 1902, Barnett made national news when he suggested that 10 million blacks would revolt against lynch law in the South at a gathering at Bethlehem Church in Chicago. In 1904 he was appointed as head of the Chicago branch of the Republican Party's Negro Bureau. This appointment was opposed by Booker T. Washington, who preferred Barnett's former partner, S. Laing Williams.\n\nIn 1906, Barnett was nominated as judge in the new municipal court in Chicago, the first black candidate for a judgeship in Illinois. Barnett lost the election by 304 votes due to a lack of support by white and black Republicans. In the campaign for the position, Barnett did not gain the full support of black ministers, particularly Archibald J. Cary. They were angry that his wife Ida B. Wells supported gambling kingpin Bob Motts's Peking Theater – which was converted from a saloon. Barnett was initially declared winner, but the results were reviewed and Barnett became the only one of 27 Republican candidates rejected. If he had been elected, Barnett would have been the second black judge in a court of record after Robert Heberton Terrell of Washington, D.C.\n\nBarnett left the position of assistant state's attorney in 1910, turning to private practice where he advocated for African-American rights. He often worked \"pro bono\", focusing on employment discrimination and criminal cases. In 1917 he was a candidate for alderman of the second ward in Chicago. His most famous case was the defense with attorneys Robert M. McMurdy and Cowen of \"Chicken Joe\" Campbell. Although Campbell was convicted for the murder of Odelle B. Allen, his death sentence was commuted to life imprisonment by Governor Frank O. Lowden on April 12, 1918.\n\nin the 1920s, Barnett and his wife supported Marcus Garvey and the Universal Negro Improvement Association. In the 1920s and 1930s, Barnett began to support the Democratic Party.\n\nBarnett died March 11, 1936.\n"}
{"id": "58630220", "url": "https://en.wikipedia.org/wiki?curid=58630220", "title": "Future self", "text": "Future self\n\nThe psychological research on the future self examines the processes and consequences associated with thinking about oneself in the future. People think about their future selves similarly to how they think about other people. The extent people feel psychological connectedness (e.g., similarity, closeness) to their future self influences how well they treat their future self. When people feel connected to their future self, they are more likely to save for retirement, make healthy decisions, and avoid ethical transgressions. Interventions that increase feelings of connectedness with future selves can improve prudent decision making across these domains. \n\nPsychological research on the future self often attributes its theoretical foundations to the philosopher Derek Parfit. Parfit argued that people might differ in the extent they feel similar and connected to themselves in the future. Under Parfit’s conceptualization, people act rationally by basing their concern for their future on the degree of connectedness between present and future selves. According to Parfit, it is rational for people that perceive very little connectedness with their future self to act in ways that neglect the future self (e.g., by smoking). \n\nThe psychological work that followed did not similarly argue that people who feel disconnected from their future self \"should\" engage in short-sighted behaviors. Rather, the psychology of the future self primarily addresses the descriptive validity of Parfit’s theory. \n\nSocial psychological and neurological evidence suggests that people think about themselves in the future similar to how they think about other people. Just as feeling close to others increases prosocial giving, feeling close to one’s future self motivates people to delay present gratification in order to benefit themselves in the future. \n\nShane Frederick initially tested whether the degree of connectedness with the future self is associated with less discounting of future benefits (in dollar amounts and time). He measured connectedness by asking participants to rate how similar they expect to be with themselves 5, 10, 20, 30, and 40 years in the future. He did not estimate a statistically significant relationship between the degree of connectedness and discounting of future benefits. \n\nIn 2009, Hal Hershfield and colleagues introduced a new method for measuring psychological connectedness with the future self. Adapting the Inclusion of Other in the Self Scale, connectedness was depicted with seven pairs of successively overlapping circles. By choosing the completely non-overlapping circles, participants indicate the minimum level of connectedness between their present and future selves, and by choosing the near completely overlapping circles, participants indicate the maximum level of connectedness. Studies by Hal Hershfield, as well as Daniel Bartels and Oleg Urminsky, have now demonstrated a robust relationship between psychological connectedness and discount rates. The more psychologically connectedness people feel between present and future selves, the more they care about the future, and the less they discount future benefits.\n\nThe research that followed showed systematic ways to enhance psychological connectedness. Experiments have manipulated connectedness by having participants:\n\n\nThe randomized experiments revealed a causal relationship between feeling connected to one’s future self and subsequently making more patient long-term decisions.\n\nIn one of the first experiments to enhance psychological connectedness with the future self, participants were given immersive virtual reality technology and instructed to look at themselves in a virtual mirror. The experimenters randomized whether participants saw an age-progressed version of themselves (meant to look approximately 70 years old) or a current-aged self. Participants that interacted with their future self were more likely to delay present monetary rewards and indicated greater intentions to save for retirement. The effect of interacting with the future self on financial decisions was mediated by enhanced feelings of psychological connectedness. Leveraging the insights from this experiment, firms such as Merrill Lynch have since adopted web applications with age-progressing software in order to increase retirement savings. \n\nThe finding has been conceptually replicated with multiple diverse samples. In one field experiment, students from economically diverse backgrounds that had weekly interactions with an avatar of their future self demonstrated heightened performance during a financial education course. A team of researchers, in collaboration with Ideas42, launched another replication with thousands of Mexican citizens. Before deciding whether to sign-up for an automatic savings account, the treatment group was asked to spend time vividly imagining their lives in the distant future. Compared to a 1% take up rate in the control condition, 3% of people in the treatment condition enrolled in the automatic savings account.\n\nThe effect of psychological connectedness on financial decision making is moderated by knowledge about future outcomes. When people are unaware of their future financial needs, regardless of how connected they feel, they are unlikely to save for the future. Similarly, people that have full information about the consequences of their financial actions will only save if they also feel connectedness with their future self. The researchers argue that policy makers who provide information to consumers on retirement savings should also consider simultaneously enhancing psychological connectedness. People are most likely to save rather than spend when they are knowledgeable about the outcomes of their decisions \"and\" feel connected to their future selves.\n\nIn 2017, the Consumer Financial Protection Bureau included a measure of psychological connectedness to the future self in its first Financial Wellbeing Survey. \n\nApplying the existing theory, researchers hypothesized that increasing feelings of connectedness with the future self should cause people to make healthier dieting and exercising decisions. Correlational evidence suggests that feeling psychologically connected to the future self corresponds with greater self-reported health. In randomized experiments, people were prompted to write about a distant future self (in 20 years time) or a near future self (in 3 months). Participants that wrote about a distant future self were more likely to exercise in the days following the writing task compared to participants in the near future condition. \n\nPeople engage in ethically dubious behavior because they tend to neglect the potential future consequences of their behavior. The more psychologically connected people feel to their future selves, however, the more they care about potential future consequences and avoid ethical transgressions in the present moment. Experiments have found that assigning participants to write about their future selves can decrease support for unethical negotiation strategies. In another set of experiments, after interacting with a 40-year old version of themselves in immersive virtual reality, college students were less likely to cheat on a following task. In a field experiment, high schoolers in the Netherlands were randomly assigned to either receive texts from an avatar of their future self in the treatment condition or texts from their current self in the control condition. For a week, the high schoolers in the treatment condition received text messages prompting them to imagine their future circumstances. Following the intervention, high schoolers in the treatment condition were less likely to engage in delinquent and anti-social behavior compared to participants in the control condition. \n\nPossible selves are specific ideas about who one might become in the future. Possibles selves include the ideal selves people hope to become, selves people could become, and selves people are afraid of becoming. Possible selves can serve as a roadmap to guide individuals from where they are in the present to where they imagine being in the future. When the possible selves people imagine are unattainable fantasies, rather than reasonable expectations, effort and performance are lower across educational, dating, and medical contexts.\n\nWant-should conflicts refer to internal conflicts between one’s \"want\" \"self\" and one’s \"should self\". The theory assumes people simultaneously hold two sets of preferences; one associated with their \"want self\" (i.e., present-focused, hedonistic) and one associated with their \"should self\" (i.e., future-focused, utilitarian). Interventions that seek to increase patient decision making, for example, can use commitment devices to ensure people act on their \"should\" preferences and avoid succumbing to their \"want\" preferences.\n\nEnhancing psychological connectedness to the future self can improve ability to delay gratification. Walter Mischel devoted a chapter to the future self in his book, \"The Marshmallow Test\".\n\n"}
{"id": "56411714", "url": "https://en.wikipedia.org/wiki?curid=56411714", "title": "Half-A-Room", "text": "Half-A-Room\n\nHalf-A-Room is a 1967 conceptual artwork by the Japanese artist Yoko Ono.\n\nThe work is made from various objects that have been cut in half and painted white. It was made with the help of Ono's second husband, Anthony Cox, and some local art students. The piece was first displayed at Ono's 'Half-A-Wind' exhibition (also called 'Yoko Plus Me') at the Lisson Gallery in West London in 1967. At the Lisson Gallery show the objects were accompanied by a row of glass bottles on a shelf with each bottle containing the words \"Half a X\" for each cut up object to represent their respective missing halves. The names in bottles was suggested by John Lennon, with Ono recognizing his contribution semi-anonymously with the inclusion of the label \"J.L.\" underneath the bottles.\n\nThe installation in Ono's 2014 exhibition at the Guggenheim Museum in Bilbao contained:\n\n\n\"Half-a-Room\" was shown at Ono's exhibition from 10 November 2007 to 4 February 2008 at the Centro Cultural Banco do Brasil in São Paulo .\n\nThe work was shown at Ono's 1967 \"Half-a-Wind\" exhibition at the Lisson Gallery in Paddington, London, from 11 October to 14 November 1967. The cost of the exhibition was underwritten by John Lennon. Ono had been to see Lennon's friend Pete Shotton, who was working for The Beatles company Apple Corps, and had asked to borrow a few thousand pounds to fund the exhibition. Shotton told Ono that he \"really not authorised to hand out two thousand quid like that\" but upon asking Lennon, Shotton said that he \"merely grunted the affirmative without further comment\". Lennon later described the exhibition as all \"beautifully cut in half and painted white...That was our first public appearance, but I didn't even go and see the show, I was too uptight\".\n\n\"Half-A-Room\" has been described by Ono as a response to her feeling at the time that \"there was a half empty space in my life\" as a result of the increasing estrangement of her and her second husband, Anthony Cox. Ono awoke one day and noticed that her Cox had not returned from a night out, and so the bed was half empty, and through this realised that there was \"a half empty space in my life\". The magazine \"Another Magazine\" wrote in 2015 that the piece \"speaks to the pointlessness of material things without the human connection that gives them meaning; the result is a pure, painful representation of heartbreak that takes the breath away.\" The emotional impact of the end of her relationship with Cox subsequently inspired Ono to make her piece \"Ceiling Painting/Yes Painting\".\n\nIn the accompanying exhibition text \"Some Notes on the Lisson Gallery Show\", Ono wrote that \"I think of this show as an elephant's tail. ...Life is only half a game. Molecules are always at the verge of half disappearing and half emerging. Somebody said I should also put half-a-person in the show. But we are halves already. It is sad that the air is the only thing we share. No matter how close we get to each other, there is always air between us. It is also nice that we share the air. No matter how far apart we are, the air links us\". In an audio guide recorded for Ono's 2015 retrospective at the Museum of Modern Art in New York City, Ono said that \"You see a room with little – a space between it. Instead of a room that's packed, you know, it has air between there. ...In those days, I still didn't have a life of just being alone. And then I thought, \"This is a great one for a work of art to show to people that we're just half.\" Anyway, everything that I see here, the other half is invisible. And that other half may be something that we might see one day, but now we don't see it\".\n"}
{"id": "24028438", "url": "https://en.wikipedia.org/wiki?curid=24028438", "title": "Holy Virility", "text": "Holy Virility\n\nHoly virility\nor Holy virility: The Social Construction of Masculinity is a book by Emmanuel Reynaud.\n\nFirst Published in France in 1981 as \"La Sainte Virilité\"\nHoly virility explores how language and society place pressure upon men to behave in line with their socially gendered role.\nReynaud calls for men to fight for self-realization in order to emancipate themselves.\nHe states that association of power in male sexuality prevents men from enjoying sensual sex.\n\nDaniel Wesler-Lang, in his article Déconstruire le masculin, problèmes épistémologiques, situates Reynauds thoughts within studies of masculinity and violence.\n\nBackground on Holy Virility\n\nHoly Virility observes the different aspects of male power that is perpetrated on women. The book The Social Construction of Masculinity explains how Holy Virility and males are oppressed to measure up to the standards of being an identified male. The author Emmanuel Renaud argues that “It is within men’s grasp to reject power relations and begin to behave in a way that is neither hierarchical nor exploitative.” The author explores different dynamics and is active in finding knowledge to educate men and women. The books main focus is on men’s biological makeup, sexuality, phallic orgasm, language, being a dad/rapist and being married. Emmanuel Reynaud explores and explains how men’s performance in society are supposed to be masculine. Emmanuel Reynaud born in France influences and educates the men’s movement in France. Today some of the concepts used in the book are seen in today’s society. Western and Eastern ideologies of being a man and women are different throughout history. Men abuse the power they have and seem hierarchal against women. Holy Virility is the social construction of masculinity. Men today are taught to act a certain way through gender, men for masculinity and female through acting feminine. Although, men and women can put on a performance of opposite genders. The Holy Virility explains the reasons why do men act the way they do and give concrete information about men's performance.\n"}
{"id": "162776", "url": "https://en.wikipedia.org/wiki?curid=162776", "title": "Hylomorphism", "text": "Hylomorphism\n\nHylomorphism (or hylemorphism) is a philosophical theory developed by Aristotle, which conceives being (ousia) as a compound of matter and form. The word is a 19th-century term formed from the Greek words ὕλη \"hyle\", \"wood, matter\", and μορφή, \"morphē\", \"form\".\n\nAristotle defines X's matter as \"that out of which\" X is made. For example, letters are the matter of syllables. Thus, \"matter\" is a relative term: an object counts as matter relative to something else. For example, clay is matter relative to a brick because a brick is made of clay, whereas bricks are matter relative to a brick house.\n\nChange is analyzed as a material transformation: matter is what undergoes a change of form. For example, consider a lump of bronze that's shaped into a statue. Bronze is the matter, and this matter loses one form (that of a lump) and gains a new form (that of a statue).\n\nAccording to Aristotle's theory of perception, we perceive an object by receiving its form with our sense organs. Thus, forms include complex \"qualia\" such as colors, textures, and flavors, not just shapes.\n\nMedieval philosophers who used Aristotelian concepts frequently distinguished between substantial forms and accidental forms. A substance necessarily possesses at least one substantial form. It may also possess a variety of accidental forms. For Aristotle, a \"substance\" (\"ousia\") is an individual thing—for example, an individual man or an individual horse. The substantial form of substance S consists of S's essential properties, the properties that S's matter needs in order to be the kind of substance that S is. In contrast, S's accidental forms are S's non-essential properties, properties that S can lose or gain without changing into a different kind of substance.\n\nIn some cases, a substance's matter will itself be a substance. If substance A is made out of substance B, then substance B is the matter of substance A. However, what is the matter of a substance that is not made out of any other substance? According to Aristotelians, such a substance has only \"prime matter\" as its matter. Prime matter is matter with no substantial form of its own. Thus, it can change into various kinds of substances without remaining any kind of substance all the time.\n\nAristotle applies his theory of hylomorphism to living things. He defines a soul as that which makes a living thing alive. Life is a property of living things, just as knowledge and health are. Therefore, a soul is a form—that is, a specifying principle or cause—of a living thing. Furthermore, Aristotle says that a soul is related to its body as form to matter.\n\nHence, Aristotle argues, there is no problem in explaining the unity of body and soul, just as there is no problem in explaining the unity of wax and its shape. Just as a wax object consists of wax with a certain shape, so a living organism consists of a body with the property of life, which is its soul. On the basis of his hylomorphic theory, Aristotle rejects the Pythagorean doctrine of reincarnation, ridiculing the notion that just any soul could inhabit just any body.\n\nAccording to Timothy Robinson, it is unclear whether Aristotle identifies the soul with the body's structure. According to one interpretation of Aristotle, a properly organized body is already alive simply by virtue of its structure. However, according to another interpretation, the property of life—that is, the soul—is something in addition to the body's structure. Robinson uses the analogy of a car to explain this second interpretation. A running car is running not only because of its structure but also because of the activity in its engine. Likewise, according to this second interpretation, a living body is alive not only because of its structure but also because of an additional property: the soul, which a properly organized body needs in order to be alive. John Vella uses Frankenstein's monster to illustrate the second interpretation: the corpse lying on Frankenstein's table is already a fully organized human body, but it is not yet alive; when Frankenstein activates his machine, the corpse gains a new property, the property of life, which Aristotle would call the soul.\n\nSome scholars have pointed out a problem facing Aristotle's theory of soul-body hylomorphism. According to Aristotle, a living thing's matter is its body, which needs a soul in order to be alive. Similarly, a bronze sphere's matter is bronze, which needs roundness in order to be a sphere. Now, bronze remains the same bronze after ceasing to be a sphere. Therefore, it seems that a body should remain the same body after death. However, Aristotle implies that a body is no longer the same body after death. Moreover, Aristotle says that a body that has lost its soul is no longer potentially alive. But if a living thing's matter is its body, then that body should be potentially alive by definition.\n\nOne approach to resolving this problem relies on the fact that a living body is constantly replacing old matter with new. A five-year-old body consists of different matter than does the same person's seventy-year-old body. If the five-year-old body and the seventy-year-old body consist of different matter, then what makes them the same body? The answer is presumably the soul. Because the five-year-old and the seventy-year-old bodies share a soul—that is, the person's life—we can identify them both as the body. Apart from the soul, we cannot identify what collection of matter is the body. Therefore, a person's body is no longer that person's body after it dies.\n\nAnother approach to resolving the problem relies on a distinction between \"proximate\" and \"non-proximate\" matter. When Aristotle says that the body is matter for a living thing, he may be using the word \"body\" to refer to the matter that makes up the fully organized body, rather than the fully organized body itself. Unlike the fully organized body, this \"body\" remains the same thing even after death. In contrast, when he says that the body is no longer the same after its death, he is using the word \"body\" to refer to the fully organized body.\n\nAristotle says that the intellect (\"nous\"), the ability to think, has no bodily organ (in contrast with other psychological abilities, such as sense-perception and imagination). In fact, he says that it is not mixed with the body and suggests that it can exist apart from the body. This seems to contradict Aristotle's claim that the soul is a form or property of the body. To complicate matters further, Aristotle distinguishes between two kinds, or two parts, of intellect. These two intellectual powers are traditionally called the \"passive intellect\" and the \"active (or agent) intellect\". Thus, interpreters of Aristotle have faced the problem of explaining how the intellect fits into Aristotle's hylomorphic theory of the soul.\n\nAccording to one interpretation, a person's ability to think (unlike his other psychological abilities) belongs to some incorporeal organ distinct from his body. This would amount to a form of dualism. However, according to some scholars, it would not be a full-fledged Cartesian dualism. This interpretation creates what Robert Pasnau has called the \"mind-soul problem\": if the intellect belongs to an entity distinct from the body, and the soul is the form of the body, then how is the intellect part of the soul?\n\nAnother interpretation rests on the distinction between the passive intellect and the agent intellect. According to this interpretation, the passive intellect is a property of the body, while the agent intellect is a substance distinct from the body. Some proponents of this interpretation think that each person has his own agent intellect, which presumably separates from the body at death. Others interpret the agent intellect as a single divine being, perhaps the unmoved mover, Aristotle's God.\n\nA third interpretation relies on the theory that an individual form is capable of having properties of its own. According to this interpretation, the soul is a property of the body, but the ability to think is a property of the soul itself, not of the body. If that is the case, then the soul is the body's form and yet thinking need not involve any bodily organ.\n\nThe Neoplatonic philosopher Avicebron (\"a.k.a.\"Solomon Ibn Gabirol) proposed a Neoplatonic version of this Aristotelian concept, according to which all things, including soul and intellect, are composed of matter and form.\n\nMedieval theologians, newly exposed to Aristotle's philosophy, applied hylomorphism to Christian doctrines such as the transubstantiation of the Eucharist's bread and wine into the body and blood of Jesus. Theologians such as Duns Scotus and Thomas Aquinas developed Christian applications of hylomorphism.\n\nMany medieval theologians and philosophers followed Aristotle in seeing a living being's soul as that being's form—specifically, its substantial form. However, they disagreed about whether X's soul is X's \"only\" substantial form. Some medieval thinkers argued that X's soul is X's only substantial form, responsible for all of the features of X's body. In contrast, other medieval thinkers argued that a living being contains at least two substantial forms—(1) the shape and structure of its body, and (2) its soul, which makes its body alive.\n\nThomas Aquinas claimed that X’s soul was X’s only substantial form, although X also had numerous accidental forms that accounted for X’s nonessential features. Aquinas defined a substantial form as that which makes X's matter constitute X, which in the case of a human being is rational capacity. He attributed all other features of a human being to accidental forms. However, Aquinas did not claim that the soul was identical to the person. He held that a proper human being is a composite of form and matter, specifically prime matter. Form and matter taken separately may retain some of the attributes of a human being but are nonetheless not identical to that person. So a dead body is not actually or potentially a human being.\n\nEleonore Stump describes Aquinas' theory of the soul in terms of \"configuration\". The body is matter that is \"configured\", i.e. structured, while the soul is a \"configured configurer\". In other words, the soul is itself a configured thing, but it also configures the body. A dead body is merely matter that was once configured by the soul. It does not possess the configuring capacity of a human being.\n\nAquinas believed that rational capacity was a property of the soul alone, not of any bodily organ. However, he did believe that the brain had some basic cognitive function. Aquinas’ attribution of rational capacity to the soul allowed him to claim that disembodied souls could retain their rational capacity, although he was adamant that such a state was unnatural.\n\nAristotle holds a teleological worldview: he sees the universe as inherently purposeful. Basically, Aristotle claims that potentiality exists for the sake of actuality. Thus, matter exists for the sake of receiving its form, and an organism has sight for the sake of seeing. Now, each thing has certain potentialities as a result of its form. Because of its form, a snake has the potential to slither; we can say that the snake \"ought to\" slither. The more a thing achieves its potential, the more it succeeds in achieving its purpose.\n\nAristotle bases his ethical theory on this teleological worldview. Because of his form, a human being has certain abilities. Hence, his purpose in life is to exercise those abilities as well and as fully as possible. Now, the most characteristic human ability, which is not included in the form of any other organism, is the ability to think. Therefore, the best human life is a life lived rationally.\n\nThe idea of hylomorphism can be said to have been reintroduced to the world when Werner Heisenberg invented his duplex world of quantum mechanics. In his 1958 text \"Physics and Philosophy\", Heisenberg states:\n\n\n"}
{"id": "1443485", "url": "https://en.wikipedia.org/wiki?curid=1443485", "title": "Implementation intention", "text": "Implementation intention\n\nAn implementation intention (II) is a self-regulatory strategy in the form of an \"if-then plan\" that can lead to better goal attainment, as well as help in habit and behavior modification. It is subordinate to goal intentions as it specifies the \"when\", \"where\" and \"how\" portions of goal-directed behavior. The concept of implementation intentions was introduced in 1999 by psychologist Peter Gollwitzer. Studies conducted by Gollwitzer in 1997 and earlier show that the use of implementation intentions can result in a higher probability of successful goal attainment, by predetermining a specific and desired goal-directed behavior in response to a particular future event or cue.\n\nThe concept of implementation intentions originated from research on goal striving throughout the late 1980s and 1990s. Developing research suggests that \"the correlations between intentions and behavior are modest, in that intentions account for only 20% to 30% of the variance in behavior.\" Strong intentions (\"I strongly intend to do X\") were observed to be more often realized than weak intentions. Past behavior still tended to be a better predictor for a person's future behavior when it was compared to goal intentions. The research also suggested that the weak intention-behavior relation is a result of people having good intentions, but failing to act on them.\n\nThis inspired a growing body of research to help determine ways in which peoples' good intentions could be made more effective in accomplishing desired goals. Emerging research proposed the notion that successful goal attainment is in part attributable to the manner in which goal setting criteria are framed. For example, a person will perform better when set goals are challenging and specific as compared to goals that are challenging but vague (known as the goal-specificity effect). Emerging research also suggested a goal-proximity effect (wherein proximal goals lead to better performance than distal goals). The strategy of implementation intentions was developed on the basis of these findings.\n\nPeople generally have positive intentions, but often fail to act on them. The question is how to ensure that the set goal intentions will reliably lead to the desired goal-directed behaviors, and subsequent attainment of those goals. Implementation intentions offer a practical solution for such a problem.\n\nAchieving one's goals requires that certain goal-directed behaviors be instituted, but people are often unsuccessful in either initiating or maintaining these behaviors. The problems of initiating and maintaining goal-directed behavior can be addressed by using the implementation intention process. This if-then plan is a very specific approach as compared to goal intentions. A goal intention may be phrased in the following way: \"I want to reach X!\" Implementation intentions on the other hand are much more specific and seek to connect a future critical situation (an opportunity for goal attainment) with a specific goal-directed behavior, thereby leading to what could be called automatization in goal attainment. They are often phrased in the following way: \"When situation X arises, I will perform response Y!\" Where goal intentions are more general and abstract, implementation intentions are much more concrete and procedural.\n\nHaving formed a concrete plan involving a specific situation, this situation then becomes mentally represented and activated, leading to better perception, attention and memory concerning the critical situation. As a result, the chosen goal-directed behavior (the then-part of the plan) will be performed automatically and efficiently, without conscious effort. The automatization of the behavior in response to the future situation or cue, removes all hesitation and deliberation on the part of the decision maker when such a critical situation arises. This also has the effect of freeing cognitive resources for other mental processing tasks, and also for avoiding goal-threatening distractions or competing goals. It is also assumed that an implementation intention, once set, will continue operating non-consciously. This process is called strategic automaticity.\n\nThe strength of commitment related to both the plan set and the goal is very important for the implementation intention to have an effect on people's behavior. Without commitment, an implementation intention will hardly have any effect on goal-directed behavior.\n\nIn the phase model of action, the use of implementation intention takes place in the post-decisional phase (implemental mindset, volition is the driving force of action) which follows the predecisional phase (deliberative mindset, motivation is the driving force of setting goals). In the implemental mindset, a person is already committed to a goal and an implementation intention can be a good strategy to reach this goal.\n\nThe basic structure of an implementation intention is as follows:\n\nActively prompting individuals to make plans increases their likelihood of following through. Effective planning prompts guide people to consider when, where, and how they can act upon their intentions. Facilitating plan creation helps to increase follow-through for several reasons. First, it helps people to consider logistic obstacles and develop specific tactics to navigate around them. Second, the process helps to reduce the likelihood that someone will underestimate the time required by a task. Third, plan creation helps people to remember to act. Lastly, the formation of an action plan serves as a commitment to act. Research shows that breaking commitments generates discomfort. As such, plans are particularly effective when they are made as commitments to another person.\n\nTodd Rogers, Katherine L. Milkman, Leslie K. John, and Michael I. Norton (2015) suggest the following situations in which the use of planning prompts is most effective:\n\nImplementation intentions have been very successful in helping individuals achieve targeted and specific goals over the years, as indicated by a survey of the research.\n\nImplementation intentions can help to increase voter turnout. A study by David W. Nickerson and Todd Rogers (2010) found that voters in a high-salience election were more likely to vote when they received an implementation intentions phone call facilitating the creation of a voting plan. Voters who were contacted with an implementation intentions phone script were asked three questions to facilitate plan-making: what time they would vote, where they would be going to the voting place from, and what they would be doing immediately prior to voting. Voters who were contacted via this phone script were 4.1 percentage points more likely to vote than those who did not receive a call.\n\nPlanning prompts can also increase the likelihood that individuals will get flu shots. In a study conducted by Katherine L. Milkman, John Beshears, James J. Choi, David Laibson, and Brigitte C. Madrian (2011), randomly assigned employees at a Midwestern company received a mailing that prompted them to write down the date and time that they would get their flu shot, while the remaining employees received a letter with only the clinic information. Those who received the plan-making letter were 4 percentage points more likely to get a flu shot.\n\nImplementation intentions have been found to be particularly effective in unpleasant goal pursuits such as health-promotion (e.g. balanced and nutritious diet) and disease-prevention (e.g. daily exercise) behaviors, where there may be significant immediate costs and only long-term rewards. Of women who set themselves the goal of performing a breast self-examination over the next month, 100% actually did so if they were induced to form an implementation intention, compared to 53% of women who were not induced to form an implementation intention.\n\nIn a 2-month study investigating the effect of implementation intentions on weight loss, obese women between the ages of 18-76 were either instructed to create specific implementation intentions regarding their dieting and exercise (e.g. when, where, and what I will eat during the upcoming week; Where, when, how will I exercise during the upcoming week), or simply attend health, diet and stress-related group meetings. The women that were asked to create specific implementation intentions lost on average 4.2 kg, compared to those who only attended weekly group meetings, who on average lost only 2.1 kg over the 2-month period.\n\nIn another example, a study sought to increase the consumption of fruits and vegetables in a young-adult population. Participants who created \"if-then\" implementation intentions significantly increased reported fruit and vegetable intake by portions of 50% per day (over the course of one week), as compared to participants who made more global and less specific implementation intentions - reportedly consuming 31% more portions per day.\n\nIn 2009 Schweiger Gallo, Keil, Gollwitzer, Rockstroh and McCulloch published another study that was conducted to address the effectiveness of implementation intentions in regulating emotional reactivity.\n\nThe two studies required that disgust (in Study 1) and fear (in Study 2) eliciting stimuli were viewed by participants subject to three different self-regulation instructions:\n\n\nDisgust was selected because it is almost universally considered to be a basic emotion in the applicable literature. Fear was selected because anxiety disorders, such as panic disorders or phobias are common, and they affect the life of many people. The participants reported on the intensity of the elicited emotions by rating experienced arousal. Only group two, the implementation intention participants, succeeded in reducing their disgust and fear reactions compared to the other two groups.\n\nThese results support the idea that self-regulation by using simple goal intentions can run into problems when immediate and strong emotional reactivity has to be down-regulated, whereas implementation intentions appear to be an effective tool for self-regulation.\n\nImplementation intentions inhibit the automatic activation of stereotypical beliefs and prejudicial feelings. In a more recent study, the use of implementation intentions was applied to interactions with new acquaintances as well as interracial interactions. The notion is that interactions with new acquaintances can be laden with anxiety and also decrease interest and desire for long-term contact. The study found that implementation intentions actually increased interest in sustained contact during anxiety provoking interactions and also led to closer interpersonal distance in anticipation of interracial interactions. The results also suggest that anxiety itself was not reduced by means of implementation intentions, but rather, implementation intentions shielded individuals from the negative effects of anxiety during social interactions.\n\nThe earlier developments of the concept suggest that implementation intentions cause the mental representation of the anticipated situation to become highly activated and therefore easily accessible. The stronger the relationship between the cue or future situation and the predetermined behavior or response, the greater the success of initiating the desired goal-directed behavior. Since all components of the future behavior are predetermined (e.g. the when, where, and how), the association and relationship between said cue and behavior become automatic over time. That is, action initiation becomes immediate, efficient, and does not require conscious intent. In a more condensed explanation, implementation intentions automate action initiation.\n\nMore contemporary developments of the concept look not only at the initiatory aspects of implementation intentions, but look also at the longer-term phenomenon of behavior maintenance as it related to implementation intentions. The research suggests that implementation intentions result not only in an association between cue and behavior, but it's the act of planning into the future that actually serves as the foundation for this phenomenon. An experiment conducted by Papies et al., investigated the rate of goal completion by means of both implementation intentions and also the learning of cue-behavior association. Initially, both approaches led to the same rate of goal completion, but a week later, only the effect of implementation intentions was maintained. This lends evidence to the notion that implementation intentions rely on more complex mechanisms than simple cue-behavior associations, as was believed to be the case in earlier research.\n\nA large amount of research has been conducted on implementation intentions to better understand the questions relating to the initiation of goal striving. Unfortunately, the prior study of shielding of ongoing goal striving has been neglected in that research.\n\nOne study regarding this question was reported by Achtziger, Gollwitzer and Sheeran . It was shown in that study that implementation intentions can even assist people to shield goal striving from unwanted thoughts and feelings, such as cravings for junk food and from distracting thoughts. Two field experiments, concerning dieting (Study 1) and performance in sports (Study 2), have shown that there was a significant positive influence of implementation intentions on protecting ongoing goal striving. Participants who formed implementation intentions were more successful with long-term dieting, as well as concentration and performance in a tennis match. The focus on \"If-then-plans\" is the prevention of distracting thoughts and an efficient accomplishment of cognitive, motivational and emotional barriers of goal striving.\n\nAs these studies were run in \"everyday\" situations outside of an artificial laboratory, they possess a high external validity, and thus display the importance and meaningfulness of implementation intentions for everyday life.\n\nAs reported by Theodore A. Powers and colleagues, implementation intentions seem to have a negative effect on the performance in people who score high on socially prescribed perfectionism.\n\n"}
{"id": "4108478", "url": "https://en.wikipedia.org/wiki?curid=4108478", "title": "Information bias (psychology)", "text": "Information bias (psychology)\n\nInformation bias is a cognitive bias to seek information when it does not affect action. People can often make better predictions or choices with less information: more information is not always better. An example of information bias is believing that the more information that can be acquired to make a decision, the better, even if that extra information is irrelevant for the decision.\n\nIn an experiment , subjects considered this diagnostic problem involving fictitious diseases:\n\nA female patient is presenting symptoms and a history which both suggest a diagnosis of globoma, with about 80% probability. If it isn't globoma, it's either popitis or flapemia. Each disease has its own treatment which is ineffective against the other two diseases. A test called the ET scan would certainly yield a positive result if the patient had popitis, and a negative result if she has flapemia. If the patient has globoma, a positive and negative result are equally likely. If the ET scan was the only test you could do, should you do it? Why or why not?\n\nMany subjects answered that they would conduct the ET scan even if it were costly, and even if it were the only test that could be done. However, the test in question does not affect the course of action as to what treatment should be done. Because the probability of globoma is so high with a probability of 80%, the patient would be treated for globoma no matter what the test says. Globoma is the most probable disease before or after the ET scan.\n\nIn this example, we can calculate the value of the ET scan. Out of 100 patients, a total of 80 people will have globoma regardless of whether the ET scan is positive or negative. Since it is equally likely for a patient with globoma to have a positive or negative ET scan result, 40 people will have a positive ET scan and 40 people will have a negative ET scan, which totals to 80 people having globoma. This means that a total of 20 people will have either popitis or flapemia regardless of the result of the ET scan. The number of patients with globoma will always be greater than the number of patients with popitis or flapemia in either case of a positive or negative ET scan so the ET scan is useless in determining what disease to treat. The ET scan will indicate that globoma should be treated regardless of the result.\n\n"}
{"id": "1990178", "url": "https://en.wikipedia.org/wiki?curid=1990178", "title": "Information technology controls", "text": "Information technology controls\n\nIn business and accounting, information technology controls (or IT controls) are specific activities performed by persons or systems designed to ensure that business objectives are met. They are a subset of an enterprise's internal control. IT control objectives relate to the confidentiality, integrity, and availability of data and the overall management of the IT function of the business enterprise. IT controls are often described in two categories: IT general controls (ITGC) and IT application controls. ITGC include controls over the Information Technology (IT) environment, computer operations, access to programs and data, program development and program changes. IT application controls refer to transaction processing controls, sometimes called \"input-processing-output\" controls. Information technology controls have been given increased prominence in corporations listed in the United States by the Sarbanes-Oxley Act. The COBIT Framework (Control Objectives for Information Technology) is a widely used framework promulgated by the IT Governance Institute, which defines a variety of ITGC and application control objectives and recommended evaluation approaches. IT departments in organizations are often led by a Chief Information Officer (CIO), who is responsible for ensuring effective information technology controls are utilized.\n\nITGC represent the foundation of the IT control structure. They help ensure the reliability of data generated by IT systems and support the assertion that systems operate as intended and that output is reliable. ITGC usually include the following types of controls:\n\nIT application or program controls are fully automated (i.e., performed automatically by the systems) designed to ensure the complete and accurate processing of data, from input through output. These controls vary based on the business purpose of the specific application. These controls may also help ensure the privacy and security of data transmitted between applications. Categories of IT application controls may include:\n\n\nThe organization's Chief Information Officer (CIO) or Chief Information Security Officer (CISO) is typically responsible for the security, accuracy and the reliability of the systems that manage and report the company's data, including financial data. Financial accounting and enterprise resource planning systems are integrated in the initiating, authorizing, processing, and reporting of financial data and may be involved in Sarbanes-Oxley compliance, to the extent they mitigate specific financial risks.\n\nCOBIT is a widely utilized framework containing best practices for both ITGC and application controls. It consists of domains and processes. The basic structure indicates that IT processes satisfy business requirements, which is enabled by specific IT control activities. It also recommends best practices and methods of evaluation of an enterprise's IT controls.\n\nThe Committee of Sponsoring Organizations of the Treadway Commission (COSO) identifies five components of internal control: control environment, risk assessment, control activities, information and communication and monitoring, that need to be in place to achieve financial reporting and disclosure objectives; COBIT provide a similar detailed guidance for IT, while the interrelated Val IT concentrates on higher-level IT governance and value-for-money issues. The five components of COSO can be visualized as the horizontal layers of a three-dimensional cube, with the COBIT objective domains-applying to each individually and in aggregate. The four COBIT major domains are: plan and organize, acquire and implement, deliver and support, and monitor and evaluate.\n\nSOX (part of United States federal law) requires the chief executive and chief financial officers of public companies to attest to the accuracy of financial reports (Section 302) and require public companies to establish adequate internal controls over financial reporting (Section 404). Passage of SOX resulted in an increased focus on IT controls, as these support financial processing and therefore fall into the scope of management's assessment of internal control under Section 404 of SOX. \n\nThe COBIT framework may be used to assist with SOX compliance, although COBIT is considerably wider in scope. The 2007 SOX guidance from the PCAOB and SEC state that IT controls should only be part of the SOX 404 assessment to the extent that specific financial risks are addressed, which significantly reduces the scope of IT controls required in the assessment. This scoping decision is part of the entity's SOX 404 top-down risk assessment. In addition, Statements on Auditing Standards No. 109 (SAS109) discusses the IT risks and control objectives pertinent to a financial audit and is referenced by the SOX guidance.\n\nIT controls that typically fall under the scope of a SOX 404 assessment may include:\n\n\nSpecific activities that may occur to support the assessment of the key controls above include:\n\n\nTo comply with Sarbanes-Oxley, organizations must understand how the financial reporting process works and must be able to identify the areas where technology plays a critical part. In considering which controls to include in the program, organizations should recognize that IT controls can have a direct or indirect impact on the financial reporting process. For instance, IT application controls that ensure completeness of transactions can be directly related to financial assertions. Access controls, on the other hand, exist within these applications or within their supporting systems, such as databases, networks and operating systems, are equally important, but do not directly align to a financial assertion. Application controls are generally aligned with a business process that gives rise to financial reports. While there are many IT systems operating within an organization, Sarbanes-Oxley compliance only focuses on those that are associated with a significant account or related business process and mitigate specific material financial risks. This focus on risk enables management to significantly reduce the scope of IT general control testing in 2007 relative to prior years.\n\nSection 409 requires public companies to disclose information about material changes in their financial condition or operations on a rapid basis. Companies need to determine whether their existing financial systems, such as enterprise resource management applications are capable of providing data in real time, or if the organization will need to add such capabilities or use specialty software to access the data. Companies must also account for changes that occur externally, such as changes by customers or business partners that could materially impact its own financial positioning (e.g. key customer/supplier bankruptcy and default).\n\nTo comply with Section 409, organizations should assess their technological capabilities in the following categories: \n\nSection 802 of Sarbanes-Oxley requires public companies and their public accounting firms to maintain all audit or review work papers for a period of five years from the end of the fiscal period in which the audit or review was concluded. This includes electronic records which are created, sent, or received in connection with an audit or review. As external auditors rely to a certain extent on the work of internal audit, it would imply that internal audit records must also comply with Section 802.\n\nIn conjunction with document retention, another issue is that of the security of storage media and how well electronic documents are protected for both current and future use. The five-year record retention requirement means that current technology must be able to support what was stored five years ago. Due to rapid changes in technology, some of today’s media might be outdated in the next three or five years. Audit data retained today may not be retrievable not because of data degradation, but because of obsolete equipment and storage media.\n\nSection 802 expects organizations to respond to questions on the management of SOX content. IT-related issues include policy and standards on record retention, protection and destruction, online storage, audit trails, integration with an enterprise repository, market technology, SOX software and more. In addition, organizations should be prepared to defend the quality of their records management program (RM); comprehensiveness of RM (i.e. paper, electronic, transactional communications, which includes emails, instant messages, and spreadsheets that are used to analyze financial results), adequacy of retention life cycle, immutability of RM practices, audit trails and the accessibility and control of RM content.\n\nPC-based spreadsheets or databases are often used to provide critical data or calculations related to financial risk areas within the scope of a SOX 404 assessment. Financial spreadsheets are often categorized as end-user computing (EUC) tools that have historically been absent traditional IT controls. They can support complex calculations and provide significant flexibility. However, with flexibility and power comes the risk of errors, an increased potential for fraud, and misuse for critical spreadsheets not following the software development lifecycle (e.g. design, develop, test, validate, deploy). To remediate and control spreadsheets, public organizations may implement controls such as:\n\n\nResponsibility for control over spreadsheets is a shared responsibility with the business users and IT. The IT organization is typically concerned with providing a secure shared drive for storage of the spreadsheets and data backup. The business personnel are responsible for the remainder.\n\n\n"}
{"id": "221299", "url": "https://en.wikipedia.org/wiki?curid=221299", "title": "Intertemporal choice", "text": "Intertemporal choice\n\nIntertemporal choice is the process by which people make decisions about what and how much to do at various points in time, when choices at one time influence the possibilities available at other points in time. These choices are influenced by the relative value people assign to two or more payoffs at different points in time. Most choices require decision-makers to trade off costs and benefits at different points in time. These decisions may be about saving, work effort, education, nutrition, exercise, health care and so forth. \n\nSince early in the twentieth century, economists have analyzed intertemporal decisions using the discounted utility model, which assumes that people evaluate the pleasures and pains resulting from a decision in much the same way that financial markets evaluate losses and gains, exponentially 'discounting' the value of outcomes according to how delayed they are in time. Discounted utility has been used to describe how people actually make intertemporal choices and it has been used as a tool for public policy. Policy decisions about how much to spend on research and development, health and education all depend on the discount rate used to analyze the decision.\n\nIntertemporal portfolio choice is the allocation of funds to various assets repeatedly over time, with the amount of investable funds at any future time depending on the portfolio returns at any prior time. Thus the future decisions may depend on the results of current decisions. In general this dependence on prior decisions implies that current decisions must take into account their probabilistic effect on future portfolio constraints. There are some exceptions to this, however: with a logarithmic utility function, or with a HARA utility function and serial independence of returns, it is optimal to act with (rational) myopia, ignoring the effects of current decisions on the future decisions.\n\nThe Keynesian consumption function was based on two major hypotheses. Firstly, the marginal propensity to consume lies between 0 and 1. Secondly, the average propensity to consume falls as income rises. Early empirical studies were consistent with these hypotheses. However, after World War II it was observed that saving did not rise as income rose. The Keynesian model therefore failed to explain the consumption phenomenon, and thus the theory of intertemporal choice was developed. The analysis of intertemporal choice was introduced by John Rae in 1834 in the \"Sociological Theory of Capital\". Later, Eugen von Böhm-Bawerk in 1889 and Irving Fisher in 1930 elaborated on the model. A few other models based on intertemporal choice include the Life Cycle Income Hypothesis proposed by Franco Modigliani and the Permanent Income Hypothesis proposed by Milton Friedman. The concept of Walrasian equilibrium may also be extended to incorporate intertemporal choice. The Walrasian analysis of such an equilibrium introduces two \"new\" concepts of prices: futures prices and spot prices.\n\nIrving Fisher developed the theory of intertemporal choice in his book \"Theory of interest\" (1930). Contrary to Keynes, who related consumption to current income, Fisher's model showed how rational forward looking consumers choose consumption for the present and future to maximize their lifetime satisfaction.\n\nAccording to Fisher, an individual's impatience depends on four characteristics of his income stream: the size, the time shape, the composition and risk. Besides this, foresight, self-control, habit, expectation of life, and bequest motive (or concern for lives of others) are the five personal factors that determine a person's impatience which in turn determines his time preference.\n\nIn order to understand the choice exercised by a consumer across different periods of time we take consumption in one period as a composite commodity. Suppose there is one consumer, formula_1 commodities, and two periods. Preferences are given by formula_2 where formula_3. Income in period formula_4 is formula_5. Savings in period 1 is formula_6, spending in period formula_4 is formula_8, and formula_9 is the interest rate. If the person is unable to borrow against future income in the first period, then he is subject to separate budget constraints in each period:\n\nOn the other hand, if such borrowing is possible then the person is subject to a single intertemporal budget constraint:\n\nThe left hand side shows the present value of expenditure and right hand side depicts the present value of income. Multiplying the equation by formula_13 would give us the corresponding future values.\n\nNow the consumer has to choose a formula_14 and formula_15 so as to\n\nA consumer may be a net saver or a net borrower. If he's initially at a level of consumption where he's neither a net borrower nor a net saver, an increase in income may make him a net saver or a net borrower depending on his preferences. An increase in current income or future income will increase current and future consumption(consumption smoothing motives).\n\nNow, let us consider a scenario where the interest rates are increased. If the consumer is a net saver, he will save more in the current period due to the substitution effect and consume more in the current period due to the income effect. The net effect thus becomes ambiguous. If the consumer is a net borrower, however, he will tend to consume less in the current period due to the substitution effect and income effect thereby reducing his overall current consumption.\n\nThe life cycle hypothesis is based on the following model:\n\nsubject to\n\nwhere\n\nTypically, a person's MPC (marginal propensity to consume) is relatively high during young adulthood, decreases during the middle-age years, and increases when the person is near or in retirement. The Life Cycle Hypothesis(LCH) model defines individual behavior as an attempt to smooth out consumption patterns over one's lifetime somewhat independent of current levels of income. This model states that early in one's life consumption expenditure may very well exceed income as the individual may be making major purchases related to buying a new home, starting a family, and beginning a career. At this stage in life the individual will borrow from the future to support these expenditure needs. In mid-life however, these expenditure patterns begin to level off and are supported or perhaps exceeded by increases in income.\nAt this stage the individual repays any past borrowings and begins to save for her or his retirement.\nUpon retirement, consumption expenditure may begin to decline however income usually declines dramatically. In this stage of life, the individual dis-saves or lives off past savings until death.\n\nAfter the Second World War, it was noticed that a model in which current consumption was just a function of current income clearly was too simplistic. It could not explain the fact that the long-run average propensity to consume seemed to be roughly constant despite the marginal propensity to consume being much lower. Thus Milton Friedman's permanent income hypothesis is one of the models which seeks to explain this apparent contradiction.\n\nAccording to the permanent income hypothesis, permanent consumption, \"C\", is proportional to permanent income, \"Y\". Permanent income is a subjective notion of likely average future income. Permanent consumption is a similar notion of consumption.\n\nActual consumption, \"C\", and actual income, \"Y\", consist of these permanent components plus unanticipated transitory components, \"C\" and \"Y\", respectively:\n\nThe choice of an individual as to how much labor to currently supply involves a trade-off between current labor and leisure. The amount of labor currently supplied influences not only current consumption opportunities but also future ones, and in particular influences the future choice of when to retire and supply no more labor. Thus the current labor supply choice is an intertemporal choice.\n\nFixed investment is the purchasing by firms of newly produced machinery, factories, and the like. The reason for such purchases is to increase the amount of output that can potentially be produced at various times in the future, so this is an intertemporal choice.\n\nThe article so far has considered cases where individuals make intertemporal choices by considering the present discounted value of their consumption and income. Every period in the future is exponentially discounted with the same interest rate. A different class of economists, however, argue that individuals are often affected by what is called the temporal myopia. The consumer's typical response to uncertainty in this case is to sharply reduce the importance of the future of their decision making.This effect is called hyperbolic discounting. In the common tongue it reflects the sentiment “Eat, drink and be merry, for tomorrow we may die.”\n\nMathematically, it may be represented as follows:\n\nwhere\n\nWhen choosing between $100 or $110 a day later, individuals may impatiently choose the immediate $100 rather than wait for tomorrow for an extra $10. Yet, when choosing between $100 in a month or $110 in a month and a day, many of these people will reverse their preferences and now patiently choose to wait the additional day for the extra $10.\n\n"}
{"id": "782795", "url": "https://en.wikipedia.org/wiki?curid=782795", "title": "Iso-elastic", "text": "Iso-elastic\n\nIn engineering, iso-elastic refers to a system of elastic and tensile parts (springs and pulleys) which are arranged in a configuration which serves to isolate physical motion at one end from affecting the same motion at the other end.\nThis type of device must be able to maintain angular direction and load-bearing over a large range of motion.\n\nThe most prominent use of an iso-elastic system is in the supporting armature of a Steadicam.\nThe Steadicam arm is used to isolate a film or video camera from the operator's movements. \nSteadicam arms all work in a fashion similar to a spring lamp since each arm has 2 sections (similar to and labelled like a human arm), both the upper and fore-arm sections consist of a parallelogram with diagonal iso-elastic cable-pulley-spring system. The iso-elastic system is tensioned to counteract the weight of the camera and steadicam sled. This tensioning allows the camera and operator to move vertically and independently of each other. For example, as the operator runs, the bouncing of his body is absorbed by the springs, keeping the camera steady. The arm also has unsprung hinges at both ends of each arm allowing it to bend in the horizontal plane (just like your elbow, not like a spring lamp).\n\nTo understand how an iso-elastic system works, we must first understand how springs work. The tension (elastic force) in a spring is proportional to its extension according to Hooke's law. This means that if a weight is hung on a spring it will oscillate with simple harmonic motion. This is because when the weight is above the balance point, the spring's tension is reduced so the weight falls due to gravity, and when the weight is below the balance point the spring's tension will pull it back upwards.\n\nIf a simple spring system were used in a steadicam, then if the operator moved vertically, the camera would execute simple harmonic motion, and bounce up and down. Instead, an iso-elastic system is employed.\n\nThe springs used are large, stiff springs with a high modulus of elasticity, and they are highly tensioned. A compound pulley system is then used so that the large force exerted by the spring can be divided by a factor of (say) five. The cable exiting the pulley system will have a moderate tension on it, but most importantly, when the cable is drawn in or out, and the extension of the spring changes by only a fifth of that distance, so that the tension force of the spring will not change much. The result is that the spring-pulley system can produce a fairly constant tension in the cable over a large range of movement.\n\nThe almost constant force exerted by an iso-elastic system is employed in the armature of a steadicam, to counteract the constant force of gravity on the camera's and mount's mass. The result is that the weight of the camera is almost exactly balanced by the tension force throughout the entire range of vertical movement, so even when the operator jumps vertically, the camera will retain its vertical position due to inertia, but remain balanced, just with the armature at a different angle.\n\nAs a result, the camera doesn't bounce up to the 'balanced' position after a move, for example when the operator steps up onto a curb from the road. This allows the camera to be more isolated and independent of the operator's moves. The operator can of course deliberately move the camera up or down, if desired. In reality however camera operators find it preferable for the arm to not be perfectly iso-elastic so that the camera will naturally rise to a comfortable operating height. Though the springs will be tensioned so this only happens very slowly and without bouncing up and down so as to maintain the smoothness of the camera's motion.\"\n\n"}
{"id": "13504122", "url": "https://en.wikipedia.org/wiki?curid=13504122", "title": "Kaula (Hinduism)", "text": "Kaula (Hinduism)\n\nKaula, also known as Kula, ' (\"the Kula practice\") and ' (\"the Kaula conduct\"), is a religious tradition in Shaktism and tantric Shaivism characterised by distinctive rituals and symbolism connected with the worship of Shakti. It flourished in India primarily in the first millennium AD.\n\nKaula preserves some of the distinctive features of the Kāpālika tradition, from which it is derived. It is subdivided into four subcategories of texts based on the goddesses Kuleśvarī, Kubjikā, Kālī and Tripurasundarī respectively. The Trika texts are closely related to the Kuleśvarī texts and can be considered as part of the Kulamārga.\n\nIn later Hatha Yoga, the Kaula visualization of Kuṇḍalini rising through a system of chakras is overlaid onto the earlier bindu-oriented system.\n\nThe translation of the term \"Kula\" in English is considered difficult and has raised some problems for researchers. The basic meaning is \"family\", \"group\" or \"self-contained unit\". This is explained by Flood as referring to the retinues of minor goddesses depicted in the schools' literature.\n\nPhilosophically the term is said to represent a unifying connectedness, beneath the various objects, processes and living entities of this world, which may be identified with these goddesses as aspects of the supreme deity, in some regions the god Shiva, elsewhere a goddess. Another meaning sometimes given to the term \"kaula\" is that of a \"group of people\" engaged together in the practice of spiritual discipline.\n\nKaula practices are based on tantra, closely related to the siddha tradition and shaktism. Kaula sects are noted for their extreme exponents who recommend the flouting of taboos and social mores as a means of liberation. Such practices were often later toned down to appeal to ordinary householders, as in Kaśmiri Śaivism.\n\nThe concepts of purity, sacrifice, freedom, the spiritual master (\"guru\") and \"the heart\" are core concepts of the Kaula tradition.\n\nActions or objects are not seen impure in themselves, rather the attitude is the determinant factor. Spiritual ignorance is the only impurity and knowledge is pure. As long as one is identified with the supreme consciousness, there is nothing impure. The adept is unaffected by any external impurity and makes use of what is reprehensible to attain transcendence. Here arises the antinomian and asocial character of Kaula and the left-handed forms of Tantra.\n\n\"Kaula\" sacrifice (\"yajna\") is defined primarily as an inward act. Any action performed with the purpose of evoking the supreme reality is said to be sacrifice. However, if sacrifice were performed only interiorly, there would be a lack of externality and therefore limitation and dualism. That is why \"Kaula\" adepts also perform symbolic external sacrifices making use of a sacred place and various rituals.\n\nThere are six main types of sacrifice according to the \"six supports\"; external reality, the couple, the body, the central channel of the subtle breath (susumna), the mind and \"Shakti\".\n\n\"Kaula\" stresses the language of self-sufficiency, liberation and freedom. Socially the \"Kaula\" may be viewed as an alternative society, complete in itself, which supports the freedom of the devotee from interior mental and egotistic limitations and from exterior social and cultural preconceptions.\n\nAt a social level deconditioning is realized by detaching from traditional restrictions with regard to what is considered pure and impure and through the adoption of the spiritual family of the guru. At the mental level freedom is attained by the awakening of \"Kundalini\" through \"asana\", \"pranayama\", \"mudra\" or \"mantras\", the amplification and sublimation of the vital and mental energy and the elevation of consciousness. The culmination of this process is spiritual illumination.\n\nAbsolute freedom is to be found only in the revelation of the unity of the spirit with God, a state described as \"Atma-vyapti\" or re-absorption into the true Self (\"atman\") or \"Shiva-vyapti\": re-absorption into the supreme consciousness of \"Shiva\". To be free is to be absolved from the necessity of rebirth conditioned by karmic restraints. Consciousness expands into the so-called pure reality, a level that is considered to exist beyond time and space, where the powers of knowledge and action are unfettered, there are no conditioning desires or needs to be fulfilled and bliss is directly present in consciousness.\n\n\"Kaula's\" basic method is the experience of the freedom of consciousness in the heart, ultimately reflected in the center of the being as \"Kechari Mudra\". This mudra (attitude) means \"the ability of consciousness to freely move (\"charati\") about in the space (\"kha\") of the heart\". The disciple learns to recognize Śiva as the ultimate reality. The practices pertaining to consciousness are explained in such texts as \"Vijñāna Bhairava Tantra\", \"Spanda Kārikās\" and \"Śiva Sūtras\".\n\nKashmiri Shaivism describes freedom as \"svātantrya\" - the freedom to create, maintain and destroy the universe pertaining to Śiva himself. It is considered that Śiva, above any restriction or conditioning, creates the universe of his free will as a playful expression of his spirit (lila). Here the kaulas are unlike Advaita and Veda, where there is the conception that maya (cosmic illusion) is superimposed upon the brahman (absolute), inducing a sort of illusory creation. Here, creation is considered real, and the will to create is considered free and unfettered. \"Svatantrya\" is identical to Ananda (bliss) and \"vimarśa\" (reflexive consciousness/auto-consciousness).\n\n\"Guru is the path\" (\"\"). This statement from the most revered sacred text of Kashmir Shaivism, the \"Śiva Sutras\", summarizes the school's conception of the guru-disciple relationship. \"Kaula\" functions as a form of guru yoga, where the disciple's only essential practice is to surrender himself to his guru, accepting the spiritual impulse bestowed upon him by his master. Disciples eminently open towards their guru's spiritual influence are named \"spiritual sons\" and held to know the highest state of consciousness by their direct link to their guru's illuminated heart.\n\nThe guru is considered to form a single Self (atman) with his disciples. As such, he leads the disciples to the discovery of their own \"Atman\" with his own consciousness, exalted into the supreme state. Like fire kindled from a candle to another candle, the revelation of the self is passed from master to disciple directly, not through words or exterior practices, but mediated by the direct transfer of \"śakti\".\n\nAham, the heart or \"subjective I\", is a central concept in \"Kaula\" ideology, conceived of as the most sacred reality, home of consciousness (\"Cit\") and bliss (\"Ananda\"), place of union of the cosmic couple Shiva and Shakti. The term \"Aham\" refers to the same reality as other terms like \"anuttara\" (unsurpassed), \"Akula\" (beyond the group), \"Shiva\" (The Lord), \"Cit\" (supreme consciousness) as well as \"feminine\" aspects as Ananda and \"Shakti\". Each term brings a specific viewpoint, but none of them can fully describe the Supreme Reality.\n\nOn the individual level, the heart is the binding force of all conscious experiences – the individual being is considered a \"Kula\" composed of eight elements: five senses, ego (\"ahamkar\"), the mind and the intellect. These eight are not disconnected, unrelated processes but rather a unified, interrelated family (\"kaula\") based on consciousness as the common substrate. \"Kaula\" prescribes practices that reintegrate the eight \"rays\" of the soul into the supreme consciousness.\n\nOn the cosmic level, the \"Heart of the Lord\" (\"aham\") is the substrate of the family of 36 elements forming all manifestation. The concept of \"Spiritual Heart\" is so important that even the supreme realization in Kashmir Shaivism is described in relation to it. The so-called \"Kechari Mudra\" is an attitude described as \"the ability of consciousness to freely move (\"charati\") about in the space (\"kha\") of the heart\". (\"kha\"+\"charati\" forming \"kechari\")\n\nSimilarly to other tantric schools, \"Kaula\" chooses a positive (affirmative) approach: instead of prescribing self-limitations and condemning various actions, it embraces such actions in a spiritual light. Thus, sexuality, love, social life and artistic pursuits are considered vectors of spiritual evolution. The main focus in \"Kaula\" is on practical methods for attaining enlightenment, rather than engaging in complex philosophical debate. Whatever is pleasant and positive can be integrated in its practice.\n\nThe principal means employed in the \"Kaula\" practice are the spiritual family, the practice of initiation rituals, the couple (sexual rituals such as maithuna), the body (spiritual alchemy inside one's own body), the energy (shakti) (controlled especially through the use of mantras and mystical phonemes) and the consciousness (seen as the epitome of one's whole being and of the universe itself).\n\nThe first phase of development is linked to the attainment of a state of non-duality described as an \"absorption into the spiritual heart\", \"nirvikalpa samadhi\" or experiencing the \"uncreated light\" of consciousness (\"prakāśa\") (read a number of subjective accounts of this experience).\n\nGroup practices, which are restricted to the members of a \"kaula\" (family), include rituals, festivities, initiations and the secretive tantric sexual union. The purposes of this practice are the initiation of novices, the expansion of consciousness and expression of the bliss already attained as participants become more and more advanced.\n\nThe key to the effectiveness of group practice is held to reside in the harmony of minds and hearts of the participants. When a compatible spiritual group is created, it can greatly accelerate the spiritual evolution of its members. Abhinavagupta declares that such a group can enter a state of oneness and universal consciousness without effort. He explains this by the notion of reflection (\"pratibimba\"), a process of unification, an effortless overflow of spiritual energy.\n\nThe relation between a \"Kaula's\" parts is realized through mutual reflection. Reflection (\"pratibimba\") is used here in the sense of \"containing an image of the other objects inside\", a concept similar to that of the hologram. The number of possible interactions and reflections between the members of a \"Kaula\" is much larger than the number of elements it contains. Kashmir Shaivism declares that each part is in fact \"Akula\" (Shiva) in essence; thus there is a connection between the parts through their common \"Akula\" substrate. As each part contains \"Akula\", in its essence, it automatically contains everything, this is how the mutual reflection is said to be realized.\n\nAlmost half of the Tantraloka is dedicated to rituals, usually evoking the union of complementary sets such as man and woman, a faculty and its object or inhalation and exhalation. The practice of ritual may involve the construction of a mandala, visualization of a goddess or group of goddesses (\"Śakti\"), recitation (japa), performed in a state of \"rest inside the creative awareness\" (\"camatkāra\"), oblation into fire and its internalized version – the burning of the objects and means of knowledge into the \"fire\" of non-dual consciousness (\"parāmarśa\").\n\nThe power of a ritual lies in its repetition. A pure disciple will attain the supreme state even by simply staying for a short time in presence of a guru without any instruction, but less prepared ones need reinforcement and gradual accumulation.\n\n\"Kaula\" puts a special emphasis on the physical body in spiritual practice \"as a vessel of the Supreme\" and, as such, not an obstacle tortured in ascetic practices. Repeated submergence into the state of non-duality is supposed to induce secondary effects on the physical body due to the activity of the spiritual energy (\"śakti\") and may be called tantric body alchemy (see internal alchemy). Starting from the expanded consciousness of the self (\"atman\"), the body (and in the end, the exterior reality too) is infused with the experience of non-duality.\n\nThe non-dual, experienced initially only in consciousness, is extended to the whole body. The \"kaula\" adept will discover \"kaulika\" – the power (\"siddhi\") of identification with the Universal Consciousness experienced in the physical body, generated spontaneously, without any effort (formal meditation, postures – \"asana\", concentration \"Dharana\" and other forms of exertion in yoga). This process is described as the descent of the energy of the non-dual consciousness into the physical. Then consciousness manifests as a free force, entering the senses and producing extroverted \"samādhi\". At this point, consciousness, mind, senses and physical body are \"dissolved\" into oneness, expanded into the spiritual light of consciousness.\n\nAs a consequence, any perception of the exterior reality becomes nondual. It becomes possible to live submerged in a continuous state of union with Shiva even while performing regular day-to-day activities. This form of extroverted, all inclusive \"samādhi\" is the pinnacle of spiritual evolution, \"bhairavi mudra\", \"jagadananda\" or \"bhava samadhi\". The yogi experiences everything as pure light and ecstasy (\"cit-ananda\") and does not feel any difference between interior and exterior any more.\n\nAbhinavagupta: \"The couple (\"yamala\") is consciousness itself, the unifying emission and the stable abode. It is the absolute, the noble cosmic bliss consisting of both Shiva and Shakti. It is the supreme secret of \"Kula\"; neither quiescent nor emergent, it is the flowing font of both quiescence and emergence.\" (Tantraloka)\n\nThe sexual practices of the \"Kaula\" schools, also known as \"the secret ritual\", are performed with a so-called \"external Shakti\" (sexual partner) as opposed to the purely meditative practices which involve only one's own spiritual energies (the \"interior Shakti\"). The role of the sexual \"Kaula\" ritual is to unite the couple, yogini (initiated woman) and siddha (initiated man), and induce one in the other a state of permanent awakening. This achievement is made possible by the intensity of their love.\n\nIn their exalted state, the two become absorbed into the consciousness of the Self. Becoming united on all the levels, physical, astral, mental and even in their consciousness, they reconstitute the supreme couple of Shiva and Shakti.\n\nThe \"Kaula\" sacrifice is reserved for the few, the elite who can maintain a state of \"Bhairava\" (spiritual illumination) in sexual union. Other couples, even if they reproduce the ritual to the letter (as perceived from outside), if they do not attain \"Bhairava\" consciousness, are merely engaging in a sexual act.\n\n\"Initiation by the mouth of the yogini (\"yoginī-vaktra\")\", is a method by which the adept unites with a purified \"yoginī\" and receives the unique experience of the illuminated consciousness. He is to see her as both his lover and guru.\n\nThe energy generated during the tantric sexual act is considered a form of \"subtle emission\", while the act of ejaculation is considered a form of physical emission. In Kashmir Shaivism, the energy of emission (\"visarga śakti\") is considered to be a form of \"ānanda\" (bliss).\n\nDepending on the orientation of one's consciousness, introverted or extroverted, emission can be of two kinds: rested and risen. In \"Śānta\", the rested form of emission, focus is absorbed just on one's own Self in an act of transcendence. In \"Udita\", the risen form, the focus is projected on the Self (\"atman\") of one's lover – a state associated with immanence.\n\n\"Santodita\" (beyond \"udita\" and \"śānta\") is the uniting form, cause of both \"śānta\" and \"udita\" emissions. \"Santodita\" is described as universal bliss (\"cidānanda\"), undivided consciousness, \"kaula\" (the group of two as one) and an \"outflow of the pulsation of Shiva and Shakti\". This kind of translation from the physical act to the mental and to consciousness itself is a characteristic of the tantric world view.\n\nMantric meditation is the most common form of tantric practice. In the \"Kaula\" system, this practice is associated especially with the group of phonemes. The 50 phonemes (\"varṇa\") of the Sanskrit alphabet are used as \"seed\" mantras denoting various aspects of consciousness (\"cit\") and energy (\"śakti\"). The group (\"kula\") of Sanskrit phonemes form a complete description of reality, from the lowest (\"earth\") to the highest (Śiva consciousness) level.\n\nThe ritual \"setting out\" of the phonemes imitates the emanation of the cosmos from the supreme I-consciousness of Śiva. In another ritual, the phonemes are identified with specific zones of the body through the practice of \"nyāsa\", infusing the body with spiritual energy. This mystical state of culminates in the \"kaula\" of the body (perfection of the ensemble of organs, senses and mind) and such a being is known as a siddha (accomplished one). The adept attains a form of bodily enlightenment where, through the power of mantras, he comes to recognize the divinities within the body.\n\nInitiation into mantric practice is based on a transfer of power and the link (lineage) of the heart. The word or phoneme is not useful in itself, as it does not have efficiency unless the disciple received his initiation from an authentic master.\n\nWhile the manifest reality is described as \"Kula\" (a variant form of the term \"Kaula\"), the unifying factor, the Deity, is termed \"Akula\". \"A\" means \"beyond\", or \"non\", thus \"Akula\" is \"beyond kula\". As the substrate of all manifestation is \"Akula\", such is also the basis of any \"Kula\". So \"Kula\" families are united by a common substrate, the transcendent \"Akula\".\n\nIn every one of its instances, on various levels of the universe, \"Kula\" is a contraction (\"saṃkoca\") of totality, thus in each \"Kula\" there is a contracted form of the universe, a contracted form of Shiva (\"Akula\") himself. Such an affirmation has been popularized under slogans like \"Consciousness is Everything\" in some recent Kashmir Shaivism related publications for the public.\n\nOften at the highest level of reality Shiva and Shakti form the supreme couple, or the ultimate \"Kula\" (family). Shiva, under various names (\"anuttara\" - absolute, \"prakāśa\" - uncreated light, \"cit\" - supreme consciousness, \"Akula\" - beyond the groups of manifestation) and \"Shakti\", under a similar plethora or names (\"Vimarsa\" - reflection in consciousness, \"Visarga\" - creative energy that emits the Universe, \"Kundalini\" - fundamental energy of the body, \"spanda\" - atemporal vibration, \"Kauliki\" - that which is \"sprung\" in \"Kula\"). The two are always in indissoluble union in a perfect state of bliss. Ultimately there is no difference between Shiva and Shakti, they are different aspects of the same reality. The supreme \"family\" by definition spans both manifestation and transcendence.\n\nIn Kashmir Shaivism, Supreme Consciousness (\"Cit\", identical to \"Akula\") is considered to be the substrate of manifestation. Consciousness is the ultimate principle, the monad of the universe, always present as substrate in every external object, be it gross (physical), subtle (mental) or subtlest (relating to the causal body or soul). Thus external world, body, mind and soul are considered kindred parts of the whole, concretisation of the supreme \"consciousness\". From this perspective, \"Kula\" is the totality of manifestation, in gross, subtle and supreme form. Even if \"Cit\" is not directly involved in the process of manifestation (as it is said to be unmanifest), it is always present in every possible facet of manifestation.\nThus, it is said to be the substantial cause of manifestation (manifestation is made of \"Cit\", \"like pots are made of clay\") and also the efficient cause (\"like the potter is the efficient cause in the activity of creating pots\").\n\nA closely related concept is \"Kaulika\", the binding force of the \"Kula\". The term literally means \"sprung in \"Kula\"\". \"Kaulika\" is another name for \"Shakti\", the spiritual energy. \"Shakti\", as described in Kashmir Shaivism, does a paradoxical thing – she creates the universe, with all its diversity and at the same time remains identical to \"Shiva\", the absolute transcendent. Thus, \"Kaulika\" is an energy both of spirit and matter. Bridging the two, \"Kaulika\" creates the path of evolution for consciousness from ego to spirit.\n\nThe manifestation of \"Kaulika\" proceeds from the absolute (\"anuttara\") in the process of cosmic creation (\"mahasristi\"). Thus \"Kaulika\" should not be seen as mere energy, or just the link between matter and spirit, but also identical to the absolute. Even if she is the dynamic aspect of the absolute, she does not rank lower than Shiva, her divine consort.\n\nKashmiri School of Kaula\n\nWhile Kaula is primarily an oral tradition and does not place a high value on the creation of texts, there are some texts associated with the tradition. Muller-Ortega, following Pandey, summarizes the literature of the Kashmiri school as follows:\n\n"}
{"id": "64204", "url": "https://en.wikipedia.org/wiki?curid=64204", "title": "Kinetic theory of gases", "text": "Kinetic theory of gases\n\nThe kinetic theory of gases describes a gas as a large number of submicroscopic particles (atoms or molecules), all of which are in constant, rapid, random motion. The randomness arises from the particles' many collisions with each other and with the walls of the container.\n\nKinetic theory of gases explains the macroscopic properties of gases, such as pressure, temperature, viscosity, thermal conductivity, and volume, by considering their molecular composition and motion. The theory posits that gas pressure results from particles' collisions with the walls of a container at different velocities.\n\nKinetic molecular theory defines temperature in its own way, in contrast with the thermodynamic definition.\n\nUnder an optical microscope, the molecules making up a liquid are too small to be visible. However, the jittery motion of pollen grains or dust particles in liquid are visible. Known as Brownian motion, the motion of the pollen or dust results from their collisions with the liquid's molecules.\n\nIn approximately 50 BCE, the Roman philosopher Lucretius proposed that apparently static macroscopic bodies were composed on a small scale of rapidly moving atoms all bouncing off each other. This Epicurean atomistic point of view was rarely considered in the subsequent centuries, when Aristotlean ideas were dominant.\n\nIn 1738 Daniel Bernoulli published \"Hydrodynamica\", which laid the basis for the kinetic theory of gases. In this work, Bernoulli posited the argument, still used to this day, that gases consist of great numbers of molecules moving in all directions, that their impact on a surface causes the gas pressure that we feel, and that what we experience as heat is simply the kinetic energy of their motion. The theory was not immediately accepted, in part because conservation of energy had not yet been established, and it was not obvious to physicists how the collisions between molecules could be perfectly elastic.\n\nOther pioneers of the kinetic theory (which were neglected by their contemporaries) were Mikhail Lomonosov (1747), Georges-Louis Le Sage (ca. 1780, published 1818), John Herapath (1816) and John James Waterston (1843), which connected their research with the development of mechanical explanations of gravitation. In 1856 August Krönig (probably after reading a paper of Waterston) created a simple gas-kinetic model, which only considered the translational motion of the particles.\n\nIn 1857 Rudolf Clausius, according to his own words independently of Krönig, developed a similar, but much more sophisticated version of the theory which included translational and contrary to Krönig also rotational and vibrational molecular motions. In this same work he introduced the concept of mean free path of a particle.\nIn 1871, Ludwig Boltzmann generalized Maxwell's achievement and formulated the Maxwell–Boltzmann distribution. Also the logarithmic connection between entropy and probability was first stated by him.\n\nIn the beginning of the twentieth century, however, atoms were considered by many physicists to be purely hypothetical constructs, rather than real objects. An important turning point was Albert Einstein's (1905) and Marian Smoluchowski's (1906)\npapers on Brownian motion, which succeeded in making certain accurate quantitative predictions based on the kinetic theory.\n\nThe theory for ideal gases makes the following assumptions:\n\n\n\n\nMore modern developments relax these assumptions and are based on the Boltzmann equation. These can accurately describe the properties of dense gases, because they include the volume of the molecules. The necessary assumptions are the absence of quantum effects, molecular chaos and small gradients in bulk properties. Expansions to higher orders in the density are known as virial expansions.\n\nAn important book on kinetic theory is that by Chapman and Cowling. An important approach to the subject is called Chapman–Enskog theory. There have been many modern developments and there is an alternative approach developed by Grad based on moment expansions.\nIn the other limit, for extremely rarefied gases, the gradients in bulk properties are not small compared to the mean free paths. This is known as the Knudsen regime and expansions can be performed in the Knudsen number.\n\nIn kinetic model of gases, the pressure is equal to the force exerted by the atoms hitting and rebounding from a unit area of the gas container surface. Consider a gas of \"N\" molecules, each of mass \"m\", enclosed in a cube of volume \"V\" = \"L\". When a gas molecule collides with the wall of the container perpendicular to the \"x\" axis and bounces off in the opposite direction with the same speed (an elastic collision), the change in momentum is given by:\n\nwhere \"p\" is the momentum, \"i\" and \"f\" indicate initial and final momentum (before and after collision), \"x\" indicates that only the \"x\" direction is being considered, and \"v\" is the speed of the particle (which is the same before and after the collision).\n\nThe particle impacts one specific side wall once every\n\nwhere \"L\" is the distance between opposite walls.\n\nThe force due to this particle is\n\nThe total force on the wall is\n\nwhere the bar denotes an average over the \"N\" particles.\n\nSince the motion of the particles is random and there is no bias applied in any direction, the average squared speed in each direction is identical:\n\nBy Pythagorean theorem in three dimensions the total squared speed \"v\" is given by\n\nTherefore:\n\nand the force can be written as:\n\nThis force is exerted on an area \"L\". Therefore, the pressure of the gas is\n\nwhere \"V\" = \"L\" is the volume of the box.\n\nIn terms of the kinetic energy of the gas \"K\":\n\nThis is a first non-trivial result of the kinetic theory because it relates pressure, a macroscopic property, to the (translational) kinetic energy of the molecules formula_12, which is a microscopic property.\n\nRewriting the above result for the pressure as formula_13, \nwe may combine it with the ideal gas law\n\nwhere formula_14 is the Boltzmann constant and formula_15 the\nabsolute temperature defined by the ideal gas law, to obtain\n\nwhich leads to simplified expression of the average kinetic energy per molecule,\n\nThe kinetic energy of the system is N times that of a molecule, namely formula_18.\nThen the temperature formula_15 takes the form\n\nwhich becomes\nEq.()\nis one important result of the kinetic\ntheory:\n\"The average molecular kinetic energy is proportional to the ideal gas law's absolute temperature\".\nFrom Eq.() and\nEq.(),\nwe have\n\nThus, the product of pressure and\nvolume per mole is proportional to the average\n(translational) molecular kinetic energy.\n\nEq.() and Eq.()\nare called the \"classical results\",\nwhich could also be derived from statistical mechanics;\nfor more details, see:\n\nSince there are\nformula_20\ndegrees of freedom in a monatomic-gas system with\nformula_21\nparticles,\nthe kinetic energy per degree of freedom per molecule is\n\nIn the kinetic energy per degree of freedom,\nthe constant of proportionality of temperature\nis 1/2 times Boltzmann constant or R/2 per mole. In addition to this, the temperature will decrease when the pressure drops to a certain point.\nThis result is related\nto the equipartition theorem.\n\nAs noted in the article on heat capacity, diatomic\ngases should have 7 degrees of freedom, but the lighter diatomic gases act\nas if they have only 5. Monatomic gases have 3 degrees of freedom.\n\nThus the kinetic energy per kelvin (monatomic ideal gas) is 3 [R/2] = 3R/2:\n\nAt standard temperature (273.15 K), we get:\nBy.Shaklain Chemist\n\nOne can calculate the number of atomic or molecular collisions with a wall of a container per unit area per unit time.\n\nAssuming an ideal gas, a derivation results in an equation for total number of collisions per unit time per area:\n\nThis quantity is also known as the \"impingement rate\" in vacuum physics.\n\nFrom the kinetic energy formula it can be shown that\n\nwhere \"v\" is in m/s, \"T\" is in kelvins, and \"m\" is the mass of one molecule of gas. The most probable (or mode) speed formula_26 is 81.6% of the rms speed formula_27, and the mean (arithmetic mean, or average) speed formula_28 is 92.1% of the rms speed (isotropic distribution of speeds).\n\nSee: \n\nThe kinetic theory of gases deals not only with gases in thermodynamic equilibrium, but also very importantly with gases not in thermodynamic equilibrium. This means considering what are known as \"transport properties\", such a viscosity and thermal conductivity.\n\nIn books on elementary kinetic \ntheory\none can find results for dilute gas modeling that has widespread use. \nDerivation of the kinetic model for shear viscosity usually starts by considering a Couette flow where two parallel plates are separated by a gas layer. The upper plate is moving at a constant velocity to the right due to a force F. The lower plate is stationary, and an equal and opposite force must therefore be acting on it to keep it at rest. The molecules in the gas layer have a forward velocity component formula_29 which increase uniformly with distance formula_30 above the lower plate. The non-equilibrium flow is superimposed on a Maxwell-Boltzmann equilibrium distribution of molecular motions.\n\nLet formula_31 be the collision cross section of one molecule colliding with another. The number density formula_32 is defined as the number of molecules per (extensive) volume formula_33. The collision cross section per volume or collision cross section density is formula_34, and it is related to the mean free path formula_35 by \n\nformula_36\n\nNotice that the unit of the collision cross section per volume formula_34 is reciprocal of length. The mean free path is the average distance traveled by a molecule, or a number of molecules per volume, before they make their first collision.\n\nLet formula_38 be the forward velocity of the gas at an imaginary horizontal surface inside the gas layer. On the average, a molecule that crosses the surface makes its last collision before crossing at a distance equal to two-thirds of the mean free path (i.e. formula_39) away from the surface. At this distance above and below the surface, the forward momentum of the molecule is respectively\n\nformula_40\n\nwhere m is the molecular mass. The molecular flux formula_41 includes all molecules arriving at one side of an element of the surface within the gas layer. The incoming molecules are coming from all directions at the one side of the surface and with all speeds. This molecular flux (i.e. the number flux) is related to the average molecular speed formula_42 by\n\nformula_43\n\nNotice that the forward velocity gradient formula_44 can be considered to be constant over a distance of mean free path. Next we multiply by the total flux to get the change of momentum per unit time and per unit area, that is carried by the molecules crossing from either above or below the surface area. This gives the equation\n\nformula_45\n\nThe net rate of momentum per unit area that is transported across the imaginary surface is thus\n\nformula_46\n\nThe defining equation for the (shear) viscosity formula_47 of the gas is\n\nformula_48\n\nCombining the above kinetic equation with defining equation for (shear) viscosity by formula_49 gives the equation for shear viscosity, which is usually denoted formula_50 when it is a dilute gas:\n\nformula_51\n\nCombining this equation with the equation for mean free path gives\n\nformula_52\n\nFrom statistical thermodynamics for gases we have equations relating average molecular speed to most likely speed and further to temperature. These statistical results gives the average (equilibrium) molecular speed as\n\nformula_53\n\nwhere formula_54 is the most probable speed, formula_55 is the Boltzmann constant. We note that\n\nformula_56\n\nand insert the velocity in the viscosity equation above. This gives the well known equation for shear viscosity for dilute gases:\n\nformula_57\n\nand formula_58 is the molar mass. The equation above presupposes that the gas density is low (i.e. the pressure is low). This implies that the kinetic translational energy dominates over rotational and vibrational molecule energies. The viscosity equation further presupposes that there is only one type of gas molecules, and that the gas molecules are perfect elastic and hard core particles of spherical shape. This assumption of elastic, hard core spherical molecules, like billiard balls, implies that the collision cross section of one molecule can be estimated by\n\nformula_59\n\nThe radius formula_60 is called collision cross section radius or kinetic radius, and the diameter formula_61 is called collision cross section diameter or kinetic diameter of a molecule in a monomolecular gas. There are no simple general relation between the collision cross section and the hard core size of the (fairly spherical) molecule. The relation depends on shape of the potential energy of the molecule. For a real spherical molecule (i.e. a noble gas atom or a reasonably spherical molecule) the interaction potential is more like the Lennard-Jones potential or Morse potential which have a negative part that attracts the other molecule from distances longer than the hard core radius. The radius for zero Lennard-Jones potential is then appropriate to use as estimate for the kinetic radius.\n\nLocal nomenclature list:\n\n\n\n\n\n\n\n"}
{"id": "13908785", "url": "https://en.wikipedia.org/wiki?curid=13908785", "title": "Law of Continuity", "text": "Law of Continuity\n\nThe law of continuity is a heuristic principle introduced by Gottfried Leibniz based on earlier work by Nicholas of Cusa and Johannes Kepler. It is the principle that \"whatever succeeds for the finite, also succeeds for the infinite\". Kepler used The Law of Continuity to calculate the area of the circle by representing the latter as an infinite-sided polygon with infinitesimal sides, and adding the areas of infinitely-many triangles with infinitesimal bases. Leibniz used the principle to extend concepts such as arithmetic operations, from ordinary numbers to infinitesimals, laying the groundwork for infinitesimal calculus. A mathematical implementation of the law of continuity is provided by the transfer principle in the context of the hyperreal numbers.\n\nA related law of continuity concerning intersection numbers in geometry was promoted by Jean-Victor Poncelet in his \"Traité des propriétés projectives des figures\". \nLeibniz expressed the law in the following terms in 1701:\n\nIn a 1702 letter to French mathematician Pierre Varignon subtitled “Justification of the Infinitesimal Calculus by that of Ordinary Algebra,\" Leibniz adequately summed up the true meaning of his law, stating that \"the rules of the finite are found to succeed in the infinite.\" \n\nThe Law of Continuity became important to Leibniz's justification and conceptualization of the infinitesimal calculus.\n\n"}
{"id": "285773", "url": "https://en.wikipedia.org/wiki?curid=285773", "title": "Limit of a sequence", "text": "Limit of a sequence\n\nIn mathematics, the limit of a sequence is the value that the terms of a sequence \"tend to\". If such a limit exists, the sequence is called convergent. A sequence which does not converge is said to be divergent. The limit of a sequence is said to be the fundamental notion on which the whole of analysis ultimately rests.\n\nLimits can be defined in any metric or topological space, but are usually first encountered in the real numbers.\n\nThe Greek philosopher Zeno of Elea is famous for formulating paradoxes that involve limiting processes.\n\nLeucippus, Democritus, Antiphon, Eudoxus and Archimedes developed the method of exhaustion, which uses an infinite sequence of approximations to determine an area or a volume. Archimedes succeeded in summing what is now called a geometric series.\n\nNewton dealt with series in his works on \"Analysis with infinite series\" (written in 1669, circulated in manuscript, published in 1711), \"Method of fluxions and infinite series\" (written in 1671, published in English translation in 1736, Latin original published much later) and \"Tractatus de Quadratura Curvarum\" (written in 1693, published in 1704 as an Appendix to his \"Optiks\"). In the latter work, Newton considers the binomial expansion of (\"x\" + \"o\") which he then linearizes by \"taking limits\" (letting \"o\" → 0).\n\nIn the 18th century, mathematicians such as Euler succeeded in summing some \"divergent\" series by stopping at the right moment; they did not much care whether a limit existed, as long as it could be calculated. At the end of the century, Lagrange in his \"Théorie des fonctions analytiques\" (1797) opined that the lack of rigour precluded further development in calculus. Gauss in his etude of hypergeometric series (1813) for the first time rigorously investigated under which conditions a series converged to a limit.\n\nThe modern definition of a limit (for any ε there exists an index \"N\" so that ...) was given by Bernhard Bolzano (\"Der binomische Lehrsatz\", Prague 1816, little noticed at the time) and by Karl Weierstrass in the 1870s.\n\nIn the real numbers, a number formula_1 is the limit of the sequence formula_2 if the numbers in the sequence become closer and closer to formula_1 and not to any other number.\n\n\n\nWe call formula_20 the limit of the sequence formula_2 if the following condition holds:\nIn other words, for every measure of closeness formula_26, the sequence's terms are eventually that close to the limit. The sequence formula_2 is said to converge to or tend to the limit formula_20, written formula_29 or formula_30.\n\nSymbolically, this is:\n\nIf a sequence converges to some limit, then it is convergent; otherwise it is divergent.\n\nLimits of sequences behave well with respect to the usual arithmetic operations. If formula_32 and formula_33, then formula_34, formula_35 and, if neither \"b\" nor any formula_36 is zero, formula_37.\n\nFor any continuous function \"f\", if formula_29 then formula_39. In fact, any real-valued function \"f\" is continuous if and only if it preserves the limits of sequences (though this is not necessarily true when using more general notions of continuity).\n\nSome other important properties of limits of real sequences include the following (provided, in each equation below, that the limits on the right exist).\n\n\nThese properties are extensively used to prove limits without the need to directly use the cumbersome formal definition. Once proven that \nformula_54 it becomes easy to show that formula_55, (formula_56), using the properties above.\n\nA sequence formula_2 is said to tend to infinity, written formula_58 or formula_59 if, for every \"K\", there is an \"N\" such that, for every formula_24, formula_61; that is, the sequence terms are eventually larger than any fixed \"K\". Similarly, formula_62 if, for every \"K\", there is an \"N\" such that, for every formula_24, formula_64. If a sequence tends to infinity, or to minus infinity, then it is divergent (however, a divergent sequence need not tend to plus or minus infinity: take for example formula_65).\n\nA point formula_20 of the metric space formula_67 is the limit of the sequence formula_2 if, for all formula_22, there is an formula_23 such that, for every formula_24, formula_72. This coincides with the definition given for real numbers when formula_73 and formula_74.\n\nFor any continuous function \"f\", if formula_29 then formula_39. In fact, a function \"f\" is continuous if and only if it preserves the limits of sequences.\n\nLimits of sequences are unique when they exist, as distinct points are separated by some positive distance, so for formula_26 less than half this distance, sequence terms cannot be within a distance formula_26 of both points.\n\nA point \"x\" of the topological space (\"X\", τ) is a limit of the sequence (\"x\") if, for every neighbourhood \"U\" of \"x\", there is an \"N\" such that, for every formula_24, formula_80. This coincides with the definition given for metric spaces if (\"X\",\"d\") is a metric space and formula_81 is the topology generated by \"d\".\n\nA limit of a sequence of points formula_82 in a topological space \"T\" is a special case of a limit of a function: the domain is formula_83 in the space formula_84 with the induced topology of the affinely extended real number system, the range is \"T\", and the function argument \"n\" tends to +∞, which in this space is a limit point of formula_83.\n\nIf \"X\" is a Hausdorff space then limits of sequences are unique where they exist. Note that this need not be the case in general; in particular, if two points \"x\" and \"y\" are topologically indistinguishable, any sequence that converges to \"x\" must converge to \"y\" and vice versa.\n\nA Cauchy sequence is a sequence whose terms ultimately become arbitrarily close together, after sufficiently many initial terms have been discarded. The notion of a Cauchy sequence is important in the study of sequences in metric spaces, and, in particular, in real analysis. One particularly important result in real analysis is the \"Cauchy criterion for convergence of sequences\": A sequence of real numbers is convergent if and only if it is a Cauchy sequence. This remains true in other complete metric spaces.\n\nThe definition of the limit using the hyperreal numbers formalizes the intuition that for a \"very large\" value of the index, the corresponding term is \"very close\" to the limit. More precisely, a real sequence formula_2 tends to \"L\" if for every infinite hypernatural \"H\", the term \"x\" is infinitely close to \"L\", i.e., the difference \"x\" − \"L\" is infinitesimal. Equivalently, \"L\" is the standard part of \"x\"\n\nThus, the limit can be defined by the formula\n\nwhere the limit exists if and only if the righthand side is independent of the choice of an infinite \"H\".\n\n\n\n"}
{"id": "613965", "url": "https://en.wikipedia.org/wiki?curid=613965", "title": "Love bombing", "text": "Love bombing\n\nLove bombing is an attempt to influence a person by demonstrations of attention and affection. It can be used in different ways and can be used for either a positive or negative purpose. Members of the Unification Church of the United States (who reportedly coined the expression) use it to convey a genuine expression of friendship, fellowship, interest, or concern. Critics of cults use the phrase with the implication that the \"love\" is feigned and that the practice is psychological manipulation in order to create a feeling of unity within the group against a society perceived as hostile. Psychologists have identified love bombing as a possible part of a cycle of abuse and have warned against it. In 2011 clinical psychologist Oliver James advocated love bombing in his book \"Love Bombing: Reset Your Child's Emotional Thermostat\", as a means for parents to rectify emotional problems in their children. \nThe expression \"love bombing\" was coined by members of the Unification Church of the United States in the 1970s and was also used by members of the Family International.\nIn 1978 Sun Myung Moon, the founder and then leader of the Unification Church, said:\n\nAnthropology professor Geri-Ann Galanti writes: \n\nPsychology professor Margaret Singer popularized the concept. In her 1996 book, \"Cults in Our Midst,\" she writes:\n\nThe expression has also been used to describe the tactics used by pimps and gang members to control their victims, as well as to describe the behavior of an abusive narcissist who tries to win the confidence of a victim. Modern social media can intensify the effect of love bombing since it enables the abuser with nearly constant contact and communication with the victim. \n\nOne of the signs of love bombing in the start of a relationship is much attention in short time, and pressure for very rapid commitment. It is often the first sign of narcissism, and if successful turns to control and degradation. Psychologist Dale Archer identifies \"The Phases of Love Bombing: Idealization, Devaluation, Discard (Repeat).\" He advises: \"Stop, Look, and Listen\" to avoid love bombing and to break off contact with the abuser, if possible, and seek support from family and friends.\n\nExcessive attention and affection does not constitute love bombing if there is no intent or pattern of further abuse. Archer explains: \n\nIn the 2010s British author and psychologist Oliver James recommended love bombing as a technique for parents to help their troubled children. He described it as, “dedicating one-on-one time spoiling and lavishing your child with love, and, within reason, pandering to their every wish.” A reporter for \"The Daily Express\" tried the technique with her son and reported:\n"}
{"id": "2679881", "url": "https://en.wikipedia.org/wiki?curid=2679881", "title": "Magnetogravitic tensor", "text": "Magnetogravitic tensor\n\nIn general relativity, the magnetogravitic tensor is one of the three pieces appearing in the Bel decomposition of the Riemann tensor.\n\nThe magnetogravitic tensor can be interpreted physically as a specifying possible spin-spin forces on spinning bits of matter, such as spinning test particles.\n\n"}
{"id": "5306891", "url": "https://en.wikipedia.org/wiki?curid=5306891", "title": "Mahāvākyas", "text": "Mahāvākyas\n\nThe Mahavakyas (\"sing.:\" mahāvākyam, महावाक्यम्; \"plural:\" mahāvākyāni, महावाक्यानि) are \"The Great Sayings\" of the Upanishads, as characterized by the Advaita school of Vedanta.\n\nMost commonly, \"Mahavakyas\" are considered four in number,\n\nThough there are many Mahavakyas, four of them, one from each of the four Vedas, are often mentioned as \"the Mahavakyas\". According to the Vedanta-tradition, the subject matter and the essence of all Upanishads is the same, and all the Upanishadic Mahavakyas express this one universal message in the form of terse and concise statements. In later Sanskrit usage, the term \"mahāvākya\" came to mean \"discourse\", and specifically, discourse on a philosophically lofty topic.\n\nAccording to the Advaita Vedanta tradition the four Upanishadic statements indicate the ultimate unity of the individual (Atman) with Supreme (Brahman).\n\nThe Mahavakyas are:\n\nPeople who are initiated into sannyasa in Advaita Vedanta are being taught the four [principal] mahavakyas as four mantras, \"to attain this highest of states in which the individual self dissolves inseparably in Brahman\".\n\n\nSeveral translations, and word-orders of these translations, are possible:\n\nPrajñānam:\n\n\"Prajñānam\" as a whole means:\n\nRelated terms are \"jñāna\", \"prajñā\" and \"prajñam\", \"pure consciousness\". Although the common translation of \"jñānam\" is \"consciousness\", the term has a broader meaning of \"knowing\"; \"becoming acquainted with\", \"knowledge about anything\", \"awareness\", \"higher knowledge\".\n\nBrahman:\n\nMost interpretations state: \"Prajñānam (noun) is Brahman (adjective)\". Some translations give a reverse order, stating \"Brahman is Prajñānam\", specifically \"Brahman (noun) is Prajñānam (adjective)\": \"The Ultimate Reality is wisdom (or consciousness)\".\n\nSahu explains:\nAnd according to David Loy, \n\n"}
{"id": "20862", "url": "https://en.wikipedia.org/wiki?curid=20862", "title": "Manorialism", "text": "Manorialism\n\nManorialism was an essential element of feudal society. It was the organizing principle of rural economy that originated in the Roman villa system of the Late Roman Empire, and was widely practiced in medieval western and parts of central Europe as well as China. It was slowly replaced by the advent of a money-based market economy and new forms of agrarian contract.\n\nManorialism was characterised by the vesting of legal and economic power in a Lord of the Manor, supported economically from his own direct landholding in a manor (sometimes called a fief), and from the obligatory contributions of a legally subject part of the peasant population under the jurisdiction of himself and his manorial court. These obligations could be payable in several ways, in labor (the French term \"corvée\" is conventionally applied), in kind, or, on rare occasions, in coin.\n\nIn examining the origins of the monastic cloister, Walter Horn found that \"as a manorial entity the Carolingian monastery ... differed little from the fabric of a feudal estate, save that the corporate community of men for whose sustenance this organization was maintained consisted of monks who served God in chant and spent much of their time in reading and writing.\"\n\nManorialism died slowly and piecemeal, along with its most vivid feature in the landscape, the open field system. It outlasted serfdom as it outlasted feudalism: \"primarily an economic organization, it could maintain a warrior, but it could equally well maintain a capitalist landlord. It could be self-sufficient, yield produce for the market, or it could yield a money rent.\" The last feudal dues in France were abolished at the French Revolution. In parts of eastern Germany, the \"Rittergut\" manors of Junkers remained until World War II. In Quebec, the last feudal rents were paid in 1970 under the modified provisions of the \"Seigniorial Dues Abolition Act\" of 1935.\n\nThe term is most often used with reference to medieval Western Europe. Antecedents of the system can be traced to the rural economy of the later Roman Empire (Dominate). With a declining birthrate and population, labor was the key factor of production. Successive administrations tried to stabilise the imperial economy by freezing the social structure into place: sons were to succeed their fathers in their trade, councilors were forbidden to resign, and \"coloni\", the cultivators of land, were not to move from the land they were attached to. The workers of the land were on their way to becoming serfs.\n\nSeveral factors conspired to merge the status of former slaves and former free farmers into a dependent class of such \"coloni\": it was possible to be described as \"servus et colonus\", \"both slave and \"colonus\"\". Laws of Constantine I around 325 both reinforced the semi-servile status of the \"coloni\" and limited their rights to sue in the courts; the \"Codex Theodosianus\" promulgated under Theodosius II extended these restrictions. The legal status of \"adscripti\", \"bound to the soil\", contrasted with barbarian \"foederati\", who were permitted to settle within the imperial boundaries, remaining subject to their own traditional law.\n\nAs the Germanic kingdoms succeeded Roman authority in the West in the fifth century, Roman landlords were often simply replaced by Germanic ones, with little change to the underlying situation or displacement of populations.\n\nThe process of rural self-sufficiency was given an abrupt boost in the eighth century, when normal trade in the Mediterranean Sea was disrupted. The thesis put forward by Henri Pirenne supposes that the Arab conquests forced the medieval economy into even greater ruralization and gave rise to the classic feudal pattern of varying degrees of servile peasantry underpinning a hierarchy of localised power centers.\n\nThe word derives from traditional inherited divisions of the countryside, reassigned as local jurisdictions known as manors or seigneuries; each manor being subject to a lord (French \"seigneur\"), usually holding his position in return for undertakings offered to a higher lord (see Feudalism). The lord held a manorial court, governed by public law and local custom. Not all territorial seigneurs were secular; bishops and abbots also held lands that entailed similar obligations.\n\nBy extension, the word \"manor\" is sometimes used in England to mean any home area or territory in which authority is held, often in a police or criminal context.\n\nIn the generic plan of a medieval manor from \"Shepherd's Historical Atlas\", the strips of individually worked land in the open field system are immediately apparent. In this plan, the manor house is set slightly apart from the village, but equally often the village grew up around the forecourt of the manor, formerly walled, while the manor lands stretched away outside, as still may be seen at Petworth House. As concerns for privacy increased in the 18th century, manor houses were often located a farther distance from the village. For example, when a grand new house was required by the new owner of Harlaxton Manor, Lincolnshire, in the 1830s, the site of the existing manor house at the edge of its village was abandoned for a new one, isolated in its park, with the village out of view.\n\nIn an agrarian society, the conditions of land tenure underlie all social or economic factors. There were two legal systems of pre-manorial landholding. One, the most common, was the system of holding land \"allodially\" in full outright ownership. The other was a use of \"precaria\" or benefices, in which land was held conditionally (the root of the English word \"precarious\").\n\nTo these two systems, the Carolingian monarchs added a third, the \"aprisio\", which linked manorialism with feudalism. The \"aprisio\" made its first appearance in Charlemagne's province of Septimania in the south of France, when Charlemagne had to settle the Visigothic refugees who had fled with his retreating forces after the failure of his Zaragoza expedition of 778. He solved this problem by allotting \"desert\" tracts of uncultivated land belonging to the royal \"fisc\" under direct control of the emperor. These holdings \"aprisio\" entailed specific conditions. The earliest specific \"aprisio\" grant that has been identified was at Fontjoncouse, near Narbonne (see Lewis, links). In former Roman settlements, a system of villas, dating from Late Antiquity, was inherited by the medieval world.\n\nManors each consisted of up to three classes of land:\n\nAdditional sources of income for the lord included charges for use of his mill, bakery or wine-press, or for the right to hunt or to let pigs feed in his woodland, as well as court revenues and single payments on each change of tenant. On the other side of the account, manorial administration involved significant expenses, perhaps a reason why smaller manors tended to rely less on villein tenure.\n\nDependent holdings were held nominally by arrangement of lord and tenant, but tenure became in practice almost universally hereditary, with a payment made to the lord on each succession of another member of the family. Villein land could not be abandoned, at least until demographic and economic circumstances made flight a viable proposition; nor could they be passed to a third party without the lord's permission, and the customary payment.\n\nAlthough not free, villeins were by no means in the same position as slaves: they enjoyed legal rights, subject to local custom, and had recourse to the law subject to court charges, which were an additional source of manorial income. Sub-letting of villein holdings was common, and labour on the demesne might be commuted into an additional money payment, as happened increasingly from the 13th century.\n\nThis description of a manor house at Chingford, Essex in England was recorded in a document for the Chapter of St Paul's Cathedral when it was granted to Robert Le Moyne in 1265:\n\nLike feudalism which, together with manorialism, formed the legal and organizational framework of feudal society, manorial structures were not uniform or coordinated. In the later Middle Ages, areas of incomplete or non-existent manorialization persisted while the manorial economy underwent substantial development with changing economic conditions.\n\nNot all manors contained all three classes of land. Typically, demesne accounted for roughly a third of the arable area, and villein holdings rather more; but some manors consisted solely of demesne, others solely of peasant holdings. The proportion of unfree and free tenures could likewise vary greatly, with more or less reliance on wage labour for agricultural work on the demesne.\n\nThe proportion of the cultivated area in demesne tended to be greater in smaller manors, while the share of villein land was greater in large manors, providing the lord of the latter with a larger supply of obligatory labour for demesne work. The proportion of free tenements was generally less variable, but tended to be somewhat greater on the smaller manors.\n\nManors varied similarly in their geographical arrangement: most did not coincide with a single village, but rather consisted of parts of two or more villages, most of the latter containing also parts of at least one other manor. This situation sometimes led to replacement by cash payments or their equivalents in kind of the demesne labour obligations of those peasants living furthest from the lord's estate.\n\nAs with peasant plots, the demesne was not a single territorial unit, but consisted rather of a central house with neighbouring land and estate buildings, plus strips dispersed through the manor alongside free and villein ones: in addition, the lord might lease free tenements belonging to neighbouring manors, as well as holding other manors some distance away to provide a greater range of produce.\n\nNor were manors held necessarily by lay lords rendering military service (or again, cash in lieu) to their superior: a substantial share (estimated by value at 17% in England in 1086) belonged directly to the king, and a greater proportion (rather more than a quarter) were held by bishoprics and monasteries. Ecclesiastical manors tended to be larger, with a significantly greater villein area than neighbouring lay manors.\n\nThe effect of circumstances on manorial economy is complex and at times contradictory: upland conditions tended to preserve peasant freedoms (livestock husbandry in particular being less labour-intensive and therefore less demanding of villein services); on the other hand, some upland areas of Europe showed some of the most oppressive manorial conditions, while lowland eastern England is credited with an exceptionally large free peasantry, in part a legacy of Scandinavian settlement.\n\nSimilarly, the spread of money economy stimulated the replacement of labour services by money payments, but the growth of the money supply and resulting inflation after 1170 initially led nobles to take back leased estates and to re-impose labour dues as the value of fixed cash payments declined in real terms.\n\n\nSpecific:\n\n\nGeneral:\n\n\n"}
{"id": "3108261", "url": "https://en.wikipedia.org/wiki?curid=3108261", "title": "Marginal value", "text": "Marginal value\n\nA marginal value is\n(This third case is actually a special case of the second).\n\nIn the case of differentiability, at the limit, a marginal change is a mathematical differential, or the corresponding mathematical derivative.\n\nThese uses of the term “marginal” are especially common in economics, and result from conceptualizing constraints as \"borders\" or as \"margins\". The sorts of marginal values most common to economic analysis are those associated with \"unit\" changes of resources and, in mainstream economics, those associated with \"infinitesimal\" changes. Marginal values associated with units are considered because many decisions are made by unit, and marginalism explains \"unit price\" in terms of such marginal values. Mainstream economics uses infinitesimal values in much of its analysis for reasons of mathematical tractability.\n\nAssume a functional relationship\n\nIf the value of formula_2 is \"discretely\" changed from formula_3 to formula_4 while other independent variables remain unchanged, then the marginal value of the change in formula_2 is\nand the “marginal value” of formula_7 may refer to\nor to\n\nIf an individual saw her income increase from $50000 to $55000 per annum, and part of her response was to increase yearly purchases of amontillado from 2 casks to three casks, then\n\nIf \"infinitesimal\" values are considered, then a marginal value of formula_2 would be formula_11, and the “marginal value” of formula_7 would typically refer to\n\nAssume that, in some economy, aggregate consumption is well-approximated by\nwhere\nThen the \"marginal propensity to consume\" is\n\n"}
{"id": "4295487", "url": "https://en.wikipedia.org/wiki?curid=4295487", "title": "One-electron universe", "text": "One-electron universe\n\nThe one-electron universe postulate, proposed by John Wheeler in a telephone call to Richard Feynman in the spring of 1940, hypothesises that all electrons and positrons are actually manifestations of a single entity moving backwards and forwards in time. According to Feynman:\n\nThe idea is based on the world lines traced out across spacetime by every electron. Rather than have myriad such lines, Wheeler suggested that they could all be parts of one single line like a huge tangled knot, traced out by the one electron. Any given moment in time is represented by a slice across spacetime, and would meet the knotted line a great many times. Each such meeting point represents a real electron at that moment.\n\nAt those points, half the lines will be directed forward in time and half will have looped round and be directed backwards. Wheeler suggested that these backwards sections appeared as the antiparticle to the electron, the positron.\n\nMany more electrons have been observed than positrons, and electrons are thought to comfortably outnumber them. According to Feynman he raised this issue with Wheeler, who speculated that the missing positrons might be hidden within protons.\n\nFeynman was struck by Wheeler's insight that antiparticles could be represented by reversed world lines, and credits this to Wheeler, saying in his Nobel speech:\n\nFeynman later proposed this interpretation of the positron as an electron moving backward in time in his 1949 paper \"The Theory of Positrons\". Yoichiro Nambu later applied it to all production and annihilation of particle-antiparticle pairs, stating that \"the eventual creation and annihilation of pairs that may occur now and then is no creation or annihilation, but only a change of direction of moving particles, from past to future, or from future to past.\"\n\n"}
{"id": "1037059", "url": "https://en.wikipedia.org/wiki?curid=1037059", "title": "Plane (esotericism)", "text": "Plane (esotericism)\n\nIn esoteric cosmology, a plane is conceived as a subtle state, level, or region of reality, each plane corresponding to some type, kind, or category of being.\n\nThe concept may be found in religious and esoteric teachings—\"e.g.\" Vedanta (Advaita Vedanta), Ayyavazhi, shamanism, Hermeticism, Neoplatonism, Gnosticism, Kashmir Shaivism, Sant Mat/Surat Shabd Yoga, Sufism, Druze, Kabbalah, Theosophy, Anthroposophy, Rosicrucianism (Esoteric Christian), Eckankar, Ascended Master Teachings, etc.—which propound the idea of a whole series of subtle planes or worlds or dimensions which, from a center, interpenetrate themselves and the physical planet in which we live, the solar systems, and all the physical structures of the universe. This interpenetration of planes culminates in the universe itself as a physical structured, dynamic and evolutive expression emanated through a series of steadily denser stages, becoming progressively more material and embodied.\n\nThe emanation is conceived, according to esoteric teachings, to have originated, at the dawn of the universe's manifestation, in \"The Supreme Being\" who sent out—from the unmanifested \"Absolute\" beyond comprehension—the dynamic force of creative energy, as \"sound-vibration\" (\"the Word\"), into the abyss of space. Alternatively, it states that this dynamic force is being sent forth, through the ages, framing all things that constitute and inhabit the universe.\n\nThe concept of planes of existence might be seen as deriving from shamanic and traditional mythological ideas of a vertical world-axis—for example a cosmic mountain, tree, or pole (such as Yggdrasil or Mount Meru)—or a philosophical conception of a Great Chain of Being, arranged metaphorically from God down to inanimate matter.\n\nHowever the original source of the word \"plane\" in this context is the late Neoplatonist Proclus, who refers to \"to platos\", \"breadth\", which was the equivalent of the 19th-century theosophical use. An example is the phrase \"en to psychiko platei\".\n\nDirectly equivalent concepts in Indian thought are lokas and bhuvanas. In Hindu cosmology, there are many lokas or worlds, that are identified with both traditional cosmology and states of meditation.\n\nPlanes of existence may have been referred to by the use of the term corresponding to the word \"egg\" in English. For example, the Sanskrit term Brahmanda translates to \"The entire creation\" as opposed to the lazy inference \"The Egg of Creation\". Certain Puranic accounts posit that the Brahmanda is the superset of a set of fractal smaller Eggs, as is seen in the assertion of the equivalence of the Brahmanda and the Pinda.\n\nThe ancient Norse mythology gave the name \"Ginnungagap\" to the primordial \"Chaos,\" which was bounded upon the northern side by the cold and foggy \"Niflheim\"—the land of mist and fog—and upon the south side by the fire \"Muspelheim.\" When heat and cold entered into space which was occupied by Chaos or Ginnungagap, they caused the crystallization of the visible universe.\n\nIn the medieval West and Middle East, one finds reference to four worlds (\"olam\") in Kabbalah, or five in Sufism (where they are also called \"tanazzulat\"; \"descents\"), and also in Lurianic Kabbalah. In Kabbalah, each of the four or five worlds are themselves divided into ten sefirot, or else divided in other ways.\n\nThe alchemists of the Middle Ages proposed ideas about the constitution of the universe through a hermetic language full of esoteric words, phrases, and signs designed to cloak their meaning from those not initiated into the ways of alchemy. In his \"Physica\" (1633), the Rosicrucian alchemist Jan Baptist van Helmont, wrote: \"Ad huc spiritum incognitum Gas voco\" q.e., \"This hitherto unknown Spirit I call Gas.\" Further on in the same work he says, \"This vapor which I have called Gas is not far removed from the Chaos the ancients spoke of.\" Later on, similar ideas would evolve around the idea of aether.\n\nIn the late 19th century, the metaphysical term \"planes\" was popularised by the theosophy of H.P. Blavatsky, who in \"The Secret Doctrine\" and other writings propounded a complex cosmology consisting of seven planes and subplanes, based on a synthesis of Eastern and Western ideas. From theosophy the term made its way to later esoteric systems such as that of Alice Bailey, who was very influential in shaping the worldview of the New Age movement. The term is also found in some Eastern teachings that have some Western influence, such as the cosmology of Sri Aurobindo and some of the later Sant Mat, and also in some descriptions of Buddhist cosmology. The teachings of Surat Shabd Yoga also include several planes of the creation within both the macrocosm and microcosm, including the Bramanda egg contained within the Sach Khand egg. Max Theon used the word \"States\" (French \"Etat\") rather than \"Planes\", in his cosmic philosophy, but the meaning is the same.\n\nThe planes in Theosophy were further systematized in the writings of C.W. Leadbeater and Annie Besant.\n\nIn the early 20th century, Max Heindel presented in \"The Rosicrucian Cosmo-Conception\" a cosmology related to the scheme of evolution in general and the evolution of the solar system and the Earth in particular, according to the Rosicrucians. He establishes, through the conceptions presented, a bridge between modern science (currently starting research into the subtler \"etheric\" plane of existence behind the physical) and religion, in order that this last one may be able to address man's \"inner\" questions raised by scientific advancement.\n\nThe spiritual teacher Meher Baba proposed that there are six planes of consciousness that must be experienced before one can attain God-realization on the seventh plane: \"Each definite stage of advancement represents a state of consciousness, and advancement from one state of consciousness to another proceeds side by side with crossing the inner planes. Thus six intermediate planes and states of consciousness have to be experienced before reaching the seventh plane which is the end of the journey and where there is final realisation of the God-state.\"\n\nMost cosmologists today believe that the universe expanded from a singularity approximately 13.8 billion years ago in a 'smeared-out singularity' called the Big Bang, meaning that space itself came into being at the moment of the big bang and has expanded ever since, creating and carrying the galaxies with it.\nHowever, in esoteric cosmology expansion refers to the emanation or unfolding of steadily denser planes or spheres from the spiritual summit, what Greek philosophy called \"The One\", until the lowest and most material world is reached.\nAccording to Rosicrucians, another difference is that there is no such thing as empty or void space. \n\"The space is Spirit in its attenuated form; while matter is crystallized space or Spirit. Spirit in manifestation is dual, that which we see as Form is the negative manifestation of Spirit--crystallized and inert. The positive pole of Spirit manifests as Life, galvanizing the negative Form into action, but both Life and Form originated in Spirit, Space, Chaos! On the other hand, Chaos is not a state which has existed in the past and has now entirely disappeared. It is all around us at the present moment. Were it not that old forms--having outlived their usefulness--are constantly being resolved back into that Chaos, which is also as constantly giving birth to new forms, there could be no progress; the work of evolution would cease and stagnation would prevent the possibility of advancement.\"\nIn occult teachings and as held by psychics and other esoteric authors there are seven planes of existence.\n\nMost occult and esoteric teachings are in agreement that seven planes of existence exist; however, many different occult and metaphysical schools label the planes of existence with different terminology.\n\nThe physical plane or physical universe, in emanationist metaphysics taught in Neoplatonism, Hermeticism, Hinduism and Theosophy, refers to the visible reality of space and time, energy and matter: the physical universe in Occultism and esoteric cosmology is the lowest or densest of a series of planes of existence.\n\nAccording to Theosophists, after the physical plane is the etheric plane and both of these planes are connected to make up the first plane. Theosophy also teaches that when the physical body dies the etheric body is left behind and the soul forms into an astral body on the astral plane.\n\nThe psychical researcher F. W. H. Myers proposed the existence of a “metetherial world”, which he wrote to be a world of images lying beyond the physical world. He wrote that apparitions have a real existence in the metetherial world which he described as a dream-like world.\n\nThe astral plane, also known as the emotional plane is where consciousness goes after physical death. According to occult philosophy man possesses an astral body. The astral plane (also known as the astral world) was postulated by classical (particularly neoplatonic), medieval, oriental, and esoteric philosophies and mystery religions. It is the world of the planetary spheres, crossed by the soul in its astral body on the way to being born and after death, and generally said to be populated by angels, spirits, or other immaterial beings. In the late 19th and early 20th century the term was popularised by Theosophy and neo-Rosicrucianism.\n\nThroughout the Renaissance, philosophers, Paracelsians, Rosicrucians and alchemists continued to discuss the nature of the astral world intermediate between Earth and the divine. The \"Barzakh\", \"olam mithal\" or intermediate world in Islam and the \"World of \"Yetzirah\"\" in Lurianic Kabbalah are related concepts.\n\nAccording to occult teachings the astral plane can be visited consciously through astral projection, meditation, and mantra, near-death experience, lucid dreaming, or other means. Individuals that are trained in the use of the astral vehicle can separate their consciousness in the astral vehicle from the physical body at will.\n\nThe Theosophist author Curuppumullage Jinarajadasa wrote: \"When a person dies, they become fully conscious in the astral body. After a certain time, the astral body disintegrates, and the person then becomes conscious on the mental plane.\" \n\nOccultist George Arundale wrote:\n\nIn the astral world exist temporarily all those physical entities, men and animals, for whom sleep involves a separation of the physical body for a time from the higher bodies. While we \"sleep\", we live in our astral bodies, either fully conscious and active, or partly conscious and semi-dormant, as the case may be, according to our evolutionary growth; when we \"wake\", the physical and the higher bodies are interlocked again, and we cease to be inhabitants of the astral world.” \n\nSome writers have claimed the astral plane can be reached by dreaming. Sylvan Muldoon and psychical researcher Hereward Carrington in their book \"The Projection of the Astral Body\" (1929) wrote:\"When you are dreaming you are not really in the same world as when you are conscious – in the physical – although the two worlds merge into one another. While dreaming, you really are in the astral plane, and usually your astral body is in the zone of quietude.\"Astral projection author Robert Bruce describes the astral as seven planes that take the form of planar surfaces when approached from a distance, separated by immense coloured \"buffer zones\". These planes are endlessly repeating ruled Cartesian coordinate system grids, tiled with a single signature pattern that is different for each plane. Higher planes have bright, colourful patterns, whereas lower planes appear far duller. Every detail of these patterns acts as a consistent portal to a different kingdom inside the plane, which itself comprises many separate realms. Bruce notes that the astral may also be entered by means of long tubes that bear visual similarity to these planes, and conjectures that the grids and tubes are in fact the same structures approached from a different perceptual angle.\n\nIn his book \"\", Paramhansa Yogananda provides details about the astral planes learned from his resurrected guru Swami Sri Yukteswar Giri. Yogananda reveals that nearly all individuals enter the astral planes after death. There they work out the seeds of past karma through astral incarnations, or (if their karma requires) they return to earthly incarnations for further refinement. Once an individual has attained the meditative state of nirvikalpa samadhi in an earthy or astral incarnation, the soul may progress upward to the \"illumined astral planet\" of Hiranyaloka. After this transitional stage, the soul may then move upward to the more subtle causal spheres where many incarnations allow them to further refine until final unification.\n\nThe mental plane, also known as the causal plane is the third lowest plane according to Theosophy. The mental plane is divided into seven sub-planes.\n\nCharles Webster Leadbeater wrote:\n\nIn the mental world one formulates a thought and it is instantly transmitted to the mind of another without any expression in the form of words. Therefore on that plane language does not matter in the least; but helpers working in the astral world, who have not yet the power to use the mental vehicle.\n\nAnnie Besant wrote that \"The mental plane, as its name implies, is that which belongs to consciousness working as thought; not of the mind as it works through the brain, but as it works through its own world, unencumbered with physical spirit-matter.\n\nA detailed description of the mental plane, along with the mental body, is provided by Arthur E. Powell, who has compiled information in the works of Besant and Leadbeater in a series of books on each of the subtle bodies.\n\nAccording to Hindu occultism the mental plane consists of two divisions, the lower division is known as heaven (\"swarglok\") and the upper division is known as the causal plane (\"maharlok\").\n\nSatguru Sivaya Subramuniyaswami wrote:\n\nSri Aurobindo developed a very different concept of the mental plane, through his own synthesis of Vedanta (including the Taittiriya Upanishad), Tantra, Theosophy, and Max Théon ideas (which he received via The Mother, who was Theon's student in occultism for two years). In this cosmology, there are seven cosmic planes, three lower, corresponding to relative existence (the Physical, Vital, and Mental), and four higher, representing infinite divine reality (\"Life Divine\" bk.1 ch.27) The Aurobindonian Mind or Mental Plane constitutes a large zone of being from the mental vital to the overmental divine region (\"Letters on Yoga\", Jyoti and Prem Sobel 1984), but as with the later Theosophical concept it constitutes an objective reality of sheer mind or thought.\n\nThe \"buddhic plane\" is described as a realm of pure consciousness. According to Theosophy the buddhic plane exists to develop buddhic consciousness which means to become unselfish and solve any problems with the ego. Charles Leadbeater wrote that in the buddhic plane man casts off the delusion of the self and enters a realization of unity.\n\nAnnie Besant defined the buddhic plane as\n\nPersistent, conscious, spiritual awareness. This is the full consciousness of the buddhic or intuitional level. This is the perceptive consciousness which is the outstanding characteristic of the Hierarchy. The life focus of the man shifts to the buddhic plane. This is the fourth or middle state of consciousness.\n\nSri Aurobindo calls the level above the mental plane the supermind.\n\nGeorge Winslow Plummer wrote that the \"spiritual plane\" is split into many sub-planes and that on these planes live spiritual beings who are more advanced in development and status than ordinary man. According to metaphysical teachings the goal of the spiritual plane is to gain spiritual knowledge and experience.\n\nAccording to some occult teachings, all souls are born on the \"divine plane\" and then descend down through the lower planes; however souls will work their way back to the divine plane. On the divine plane souls can be opened to conscious communication with the sphere of the divine known as the Absolute and receive knowledge about the nature of reality.\n\nRosicrucianism teaches that the divine plane is where Jesus dwelt in Christ consciousness.\n\nThe \"logoic plane\" is the highest plane, it has been described as a plane of total oneness, the \"I AM Presence\". Joshua David Stone describes the plane as complete unity with God.\n\nThe \"monadic plane (hyperplane)\" or continuum/universe, enclosing and interpenetrating grosser hyperplanes, respectively is the plane in which the monad or holy spirit or oversoul is said to exist.\n\nIn Buddhism, the world is made up of 31 planes of existence that one can be reborn into, separated into 3 realms.\n\nThe Summerland is the name given by Theosophists, Spiritualists, Wiccans, and some earth-based contemporary pagan religions to their conceptualization of existence on a plane in an afterlife.\n\nEmanuel Swedenborg (1688–1772) inspired Andrew Jackson Davis (1826–1910), in his major work \"The Great Harmonia\" to say that Summerland is the pinnacle of spiritual achievement in the afterlife; that is, it is the highest level, or \"sphere\", of the afterlife we can hope to enter. The common portrayal of the Summerland is as a place of rest for souls after or between their earthly incarnations. Some believe spirits will stay in the Summerland for an eternal afterlife, though others believe after an amount of time some spirits will reincarnate. The Summerland is also envisioned as a place for recollection and reunion with deceased loved ones.\n\nAs the name suggests, it is often imagined as a place of beauty and peace, where everything people hold close to their hearts is preserved in its fullest beauty for eternity. It is envisioned as containing wide (possibly eternal) fields of rolling green hills and lush grass. In many ways, this ideology is similar to the Welsh view of Annwn as an afterlife realm. The Summerland is also viewed as the place where one goes in the afterlife in traditions of Spiritualism and Theosophy, which is where Wicca got the term.\n\nIn Theosophy, the term \"Summerland\" is used without the definite article \"the\". Summerland, also called the Astral plane Heaven, is depicted as where souls who have been good in their previous lives go between incarnations. Those who have been bad go to Hell, which is believed to be located below the surface of the Earth and is on the astral plane and is composed of the densest astral matter; the Spiritual Hierarchy functioning within Earth functions on the etheric plane below the surface of the Earth.\n\nIt is believed by Theosophists that most people (those at high levels of initiation) go to a specific Summerland that is set up for people of each religion. For example, Christians go to a Christian heaven, Jews go to a Jewish heaven, Muslims go to a Muslim heaven, Hindus goes to a Hindu heaven, Theosophists go to a Theosophical heaven, and so forth, each heaven being like that described in the scriptures of that religion. There is also a generic Summerland for those who were atheists or agnostics in their previous lives. People who belong to religions that don't believe in reincarnation are surprised to find out when they get to heaven that they will have to reincarnate again within a few dozen to a few hundred years. Each heaven is believed to be an extensive structure composed of astral matter located on the astral plane about three or four miles (5–6 km) above the surface of Earth, above that part of the world where the particular religion that the heaven is meant for is most predominant.\n\nTheosophists also believe there is another higher level of heaven called Devachan, also called the Mental plane Heaven, which some but not all souls reach between incarnations—only those souls that are more highly developed spiritually reach this level, those souls that are at the first, second, and third levels of initiation. Devachan is several miles (around 10 km) higher above the surface of Earth than Summerland.\n\nThe final permanent eternal afterlife heaven to which Theosophists believe most people will go millions or billions of years in the future, after our cycle of reincarnations in this Round is over. In order to go to Nirvana, it is necessary to have attained the fourth level of initiation or higher, meaning one is an \"arhat\" and thus no longer needs to reincarnate.\n\nOccult writers such as Geoffrey Hodson, Mellie Uyldert, and Dora van Gelder had attempted to classify different spiritual beings into a hierarchy based on their assumed place and function on the planes of existence.\n\nCharles Webster Leadbeater fundamentally described and incorporated his comprehension of intangible beings for Theosophy. Along with him there are various planes intertwined with the quotidian human world and are all inhabited by multitudes of entities. Each plane is purported as composed of discrete density of \"astral or ethereal matter\" and frequently the denizens of a plane have no discernment of other ones. Other Theosophical writers such as Alice Bailey, a contemporary of Leadbeater, also gave continuousness to Theosophical concepts of ethereal beings and her works had a great impact over New Age movement. She puts the nature spirits and devas as ethereal beings immersed in macro divisions of an interwoven threefold universe, usually they belong to the etheric, astral, or mental planes. The ethereal entities of the four kingdoms, Earth, Air, Fire, and Water, are forces of nature.\n\nThe Dutch writer Mellie Uyldert, self-proclaimed clairvoyant, characterized the semblance and behavior of ethereal entities on the etheric plane, which, she said, hover above plants and transfer energy for vitalizing the plant, then nourishing themselves on rays of sunlight. She depicted them as asexual gender, and composed of \"etheric\" matter.\nThey fly three meters over the ground, some have wings like butterflies while others only have a small face and an aura waving graciously. Some are huge while others may have the size of one inch.\n\n\n\n"}
{"id": "36055387", "url": "https://en.wikipedia.org/wiki?curid=36055387", "title": "Planetary consciousness", "text": "Planetary consciousness\n\nPlanetary consciousness is the idea that human beings are members of a planetary society of Earth as much as they are members of their nations, provinces, districts, islands, cities or villages.\n\nIn his 1906 book \"American Character\", author Brander Matthews mentions the idea of a \"league of nations\" and a \"planetary consciousness\", believing it would be created by American politicians in the coming centuries. Key planetary consciousness events of the 20th century include the creation of the League of Nations, the signing of Kellogg-Briand Pact, the creation of the United Nations, and the creation of the Bretton Woods system. Democratic globalization advocate Abhay Kumar points to the International Corporation of Assigned Names and Numbers (ICANN) board of directors election in 2000, which were conducted globally, as the first example of global democracy. In September 2001, Ervin László and the Dalai Lama wrote an essay titled \"Manifesto on Planetary Consciousness\", which was adopted at a meeting at the Hungarian Academy of Sciences in Budapest. Its introduction begins:\n\nAndreas Bummel, CEO of the Committee for a Democratic UN, says, \"The first step into the direction of a world parliament would be the establishment of a Parliamentary Assembly at the United Nations\".\n\nAdvocacy for the idea of planetary consciousness is based on the technological advancements made by the mankind in the fields of transport and telecommunications during the 20th century and in the first decade of the 21st century. Kumar claims that these technological advancements have turned the whole planet into an interdependent economic, political and communication community. He specifically cites the invention of the Internet and the mobile phones as key technological achievements of the 20th century which brought humans into more continuous interconnected communication. He believes that these inventions will lead to a second Renaissance and global democracy, just as the Gutenberg press in 1439 led to the first Renaissance, the Age of Enlightenment, and Nation states. Bummel describes planetary consciousness as integral, insofar as it does not conflict with other levels of social identity, but instead is a holistic perspective on humanity and the planet as a whole. Steven Kull writes that while nation states are reluctant to work cooperatively, individuals seem more willing.\n\nAuthor Shashi Tharoor feels that an Earth Anthem sung by people across the world can inspire planetary consciousness and global citizenship among people.\n\n"}
{"id": "45477986", "url": "https://en.wikipedia.org/wiki?curid=45477986", "title": "Princesses combinées", "text": "Princesses combinées\n\nLes princesses combinées (\"the combined princesses\") was a group of French aristocrats during the reign of Louis XVI of France.\nSome authors named two, three or four members of the coterie.\nThey were:\n\nThey were close friends since childhood.\nThey followed Rousseau, Voltaire and the Encyclopedistes, supported each other and were discreet in their love affairs, contrasting with other ladies of the court.\nTheir social circle included the De Lameth brothers, the duke of Guines and Henriette-Lucy, Marquise de La Tour du Pin Gouvernet.\n\n"}
{"id": "59103038", "url": "https://en.wikipedia.org/wiki?curid=59103038", "title": "Psychological inertia", "text": "Psychological inertia\n\nPsychological inertia refers to a lack of intervention in the course of affairs. Psychological inertia can be distinguished from status quo maintenance which refers to maintaining the existing state of affairs. For example, consider a pristine lake where an industrial firm is planning to dump toxic chemicals. Maintaining the status quo would involve intervening to prevent the firm from dumping toxic chemicals in the lake. Conversely, inertia would involve not intervening in the course of events that will change the lake.\n\nRitov and Baron (1992) have shown that the status quo bias often results from psychological inertia rather than from status quo maintenance. They have attributed this finding to omission bias. \n\nDavid Gal has argued that many effects commonly attributed to loss aversion, including status quo bias, the endowment effect, and a tendency to choose risky over safe options in some contexts, are better explained by psychological inertia.\n"}
{"id": "25082257", "url": "https://en.wikipedia.org/wiki?curid=25082257", "title": "Reach (mathematics)", "text": "Reach (mathematics)\n\nIn mathematics, the reach of a subset of Euclidean space R is a real number that roughly describes how curved the boundary of the set is.\n\nLet \"X\" be a subset of R. Then reach of \"X\" is defined as \n\nShapes that have reach infinity include\n\nThe graph of \"ƒ\"(\"x\") = |\"x\"| has reach zero.\n\nA circle of radius \"r\" has reach \"r\".\n"}
{"id": "23553600", "url": "https://en.wikipedia.org/wiki?curid=23553600", "title": "Safe mode (spacecraft)", "text": "Safe mode (spacecraft)\n\nSafe mode is an operating mode of a modern spacecraft during which all non-essential systems are shut down and only essential functions such as thermal management, radio reception and attitude control are active.\n\nSafe mode is entered automatically upon the detection of a predefined operating condition or event that may indicate loss of control or damage to the spacecraft. Usually the trigger event is a system failure or detection of operating conditions considered dangerously out of the normal range. Cosmic rays penetrating spacecraft electrical systems can create false signals or commands and thus cause a trigger event. The central processor electronics are especially prone to such events.\nAnother trigger is the lack of a received command within a given time window. Lack of received commands can be caused by hardware failures or mis-programming of the spacecraft, as in the case of the Viking 1 lander.\n\nThe process of entering safe mode, sometimes referred to as \"safing\", involves a number of immediate physical actions taken to prevent damage or complete loss. Power is removed from non-essential subsystems. Regaining attitude control, if lost, is the highest priority because it is necessary to maintain thermal balance and proper illumination of the solar panels. A tumbling or cartwheeling spacecraft can quickly roast, freeze or exhaust its battery power and be lost forever.\n\nWhile in safe mode the preservation of the spacecraft is the highest priority. Typically all non-essential systems, such as science instruments, are shut down. The spacecraft attempts to maintain orientation with respect to the Sun for illumination of solar panels and for thermal management. The spacecraft then awaits radio commands from its mission control center monitoring for signals on its low-gain omnidirectional antenna. Exactly what happens while in safe mode is dependent on the spacecraft design and its mission.\n\nRecovery from safe mode involves reestablishing communication between the spacecraft and mission control, downloading any diagnostic data and sequencing power back on to the various subsystems to resume the mission. The recovery time can be anywhere from a few hours to days or weeks depending on the difficulty in reestablishing communications, conditions found on the spacecraft, distance to the spacecraft and the nature of the mission.\n\nA spacecraft's ability to enter safe mode may be suppressed during crucial spacecraft operations (such as the orbit insertion maneuver of the \"Cassini\" spacecraft at Saturn), during which – if a critical failure were to occur – most, if not all, of the mission objectives would be lost anyway.\n\nOn occasion, a spacecraft is placed in safe mode deliberately by mission control, as the Spirit rover was on sol 451.\n\n\n\n\n\n\n\n\n\nThe term \"safing\" is also used to describe the process of rendering a weapon inactive (safe).\n"}
{"id": "659686", "url": "https://en.wikipedia.org/wiki?curid=659686", "title": "Schizotypy", "text": "Schizotypy\n\nIn psychology, schizotypy is a theoretical concept that posits a continuum of personality characteristics and experiences, ranging from normal dissociative, imaginative states to extreme states of mind related to psychosis, especially schizophrenia. The continuum of personality proposed in schizotypy is in contrast to a categorical view of psychosis, wherein psychosis is considered a particular (usually pathological) state of mind, which the person either has or does not have.\n\nThe categorical view of psychosis is most associated with Emil Kraepelin, who created criteria for the medical diagnosis and classification of different forms of psychotic illness. Particularly, he made the distinction between dementia praecox (now called schizophrenia), manic depressive insanity and non-psychotic states. Modern diagnostic systems used in psychiatry (such as the DSM) maintain this categorical view.\n\nIn contrast, psychiatrist Eugen Bleuler did not believe there was a clear separation between sanity and madness, believing instead that psychosis was simply an extreme expression of thoughts and behaviours that could be present to varying degrees throughout the population.\n\nThe concept of psychosis as a spectrum was further developed by psychologists such as Hans Eysenck and Gordon Claridge, who sought to understand unusual variations in thought and behaviour in terms of personality theory. Eysenck conceptualised cognitive and behavioral variations as all together forming a single personality trait, \"psychoticism\".\n\nClaridge named his concept \"schizotypy,\" and through examination of unusual experiences in the general population and clustering of symptoms in individuals diagnosed with schizophrenia, the work of Claridge suggested that this personality trait was more complex than had been previously thought and could be broken down into four factors.\n\n\nAlthough aiming to reflect some of the features present in diagnosable mental illness, schizotypy does not necessarily imply that someone who is more schizotypal than someone else is more ill. For example, certain aspects of schizotypy may be beneficial. Both the \"unusual experiences\" and \"cognitive disorganisation\" aspects have been linked to creativity and artistic achievement. Jackson proposed the concept of ‘benign schizotypy’ in relation to certain classes of religious experience, which he suggested might be regarded as a form of problem-solving and therefore of adaptive value. The link between positive schizotypy and certain facets of creativity is consistent with the notion of a \"healthy schizotypy\", which may account for the persistence of schizophrenia-related genes in the population despite their many dysfunctional aspects.\n\nHowever, the exact nature of the relationship between schizotypy and diagnosable psychotic illness is still controversial. One of the key concerns that researchers have had is that questionnaire-based measures of schizotypy, when analysed using factor analysis, do not suggest that schizotypy is a unified, homogeneous concept. The three main approaches have been labelled as 'quasi-dimensional', ‘dimensional’ and ‘fully dimensional’.\n\nEach approach is sometimes used to imply that schizotypy reflects a cognitive or biological vulnerability to psychosis, although this may remain dormant and never express itself, unless triggered by appropriate environmental events or conditions (such as certain doses of drugs or high levels of stress).\n\nThe quasi-dimensional model may be traced back to Bleuler (the inventor of the term ‘schizophrenia’), who commented on two types of continuity between normality and psychosis: that between the schizophrenic and his or her relatives, and that between the patient’s premorbid and post-morbid personalities (i.e. their personality before and after the onset of overt psychosis).\n\nOn the first score he commented: ‘If one observes the relatives of our patients, one often finds in them peculiarities which are qualitatively identical with those of the patients themselves, so that the disease appears to be only a quantitative increase of the anomalies seen in the parents and siblings.’\n\nOn the second point, Bleuler discusses in a number of places whether peculiarities displayed by the patient before admission to hospital should be regarded as premonitory \"symptoms\" of the disease or merely indications of a \"predisposition\" to develop it.\n\nDespite these observations of continuity Bleuler himself remained an advocate of the disease model of schizophrenia. To this end he invoked a concept of \"latent schizophrenia\", writing: ‘In [the latent] form, we can see \"in nuce\" [in a nutshell] all the symptoms and all the combinations of symptoms which are present in the manifest types of the disease.’\n\nLater advocates of the quasi-dimensional view of schizotypy are Rado and Meehl, according to both of whom schizotypal symptoms merely represent less explicitly expressed manifestations of the underlying disease process which is schizophrenia. Rado proposed the term ‘schizotype’ to describe the person whose genetic make-up gave him or her a lifelong predisposition to schizophrenia.\n\nThe quasi-dimensional model is so called because the only dimension it postulates is that of gradations of severity or explicitness in relation to the symptoms of a disease process: namely schizophrenia.\n\nThe dimensional approach, influenced by personality theory, argues that full blown psychotic illness is just the most extreme end of the schizotypy spectrum and there is a natural continuum between people with low and high levels of schizotypy. This model is most closely associated with the work of Hans Eysenck, who regarded the person exhibiting the full-blown manifestations of psychosis as simply someone occupying the extreme upper end of his ‘psychoticism’ dimension.\n\nSupport for the dimensional model comes from the fact that high-scorers on measures of schizotypy may meet, or partially fulfill, the diagnostic criteria for schizophrenia spectrum disorders, such as schizophrenia, schizoaffective disorder, schizoid personality disorder and schizotypal personality disorder. Similarly, when analyzed, schizotypy traits often break down into similar groups as do symptoms from schizophrenia (although they are typically present in much less intense forms).\n\nClaridge calls the latest version of his model ‘the fully dimensional approach’. However, it might also be characterised as the \"hybrid\" or \"composite\" approach, as it incorporates elements of both the disease model and the dimensional one.\n\nOn this latest Claridge model, schizotypy is regarded as a dimension of personality, normally distributed throughout the population, as in the Eysenck model. However, schizophrenia itself is regarded as a breakdown process, quite distinct from the continuously distributed trait of schizotypy, and forming a second, graded continuum, ranging from schizotypal personality disorder to full-blown schizophrenic psychosis.\n\nThe model is characterised as fully dimensional because, not only is the personality trait of schizotypy continuously graded, but the independent continuum of the breakdown processes is also graded rather than categorical.\n\nThe fully dimensional approach argues that full blown psychosis is not just high schizotypy, but must involve other factors that make it qualitatively different and pathological.\n\nMany research studies have examined the relationship between schizotypy and various standard models of personality, such as the Five factor model. Research has linked the unusual experiences factor to high neuroticism and openness to experience. The introvertive anhedonia factor has been linked to high neuroticism and low extraversion. The cognitive disorganisation factor has been linked to low conscientiousness. It has been argued that these findings provide evidence for a fully dimensional model of schizotypy and that there is a continuum between normal personality and schizotypy. \n\nRelationships between schizotypy and the Temperament and Character Inventory have also been examined. Self-transcendence, a trait associated with openness to \"spiritual\" ideas and experiences, has moderate positive associations with schizotypy, particularly with unusual experiences. Cloninger described the specific combination of high self-transcendence, low cooperativeness, and low self-directedness as a \"schizotypal personality style\" and research has found that this specific combination of traits is associated with a \"high risk\" of schizotypy. Low cooperativeness and self-directedness combined with high self-transcendence may result in openness to odd or unusual ideas and behaviours associated with distorted perceptions of reality. On the other hand, high levels of cooperativeness and self-directedness may protect against the schizotypal tendencies associated with high self-transcendence.\n\nAnhedonia, or a reduced ability to experience pleasure, is a feature of full-blown schizophrenia that was commented on by both Kraepelin and Bleuler. However, they regarded it as just one among a number of features that tended to characterise the ‘deterioration’, as they saw it, of the schizophrenic’s emotional life. In other words, it was an effect, rather than a cause, of the disease process.\n\nRado reversed this way of thinking, and ascribed anhedonia a causal role. He considered that the crucial neural deficit in the schizotype was an ‘integrative pleasure deficiency’, i.e. an innate deficiency in the ability to experience pleasure. Meehl took on this view, and attempted to relate this deficiency to abnormality in the dopamine system in the brain, which is implicated in the human reward system.\n\nQuestionnaire research on schizotypy in normal subjects is ambiguous with regard to the causal role, if any, of anhedonia. Nettle and McCreery and Claridge found that high schizotypes as measured by factor 1 (above) scored lower than controls on the introverted anhedonia factor, as if they were particularly enjoying life.\n\nVarious writers, including Kelley and Coursey and L.J. and J.P. Chapman suggest that anhedonia, if present as a pre-existent trait in a person, may act as a potentiating factor, whereas a high capacity for hedonic enjoyment might act as a protecting one.\n\nA number of studies have found that high schizotypes, as measured by questionnaire, show less negative priming than controls. Negative priming is said to occur when a person reacts more slowly than usual to a stimulus which has previously been presented as a distractor and which has therefore had to be ignored. Beech interprets the relative weakness of the negative priming effect in schizotypes as a sign that ‘inhibition of distracting information is reduced in schizophrenia and high schizotypes’.\n\nThe reduced negative priming shown by high schizotypes has the interesting effect that they actually perform better on certain tasks (those that require them to respond to previously ignored stimuli) than low schizotypes. This phenomenon may be of significance in the relation to the question of why schizotypy, and indeed schizophrenia itself, is not progressively ‘weeded out’ by the process of natural selection.\n\nThe phenomenon of semantic activation without conscious identification (SAWCI) is said to be displayed when a person shows a priming effect from the processing of consciously undetectable words. For example, a person who has just been shown the word ‘giraffe’, but at a speed at which he or she was not able consciously to report what it was, may nevertheless identify more quickly than usual another animal word on the next trial. Evans found that high schizotypes showed a greater priming effect than controls in such a situation. She argued that this could be accounted for by a relative weakness of inhibitory mechanisms in the semantic networks of high schizotypes.\n\nSchizotypy symptoms have been related to deficits in executive functions, which entails the psychological processes that supersede habitual inclinations with novel responses and behaviors to fulfill important goals. In particular, when schizotypy is elevated, the ability to filter out task-irrelevant stimuli may be impaired. That is, participants who score highly on schizotypy tend to fail to ignore a previously preexposed, non-reinforced stimulus as compared to a non-preexposed, novel and potentially important event.\n\nEnhanced performance on verbal fluency has been associated with high levels of positive schizotypy, i.e. increased reports of hallucination-like experiences, delusional ideation, and perceptual aberrations. However, decreased performance was associated with negative schizotypy, such as anhedonia\nMany studies have also shown that individuals who exhibit schizotypy features demonstrate deficits in Attention and Working memory.\n\nClaridge suggested that one consequence of a weakness of inhibitory mechanisms in high schizotypes and schizophrenics might be a relative failure of homeostasis in the central nervous system. This, it was proposed, could lead, both to lability of arousal, and to dissociation of arousal in different parts of the nervous system.\n\nClaridge and co-workers have found various types of abnormal co-variation between different psychophysiological variables in schizotypes, including between measures of cortical and autonomic arousal.\n\nMcCreery and Claridge found evidence of a relative activation of the right cerebral hemisphere as compared with the left in high schizotypes attempting to induce a hallucinatory episode in the laboratory. This suggested a relative dissociation of arousal between the two hemispheres in such people as compared with controls.\n\nA failure of homeostasis in the central nervous system could lead to episodes of hyper-arousal. Oswald has pointed out that extreme stress and hyper-arousal can lead to sleep as a provoked reaction. McCreery has suggested that this could account for the phenomenological similarities between Stage 1 sleep and psychosis, which include hallucinations, delusions, and flattened or inappropriate affect (emotions). On this model, high schizotypes and schizophrenics are people who are liable to what Oswald calls ‘micro-sleeps’, or intrusions of Stage 1 sleep phenomena into waking consciousness, on account of their tendency to high arousal.\n\nIn support of this view McCreery points to the high correlation that has been found to exist between scores on the Chapmans’ Perceptual Aberration scale, which measures proneness to perceptual anomalies such as hallucinations, and the Chapmans’ Hypomania scale, which measures a tendency to episodes of heightened arousal. This correlation is found despite the fact that there is no overlap of item content between the two scales.\n\nIn the clinical field there is also the paradoxical finding of Stevens and Darbyshire, that schizophrenic patients exhibiting the symptom of catatonia can be aroused from their apparent stupor by the administration of sedative rather than stimulant drugs. They wrote: ‘The psychic state in catatonic schizophrenia can be described as one of great excitement (i.e., hyperalertness)[…] The inhibition of activity apparently does not alter the inner seething excitement.'\n\nIt is argued that such a view would be consistent with the model that suggests schizophrenics and high schizotypes are people with a tendency to hyper-arousal.\n\nKapur (2003) proposed that a hyperdopaminergic state, at a \"brain\" level of description, leads to an aberrant assignment of salience to the elements of one’s experience, at a \"mind\" level. Dopamine mediates the conversion of the neural representation of an external stimulus from a neutral bit of information into an attractive or aversive entity, i.e. a salient event. Symptoms of schizophrenia and schizotypy may arise out of ‘the aberrant assignment of salience to external objects and internal representations’; and antipsychotic medications may reduce positive symptoms by attenuating aberrant motivational salience, via blockade of the Dopamine D2 receptors (Kapur, 2003). There is no evidence however on a link between attentional irregularities and enhanced stimulus salience in schizotypy.\n\n\n"}
{"id": "20130360", "url": "https://en.wikipedia.org/wiki?curid=20130360", "title": "Seasonal subseries plot", "text": "Seasonal subseries plot\n\nSeasonal subseries plots are a graphical tool to visualize and detect seasonality in a time series. Seasonal subseries plots involves the extraction of the seasons from a time series into a subseries. Based on a selected periodicity, it is an alternative plot that emphasizes the seasonal patterns are where the data for each season are collected together in separate mini time plots.\n\nSeasonal subseries plots enables the underlying seasonal pattern to be seen clearly, and also shows the changes in seasonality over time. Especially, it allows to detect changes between different seasons, changes within a particular season over time.\n\nHowever, this plot is only useful if the period of the seasonality is already known. In many cases, this will in fact be known. For example, monthly data typically has a period of 12. If the period is not known, an autocorrelation plot or spectral plot can be used to determine it. If there is a large number of observations, then a box plot may be preferable.\n\nSeasonal sub-series plots are formed by\n\nThe horizontal line displays the mean value for each month over the time series.\n\nThe analyst must specify the length of the seasonal pattern before generating this plot. In most cases, the analyst will know this from the context of the problem and data collection.\n\nIt is important to know when analyzing a time series if there is a significant seasonality effect. The seasonal subseries plot is an excellent tool for determining if there is a seasonal pattern. The seasonal subseries plot can provide answers to the following questions:\nPractically, seasonal subseries plots are often inspected as a preliminary screening tool. They allow visual inferences to be drawn from data prior to modelling and forecasting.\n\n\nSeasonal subseries plots can be implemented in the R programming language using function monthplot().\n\nExample\n\nThe following R code results in the above seasonal deviation plot of \"antidiabetic drug sales\";\n\n> monthplot(a10, ylab= \"$ million\" , xlab= \"Month\", xaxt= \"n\", main= \"Seasonal deviation plot: antidiabetic drug sales\")\n\n> axis(1, at=1:12, labels=month.abb, cex=0.8)\n\n\n"}
{"id": "2534964", "url": "https://en.wikipedia.org/wiki?curid=2534964", "title": "Sensory processing", "text": "Sensory processing\n\nSensory processing is the process that organizes sensation from one’s own body and the environment, thus making it possible to use the body effectively within the environment. Specifically, it deals with how the brain processes multiple sensory modality inputs, such as proprioception, vision, auditory system, tactile, olfactory, vestibular system, interoception, and taste into usable functional outputs.\n\nIt has been believed for some time that inputs from different sensory organs are processed in different areas in the brain. The communication within and among these specialized areas of the brain is known as functional integration. Newer research has shown that these different regions of the brain may not be solely responsible for only one sensory modality, but could use multiple inputs to perceive what the body senses about its environment. Multisensory integration is necessary for almost every activity that we perform because the combination of multiple sensory inputs is essential for us to comprehend our surroundings.\n\nIt has been believed for some time that inputs from different sensory organs are processed in different areas in the brain, relating to systems neuroscience. Using functional neuroimaging, it can be seen that sensory-specific cortices are activated by different inputs. For example, regions in the occipital cortex are tied to vision and those on the superior temporal gyrus are recipients of auditory inputs. There exist studies suggesting deeper multisensory convergences than those at the sensory-specific cortices, which were listed earlier. This convergence of multiple sensory modalities is known as multisensory integration.\n\nSensory processing deals with how the brain processes sensory input from multiple sensory modalities. These include the five classic senses of vision (sight), audition (hearing), tactile stimulation (touch), olfaction (smell), and gustation (taste). Other sensory modalities exist, for example the vestibular sense (balance and the sense of movement) and proprioception (the sense of knowing one's position in space) Along with Time (The sense of knowing where one is in time or activities). It is important that the information of these different sensory modalities must be relatable. The sensory inputs themselves are in different electrical signals, and in different contexts. Through sensory processing, the brain can relate all sensory inputs into a coherent percept, upon which our interaction with the environment is ultimately based.\n\nThe different senses were always thought to be controlled by separate lobes of the brain, called projection areas. The lobes of the brain are the classifications that divide the brain both anatomically and functionally. These lobes are the Frontal lobe, responsible for conscious thought, Parietal lobe, responsible for visuospatial processing, the Occipital lobe, responsible for the sense of sight, and the temporal lobe, responsible for the senses of smell and sound. From the earliest times of neurology, it has been thought that these lobes are solely responsible for their one sensory modality input. However, newer research has shown that that may not entirely be the case.\n\nSometimes there can be a problem with the encoding of the sensory information. This disorder is known as Sensory processing disorder (SPD). This disorder can be further classified into three main types. \nThere are several therapies used to treat SPD. Anna Jean Ayres claimed that a child needs a healthy \"sensory diet,\" which is all of the activities that children engage in, that gives them the necessary sensory inputs that they need to get their brain into improving sensory processing.\n\nIn the 1930s, Dr. Wilder Penfield was conducting a very bizarre operation at the Montreal Neurological Institute. Dr. Penfield \"pioneered the incorporation of neurophysiological principles in the practice of neurosurgery. Dr. Penfield was interested in determining a solution to solve the epileptic seizure problems that his patients were having. He used an electrode to stimulate different regions of the brain's cortex, and would ask his still conscious patient what he or she felt. This process led to the publication of his book, The Cerebral Cortex of Man. The \"mapping\" of the sensations his patients felt led Dr. Penfield to chart out the sensations that were triggered by stimulating different cortical regions. Mrs. H. P. Cantlie was the artist Dr. Penfield hired to illustrate his findings. The result was the conception of the first sensory Homunculus.\n\nThe Homonculus is a visual representation of the intensity of sensations derived from different parts of the body. Dr. Wilder Penfield and his colleague Herbert Jasper developed the Montreal procedure using an electrode to stimulate different parts of the brain to determine which parts were the cause of the epilepsy. This part could then be surgically removed or altered in order to regain optimal brain performance. While performing these tests, they discovered that the functional maps of the sensory and motor cortices were similar in all patients. Because of their novelty at the time, these Homonculi were hailed as the \"E=mc² of Neuroscience\".\n\nThere are still no definitive answers to the questions regarding the relationship between functional and structural asymmetries in the brain. There are a number of asymmetries in the human brain including how language is processed mainly in the left hemisphere of the brain. There have been some cases, however, in which individuals have comparable language skills to someone who uses his left hemisphere to process language, yet they mainly use their right or both hemispheres. These cases pose the possibility that function may not follow structure in some cognitive tasks. Current research in the fields of sensory processing and multisensory integration is aiming to hopefully unlock the mysteries behind the concept of brain lateralization.\n\nResearch on sensory processing has much to offer towards understanding the function of the brain as a whole. The primary task of multisensory integration is to figure out and sort out the vast quantities of sensory information in the body through multiple sensory modalities. These modalities not only are not independent, but they are also quite complementary. Where one sensory modality may give information on one part of a situation, another modality can pick up other necessary information. Bringing this information together facilitates the better understanding of the physical world around us.\n\nIt may seem redundant that we are being provided with multiple sensory inputs about the same object, but that is not necessarily the case. This so-called \"redundant\" information is in fact verification that what we are experiencing is in fact happening. Perceptions of the world are based on models that we build of the world. Sensory information informs these models, but this information can also confuse the models. Sensory illusions occur when these models do not match up. For example, where our visual system may fool us in one case, our auditory system can bring us back to a ground reality. This prevents sensory misrepresentations, because through the combination of multiple sensory modalities, the model that we create is much more robust and gives a better assessment of the situation. Thinking about it logically, it is far easier to fool one sense than it is to simultaneously fool two or more senses.\n\nOne of the earliest sensations is the olfactory sensation. Evolutionary, gustation and olfaction developed together. This multisensory integration was necessary for early humans in order to ensure that they were receiving proper nutrition from their food, and also to make sure that they were not consuming poisonous materials. There are several other sensory integrations that developed early on in the human evolutionary time line. The integration between vision and audition was necessary for spatial mapping. Integration between vision and tactile sensations developed along with our finer motor skills including better hand-eye coordination. While humans developed into bipedal organisms, balance became exponentially more essential to survival. The multisensory integration between visual inputs, vestibular (balance) inputs, and proprioception inputs played an important role in our development into upright walkers.\n\nPerhaps one of the most studied sensory integrations is the relationship between vision and audition. These two senses perceive the same objects in the world in different ways, and by combining the two, they help us understand this information better. Vision dominates our perception of the world around us. This is because visual spatial information is one of the most reliable sensory modalities. Visual stimuli are recorded directly onto the retina, and there are few, if any, external distortions that provide incorrect information to the brain about the true location of an object. Other spatial information is not as reliable as visual spatial information. For example, consider auditory spatial input. The location of an object can sometimes be determined solely on its sound, but the sensory input can easily be modified or altered, thus giving a less reliable spatial representation of the object. Auditory information therefore is not spatially represented unlike visual stimuli. But once one has the spatial mapping from the visual information, multisensory integration helps bring the information from both the visual and auditory stimuli together to make a more robust mapping.\n\nThere have been studies done that show that a dynamic neural mechanism exists for matching the auditory and visual inputs from an event that stimulates multiple senses. One example of this that has been observed is how the brain compensates for target distance. When you are speaking with someone or watching something happen, auditory and visual signals are not being processed concurrently, but they are perceived as being simultaneous. This kind of multisensory integration can lead to slight misperceptions in the visual-auditory system in the form of the ventriloquist effect. An example of the ventriloquism effect is when a person on the television appears to have his voice coming from his mouth, rather than the television's speakers. This occurs because of a pre-existing spatial representation within the brain which is programmed to think that voices come from another human's mouth. This then makes it so the visual response to the audio input is spatially misrepresented, and therefore misaligned.\n\nHand eye coordination is one example of sensory integration. In this case, we require a tight integration of what we visually perceive about an object, and what we tactilely perceive about that same object. If these two senses were not combined within the brain, then one would have less ability to manipulate an object. Hand-eye coordination is the tactile sensation in the context of the visual system. The visual system is very static, in that it doesn't move around much, but the hands and other parts used in tactile sensory collection can freely move around. This movement of the hands must be included in the mapping of both the tactile and visual sensations, otherwise one would not be able to comprehend where they were moving their hands, and what they were touching and looking at. An example of this happening is looking at an infant. The infant picks up objects and puts them in his mouth, or touches them to his feet or face. All of these actions are culminating to the formation of spatial maps in the brain and the realization that \"Hey, that thing that's moving this object is actually a part of me.\" Seeing the same thing that they are feeling is a major step in the mapping that is required for infants to begin to realize that they can move their arms and interact with an object. This is the earliest and most explicit way of experiencing sensory integration.\n\nIn the future, research on sensory integration will be used to better understand how different sensory modalities are incorporated within the brain to help us perform even the simplest of tasks. For example, we do not currently have the understanding needed to comprehend how neural circuits transform sensory cues into changes in motor activities. More research done on the sensorimotor system can help understand how these movements are controlled. This understanding can potentially be used to learn more about how to make better prosthetics, and eventually help patients who have lost the use of a limb. Also, by learning more about how different sensory inputs can combine can have profound effects on new engineering approaches using robotics. The robot's sensory devices may take in inputs of different modalities, but if we understand multisensory integration better, we might be able to program these robots to convey these data into a useful output to better serve our purposes.\n\n\n"}
{"id": "61117", "url": "https://en.wikipedia.org/wiki?curid=61117", "title": "Taboo", "text": "Taboo\n\nIn any given society, a taboo is an implicit prohibition on something (usually against an utterance or behavior) based on a cultural sense that it is excessively repulsive or, perhaps, too sacred for ordinary people. Such prohibitions are present in virtually all societies. On a comparative basis taboos, for example related to food items, seem to make no sense at all as what may be declared unfit for one group by custom or religion may be perfectly acceptable to another.\n\nWhether scientifically correct or not, taboos are often meant to protect the human individual, but there are numerous other reasons for their existence. An ecological or medical background is apparent in many, including some that are seen as religious or spiritual in origin. Taboos can help utilize a resource more efficiently, but when applied to only a subsection of the community they can also serve to suppress a subsection of the community. A taboo acknowledged by a particular group or tribe as part of their ways, aids in the cohesion of the group, helps that particular group to stand out and maintain its identity in the face of others and therefore creates a feeling of \"belonging\".\n\nThe meaning of the word \"taboo\" has been somewhat expanded in the social sciences to strong prohibitions relating to any area of human activity or custom that is sacred or forbidden based on moral judgment, religious beliefs, or cultural norms. \"Breaking a taboo\" is usually considered objectionable by society in general, not merely a subset of a culture.\n\nThe term \"taboo\" comes from the Tongan \"tapu\" or Fijian \"tabu\" (\"prohibited\", \"disallowed\", \"forbidden\"), related among others to the Maori \"tapu\", Hawaiian \"kapu\", and Malagasy \"fady\". Its English use dates to 1777 when the British explorer James Cook visited Tonga, and referred to the Tongans' use of the term \"taboo\" for \"any thing is forbidden to be eaten, or made use of\". He wrote:\nThe term was translated to him as \"consecrated, inviolable, forbidden, unclean or cursed.\" \"Tabu\" itself has been derived from alleged Tongan morphemes \"ta\" (\"mark\") and \"bu\" (\"especially\"), but this may be a folk etymology (Tongan does not actually have a phoneme /b/), and \"tapu\" is usually treated as a unitary, non-compound word inherited from Proto-Polynesian *\"tapu\", in turn inherited from Proto-Oceanic *\"tabu\", with the reconstructed meaning \"sacred, forbidden.\" In its current use on Tonga, the word \"tapu\" means \"sacred\" or \"holy\", often in the sense of being restricted or protected by custom or law. On the main island, the word is often appended to the end of \"Tonga\" as \"Tongatapu\", here meaning \"Sacred South\" rather than \"Forbidden South\".\n\nSigmund Freud speculated that incest and patricide were the only two universal taboos and formed the basis of civilization. However, although cannibalism, in-group murder, and incest are taboo in the majority of societies, exceptions can be found, such as marriages between brothers and sisters in Roman Egypt. Modern Western societies, however, do not condone such relationships. These familial sexual activities are criminalised, even if all parties are consenting adults. Through an analysis of the language surrounding these laws, it can be seen how the policy makers, and society as a whole, find these acts to be immoral.\n\nCommon taboos involve restrictions or ritual regulation of killing and hunting; sex and sexual relationships; reproduction; the dead and their graves; as well as food and dining (primarily cannibalism and dietary laws such as vegetarianism, \"kashrut\", and \"halal\") or religious (treif and haram). In Madagascar, a strong code of taboos, known as \"fady\", constantly change and are formed from new experiences. Each region, village or tribe may have its own \"fady\".\n\nThe word \"taboo\" gained popularity at times, with some scholars looking for ways to apply it where other English words had previously been applied. For example, J. M. Powis Smith, in his book \"The American Bible\" (editor's preface 1927), used \"taboo\" occasionally in relation to Israel's Tabernacle and ceremonial laws, including , ; ; , , and .\n\nAlbert Schweitzer wrote a chapter about taboos of the people of Gabon. As an example, it was considered a misfortune for twins to be born, and they would be subject to many rules not incumbent on other people.\n\nCommunist and materialist theorists have argued that taboos can be used to reveal the histories of societies when other records are lacking. Marvin Harris particularly endeavored to explain taboos as a consequence of ecologic and economic conditions.\n\nSome argue that contemporary multicultural societies have taboos against tribalisms (for example, ethnocentrism and nationalism) and prejudices (racism, sexism, religious extremism).\n\nChanging social customs and standards also create new taboos, such as bans on slavery; extension of the pedophilia taboo to ephebophilia; prohibitions on alcohol, tobacco, or psychopharmaceutical consumption (particularly among pregnant women); and the employment of politically correct euphemismsat times quite unsuccessfullyto mitigate various alleged forms of discrimination.\n\nIncest itself has been pulled both ways, with some seeking to normalize consensual adult relationships regardless of the degree of kinship (notably in Europe) and others expanding the degrees of prohibited contact (notably in the United States.)\n\nIn medicine, professionals who practice in ethical and moral grey areas, or fields subject to social stigma such as late termination of pregnancy, may refrain from public discussion of their practice. Among other reasons, this taboo may come from concern that comments may be taken out of the appropriate context and used to make ill-informed policy decisions.\n\n"}
{"id": "35669023", "url": "https://en.wikipedia.org/wiki?curid=35669023", "title": "Tensors in curvilinear coordinates", "text": "Tensors in curvilinear coordinates\n\nCurvilinear coordinates can be formulated in tensor calculus, with important applications in physics and engineering, particularly for describing transportation of physical quantities and deformation of matter in fluid mechanics and continuum mechanics.\n\nElementary vector and tensor algebra in curvilinear coordinates is used in some of the older scientific literature in mechanics and physics and can be indispensable to understanding work from the early and mid 1900s, for example the text by Green and Zerna. Some useful relations in the algebra of vectors and second-order tensors in curvilinear coordinates are given in this section. The notation and contents are primarily from Ogden, Naghdi, Simmonds, Green and Zerna, Basar and Weichert, and Ciarlet.\n\nConsider two coordinate systems with coordinate variables formula_1 and formula_2, which we shall represent in short as just formula_3 and formula_4 respectively and always assume our index formula_5 runs from 1 through 3. We shall assume that these coordinates systems are embedded in the three-dimensional euclidean space. Coordinates formula_3 and formula_4 may be used to explain each other, because as we move along the coordinate line in one coordinate system we can use the other to describe our position. In this way Coordinates formula_3 and formula_4 are functions of each other\n\nformula_10 for formula_11\n\nwhich can be written as\n\nformula_12 for formula_13\n\nThese three equations together are also called a coordinate transformation from formula_4 to formula_3.Let us denote this transformation by formula_16. We will therefore represent the transformation from the coordinate system with coordinate variables formula_4 to the coordinate system with coordinates formula_3 as:\n\nformula_19\n\nSimilarly we can represent formula_4 as a function of formula_3 as follows:\n\nformula_22 for formula_23\n\nsimilarly we can write the free equations more compactly as\n\nformula_24 for formula_13\n\nThese three equations together are also called a coordinate transformation from formula_3 to formula_4. Let us denote this transformation by formula_28. We will represent the transformation from the coordinate system with coordinate variables formula_3 to the coordinate system with coordinates formula_4 as:\n\nformula_31\n\nIf the transformation formula_16 is bijective then we call the image of the transformation,namely formula_3, a set of admissible coordinates for formula_4. If formula_16 is linear the coordinate system formula_3 will be called an affine coordinate system ,otherwise formula_3 is called a curvilinear coordinate system\n\nAs we now see that the Coordinates formula_3 and formula_4 are functions of each other, we can take the derivative of the coordinate variable formula_3 with respect to the coordinate variable formula_4\n\nconsider\n\nformula_42formula_43formula_44 for formula_13, these derivatives can be arranged in a matrix, say formula_46,in which formula_44 is the element in the formula_48row and formula_49column\n\nformula_46formula_51formula_52formula_51formula_54\n\nThe resultant matrix is called the Jacobian matrix.\n\nLet (b, b, b) be an arbitrary basis for three-dimensional Euclidean space. In general, the basis vectors are neither unit vectors nor mutually orthogonal. However, they are required to be linearly independent. Then a vector v can be expressed as\nThe components \"v\" are the contravariant components of the vector v.\n\nThe reciprocal basis (b, b, b) is defined by the relation \nwhere \"δ \" is the Kronecker delta.\n\nThe vector v can also be expressed in terms of the reciprocal basis:\nThe components \"v\" are the covariant components of the vector formula_58.\n\nA second-order tensor can be expressed as\nThe components \"S\" are called the contravariant components, \"S \" the mixed right-covariant components, \"S \" the mixed left-covariant components, and \"S\" the covariant components of the second-order tensor.\n\nThe quantities \"g\", \"g\" are defined as\n\nFrom the above equations we have\n\nThe components of a vector are related by\nAlso,\n\nThe components of the second-order tensor are related by\n\nIn an orthonormal right-handed basis, the third-order alternating tensor is defined as\nIn a general curvilinear basis the same tensor may be expressed as\nIt can be shown that\nNow,\nHence,\nSimilarly, we can show that\n\n\n\nLet (e, e, e) be the usual Cartesian basis vectors for the Euclidean space of interest and let\nwhere \"F\" is a second-order transformation tensor that maps e to b. Then,\nFrom this relation we can show that\nLet formula_101 be the Jacobian of the transformation. Then, from the definition of the determinant,\nSince\nwe have\nA number of interesting results can be derived using the above relations.\n\nFirst, consider\nThen\nSimilarly, we can show that\nTherefore, using the fact that formula_108,\n\nAnother interesting relation is derived below. Recall that\nwhere \"A\" is a, yet undetermined, constant. Then\nThis observation leads to the relations\nIn index notation,\nwhere formula_114 is the usual permutation symbol.\n\nWe have not identified an explicit expression for the transformation tensor F because an alternative form of the mapping between curvilinear and Cartesian bases is more useful. Assuming a sufficient degree of smoothness in the mapping (and a bit of abuse of notation), we have\nSimilarly,\nFrom these results we have\nand\n\nSimmonds, in his book on tensor analysis, quotes Albert Einstein saying\n\nVector and tensor calculus in general curvilinear coordinates is used in tensor analysis on four-dimensional curvilinear manifolds in general relativity, in the mechanics of curved shells, in examining the invariance properties of Maxwell's equations which has been of interest in metamaterials and in many other fields.\n\nSome useful relations in the calculus of vectors and second-order tensors in curvilinear coordinates are given in this section. The notation and contents are primarily from Ogden, Simmonds, Green and Zerna, Basar and Weichert, and Ciarlet.\n\nLet the position of a point in space be characterized by three coordinate variables formula_119.\n\nThe coordinate curve \"q\" represents a curve on which \"q\", \"q\" are constant. Let x be the position vector of the point relative to some origin. Then, assuming that such a mapping and its inverse exist and are continuous, we can write \nThe fields ψ(x) are called the curvilinear coordinate functions of the curvilinear coordinate system ψ(x) = ψ(x).\n\nThe \"q\" coordinate curves are defined by the one-parameter family of functions given by\nwith \"q, q\" fixed.\n\nThe tangent vector to the curve x at the point x(α) (or to the coordinate curve \"q\" at the point x) is\n\nLet \"f\"(x) be a scalar field in space. Then\nThe gradient of the field \"f\" is defined by\nwhere c is an arbitrary constant vector. If we define the components \"c\" of c are such that\nthen\n\nIf we set formula_127, then since formula_128, we have\nwhich provides a means of extracting the contravariant component of a vector c.\n\nIf b is the covariant (or natural) basis at a point, and if b is the contravariant (or reciprocal) basis at that point, then\nA brief rationale for this choice of basis is given in the next section.\n\nA similar process can be used to arrive at the gradient of a vector field f(x). The gradient is given by\nIf we consider the gradient of the position vector field r(x) = x, then we can show that\nThe vector field b is tangent to the \"q\" coordinate curve and forms a natural basis at each point on the curve. This basis, as discussed at the beginning of this article, is also called the covariant curvilinear basis. We can also define a reciprocal basis, or contravariant curvilinear basis, b. All the algebraic relations between the basis vectors, as discussed in the section on tensor algebra, apply for the natural basis and its reciprocal at each point x.\n\nSince c is arbitrary, we can write\n\nNote that the contravariant basis vector b is perpendicular to the surface of constant ψ and is given by\n\nThe Christoffel symbols of the first kind are defined as\nTo express Γ in terms of \"g\" we note that\nSince b = b we have Γ = Γ. Using these to rearrange the above relations gives\n\nThe Christoffel symbols of the second kind are defined as\nin which\n\nThis implies that\nOther relations that follow are\n\nAnother particularly useful relation, which shows that the Christoffel symbol depends only on the metric tensor and its derivatives, is\n\nThe following expressions for the gradient of a vector field in curvilinear coordinates are quite useful.\n\nThe vector field v can be represented as\nwhere formula_145 are the covariant components of the field, formula_146 are the physical components, and (no summation)\nis the normalized contravariant basis vector.\n\nThe gradient of a second order tensor field can similarly be expressed as\n\nIf we consider the expression for the tensor in terms of a contravariant basis, then\nWe may also write\n\nThe physical components of a second-order tensor field can be obtained by using a normalized contravariant basis, i.e.,\nwhere the hatted basis vectors have been normalized. This implies that (again no summation)\n\nThe divergence of a vector field (formula_58)is defined as\nIn terms of components with respect to a curvilinear basis\n\nAn alternative equation for the divergence of a vector field is frequently used. To derive this relation recall that\nNow,\nNoting that, due to the symmetry of formula_158,\nwe have\nRecall that if [\"g\"] is the matrix whose components are \"g\", then the inverse of the matrix is formula_161. The inverse of the matrix is given by\nwhere \"A\" are the Cofactor matrix of the components \"g\". From matrix algebra we have\nHence,\nPlugging this relation into the expression for the divergence gives\nA little manipulation leads to the more compact form\n\nThe divergence of a second-order tensor field is defined using\nwhere a is an arbitrary constant vector.\n\nIn curvilinear coordinates,\n\nThe Laplacian of a scalar field φ(x) is defined as\nUsing the alternative expression for the divergence of a vector field gives us\nNow\nTherefore,\n\nThe curl of a vector field v in covariant curvilinear coordinates can be written as\nwhere\n\nAssume, for the purposes of this section, that the curvilinear coordinate system is orthogonal, i.e.,\nor equivalently,\nwhere formula_177. As before, formula_178 are covariant basis vectors and b, b are contravariant basis vectors. Also, let (e, e, e) be a background, fixed, Cartesian basis. A list of orthogonal curvilinear coordinates is given below.\n\nLet r(x) be the position vector of the point x with respect to the origin of the coordinate system. The notation can be simplified by noting that x = r(x). At each point we can construct a small line element dx. The square of the length of the line element is the scalar product dx • dx and is called the metric of the space. Recall that the space of interest is assumed to be Euclidean when we talk of curvilinear coordinates. Let us express the position vector in terms of the background, fixed, Cartesian basis, i.e.,\n\nUsing the chain rule, we can then express dx in terms of three-dimensional orthogonal curvilinear coordinates (\"q\", \"q\", \"q\") as\nTherefore, the metric is given by\n\nThe symmetric quantity\nis called the fundamental (or metric) tensor of the Euclidean space in curvilinear coordinates.\n\nNote also that\nwhere \"h\" are the Lamé coefficients.\n\nIf we define the scale factors, \"h\", using\nwe get a relation between the fundamental tensor and the Lamé coefficients.\n\nIf we consider polar coordinates for R, note that\n(r, θ) are the curvilinear coordinates, and the Jacobian determinant of the transformation (\"r\",θ) → (\"r\" cos θ, \"r\" sin θ) is \"r\".\n\nThe orthogonal basis vectors are b = (cos θ, sin θ), b = (−\"r\" sin θ, \"r\" cos θ). The normalized basis vectors are e = (cos θ, sin θ), e = (−sin θ, cos θ) and the scale factors are \"h\" = 1 and \"h\"= \"r\". The fundamental tensor is \"g\" =1, \"g\" =\"r\", \"g\" = \"g\" =0.\n\nIf we wish to use curvilinear coordinates for vector calculus calculations, adjustments need to be made in the calculation of line, surface and volume integrals. For simplicity, we again restrict the discussion to three dimensions and orthogonal curvilinear coordinates. However, the same arguments apply for formula_186-dimensional problems though there are some additional terms in the expressions when the coordinate system is not orthogonal.\n\nNormally in the calculation of line integrals we are interested in calculating\nwhere x(\"t\") parametrizes C in Cartesian coordinates.\nIn curvilinear coordinates, the term\nby the chain rule. And from the definition of the Lamé coefficients,\nand thus\nNow, since formula_191 when formula_192, we have\nand we can proceed normally.\n\nLikewise, if we are interested in a surface integral, the relevant calculation, with the parameterization of the surface in Cartesian coordinates is:\nAgain, in curvilinear coordinates, we have\nand we make use of the definition of curvilinear coordinates again to yield\n\nTherefore,\nwhere formula_198 is the permutation symbol.\n\nIn determinant form, the cross product in terms of curvilinear coordinates will be:\n\nIn orthogonal curvilinear coordinates of 3 dimensions, where\none can express the gradient of a scalar or vector field as\nFor an orthogonal basis\nThe divergence of a vector field can then be written as\nAlso,\nTherefore,\nWe can get an expression for the Laplacian in a similar manner by noting that\nThen we have\nThe expressions for the gradient, divergence, and Laplacian can be directly extended to \"n\"-dimensions.\n\nThe curl of a vector field is given by\nwhere ε is the Levi-Civita symbol.\n\nFor cylindrical coordinates we have\nand\nwhere\n\nThen the covariant and contravariant basis vectors are\nwhere formula_213 are the unit vectors in the formula_214 directions.\n\nNote that the components of the metric tensor are such that\nwhich shows that the basis is orthogonal.\n\nThe non-zero components of the Christoffel symbol of the second kind are\n\nThe normalized contravariant basis vectors in cylindrical polar coordinates are\nand the physical components of a vector v are\n\nThe gradient of a scalar field, \"f\"(x), in cylindrical coordinates can now be computed from the general expression in curvilinear coordinates and has the form\n\nSimilarly, the gradient of a vector field, v(x), in cylindrical coordinates can be shown to be\n\nUsing the equation for the divergence of a vector field in curvilinear coordinates, the divergence in cylindrical coordinates can be shown to be\n\nThe Laplacian is more easily computed by noting that formula_222. In cylindrical polar coordinates\nHence,\n\nThe physical components of a second-order tensor field are those obtained when the tensor is expressed in terms of a normalized contravariant basis. In cylindrical polar coordinates these components are\n\nUsing the above definitions we can show that the gradient of a second-order tensor field in cylindrical polar coordinates can be expressed as\n\nThe divergence of a second-order tensor field in cylindrical polar coordinates can be obtained from the expression for the gradient by collecting terms where the scalar product of the two outer vectors in the dyadic products is nonzero. Therefore,\n\n\n"}
{"id": "604707", "url": "https://en.wikipedia.org/wiki?curid=604707", "title": "Truth function", "text": "Truth function\n\nIn logic, a truth function is a function that accepts truth values as input and produces a truth value as output, i.e., the input and output are all truth values. The typical example is in propositional logic, wherein a compound statement is constructed by one or two statements connected by a logical connective; if the truth value of the compound statement is determined by the truth value(s) of the constituent statement(s), the compound statement is called a truth function, and the logical connective is said to be truth functional.\n\nClassical propositional logic is a truth-functional propositional logic, in that every statement has exactly one truth value which is either true or false, and every logical connective is truth functional (with a correspondent truth table), thus every compound statement is a truth function. On the contrary, modal logic is non-truth-functional.\n\nA logical connective is truth-functional if the truth-value of a compound sentence is a function of the truth-value of its sub-sentences. A class of connectives is truth-functional if each of its members is. For example, the connective \"\"and\" is truth-functional since a sentence like \"Apples are fruits and carrots are vegetables\" is true \"if, and only if\" each of its sub-sentences \"apples are fruits\" and \"carrots are vegetables\"\" is true, and it is false otherwise. Some connectives of a natural language, such as English, are not truth-functional.\n\nConnectives of the form \"x \"believes that\" ...\" are typical examples of connectives that are not truth-functional. If e.g. Mary mistakenly believes that Al Gore was President of the USA on April 20, 2000, but she does not believe that the moon is made of green cheese, then the sentence\n\nis true while\n\nis false. In both cases, each component sentence (i.e. \"Al Gore was president of the USA on April 20, 2000\" and \"the moon is made of green cheese\") is false, but each compound sentence formed by prefixing the phrase \"Mary believes that\" differs in truth-value. That is, the truth-value of a sentence of the form \"Mary believes that...\"\" is not determined solely by the truth-value of its component sentence, and hence the (unary) connective (or simply \"operator\" since it is unary) is non-truth-functional.\n\nThe class of classical logic connectives (e.g. &, →) used in the construction of formulas is truth-functional. Their values for various truth-values as argument are usually given by truth tables. Truth-functional propositional calculus is a formal system whose formulae may be interpreted as either true or false.\n\nIn two-valued logic, there are sixteen possible truth functions, also called Boolean functions, of two inputs \"P\" and \"Q\". Any of these functions corresponds to a truth table of a certain logical connective in classical logic, including several degenerate cases such as a function not depending on one or both of its arguments. Truth and falsehood is denoted as 1 and 0 in the following truth tables, respectively, for sake of brevity.\n\nBecause a function may be expressed as a composition, a truth-functional logical calculus does not need to have dedicated symbols for all of the above-mentioned functions to be functionally complete. This is expressed in a propositional calculus as logical equivalence of certain compound statements. For example, classical logic has equivalent to . The conditional operator \"→\" is therefore not necessary for a classical-based logical system if \"¬\" (not) and \"∨\" (or) are already in use.\n\nA minimal set of operators that can express every statement expressible in the propositional calculus is called a \"minimal functionally complete set\". A minimally complete set of operators is achieved by NAND alone {↑} and NOR alone {↓}.\n\nThe following are the minimal functionally complete sets of operators whose arities do not exceed 2:\n\nSome truth functions possess properties which may be expressed in the theorems containing the corresponding connective. Some of those properties that a binary truth function (or a corresponding logical connective) may have are:\n\n\nA set of truth functions is functionally complete if and only if for each of the following five properties it contains at least one member lacking it:\n\nA concrete function may be also referred to as an \"operator\". In two-valued logic there are 2 nullary operators (constants), 4 unary operators, 16 binary operators, 256 ternary operators, and formula_33 \"n\"-ary operators. In three-valued logic there are 3 nullary operators (constants), 27 unary operators, 19683 binary operators, 7625597484987 ternary operators, and formula_34 \"n\"-ary operators. In \"k\"-valued logic, there are \"k\" nullary operators, formula_35 unary operators, formula_36 binary operators, formula_37 ternary operators, and formula_38 \"n\"-ary operators. An \"n\"-ary operator in \"k\"-valued logic is a function from formula_39. Therefore, the number of such operators is formula_40, which is how the above numbers were derived.\n\nHowever, some of the operators of a particular arity are actually degenerate forms that perform a lower-arity operation on some of the inputs and ignores the rest of the inputs. Out of the 256 ternary boolean operators cited above, formula_41 of them are such degenerate forms of binary or lower-arity operators, using the inclusion–exclusion principle. The ternary operator formula_42 is one such operator which is actually a unary operator applied to one input, and ignoring the other two inputs.\n\n\"Not\" is a unary operator, it takes a single term (¬\"P\"). The rest are binary operators, taking two terms to make a compound statement (\"P\" ∧ \"Q\", \"P\" ∨ \"Q\", \"P\" → \"Q\", \"P\" ↔ \"Q\").\n\nThe set of logical operators may be partitioned into disjoint subsets as follows:\n\nIn this partition, formula_44 is the set of operator symbols of \"arity\" .\n\nIn the more familiar propositional calculi, formula_45 is typically partitioned as follows:\n\nInstead of using truth tables, logical connective symbols can be interpreted by means of an interpretation function and a functionally complete set of truth-functions (Gamut 1991), as detailed by the principle of compositionality of meaning.\nLet \"I\" be an interpretation function, let \"Φ\", \"Ψ\" be any two sentences and let the truth function \"f\" be defined as:\n\nThen, for convenience, \"f\", \"f\" \"f\" and so on are defined by means of \"f\":\n\n\nor, alternatively \"f\", \"f\" \"f\" and so on are defined directly:\n\n\nThen\n\netc.\n\nThus if \"S\" is a sentence that is a string of symbols consisting of logical symbols \"v\"...\"v\" representing logical connectives, and non-logical symbols \"c\"...\"c\", then if and only if have been provided interpreting \"v\" to \"v\" by means of \"f\" (or any other set of functional complete truth-functions) then the truth-value of is determined entirely by the truth-values of \"c\"...\"c\", i.e. of . In other words, as expected and required, \"S\" is true or false only under an interpretation of all its non-logical symbols.\n\nLogical operators are implemented as logic gates in digital circuits. Practically all digital circuits (the major exception is DRAM) are built up from NAND, NOR, NOT, and transmission gates. NAND and NOR gates with 3 or more inputs rather than the usual 2 inputs are fairly common, although they are logically equivalent to a cascade of 2-input gates. All other operators are implemented by breaking them down into a logically equivalent combination of 2 or more of the above logic gates.\n\nThe \"logical equivalence\" of \"NAND alone\", \"NOR alone\", and \"NOT and AND\" is similar to Turing equivalence.\n\nThe fact that all truth functions can be expressed with NOR alone is demonstrated by the Apollo guidance computer.\n\n\n\n"}
{"id": "191445", "url": "https://en.wikipedia.org/wiki?curid=191445", "title": "Vocabulary", "text": "Vocabulary\n\nA vocabulary is a set of familiar words within a person's language. A vocabulary, usually developed with age, serves as a useful and fundamental tool for communication and acquiring knowledge. Acquiring an extensive vocabulary is one of the largest challenges in learning a second language.\n\nVocabulary is commonly defined as \"all the words known and used by a particular person\". \"Knowing\" a word, however, is not as simple as merely being able to recognize or use it. There are several aspects of word knowledge that are used to measure word knowledge.\n\nThe first major distinction that must be made when evaluating word knowledge is whether the knowledge is productive (also called achieve) or receptive (also called receive); even within those opposing categories, there is often no clear distinction. Words that are generally understood when heard or read or seen constitute a person's receptive vocabulary. These words may range from well-known to barely known (see degree of knowledge below). A person's receptive vocabulary is the larger of the two. For example, although a young child may not yet be able to speak, write, or sign, he or she may be able to follow simple commands and appear to understand a good portion of the language to which they are exposed. In this case, the child's receptive vocabulary is likely tens, if not hundreds of words, but his or her active vocabulary is zero. When that child learns to speak or sign, however, the child's active vocabulary begins to increase. It is also possible for the productive vocabulary to be larger than the receptive vocabulary, for example in a second-language learner who has learned words through study rather than exposure, and can produce them, but has difficulty recognizing them in conversation.\n\nProductive vocabulary, therefore, generally refers to words that can be produced within an appropriate context and match the intended meaning of the speaker or signer. As with receptive vocabulary, however, there are many degrees at which a particular word may be considered part of an active vocabulary. Knowing how to pronounce, sign, or write a word does not necessarily mean that the word that has been used correctly or accurately reflects the intended message; but it does reflect a minimal amount of productive knowledge.\n\nWithin the receptive–productive distinction lies a range of abilities that are often referred to as \"degree of knowledge\". This simply indicates that a word gradually enters a person's vocabulary over a period of time as more aspects of word knowledge are learnt. Roughly, these stages could be described as:\n\n\nThe differing degrees of word knowledge imply a greater \"depth of knowledge\", but the process is more complex than that. There are many facets to knowing a word, some of which are not hierarchical so their acquisition does not necessarily follow a linear progression suggested by \"degree of knowledge\". Several frameworks of word knowledge have been proposed to better operationalise this concept. One such framework includes nine facets:\n\n\nWords can be defined in various ways, and estimates of vocabulary size differ depending on the definition used. The most common definition is that of a lemma (the uninflected or dictionary form; this includes \"walk\", but not \"walks, walked or walking\"). Most of the time lemmas do not include proper nouns (names of people, places, companies, etc). Another definition often used in research of vocabulary size is that of word family. These are all the words that can be derived from a ground word (e.g., the words \"effortless, effortlessly, effortful, effortfully\" are all part of the word family \"effort\"). Estimates of vocabulary size range from as high as 200 thousand to as low as 10 thousand, depending on the definition used. \n\n\"Listed in order of most ample to most limited:\"\n\nA literate person's vocabulary is all the words they can recognize when reading. This is generally the largest type of vocabulary simply because a reader tends to be exposed to more words by reading than by listening.\n\nA person's listening vocabulary is all the words they can recognize when listening to speech. People may still understand words they were not exposed to before using cues such as tone, gestures, the topic of discussion and the social context of the conversation.\n\nA person's speaking vocabulary is all the words they use in speech. It is likely to be a subset of the listening vocabulary. Due to the spontaneous nature of speech, words are often misused. This misuse, though slight and unintentional, may be compensated by facial expressions and tone of voice.\n\nWords are used in various forms of writing from formal essays to social media feeds. Many written words do not commonly appear in speech. Writers generally use a limited set of words when communicating. For example, if there are a number of synonyms, a writer may have a preference as to which of them to use, and they are unlikely to use technical vocabulary relating to a subject in which they have no knowledge or interest.\n\nFocal vocabulary is a specialized set of terms and distinctions that is particularly important to a certain group: those with a particular focus of experience or activity. A lexicon, or vocabulary, is a language's dictionary: its set of names for things, events, and ideas. Some linguists believe that lexicon influences people's perception of things, the Sapir–Whorf hypothesis. For example, the Nuer of Sudan have an elaborate vocabulary to describe cattle. The Nuer have dozens of names for cattle because of the cattle's particular histories, economies, and environments. This kind of comparison has elicited some linguistic controversy, as with the number of \"Eskimo words for snow\". English speakers with relevant specialised knowledge can also display elaborate and precise vocabularies for snow and cattle when the need arises.\n\nDuring its infancy, a child instinctively builds a vocabulary. Infants imitate words that they hear and then associate those words with objects and actions. This is the listening vocabulary. The speaking vocabulary follows, as a child's thoughts become more reliant on his/her ability to self-express without relying on gestures or babbling. Once the reading and writing vocabularies start to develop, through questions and education, the child starts to discover the anomalies and irregularities of language.\n\nIn first grade, a child who can read learns about twice as many words as one who cannot. Generally, this gap does not narrow later. This results in a wide range of vocabulary by age five or six, when an English-speaking child will have learned about 1500 words.\n\nVocabulary grows throughout our entire life. Between the ages of 20 and 60, people learn some 6,000 more lemmas, or one every other day. An average 20-year-old knows 42,000 words coming from 11,100 word families; an average 60-year-old knows 48,200 lemmas coming from 13,400 word families. People expand their vocabularies by e.g. reading, playing word games, and participating in vocabulary-related programs. Exposure to traditional print media teaches correct spelling and vocabulary, while exposure to text messaging leads to more relaxed word acceptability constraints.\n\n\nEstimating average vocabulary size poses various difficulties and limitations due to the different definitions and methods employed such as what is the word, what is to know a word, what sample dictionaries were used, how tests were conducted, and so on. Native speakers' vocabularies also vary widely within a language, and are dependent on the level of the speaker's education.\n\nAs a result estimates vary from as little as 10,000 to as many as over 50,000 for young adult native speakers of English.\n\nOne most recent 2016 study shows that 20-year-old English native speakers recognize on average 42,000 lemmas, ranging from 27,100 for the lowest 5% of the population to 51,700 lemmas for the highest 5%. These lemmas come from 6,100 word families in the lowest 5% of the population and 14,900 word families in the highest 5%. 60-year-olds know on average 6,000 lemmas more.\nAccording to another, earlier 1995 study junior-high students would be able to recognize the meanings of about 10,000–12,000 words, whereas for college students this number grows up to about 12,000–17,000 and for elderly adults up to about 17,000 or more.\n\nFor native speakers of German average absolute vocabulary sizes range from 5,900 lemmas in first grade to 73,000 for adults.\n\nThe knowledge of the 3000 most frequent English word families or the 5000 most frequent words provides 95% vocabulary coverage of spoken discourse.\nFor minimal reading comprehension a threshold of 3,000 word families (5,000 lexical items) was suggested and for reading for pleasure 5,000 word families (8,000 lexical items) are required. An \"optimal\" threshold of 8,000 word families yields the coverage of 98% (including proper nouns).\n\nLearning vocabulary is one of the first steps in learning a second language, but a learner never finishes vocabulary acquisition. Whether in one's native language or a second language, the acquisition of new vocabulary is an ongoing process. There are many techniques that help one acquire new vocabulary.\n\nAlthough memorization can be seen as tedious or boring, associating one word in the native language with the corresponding word in the second language until memorized is considered one of the best methods of vocabulary acquisition. By the time students reach adulthood, they generally have gathered a number of personalized memorization methods. Although many argue that memorization does not typically require the complex cognitive processing that increases retention (Sagarra and Alba, 2006), it does typically require a large amount of repetition, and spaced repetition with flashcards is an established method for memorization, particularly used for vocabulary acquisition in computer-assisted language learning. Other methods typically require more time and longer to recall.\n\nSome words cannot be easily linked through association or other methods. When a word in the second language is phonologically or visually similar to a word in the native language, one often assumes they also share similar meanings. Though this is frequently the case, it is not always true. When faced with a false friend, memorization and repetition are the keys to mastery. If a second language learner relies solely on word associations to learn new vocabulary, that person will have a very difficult time mastering false friends. When large amounts of vocabulary must be acquired in a limited amount of time, when the learner needs to recall information quickly, when words represent abstract concepts or are difficult to picture in a mental image, or when discriminating between false friends, rote memorization is the method to use. A neural network model of novel word learning across orthographies, accounting for L1-specific memorization abilities of L2-learners has recently been introduced (Hadzibeganovic and Cannas, 2009).\n\nOne useful method of building vocabulary in a second language is the keyword method. If time is available or one wants to emphasize a few key words, one can create mnemonic devices or word associations. Although these strategies tend to take longer to implement and may take longer in recollection, they create new or unusual connections that can increase retention. The keyword method requires deeper cognitive processing, thus increasing the likelihood of retention (Sagarra and Alba, 2006). This method uses fits within Paivio's (1986) dual coding theory because it uses both verbal and image memory systems. However, this method is best for words that represent concrete and imageable things. Abstract concepts or words that do not bring a distinct image to mind are difficult to associate. In addition, studies have shown that associative vocabulary learning is more successful with younger students (Sagarra and Alba, 2006). Older students tend to rely less on creating word associations to remember vocabulary.\n\nSeveral word lists have been developed to provide people with a limited vocabulary either for the purpose of rapid language proficiency or for effective communication. These include Basic English (850 words), Special English (1,500 words), General Service List (2,000 words), and Academic Word List. Some learner's dictionaries have developed defining vocabularies which contain only most common and basic words. As a result word definitions in such dictionaries can be understood even by learners with a limited vocabulary. Some publishers produce dictionaries based on word frequency or thematic groups.\n\nThe Swadesh list was made for investigation in linguistics.\n\n\n\n"}
