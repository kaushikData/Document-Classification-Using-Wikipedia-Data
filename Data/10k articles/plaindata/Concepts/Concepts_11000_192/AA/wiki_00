{"id": "2954769", "url": "https://en.wikipedia.org/wiki?curid=2954769", "title": "Acceptance and commitment therapy", "text": "Acceptance and commitment therapy\n\nAcceptance and commitment therapy (ACT, typically pronounced as the word \"act\") is a form of counseling and a branch of clinical behavior analysis. It is an empirically-based psychological intervention that uses acceptance and mindfulness strategies mixed in different ways with commitment and behavior-change strategies, to increase psychological flexibility. The approach was originally called \"comprehensive distancing\". Steven C. Hayes developed Acceptance and Commitment Therapy in 1982 in order to create a mixed approach which integrates both cognitive and behavioral therapy. There are a variety of protocols for ACT, depending on the target behavior or setting. For example, in behavioral health areas a brief version of ACT is called \"focused acceptance and commitment therapy\" (FACT).\n\nThe objective of ACT is not elimination of difficult feelings; rather, it is to be present with what life brings us and to \"move toward valued behavior\". Acceptance and commitment therapy invites people to open up to unpleasant feelings, and learn not to overreact to them, and not avoid situations where they are invoked. Its therapeutic effect is a positive spiral where feeling better leads to a better understanding of the truth. In ACT, 'truth' is measured through the concept of 'workability', or what works to take another step toward what matters (e.g. values, meaning).\n\nACT is developed within a pragmatic philosophy called functional contextualism. ACT is based on relational frame theory (RFT), a comprehensive theory of language and cognition that is an offshoot of behavior analysis. Both ACT and RFT are based on B. F. Skinner's philosophy of Radical Behaviorism\n\nACT differs from traditional cognitive behavioral therapy (CBT) in that rather than trying to teach people to better control their thoughts, feelings, sensations, memories and other private events, ACT teaches them to \"just notice,\" accept, and embrace their private events, especially previously unwanted ones. ACT helps the individual get in contact with a transcendent sense of self known as \"self-as-context\"—the you who is always there observing and experiencing and yet distinct from one's thoughts, feelings, sensations, and memories. ACT aims to help the individual clarify their personal values and to take action on them, bringing more vitality and meaning to their life in the process, increasing their psychological flexibility.\n\nWhile Western psychology has typically operated under the \"healthy normality\" assumption which states that by their nature, humans are psychologically healthy, ACT assumes, rather, that psychological processes of a normal human mind are often destructive. The core conception of ACT is that psychological suffering is usually caused by experiential avoidance, cognitive entanglement, and resulting psychological rigidity that leads to a failure to take needed behavioral steps in accord with core values. As a simple way to summarize the model, ACT views the core of many problems to be due to the concepts represented in the acronym, FEAR:\n\nAnd the healthy alternative is to ACT:\n\nACT commonly employs six core principles to help clients develop psychological flexibility:\n\nCorrelational evidence has found that absence of psychological flexibility predicts many forms of psychopathology. A 2005 meta-analysis showed that the six ACT principles, on average, account for 16–29% of the variance in psychopathology (general mental health, depression, anxiety) at baseline, depending on the measure, using correlational methods. A 2012 meta-analysis of 68 laboratory-based studies on ACT components has also provided support for the link between psychological flexibility concepts and specific components.\n\nA 2008 meta-analysis concluded that the evidence was still too limited for ACT to be considered a supported treatment, and raised methodological concerns about the research base. A 2009 meta-analysis found that ACT was more effective than placebo and \"treatment as usual\" for most problems (with the exception of anxiety and depression), but not more effective than CBT and other traditional therapies. A 2012 meta-analysis was more positive and reported that ACT outperformed CBT, except for treating depression and anxiety.\n\nA 2015 review found that ACT was better than placebo and typical treatment for anxiety disorders, depression, and addiction. Its effectiveness was similar to traditional treatments like cognitive behavioral therapy (CBT). The authors suggested that the CBT comparison of the previous 2012 meta-analysis may have been compromised by the inclusion of nonrandomized trials with small sample sizes. They also noted that research methodologies had improved since the studies described in the 2008 meta-analysis.\n\nThe number of randomized clinical trials and controlled time series evaluating ACT for a variety of problems is growing. In 2006, only about 30 such studies were known, but in 2011 the number had approximately doubled. The website of the Association for Contextual Behavioral Science states that there were 171 randomized controlled trials (RCTs) of ACT published as of December 2016, and over 20 meta-analyses and 45 mediational studies of the ACT literature as of Spring 2016. Most studies of ACT so far have been conducted on adults and therefore the knowledge of its effectiveness when applied to children and adolescents is limited.\n\nACT, dialectical behavior therapy (DBT), functional analytic psychotherapy (FAP), mindfulness-based cognitive therapy (MBCT) and other acceptance- and mindfulness-based approaches are commonly grouped under the name \"the third wave of cognitive behavior therapy\". The first wave, behaviour therapy, commenced in the 1920s based on Pavlov's classical (respondent) conditioning and operant conditioning that was correlated to reinforcing consequences. The second wave emerged in the 1970s and included cognition in the form of irrational beliefs, dysfunctional attitudes or depressogenic attributions. In the late 1980s empirical limitations and philosophical misgivings of the second wave gave rise to Steven Hayes' ACT theory which modified the focus of abnormal behaviour away from the content or form towards the context in which it occurs. ACT research has suggested that many of the emotional defenses individuals use with conviction to try to solve their problems actually entangle humans into greater suffering. Rigid ideas about themselves, lack of focus on what is important in their life and struggling to change sensations, feelings or thoughts that are troublesome only serve to create greater distress.\n\nSteven C. Hayes described this group in his ABCT President Address as follows:\n\nACT has also been adapted to create a non-therapy version of the same processes called \"Acceptance and Commitment Training\". This training process, oriented towards the development of mindfulness, acceptance, and valued skills in non-clinical settings such as businesses or schools, has also been investigated in a handful of research studies with good preliminary results. This is somewhat similar to the awareness–management movement in business training programs, where mindfulness and cognitive-shifting techniques are employed.\n\nThe emphasis of ACT on ongoing present moment awareness, valued directions and committed action is similar to other psycho-therapeutic approaches that, unlike ACT, are not as focused on outcome research or consciously linked to a basic behavioral science program, including approaches such as Gestalt therapy, Morita therapy and Voice Dialogue, IFS and others.\n\nWilson, Hayes & Byrd explore at length the compatibilities between ACT and the 12-step treatment of addictions and argue that, unlike most other psychotherapies, both approaches can be implicitly or explicitly integrated due to their broad commonalities. Both approaches endorse acceptance as an alternative to unproductive control. ACT emphasizes the hopelessness of relying on ineffectual strategies to control private experience, similarly the 12-step approach emphasizes the acceptance of powerlessness over addiction. Both approaches encourage a broad life-reorientation, rather than a narrow focus on the elimination of substance use, and both place great value on the long-term project of building of a meaningful life aligned with the clients' values. ACT and 12-step both encourage the pragmatic utility of cultivating a transcendent sense of self (higher power) within an unconventional, individualized spirituality. Finally they both openly accept the paradox that acceptance is a necessary condition for change and both encourage a playful awareness of the limitations of human thinking.\n\nSome published empirical studies in clinical psychology have argued that ACT is not different from other interventions. Stefan Hofmann argued that ACT is similar to the much older Morita therapy.\n\nA meta-analysis by Öst in 2008 concluded that ACT did not yet qualify as an \"empirically supported treatment\", that the research methodology for ACT was less stringent than cognitive behavioral therapy, and that the mean effect size was moderate. Supporters of ACT have challenged those conclusions by showing that the quality difference in Öst's review was accounted for by the larger number of funded trials in the CBT comparison group.\n\nSeveral concerns, both theoretical and empirical, have arisen in response to the ascendancy of ACT. One major theoretical concern is that the primary authors of ACT and of the corresponding theories of human behavior, relational frame theory (RFT) and functional contextualism (FC), recommend their approach as the proverbial holy grail of psychological therapies. Psychologist James C. Coyne, in a discussion of \"disappointments and embarrassments in the branding of psychotherapies as evidence supported\", said: \"Whether or not ACT is more efficacious than other therapies, as its proponents sometimes claim, or whether it is efficacious for psychosis, is debatable\". The textbook \"Systems of Psychotherapy: A Transtheoretical Analysis\" provides criticisms of third-wave behaviour therapies including ACT from the perspectives of other systems of psychotherapy.\n\nPsychologist Jonathan W. Kanter said that Hayes and colleagues \"argue that empirical clinical psychology is hampered in its efforts to alleviate human suffering and present \"contextual behavioral science\" (CBS) to address the basic philosophical, theoretical and methodological shortcomings of the field. CBS represents a host of good ideas but at times the promise of CBS is obscured by excessive promotion of Acceptance and Commitment Therapy (ACT) and Relational Frame Theory (RFT) and demotion of earlier cognitive and behavior change techniques in the absence of clear logic and empirical support.\" Nevertheless, Kanter concluded that \"the ideas of CBS, RFT, and ACT deserve serious consideration by the mainstream community and have great potential to shape a truly progressive clinical science to guide clinical practice.\"\n\nACT currently appears to be about as effective as standard CBT, with some meta-analyses showing small differences in favor of ACT and others not. For example, a meta-analysis published by Francisco Ruiz in 2012 looked at 16 studies comparing ACT to standard CBT. ACT failed to separate from CBT on effect sizes for depression, anxiety or quality of life. The author did find separation between ACT and CBT on the \"primary outcome\" – a heterogeneous class of 14 separate outcome measures that were aggregated into the effect size analysis. This analysis however is limited by the highly heterogeneous nature of the outcome variables used in the analysis, which has the tendency to increase the number needed to treat (NNT) to replicate the effect size reported. More limited measures, such as depression, anxiety and quality of life decrease the NNT, making the analysis more clinically relevant, and on these measures ACT did not outperform CBT.\n\nA 2013 paper comparing ACT to cognitive therapy (CT) concluded that \"like CT, ACT cannot yet make strong claims that its unique and theory-driven intervention components are active ingredients in its effects.\" The authors of the paper suggested that many of the assumptions of ACT and CT \"are pre-analytical, and cannot be directly pitted against one another in experimental tests.\"\n\nThe Association for Contextual Behavioral Science is committed to research and development in the area of ACT, RFT, and contextual behavioral science more generally. As of 2017 it had over 7,600 members worldwide, about half outside of the United States. It holds annual \"world conference\" meetings: The 16th will be held in Montreal, in July 2018.\n\nThe Association for Behavior Analysis International (ABAI) has a special interest group for practitioner issues, behavioral counseling, and clinical behavior analysis ABA:I. ABAI has larger special interest groups for autism and behavioral medicine. ABAI serves as the core intellectual home for behavior analysts. ABAI sponsors three conferences/year—one multi-track in the U.S., one specific to Autism and one international.\n\nThe Association for Behavioral and Cognitive Therapies (ABCT) also has an interest group in behavior analysis, which focuses on clinical behavior analysis. ACT work is commonly presented at ABCT and other mainstream CBT organizations.\n\nThe British Association for Behavioural and Cognitive Psychotherapies (BABCP) has a large special interest group in ACT, with over 1,200 members.\n\nDoctoral-level behavior analysts who are psychologists belong to the American Psychological Association's (APA) Division 25—Behavior analysis. ACT has been called a \"commonly used treatment with empirical support\" within the APA-recognized specialty of behavioral and cognitive psychology.\n\n\n"}
{"id": "31268802", "url": "https://en.wikipedia.org/wiki?curid=31268802", "title": "American-Soviet Peace Walks", "text": "American-Soviet Peace Walks\n\nDuring the 20th century a number of peace walks were organized involving the citizens of the United States and the USSR. These peace walks, or peace marches, represented citizen diplomacy initiatives promoting peace and Nuclear disarmament through direct person-to-person interaction among the citizens of the two Cold War opponent states.\n\nA peace walk from San Francisco, US, to Moscow, USSR, took place from December 1960 to October 1961. The walk was organized by Committee for Nonviolent Action and promoted nonviolence and unilateral nuclear disarmament.\n\nLyttle, B. (1966). You come with naked hands: The story of the San Francisco to Moscow march for peace. Raymond, N.H: Greenleaf Books.\n\nLehmann, J. (1966). We walked to Moscow. Raymond, N.H: Greenleaf Books.\n\nDeming, Barabara. The long walk for peace: new mission to Moscow. In Christman, H. M. (1964). Peace and arms. New York: Sheed and Ward.\n\nWernicke, Gunter and Wittner, Lawrence S.(1999) Lifting the Iron Curtain: The Peace March to Moscow of 1960–1961. The International History Review, 21: 4, 900–917.\n\nUnited Press International. (1961, October 4). Peace Marchers Reach Red Square but Soviet Prohibits Speeches. \"The New York Times\", pp. 1–2.\n\nAssociated Press. (1961, October 4). Banners Urge Disarmament. \"The New York Times\", p. 2.\n\nSyracuse Peace Council (October 16, 1961). San Francisco to Moscow. Peace News Letter, Syracuse, NY: New York State Peace Council.\n\nDavid N. Rich - San Francisco to Moscow - Walk for Peace 1960 - 1961. David N. Rich's website\n\nA 450-mile peace walk from Leningrad (now Saint Petersburg) to Moscow in the USSR took place from June 15 to July 8, 1987. The walk, intended to promote peace and help end the nuclear arms race, was organized by the International Peace Walk, Inc. About 230 American and 200 Soviet citizens took part in the walk. To mark the conclusion of the walk, the first rock concert featuring American and Soviet performers took place at the Ismailovo Stadium in Moscow on the 4 July, symbolically coinciding with the Independence Day holiday in the U.S.\n\nSegal, F., & Basten, F. E. (1988). The American Soviet walk: Taking steps to end the nuclear arms race. Santa Monica, CA: United World of the Universe Foundation.\n\nGraham, B., & Greenfield, R. (2004). Chapter Twenty-One: Rocking Behind the Curtain, In Bill Graham presents: My life inside rock and out (pp. 491–505). New York, N.Y: Da Capo Press.\n\nBrigham, S. (October 1, 2010). The American-Soviet Walks: Large-Scale Citizen Diplomacy at Glasnost's Outset. Peace & Change, 35: 4, pp. 594–625.\n\nHendrix, Kathleen. (1987, April 7). U.S.-Soviet Anti-Arms March to Moscow Planned. \"Los Angeles Times\", p. 4.\n\nHendrix, Kathleen. (1987, June 12). Mission to Moscow: Joint Venture for Peace American Contingent in Virginia Trains for Start of Walk Next week at Leningrad. \"Los Angeles Times\", p. 1.\n\nHendrix, Kathleen. (1987, June 15). Americans Embark on Person-to-Person Soviet Peace Walk. \"Los Angeles Times\", p. 1.\n\nHendrix, Kathleen. (1987, June 18). Peace Walkers Fit Fun, Friendships Into Tight Schedule. \"Los Angeles Times\", p. 1.\n\nHendrix, Kathleen. (1987, June 24). Peace Marchers Capture Soviets' Attention: Thousands Greet Walkers With Curiosity and Emotional Displays. \"Los Angeles Times\", p. 1.\n\nHendrix, Kathleen. (1987, June 26). Rain Fails to Dampen Peace March Spirit. \"Los Angeles Times\", p. 1.\n\nHendrix, Kathleen. (1987, July 1). Americans Encounter the Soviet Curious. \"Los Angeles Times\", p. 1.\n\nHendrix, Kathleen. (1987, July 3). The Peace Marchers Arrive in Moscow: Muted Welcome Suggests Parade Played Better in the Provinces. \"Los Angeles Times\", p. 1.\n\nHendrix, Kathleen. (1987, July 5). Laid-Back Listeners at Moscow Fete 1st American-Soviet Rock Concert Held. \"Los Angeles Times\", p. 5.\n\nHendrix, Kathleen. (1987, July 9). Dramatic Encounter Caps U.S.-Soviet Peace March. \"Los Angeles Times\", p. 1.\n\nHilburn, Robert. (1987, July 8). City Of Hope Honoree Graham Hopeful About More Rock With Soviets…BUT. \"Los Angeles Times\", p. 1.\n\nSimons, Jamie and Lapidese, Jon. (1987, July 5). Reebok Diplomacy: Allan Affeldt of Newport Beach, the Activist Behind the Peace March on Moscow. \"Los Angeles Times\", p. 16.\n\nSimons, Jamie and Lapidese, Jon. (1987, July 5). Rock in a Hard Place. \"Los Angeles Times\", p. 20.\n\nWillman, Chris. (1987, October 24). Television Reviews: Rock on Cable. \"Los Angeles Times\", p. 11.\n\nTaubman, Philip. (1987, July 5). At Soviet Rock Concert, the Beat of Security. The New York Times.\n\nJanet Kinosian (July 1987). Walking for Peace. Orange Coast Magazine.\n\nIn June–July 1988, The American Soviet Peace Walk (ASPW) sponsored by the International Peace Walk, Inc. (IPW), organized and sponsored 230 Soviets citizens and 200 Americans from all walks of life. They started their travels from Washington, D.C. went on to Santa Monica, CA and continued on to San Francisco, to experience the America way of life. On July 16, 1988, the final concert was organized and produced by Summer of Love Productions Producers, Ron Frazier and Bill McCarthy, who had hosted the previous June concert event for the marchers in Los Angeles. On July 16, 1988, the American Soviet Peace Walk concert finale event happened at the Band Shell in San Francisco's Golden Gate Park with an estimated public attendance of 25,000 plus. The Producers give many thanks to all participants, the volunteers, and performing friends of the Summer of Love 20th Anniversary 1987 series that benefited the San Francisco Food Bank and ran through to the Concert of July 16, 1988. Congratulations are given to both the American and Soviet Performing Artists of the \"American Soviet Peace Walk Concert\". Participants and performances achieved the results as Change Makers that has advanced the cause for Peace and People-to-People Awareness. Artists' performances in concerts that achieve Global attention are needed as an ongoing effort to sustain awareness of and for Peace. Special Thanks go to; Susan Ramser, Producers Assistant who hosted the Soviet Artists on their arrival to San Francisco with the help of a Cadillac mini-fleet, Arthur Meyer, Artistic Director, and to Pete Sears of the original Jefferson Starship who assisted in the musical coordination and inviting cause-aware musician friends to participate. He organized the final musical portion of the show, as well as playing piano with each act. The Crowd was pleased by Mr. Jerry Garcia helped to promote the event, Pete Sears was responsible for Jerry's appearance. The Recreation and Parks Department prohibited advertising because they were concerned about fans camping and overcrowding, and so the concert happened without mainstream advertisement. But with the assisted word of mouth promotion, and one free newspaper paragraph event notice and the 1988 finale Poster presented 3 days out, all led to the success of the event. Guest Artists Performers: ALEXANDER GRADSKY, TIME MACHINE, COLLECTIVE VISION, THE TELEPHONE TRUST, YKRANIAN WOMEN'S CHOUS, THE SOVIET YOUTH PERFORMERS…Friends of Summer of Love and special invites; BABA OLATUNJI, JERRY GARCIA. MICKEY HART, GRACE SLICK, MERL SAUNDERS, MIMI FARINA, JOHN CIPPOLINA, PETE SEARS, ZERO, NORTON BUFFALO, MARK BENNO, EMMIT POWELL & THE GOSPELL ELITES, OGIE YOCHA, and surprise guest PAUL KANTNER. The American Soviet Peace Walk 1988 Poster & Beyond Web-Wall: http://www.summeroflove.tv/\n\nNews Advisory. (1988, June 17). 220 Soviets arrive in Washington, ready to start cross-country trek. PR Newswire.\n\nRubin, Trudy. (1988, June 29). Soviet Press: The 'Warriors Of Perestroika'. \"The Philadelphia Inquirer\".\n\nWestside Digest. (1988, June 30). Santa Monica: Cross-Country March Honored. \"Los Angeles Times\".\n\nZiaya, Christine. (1988, July 8). Peace Walkers Reach High Note in Trek: Soviets, Americans Due at Concert. \"Los Angeles Times\".\n\nHendrix, Kathleen. (1988, July 15). The Great American Glasnost Tour: In a Peace Walk Across the U.S., Soviets Explore the Basics: Bikers, Barbecues and the Beach. \"Los Angeles Times\".\n\nAbout 460 American and Soviet citizens walked for peace from Odessa to Kiev in the Ukraine, USSR over five weeks in late summer of 1988.\n\nMartin, James. Soviet-American Peace Walk 1988. exKZ.org. Retrieved April 2, 2011.\n\nAbout 50 Americans and 100 Soviets calling themselves \"Russian North,\" participated in an international peace walk, passing through cities like Archangelsk and Severodvinsk. The march promoted nonviolence and a ban on nuclear testing.\n\nThis section includes materials that contain references to more than one peace walk, such as reviews of events which span longer time periods and historical trends.\n\nSherbakova, V. A. (2009) The Marches of High Hopes. Peace and Conciliation, Journal of the International Federation for Peace and Conciliation of the Russian Federation, 2: 39, 37–45. (text is in Russian language)\n\nSubheadings:\n\n– American-Soviet \"Peace March\" June 16 – July 19, 1988.\n\n– The 20 Year Anniversary of the Soviet-American Peace Walk: Odessa-Kiev.\n\n– Address to all governments and peoples.\n\n– Soviet-American Meeting for Peace, October 12–18, 1990.\n\n\n"}
{"id": "31294792", "url": "https://en.wikipedia.org/wiki?curid=31294792", "title": "Anti-union violence", "text": "Anti-union violence\n\nAnti-union violence is physical force intended to harm union officials, union organizers, union members, union sympathizers, or their families. It is most commonly used either during union organizing efforts, or during strikes. The aim most often is to prevent a union from forming, to destroy an existing union, or to reduce the effectiveness of a union or a particular strike action. If strikers prevent people or goods to enter or leave a workplace, violence may be used to allow people and goods to pass the picket line.\n\nViolence against unions may be isolated, or may occur as part of a campaign that includes spying, intimidation, impersonation, disinformation, and sabotage. Violence in labor disputes may be the result of unreasonable polarization, or miscalculation. It may be willful and provoked, or senseless and tragic. On some occasions, violence in labor disputes may be purposeful and calculated, for example the hiring and deployment of goon squads to assault strikers.\n\nIncidents of violence during periods of labor unrest are sometimes perceived differently by different parties. It is sometimes a challenge to ascertain the truth about labor-related violence, and incidents of violence committed by, or in the name of, unions or union workers have occurred as well.\n\nThe practice of workers organizing, and meeting resistance for organizing, dates to antiquity. The first known individual killed by authorities for labor activities is likely Cinto Brandini, executed with nine others in 1345 Florence for attempting to organize woolcombers.\n\nAccording to a study in 1969, the United States has had the bloodiest and most violent labor history of any industrial nation in the world. Mass labor violence in the U.S. peaked in the early 20th century and has largely subsided since the 1940s. But the deadly suppression of labor unions on a large scale persists into the new century, in the 2012 Marikana killings in South Africa, in the ongoing assassinations of trade union members in Colombia, and the South Korean government's response to Korean Confederation of Trade Unions protests.\n\nSince unions are organized to achieve collective bargaining power to begin with, most union conflicts have been motivated primarily by economic issues (wages, working hours, safety conditions, work rules, etc.), and have engaged antagonists (employers, hired strikebreakers, replacement workers, local law enforcement) with economic goals in mind. In some instances, however, other causes emerge.\n\nThe 1887 Thibodaux massacre in Louisiana, the 1899 Pana riot in southern Illinois, and the 1911 Queen & Crescent killings in Kentucky and Tennessee are three examples of deliberate campaigns of murder against organized black workers in the American south, the first committed by landowners, the other two by white competitors.\n\nIn South Africa the 1922 Rand Rebellion also had underlying racial causes, taking on the slogan \"Workers of the world, unite and fight for a white South Africa!\", before their strike grew to a small-scale rebellion at the cost of 200 lives. The behavior of South African police in the 1946 African Mine Workers' Union strike is said to have led to the formation of the Northern Rhodesian African Mineworkers' Union in 1949 as a cornerstone of the anti-apartheid movement.\n\nAs with race, for some incidents there is no clear distinction between anti-union violence and political suppression. Polish labor unions were centrally involved in workers’ uprisings and/or general strikes that challenged the sitting governments in 1905, 1923, and 1937. In a similar way the strike of Asturian miners in 1934, put down by right-wing Spanish government forces with great loss of life, amounted to an insurrection through work stoppage, not an economic labor action. Unions continued to play a political and military role in the subsequent Spanish Civil War.\n\nAlong with Franco in Spain, other totalitarian regimes in Europe brought their labor unions under government control, violently when necessary. After coming to power as chancellor in January 1933, Adolf Hitler declared May Day a national holiday, then on May 2, 1933 unexpectedly moved to outlaw labor unions as part of the Nazi \"synchronization\" process. Major unions such as the Allgemeiner Deutscher Gewerkschaftsbund were raided that day, their accounts seized, and their leaders (Gustav Schiefer, Wilhelm Leuschner, Erich Luebbe) arrested and sent to concentration camps. (The bodies of four murdered trade union officials in Duisburg were only found a year later, in April 1934.) Every worker in the nation was then compelled to join the single party-controlled union, the German Labour Front. Similar coercive violence was exercised against labor unions in conquered nations, as in the Netherlands in 1941.\n\nSome anti-union violence appears to be random, such as an incident during the 1912 textile strike in Lawrence, Massachusetts, in which a police officer fired into a crowd of strikers, killing Anna LoPizzo.\n\nAnti-union violence may be used as a means to intimidate others, as in the hanging of union organizer Frank Little from a railroad trestle in Butte, Montana. A note was pinned to his body which said, \"Others Take Notice! First And Last Warning!\" The initial of the last names of seven well-known union activists in the Butte area were on the note, with the \"L\" for Frank Little circled.\n\nAnti-union violence may be abrupt and unanticipated. Three years after Frank Little was lynched, a strike by Butte miners was suppressed with gunfire when deputized mine guards suddenly fired upon unarmed picketers in the Anaconda Road Massacre. Seventeen were shot in the back as they tried to flee, and one man died.\n\nOther anti-union violence may seem orchestrated, as in 1914 when mine guards and the state militia fired into a tent colony of striking miners in Colorado, an incident that came to be known as the Ludlow Massacre. During that strike, the company hired the Baldwin Felts agency, which built an armored car so their agents could approach the strikers' tent colonies with impunity. The strikers called it the \"Death Special\". At the Forbes tent colony,\n\"[The Death Special] opened fire, a protracted spurt that sent some six hundred bullets tearing through the thin tents. One of the shots struck miner Luka Vahernik, fifty, in the head, killing him instantly. Another striker, Marco Zamboni, eighteen ... suffered nine bullet wounds to his legs... One tent was later found to have about 150 bullet holes...\" \nSometimes, there is simultaneous violence on both sides. In an auto workers strike organised by Victor Reuther and others in 1937, \"[u]nionists assembled rocks, steel hinges, and other objects to throw at the cops, and police organized tear gas attacks and mounted charges.\"\n\nThere have been cases where violence has been perpetrated or encouraged by agents of management, intending it to be blamed on the union.\n\n\n\n\n\nHistorically, violence against unions in the United States has included attacks by detective and guard agencies, such as the Pinkertons, Baldwin Felts, Burns, or Thiel detective agencies; citizens groups, such as the Citizens' Alliance; company guards; police; national guard; or even the military. In the book \"From Blackjacks To Briefcases\", Robert Michael Smith states that during the late nineteenth and early twentieth centuries, anti-union agencies \"spawned violence and wreaked havoc\" on the labor movement. According to Morris Friedman, detective agencies were themselves for-profit companies, and a \"bitter struggle\" between capital and labor could be counted upon to create \"satisfaction and immense profit\" for agencies such as the Pinkerton company. Harry Wellington Laidler wrote a book in 1913 detailing how one of the largest union busters in the United States, Corporations Auxiliary Company, had a sales pitch offering the use of provocation and violence.\n\nDuring the Lattimer massacre, nineteen unarmed immigrant coal miners were suddenly gunned down at the Lattimer mine near Hazleton, Pennsylvania, on September 10, 1897. In the Colorado Labor Wars, martial law was imposed by the Colorado National Guard in order to put down striking miners. A study of industrial violence in 1969 concluded, \"There is no episode in American labor history in which violence was as systematically used by employers as in the Colorado labor war of 1903 and 1904.\" In 1914, mine guards and the state militia fired into a tent colony of striking miners in Colorado, an incident that came to be known as the Ludlow Massacre. During that strike, the company hired the Baldwin Felts agency, which built an armored car so their agents could approach the strikers' tent colonies with impunity. The strikers called it the \"Death Special\". In 1917, union organizer Frank Little was hanged from a railroad trestle in Butte, Montana, with a note pinned to his body which carried a \"warning\" to other labor activists. In 1927, during another coal strike in Colorado, state police and mine guards fired pistols, rifles and a machine gun into a group of five hundred striking miners and their wives in what came to be called the Columbine Mine Massacre.\n\nBy the early 1900s, public tolerance for violence during labor disputes began to decrease. Yet violence involving strikebreaking troops and armed guards continued into the 1930s. Legislation related to employer strategies such as violent strike breaking would have to wait until after World War II. Beginning in the 1950s, employers began to embrace new methods of managing workers and unions which were still effective, but much more subtle.\n\nRosvall and Voutilainen were murdered for their pro-union efforts resulting in the authorities in Thunder Bay conducting a major cover up in an attempt to conceal the truth. Thunder Bay remains a hot bed of anti-union violence against pro-union individuals resulting in Thunder Bay being labelled the Capital of Anti-union Violence of Canada.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "174914", "url": "https://en.wikipedia.org/wiki?curid=174914", "title": "Atomic units", "text": "Atomic units\n\nAtomic units (au or a.u.) form a system of natural units which is especially convenient for atomic physics calculations. There are two different kinds of atomic units, Hartree atomic units and Rydberg atomic units, which differ in the choice of the unit of mass and charge. This article deals with Hartree atomic units, where the numerical values of the following four fundamental physical constants are all unity by definition:\n\n\nIn Hartree units, the speed of light is approximately formula_5. Atomic units are often abbreviated \"a.u.\" or \"au\", not to be confused with the same abbreviation used also for astronomical units, arbitrary units, and absorbance units in different contexts.\n\nAtomic units, like SI units, have a unit of mass, a unit of length, and so on. However, the use and notation is somewhat different from SI.\n\nSuppose a particle with a mass of \"m\" has 3.4 times the mass of electron. The value of \"m\" can be written in three ways:\n\n\nThese four fundamental constants form the basis of the atomic units (see above). Therefore, their numerical values in the atomic units are unity by definition.\n\nDimensionless physical constants retain their values in any system of units. Of particular importance is the fine-structure constant formula_12. This immediately gives the value of the speed of light, expressed in atomic units.\n\nBelow are given a few derived units. Some of them have proper names and symbols assigned, as indicated in the table. \"k\" is the Boltzmann constant.\n\nThere are two common variants of atomic units, one where they are used in conjunction with SI units for electromagnetism, and one where they are used with Gaussian-CGS units. Although the units written above are the same either way (including the unit for electric field), the units related to magnetism are not. In the SI system, the atomic unit for magnetic field is\nand in the Gaussian-cgs unit system, the atomic unit for magnetic field is\n\nOther magnetism-related quantities are also different in the two systems. An important example is the Bohr magneton: In SI-based atomic units,\nand in Gaussian-based atomic units,\n\nAtomic units are chosen to reflect the properties of electrons in atoms. This is particularly clear from the classical Bohr model of the hydrogen atom in its ground state. The ground state electron orbiting the hydrogen nucleus has (in the classical Bohr model):\n\n\nThe Schrödinger equation for an electron in SI units is\n\nThe same equation in au is\n\nFor the special case of the electron around a hydrogen atom, the Hamiltonian in SI units is:\n\nwhile atomic units transform the preceding equation into\n\nBoth Planck units and au are derived from certain fundamental properties of the physical world, and are free of anthropocentric considerations. It should be kept in mind that au were designed for atomic-scale calculations in the present-day universe, while Planck units are more suitable for quantum gravity and early-universe cosmology. Both au and Planck units normalize the reduced Planck constant. Beyond this, Planck units normalize to 1 the two fundamental constants of general relativity and cosmology: the gravitational constant \"G\" and the speed of light in a vacuum, \"c\". Atomic units, by contrast, normalize to 1 the mass and charge of the electron, and, as a result, the speed of light in atomic units is a large value, formula_21. The orbital velocity of an electron around a small atom is of the order of 1 in atomic units, so the discrepancy between the velocity units in the two systems reflects the fact that electrons orbit small atoms much slower than the speed of light (around 2 orders of magnitude slower).\n\nThere are much larger discrepancies in some other units. For example, the unit of mass in atomic units is the mass of an electron, while the unit of mass in Planck units is the Planck mass, a mass so large that if a single particle had that much mass it might collapse into a black hole. Indeed, the Planck unit of mass is 22 orders of magnitude larger than the au unit of mass. Similarly, there are many orders of magnitude separating the Planck units of energy and length from the corresponding atomic units.\n\n\n"}
{"id": "24744037", "url": "https://en.wikipedia.org/wiki?curid=24744037", "title": "Bamboo bicycle", "text": "Bamboo bicycle\n\nBamboo bicycles are pedal-and-chain-driven, human-powered, single-track vehicles that have two wheels attached to a bamboo frame. Because of its light weight, vibration damping, and sustainability, bamboo is slowly starting to be used in bicycle frame production, though the industry is still dominated by aluminium frames.\n\nBamboo bikes were first patented in England by the Bamboo Cycle Company and introduced to the general public on 26 April 1894. A US patent was applied for in 1895, by August Oberg and Andrew Gustafson, and granted in 1896. However, with the development of tougher industrial metals, such as steel and aluminium, large-scale usage of bamboo to build bicycles never happened.\n\nThough bicycles are a staple of human transportation, in both rural and urbanised areas, bamboo bicycles are not widely used. However, with the advent of the Green movement, bamboo is being used again, primarily for high-end racing and touring bicycles. Bamboo bikes are entering the market as low cost alternatives to relatively expensive and unsustainable aluminium and metal bikes.\n\nSeveral aspects of bamboo are extremely valuable to both cyclists and bicycle manufacturers: high strength-to-weight ratio, vibration control, and sustainable growth. Because of bamboo's tendency to grow straight, it does not exhibit \"knots\" and \"turns\" in its wood, unlike other types of wood. As a result, bamboo has a higher specific tensile strength than steel, as well as a higher specific compressive strength than concrete. This tendency also allows for excellent vibration control, which, in turn, provides for a smoother ride and increased stability on rough terrain.\n\nThe bamboo poles can be joined in a number of different ways. The earliest models used metal joints which were then tightened around the bamboo. Another approach is to wrap the joints with resin saturated fibers to make composite lugs around the bamboo frame members. For modernised track bicycles, carbon fibre is used for the remainder of the parts that are not made of bamboo due to its light weight - for example, the fork, because it is difficult to find a perfect piece of bamboo that fits into the fork socket of the frame.\n\nBamboo could be used as an alternative to traditional steel and aluminium bikes in many rural areas because of its potential sustainability. \n\nA \"Bamboo Bike Project\", started by engineers at Columbia University, created a small number of bamboo bikes from 2007 to 2011. This was done with the intention of providing low cost bikes for Africans in rural areas, and stimulating local bike building industries, without major assistance from outside sources.\n\nThe United Nations and the United States have invested in the Ghana Bamboo Bike Initiative, which was created by Bernice Dapaah. This business markets itself as addressing climate change, poverty, youth unemployment, rural-urban migration and by creating jobs for women. As of 2015 over 1000 of these bikes had been sold in Ghana, Europe and the United States.\n\nAs of 2016 few bicycles have also been made in Bangladesh.\n\nThe most expensive bamboo bikes cost thousands of dollars. Conversely, the bikes sold in Ghana for the Ghana Bamboo Bike Initiative have been said to cost about $120, which is significantly more expensive than conventional bicycles. The cheapest for adults retail at $350 as of 2018, thus the bikes are retailed specifically for the export market.<ref>\n"}
{"id": "315929", "url": "https://en.wikipedia.org/wiki?curid=315929", "title": "Calendar reform", "text": "Calendar reform\n\nCalendar reform, properly calendrical reform, is any significant revision of a calendar system. The term sometimes is used instead for a proposal to switch to a different calendar design.\n\nThe prime objective of a calendar is to unambiguously identify any day throughout history by a specific date. \nPeriods that contain multiple days, like weeks, months and even years, are secondary features of a calendar for convenience.\nMost cultures adopt a primary dating system, but different cultures have always needed to align multiple calendars with each other, because they coexisted in the same space (e.g. secular and religious groups with different demands) or had established trading relations. \nOnce specified, a \"calendar design\" cannot be altered without becoming a new one, but if a design is sufficiently close to the one used before in the local \"calendar system\", switching to it and hence a calendar reform is possible without disruption. \nSome design changes, however, will yield date identifiers different from the previous design for some days, often in the distant past or future. \nThe calendar system must clarify whether dates are changed to the new design retroactively (using a \"proleptic calendar\") or whether the design in use then and there shall be respected. \nCalendar schisms happen if not all cultures that adopted a common calendar system before perform a calendar reform at the same time. \nIf date identifiers are similar but different, confusion and mistakes are unavoidable. \nMost calendars have several rules which could be altered by reform:\n\nHistorically, most calendar reforms have been made in order to synchronize the calendar with the astronomical year (either solar or sidereal) and/or the synodic month in lunar or lunisolar calendars. Most reforms for calendars have been to make them more accurate. This has happened to various lunar and lunisolar calendars, and also the Julian calendar when it was altered to the Gregorian calendar.\n\nThe fundamental problems of the calendar are that the astronomical year has neither a whole number of days nor a whole number of lunar months; neither does the lunar month have a whole number of days: in each case there are fractions \"left over\". (In some physical circumstances the rotations and revolutions of a planet and its satellite(s) can be phase-locked —for example the same side of the moon always faces us— but this has not operated to lock together the lengths of the Earth's year, day and month.) Such remainders could accumulate from one period to the next, thereby driving the cycles out of synch.\n\nA typical solution to force synchronization is 'intercalation'. This means occasionally adding an extra day (or month) into the cycle. An alternative approach is to ignore the mismatch and simply let the cycles continue to drift apart. The general approaches include:\n\n\nAn obvious disadvantage of the lunisolar method of inserting a whole extra month is the large irregularity of the length of the year from one to the next. The simplicity of a lunar calendar has always been outweighed by its inability to track the seasons, and a solar calendar is used in conjunction to remedy this defect. Identifying the lunar cycle month requires straightforward observation of the Moon on a clear night. However, identifying seasonal cycles requires much more methodical observation of stars or a device to track solar day-to-day progression, such as that established at places like Stonehenge. After centuries of empirical observations, the theoretical aspects of calendar construction could become more refined, enabling predictions that identified the need for reform.\n\nThere have been 50 to 100 reforms of the traditional Chinese calendar over 2500 years, most of which were intended to better fit the calendar months to astronomical lunations and to more accurately add the extra month so that the regular months maintain their proper seasonal positions, even though each seasonal marker can occur anywhere within its month.\nThere have been at least four similar reforms of the lunisolar version of the Hindu calendar, all intended to make the month a better match to the lunation and to make the year a better fit to the \"sidereal\" year. There have been reforms of the \"solar\" version of the Hindu calendar which changed the distribution of the days in each month to better match the length of time that the Sun spends in each \"sidereal\" zodiacal sign. The same applies to the Buddhist calendar. The first millennium reform of the Hebrew calendar changed it from an observational calendar into a calculated calendar. The Islamic calendar was a reform of the preceding lunisolar calendar which completely divorced it from the solar year.\n\nAnother reform was performed in Seljuk Persia by Omar Khayyam and others, developing the precisely computed Jalali calendar.\n\nWhen Julius Caesar took power in Rome, the Roman calendar had ceased to reflect the year accurately.\n\nThe Julian reform lengthened seven months and replaced the intercalary month with an intercalary day to be inserted within February every four years. This produced a noticeably more accurate calendar, but it had an average year length of 365 days and 6 hours (365.25 d). This had the effect of adding about three-quarters of an hour every four years. The effect accumulated from inception in 45 BC until by the 16th century the northward equinox was falling on March 10 or 11.\n\nUnder Pope Gregory XIII, the leap year rule was altered: century years which are not divisible by 400 would be common years. So 1700, 1800, 1900, 2100, 2200, 2300 and 2500 are not leap years. This rule makes the mean year 365.2425 days long. While this does not synchronize the years entirely, it would require a few thousand years to accumulate a day.\n\nSo that the northward equinox would have the same date in the new Gregorian calendar as it had when the Council of Nicaea made recommendations in AD 325 (), ten days were dropped so that became in 1582. This reform took a few centuries to spread through the nations that used the Julian calendar, although the Russian church year still uses the Julian calendar. Those nations that adopted this calendar on or after 1700 had to drop more than ten days: Great Britain, for instance, dropped eleven.\n\nIn 1923, Milutin Milanković proposed to a synod of some Eastern Orthodox Churches at Constantinople that only those centennial years (those ending in 00) that leave a remainder of 200 or 600 upon division by 900 would be leap years, decreasing the average year length to 365.242222 days. These remainders were chosen to delay the first year (after 1923) that this calendar would disagree with the Gregorian calendar as much as possible, until 2800. It was adopted by some Eastern Orthodox Churches under the names Revised Julian calendar or New calendar, but was rejected by others.\n\nThe Gregorian calendar is currently used by most of the world. There is also an international standard describing the calendar, ISO 8601, with some differences to traditional conceptions in many cultures.\n\nSince the papal reform in 1582, several proposals have been offered to make the Gregorian calendar more useful or regular. Very few reforms have gained official acceptance. The rather different decimal French Republican Calendar was one such official reform, but was abolished twelve years later by Napoleon. After World War II, the newly formed United Nations continued efforts of its predecessor, the League of Nations, to establish the proposed World Calendar but postponed the issue after a veto from the government of the United States, which was mainly based upon concerns of religious groups about the proposed days that would be outside the seven-day week cycle (\"blank days\") and thus disrupt having a sabbath every seven days. Independently the World Council of Churches still tries to find a common rule for the date of Easter, which might be eased by a new common calendar.\n\nReformers cite several problems with the Gregorian calendar:\n\nIt is hard or even impossible to solve all these issues in just one calendar.\n\nMost plans evolve around the solar year of a little more than 365 days. This number does not divide well by seven or twelve, which are the traditional numbers of days per week and months per year respectively. The nearby numbers 360, 364 and 366 are divisible in better ways. There are also lunar-centric proposals.\n\nMany calendar reforms have offered solutions to make the Gregorian calendar perennial. These reforms would make it easy to work out the day of the week of a particular date, and would make changing calendars each year unnecessary. There are, roughly speaking, two options to achieve this goal: leap week calendars and intercalary days. Leap week calendars add a leap week of seven days to the calendar every five or six years to keep the calendar roughly in step with the tropical year. They have years of either 364 days (52 weeks) or 371 days (53 weeks), thus preserving the 7-day week.\n\nProposals mainly differ in their selection of a leap rule, placing of the leap item (usually middle or end of the year), in the start day of the week and year, in the number (12 or 13) and size of months and in connected naming; some are compatible to the week date of ISO 8601.\n\nThe World Calendar, favored by the UN in the 1950s, and the International Fixed Calendar, quite popular among economists between the World Wars, are proposals that start each year on a Sunday. The 364 days within the week system form 52 weeks of 7 days. The World Calendar has every quarter beginning on the same day of the week. In the World Calendar, the 365th and 366th day are considered holidays and named Worlds Day and Leap Year Day. These \"off-calendar\" days stand outside the seven-day week and caused some religious groups to strongly oppose adoption of the World Calendar. Such concerns helped prevent the World Calendar from being adopted. Supporters of the World Calendar, however, argue that the religious groups' opposition overlooked every individual's right to celebrate these holidays as extra days of worship, or Sabbaths. This option, they reason, maintained the seven-day worship cycle for those who share that concern, while allowing benefits of a perennial calendar to be shared by all.\n\nSome calendar reform ideas, such as the Pax Calendar, Symmetry454 calendar and the Hanke-Henry Permanent Calendar, were created to solve this problem by having years of either 364 days (52 weeks) or 371 days (53 weeks), thus preserving the 7-day week. The 53-week calendar, used in government and in business for fiscal years, is a variant of this concept. Each year of this calendar can be up to 371 days long.\n\nSome calendars have quarters of regularly patterned uneven months e.g., a 35-day (five-week) month and a pair of 28-day (four-week) months, with a leap week appended to the final month when needed. The Common Civil Calendar and Time calendar has months of 30 and 31 days, but inserts a leap week in the middle of the year, when needed, whereas its successor, the Hanke-Henry Permanent Calendar, moves the extra week to the end of the year.\n\nIn the World Season Calendar, months are discarded altogether; instead, the year is divided into four seasons of 13 weeks each. An extra day (two days during leap year) is added to the calendar that is not assigned a day of the week in order to perennialize the calendar. The same calendar of 91 days is used for each season of every year.\n\nStill other proposals, like the 30x11 Calendar, abandon attempts to make the calendar perennial, instead opting for eleven 30-day months and one long month at the end of 35 (or 36) days.\n\nThe lengths of the months inherited from the old Roman calendar as reformed by Julius Caesar do not follow any apparent logic systematically. Many reform proposals seek to make the pattern more uniform. \nWhen keeping the traditional dozen of months and the close approximation of a solar year, this usually yields four equal quarters of three months each where one month is longer than the other two. The most common approaches are 30∶30∶31, 30∶31∶30 and 31∶30∶30 days per month, but 4∶4∶5, 4∶5∶4 and 5∶4∶4 weeks per months have also been proposed or used. They all result in 364 systematically distributed days and hence have to add either one extra and one leap day or a leap week. \nSome calendar designs do not use the same pattern for all quarters but, for instance, flip it in the middle of the year. This is more common with 31∶30 and 30∶31 patterns, which lead to 366 days of which one needs to be dropped in common years, since that is basically what the Julian and Gregorian calendars do except that July also has 31 days and February never has 30.\n\nIncidentally, the close correspondence to existing conventions is often considered an advantage, but even patterns that deviate more may have strong benefits. For instance, all quarterly patterns with 30 and 31 days per month result in a week distribution of 4∶5∶4 if a week is counted for the month most of its days belong to and the first day of the first month is also the first day of a week.\n\nA change in month lengths will frequently also result in months starting at a different ordinal day of the year, especially since all regular designs lengthen February. For a non-leap year, all designs with 91-day quarters realign in October with the Julian pattern and some as early as June, while all designs that are alternating odd and even months completely mismatch after January even if the last day of February remained the leap day.\n\nSome calendar reformers seek to equalize the length of each month in the year. This is often accomplished by creating a calendar that has 13 months of 4 weeks (28 days) each, making 364 days. The earliest known proposal of this type was the Georgian Calendar (1745) by Rev. Hugh Jones.\n\nThe Positivist calendar (1849), created by Auguste Comte, was based on a 364-day year which included one or two “blank” days. Each of the 13 months had 28 days and exactly four weeks, and each started on a Monday. The International Fixed Calendar is a more modern descendant of this calendar.\n\nSome proposals add one or two days to the calendar each year to account for the annual solar cycle, while others keep these days off the calendar entirely, to make the calendar perennial.\n\nAround 1930 Colligan invented the Pax Calendar, which avoids off-calendar days by adding a 7-day leap week to the 364-day common year for 71 out of 400 years. \n\nIn 1993, Lloydine and José Argüelles founded the World Thirteen Moon Calendar Change Peace Movement.\n\nDividing the years into 52 weeks creates 13 months of 28 days (4 weeks), and 4 quarters of 91 days (13 weeks). This creates months and quarters of fixed, even duration, but not quarters containing a whole number of months.\n\nInstead of a leap week or an extra day and a leap day, such a calendar design could add a 14th \"leap month\" of 4 weeks about every 22 years.\nWhile the Gregorian leap cycle of 400 years contains a whole number of 7-day weeks, this number is not divisible by 4, so this cycle would not be a good fit for a leap month calendar.\n\nThe Bahá'í calendar, established by the founders of the Bahá'í Faith, does not seek to reform the Gregorian calendar, but establishes a purely solar calendar. As of 2015, the year always starts in spring on the sunset to sunset day of the vernal equinox (calculated in advance). This calendar has 19 months of 19 days, with an extra 4 or 5 intercalary days added so that the year ends on the day before the next vernal equinox.\n\nLunisolar calendars usually have 12 or 13 months of 29 or 30 days.\n\nSome propose to improve leap rules of existing calendars, such as the Hebrew calendar. The Rectified Hebrew calendar uses a more accurate leap cycle of 4366 months per 353-year cycle, with 130 leap years per cycle, and a progressively shorter \"molad\" interval, intended to replace the 19-year leap cycle and the constant \"molad\" interval of the traditional fixed arithmetic Hebrew calendar, respectively.\n\nCalendar proposals that introduce a thirteenth month or change the Julian-Gregorian system of months often also propose new names for these months. New names have also been proposed for days out of the week cycle (e.g., 365th and leap) and weeks out of the month cycle.\n\nProposals to change the traditional month and weekday names are less frequent. The Gregorian calendar obtains its names mostly from gods of historical religions (e.g., Thursday from Nordic Thor or March from Roman Mars) or leaders of vanished empires (July and August from the first Cæsars), or ordinals that got out of synchronization (September through December, originally seventh through tenth, now ninth through twelfth).\n\nCalendar reformers, therefore, seek to correct what they see as deficiencies by focusing on more homogeneous sets of individuals, who usually share common traits.\n\nComte’s Positivist calendar, for example, proposed naming the 13 months in his calendar after figures from religion, literature, philosophy and science. Similarly, the Hermetic Lunar Week Calendar uses 12 or 13 lunar months named after 13 contributors to research on psychoactive plants and chemicals. The Simple Lunisolar Calendar names its months after the letters of the Greek alphabet.\n\nSome, such as Karl Palmen, have suggested reusing an existing 13 × 4 naming system, the one found in playing cards. Thus either months are numbered Ace, Two through Ten, Jack, Queen and King with four weeks each, named after the four suits (♠♣♥♦); or the roles are reversed if the calendar has four quarters with thirteen weeks each. Leap days or weeks are assigned the Joker. This system has internationalisation problems, though, because even where the 52-card deck is known, the order of suits may vary.\n\nThe contemporary ISO basic Latin alphabet has 26 letters, which could be used, together with a further binary indicator, as keys for 52 weeks.\n\nThere have been many specific calendar proposals to replace the Gregorian calendar:\n\nThe following count one or more days outside the standard seven-day week:\n\nThe following are leap week calendars:\n\nThe following track the moon as well as the sun:\n\nThere have also been proposals to revise the way years are numbered:\n\nReform of the Islamic calendar:\n\nSince the beginning of the 21st century, there is a trend within the Muslim communities of North America and Europe to substitute a lunar calendar based on calculations for the traditional Islamic method of monthly observation of the new moon to declare the beginning of the new month in each country separately. The details are in the following study:\n\n\n\n\n"}
{"id": "769680", "url": "https://en.wikipedia.org/wiki?curid=769680", "title": "Castellan", "text": "Castellan\n\nA castellan was the governor or captain of a castellany and its castle. The word stems from the Latin \"Castellanus\", derived from \"castellum\" \"castle\". Sometimes also known as a constable, governor of the castle district or captain, the Constable of the Tower of London is, in fact, a form of castellan. A castellan was almost always male, but could occasionally be female, as when, in 1194, Beatrice inherited her father's castellany of Bourbourg upon the death of her brother, Roger.\n\nAfter the fall of the Western Roman Empire, many tribes migrated into western Europe, causing strife and war. The answer to recurrent invasion was to create fortified areas which evolved into castles. Some leaders gained control of several areas, each with a castle. The problem lay in exerting proper control and authority in each area when a leader could only be in one place at a time. To answer this, lords gave their trusted vassals direct control of a castle, reporting to the lord only. In the ninth century, as fortifications improved and kings had difficulty making their subordinates pay their taxes or send the military aid the kings demanded, the castellans grew in power, holding their fief without much concern for their overlord's demands. This changed as kings grew in power and as the Holy Roman emperors replaced recalcitrant vassals with ministerials.\n\nUsually the duties of a castellan were combined with the duties of a majordomo, making the castellan responsible for a castle's domestic staff and its garrison, as well as a military administrator responsible for maintaining defenses and protecting the castle's lands. This was particularly the case if there was no lord resident at the castle, or if the resident lord was frequently absent. A castellan may exercise the power of the \"ban\" - that is, to hear court cases and collect the fines, collect taxes from residents, and muster local men for the defense Castellans had both the power of low justice and high justice, which allowed them to implement up to and including the death penalty, as when, in 1111, the Salzburg castellan caught the ministerial who fomented armed rebellion, he had the offender blinded, as one would a serf. At times, the castellan served as the representative of the people of his castellany, as did the castellan of Bruges, when the burghers stood up for more privileges and liberties from the counts of Flanders.\n\nOne unusual responsibility in western Europe concerned jurisdiction over the resident Jewish communities near the English Channel. The Constable of the Tower of London and those castellans subordinate to the dukes of Normandy were responsible for their administration. Vivian Lipman posits four reasons for this: the castles provided defense, they were centres of administration, they were potential prisons, and possibly, as the residence of a local magnate, they could be a valuable client.\n\nIn France, castellans (known in French as châtelains) who governed castles without resident nobles acquired considerable powers, and the position actually became a hereditary fiefdom. By the tenth century, the fragmentation of power became so widespread in Mâcon that the castellany was considered the basic unit of governance, without effective levels above it - the counts of Mâcon were largely ignored by their castellans from about 980 to 1030. In the 12th century \"châtelains\" had become lords, they expanded their territories to include taking weaker castellanies under their control, as when then castellan of Beaujeu took over lands in Lyons, or when Uxelles annexed Briançon, then Sennecey-le-Grand and L'Épervieres.\n\nIn other areas, the castellans never managed to rise to the lordship and they remained a local officer of an overlord. During the Ancien Régime, castellans were heads of the local royal administration, but their power was delegated to their lieutenants.\n\nAll remaining lordships and royal local administration were suppressed during the French Revolution. During the 19th and 20th centuries, \"châtelain\" is used to describe the owner of a castle or manor house, in many cases a figure of authority in his parish, akin the English squire.\n\n In Germany the castellan was known as a \"Burgmann\", or sometimes \"Hauptmann\" (\"captain\"), who reported to the lord of the castle, or \"Burgherr\", also often known as the burgrave (\"Burggraf\"). The \"burgmann\" may have been either a free noble or a \"ministerialis\", but either way administered the castle as a vassal. A \"ministerialis\", being wholly indebted to a lord, was more easily controlled. \"Ministeriales\" replaced free nobles as castellans of Hohensalzburg under Conrad I of Abensberg’s tenure as Archbishop of Salzburg from 1106 to 1147, starting with Henry of Seekirchen in the 1130s.\n\nIn the Medieval Kingdom of Hungary usually was called \"várnagy\", and in the Latin charts it appeared as \"castellanus\". The lord of the castle had very similar functions than the German ones. In Hungary first the King, later the most powerful noblemen designed the castellans between their followers for the administration of their castles and the states that belonged to the fortress.\n\nAt times, there was a castellan among the Officers of the Kingdom of Jerusalem, starting with Anselm (castellan, c. 1110).\n\nIn the Kingdom of Poland and later the Polish-Lithuanian Commonwealth, the castellans () were in most cases lower in precedence to the voivodes (with the exception of the Burgrave of Kraków [Polish \"Burgrabia krakowski\"] who had precedence before the Voivode of Kraków). Castellans were in charge of a part of a voivodeship called the castellany (Polish \"Kasztelania\") until the 15th-century and from that time on their domain was divided into provinces for greater castellans, and powiats for minor castellans. Castellans in the Polish–Lithuanian Commonwealth were of senator rank.\n\nIn Portugal, a castellan was known as \"alcaide\". Later, the role of \"alcaide\" became an honorary title awarded by the King of Portugal to certain nobles. As the honorary holder of the office of \"alcaide\" did not often live near its castle, a delegate of him started to be appointed to effectively govern it. An honorary holder of the office became known as \"alcaide-mor\" (major \"alcaide\") and its delegate became known as \"alcaide pequeno\" (little \"alcaide\") or \"alcaide-menor\" (minor \"alcaide).\n\nA castellany, or castellania, is a term denoting a district administered by a castellan. Castellanies appeared during the Middle Ages and in most current states are now replaced by a more modern type of country subdivision. The word is derived from castle and literally means the extent of land and jurisdiction belonging to a given castle.\n\nIt also renders equivalent, often cognate, terms in other languages. Examples of French \"châtelainies\" include the castellanies of Ivry-la-Bataille, Nonancourt, Pacy, Vernon and Gaillon, all in Normandy, which under in the treaty of Issoudun (1195, after a war with king Richard of England) were acquired for the French crown by Philip Augustus. Examples from Poland include Leczyca and Sieradz (both once a duchy), Spycimierz, Rozprza, Wolbórz and Wojnicz (in Cracow diocese) and Otmuchow in Silesia.\n\n"}
{"id": "53075595", "url": "https://en.wikipedia.org/wiki?curid=53075595", "title": "Civic space", "text": "Civic space\n\nCivic space is created by a set of universally-accepted rules, which allow people to organise, participate and communicate with each other freely and without hindrance, and in doing so, influence the political and social structures around them. It is a concept central to any open and democratic society and means that states have a duty to protect people while respecting and facilitating the fundamental rights to associate, assemble peacefully and express views and opinions.\n\nAlthough the term \"civic space\" has only been in common usage since after the turn of this millennium, the rights upon which the concept is based have been a necessary feature of every democratic society. Global civil society alliance CIVICUS began using the term regularly after the inception of the Civic Space Initiative in 2011, defining it as \"the place, physical, virtual, and legal, where people exercise their rights to freedom of association, expression, and peaceful assembly. By forming associations, by speaking out on issues of public concern, by gathering together in online and offline fora, and by participating in public decision-making, individuals use civic space to solve problems and improve lives. A robust and protected civic space forms the cornerstone of accountable, responsive democratic governance and stable societies.\" Civic space is also closely connected to the development of post–World War II human rights norms, and particularly the Universal Declaration on Human Rights in 1948 which established clear protections for the rights to freedom of association, peaceful assembly and expression.\n\nAs a concept, civic space is also closely related to the evolution of the concept of civil society. While the ideas embodied in civil society can be traced to many different civilisations, the term civil society has many different definitions but has its roots in ancient Greece and the early work of Aristotle on the concepts of \"community\" or \"polity\" characterised by a shared set of norms or ethos. Some theorists interpret civil society very closely to the modern understanding of civic space, taking it to mean \"the elements such as freedom of speech, an independent judiciary, etc, that make up a democratic society.\" Especially in the discussions among thinkers of Eastern and Central Europe, civil society is seen also as a concept of civic values. Most modern-day definitions of civil society however view it more in terms of \"the arena, outside of the family, the state and the market where people associate to advance common interests.\"\n\nCore civic space rights – the rights to freedom of association, freedom of peaceful assembly and freedom of expression – are guaranteed by law. States that are signatories of international conventions (such as the United Nations International Covenant on Civil and Political Rights) and other regional treaties containing similar provisions, are obliged to respect and protect these rights. These states are also obligated to protect these rights in domestic law and many of the ratifying States have included protection of these rights in their constitutions as fundamental freedoms. States are obliged not only to respect and promote these rights but also to protect them from infringement by both state and non-state actors. Therefore, a progressive constitution supported by a sound legislative framework that is upheld by a responsive and independent law enforcement machinery is key to enabling and preserving civic space. Notably, Goal 16 of the Sustainable Development Goals framework agreed to by all UN member states promises to 'promote just, peaceful and inclusive societies.’\n\nAs a starting point, while there have been a number of international initiatives launched over recent years to defend civic space, civil society actors were lacking a strong common definition of the terms and shape of the civic space to be defended. To fill this gap, an informal meeting of CSOs, which took place in Bangkok in November 2015, asked the International Civil Society Centre to facilitate the development of a Civic Charter. The Civic Charter provides a global framework for people’s participation in shaping their societies. The two-page document, which people and organisations can sign on to and use as a basis for joint action, articulates a common set of civic and political rights. Aimed at civil society activists and their organisations, the Civic Charter connects those engaged in the everyday struggle for civic space – on a local, national, regional or international level. Firstly, it draws together the most crucial terms for civic participation in an easily understandable way. Secondly, it serves as a global reference framework to civil society actors for their rights enshrined in international law. Thirdly, it reaffirms people’s rights to participate in shaping their societies. The Civic Charter can provide a more effective basis for campaigning and advocacy for civic participation, as well as for promoting international solidarity with CSOs and activists in a specific country or region.\n\nThe right to freely associate includes the right of every person without distinction of any kind, such as race, colour, sex, language, religion, political or other opinion, national or social origin, property, birth, sexual orientation or other status, to “establish a civil society organisation and also to freely join one or choosing not to participate. Individuals may operate civil society organisations and participate in their activities without fear or unwarranted interference. Freedom of association also encompasses the right to establish branches, recruit staff, raise funds freely, to fair taxation levels and to affiliate and cooperate with other organisations locally, nationally or internationally.” It also includes the right to form and join trade unions for the protection of his interests. \nInternational law protects the freedom of association and obligates states not to interfere with this right and only in instances where intervention by the state is necessary in the interests of national security, public safety or public order; the protection of public health or morals; or the protection of the rights and freedoms of others. Like the freedom of expression, the margin for restricting this right is very limited. State actions must lean in favour of permitting civil society activities and creating an enabling environment for civil society to function and thrive.\n\nThe right to freedom of expression entails, according to the Universal Declaration of Human Rights, the \"freedom to hold opinions without interference and to seek, receive and impart information and ideas through any media and regardless of frontiers.\" \nThe right is fundamental to the existence of civil society. It includes \"the right to access information, critically evaluate and speak out against the policies and actions of state and non-state actors, as well as publicly draw attention to and carry out advocacy actions to promote shared concerns, without fear of retribution from any quarter. Civil society organisations are also assured the freedom to carry out investigations and document their findings under this right.\" \nAccording to international law (for example see Article 20 of the ICCPR), freedom of expression can only be restricted in certain limited circumstances, provided by law and where it is necessary to protect the rights and reputations of others and to safeguard national security, public order, public health and morals.\n\nThe right to freely assemble \"assures civil society the freedom to exercise legitimate dissent through peaceful forms of protest as well as organise meetings and hold demonstrations to forward matters of common interest.\" International law places the same limitations on the restriction of this right as in the case of freedom of association. Moreover, international standards limit the use of force by the authorities in managing public assemblies.\n\nAt the international level, civic space violations are regularly documented and reported by international human rights organisations like CIVICUS, Human Rights Watch, Amnesty International, the International Centre for Not-for-profit Law (ICNL), Article 19, Reporters Without Borders, Frontline Defenders, the International Service for Human Rights, the International Federation for Human Rights (FIDH) and others. At the regional, national and sub-national levels there are innumerable human rights organisations, development organisations, think tanks and academic institutions devoted to researching and campaigning on civic space issues.\n\nViolations of civic space take many forms and can be perpetrated by the state - either directly through officials, police officers or security personnel, or at arm's length through agents or third parties - or non-state actors including criminal gangs, terrorist organisations, private corporations, political parties, religious organisations and civil society groups themselves.\n\nStates can violate freedom of association by establishing restrictive laws that restrict people's ability to freely form and operate groups to advance their interests or even prevent them for doing so. For example, in some countries, legislation banned organisations that work in certain topics like human rights to form legally. Moreover, these restrictive laws place broad oversight powers in the hands of government institutions with which civil society organisations must register and to whom they must account or report regularly. In some cases those governmental oversight bodies have a discretionary power to suspend or terminate an organisation's registration. In the most extreme cases, penalties for noncompliance with NGO laws can include fines and imprisonment. In more recent years, and with a spike in concern about international terrorism, more and more states have begun to pass anti-terrorism laws which negatively impact upon civic space, and particularly the ability of organisations to operate free from state interference. States can restrict freedom of association by targeting members of the organisations, human rights defenders and activists using intimidation tactics, harassment, travel bans, detention, among others.\n\nCurtailing the ability to protest can be done within the legislative framework, when the norms in placed unduly restricted the right to assembly by imposing prior authorisation requirements, time and place restrictions, among others. In practice, this right can be restricted when security forces use excessive force to disperse protesters, arbitrary arrest them and even charge protesters with provisions of the criminal code and even anti terrorism legislation.\n\nTargeting free speech can take various forms. As the media plays a crucial role in disseminating information, States often overregulated media in order to limit, control or prevent critical and dissenting voices to express. In some cases, the pluralistic views are also undermined by media concentration in just a few private corporations. Criminalisation of dissent by using defamation provisions is also commonly used to restrict freedom of expression. Acts of violence against journalists and media workers for reason related to their professional work often occurs and limits the right to freedom of expression and access to information as it also prevents the public from accessing that information.\n\nThe internet is a unique tool that allowed individuals to communicate instantly, with a positive impact on the way we currently share and access information. For that reason, it is particularly targeted by authorities to curtail dissent. Some governments restrict the dissemination of content on certain sensitive issues by blocking access to social media platforms, deleting certain pages or content and even arresting people for disseminating sensitive information.\n\n"}
{"id": "47534588", "url": "https://en.wikipedia.org/wiki?curid=47534588", "title": "Civility", "text": "Civility\n\nCivility comes from the word \"civilis\", which in Latin means \"citizen\". Civility is caused by a person's emotions or lack thereof. If a person is emotionally affected by the negative feedback that they get from other people in a psychologically normal manner then they are defined as civil.\n\nLate Middle English: from Old French \"civilite\", from Latin \"civilitas\", from civilis 'relating to citizens' (see civil). In early use, the term denoted the state of being a citizen and hence good citizenship or orderly behavior. The sense 'politeness' arose in the mid-16th century.\n\nAdolf G. Gundersen and Suzanne Goodney Lea have developed a civility model grounded in empirical data that \"stresses the notion that civility is a sequence, not a single thing or set of things\". The model conceives of civility as a continuum or scale consisting of increasingly demanding traits ranging from \"indifference\" to \"commentary\", \"conversation\", \"co-exploration\" and, from there, to \"habituation\". According to the authors, such a developmental model has several distinct advantages, not least of which is that it allows civility to be viewed as something everyone can get better at.\n\nThe International Day of Peace (\"Peace Day\") is observed by many countries on September 21. Peace day was first started in 1981 by declaration of the United Nations General Assembly. The voting was overwhelmingly in favor of enacting Peace Day, and so the observance was thus born. The goal of Peace Day is to help bring the world together, and share a collective attitude of peace and harmony. Many countries around the world celebrate September 21 as a day of non-violence and cease-fires to emulate the mission of global peace. Since its beginnings the day has spread more throughout the world, with many countries annually participating in this observance.\n\nParticipation is open to all people of the world. People may choose to celebrate Peace Day in different ways, but the goal is to have peace at the heart of all the activity's intentions. Individuals, businesses and organizations are also welcome and encouraged to celebrate Peace Day. Spreading peace and good will towards all mankind is the great aim that's in view for raising awareness for Peace Day.\n\nIn May 2007, the Global Peace Index (GPI) was launched in an attempt to measure the relative ranking of peacefulness around different countries around the world. Today, the Global Peace Index is maintained the Institute for Economics and Peace, and is conducted on an annual basis. The index primarily measures three different categories to determine the level of peace in the world. These levels look at the overall security, crime levels, and the build up of military forces. By measuring levels of peace, the hope is that a greater general awareness and attention is given towards making the world a more peaceful place.\n\nA 2010 Allegheny College poll found that nearly all Americans (95 percent) believe civility is important in politics.\n\nIn a 2012 poll conducted by Weber Shandwick, 65% of Americans reported an increase in incivility due to a weakened U.S. economy during the Great Recession. Almost 50% of those same Americans surveyed indicated they have removed themselves from participating in any politics because of fear of incivility or bullying. Of the 1000 people surveyed, a follow-up study revealed that 86% of those people reported being subjected to incivility. In this report, a part of an annual follow-up research report in January 2016 sharing findings on attitudes and sentiment about civility, 95% of Americans believe that incivility is a very visible issue, while 74% recognized that civility in general had declined during the past few years. Over 90% of voters claimed that the presidential candidates' attitudes and civil behavior would play a significant role in their voting decision in the upcoming 2016 election.\n\nMany projects are led by State Supreme Courts to foster civility. One of these initiatives is led by the California Judicial Branch for their Civics Education Outreach program. The primary objective of this program is to teach young adults and students how democracy is supposed to function in the United States and other details about how legal processes work. The mission is to have students leave the program with a greater interest and understanding of U.S. courts, legal processes, and democracy.\n\nPenn State University conducted a study on perceptions of the legal professional and its relation to civility. They found that general opinion pointed to a drop or decline in civility with the legal profession. To counteract demeaning and unprofessional behavior, there has been several initiatives put in place by state bar associations. However, the legal profession is not the only industry that has adopted civility standards. Many other companies and organizations across several industries have adopted civility standards that have also help to increase workplace civility.\n\nNumerous universities in the U.S., such as the University of Colorado, the University of Missouri, University of California Davis, Johns Hopkins University, University of Wisconsin, Rutgers University, American University, and California State University San Marcos have created programs designed to foster and define what civility means on their campuses. Some colleges, such as the Arizona State University, offer an undergraduate certificate in Civil Communication. Still other universities, such as Kansas State University, have developed programs in dialogue and deliberation which involve codes of behavior that foster constructive, civil discourse. Although many colleges have adopted programs to foster civility efforts, there are still many colleges and universities, including many of the Ivy League schools, that do not have or list no visible place online about any civility initiatives, codes or standards.\n\nNumerous community groups have formed throughout the U.S. to restore constructive civility in the public space. The Civility Toolkit with approximately 300 civility tools aggregated by the Civility Center with a mission to help provide access to resources regarding civility and to help restore civility in society. Many of these groups are members of the National Coalition for Dialogue and Deliberation. Other programs like iCivics, which was started by Sandra Day O'Conner U.S. Justice, provides educational tools for students that teach about the importance of actively taking part in democracy. Although some private schools offer courses geared to teach about the U.S. government and legal system, most public schools do not teach about the U.S. government until their junior or senior year in high school. To help bring these lessons into the classroom, O'Conner's program provides teachers with educational tools, such as printable goals, games, and various lesson plans.\n\nArnett and Arneson define civility \"a metaphor that points to the importance of public respect in interpersonal interaction.\" The difference between tolerating someone, and respecting them are concerned with the outlook that toleration does not imply respect, but respect requires understanding of another person's perspective. Having social intelligence or \"Social IQ\" impacts our ability to empathize with people, and realize all people are human and that if respect or common ground cannot be met that we strive for at least toleration in order to be civil.\n\nIn \"Psychology Today\", Price-Mitchell describes civility as a personal attitude that acknowledges other humans' rights to live and coexist together in a manner that does not harm others. The Psychology of civility indicates awareness, ability to control oneself's passions, as well as have a deeper understanding of others are a part of civil obligation, which everyone should strive to participate. This may suggest that civility goes beyond mere toleration, but may inherently imply a mutual co-existence and respect for humankind. Some may relate this to the ideas expressed by singer John Lennon in the song \"Imagine\", with the words \"Imagine all the people sharing all the world.\" Although the level of peace can be a subjective topic, many people would agree that it requires a certain degree of harmony and opposes violence in order to remain civil.\n\nIn the academic journal \"Philosophy & Public Affairs\", Calhoun delineates civility as an element of dialogue that sheds light on \"basic moral attitudes of respect, tolerance, and considerateness\". The topic of civility is expansive, and can be viewed from many different perspectives. Calhoun considers civility to be a part of the moral virtues that can differ from what is socially acceptable, since what is socially acceptable is not always morally correct.\n\nThe Freemasons and members of the entire Masonic family, have had a long history of fostering civil dialogue and building civil societies. Masonic Lodges represent a peaceful assembly of people that are of different places of origin, languages, backgrounds, and beliefs. In particular, the principles and tenets of Freemasonry's values aim to promote and restore civility in the United States and around the globe. In 2015, Grand Master Charvonia of the Grand Lodge of California declared May 25, 2015 to be the \"Champion Civility Month\", which encouraged Freemasons throughout California to make an effort to bring more civility into their local lodges and community. Additionally, Freemasons from around the world have been working to repair civility through the Masonic Family Civility Project. This Civility Project was built to help raise awareness of Civility, and by providing social conversations, civility resources, multimedia education, and information for anyone to access.\n\nRecent studies and polls from 2014 indicate that Americans find workplace incivility to be a growing problem that has had a negative impact on them and their duties at work. One study's research suggests 60% of employees think that their co-workers' irritating habits to have negatively affected them at their job. In the same study, 40% reported that they are looking for another job opportunity because of another negative co-worker. These studies suggest that incivility in the workplace dampens productivity and an adverse effect on an organization's bottom line. Although this data is only looking to quantify how widespread workplace incivility is in the workplace, it does not account for how many people encounter workplace incivility and are not sure what they can do about it. Furthermore, it is not taking into consideration how many of these workplaces have civility tools or initiatives at the researched companies.\n\nNumerous organizations, including the United States government, have actively attempted to put in place measures to prevent incivility in the workforce. One measure that was initiated to reduce workplace incivility, was processing cases of sexual harassment to be illegal, which is defined by the US Equal Employment Opportunity Commission (EEOC) as being against the law in every state to harass any person during the employment or hiring process because of that person's gender. Harassment can include \"sexual harassment\", but is not limited to workplace bullying, cyber bullying, physical and verbal threats. Although many would agree that sexual harassment is an issue that should be illegal, it has really been in the spotlight of the attention in the U.S. since 1964. Because of the legal ramifications from poor prior classification of sexual harassment cases in the past, its boundaries were more loosely interpreted and more people were subject to unwanted contact or attention. Since this the term has been redefined, people are greater protected from a legal perspective in their place of work, but must actively participate in preventing these issues by speaking up and/or reporting issues. The definition for these laws are still being written today, as more people are speaking out against the abuse.\n\nIncivility is the polar opposite of civility, or in other words a lack or completely without civility. Verbal or physical attacks on others, cyber bullying, rudeness, religious intolerance, discrimination, and vandalism are just some of the acts that are generally considered acts of incivility. Incivility is a negative part of society that has impacted many people in the United States, but as the world is becoming increasingly more transparent in social interactions, it has become more increasingly apparent that incivility has become an issue on the global stage. Social media and the web have given people the ability around the globe to freely exchange ideas, but it has not come without its consequences.\n\nPoliticians in the U.S. have frequently cited that they encounter a lack of civility in their workplace, and have disregarded it as unfortunate aspect of politics, but polls indicate that \"going negative\" can help candidates win elections. During the 2016 presidential campaign, candidate Donald Trump has regularly called his rivals \"stupid, incompetent and losers\".\n\nRecognizing that incivility online, more commonly referred to as cyber bullying, has become an increasing problem that takes away from a positive online experience. Recognizing that people harassing others online has become a problem and can have negative consequences for businesses, many companies have stepped up to create more awareness and initiatives to help. Intel in collaboration with organizations such as the Born This Way Foundation and Vox Media have made an initiative called \"Hack Harassment\" aimed to increase awareness of online harassment and anti-harassment technologies. There is no claim of solution for what can be done to stop online harassment, but many studies suggest that a large number of people are harassed online. Although there are many tactics to block cyber-bullying, such as censorship and banning users from accessing a site, it does not correct the underlying issues on what causes it in the first place. Although blocking people online from bullying others may solve some of the issues on the web, it may only manifest itself in other forms offline and in person where the possibility for violence and other physical harm could take place.\n\nOn April 22, 2016 The Associated Press-NORC Center for Public Affairs Research at the University of Chicago released a report citing that 74 percent of Americans think manners and behavior have declined in the United States. In this study they discovered that people in most cases can agree with what is appropriate and inappropriate behavior. They found that 8 out of 10 Americans find jokes made based on race, gender, or sexuality are considered inappropriate, but only a small number of people owning up to actually making these types of jokes ever. Although there were some differences between age demographics on newer technologies, such as the use of cell phones. The report suggests that nearly half of all Americans 18-29 find it acceptable to use their cell phones in a restaurant, while less than 22 percent of people over the age of 60 years old agrees.\n\nIn July 2012, the President of the Federation of Law Societies of Canada made a strong point on civility at the 5th Biennial International Legal Ethics Conference. Legislation is often very open for interpretation, unless strictly and prohibitively defined by law, but in most cases where the law is yet to be defined many lawyers can see opportunity to act immorally to win their case. The \"anything to get the job done\" mentality can not only have negative consequences in the legal system, but it could possibly further spread the potential for laws and regulations to be exploited in immoral manner.\n\nAdditionally, during 2012, the Law Society of Upper Canada decided that Joe Groia was guilty of his incivility to opposing counsel during his successful defense of John Felderhof from Insider trading and securities charges. On the same case, the Supreme Court of Canada confirmed the decision of Bar of Quebec that Giles Dore was guilty of professional misconduct because of an uncivil letter he wrote to a judge. This high-profile case brought a lot of attention to the legal definition of the word civility, and what it means to be civil in the legal profession. It has since defined a broader set of rules of what is legally considered civil in the court of law in Canada.\n\nSince the case with Joe Groia, The Law Society of Upper Canada has launched several initiatives to guard against incivility in the Canadian legal profession. To enforce The Law Society's stance on the issues of civility in the Canadian legal system, they have issued verbal warnings to lawyers who are not civil with judges and other lawyers. The counter argument against civility measures in if the new guidelines inhibit their ability to defend their clients. Since laws and rules are often open to interpretation, some lawyers consider it a conflict of interest to be civil with their opponents as they do not believe there is any way to accomplish their goals while remaining civil.\n\nIn January 2017, the BC & Yukon Freemasons in Canada stated civility being like The Golden Rule, defined as \"treating others as you would want them to treat you\". This statement was in part to a recent civility initiative grounded in respect and attitude of inclusiveness with family, community, and the society at large.\n\nAt a recent address with Gisborne's top businesswomen in early 2016, Lara Meyer an adviser to the Australian Government cited incivility in the workplace has cost New Zealand approximately $15 Million a year. Noting that Australia is also losing out about $26 Million a year due to a lack of workplace civility. There could even potentially be more loss that is unaccounted for in New Zealand businesses, as the cost of rudeness could be holding them back from working together more politely and agreeably.\n\nCivil Összefogás Fórum (Civil Cooperation Forum), founded on April 5, 2009, is a kind of umbrella organisation for numerous community groups throughout Hungary.\n\nEtiquette and language\n\nEtiquette and society\n\nWorldwide etiquette\n\n"}
{"id": "296059", "url": "https://en.wikipedia.org/wiki?curid=296059", "title": "Deontological ethics", "text": "Deontological ethics\n\nIn moral philosophy, deontological ethics or deontology (from Greek δέον, \"deon\", \"obligation, duty\")\nis the normative ethical theory that the morality of an action should be based on whether that action itself is right or wrong under a series of rules, rather than based on the consequences of the action.\n\nIt is sometimes described as \"duty-\" or \"obligation-\" or \"rule-\" based ethics, because rules \"bind one to one's duty\". Deontological ethics is commonly contrasted to consequentialism, virtue ethics, and pragmatic ethics. In this terminology, action is more important than the consequences.\n\nIt is an ethical framework that depends on the predefined sets of rules and policies for the proper functioning of a system in the environment. The deontology is simply based on the checklist which includes certain rules to be followed while performing a particular task. According to this framework, the work is considered virtuous only if this checklist is completed.\nThis procedure is very simple to implement and understand. Minimum time is consumed to decide between right and wrong However, its simplicity ignores the consequences of the decision taken under this approach.\n\nThe term \"deontological\" was first used to describe the current, specialised definition by C. D. Broad in his book, \"Five Types of Ethical Theory\", which was published in 1930. Older usage of the term goes back to Jeremy Bentham, who coined it before 1816 as a synonym of \"Dicastic\" or \"Censorial Ethics\" (i.e. ethics based on judgement).\nThe more general sense of the word is retained in French, especially in the term \"code de déontologie\" (ethical code), in the context of professional ethics.\n\nDepending on the system of deontological ethics under consideration, a moral obligation may arise from an external or internal source, such as a set of rules inherent to the universe (ethical naturalism), religious law, or a set of personal or cultural values (any of which may be in conflict with personal desires).\nThere are numerous formulations of deontological ethics.\n\nImmanuel Kant's theory of ethics is considered deontological for several different reasons. First, Kant argues that to act in the morally right way, people must act from duty (\"Pflicht\"). Second, Kant argued that it was not the consequences of actions that make them right or wrong but the motives of the person who carries out the action.\n\nKant's argument that to act in the morally right way one must act purely from duty begins with an argument that the highest good must be both good in itself and good without qualification. Something is \"good in itself\" when it is intrinsically good, and \"good without qualification\", when the addition of that thing never makes a situation ethically worse. Kant then argues that those things that are usually thought to be good, such as intelligence, perseverance and pleasure, fail to be either intrinsically good or good without qualification. Pleasure, for example, appears not to be good without qualification, because when people take pleasure in watching someone suffer, this seems to make the situation ethically worse. He concludes that there is only one thing that is truly good:\nKant then argues that the consequences of an act of willing cannot be used to determine that the person has a good will; good consequences could arise by accident from an action that was motivated by a desire to cause harm to an innocent person, and bad consequences could arise from an action that was well-motivated. Instead, he claims, a person has a good will when he 'acts out of respect for the moral law'. People 'act out of respect for the moral law' when they act in some way \"because\" they have a duty to do so. So, the only thing that is truly good in itself is a good will, and a good will is only good when the willer chooses to do something because it is that person's duty, i.e. out of \"respect\" for the law. He defines respect as \"the concept of a worth which thwarts my self-love\".\n\nKant's three significant formulations of the categorical imperative are:\n\nKant argued that the only absolutely good thing is a good will, and so the single determining factor of whether an action is morally right is the will, or motive of the person doing it. If they are acting on a bad maxim, e.g. \"I will lie\", then their action is wrong, even if some good consequences come of it.\nIn his essay, \"On a Supposed Right to Lie Because of Philanthropic Concerns\", arguing against the position of \nBenjamin Constant, \"Des réactions politiques\", Kant states that \"Hence a lie defined merely as an intentionally untruthful declaration to another man does not require the additional condition that it must do harm to another, as jurists require in their definition (\"mendacium est falsiloquium in praeiudicium alterius\"). For a lie always harms another; if not some human being, then it nevertheless does harm to humanity in general, inasmuch as it vitiates the very source of right [\"Rechtsquelle\"] ... All practical principles of right must contain rigorous truth ... This is because such exceptions would destroy the universality on account of which alone they bear the name of principles.\"\n\nAlthough not all deontologists are religious, some believe in the 'divine command theory', which is actually a cluster of related theories which essentially state that an action is right if God has decreed that it is right. According to Ralph Cudworth, an English philosopher, William of Ockham, René Descartes, and eighteenth-century Calvinists all accepted various versions of this moral theory, as they all held that moral obligations arise from God's commands. The Divine Command Theory is a form of deontology because, according to it, the rightness of any action depends upon that action being performed because it is a duty, not because of any good consequences arising from that action. If God commands people not to work on Sabbath, then people act rightly if they do not work on Sabbath \"because God has commanded that they do not do so\". If they do not work on Sabbath because they are lazy, then their action is not truly speaking \"right\", even though the actual physical action performed is the same. If God commands not to covet a neighbour's goods, this theory holds that it would be immoral to do so, even if coveting provides the beneficial outcome of a drive to succeed or do well.\n\nOne thing that clearly distinguishes Kantian deontologism from divine command deontology is that Kantianism maintains that man, as a rational being, makes the moral law universal, whereas divine command maintains that God makes the moral law universal.\n\n\nContemporary deontologists (scholars born in the first half of the 20th century) include Józef Maria Bocheński, Thomas Nagel, Thomas Scanlon, and Roger Scruton.\n\nBocheński (1965) makes a distinction between deontic and epistemic authority. \nA typical example of epistemic authority in Bocheński's usage would be \"the relation of a teacher to his students\" or \"the relation between an employer and his employee\". A teacher has epistemic authority when making declarative sentences that the student presumes is reliable knowledge and appropriate but feels no obligation to accept or obey; in contrast, an employer has deontic authority in the act of issuing an order that the employee is obliged to accept and obey regardless of its reliability or appropriateness.\n\nFrances Kamm's \"Principle of Permissible Harm\" (1996) is an effort to derive a deontological constraint which coheres with our considered case judgments while also relying heavily on Kant's categorical imperative. The Principle states that one may harm in order to save more if and only if the harm is an effect or an aspect of the greater good itself. This principle is meant to address what Kamm feels are most people's considered case judgments, many of which involve deontological intuitions. \nFor instance, Kamm argues that we believe it would be impermissible to kill one person to harvest his organs in order to save the lives of five others. Yet, we think it is morally permissible to divert a runaway trolley that would otherwise kill five innocent and immobile people onto a side track where one innocent and immobile person will be killed. \nKamm believes the Principle of Permissible Harm explains the moral difference between these and other cases, and more importantly expresses a constraint telling us exactly when we may not act to bring about good ends—such as in the organ harvesting case. \nIn 2007, Kamm published a book that presents new theory that incorporates aspects of her \"Principle of Permissible Harm\", the \"Doctrine of Productive Purity\". Like the \"Principle of Permissible Harm\", the \"Doctrine of Productive Purity\" is an attempt to provide a deontological prescription for determining the circumstances in which people are permitted to act in a way that harms others.\n\nAttempts have been made to reconcile deontology with virtue-based ethics and consequentialism. \nIain King's 2008 book \"How to Make Good Decisions and Be Right All the Time\" uses quasi-realism and a modified form of utilitarianism to develop deontological principles which are compatible with ethics based on virtues and consequences. King develops a hierarchy of principles to link his meta-ethics, which are more inclined towards consequentialism, with the deontological conclusions he presents in his book.\n\n\n\n"}
{"id": "201154", "url": "https://en.wikipedia.org/wiki?curid=201154", "title": "Divide and conquer algorithm", "text": "Divide and conquer algorithm\n\nIn computer science, divide and conquer is an algorithm design paradigm based on multi-branched recursion. A divide and conquer algorithm works by recursively breaking down a problem into two or more sub-problems of the same or related type, until these become simple enough to be solved directly. The solutions to the sub-problems are then combined to give a solution to the original problem.\n\nThis divide and conquer technique is the basis of efficient algorithms for all kinds of problems, such as sorting (e.g., quicksort, merge sort), multiplying large numbers (e.g. the Karatsuba algorithm), finding the closest pair of points, syntactic analysis (e.g., top-down parsers), and computing the discrete Fourier transform (FFTs) .\n\nUnderstanding and designing divide and conquer algorithms is a complex skill that requires a good understanding of the nature of the underlying problem to be solved. As when proving a theorem by induction, it is often necessary to replace the original problem with a more general or complicated problem in order to initialize the recursion, and there is no systematic method for finding the proper generalization. These divide and conquer complications are seen when optimizing the calculation of a Fibonacci number with efficient double recursion .\n\nThe correctness of a divide and conquer algorithm is usually proved by mathematical induction, and its computational cost is often determined by solving recurrence relations.\n\nThe divide and conquer paradigm is often used to find the optimal solution of a problem. Its basic idea is to decompose a given problem into two or more similar, but simpler, subproblems, to solve them in turn, and to compose their solutions to solve the given problem. Problems of sufficient simplicity are solved directly. \nFor example, to sort a given list of \"n\" natural numbers, split it into two lists of about \"n\"/2 numbers each, sort each of them in turn, and interleave both results appropriately to obtain the sorted version of the given list (cf. picture). This approach is known as the merge sort algorithm.\n\nThe name \"divide and conquer\" is sometimes applied to algorithms that reduce each problem to only one sub-problem, such as the binary search algorithm for finding a record in a sorted list (or its analog in numerical computing, the bisection algorithm for root finding). These algorithms can be implemented more efficiently than general divide-and-conquer algorithms; in particular, if they use tail recursion, they can be converted into simple loops. Under this broad definition, however, every algorithm that uses recursion or loops could be regarded as a \"divide and conquer algorithm\". Therefore, some authors consider that the name \"divide and conquer\" should be used only when each problem may generate two or more subproblems. The name decrease and conquer has been proposed instead for the single-subproblem class.\n\nAn important application of divide and conquer is in optimization, where if the search space is reduced (\"pruned\") by a constant factor at each step, the overall algorithm has the same asymptotic complexity as the pruning step, with the constant depending on the pruning factor (by summing the geometric series); this is known as prune and search.\n\nEarly examples of these algorithms are primarily decrease and conquer – the original problem is successively broken down into \"single\" subproblems, and indeed can be solved iteratively.\n\nBinary search, a decrease and conquer algorithm where the subproblems are of roughly half the original size, has a long history. While a clear description of the algorithm on computers appeared in 1946 in an article by John Mauchly, the idea of using a sorted list of items to facilitate searching dates back at least as far as Babylonia in 200 BC. Another ancient decrease and conquer algorithm is the Euclidean algorithm to compute the greatest common divisor of two numbers by reducing the numbers to smaller and smaller equivalent subproblems, which dates to several centuries BC.\n\nAn early example of a divide-and-conquer algorithm with multiple subproblems is Gauss's 1805 description of what is now called the Cooley–Tukey fast Fourier transform (FFT) algorithm, although he did not analyze its operation count quantitatively and FFTs did not become widespread until they were rediscovered over a century later.\n\nAn early two-subproblem D&C algorithm that was specifically developed for computers and properly analyzed is the merge sort algorithm, invented by John von Neumann in 1945.\n\nAnother notable example is the algorithm invented by Anatolii A. Karatsuba in 1960 that could multiply two \"n\"-digit numbers in formula_1 operations (in Big O notation). This algorithm disproved Andrey Kolmogorov's 1956 conjecture that formula_2 operations would be required for that task.\n\nAs another example of a divide and conquer algorithm that did not originally involve computers, Donald Knuth gives the method a post office typically uses to route mail: letters are sorted into separate bags for different geographical areas, each of these bags is itself sorted into batches for smaller sub-regions, and so on until they are delivered. This is related to a radix sort, described for punch-card sorting machines as early as 1929.\n\nDivide and conquer is a powerful tool for solving conceptually difficult problems: all it requires is a way of breaking the problem into sub-problems, of solving the trivial cases and of combining sub-problems to the original problem. Similarly, decrease and conquer only requires reducing the problem to a single smaller problem, such as the classic Tower of Hanoi puzzle, which reduces moving a tower of height \"n\" to moving a tower of height \"n\" − 1.\n\nThe divide-and-conquer paradigm often helps in the discovery of efficient algorithms. It was the key, for example, to Karatsuba's fast multiplication method, the quicksort and mergesort algorithms, the Strassen algorithm for matrix multiplication, and fast Fourier transforms.\n\nIn all these examples, the D&C approach led to an improvement in the asymptotic cost of the solution.\nFor example, if (a) the base cases have constant-bounded size, the work of splitting the problem and combining the partial solutions is proportional to the problem's size \"n\", and (b) there is a bounded number \"p\" of subproblems of size ~ \"n\"/\"p\" at each stage, then the cost of the divide-and-conquer algorithm will be O(\"n\" log\"n\").\n\nDivide and conquer algorithms are naturally adapted for execution in multi-processor machines, especially shared-memory systems where the communication of data between processors does not need to be planned in advance, because distinct sub-problems can be executed on different processors.\n\nDivide-and-conquer algorithms naturally tend to make efficient use of memory caches. The reason is that once a sub-problem is small enough, it and all its sub-problems can, in principle, be solved within the cache, without accessing the slower main memory. An algorithm designed to exploit the cache in this way is called \"cache-oblivious\", because it does not contain the cache size as an explicit parameter.\nMoreover, D&C algorithms can be designed for important algorithms (e.g., sorting, FFTs, and matrix multiplication) to be \"optimal\" cache-oblivious algorithms–they use the cache in a probably optimal way, in an asymptotic sense, regardless of the cache size. In contrast, the traditional approach to exploiting the cache is \"blocking\", as in loop nest optimization, where the problem is explicitly divided into chunks of the appropriate size—this can also use the cache optimally, but only when the algorithm is tuned for the specific cache size(s) of a particular machine.\n\nThe same advantage exists with regards to other hierarchical storage systems, such as NUMA or virtual memory, as well as for multiple levels of cache: once a sub-problem is small enough, it can be solved within a given level of the hierarchy, without accessing the higher (slower) levels.\n\nIn computations with rounded arithmetic, e.g. with floating point numbers, a divide-and-conquer algorithm may yield more accurate results than a superficially equivalent iterative method. For example, one can add \"N\" numbers either by a simple loop that adds each datum to a single variable, or by a D&C algorithm called pairwise summation that breaks the data set into two halves, recursively computes the sum of each half, and then adds the two sums. While the second method performs the same number of additions as the first, and pays the overhead of the recursive calls, it is usually more accurate.\n\nDivide-and-conquer algorithms are naturally implemented as recursive procedures. In that case, the partial sub-problems leading to the one currently being solved are automatically stored in the procedure call stack. A recursive function is a function that calls itself within its definition.\n\nDivide and conquer algorithms can also be implemented by a non-recursive program that stores the partial sub-problems in some explicit data structure, such as a stack, queue, or priority queue. This approach allows more freedom in the choice of the sub-problem that is to be solved next, a feature that is important in some applications — e.g. in breadth-first recursion and the branch and bound method for function optimization. This approach is also the standard solution in programming languages that do not provide support for recursive procedures.\n\nIn recursive implementations of D&C algorithms, one must make sure that there is sufficient memory allocated for the recursion stack, otherwise the execution may fail because of stack overflow. D&C algorithms that are time-efficient often have relatively small recursion depth. For example, the quicksort algorithm can be implemented so that it never requires more than formula_3 nested recursive calls to sort formula_4 items.\n\nStack overflow may be difficult to avoid when using recursive procedures, since many compilers assume that the recursion stack is a contiguous area of memory, and some allocate a fixed amount of space for it. Compilers may also save more information in the recursion stack than is strictly necessary, such as return address, unchanging parameters, and the internal variables of the procedure. Thus, the risk of stack overflow can be reduced by minimizing the parameters and internal variables of the recursive procedure, or by using an explicit stack structure.\n\nIn any recursive algorithm, there is considerable freedom in the choice of the \"base cases\", the small subproblems that are solved directly in order to terminate the recursion.\n\nChoosing the smallest or simplest possible base cases is more elegant and usually leads to simpler programs, because there are fewer cases to consider and they are easier to solve. For example, an FFT algorithm could stop the recursion when the input is a single sample, and the quicksort list-sorting algorithm could stop when the input is the empty list; in both examples there is only one base case to consider, and it requires no processing.\n\nOn the other hand, efficiency often improves if the recursion is stopped at relatively large base cases, and these are solved non-recursively, resulting in a hybrid algorithm. This strategy avoids the overhead of recursive calls that do little or no work, and may also allow the use of specialized non-recursive algorithms that, for those base cases, are more efficient than explicit recursion. A general procedure for a simple hybrid recursive algorithm is \"short-circuiting the base case,\" also known as \"arm's-length recursion.\" In this case whether the next step will result in the base case is checked before the function call, avoiding an unnecessary function call. For example, in a tree, rather than recursing to a child node and then checking if it is null, checking null before recursing; this avoids half the function calls in some algorithms on binary trees. Since a D&C algorithm eventually reduces each problem or sub-problem instance to a large number of base instances, these often dominate the overall cost of the algorithm, especially when the splitting/joining overhead is low. Note that these considerations do not depend on whether recursion is implemented by the compiler or by an explicit stack.\n\nThus, for example, many library implementations of quicksort will switch to a simple loop-based insertion sort (or similar) algorithm once the number of items to be sorted is sufficiently small. Note that, if the empty list were the only base case, sorting a list with \"n\" entries would entail maximally \"n\" quicksort calls that would do nothing but return immediately. Increasing the base cases to lists of size 2 or less will eliminate most of those do-nothing calls, and more generally a base case larger than 2 is typically used to reduce the fraction of time spent in function-call overhead or stack manipulation.\n\nAlternatively, one can employ large base cases that still use a divide-and-conquer algorithm, but implement the algorithm for predetermined set of fixed sizes where the algorithm can be completely unrolled into code that has no recursion, loops, or conditionals (related to the technique of partial evaluation). For example, this approach is used in some efficient FFT implementations, where the base cases are unrolled implementations of divide-and-conquer FFT algorithms for a set of fixed sizes. Source code generation methods may be used to produce the large number of separate base cases desirable to implement this strategy efficiently.\n\nThe generalized version of this idea is known as recursion \"unrolling\" or \"coarsening\" and various techniques have been proposed for automating the procedure of enlarging the base case.\n\nFor some problems, the branched recursion may end up evaluating the same sub-problem many times over. In such cases it may be worth identifying and saving the solutions to these overlapping subproblems, a technique commonly known as memoization. Followed to the limit, it leads to bottom-up divide-and-conquer algorithms such as dynamic programming and chart parsing.\n\n"}
{"id": "41560174", "url": "https://en.wikipedia.org/wiki?curid=41560174", "title": "Electoral integrity", "text": "Electoral integrity\n\nElectoral integrity refers to international standards and global norms governing the appropriate conduct of elections.\n\nThese standards have been endorsed in a series of authoritative conventions, treaties, protocols, and guidelines by agencies of the international community, notably by the decisions of the UN General Assembly, by regional bodies such as the Organization for Security and Cooperation in Europe (OSCE), the Organization of American States (OAS), and the African Union (AU), and by member states in the United Nations. Following endorsement, these standards apply universally to all countries throughout the electoral cycle, including during the pre-electoral period, the campaign, on polling day, and in its aftermath.\n\nThe contrary notion of 'electoral malpractice' refers to contests violating international standards and global norms. Problems can arise at every stage of the process, from electoral and ballot access laws favoring incumbents to lack of a level playing field in money and media during campaigns to inaccurate voter registers, flawed counts and partial electoral management bodies.\n\nThere is nothing novel about problems of flawed or failed elections which suffer from fraud, corruption, or vote-rigging. Indeed, during the 18th and 19th Centuries, such practices were common in countries holding popular contests, including in rotten and pocket boroughs in Britain and machine politics in the United States. Concern about malpractices has grown in recent decades, however, along with the spread of elections to almost every state worldwide.\n\nContemporary campaigns attracting considerable international concern include allegations of irregularities occurring during the Russian presidential election, 2012, problems of violence during and after the Kenyan general election, 2007, and controversies in the Cambodian general election, 2013.\n\nThe foundation of these standards rests on Article 21(3) in the \"Universal Declaration of Human Rights\" (1948). This specifies that \"[t]he will of the people shall be the basis of the authority of government; this will shall be expressed in periodic and genuine elections which shall be by universal and equal suffrage and shall be held by secret vote or by equivalent free voting procedures.\"\n\nThese commitments were further developed in Article 25 of the UN International Covenant for Civil and Political Rights (ICCPR of 1966), namely the need for:\n\n\nThe \"1990 Copenhagen Document of the Conference on Security and Cooperation in Europe\" (CSCE) made commitments that included free elections at regular intervals; the popular election of all seats in at least one chamber; universal and equal suffrage; the right to establish political parties and their clear separation from the state; campaigning in a free and fair atmosphere; unimpeded access to media; secret ballots, with counting and reporting conducted honestly and the results reported publicly; and the due winners being installed and allowed to serve their full terms.\n\nThe \"2002 Venice Commission’s Code of Good Practice in Electoral Matters\" spells out in detail what is meant by principles such as the universal, equal, free, secret, and direct suffrage.\n\nSome of the most detailed standards are contained in the practical guidelines for electoral observers published by regional intergovernmental organizations, exemplified by the \"OSCE Election Observation Handbook\". Similar principles have been adopted in the guidelines developed by the African Union, European Union, and Organization of American States.\n\nThe most recent statement of these norms in the \"UN General Assembly resolution 63/163\" (April 12, 2012): “Strengthening the role of the United Nations in enhancing periodic and genuine elections and the promotion of democratization.” The language in this document reflects and extends a series of similar statements of principle endorsed regularly by the United Nations since 1991. Resolution 63/163 reaffirms that “democracy is a universal value based on the freely expressed will of the people to determine their own political, economic, social and cultural systems and their full participation in all aspects of their lives.” Thus, democratic principles are explicitly endorsed by the United Nations General Assembly, along with a commitment to “the importance of fair, periodic and genuine elections” as the primary mechanism that allows citizens “to express their will.”\n\nThis does not imply, however, that the United Nations or the international community endorse any specific institutional design or constitutional mechanisms that can best achieve global norms, leaving this as a matter for national sovereignty. The UN resolution recognizes the responsibility of member states, “for ensuring free and fair elections, free of intimidation, coercion and tampering of vote counts, and that all such acts are sanctioned accordingly.” The United Nations’ role (especially through the Electoral Assistance Division of the Department of Political Affairs and the United Nations Development Programme) is seen as one of providing electoral assistance and support for the promotion of democratization, but only at the specific request of the member state.\n\nAttempts to document evidence of violations of international standards of electoral integrity can be found in the electoral observer monitoring reports published after each election by regional organizations.\n\nComparative evidence is also available from the expert survey of Perceptions of Electoral Integrity conducted by the Electoral Integrity Project. This includes the report, part of an annual series, \"The year in Elections, 2013\" monitoring the quality of 73 presidential and parliamentary elections. In 2016, Arizona scored the worst in the U.S., with a 53, and Vermont the best, with a 75, on a scale of 100, the Arizona Republic reported. \"Slate\" reported that a score of 58 is around the same as Cuba. The WSJ remarked that with a score of 56, Cuba \"jails political dissidents, hasn’t transferred power since 1959, unless the 2008 presidential handoff to Raúl Castro from Fidel Castro counts.\"\n\n\n"}
{"id": "11286213", "url": "https://en.wikipedia.org/wiki?curid=11286213", "title": "Entitativity", "text": "Entitativity\n\nEntitativity means the consideration of something as pure entity, i.e., the mental abstraction from attendant circumstances.\n\nIn psychology, it typically refers to \"the perception of a group as pure entity\" (an entitative group), abstracted from its attendant individuals. It is different from holistic perception. Operationally, entitativity can also be defined as \"perceiving a collection of social targets (e.g., individuals) as possessing unity and coherence (e.g., a group)\". Entitativity is highest for intimacy groups, such as the family, lower for task groups, lower yet for social categories (e.g., people of the same religion), and lowest for transitory groups, such as people waiting at the same bus stop (Lickel et al., 2000).\n\nCampbell (1958) coined the term \"entitativity\" in order to explain why some groups are considered real groups while others are thought to be mere aggregates of individuals. He suggested that people rely on certain perceptual cues as they intuitively determine which aggregations of individuals are groups, and which are not (e.g. Spectators at a football game may seem like a disorganized collection of people, but when they shout the same cheers or express similar emotions, this gives them entitativity)(Forsyth, 2010). \n\nAdditionally, Campbell (1958) emphasized three cues that individuals can use to make judgments regarding entitativity: \"common fate\" (the extent to which individuals in the aggregate seem to experience interrelated outcomes), \"similarity\" (the extent to which the individuals display the same behaviors or resemble one another), and \"proximity\" (the distance between individuals in the aggregate). To illustrate how we make those judgments, consider the example of people sharing a table at a library. They could be friends who are studying together, or they may also be strangers happening to share the same table. If you're wondering whether this is an actual group, you would examine their common fate, similarity, and proximity. Common fate may be something like the group all getting up and leaving together while talking or laughing amongst themselves. Similarity could be as simple as noticing that they are all using the same textbooks or notes, or that they happen to be wearing the same t-shirts to organizations (i.e., fraternity, university group). Finally, their physical proximity to one another (i.e., moving to sit closer) would be the final characteristic to judge that you are witnessing individuals with entitativity (Forsyth, 2010). \n\nThere are two proposed antecedents for the entitativity perception (Ip, Chiu, & Wan, 2006):\n\n\n"}
{"id": "37293348", "url": "https://en.wikipedia.org/wiki?curid=37293348", "title": "Equinet", "text": "Equinet\n\nEquinet is the European Network of Equality Bodies. It serves as a professional platform for cooperation, capacity building and peer support amongst equality bodies around the legal interpretation and implementation in practice of the EU equal treatment Directives and around the promotion of equality and the elimination of discrimination.\n\nEquinet currently consists of 46 Equality Bodies from 34 different European countries. Members of Equinet have been established on the basis of the EU equal treatment Directives and they are empowered to counteract discrimination across the range of grounds including age, disability, gender, race or ethnic origin, religion or belief, and sexual orientation.\n\nEquinet builds upon the two-year project \"Strengthening the co-operation between specialised bodies for the implementation of equal treatment legislation\" (2002-2004). The initiative to create a network of equality bodies was taken by the Migration Policy Group, who also acted as Equinet's Secretariat until 2007. Equinet was established as an independent structure in 2007 with the creation of the Equinet Secretariat in Brussels and its registration as a not-for-profit international association (AISBL) under Belgian law.\n\nEquinet is the European network of equality bodies. The network promotes equality in Europe through supporting and enabling the work of national equality bodies. It supports equality bodies to be independent and effective as valuable catalysts for more equal societies. Apart from being a platform for exchange among member equality bodies, Equinet contributes to the wider European debate on matters of equal-treatment and non-discrimination by sharing the experience of equality bodies through so-called perspectives aimed at equality experts and policy-makers.\n\nEquinet attempts to strengthen the voice and contributions of national equality bodies to the wider European equality debate. Equinet's input in advancing equality is achieved by:\n\nEquinet is an umbrella organisation for European equality bodies and has no mandate to provide any kind of legal assistance to individual victims of discrimination. The organisation however provides contact details (see next section) for equality bodies based in all EU members states and beyond. EU anti-discrimination legislation provides that each Member State shall have (at least) one such equality body with the power to, among other, give independent assistance to victims of discrimination. The equality bodies are specialised authorities whose staff are trained and experienced to handle cases of discrimination.\n\nMost Equinet members belong to the European Union, while six equality bodies come from outside the European Union (Albania, Bosnia and Herzegovina, Macedonia, Montenegro, Norway and Serbia).\n\nThe main decisions concerning the general direction of Equinet are taken by the General Assembly of Members. This assembly is made up of all the members of the network and is convened at least once a year for an annual general meeting (AGM). The General Assembly has the power to approve new members and, following a nomination process by members, to vote for representatives on the Executive Board etc.\n\nManagement and administration of the network is delegated to the Executive Board. The Board is also responsible for the preparation and implementation of the AGM’s decisions. Executive Board Members, the advisor to the Board and the treasurer receive no salary for their input.\n\nWorking groups are composed of staff from member organisations and led by a moderator. Working groups are the main medium for the sharing of expertise between different equality bodies. There were four Equinet Working Groups in 2015:\n\nThe Secretariat reports to the Executive Board and implements the annual work plan of the organisation. It is responsible for the daily activities of the network and assists individual members with their requests. It assists and coordinates the work of the Working Groups. There are 6 employees working at the Equinet secretariat.\n\nEquinet has two key sources of income: \nProgramme of the European Union\" \n"}
{"id": "22222481", "url": "https://en.wikipedia.org/wiki?curid=22222481", "title": "Euler's laws of motion", "text": "Euler's laws of motion\n\nIn classical mechanics, Euler's laws of motion are equations of motion which extend Newton's laws of motion for point particle to rigid body motion. They were formulated by Leonhard Euler about 50 years after Isaac Newton formulated his laws.\n\nEuler's first law states that the linear momentum of a body, (also denoted ) is equal to the product of the mass of the body and the velocity of its center of mass :\n\nInternal forces between the particles that make up a body do not contribute to changing the total momentum of the body as there is an equal and opposite force resulting in no net effect. The law is also stated as:\n\nwhere is the acceleration of the centre of mass and is the total applied force on the body. This is just the time derivative of the previous equation ( is a constant).\n\nEuler's second law states that the rate of change of angular momentum (sometimes denoted ) about a point that is fixed in an inertial reference frame (often the mass center of the body), is equal to the sum of the external moments of force (torques) acting on that body (also denoted or ) about that point: \n\nNote that the above formula holds only if both and are computed with respect to a fixed inertial frame or a frame parallel to the inertial frame but fixed on the center of mass. \nFor rigid bodies translating and rotating in only 2D, this can be expressed as:\nwhere is the position vector of the center of mass with respect to the point about which moments are summed, is the angular acceleration of the body about its center of mass, and is the moment of inertia of the body about its center of mass.\nSee also Euler's equations (rigid body dynamics).\n\nThe distribution of internal forces in a deformable body are not necessarily equal throughout, i.e. the stresses vary from one point to the next. This variation of internal forces throughout the body is governed by Newton's second law of motion of conservation of linear momentum and angular momentum, which for their simplest use are applied to a mass particle but are extended in continuum mechanics to a body of continuously distributed mass. For continuous bodies these laws are called Euler’s laws of motion. If a body is represented as an assemblage of discrete particles, each governed by Newton’s laws of motion, then Euler’s equations can be derived from Newton’s laws. Euler’s equations can, however, be taken as axioms describing the laws of motion for extended bodies, independently of any particle distribution.\n\nThe total body force applied to a continuous body with mass , mass density , and volume , is the volume integral integrated over the volume of the body:\n\nwhere is the force acting on the body per unit mass (dimensions of acceleration, misleadingly called the \"body force\"), and is an infinitesimal mass element of the body.\n\nBody forces and contact forces acting on the body lead to corresponding moments (torques) of those forces relative to a given point. Thus, the total applied torque about the origin is given by\n\nwhere and respectively indicate the moments caused by the body and contact forces.\n\nThus, the sum of all applied forces and torques (with respect to the origin of the coordinate system) acting on the body can be given as the sum of a volume and surface integral:\n\nwhere is called the surface traction, integrated over the surface of the body, in turn denotes a unit vector normal and directed outwards to the surface .\n\nLet the coordinate system be an inertial frame of reference, be the position vector of a point particle in the continuous body with respect to the origin of the coordinate system, and be the velocity vector of that point.\n\nEuler’s first axiom or law (law of balance of linear momentum or balance of forces) states that in an inertial frame the time rate of change of linear momentum of an arbitrary portion of a continuous body is equal to the total applied force acting on that portion, and it is expressed as\n\nEuler’s second axiom or law (law of balance of angular momentum or balance of torques) states that in an inertial frame the time rate of change of angular momentum of an arbitrary portion of a continuous body is equal to the total applied torque acting on that portion, and it is expressed as\n\nWhere formula_11 is the velocity, formula_12 the volume, and the derivatives of and are material derivatives.\n\n"}
{"id": "51414", "url": "https://en.wikipedia.org/wiki?curid=51414", "title": "Fundamental theorem of algebra", "text": "Fundamental theorem of algebra\n\nThe fundamental theorem of algebra states that every non-constant single-variable polynomial with complex coefficients has at least one complex root. This includes polynomials with real coefficients, since every real number is a complex number with an imaginary part equal to zero.\n\nEquivalently (by definition), the theorem states that the field of complex numbers is algebraically closed.\n\nThe theorem is also stated as follows: every non-zero, single-variable, degree \"n\" polynomial with complex coefficients has, counted with multiplicity, exactly \"n\" complex roots. The equivalence of the two statements can be proven through the use of successive polynomial division.\n\nIn spite of its name, there is no purely algebraic proof of the theorem, since any proof must use some form of completeness, which is not an algebraic concept. Additionally, it is not fundamental for modern algebra; its name was given at a time when algebra was synonymous with theory of equations.\n\nPeter Roth, in his book \"Arithmetica Philosophica\" (published in 1608, at Nürnberg, by Johann Lantzenberger), wrote that a polynomial equation of degree \"n\" (with real coefficients) \"may\" have \"n\" solutions. Albert Girard, in his book \"L'invention nouvelle en l'Algèbre\" (published in 1629), asserted that a polynomial equation of degree \"n\" has \"n\" solutions, but he did not state that they had to be real numbers. Furthermore, he added that his assertion holds \"unless the equation is incomplete\", by which he meant that no coefficient is equal to 0. However, when he explains in detail what he means, it is clear that he actually believes that his assertion is always true; for instance, he shows that the equation formula_1 although incomplete, has four solutions (counting multiplicities): 1 (twice), formula_2 and formula_3\n\nAs will be mentioned again below, it follows from the fundamental theorem of algebra that every non-constant polynomial with real coefficients can be written as a product of polynomials with real coefficients whose degree are either 1 or 2. However, in 1702 Leibniz said that no polynomial of the type \"x\" + \"a\" (with \"a\" real and distinct from 0) can be written in such a way. Later, Nikolaus Bernoulli made the same assertion concerning the polynomial \"x\" − 4\"x\" + 2\"x\" + 4\"x\" + 4, but he got a letter from Euler in 1742 in which he was told that his polynomial happened to be equal to\n\nAlso, Euler mentioned that\n\nA first attempt at proving the theorem was made by d'Alembert in 1746, but his proof was incomplete. Among other problems, it assumed implicitly a theorem (now known as Puiseux's theorem) which would not be proved until more than a century later, and furthermore the proof assumed the fundamental theorem of algebra. Other attempts were made by Euler (1749), de Foncenex (1759), Lagrange (1772), and Laplace (1795). These last four attempts assumed implicitly Girard's assertion; to be more precise, the existence of solutions was assumed and all that remained to be proved was that their form was \"a\" + \"bi\" for some real numbers \"a\" and \"b\". In modern terms, Euler, de Foncenex, Lagrange, and Laplace were assuming the existence of a splitting field of the polynomial \"p\"(\"z\").\n\nAt the end of the 18th century, two new proofs were published which did not assume the existence of roots, but neither of which was complete. One of them, due to James Wood and mainly algebraic, was published in 1798 and it was totally ignored. Wood's proof had an algebraic gap. The other one was published by Gauss in 1799 and it was mainly geometric, but it had a topological gap, filled by Alexander Ostrowski in 1920, as discussed in Smale 1981 (Smale writes, \"...I wish to point out what an immense gap Gauss' proof contained. It is a subtle point even today that a real algebraic plane curve cannot enter a disk without leaving. In fact even though Gauss redid this proof 50 years later, the gap remained. It was not until 1920 that Gauss' proof was completed. In the reference Gauss, A. Ostrowski has a paper which does this and gives an excellent discussion of the problem as well...\"). A rigorous proof was first published by Argand in 1806 (and revisited in 1813); it was here that, for the first time, the fundamental theorem of algebra was stated for polynomials with complex coefficients, rather than just real coefficients. Gauss produced two other proofs in 1816 and another version of his original proof in 1849.\n\nThe first textbook containing a proof of the theorem was Cauchy's \"Cours d'analyse de l'École Royale Polytechnique\" (1821). It contained Argand's proof, although Argand is not credited for it.\n\nNone of the proofs mentioned so far is constructive. It was Weierstrass who raised for the first time, in the middle of the 19th century, the problem of finding a constructive proof of the fundamental theorem of algebra. He presented his solution, that amounts in modern terms to a combination of the Durand–Kerner method with the homotopy continuation principle, in 1891. Another proof of this kind was obtained by Hellmuth Kneser in 1940 and simplified by his son Martin Kneser in 1981.\n\nWithout using countable choice, it is not possible to constructively prove the fundamental theorem of algebra for complex numbers based on the Dedekind real numbers (which are not constructively equivalent to the Cauchy real numbers without countable choice). However, Fred Richman proved a reformulated version of the theorem that does work.\n\nAll proofs below involve some analysis, or at least the topological concept of continuity of real or complex functions. Some also use differentiable or even analytic functions. This fact has led to the remark that the Fundamental Theorem of Algebra is neither fundamental, nor a theorem of algebra.\n\nSome proofs of the theorem only prove that any non-constant polynomial with real coefficients has some complex root. This is enough to establish the theorem in the general case because, given a non-constant polynomial \"p\"(\"z\") with complex coefficients, the polynomial\n\nhas only real coefficients and, if \"z\" is a zero of \"q\"(\"z\"), then either \"z\" or its conjugate is a root of \"p\"(\"z\").\n\nA large number of non-algebraic proofs of the theorem use the fact (sometimes called \"growth lemma\") that an \"n\"-th degree polynomial function \"p\"(\"z\") whose dominant coefficient is 1 behaves like \"z\" when |\"z\"| is large enough. A more precise statement is: there is some positive real number \"R\" such that:\n\nwhen |\"z\"| > \"R\".\n\nFind a closed disk \"D\" of radius \"r\" centered at the origin such that |\"p\"(\"z\")| > |\"p\"(0)| whenever |\"z\"| ≥ \"r\". The minimum of |\"p\"(\"z\")| on \"D\", which must exist since \"D\" is compact, is therefore achieved at some point \"z\" in the interior of \"D\", but not at any point of its boundary. The Maximum modulus principle (applied to 1/\"p\"(\"z\")) implies then that \"p\"(\"z\") = 0. In other words, \"z\" is a zero of \"p\"(\"z\").\n\nA variation of this proof does not require the use of the maximum modulus principle (in fact, the same argument with minor changes also gives a proof of the maximum modulus principle for holomorphic functions). If we assume by contradiction that \"a\" := \"p\"(\"z\") ≠ 0, then, expanding \"p\"(\"z\") in powers of \"z\" − \"z\" we can write\n\nHere, the \"c\" are simply the coefficients of the polynomial \"z\" → \"p\"(\"z\" + \"z\"), and we let \"k\" be the index of the first coefficient following the constant term that is non-zero. But now we see that for \"z\" sufficiently close to \"z\" this has behavior asymptotically similar to the simpler polynomial formula_9, in the sense that (as is easy to check) the function:\n\nis bounded by some positive constant \"M\" in some neighborhood of \"z\". Therefore if we define formula_11 and let formula_12, then for any sufficiently small positive number \"r\" (so that the bound \"M\" mentioned above holds), using the triangle inequality we see that\n\nWhen \"r\" is sufficiently close to 0 this upper bound for |\"p\"(\"z\")| is strictly smaller than |\"a\"|, in contradiction to the definition of \"z\". (Geometrically, we have found an explicit direction θ such that if one approaches \"z\" from that direction one can obtain values \"p\"(\"z\") smaller in absolute value than |\"p\"(\"z\")|.)\n\nAnother analytic proof can be obtained along this line of thought observing that, since |\"p\"(\"z\")| > |\"p\"(0)| outside \"D\", the minimum of |\"p\"(\"z\")| on the whole complex plane is achieved at \"z\". If |\"p\"(\"z\")| > 0, then 1/\"p\" is a bounded holomorphic function in the entire complex plane since, for each complex number \"z\", |1/\"p\"(\"z\")| ≤ |1/\"p\"(\"z\")|. Applying Liouville's theorem, which states that a bounded entire function must be constant, this would imply that 1/\"p\" is constant and therefore that \"p\" is constant. This gives a contradiction, and hence \"p\"(\"z\") = 0.\n\nYet another analytic proof uses the argument principle. Let \"R\" be a positive real number large enough so that every root of \"p\"(\"z\") has absolute value smaller than \"R\"; such a number must exist because every non-constant polynomial function of degree \"n\" has at most \"n\" zeros. For each \"r\" > \"R\", consider the number\n\nwhere \"c\"(\"r\") is the circle centered at 0 with radius \"r\" oriented counterclockwise; then the argument principle says that this number is the number \"N\" of zeros of \"p\"(\"z\") in the open ball centered at 0 with radius \"r\", which, since \"r\" > \"R\", is the total number of zeros of \"p\"(\"z\"). On the other hand, the integral of \"n\"/\"z\" along \"c\"(\"r\") divided by 2π\"i\" is equal to \"n\". But the difference between the two numbers is\n\nThe numerator of the rational expression being integrated has degree at most \"n\" - 1 and the degree of the denominator is \"n\" + 1. Therefore, the number above tends to 0 as \"r\" → +∞. But the number is also equal to \"N\" − \"n\" and so \"N\" = \"n\".\n\nStill another complex-analytic proof can be given by combining linear algebra with the Cauchy theorem. To establish that every complex polynomial of degree \"n\" > 0 has a zero, it suffices to show that every complex square matrix of size \"n\" > 0 has a (complex) eigenvalue. The proof of the latter statement is by contradiction.\n\nLet \"A\" be a complex square matrix of size \"n\" > 0 and let \"I\" be the unit matrix of the same size. Assume \"A\" has no eigenvalues. Consider the resolvent function\n\nwhich is a meromorphic function on the complex plane with values in the vector space of matrices. The eigenvalues of \"A\" are precisely the poles of \"R\"(\"z\"). Since, by assumption, \"A\" has no eigenvalues, the function \"R\"(\"z\") is an entire function and Cauchy theorem implies that\n\nOn the other hand, \"R\"(\"z\") expanded as a geometric series gives:\n\nThis formula is valid outside the closed disc of radius formula_19 (the operator norm of \"A\"). Let formula_20 Then\n\n(in which only the summand \"k\" = 0 has a nonzero integral). This is a contradiction, and so \"A\" has an eigenvalue.\n\nFinally, Rouché's theorem gives perhaps the shortest proof of the theorem.\n\nSuppose the minimum of |\"p\"(\"z\")| on the whole complex plane is achieved at \"z\"; it was seen at the proof which uses Liouville's theorem that such a number must exist. We can write \"p\"(\"z\") as a polynomial in \"z\" − \"z\": there is some natural number \"k\" and there are some complex numbers \"c\", \"c\", ..., \"c\" such that \"c\" ≠ 0 and:\n\nIf \"p\"(\"z\") is nonzero, it follows that if \"a\" is a \"k\" root of −\"p\"(\"z\")/\"c\" and if \"t\" is positive and sufficiently small, then |\"p\"(\"z\" + \"ta\")| < |\"p\"(\"z\")|, which is impossible, since |\"p\"(\"z\")| is the minimum of |\"p\"| on \"D\".\n\nFor another topological proof by contradiction, suppose that the polynomial \"p\"(\"z\") has no roots, and consequently is never equal to 0. Think of the polynomial as a map from the complex plane into the complex plane. It maps any circle |\"z\"| = \"R\" into a closed loop, a curve \"P\"(\"R\"). We will consider what happens to the winding number of \"P\"(\"R\") at the extremes when \"R\" is very large and when \"R\" = 0. When \"R\" is a sufficiently large number, then the leading term \"z\" of \"p\"(\"z\") dominates all other terms combined; in other words, \n\nWhen \"z\" traverses the circle formula_24 once counter-clockwise formula_25 then formula_26 winds \"n\" times counter-clockwise formula_27 around the origin (0,0), and \"P\"(\"R\") likewise. At the other extreme, with |\"z\"| = 0, the curve \"P\"(0) is merely the single point \"p\"(0), which must be nonzero because \"p\"(\"z\") is never zero. Thus \"p\"(0) must be distinct from the origin (0,0), which denotes 0 in the complex plane. The winding number of \"P\"(0) around the origin (0,0) is thus 0. Now changing \"R\" continuously will deform the loop continuously. At some \"R\" the winding number must change. But that can only happen if the curve \"P\"(\"R\") includes the origin (0,0) for some \"R\". But then for some \"z\" on that circle |\"z\"| = \"R\" we have \"p\"(\"z\") = 0, contradicting our original assumption. Therefore, \"p\"(\"z\") has at least one zero.\n\nThese proofs use two facts about real numbers that require only a small amount of analysis (more precisely, the intermediate value theorem):\n\nThe second fact, together with the quadratic formula, implies the theorem for real quadratic polynomials. In other words, algebraic proofs of the fundamental theorem actually show that if \"R\" is any real-closed field, then its extension \"C\" = \"R\"() is algebraically closed.\n\nAs mentioned above, it suffices to check the statement \"every non-constant polynomial \"p\"(\"z\") with real coefficients has a complex root\". This statement can be proved by induction on the greatest non-negative integer \"k\" such that 2 divides the degree \"n\" of \"p\"(\"z\"). Let \"a\" be the coefficient of \"z\" in \"p\"(\"z\") and let \"F\" be a splitting field of \"p\"(\"z\") over \"C\"; in other words, the field \"F\" contains \"C\" and there are elements \"z\", \"z\", ..., \"z\" in \"F\" such that\n\nIf \"k\" = 0, then \"n\" is odd, and therefore \"p\"(\"z\") has a real root. Now, suppose that \"n\" = 2\"m\" (with \"m\" odd and \"k\" > 0) and that the theorem is already proved when the degree of the polynomial has the form 2\"m\"′ with \"m\"′ odd. For a real number \"t\", define:\n\nThen the coefficients of \"q\"(\"z\") are symmetric polynomials in the \"z\" with real coefficients. Therefore, they can be expressed as polynomials with real coefficients in the elementary symmetric polynomials, that is, in −\"a\", \"a\", ..., (−1)\"a\". So \"q\"(\"z\") has in fact \"real\" coefficients. Furthermore, the degree of \"q\"(\"z\") is \"n\"(\"n\" − 1)/2 = 2\"m\"(\"n\" − 1), and \"m\"(\"n\" − 1) is an odd number. So, using the induction hypothesis, \"q\" has at least one complex root; in other words, \"z\" + \"z\" + \"tzz\" is complex for two distinct elements \"i\" and \"j\" from {1, ..., \"n\"}. Since there are more real numbers than pairs (\"i\", \"j\"), one can find distinct real numbers \"t\" and \"s\" such that \"z\" + \"z\" + \"tzz\" and \"z\" + \"z\" + \"szz\" are complex (for the same \"i\" and \"j\"). So, both \"z\" + \"z\" and \"zz\" are complex numbers. It is easy to check that every complex number has a complex square root, thus every complex polynomial of degree 2 has a complex root by the quadratic formula. It follows that \"z\" and \"z\" are complex numbers, since they are roots of the quadratic polynomial \"z\" −  (\"z\" + \"z\")\"z\" + \"zz\".\n\nJoseph Shipman showed in 2007 that the assumption that odd degree polynomials have roots is stronger than necessary; any field in which polynomials of prime degree have roots is algebraically closed (so \"odd\" can be replaced by \"odd prime\" and furthermore this holds for fields of all characteristics). For axiomatization of algebraically closed fields, this is the best possible, as there are counterexamples if a single prime is excluded. However, these counterexamples rely on −1 having a square root. If we take a field where −1 has no square root, and every polynomial of degree \"n\" ∈ \"I\" has a root, where \"I\" is any fixed infinite set of odd numbers, then every polynomial \"f\"(\"x\") of odd degree has a root (since has a root, where \"k\" is chosen so that ). Mohsen Aliabadi generalized Shipman's result for any field in 2013, proving that the sufficient condition for an arbitrary field (of any characteristic) to be algebraically closed is having a root for any polynomial of prime degree.\n\nAnother algebraic proof of the fundamental theorem can be given using Galois theory. It suffices to show that C has no proper finite field extension. Let \"K\"/C be a finite extension. Since the normal closure of \"K\" over R still has a finite degree over C (or R), we may assume without loss of generality that \"K\" is a normal extension of R (hence it is a Galois extension, as every algebraic extension of a field of characteristic 0 is separable). Let \"G\" be the Galois group of this extension, and let \"H\" be a Sylow 2-subgroup of \"G\", so that the order of \"H\" is a power of 2, and the index of \"H\" in \"G\" is odd. By the fundamental theorem of Galois theory, there exists a subextension \"L\" of \"K\"/R such that Gal(\"K\"/\"L\") = \"H\". As [\"L\":R] = [\"G\":\"H\"] is odd, and there are no nonlinear irreducible real polynomials of odd degree, we must have \"L\" = R, thus [\"K\":R] and [\"K\":C] are powers of 2. Assuming by way of contradiction that [\"K\":C] > 1, we conclude that the 2-group Gal(\"K\"/C) contains a subgroup of index 2, so there exists a subextension \"M\" of C of degree 2. However, C has no extension of degree 2, because every quadratic complex polynomial has a complex root, as mentioned above. This shows that [\"K\":C] = 1, and therefore \"K\" = C, which completes the proof.\n\nThere exists still another way to approach the fundamental theorem of algebra, due to J. M. Almira and A. Romero: by Riemannian geometric arguments. The main idea here is to prove that the existence of a non-constant polynomial \"p\"(\"z\") without zeros implies the existence of a flat Riemannian metric over the sphere S. This leads to a contradiction, since the sphere is not flat.\n\nA Riemannian surface (\"M\", \"g\") is said to be flat if its Gaussian curvature, which we denote by \"K\", is identically null. Now, Gauss–Bonnet theorem, when applied to the sphere S, claims that\n\nwhich proves that the sphere is not flat.\n\nLet us now assume that \"n\" > 0 and \n\nfor each complex number \"z\". Let us define \n\nObviously, \"p*\"(\"z\") ≠ 0 for all \"z\" in C. Consider the polynomial \"f\"(\"z\") = \"p\"(\"z\")\"p*\"(\"z\"). Then \"f\"(\"z\") ≠ 0 for each \"z\" in C. Furthermore,\n\nWe can use this functional equation to prove that \"g\", given by\n\nfor \"w\" in C, and\n\nfor \"w\" ∈ S\\{0}, is a well defined Riemannian metric over the sphere S (which we identify with the extended complex plane C ∪ {∞}).\n\nNow, a simple computation shows that\n\nsince the real part of an analytic function is harmonic. This proves that \"K\" = 0.\n\nSince the fundamental theorem of algebra can be seen as the statement that the field of complex numbers is algebraically closed, it follows that any theorem concerning algebraically closed fields applies to the field of complex numbers. Here are a few more consequences of the theorem, which are either about the field of real numbers or about the relationship between the field of real numbers and the field of complex numbers:\n\n\n\n\n\n\nWhile the fundamental theorem of algebra states a general existence result, it is of some interest, both from the theoretical and from the practical point of view, to have information on the location of the zeros of a given polynomial. The simpler result in this direction is a bound on the modulus: all zeros ζ of a monic polynomial formula_37 satisfy an inequality |ζ| ≤ \"R\", where\n\nNotice that, as stated, this is not yet an existence result but rather an example of what is called an a priori bound: it says that \"if there are solutions\" then they lie inside the closed disk of center the origin and radius \"R\". However, once coupled with the fundamental theorem of algebra it says that the disk contains in fact at least one solution. More generally, a bound can be given directly in terms of any p-norm of the \"n\"-vector of coefficients formula_39 that is |ζ| ≤ \"R\", where \"R\" is precisely the \"q\"-norm of the 2-vector formula_40 \"q\" being the conjugate exponent of \"p\", formula_41 for any 1 ≤ \"p\" ≤ ∞. Thus, the modulus of any solution is also bounded by\n\nfor 1 < \"p\" < ∞, and in particular\n\n(where we define \"a\" to mean 1, which is reasonable since 1 is indeed the \"n\"-th coefficient of our polynomial). The case of a generic polynomial of degree \"n\", \n\nis of course reduced to the case of a monic, dividing all coefficients by \"a\" ≠ 0. Also, in case that 0 is not a root, i.e. \"a\" ≠ 0, bounds from below on the roots ζ follow immediately as bounds from above on formula_46, that is, the roots of \n\nFinally, the distance formula_48 from the roots ζ to any point formula_49 can be estimated from below and above, seeing formula_50 as zeros of the polynomial formula_51, whose coefficients are the Taylor expansion of \"P\"(\"z\") at formula_52\n\nLet ζ be a root of the polynomial \n\nin order to prove the inequality |ζ| ≤ \"R\" we can assume, of course, |ζ| > 1. Writing the equation as \n\nand using the Hölder's inequality we find \n\nNow, if \"p\" = 1, this is \n\nthus \n\nIn the case 1 < \"p\" ≤ ∞, taking into account the summation formula for a geometric progression, we have\n\nthus \n\nand simplifying, \n\nTherefore \n\nholds, for all 1 ≤ \"p\" ≤ ∞.\n\n\n\n"}
{"id": "50484294", "url": "https://en.wikipedia.org/wiki?curid=50484294", "title": "Garden waste dumping", "text": "Garden waste dumping\n\nGarden waste, or green waste dumping is the act of discarding or depositing garden waste somewhere it does not belong. \nGarden waste is the accumulated plant matter from gardening activities which involve cutting or removing vegetation, i.e. cutting the lawn, weed removal, hedge trimming or pruning consisting of lawn clippings. leaf matter, wood and soil.\n\nThe composition and volume of garden waste can vary from season to season and location to location. A study in Aarhus, Denmark, found that on average, garden waste generation per person ranged between 122 kg to 155 kg per year.\n\nGarden waste may be used to create compost or mulch, which can be used as a soil conditioner, adding valuable nutrients and building humus. The creation of compost requires a balance between, nitrogen, carbon, moisture and oxygen. Without the ideal balance, plant matter may take a long time to break down, drawing nitrogen from other sources, reducing nitrogen availability to existing vegetation which requires it for growth.\n\nThe risk of dumping garden waste is that it may contain seeds and plant parts that may grow (propagules), as well as increase fire fuel loads, disrupt visual amenity, accrue economic costs associated with the removal of waste as well as costs associated with the mitigation of associated impacts such as weed control, forest fire.\n\nThere are strong links between weed invasion of natural areas and the proximity and density of housing. The size and duration of the community have a direct relation to the density of weed infestation. Of the various means in which migration of exotic species from gardens take place, such as vegetative dispersal of runners, wind born and fallen seed, garden waste dumping can play a significant role. The results of one North German study found that of the problematic population of Fallopia, app. 29% originated from garden waste. Of a population of Heracleum mantegazzianum, 18% was found by Schepker to be generated by garden waste (as cited by Kowarik & von der Lippe, 2008) pg 24-25.\n\nAn Australia government publication suggest that some of the main reasons for the dumping of garden waste can be attributed to lack of care for the environment, convenience, or a reluctance to pay for the correct collection or disposal of the waste. (Environmental Protection Agency [EPA]. 2013). People dump garden waste to avoid disposal fees at landfill sites or because they do not want to spend the time or effort disposing of or recycling their waste properly. This activity is carried out by people in all parts of the community, from householders to businesses, such as professional landscapers and gardeners.\nThe spread of exotic vegetation can out-compete locally endemic vegetation, altering the composition and structure of an ecosystem.\n\nDumping of garden waste in particular facilitates the spread of exotic vegetation into forest remnants via the introduction of seeds and propagules contained within the garden waste. Common selection criteria for home gardeners when choosing plants are often based on ease of propagation, suitability to local environmental conditions and novelty. These specific chosen characteristics increase the chance of plant parts and seeds that are introduced into forested areas becoming a problem.\n\nThe three major causes of animal habitat degradation are; the disruption or loss of ecosystem functions, the loss of food resources and loss of resident species. Non-native invaders can cause extinctions of vulnerable native species through competition, pest and disease transportation and habitat and ecosystem alteration.\nThe dumping of garden waste in nature reserves surrounding and near urban areas increases the risk of fires. The dumped garden waste will eventually dry out creating fuel adding to already fallen debris fuel load on which a fire can thrive and spread on. Garden waste can spread weeds and these weeds build fuel for fires.\nDumped garden waste can facilitate higher rates of erosion by smothering natural vegetation cover.\nWith no root systems for stabilisation the top soil is vulnerable to erosion (Ritter, J. 2015), This can add higher levels of sediments, contributing to the siltation of creeks and waterways.\n\nIf plant matter gets into waterways it can create reduced oxygen levels through the process of decomposition of green waste such as lawn clippings. This directly upsets the quality of water, affecting fish and aquatic wildlife.\nThis dumping of green waste can also lead to the blocking of drainage systems; directly through the build-up of plant debris, and indirectly through the spread of invasive plant species that colonise wet areas, reducing and or changing the flow of waterways. This change in flow, including path and velocity, can alter hydrological cycles, affecting frequency and intensity of floods.\n\nGreen and garden waste has a direct impact to the visual aesthetics of land and can often attract further illegal dumping.\n\nDumping garden waste in nature reserves and parks surrounding and near urban areas can effect directly and indirectly the existing flora and fauna, as well as human life through the increased risk of fires. The dumped garden waste will eventually dry, creating additional fuel, adding to already fallen debris on which a fire can thrive and spread. Garden waste can spread weeds and these weeds also build fuel for fires. Fires may also spread to the suburban areas where humans can also be impacted by losing their homes from fire, incur injury or death from smoke or burns, and suffer economic loses such as income loss and clean-up costs. Fires can lead to an overall loss of habitat and biodiversity.\n\nThe invasion of exotic plant species into remnant native forests is a threat to biodiversity. Some impacts of habitat degradation include; when native animals, insects and birds become vulnerable and put at risk; loss of food source for native wildlife; disruption of native plant-animal relationships ie pollination and seed dispersal and disconnection of plant-host relationships. Highly adaptive plants chosen for their ease of cultivation out compete more specialised species. \nWeed invasion of a forest system can change the processes of plant succession (the system of one species replacing another due to disturbance factors), the composition of the plant community and the composition and availability of nutrients. The change in forest composition can lead to loss of unique plant species.\nWhen a habitat is destroyed, the plants, animals, and other organisms that occupied the habitat have a reduced carrying capacity so that populations decline and extinction becomes a threat. Many endemic organisms have very specific requirements for their survival that can only be found within a certain ecosystem. The term 'hotspot' is used to describe areas featuring exceptional concentrations of endemic species and facing high potential of habitat degradation. The 25 most significant hotspots contain the habitats of 133,149 plant species (44% of all plant species worldwide; table 1) and 9,645 vertebrate species (35% of all vertebrate worldwide; table 2). These endemics are confined to an expanse of 2.1 million square kilometers (1.4% of land surface). Having lost 88% of their primary vegetation, they formerly occupied 17.4 million square kilometers or 11.8% of land surface.\nThe recruitment of alien invasive species may lead to a homogenisation of landscapes. Although increased bio diversity in subregions created by newly introduced species may occur, the displacement of the existing plant species may lead to reduced biodiversity on a global scale.\n\nWhen population-level properties that indicate superior competitive ability of the invading species are examined, 13–24 (42–77%) of the species are included, with the majority of species showing traits capable of modifying natural systems at both ecosystem and community/population scales.\n\nThe dumping of green waste such as lawn clippings can block drainage systems which pollutes water therefore effecting water quality and the health of aquatic plants and animals. Dumped garden waste can add high levels of sediments, reducing the light available for photosynthesis. Dumping also block waterways and roads, cause flooding and facilitate higher rates of erosion by smothering natural vegetation cover.\n\nIllegal dumping is carried out by all types of people in all parts of the community, from householders to businesses and other organizations. Addressing these motivations will enable strategies to be developed that deal with the root causes, rather than the results, of illegal dumping.\n\nSome of the main reasons for this careless disregard for waste can be put down to sheer convenience, lack of care for the environment and also a reluctance to pay for the correct collection or disposal of the waste. The monitoring of illegally dumped garden waste by the community and industries will drive effectual tactics to battle illegal depositing. People dump waste illegally to avoid disposal fees at landfill sites or because they do not want to spend the time or effort disposing of or recycling their waste properly. Alligator weed (\"Alternanthera philoxeroides\" (Mart.) Griseb.) is an introduced weed originating from Sri Lanka and is creating major issues throughout the Australia since its introduction into the country. Alligator weed has the potential to affect aquatic and terrestrial biodiversity severely and to cause considerable social and economic costs, particularly in aquatic situations.\n\nEducation on the value of biodiversity and the negative effects of weed invasion have been identified as key items in changing current trends. Specific education campaigns on the risks of dumping garden waste could be targeted at high-risk societal groups such as residents of housing in close proximity to reserves as well as members of gardening communities and plant sellers. \nRestricting the selection of garden species in new housing developments adjacent to reserves may reduce the effects of illegal dumping, thereby reducing requirement and associated cost of weed management. Creating habitat for wildlife by planting native plants, making a water source available, provide shelter and places to raise young. Healthy ecosystems are necessary for the survival and health of all organisms, and there are a number of ways to reduce negative impact on the environment. Cultivation of native plant species may benefit not only native plant populations but also native animal populations. For example, Sears & Anderson suggest that native bird species diversity in Australia and North America tend to match the volume and diversity of native vegetation. Crisp also explains the percentage of native insect species in a fauna has been found to be consistent with the percentage of native plant species.\n\nComposting is a great way to recycle nutrients back into soils. Mulching the garden with leaves and clippings (BMCC, n.d). \nFostering an appreciation of local natural environmental features and plant species may also help mitigate the issue. as well as the restriction of highly invasive plant species through international policy.\nUtilization of green waste bins that are provided by some councils or shires that are emptied via curbside collection (BMCC. n.d). The addition of facilities for waste disposal could also improve the issue (DECC. 2008).\nMitigation may involve governments holding campaigns that show people disposing legally and reporting the consequences for disposing illegally. A way Australian governments are addressing the problem is through the increase of fines in conjunction with better law enforcement. In Australia, fines can be up to $1,000,000 and can also incur imprisonment. The Protection of the Environment Operations Act imposes penalties for offences including polluting waters with waste, polluting land, illegally dumping waste or using land as an illegal waste facility.\n\nThe new section of the \"POEO Act\" (The Protection of the Environment Operations Act 1997) now imposes further penalties for offences including polluting waters with waste, polluting land, illegally dumping waste or using land as an illegal waste facility (Parrino, Maysaa, Kaoutarani & Salam, 2014). Communities are encouraged to report illegal dumping. In accordance with NSW Illegal Dumping Strategy 2014-16, hefty fines and a maximum jail sentence of 2 years can be handed down to repeat offenders.\n"}
{"id": "55321544", "url": "https://en.wikipedia.org/wiki?curid=55321544", "title": "Housing (engineering)", "text": "Housing (engineering)\n\nIn engineering, a housing is an exterior case or enclosure used to protect an interior mechanism, including integrated fittings or brackets to keep internal components in place. The housing prevents the interior mechanism from being fouled by outside debris or to contain the internal components. Housing may be the body of the device, vital to its function.\n\nHousing is an exterior case or enclosure used to protect an interior mechanism. The housing prevents the interior mechanism from being fouled by outside debris. It may also have integrated fittings or brackets to keep internal components in place; sometimes a housing is the body of the device, vital to its function.\n\nHousings are most commonly made of metal or plastic. The design of housing is specific to the item and its use. Housing may provide a number of functions.\nHousing prevents the interior mechanism from being fouled by outside debris. Housings are sometimes made watertight, especially when the interior mechanisms contain electronics.\n\nHousings are commonly used to protect gearboxes, where the housing also is responsible for containing the lubricant. Housings can also play a safety role, by providing a barrier between people and dangerous or fast-moving mechanisms.\n\nHousing may need to provide a user interface for the internal devices, such as for televisions and video game controllers.\n\nHousing may include decorative elements. When these elements are removable and replaceable panels, they may be known as faceplates. Interchangeable faceplates provide a method to update the cosmetics of the housing without replacing the entire enclosure.\n\n"}
{"id": "38953575", "url": "https://en.wikipedia.org/wiki?curid=38953575", "title": "Human sacrifice in Maya culture", "text": "Human sacrifice in Maya culture\n\nDuring the pre-Columbian era, human sacrifice in Maya culture was the ritual offering of nourishment to the gods. Blood was viewed as a potent source of nourishment for the Maya deities, and the sacrifice of a living creature was a powerful blood offering. By extension, the sacrifice of a human life was the ultimate offering of blood to the gods, and the most important Maya rituals culminated in human sacrifice. Generally only high status prisoners of war were sacrificed, with lower status captives being used for labour.\n\nHuman sacrifice among the Maya is evident from at least the Classic period (c. AD 250–900) right through to the final stages of the Spanish conquest in the 17th century. Human sacrifice is depicted in Classic Maya art, is mentioned in Classic period hieroglyphic texts and has been verified archaeologically by analysis of skeletal remains from the Classic and Postclassic (c. AD 900–1524) periods. Additionally, human sacrifice is described in a number of late Maya and early Spanish colonial texts, including the \"Madrid Codex\", the K'iche' epic \"Popol Vuh\", the K'iche' \"Título de Totonicapán\", the K'iche' language \"Rabinal Achi\", the \"Annals of the Kaqchikels\", the Yucatec \"Songs of Dzitbalche\" and Diego de Landa's \"Relación de las cosas de Yucatán\".\n\nA number of methods were employed by the Maya, the most common being decapitation and heart extraction. Additional forms of sacrifice included ritually shooting the victim with arrows, hurling sacrifices into a deep sinkhole, entombing alive to accompany a noble burial, tying the sacrifice into a ball for a ritual reenactment of the Mesoamerican ballgame and disembowelment.\n\nA variety of methods were used by the ancient Maya to perform human sacrifice, such as:\n\nImportant rituals such as the dedication of major building projects or the enthronement of a new ruler required a human sacrificial offering. The sacrifice of an enemy king was the most prized offering, and such a sacrifice involved decapitation of the captive ruler in a ritual reenactment of the decapitation of the Maya maize god by the Maya death gods. In AD 738, the vassal king K'ak' Tiliw Chan Yopaat of Quiriguá captured his overlord, Uaxaclajuun Ub'aah K'awiil of Copán and a few days later he ritually decapitated him; such royal sacrifices were often recorded in Maya script with the \"ax event\" glyph. The decapitation of an enemy king may have been performed as part of a ritual ballgame reenacting the victory of the Maya Hero Twins over the gods of the underworld.\n\nSacrifice by decapitation is depicted in Classic period Maya art, and sometimes took place after the victim was tortured, being variously beaten, scalped, burnt or disembowelled. Sacrifice by decapitation is depicted on reliefs at Chichen Itza in two of the ballcourts (the Great Ballcourt and the Monjas Ballcourt). The Hero Twins myth recounted in the Popol Vuh relates how one of each pair of twins (the Hero Twins themselves and their father and uncle) was decapitated by their ballgame opponents.\n\nDuring the Postclassic period (c. 900–1524) the most common form of human sacrifice was heart extraction, influenced by the method used by the Aztecs in the Valley of Mexico; this usually took place in the courtyard of a temple, or upon the summit of the pyramid-temple. The sacrifice was stripped and painted blue, which was the colour representing sacrifice, and was made to wear a peaked headdress.\n\nFour blue-painted attendants representing the four Chaacs of the cardinal directions stretched the sacrifice out over a convex stone that pushed the victim's chest upwards; An official referred to as a \"nacom\" in Landa's \"Relación de las cosas de Yucatán\" used a sacrificial knife made from flint to cut into the ribs just below the victim's left breast and pull out the still-beating heart. The \"nacom\" then passed the heart to the officiating priest, or \"chilan\", who smeared blood upon the image of the temple's deity.\n\nDepending upon the exact ritual, sometimes the four Chaacs would throw the corpse down the pyramid steps to the courtyard below, where it would be skinned by assistant priests, except for the hands and feet. The \"chilan\" would then remove his ritual attire and dress himself in the skin of the sacrificial victim before performing a ritual dance that symbolised the rebirth of life. If it was a notably courageous warrior who had been sacrificed, then the corpse would be cut into portions and parts would be eaten by attending warriors and other bystanders. The hands and feet were given to the \"chilan\" who, if they had belonged to a war captive, wore the bones as a trophy. Archaeological investigations indicate that heart sacrifice was practised as early as the Classic period.\n\nSome rituals involved the sacrifice being killed with bow and arrows. The sacrificial victim was stripped and painted blue and made to wear a peaked cap, in a similar manner to the preparation for heart sacrifice. The victim was bound to a stake during a ritual dance, and blood was drawn from the genitals and smeared onto the image of the presiding deity. A white symbol was painted over the victim's heart, which served as a target for the archers. The dancers then passed in front of the sacrificial victim, shooting arrows in turn at the target until the whole chest was filled with arrows. Sacrifice with bow and arrow is recorded as far back as the Classic Period (c. 250-900), and was depicted with graffiti upon the walls of Tikal Temple II. The \"Songs of Dzitbalche\" are a collection of Yucatec Maya poems written down in the mid-18th century; two poems deal with arrow sacrifice and they are believed to be copies of poems dating to the 15th century, during the Postclassic period. The first, called \"Little Arrow\", is a song calling upon the sacrifice to be brave and take comfort. The second is entitled \"Dance of the Archer\" and is a ritual dedicated to the rising sun; it includes instructions to the archer; the archer is instructed upon how to prepare his arrows and to dance three times around the sacrifice. The archer is instructed not to shoot until the second circuit, and to be careful to make sure that the sacrifice dies slowly. On the third circuit, whilst still dancing, the archer is instructed to shoot twice. A similar scene is described in the Annals of the Kaqchikels, where an important prisoner is bound to a scaffold; the Kaqchikel warriors begin a ritual \"blood dance\" and proceed to shoot him full of arrows. In the Late Postclassic K'iche' language drama \"Rabinal Achi\", an important war captive is tied to a stake representing the mythological Maize Tree and is sacrificed by being shot with arrows; the text compares the archers to hunters and the sacrifice to game.\n\nLate Classic graffiti from a structure buried under Group G in Tikal depicts a sacrifice bound to a stake with his hands tied behind his head; the victim was disembowelled. At the Classic period city of Palenque, a woman in her twenties was entombed alive to accompany a deceased nobleman as a funerary offering.\n\nAt the Sacred Cenote in Chichen Itza, people were hurled into the cenote during times of drought, famine or disease. The Sacred Cenote is a naturally occurring sinkhole eroded from the local limestone; it is approximately wide and drops to the water surface, with the water another deep. The sides of the cenote are sheer. Human sacrifice was practiced right up until the Spanish conquest of Yucatán, well after the decline of the city.\n\nAt times sacrifices were tightly bound into a ball and were bounced in a ritual reenactment of the ballgame.\n\nHuman sacrifice is depicted in Late Classic artwork and sometimes involved torture; sacrifice was generally via decapitation. At times the sacrificial victim was dressed as a deer. The intended sacrifice may have been publicly displayed and paraded before the act of sacrifice itself. Images of human sacrifice were often sculpted into the steps of Maya architecture and such stairways may have been the site of periodic sacrifice. Ritual decapitation is well attested from Maya hieroglyphic texts throughout the Classic period. Evidence of mass sacrifice during the Classic period has not been recovered archaeologically. Archaeological excavations at a number of sites, including Palenque, Calakmul and Becan, have uncovered skeletons that bear marks to the vertebrae and ribs consistent with heart extraction at the time of death using a long-bladed flint knife. During the Classic period, the sacrifice of companions to accompany high-ranking burials is likely to have been widespread and performed using the heart extraction method, leaving little evidence on skeletal remains. Analysis of those remains that do bear marks suggestive of heart sacrifice indicates that during the Classic period the Maya used a method involving cutting across the diaphragm immediately below the ribcage and cutting the heart free.\n\nA Postclassic mass burial in Champotón in Campeche, Mexico, included skeletons bearing evidence of violent blows to the sternum that have been interpreted as evidence of heart sacrifice. The Madrid Codex, a Postclassic hieroglyphic Maya book, has an illustration of sacrifice by heart extraction, with the victim stretched over an arched stone.\n\nAmong the K'iche' of highland Guatemala, human sacrifice was performed to the K'iche' gods. Writing at the end of the 17th century, Francisco Ximénez described the tradition that upon the temple of Tohil, human sacrifices were tied before the representation of the deity, where the priest would open the victim's chest and cut out his heart. After sacrifice, the victim's body was probably hurled down the front stairway of the temple where his head would be severed to be placed on a skull rack that was located in front of the temple. In the K'iche' epic Popol Vuh, the god Tohil demands his right to suckle from his people, as an infant to its mother, but Tohil suckled upon human blood from the chest of the sacrificial victim. The \"Popol Vuh\" also describes how the Hero Twin Hunahpu was sacrificed with both the removal of his heart and his head. Human sacrifice was probably also performed to the K'iche' mountain god Jacawitz. Human sacrifice is also mentioned in the K'iche' document \"Título de Totonicapán\" (\"Title of Totonicapán\"). A long passage describing human sacrifice is difficult to interpret but features heart and arrow sacrifice, the flaying of the victim and wearing of his skin in a manner similar to the Aztec rituals associated with their god Xipe Totec, and mention of the sacrificial knife of Tohil.\nThe Kaqchikel Maya, neighbours of the K'iche', also practised human sacrifice. Ample evidence of human sacrifice has been excavated at Iximche, their capital. Human sacrifice is evidenced at the site by the altar upon Structure 2, of a type used in heart sacrifice, and by a cylindrical cache of skulls taken from decapitated victims accompanied by obsidian knives. A pentatonic flute crafted from a child's femur was recovered from one of the temples and is also indicative of human sacrifice. A sacrificial flint knife was also recovered from Structure 3, and a circular altar at the site is very similar to those used for so-called \"gladiatorial sacrifice\" by the Aztecs and it may have served this purpose. The \"Annals of the Kaqchikels\" record that around 1491 the rulers of Iximche captured the rulers of the K'iche', as well as the image of Tohil. The captured king and his co-ruler were sacrificed together with the son and grandson of the king, other noblemen and high-ranking warriors. The same text describes how the Kaqchikel captured a powerful lord, called Tolk'om, who was tied to a scaffold and was shot with arrows during a ritual dance.\n\nIn 1511 the Spanish caravel \"Santa María de la Barca\" set sail along the Central American coast to Santo Domingo from Darien under the command of Pedro de Valdivia. The ship was wrecked upon a reef somewhere off Jamaica. There were just twenty survivors from the wreck, including Captain Valdivia, Gerónimo de Aguilar and Gonzalo Guerrero. The survivors set themselves adrift in one of the ship's boats, with bad oars and no sail; after thirteen days during which half of the survivors died, they made landfall upon the coast of Yucatán. There they were seized by the Maya Lord Halach Uinik. Captain Valdivia was sacrificed with four of his companions, and their flesh was served at a feast. The other prisoners were fattened for killing, although Aguilar and Guerrero managed to escape.\n\nAfter the disastrous Spanish-led assault on Uspantán in 1529, captives taken by the Uspanteks were sacrificed to Exbalamquen, one of the Hero Twins. In 1555 the Acala and their Lacandon allies killed the Spanish friar Domingo de Vico. De Vico, who had established a small missionary church in San Marcos (in Alta Verapaz, Guatemala), had offended a local Maya ruler; the indigenous leader shot the friar through the throat with an arrow; the angry natives then sacrificed him by cutting open his chest and extracting his heart. His corpse was then decapitated; the natives carried off his head as a trophy, which was never recovered by the Spanish. In the early 1620s a Spanish party received permission to visit the still independent Itza capital at Nojpetén, headed by friar Diego Delgado who was accompanied by 13 Spanish soldiers and 80 Christianised Maya guides from Tipu, now in Belize. The party was seized when they arrived at Nojpetén and sacrificed with their hearts cut out. They were then decapitated and their heads displayed on stakes around the city; Delgado was dismembered. The main Spanish party was ambushed at Sakalum in January 1624 and slaughtered. The Spanish Captain Francisco de Mirones and a Franciscan priest were sacrificed using the heart extraction method after being bound to the forked posts of the church. The rest of the Spanish party were also sacrificed, and their bodies impaled on stakes at the village entrance.\n\nIn 1684 three Franciscan friars were killed, probably by heart sacrifice, at the Manche Ch'ol settlement of Paliac on the Caribbean coast of Belize. They included Francisco Custodio, Marcos de Muros, and an unnamed lay brother.\n\nA number of additional Spanish missionaries were sacrificed at Nojpetén. In February 1696 Franciscan friar Juan de San Buenaventura and an unspecified Franciscan companion were taken to Nojpetén during a skirmish between the Yucatec Spanish and the Itza on the west shore of Lake Petén Itzá. The Itza high priest AjKin Kan Ek' later related that he had the Franciscans bound in the form of crosses and then cut out their hearts. About a month later a Guatemalan Spanish expedition was ambushed and slaughtered; Dominican friars Cristóbal de Prada and Jacinto de Vargas were taken across to the island of Nojpetén and were similarly bound to X-shaped crosses before having their hearts cut out.\n\n"}
{"id": "55765668", "url": "https://en.wikipedia.org/wiki?curid=55765668", "title": "Jonathan Laidlaw", "text": "Jonathan Laidlaw\n\nJonathan Laidlaw QC (February 1960) is an English barrister notable for prosecuting and defending in many high-profile criminal cases, including defence of News International chief executive Rebekah Brooks who was acquitted of all charges after the phone hacking trial.\nIn 2012 he was chosen by the Football Association to present their case against John Terry.\n\nLaidlaw was called to the Bar in 1982. He practises at 2 Hare Court in the Inner Temple, where in 2013 he was appointed the head of chambers. He sat as a Crown Court Recorder since 1998 and in 2008 was appointed Queen's Counsel.\n\nAs Treasury Counsel, Laidlaw brought Britain’s first war crimes case and acted as prosecution in the trial of the Provisional IRA bombing of Canary Wharf, the Official Secrets Act prosecution of Richard Tomlinson and David Shayler, the Jill Dando murder trial, the trial of the Al Qaeda cell that planned pre-9/11 attacks in the United States and United Kingdom, the trial of the Al Qaeda attack on Glasgow Airport and of Delroy Grant who received four life sentences and is believed responsible for roughly 100 cases of rape, sexual assault and burglary.\n\n"}
{"id": "54035", "url": "https://en.wikipedia.org/wiki?curid=54035", "title": "Jus soli", "text": "Jus soli\n\nJus soli (; ), meaning \"right of the soil\", commonly referred to as birthright citizenship in the United States, is the right of anyone born in the territory of a state to nationality or citizenship.\n\n\"Jus soli\" was part of the English common law, in contrast to \"jus sanguinis\", which derives from the Roman law that influenced the civil-law systems of continental Europe. Where it exists universally, it is often not quite unconditional. For instance, some countries deny citizenship to children of foreign diplomats. As an unconditional (or near-unconditional) basis for citizenship, \"jus soli\" is the predominant rule in the Americas, but it is rare elsewhere. Since the Twenty-seventh Amendment of the Constitution of Ireland was enacted in 2004, no European country grants citizenship based on unconditional or near-unconditional \"jus soli\".\n\nAlmost all states in Europe, Asia, Africa and Oceania grant citizenship at birth based upon the principle of \"jus sanguinis\" (right of blood), in which citizenship is inherited through parents not by birthplace, or a restricted version of \"jus soli\" in which citizenship by birthplace is automatic only for the children of certain immigrants. Countries that have acceded to the 1961 Convention on the Reduction of Statelessness will grant nationality to otherwise stateless persons who were born on their territory, or on a ship or aircraft flagged by that country.\n\n\"Jus soli\" is associated with permissive citizenship rights. Most countries with unconditional or near-unconditional \"jus soli\" laws tend to give birthright citizenship (and nationality) based on \"jus sanguinis\" rules as well, although these stipulations tend to be more restrictive than in countries that use \"jus sanguinis\" as the primary basis for nationality.\n\nAn early form of \"jus soli\" dates from Cleisthenes' reforms of ancient Athenian law in the 6th century BC. It developed further in the Roman world, where citizenship was extended to all free inhabitants of the Roman Empire by the Edict of Caracalla in AD 212.\n\nMuch later, the independence of the English colonies in America and the French Revolution in the late 18th century laid the foundations for \"jus soli\". With the social and economic development of the 19th and 20th centuries and the massive migrations to the Americas and Western Europe, \"jus soli\" was established in a greater and greater number of countries.\n\nAt the turn of the 19th century, nation-states commonly divided themselves between those granting nationality on the grounds of \"jus soli\" (France, for example) and those granting it on the grounds of \"jus sanguinis\" (for example, Germany before 1990). However, since 2007 the European migrant crisis has focused attention on these two conflicting sources of nationality rights.\n\n\"Lex soli\" is a law used in practice to regulate who and under what circumstances an individual can assert the right of \"jus soli\". Most states provide a specific \"lex soli—\"in application of the respective \"jus soli—\"and it is the most common means of acquiring nationality. However, a frequent exception to \"lex soli\" is imposed when a child is born to a parent in the diplomatic or consular service of another state on a mission to the state in question.\n\n"}
{"id": "16659679", "url": "https://en.wikipedia.org/wiki?curid=16659679", "title": "Kaldor's growth laws", "text": "Kaldor's growth laws\n\nKaldor's growth laws are a series of three \"laws\" relating to the causation of economic growth.\n\nLooking at the countries of the world now and through time Nicholas Kaldor noted a high correlation between living standards and the share of resources devoted to industrial activity, at least up to some level of income. Only New Zealand, Australia and Canada have become rich whilst relying mainly on agriculture. He proposed three laws on these empirical regularities:\n\nThirlwall (2003, p123–124) also reports Kaldor's highlighting of three subsidiary propositions which are also important to take into account. They are:\n"}
{"id": "1424162", "url": "https://en.wikipedia.org/wiki?curid=1424162", "title": "Khitan scripts", "text": "Khitan scripts\n\nThe Khitan scripts were the writing systems for the now-extinct Para-Mongolic Khitan language used in the 10th-12th century by the Khitan people who had established the Liao dynasty in Northeast China. There were two scripts, the large script and the small script. These were functionally independent and appear to have been used simultaneously. The Khitan scripts continued to be in use to some extent by the Jurchen people for several decades after the fall of the Liao dynasty until the Jurchens fully switched to a script of their own. Examples of the scripts appeared most often on epitaphs and monuments, although other fragments sometimes surface.\n\nMany scholars recognize that the Khitan scripts have not been fully deciphered and that more research and discoveries would be necessary for a proficient understanding of them. The Khitan scripts are part of the Chinese family of scripts.\n\nKnowledge of the Khitan language, which was written by the Khitan script, is quite limited as well. Although there are several clues to its origins, which might point in different directions, the Khitan language shares an ancestor with the Mongolian languages but is not one.\n\nAbaoji of the Yelü clan, founder of the Khitan, or Liao Dynasty, introduced the original Khitan script in 920 CE. “Large script”, or “big characters\" (大字), as it was referred to in some Chinese sources, was established to keep the record of the new Khitan state. The Khitan script was based on the idea of the Chinese script.\nThe Khitan large script was considered to be relatively simple. The large script characters were written equally spaced, in vertical columns, in the same way as the Chinese has been traditionally written. Although large script mostly uses logograms, it is possible that ideograms and syllabograms are used for grammatical functions. The large script has a few similarities to Chinese, with several words taken directly with or without modifications from the Chinese (e.g. characters 二,三,十,廿,月,日, which appear in dates in the apparently bilingual \"Xiao Xiaozhong muzhi\" inscription from Xigushan, Jinxi, Liaoning Province). Most large script characters, however, cannot be directly related to any Chinese characters. The meaning of most of them remains unknown, but that of a few of them (numbers, symbols for some of the five elements and the twelve animals that the Khitans apparently used to designate years of the sexagenary cycle) has been established by analyzing dates in Khitan inscriptions.\n\nWhile there has long been controversy as to whether a particular monument belong to the large or small script,\nthere are several monuments (steles or fragments of stelae) that the specialists at least tentatively identify as written in the Khitan large script. However, one of the first inscriptions so identified (the \"Gu taishi mingshi ji\" epitaph, found in 1935) has been since lost, and the preserved rubbings of it are not very legible; moreover, some believe that this inscription was a forgery in the first place. In any event, the total of about 830 different large-script characters are thought to have been identified, even without the problematic \"Gu taishi mingshi ji\"; including it, the character count rises to about 1000. The Memorial for Yelü Yanning (dated 986 CE) is one of the earliest inscriptions in Khitan large script.\n\nThe Khitan small script was invented in about 924 or 925 CE by a scholar named Yelü Diela. He drew his inspiration from “the Uyghur language and script,” which he was shown by a visiting Uyghur ambassador at the Khitan court. For this reason, Khitan small script was originally thought to be a daughter script of the Uyghur alphabet.\n\nUsing a smaller number of symbols than large script, small script was less complex, yet still “able to record any word.” While small-script inscriptions employed some logograms as well, most words in small script were made using a blocked system reminiscent of the later Hangul writing of Korea, meaning that a word is represented by one group (square block) composed of several glyphs with individual phonetic meanings (somewhat similar to the \"jamo\" units of Hangul). Unlike Hangul's \"jamo\", a Khitan phonetic symbol could represent not just a single vowel or consonant, but a CV or VC pair as well. Each block could incorporate two to seven such \"phonetic element\" characters, written in pairs within the block, with the first half of the pair on the left. If there were an odd number of characters in a block, the unpaired character would be centered below the preceding pair.\n\nAlthough there is some speculation, it appears there are no characters that both scripts share. Periodically, epitaphs written using small script will be written using the large script method of linearity. Although small script had some similarities to Chinese, Khitan characters were often used to record Chinese words. The appearance of a likeness between a small script and a Chinese character does not aide in the reading of Khitan. For example, the Chinese character for ‘mountain’(山) is the same as the Khitan small script logogram for ‘gold’(and, thus, the name of the Jin Dynasty).\n\nOf the 378 known small script characters, 125 are semantic, 115 are phonetic, and the remainder have not been deciphered. (Usually, it was possible to guess the phonetic value of an element if it has been used to transcribe a Chinese loanword in a Khitan inscription; otherwise, such phonetic values are hard to determine, as very little of the Khitan language is known.)\nSmall script uses a mixture of logograms, syllabograms, and, as some as sources claim, a few single sound phonograms. Sometimes suffixes were written with syllabograms, just as single syllables sometimes were written with three syllabograms (with one each for the initial, medial, and final sounds of the syllable). Sometimes the initial consonants of syllables are indicated to be dental, labial, guttural, or nasal etc., based on the syllabograms involved. Additionally, vowels are sometimes indicated to be labial or non-labial, or pronounced in the front or back of the mouth.\n\nMuch of this information came from the \"Khitan Script Research Group\", led by the Mongolian scholar named Činggeltei, who used monuments, calendar, and similar Chinese texts to decipher sections of small script. A particularly valuable object of their study was the inscription on the \"Da Jin huangdi dotong jinglüe langjun xingji\" (大金皇帝都统经略郎君行记) stele, which is the only known bilingual Chinese-Khitan inscription. Produced during the Jurchen Jin Dynasty, it was originally (before the discovery of other Khitan inscriptions in 1922) thought to be in Jurchen.\n\nSome of the characters of the Jurchen scripts have similarities to Khitan large script. According to some sources, the discoveries of inscriptions on monuments and epitaphs give clues to the connection between Khitan and Jurchen. After the fall of the Liao Dynasty, the Khitan (small-character) script continued to be used by the Jurchen people for a few decades, until fully replaced with Jurchen script and, in 1191, suppressed by imperial order.\n\nThere are no surviving examples of printed texts in the Khitan language, and aside from five example Khitan large characters with Chinese glosses in a book on calligraphy written by Tao Zongyi (陶宗儀) during the mid 14th century, there are no Chinese glossaries or dictionaries of Khitan.\n\nThe main source of Khitan texts are monumental inscriptions, mostly comprising memorial tablets buried in the tombs of Khitan nobility. There are about 17 known monuments with inscriptions in the Khitan large script, ranging in date from 986 to 1176, and about 33 known monuments with inscriptions in the Khitan small script, ranging in date from 1053 to 1171. The two scripts are mutually exclusive (never occurring together on the same monument), but it is not known why the Khitan people used two different scripts, or what determined the choice of which script to use.\n\nIn addition to monumental inscriptions, short inscriptions in both Khitan scripts have also been found on tomb murals and rock paintings, and on various portable artefacts such as mirrors, amulets, paiza (tablets of authority given to officials and envoys), and special non-circulation coins. A number of bronze official seals with the seal face inscribed in a convoluted seal script style of Khitan characters are also known.\n\n\n"}
{"id": "49609771", "url": "https://en.wikipedia.org/wiki?curid=49609771", "title": "Kimiyo Mishima", "text": "Kimiyo Mishima\n\nKimiyo Mishima (born 1932) is a contemporary Japanese artist, best known for creating highly realistic versions of \"breakable printed matter\" in ceramic such as newspapers, comic books and boxes out of clay. Mishima began her artistic career as a painter in the early 1960s, then started working in ceramics in 1971. At this time, she began to use the silk screen technique to print newspaper and ad poster images onto clay.\n\nKimiyo Mishima was born in 1932 in Osaka, Japan. She graduated in 1951 from the Ohgimachi Public High School in Osaka. In 1986/87, she studied in New York, supported by a grant by the Rockefeller Scholarship ACC, New York, USA. She currently lives in Osaka, Japan.\n\nThe National Museum of Modern Art, Kyoto, JP\n\nKyoto Municipal Museum of Art, Kyoto, JP\n\nThe International Museum of Art, Osaka, JP\n\nBenesse Art Site Naoshima, Naoshima, JP\n\nHara Museum of Art, Tokyo, JP\n\nThe Metropolitan Museum of Art, Tokyo, JP\n\nHokkaido Prefectural Museum of Modern Art, Sapporo, JP\n\nThe Museum of Art, Hakodate, JP\n\nIwaki City Museum of Modern Art, Fukushima, JP\n\nTochigi Prefectural Museum of Art, JP\n\nOkegawa City Park, Saitama, JP\n\nMatsumoto City Museum of Art, Matsumoto, JP\n\nThe Gifu Prefectural Contemporary Ceramic Museum of Art, Gifu, JP\n\nThe Wakayama Prefectural Modern Museum of Art, Wakayama, JP\n\nThe Shigaraki Ceramic Cultural Park, Shiga, JP\n\nThe Hyogo Prefectural Museum of Art, Kobe, JP\n\nAshiya City Museum of Art and History, Ashiya, Hyogo, JP\n\nTakamatsu City Museum of Art, Takamatsu, Kagawa, JP\n\nYamaguchi Prefectural Museum of Art, Yamaguchi, JP\n\nHamada Children’s Museum of Art, Hamada, Shimane, JP\n\nIkeda Museum of 20th Century Art, Ikeda, Shizuoka, JP\n\nContemporary Art Museum ISE, Mie, JP\n\nOhara Museum of Art, Okayama, JP\n\nThe Japan Foundation, Tokyo, JP\n\nThe Korean Culture & Arts Foundation Seoul, KR\n\nInstitute of Contemporary Art, Kunsan National University, KR\n\nNational Museum of History, TW\n\nHAP POTTERY, Beijing, CN\n\nM+ Museum, Hong Kong, HK\n\nArt Institute of Chicago, Chicago, USA\n\nMinneapolis Institute of Arts, Minneapolis, USA\n\nThe Everson Museum of Art, New York, USA\n\nSamuel P. Harn Museum of Art, University of Florida, Gainesville Florida, USA\n\nThe First National Bank of Chicago, USA\n\nAsian Cultural Council, New York, USA\n\nSMITH COLLEGE, Northampton, USA\n\nWorcester Art Museum, Worcester, USA\n\nMuseum of Fine Arts, St. Petersburg, Florida, USA\n\nThe Keramion Museum for Contemporary Ceramic Art, DE\n\nThe Museum of Faenza, IT\n\nJapanese Culture Center, Roma, IT\n\nAriana Museum, Geneve, CH\n\nKunst Gesellschaft, Spiez, CH\n\nThe Museum of Art, Olot, ES\n\nInternational Ceramics Studio, Kecskemét, HU\n\nMusee Cernoschi, Paris, FR\n\nVehbi Koç Foundation, ARTER, Istanbul, TR\n\nWebpage of the artist in English and Japanese: http://www.mishimakimiyo.com/\n"}
{"id": "40910763", "url": "https://en.wikipedia.org/wiki?curid=40910763", "title": "Kimshin Myongsuk", "text": "Kimshin Myongsuk\n\nKimshin Myongsuk (, born March 5, 1961) is a South Korean feminism activist and human rights activist, journalist and reporter for If News, a liberal feminist Korean news journal.\n\nIn her early years Kim worked as a \"Dong-a Ilbo\" journalist. In the 1990s she supported feminism movements. She worked for KBS, MBC and other TV broadcasts. In the 2000s she joined If News. Kimshin Myongsuk was appointed as an editor and director there. In 1999, she made controversial appearances on KBS.\n\nShe is known for obtaining the first doctoral degree in the field of 'A theory of Feminine God' in South Korea.\n"}
{"id": "16158579", "url": "https://en.wikipedia.org/wiki?curid=16158579", "title": "Language policy in Latvia", "text": "Language policy in Latvia\n\nArticles 4 and 114 of the Constitution of Latvia form the foundation for language policy in Latvia, declaring Latvian to be the official state language and affirming the rights of ethnic minorities to preserve and develop their languages. Latgalian and the Livonian language, in addition to Latvian, are considered indigenous and all other languages foreign, including Russian (the first language for more than one third of the population). Other significant minority foreign languages include Belarusian, Ukrainian, Lithuanian, Polish, and Romani.\n\nThe preamble to the State Language Law includes as its goals \"the integration of national minorities into Latvian society while respecting their right to use their mother tongue or any other language; [and] the increase of the influence of the Latvian language in the cultural environment of Latvia by promoting a faster integration of society.\"\n\nThe official language (\"valsts valoda\", literally \"state language\") in Latvia is Latvian; this status has been explicitly defined since 1988. In 1992, amendments to the 1989 Law on Languages strengthened the position of Latvian. All other languages, except the extinct Livonian language, are defined as foreign languages in Section 5 of the State Language Law of 1999.\n\nSince 1998, the official status of the Latvian language has been written into the Constitution (Article 4); and since 2002, MPs have been asked to promise to strengthen Latvian as the only official language in order to take their seats (Article 18). In the Constitution's chapter on human rights, rights to get answers from authorities in Latvian are specified since 2002 (Article 104). The current State Language Law was not amended since its adoption in 1999 (as at 2017).\n\nIn 1995, Latvia signed, and in 2005 ratified the Council of Europe's Framework Convention for the Protection of National Minorities. When ratifying it, the Latvian Saeima (Parliament) made two declarations (worded as reservations) limiting the implementation of Articles 10 and 11. As at 2008, Latvia did not plan to sign the European Charter for Regional or Minority Languages.\n\nLanguage policy is implemented by a number of institutions: the State Language Commission (under the President) prepares proposals in this field; the State Language Centre (under the Ministry of Justice) executes control, imposes fines for administrative violations and translates documents of international significance, the Latvian Language Agency (under the Ministry of Education and Science) provides consultations and opportunities for learning the Latvian language, analyses the language situation.\n\nSince the State Language Law came into force in 2000, submitting documents to government (local included) and state public enterprises is allowed in Latvian only, except in cases specially defined in the law (emergency services, foreign residents etc.), according to Section 10. From 1992–2000, authorities had to accept documents in Russian, German and English, too, and were allowed to answer in the language of application.\n\nBefore the losses of the Latvian government in the cases \"Podkolzina v. Latvia\" (ECHR) and \"Ignatāne v. Latvia\" (UN HRC), a certain level of command in Latvian was asked for eligibility to Parliament and local councils. In practice, this had led to re-examinations of various candidates, at least sometimes unexpected, which prevented Ignatāne and Podkolzina (representatives of the Equal Rights party in the 1997 local and 1998 parliamentary elections) from participation. As of 2011, candidates do not need to prove language proficiency, but elected members of Saeima and local councilors can be deprived of mandate for insufficient command of Latvian.\n\nNames and surnames in Latvian-issued documents are formed in Latvianized form, according to Section 19. These provisions were subject in ECHR cases \"Kuhareca v. Latvia\" and \"Mencena v. Latvia\" (both declared inadmissible in 2004), since the Latvian Constitutional Court had found them constitutional in 2001. An analogous application was submitted to UN HRC in 2007 and won by the applicant on grounds of privacy (\"Raihman v. Latvia\").\n\nToponyms are formed in Latvian language only (on the Livonian coast - in Livonian as well), according to Section 18 of the State Language Law.\n\nThe Electronic Mass Media Law orders to use only Latvian language in the first channels of public radio and television, and basically Latvian language in their second channels (Section 66).\nThe government of Latvia in its policy documents refers to Latvia as a (democratic) nation state, constructing societal integration on the basis of the Latvian language, while respecting the diversity of languages. Unity block, comprising most of the governing coalition as of 2011, also describes Latvia as a nation state. The idea of the nation state, where \"language = nation\", is seen as the core and main engine of the language policy of the Latvian state. Critics draw parallels between measures of the Latvian government and the assimilation of linguistic minorities in various countries.\n\nOne critic, James Hughes, Reader in Comparative Politics at the London School of Economics and Political Science, has pointed out that Russian-speakers in Latvia constitute one of the largest linguistic minorities in Europe, therefore he considers Latvia's language laws to be denying Russophones their language rights, and thus they are contrary to international practice in the field of minority rights. Nataliya Pulina in \"Moskovskiye Novosti\" asserts that Latvia's Russophones are by percentage actually the largest linguistic minority in the EU whose language has no official status. Regarding the demographic arguments for Russian language rights in Latvia, the BBC's Angus Roxburgh reported in 2005:\nAmong the political parties, ForHRUL offers in its programme to grant co-official status to Russian, Latgalian and possibly others languages in municipalities where these are native for more than 20% of population. In a draft of its political programme, HC offers to grant co-official status to Latgalian and Russian in printed media, public sphere and education (for Russian, in communication with authorities, as well), stressing its support for the sole state language. Both these parties are in permanent opposition on the state level.\n\nOn the other hand, TB/LNNK, a member of governing coalition between 2006 and 2010, is demanding that Latvian be made the sole language of instruction, even in minority schools.\n\nAccording to research conducted by the Baltic Institute of Social Sciences in 2004, the majority (77%) of ethnic Latvians opposed (56%) or mostly opposed (21%) granting Russian status as a second official language, while the majority (87%) of Russians supported (59%) or mostly supported (28%) such status, while a majority (75%) of other ethnicities also supported (40%) or rather supported (35%) such status.\n\nThe Law on Electronic Media prescribes that national and regional electronic media need to broadcast at least 65 % in Latvian language (section 32). Besides, films aired in any channel should be dubbed in Latvian or to have original soundtrack and Latvian subtitles; TV broadcasts in languages other than Latvian, except news, live events, language learning broadcasts and retranslated content, must be subtitled in Latvian.(Section 28). The same concerns films shown in cinemas, according to Section 17 of State Language Law. Until a judgement of the Constitutional Court upon request of 24 ForHRUL MPs (delivered in 2003), broadcasting in minority languages was limited for private TV and radio (originally within 30%, since 1998 within 25%).\n\nAccording to Section 6 of State Language Law, levels of skills in Latvian are defined for various professions, which concern legitimate public interest. Totally, there are six levels and two lists of professions (longer for public sector and shorter for private sector), classified by needed level. For those who didn't get education in Latvian and aren't disabled, an examination is needed to define their skills in Latvian, to work in these professions. Those who fail to show needed level during inspections, can be fined. Labour market shows high demand for skills in Latvian, Russian and English languages.\n\nAccording to Section 11 of State Language Law, organizers of public events have to provide in Latvian information, which concerns legitimate public interest (defined in Section 2 — public safety, health care \"et cetera\"). The same affects posters, billboards and signboards, according to Section 21. Previously, according to the Law of languages as amended in 1992 (Section 5), organizers of any public event had to provide translation into Latvian in their conferences. An exemption had existed for organizations of ethnic minorities and religious organizations; 1997 Law on Meetings, Processions and Pickets has foreseen free choice of language in meetings, pickets and processions, too (Section 19).\n\nSince the beginning of the 1990s, some Polish language schools were created besides the existing schools with Latvian and Russian language of instruction. Certain schools (e.g., Riga Dubnov Jewish Secondary school, founded in 1989, and Riga Ukrainian Secondary School, founded in 1991, which had originally used Ukrainian as language of instruction, but switched to Latvian in 1993/1994) now include in their curriculum lessons in respective minority languages. The number of Russian schools is decreasing, partly due to natural demographic decline and partly due to emigration, as the following table demonstrates, with some schools with apparent viability closed. \nAs at 2007, there was also increasing number of minority children attending Latvian-language schools.\n\nAccording to Education law, as adopted in 1998, the language of instruction in public secondary schools (Forms 10-12) had to be only Latvian since 2004. This has mostly affected Russian schools, some existing in Latvia without interruption since at least 1789. After wide protests in 2003 and 2004, the law was amended allowing to teach up to 40% of curricula in minority languages (Transition Rules) and allowing orphans to continue their education not only in Latvian, but also in the language he or she began it (Section 56).\n\nIn 2005, one judgment of the Constitutional Court (upon request of ForHRUL, NHP and LSP MPs) has declared unconstitutional the ban of public co-funding for private minority schools, another has declared the proportion \"60+:40\" constitutional.\n\nAccording to the same 1998 Education Law, the tertiary education in public colleges and universities has to be in Latvian only since 1999 (it had to be basically in Latvian since the second year, according to 1992 Law on Languages, Section 11). In fact, there still exist programmes with education in English for foreigners (Riga Technical University) or according to special laws (Riga Graduate School of Law). There is a demand for tertiary education in Russian, too: it is used, for example, at the Baltic International Academy.\n\nIn 2018, President Raimonds Vejonis promulgated the bill banning use of Russian as the language of instruction also at private universities in Latvia. The Latvian parliament passed the legislation on June 21.\n\nIn the medieval Livonian Confederation, Latin and German were the dominant languages of education and administration. German kept this position under subsequent periods of rule by Poland, Sweden and, initially, under the Russian Empire. German was the language of instruction in the first institution of tertiary education on the territory of Latvia (Riga Polytechnicum, founded in 1862). In Latgale, the Polish language gained some influence, beginning from the 16th century.\n\nFrom the mid-19th century, Latvian started to rise in influence. At the end of the 19th century, tsar Alexander III instigated a policy of Russification in non-Russian areas of the Empire. As a result, language of administration, that of Riga Polytechnicum and most schools was changed from German to Russian, and some German toponyms in eastern Latvia were Russianized (e.g., Dünaburg became Dvinsk). After the 1905 revolution, possibilities for schooling in Latvian increased.\n\nThe pro-Bolshevik revolutionary soviet, Iskolat, declared on 4 January 1918 that Latvian should be the primary language of administration on the territory of Latvia.\n\nUnder the short-lived Latvian Socialist Soviet Republic in 1919, Latgalian enjoyed co-equal status with both Latvian and Russian as an official language of administration.\n\nThe Republic of Latvia (founded in 1918) was initially liberal in its language policy: while Latvianizing toponyms (e.g., Dvinsk became Daugavpils), it also allowed Russian and German languages to be used in Parliament along Latvian, acknowledged minorities' rights to learn in schools in their mother tongues and, despite switching public tertiary education to Latvian, did not forbid private post-secondary education in minority languages. State had acknowledged public use of Latgalian. After the 1934 Ulmanis coup d'état the policy changed, and many minority high schools were closed. Particularly hard hit were the Belarusian primary schools, all but 5 of which were closed. Belarusian schoolteachers and other intellectuals in Latvia were suspected of having a pro-Soviet agenda harmful to national security.\n\nDuring World War II, Latvia's German community was mostly moved to Germany, and the Jewish community was destroyed (hit first by the Stalinist deportations in 1941, then by the Holocaust). Due to that, these groups' respective schools disappeared.\n\nIn the postwar Latvian Soviet Socialist Republic, the proportion of Latvian-speaking population decreased due to large losses in World War II and mass deportation, while the Russian-speaking population increased due to the presence of military forces and mass immigration of labour to implement the Soviet Union's industrialization policy (still, due to low birth rate, the population of Latvia had grown by 27.4% between 1959 and 1989 censuses, while that of the whole USSR — by 36.8%). Consequently, the use of Russian increased and it started to dominate in the areas integrated on a federal level (state security, railway etc.). As concerns tertiary education, in some faculties, the language of instruction was only Latvian, in some, only Russian; in some there were two language \"streams\". Under Stalinism, Polish schools were closed and after Arvīds Pelše's 1959 victory over the \"national communists\" (Eduards Berklavs \"et al.\"), the last Latgalian newspaper was closed.\n\nLatvian was declared the state language of the Latvian SSR by a decree of the republican Supreme Soviet on 6 October 1988. Nevertheless, citizens could still choose to communicate with state authorities in Russian, and all correspondence with the USSR's federal bodies was to be in Russian.\n\nIn the first post-Soviet census in 2000, 1,311,093 persons in Latvia reported Latvian as their mother tongue, representing the vast majority of the estimated 1.5 million Latvian speakers worldwide.\n\nIn the year 2000, Livonian was a moribund language spoken by some 35 people, of whom only 10 were fluent. In the first decade of the 21st century, it was estimated that Livonian was the native tongue of 4 people in Latvia, all of whom were older than 70.\nGrizelda Kristiņa, the last native speaker of Livonian, died on June 2, 2013.\n\nLatvia's current territory is a close approximation to the range of Latvian habitation since the Latvian people emerged. As such, Latvian and Livonian are native only to Latvia.\n\nIn the 2000 census, 891,451 respondents listed Russian as their mother tongue, representing 37.5% of the total population, whereas Latvian was recorded as the mother tongue for 58.2%. Latvian was spoken as a second language by 20.8% of the population, and 43.7% spoke Russian as a second language. At that time, in age groups up to 10–14 years, a greater proportion of Russians could speak Latvian than ethnic Latvians could speak Russian. In age groups over 15 years, however, more Latvians expressed proficiency in Russian than vice versa. In total, 71% of ethnic Latvians said they could speak Russian, and 52% of Russians could speak Latvian.\n\nOf all districts and cities in Latvia, the highest command of Latvian was in Talsi District (98.8%), while the lowest was in Daugavpils (41.4%). In Daugavpils was also the highest percentage of people speaking Russian (95.7%), and in Kuldīga District the lowest (57.6%). There was a similar breakdown with regards to mother tongue: 94.6% in Talsi District and for 11.6% in Daugavpils for Latvian, 80.4% in Daugavpils and for 3.0% in Talsi District for Russian.\n\nIn the previous 1989 census, conducted while Latvia was still part of the USSR, Latvian was reported as the native language for 52.0% of the population, Russian for 42.1%; 62.4% of population could speak Latvian, and 81.6% could speak Russian.\n\nIt should be noted that Latgalian was not considered a language separate from Latvian in any census, whether during the Soviet period, or since the restoration of independence. Therefore, no specific data on the number of its native speakers were available until the 2011 census. Then, 8.8. % of the population indicated they use Latgalian, described as a variety of Latvian.\nOther than native speakers of Latvian and Russian, the numbers of speakers of different mother tongues recorded in the 2000 census were:\n\nIn 1999, the Organization for Security and Co-operation in Europe High Commissioner on National Minorities found Latvia's new language law to be \"essentially in conformity with Latvia's international obligations and commitments\". In 2000, he stated that the government regulations were \"essentially in conformity with both the Law and Latvia's international obligations\", but that \"specific matters will have to be reviewed upon Latvia's anticipated ratification of the Framework Convention for the Protection of National Minorities\". The ratification took place in 2005.\n\nInternational organizations have recommended to Latvia on various occasions to:\n\n\n"}
{"id": "18568", "url": "https://en.wikipedia.org/wiki?curid=18568", "title": "List of algorithms", "text": "List of algorithms\n\nThe following is a list of algorithms along with one-line descriptions for each.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "19376", "url": "https://en.wikipedia.org/wiki?curid=19376", "title": "Materialism", "text": "Materialism\n\nMaterialism is a form of philosophical monism which holds that matter is the fundamental substance in nature, and that all things, including mental aspects and consciousness, are results of material interactions.\n\nIn Idealism, mind and consciousness are first-order realities to which matter is subject and secondary. In philosophical materialism the converse is true. Here mind and consciousness are by-products or epiphenomena of material processes (the biochemistry of the human brain and nervous system, for example) without which they cannot exist. According to this doctrine the material creates and determines consciousness, not vice versa. \n\nMaterialist theories are mainly divided into three groups. Naive materialism identifies the material world with specific elements (e.g. the scheme of the four elements—fire, air, water and earth—devised by the pre-Socratic philosopher Empedocles). Metaphysical materialism examines separated parts of the world in a static, isolated environment. Dialectical materialism adapts the Hegelian dialectic for materialism, examining parts of the world in relation to each other within a dynamic environment.\n\nMaterialism is closely related to physicalism, the view that all that exists is ultimately physical. Philosophical physicalism has evolved from materialism with the discoveries of the physical sciences to incorporate more sophisticated notions of physicality than mere ordinary matter, such as: spacetime, physical energies and forces, dark matter, and so on. Thus the term \"physicalism\" is preferred over \"materialism\" by some, while others use the terms as if they are synonymous.\n\nPhilosophies contradictory to materialism or physicalism include idealism, pluralism, dualism, and other forms of monism.\n\nMaterialism belongs to the class of monist ontology. As such, it is different from ontological theories based on dualism or pluralism. For singular explanations of the phenomenal reality, materialism would be in contrast to idealism, neutral monism, and spiritualism.\n\nDespite the large number of philosophical schools and subtle nuances between many, all philosophies are said to fall into one of two primary categories, which are defined in contrast to each other: idealism and materialism. The basic proposition of these two categories pertains to the nature of reality, and the primary distinction between them is the way they answer two fundamental questions: \"what does reality consist of?\" and \"how does it originate?\" To idealists, spirit or mind or the objects of mind (ideas) are primary, and matter secondary. To materialists, matter is primary, and mind or spirit or ideas are secondary, the product of matter acting upon matter.\n\nThe materialist view is perhaps best understood in its opposition to the doctrines of immaterial substance applied to the mind historically, famously by René Descartes. However, by itself materialism says nothing about how material substance should be characterized. In practice, it is frequently assimilated to one variety of physicalism or another.\n\nMaterialism is often associated with reductionism, according to which the objects or phenomena individuated at one level of description, if they are genuine, must be explicable in terms of the objects or phenomena at some other level of description—typically, at a more reduced level. Non-reductive materialism explicitly rejects this notion, however, taking the material constitution of all particulars to be consistent with the existence of real objects, properties, or phenomena not explicable in the terms canonically used for the basic material constituents. Jerry Fodor influentially argues this view, according to which empirical laws and explanations in \"special sciences\" like psychology or geology are invisible from the perspective of basic physics. A lot of vigorous literature has grown up around the relation between these views.\n\nModern philosophical materialists extend the definition of other scientifically observable entities such as energy, forces, and the curvature of space. However philosophers such as Mary Midgley suggest that the concept of \"matter\" is elusive and poorly defined.\n\nMaterialism typically contrasts with dualism, phenomenalism, idealism, vitalism, and dual-aspect monism. Its materiality can, in some ways, be linked to the concept of determinism, as espoused by Enlightenment thinkers.\n\nDuring the 19th century, Karl Marx and Friedrich Engels extended the concept of materialism to elaborate a \"materialist conception of history\" centered on the roughly empirical world of human activity (practice, including labor) and the institutions created, reproduced, or destroyed by that activity (see materialist conception of history). Later Marxists, such as Vladimir Lenin and Leon Trotsky developed the notion of dialectical materialism which characterized later Marxist philosophy and method.\n\nMaterialism developed, possibly independently, in several geographically separated regions of Eurasia during what Karl Jaspers termed the Axial Age ( 800–200 BC).\n\nIn ancient Indian philosophy, materialism developed around 600 BC with the works of Ajita Kesakambali, Payasi, Kanada, and the proponents of the Cārvāka school of philosophy. Kanada became one of the early proponents of atomism. The Nyaya–Vaisesika school ( 600 BC – 100 BC) developed one of the earliest forms of atomism, though their proofs of God and their positing that consciousness was not material precludes labelling them as materialists. Buddhist atomism and the Jaina school continued the atomic tradition.\n\nAncient Greek atomists like Leucippus, Democritus, and Epicurus prefigure later materialists. The Latin poem \"De Rerum Natura\" by Lucretius (99 BC – c. 55 BC) reflects the mechanistic philosophy of Democritus and Epicurus. According to this view, all that exists is matter and void, and all phenomena result from different motions and conglomerations of base material particles called \"atoms\" (literally: \"indivisibles\"). \"De Rerum Natura\" provides mechanistic explanations for phenomena such as erosion, evaporation, wind, and sound. Famous principles like \"nothing can touch body but body\" first appeared in the works of Lucretius. Democritus and Epicurus however did not hold to a monist ontology since they held to the ontological separation of matter and space i.e. space being \"another kind\" of being, indicating that the definition of \"materialism\" is wider than given scope for in this article.\n\nWang Chong (27 – c. 100 AD) was a Chinese thinker of the early Common Era said to be a materialist.\n\nLater Indian materialist Jayaraashi Bhatta (6th century) in his work \"Tattvopaplavasimha\" (\"The upsetting of all principles\") refuted the Nyaya Sutra epistemology. The materialistic Cārvāka philosophy appears to have died out some time after 1400. When Madhavacharya compiled \"Sarva-darśana-samgraha\" (a digest of all philosophies) in the 14th century, he had no Cārvāka/Lokāyata text to quote from, or even refer to.\n\nIn early 12th-century al-Andalus, the Arabian philosopher, Ibn Tufail (Abubacer), wrote discussions on materialism in his philosophical novel, \"Hayy ibn Yaqdhan\" (\"Philosophus Autodidactus\"), while vaguely foreshadowing the idea of a historical materialism.\n\nThomas Hobbes (1588–1679) and Pierre Gassendi (1592–1665) represented the materialist tradition in opposition to the attempts of René Descartes (1596–1650) to provide the natural sciences with dualist foundations. There followed the materialist and atheist \"abbé\" Jean Meslier (1664–1729) and the works of the French materialists: Julien Offray de La Mettrie, the German-French Baron d'Holbach (1723–1789), Denis Diderot (1713–1784), and other French Enlightenment thinkers. In England John \"Walking\" Stewart (1747–1822) insisted in seeing matter as endowed with a moral dimension had a major impact on the philosophical poetry of William Wordsworth (1770–1850).\n\nIn late modern philosophy, German dialectical materialist and atheist anthropologist Ludwig Feuerbach would signal a new turn in materialism through his book, \"The Essence of Christianity\" (1841), which presented a humanist account of religion as the outward projection of man's inward nature. \n\nAnother notable school of naturalist thought that developed in the middle of the 19th century was German materialism: members included Ludwig Büchner, Jacob Moleschott, and Karl Vogt.\n\nFeuerbach's materialism would later heavily influence Karl Marx, who in the late 19th century elaborated the concept of historical materialism, which is the basis for what Marx and Engels outlined as scientific socialism:\n\nLater, Vladimir Lenin outlined philosophical materialism in his book \"Materialism and Empirio-criticism\", which connected the political conceptions put forth by his opponents to their anti-materialist philosophies. Therein, Lenin attempted to answer questions concerning matter, experience, sensations, space and time, causality, and freedom.\n\nContemporary Continental philosopher Gilles Deleuze has attempted to rework and strengthen classical materialist ideas. Contemporary theorists such as Manuel DeLanda, working with this reinvigorated materialism, have come to be classified as \"new materialist\" in persuasion. New materialism has now become its own specialized subfield of knowledge, with courses being offered on the topic at major universities, as well as numerous conferences, edited collections, and monographs devoted to it. Jane Bennett's book \"Vibrant Matter\" (Duke UP, 2010) has been particularly instrumental in bringing theories of monist ontology and vitalism back into a critical theoretical fold dominated by poststructuralist theories of language and discourse. Scholars such as Mel Y. Chen and Zakiyyah Iman Jackson, however, have critiqued this body of new materialist literature for its neglect in considering the materiality of race and gender in particular. Other scholars such as Hélene Vosters have questioned whether there is anything particularly \"new\" about this so-called \"new materialism\", as Indigenous and other animist ontologies have attested to what might be called the \"vibrancy of matter\" for centuries.\n\nContemporary analytic philosophers—e.g., Daniel Dennett, Willard Van Orman Quine, Donald Davidson, and Jerry Fodor—operate within a broadly physicalist or scientific materialist framework, producing rival accounts of how best to accommodate mind, including functionalism, anomalous monism, identity theory, and so on.\n\nScientific materialism is often synonymous with, and has so far been described, as being a reductive materialism. In recent years, Paul and Patricia Churchland have advocated a radically contrasting position (at least, in regards to certain hypotheses); eliminativist materialism holds that some mental phenomena simply do not exist at all, and that talk of those mental phenomena reflects a totally spurious \"folk psychology\" and introspection illusion. That is, an eliminative materialist might believe that a concept like \"belief\" simply has no basis in fact—the way folk science speaks of demon-caused illnesses would be just one obvious example. Reductive materialism being at one end of a continuum (our theories will \"reduce\" to facts) and eliminative materialism on the other (certain theories will need to be \"eliminated\" in light of new facts), Revisionary materialism is somewhere in the middle.\n\nThe nature and definition of matter—like other key concepts in science and philosophy—have occasioned much debate. Is there a single kind of matter (hyle) which everything is made of, or multiple kinds? Is matter a continuous substance capable of expressing multiple forms (hylomorphism), or a number of discrete, unchanging constituents (atomism)? Does it have intrinsic properties (substance theory), or is it lacking them (prima materia)?\n\nOne challenge to the traditional concept of matter as tangible \"stuff\" came with the rise of field physics in the 19th century. Relativity shows that matter and energy (including the spatially distributed energy of fields) are interchangeable. This enables the ontological view that energy is prima materia and matter is one of its forms. On the other hand, the Standard Model of Particle physics uses quantum field theory to describe all interactions. On this view it could be said that fields are prima materia and the energy is a property of the field.\n\nAccording to the dominant cosmological model, the Lambda-CDM model, less than 5% of the universe's energy density is made up of the \"matter\" described by the Standard Model of Particle Physics, and the majority of the universe is composed of dark matter and dark energy—with little agreement amongst scientists about what these are made of.\n\nWith the advent of quantum physics, some scientists believed the concept of matter had merely changed, while others believed the conventional position could no longer be maintained. For instance Werner Heisenberg said \"The ontology of materialism rested upon the illusion that the kind of existence, the direct 'actuality' of the world around us, can be extrapolated into the atomic range. This extrapolation, however, is impossible... atoms are not things.\" Likewise, some philosophers feel that these dichotomies necessitate a switch from materialism to physicalism. Others use the terms \"materialism\" and \"physicalism\" interchangeably.\n\nThe concept of matter has changed in response to new scientific discoveries. Thus materialism has no definite content independent of the particular theory of matter on which it is based. According to Noam Chomsky, any property can be considered material, if one defines matter such that it has that property.\n\nGeorge Stack distinguishes between materialism and physicalism: \nHowever, not all conceptions of physicalism are tied to verificationist theories of meaning or direct realist accounts of perception. Rather, physicalists believe that no “element of reality” is missing from the mathematical formalism of our best description of the world. “Materialist” physicalists also believe that the formalism describes fields of insentience. In other words, the intrinsic nature of the physical is non-experiential. \n\nRudolf Peierls, a physicist who played a major role in the Manhattan Project, rejected materialism, saying \"The premise that you can describe in terms of physics the whole function of a human being...including knowledge and consciousness, is untenable. There is still something missing.\"\n\nErwin Schrödinger said \"Consciousness cannot be accounted for in physical terms. For consciousness is absolutely fundamental. It cannot be accounted for in terms of anything else\".\n\nWerner Heisenberg, who came up with the uncertainty principle wrote \"The ontology of materialism rested upon the illusion that the kind of existence, the direct ‘actuality’ of the world around us, can be extrapolated into the atomic range. This extrapolation, however, is impossible…Atoms are not things\".\n\nSome 20th century physicists (such as Eugene Wigner and Henry Stapp) and modern day physicists and science writers—such as Paul Davies and John Gribbin—have argued that materialism has been disproven by certain scientific findings in physics, such as quantum mechanics and chaos theory. In 1991, Gribbin and Davies released their book \"The Matter Myth\", the first chapter of which, \"The Death of Materialism\", contained the following passage:\n\nDavies' and Gribbin's objections are shared by proponents of digital physics who view information rather than matter to be fundamental. Famous physicist and proponent of digital physics John Archibald Wheeler wrote \"all matter and all things physical are information-theoretic in origin and this is a participatory universe.\" Their objections were also shared by some founders of quantum theory, such as Max Planck, who wrote:\n\nAccording to Constantin Gutberlet writing in \"Catholic Encyclopedia\" (1911), materialism, defined as \"a philosophical system which regards matter as the only reality in the world [...] denies the existence of God and the soul\". In this view materialism could be perceived incompatible with most world religions. Materialism could be conflated with atheism. However Friedrich Lange wrote in 1892 \"Diderot has not always in the Encyclopaedia expressed his own individual opinion, but it is just as true that at its commencement he had not yet got as far as Atheism and Materialism\".\n\nMost of Hinduism and transcendentalism regards all matter as an illusion called Maya, blinding humans from knowing the truth. Transcendental experiences like the perception of Brahman are considered to destroy the illusion.\n\nJoseph Smith, the founder of the Latter Day Saint movement, taught: \"There is no such thing as immaterial matter. All spirit is matter, but it is more fine or pure, and can only be discerned by purer eyes; We cannot see it; but when our bodies are purified we shall see that it is all matter.\" This spirit element is believed to always have existed and to be co-eternal with God.\n\nKant argued against all three forms of materialism, subjective idealism (which he contrasts with his \"transcendental idealism\") and mind–body dualism. However, Kant also argues that change and time require an enduring substrate, and does so in connection with his refutation of idealism. Postmodern/poststructuralist thinkers also express a skepticism about any all-encompassing metaphysical scheme. Philosopher Mary Midgley, among others, argues that materialism is a self-refuting idea, at least in its eliminative form.\n\nAn argument for idealism, such as those of Hegel and Berkeley, is \"ipso facto\" an argument against materialism. Matter can be argued to be redundant, as in bundle theory, and mind-independent properties can in turn be reduced to subjective percepts. Berkeley presents an example of the latter by pointing out that it is impossible to gather direct evidence of matter, as there is no direct experience of matter; all that is experienced is perception, whether internal or external. As such, the existence of matter can only be assumed from the apparent (perceived) stability of perceptions; it finds absolutely no evidence in direct experience.\n\nIf matter and energy are seen as necessary to explain the physical world, but incapable of explaining mind, dualism results. Emergence, holism, and process philosophy seek to ameliorate the perceived shortcomings of traditional (especially mechanistic) materialism without abandoning materialism entirely.\n\nSome critics object to materialism as part of an overly skeptical, narrow or reductivist approach to theorizing, rather than to the ontological claim that matter is the only substance.\nParticle physicist and Anglican theologian John Polkinghorne objects to what he calls \"promissory materialism\"—claims that materialistic science \"will\" eventually succeed in explaining phenomena it has not so far been able to explain. Polkinghorne prefers \"dual-aspect monism\" to materialism.\n\nSome scientific materialists have been criticized, for example by Noam Chomsky, for failing to provide clear definitions for what constitutes matter, leaving the term \"materialism\" without any definite meaning. Chomsky also states that since the concept of matter may be affected by new scientific discoveries, as has happened in the past, scientific materialists are being dogmatic in assuming the opposite.\n\na. Indeed, it has been noted it is difficult if not impossible to define one category without contrasting it with the other.\n\n"}
{"id": "11489484", "url": "https://en.wikipedia.org/wiki?curid=11489484", "title": "Mississippi Goddam", "text": "Mississippi Goddam\n\n\"Mississippi Goddam\" is a song written and performed by American singer and pianist Nina Simone, who later announced the anthem to be her \"first civil rights song\". It was released on her album \"Nina Simone in Concert\" in 1964. The album was based on recordings of three concerts she gave at Carnegie Hall in 1964. The album was her first release for the Dutch label Philips Records and is indicative of the more political turn her recorded music took during this period. \n\nSimone composed \"Mississippi Goddam\" in less than an hour. Together with \"Four Women\" and \"To Be Young, Gifted and Black\", it is one of her most famous protest songs and self-written compositions.\n\nThe song captures Simone's response to the murder of Medgar Evers in Mississippi; and the 16th Street Baptist Church bombing in Birmingham, Alabama, killing four black children. On the recording she cynically announces the song as \"a show tune, but the show hasn't been written for it yet.\" The song begins jauntily, with a show tune feel, but demonstrates its political focus early on with its refrain \"Alabama's got me so upset, Tennessee's made me lose my rest, and everybody knows about Mississippi \"goddam.\"\" In the song she says: \"Keep on sayin' 'go slow'...to do things gradually would bring more tragedy. Why don't you see it? Why don't you feel it? I don't know, I don't know. You don't have to live next to me, just give me my equality!\"\n\nSimone first performed the song at the Village Gate nightclub in Greenwich Village, and shortly thereafter in March 1964 at Carnegie Hall, in front of a mostly white audience. The Carnegie Hall recording was subsequently released as a single and became an anthem during the Civil Rights Movement. \"Mississippi Goddam\" was banned in several Southern states, ostensibly because of the word \"goddam\" in the title. Boxes of promotional singles sent to radio stations around the country were returned with each record cracked in half.\n\nSimone performed the song in front of 10,000 people at the end of the Selma to Montgomery marches when she and other black activists, including Sammy Davis Jr., James Baldwin and Harry Belafonte crossed police lines.\n\n"}
{"id": "41209344", "url": "https://en.wikipedia.org/wiki?curid=41209344", "title": "Mudutā", "text": "Mudutā\n\nMadutā (Pali) is a Buddhist term translated as \"malleability\", and it is the basis for the following pair of mental factors within the Theravada Abhidharma teachings:\n\nThese two mental factors have the characteristic of the subsiding of rigidity (\"thambha\") in the mental body and consciousness, respectively.\n\nBhikkhu Bodhi states:\n\nNina van Gorkom explains:\n\nThe \"Atthasālinī\" (I, Book I, Part IV, Chapter I, 130) states:\n\n\n"}
{"id": "634094", "url": "https://en.wikipedia.org/wiki?curid=634094", "title": "Nondelegation doctrine", "text": "Nondelegation doctrine\n\nThe doctrine of nondelegation is the theory that one branch of government must not authorize another entity to exercise the power or function which it is constitutionally authorized to exercise itself. It is explicit or implicit in all written constitutions that impose a strict structural separation of powers. It is usually applied in questions of constitutionally improper delegations of powers of any of the three branches of government to either of the other, to the administrative state, or to private entities. Although it is usually constitutional for executive officials to delegate executive powers to executive branch subordinates, there can also be improper delegations of powers within an executive branch.\n\nAustralian federalism does not permit the federal Parliament or Government to delegate its powers to state or territorial parliaments or governments, nor territorial parliaments or governments to delegate their powers to the federal Parliament or Government, but the states parliaments delegate its powers to the federal parliament by means of section 51 subsection (xxxvii) of the Constitution Act 1901.\n\nCanadian federalism does not permit Parliament or the provincial legislatures to delegate their powers to each other.\n\nIn the Federal Government of the United States, the \"nondelegation doctrine\" is the principle that the Congress of the United States, being vested with \"all legislative powers\" by Article One, Section 1 of the United States Constitution, cannot delegate that power to anyone else. However, the Supreme Court ruled in \"J. W. Hampton, Jr. & Co. v. United States\" (1928) that congressional delegation of legislative authority is an implied power of Congress that is constitutional so long as Congress provides an \"intelligible principle\" to guide the executive branch: \"'In determining what Congress may do in seeking assistance from another branch, the extent and character of that assistance must be fixed according to common sense and the inherent necessities of the government co-ordination.' So long as Congress 'shall lay down by legislative act an intelligible principle to which the person or body authorized to [exercise the delegated authority] is directed to conform, such legislative action is not a forbidden delegation of legislative power.'\"\n\nFor example, the Food and Drug Administration (FDA) is an agency in the Executive branch created by Congress with the power to regulate food and drugs in the United States. Congress has given the FDA a broad mandate to ensure the safety of the public and prevent false advertising, but it is up to the agency to assess risks and announce prohibitions on harmful additives, and to determine the process by which actions will be brought based on the same. Similarly, the Internal Revenue Service has been given the responsibility of collecting taxes that are assessed under the Internal Revenue Code. Although Congress has determined the amount of the tax to be assessed, it has delegated to the IRS the authority to determine how such taxes are to be collected. Administrative agencies like these are sometimes referred to as the Fourth Branch of government.\n\nThe origins of the nondelegation doctrine, as interpreted in U.S., can be traced back to, at least, 1690, when John Locke wrote:\n\nThe Legislative transfer the Power of Making Laws to any other hands. For it being but a delegated Power from the People, they, who have it, cannot pass it over to others. . . . And when the people have said, We will submit to rules, and be govern'd by Laws made by such Men, and in such Forms, no Body else can say other Men shall make Laws for them; nor can the people be bound by any Laws but such as are Enacted by those, whom they have Chosen, and Authorised to make Laws for them. The power of the Legislative being derived from the People by a positive voluntary Grant and Institution, can be no other, than what the positive Grant conveyed, which being only to make Laws, and not to make Legislators, the Legislative can have no power to transfer their Authority of making laws, and place it in other hands.\nOne of the earliest cases involving the exact limits of nondelegation was \"Wayman v. Southard\" (1825). Congress had delegated to the courts the power to prescribe judicial procedure; it was contended that Congress had thereby unconstitutionally clothed the judiciary with legislative powers. While Chief Justice John Marshall conceded that the determination of rules of procedure was a legislative function, he distinguished between \"important\" subjects and mere details. Marshall wrote that \"a general provision may be made, and power given to those who are to act under such general provisions, to fill up the details.\" In 1892, the Court in \"Field v. Clark\", 143 U.S. 649, noted \"That congress cannot delegate legislative power to the president is a principle universally recognized as vital to the integrity and maintenance of the system of government ordained by the constitution\" while holding that the tariff-setting authority delegated in the McKinley Act \"was not the making of law,\" but rather empowered the executive branch to serve as a \"mere agent\" of Congress.\n\nDuring the 1930s, Congress provided the executive branch with wide powers to combat the Great Depression. The Supreme Court case of \"Panama Refining v. Ryan\", 293 U.S. 388 (1935) involved the National Industrial Recovery Act, which included a provision granting the President the authority to prohibit the interstate shipment of petroleum in excess of certain quotas. In the \"Panama Refining\" case, however, the Court struck down the provision on the ground that Congress had set \"no criterion to govern the President's course.\"\n\nOther provisions of the National Industrial Recovery Act were also challenged. In \"Schechter Poultry Corp. v. United States\" (1935), the Supreme Court considered a provision which permitted the President to approve trade codes, drafted by the businesses themselves, so as to ensure \"fair competition.\" The Supreme Court found that, since the law sets no explicit guidelines, businesses \"may roam at will and the President may approve or disapprove their proposal as he may see fit.\" Thus, they struck down the relevant provisions of the Recovery Act.\n\nIn the 1989 case \"Mistretta v. United States\", the Court stated that:\n\nOnly rarely has the Supreme Court invalidated laws as violations of the nondelegation doctrine. Exemplifying the Court's legal reasoning on this matter, it ruled in the 1998 case \"Clinton v. City of New York\" that the Line Item Veto Act of 1996, which authorized the President to selectively void portions of appropriation bills, was a violation of the Presentment Clause, which sets forth the formalities governing the passage of legislation. Although the Court noted that the attorneys prosecuting the case had extensively discussed the nondelegation doctrine, the Court declined to consider that question. However, Justice Kennedy, in a concurring opinion, wrote that he would have found the statute to violate the exclusive responsibility for laws to be made by Congress.\n\nThe original Bill of Rights approved by the House of Representatives included a Sixteenth Article, which stated that \"(t)he powers delegated by the Constitution to the government of the United States, shall be exercised as therein appropriated, so that the Legislative shall never exercise the powers vested in the Executive or Judicial; nor the Executive the powers vested in the Legislative or Judicial; nor the Judicial the powers vested in the Legislative or Executive.\" This article was not included in subsequent versions of the Articles or in the final Amendments.\n\n\n"}
{"id": "1791712", "url": "https://en.wikipedia.org/wiki?curid=1791712", "title": "Normality (behavior)", "text": "Normality (behavior)\n\nNormality is a behavior that can be normal for an individual (intrapersonal normality) when it is consistent with the most common behaviour for that person. Normal is also used to describe individual behaviour that conforms to the most common behaviour in society (known as conformity). Definitions of normality vary by person, time, place, and situation – it changes along with changing societal standards and norms. Normal behavior is often only recognized in contrast to abnormality. In its simplest form, normality is seen as good while abnormality is seen as bad. Someone being seen as normal or not normal can have social ramifications, such as being included, excluded or stigmatized by larger society.\n\nNormality has been functionally and differentially defined by a vast number of disciplines, so there is not one single definition.\n\nIn general, 'normal' refers to a lack of significant deviation from the average. The word normal is used in a more narrow sense in mathematics, where a normal distribution describes a population whose characteristics centers around the average or the norm. When looking at a specific behaviour, such as the frequency of lying, a researcher may use a Gaussian bell curve to plot all reactions, and a normal reaction would be within one standard deviation, or the most average 68.3%. However, this mathematical model only holds for one particular trait at a time, since, for example, the probability of a single individual being within one standard deviation for 36 independent variables would be one in a million. In statistics, normal is often arbitrarily considered anything that falls within about 1.96 standard deviations of the mean, or the most average 95% (see 1.96). The probability of an individual being within 1.96 standard deviations for 269 independent variables is approximately one in a million. For only 59 independent variables, the probability is just under 5%. Under this definition of normal, it is abnormal to be normal for 59 independent variables.\n\nThe French sociologist Émile Durkheim indicated in his \"Rules of the Sociological Method\" that it was necessary for the sociological method to offer parameters to distinguish normality from pathology or abnormality. He suggested that behaviors or \"social facts\" which are present in the majority of cases are normal, and exceptions to that behavior indicate pathology. Durkheim's model of normality further explained that the most frequent or general behaviors, and thus the most normal behaviors, will persist through transition periods in society. Crime, for instance, exists under every society through every time period, and so should be considered normal. There is a two-fold version of normality; behaviors considered normal on a societal level may still be considered pathological on an individual level. On the individual level, people who violate social norms, such as criminals, will invite a punishment from others in the society.\n\nIndividuals' behaviours are guided by what they perceive to be society's expectations and their peers' norms. People measure the appropriateness of their actions by how far away they are from those social norms. However, what is perceived as the norm may or may not actually be the most common behaviour. In some cases of pluralistic ignorance, most people falsely believe the social norm is one thing, but in fact very few people hold that view.\n\nWhen people are made more aware of a social norm, particularly a descriptive norm (a norm describing what is done), their behaviour changes to become closer to that norm. The power of these norms can be harnessed by social norms marketing, where the social norm is advertised to people in an attempt to stop extreme behaviour, such as binge drinking. However, people at the other extreme (very little alcohol consumption) are equally likely to change their behaviour to become closer to the norm, in this case by increasing alcohol consumption. Instead of using descriptive norms, more effective social norms marketing may use injunctive norms. Instead of describing what behaviour is most commonly done, an injunctive norm is what is approved or disapproved of by society. When individuals become aware of the injunctive norm, only the extremes will change their behaviour (by decreasing alcohol consumption) without the boomerang effect of under-indulgers increasing their drinking.\n\nThe social norms that guide people are not always normal for everyone. Behaviours that are abnormal for most people may be considered normal for a subgroup or subculture. For example, normal college student behaviour may be to party and drink alcohol, but for a subculture of religious students, normal behaviour may be to go to church and pursue religion related activities. Subcultures may actively reject \"normal\" behaviour, instead replacing society norms with their own.\n\nA disharmony exists between a virtual identity of the self and a real social identity, whether it be in the form of a trait or attribute. If a person does not have this disharmony, then he or she is described as normal. A virtual identity can take many definitions, but in this case a virtual identity is the identity that persons mentally create that conforms to societal standards and norms, it may not represent how they actually are, but it represents what they believe is the typical \"normal\" person. A real social identity is the identity that persons actually have in their society or is perceived, by themselves or others, to have. If these two identities have differences between each other, there is said to be disharmony. Individuals may monitor and adapt their behaviour in terms of others' expected perceptions of the individual, which is described by the social psychology theory of self-presentation. In this sense, normality exists based on societal norms, and whether someone is normal is entirely up to how he or she views him- or herself in contrast to how society views him or her. While trying to define and quantify normality is a good start, all definitions confront the problem of whether we are even describing an idea that even exists since there are so many different ways of viewing the concept.\n\nMany difficulties arise in measuring normal behaviors – biologists come across parallel issues when defining normality. One complication which arises regards whether 'normality' is used correctly in everyday language. People say \"This heart is abnormal\" if only a portion of it is not working correctly, yet it may be inaccurate to include the entirety of the heart under the abnormal description. There can be a difference between the normality of the structure and function of a body part. Similarly, a behavioural pattern may not conform to social norms, but still be effective and non-problematic for that individual. Where there is a dichotomy between appearance and function of a behaviour, it may be difficult to measure its normality. This is applicable when trying to diagnose a pathology and is addressed in the DSM.\n\nWhat is viewed as normal can change dependent on both timeframe and environment. Normality can be viewed as \"an endless process of man's self-creation and his reshaping of the world\". Within this idea, it is possible to surmise that normality is not an all-encompassing term, but simply a relative term based around a current trend in time. With statistics, this is likened to the thought that if the data gathered provides a mean and standard deviation, over time these data that predict \"normalness\" start to predict or dictate it less and less since the social idea of normality is dynamic. This is shown in studies done on behavior in psychology and sociology where behavior in mating rituals or religious rituals can change within a century in humans, showing that the \"normal\" way that these rituals are performed shift and a new procedure becomes the normal one.\n\nAs another example, understandings of what is normal sexual behaviour varies greatly across time and place. In many countries, perceptions on sexuality are largely becoming more liberal, especially views on the normality of masturbation and homosexuality. Social understanding on normal sexual behaviour also varies greatly country by country – countries can be divided into categories of how they approach sexual normality, as conservative, homosexual-permissive, or liberal. The United States, Ireland, and Poland have more conservative social understanding of sexuality among university students, while Scandinavian students consider a wider variety of sexual acts as normal. Although some attempts have been made to define sexual acts as normal, abnormal, or indeterminate, these definitions are time-sensitive. Gayle Rubin's 1980s model of sexual 'normality' was comprehensive at the time but has since become outdated as society has liberalized.\n\nSince normality shifts in time and environment, the mean and standard deviation are only useful for describing normality from the environment from which they are collected.\n\nMost definitions of normality consider interpersonal normality, the comparison between many different individual's behaviours to distinguish normality from abnormality. Intrapersonal normality looks at what is normal behaviour for one particular person (consistency within a person) and would be expected to vary person-to-person. A mathematical model of normality could still be used for intrapersonal normality, by taking a sample of many different occurrences of behaviour from one person over time. Also like interpersonal normality, intrapersonal normality may change over time, due to changes in the individual as they age and due to changes in society (since society's view of normality influences individual peoples' behaviour).\n\nIt is most comfortable for people to engage in behaviour which conforms to their own personal habitual norms. When things go wrong, people are more likely to attribute the negative outcome on any abnormal behaviour leading up to the mishap. After a car crash, people may say \"if only I didn't leave work early\", blaming the crash on their actions which were not normal. This counterfactual thinking particularly associates abnormal behaviour with negative outcomes.\n\nIn medicine, behavioral normality pertains to a patient's mental condition aligning with that of a model, healthy patient. A person without any mental illness is considered a normal patient, whereas a person with a mental disability or illness is viewed as abnormal. These normals and abnormals in the context of mental health subsequently create negative stigmatic perceptions towards individuals with mental illness. The Brain & Behavior Research Foundation stated that \"an estimated 26.2 percent of Americans ages 18 and older – about 1 in 4 adults – suffer from one or more of (several) disorders in a given year\". Though the population of American individuals living with mental illness is not as small of a minority as commonly perceived, it is considered abnormal nonetheless, therefore the subject of discrimination and abuse such as violent therapies, punishments, or labeling for life by the normal, healthy majority. The CDC reported that \"cluster[s] of negative attitudes and beliefs motivate the general public to fear, reject, avoid, and discriminate against people with mental illnesses\". In continuum, the resources available to those who suffer from such illness are limited, and government support is constantly being cut from programs that help individuals living with mental illness live more comfortable, accommodative, happier lives.\n\nHebbian associative learning and memory maintenance depends on synaptic normalization mechanisms to prevent synaptic runaway. Where synaptic runaway describes overcrowding of dendritic associations, which reduce sensory or behavioural acuteness proportional to the level of synaptic runaway. Synaptic/neuronal normalization refers to synaptic competition, where the prosper of one synapse may weakening the efficacy of other nearby surrounding synapses with redundant neurotransmission.\n\nAnimal dendritic density greatly increases throughout waking hours despite intrinsic normalization mechanisms as described as above. The growth rate of synaptic density is not sustained in a cumulative fashion. Without a pruning state, the signal to noise ratio of CNS mechanism would not be able to operate with maximum effectiveness, and learning would be detrimental to animal survival. Neuronal and synaptic normalization mechanisms must operate so positive association feedback loops to not become rampant while constantly processing new environmental information.\n\nSome researchers speculate that the slow oscillation (nREM) cycles of animal sleep constitute an essential 're-normalization' phase. The re-normalization occurs from cortical large amplitude brain rhythm, in the low delta range (0.5–2 Hz), synaptically downscaling the associations from the wakeful learning state. Only the strongest associations survive the pruning from this phase. This allows retention of salient information coding from the previous day, but also allows more cortical space and energy distribution to continue effective learning subsequently after a slow-wave oscillation episode of sleep.\n\nAlso, organisms tend to have a normal biological developmental pathway as a central nervous system ages and/or learns. Deviations for a species' normal development frequently will result in behaviour dysfunction, or death, of that organism.\n\nWhen people do not conform to the normal standard, they are often labelled as sick, disabled, abnormal, or unusual, which can lead to marginalization or stigmatization. Most people want to be normal and strive to be perceived as such, so that they can relate to society at large. Without having things in common with the general population, people may feel isolated among society. The abnormal person feels like they have less in common with the normal population, and others have difficulty relating to things that they have not experienced themselves. Additionally, abnormality may make others uncomfortable, further separating the abnormally labelled individual.\n\nSince being normal is generally considered an ideal, there is often pressure from external sources to conform to normality, as well as pressure from people's intrinsic desire to feel included. For example, families and the medical community will try to help disabled people live a normal life. However, the pressure to appear normal, while actually having some deviation, creates a conflict – sometimes someone will appear normal, while actually experiencing the world differently or struggling. When abnormality makes society feel uncomfortable, it is the exceptional person themselves who will laugh it off to relieve social tension. A disabled person is given normal freedoms, but may not be able to show negative emotions. Lastly, society's rejection of deviance and the pressure to normalize may cause shame in some individuals. Abnormalities may not be included in an individual's sense of identity, especially if they are unwelcome abnormalities.\n\nWhen an individual's abnormality is labelled as a pathology, it is possible for that person to take on both elements of the sick role or the stigmatization that follows some illnesses. Mental illness, in particular, is largely misunderstood by the population and often overwhelms others' impression of the patient.\n\nApplying normality clinically depends on the field and situation a practitioner is in. In the broadest sense, clinical normality is the idea of uniformity of physical and psychological functioning across individuals. Normality, and abnormality, can be characterized statistically.\nRelated to the previous definition, statistically normality is usually defined it in terms of a normal distribution curve, with the so-called 'normal zone' commonly accounting for 95.45% percent of all the data. The remaining 4.55% will lie split outside of two standard deviations from the mean. Thus any variable case that lies outside of two deviations from the mean would be considered abnormal. However, the critical value of such statistical judgments may be subjectively altered to a less conservative estimate. It is in fact normal for a population to have a proportion of abnormals. The presence of abnormals is important because it is necessary to define what 'normal' is, as normality is a relative concept. So at a group, or macro level, of analysis; abnormalities are normal given a demographic survey, but at an individual level abnormal individuals are seen as being deviant in someway that needs to be corrected.\nStatistical normality is important in determining demographic pathologies. When a variable rate, such as virus spread within a human population, exceeds its normal infection rate then preventative or emergency measures can be introduced. \nIt is often impractical to apply statistical normality to diagnose individuals. Symptom normality is the current, and assumed most effective, way to assess patient pathology. Psychiatric normality, in a broad sense, states that psychopathology are disorders that are deviations from normality.\n\nNormality, as a relative concept, is intrinsically involved with contextual elements. As a result, clinical disorder classification has particular challenges in discretely diagnosing 'normal' constitutions from true disorders. The \"Diagnostic and Statistical Manual of Mental Disorders\" (DSM) is the psychiatric profession's official classification manual of mental disorders since its first published version DSM-I in by the APA, 1952. As the DSM evolved into its current version, DSM-5 in late 2013, there have been numerous conflicts in proposed classification between mental illness and normal mentality. Dr. Allen Frances, who chaired the task force for content in the DSM-IV and DSM-IV-TR even wrote a scathing indictment of the pressures incumbent on the definition of \"normal\" relative to psychological constructs and mental illness in his book, \"Saving Normal\".\n\nMost of this difficulty stems from the DSM's ambiguity of natural contextual stressor reactions versus individual dysfunction. There are some key progressions along the DSM history that have attempted to integrate some aspects of normality into proper diagnosis classification. As a diagnostic manual for classification of abnormalities, all DSMs have been biased towards classifying symptoms as disorders by emphasizing symptomatic singularity. The result is an encompassing misdiagnosis of possible normal symptoms, appropriate as contextually derived.\n\nThe second edition of the DSM, DSM-II, could not be effectively applied because of its vague descriptive nature. Psychodynamic etiology was a strong theme in classifying mental illnesses. The applied definitions became idiosyncratic, stressing individual unconscious roots. This made applying the DSM unreliable across psychiatrists. No distinction between abnormal to normal was established.\n\nEvidence of the classification ambiguity were punctated by the Rosenhan experiment of 1972. This experiment demonstrated that the methodology of psychiatric diagnosis could not effectively distinguish normal from disordered mentalities. DSM-II labelled 'excessive' behavioral and emotional response as an index of abnormal mental wellness to diagnose some particular disorders. 'Excessiveness' of a reaction implied alternative normal behaviour which would have to include a situational factor in evaluation. As an example; a year of intense grief from the death of a spouse may be a normal appropriate response. To have intense grief for twenty years would be indicative of a mental disorder. As well, to grieve intensely over the loss of a sock would also not be considered normal responsiveness and indicate a mental disorder. The consideration of proportionality to stimuli was a perceived strength in psychiatric diagnosis for the DSM-II.\n\nAnother characteristic of the DSM-II systemization was that it classified homosexuality as a mental disorder. Thus, homosexuality was psychiatrically defined a pathological deviation from 'normal' sexual development. Homosexuality was later replaced in the 7th printing of DSM-II, instead categorized as a 'Sexual orientation disturbance'. The intent was to have a label that applied only to those homosexual individuals who were bothered by their sexual orientation. In this manner homosexuality would not be viewed as an atypical illness. Only if it was distressing would homosexuality be classified as a mental illness. However, the DMS-II did not explicitly state that any homosexuality was normal either. This stigma lasted into DSM-III until it was reformed entirely from DSM classifications in 1987.\n\nDSM-III was a best attempt to credit psychiatry as a scientific discipline, from the opprobrium resulting from DSM-II. A reduction in the psychodynamic etiologies of DSM-II spilled over into a reduction symptom etiology altogether. Thus, DSM-III was a specific set of definitions for mental illnesses, and entities more suited to diagnostic psychiatry, but which annexed response proportionality as a classification factor. The product was that all symptoms, whether normal proportional response or inappropriate pathological tendencies, could both be treated as potential signs of mental illness.\n\nDSM-IV explicitly distinguishes mental disorders and non-disordered conditions. A non-disordered condition results from, and is perpetuated by, social stressors. Included in DSM-IV's classification is that a mental disorder \"must not be merely an expectable and culturally sanctioned response to a particular event, for example, the death of a loved one. Whatever its original cause, it must currently be considered a manifestation of a behavioral, psychological, or biological dysfunction in the individual\" (American Psychiatric Association 2000:xxxi)\nThis had supposedly injected normality consideration back into the DSM, from its removal from DSM-II. However, it has been speculated that DSM-IV still does not escape the problems DSM-III faced, where psychiatric diagnoses still include symptoms of expectable responses to stressful circumstances to be signs of disorders, along with symptoms that are individual dysfunctions. The example set by DSM-III, for principally symptom-based disorder classification, has been integrated as the norm of mental diagnostic practice.\n\nThe DSM-5 was released in the second half of 2013. It has significant differences from DSM IV-TR, including the removal of the multi-axial classifications and reconfiguring the Asperger's/autistic spectrum classifications.\n\nSince the advent of DSM-III, the subsequent editions of the DSM have all included a heavy symptom based pathology diagnosis system. Although there have been some attempts to incorporate environmental factors into mental and behavioural diagnostics, many practitioners and scientists believe that the most recent DSM's are misused. The symptom bias makes diagnosing quick and easier allowing for practitioners to increase their clientele because symptoms can be easier to classify and deal with than dealing with life or event histories which have evoked what may be a temporary and normal mental state in reaction to a patients environmental circumstances. \nThe easy-to-use manual not only has increased the perceived need for more mental health care, stimulating funding for mental health care facilities, but also has had a global impact on marketing strategies. Many pharmaceutical commercial ads list symptoms such as fatigue, depression, or anxiety. However, such symptoms are not necessarily abnormal, and are appropriate responses to such occurrences as the loss of a loved one. The targets of such ads in such cases do not need medication, and can naturally overcome their grief, but with such an advertising strategy pharmaceutical companies can greatly expand their marketing.\n\n\n"}
{"id": "53552374", "url": "https://en.wikipedia.org/wiki?curid=53552374", "title": "On Weights and Measures", "text": "On Weights and Measures\n\nOn Weights and Measures is a historical, lexical, metrological, and geographical treatise compiled in 392 CE in Constantia by Epiphanius of Salamis (c. 315–403). The greater part of the work is devoted to a discussion on ancient Greek and Roman weights and measures.\n\nThe composition was written at the request of a Persian priest, sent to Epiphanius by letter from the Roman emperor in Constantinople. Although five fragments of an early Greek version are known to exist, with one entitled περὶ μέτρων καὶ στάθμων (\"On Weights & Measures\"), added by a later hand, this Syriac version is the only complete copy that has survived. Partial translations in Armenian and Georgian are also known to exist. Its modern title belies its content, as the work also contains important historical anecdotes about people and places not written about elsewhere.\n\nTwo manuscripts of \"On Weights and Measures\", written in Syriac on parchment, are preserved at the British Museum in London. The older was found in Egypt and, according to the colophon, was written in the Seleucid era, in \"nine-hundred and sixty-[...]\" (with the last digit effaced, meaning, that it was written between the years 649 CE–659 CE). The younger manuscript is designated \"Or. Add. 14620\".\n\nThe first to attempt a modern publication of Epiphanius' work was Paul de Lagarde in 1880, who reconstructed the original Syriac text by exchanging it with Hebrew characters, and who had earlier published excerpts from several of the Greek fragments treating on weights and measures in his \"Symmicta\". In 1973, a critical edition of the Greek text was published by E.D. Moutsoulas in \"Theologia\". \n\nIn folios [54b–55c], Hadrian's journey and arrival in the East is dated \"47 years after the destruction of Jerusalem.\"\nIn folios [47a–49a]; [51d–52a]; [56d–57b] Epiphanius names four major translations of the Hebrew Bible, made in the Greek tongue: the LXX made by the seventy-two translators, another by Aquila of Pontus, one by Theodotion, and yet another by Symmachus. A fifth Greek translation was discovered in wine jars in Jericho, and a sixth in Nicopolis near Actium. Afterwards, Origen arranged six columns of the extant Greek translations and two of the Hebrew side by side, naming it the Hexapla. Epiphanius expands his description of the translation of the seventy-two translators (known as the Septuagint) and how they were assigned thirty-six cells, two to each cell, on the Pharian island. Two translators translated the Book of Genesis, another two the Book of Exodus, another two the Book of Leviticus, and so forth, until the entire 22 canonical books of the Hebrew Bible had all been translated into the Greek tongue. The seventy-two translators were drawn from the twelve tribes of Israel, six men to each tribe who were skilled in the Greek language.\nIn folios [49a–50a] Epiphanius gives a description of the canonical books of the Hebrew Bible and translations made of the same. In his day, he notes that the Scroll of Ruth and the Book of Judges were joined together, and considered as one book. So, too, the Books of Ezra and Nehemiah were joined, and considered as one book, as were First and Second Chronicles (\"Paraleipomena\") considered as one book, as were the First and Second Samuel (\"Book of First and Second Kingdoms\") considered as one book, and the First and Second Kings (\"Book of Third and Fourth Kingdoms\") considered as one book.\nIn spite of Epiphanius' interest in Jewish themes, his narrative often takes on a distorted and stereotypical view of Judaism, based on the dogmas of his persuasion. Still, he is an invaluable source on the lives of people and places that figure highly in Jewish lore. In folios [54a–55c]; [55c–55d] Epiphanius treats on the lives of two prominent persons who became proselytes to the Jewish religion; the one Aquila (known also as Onkelos) who was a relation of Hadrian, and whom he made the overseer of Jerusalem's rebuilding around 115 CE. The other person of interest who is described by him is Symmachus, also known as \"Sūmkos\" () in rabbinic literature. Symmachus is mentioned as belonging originally to the Samaritan nation, and is said to have converted to Judaism during the reign of Verus. He subsequently underwent a second circumcision and became a disciple of Rabbi Meir. Symmachus belonged to the fifth generation (165–200 CE) of Rabbinical teachers referred to in the text of the Mishnah. The Emperor Hadrian is said to have passed through Palestine while \"en route\" to Egypt, some 47 years after the destruction of Jerusalem.\nFolios [61d–73b] contain a treatise on the known weights and measures used in his day among the Hebrews, the Greeks and the Romans. He states the equivalent weights for the \"kab\" (\"cab\"), \"kor\", the \"lethekh\" (Lethek), \"homer\", \"bath\", \"modius\" (Hebrew: \"seah\" = lit. \"measure\"), and \"mina\" (Hebrew: \"maneh\"), among others. In folios [62b–62c] Epiphanius distinguishes between \"a handful\" () in I Kings 17:12 and \"a handful\" () in Exodus 9:8 and Leviticus 16:12; in the former case it refers to only one handful, but in latter cases it refers to \"a measure of two handfuls.\"\nIn folios [73b–75a] Epiphanius gives the names of several cities and places of renown, both in his time and in ancient times, such as: Mount Ararat (§ 61), Aṭaṭ (§ 62), or what is known as the \"threshing floor of the thorn bush\" (), and whose description echoes that of Rashi's commentary on Genesis 50:10, Abarim (§ 63); Aviʿazar (§ 68), or what is \"Eḇen haʿezer\" of I Samuel 4:1, said to be \"fourteen [Roman] miles distant east and north of Eleutheropolis, in a valley\"; Carmel(§ 77); Carmel of the sea (§ 78); Akko (§ 76); Anathoth (§ 66); Azekah (§ 64) - a city in whose time was called Ḥwarta; Bethel (§ 73); Ophrah (§ 67); Carthage (§ 79) - where the Canaanites had migrated from Phoenicia and who were called in his day \"Bizakanoi\" (scattered people); Rekem (§ 71), Jaffa (§ 75), Jerusalem(§ 74), \"et al.\"\n\n\n\nThe regnal years of the Caesars as stated by Epiphanius differ slightly in some places from the extant Greek versions. With respect to events in Rome after the reign of Pertinax, both Epiphanius and Jerome do not mention the ascension of Didius Julianus after the assassination of Pertinax, but write only that Severus succeeded him. This may have been because they did not consider his 9-week reign, which he obtained through usurpation, to be legitimate. Similarly, Epiphanius does not mention the ascension of Aemilian. It can be adduced from Jerome's \"Chronicon\" that Aemilian, who \"caused a revolt in Moesia,\" was never officially confirmed by the Senate in Rome. Epiphanius' method of recording the regnal years from Augustus to Hadrian, with his pinpoint recollection of the number of months and days to each reign, can be said to be accurate, based on Josephus' own testimony about himself, saying that he was aged 56 in the 13th year of the reign of Caesar Domitian, and that he (Josephus) was born in the 1st year of Caesar Gaius. Using Epiphanius' chronology, the years are indeed collected as 56. By comparison, the span of years in Suetonius' \"De vita Caesarum\" (Lives of the Caesars), which gives 14 years for Claudius and 15 years for Nero, the same time frame would span a period of some 58 years!\n\n\n"}
{"id": "28873086", "url": "https://en.wikipedia.org/wiki?curid=28873086", "title": "Open ModelSphere", "text": "Open ModelSphere\n\nOpen ModelSphere is a data, process and UML modeling tool written in Java and distributed as free software under the GPL License. It provides support for forward and reverse engineering between UML and relational schemas.\n\nOpen ModelSphere has SILVERRUN PerfectO for an ancestor, proprietary software developed by Computer Systems Advisers and released in 1996. PerfectO was part of the SILVERRUN suite of modeling tools, known in the modeling community since the 1990s; PerfectO was used to support object-oriented modeling (limited to class modeling at that time) and object-relational modeling.\n\nIn 1998, PerfectO was translated into Java resulting in SILVERRUN-JD (Java Designer). With the addition of relational data modeling, the product was renamed to SILVERRUN ModelSphere and released in 2002. Later on, more features were added including support for business process modeling, conceptual data modeling, and UML diagramming.\n\nIn September 2008, Grandite released ModelSphere's core application as an open source product based on the GNU Public License version 3. Its development environment was hosted on JavaForge which shut down March 31, 2016. An empty project is hosted on SourceForge which was registered on Sep 16th, 2008 and last updated on Mar 27th, 2013. No releases, files, or source code are available on the SourceForge project page as of Oct 18th, 2016.\n\nOpen ModelSphere works with\n\n\nJanuary 6, 2016: Open ModelSphere 3.2.2\n\nNovember 2009: Open ModelSphere 3.1, featuring\n\nSeptember 2008: Open ModelSphere 3.0\n\nJuly 2002: SILVERRUN ModelSphere 2.0\n\nFebruary 2002: SILVERRUN ModelSphere 1.0\n\n\n"}
{"id": "57989", "url": "https://en.wikipedia.org/wiki?curid=57989", "title": "PLUR", "text": "PLUR\n\nPeace Love Unity Respect, commonly shortened to PLUR, is a set of principles that is associated with the rave culture. Originating from early online discussions about rave culture conducted on Usenet, it has been commonly used since the early 1990s when it became commonplace in club flyers and especially on club paraphernalia advertising underground outdoor trance parties. It has since expanded to the larger rave dance music culture as well.\n\nIt may be interpreted as the essential philosophy of life for ravers and clubbers, at least insomuch as it relates to interpersonal relationships, with basic directions on how people are expected to behave at a rave gathering. This universalist philosophy underpinning the tribal dance culture which began circling the globe with the rise of the internet, theoretically takes precedence over any chemical or musical aspects of the rave scene. Raves represent a modern ritualistic experience, promoting a strong communal sense, where PLUR is considered an ideology.\n\n\nPLUR is an aggregation of ideas synonymous with the earlier hippy and hip hop culture, with the peace movement being an essential starting point to any be-in encounter or rave. \n\nUse of the term 'PLUR' dates to the late 1980s and early 1990s rave scene in the UK which incorporated house and acid house music that originated in Chicago during the 1980s. The term began as an informal discussion on usenet lists alt.rave and alt.culture.zippies. SF-raves mailing list archived at hyper-real also noted the use of the term, and there is a flyer archive which might contain evidence of the existence of the term.\n\nOne of the earliest uses of the term outside the internet, mostly anecdotal, appears to be DJ Frankie Bones in June 1993. Supposedly in response to a fight that broke out at one of his epic Storm Raves in Brooklyn, New York, Bones is said to have got on the microphone and yelled: \"If you don't start showing some peace, love, and unity, I'll break your faces.\"\n\nLater incarnations and variations of PLUR can be seen in the adoption of Pronoia and also Ubuntu, with PLUR and Pronoia often being interchangeable terms, depending upon one's company. \n\nSeveral other variations on the same four words, but in a different order (e.g. LURP), have been proposed. However, none of these are commonly used.\n\n"}
{"id": "50319719", "url": "https://en.wikipedia.org/wiki?curid=50319719", "title": "Pafama (Seissel)", "text": "Pafama (Seissel)\n\nPafama, short for Papierfarbenmalerei, is a 1922 painting by Croatian painter Josip Seissel. This painting is the first known abstract composition in Croatian art. It is currently in the collection of the Museum of Contemporary Art, Zagreb.\n\nWord \"Pafama\" is an abbreviation of the German \"PApier-FArben-MAlerei\", () which is a word from zenitism vocabulary, meaning that the artist focuses on the painting process and materials, a radical approach at the beginning of the third decade of the 20th century.\n\nThis is a square painting of small dimensions whose center is dominated by multi-colored geometric elements on a black background. It is based on the principles of suprematism and is reduced to primary geometric shapes - triangle, rectangle, polygon and cone. In addition, it is reduced to the primary colors - red, yellow, blue and their derivatives which are projected on a black background.\n\nThe painting is characterized by structural imaging techniques and is synonymous with European avant-garde art at the time. The painting marks the appearance of avant-garde art in Croatia and leaves lasting effects on the Croatian art in the 20th century and especially in the 1950s and 1960s, when neo-constructivism trends were formed. Its size is 17x17 cm.\n"}
{"id": "215539", "url": "https://en.wikipedia.org/wiki?curid=215539", "title": "Pascal's Wager", "text": "Pascal's Wager\n\nPascal's Wager is an argument in philosophy presented by the seventeenth-century French philosopher, mathematician and physicist Blaise Pascal (1623–1662). It posits that humans bet with their lives that God either exists or does not.\n\nPascal argues that a rational person should live as though God exists and seek to believe in God. If God does not actually exist, such a person will have only a finite loss (some pleasures, luxury, etc.), whereas he stands to receive infinite gains (as represented by eternity in Heaven) and avoid infinite losses (eternity in Hell).\n\nPascal's Wager was based on the idea of the Christian God, though similar arguments have occurred in other religious traditions. The original wager was set out in section 233 of Pascal's posthumously published \"Pensées\" (\"Thoughts\"). These previously unpublished notes were assembled to form an incomplete treatise on Christian apologetics.\n\nHistorically, Pascal's Wager was groundbreaking because it charted new territory in probability theory, marked the first formal use of decision theory, and anticipated future philosophies such as existentialism, pragmatism and voluntarism.\n\nThe Wager uses the following logic (excerpts from \"Pensées\", part III, §233):\n\n\nPascal asks the reader to analyze humankind's position, where our actions can be enormously consequential but our understanding of those consequences is flawed. While we can discern a great deal through reason, we are ultimately forced to gamble. Pascal cites a number of distinct areas of uncertainty in human life:\nPascal describes humanity as a finite being trapped within an incomprehensible infinity, briefly thrust into being from non-being, with no explanation of \"Why?\" or \"What?\" or \"How?\" On Pascal's view, human finitude constrains our ability to reliably achieve truth.\n\nGiven that reason alone cannot determine whether God exists, Pascal concludes that this question functions like a coin toss. However, even if we do not know the outcome of this coin toss, we must base our actions on some expectation about the outcome. We must decide whether to live as though God exists, or whether to live as though God does not exist, even though we may be mistaken in either case.\n\nIn Pascal's assessment, participation in this wager is not optional. Merely by existing in a state of uncertainty, we are forced to choose between the available courses of action for practical purposes.\n\nThe \"Pensées\" passage on Pascal's Wager is as follows:\n\nPascal begins by painting a situation where both the existence and non-existence of God are impossible to prove by human reason. So, supposing that reason cannot determine the truth between the two options, one must \"wager\" by weighing the possible consequences. Pascal's assumption is that, when it comes to making the decision, no one can refuse to participate; withholding assent is impossible because we are already \"embarked\", effectively living out the choice.\n\nWe only have two things to stake, our \"reason\" and our \"happiness\". Pascal considers that if there is \"\"equal\" risk of loss and gain\" (i.e. a coin toss), then human reason is powerless to address the question of whether God exists. That being the case, then human reason can only decide the question according to possible resulting happiness of the decision, weighing the gain and loss in believing that God exists and likewise in believing that God does not exist.\n\nHe points out that if a wager was between the equal chance of gaining two lifetimes of happiness and gaining nothing, then a person would be a fool to bet on the latter. The same would go if it was three lifetimes of happiness versus nothing. He then argues that it is simply unconscionable by comparison to bet against an eternal life of happiness for the possibility of gaining nothing. The wise decision is to wager that God exists, since \"If you gain, you gain all; if you lose, you lose nothing\", meaning one can gain eternal life if God exists, but if not, one will be no worse off in death than if one had not believed. On the other hand, if you bet against God, win or lose, you either gain nothing or lose everything. You are either unavoidably annihilated (in which case, nothing matters one way or the other) or lose the opportunity of eternal happiness. In note 194, speaking about those who live apathetically betting against God, he sums up by remarking, \"It is to the glory of religion to have for enemies men so unreasonable...\"\n\nPascal addressed the difficulty that 'reason' and 'rationality' pose to genuine belief by proposing that \"acting as if [one] believed\" could \"cure [one] of unbelief\":\n\nThe possibilities defined by Pascal's Wager can be thought of as a decision under uncertainty with the values of the following decision matrix.\n\nGiven these values, the option of living as if God exists (B) dominates the option of living as if God does not exist (¬B), as long as one assumes a positive probability that God exists. In other words, the expected value gained by choosing B is greater than or equal to that of choosing ¬B.\n\nIn fact, according to decision theory, the only value that matters in the above matrix is the +∞ (infinitely positive). Any matrix of the following type (where f, f, and f are all negative or finite positive numbers) results in (B) as being the only rational decision.\n\nMany criticisms have explained that the wager has been used as a supposed theory of the necessity to believe, although that was never Pascal's intention. As Laurent Thirouin writes: \n\nTo be put at the beginning of Pascal's planned book, the wager was meant to show that logical reasoning cannot support faith or lack thereof, Pascal's intended book was precisely to find other ways to establish the value of faith, an apology for the Christian faith.\n\nCriticism of Pascal's Wager began in his own day, and came from both atheists, who questioned the 'benefits' of a deity whose 'realm' is beyond reason, and the religiously orthodox, who primarily took issue with the wager's deistic and agnostic language. It is criticized for not proving God's existence, the encouragement of false belief, and the problem of which religion and which God should be worshipped. \n\nVoltaire (another prominent French writer of the Enlightenment), a generation after Pascal, rejected the idea that the wager was \"proof of God\" as \"indecent and childish\", adding, \"the interest I have to believe a thing is no proof that such a thing exists\". Pascal, however, did not advance the wager as a proof of God's existence but rather as a necessary pragmatic decision which is \"impossible to avoid\" for any living person. He argued that abstaining from making a wager is not an option and that \"reason is incapable of divining the truth\"; thus, a decision of whether to believe in the existence of God must be made by \"considering the consequences of each possibility\".\n\nVoltaire's critique concerns not the nature of the Pascalian wager as proof of God's existence, but the contention that the very belief Pascal tried to promote is not convincing. Voltaire hints at the fact that Pascal, as a Jansenist, believed that only a small, and already predestined, portion of humanity would eventually be saved by God.\n\nVoltaire explained that no matter how far someone is tempted with rewards to believe in Christian salvation, the result will be at best a faint belief. Pascal, in his Pensees, agrees with this, not stating that people can choose to believe (and therefore make a safe wager), but rather that some cannot believe.\n\nAs Étienne Souriau explained, in order to accept Pascal's argument, the bettor needs to be certain that God seriously intends to honour the bet; he says that the Wager assumes that God also accepts the bet, which is not proved; Pascal's bettor is here like the fool who seeing a leaf floating on a river's waters and quivering at some point, for a few seconds, between the two sides of a stone, says: \"I bet a million with Rothschild that it takes finally the left path.\" And, effectively, the leaf passed on the left side of the stone, but unfortunately for the fool Rothschild never said \"I [will take that] bet\".\n\nSince there have been many religions throughout history, and therefore many conceptions of God (or gods), some assert that all of them need to be factored into the Wager, in an argument known as the argument from inconsistent revelations. This, its proponents argue, would lead to a high probability of believing in \"the wrong god\", which, they claim, eliminates the mathematical advantage Pascal claimed with his Wager. Denis Diderot, a contemporary of Voltaire, concisely expressed this opinion when asked about the Wager, saying \"an Imam could reason the same way\". J. L. Mackie notes that \"the church within which alone salvation is to be found is not necessarily the Church of Rome, but perhaps that of the Anabaptists or members of The Church of Jesus Christ of Latter-day Saints or the Muslim Sunnis or the worshipers of Kali or of Odin.\"\n\nAnother version of this objection argues that for every religion that promulgates rules, there exists another religion that has rules of the opposite kind. If a certain action leads one closer to salvation in the former religion, it leads one further away from it in the latter. Therefore, the expected value of following a certain religion could be negative. Or, one could also argue that there are an infinite number of mutually exclusive religions (which is a subset of the set of all possible religions), and that the probability of any one of them being true is zero; therefore, the expected value of following a certain religion is zero.\n\nPascal considers this type of objection briefly in the notes compiled into the \"Pensées\", and dismisses it as obviously wrong and disingenuous:\n\nThis short but densely packed passage, which alludes to numerous themes discussed elsewhere in the \"Pensées\", has given rise to many pages of scholarly analysis.\n\nPascal says that unbelievers who rest content with the many-religions objection are people whose scepticism has seduced them into a fatal \"repose\". If they were really bent on knowing the truth, they would be persuaded to examine \"in detail\" whether Christianity is like any other religion, but they just cannot be bothered. Their objection might be sufficient were the subject concerned merely some \"question in philosophy\", but not \"here, where everything is at stake\". In \"a matter where they themselves, their eternity, their all are concerned\", they can manage no better than \"a superficial reflection\" (\"une reflexion légère\") and, thinking they have scored a point by asking a leading question, they go off to amuse themselves.\n\nAs Pascal scholars observe, Pascal regarded the many-religions objection as a rhetorical ploy, a \"trap\" that he had no intention of falling into. If, however, any who raised it were sincere, they would want to examine the matter \"in detail\". In that case, they could get some pointers by turning to his chapter on \"other religions\".\n\nAs David Wetsel notes, Pascal's treatment of the pagan religions is brisk: \"As far as Pascal is concerned, the demise of the pagan religions of antiquity speaks for itself. Those pagan religions which still exist in the New World, in India, and in Africa are not even worth a second glance. They are obviously the work of superstition and ignorance and have nothing in them which might interest 'les gens habiles' ('clever men')\". Islam warrants more attention, being distinguished from paganism (which for Pascal presumably includes all the other non-Christian religions) by its claim to be a revealed religion. Nevertheless, Pascal concludes that the religion founded by Mohammed can on several counts be shown to be devoid of divine authority, and that therefore, as a path to the knowledge of God, it is as much a dead end as paganism. Judaism, in view of its close links to Christianity, he deals with elsewhere.\n\nThe many-religions objection is taken more seriously by some later apologists of the Wager, who argue that, of the rival options, only those awarding infinite happiness affect the Wager's dominance. In the opinion of these apologists \"finite, semi-blissful promises such as Kali's or Odin's\" therefore drop out of consideration. Also, the infinite bliss that the rival conception of God offers has to be mutually exclusive. If Christ's promise of bliss can be attained concurrently with Jehovah's and Allah's (all three being identified as the God of Abraham), there is no conflict in the decision matrix in the case where the cost of believing in the wrong conception of God is neutral (limbo/purgatory/spiritual death), although this would be countered with an infinite cost in the case where not believing in the correct conception of God results in punishment (hell).\n\nFurthermore, ecumenical interpretations of the Wager argue that it could even be suggested that believing in a generic God, or a god by the wrong name, is acceptable so long as that conception of God has similar essential characteristics of the conception of God considered in Pascal's Wager (perhaps the God of Aristotle). Proponents of this line of reasoning suggest that either all of the conceptions of God or gods throughout history truly boil down to just a small set of \"genuine options\", or that if Pascal's Wager can simply bring a person to believe in \"generic theism\" it has done its job.\n\nPascal argues implicitly for the uniqueness of Christianity in the Wager itself, writing: \"If there is a God, He is infinitely incomprehensible...Who then can blame the Christians for not being able to give reasons for their beliefs, professing as they do a religion which they cannot explain by reason?\"\n\nSome critics argue that Pascal's Wager, for those who cannot believe, suggests feigning belief to gain eternal reward. This would be dishonest and immoral. In addition, it is absurd to think that God, being just and omniscient, would not see through this deceptive strategy on the part of the \"believer\", thus nullifying the benefits of the Wager.\n\nSince these criticisms are concerned not with the validity of the Wager itself, but with its possible aftermath—namely that a person who has been convinced of the overwhelming odds in favor of belief might still find himself unable to sincerely believe—they are tangential to the thrust of the Wager. What such critics are objecting to is Pascal's subsequent advice to an unbeliever who, having concluded that the only rational way to wager is in favor of God's existence, points out, reasonably enough, that this by no means makes him a believer. This hypothetical unbeliever complains, \"I am so made that I cannot believe. What would you have me do?\" Pascal, far from suggesting that God can be deceived by outward show, says that God does not regard it at all: \"God looks only at what is inward.\" For a person who is already convinced of the odds of the Wager but cannot seem to put his heart into the belief, he offers practical advice.\n\nExplicitly addressing the question of inability to believe, Pascal argues that if the Wager is valid, the inability to believe is irrational, and therefore must be caused by feelings: \"your inability to believe, because reason compels you to [believe] and yet you cannot, [comes] from your passions.\" This inability, therefore, can be overcome by diminishing these irrational sentiments: \"Learn from those who were bound like you. . . . Follow the way by which they began; by acting as if they believed, taking the holy water, having masses said, etc. Even this will naturally make you believe, and deaden your acuteness.—'But this is what I am afraid of.'—And why? What have you to lose?\"\n\nSome other critics have objected to Pascal's Wager on the grounds that he wrongly assumes what type of epistemic character God would likely value in his rational creatures if he existed. More specifically, Richard Carrier has objected by positing an alternative conception of God that prefers his creatures to be honest inquirers and disapproves of thoughtless or feigned belief:\n\n\n\n"}
{"id": "24497161", "url": "https://en.wikipedia.org/wiki?curid=24497161", "title": "Periodic matrix set", "text": "Periodic matrix set\n\nIn mathematics, a periodic matrix set is a set of square matrices in which each square matrix is of a different size, and such that each cell within each matrix within a set contains data associated with some type of periodic distribution. \n\nA set may be specified to contain a fixed number of matrices and is identified by a set number (\"S\"), where \"S\" is the set identification number and \"M\" is the number of matrices included in the set. There is no limit to the number of matrices which may be members of a periodic set.\n\nEach matrix within a set has an identification number (a) and must contain a \"root cell\". A root cell must be located at any corner of a matrix. All root cells must be located at the same corner of each matrix within a single set. A diagonal line drawn from a root cell to the opposite corner of the same matrix is a \"root diagonal\".\n\nThe periodicity is defined by \"partial square rings\" (rings) of cells adjoining a root cell on two sides. All cells within the same ring, (even if they are located in a different matrix) have a similar \"period\". If a matrix contains (n+1) cells then the outermost ring contains \"2n+1\" cells which are all included in the same period. A ring identification number (n) identifies each period. The root cell is also the smallest ring and is identified as; n = 0. Each subsequent ring (1, 2, 3, etc.) has 2n+1 cells (3, 5, 7, etc.).\n\nIndividual cells contained within a ring are identified by their deviation from the root diagonal. Each cell within a ring is assigned a deviation number (D). All cells intersected by the root diagonal have; D = 0. All cell locations in a column deviation have positive values of D. All cell locations in a row deviation have negative values of D.\n\nAny cell within a set will require three numbers for the identification of its location;a is the matrix numbern is the ring numberD is the deviation number\n\nThe cell could also have its location identified as;a is the matrix numberx is the column number (root cell = 0)y is the row number (root cell = 0)\n\nThe two locational systems are analogous to Radial (anD) and Cartesian (axy) systems. Generally this article will use the \"anD\" locational method.\n\nThe contents of any cell must contain data that is periodic in some manner.\n\nCombinations of sets are possible; however each set must be conformable for combination. A resultant set (R) is the combination of N sets each having M matrices.\n\nTwo sets (of compatible construction) may combine so that the root cells on similar sized matrices are adjacent. This is a \"set pair\" and is identified by a \"pair number\" (P). The resultant matrices are not square but are 2n x n rectangular.\n\nFour sets may also combine so that all root cells on similar sized matrices are adjacent. This gives a resultant set of square matrices having an even number of cells on each side. All root cells will form a central 2x2 \"core\" within each resultant matrix. The resultant set is actually two pairs. Each pair forms half of the resultant set. The identifiers (P,S) will tag each quadrant of the resultant set, which is all of the original sets. P = +½ represents the upper pairP = -½ represents the lower pairS = +½ represents the right set of each pair.S = -½ represents the left set of each pair.\n\nFive identifiers are required to locate any cell in R;P is the pair numberS is the set numbera is the matrix numbern is the ring numberD is the deviation number\n\nPeriodic matrix sets have an application to chemistry (for example, in the periodic table) and particle physics (for example, with sub atomic particles). The resultant set R is of special interest.\n\nThe periodic rings may be associated with quantum harmonic oscillation. A quantum harmonic oscillator has energy (E) defined as; E = (n + ½)ћω. Where; ћ = h/2π and h is Planck's constant, and ω is frequency. The number of cells in each period may be written as; 2E/ћω.\n\nThe rings may also be associated with atomic orbitals. If the ring number (n) is equal to the quantum number for orbital angular momentum (the azimuthal number l ), then the rings (0, 1, 2, 3) correspond to the orbitals (s, p, d, f). The ring number is NOT equal to the principal quantum number (n ). The number of cells per ring is half the number of electrons per orbital due to spin duality of the electrons.\n\nThe quantum numbers are; n is the principal quantum numberl is the quantum number for orbital angular momentum (the azimuthal number)ml is the orbital magnetic momentms is the spin magnetic moment\n\nThe spin quantum number (s) is not normally used in chemistry applications as all electrons are; s = ½.\nThe atomic number (Z) may be expressed as a function of energies which in turn are functions of the quantum numbers.\n\nIf a resultant set is R then the locational numbers correspond to the quantum numbers as follows.\nS = ms\nn = l\nD = ml\n\nThe Madelung rule gives the \"P\" and \"a\" relationships. This rule may be generalized as follows; 2a - P = n + l + s\n(2n + 2l + 2s - 4a) = 1\n\nThis generalization may also be obtained from J coupling.\n\nIf; P = -½\nThen; a = ½(n + l)\n\nIf; P = +½\nThen; a = ½(n + l) + ½\n\nThe sub-atomic particles may be grouped as an R combination.\n\nA set is considered to be \"locationally compliant\" if the data contained in each cell is also a function of the location of the cell. Let an R resultant set be populated with atomic numbers. Each cell contains one atomic number (Z). The number in each cell should be a function of the locators of the cell. If a term is associated with each locator then the atomic number will be the sum of all terms and a constant.\n\nZ = Z + Z + Z + Z + Z - ½\n\nThe five locator terms are as follows.\n\nZ = -2a(P+½)\n\nZ = -2(n+½)(S+½)\n\nZ = 4a(a+1)(a+½)/3\n\nZ = -2n(n+½)\n\nZ = (D+½)\n\nThis distribution of atomic numbers in R is a locationally compliant matrix set of the Periodic Table. The following tables show the resultant matrices populated with the atomic numbers.\n\nR showing combined matrices 1 to 4 populated with atomic number (Z)\n\na = 1\na = 2\n\na = 3\n\na = 4\n\n\n"}
{"id": "2468312", "url": "https://en.wikipedia.org/wiki?curid=2468312", "title": "Punctuality", "text": "Punctuality\n\nPunctuality is the characteristic of being able to complete a required task or fulfill an obligation before or at a previously designated time. \"Punctual\" is often used synonymously with \"on time\". \nIt is also acceptable that punctual can also, be related to talking about grammar, means \"to be accurate\".\n\nAn opposite personality trait is tardiness.\n\nAccording to each culture, there is often an understanding about what is considered an acceptable degree of punctuality. Usually, a small amount of lateness is acceptable; this is commonly about ten or fifteen minutes in Western cultures, but this is not the case in such instances as doctor's appointments or school lessons. In some cultures, such as Japanese society, and settings, such as military ones, expectations may be much stricter.\n\nSome cultures have an unspoken understanding that actual deadlines are different from stated deadlines, for example with Africa time. For example, it may be understood in a particular culture that people will turn up an hour later than advertised. In this case, since everyone understands that a 9 pm party will actually start at around 10 pm, no-one is inconvenienced when everyone arrives at 10 pm.\n\nIn cultures which value punctuality, being late is seen as disrespectful of others' time and may be considered insulting. In such cases, punctuality may be enforced by social penalties, for example by excluding low-status latecomers from meetings entirely. Such considerations can lead on to considering the value of punctuality in econometrics and to considering the effects of non-punctuality on others in queueing theory.\n\n\n"}
{"id": "371914", "url": "https://en.wikipedia.org/wiki?curid=371914", "title": "Real tree", "text": "Real tree\n\nIn mathematics, real trees (also called formula_1-trees) are a class of metric spaces generalising simplicial trees. They arise naturally in many mathematical contexts, in particular geometric group theory and probability theory. They are also the simplest examples of Gromov hyperbolic spaces.\n\nA metric space formula_2 is a real tree if it is a geodesic space where every triangle is a tripod. That is, for every three points formula_3 there exists a point formula_4 such that the geodesic segments formula_5 intersect in the segment formula_6 and also formula_7. This definition is equivalent to formula_2 being a \"zero-hyperbolic space\" in the sense of Gromov (all triangles are \"zero-thin\"). \nReal trees can also be characterised by a topological property. A metric space formula_2 is a real tree if for any pair of points formula_10 all (topological) embeddings formula_11 of the segment formula_12 into formula_2 such that formula_14 have the same image (which is then a geodesic segment from formula_15 to formula_16).\n\n\nReal trees often appear, in various situations, as limits of more classical metric spaces.\n\nA brownian tree is a (non-simplicial) real tree almost surely. Brownian trees arise as limits of various random processes on finite trees.\n\nAny ultralimit of a sequence of hyperbolic spaces is a real tree. In particular, the asymptotic cone of any hyperbolic space is a real tree.\n\nLet formula_25 be a group. For a sequence of based formula_25-spaces formula_27 there is a notion of convergence to a based formula_25-space formula_29 due to M. Bestvina and F. Paulin. When the spaces are hyperbolic and the actions are unbounded the limit (if it exists) is a real tree.\n\nA simple example is obtained by taking formula_30 where formula_31 is a compact surface, and formula_32 the universal cover of formula_31 with the metric formula_34 (where formula_35 is a fixed hyperbolic metric on formula_31).\n\nThis is useful to produce actions of hyperbolic groups on real trees. Such actions are analyzed using the so-called Rips machine. A case of particular interest is the study of degeneration of groups acting properly discuntinuously on a real hyperbolic space (this predates Rips', Bestvina's and Paulin's work and is due to J. Morgan and P. Shalen).\n\nIf formula_37 is a field with an ultrametric valuation then the Bruhat–Tits building of formula_38 is a real tree. It is simplicial if and only if the valuations is discrete.\n\nIf formula_39 is a totally ordered abelian group there is a natural notion of a distance with values in formula_39 (classical metric spaces correspond to formula_42). There is a notion of formula_39-tree which recovers simplicial trees (for formula_44) and real trees (for formula_42). The structure of finitely presented groups acting freely on formula_39 -trees was described. In particular, such a group acts freely on some formula_47 -tree.\n\nThe axioms for a building can be generalized to give a definition of a real building. These arise for example as asymptotic cones of higher-rank symmetric spaces or as Bruhat—Tits buildings of higher-rank groups over valued fields.\n\n"}
{"id": "18421474", "url": "https://en.wikipedia.org/wiki?curid=18421474", "title": "Reciprocity (social and political philosophy)", "text": "Reciprocity (social and political philosophy)\n\nThe social norm of reciprocity is the expectation that people will respond to each other in similar ways—responding to gifts and kindnesses from others with similar benevolence of their own, and responding to harmful, hurtful acts from others with either indifference or some form of retaliation. Such norms can be crude and mechanical, such as a literal reading of the eye-for-an-eye rule lex talionis, or they can be complex and sophisticated, such as a subtle understanding of how anonymous donations to an international organization can be a form of reciprocity for the receipt of very personal benefits, such as the love of a parent.\nThe norm of reciprocity varies widely in its details from situation to situation, and from society to society. Anthropologists and sociologists have often claimed, however, that having some version of the norm appears to be a social inevitability. Reciprocity figures prominently in social exchange theory, evolutionary psychology, social psychology, cultural anthropology and rational choice theory.\n\nOne-to-one reciprocity.Some reciprocal relationships are direct one-to-one arrangements between individuals, or between institutions, or between governments. Some of these are one-time arrangements, and others are embedded in long-term relationships. Families often have expectations that children will reciprocate for the care they receive as infants by caring for their elderly parents; businesses may have long-term contractual obligations with each other: governments make treaties with each other.\n\nThere are also one-to-one reciprocal relationships that are indirect. For example, there are sometimes long chains of exchanges, in which A gives a benefit to B, who passes on a similar benefit to C, and so on, in which each party in the chain expects that what goes around will eventually come back around. The classic anthropological example is the Kula exchange in the Trobriand Islands.\n\nOne-to-many and many-to-one reciprocity often lies somewhere between direct reciprocal arrangements and generalized reciprocity. Informal clubs in which the hosting arrangements circulate among members are examples of the one-to-many variety. Bridal showers are examples of the many-to-one variety. So are barn raising practices in some frontier communities. All of these are similar to direct reciprocity, since the beneficiaries are identified as such in each case, and contributors know exactly what they can expect in return. But because membership in the group changes, and needs for new meetings or marriages or barns are not always predictable, these cases differ significantly from precisely defined one-to-one cases.\n\nGeneralized reciprocity is even less precise. Here donors operate within a large network of social transactions largely unknown to each other, and without expectations about getting specific benefits in return — other than, perhaps, the sort of social insurance provided by the continuance of the network itself. Recipients may not know the donors, and may not themselves be able to make a return in-kind to that network, but perhaps feel obligated to make a return to a similar network. Blood banks and food banks are examples. But in fact any stable social structure in which there is a division of labor will involve a system of reciprocal exchanges of this generalized sort, as a way of sustaining social norms.\n\nAll of these patterns of reciprocity, along with related ideas such as gratitude, have been central to social and political philosophy from Plato onward. These philosophical discussions concern the ways in which patterns and norms of reciprocity might have a role in theories of justice, stable and productive social systems, healthy personal relationships, and ideals for human social life generally.\n\nPhilosophical work on reciprocity often pays considerable attention, directly or indirectly, to the proper interpretation of one or more of the following conceptual issues.\n\nReciprocity as distinct from related ideas. In Plato’s Crito, Socrates considers whether citizens might have a duty of gratitude to obey the laws of the state, in much the way they have duties of gratitude to their parents. Many other philosophers have considered similar questions. (See the references below to Sidgwick, English, and Jecker for modern examples.) This is certainly a legitimate question. Charging a child or a citizen with ingratitude can imply a failure to meet a requirement. But confining the discussion to gratitude is limiting. There are similar limitations in discussions of the do-unto-others golden rule, or ethical principles that are modeled on the mutuality and mutual benevolence that come out of the face-to-face relations envisaged by Emmanuel Levinas or the I-Thou relationships described by Martin Buber. Like gratitude, these other ideas have things in common with the norm of reciprocity, but are quite distinct from it.\nGratitude, in its ordinary sense, is as much about having warm and benevolent feelings toward one’s benefactors as it is about having obligations to them. Reciprocity, in its ordinary dictionary sense, is broader than that, and broader than all discussions that begin with a sense of mutuality and mutual benevolence. (See the reference below to Becker, \"Reciprocity\", and the bibliographic essays therein.) Reciprocity pointedly covers arm’s-length dealings between egoistic or mutually disinterested people.\nMoreover, norms of gratitude do not speak very directly about what feelings and obligations are appropriate toward wrongdoers, or the malicious. Reciprocity, by contrast, speaks directly to both sides of the equation – requiring responses in kind: positive for positive, negative for negative. In this, it also differs from the golden rule, which is compatible with forgiveness and “turning the other cheek” but has notorious difficulties as a basis for corrective justice, punishment, and dealing with people (e.g., masochists) who have unusual motivational structures.\n\nFinally, the idea of enforcing, or carrying out a duty of gratitude, as well as calibrating the extent one’s gratitude, seems inconsistent with the warm and benevolent feelings of “being grateful.” There is a similar inconsistency in the idea of enforcing a duty to love. Reciprocity, by contrast, because it does not necessarily involve having special feelings of love or benevolence, fits more comfortably into discussions of duties and obligations. Further, its requirement of an in-kind response invites us to calibrate both the quality and the quantity of the response. \nThe norm of reciprocity thus requires that we make fitting and proportional responses to both the benefits and harms we receive – whether they come from people who have been benevolent or malicious. Working out the conceptual details of this idea presents interesting questions of its own. The following matters are all considered at length in many of the sources listed below under References, and those authors typically defend particular proposals about how best to define the conceptual details of reciprocity. What follows here is simply an outline of the topics that are under philosophical scrutiny.\n\nQualitative similarity. What counts as making a qualitatively appropriate or “fitting” response in various settings—positive for positive, negative for negative? If one person invites another to dinner, must the other offer a dinner in return? How soon? Must it be directly to the original benefactor, or will providing a comparable favor to someone else be appropriate? If the dinner one receives is unintentionally awful, must one reciprocate with something similarly awful? Sometimes an immediate tit-for-tat response seems inappropriate, and at other times it is the only thing that will do.\n\nAre there general principles for assessing the qualitative appropriateness of reciprocal responses? Reflective people typically practice a highly nuanced version of the norm of reciprocity for social life, in which the qualitative similarity or fittingness of the response appears to be determined by a number of factors.\n\n\"The nature of the transaction\". One is the general nature of the transaction or relationship between the parties – the rules and expectations involved in a particular interaction itself. Tit for tat, defined in a literal way as an exchange of the identical kinds of goods (client list for client list, referral for referral) may be the only sort of reciprocal response that is appropriate in a clearly defined business situation. Similarly, dinner-for-dinner may be the expectation among members of a round robin dinner club. But when the nature of the transaction is more loosely defined, or is embedded in a complex personal relationship, an appropriate reciprocal response often requires spontaneity, imagination, and even a lack of premeditation about where, what, and how soon.\n\n\"Fitting the response to the recipient\". Another aspect of qualitative fit is what counts subjectively, for the recipient, as a response in-kind. When we respond to people who have benefited us, it seems perverse to give them things they do not regard as benefits. The general principle here is that, other things equal, a return of good for good received will require giving something that will actually be appreciated as good by the recipient – at least eventually. Similarly for the negative side. When we respond to bad things, reciprocity presumably requires a return that the recipient regards as a bad thing.\n\n\"Unusual circumstances\". A third aspect of qualitative fit is the presence or absence of circumstances that undermine the usual expectations about reciprocity. If a pair of friends often borrow each other’s household tools, and one of them (suddenly deranged with anger) asks to borrow an antique sword from the other’s collection, what is a fitting response? The example, in a slightly different form, goes back to Plato. The point is that in this unusual circumstance, reciprocity (as well as other considerations) may require that the recipient not get what he wants at the moment. Rather, it may be that the recipient should be given what he needs, in some objective sense, whether he ever comes to appreciate that it is good for him.\n\n\"General rationale\". A final determinant of qualitative fit is the general rationale for having the norm of reciprocity in the first place. For example, if the ultimate point of practicing reciprocity is to produce stable, productive, fair, and reliable social interactions, then there may be some tensions between things that accomplish this general goal and things that satisfy only the other three determinants. Responding to others’ harmful conduct raises this issue. As Plato observed (Republic, Book I), is not rational to harm our enemies in the sense of making them worse, as enemies or as people, than they already are. We may reply to Plato by insisting that reciprocity merely requires us to make them worse-off, not worse, period. But if it turns out that the version of the reciprocity norm we are using actually has the consequence of doing both, or at any rate not improving the situation, then we will have undermined the point of having it.\n\n\"Quantitative similarity\". Another definitional issue concerns proportionality. What counts as too little, or too much in return for what we receive from others? In some cases, such as borrowing a sum of money from a friend who has roughly the same resources, a prompt and exact return of the same amount seems right. Less will be too little, and a return with interest will often be too much, between friends. But in other cases, especially in exchanges between people who are very unequal in resources, a literal reading of tit-for-tat may be a perverse rule – one that undermines the social and personal benefits of the norm of reciprocity itself. How, for example, may badly disadvantaged people reciprocate for the public or private assistance they receive? Requiring a prompt and exact return of the benefit received may defeat the general purpose of the norm of reciprocity by driving disadvantaged people further into debt. Yet to waive the debt altogether, or to require only some discounted amount seems to defeat the purpose also.\n\nAnglo-American legal theory and practice has examples of two options for dealing with this problem. One is to require a return that is equal to the benefit received, but to limit the use of that requirement in special cases. Bankruptcy rules are in part designed to prevent downward, irrecoverable spirals of debt while still exacting a considerable penalty. Similarly, there are rules for rescinding unconscionable contracts, preventing unjust enrichment, and dealing with cases in which contractual obligations have become impossible to perform. These rules typically have considerable transaction costs.\n\nAnother kind of option is to define a reciprocal return with explicit reference to ability to pay. Progressive tax rates are an example of this. Considered in terms of reciprocity, this option seems based on an equal sacrifice interpretation of proportionality, rather than an equal benefit one. Under an equal sacrifice rule, making a quantitatively similar return will mean giving something back whose marginal value to oneself, given one’s resources, equals the marginal value of the sacrifice made by the original giver, given her resources.\n\nStandard usage of the term justice shows its close general connection to the concept of reciprocity. Justice includes the idea of fairness, and that in turn includes treating similar cases similarly, giving people what they deserve, and apportioning all other benefits and burdens in an equitable way. Those things, further, involve acting in a principled, impartial way that forbids playing favorites and may require sacrifices. All of those things are certainly in the neighborhood of the elements of reciprocity (e.g., fittingness, proportionality), but it is challenging to explain the precise connections.\n\nDiscussions of merit, desert, blame, and punishment inevitably involve questions about the fittingness and proportionality of our responses to others, and retributive theories of punishment put the norm of reciprocity at their center. The idea is to make the punishment fit the crime. This differs from utilitarian theories of punishment, which may use fittingness and proportionality as constraints, but whose ultimate commitment is to make punishment serve social goals such as general deterrence, public safety, and the rehabilitation of wrongdoers.\n\nIn just war theory, notions of fittingness and proportionality are central, at least as constraints both on the justification of a given war, and the methods used to prosecute it. When war represents a disproportionate response to a threat or an injury, it raises questions of justice related to reciprocity. When war fighting employs weapons that do not discriminate between combatants and noncombatants, it raises questions of justice related to reciprocity. A profound sense of injustice related to a lack of reciprocity – for example, between those privileged by socioeconomic status, political power, or wealth, and those who are less privileged, and oppressed – sometimes leads to war in the form of revolutionary or counterrevolutionary violence. It has been argued that the use of autonomous or remote controlled weaponized drones violate reciprocity. Political solutions which end the violence without dealing with the underlying injustice run the risk of continued social instability.\n\nA very deep and persistent line of philosophical discussion explores the way in which reciprocity can resolve conflicts between justice and self-interest, and can justify the imposition (or limitation) of social, political, and legal obligations that require individuals to sacrifice their own interests.\n\nThis aspect of the philosophical discussion of reciprocity attempts to bring together two ways of approaching a very basic question: What is the fundamental justification for the existence of social and political institutions – institutions that impose and enforce duties and obligations upon their members?\n\nIndividual well-being. One obvious answer is that people need to stay out of each other’s way enough so that each can pursue his or her individual interests as far as possible, without interference from others. This immediately justifies rules that are mutually advantageous, but it raises questions about requiring obedience from people whenever it turns out that they will be disadvantaged by following the rules, or can get away with disobeying them. So the problem becomes one of showing whether, and when, it might actually be mutually advantageous to follow the rules of justice even when it is inconvenient or costly to do so.\n\nSocial contract theorists often invoke the value of reciprocal relationships to deal with this. Many human beings need help from one another from time to time in order to pursue their individual interests effectively. So if we can arrange a system of reciprocity in which all the benefits we are required to contribute are typically returned to us in full (or more), that may justify playing by the rules—even in cases where it looks as though we can get away with not doing so.\n\nSocial well-being. Another obvious answer to the question of why people organize themselves into groups, however, is in order to achieve levels of cooperation needed for improving society generally – for example by improving public health, and society-wide levels of education, wealth, or individual welfare. This also gives a reason for rules of justice, but again raises problems about requiring individuals to sacrifice their own welfare for the good of others—especially when some individuals might not share the particular goals for social improvements at issue.\n\nHere too, the value of reciprocal relationships can be invoked, this time to limit the legitimacy of the sacrifices a society might require. For one thing, it seems perverse to require sacrifices in pursuit of some social goal if it turns out those sacrifices are unnecessary, or in vain because the goal cannot be achieved.\n\nTo some philosophers, a theory of justice based on reciprocity (or fairness, or fair play) is an attractive middle ground between a thoroughgoing concern with individual well-being and a thoroughgoing concern with social well-being. This has been part of the attraction of the most influential line of thought on distributive justice in recent Anglo-American philosophy – the one carried on in the context of John Rawls’ work.\n\nFuture generations. It may also be that there is something to be gained, philosophically, from considering what obligations of generalized reciprocity present generations of human beings may have to future ones. Rawls considers (briefly) the problem of defining a “just savings principle” for future generations, and treats it as a consequence of the interests people typically have in the welfare of their descendents, and the agreements fully reciprocal members of society would come to among themselves about such matters. Others (e.g., Lawrence Becker) have explored the intuitive idea that acting on behalf of future generations may be required as a generalized form of reciprocity for benefits received from previous generations.\n\nWhat is the relation between reciprocity and love, friendship or family relationships? If such relationships are ideally ones in which the parties are connected by mutual affection and benevolence, shouldn’t justice and reciprocity stay out of their way? Isn’t impartiality inconsistent with love? Doesn’t acting on principle take the affection out of friendship or family relationships? Doesn’t following the norm of reciprocity eliminate unconditional love or loyalty?\n\nSome contemporary philosophers have criticized major figures in the history of Western philosophy, including John Rawls’ early work, for making familial relationships more or less opaque in theories of justice. (See the reference below to Okin.) The argument is that families can be grossly unjust, and have often been so. Since the family is “the school of justice,” if it is unjust the moral education of children is distorted, and the injustice tends to spread to the society at large, and to be perpetuated in following generations. If that is right, then justice and reciprocity must define the boundaries within which we pursue even the most intimate relationships.\nA somewhat different thread on these matters begins with Aristotle’s discussion of friendship, in \"Nicomachean Ethics\" 1155-1172a. He proposes that the highest or best form of friendship involves a relationship between equals – one in which a genuinely reciprocal relationship is possible. This thread appears throughout the history of Western ethics in discussions of personal and social relationships of many sorts: between children and parents, spouses, humans and other animals, and humans and god(s). The question is the extent to which the kind of reciprocity possible in various relationships determines the kind of mutual affection and benevolence possible in those relationships.\n\nThis said, reciprocation in personal relationships rarely follows a mathematical formula and the level of reciprocation, i.e. the give and take, will vary depending on the personalities involved, and situational factors such as which party has more control, persuasive power or influence. It is often the case that one party will typically be the lead reciprocator with the other being the responsive reciprocator. The form of reciprocation can also be influenced by the level of emotional need. Sometimes one party will need more support than the other and this can switch at different times depending on the life situation of each party. Because reciprocation is influenced by personal circumstances and since people do not follow a set pattern like robots, reciprocation from a friend to a friend for example will vary in intensity and an absolutely consistent pattern cannot be expected. If for example a person has a large inner circle of friendships with reciprocation as the key element of friendship, then the level of reciprocation within the inner circle will influence the depth of a friendship therein. Reciprocation can be responsive or initiative. It is also a fundamental principle in parenting, a successful work place, religion and karma.\n\nSo for example, in the friendship context, reciprocation means to give or take mutually but not necessarily equally. Overall reciprocal balance is more important than strict equality at every moment. Friendship based on reciprocity means caring for each other, being responsive and supportive and in tune with each other. But without some form of overall reciprocal balance, the relationship may become transformed into a nonreciprocal form of friendship, or the friendship may fail altogether.\n\nTo provide an everyday life example, should one's (person A) dog die, a good friend (person B) would offer support and a \"shoulder to cry on\" for person A struggling to deal with the death of their dog. After time, person B might suggest a new dog, to help person A move on from their loss. Reciprocation occurs from person A to person B, if person B obtains assistance from person A at a future time.\n\n\n"}
{"id": "31540628", "url": "https://en.wikipedia.org/wiki?curid=31540628", "title": "Relative scalar", "text": "Relative scalar\n\nIn mathematics, a relative scalar (of weight \"w\") is a scalar-valued function whose transform under a coordinate transform,\n\non an \"n\"-dimensional manifold obeys the following equation\n\nwhere\n\nthat is, the determinant of the Jacobian of the transformation. A scalar density refers to the formula_4 case.\n\nRelative scalars are an important special case of the more general concept of a relative tensor.\n\nAn ordinary scalar or absolute scalar refers to the formula_5 case.\n\nIf formula_6 and formula_7 refer to the same point formula_8 on the manifold, then we desire formula_9. This equation can be interpreted two ways when formula_7 are viewed as the \"new coordinates\" and formula_6 are viewed as the \"original coordinates\". The first is as formula_12, which \"converts the function to the new coordinates\". The second is as formula_13, which \"converts back to the original coordinates. Of course, \"new\" or \"original\" is a relative concept.\n\nThere are many physical quantities that are represented by ordinary scalars, such as temperature and pressure.\n\nSuppose the temperature in a room is given in terms of the function formula_14 in Cartesian coordinates formula_15 and the function in cylindrical coordinates formula_16 is desired. The two coordinate systems are related by the following sets of equations:\nand\n\nUsing formula_12 allows one to derive formula_24 as the transformed function.\n\nConsider the point formula_8 whose Cartesian coordinates are formula_26 and whose corresponding value in the cylindrical system is formula_27. A quick calculation shows that formula_28 and formula_29 also. This equality would have held for any chosen point formula_8. Thus, formula_31 is the \"temperature function in the Cartesian coordinate system\" and formula_32 is the \"temperature function in the cylindrical coordinate system\".\n\nOne way to view these functions is as representations of the \"parent\" function that takes a point of the manifold as an argument and gives the temperature.\n\nThe problem could have been reversed. One could have been given formula_33 and wished to have derived the Cartesian temperature function formula_34. This just flips the notion of \"new\" vs the \"original\" coordinate system.\n\nSuppose that one wishes to \"integrate\" these functions over \"the room\", which will be denoted by formula_35. (Yes, integrating temperature is strange but that's partly what's to be shown.) Suppose the region formula_35 is given in cylindrical coordinates as formula_37 from formula_38, formula_39 from formula_40 and formula_41 from formula_38 (that is, the \"room\" is a quarter slice of a cylinder of radius and height 2).\nThe integral of formula_34 over the region formula_35 is\nThe value of the integral of formula_33 over the same region is\nThey are not equal. The integral of temperature is not independent of the coordinate\nsystem used. It is non-physical in that sense, hence \"strange\". Note that if the integral of formula_33 included a factor of the Jacobian (which is just formula_37),\nwe get\nwhich \"is\" equal to the original integral but it is not however the integral of \"temperature\" because\ntemperature is a relative scalar of weight 0, not a relative scalar of weight 1.\n\nIf we had said formula_14 was representing mass density, however, then its transformed value\nshould include the Jacobian factor that takes into account the geometric distortion of the coordinate\nsystem. The transformed function is now formula_52. This time\nformula_28 but formula_54. As before\nis integral (the total mass) in Cartesian coordinates is\nThe value of the integral of formula_33 over the same region is\nThey are equal. The integral of mass \"density\" gives total mass which is a coordinate-independent concept.\nNote that if the integral of formula_33 also included a factor of the Jacobian like before, we get\nwhich is not equal to the previous case.\n\nWeights other than 0 and 1 do not arise as often. It can be shown the determinant of a type (0,2) tensor is a relative scalar of weight 2.\n\n"}
{"id": "16017389", "url": "https://en.wikipedia.org/wiki?curid=16017389", "title": "Renewable energy industry", "text": "Renewable energy industry\n\nThe renewable-energy industry is the part of the energy industry focusing on new and appropriate renewable energy technologies. Investors worldwide have paid greater attention to this emerging industry in recent years. In many cases, this has translated into rapid renewable energy commercialization and considerable industry expansion. The wind power and solar photovoltaics (PV) industries provide good examples of this.\n\nRenewable energy industries expanded during most of 2008, and by August 2008, there were at least 160 publicly traded renewable energy companies with a market capitalization greater than $100 million. An estimated $120 billion was invested in renewable energy globally in 2008. \nDuring 2006/2007, several renewable energy companies went through high profile Initial Public Offerings (IPOs), resulting in market capitalization near or above $1 billion. These corporations included the solar PV companies First Solar (USA), Trina Solar (USA), Centrosolar (Germany), and Renesola (U.K.), wind power company Iberdrola (Spain), and U.S. biofuels producers VeraSun Energy, Aventine, and Pacific Ethanol.\n\nRenewable energy industries expanded during most of 2008, with large increases in manufacturing capacity, diversification of manufacturing locations, and shifts in leadership. By August 2008, there were at least 160 publicly traded renewable energy companies with a market capitalization greater than $100 million. The number of companies in this category has expanded from around 60 in 2005. \n\nSome $150 billion was invested in renewable energy globally in 2009, including new capacity (asset finance and projects) and biofuels refineries. This is more than double the 2006 investment figure of $63 billion. Almost all of the increase was due to greater investment in wind power, solar PV, and biofuels.\n\nIn 2000, venture capital (VC) investment in renewable energy was about 1% of total VC investment. In 2007 that figure was closer to 10%, with solar power alone making up about 3% of the entire Venture Capital asset class of ~$33B. More than 60 start-ups have been funded by\nVCs in the last three years. Venture capital and private equity investments in renewable energy companies increased by 167 percent in 2006, according to investment analysts at New Energy Finance Limited.\n\nNew investment into the sector jumped US$148 billion in 2007, up 60 per cent over 2006, noted a report by the Sustainable Energy Finance Initiative (SEFI). Wind energy attracted one-third of the new capital and solar one-fifth. But interest in solar is growing rapidly on the back of major technological advances which saw solar investment increase 254 per cent. The IEA predicts US$20 trillion will be invested into alternative energy projects over the next 22 years.\n\nIn December 2008, worldwide capacity of wind power was 122,000 MW, of which 28,190 MW was capacity added in 2008.\n\nVestas is the largest wind turbine manufacturer in the world with a 20% market share in 2008. The company operates plants in Denmark, Germany, India, Italy, Britain, Spain, Sweden, Norway, Australia and China, and employs more than 20,000 people globally. After a sales slump in 2005, Vestas recovered and was voted \"Top Green Company of 2006\".\n\nGE Energy was the world's second largest wind turbine manufacturer in 2008, with 19% market share. The company has installed over 5,500 wind turbines and 3,600 hydro turbines, and its installed capacity of renewable energy worldwide exceeds 160,000 MW. GE Energy bought out Enron Wind in 2002 and also has nuclear energy operations in its portfolio.\n\nGamesa, founded in 1976 with headquarters in Vitoria, Spain, was the world's third largest wind turbine manufacturer in 2008, and it is also a major builder of wind farms. Gamesa’s main markets are within Europe, the US and China.\n\nOther major wind power companies include Siemens, Suzlon, Sinovel and Goldwind.\n\nAlthough the wind power industry will be impacted by the global financial crisis in 2009 and 2010, a BTM Consult five year forecast up to 2013 projects substantial growth. Over the past five years the average growth in new installations has been 27.6 per cent each year. In the forecast to 2013 the expected average annual growth rate is 15.7 per cent. More than 200 GW of new wind power capacity could come on line before the end of 2013. Wind power market penetration is expected to reach 3.35 per cent by 2013 and 8 per cent by 2018.\n\nOffshore wind power installations are emerging, and recent years have seen several hundred megawatts added annually, mostly in Europe.\n\nFirst Solar became the world's largest solar cell maker in 2009, producing some 1,100 MW of product, with a 13% market share. Suntech was in second place with a production of 595 MW in 2009 and market share of 7%. Sharp Solar was far behind the leader with 580 MW of output. Q-Cells and its 540 MW output was fourth in 2009. Yingli Green Energy, JA Solar Holdings, SunPower, Kyocera, Motech Solar and Gintech rounded out the 2009 Top 10 ranking.\n\nPhotovoltaic production has been increasing by an average of some 20 percent each year since 2002, making it the world’s fastest-growing energy technology. At the end of 2009, the cumulative global PV installations surpassed 21,000 megawatts.\n\nAccording to the China Greentech Report 2009, jointly issued by the PricewaterhouseCoopers and American Chamber of Commerce in Shanghai and released on 10 Sept in Dalian, China, the estimated size of China's green technology market could be between US$500 billion and US$1 trillion annually, or as much as 15 percent of China's forecasted GDP, in 2013. With the positive drivers from the Chinese government’s policies to develop green technology solution, China has already played a more important role in green technology market development. Following the announcements of the Chinese government in 2009 about the new subsidy scheme of “Golden Sun” to support solar industry development in China, some of the worldwide industry players have announced their development plans in this region, such as the agreement signed by LDK Solar regarding a solar project in Jiangsu province with a total capacity of 500MW, manufacturing facilities of polysilicon ingots and wafers, PV cells and PV modules to be built by Yingli Green Energy in Hainan Province, and the new thin film manufacturing plants of Tianwei Baoding and Anwell Technologies.\n\nSince 2004 there has been renewed interest in concentrating solar power (CSP) and three plants were completed during 2006/2007: the 64 MW Nevada Solar One, a 1 MW trough plant in Arizona, and the 11 MW PS10 solar power tower in Spain. Three 50 MW trough plants were under construction in Spain at the end of 2007 with ten additional 50 MW plants planned. In the United States, utilities in California and Florida have announced plans (or contracted for) at least eight new projects totaling more than 2,000 MW. Companies involved in new projects include Abengoa Solar, Acciona, Ausra, BrightSource Energy, Iberdrola, Solar Euromed, Solar Millennium and Stirling Energy Systems.\n\nBrazil continued its ethanol expansion plans which began in the 70's and now has the largest ethanol distribution and the largest fleet of cars run by any mix of ethanol and gasoline.\n\nIn the ethanol fuel industry, the United States dominated, with 130 operating ethanol plants in 2007, and production capacity of 26 billion liters/year (6.87 billion gallons/year), a 60 percent increase over 2005. Another 84 plants were under construction or undergoing expansion, and this will result in a doubled production capacity.\nThe biodiesel industry opened many new production facilities during 2006/2007 and continued expansion plans in several countries. New biodiesel capacity appeared throughout Europe, including in\nBelgium, Czech Republic, France, Germany, Italy, Poland, Portugal, Spain, Sweden, and the United Kingdom.\n\nCommercial investment in second-generation biofuels began in 2006/2007, and much of this investment went beyond pilot-scale plants. The world’s first commercial wood-to-ethanol plant began operation in Japan in 2007, with a capacity of 1.4 million liters/year. The first wood-to-ethanol plant in the United States is planned for 2008 with an initial output of 75 million liters/year.\n\nRenewable energy use tends to be more labor-intensive than fossil fuels, and so a transition toward renewables promises employment gains. Globally, about 2.3 million people work either directly in renewables or indirectly in supplier industries. The wind power industry employs some 300,000 people, the PV sector accounts for an estimated 170,000 jobs, and the solar thermal industry accounts for about 624,000. More than 1 million jobs are located in the biomass and biofuels sector.\n\n\n"}
{"id": "509678", "url": "https://en.wikipedia.org/wiki?curid=509678", "title": "Repressed memory", "text": "Repressed memory\n\nRepressed memories are memories that have been unconsciously blocked due to the memory being associated with a high level of stress or trauma. The theory postulates that even though the individual cannot recall the memory, it may still be affecting them subconsciously, and that these memories can emerge later into the consciousness. Ideas on repressed memory hiding trauma from awareness were an important part of Sigmund Freud's early work on psychoanalysis. He later took a different view.\n\nThe existence of repressed memories is an extremely controversial topic in psychology; although some studies have concluded that it can occur in a varying but generally small percentage of victims of trauma, many other studies dispute its existence entirely. Some psychologists support the theory of repressed memories and claim that repressed memories can be recovered through therapy, but most psychologists argue that this is in fact rather a process through which false memories are created by blending actual memories and outside influences. One study concluded that repressed memories were a cultural symptom for want of written proof of their existence before the nineteenth century, but its results were disputed by some psychologists, and a work discussing a repressed memory from 1786 was eventually acknowledged, though the others stand by their hypothesis.\n\nAccording to the American Psychological Association, it is not possible to distinguish repressed memories from false ones without corroborating evidence. The term repressed memory is sometimes compared to the term dissociative amnesia, which is defined in the DSM-V as an \"inability to recall autobiographical information. This amnesia may be localized (i.e., an event or period of time), selective (i.e., a specific aspect of an event), or generalized (i.e., identity and life history).\"\n\nAccording to the Mayo Clinic, amnesia refers to any instance in which memories stored in the long-term memory are completely or partially forgotten, usually due to brain injury.\nAccording to proponents of the existence of repressed memories, such memories can be recovered years or decades after the event, most often spontaneously, triggered by a particular smell, taste, or other identifier related to the lost memory, or via suggestion during psychotherapy.\n\nIt was initially claimed that there was no documented writing about repressed memories or dissociative amnesia (as it is sometimes referred to), before the 1800s. This finding, by Harrison G. Pope, was based on a competition in which entrants could win $1000 if they could identify \"a pre-1800 literary example of traumatic memory that has been repressed by an otherwise healthy individual, and then recovered.\" Pope claimed that no entrant had satisfied the criteria. Ross Cheit, a political scientist at Brown University, cited Nina, a 1786 opera by the French composer Nicolas Dalayrac.\n\nThe concept of repressed memory originated with Sigmund Freud in his 1896 essay \"Zur Ätiologie der Hysterie\" (\"On the etiology of hysteria\").\nOne of the studies published in his essay involved a young woman by the name of Anna O. Among her many ailments, she suffered from stiff paralysis on the right side of her body. Freud stated her symptoms to be attached to psychological traumas. The painful memories had separated from her consciousness and brought harm to her body. Freud used hypnosis to treat Anna O. She is reported to have gained slight mobility on her right side. Freud's repressed memory theory joined his philosophy of psychoanalysis. Repressed memory has remained a heavily debated topic inside of Freud's psychoanalysis philosophy.\n\nSome research indicates that memories of child sexual abuse and other traumatic incidents may be forgotten. Evidence of the spontaneous recovery of traumatic memories has been shown, and recovered memories of traumatic childhood abuse have been corroborated.\n\nVan der Kolk and Fisler's research shows that traumatic memories are retrieved, at least at first, in the form of mental imprints that are dissociated. These imprints are of the affective and sensory elements of the traumatic experience. Clients have reported the slow emergence of a personal narrative that can be considered explicit (conscious) memory. The level of emotional significance of a memory correlates directly with the memory's veracity. Studies of subjective reports of memory show that memories of highly significant events are unusually accurate and stable over time. The imprints of traumatic experiences appear to be qualitatively different from those of nontraumatic events. Traumatic memories may be coded differently from ordinary event memories, possibly because of alterations in attentional focusing or the fact that extreme emotional arousal interferes with the memory functions of the hippocampus.\n\nAnother possibility is that traumatic events are pushed out of consciousness until a later events elicits or triggers a psychological response. Support for this idea has come from studies in which trauma was temporarily induced. For example, a high percentage of female psychiatric in-patients, and outpatients have reported experiencing histories of childhood sexual abuse. Other clinical studies have concluded that patients who experienced incestuous abuse reported higher suicide attempts and negative identity formation as well as more disturbances in interpersonal relationships.\n\nThere has also been significant questioning of the reality of repressed memories. There is considerable evidence that rather than being pushed out of consciousness, the difficulty with traumatic memories for most people are their intrusiveness and inability to forget. One case that is held up as definitive proof of the reality of repressed memories, recorded by David Corwin has been criticized by Elizabeth Loftus and Melvin Guyer for ignoring the context of the original complaint and falsely presenting the sexual abuse as unequivocal and true when in reality there was no definitive proof.\n\nRetrospective studies (studying the extent to which participants can recall past events) depend critically on the ability of informants to recall accurate memories. The issue of reliability in participants’ introspective abilities has been questioned by modern psychologists. In other words, a participant accurately recalling and remembering their own past memories is highly criticized, because memories are undoubtedly influenced by external, environmental factors.\n\nPsychologists Elizabeth Loftus and Katherine Ketcham are authors of the seminal work on the fallacy of repressed memory, \"The Myth of Repressed Memory\" (St. Martin's Press, 1994).\n\nIt is hypothesised that repression may be one method used by individuals to cope with traumatic memories, by pushing them out of awareness (perhaps as an adaptation via psychogenic amnesia) to allow a child to maintain attachment to a person on whom they are dependent for survival. Researchers have proposed that repression can operate on a social level as well.\n\nOther theoretical causes of forgotten memories have stemmed from the idea of \"Retrieval-Influenced Forgetting\", which states that “false” memories will be more accurately recalled when rehearsed more, than when actual memories get rehearsed. In this scenario, the action of rehearsing a falsified memory can actually take precedence over the actual memory that a person experiences. Anderson et al. discovered that rehearsal of novel information exhibits inhibitive processes on one’s ability to remember or recall the prior (real) memory. This conclusion indicates that past memories can be easily forgotten, simply by attending to “real”, novel memories that are brought into awareness.\n\nMemories \"can\" be accurate, but they are not \"always\" accurate. For example, eyewitness testimony even of relatively recent dramatic events is notoriously unreliable. Memories of events are a mix of fact overlaid with emotions, mingled with interpretation and \"filled in\" with imaginings. Skepticism regarding the validity of a memory as factual detail is warranted. For example, one study where victims of documented child abuse were reinterviewed many years later as adults, 38% of the women denied any memory of the abuse.\n\nArguments against the existence of \"traumatic amnesia\" note that various manipulations can be used to implant false memories (sometimes called \"pseudomemories\"). These can be quite compelling for those who develop them, and can include details that make them seem credible to others. A classic experiment in memory research, conducted by Elizabeth Loftus, became widely known as \"Lost in the Mall\"; in this, subjects were given a booklet containing three accounts of real childhood events written by family members and a fourth account of a wholly fictitious event of being lost in a shopping mall. A quarter of the subjects reported remembering the fictitious event, and elaborated on it with extensive circumstantial detail. This experiment inspired many others, and in one of these, Porter et al. could convince about half of his subjects that they had survived a vicious animal attack in childhood.\n\nSuch experimental studies have been criticized in particular about whether the findings are really relevant to trauma memories and psychotherapeutic situations. Nevertheless, these studies prompted public and professional concern about recovered memory therapy for past sexual abuse. When memories are \"recovered\" after long periods of amnesia, particularly when extraordinary means were used to secure the recovery of memory, it is now widely (but not universally) accepted that the memories are quite likely to be false, i.e. of incidents that had not occurred. It is thus recognised by professional organizations that a risk of implanting false memories is associated with some similar types of therapy. The \"American Psychiatric Association\" advises: \"...most leaders in the field agree that although it is a rare occurrence, a memory of early childhood abuse that has been forgotten can be remembered later. However, these leaders also agree that it is possible to construct convincing pseudomemories for events that never occurred.\n\nNevertheless, many therapists believe in the authenticity of the recovered memories that they hear from their clients. In a non-random study by Loftus and Herzog (1991) with 16 clinicians, 13 (81%) said that they invariably believed their clients. The most common basis for this belief was the patient’s symptomology (low self-esteem, sexual dysfunction, self-destructive behaviour) or body memories (voice frozen etc.).\n\nThe mechanism(s) by which both of these phenomena happen are not well understood and, at this point it is impossible, without other corroborative evidence, to distinguish a true memory from a false one.\" Sheflin and Brown state that a total of 25 studies on amnesia for child sexual abuse exist and that they demonstrate amnesia in their study subpopulations. However, an editorial in the \"British Medical Journal\" states on the Sheflin and Brown study that \"on critical examination, the scientific evidence for repression crumbles.\"\n\nObviously, not all therapists agree that false memories are a major risk of psychotherapy and they argue that this idea overstates the data and is untested. Several studies have reported high percentages of the corroboration of recovered memories, and some authors have claimed that the false memory movement has tended to conceal or omit evidence of (the) corroboration\" of recovered memories.\n\nBoth true and false \"memories\" can be recovered using memory work techniques, but there is no evidence that reliable discriminations can be made between them. Some believe that memories \"recovered\" under hypnosis are particularly likely to be false.\nAccording to The Council on Scientific Affairs for the American Medical Association, recollections obtained during hypnosis can involve confabulations and pseudomemories and appear to be less reliable than nonhypnotic recall.\nBrown et al. estimate that 3 to 5% of laboratory subjects are vulnerable to post-event misinformation suggestions. They state that 5–8% of the general population is the range of high-hypnotizability. Twenty-five percent of those in this range are vulnerable to suggestion of pseudomemories for peripheral details, which can rise to 80% with a combination of other social influence factors. They conclude that the rates of memory errors run 0–5% in adult studies, 3–5% in children's studies and that the rates of false allegations of child abuse allegations run 4–8% in the general population.\n\nThe neuroscientist Donald Hebb (1904–1985) was the first to distinguish between short-term memory and long-term memory. According to current theories in neuroscience, things that we \"notice\" are stored in short-term memory for up to a few minutes; this memory depends on \"reverberating\" electrical activity in neuronal circuits, and is very easily destroyed by interruption or interference. Memories stored for longer than this are stored in \"long-term memory\". Whether information is stored in long-term memory depends on its \"importance\"; for any animal, memories of traumatic events are potentially important for the adaptive value that they have for future avoidance behavior, and hormones that are released during stress have a role in determining what memories are preserved. In humans, traumatic stress is associated with acute secretion of epinephrine and norepinephrine (adrenaline and noradrenaline) from the adrenal medulla and cortisol from the adrenal cortex. Increases in these facilitate memory, but chronic stress associated with prolonged hypersecretion of cortisol may have the opposite effect. The limbic system is involved in memory storage and retrieval as well as giving emotional significance to sensory inputs. Within the limbic system, the hippocampus is important for explicit memory, and for memory consolidation; it is also sensitive to stress hormones, and has a role in recording the emotions of a stressful event. The amygdala may be particularly important in assigning emotional values to sensory inputs.\n\nAlthough memory distortion occurs in everyday life, the brain mechanisms involved are not easy to study in the laboratory, but neuroimaging techniques have recently been applied to this subject. In particular, there have recently been studies of false recognition, where individuals incorrectly claim to have encountered a novel object or event, and the results suggest that the hippocampus and several cortical regions may contribute to such false recognition, while the prefrontal cortex may be involved in retrieval monitoring that can limit the rate of false recognition.\n\nAmnesia is partial or complete loss of memory that goes beyond mere forgetting. Often it is temporary and involves only part of a person's experience. Amnesia is often caused by an injury to the brain, for instance after a blow to the head, and sometimes by psychological trauma. Anterograde amnesia is a failure to remember new experiences that occur after damage to the brain; retrograde amnesia is the loss of memories of events that occurred before a trauma or injury. For a memory to become permanent (consolidated), there must be a persistent change in the strength of connections between particular neurons in the brain. Anterograde amnesia can occur because this consolidation process is disrupted; retrograde amnesia can result either from damage to the site of memory storage or from a disruption in the mechanisms by which memories can be retrieved from their stores. Many specific types of amnesia are recognized, including:\nThe form of amnesia that is linked with recovered memories is dissociative amnesia (formerly known as psychogenic amnesia). This results from a psychological cause, not by direct damage to the brain, and is a loss of memory of significant personal information, usually about traumatic or extremely stressful events. Usually this is seen as a gap or gaps in recall for aspects of someone's life history, but with severe acute trauma, such as during wartime, there can be a sudden acute onset of symptoms.\n\n\"Betrayal Trauma Theory\" proposes that in cases of childhood abuse, dissociative amnesia is an adaptive response, and that “victims may need to remain unaware of the trauma not to reduce suffering but rather to promote survival.”\nWhen stress interferes with memory, it is possible that some of the memory is kept by a system that records emotional experience, but there is no symbolic placement of it in time or space.\nTraumatic memories are retrieved, at least at first, in the form of dissociated mental imprints of the affective and sensory elements of the traumatic experience. Clients have reported the slow emergence of a personal narrative that can be considered explicit (conscious) memory.\n\nPsychiatrist Bessel van der Kolk divided the effects of traumas on memory functions into four sets:\n\nAccording to van der Kolk, memories of highly significant events are usually accurate and stable over time; aspects of traumatic experiences appear to get stuck in the mind, unaltered by time passing or experiences that may follow. The imprints of traumatic experiences appear to be different from those of nontraumatic events, perhaps because of alterations in attentional focusing or the fact that extreme emotional arousal interferes with memory. van der Kolk and Fisler's hypothesis is that under extreme stress, the memory categorization system based in the hippocampus fails, with these memories kept as emotional and sensory states. When these traces are remembered and put into a personal narrative, they are subject to being condensed, contaminated and embellished upon.\n\nWhen there is inadequate recovery time between stressful situations, alterations may occur to the stress response system, some of which may be irreversible, and cause pathological responses, which may include memory loss, learning deficits and other maladaptive symptoms. In animal studies, high levels of cortisol can cause hippocampal damage, which may cause short-term memory deficits; in humans, MRI studies have shown reduced hippocampal volumes in combat veterans with PTSD, adults with posttraumatic symptoms and survivors of repeated childhood sexual or physical abuse. Trauma may also interfere with implicit memory, where periods of avoidance may be interrupted by intrusive emotional occurrences with no story to guide them. A difficult issue is whether those presumably abused accurately recall their experiences.\n\nThe existence of repressed memory recovery has not been accepted by mainstream psychology, nor unequivocally proven to exist, and some experts in the field of human memory feel that no credible scientific support exists for the notions of repressed/recovered memories. A survey revealed that whilst memory and cognition experts tend to be skeptical of repressed memory, clinicians are much more apt to believe that traumatic memory is often repressed. One research report states that a distinction should be made between spontaneously recovered memories and memories recovered during suggestions in therapy. A criticism from Loftus is that recovered memories can be tainted by the process of recovery, the suggestions used in that process, or even cultural and environmental influences.\n\nThe Working Group on Investigation of Memories of Child Abuse of the American Psychological Association presented findings mirroring those of the other professional organizations. The Working Group made five key conclusions:\n\n\nMany critics believe that memories may be distorted and false. Psychologist Elizabeth Loftus questions the concept of repressed memories and the possibility of them being accurate. Loftus focuses on techniques that therapists use in order to help the patients recover their memory. Such techniques include age regression, guided visualization, trance writing, dream work, body work, and hypnosis.\nLoftus' research indicates that repressed memory faces problems, such as memory alteration. In one case a teenage boy was able to “conjure a memory of an event that never occurred.” According to Loftus, if a stable person could be influenced to remember an event that never occurred, an emotionally stressed person would be even more susceptible.\nWriter Mark Pendergrast has denounced the theory of repressed memories and its applications in sex abuse cases, including in particular the Jerry Sandusky case.\n\nSerious issues arise when recovered but false memories result in public allegations; false complaints carry serious consequences for the accused. Many of those who make false claims sincerely believe the truth of what they report. A special type of false allegation, the false memory syndrome, arises typically within therapy, when people report the \"recovery\" of childhood memories of previously unknown abuse. The influence of practitioners' beliefs and practices in the eliciting of false \"memories\" and of false complaints has come under particular criticism.\n\nIt is generally accepted that people sometimes are unable to recall traumatic experiences. An old version of the \"Diagnostic and Statistical Manual of Mental Disorders\" (DSM-IV), published by the \"American Psychiatric Association\", states that \"Dissociative amnesia is characterized by an inability to recall important personal information, usually of a traumatic or stressful nature, that is too extensive to be explained by ordinary forgetfulness.\"\nThe term \"recovered memory\", however, is not listed in DSM-IV or used by any mainstream formal psychotherapy modality.\n\nSome criminal cases have been based on a witness's testimony of recovered repressed memories, often of alleged childhood sexual abuse. In some jurisdictions, the statute of limitations for child abuse cases has been extended to accommodate the phenomena of repressed memories as well as other factors. The repressed memory concept came into wider public awareness in the 1980s and 1990s followed by a reduction of public attention after a series of scandals, lawsuits, and license revocations.\n\nA U.S. District Court accepted repressed memories as admissible evidence in a specific case. Dalenberg argues that the evidence shows that recovered memory cases should be allowed to be prosecuted in court.\n\nThe apparent willingness of courts to credit the recovered memories of complainants but not the absence of memories by defendants has been commented on: \"It seems apparent that the courts need better guidelines around the issue of dissociative amnesia in both populations.\"\n\nIn 1995, the Ninth Circuit Court of Appeals ruled, in Franklin v. Duncan and Franklin v. Fox, Murray \"et al\". (312 F3d. 423, see also 884 FSupp 1435, N.D. Calif.), that repressed memory is not admissible as evidence in a legal action because of its unreliability, inconsistency, unscientific nature, tendency to be therapeutically induced evidence, and subject to influence by hearsay and suggestibility. The court overturned the conviction of a man accused of murdering a nine-year-old girl purely based upon the evidence of a 21-year-old repressed memory by a lone witness, who also held a complex personal grudge against the defendant.\n\nIn a 1996 ruling, a U.S. District Court allowed repressed memories entered into evidence in court cases. Jennifer Freyd writes that Ross Cheit's case of suddenly remembered sexual abuse is one of the most well-documented cases available for the public to see. Cheit prevailed in two lawsuits, located five additional victims and tape-recorded a confession.\n\nOn December 16, 2005, the Irish Court of Criminal Appeal issued a certificate confirming a Miscarriage of Justice to a former nun, Nora Wall whose 1999 conviction for child rape was partly based on repressed-memory evidence. The judgement stated that:\n\nThere was no scientific evidence of any sort adduced to explain the phenomenon of \"flashbacks\" and/or \"retrieved memory\", nor was the applicant in any position to meet such a case in the absence of prior notification thereof.\n\nRecovered memory therapy is a range of psychotherapy methods based on recalling memories of abuse that had previously been forgotten by the patient. The term \"recovered memory therapy\" is not listed in DSM-IV or used by mainstream formal psychotherapy modality. Opponents of the therapy advance the explanation that therapy can create false memories through suggestion techniques; this has not been corroborated, though some research has shown supportive evidence. Nevertheless, the evidence is questioned by some researchers. It is possible for patients who retract their claims—after deciding their recovered memories are false—to suffer post-traumatic stress disorder due to the trauma of illusory memories.\n"}
{"id": "150497", "url": "https://en.wikipedia.org/wiki?curid=150497", "title": "Self-esteem", "text": "Self-esteem\n\nSelf-esteem reflects an individual's overall subjective emotional evaluation of their own worth. It is the decision made by an individual as an attitude towards the self. Self-esteem encompasses beliefs about oneself, (for example, \"I am competent\", \"I am worthy\"), as well as emotional states, such as triumph, despair, pride, and shame. Smith and Mackie (2007) defined it by saying \"The self-concept is what we think about the self; self-esteem, is the positive or negative evaluations of the self, as in how we feel about it.\" \n\nSelf-esteem is attractive as a social psychological construct because researchers have conceptualized it as an influential predictor of certain outcomes, such as academic achievement, happiness, satisfaction in marriage and relationships, and criminal behaviour. Self-esteem can apply specifically to a particular dimension (for example, \"I believe I am a good writer and feel happy about that\") or a global extent (for example, \"I believe I am a bad person, and feel bad about myself in general\"). Psychologists usually regard self-esteem as an enduring personality characteristic (\"trait\" self-esteem), though normal, short-term variations (\"state\" self-esteem) also exist. Synonyms or near-synonyms of self-esteem include many things: self-worth, self-regard, self-respect, and self-integrity.\n\nThe identification of self-esteem as a distinct psychological construct is thought to have its origins in the work of philosopher and psychologist, geologist, anthropologist William James (1892). James identified multiple dimensions of the self, with two levels of hierarchy: processes of knowing (called the 'I-self') and the resulting knowledge about the self (the 'Me-self'). Observation about the self and storage of those observations by the I-self create three types of knowledge, which collectively account for the Me-self, according to James. These are the material self, social self, and spiritual self. The social self comes closest to self-esteem, comprising all characteristics recognized by others. The material self consists of representations of the body and possessions, and the spiritual self of descriptive representations and evaluative dispositions regarding the self. This view of self-esteem as the collection of an individual's attitudes toward oneself remains today.\n\nIn the mid-1960s, sociologist Morris Rosenberg defined self-esteem as a feeling of self-worth and developed the Rosenberg self-esteem scale (RSES), which became the most-widely used scale to measure self-esteem in the social sciences.\n\nIn the early 20th century, the behaviorist movement minimized introspective study of mental processes, emotions and feelings, replacing introspection with objective study through experiments on behaviors observed in relation with environment. Behaviorism viewed the human being as an animal subject to reinforcements, and suggested placing psychology as an experimental science, similar to chemistry or biology. As a consequence, clinical trials on self-esteem were overlooked, since behaviorists considered the idea less liable to rigorous measurement.\nIn the mid-20th century, the rise of phenomenology and humanistic psychology led to renewed interest in self-esteem. Self-esteem then took a central role in personal self-actualization and in the treatment of psychic disorders. Psychologists started to consider the relationship between psychotherapy and the personal satisfaction of persons with high self-esteem as useful to the field. This led to new elements being introduced to the concept of self-esteem, including the reasons why people tend to feel less worthy and why people become discouraged or unable to meet challenges by themselves.\n\nIn 1992 the political scientist Francis Fukuyama associated self-esteem with what Plato called \"thymos\" - the \"spiritedness\" part of the Platonic soul.\n\nThe importance of self-esteem gained endorsement from some government and non-government groups starting around the 1970s, such that one can speak of a self-esteem movement. This movement can be used as an example of promising evidence that psychological research can have an effect on forming public policy. The underlying idea of the movement was that low self-esteem was the root of the problem for individuals, making it the root of societal problems and dysfunctions. A leading figure of the movement, psychologist Nathaniel Branden, stated: \"[I] cannot think of a single psychological problem – from anxiety and depression, to fear of intimacy or of success, to spouse battery or child molestation – that is not traced back to the problem of low self-esteem\".\n\nSelf-esteem was believed to be a cultural phenomenon of Western individualistic societies since low self-esteem was not found in collectivist countries such as Japan.\nConcern about low self-esteem and its many presumed negative consequences led California assemblyman John Vasconcellos to work to set up and fund the Task Force on Self-Esteem and Personal and Social Responsibility in California in 1986. Vasconcellos argued that this task force could combat many of the state's problems - from crime and teen pregnancy to school underachievement and pollution. He compared increasing self-esteem to giving out a vaccine for a disease: it could help protect people from being overwhelmed by life's challenges.\n\nThe task force set up committees in many California counties and formed a committee of scholars to review the available literature on self-esteem. This committee found very small associations between low self-esteem and its assumed consequences, ultimately showing that low self-esteem is not the root of all societal problems and not as important as the committee had originally thought. However, the authors of the paper that summarized the review of the literature still believe that self-esteem is an independent variable that affects major social problems. The task force disbanded in 1995, and the National Council for Self-Esteem and later the National Association for Self-Esteem (NASE) was established, taking on the task force's mission. Vasconcellos and Jack Canfield were members of its advisory board in 2003, and members of its Masters' Coalition included Anthony Robbins, Bernie Siegel, and Gloria Steinem.\n\nMany early theories suggested that self-esteem is a basic human need or motivation. American psychologist Abraham Maslow included self-esteem in his hierarchy of human needs. He described two different forms of \"esteem\": the need for respect from others in the form of recognition, success, and admiration, and the need for self-respect in the form of self-love, self-confidence, skill, or aptitude. Respect from others was believed to be more fragile and easily lost than inner self-esteem. According to Maslow, without the fulfillment of the self-esteem need, individuals will be driven to seek it and unable to grow and obtain self-actualization. Maslow also states that the healthiest expression of self-esteem \"is the one which manifests in respect we deserve for others, more than renown, fame and flattery\". Modern theories of self-esteem explore the reasons humans are motivated to maintain a high regard for themselves. Sociometer theory maintains that self-esteem evolved to check one's level of status and acceptance in ones' social group. According to \"Terror Management Theory\", self-esteem serves a protective function and reduces anxiety about life and death.\n\nSelf-esteem is important because it shows ourselves how we view the way we are and the sense of our personal value. Thus, it affects the way we are and act in the world and the way we are related to everybody else.\n\nCarl Rogers (1902-1987), an advocate of humanistic psychology, theorized the origin of many people's problems to be that they despise themselves and consider themselves worthless and incapable of being loved. This is why Rogers believed in the importance of giving unconditional acceptance to a client and when this was done it could improve the client's self-esteem. In his therapy sessions with clients, he offered positive regard no matter what. Indeed, the concept of self-esteem is approached since then in humanistic psychology as an inalienable right for every person, summarized in the following sentence:\n\nSelf-esteem is typically assessed using self-report inventories.\n\nOne of the most widely used instruments, the Rosenberg self-esteem scale (RSES) is a 10-item self-esteem scale score that requires participants to indicate their level of agreement with a series of statements about themselves. An alternative measure, The Coopersmith Inventory uses a 50-question battery over a variety of topics and asks subjects whether they rate someone as similar or dissimilar to themselves. If a subject's answers demonstrate solid self-regard, the scale regards them as well adjusted. If those answers reveal some inner shame, it considers them to be prone to social deviance.\n\nImplicit measures of self-esteem began to be used in the 1980s. These rely on indirect measures of cognitive processing thought to be linked to implicit self-esteem, including the Name Letter Task. Such indirect measures are designed to reduce awareness of the process of assessment. When used to assess implicit self-esteem, psychologists feature self-relevant stimuli to the participant and then measure how quickly a person identifies positive or negative stimuli. For example, if a woman was given the self-relevant stimuli of female and mother, psychologists would measure how quickly she identified the negative word, evil, or the positive word, kind.\n\nExperiences in a person's life are a major source of how self-esteem develops. In the early years of a child's life, parents have a significant influence on self-esteem and can be considered a main source of positive and negative experiences a child will have. Unconditional love from parents helps a child develop a stable sense of being cared for and respected. These feelings translate into later effects on self-esteem as the child grows older. Students in elementary school who have high self-esteem tend to have authoritative parents who are caring, supportive adults who set clear standards for their child and allow them to voice their opinion in decision making.\n\nAlthough studies thus far have reported only a correlation of warm, supportive parenting styles (mainly authoritative and permissive) with children having high self-esteem, these parenting styles could easily be thought of as having some causal effect in self-esteem development. Childhood experiences that contribute to healthy self-esteem include being listened to, being spoken to respectfully, receiving appropriate attention and affection and having accomplishments recognized and mistakes or failures acknowledged and accepted. Experiences that contribute to low self-esteem include being harshly criticized, being physically, sexually or emotionally abused, being ignored, ridiculed or teased or being expected to be \"perfect\" all the time.\n\nDuring school-aged years, academic achievement is a significant contributor to self-esteem development. A student consistently achieving success or consistently failing will have a strong academic effect on their individual self-esteem. Social experiences are another important contributor to self-esteem. As children go through school, they begin to understand and recognize differences between themselves and their classmates. Using social comparisons, children assess whether they did better or worse than classmates in different activities. These comparisons play an important role in shaping the child's self-esteem and influence the positive or negative feelings they have about themselves. As children go through adolescence, peer influence becomes much more important. Adolescents make appraisals of themselves based on their relationships with close friends. Successful relationships among friends are very important to the development of high self-esteem for children. Social acceptance brings about confidence and produces high self-esteem, whereas rejection from peers and loneliness brings about self-doubts and produces low self-esteem.\n\nAdolescence shows an increase in self-esteem that continues to increase in young adulthood and middle age. A decrease is seen from middle age to old age with varying findings on whether it is a small or large decrease. Reasons for the variability could be because of differences in health, cognitive ability, and socioeconomic status in old age. No differences have been found between males and females in their development of self-esteem. Multiple cohort studies show that there is not a difference in the life-span trajectory of self-esteem between generations due to societal changes such as grade inflation in education or the presence of social media.\n\nHigh levels of mastery, low risk taking, and better health are ways to predict higher self-esteem. In terms of personality, emotionally stable, extroverted, and conscientious individuals experience higher self-esteem. These predictors have shown us that self-esteem has trait-like qualities by remaining stable over time like personality and intelligence. However, this does not mean it can not be changed. Hispanic adolescents have a slightly lower self-esteem than their black and white peers, but then slightly higher levels by age 30. African Americans have a sharper increase in self-esteem in adolescence and young adulthood compared to Whites. However, during old age, they experience a more rapid decline in self-esteem.\n\nShame can be a contributor to those with problems of low self-esteem. Feelings of shame usually occur because of a situation where the social self is devalued, such as a socially evaluated poor performance. A poor performance leads to higher responses of psychological states that indicate a threat to the social self namely a decrease in social self-esteem and an increase in shame. This increase in shame can be helped with self-compassion.\n\nThere are three levels of self-evaluation development in relation to the real self, ideal self, and the dreaded self. The real, ideal, and dreaded selves develop in children in a sequential pattern on cognitive levels.\n\nThis development brings with it increasingly complicated and encompassing moral demands. Level 3 is where individuals' self-esteem can suffer because they do not feel as though they are living up to certain expectations. This feeling will moderately effect one's self-esteem with an even larger effect seen when individuals believe they are becoming their Dreaded Self\n\nPeople with a healthy level of self-esteem:\n\nA person can have a high self-esteem and hold it confidently where they do not need reassurance from others to maintain their positive self view, whereas others with defensive, high self-esteem may still report positive self-evaluations on the Rosenberg Scale, as all high self-esteem individuals do; however, their positive self-views are fragile and vulnerable to criticism. Defensive high self-esteem individuals internalize subconscious self-doubts and insecurities, causing them to react very negatively to any criticism they may receive. There is a need for constant positive feedback from others for these individuals to maintain their feelings of self-worth. The necessity of repeated praise can be associated with boastful, arrogant behavior or sometimes even aggressive and hostile feelings toward anyone who questions the individual's self-worth, an example of threatened egotism.\n\nImplicit self-esteem refers to a person's disposition to evaluate themselves positively or negatively in a spontaneous, automatic, or unconscious manner. It contrasts with explicit self-esteem, which entails more conscious and reflective self-evaluation. Both explicit self-esteem and implicit self-esteem are subtypes of self-esteem proper.\n\nNarcissism is a disposition people may have that represents an excessive love for one's self. It is characterized by an inflated view of self-worth. Individuals who score high on Narcissism measures, Robert Raskin's \"40 Item True or False Test\", would likely select true to such statements as \"If I ruled the world, it would be a much better place.\" There is only a moderate correlation between narcissism and self-esteem; that is to say that an individual can have high self-esteem but low narcissism or can be a conceited, obnoxious person and score high self-esteem and high narcissism.\n\nThreatened egotism is characterized as a response to criticism that threatens the ego of narcissists; they often react in a hostile and aggressive manner.\n\nLow self-esteem can result from various factors, including genetic factors, physical appearance or weight, mental health issues, socioeconomic status, significant emotional experiences, peer pressure or bullying.\n\nA person with low self-esteem may show some of the following characteristics:\n\n\nIndividuals with low self-esteem tend to be critical of themselves. Some depend on the approval and praise of others when evaluating self-worth. Others may measure their likability in terms of successes: others will accept themselves if they succeed but will not if they fail.\n\nThis classification proposed by Martin Ross distinguishes three states of self-esteem compared to the \"feats\" (triumphs, honors, virtues) and the \"anti-feats\" (defeats, embarrassment, shame, etc.) of the individuals.\n\nThe individual does not regard themselves as valuable or lovable. They may be overwhelmed by defeat, or shame, or see themselves as such, and they name their \"anti-feat\". For example, if they consider that being over a certain age is an anti-feat, they define themselves with the name of their anti-feat, and say, \"I am old\". They express actions and feelings such as pity, insulting themselves, and they may become paralyzed by their sadness.\n\nThe individual has a generally positive self-image. However, their self-esteem is also vulnerable to the perceived risk of an imminent anti-feat (such as defeat, embarrassment, shame, discredit), consequently they are often nervous and regularly use defense mechanisms. A typical protection mechanism of those with a Vulnerable Self-Esteem may consist in avoiding decision-making. Although such individuals may outwardly exhibit great self-confidence, the underlying reality may be just the opposite: the apparent self-confidence is indicative of their heightened fear of anti-feats and the fragility of their self-esteem. They may also try to blame others to protect their self-image from situations which would threaten it. They may employ defense mechanisms, including attempting to lose at games and other competitions in order to protect their self-image by publicly dissociating themselves from a 'need to win', and asserting an independence from social acceptance which they may deeply desire. In this deep fear of being unaccepted by an individual's peers, they make poor life choices by making risky choices.\n\nPeople with strong self-esteem have a positive self-image and enough strength so that anti-feats do not subdue their self-esteem. They have less fear of failure. These individuals appear humble, cheerful, and this shows a certain strength not to boast about feats and not to be afraid of anti-feats.\nThey are capable of fighting with all their might to achieve their goals because, if things go wrong, their self-esteem will not be affected. They can acknowledge their own mistakes precisely because their self-image is strong, and this acknowledgment will not impair or affect their self-image. They live with less fear of losing social prestige, and with more happiness and general well-being.\nHowever, no type of self-esteem is indestructible, and due to certain situations or circumstances in life, one can fall from this level into any other state of self-esteem.\n\nA distinction is made between contingent (or conditional) and non-contingent (or unconditional) self-esteem.\n\nContingent self-esteem is derived from external sources, such as (a) what others say, (b) one's success or failure, (c) one's competence, or (d) relationship-contingent self-esteem.\n\nTherefore, contingent self-esteem is marked by instability, unreliability, and vulnerability. Persons lacking a non-contingent self-esteem are \"predisposed to an incessant pursuit of self-value.\" However, because the pursuit of contingent self-esteem is based on receiving approval, it is doomed to fail. No one receives constant approval and disapproval often evokes depression. Furthermore, fear of disapproval inhibits activities in which failure is possible.\nNon-contingent self-esteem is described as true, stable, and solid. It springs from a belief that one is \"acceptable period, acceptable before life itself, ontologically acceptable\". Belief that one is \"ontologically acceptable\" is to believe that one's acceptability is \"the way things \"be\" without contingency\". In this belief, as expounded by theologian Paul Tillich, acceptability is not based on a person's virtue. It is an acceptance given \"\"in spite of our guilt\", not because we \"have no guilt\"\".\n\nPsychiatrist Thomas A Harris drew on theologian Paul Tillich for his classic \"I'm OK – You're OK\" that addresses non-contingent self-esteem. Harris translated Tillich's \"acceptable\" by the vernacular \"OK\", a term that means \"acceptable\". The Christian message, said Harris, is not \"YOU CAN BE OK, IF\", It is \"YOU ARE ACCEPTED, unconditionally\".\n\nA secure non-contingent self-esteem springs from the belief that one is ontologically acceptable and accepted.\n\nAbraham Maslow states that psychological health is not possible unless the essential core of the person is fundamentally accepted, loved and respected by others and by her or his self. Self-esteem allows people to face life with more confidence, benevolence and optimism, and thus easily reach their goals and self-actualize.\n\nSelf-esteem may make people convinced they deserve happiness. Understanding this is fundamental, and universally beneficial, since the development of positive self-esteem increases the capacity to treat other people with respect, benevolence and goodwill, thus favoring rich interpersonal relationships and avoiding destructive ones. For Erich Fromm, love of others and love of ourselves are not alternatives. On the contrary, an attitude of love toward themselves will be found in all those who are capable of loving others. Self-esteem allows creativity at the workplace, and is a specially critical condition for teaching professions.\n\nJosé-Vicente Bonet claims that the importance of self-esteem is obvious as a lack of self-esteem is, he says, not a loss of esteem from others, but self-rejection. Bonet claims that this corresponds to Major depressive disorder. Freud also claimed that the depressive has suffered \"an extraordinary diminution in his self-regard, an impoverishment of his ego on a grand scale...He has lost his self-respect\".\n\nThe Yogyakarta Principles, a document on international human rights law addresses the discriminatory attitude toward LGBT peoples that makes their self-esteem low to be subject to human rights violation including human trafficking. and World Health Organization recommends in \"Preventing Suicide\" published in 2000 that strengthening students' self-esteem is important to protect children and adolescents against mental distress and despondency, enabling them to cope adequately with difficult and stressful life situations. How this might be done, and whether it would be effective is unclear.\n\nOther than increased happiness, higher self-esteem is also known to be correlated with a better ability to cope with stress and a higher likeliness that the individual takes on difficult tasks relative to those with low self-esteem.\n\nFrom the late 1970s to the early 1990s many Americans assumed as a matter of course that students' self-esteem acted as a critical factor in the grades that they earn in school, in their relationships with their peers, and in their later success in life. Under this assumption, some American groups created programs which aimed to increase the self-esteem of students. Until the 1990s little peer-reviewed and controlled research took place on this topic.\n\nPeer-reviewed research undertaken since then has not validated previous assumptions. Recent research indicates that inflating students' self-esteem in and of itself has no positive effect on grades. Roy Baumeister has shown that inflating self-esteem by itself can actually decrease grades. The relationship involving self-esteem and academic results does not signify that high self-esteem contributes to high academic results. It simply means that high self-esteem may be accomplished as a result of high academic performance due to the other variables of social interactions and life events affecting this performance.\n\"Attempts by pro-esteem advocates to encourage self-pride in students solely by reason of their uniqueness as human beings will fail if feelings of well-being are not accompanied by well-doing. It is only when students engage in personally meaningful endeavors for which they can be justifiably proud that self-confidence grows, and it is this growing self-assurance that in turn triggers further achievement.\"\n\nHigh self-esteem has a high correlation to self-reported happiness; whether this is a causal relationship has not been established. The relationship between self-esteem and life satisfaction is stronger in individualistic cultures.\n\nAdditionally, self-esteem has been found to be related to forgiveness in close relationships, in that people with high self-esteem will be more forgiving than people with low self-esteem.\n\nHigh self-esteem does not prevent children from smoking, drinking, taking drugs, or engaging in early sex. One important exception is that high self-esteem reduces the chances of bulimia in females.\n\nIn a 2014 research conducted by Robert S. Chavez and Todd F. Heatherton, it was found that self-esteem is related to the connectivity of the frontostriatal circuit. The frontostriatal pathway connects the medial prefrontal cortex, which deals with self-knowledge, to the ventral striatum, which deals with feelings of motivation and reward. Stronger anatomical pathways are correlated with higher long-term self-esteem, while stronger functional connectivity is correlated with higher short-term self-esteem.\n\nThe American psychologist Albert Ellis criticized on numerous occasions the concept of self-esteem as essentially self-defeating and ultimately destructive. Although acknowledging the human propensity and tendency to ego rating as innate, he has critiqued the philosophy of self-esteem as unrealistic, illogical and self- and socially destructive – often doing more harm than good. Questioning the foundations and usefulness of generalized ego strength, he has claimed that self-esteem is based on arbitrary definitional premises, and over-generalized, perfectionistic and grandiose thinking. Acknowledging that rating and valuing behaviours and characteristics is functional and even necessary, he sees rating and valuing human beings' totality and total selves as irrational and unethical. The healthier alternative to self-esteem according to him is unconditional self-acceptance and unconditional other-acceptance. \"Rational Emotive Behavior Therapy\" is a psychotherapy based on this approach. \n\nFor persons with low self-esteem, any positive stimulus will temporarily raise self-esteem. Therefore, possessions, sex, success, or physical appearance will produce development of self-esteem, but the development is ephemeral at best.\n\nSuch attempts to raise one's self-esteem by positive stimulus produce a \"boom or bust\" pattern. \"Compliments and positive feedback\" produce a boost, but a bust follows a lack of such feedback. For a person whose \"self-esteem is contingent\", success is \"not extra sweet\", but \"failure is extra bitter\".\n\nLife satisfaction, happiness, healthy behavioral practices, perceived efficacy, and academic success and adjustment have been associated with having high levels of self-esteem (Harter, 1987; Huebner, 1991; Lipschitz-Elhawi & Itzhaky, 2005; Rumberger 1995; Swenson & Prelow, 2005; Yarcheski & Mahon, 1989). However, a common mistake is to think that loving oneself is necessarily equivalent to narcissism, as opposed for example to what Erik Erikson speaks of as \"a post-narcissistic love of the ego\". A person with a healthy self-esteem accepts and loves himself/herself unconditionally, acknowledging both virtues and faults in the self, and yet, in spite of everything, is able to continue to love her/himself.\n\nIn narcissists, by contrast, an \" uncertainty about their own worth gives rise to...a self-protective, but often totally spurious, aura of grandiosity\" – producing the class \"of narcissists, or people with very high, but insecure, self-esteem... fluctuating with each new episode of social praise or rejection.\" Narcissism can thus be seen as a symptom of fundamentally low self-esteem, that is, lack of love towards oneself, but often accompanied by \"an immense increase in self-esteem\" based on \"the defense mechanism of denial by overcompensation.\"\n\"idealized love of self...rejected the part of him\" that he denigrates – \"this destructive little child\" within. Instead, the narcissist emphasizes his virtues in the presence of others, just to try to convince himself that he is a valuable person and to try to stop feeling ashamed for his faults; unfortunately such \"people with unrealistically inflated self-views, which may be especially unstable and highly vulnerable to negative information...tend to have poor social skills.\"\n\n"}
{"id": "5439872", "url": "https://en.wikipedia.org/wiki?curid=5439872", "title": "The Three Laws of Robotics in popular culture", "text": "The Three Laws of Robotics in popular culture\n\nReferences to Isaac Asimov's Three Laws of Robotics have appeared in a wide variety of circumstances. In some cases, other authors have explored the Laws in a serious fashion. Other references, like those made in the satirical newspaper \"The Onion\", are clearly parodic.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "53341722", "url": "https://en.wikipedia.org/wiki?curid=53341722", "title": "Tomlinson D. Todd", "text": "Tomlinson D. Todd\n\nTomlinson D. Todd was a civil rights activist located in Washington, D.C. He was the acting director for the Institute on Race in 1941–1951 and the creator/host of the radio program \"American All\", which focused on education reform and implementing strategies for political empowerment.\n\nTomlinson was born in Reading, PA but was raised in Washington, D.C.\n\nTomlinson was credited for bringing many issues to light, and worked to bring an end to segregation and the Jim Crow laws through non-violent demonstrations. His work as president-elect of the Institute on Race lead him to discover laws from 1872 (entitled \"Lost Laws\") where it was legal for restaurants and other public spaces to discriminate against African Americans after the Civil War. Once discovered, these issues were able to be addressed and segregation soon ended for restaurants in the 1950s.\n\nIn his later life he ended up serving the Washington, D.C. community as a drivers education teacher, before passing away at the age of 76.\n"}
