{"id": "37047", "url": "https://en.wikipedia.org/wiki?curid=37047", "title": "Akasha", "text": "Akasha\n\nAkasha (Sanskrit \"\" ) is a term for either space or æther in traditional Indian cosmology, depending on the religion. The term has also been adopted in Western occultism and spiritualism in the late 19th century. In Hindustani, Nepali, Bengali, Marathi, Kannada, Telugu, Tamil it means \"sky\". In many modern Indo-Aryan languages and Dravidian languages the corresponding word (often rendered \"Akash\") retains a generic meaning of \"sky\".\n\nThe word in Sanskrit is derived from a root \"kāś\" meaning \"to be visible\". It appears as a masculine noun in Vedic Sanskrit with a generic meaning of \"open space, vacuity\". In Classical Sanskrit, the noun acquires the neuter gender and may express the concept of \"sky; atmosphere\" (Manusmrti, Shatapathabrahmana).\nIn Vedantic philosophy, the word acquires its technical meaning of \"an ethereal fluid imagined as pervading the cosmos\".\n\nIn Vedantic Hinduism, Akasha means the basis and essence of all things in the material world; the first element created. A Vedic mantra “pṛthivyāpastejovāyurākāśāt” indicates the sequence of initial appearance of the five basic gross elements. Thus, first appeared the space, from which appeared air, from that fire or energy, from which the water, and therefrom the earth. It is one of the \"Panchamahabhuta,\" or \"five gross elements\"; its main characteristic is \"Shabda\" (sound). The direct translation of Akasha is the word meaning \"upper sky\" or 'space' in Hinduism.\n\nThe Nyaya and Vaisheshika schools of Hindu philosophy state that Akasha or aether is the fifth physical substance, which is the substratum of the quality of sound. It is the \"One\", \"Eternal\", and \"All Pervading\" physical substance, which is imperceptible.\n\nAccording to the Samkhya school, Akasha is one of the five Mahābhūtas (grand physical elements) having the specific property of sound.\n\nAdherents of the heterodox Cārvāka or Lokāyata philosophy held that this world is made of four elements only. They exclude the fifth, Akasha, because its existence cannot be perceived.\n\nAkasha is space in the Jain conception of the cosmos. Akasha is one of the six \"dravyas\" (substances) and it accommodates the other five, namely sentient beings or souls (\"jīva\"), non-sentient substance or matter (\"pudgala\"), principle of motion (\"dharma\"), the principle of rest (\"adharma\") and time (\"kāla\")\n\nIt is all-pervading, infinite and made of infinite space-points.\n\nIt falls into the \"Ajiva\" category, divided into two parts: \"Loakasa\" (the part occupied by the material world) and \"Aloakasa\" (the space beyond it which is absolutely void and empty). In \"Loakasa\" the universe forms only a part. Akasha is that which gives space and makes room for the existence of all extended substances.\n\nAt the summit of the \"lokākāśa\" is the \"Siddhashila\" (abode of the liberated souls).\n\nIn Buddhist phenomenology Akasha is divided into limited space (ākāsa-dhātu) and endless space (ajatākasā).\n\nThe Vaibhashika, an early school of Buddhist philosophy, hold Akasha's existence to be real.\n\nĀkāsa is identified as the first arūpa jhāna, but usually translates as \"infinite space.\"\n\nThe Western mystic-religious philosophy called Theosophy has popularized the word Akasha as an adjective, through the use of the term \"Akashic records\" or \"Akashic library\", referring to an etheric compendium of all knowledge and history.\n\nScott Cunningham (1995) uses the term Akasha to refer to \"the spiritual force that Earth, Air, Fire, and Water descend from\".\n\nErvin László in \"Science and the Akashic Field: An Integral Theory of Everything\" (2004), based on ideas by Rudolf Steiner, posits \"a field of information\" as the substance of the cosmos, which he calls \"Akashic field\" or \"A-field\".\n\n"}
{"id": "2742000", "url": "https://en.wikipedia.org/wiki?curid=2742000", "title": "Alliance Quebec", "text": "Alliance Quebec\n\nAlliance Quebec (AQ) was a group formed in 1982 to lobby on behalf of English-speaking Quebecers in the province of Quebec, Canada. It began as an umbrella group of many English-speaking organizations and institutions in the province, with approximately 15,000 members. At its height in the mid-1980s, the group had a network of affiliated anglophone groups throughout the province. However, a prolonged decline in influence, group cohesion, membership and funding ultimately led to its closure in 2005.\n\nThe Parti Québécois (PQ), a party that supports the sovereignty of Quebec and the dominant use of French in most areas of public and business life, won a majority in the Quebec National Assembly (the province's legislature) in 1976. The vast majority of Quebec anglophones (i.e., Quebecers who speak English as a first language), who at that time made up approximately 13% of Quebec's population (see Language demographics of Quebec), did not support this party. Some anglophones formed local lobby groups to promote federalism and argue against new laws such as the Charter of the French Language (also known as \"Bill 101\"). After the Parti Québécois was re-elected in 1981, several of these groups (notably the \"Positive Action Committee\" and the \"Council of Quebec Minorities\") joined together in May 1982, as \"The Alliance of Language Communities in Quebec\" (or \"Alliance Québec\") in an effort to gain more influence and to start a province-level dialogue between linguistic groups.\n\nAQ's ideology reflected a desire to promote the rights and interests of the English language community while recognizing there were legitimate goals being pursued by the provincial government in promoting the French language, such promotion having strong support in the majority Francophone population. Alliance Quebec's best-known accomplishments from its earliest years included:\n\n\nThe group had widespread grassroots volunteer activity in its early years. It formed at least 20 regional chapters, including 8 in the anglophone neighbourhoods of Montreal. The federal government subsidized AQ in an effort to promote minority official language groups in the province, providing it with most of AQ's budget ($1.4 million in 1986). Similar funding was provided to French language groups outside Quebec.\n\nIn addition to AQ's regional chapters, six federally funded anglophone groups outside of Montreal became affiliated with AQ and sent delegates to its annual convention. Affiliated Quebec anglophone universities, CEGEPs and health and social service institution and community associations were also allowed to send delegates. Institutional members brought AQ substantial public policy expertise and participated in the policy formulation process.\n\nBy the end of the 1980s, AQ claimed to have 40,000 people on its membership list (including members of regional associations affiliated with AQ, such as the Townshippers' Association). This led to critiques that people who did not renew their memberships were not removed from this list and that the actual number of dues-paying members hovered around 5,000.\n\nMany of Alliance Quebec's founders were active in the Liberal Party of Quebec, the main opposition party while the PQ was in government. The Liberal Party of Quebec won the provincial election of 1985, and many of AQ's initial leaders were recruited to work for the new government. Several of AQ's highly educated and bilingual early staff members went on to become Liberal cabinet ministers in later years, such as Thomas Mulcair, Kathleen Weil, and Geoffrey Kelley (Jacques-Cartier), while others served as MNAs—Russell Williams (Nelligan), Russell Copeman (NDG). While this initially gave AQ strong lobbying contacts within the government, the departure of many of the group's founding leaders eventually hurt the group. Some have argued that the 1985 election was the beginning of a decline in influence of Alliance Quebec, as English-speakers believed the new government was friendlier and so the need for AQ was lower, while the Liberals had more connections with English-speakers than the PQ and so relied less on AQ to transmit their points of view. The group also faced criticism, almost since its founding, that it was not aggressive enough in its demands.\n\nIn December 1988, AQ's offices were destroyed in a case of arson. Then-president Royal Orr sued \"Le Journal de Montréal\" and Télé-Métropole, for falsely reporting that he was the \"prime suspect\" in setting the blaze. The lawsuit was eventually settled out of court.\n\nAQ's weakened bargaining position was brought to prominence in 1989 when the Liberal government passed Bill 178. Although the Liberal Party had campaigned in 1985 to loosen the legal restrictions on languages other than French, Bill 178 overturned the Supreme Court's \"Ford\" decision (see above), restoring the prohibition on non-French commercial signage (with an exception for small signs inside stores). Alliance Quebec's inability to prevent the adoption of Bill 178 by the Liberal government it had perceived as an ally, opened it to criticism from the anglophone community. Right-wing critics of AQ dubbed it \"Compliance Quebec\" and \"the lamb lobby\" for its perceived unwillingness to challenge the government.\n\nInternal tension arose among the directors of AQ over the issue of denied access to English language schools in the province. Tensions increased over whether or not to support the Liberals in the Quebec general election of 1989 in spite of Bill 178. Some prominent AQ leaders urged a protest vote by anglophones, either by spoiling their ballots or voting for the upstart Equality Party that opposed the Liberals' legislation. The Equality Party won four seats in the National Assembly in 1989, but quickly lost its support due to infighting, garnering only 0.3% of the vote in 1994. Rebuffed at the ballot box, some of the Equality Party's remaining active members instead concentrated on winning elected positions in Alliance Quebec in order to have AQ promote the Equality Party's platform (notably, favouring a complete repeal of all mandatory use of French in Quebec, and partitioning Quebec in the event of independence). Public infighting between so-called \"moderates\" (sympathetic to the Liberal Party) and \"radicals\" (the remaining members of the Equality Party) within AQ throughout the 1990s, along with a gradual decrease in interest among the general Quebec population in language politics, led to the marginalization of AQ in the province's politics.\n\nWhile infighting preoccupied the board of directors and annual convention, the grassroots elements of AQ became less active. Government funding allowed for a permanent staff for the group, which relied less and less on volunteers. AQ's smallest regional chapters, in Quebec's Magdalen Islands, Lower North Shore and Baie-Comeau, closed down for lack of members, while six of the group's eight chapters on the Island of Montreal merged in order to avoid closing.\n\nIn 1996, the Chief Electoral Officer of Quebec investigated alleged irregularities during the 1995 Quebec sovereignty referendum, finding among other things that votes in three mainly federalist ridings had been rejected without valid reasons. Alliance Quebec sued the Quebec government to try to force it to re-examine the rejected ballots in all 125 Quebec ridings. The trial judge ruled against AQ in 2000. AQ appealed, but ceased operations in 2005 (see below). In 2008 the Chief Electoral Officer got the court's permission to destroy the ballots after ruling that AQ's appeal had taken too long.\n\nIn May 1998, the group elected a \"radical\" president, Montreal Gazette columnist William Johnson. Previously a vocal critic of AQ, he won by rallying a group known derisively as \"angryphones\" (mainly members of the tiny Equality Party and some listeners of right wing talk radio shows, particularly the Howard Galganov show). He in turn supported Equality Party members for positions on AQ's board of directors. Unlike previous presidents, he made no attempt to meet with political leaders, preferring to conduct his lobbying through media, such as on radio talk shows. Also unlike previous presidents, who generally accepted the objectives of Bill 101 and focused on changing how it was applied, Johnson questioned the commitment to tolerance and human rights of those who supported Bill 101.\n\nAfter his election, Johnson organized two demonstrations against stores, in particular Eaton's and The Hudson's Bay Company, that did not place English on their in-store advertising (which was once again legal after amendments to the Charter of the French Language in 1994). Johnson told a crowd of demonstrators that he refused an Eaton's offer to put up English signs in their stores if AQ would quietly call off the protest, as Johnson wanted to make it a public issue. Johnson also had AQ's constitution amended to add his view that Canada's federal government should refuse to recognize a Quebec unilateral declaration of independence. Johnson also made headlines when the Entartistes threw a cream pie in his face while he marched in Montreal's 1998 Saint-Jean-Baptiste Day parade.\n\nJohnson's presidential campaign and his first six months as president temporarily brought more media attention and members to Alliance Quebec, as the PQ government and Quebec nationalist groups publicly criticized AQ's new, more confrontational tone. However, Johnson called off the group's protests in October 1998 and AQ's media coverage fell considerably, never to recover. Johnson's presidency and those of his similarly minded successors also provoked a negative reaction from the mainstream community of anglophones that formerly supported AQ. Links to the community's key healthcare, educational and community institutions vanished. Several events during these years highlighted the group's lack of support, which ultimately caused it to close down:\n\n\nAlliance Quebec was registered as a non-profit association in Quebec, with a headquarters in Montreal. Anyone could join as a member by paying a nominal fee of $5–$10. Membership entitled them to participate in the regional chapter in which they lived. Each regional chapter had its own board of directors and executive and obtained their budget mainly from membership fees.\n\nEach chapter could send 9 delegates to the annual convention, which took place each May in Montreal. In the 1990s, six chapters in central Montreal merged, which allowed the merged chapter to send up to 54 delegates. (In the William Johnson era, the West Island chapter's number of delegates was increased to reflect that it was the chapter with the second-largest number of members.) A youth commission of members under thirty years of age also existed and could send up to 18 delegates to the convention. Affiliated groups such as the Townshippers' Association could also send delegates, as could affiliated universities, hospitals and community groups. The outgoing president and board of directors were also entitled to vote at the convention, representing 41 delegates. In theory, an annual convention could have nearly 400 delegates during the 1990s. In practice, the number of delegates attending annual conventions decreased over time as membership fell and affiliated groups stopped participating; the annual convention in 1985 had about 470 delegates while each of the conventions from 1999 onward had under 100 delegates.\n\nThe annual convention chose the president and treasurer of AQ for the upcoming year. (In the William Johnson era, the president's term was lengthened to two years, although each subsequent president resigned before the end of their full two-year term.) The delegates would also choose the board of directors. Half of the forty-person board of directors of AQ would be up for election annually for a two-year term; the top twenty candidates who received the most votes being elected. The new board of directors would meet immediately after the convention to choose the remaining executive positions, including vice president, vice president \"off-island\" (meaning from outside of Montreal), secretary, and chairman of the board. The group also had an \"Advisory Council\" of prominent anglophones to advise the group on important issues from time to time, but this fell into disuse by the late 1990s.\n\nThe executives and directors were volunteer positions (although the president received a stipend.) There were also a number of paid staff members (around two dozen in 1994), such as a general director, a fundraiser, receptionists, researchers and organizers, paid for mainly from federal government grants.\n\nFor most of its existence, AQ also maintained committees to study issues. These included (at various times)committees for health and social services, education, access to English schools, youth employment, legal affairs, communication, internal rules, and membership. Some committees organized events and activities. One of these activities, \"Youth Employment Services\", became independent of AQ in the 1990s and continues to operate. Larger chapters also had some committees, especially in the group's early years.\n\n\n"}
{"id": "45035673", "url": "https://en.wikipedia.org/wiki?curid=45035673", "title": "Ananyata", "text": "Ananyata\n\nAnanyatā (Sanskrit:अनन्यता) means – 'having no equal', 'matchless', 'peerless', 'identity', 'sameness' It is a form of devotion in which the devotee is solely dependent on God. \"Ananyata\" is the doctrine that makes no distinction between God and the Atman.\n\nNarada Bhakti Sutras No. IX and X read as follows :- \n\nWith these words Nārada explains \"ananyatā\" as the state in which the mind of the devotee does not waver or goes astray, remains one-pointed ever steady in the contemplation of God to the exclusion of everything else, in that state everything is envisioned in God as His cosmic play.\n\nFor the Advaita Vedantists, \"ananyatā\" means absolute oneness or non-otherness of the individual self (\"Jiva\") and the Universal Self (\"Brahman\"), the absolute height of enlightenment, \"atmaikatya\" which is the absolute identity of the phenomenal selves with the Supreme Self.\n"}
{"id": "28923195", "url": "https://en.wikipedia.org/wiki?curid=28923195", "title": "Baumslag–Gersten group", "text": "Baumslag–Gersten group\n\nIn the mathematical subject of geometric group theory, the Baumslag–Gersten group, also known as the Baumslag group, is a particular one-relator group exhibiting some remarkable properties regarding its finite quotient groups, its Dehn function and the complexity of its word problem.\n\nThe group is given by the presentation\n\nHere exponential notation for group elements denotes conjugation, that is, for formula_2\n\nThe Baumslag–Gersten group \"G\" was originally introduced in a 1969 paper of Gilbert Baumslag, as an example of a non-residually finite one-relator group with an additional remarkable property that all finite quotient groups of this group are cyclic. Later, in 1992, Gersten showed that \"G\", despite being a one-relator group given by a rather simple presentation, has the Dehn function growing very quickly, namely faster than any fixed iterate of the exponential function. This example remains the fastest known growth of the Dehn function among one-relator groups. In 2011 Myasnikov, Ushakov and Won proved that \"G\" has the word problem solvable in polynomial time.\n\nThe Baumslag–Gersten group \"G\" can also be realized as an HNN extension of the Baumslag–Solitar group formula_3 with stable letter \"t\" and two cyclic associated subgroupsformula_4:\n\n\n\nand generalized many of Baumslag's original results in that context.\n\n\n"}
{"id": "48048702", "url": "https://en.wikipedia.org/wiki?curid=48048702", "title": "Beyond Natural Selection", "text": "Beyond Natural Selection\n\nBeyond Natural Selection is a 1991 book by Robert G. Wesson, published by MIT Press.\n\nWesson argues for the case of pluralism in biology. He suggests alternative mechanisms of evolution rather than natural selection. Wesson argues that reductionism is inadequate and looks for chaos theory as an example of a different approach that is needed to explain evolution. The book provides unsolved problems that Wesson believed natural selection could not account for.\n\nThe paleontologist Joseph G. Carter in a critical review for the \"American Scientist\" wrote the book \"includes innumerable oversimplifications and misrepresentations of both evolutionary theory and the paleontological record.\" Carter noted that the book was filled with errors such as Wesson's claim there is a lack of transitional fossils. Carter wrote that the \"book approaches the scientific illiteracy\" of the intelligent design text \"Of Pandas and People\", and concluded it was an \"embarrassment to the editors of the MIT Press\".\n\nThe ecologist Arthur M. Shapiro in a review for \"The Quarterly Review of Biology\" negatively reviewed the book for misunderstanding evolutionary biology and poor scholarship. According to Shapiro he \"found an average of just over one error... per page by checking pages of this book at random.\"\n\nThe historian Peter J. Bowler compared the book to anti-Darwinian and creationist works. He criticized the book for utilizing straw man arguments and presenting no valid scientific alternative to natural selection. Bowler noted that it is \"easy to criticize an established theory, much more difficult to come up with a workable alternative... Hand waving about the creative power of the organism makes a nice-sounding philosophical position but a poor scientific theory\".\n"}
{"id": "432276", "url": "https://en.wikipedia.org/wiki?curid=432276", "title": "Brownian ratchet", "text": "Brownian ratchet\n\nIn the philosophy of thermal and statistical physics, the Brownian ratchet or Feynman-Smoluchowski ratchet is a thought experiment about an apparent perpetual motion machine first analysed in 1912 by Polish physicist Marian Smoluchowski and popularised by American Nobel laureate physicist Richard Feynman in a physics lecture at the California Institute of Technology on May 11, 1962, during his Messenger Lectures series The Character of Physical Law in Cornell University in 1964 and in his text \"The Feynman Lectures on Physics\" as an illustration of the laws of thermodynamics. The simple machine, consisting of a tiny paddle wheel and a ratchet, appears to be an example of a Maxwell's demon, able to extract useful work from random fluctuations (heat) in a system at thermal equilibrium in violation of the second law of thermodynamics. Detailed analysis by Feynman and others showed why it cannot actually do this.\n\nThe device consists of a gear known as a ratchet that rotates freely in one direction but is prevented from rotating in the opposite direction by a pawl. The ratchet is connected by an axle to a paddle wheel that is immersed in a fluid of molecules at temperature formula_1. The molecules constitute a heat bath in that they undergo random Brownian motion with a mean kinetic energy that is determined by the temperature. The device is imagined as being small enough that the impulse from a single molecular collision can turn the paddle. Although such collisions would tend to turn the rod in either direction with equal probability, the pawl allows the ratchet to rotate in one direction only. The net effect of many such random collisions would seem to be that the ratchet rotates continuously in that direction. The ratchet's motion then can be used to do work on other systems, for example lifting a weight (\"m\") against gravity. The energy necessary to do this work apparently would come from the heat bath, without any heat gradient. Were such a machine to work successfully, its operation would violate the second law of thermodynamics, one form of which states: \"It is impossible for any device that operates on a cycle to receive heat from a single reservoir and produce a net amount of work.\"\n\nAlthough at first sight the Brownian ratchet seems to extract useful work from Brownian motion, Feynman demonstrated that if the entire device is at the same temperature, the ratchet will not rotate continuously in one direction but will move randomly back and forth, and therefore will not produce any useful work. The reason is that since the pawl is at the same temperature as the paddle, it will also undergo Brownian motion, \"bouncing\" up and down. It therefore will intermittently fail by allowing a ratchet tooth to slip backward under the pawl while it is up. Another issue is that when the pawl rests on the sloping face of the tooth, the spring which returns the pawl exerts a sideways force on the tooth which tends to rotate the ratchet in a backwards direction. Feynman demonstrated that if the temperature formula_2 of the ratchet and pawl is the same as the temperature formula_1 of the paddle, then the failure rate must equal the rate at which the ratchet ratchets forward, so that no net motion results over long enough periods or in an ensemble averaged sense. A simple but rigorous proof that no net motion occurs no matter what shape the teeth are was given by Magnasco. \n\nIf, on the other hand, formula_2 is smaller than formula_1, the ratchet will indeed move forward, and produce useful work. In this case, though, the energy is extracted from the temperature gradient between the two thermal reservoirs, and some waste heat is exhausted into the lower temperature reservoir by the pawl. In other words, the device functions as a miniature heat engine, in compliance with the second law of thermodynamics. Conversely, if formula_2 is greater than formula_1, the device will rotate in the opposite direction.\n\nThe Feynman ratchet model led to the similar concept of Brownian motors, nanomachines which can extract useful work not from thermal noise but from chemical potentials and other microscopic nonequilibrium sources, in compliance with the laws of thermodynamics. Diodes are an electrical analog of the ratchet and pawl, and for the same reason cannot produce useful work by rectifying Johnson noise in a circuit at uniform temperature.\n\nMillonas \nas well as Mahato\nextended the same notion to correlation ratchets driven by mean-zero (unbiased) nonequilibrium noise with a \nnonvanishing correlation function of odd order greater than one.\n\nThe ratchet and pawl was first discussed as a Second Law-violating device by Gabriel Lippmann in 1900. In 1912, Polish physicist Marian Smoluchowski gave the first correct qualitative explanation of why the device fails; thermal motion of the pawl allows the ratchet's teeth to slip backwards. Feynman did the first quantitative analysis of the device in 1962 using the Maxwell–Boltzmann distribution, showing that if the temperature of the paddle \"T\" was greater than the temperature of the ratchet \"T\", it would function as a heat engine, but if \"T\" = \"T\" there would be no net motion of the paddle. In 1996, Juan Parrondo and Pep Español used a variation of the above device in which no ratchet is present, only two paddles, to show that the axle connecting the paddles and ratchet conducts heat between reservoirs; they argued that although Feynman's conclusion was correct, his analysis was flawed because of his erroneous use of the quasistatic approximation, resulting in incorrect equations for efficiency. Magnasco and Stolovitzky (1998) extended this analysis to consider the full ratchet device, and showed that the power output of the device is far smaller than the Carnot efficiency claimed by Feynman. A paper in 2000 by Derek Abbott, Bruce R. Davis and Juan Parrondo, reanalyzed the problem and extended it to the case of multiple ratchets, showing a link with Parrondo's paradox.\n\nLéon Brillouin in 1950 discussed an electrical circuit analogue that uses a rectifier (such as a diode) instead of a ratchet. The idea was the diode would rectify the Johnson noise thermal current fluctuations produced by the resistor, generating a direct current which could be used to perform work. In the detailed analysis it was shown that the thermal fluctuations within the diode generate an electromotive force that cancels the voltage from rectified current fluctuations. Therefore, just as with the ratchet, the circuit will produce no useful energy if all the components are at thermal equilibrium (at the same temperature); a DC current will be produced only when the diode is at a lower temperature than the resistor.\n\nResearchers from the University of Twente, the University of Patras in Greece, and the Foundation for Fundamental Research on Matter have constructed a Feynman-Smoluchowski engine which, when not in thermal equilibrium, converts pseudo-Brownian motion into work by means of a granular gas, which is a conglomeration of solid particles vibrated with such vigour that the system assumes a gas-like state. The constructed engine consisted of four vanes which were allowed to rotate freely in a vibrofluidized granular gas. Because the ratchet's gear and pawl mechanism, as described above, permitted the axle to rotate only in one direction, random collisions with the moving beads caused the vane to rotate. This seems to contradict Feynman's hypothesis. However, this system is not in perfect thermal equilibrium: energy is constantly being supplied to maintain the fluid motion of the beads. Vigorous vibrations on top of a shaking device mimic the nature of a molecular gas. Unlike an ideal gas, though, in which tiny particles move constantly, stopping the shaking would simply cause the beads to drop. In the experiment, this necessary out-of-equilibrium environment was thus maintained. Work was not immediately being done, though; the ratchet effect only commenced beyond a critical shaking strength. For very strong shaking, the vanes of the paddle wheel interacted with the gas, forming a convection roll, sustaining their rotation. The experiment was filmed.\n\n\n\n"}
{"id": "54018249", "url": "https://en.wikipedia.org/wiki?curid=54018249", "title": "Common beisa oryx", "text": "Common beisa oryx\n\nThe common beisa oryx (\"Oryx beisa beisa\") also known as the beisa oryx is a nominate subspecies of the East African oryx native to the Horn of Africa and Kenya. It is closely related to the fringe-eared oryx. \nThere are five species of Oryx. Although they are very similar in appearance they have few distinct characteristics. Common Beisa Oryx have fringe-ears and black tufts of hair that extend past their ears. However, all species of Oryx are compact and muscular with relative long bodies and broad necks. There are not any marked difference between male and female Oryx. \nThe Common Beisa Oryx enjoy feeding on variety of grass species. They feed during the day when the plants hold the most water. During dry season, they feed on poisonous adenium plant. \n\nBeisa oryx once inhibited a large region of Northeastern Africa, from Sudan to Africa down to Tanzania, but it has been going extinct rapidly. Now they mostly remain in Ethiopia and Northern Kenya. Beisa oryx stay in bushland and grassland areas. During wet season, they move to high ground, avoid tall grass and saturated areas. They move great distant to find a perfect location and stay for few seasons. \n"}
{"id": "6042", "url": "https://en.wikipedia.org/wiki?curid=6042", "title": "Compact space", "text": "Compact space\n\nIn mathematics, and more specifically in general topology, compactness is a property that generalizes the notion of a subset of Euclidean space being closed (that is, containing all its limit points) and bounded (that is, having all its points lie within some fixed distance of each other). Examples include a closed interval, a rectangle, or a finite set of points. This notion is defined for more general topological spaces than Euclidean space in various ways.\n\nOne such generalization is that a topological space is \"sequentially\" compact if every infinite sequence of points sampled from the space has an infinite subsequence that converges to some point of the space. The Bolzano–Weierstrass theorem states that a subset of Euclidean space is compact in this sequential sense if and only if it is closed and bounded. Thus, if one chooses an infinite number of points in the \"closed\" unit interval some of those points will get arbitrarily close to some real number in that space. For instance, some of the numbers accumulate to 0 (others accumulate to 1). The same set of points would not accumulate to any point of the \"open\" unit interval ; so the open unit interval is not compact. Euclidean space itself is not compact since it is not bounded. In particular, the sequence of points has no subsequence that converges to any real number.\n\nApart from closed and bounded subsets of Euclidean space, typical examples of compact spaces include spaces consisting not of geometrical points but of functions. The term \"compact\" was introduced into mathematics by Maurice Fréchet in 1904 as a distillation of this concept. Compactness in this more general situation plays an extremely important role in mathematical analysis, because many classical and important theorems of 19th-century analysis, such as the extreme value theorem, are easily generalized to this situation. A typical application is furnished by the Arzelà–Ascoli theorem or the Peano existence theorem, in which one is able to conclude the existence of a function with some required properties as a limiting case of some more elementary construction.\n\nVarious equivalent notions of compactness, including sequential compactness and limit point compactness, can be developed in general metric spaces. In general topological spaces, however, different notions of compactness are not necessarily equivalent. The most useful notion, which is the standard definition of the unqualified term \"compactness\", is phrased in terms of the existence of finite families of open sets that \"cover\" the space in the sense that each point of the space lies in some set contained in the family. This more subtle notion, introduced by Pavel Alexandrov and Pavel Urysohn in 1929, exhibits compact spaces as generalizations of finite sets. In spaces that are compact in this sense, it is often possible to patch together information that holds locally—that is, in a neighborhood of each point—into corresponding statements that hold throughout the space, and many theorems are of this character.\n\nThe term compact set is sometimes a synonym for compact space, but usually refers to a compact subspace of a topological space.\n\nIn the 19th century, several disparate mathematical properties were understood that would later be seen as consequences of compactness. On the one hand, Bernard Bolzano (1817) had been aware that any bounded sequence of points (in the line or plane, for instance) has a subsequence that must eventually get arbitrarily close to some other point, called a limit point. Bolzano's proof relied on the method of bisection: the sequence was placed into an interval that was then divided into two equal parts, and a part containing infinitely many terms of the sequence was selected. The process could then be repeated by dividing the resulting smaller interval into smaller and smaller parts until it closes down on the desired limit point. The full significance of Bolzano's theorem, and its method of proof, would not emerge until almost 50 years later when it was rediscovered by Karl Weierstrass.\n\nIn the 1880s, it became clear that results similar to the Bolzano–Weierstrass theorem could be formulated for spaces of functions rather than just numbers or geometrical points. The idea of regarding functions as themselves points of a generalized space dates back to the investigations of Giulio Ascoli and Cesare Arzelà. The culmination of their investigations, the Arzelà–Ascoli theorem, was a generalization of the Bolzano–Weierstrass theorem to families of continuous functions, the precise conclusion of which was that it was possible to extract a uniformly convergent sequence of functions from a suitable family of functions. The uniform limit of this sequence then played precisely the same role as Bolzano's \"limit point\". Towards the beginning of the twentieth century, results similar to that of Arzelà and Ascoli began to accumulate in the area of integral equations, as investigated by David Hilbert and Erhard Schmidt. For a certain class of Green functions coming from solutions of integral equations, Schmidt had shown that a property analogous to the Arzelà–Ascoli theorem held in the sense of mean convergence—or convergence in what would later be dubbed a Hilbert space. This ultimately led to the notion of a compact operator as an offshoot of the general notion of a compact space. It was Maurice Fréchet who, in 1906, had distilled the essence of the Bolzano–Weierstrass property and coined the term \"compactness\" to refer to this general phenomenon (he used the term already in his 1904 paper which led to the famous 1906 thesis).\n\nHowever, a different notion of compactness altogether had also slowly emerged at the end of the 19th century from the study of the continuum, which was seen as fundamental for the rigorous formulation of analysis. In 1870, Eduard Heine showed that a continuous function defined on a closed and bounded interval was in fact uniformly continuous. In the course of the proof, he made use of a lemma that from any countable cover of the interval by smaller open intervals, it was possible to select a finite number of these that also covered it. The significance of this lemma was recognized by Émile Borel (1895), and it was generalized to arbitrary collections of intervals by Pierre Cousin (1895) and Henri Lebesgue (1904). The Heine–Borel theorem, as the result is now known, is another special property possessed by closed and bounded sets of real numbers.\n\nThis property was significant because it allowed for the passage from local information about a set (such as the continuity of a function) to global information about the set (such as the uniform continuity of a function). This sentiment was expressed by , who also exploited it in the development of the integral now bearing his name. Ultimately the Russian school of point-set topology, under the direction of Pavel Alexandrov and Pavel Urysohn, formulated Heine–Borel compactness in a way that could be applied to the modern notion of a topological space. showed that the earlier version of compactness due to Fréchet, now called (relative) sequential compactness, under appropriate conditions followed from the version of compactness that was formulated in terms of the existence of finite subcovers. It was this notion of compactness that became the dominant one, because it was not only a stronger property, but it could be formulated in a more general setting with a minimum of additional technical machinery, as it relied only on the structure of the open sets in a space.\nAny finite space is trivially compact. A non-trivial example of a compact space is the (closed) unit interval of real numbers. If one chooses an infinite number of distinct points in the unit interval, then there must be some accumulation point in that interval. For instance, the odd-numbered terms of the sequence get arbitrarily close to 0, while the even-numbered ones get arbitrarily close to 1. The given example sequence shows the importance of including the boundary points of the interval, since the limit points must be in the space itself — an open (or half-open) interval of the real numbers is not compact. It is also crucial that the interval be bounded, since in the interval one could choose the sequence of points , of which no sub-sequence ultimately gets arbitrarily close to any given real number.\n\nIn two dimensions, closed disks are compact since for any infinite number of points sampled from a disk, some subset of those points must get arbitrarily close either to a point within the disc, or to a point on the boundary. However, an open disk is not compact, because a sequence of points can tend to the boundary without getting arbitrarily close to any point in the interior. Likewise, spheres are compact, but a sphere missing a point is not since a sequence of points can tend to the missing point, thereby not getting arbitrarily close to any point \"within\" the space. Lines and planes are not compact, since one can take a set of equally-spaced points in any given direction without approaching any point.\n\nVarious definitions of compactness may apply, depending on the level of generality. A subset of Euclidean space in particular is called compact if it is closed and bounded. This implies, by the Bolzano–Weierstrass theorem, that any infinite sequence from the set has a subsequence that converges to a point in the set. Various equivalent notions of compactness, such as sequential compactness and limit point compactness, can be developed in general metric spaces.\n\nIn general topological spaces, however, the different notions of compactness are not equivalent, and the most useful notion of compactness—originally called \"bicompactness\"—is defined using covers consisting of open sets (see \"Open cover definition\" below). That this form of compactness holds for closed and bounded subsets of Euclidean space is known as the Heine–Borel theorem. Compactness, when defined in this manner, often allows one to take information that is known locally—in a neighbourhood of each point of the space—and to extend it to information that holds globally throughout the space. An example of this phenomenon is Dirichlet's theorem, to which it was originally applied by Heine, that a continuous function on a compact interval is uniformly continuous; here, continuity is a local property of the function, and uniform continuity the corresponding global property.\n\nFormally, a topological space is called \"compact\" if each of its open covers has a finite subcover. That is, is compact if for every collection of open subsets of such that\n\nthere is a finite subset of such that\n\nSome branches of mathematics such as algebraic geometry, typically influenced by the French school of Bourbaki, use the term \"quasi-compact\" for the general notion, and reserve the term \"compact\" for topological spaces that are both Hausdorff and \"quasi-compact\". A compact set is sometimes referred to as a \"compactum\", plural \"compacta\".\n\nA subset of a topological space is said to be compact if it is compact as a subspace (in the subspace topology). That is, is compact if for every arbitrary collection of open subsets of such that\n\nthere is a finite subset of such that\n\nCompactness is a \"topological\" property. That is, if formula_5, with subset equipped with the subspace topology, then is compact in if and only if is compact in .\n\nAssuming the axiom of choice, the following are equivalent:\n\nFor any subset \"A\" of Euclidean space R, \"A\" is compact if and only if it is closed and bounded; this is the Heine–Borel theorem.\n\nAs a Euclidean space is a metric space, the conditions in the next subsection also apply to all of its subsets. Of all of the equivalent conditions, it is in practice easiest to verify that a subset is closed and bounded, for example, for a closed interval or closed \"n\"-ball.\n\nFor any metric space (\"X\", \"d\"), the following are equivalent:\n\nA compact metric space (\"X\", \"d\") also satisfies the following properties:\n\nLet \"X\" be a topological space and C(\"X\") the ring of real continuous functions on \"X\". For each \"p\"∈\"X\", the evaluation map formula_6\ngiven by ev(\"f\")=\"f\"(\"p\") is a ring homomorphism. The kernel of ev is a maximal ideal, since the residue field is the field of real numbers, by the first isomorphism theorem. A topological space \"X\" is pseudocompact if and only if every maximal ideal in C(\"X\") has residue field the real numbers. For completely regular spaces, this is equivalent to every maximal ideal being the kernel of an evaluation homomorphism. There are pseudocompact spaces that are not compact, though.\n\nIn general, for non-pseudocompact spaces there are always maximal ideals \"m\" in C(\"X\") such that the residue field C(\"X\")/\"m\" is a (non-archimedean) hyperreal field. The framework of non-standard analysis allows for the following alternative characterization of compactness: a topological space \"X\" is compact if and only if every point \"x\" of the natural extension \"*X\" is infinitely close to a point \"x\" of \"X\" (more precisely, \"x\" is contained in the monad of \"x\").\n\nA space \"X\" is compact if its hyperreal extension \"*X\" (constructed, for example, by the ultrapower construction) has the property that every point of \"*X\" is infinitely close to some point of \"X\"⊂\"*X\". For example, an open real interval is not compact because its hyperreal extension *(0,1) contains infinitesimals, which are infinitely close to 0, which is not a point of \"X\".\n\nA continuous image of a compact space is compact.\nThis implies the extreme value theorem: a continuous real-valued function on a nonempty compact space is bounded above and attains its supremum. (Slightly more generally, this is true for an upper semicontinuous function.) As a sort of converse to the above statements, the pre-image of a compact space under a proper map is compact.\n\nA closed subset of a compact space is compact, and a finite union of compact sets is compact.\n\nThe product of any collection of compact spaces is compact. (This is Tychonoff's theorem, which is equivalent to the axiom of choice.)\n\nEvery topological space \"X\" is an open dense subspace of a compact space having at most one point more than \"X\", by the Alexandroff one-point compactification. By the same construction, every locally compact Hausdorff space \"X\" is an open dense subspace of a compact Hausdorff space having at most one point more than \"X\".\n\nA nonempty compact subset of the real numbers has a greatest element and a least element.\n\nLet \"X\" be a simply ordered set endowed with the order topology. Then \"X\" is compact if and only if \"X\" is a complete lattice (i.e. all subsets have suprema and infima).\n\n\n\n\n\n"}
{"id": "346755", "url": "https://en.wikipedia.org/wiki?curid=346755", "title": "Conceptual graph", "text": "Conceptual graph\n\nConceptual graphs (CGs) are a formalism for knowledge representation. In the first published paper on CGs, John F. Sowa used them to represent the conceptual schemas used in database systems. The first book on CGs applied them to a wide range of topics in artificial intelligence, computer science, and cognitive science.\n\nIn this approach, a formula in first-order logic (predicate calculus) is represented by a labeled graph.\n\nA linear notation, called the Conceptual Graph Interchange Format (CGIF), has been standardized in the ISO standard for common logic.\n\nThe diagram above is an example of the \"display form\" for a conceptual graph. Each box is called a \"concept node\", and each oval is called a \"relation node\". In CGIF, this CG would be represented by the following statement:\n\nIn CGIF, brackets enclose the information inside the concept nodes, and parentheses enclose the information inside the relation nodes. The letters x and y, which are called \"coreference labels\", show how the concept and relation nodes are connected. In CLIF, those letters are mapped to variables, as in the following statement:\n\nAs this example shows, the asterisks on the coreference labels and in CGIF map to existentially quantified variables in CLIF, and the question marks on and map to bound variables in CLIF. A universal quantifier, represented in CGIF, would be represented in CLIF.\n\nReasoning can be done by translating graphs into logical formulas, then applying a logical inference engine.\n\nAnother research branch continues the work on existential graphs of Charles Sanders Peirce, which were one of the origins of conceptual graphs as proposed by Sowa. In this approach, developed in particular by Dau , conceptual graphs are conceptual diagrams rather than graphs in the sense of graph theory, and reasoning operations are performed by operations on these diagrams.\n\nKey features of GBKR, the graph-based knowledge representation and reasoning model developed by Chein and Mugnier and the Montpellier group , can be summarized as follows:\n\n\nCOGITANT and COGUI are tools that implement the GBKR model. COGITANT is a library of C++ classes that implement most of the GBKR notions and reasoning mechanisms. COGUI is a graphical user interface dedicated to the construction of a GBKR knowledge base (it integrates COGITANT and, among numerous functionalities, it contains a translator from GBKR to RDF/S and conversely).\n\nSentence generalization and generalization diagrams can be defined as a special sort of conceptual graphs which can be constructed automatically from syntactic parse trees and support semantic classification task . Similarity measure between syntactic parse trees can be done as a generalization operation on the lists of sub-trees of these trees. The diagrams are representation of mapping between the syntax generalization level and semantics generalization level (anti-unification of logic forms). Generalization diagrams are intended to be more accurate semantic representation than conventional conceptual graphs for individual sentences because only syntactic commonalities are represented at semantic level.\n\n\n\n"}
{"id": "16373888", "url": "https://en.wikipedia.org/wiki?curid=16373888", "title": "Contemplative psychotherapy", "text": "Contemplative psychotherapy\n\nContemplative psychotherapy is an approach to psychotherapy that includes the use of personal contemplative practices and insights informed by the spiritual tradition of Buddhism. Contemplative psychotherapy differs from other, more traditional methods of counseling in that the therapist brings to the therapeutic relationship qualities of mindfulness and compassion in order to help clients access their fundamental goodness and natural wisdom. The practice of Contemplative Psychotherapy grew out of a dialogue between Tibetan Buddhist master Chogyam Trungpa Rinpoche and Western psychologists and psychiatrists. This discussion led to the opening of the Contemplative Psychotherapy Department at Naropa University in 1978 by Edward M. Podvoll, a psychiatrist, psychoanalyst and dedicated student of Trungpa.\n\nContemplative psychotherapy may be said to have two parents: the 2,500-year-old wisdom tradition of Buddhism and the clinical traditions of Western Psychology, especially the Humanistic school. Like all offspring it has much in common with both of its parents and yet is uniquely itself at the same time. From Buddhism comes the practice of mindfulness/awareness meditation, together with a highly sophisticated understanding of the functioning of the mind in sanity and in confusion. From Western psychology come the investigation of the stages of human development, a precise language for discussing mental disturbance and the intimate method of working with others known as \"psychotherapy\". The root teaching of the contemplative psychotherapy program is the notion of \"brilliant sanity\". This means that we all have within us a natural dignity and wisdom. Practitioners of contemplative psychotherapy become experts at recognizing sanity within even the most confused and distorted states of mind and are trained to nurture this sanity in themselves and in their clients.\n\nThe basic premise of contemplative psychotherapy rests on the notion of \"brilliant sanity\", which suggests that we all have within us a natural dignity and wisdom and that our basic nature is characterized by clarity, openness, and compassion. This wisdom may be temporarily covered over, but it is there and may be cultivated. Practitioners of contemplative psychotherapy become experts at recognizing sanity within even the most confused and distorted states of mind and are trained to nurture this sanity in themselves and in their clients. \n\nBuddhist psychology emphasizes the primacy of immediate experience. In the training of a contemplative psychotherapist, theoretical training is balanced with experiential training. By studying and experiencing his or her own mind, the contemplative therapist can then study and experience accurately the mind of others while engaging in therapeutic practices. The study of one's own mind can be achieved through meditation practice and body/mind awareness disciplines.\n\nSpace awareness practice is designed to intensify and familiarize oneself with different\nemotional and psychological states: both the \"wisdom\" aspects and the confused aspects. Maitri space awareness practice, when integrated with sitting meditation within a community environment, can assist practitioners in recognizing their own patterns, become friendly toward themselves in different states of mind, and develop genuine humor and compassion toward themselves and others. This often leads to relaxation and fearlessness in working with others. \n\nContemplative psychotherapy makes use of the Body-Speech-Mind approach to clinical supervision as a way to bring the client, the client's world and the therapeutic relationship (as experienced by the contemplative psychotherapist) vividly into the group supervisory situation. This presence in turn is used to directly facilitate working with energetic and conceptual obstacles and provides a basis for subsequent therapeutic interventions.\n"}
{"id": "13463690", "url": "https://en.wikipedia.org/wiki?curid=13463690", "title": "Contraction principle (large deviations theory)", "text": "Contraction principle (large deviations theory)\n\nIn mathematics — specifically, in large deviations theory — the contraction principle is a theorem that states how a large deviation principle on one space \"pushes forward\" (via the pushforward of a probability measure) to a large deviation principle on another space \"via\" a continuous function.\n\nLet \"X\" and \"Y\" be Hausdorff topological spaces and let (\"μ\") be a family of probability measures on \"X\" that satisfies the large deviation principle with rate function \"I\" : \"X\" → [0, +∞]. Let \"T\" : \"X\" → \"Y\" be a continuous function, and let \"ν\" = \"T\"(\"μ\") be the push-forward measure of \"μ\" by \"T\", i.e., for each measurable set/event \"E\" ⊆ \"Y\", \"ν\"(\"E\") = \"μ\"(\"T\"(\"E\")). Let\n\nwith the convention that the infimum of \"I\" over the empty set ∅ is +∞. Then:\n\n"}
{"id": "97645", "url": "https://en.wikipedia.org/wiki?curid=97645", "title": "Death by burning", "text": "Death by burning\n\nDeath by burning is an execution method involving deliberately causing death through the effects of combustion or exposure to extreme heat. It has a long history as a form of capital punishment, and many societies have employed it for activities considered criminal such as treason, rebellious actions by slaves, heresy, witchcraft, arson (in Japan) and sexual transgressions, such as incest or homosexuality.\n\nThe best known executions of this type are those where the condemned is bound to a large wooden stake and a fire lit beneath them. This is usually called burning at the stake, or in some cases, \"auto-da-fé\". For burnings at the stake, if the fire was large (for instance, when a number of prisoners were executed at the same time), death often came from carbon monoxide poisoning before flames actually caused lethal harm to the body. If the fire was small, however, the condemned would burn for some time until death from hypovolemia (the loss of blood or other fluids, since extensive burns often require large amounts of intravenous fluid, because the subsequent inflammatory response causes significant capillary fluid leakage and oedema), heatstroke or the simple thermal decomposition of vital body parts.\n\nOther forms of death resulting from exposure to extreme heat are known. For example, pouring substances such as molten metal onto a person (or down their throat or into their ears), as well as enclosing persons within, or attaching them to, metal contraptions subsequently heated. Immersion in a heated liquid as a form of execution is considered distinct from death by burning, and classified as death by boiling.\n\nThe 18th century BC law code promulgated by Babylonian king Hammurabi specifies several crimes in which death by burning was thought appropriate. Looters of houses on fire could be cast into the flames, and priestesses who abandoned cloisters and began frequenting inns and taverns could also be punished by being burnt alive. Furthermore, a man who began committing incest with his mother after the death of his father could be ordered by courts to be burned alive.\n\nIn Ancient Egypt, several incidents of burning alive perceived rebels are attested. For example, Senusret I (r. 1971–1926 BC) is said to have rounded up the rebels in campaign, and burnt them as human torches. Under the civil war flaring under Takelot II more than a thousand years later, the Crown Prince Osorkon showed no mercy, and burned several rebels alive. On the statute books, at least, women committing adultery might be burned to death. Jon Manchip White, however, did not think capital judicial punishments were often carried out, pointing to the fact that the pharaoh had to personally ratify each verdict. Furthermore, the Greek historian Diodorus Siculus (fl. 1st century BC) asserts that the Egyptians had a particularly terrible punishment for children who murdered their parents: With sharpened reeds, bits of flesh the size of a finger were cut from the criminal's body. Then he was placed on a bed of thorns and burnt alive.\n\nIn the Middle Assyrian period, paragraph 40 in a preserved law text concerns the obligatory unveiled face for the professional prostitute, and the concomitant punishment if she violated that by veiling herself (the way wives were to dress in public):\nFor the Neo-Assyrians, mass executions seem to have been not only designed to instill terror and to enforce obedience, but also as proof of their might. For example, Neo-Assyrian King Asuhurnasirpal II (r. 883–859 BC) was evidently proud enough of his bloody work that he committed it to monument and eternal memory as follows:\n\nIn Genesis 38, Judah orders Tamar—the widow of his son, living in her father's household—to be burned when she is believed to have become pregnant by an extramarital sexual relation. Tamar saves herself by proving that Judah is himself the father of her child. In the Book of Jubilees, the same story is basically told, with some intriguing differences, according to Caryn A. Reeder. In Genesis, Judah is exercising his patriarchal power at a distance, whereas he and the relatives seem more actively involved in Tamar's impending execution.\n\nIn Hebraic law, death by burning was prescribed for ten forms of sexual crimes: The imputed crime of Tamar, namely that a married daughter of a priest commits adultery, and nine versions of relationships considered as incestuous, such as having sex with one's own daughter, or granddaughter, but also, for example, to have sex with one's mother-in-law or with one's wife's daughter.\n\nIn the Mishnah, the following manner of burning the criminal is described:\nThat is, the person dies from being fed molten lead. The Mishnah is, however, a fairly late collections of laws, from about the 3rd century AD, and scholars believe it \"replaced\" the actual punishment of burning in the old biblical texts.\n\nIn the 6th century AD collection of the sayings and rulings of the pre-eminent jurists from earlier ages, the Digest, a number of crimes are regarded as punishable by death by burning. The 3rd century jurist Ulpian, for example, says that enemies of the state, and deserters to the enemy are to be burned alive. His rough contemporary, the juristical writer Callistratus mentions that arsonists are typically burnt, as well as slaves who have conspired against the well-being of their masters (this last also, on occasion, being meted out to free persons of \"low rank\"). The punishment of burning alive arsonists (and traitors) seems to have been particularly ancient; it was included in the Twelve Tables, a mid-5th BC law code, that is, about 700 years prior to the times of Ulpian and Callistratus. According to ancient reports, Roman authorities executed many of the early Christian martyrs by burning. An example of this is the earliest chronicle of a martyrdom, that of Polycarp. Sometimes this was by means of the \"tunica molesta\", a flammable tunic:\n\nIn AD 326, Constantine the Great promulgated a law that increased the penalties for parentally non-sanctioned \"abduction\" of their girls, and concomitant sexual intercourse/rape. The man would be burnt alive without the possibility of appeal, and the girl would receive the same treatment if she had participated willingly. Nurses who had corrupted their female wards and led them to sexual encounters would have molten lead poured down their throats. In the same year, Constantine also passed a law that said if a woman married her own slave, both would be subjected to capital punishment, the slave by burning. In AD 390, Emperor Theodosius issued an edict against male prostitutes and brothels offering such services; those found guilty should be burned alive.\n\nBeginning in the early 3rd century BC, Greek and Roman writers have commented on the purported institutionalized child sacrifice the North African Carthaginians are said to have performed in honour of the gods Baal Hammon and Tanit. The earliest writer, Cleitarchus is among the most explicit. He says live infants were placed in the arms of a bronze statue, the statue's hands over a brazier, so that the infant slowly rolled into the fire. As it did so, the limbs of the infant contracted and the face was distorted into a sort of laughing grimace, hence called \"the act of laughing\". Other, later authors such as Diodorus Siculus and Plutarch says the throats of the infants were generally cut, before they were placed in the statue's embrace In the vicinity of ancient Carthage, large scale grave yards containing the incinerated remains of infants, typically up to the age of 3, have been found; such graves are called \"tophets\". However, some scholars have argued that these findings are not evidence of \"systematic\" child sacrifice, and that estimated figures of ancient natural infant mortality (with cremation afterwards and reverent separate burial) might be the real historical basis behind the hostile reporting from non-Carthaginians. A late charge of the imputed sacrifice is found by the North African bishop Tertullian, who says that child sacrifices were still carried out, in secret, in the countryside at his time, 3rd century AD.\n\nAccording to Julius Caesar, the ancient Celts practiced the burning alive of humans in a number of settings. For example, in Book 6, chapter 16, he writes of the Druidic sacrifice of criminals within huge wicker frames shaped as men:\nSlightly later, in Book 6, chapter 19, Caesar also says the Celts perform, on the occasion of death of great men, the funeral sacrifice on the pyre of living slaves and dependants ascertained to have been \"beloved by them\". Earlier on, in Book 1, chapter 4, he relates of the conspiracy of the nobleman Orgetorix, charged by the Celts for having planned a \"coup d'état\", for which the customary penalty would be burning to death. It is said Orgetorix committed suicide to avoid that fate.\n\nThroughout the 12th–14th centuries, a number of non-Christian peoples living around the Eastern Baltic Sea, such as Old Prussians and Lithuanians were charged by Christian writers with performing human sacrifice. For example, Pope Gregory IX issued a papal bull denouncing an alleged practice among the Prussians, that girls were dressed in fresh flowers and wreaths and were then burned alive as offerings to evil spirits.\n\nUnder 6th-century emperor Justinian I, the death penalty had been decreed for impenitent Manicheans, but a specific punishment was not made explicit. By the 7th century, however, those found guilty of \"dualist heresy\" could risk being burned at the stake. Those found guilty of performing magical rites, and corrupting sacred objects in the process, might face death by burning, as evidenced in a 7th-century case. In the 10th century AD, the Byzantines instituted death by burning for parricides, i.e. those who had killed their own relatives, replacing the older punishment of \"poena cullei\", the stuffing of the convict in a leather sack along with a rooster, a viper, a dog and a monkey, and then throwing the sack into the sea.\n\nCivil authorities burned persons judged to be heretics under the medieval Inquisition. Burning heretics had become customary practice in the latter half of the twelfth century in continental Europe, and death by burning became \"statutory\" punishment from the early 13th century. Death by burning for heretics was made positive law by Pedro II of Aragon in 1197. In 1224, Frederick II, Holy Roman Emperor, made burning a legal alternative, and in 1238, it became the principal punishment in the Empire. In Sicily, the punishment was made law in 1231, whereas in France, Louis IX made it binding law in 1270.\n\nAs England in the 15th century grew weary of the teachings of John Wycliffe and the Lollards, kings, priests, and parliaments reacted with fire. In 1401, Parliament passed the De heretico comburendo act, which can be loosely translated as \"Regarding the burning of heretics.\" Lollard persecution would continue for over a hundred years in England. The Fire and Faggot Parliament met in May 1414 at Grey Friars Priory in Leicester to lay out the notorious Suppression of Heresy Act 1414, enabling the burning of heretics by making the crime enforceable by the Justices of the peace. John Oldcastle, a prominent Lollard leader, was not saved from the gallows by his old friend King Henry V. Oldcastle was hanged and his gallows burned in 1417. Jan Hus was burned at the stake after being accused at the Roman Catholic Council of Constance (1414–18) of heresy. The ecumenical council also decreed that the remains of John Wycliffe, dead for 30 years, should be exhumed and burned. (This posthumous execution was carried out in 1428.)\n\nSeveral incidents are recorded of massacres on Jews from the 12th through 16th centuries in which they were burned alive, often on account of the blood libel. In 1171 in Blois, for example, 51 Jews were burned alive (the entire adult community). In 1191, King Philip Augustus ordered around 100 Jews burnt alive. That Jews purportedly performed host desecration also led to mass burnings; In 1243 in Beelitz, the entire Jewish community was burnt alive, and in 1510 in Berlin, some 26 Jews were burnt alive for the same crime. During the \"Black Death\" in the mid-14th century a spate of large-scale massacres occurred. One libel was that the Jews had poisoned the wells. In 1349, as panic grew along with the increasing death toll from the plague, general massacres, but also specifically mass burnings, began to occur. Six hundred (600) Jews were burnt alive in Basel alone. A large mass burning occurred in Strasbourg, where several hundred Jews were burnt alive in what became known as the Strasbourg massacre.\n\nA Jewish male, Johannes Pfefferkorn, met a particularly gruesome death in 1514 in Halle. He had been charged with a number of crimes, such as having impersonated a priest for twenty years, performed host desecration, stolen Christian children to be tortured and killed by other Jews, poisoning 13 people and poisoning wells. He was lashed to a pillar in such a way that he could run about it. Then, a ring of glowing coal was made around him, a fiery ring that was gradually pushed ever closer to him, until he was roasted to death.\n\nNot only Jews could be victims of mass hysteria on charges like that of poisoning wells. This particular charge, well-poisoning, was the basis for a large scale hunt of lepers in 1321 France. In the spring of 1321, in Périgueux, people became convinced that the local lepers had poisoned the wells, causing ill-health among the normal populace. The lepers were rounded up and burned alive. The action against the lepers didn't stay local, though, but had repercussions throughout France, not least because King Philip V issued an order to arrest all lepers, those found guilty to be burnt alive. Jews became tangentially included as well; at Chinon alone, 160 Jews were burnt alive. All in all, around 5000 lepers and Jews are recorded in one tradition to have been killed during the Lepers' Plot hysteria.\n\nThe charge of the lepers' plot was not wholly confined to France; existent records from England show that on Jersey the same year, at least one family of lepers were burnt alive for having poisoned others.\n\nThe Spanish Inquisition was established in 1478, with the aim of preserving Catholic orthodoxy; some of its principal targets were \"Marranos\", formally converted Jews thought to have relapsed into Judaism, or the Moriscos, formally converted Muslims thought to have relapsed into Islam. The public executions of the Spanish Inquisition were called autos-da-fé; convicts were \"released\" (handed over) to secular authorities in order to be burnt.\n\nEstimates of how many were executed on behest of the Spanish Inquisition have been offered from early on; historian Hernando del Pulgar (1436–c. 1492) estimated that 2000 people were burned at the stake between 1478 and 1490. Estimates range from 30,000 to 50,000 burnt at the stake (alive or not) at the behest of the Spanish Inquisition during its 300 years of activity have previously been given and are still to be found in popular books.\n\nIn February 1481, in what is said to be the first auto-da-fé, six Marranos were burnt alive in Seville. In November 1481, 298 Marranos were burnt publicly at the same place, their property confiscated by the Church. Not all Maranos executed by being burnt at the stake seem to have been burnt alive. If the Jew \"confessed his heresy\", the Church would show mercy, and he would be strangled prior to the burning. Autos-da-fé against Maranos extended beyond the Spanish heartland. In Sicily, in 1511–15, 79 were burnt at the stake, while from 1511 to 1560, 441 Maranos were condemned to be burned alive. In Spanish American colonies, autos-da-fé were held as well. For example, in 1664, a man and his wife were burned alive in Río de la Plata, and in 1699, a Jew was burnt alive in Mexico City.\n\nIn 1535, five Moriscos were burned at the stake on Majorca, the images of a further four were also burnt in effigy, since the actual individuals had managed to flee. During the 1540s, some 232 Moriscos were paraded in autos-da-fé in Zaragoza; five of those were burnt at the stake. The claim that out of 917 Moriscos appearing in autos of the Inquisition in Granada between 1550–95, just 20 were executed seems at odds with the English government's state papers which claim that, while at war with Spain, they received a report from Seville of 17 June 1593 that over 70 of the richest men of Granada were burnt. As late as 1728 as many as 45 Moriscos were recorded burned for heresy. In the May 1691 \"bonfire of the Jews\", Rafael Valls, Rafael Benito Terongi and Catalina Terongi were burned alive.\n\nIn 1560, the Portuguese Inquisition opened offices in the Indian colony Goa, known as Goa Inquisition. Its aim was to protect Catholic orthodoxy among new converts to Christianity, and retain hold on the old, particularly against \"Judaizing\" deviancy. From the 17th century, Europeans were shocked at the tales of how brutal and extensive the activities of the Inquisition were. What modern scholars have established, is that some 4046 individuals in the time 1560–1773 received some sort of punishment from the Portuguese Inquisition, whereof 121 persons were condemned to be burned alive, of those 57 who actually suffered that fate, while the rest escaped it, and were burnt in effigy, instead. For the Portuguese Inquisition in total, not just at Goa, modern estimates of persons actually executed on its behest is about 1200, whether burnt alive or not.\n\nFrom the 12th to the 18th centuries, various European authorities legislated (and held judicial proceedings) against sexual crimes such as sodomy or bestiality; often, the prescribed punishment was that of death by burning. Many scholars think that the first time death by burning appeared within explicit codes of law for the crime of sodomy was at the ecclesiastical 1120 Council of Nablus in the crusader Kingdom of Jerusalem. Here, if public repentance were done, the death penalty might be avoided. In Spain, the earliest records for executions for the crime of sodomy are from the 13th–14th centuries, and it is noted there that the preferred mode of execution was death by burning. The Partidas of King Alfonso \"El Sabio\" condemned sodomites to be castrated and hung upside down to die from the bleeding, following the old testament phrase \"their blood shall be upon them\". At Geneva, the first recorded burning of sodomites occurred in 1555, and up to 1678, some two dozen met the same fate. In Venice, the first burning took place in 1492, and a monk was burnt as late as 1771. The last case in France where two men were condemned by court to be burned alive for engaging in consensual homosexual sex was in 1750 (although, it seems, they were actually strangled prior to being burned). The last case in France where a man was condemned to be burned for a murderous rape of a boy occurred in 1784.\n\nCrackdowns and the public burning of a couple of homosexuals might lead to local panic, and persons thus inclined fleeing from the place. The traveller William Lithgow witnessed such a dynamic when he visited Malta in 1616 :\n\nThe actual punishment meted out to, for example, pederasts could differ according to status. While both in 1532 and 1409 Augsburg two men were burned alive for their offenses, a rather different procedure was meted out to four \"clerics\" in the 1409 case guilty of the same offence: Instead of being burnt alive, they were locked into a wooden casket that was hung up in the Perlachturm and they starved to death in that manner.\n\nIn 1532, Holy Roman Emperor Charles V promulgated his penal code Constitutio Criminalis Carolina. A number of crimes were punishable with death by burning, such as coin forgery, arson, and sexual acts \"contrary to nature\". Also, those guilty of aggravated theft of sacred objects from a church could be condemned to be burnt alive. Only those found guilty of \"malevolent\" witchcraft could be punished by death by fire.\n\nAccording to the jurist , the last case he knew of where a person had been judicially burned alive on account of arson in Germany happened in 1804, in , close by Eisenach. The manner in which Johannes Thomas was executed on 13 July that year is described as follows: Some feet above the actual pyre, attached to a stake, a wooden chamber had been constructed, into which the delinquent was placed. Pipes or chimneys, filled with sulphuric material led up to the chamber, and that was first lit, so that Thomas died from inhaling the sulphuric smoke, rather than being strictly burnt alive, before his body was consumed by the general fire. Some 20,000 people had gathered to watch Thomas' execution.\n\nAlthough Thomas is regarded as the last to have been actually executed by means of fire (in this case, through suffocation), the couple Johann Christoph Peter Horst and his lover Friederike Louise Christiane Delitz, who had made a career of robberies in the confusion made by their acts of arson, were condemned to be burnt alive in Berlin 28 May 1813. They were, however, according to Gustav Radbruch, secretly strangled just prior to being burnt, namely when their arms and legs were tied fast to the stake.\n\nAlthough these two cases are the last where execution by burning might be said to have been \"carried out\" in some degree, Eduard Osenbrüggen mentions that \"verdicts\" to be burned alive were given in several cases in different German states afterwards, such as in cases from 1814, 1821, 1823, 1829 and finally in a case from 1835.\n\nBurning was used by Christians during the witch-hunts of Europe. The penal code known as the Constitutio Criminalis Carolina (1532) decreed that sorcery throughout the Holy Roman Empire should be treated as a criminal offence, and if it purported to inflict injury upon any person the witch was to be burnt at the stake. In 1572, Augustus, Elector of Saxony imposed the penalty of burning for witchcraft of every kind, including simple fortunetelling. From the latter half of the 18th century, the number of \"nine million witches burned in Europe\" has been bandied about in popular accounts and media, but has never had a following among specialist researchers. Today, based on meticulous study of trial records, ecclesiastical and inquisitorial registers and so on, as well as on the utilization of modern statistical methods, the specialist research community on witchcraft has reached an agreement for roughly 40,000–50,000 people executed for witchcraft in Europe in total, and by no means all of them executed by being burned alive. Furthermore, it is solidly established that the peak period of witch-hunts was the century 1550–1650, with a slow increase preceding it, from the 15th century onward, as well as a sharp drop following it, with \"witch-hunts\" having basically fizzled out by the first half of the 18th century.\n\nNotable individuals executed by burning include Jacques de Molay (1314), Jan Hus (1415), Joan of Arc (1431), Girolamo Savonarola (1498), Patrick Hamilton (1528), John Frith (1533), William Tyndale (1536), Michael Servetus (1553), Giordano Bruno (1600), Urbain Grandier (1634), and Avvakum (1682). Anglican martyrs John Rogers, Hugh Latimer and Nicholas Ridley were burned at the stake in 1555. Thomas Cranmer followed the next year (1556).\n\nIn Denmark, after the 1536 reformation, Christian IV of Denmark (r. 1588–1648) encouraged the practice of burning witches, in particular by the law against witchcraft in 1617. In Jutland, the mainland part of Denmark, more than half the recorded cases of witchcraft in the 16th and 17th centuries occurred after 1617. Rough estimates says about a thousand persons were executed due to convictions for witchcraft in the 1500–1600s, but it is not wholly clear if all of the transgressors were burned to death.\n\nMary I ordered hundreds of Protestants burnt at the stake during her reign (1553–58) in what would be known as the \"Marian Persecutions\" earning her the epithet of 'Bloody' Mary. Many of those martyred by Mary and the Roman Catholic Church are listed in \"Actes and Monuments\", written by Foxe in 1563 and 1570.\nEdward Wightman, a Baptist from Burton on Trent, was the last person burned at the stake for heresy in England in Lichfield, Staffordshire on 11 April 1612. Although cases can be found of burning heretics in the 16th and 17th centuries England, that penalty for heretics was historically relatively new. For example, it did not exist in 14th century England, and when the bishops in England petitioned King Richard II to institute death by burning for heretics in 1397, he flatly refused, and no one was burnt for heresy during his reign. Just one year after his death, however, in 1401, William Sawtrey was burnt alive for heresy. Death by burning for heresy was formally abolished by King Charles II in 1676.\n\nThe traditional punishment for women found guilty of treason was to be burned at the stake, where they did not need to be publicly displayed naked, whereas men were hanged, drawn and quartered. The jurist William Blackstone argued as follows for the differential punishment of females vs. males:\nHowever, as described in Camille Naish's \"Death Comes to the Maiden\", in practice, the woman's shift would burn away at the beginning, and she would be left naked anyway. There were two types of treason, high treason for crimes against the sovereign, and petty treason for the murder of one's lawful superior, including that of a husband by his wife. Commenting on the 18th century execution practice, Frank McLynn says that most convicts condemned to burning were not burnt alive, and that the executioners made sure the women were dead before consigning them to the flames.\n\nThe last to have been condemned to death for \"petty treason\" was Mary Bailey, whose body was burned in 1784. The last woman to be convicted for \"high treason\", and have her body burnt, in this case for the crime of coin forgery, was Catherine Murphy in 1789. The last case where a woman was actually burnt alive in England is that of Catherine Hayes in 1726, for the murder of her husband. In this case, one account says this happened because the executioner accidentally set fire to the pyre before he had hanged Hayes properly. The historian Rictor Norton has assembled a number of contemporary newspaper reports on the actual death of Mrs. Hayes, internally somewhat divergent. The following excerpt is one example: \n\nJames VI of Scotland (later James I of England) shared the Danish king's interest in witch trials. This special interest of the king resulted in the North Berwick witch trials, which led more than seventy people to be accused of witchcraft in Scotland due to inclement weather. James sailed in 1590 to Denmark to meet his betrothed, Anne of Denmark, who, ironically, is believed by some to have secretly converted to Roman Catholicism herself from Lutheranism around 1598, although historians are divided on whether she ever was received into the Roman Catholic faith.\n\nThe last to be executed as a witch in Scotland was Janet Horne in 1727, condemned to death for using her own daughter as a flying horse in order to travel. Janet Horne was burnt alive in a tar barrel.\n\nPetronilla de Meath (c. 1300–1324) was the maidservant of Dame Alice Kyteler, a 14th-century Hiberno-Norman noblewoman. After the death of Kyteler's fourth husband, the widow was accused of practicing witchcraft and Petronilla of being her accomplice. Petronilla was tortured and forced to proclaim that she and Kyteler were guilty of witchcraft. Petronilla was then flogged and eventually burnt at the stake on 3 November 1324, in Kilkenny, Ireland. Hers was the first known case in the history of the British Isles of death by fire for the crime of heresy. Kyteler was charged by the Bishop of Ossory, Richard de Ledrede, with a wide slate of crimes, from sorcery and demonism to the murders of several husbands. She was accused of having illegally acquired her wealth through witchcraft, which accusations came principally from her stepchildren, the children of her late husbands by their previous marriages. The trial predated any formal witchcraft statute in Ireland, thus relying on ecclesiastical law (which treated witchcraft as heresy) rather than English common law (which treated it as a felony). Under torture, Petronilla claimed she and her mistress applied a magical ointment to a wooden beam, which enabled both women to fly. She was then forced to proclaim publicly that Lady Alice and her followers were guilty of witchcraft. Some were convicted and whipped, but others, Petronilla included, were burnt at the stake. With the help of relatives, Alice Kyteler fled, taking with her Petronilla's daughter, Basilia.\n\nIn 1895, Bridget Cleary (née Boland), a County Tipperary woman, was burnt by her husband and others, the stated motive for the crime being the belief that the real Bridget had been abducted by fairies with a changeling left in her place. Her husband claimed to have slain only the changeling. The gruesome nature of the case prompted extensive press coverage. The trial was closely followed by newspapers in both Ireland and Britain. As one reviewer commented, nobody, with the possible exception of the presiding judge, thought it was an ordinary murder case.\n\nIn Massachusetts, there are two known cases of burning at the stake. First, in 1681, a slave named Maria tried to kill her owner by setting his house on fire. She was convicted of arson and burned at the stake in Roxbury. Concurrently, a slave named Jack, convicted in a separate arson case, was hanged at a nearby gallows, and after death his body was thrown into the fire with that of Maria. Second, in 1755, a group of slaves had conspired and killed their owner, with servants Mark and Phillis executed for his murder. Mark was hanged and his body gibbeted, and Phillis burned at the stake, at Cambridge.\n\nIn Montreal, then part of New France, Marie-Joseph Angélique, a black slave, was sentenced to being burned alive for an arson which destroyed 45 homes and a hospital in 1734. The sentence was commuted on appeal to burning after death by stangulation.\n\nIn New York, several burnings at the stake are recorded, particularly following suspected slave revolt plots. In 1708, one woman was burnt and one man hanged. In the aftermath of the New York Slave Revolt of 1712, 20 people were burnt (one of the leaders slowly roasted, before he died after 10 hours of torture) and during the alleged slave conspiracy of 1741, at least 13 slaves were burnt at the stake.\n\nBartolomé de las Casas, a 16th-century eyewitness to the brutal subjugation of the Native Americans by the Spanish conquistadores, has left a particularly harrowing description of how roasting alive was a favoured technique of repression:\nThe last known burning by the Spanish Colonial government in Latin America was of Mariana de Castro, in Lima, Peru in February 1732.\n\nIn 1760, the slave rebellion known as Tacky's War broke out in Jamaica. Apparently, some of the defeated rebels were burned alive, while others were gibbeted alive, left to die of thirst and starvation.\n\nIn 1774, nine African slaves at Tobago were found complicit of murdering a white man. Eight of them had first their right arms chopped off, and were then burned alive bound to stakes, according to the report of an eyewitness.\n\nIn 1855 the Dutch abolitionist and historian spoke to the Anti Slavery Society in Amsterdam. Painting a dark picture of the condition of slaves in Suriname, he mentions in particular that as late as in 1853, just two years previously, \"three Negroes were burnt alive\".\n\nThe Greek War of Independence in the 1820s contained several instances of death by burning. When the Greeks in April 1821 captured a corvette near Hydra, the Greeks chose to roast to death the 57 Ottoman crew members. After the fall of Tripolitsa in September 1821, European officers were horrified to note that not only were Muslims suspected of hiding money being slowly roasted after having had their arms and legs cut off but, in one instance, three Muslim children were roasted over a fire while their parents were forced to watch. On their part, the Ottomans committed many similar acts; for example, in retaliation they gathered up Greeks in Constantinople, throwing several of them into huge ovens, baking them to death.\n\nThe Arab chieftain Tulayha ibn Khuwaylid ibn Nawfal al-Asad set himself up as a prophet in AD 630. Tulayha had a strong following which was, however, soon quashed in the so-called Ridda Wars. He himself escaped, though, and later was reconverted to Islam, but many of his rebel followers were burnt to death; his mother chose to embrace the same fate.\n\nA number of monks are said to have been burnt alive in Tunis and Morocco in the 13th century. In 1243, two English monks, Brothers Rodulph and Berengarius, after having secured the release of some 60 captives, were charged with being English spies, and were burnt alive on 9 September. In 1262, Brothers Patrick and William, again having freed captives, but also sought to proselytize among Muslims, were burnt alive in Morocco. In 1271, 11 Catholic monks were burnt alive in Tunis. Several other cases are reported.\n\nApostasy, i.e. the act of converting to another religion, was (and remains so in a few countries) punishable with death.\nThe French traveller Jean de Thevenot, traveling the East in the 1650s, says: \"Those that turn Christians, they burn alive, hanging a bag of Powder about their neck, and putting a pitched Cap upon their Head.\" Travelling the same regions some 60 years earlier, Fynes Moryson writes:\n\n\"Certain accursed ones of no significance\" is the term used by Taş Köprü Zade in the \"Şakaiki Numaniye\" to describe some members of the Hurufiyya who became intimate with the Sultan Mehmed II to the extent of initiating him as a follower. This alarmed members of the Ulema, particularly Mahmut Paşa, who then consulted Mevlana Fahreddin. Fahreddin hid in the Sultan's palace and heard the Hurufis propound their doctrines. Considering these heretical, he reviled them with curses. The Hurufis fled to the Sultan, but Fahreddin's denunciation of them was so virulent that Mehmed II was unable to defend them. Farhreddin then took them in front of the Üç Şerefeli Mosque, Edirne, where he publicly condemned them to death. While preparing the fire for their execution, Fahreddin accidentally set fire to his beard. However the Hurufis were burnt to death.\n\nJohn Braithwaite, staying in Morocco in the late 1720s, says that apostates from Islam would be burnt alive:\nSimilarly, he notes that non-Muslims entering mosques or being blasphemous against Islam will be burnt, unless they convert to Islam. The chaplain for the English in Algiers at the same time, Thomas Shaw, wrote that whenever capital crimes were committed either by Christian slaves or Jews, the Christian or Jew was to be burnt alive. Several generations later, in Morocco in 1772, a Jewish interpreter to the British, and a merchant in his own right, sought from the Emperor of Morocco restitution for some goods confiscated, and was burnt alive for his impertinence. His widow made her woes clear in a letter to the British.\n\nIn 1792 in Ifrane, Morocco, 50 Jews preferred to be burned alive, rather than convert to Islam. In 1794 in Algiers, the Jewish Rabbi Mordecai Narboni was accused of having maligned Islam in a quarrel with his neighbour. He was ordered to be burnt alive unless he converted to Islam, but he refused and was therefore executed on 14 July 1794.\n\nIn 1793, Ali Benghul made a short-lived \"coup d'état\" in Tripoli, deposing the ruling Karamanli dynasty. During his short, violent reign he seized for example, the two interpreters for the Dutch and English consuls, both of them Jews, and roasted them over a slow fire, on charges of conspiracy and espionage.\n\nDuring a famine in Persia in 1668, the government took severe measures against those trying to profiteer from the misfortune of the populace. Restaurant owners found guilty of profiteering were slowly roasted on spits, and greedy bakers were baked in their own ovens.\n\nA physician, Dr C.J. Wills, traveling through Persia in 1866–81 noted that shortly before his (Wills') arrival, a \"priest\" had been burned alive. Wills wrote:\nThe previous cases concern primarily death by burning through contact with open fire or burning material; a slightly different principle is to enclose an individual within, or attach him to, a metal contraption which is subsequently heated. In the following, some reports of such incidents, or anecdotes about such are included.\n\nPerhaps the most infamous example of a brazen bull, which is a hollow metal structure shaped like a bull within which the condemned is put, and then roasted alive as the metal bull is gradually heated up, is the one allegedly constructed by Perillos of Athens for the 6th-century BC tyrant Phalaris at Agrigentum, Sicily. As the story goes, the first victim of the bull was its constructor Perillos himself. The historian George Grote was among those regarding this story as having sufficient evidence behind it to be true, and points particularly to that the Greek poet Pindar, working just one or two generations after the times of Phalaris refers to the brazen bull. A bronze bull was, in fact, one of the spoils of victory when the Carthaginians conquered Agrigentum. The story of a brazen bull as an execution device is not wholly unique. About 1000 years later, for example, in AD 497, it can be read in an old chronicle about the Visigoths on the Iberian Peninsula and the south of France:\nWalter Stewart, Earl of Atholl was a Scottish nobleman complicit in the murder of King James I of Scotland. On 26 March 1437 Stewart had a red hot iron crown placed upon his head, was cut in pieces alive, his heart was taken out, and then thrown in a fire. A papal nuncio, the later Pope Pius II witnessed the execution of Stewart and his associate Sir Robert Graham, and, reportedly, said he was at a loss to determine whether the \"crime\" committed by the regicides, or the \"punishment\" of them was the greater.\n\nGyörgy Dózsa led a peasants' revolt in Hungary, and was captured in 1514. He was bound to a glowing iron throne and a likewise hot iron crown was placed on his head, and he was roasted to death.\n\nIn a few English 18th- and 19th-century newspapers and magazines, a tale was circulated about the particularly brutal manner in which a French midwife was put to death on 28 May 1673 in Paris. No fewer than 62 infant skeletons were found buried on her premises, and she was condemned on multiple accounts of abortion/infanticide. One detailed account of her supposed execution runs as follows:\nThe English commentator adds his own view on the matter:\nThe English story is derived from a pamphlet published in 1673.\n\nA number of stories concern individuals who are said to have been executed by having molten gold (melting point 1064 °C/1947 °F) poured down their throats. For example, in 88 BC, Mithridates VI of Pontus captured the Roman general Manius Aquillius, and executed him by pouring molten gold down his throat. A popular but unsubstantiated rumor also had the Parthians executing the famously greedy Roman general Marcus Licinius Crassus in this manner in 53 BC.\nGenghis Khan is said to have poured molten gold down the throat of a perfidious governor in 1220, and an early 14th-century chronicle mentions that his grandson Hulagu Khan did likewise to the sultan Al-Musta'sim after the fall of Baghdad in 1258 to the Mongol army. (Marco Polo's version is that Al-Musta'sim was locked without food or water to starve in his treasure room)\n\nThe Spanish in 16th-century Americas gave horrified reports that the Spanish who had been captured by the natives (who had learnt of the Spanish thirst for gold) had their feet and hands bound, and then molten gold poured down their throats as the victims were mocked: \"Eat, eat gold, Christians\".\n\nFrom the 19th century reports from the Kingdom of Siam (present day Thailand) stated that those who have defrauded the public treasury could have either molten gold or silver poured down their throat.\n\nThe 16th-/early 17th-century prime minister Malik Ambar in the Deccan Ahmadnagar Sultanate would not tolerate inebriation among his subjects, and would pour molten lead (melting point 327 °C/621.43 °F) down the mouths of those caught in that condition. Similarly, in the 17th century Sultanate of Aceh Sultan Iskandar Muda (r. 1607–36) is said to have poured molten lead into the mouths of at least two drunken subjects. Military discipline in 19th-century Burma was reportedly harsh, with strict prohibition of smoking opium or drinking arrack. Some monarchs, it appears, had ordained pouring molten lead down the throats of those who drank anyway, \"but it has been found necessary to relax this severity, in order to conciliate the army\"\n\nShah Safi I of Persia is said to have abhorred tobacco, and apparently in 1634, he prescribed the punishment of pouring molten lead into the throats of smokers.\n\nAccording to historian Pushpa Sharma, stealing a horse was considered the most heinous offence within the Mongol army, and the culprit would either have molten lead poured into his ears, or alternatively, his punishment would be the breaking of the spinal cord or beheading.\n\nApparently, for many centuries, a tradition of devotional self-immolation existed among Buddhist monks in China. One monk who immolated himself in AD 527, explained his intent a year before, in the following manner:\nA severe critic in the 16th century wrote the following comment on this practice:\n\nIn the first half of the 17th century, Japanese authorities sporadically persecuted Christians, with some executions seeing persons being burnt alive. At Nagasaki in 1622, for example, some 25 monks were burnt alive, and in Edo in 1624, 50 Christians were burnt alive.\n\nEven fateful encounters with cannibals are recorded: in 1514, in the Americas, Francis of Córdoba and five companions were, reportedly, caught, impaled on spits, roasted and eaten by the natives. In 1543, such was also the end of a previous bishop, Vincent de Valle Viridi.\n\nIn 1844, the missionary John Watsford wrote a letter about the internecine wars on Fiji, and how captives could be eaten, after being roasted alive:\nThe actual manner of the roasting process was described by the missionary pioneer David Cargill, in 1838:\n\nSati refers to a funeral practice among some communities of Indian subcontinent in which a recently widowed woman immolates herself on her husband's funeral pyre. The first reliable evidence for the practice of \"sati\" appears from the time of the Gupta Empire (AD 400), when instances of sati began to be marked by inscribed memorial stones.\n\nAccording to one model of history thinking, the practice of \"sati\" only became really widespread with the Muslim invasions of India, and the practice of \"sati\" now acquired a new meaning as a means to preserve the honour of women whose men had been slain. As S.S.Sashi lays out the argument, \"The argument is that the practice came into effect during the Islamic invasion of India, to protect their honor from Muslims who were known to commit mass rape on the women of cities that they could capture successfully.\" It is also said that according to the memorial stone evidence, the practice was carried out in appreciable numbers in western and southern parts of India, and even in some areas, before pre-Islamic times. Some of the rulers and activist of the time sought actively to suppress the practice of \"sati\".\n\nThe British began to compile statistics of the incidences of \"sati\" for all their domains from 1815 and onwards. The official statistics for Bengal represents that the practice was much more common here than elsewhere, recorded numbers typically in the range 500-600 per year, up to the year 1829, when the British authorities banned the practice. Since 19th - 20th Century, the practice remains outlawed in Indian subcontinent.\n\nJauhar was a practice among royal Hindu women to prevent capture by Muslim conquerors.\n\nThe practice of burning widows has not been restricted to the Indian subcontinent; at Bali, the practice was called \"masatia\" and, apparently, restricted to the burning of royal widows. Although the Dutch colonial authorities had banned the practice, one such occasion is attested as late as in 1903, probably for the last time. In Nepal, the practice was not banned until 1920.\n\nC.H.L. Hahn wrote that within the O-ndnonga tribe among the Ovambo people in modern-day Namibia, abortion was not used at all (in contrast to among the other tribes), and that furthermore, if two young unwed individuals had sex resulting in pregnancy, then both the girl and the boy were \"taken out to the bush, bound up in bundles of grass and ... burnt alive.\"\n\nIn 1790, Sir Benjamin Hammett introduced a bill into Parliament to end the practice of judicial burning. He explained that the year before, as Sheriff of London, he had been responsible for the burning of Catherine Murphy, found guilty of counterfeiting, but that he had allowed her to be hanged first. He pointed out that as the law stood, he himself could have been found guilty of a crime in not carrying out the lawful punishment and, as no woman had been burnt alive in the kingdom for more than half a century, so could all those still alive who had held an official position at all of the previous burnings. The Treason Act 1790 was duly passed by Parliament and given royal assent by King George III (30 George III. C. 48).\n\nBenjamin B. Ferencz, one of the prosecutors in the Nuremberg trials who, in May 1945, investigated occurrences at the Ebensee concentration camp, and narrated them to Tom Hofmann, a family member and biographer. He was completely outraged at what the Nazis had done there. When people discovered an SS guard who attempted to flee, they tied him to one of the metal trays used to transport bodies into the crematorium. They then proceeded to light the oven, and slowly roast the SS guard to death, taking him in and out of the oven several times. Ferencz said to Hofmann that at the time, he was in no position to stop the proceedings of the mob, and frankly admitted that he had not been inclined to try. Hofmann adds, \"There seemed to be no limit to human brutality in wartime.\"\n\nDuring the expulsion of Germans from Czechoslovakia after the end of World War II, a number of massacres against the German minority occurred. In one case in Prague in May 1945, a Czech mob hanged several Germans upside down on lampposts, doused them in fuel and set them on fire, burning them alive.\n\nNecklacing is the practice of summary execution and torture carried out by forcing a rubber tire, filled with petrol, around a victim's chest and arms, and setting it on fire. The victim may take up to 20 minutes to die, suffering severe burns in the process. The method was widely used in Haiti and South Africa.\n\nIn Rio de Janeiro, Brazil, burning people standing inside a pile of tires is a common form of murder used by drug dealers to punish those who have supposedly collaborated with the police. This form of burning is called \"micro-ondas\" (microwave oven). The film \"Tropa de Elite\" (\"Elite Squad\") and the video game \"Max Payne 3\" contain scenes depicting this practice.\n\nDuring the Guatemalan Civil War the Guatemalan Army and security forces carried out an unknown number of extrajudicial killings by burning. In one instance in March 1967, Guatemalan guerrilla and poet Otto René Castillo was captured by Guatemalan government forces and taken to Zacapa army barracks alongside one of his comrades, Nora Paíz Cárcamo. The two were interrogated, tortured for four days, and burned alive. Other reported instances of immolation by Guatemalan government forces occurred in the Guatemalan government's rural counterinsurgency operations in the Guatemalan Altiplano in the 1980s. In April 1982, 13 members of a Quanjobal Pentecostal congregation in Xalbal, Ixcan, were burnt alive in their church by the Guatemalan Army.\n\nOn 31 August 1996, a Mexican man, Rodolfo Soler Hernandez, was burned to death in Playa Vicente, Mexico, after he was accused of raping and strangling a local woman to death. Local residents tied Hernandez to a tree, doused him in a flammable liquid and then set him ablaze. His death was also filmed by residents of the village. Shots taken before the killing showed that he had been badly beaten. On 5 September 1996, Mexican television stations broadcast footage of the murder. Locals carried out the killing because they were fed up with crime and believed that the police and courts were both incompetent. Footage was also shown in the 1998 shockumentary film, \"Banned from Television\".\n\nA young Guatemalan woman, Alejandra María Torres, was attacked by a mob in Guatemala City on 15 December 2009. The mob alleged that Torres had attempted to rob passengers on a bus. Torres was beaten, doused with gasoline, and set on fire, but was able to put the fire out before sustaining life-threatening burns. Police intervened and arrested Torres. Torres was forced to go topless throughout the ordeal and subsequent arrest, and many photographs were taken and published. Approximately 219 people were lynched in Guatemala in 2009, of whom 45 died.\n\nIn May 2015, a sixteen-year-old girl was allegedly burned to death in Rio Bravo by a vigilante mob after being accused by some of involvement in the killing of a taxi driver earlier in the month.\n\nIn Chile during public mass protests held against the military regime of General Augusto Pinochet on 2 July 1986, engineering student Carmen Gloria Quintana, 18, and Chilean-American photographer Rodrigo Rojas DeNegri, 19, were arrested by a Chilean Army patrol in the Los Nogales neighborhood of Santiago. The two were searched and beaten before being doused in gasoline and burned alive by Chilean troops. Rojas was killed, while Quintana survived but with severe burns.\n\nDuring the 1980 New Mexico State Penitentiary riot, a number of inmates were burnt to death by fellow inmates, who used blow torches. Modern burnings continued as a method of lynching in the United States in the late 19th and early 20th centuries, particularly in the South. One of the most notorious extrajudicial burnings in modern history occurred in Waco, Texas on 15 May 1916. Jesse Washington, an African-American farmhand, after having been convicted of the rape and subsequent murder of a white woman, was taken by a mob to a bonfire, castrated, doused in coal oil, and hanged by the neck from a chain over the bonfire, slowly burning to death. A postcard from the event still exists, showing a crowd standing next to Washington's charred corpse with the words on the back \"This is the barbecue we had last night. My picture is to the left with a cross over it. Your son, Joe\". This attracted international condemnation and is remembered as the \"Waco Horror\".\n\nA former Soviet Main Intelligence Directorate (GRU) officer writing under the alias Victor Suvorov described, in his book \"Aquarium\", a Soviet \"traitor\" being burned alive in a crematorium. There has been some speculation that this officer was Oleg Penkovsky. However, during a radio interview with the Echo of Moscow, Suvorov denied this, saying \"I never mentioned it was Penkovsky\". No executed GRU traitors other than Penkovsky are known to match Suvorov's description of the spy in \"Aquarium\".\n\nIn connection to the purge of Jang Song-taek, O Sang-hon, a deputy minister at the Ministry of Public Security (North Korea) associated with Jang, was 'executed by flamethrower' in 2014, according to unconfirmed reports.\n\nIn South Africa, extrajudicial executions by burning were carried out via \"necklacing\", wherein rubber tires filled with kerosene (or gasoline) are placed around the neck of a live individual. The fuel is then ignited, the rubber melts, and the victim is burnt to death.\n\nIt was reported that in Kenya, on 21 May 2008, a mob had burned to death at least 11 accused witches.\n\nDr Graham Stuart Staines, an Australian Christian missionary, and his two sons Philip (aged ten) and Timothy (aged six), were burnt to death by a gang while the three slept in the family car (a station wagon), at Manoharpur village in Keonjhar District, Odisha, India on 22 January 1999. Four years later, in 2003, a Bajrang Dal activist, Dara Singh, was convicted of leading the gang that murdered Staines and his sons, and was sentenced to life in prison. Staines had worked in Odisha with the tribal poor and lepers since 1965. Some Hindu groups made allegations that Staines had forcibly converted or lured many Hindus into Christianity.\n\nOn 19 June 2008, the Taliban, at Sadda, Lower Kurram, Pakistan, burned three truck drivers of the Turi tribe alive after attacking a convoy of trucks en route from Kohat to Parachinar, possibly for supplying the Pakistan Armed Forces.\n\nIn January 2015, Jordanian pilot Moaz al-Kasasbeh was burned in a cage by the Islamic State of Iraq and the Levant (ISIS). The pilot was captured when his plane crashed near Raqqa, Syria, during a mission against IS in December 2014.\n\nIn August 2015, ISIS burned to death four Iraqi Shia prisoners.\n\nIn December 2016, ISIS burned to death two Turkish soldiers, publishing high quality video of the atrocity.\n\nOn 20 January 2011, a 28-year-old woman, Ranjeeta Sharma, was found burning to death on a road in rural New Zealand. The police confirmed the woman was alive before being covered in an accelerant and set on fire. Sharma's husband, Davesh Sharma, was charged with her murder.\n\n\n\n"}
{"id": "33548913", "url": "https://en.wikipedia.org/wiki?curid=33548913", "title": "Dehaene–Changeux model", "text": "Dehaene–Changeux model\n\nThe Dehaene–Changeux model (DCM), also known as the global neuronal workspace or the global cognitive workspace model is a part of Bernard Baars's \"global workspace model\" for consciousness.\n\nIt is a computer model of the neural correlates of consciousness programmed as a neural network. It attempts to reproduce the swarm behaviour of the brain's \"higher cognitive functions\" such as consciousness, decision-making and the central executive functions. It was developed by cognitive neuroscientists Stanislas Dehaene and Jean-Pierre Changeux beginning in 1986. It has been used to provide a predictive framework to the study of inattentional blindness and the solving of the Tower of London test.\n\nThe Dehaene–Changeux model was initially established as a spin glass neural network attempting to represent learning and to then provide a stepping stone towards artificial learning among other objectives. It would later be used to predict observable reaction times within the priming paradigm and in inattentional blindness.\n\nThe Dehaene–Changeux model is a meta neural network (i.e. a network of neural networks) composed of a very large number of integrate-and-fire neurons programmed in either a stochastic or deterministic way. The neurons are organised in complex thalamo-cortical columns with long-range connexions and a critical role played by the interaction between von Economo's areas. Each thalamo-cortical column is composed of pyramidal cells and inhibitory interneurons receiving a long-distance excitatory neuromodulation which could represent noradrenergic input.\n\nAmong others Cohen & Hudson (2002) had already used \"\"Meta neural networks as intelligent agents for diagnosis \" Similarly to Cohen & Hudson, Dehaene & Changeux have established their model as an interaction of meta-neural networks (thalamocortical columns) themselves programmed in the manner of a \"hierarchy of neural networks that together act as an intelligent agent\"\", in order to use them as a system composed of a large scale of inter-connected intelligent agents for predicting the self-organized behaviour of the neural correlates of consciousness. It may also be noted that Jain et al. (2002) had already clearly identified spiking neurons as intelligent agents since the lower bound for computational power of networks of spiking neurons is the capacity to simulate in real-time for boolean-valued inputs any Turing machine. The DCM being composed of a very large number of interacting sub-networks which are themselves intelligent agents, it is formally a Multi-agent system programmed as a Swarm or neural networks and \"a fortiori\" of spiking neurons.\n\nThe DCM exhibits several surcritical emergent behaviors such as multistability and a Hopf bifurcation between two very different regimes which may represent either sleep or arousal with a various all-or-none behaviors which Dehaene et al. use to determine a testable taxonomy between different states of consciousness. \n\nThe Dehaene-Changeux Model contributed to the study of nonlinearity and self-organized criticality in particular as an explanatory model of the brain's emergent behaviors, including consciousness. Studying the brain's phase-locking and large-scale synchronization, Kitzbichler et al. (2011a) confirmed that criticality is a property of human brain functional network organization at all frequency intervals in the brain's physiological bandwidth.\n\nFurthermore, exploring the neural dynamics of cognitive efforts after, \"inter alia\", the Dehaene-Changeux Model, Kitzbichler et al. (2011b) demonstrated how cognitive effort breaks the modularity of mind to make human brain functional networks transiently adopt a more efficient but less economical configuration. Werner (2007a) used the Dehaene-Changeux Global Neuronal Workspace to defend the use of statistical physics approaches for exploring phase transitions, scaling and universality properties of the so-called \"Dynamic Core\" of the brain, with relevance to the macroscopic electrical activity in EEG and EMG. Furthermore, building from the Dehaene-Changeux Model, Werner (2007b) proposed that the application of the twin concepts of scaling and universality of the theory of non-equilibrium phase transitions can serve as an informative approach for elucidating the nature of underlying neural-mechanisms, with emphasis on the dynamics of recursively reentrant activity flow in intracortical and cortico-subcortical neuronal loops. Friston (2000) also claimed that \"the nonlinear nature of asynchronous coupling enables the rich, context-sensitive interactions that characterize real brain dynamics, suggesting that it plays a role in functional integration that may be as important as synchronous interactions\".\n\nIt contributed to the study of phase transition in the brain under sedation, and notably GABA-ergic sedation such as that induced by propofol (Murphy et al. 2011, Stamatakis et al. 2010). The Dehaene-Changeux Model was contrasted and cited in the study of collective consciousness and its pathologies (Wallace et al. 2007). Boly et al. (2007) used the model for a reverse somatotopic study, demonstrating a correlation between baseline brain activity and somatosensory perception in humans. Boly et al. (2008) also used the DCM in a study of the baseline state of consciousness of the human brain's default network.\n\n\n"}
{"id": "22666826", "url": "https://en.wikipedia.org/wiki?curid=22666826", "title": "EATPUT", "text": "EATPUT\n\nThe EATPUT model is a model for analyzing an information system designed by Anthony Debons of\nthe University of Pittsburgh's School of Information Science in 1961. It has been widely used in the fields of information systems and information science, in a variety of areas. One example is the use of the model in the design of information systems to serve remote islands.\n\nThe EATPUT model is so called because of the six fundamental components, which taken together form the acronym EATPUT.\n\nThe Event Phase details occurrences relevant to the information system and their representation to the information system. Representation to the system could take many forms such as sound or digitally coded data, depending on the information system. For example, a weather predicting machine's event could be an increase in humidity, represented to the system as directly as an increase in water vapor in the air.\n\nThe Acquisition Phase is the sensor of an information system. The Acquisition Phase is where the system captures its data pertaining to the Event Phase. Continuing with the example of a weather predicting machine, the increase in water vapor in the air is detected by an instrument on the device.\n\nTransmission actually occurs between each phase, but its most important (and most aesthetically pleasing to the acronym EATPUT) occurrence is the one between the Acquisition and Processing phases. Transmission is a fairly straightforward concept; it is how the different components and phases of an information system communicate with each other. Continuing with the weather predicting machine example again, the water vapor sensors might code the information digitally (this process itself would require yet another information system) and send it to the next phase.\n\nProcessing is where the data is ordered, stored, retrieved, and operated on appropriately in order to form knowledge. The amount of humidity can be compared to past years and general meteorological principles in order to analyze the event of higher or lower humidity and translate it to something more useful (e.g. \"the humidity is 60%).\n\nThe Utilization Phase of the system evaluates and interprets the Processing Phase's result. 60% humidity might mean that the chance of a thunderstorm is higher.\n\nThe Transfer Phase is the action component of the system, the implementation of the knowledge the other phases have generated. Perhaps the weather predicting machine, as it becomes more and more certain a severe thunderstorm is likely, will notify a human that can declare an appropriate storm warning or use the information for the television weather forecast.\n"}
{"id": "70432", "url": "https://en.wikipedia.org/wiki?curid=70432", "title": "Emic and etic", "text": "Emic and etic\n\nIn anthropology, folkloristics, and the social and behavioral sciences, emic and etic refer to two kinds of field research done and viewpoints obtained: \"emic,\" from within the social group (from the perspective of the subject) and \"etic,\" from outside (from the perspective of the observer).\n\n\"The emic approach investigates how local people think\" (Kottak, 2006): How they perceive and categorize the world, their rules for behavior, what has meaning for them, and how they imagine and explain things. \"The etic (scientist-oriented) approach shifts the focus from local observations, categories, explanations, and interpretations to those of the anthropologist. The etic approach realizes that members of a culture often are too involved in what they are doing... to interpret their cultures impartially. When using the etic approach, the ethnographer emphasizes what he or she considers important.\"\n\nAlthough emics and etics are sometimes regarded as inherently in conflict and one can be preferred to the exclusion of the other, the complementarity of emic and etic approaches to anthropological research has been widely recognized, especially in the areas of interest concerning the characteristics of human nature as well as the form and function of human social systems.\nEmic and etic approaches of understanding behavior and personality fall under the study of cultural anthropology. Cultural anthropology states that people are shaped by their cultures and their subcultures, and we must account for this in the study of personality. One way is looking at things through an emic approach. This approach \"is culture specific because it focuses on a single culture and it is understood on its own terms.\" As explained below, the term \"emic\" originated from the specific linguistic term \"phonemic\", from \"phoneme\", which is a language-specific way of abstracting speech sounds.\nWhen these two approaches are combined, the “richest” view of a culture or society can be understood. On its own, an emic approach would struggle with applying overarching values to a single culture. The etic approach is helpful in enabling researchers to see more than one aspect of one culture, and in applying observations to cultures around the world.\n\nThe terms were coined in 1954 by linguist Kenneth Pike, who argued that the tools developed for describing linguistic behaviors could be adapted to the description of any human social behavior. As Pike noted, social scientists have long debated whether their knowledge is objective or subjective. Pike's innovation was to turn away from an epistemological debate, and turn instead to a methodological solution. \"Emic\" and \"etic\" are derived from the linguistic terms phonemic and phonetic respectively, which are in turn derived from Greek roots. The possibility of a truly objective description was discounted by Pike himself in his original work; he proposed the emic/etic dichotomy in anthropology as a way around philosophic issues about the very nature of objectivity.\n\nThe terms were also championed by anthropologists Ward Goodenough and Marvin Harris with slightly different connotations from those used by Pike. Goodenough was primarily interested in understanding the culturally specific meaning of specific beliefs and practices; Harris was primarily interested in explaining human behavior.\n\nPike, Harris, and others have argued that cultural \"insiders\" and \"outsiders\" are equally capable of producing emic \"and\" etic accounts of their culture. Some researchers use \"etic\" to refer to objective or outsider accounts, and \"emic\" to refer to subjective or insider accounts.\n\nMargaret Mead was an anthropologist who studied the patterns of adolescence in Samoa. She discovered that the difficulties and the transitions that adolescents faced are culturally influenced. The hormones that are released during puberty can be defined using an etic framework, because adolescents globally have the same hormones being secreted. However, Mead concluded that how adolescents respond to these hormones is greatly influenced by their cultural norms. Through her studies, Mead found that simple classifications about behaviors and personality could not be used because peoples’ cultures influenced their behaviors in such a radical way. Her studies helped create an emic approach of understanding behaviors and personality. Her research deduced that culture has a significant impact in shaping an individual’s personality.\nCarl Jung, a Swiss psychoanalyst, is a researcher who took an etic approach in his studies. Jung studied mythology, religion, ancient rituals, and dreams leading him to believe that there are archetypes that can be identified and used to categorize people’s behaviors. Archetypes are universal structures of the collective unconscious that refer to the inherent way people are predisposed to perceive and process information. The main archetypes that Jung studied were the persona (how people choose to present themselves to the world), the animus/ anima (part of people experiencing the world in viewing the opposite sex, that guides how they select their romantic partner), and the shadow (dark side of personalities because people have a concept of evil. Well-adjusted people must integrate both good and bad parts of themselves). Jung looked at the role of the mother and deduced that all people have mothers and see their mothers in a similar way; they offer nurture and comfort. His studies also suggest that “infants have evolved to suck milk from the breast, it is also the case that all children have inborn tendencies to react in certain ways.” This way of looking at the mother is an etic way of applying a concept cross-culturally and universally.\n\nEmic and etic approaches are important to understanding personality because problems can arise “when concepts, measures, and methods are carelessly transferred to other cultures in attempts to make cross- cultural generalizations about personality.” It is hard to apply certain generalizations of behavior to people who are so diverse and culturally different. One example of this is the F-scale (Macleod). The F-scale, which was created by Theodor Adorno, is used to measure Authoritarian Personality, which can, in turn, be used to predict prejudiced behaviors. This test, when applied to Americans accurately depicts prejudices towards black individuals. However, when a study was conducted in South Africa using the F-Scale, (Pettigrew and Friedman) results did not predict any anti-Black prejudices. This study used emic approaches of study by conducting interviews with the locals and etic approaches by giving participants generalized personality tests.\n\nOther explorations of the differences between reality and humans' models of it:\n\n\n"}
{"id": "38200354", "url": "https://en.wikipedia.org/wiki?curid=38200354", "title": "Fanaroff-Riley classification", "text": "Fanaroff-Riley classification\n\nThe Fanaroff-Riley classification is a scheme created by B.L. Fanaroff and J.M. Riley in 1974, which is used to distinguish radio galaxies with active nuclei based on their radio luminosity or brightness of their radio emissions in relation to their hosting environment. \"Class I\" (abbreviated FR-I) are sources whose luminosity decreases as the distance from the central galaxy or quasar host increase, while \"Class II\" (FR-II) sources exhibit increasing luminosity in the lobes. These sources are called also \"edge-brightened\". This distinction is important because it presents a direct link between the galaxy's luminosity and the way in which energy is transported from the central region and converted to radio emission in the outer parts.\n\n"}
{"id": "175573", "url": "https://en.wikipedia.org/wiki?curid=175573", "title": "God's Debris", "text": "God's Debris\n\nGod's Debris: A Thought Experiment is a 2001 novella by \"Dilbert\" creator Scott Adams.\n\n\"God's Debris\" espouses a philosophy based on the idea that the simplest explanation tends to be the best. It proposes a form of pandeism and monism, postulating that an omnipotent god annihilated himself in the Big Bang, because an omniscient entity would already know everything possible except his own lack of existence, and exists now as the smallest units of matter and the law of probability, or \"God's debris\".\n\nThe introduction disclaims any personal views held by the author, \"The opinions and philosophies expressed by the characters are not my own, except by coincidence in a few spots not worth mentioning\".\n\nThe central character, according to the introduction, knows \"everything. Literally everything.\" Adams, whose knowledge is as incomplete as the next person, got around this by using the aforementioned \"simplest explanation\" for each concept raised in the book because, while \"in this complicated world the simplest explanation is usually dead wrong\", a more simple explanation often sounds more right and more convincing than anything complex.\n\nThis character, the Avatar, defines God as primordial matter (like quarks and leptons) and the law of probability. He offers recommendations on everything from an alternative theory for planetary motion to successful recipes for relationships under his system. He proposes that God is currently reassembling himself through the continuing formation of a collective intelligence in the form of the human race, modern examples of which include the development of the internet; this is related to the idea of the Omega Point.\n\nHowever, in the introduction, Adams describes \"God's Debris\" as a thought experiment, challenging readers to differentiate its scientifically accepted theories from \"creative baloney designed to sound true,\" and to \"Try to figure out what's wrong with the simplest explanation.\"\n\nThe chapter \"Fifth Level\" (p. 124) describes five levels of human awareness, or consciousness.\n\nThe book subscribes to the Lakoffian point of view, in that the mind is viewed as a \"delusion generator\" rather than a window to true understanding. As George Lakoff said: \"Our ordinary conceptual system, in terms of which we both think and act, is fundamentally metaphorical in nature.\"\n\nThe particular philosophy espoused has been identified as a form of pandeism, the concept that a god created the universe by becoming the universe.\n\nGiven Adams' fame as the author of the \"Dilbert\" comics, publishers were wary of publishing any book by Adams without \"Dilbert\" content. The book was therefore released initially as an e-book (with comparatively small \"publishing\" costs). Based on its rapid success, however, it was also quickly released in hard-cover format. The book can be found on-line (see external links below).\n\n\n"}
{"id": "53471343", "url": "https://en.wikipedia.org/wiki?curid=53471343", "title": "Humean definition of causality", "text": "Humean definition of causality\n\nDavid Hume coined a sceptical, reductionist viewpoint on causality that inspired the logical-positivist definition of empirical law that \"is a regularity or universal generalization of the form 'All Cs are Es' or, whenever C, then E\". The Scottish philosopher and economist believed that human mind is not equipped with the ability to observe causal relations. What can be seen is one event following another. The reductionist approach to causation can be exemplified with the case of two billiard balls: one ball is moving, hits another one and stops, and the second ball is moving.\n\nDavid Hume listed three requirements for calling a relation causal:\n\n(1) universal association between X and Y,\n\n(2) time precedence of Y by X,\n\n(3) spatiotemporal connection between X and Y.\n\n"}
{"id": "24024923", "url": "https://en.wikipedia.org/wiki?curid=24024923", "title": "Identifiable victim effect", "text": "Identifiable victim effect\n\nThe \"identifiable victim effect\" refers to the tendency of individuals to offer greater aid when a specific, identifiable person (\"victim\") is observed under hardship, as compared to a large, vaguely defined group with the same need. The effect is also observed when subjects administer punishment rather than reward. Research has shown that individuals can be more likely to mete out punishment, even at their own expense, when they are punishing specific, identifiable individuals (\"perpetrators\").\n\nConcrete images and representations are often more powerful sources of persuasion than are abstract statistics. For example, Ryan White contracted HIV at age 13 and struggled with the disease until succumbing some six years later. Following his death, the US congress passed the Ryan White Care Act, which funded the largest set of services for people living with the AIDS in the country.\n\nThe effect is epitomized by the phrase (commonly attributed to Joseph Stalin) \"A single death is a tragedy; a million deaths is a statistic.\"\n\nThe conceptualization of the identifiable victim effect as it is known today is commonly attributed to American economist Thomas Schelling. He wrote that harm to a particular person invokes “anxiety and sentiment, guilt and awe, responsibility and religion, [but]…most of this awesomeness disappears when we deal with statistical death”.\n\nThe decision to save an identifiable victim is made \"ex post\", meaning it is done \"after\" the victim is in danger. In contrast, the decision to save a statistical victim is often made \"ex ante\", as a pre-emptive measure to \"prevent\" the individual from being in danger. When people consider the risks of not helping a victim, they consider the probability of being responsible and blamed. This probability is much greater with identifiable victims than with statistical victims because one cannot accurately predict the likelihood of a tragedy occurring in the future and thus cannot be held responsible for tragedies that \"might\" occur in the future. (Although, Jenni, & Loewenstein (1997) have found no evidence to support the effect of ex post/ex ante on IVE)\n\nThis explanation is closest to what Thomas Schelling implied in his now-famous paper.\n\nIdentifiable victims, as their name suggests, have features that make them identifiable. Details about their predicament, family background, educational history, etc., are shared through the media and brought to public attention. The stories are emotional, with victims often portrayed as innocent and helpless. For example, Perrault et al. tested an identifiable human victim message, in relation to an identifiable animal victim (i.e., a squirrel) using the context of the consequences of littering, and found the identifiable animal message - an innocent and helpless creature - elicited greater levels of distress and empathy than the identifiable human message. Images and videos of the victim are often shared, and the public is able to follow the victim's predicament in real-time. Studies have previously indicated that people respond more to concrete, graphic information than abstract, statistical information. The vividness of identifiable victims creates a sense of familiarity and social closeness(opposite of social distance).Therefore, identifiable victims elicit greater reactions from people than statistical victims. (Although, Jenni, & Loewenstein (1997) have found no evidence to support this effect)\n\nThe certainty effect and risk seeking for losses reinforce each other. The certainty effect is the inclination to place disproportionately greater weighting to certain outcomes than to uncertain but likely outcomes. The consequences to identifiable victims are viewed as certain to occur whereas the consequences to statistical victims are viewed as probabilistic. Research has also shown the tendency of people to be risk-seeking for losses. A certain loss is viewed more negatively than an uncertain loss with the same expected value. Closely related to this is people tend to be loss averse. They view saving a statistical life as a gain whereas saving an identifiable victim is seen as avoiding a loss. Together, these effects result in people being more likely to aid identifiable, certain victims than statistical, uncertain victims.\n\nRisk that is concentrated is perceived as greater than the same risk dispersed over a wider population. Identifiable victims are their own reference group; if they do not receive aid then the entire reference group is regarded to perish. To illustrate this point, consider an explosion at an offshore oil rig where 50 people work. Suppose all 50 people die in the explosion, this represents 50 of the thousands of people working on offshore oil rigs. Yet, the reference group is not the thousands of people working on offshore oil rigs but rather the 50 people working on that particular offshore oil rig. Therefore, this is perceived as 50 of 50 people certain to die so by aiding them, a significant proportion of the reference group can be saved.\n\nPeople's response to the proportion of the reference group that can be saved is such a significant contributor to the identifiable victim effect that this effect could be re-labelled as the “percentage of reference group saved effect”.\n\nOne implication of the identifiable victim effect is identifiable victims are more likely to be helped than statistical victims.\n\nAn incident that frequently features in literature is the aid given to Jessica McClure and her family. On October 14, 1987, 18-month old Jessica McClure fell into a narrow well in her aunt's home cum day-care centre in Midland, Texas. Within hours, 'Baby Jessica', as she became known, made headlines around the US. The public reacted with sympathy towards her ordeal. While teams of rescue workers, paramedics and volunteers worked to successfully rescue 'Baby Jessica' in 58 hours, a total of $700,000 was amassed in that time. Even after being discharged from hospital, the McClure family were flooded with cards and gifts from members of the public as well as a visit from then-Vice President George H.W. Bush and a telephone call from then-President Ronald Reagan.\n\nThe identifiable victim effect is suggested to be a specific case of a more general 'identifiable other effect'. As such, it also has an effect on punishments. People prefer to punish identified transgressors rather than unidentified transgressors when given a choice between the two. People also exert more severe punishments on identified than unidentified transgressors.\n\nIndividuals are also more likely to assign blame to an identified transgressor if they feel they have been wronged. There is also an increased desire for harsh punishments that persists even when self-sacrifice is required in order to punish the transgressor. This effect can possibly be explained by the increased anger felt towards an identified rather than an unidentified transgressor. This supports the theory of “vividness” as a source of the identifiable victim effect (Small & Loewenstein, 2005).\n\nThe identifiable victim effect may also influence healthcare, both at the individual and national level (Redelmeir & Tversky, 1990). On the individual level, doctors are more likely to recommend expensive, but potentially life-saving, treatments to an individual patient rather than to a group of patients. This effect is not limited to medical professionals, as laymen demonstrate this same bias towards providing more expensive treatments for individual patients (Redelmeir & Tversky, 1990). On the national level, the American people are far more likely to contribute to an expensive treatment to save the life of one person rather than spend much smaller amounts on preventative measures that could save the lives of thousands per year. A function of American individualism, this nationwide bias towards expensive treatments is still prevalent today (Toufexis & Bjerklie, 1993).\n\nThe need to tackle the problems faced by AIDS sufferers was brought to the political forefront as a result of the legal and social plight of one particular AIDS victim, Ryan White. His circumstances and campaign for greater funding for AIDS research were widely publicised in the media which culminated in legislation being passed to provide financial support to AIDS sufferers and their families in 1990 shortly after White's death.\n\nSince the identifiable victim effect can influence punishment, it has the potential to undermine the system of trial by jury (Small & Loewenstein, 2005). Jurors, when deliberating, work with an identifiable perpetrator, and thus may attach negative emotions (e.g. disgust, anger) to the individual or assign increased blame when handing down a harsh sentence. Policymakers, who are unable to see the individual offender, being almost entirely emotionally removed, may actually have intended a more lenient sentence. This may produce a harsher verdict than the legal guidelines recommend or allow. On the other extreme, jurors may feel sympathy, relating with the perpetrator on a level not experienced by policymakers, leading to a milder verdict than legally appropriate or allowable (Small & Loewenstein, 2005).\n\nTypically in crime investigations, law enforcement forces conceal any information regarding the identities of the suspects until they have strong evidence that the suspects are credible. When identities of suspects are revealed through description of their features or release of their images, media coverage and public discussion on the issue grows. On one side, the public discourse can become increasingly negative and hostile, or, if the perpetrator is sympathetic, support for the perpetrator may grow. This is because people experience a greater emotional reaction towards a concrete, identifiable perpetrator than an abstract, unidentifiable one.\n\nJames S. Brady, the then-White House press secretary, was among three collateral damage victims in the attempted assassination of President Reagan in 1981. Brady was explicitly named in reports of the shooting in contrast to the other two injured, a District of Columbia police officer and a Secret Service agent. The political reaction was largely focused on the injuries of Brady which led to the enactment of the Brady Handgun Violence Prevention Act of 1993. It states that it is mandatory for firearm dealers to perform background searches on firearm purchasers.\n\nAccording to a 2016 study by Yam and Reynolds, the growing absence of the identifiable victim effect in the business world may contribute to an increase in unethical business behavior. With an increasingly globalized business framework, the identifiable victim effect may become naturally mediated, freeing business leaders and employees alike to engage in unethical behavior without guilt or emotional distress. This may be possible because globalization results in less face-to-face contact, decreasing the identifiability of possible victims. Research suggests that business leaders, as well as workers, are more likely to engage in unethical behavior when the victims of their behavior are anonymous. At the executive level, one possible result of this is worker commodification and exploitation. At the worker level, employees of a company are possibly more likely to steal from the company or lie on a report if they do not believe this behavior will negatively affect a recognizable coworker. A decrease in recognizable coworkers could thus potentially lead to an increase in unethical behavior by workers (Yam & Reynolds, 2016). Research also suggests that outside observers, not only perpetrators, view unethical behavior as less unethical if the victim of the unethical behavior is unidentified (Gino, Shu, & Bazerman, 2010). This could possibly result in less public outcry against unethical practices in a globalized business environment, where the victims are often unseen.\n\nHigh levels of attachment anxiety may increase the power of identifiable victim effect. Research indicates that individuals with high levels of attachment anxiety may donate more to identified victims and donate less to unidentified victims than the average person (Kogut & Kogut, 2013). According to a study by Kogut and Kogut, attachment anxiety may reduce the expression of altruistic tendencies, commonly demonstrated by charitable giving. Researchers hypothesize that this is because anxiously-attached individuals focus their time and energy on dealing with their own vulnerabilities, leaving them no mental energy to focus on the well-being of others. However, this would only be true of the unidentified individual. When faced with an identified victim, individuals who are anxiously attached tend to donate more money than the average individual. This aligns with past research indicating that anxiously attached people experience significantly more personal distress than those securely attached when confronted with victims in need (Mikulincer et al., 2001).\n\nAlthough anxiously-attached people may participate in prosocial behaviors, such as donating money to a charity, their actions are suggested not to be the result of altruistic tendencies, but instead \"positively correlated with egoistic, rather than altruistic motives for helping and volunteering.\"(p 652)(Kogut, T. & Kogut E., 2013). Thus, researchers hypothesize that anxiously attached individuals are more likely to help identified victims only because they will personally benefit. This is possibly because the identified victim can fulfill the desire for personal attachment, not because the victim is in need. It is important to note that their increased helpfulness only extends to easy, effortless actions, such as donating money. It does not extend to particularly difficult or effortful actions, such as donating time (Kogut & Kogut, 2013).\n\nResearch suggests that guilt reduces the power of the identifiable victim effect (Yam & Reynolds, 2016). Before engaging in a behavior, an individual conducts an assessment of the possible emotional outcomes of that behavior. An individual is drawn towards behaviors that would make them happy and repulsed by those that would make them upset. Thus, a person with a high level of guilt is drawn towards altruistic acts because they serve to alleviate the negative emotions that they are experiencing. Consequently, the presence of guilt may actually increase the occurrence of altruistic behavior, such as charitable donations, regardless of whether the victim they are helping is identified or not. Research also suggests that anticipated guilt reduces the occurrence of unethical behavior that may negatively affect an identified victim (Yam & Reynolds, 2016). This may be because knowingly and negatively affecting a recognizable victim causes the individual engaging in unethical behavior to experience distress and negative emotions.\n\nResearch suggests that individual differences in reasoning style moderate the identifiable victim effect (Friedrich & McGuire, 2010). Two different methods of reasoning are “experiential” and “rational”. Experiential thinking (e.g. emotionally-based thinking) is automatic, contextual and fluid, and rational thinking (e.g. logically-based thinking) is deliberative, analytical, and decontextualized. Experiential thinking styles may increase the power of the identifiable victim effect, and rational thinking styles may decrease the power of the identifiable victim effect. Researchers theorize that these differences result because experiential thinkers rely on emotional responses towards an issue when making a decision. In contrast, rational thinkers analyze the situation as a whole before making a decision. Thus, a person thinking rationally would respond to all victims equally, not giving preference to those specifically named or otherwise identified, just as experiential thinkers would be drawn towards the more emotionally charged identified victim (Friedrich & McGuire, 2010).\n\nThe identifiable victim effect has been contested in academia. Critics argue that, when a victim is identified, information such as age and gender of the victim are revealed and people are especially sympathetic in response to that information rather than to identifiability \"per se\".\n\nIn 2003, Deborah Small and George Loewenstein conducted an experiment that mitigated this issue. Identifiability was strictly limited to the \"determination\" of the victim's identity. Therefore, the victim had already been identified regardless of whether the participants had known anything specific about their identity or not. The circumstances of the victim were more palpable and thus elicited greater sympathy from participants. In contrast, the identities of statistical victims were not yet determined. As such, participants found it more difficult to sympathize with indeterminate victims.\n\nIn certain situations, identification of a victim can actually reduce the amount of help offered to that individual. Research suggests that if an individual is seen as responsible for their plight, people are less likely to offer help than if the victim was not identified at all (Kogut, 2011). Most research dedicated to the identifiable victim effect avoids the topic of blame, using explicitly blameless individuals, such as children suffering from an illness (Kogut & Ritov, 2005). However, there are real-world situations where victims may be seen as to blame for their current situation. For example, in a 2011 study by Kogut, individuals were less likely to offer help to an AIDS victim if the victim had contracted AIDS through sexual contact than if the individual was born with AIDS. In other words, individuals were less likely to offer help to victims if they were seen as at least partially responsible for their plight. A meta-study conducted in 2016 supports these findings, reporting that charitable donations were highest when the victim showed little responsibility for their victimization (Lee & Feeley, 2016).\n\nIn such cases where victim blaming is possible, identification of individuals may not induce sympathy and may actually increase negative perception of the victim (Kogut, 2011). This reduction in help is even more pronounced if the individual believes in the just world hypothesis, which is the tendency for people to blame the victim for what has happened to them. This pattern of blame results from a desire to believe that the world is predictable and orderly and that those who suffer must have done something to deserve their suffering.\n\nResearch may indicate that the identifiable victim effect only affects identified individuals, not identified groups (Kogut & Ritov, 2005). A 2005 study by Kogut and Ritov asked participants how much they would be willing to donate to either a critically ill child or a group of eight critically ill children. Although identification of the individual child increased donations, identification of the group of children as a whole did not. Researchers also found that, although both the individual and group evoked similar amounts of empathy, individual victims evoked more emotional distress than groups of victims. Continuing from this, researchers hypothesized that emotional distress, not empathy, appears to be positively correlated with desire to help, or “willingness to contribute.” This supports the idea that altruistic acts may serve as coping mechanisms to alleviate negative emotions, such as guilt (Yam & Reynolds, 2016). This also supports later research that suggests distress and sympathy are the driving emotional factors behind the identifiable victim effect (Erlandsson, Björklund, & Bäckström, 2015).\n\n"}
{"id": "865767", "url": "https://en.wikipedia.org/wiki?curid=865767", "title": "In loco parentis", "text": "In loco parentis\n\nThe term in loco parentis, Latin for \"in the place of a parent\" refers to the legal responsibility of a person or organization to take on some of the functions and responsibilities of a parent. Originally derived from English common law, it is applied in two separate areas of the law.\n\nFirst, it allows institutions such as colleges and schools to act in the best interests of the students as they see fit, although not allowing what would be considered violations of the students' civil liberties.\n\nSecond, this doctrine can provide a non-biological parent to be given the legal rights and responsibilities of a biological parent if they have held themselves out as the parent.\n\nThe \"in loco parentis\" doctrine is distinct from the doctrine of \"parens patriae\", the psychological parent doctrine, and adoption.\n\nCheadle Hulme School, founded in Manchester, England, in 1855, adopted \"in loco parentis\" as its motto, well before the world's first public education act, the Elementary Education Act 1870. The school was established to educate and care for orphans and children of distressed parents.\n\nThe first major limitation to this came in the U.S. Supreme Court case \"West Virginia State Board of Education v. Barnette\" (1943), in which the court ruled that students cannot be forced to salute the American flag. More prominent change came in the 1960s and 1970s in such cases as \"Tinker v. Des Moines Independent Community School District\" (1969), when the Supreme Court decided that \"conduct by the student, in class or out of it, which for any reason - whether it stems from time, place, or type of behavior - materially disrupts classwork or involves substantial disorder or invasion of the rights of others is, of course, not immunized by the constitutional guarantee of freedom of speech.\" Adult speech is also limited by \"time, place and manner\" restrictions and therefore such limits do not rely on schools acting in loco parentis.\n\nIn \"Tinker v. Des Moines Independent Community School District\" (1969), the Supreme Court held that for school officials to justify censoring speech, they \"must be able to show that [their] action was caused by something more than a mere desire to avoid the discomfort and unpleasantness that always accompany an unpopular viewpoint,\" allowing schools to forbid conduct that would \"materially and substantially interfere with the requirements of appropriate discipline in the operation of the school.\" The Court found that the actions of the Tinkers in wearing armbands did not cause disruption and held that their activity represented constitutionally protected symbolic speech.\n\nIn \"New Jersey v. T.L.O.\" (1985) Justice White wrote: \"In carrying out searches and other disciplinary functions pursuant to such policies, school officials act as representatives of the State, not merely as surrogates for the parents, and they cannot claim the parents' immunity from the strictures of the Fourth Amendment.\" The case upheld the search of a purse while on public school property based upon reasonable suspicion, indicating there is a balancing between the student's legitimate expectation of privacy and the public school's interest in maintaining order and discipline. However, in \"Hazelwood School District v. Kuhlmeier\" (1987) the Supreme Court ruled that \"First Amendment rights of students in the public schools are not automatically coextensive with the rights of adults in other settings, and must be applied in light of the special characteristics of the school environment\" and schools may censor school-sponsored publications (such as a school newspaper) if content is \"...inconsistent with its basic educational mission.\" Other student issues such as school dress codes along with locker, cell phone, and personal laptop computer searches by public school officials have not yet been tested in the Supreme Court.\n\nPrivate institutions are given significantly more authority over their students than public ones, and are generally allowed to arbitrarily dictate rules. In the Kentucky State Supreme Court case \"Gott v. Berea College\" (1913), it was upheld that a \"college or university may prescribe requirements for admission and rules for the conduct of its students, and one who enters as a student implicitly agrees to conform to such rules of government\", while publicly funded institutions could not claim the same ability.\n\nIn \"Morse v. Frederick\" (2007) Justice Clarence Thomas, concurring with the majority, argued that \"Tinker's\" ruling contradicted \"the traditional understanding of the judiciary's role in relation to public schooling,\" and ignored the history of public education (127 S.Ct. 2634). He believed the judiciary's role to determine whether students have freedom of expression was limited by \"in loco parentis\". He cited \"Lander v. Seaver\" (1859), which held that \"in loco parentis\" allowed schools to punish student expression that the school or teacher believed contradicted the school's interests and educational goals. This ruling declared that the only restriction the doctrine imposed were acts of legal malice or acts that caused permanent injury. Neither of these were the case with Tinker.\n\nThough \"in loco parentis\" continues to apply to primary and secondary education in the U.S., application of the concept has largely disappeared in higher education. This was not always the case.\n\nPrior to the 1960s, undergraduates were subject to many restrictions on their private lives. Women were generally subject to curfews as early as 10:00, and dormitories were sex-segregated. Some universities expelled students—especially female students—who were somehow \"morally\" undesirable. More importantly, universities saw fit to restrict freedom of speech, on campus, often forbidding organizations out of favor or with different views from speaking, organizing, demonstrating, or otherwise acting on campus. These restrictions were severely criticized by the student movements of the 1960s, and the Free Speech Movement at the University of California, Berkeley formed partly on account of them, inspiring students elsewhere to step up their opposition.\n\nThe landmark 1961 case \"Dixon v. Alabama\" was the beginning of the end for \"in loco parentis\" in U.S. higher education. The United States Court of Appeals for the Fifth Circuit found that Alabama State College could not summarily expel students without due process. However, that still does not prevent students who exercise their rights from being subject to more legal action for violation of institutional rules.\n\n\n"}
{"id": "542949", "url": "https://en.wikipedia.org/wiki?curid=542949", "title": "Information ecology", "text": "Information ecology\n\nIn the context of an evolving information society, the term information ecology marks a connection between ecological ideas with the dynamics and properties of the increasingly dense, complex and important digital informational environment and has been gaining acceptance in a growing number of disciplines. \"Information ecology\" often is used as metaphor, viewing the informational space as an ecosystem.\n\nInformation ecology is a science which studies the laws governing the influence of information summary on the formation and functioning of biosystems, including that of individuals, human communities and humanity in general and on the health and psychological, physical and social well-being of the human being; and which undertakes to develop methodologies to improve the information environment .\n\nInformation ecology also makes a connection to the concept of collective intelligence and knowledge ecology . Eddy et. al. (2014) use information ecology for science-policy integration in ecosystems-based management (EBM).\n\nInformation ecology draws on the language of ecology - habitat, species, evolution, ecosystem, niche, growth, equilibrium, etc. - to describe and analyze information systems from a perspective that considers the distribution and abundance of organisms, their relationships with each other, and how they influence and are influenced by their environment. The virtual lack of boundaries between information systems and the impact of information technology on economic, social and environmental activities frequently calls on an information ecologist to consider local information ecosystems in the context of larger systems, and of the evolution of global information ecosystems. See also list of ecology topics.\n\nIn \"The Wealth of Networks: How Social Production Transforms Markets and Freedom\", a book published in 2006 and available under a Creative Commons license on its own wikispace, Yochai Benkler provides an analytic framework for the emergence of the networked information economy that draws deeply on the language and perspectives of information ecology together with observations and analyses of high-visibility examples of successful peer production processes, citing Wikipedia as a prime example.\n\nBonnie Nardi and Vicki O'Day in their book \"Information Ecologies: Using Technology with Heart,\" apply the ecology metaphor to local environments, such as libraries and schools, in preference to the more common metaphors for technology as tool, text, or system.\n\nNardi and O’Day’s book represents the first specific treatment of information ecology by anthropologists. H.E. Kuchka situates information within socially-distributed cognition of cultural systems. Casagrande and Peters use information ecology for an anthropological critique of Southwest US water policy. Stepp (1999) published a prospectus for the anthropological study of information ecology.\n\nInformation ecology was used as book title by Thomas H. Davenport and Laurence Prusak , with a focus on the organization dimensions of information ecology. There was also an academic research project at DSTC called \"Information ecology\", concerned with distributed information systems and online communities.\n\nLaw schools represent another area where the phrase is gaining increasing acceptance, e.g.\nNYU Law School Conference Towards a Free Information Ecology and a lecture series on Information ecology at Duke University Law School's Center for the Study of the Public Domain.\n\nThe field of library science has seen significant adoption of the term and librarians have been described by Nardi and O'Day as a \"keystone species in information ecology\", and references to information ecology range as far afield as the Collaborative Digital Reference Service of the Library of Congress, to children's library database administrator in Russia.\n\nThere has also been increasing use of \"information ecology\" as a concept among ecologists involved in digital mapping of botanical resources, including research by Zhang Xinshi at the Institute of Botany of the China Academy of Science; also see a presentation to the Information Ecology SIG at Yale University's Forestry School.\n\nFrom the analysis of specific examples of the nature and physiology are determined 10 axioms and laws of information ecology, which serves as the basis for creating information strategies and tactics in social, economic, political and other spheres that affect human health and human communities.\n\nEddy et. al. (2014) use principles of information ecology to develop a framework for integrating scientific information in decision-making in ecosystem-based management (EBM). Using a metaphor of how a species adapts to environmental changes through information processing, they developed a 3-tiered model that differentiates primary, secondary and tertiary levels of information processing, within both the technical and human domains.\n\n"}
{"id": "12788759", "url": "https://en.wikipedia.org/wiki?curid=12788759", "title": "Karamanli Turkish", "text": "Karamanli Turkish\n\nKaramanlı Turkish is both a form of written Turkish, and a dialect of Turkish spoken by the Karamanlides, a community of Turkish-speaking Orthodox Christians in Ottoman Turkey. While the official Ottoman Turkish was written in the Arabic script, the Karamanlides used the Greek alphabet for writing its form of Turkish. Such texts are called Karamanlidika (Καραμανλήδικα / Καραμανλήδεια γραφή) or Karamanli Turkish today. Karamanli Turkish had its own literary tradition and produced numerous published works in print in the 19th century, some of them published by Evangelinos Misailidis, by the Anatoli or Misailidis publishing house (Misailidis 1986, p. 134).\n\nKaramanli writers and speakers were expelled from Turkey as part of the Greek-Turkish population exchange of 1923. Some speakers preserved their language in the diaspora. The writing form stopped being used immediately after the Turkish state adopted the Latin alphabet.\n\nA fragment of a Manuscript written in Karamanli was also found in the Cairo Geniza.\n"}
{"id": "1570429", "url": "https://en.wikipedia.org/wiki?curid=1570429", "title": "Loneliness", "text": "Loneliness\n\nLoneliness is a complex and usually unpleasant emotional response to isolation. Loneliness typically includes anxious feelings about a lack of connection or communication with other beings, both in the present and extending into the future. As such, loneliness can be felt even when surrounded by other people. The causes of loneliness are varied and include social, mental, emotional, and physical factors.\n\nResearch has shown that loneliness is prevalent throughout society, including people in marriages, relationships, families, veterans, and those with successful careers. It has been a long explored theme in the literature of human beings since classical antiquity. Loneliness has also been described as social pain—a psychological mechanism meant to motivate an individual to seek social connections. Loneliness is often defined in terms of one's connectedness to others, or more specifically as \"the unpleasant experience that occurs when a person's network of social relations is deficient in some important way\".\n\nPeople can experience loneliness for many reasons, and many life events may cause it, such as a lack of friendship relations during childhood and adolescence, or the physical absence of meaningful people around a person. At the same time, loneliness may be a symptom of another social or psychological problem, such as chronic depression.\n\nMany people experience loneliness for the first time when they are left alone as infants. It is also a very common, though normally temporary, consequence of a breakup, divorce, or loss of any important long-term relationship. In these cases, it may stem both from the loss of a specific person and from the withdrawal from social circles caused by the event or the associated sadness.\n\nThe loss of a significant person in one's life will typically initiate a grief response; in this situation, one might feel lonely, even while in the company of others. Loneliness may also occur after the birth of a child (often expressed in postpartum depression), after marriage, or following any other socially disruptive event, such as moving from one's home town into an unfamiliar community, leading to homesickness. Loneliness can occur within unstable marriages or other close relationships of a similar nature, in which feelings present may include anger or resentment, or in which the feeling of love cannot be given or received. Loneliness may represent a dysfunction of communication, and can also result from places with low population densities in which there are comparatively few people to interact with. Loneliness can also be seen as a social phenomenon, capable of spreading like a disease. When one person in a group begins to feel lonely, this feeling can spread to others, increasing everybody's risk for feelings of loneliness. People can feel lonely even when they are surrounded by other people.\n\nA twin study found evidence that genetics account for approximately half of the measurable differences in loneliness among adults, which was similar to the heritability estimates found previously in children. These genes operate in a similar manner in males and females. The study found no common environmental contributions to adult loneliness.\n\nThere is a clear distinction between feeling lonely and being socially isolated (for example, a loner). In particular, one way of thinking about loneliness is as a discrepancy between one's necessary and achieved levels of social interaction, while solitude is simply the lack of contact with people. Loneliness is therefore a subjective experience; if a person thinks they are lonely, then they are lonely. People can be lonely while in solitude, or in the middle of a crowd. What makes a person lonely is the fact that they need more social interaction or a certain type of social interaction that is not currently available. A person can be in the middle of a party and feel lonely due to not talking to enough people. Conversely, one can be alone and not feel lonely; even though there is no one around that person is not lonely because there is no desire for social interaction. There have also been suggestions that each person has their own optimal level of social interaction. If a person gets too little or too much social interaction, this could lead to feelings of loneliness or over-stimulation.\n\nSolitude can have positive effects on individuals. One study found that, although time spent alone tended to depress a person's mood and increase feelings of loneliness, it also helped to improve their cognitive state, such as improving concentration. Furthermore, once the alone time was over, people's moods tended to increase significantly. Solitude is also associated with other positive growth experiences, religious experiences, and identity building such as solitary quests used in rites of passages for adolescents.\n\nLoneliness can also play an important role in the creative process. In some people, temporary or prolonged loneliness can lead to notable artistic and creative expression, for example, as was the case with poets Emily Dickinson and Isabella di Morra, and numerous musicians. This is not to imply that loneliness itself ensures this creativity, rather, it may have an influence on the subject matter of the artist and more likely be present in individuals engaged in creative activities.\n\nThe other important typology of loneliness focuses on the time perspective. In this respect, loneliness can be viewed as either transient or chronic. It has also been referred to as state and trait loneliness.\n\nTransient (state) loneliness is temporary in nature, caused by something in the environment, and is easily relieved. Chronic (trait) loneliness is more permanent, caused by the person, and is not easily relieved. For example, when a person is sick and cannot socialize with friends would be a case of transient loneliness. Once the person got better it would be easy for them to alleviate their loneliness. A person who feels lonely regardless of if they are at a family gathering, with friends, or alone is experiencing chronic loneliness. It does not matter what goes on in the surrounding environment, the experience of loneliness is always there.\n\nThe existentialist school of thought views loneliness as the essence of being human. Each human being comes into the world alone, travels through life as a separate person, and ultimately dies alone. Coping with this, accepting it, and learning how to direct our own lives with some degree of grace and satisfaction is the human condition.\n\nSome philosophers, such as Sartre, believe in an epistemic loneliness in which loneliness is a fundamental part of the human condition because of the paradox between people's consciousness desiring meaning in life and the isolation and nothingness of the universe. Conversely, other existentialist thinkers argue that human beings might be said to actively engage each other and the universe as they communicate and create, and loneliness is merely the feeling of being cut off from this process.\n\nThere are several estimates and indicators of loneliness. It has been estimated that approximately 60 million people in the United States, or 20% of the total population, feel lonely. Another study found that 12% of Americans have no one with whom to spend free time or to discuss important matters. Other research suggests that this rate has been increasing over time. The \"General Social Survey\" found that between 1985 and 2004, the number of people the average American discusses important matters with decreased from three to two. Additionally, the number of Americans with no one to discuss important matters with tripled (though this particular study may be flawed). In the UK research by Age UK shows half a million people more than 60 years old spend each day alone without social interaction and almost half a million more see and speak to no one for 5 or 6 days a week. On the other hand, the \"Community Life Survey, 2016 to 2017\", by the UK's Office for National Statistics, found that young adults in England aged 16 to 24 reported feeling lonely more often than those in older age groups.\n\nLoneliness appears to have intensified in every society in the world as modernization occurs. A certain amount of this loneliness appears to be related to greater migration, smaller household sizes, a larger degree of media consumption (all of which have positive sides as well in the form of more opportunities, more choice in family size, and better access to information), all of which relates to social capital.\n\nWithin developed nations, loneliness has shown the largest increases among two groups: seniors and people living in low-density suburbs. Seniors living in suburban areas are particularly vulnerable, for as they lose the ability to drive, they often become \"stranded\" and find it difficult to maintain interpersonal relationships.\n\nLoneliness is prevalent in vulnerable groups in society. In New Zealand the fourteen surveyed groups with the highest prevalence of loneliness most/all of the time in descending order are: disabled, recent migrants, low income households, unemployed, single parents, rural (rest of South Island), seniors aged 75+, not in the labour force, youth aged 15-24, no qualifications, not housing owner-occupier, not in a family nucleus, Māori, and low personal income.\n\nAmericans seem to report more loneliness than any other country, though this finding may simply be an effect of greater research volume. A 2006 study in the \"American Sociological Review\" found that Americans on average had only two close friends in which to confide, which was down from an average of three in 1985. The percentage of people who noted having no such confidant rose from 10% to almost 25%, and an additional 19% said they had only a single confidant, often their spouse, thus raising the risk of serious loneliness if the relationship ended. The modern office environment has been demonstrated to give rise to loneliness. This can be especially prevalent in individuals prone to social isolation who can interpret the business focus of co-workers for a deliberate ignoring of needs.\n\nWhether a correlation exists between Internet usage and loneliness is a subject of controversy, with some findings showing that Internet users are lonelier and others showing that lonely people who use the Internet to keep in touch with loved ones (especially seniors) report less loneliness, but that those trying to make friends online became lonelier. On the other hand, studies in 2002 and 2010 found that \"Internet use was found to decrease loneliness and depression significantly, while perceived social support and self-esteem increased significantly\" and that the Internet \"has an enabling and empowering role in people's lives, by increasing their sense of freedom and control, which has a positive impact on well-being or happiness.\"\nThe one apparently unequivocal finding of correlation is that long driving commutes correlate with dramatically higher reported feelings of loneliness (as well as other negative health impacts).\n\nLoneliness has been linked with depression, and is thus a risk factor for suicide. Émile Durkheim has described loneliness, specifically the inability or unwillingness to live for others, i.e. for friendships or altruistic ideas, as the main reason for what he called \"egoistic suicide\". In adults, loneliness is a major precipitant of depression and alcoholism. People who are socially isolated may report poor sleep quality, and thus have diminished restorative processes. Loneliness has also been linked with a schizoid character type in which one may see the world differently and experience social alienation, described as \"the self in exile\".\n\nIn children, a lack of social connections is directly linked to several forms of antisocial and self-destructive behavior, most notably hostile and delinquent behavior. In both children and adults, loneliness often has a negative impact on learning and memory. Its disruption of sleep patterns can have a significant impact on the ability to function in everyday life.\n\nResearch from a large-scale study published in the journal Psychological Medicine, showed that \"lonely millennials are more likely to have mental health problems, be out of work and feel pessimistic about their ability to succeed in life than their peers who feel connected to others, regardless of gender or wealth\".\n\nPain, depression, and fatigue function as a symptom cluster and thus may share common risk factors. Two longitudinal studies with different populations demonstrated that loneliness was a risk factor for the development of the pain, depression, and fatigue symptom cluster over time. These data also highlight the health risks of loneliness; pain, depression, and fatigue often accompany serious illness and place people at risk for poor health and mortality.\n\nChronic loneliness can be a serious, life-threatening health condition. It has been found to be associated with an increased risk of stroke and cardiovascular disease. Loneliness shows an increased incidence of high blood pressure, high cholesterol, and obesity.\n\nLoneliness is shown to increase the concentration of cortisol levels in the body. Prolonged, high cortisol levels can cause anxiety, depression, digestive problems, heart disease, sleep problems, and weight gain.\n\n″Loneliness has been associated with impaired cellular immunity as reflected in lower natural killer (NK) cell activity and higher antibody titers to the Epstein Barr Virus and human herpes viruses\". Because of impaired cellular immunity, loneliness among young adults shows vaccines, like the flu vaccine, to be less effective. Data from studies on loneliness and HIV positive men suggests loneliness increases disease progression.\n\nThere are a number of potential physiological mechanisms linking loneliness to poor health outcomes. In 2005, results from the American \"Framingham Heart Study\" demonstrated that lonely men had raised levels of Interleukin 6 (IL-6), a blood chemical linked to heart disease. A 2006 study conducted by the \"Center for Cognitive and Social Neuroscience\" at the University of Chicago found loneliness can add thirty points to a blood pressure reading for adults over the age of fifty. Another finding, from a survey conducted by John Cacioppo from the University of Chicago, is that doctors report providing better medical care to patients who have a strong network of family and friends than they do to patients who are alone. Cacioppo states that loneliness impairs cognition and willpower, alters DNA transcription in immune cells, and leads over time to high blood pressure. Lonelier people are more likely to show evidence of viral reactivation than less lonely people. Lonelier people also have stronger inflammatory responses to acute stress compared with less lonely people; inflammation is a well known risk factor for age-related diseases.\n\nWhen someone feels left out of a situation, they feel excluded and one possible side effect is for their body temperature to decrease. When people feel excluded blood vessels at the periphery of the body may narrow, preserving core body heat. This class protective mechanism is known as vasoconstriction.\n\nThere are many different ways used to treat loneliness, social isolation, and clinical depression. The first step that most doctors recommend to patients is therapy. Therapy is a common and effective way of treating loneliness and is often successful. Short-term therapy, the most common form for lonely or depressed patients, typically occurs over a period of ten to twenty weeks. During therapy, emphasis is put on understanding the cause of the problem, reversing the negative thoughts, feelings, and attitudes resulting from the problem, and exploring ways to help the patient feel connected. Some doctors also recommend group therapy as a means to connect with other sufferers and establish a support system. Doctors also frequently prescribe anti-depressants to patients as a stand-alone treatment, or in conjunction with therapy. It may take several attempts before a suitable anti-depressant medication is found.\n\nAlternative approaches to treating depression are suggested by many doctors. These treatments include exercise, dieting, hypnosis, electro-shock therapy, acupuncture, and herbs, amongst others. Many patients find that participating in these activities fully or partially alleviates symptoms related to depression.\nAnother treatment for both loneliness and depression is pet therapy, or animal-assisted therapy, as it is more formally known. Studies and surveys, as well as anecdotal evidence provided by volunteer and community organizations, indicate that the presence of animal companions such as dogs, cats, rabbits, and guinea pigs can ease feelings of depression and loneliness among some sufferers. Beyond the companionship the animal itself provides there may also be increased opportunities for socializing with other pet owners. According to the Centers for Disease Control and Prevention there are a number of other health benefits associated with pet ownership, including lowered blood pressure and decreased levels of cholesterol and triglycerides.\n\nNostalgia has also been found to have a restorative effect, counteracting loneliness by increasing perceived social support.\n\nA 1989 study found that the social aspect of religion had a significant negative association with loneliness among elderly people. The effect was more consistent than the effect of social relationships with family and friends, and the subjective concept of religiosity had no significant effect on loneliness.\n\nOne study compared the effectiveness of four interventions: improving social skills, enhancing social support, increasing opportunities for social interaction, addressing abnormal social cognition (faulty thoughts and patterns of thoughts). The results of the study indicated that all interventions were effective in reducing loneliness, possibly with the exception of social skill training. Results of the meta-analysis suggest that correcting maladaptive social cognition offers the best chance of reducing loneliness.\n\n"}
{"id": "54866690", "url": "https://en.wikipedia.org/wiki?curid=54866690", "title": "Maas–Hoffman model", "text": "Maas–Hoffman model\n\nThe Maas–Hoffman model is a mathematical tool to characterize the relation between crop production and soil salinity. It describes the crop response by a broken line of which the first part is horizontal and the second is sloping downward. The \"breakpoint\" (Pb) or \"threshold\" is also called \"tolerance\" because up to that point the yield is unaffected by the salinity, so the salt is tolerated, while at greater salinity values the crops are affected negatively and the yield goes down.\n\nMathematically the two lines are represented by the equations:\n\nwhere \"Y\" = crop production or yield, \"C\" = maximum yield, \"X\" = soil salinity, \"A\" = slope (regression coefficient) of the descending line, and \"B\" = regression constant of that line.\n\nIn the example of the figure: \"C\" = 1.2, \"A\" = —0.10, Pb = 7.0\n\nThe value of Pb is to be found by regression analysis and optimization so that the goodness of fit of the data to the model is maximum.\n\nThe Maas–Hoffman model is used in crop tolerance to seawater.\n\nFor growth factors, like the depth of the watertable, that affect crop production negatively at low values while there is no effect at high values, the inverted Maas–Hoffman model can be used.\n\n"}
{"id": "27937488", "url": "https://en.wikipedia.org/wiki?curid=27937488", "title": "Mahayana", "text": "Mahayana\n\nMahāyāna (; Sanskrit for \"Great Vehicle\") is one of two main existing branches of Buddhism (the other being Theravada) and a term for classification of Buddhist philosophies and practice. This movement added a further set of discourses, and although it was initially small in India, it had long-term historical significance. The Buddhist tradition of Vajrayana is sometimes classified as a part of Mahayana Buddhism, but some scholars consider it to be a different branch altogether.\n\nAccording to the teachings of Mahāyāna traditions, \"Mahāyāna\" also refers to the path of the Bodhisattva seeking complete enlightenment for the benefit of all sentient beings, also called \"Bodhisattvayāna\", or the \"Bodhisattva Vehicle\". A bodhisattva who has accomplished this goal is called a samyaksaṃbuddha, or \"fully enlightened Buddha\". A samyaksaṃbuddha can establish the Dharma and lead disciples to enlightenment. Mahayana Buddhists teach that enlightenment can be attained in a single lifetime, and this can be accomplished even by a layperson.\n\nThe Mahāyāna tradition is the largest major tradition of Buddhism existing today, with 53.2% of practitioners, compared to 35.8% for Theravada and 5.7% for Vajrayana in 2010.\n\nIn the course of its history, Mahāyāna Buddhism spread from India to various other South, East and Southeast Asian countries such as Bangladesh, Nepal, Bhutan, China, Taiwan, Mongolia, Korea, Japan, Vietnam, Indonesia, Malaysia and Singapore. Mahayana Buddhism also spread to other South and Southeast Asian countries, such as Afghanistan, Thailand, Cambodia, Laos, the Maldives, Pakistan, Sri Lanka, Burma, Iran and other Central Asian countries before being replaced by Theravada Buddhism or other religions.\nLarge Mahāyāna scholastic centers thrived during the latter period of Buddhism in India, between the seventh and twelfth centuries. Major traditions of Mahāyāna Buddhism today include Chan Buddhism, Korean Seon, Japanese Zen, Pure Land Buddhism, Nichiren Buddhism and Vietnamese Buddhism. It may also include the Vajrayana traditions of Tiantai, Tendai, Shingon Buddhism, and Tibetan Buddhism, which add esoteric teachings to the Mahāyāna tradition.\n\nAccording to Jan Nattier, the term \"Mahāyāna\" (\"Great Vehicle\") was originally an honorary synonym for \"Bodhisattvayāna\" (\"Bodhisattva Vehicle\") — the vehicle of a bodhisattva seeking buddhahood for the benefit of all sentient beings. The term \"Mahāyāna\" (which had earlier been used simply as an epithet for Buddhism itself) was therefore adopted at an early date as a synonym for the path and the teachings of the bodhisattvas. Since it was simply an honorary term for \"Bodhisattvayāna\", the adoption of the term \"Mahāyāna\" and its application to Bodhisattvayāna did not represent a significant turning point in the development of a Mahāyāna tradition.\n\nThe earliest Mahāyāna texts often use the term \"Mahāyāna\" as a synonym for \"Bodhisattvayāna\", but the term \"Hīnayāna\" is comparatively rare in the earliest sources. The presumed dichotomy between \"Mahāyāna\" and \"Hīnayāna\" can be deceptive, as the two terms were not actually formed in relation to one another in the same era.\n\nAmong the earliest and most important references to \"Mahāyāna\" are those that occur in the \"Lotus Sūtra\" (Skt. \"Saddharma Puṇḍarīka Sūtra\") dating between the 1st century BCE and the 1st century CE. Seishi Karashima has suggested that the term first used in an earlier Gandhāri Prakrit version of the \"Lotus Sūtra\" was not the term \"mahāyāna\" but the Prakrit word \"mahājāna\" in the sense of \"mahājñāna\" (great knowing). At a later stage when the early Prakrit word was converted into Sanskrit, this \"mahājāna\", being phonetically ambivalent, was mistakenly converted into \"mahāyāna\", possibly because of what may have been a double meaning in the famous Parable of the Burning House, which talks of three vehicles or carts (Skt: \"yāna\").\n\nThe origins of Mahāyāna are still not completely understood. The earliest Western views of Mahāyāna assumed that it existed as a separate school in competition with the so-called \"Hīnayāna\" schools. The earliest Mahāyāna texts often depict strict adherence to the path of a bodhisattva, and engagement in the ascetic ideal of a monastic life in the wilderness, akin to the ideas expressed in the \"Rhinoceros Sūtra\".\n\nThe earliest textual evidence of \"Mahāyāna\" comes from sūtras originating around the beginning of the common era. Jan Nattier has noted that some of the earliest Mahāyāna texts such as the \"Ugraparipṛccha Sūtra\" use the term \"Mahāyāna\", yet there is no doctrinal difference between Mahāyāna in this context and the early schools, and that \"Mahāyāna\" referred rather to the rigorous emulation of Gautama Buddha in the path of a bodhisattva seeking to become a fully enlightened buddha.\n\nThere is also no evidence that Mahāyāna ever referred to a separate formal school or sect of Buddhism, but rather that it existed as a certain set of ideals, and later doctrines, for bodhisattvas. Paul Williams has also noted that the Mahāyāna never had nor ever attempted to have a separate Vinaya or ordination lineage from the early schools of Buddhism, and therefore each bhikṣu or bhikṣuṇī adhering to the Mahāyāna formally belonged to an early school. Membership in these \"nikāyas\", or monastic sects, continues today with the Dharmaguptaka nikāya in East Asia, and the Mūlasarvāstivāda nikāya in Tibetan Buddhism. Therefore, Mahāyāna was never a separate rival sect of the early schools. Paul Harrison clarifies that while monastic Mahāyānists belonged to a nikāya, not all members of a nikāya were Mahāyānists. From Chinese monks visiting India, we now know that both Mahāyāna and non-Mahāyāna monks in India often lived in the same monasteries side by side. It is also possible that, formally, Mahāyāna would have been understood as a group of monks or nuns within a larger monastery taking a vow together (known as a \"kriyākarma\") to memorize and study a Mahāyāna text or texts.\n\nThe Chinese monk Yijing, who visited India in the 7th century CE, distinguishes Mahāyāna from Hīnayāna as follows:\n\nMuch of the early extant evidence for the origins of Mahāyāna comes from early Chinese translations of Mahāyāna texts. These Mahāyāna teachings were first propagated into China by Lokakṣema, the first translator of Mahāyāna sūtras into Chinese during the 2nd century CE.\n\nBased on the testimony of Candrakīrti (7th cent.) several scholars have suggested that the Prajñāpāramitā sūtras, which are among the earliest Mahayana sutras, developed among the Mahāsāṃghika along the Kṛṣṇa River in the Āndhra region of southern India. However, more recently Seishi Karashima has argued for their origin in the Gandhara region. \n\nThe earliest Mahāyāna sūtras include the very first versions of the Prajñāpāramitā genre, along with texts concerning Akṣobhya Buddha, which were probably written down in the 1st century BCE in the south of India. Guang Xing states, \"Several scholars have suggested the Prajñāpāramitā probably developed among the Mahāsāṃghikas in southern India, in the Āndhra country, on the Kṛṣṇa River.\" A.K. Warder believes that \"the Mahāyāna originated in the south of India and almost certainly in the Āndhra country.\"\n\nAnthony Barber and Sree Padma note that \"historians of Buddhist thought have been aware for quite some time that such pivotally important Mahayana Buddhist thinkers as Nāgārjuna, Dignaga, Candrakīrti, Āryadeva, and Bhavaviveka, among many others, formulated their theories while living in Buddhist communities in Āndhra.\" They note that the ancient Buddhist sites in the lower Kṛṣṇa Valley, including Amaravati, Nāgārjunakoṇḍā and Jaggayyapeṭa \"can be traced to at least the third century BCE, if not earlier.\" Akira Hirakawa notes the \"evidence suggests that many Early Mahayana scriptures originated in South India.\"\n\nSome scholars think that the earliest Mahāyāna sūtras were mainly composed in the south of India, and later the activity of writing additional scriptures was continued in the north. However, the assumption that the presence of an evolving body of Mahāyāna scriptures implies the contemporaneous existence of distinct religious movement called \"Mahāyāna\", may be a serious misstep. Some scholars further speculate that the Prajñāpāramitā sūtras were written in response to the ultrarealism of abhidharma.\n\nSome early Mahāyāna sūtras were translated by the Kuṣāṇa monk Lokakṣema, who came to China from the kingdom of Gandhāra. His first translations to Chinese were made in the Chinese capital of Luoyang between 178 and 189 CE. Some Mahāyāna sūtras translated during the 2nd century CE include the following:\n\nThis corpus of texts often emphasizes ascetic practices and forest dwelling, absorbed in states of meditative concentration.\n\nThe earliest stone inscription containing a recognizably Mahāyāna formulation and a mention of the Buddha Amitābha was found in the Indian subcontinent in Mathura, and dated to around 180 CE. Remains of a statue of a Buddha bear the Brāhmī inscription: \"Made in the year 28 of the reign of King Huviṣka, ... for the Blessed One, the Buddha Amitābha.\" There is also some evidence that Emperor Huviṣka himself was a follower of Mahāyāna Buddhism, and a Sanskrit manuscript fragment in the Schøyen Collection describes Huviṣka as having \"set forth in the Mahāyāna.\" Evidence of the name \"Mahāyāna\" in Indian inscriptions in the period before the 5th century is very limited in comparison to the multiplicity of Mahāyāna writings transmitted from Central Asia to China at that time.\n\nDuring the period of early Mahāyāna Buddhism, four major types of thought developed: Mādhyamaka, Yogācāra, Buddha-nature (\"Tathāgatagarbha\"), and Buddhist logic as the last and most recent. In India, the two main philosophical schools of the Mahāyāna were the Mādhyamaka and the later Yogācāra. During the Kushan Empire, Mahayana Buddhism teachings encouraged societies to give generous donations to the Buddhist monasteries, which gave the people \"religious merits\".\n\nEarlier stage forms of Mahāyāna such as the doctrines of Prajñāpāramitā, Yogācāra, Buddha Nature, and the Pure Land teachings are still popular in East Asia. In some cases these have spawned new developments, while in others they are treated in the more traditional syncretic manner. Paul Williams has noted that in this tradition in the Far East, primacy has always been given to study of the sūtras.\n\nVarious classes of Vajrayana literature developed as a result of royal courts sponsoring both Buddhism and Saivism. The Mañjusrimulakalpa, which later came to classified under Kriyatantra, states that mantras taught in the Shaiva, Garuda and Vaishnava tantras will be effective if applied by Buddhists since they were all taught originally by Manjushri. The Guhyasiddhi of Padmavajra, a work associated with the Guhyasamaja tradition, prescribes acting as a Shaiva guru and initiating members into Saiva Siddhanta scriptures and mandalas. The Samvara tantra texts adopted the pitha list from the Shaiva text \"Tantrasadbhava\", introducing a copying error where a deity was mistaken for a place.\n\nFew things can be said with certainty about Mahāyāna Buddhism, especially its early Indian form, other than that the Buddhism practiced in China, Indonesia, Vietnam, Korea, Tibet, and Japan is Mahāyāna Buddhism. Mahāyāna can be described as a loosely bound collection of many teachings with large and expansive doctrines that are able to exist simultaneously.\n\nMahāyāna constitutes an inclusive tradition characterized by plurality and the adoption of new Mahayana sutras in addition to the earlier āgamas. Mahāyāna sees itself as penetrating further and more profoundly into the Buddha's Dharma. An Indian commentary on the \"Mahāyānasaṃgraha\", entitled \"Vivṛtaguhyārthapiṇḍavyākhyā\", gives a classification of teachings according to the capabilities of the audience:\n\nThere is also a tendency in Mahāyāna sūtras to regard adherence to these sūtras as generating spiritual benefits greater than those that arise from being a follower of the non-Mahāyāna approaches to Dharma. Thus the \"Śrīmālādevī Siṃhanāda Sūtra\" claims that the Buddha said that devotion to Mahāyāna is inherently superior in its virtues to following the śrāvaka or pratyekabuddha paths.\n\nThe fundamental principles of Mahāyāna doctrine were based on the possibility of universal liberation from dukkha for all beings (hence the \"Great Vehicle\") and the existence of buddhas and bodhisattvas embodying Buddha-nature. The Pure Land school of Mahāyāna simplifies the expression of faith by allowing salvation to be alternatively obtained through the grace of the buddha Amitābha by having faith and devoting oneself to mindfulness of the Buddha. This devotional lifestyle of Buddhism has greatly contributed to the success of Mahāyāna in East Asia, where spiritual elements traditionally relied upon mindfulness of the Buddha, mantras and dhāraṇīs, and reading sutras. In Chinese Buddhism, most monks, let alone lay people, practice Pure Land, some combining it with Chan Buddhism.\n\nMost Mahāyāna schools believe in supernatural bodhisattvas who devote themselves to the pāramitās, ultimate knowledge (Skt. \"sarvajñāna\"), and the liberation of all sentient beings.\n\nThe Mahāyāna tradition holds that pursuing only the release from suffering and attainment of Nirvāṇa is too narrow an aspiration, because it lacks the motivation of actively resolving to liberate all other sentient beings from saṃsāra, \"suffering\". One who engages in this path is called a bodhisattva. Bodhisattvas could reach nirvana, but they believe it is more important to help others on their path of finding nirvana rather than committing fully to nirvana themselves.\n\nThe defining characteristic of a bodhisattva is bodhicitta, the intention to achieve omniscient Buddhahood (Trikaya) as fast as possible, so that one may benefit infinite sentient beings. Sometimes the term bodhisattva is used more restrictively to refer to those sentient beings on the grounds. As Ananda Coomaraswamy notes, \"The most essential part of the Mahayana is its emphasis on the Bodhisattva ideal, which replaces that of the arhat, or ranks before it.\" According to Mahāyāna teachings, being a high-level bodhisattva involves possessing a mind of great compassion and prajñā (wisdom) to realize the reality of inherent emptiness and dependent origination. Mahāyāna teaches that the practitioner will finally realize the attainment of Buddhahood.\n\nSix pāramitās are traditionally required for bodhisattvas:\n\nExpedient means (Skt. \"upāya\") is found in the \"Lotus Sutra\", one of the earliest-dated sutras, and is accepted in all Mahāyāna schools of thought. It is any effective method that aids awakening. It does not necessarily mean that some particular method is \"untrue\" but is simply any means or stratagem that is conducive to spiritual growth and leads beings to awakening and nirvana. Expedient means could thus be certain motivational words for a particular listener or even the Noble Eightfold Path itself. Basic Buddhism (what Mahāyāna would term \"śrāvakayāna\" or \"pratyekabuddhayāna\") is an expedient method for helping people begin the noble Buddhist path and advance quite far. But the path is not wholly traversed, according to some schools, until the practitioner has striven for and attained Buddhahood for the liberation of all other sentient beings from suffering.\n\nSome scholars have stated that the exercise of expedient means, \"the ability to adapt one's message to the audience, is also of enormous importance in the Pāli canon.\" In fact the Pāli term \"upāya-kosalla\" does occur in the Pāli Canon, in the \"Sangiti Sutta\" of the \"Digha Nikāya\".\n\nMahāyāna Buddhism includes a rich cosmology, with various Buddhas and bodhisattvas residing in different worlds and buddha-realms. The concept of the three bodies (\"trikāya\") supports these constructions, making the Buddha himself a transcendental figure. Dr. Guang Xing describes the Mahāyāna Buddha as \"an omnipotent divinity endowed with numerous supernatural attributes and qualities ...[He] is described almost as an omnipotent and almighty godhead.\"\n\n\"Buddha-nature\", \"Buddha-dhatu\" or \"Buddha Principle\" (Skt: \"Buddha-dhātu\", \"Tathāgatagarbha\"; Jap: \"Bussho\"), is taught differently in various Mahayana Buddhism traditions. Broadly speaking Buddha-nature is concerned with ascertaining what allows sentient beings to become Buddhas. The term, Buddha nature, is a translation of the Sanskrit coinage, 'Buddha-dhātu', which seems first to have appeared in the \"Mahāyāna Mahāparinirvāṇa Sūtra\", where it refers to \"a sacred nature that is the basis for [beings'] becoming buddhas\", and where it is also spoken of as the 'Self' (\"atman\").\n\nIt is called \"Tathāgatagarbha\" \"Buddha-dhātu\" at the stage of sentient beings because it is covered with defilements, and it is called \"Dharmakāya\" at the stage of Buddhahood, because its pure nature is revealed.\n\nThe teaching of a \"Buddha nature\" (Skt. \"tathāgatagarbha\") may be based on the \"luminous mind\" concept found in the \"Āgamas\". The essential idea, articulated in the Buddha nature sūtras, but not accepted by all Mahāyānists, is that no being is without a concealed but indestructible interior link to the awakening of \"bodhi\" and that this link is an uncreated element (\"dhātu\") or principle deep inside each being, which constitutes the deathless, diamond-like \"essence of the self\". The \"Mahāyāna Mahāparinirvāṇa Sūtra\" states: \"The essence of the Self (\"ātman\") is the subtle Buddha nature...\" while the later \"Laṅkāvatāra Sūtra\" states that the Buddha nature might be taken to be self (\"ātman\"), but it is not. In the \"sagathakam\" section of that same sutra, however, the Tathagatagarbha as the Self is not denied, but affirmed: \"The \"Atma\" [Self] characterised with purity is the state of self-realization; this is the Tathagata's Womb (\"garbha\"), which does not belong to the realm of the theorisers.\" In the Buddha nature class of sūtras, the word \"self\" (\"ātman\") is used in a way defined by and specific to these sūtras. (See Atman (Buddhism).)\n\nAccording to some scholars, the Buddha nature discussed in some Mahāyāna sūtras does not represent a substantial self (\"ātman\"); rather, it is a positive language and expression of emptiness (\"śūnyatā\") and represents the potentiality to realize Buddhahood through Buddhist practices. It is the \"true self\" in representing the innate aspect of the individual that makes actualizing the ultimate personality possible.\n\nThe actual \"seeing and knowing\" of this Buddha essence is said to usher in nirvanic liberation. This Buddha essence or \"Buddha nature\" is stated to be found in every single person, ghost, god and sentient being. In the Buddha nature sūtras, the Buddha is portrayed as describing the Buddha essence as uncreated, deathless and ultimately beyond rational grasping or conceptualisation. Yet, it is this already real and present, hidden internal element of awakeness (\"bodhi\") that, according to the Buddha nature sūtras, prompts beings to seek liberation from worldly suffering, and lets them attain the spotless bliss that lies at the heart of their being. Once the veils of negative thoughts, feelings, and unwholesome behaviour (the \"kleśas\") are eliminated from the mind and character, the indwelling Buddha principle (\"Buddha-dhātu\": Buddha nature) can shine forth unimpededly and transform the seer into a Buddha.\n\nPrior to the period of these sūtras, Mahāyāna metaphysics was dominated by teachings on emptiness, in the form of Madhyamaka philosophy. The language used by this approach is primarily negative, and the Buddha nature genre of sūtras can be seen as an attempt to state orthodox Buddhist teachings of dependent origination and on the mysterious reality of nirvana using positive language instead, to prevent people from being turned away from Buddhism by a false impression of nihilism. In these sūtras the perfection of the wisdom of not-self is stated to be the true self; the ultimate goal of the path is then characterized using a range of positive language that had been used in Indian philosophy previously by essentialist philosophers, but was now transmuted into a new Buddhist vocabulary that described a being who has successfully completed the Buddhist path.\n\nA different view is propounded by Tathagatagarbha specialist, Michael Zimmermann, who sees key Buddha-nature sutras such as the \"Nirvana Sutra\" and the \"Tathagatagarbha Sutra\", as well as the \"Lankavatara Sutra\", enunciating an affirmative vision of an eternal, indestructible Buddhic Self. Zimmermann observes:\n\nThe \"Uttaratantra\" (an exegetical treatise on Buddha nature) sees Buddha nature not as caused and conditioned (\"saṃskṛta\"), but as eternal, uncaused, unconditioned, and incapable of being destroyed, although temporarily concealed within worldly beings by adventitious defilements. According to C. D. Sebastian, the \"Uttaratantra's\" reference to a transcendental self (\"ātma-pāramitā\") should be understood as \"the unique essence of the universe\", thus the universal and immanent essence of Buddha nature is the same throughout time and space.\n\nMahāyāna Buddhism takes the basic teachings of the Buddha as recorded in early scriptures as the starting point of its teachings, such as those concerning karma and rebirth, anātman, emptiness, dependent origination, and the Four Noble Truths. Mahāyāna Buddhists in East Asia have traditionally studied these teachings in the Āgamas preserved in the Chinese Buddhist canon. \"Āgama\" is the term used by those traditional Buddhist schools in India who employed Sanskrit for their basic canon. These correspond to the Nikāyas used by the Theravāda school. The surviving Āgamas in Chinese translation belong to at least two schools, while most of the Āgamas teachings were never translated into Tibetan.\n\nIn addition to accepting the essential scriptures of the early Buddhist schools as valid, Mahāyāna Buddhism maintains large collections of sūtras that are not used or recognized by the Theravāda school. These were not recognized by some individuals in the early Buddhist schools. In other cases, Buddhist communities were divided along these doctrinal lines. In Mahāyāna Buddhism, the Mahāyāna sūtras are often given greater authority than the Āgamas. The first of these Mahāyāna-specific writings were written probably around the 1st century BCE or 1st century CE.\n\nIn the 4th century Mahāyāna abhidharma work \"Abhidharmasamuccaya\", Asaṅga refers to the collection which contains the āgamas as the \"Śrāvakapiṭaka\" and associates it with the śrāvakas and pratyekabuddhas. Asaṅga classifies the Mahāyāna sūtras as belonging to the \"Bodhisattvapiṭaka\", which is designated as the collection of teachings for bodhisattvas.\n\nDating back at least to the \"Saṃdhinirmocana Sūtra\" is a classification of the corpus of Buddhism into three categories, based on ways of understanding the nature of reality, known as the \"Three Turnings of the Dharma Wheel\". According to this view, there were three such \"turnings\":\n\n\nSome traditions of Tibetan Buddhism consider the teachings of Esoteric Buddhism and Vajrayāna to be the third turning of the Dharma Wheel. Tibetan teachers, particularly of the Gelugpa school, regard the second turning as the highest teaching, because of their particular interpretation of Yogācāra doctrine. The Buddha Nature teachings are normally included in the third turning of the wheel. The Chinese tradition has a different scheme.\n\nThe Chinese T'ien-T'ai believed the Buddha taught over Five Periods. These are:\n\n\nScholars have noted that many key Mahāyāna ideas are closely connected to the earliest texts of Buddhism. The seminal work of Mahāyāna philosophy, Nāgārjuna's \"Mūlamadhyamakakārikā\", mentions the canon's \"Katyāyana Sūtra\" (SA 301) by name, and may be an extended commentary on that work. Nāgārjuna systematized the Mādhyamaka school of Mahāyāna philosophy. He may have arrived at his positions from a desire to achieve a consistent exegesis of the Buddha's doctrine as recorded in the canon. In his eyes the Buddha was not merely a forerunner, but the very founder of the Mādhyamaka system. Nāgārjuna also referred to a passage in the canon regarding \"nirvanic consciousness\" in two different works.\n\nYogācāra, the other prominent Mahāyāna school in dialectic with the Mādhyamaka school, gave a special significance to the canon's \"Lesser Discourse on Emptiness\" (MA 190). A passage there (which the discourse itself emphasizes) is often quoted in later Yogācāra texts as a true definition of emptiness. According to Walpola Rahula, the thought presented in the Yogācāra school's \"Abhidharma-samuccaya\" is undeniably closer to that of the Pali Nikayas than is that of the Theravadin Abhidhamma.\n\nBoth the Mādhyamikas and the Yogācārins saw themselves as preserving the Buddhist Middle Way between the extremes of nihilism (everything as unreal) and substantialism (substantial entities existing). The Yogācārins criticized the Mādhyamikas for tending towards nihilism, while the Mādhyamikas criticized the Yogācārins for tending towards substantialism.\n\nKey Mahāyāna texts introducing the concepts of bodhicitta and Buddha nature also use language parallel to passages in the canon containing the Buddha's description of \"luminous mind\" and appear to have evolved from this idea. \n\nIn the early Buddhist texts, and as taught by the modern Theravada school, the goal of becoming a teaching Buddha in a future life is viewed as the aim of a small group of individuals striving to benefit future generations after the current Buddha's teachings have been lost, but in the current age there is no need for most practitioners to aspire to this goal. Theravada texts do, however, hold that this is a more perfectly virtuous goal.\n\nPaul Williams writes that some modern Theravada meditation masters in Thailand are popularly regarded as bodhisattvas.\n\nIn the 7th century, the Chinese Buddhist monk Xuanzang describes the concurrent existence of the Mahāvihara and the Abhayagiri Vihara in Sri Lanka. He refers to the monks of the Mahāvihara as the \"Hīnayāna Sthaviras\" (\"Theras\"), and the monks of the Abhayagiri Vihara as the \"Mahāyāna Sthaviras\". Xuanzang further writes:\n\nThe modern Theravāda school is usually described as belonging to Hīnayāna. Some authors have argued that it should not be considered such from the Mahāyāna perspective. Their view is based on a different understanding of the concept of Hīnayāna. Rather than regarding the term as referring to any school of Buddhism that hasn't accepted the Mahāyāna canon and doctrines, such as those pertaining to the role of the bodhisattva, these authors argue that the classification of a school as \"Hīnayāna\" should be crucially dependent on the adherence to a specific phenomenological position. They point out that unlike the now-extinct Sarvāstivāda school, which was the primary object of Mahāyāna criticism, the Theravāda does not claim the existence of independent entities (\"dharmas\"); in this it maintains the attitude of early Buddhism. Adherents of Mahāyāna Buddhism disagreed with the substantialist thought of the Sarvāstivādins and Sautrāntikas, and in emphasizing the doctrine of emptiness, Kalupahana holds that they endeavored to preserve the early teaching. The Theravādins too refuted the Sarvāstivādins and Sautrāntikas (and other schools) on the grounds that their theories were in conflict with the non-substantialism of the canon. The Theravāda arguments are preserved in the \"Kathāvatthu\".\n\nSome contemporary Theravādin figures have indicated a sympathetic stance toward the Mahāyāna philosophy found in texts such as the \"Heart Sūtra\" (Skt. \"Prajñāpāramitā Hṛdaya\") and Nāgārjuna's \"Fundamental Stanzas on the Middle Way\" (Skt. \"Mūlamadhyamakakārikā\").\n\n\n\n"}
{"id": "19735911", "url": "https://en.wikipedia.org/wiki?curid=19735911", "title": "Meinong's jungle", "text": "Meinong's jungle\n\nMeinong's jungle is the name given to the repository of non-existent entities in the ontology of Alexius Meinong.\n\nMeinong, an Austrian philosopher active at the turn of the 20th century, believed that since non-existent things could apparently be referred to, they must have some sort of being, which he termed \"sosein\" (\"being so\"). A unicorn and a pegasus are both non-being; yet it's true that unicorns have horns and pegasi have wings. Thus non-existent things like unicorns, square circles, and golden mountains can have different properties, and must have a 'being such-and-such' even though they lack 'being' proper. The strangeness of such entities led to this ontological realm being referred to as \"Meinong's jungle\". The jungle is described in Meinong's work \"Über Annahmen\" (1902). The name is credited to William C. Kneale, whose \"Probability and Induction\" (1949) includes the passage \"after wandering in Meinong's jungle of subsistence ... philosophers are now agreed that propositions cannot be regarded as ultimate entities\".\n\nThe Meinongian theory of objects (\"Gegenstandstheorie\") was influential in the debate over sense and reference between Gottlob Frege and Bertrand Russell which led to the establishment of analytic philosophy and contemporary philosophy of language. Russell's theory of descriptions, in the words of P.M.S. Hacker, enables him to \"thin out the luxuriant Meinongian jungle of entities (such as the round square), which, it had appeared, must \"in some sense\" subsist in order to be talked about\". According to the theory of descriptions, speakers are not committed to asserting the existence of referents for the names they use.\n\nMeinong's jungle is cited as an objection to Meinong's semantics, as the latter commits one to ontically undesirable objects; it is desirable to be able to speak meaningfully about unicorns, the objection goes, but not to have to believe in them. Nominalists (who believe that general or abstract terms and predicates exist but that either universals or abstract objects do not) find Meinong's jungle particularly unpalatable. As Colin McGinn puts it, \"[g]oing naively by the linguistic appearances leads not only to logical impasse but also to metaphysical extravagance—as with Meinong's jungle, infested with shadowy Being.\" An uneasiness with the ontological commitments of Meinong's theory is commonly expressed in the \"bon mot\" \"we should cut back Meinong's jungle with Occam's razor\".\n\nMeinong's jungle was defended by modal realists, whose possible world semantics offered a more palatable variation of Meinong's \"Gegenstandstheorie\", as Jaakko Hintikka explains:\nHowever, modal realists retain the problem of explaining reference to impossible objects such as square circles. For Meinong, such objects simply have a 'being so' that precludes their having ordinary 'being'. But this entails that 'being so' in Meinong's sense is not equivalent to existing in a possible world.\n\n\n"}
{"id": "44216", "url": "https://en.wikipedia.org/wiki?curid=44216", "title": "Miniature effect", "text": "Miniature effect\n\nA miniature effect is a special effect created for motion pictures and television programs using scale models. Scale models are often combined with high speed photography or matte shots to make gravitational and other effects appear convincing to the viewer. The use of miniatures has largely been superseded by computer-generated imagery in the contemporary cinema.\n\nWhere a miniature appears in the foreground of a shot, this is often very close to the camera lens — for example when matte painted backgrounds are used. Since the exposure is set to the object being filmed so the actors appear well lit, the miniature must be over-lit in order to balance the exposure and eliminate any depth of field differences that would otherwise be visible. This foreground miniature usage is referred to as forced perspective. Another form of miniature effect uses stop motion animation.\n\nUse of scale models in the creation of visual effects by the entertainment industry dates back to the earliest days of cinema. Models and miniatures are copies of people, animals, buildings, settings and objects. Miniatures or models are used to represent things that do not really exist, or that are too expensive or difficult to film in reality, such as explosions, floods or fires.\n\nFrench director Georges Méliès incorporated special effects in his 1902 film \"Le Voyage dans la Lune\" (\"A Trip to the Moon\") — including double-exposure, split screens, miniatures and stop-action.\n\nSome of the most influential visual effects films of these early years such as \"Metropolis\" (1927), \"Citizen Kane\" (1941), \"Godzilla\" (1954) \"The Ten Commandments\" (1956). The 1933 film \"King Kong\" made extensive use of miniature effects including scale models and stop-motion animation of miniature elements.\n\nThe use of miniatures in \"\" was a major development. In production for three years, the film was a significant advancement in creating convincing models.\n\nIn the early 1970s, miniatures were often used to depict disasters in such films as \"The Poseidon Adventure\" (1972), \"Earthquake\" (1974) and \"The Towering Inferno\" (1975).\n\nThe resurgence of the science fiction genre in film in the late 1970s saw miniature fabrication rise to new heights in such films as \"Close Encounters of the Third Kind\", (1977), \"Star Wars\" (also 1977), \"Alien\" (1979), \"\" (1979) and \"Blade Runner\" (1982). Iconic film sequences such as the tanker truck explosion from \"The Terminator\" (1984) and the bridge destruction in \"True Lies\" (1994) were achieved through the use of large-scale miniatures.\n\nThe release of \"Jurassic Park\" (1993) was a turning point in the use of computers to create effects for which physical miniatures would have previously been employed.\n\nWhile the use of computer generated imagery (CGI) has largely overtaken their use since then, they are still often employed, especially for projects requiring physical interaction with fire, explosions or water.\n\"Independence Day\" (1996), \"Titanic\" (1997), \"Godzilla\" (1998), The \"Star Wars\" prequel trilogy (1999-2005), \"The Lord of the Rings\" trilogy (2001-3), \"Casino Royale\" (2006), \"The Dark Knight\" (2008), \"Inception\" (2010), and \"Interstellar\" (2014) are examples of highly successful films that have utilized miniatures for a significant component of their visual effects work.\n\n\n\"Slurpasaur\" is a nickname given to optically enlarged reptiles (and occasionally other animals) that are presented as dinosaurs in motion pictures.\n\nConcurrently with Willis O'Brien and others in making stop-motion animated dinosaurs since the early days of cinema, producers have used optically enlarged lizards, often with horns and fins glued on, to represent dinosaurs, to cut costs, and to present a living analog to dinosaurs, despite huge morphological differences between dinosaurs and reptiles. The first film that used reptiles dressed as dinosaurs was D.W. Griffith's \"Brute Force\". Various slurpasaurs appeared in the 1929 film version of \"The Mysterious Island\", the 1933 British film \"Secret of the Loch\", and the 1936 \"Flash Gordon\" serial. The first major use of the slurpasaur was in \"One Million B.C.\" (1940), which included a pig dressed as a triceratops, with the special effects in this film re-used often, such as in the 1955 movie \"King Dinosaur\".\n\nOther notable films with slurpasaurs include \"Journey to the Center of the Earth\" (1959) and \"The Lost World\" (1960). The former featured reptiles with attached tall spinal fans, simulating Dimetrodons and looked superficially similar to those creatures, as Dimetrodons had a low slung body structure more reminiscent of lizards. The latter is notable for a dinosaur battle wherein a monitor lizard and a young alligator engage in an unsimulated, fierce battle. On the 1960 \"Lost World\", O'Brien, who did the stop-motion dinosaurs for the original, was hired as the effects technician, but was disappointed that producer Irwin Allen opted for live animals.\n\n\n\n"}
{"id": "42357488", "url": "https://en.wikipedia.org/wiki?curid=42357488", "title": "Mode deactivation therapy", "text": "Mode deactivation therapy\n\nMode deactivation therapy (MDT) is a psychotherapeutic approach that addresses dysfunctional emotions, maladaptive behaviors and cognitive processes and contents through a number of goal-oriented, explicit systematic procedures. The name refers to the process of mode deactivation that is based on the concept of cognitive modes as introduced by Aaron T. Beck. The MDT methodology was developed by Jack A. Apsche by combining the unique validation–clarification–redirection (VCR) process step with elements from acceptance and commitment therapy (ACT), dialectical behavior therapy (DBT), and mindfulness to bring about durable behavior change.\n\nMode deactivation therapy (MDT) was developed by Jack A. Apsche who recognized shortcomings of cognitive theory and cognitive-behavioral therapies, especially for the treatment of populations with complex psychological problems. Cognitive Behavioral Therapy (CBT) was primarily conceptualized through an integration of behavior therapy with cognitive psychology that were formulated by Aaron T. Beck. As such, the CBT approaches focus primarily on the present rather than the past, behavioral change as the main goal, and current processes that are maintaining the problem rather than the root causes. Traditionally CBT views problem manifestation as brought about by dysfunctional thinking, which is disputed as irrational beliefs and replaced with the use of logical arguments.\n\nEventually some practitioners realized that dysfunctional cognitions should not be disputed. As a result, a new wave of cognitive-behavioral therapies began to form, which was termed the \"third wave\" by Prof. Steven C. Hayes, who went on to develop Relational frame theory and Acceptance and commitment therapy. (Behaviour therapy was the first wave and Cognitive therapy was the second.) Dr. Jack A. Apsche agreed in general with this principle, but also believed that there is value in exploring the origins of maladaptive thought processes in addition to validating their existence as reasonable given an individual's past experiences upon which his or her core beliefs are based.\n\nAaron T. Beck asserted that how people feel and behave are largely determined by their thought processes or cognitions, which may make us vulnerable to psychological distress. These vulnerabilities are related to personality structures—a person's fundamental beliefs about themselves and the world around them. Personality structures largely develop as a result of responding to environmental stimuli and experiences. When these are distressing and deprive a person of psychological needs, the coping mechanism may be viewed as maladaptive compared with normal circumstances. The personality structures are referred to as cognitive schemas, which—in combinations—inform a person how to behave in a certain situation. Cognitive schemas are often automatically activated and group together to form cognitive modes that are deep-seated and durable behavioral manifestations such as depression and aggression.\n\nIn MDT these modes and their associated core beliefs are validated and normalized in the client's perspective by cultivating awareness and acceptance rather than disputing any belief as irrational or \"bad\". The proposition is that awareness and acceptance improves the therapist-client bond, client cooperation, commitment and motivation, which enables an effective and durable therapeutic change process.\n\nThe application of MDT integrates the unique validation–clarification–redirection (VCR) process step with selected elements from Acceptance and commitment therapy, Dialectical behavior therapy, and mindfulness (psychology) through a systematic and collaborative case conceptualization and implementation process.\n\nThe case conceptualization forms the blueprint of the MDT planning and implementation process, and is based on a systematic assessment procedure that is aimed at identifying, clarifying, and formulating the core beliefs → fears → thoughts and feelings → behavior sequence. First, a semi-structured clinical interview is conducted to form the foundation of further psychometric testing. The client typology survey is completed by the therapist with inputs from the client, parent/guardian, family members, and other records, including arrest and medical where relevant. It includes family information, substance abuse, medical, neglect, physical and sexual abuse and offending history, educational, emotional, behavioral, physiological, and interpersonal information. The expectations of treatment and willingness to cooperate are also noted.\n\nSecond, the Strength of Fears Questionnaire is completed and scored. The 60-item 4-point Likert scale responses is scored to examine five sub-categories of fear, namely personal reactive-external, personal reactive-internal/self-concept, environmental, physical, and abuse. The test is sensitive to the detection of trauma and identifies and rates specific fear and associated situations. Life-interfering fears are also identified.\n\nThen, the client completes the Compound Core Beliefs Questionnaire (CCBQ), a 96-item 4-point Likert scale form (short version). The score primarily informs the therapist of the client's personality traits and structure, as well as potential life-threatening and treatment-interfering beliefs. Hereby, the CCBQ helps identify the client's underlying beliefs and thoughts that guide his or her behavior. Each belief is clarified and completed with examples.\n\nThe results and analyses of the client typology, Fear Assessment and Compound Core Beliefs Questionnaire (CCBQ) are used to compile the Triggers, Fears, Avoids, and Compound Core Beliefs Correlation (TFAB) and the Conglomerate of Beliefs and Behaviors (COBB) worksheets. A situational analysis associate the problem beliefs, fears, and behaviors with triggers to identify the mode activation processes that have to be deactivated. The collaborative case conceptualization process is completed with the Functional Treatment Development Form (FTDF), which informs and monitors the treatment planning and progress.\n\nThe TFAB form is used to link specific triggers with fears and core beliefs, while the COBB takes the process one step further by associating each core belief with a specific behavior. Now functional alternative beliefs, healthy alternative thoughts and compensatory strategies are identified, which is developed and reinforced through the validation–clarification–redirection (VCR) process.\n\nThe validation–clarification–redirection (VCR) of the functional alternative belief is what separates MDT from other CBT-based approaches. In validation, the therapist explores the grain of truth in the client's perceptions or beliefs and views them as reasonable responses given his or her life experiences. In clarification, the content of the client's responses is elucidated while awareness and acceptance is encouraged. In redirection, the therapist moves the client towards accepting a functional alternative belief through commitment and motivation to work towards positive alternatives that are more supportive of his or her life goals and aspirations.\n\nMindfulness is defined as a mental state achieved by focusing one’s awareness on the present moment, while calmly acknowledging and accepting one’s feelings, thoughts, and bodily sensations. It is helpful to cultivate awareness and acceptance of distressful thoughts and feelings in the present, a state that is necessary to be able to consciously affect change in one's condition. MDT utilizes this perspective to normalize the client's thoughts and feelings, while developing healthier functional alternative beliefs. Remember that problem thoughts, feelings, and behaviors are the products of dysfunctional core beliefs that are often cultivated by distressful events.\n\nMDT was specifically developed as a psychotherapy protocol for adolescents with complex problems such as conduct, mood, and mixed personality disorders that are co-existing with trauma-related and substance abuse issues, aggression. This type of psychopathology constellation is typically associated with childhood abuse and neglect.\n\nThe MDT methodology was proved effective to treat adolescent populations aged 14- to 18-years with a variety of problems. These include Conduct disorder, Oppositional defiant disorder, Substance use disorder, mixed multiple Personality disorder, Posttraumatic stress disorder (PTSD), Mood disorder, Aggression, Sexual offending, and Child abuse. In addition to this complex population, other conditions that are often considered as difficult-to-treat also had effective outcomes compared to traditional CBT approaches. These include aggressive narcissistic, antisocial, and psychopathic youth. Although the research studies to date have not included adults or adolescent females, there are no apparent reason why the MDT treatment approach would not be equally effective for these populations.\n\nMDT is also applied in a family context. In fact, involving the family in the MDT treatment process has proven to be beneficial to improve collaboration, treatment outcome, and durability of changes. MDT has been applied in outpatient and institutional settings.\n\nTo date, there has been 10 separate MDT research studies. Results of a meta-analysis suggest that there is a large effect size for both family-based and individual Mode Deactivation Therapy (MDT). There was a significant reduction of all negative behaviors from intake to post-treatment and beyond as measured by the Child Behavior Checklist (CBCL) and State-Trait Anxiety Inventory (STAXI-II). Conventional treatments for the same populations produced insignificant change. Applying MDT, the CBCL internalizing and externalizing scales declined by an average of about 35% and the STAXI total anger expression decreased by a similar margin (37%). Comparative improvements with treatment as usual were consistently around 5%.\n\nIn a review of the 2010-book, Nancy Calleja, remarked that MDT also incorporated a psychodynamic element by exploring early childhood experiences and deterministic behaviors. \"Whereas some believe that this type of theoretical eclecticism is precisely what is needed to treat complex issues, others may have difficulty finding coherence in this type of model.\" (p. 136). Furthermore, as an approach that is claimed to be evidence-based, research that support the conceptual model remain fairly limited, especially independent studies.\n\nThe current list of peer-reviewed professional publications that cover the theory and research of MDT are listed below in categorized format.\n\n\n\n\n\n\n\n"}
{"id": "2143806", "url": "https://en.wikipedia.org/wiki?curid=2143806", "title": "Multitude", "text": "Multitude\n\nMultitude is a term for a group of people who cannot be classed under any other distinct category, except for their shared fact of existence. The term has a history of use reaching back to antiquity, but took on a strictly political concept when it was first used by Machiavelli and reiterated by Spinoza. The multitude is a concept of a population that has not entered into a social contract with a sovereign political body, such that individuals retain the capacity for political self-determination. A multitude typically classified as a quantity exceeding 100. For Hobbes the multitude was a rabble that needed to enact a social contract with a monarch, thus turning them from a multitude into a people. For Machiavelli and Spinoza both, the role of the multitude vacillates between admiration and contempt. Recently the term has returned to prominence as a new model of resistance against global systems of power as described by political theorists Michael Hardt and Antonio Negri in their international best-seller \"Empire\" (2000) and expanded upon in their \"\" (2004). Other theorists recently began to use the term include political thinkers associated with Autonomist Marxism and its sequelae, including Sylvère Lotringer, Paolo Virno, and thinkers connected with the eponymous review \"Multitudes\".\n\nThe concept originates in Machiavelli’s \"Discorsi\". It is, however, with Hobbes's recasting of the concept as the war-disposed, dissolute pole of the opposition between a Multitude and a People in \"De Cive\", that Spinoza’s conceptualization seems, according to Negri, contrasted.\n\nThe multitude is used as a term and implied as a concept throughout Spinoza's work. In the \"Tractatus Theologico-Politicus\", for instance, he acknowledges that the (fear of\nthe) power (potentia) of the multitude is the limit of sovereign power (potestas): ‘Every ruler has\nmore to fear from his own citizens […] than from any foreign enemy, and it is this “fear of the\nmasses” […that is] the principal brake on the power of the sovereign or state.’ The explication of\nthis tacit concept, however, only comes in Spinoza's last and unfinished work known as the \"Political Treatise\":\nIt must next be observed, that in laying foundations it is very necessary to study the human passions: and it is not enough to have shown, what ought to be done, but it ought, above all, to be shown how it can be effected, that men, whether led by passion or reason, should yet keep the laws firm and unbroken. For if the constitution of the dominion, or the public liberty depends only on the weak assistance of the laws, not only will the citizens have no security for its maintenance […], but it will even turn to their ruin. […] And, therefore, it would be far better for the subjects to transfer their rights absolutely to one man, than to bargain for unascertained and empty, that is unmeaning, terms of liberty, and so prepare for their posterity a way to the most cruel servitude. But if I succeed in showing that the foundation of monarchical dominion […], are firm and cannot be plucked up, without the indignation of the larger part of an armed multitude, and that from them follow peace and security for king and multitude, and if I deduce this from general human nature, no one will be able to doubt, that these foundations are the best and the true ones.\n\nThe concept of the multitude resolves the tension that scholars have observed in Spinoza’s\npolitical project between the insistence on the benign function of sovereignty (as witnessed in the\nquotation above) and the insistence on individual freedom. It is, we see here, a truly\nrevolutionary concept, and it is not difficult to see why Spinoza’s contemporaries (and, as for instance Étienne Balibar has implied, even Spinoza himself ) saw it as a dangerous political idea.\n\nNegri describes the multitude in his \"The Savage Anomaly\" as an unmediated, revolutionary, immanent, and positive collective social subject which can found a ‘nonmystified’ form of democracy (p. 194). In his more recent writings with Michael Hardt, however, he does not so much offer a direct definition, but presents the concept through a series of mediations. In \"Empire\" it is mediated by the concept of Empire (the new global constitution that Negri and Hardt describe as a copy of Polybius's description of Roman government):\n\nNew figures of struggle and new subjectivities are produced in the conjecture of events, in the universal nomadism […] They are not posed merely against the imperial system—they are not simply negative forces. They also express, nourish, and develop positively their own constituent projects. […] This constituent aspect of the movement of the multitude, in its myriad faces, is really the positive terrain of the historical construction of Empire, […] an antagonistic and creative positivity. The deterritorializing power of the multitude is the productive force that sustains Empire and at the same time the force that calls for and makes necessary its destruction.\n\nThey remain however vague as to this 'positive' or 'constituent' aspect of the Multitude:\n\nCertainly, there must be a moment when reappropriation [of wealth from capital] and selforganization [of the multitude] reach a threshold and configure a real event. This is when the political is really affirmed—when the genesis is complete and self-valorization, the cooperative convergence of subjects, and the proletarian management of production become a constituent power. […] We do not have any models to offer for this event. Only the multitude through its practical experimentation will offer the models and determine when and how the possible becomes real.\n\nIn their sequel \"\" they still refrain from a clear definition of the concept but approach the concept through mediation of a host of ‘contemporary’ phenomena, most importantly the new type of postmodern war they postulate and the history of post-WWII resistance movements. It remains a rather vague concept which is assigned a revolutionary potential without much theoretical substantiation apart from a generic potential of love.\n\nSylvère Lotringer has criticized Negri and Hardt's use of the concept for its ostensible return to the dialectical dualism in the introduction to Paulo Virno's \"A Grammar of the Multitude\" (see external links).\n\n\n"}
{"id": "66012", "url": "https://en.wikipedia.org/wiki?curid=66012", "title": "Newcomb's paradox", "text": "Newcomb's paradox\n\nIn philosophy and mathematics, Newcomb's paradox, also referred to as Newcomb's problem, is a thought experiment involving a game between two players, one of whom purports to be able to predict the future. Whether the problem actually is a paradox is disputed.\n\nNewcomb's paradox was created by William Newcomb of the University of California's Lawrence Livermore Laboratory. However, it was first analyzed and was published in a philosophy paper spread to the philosophical community by Robert Nozick in 1969, and appeared in the March 1973 issue of Scientific American, in Martin Gardner's \"Mathematical Games.\" Today it is a much debated problem in the philosophical branch of decision theory.\n\nThere is a predictor, a player, and two boxes designated A and B. The player is given a choice between taking only box B, or taking both boxes A and B. The player knows the following:\nEven while the player knows the above, he does not know what prediction has been made prior to his making a choice.\n\nIn his 1969 article, Nozick noted that \"To almost everyone, it is perfectly clear and obvious what should be done. The difficulty is that these people seem to divide almost evenly on the problem, with large numbers thinking that the opposing half is just being silly.\" The problem continues to divide philosophers today.\n\nGame theory offers two strategies for this game that rely on different principles: the expected utility principle and the strategic dominance principle. The problem is called a \"paradox\" because two analyses that both sound intuitively logical give conflicting answers to the question of what choice maximizes the player's payout.\n\nDavid Wolpert and Gregory Benford suggest that there is no conflict between the two strategies; Newcomb's problem actually represents two different games with different probabilistic outcomes, and the conflict arises because of this imprecise definition of the game. They also note that the optimal strategy for either of the games does not depend on the infallibility of the predictor, and the questions of causality, determinism, and free will do not factor into these strategies.\n\nCausality issues arise when the predictor is posited as infallible and incapable of error; Nozick avoids this issue by positing that the predictor's predictions are \"\"almost\" certainly\" correct, thus sidestepping any issues of infallibility and causality. Nozick also stipulates that if the predictor predicts that the player will choose randomly, then box B will contain nothing. This assumes that inherently random or unpredictable events would not come into play anyway during the process of making the choice, such as free will or quantum mind processes. However, these issues can still be explored in the case of an infallible predictor. Under this condition, it seems that taking only B is the correct option. This analysis argues that we can ignore the possibilities that return $0 and $1,001,000, as they both require that the predictor has made an incorrect prediction, and the problem states that the predictor is never wrong. Thus, the choice becomes whether to take both boxes with $1,000 or to take only box B with $1,000,000—so taking only box B is always better.\n\nWilliam Lane Craig has suggested that, in a world with perfect predictors (or time machines, because a time machine could be used as a mechanism for making a prediction), retrocausality can occur. If a person truly knows the future, and that knowledge affects their actions, then events in the future will be causing effects in the past. The chooser's choice will have already \"caused\" the predictor's action. Some have concluded that if time machines or perfect predictors can exist, then there can be no free will and choosers will do whatever they're fated to do. Taken together, the paradox is a restatement of the old contention that free will and determinism are incompatible, since determinism enables the existence of perfect predictors. Put another way, this paradox can be equivalent to the grandfather paradox; the paradox presupposes a perfect predictor, implying the \"chooser\" is not free to choose, yet simultaneously presumes a choice can be debated and decided. This suggests to some that the paradox is an artifact of these contradictory assumptions.\n\nGary Drescher argues in his book \"Good and Real\" that the correct decision is to take only box B, by appealing to a situation he argues is analogous—a rational agent in a deterministic universe deciding whether or not to cross a potentially busy street.\n\nAndrew Irvine argues that the problem is structurally isomorphic to Braess' paradox, a non-intuitive but ultimately non-paradoxical result concerning equilibrium points in physical systems of various kinds.\n\nSimon Burgess has argued that we need to recognize two stages to the problem. The first stage is that before which the predictor has gained all the information on which the prediction will be based. If, for example, we suppose that the prediction is at least partially based on a brain scan of the player then the first stage will not be over at least until that brain scan has been taken. An important point to appreciate is that while the player is still in that first stage, they will presumably be able to influence the predictor's prediction (e.g., by committing to taking only one box). The second stage commences after the completion of the brain scan (and/or after the gathering of any other information on which the prediction is based). As Burgess points out, the first stage is the one in which all of us currently find ourselves. Moreover, there is a clear sense in which the first stage is more significant than the second because it is then that the player can determine whether the $1,000,000 is in box B. Once they get to the second stage, the best that can be done is to determine whether to get the $1,000 in box A.\n\nThose persuaded by Burgess's approach do not say either that it is simply rational to take just box B or that it is conversely rational to take both boxes, but\nrather argue a player should make their decision while in the first stage and that that decision should be to commit to only box B. Once in the second stage, the rational decision would be to take both boxes, although by that stage the player should already have made up their mind to take just box B. Burgess has repeatedly emphasized that he is not arguing that the player should change their mind on getting to the second stage. The safe and rational strategy to adopt is to simply make a commitment to just box B while in the first stage and to have no intention of wavering from that commitment, i.e., make an 'unqualified resolution'. Burgess points out that those who make no such commitment and therefore miss out on the $1,000,000 have simply failed to be prepared. In a more recent paper Burgess has explained that, given his analysis, Newcomb's problem should be seen as being akin to the toxin puzzle. This is because both problems highlight the fact that one can have a reason to intend to do something without having a reason to actually do it.\n\nWith regard to causal structure, Burgess has consistently followed Ellery Eells and others in treating Newcomb's problem as a common cause problem. Contrary to David Lewis, he argues against the idea that Newcomb's problem is another version of the prisoner's dilemma. Burgess's argument on this point emphasizes the contrasting causal structures of the two problems.\n\nNewcomb's paradox can also be related to the question of machine consciousness, specifically if a perfect simulation of a person's brain will generate the consciousness of that person. Suppose we take the predictor to be a machine that arrives at its prediction by simulating the brain of the chooser when confronted with the problem of which box to choose. If that simulation generates the consciousness of the chooser, then the chooser cannot tell whether they are standing in front of the boxes in the real world or in the virtual world generated by the simulation in the past. The \"virtual\" chooser would thus tell the predictor which choice the \"real\" chooser is going to make.\n\nNewcomb's paradox is related to logical fatalism in that they both suppose absolute certainty of the future. In logical fatalism, this assumption of certainty creates circular reasoning (\"a future event is certain to happen, therefore it is certain to happen\"), while Newcomb's paradox considers whether the participants of its game are able to affect a predestined outcome.\n\nMany thought experiments similar to or based on Newcomb's problem have been discussed in the literature. For example, a quantum-theoretical version of Newcomb's problem in which box B is entangled with box A has been proposed.\n\nAnother related problem is the meta-Newcomb problem. The setup of this problem is similar to the original Newcomb problem. However, the twist here is that the predictor may elect to decide whether to fill box B after the player has made a choice, and the player does not know whether box B has already been filled. There is also another predictor: a \"meta-predictor\" who has correctly predicted \"every\" outcome in the past, and who predicts the following: \"Either you will choose both boxes, and the predictor will make its decision after you, or you will choose only box B, and the predictor will already have made its decision.\"\n\nIn this situation, a proponent of choosing both boxes is faced with the following dilemma: if the player chooses both boxes, the predictor will not yet have made its decision, and therefore a more rational choice would be for the player to choose box B only. But if the player so chooses, the predictor will already have made its decision, making it impossible for the player's decision to affect the predictor's decision.\n\n"}
{"id": "48398", "url": "https://en.wikipedia.org/wiki?curid=48398", "title": "Philosophical analysis", "text": "Philosophical analysis\n\nPhilosophical analysis (from ) is the techniques typically used by philosophers in the analytic tradition that involve \"breaking down\" (i.e. analyzing) philosophical issues. Arguably the most prominent of these techniques is the analysis of concepts (known as \"conceptual analysis\").\n\nWhile analysis is characteristic of the analytic tradition in philosophy, what is to be analyzed (the \"analysandum\") often varies. Some philosophers focus on analyzing linguistic phenomena, such as sentences, while others focus on psychological phenomena, such as sense data. However, arguably the most prominent analyses are of concepts or propositions, which is known as \"conceptual analysis\" (Foley 1996).\n\nConceptual analysis consists primarily in breaking down or analyzing concepts into their constituent parts in order to gain knowledge or a better understanding of a particular philosophical issue in which the concept is involved (Beaney 2003). For example, the problem of free will in philosophy involves various key concepts, including the concepts of freedom, moral responsibility, determinism, ability, etc. The method of conceptual analysis tends to approach such a problem by breaking down the key concepts pertaining to the problem and seeing how they interact. Thus, in the long-standing debate on whether free will is compatible with the doctrine of determinism, several philosophers have proposed analyses of the relevant concepts to argue for either compatibilism or incompatibilism.\n\nA famous author of conceptual analysis at its best is Bertrand Russell's theory of descriptions. Russell attempted to analyze propositions that involved \"definite descriptions\" (such as \"The tallest spy\"), which pick out a unique individual, and \"indefinite descriptions\" (such as \"a spy\"), which pick out a set of individuals. Take Russell's analysis of definite descriptions as an example. Superficially, definite descriptions have the standard subject-predicate form of a proposition. For example, \"The present king of France is bald\" appears to be predicating \"baldness\" of the subject \"the present king of France\". However, Russell noted that this is problematic, because there is no present king of France (France is no longer a monarchy). Normally, to decide whether a proposition of the standard subject-predicate form is true or false, one checks whether the subject is in the extension of the predicate. The proposition is then true if and only if the subject is in the extension of the predicate. The problem is that there is no present king of France, so the present king of France cannot be found on the list of bald things or non-bald things. So, it would appear that the proposition expressed by \"The present king of France is bald\" is neither true nor false. However, analyzing the relevant concepts and propositions, Russell proposed that what definite descriptions really express are not propositions of the subject-predicate form, but rather they express existentially quantified propositions. Thus, \"The present king of France\" is \"analyzed\", according to Russell's theory of descriptions, as \"There exists an individual who is currently the king of France, there is only one such individual, and that individual is bald.\" Now one can determine the truth value of the proposition. Indeed, it is false, because it is not the case that there exists a unique individual who is currently the king of France and is bald—since there is no present king of France (Bertolet 1999).\n\nWhile the method of analysis is characteristic of contemporary analytic philosophy, its status continues to be a source of great controversy even among analytic philosophers. Several current criticisms of the analytic method derive from W.V. Quine's famous rejection of the analytic–synthetic distinction. While Quine's critique is well-known, it is highly controversial.\n\nFurther, the analytic method seems to rely on some sort of definitional structure of concepts, so that one can give necessary and sufficient conditions for the application of the concept. For example, the concept \"bachelor\" is often analyzed as having the concepts \"unmarried\" and \"male\" as its components. Thus, the definition or analysis of \"bachelor\" is thought to be an unmarried male. But one might worry that these so-called necessary and sufficient conditions do not apply in every case. Wittgenstein, for instance, argues that language (e.g., the word 'bachelor') is used for various purposes and in an indefinite number of ways. Wittgenstein's famous thesis states that meaning is determined by use. This means that, in each case, the meaning of 'bachelor' is determined by its use in a context. So if it can be shown that the word means different things across different contexts of use, then cases where its meaning cannot be essentially defined as 'married bachelor' seem to constitute counterexamples to this method of analysis. This is just one example of a critique of the analytic method derived from a critique of definitions. There are several other such critiques (Margolis & Laurence 2006). This criticism is often said to have originated primarily with Wittgenstein's \"Philosophical Investigations\".\n\nA third critique of the method of analysis derives primarily from psychological critiques of intuition. A key part of the analytic method involves analyzing concepts via \"intuition tests\". Philosophers tend to motivate various conceptual analyses by appeal to their intuitions about thought experiments. (See DePaul and Ramsey (1998) for a collection of current essays on the controversy over analysis as it relates to intuition and reflective equilibrium.)\n\nIn short, some philosophers feel strongly that the analytic method (especially conceptual analysis) is essential to and defines philosophy—e.g. Jackson (1998), Chalmers (1996), and Bealer (1998). Yet, some philosophers argue that the method of analysis is problematic—e.g. Stich (1998) and Ramsey (1998). Some, however, take the middle ground and argue that while analysis is largely a fruitful method of inquiry, philosophers should not limit themselves to only using the method of analysis.\n\n\n\n"}
{"id": "831689", "url": "https://en.wikipedia.org/wiki?curid=831689", "title": "Pontryagin's maximum principle", "text": "Pontryagin's maximum principle\n\nPontryagin's maximum (or minimum) principle is used in optimal control theory to find the best possible control for taking a dynamical system from one state to another, especially in the presence of constraints for the state or input controls. It was formulated in 1956 by the Russian mathematician Lev Pontryagin and his students. It has as a special case the Euler–Lagrange equation of the calculus of variations.\n\nThe principle states, informally, that the \"control Hamiltonian\" must take an extreme value over controls in the set of all permissible controls. Whether the extreme value is maximum or minimum depends both on the problem and on the sign convention used for defining the Hamiltonian. The normal convention, which is the one used in Hamiltonian, leads to a maximum hence \"maximum principle\" but the sign convention used in this article makes the extreme value a minimum.\n\nIf formula_1 is the set of values of permissible controls then the principle states that the optimal control formula_2 must satisfy:\nwhere formula_4 is the optimal state trajectory and formula_5 is the optimal costate trajectory.\n\nThe result was first successfully applied to minimum time problems where the input control is constrained, but it can also be useful in studying state-constrained problems.\n\nSpecial conditions for the Hamiltonian can also be derived. When the final time formula_6 is fixed and the Hamiltonian does not depend explicitly on time formula_7, then:\nand if the final time is free, then:\nMore general conditions on the optimal control are given below.\n\nWhen satisfied along a trajectory, Pontryagin's minimum principle is a necessary condition for an optimum. The Hamilton–Jacobi–Bellman equation provides a necessary and sufficient condition for an optimum, but this condition must be satisfied over the whole of the state space.\n\nWhile the Hamilton-Jacobi-Bellman equation admits a straightforward extension to stochastic optimal control problems, the minimum principle does not.\n\nThe principle was first known as \"Pontryagin's maximum principle\" and its proof is historically based on maximizing the Hamiltonian. The initial application of this principle was to the maximization of the terminal speed of a rocket. However, as it was subsequently mostly used for minimization of a performance index it has here been referred to as the \"minimum principle\". Pontryagin's book solved the problem of minimizing a performance index.\n\nIn what follows we will be making use of the following notation.\n\nHere the necessary conditions are shown for minimization of a functional. Take formula_15 to be the state of the dynamical system with input formula_16, such that\nwhere formula_1 is the set of admissible controls and formula_19 is the terminal (i.e., final) time of the system. The control formula_20 must be chosen for all formula_21 to minimize the objective functional formula_22 which is defined by the application and can be abstracted as\n\nThe constraints on the system dynamics can be adjoined to the Lagrangian formula_24 by introducing time-varying Lagrange multiplier vector formula_25, whose elements are called the costates of the system. This motivates the construction of the Hamiltonian formula_26 defined for all formula_21 by:\nwhere formula_29 is the transpose of formula_25.\n\nPontryagin's minimum principle states that the optimal state trajectory formula_31, optimal control formula_2, and corresponding Lagrange multiplier vector formula_33 must minimize the Hamiltonian formula_26 so that\n\nfor all time formula_21 and for all permissible control inputs formula_20. It must also be the case that\n\nAdditionally, the costate equations\n\nmust be satisfied. If the final state formula_40 is not fixed (i.e., its differential variation is not zero), it must also be that the terminal costates are such that\n\nThese four conditions in (1)-(4) are the necessary conditions for an optimal control. Note that (4) only applies when formula_40 is free. If it is fixed, then this condition is not necessary for an optimum.\n\n\n"}
{"id": "23538", "url": "https://en.wikipedia.org/wiki?curid=23538", "title": "Probability interpretations", "text": "Probability interpretations\n\nThe word probability has been used in a variety of ways since it was first applied to the mathematical study of games of chance. Does probability measure the real, physical tendency of something to occur or is it a measure of how strongly one believes it will occur, or does it draw on both these elements? In answering such questions, mathematicians interpret the probability values of probability theory.\n\nThere are two broad categories of probability interpretations which can be called \"physical\" and \"evidential\" probabilities. Physical probabilities, which are also called objective or frequency probabilities, are associated with random physical systems such as roulette wheels, rolling dice and radioactive atoms. In such systems, a given type of event (such as a die yielding a six) tends to occur at a persistent rate, or \"relative frequency\", in a long run of trials. Physical probabilities either explain, or are invoked to explain, these stable frequencies. The two main kinds of theory of physical probability are frequentist accounts (such as those of Venn, Reichenbach and von Mises) and propensity accounts (such as those of Popper, Miller, Giere and Fetzer).\n\nEvidential probability, also called Bayesian probability, can be assigned to any statement whatsoever, even when no random process is involved, as a way to represent its subjective plausibility, or the degree to which the statement is supported by the available evidence. On most accounts, evidential probabilities are considered to be degrees of belief, defined in terms of dispositions to gamble at certain odds. The four main evidential interpretations are the classical (e.g. Laplace's) interpretation, the subjective interpretation (de Finetti and Savage), the epistemic or inductive interpretation (Ramsey, Cox) and the logical interpretation (Keynes and Carnap). There are also evidential interpretations of probability covering groups, which are often labelled as 'intersubjective' (proposed by Gillies and Rowbottom).\n\nSome interpretations of probability are associated with approaches to statistical inference, including theories of estimation and hypothesis testing. The physical interpretation, for example, is taken by followers of \"frequentist\" statistical methods, such as Ronald Fisher, Jerzy Neyman and Egon Pearson. Statisticians of the opposing Bayesian school typically accept the existence and importance of physical probabilities, but also consider the calculation of evidential probabilities to be both valid and necessary in statistics. This article, however, focuses on the interpretations of probability rather than theories of statistical inference.\n\nThe terminology of this topic is rather confusing, in part because probabilities are studied within a variety of academic fields. The word \"frequentist\" is especially tricky. To philosophers it refers to a particular theory of physical probability, one that has more or less been abandoned. To scientists, on the other hand, \"frequentist probability\" is just another name for physical (or objective) probability. Those who promote Bayesian inference view \"frequentist statistics\" as an approach to statistical inference that recognises only physical probabilities. Also the word \"objective\", as applied to probability, sometimes means exactly what \"physical\" means here, but is also used of evidential probabilities that are fixed by rational constraints, such as logical and epistemic probabilities.\n\nThe philosophy of probability presents problems chiefly in matters of epistemology and the uneasy interface between mathematical concepts and ordinary language as it is used by non-mathematicians.\nProbability theory is an established field of study in mathematics. It has its origins in correspondence discussing the mathematics of games of chance between Blaise Pascal and Pierre de Fermat in the seventeenth century, and was formalized and rendered axiomatic as a distinct branch of mathematics by Andrey Kolmogorov in the twentieth century. In axiomatic form, mathematical statements about probability theory carry the same sort of epistemological confidence within the philosophy of mathematics as are shared by other mathematical statements.\n\nThe mathematical analysis originated in observations of the behaviour of game equipment such as playing cards and dice, which are designed specifically to introduce random and equalized elements; in mathematical terms, they are subjects of indifference. This is not the only way probabilistic statements are used in ordinary human language: when people say that \"it will probably rain\", they typically do not mean that the outcome of rain versus not-rain is a random factor that the odds currently favor; instead, such statements are perhaps better understood as qualifying their expectation of rain with a degree of confidence. Likewise, when it is written that \"the most probable explanation\" of the name of Ludlow, Massachusetts \"is that it was named after Roger Ludlow\", what is meant here is not that Roger Ludlow is favored by a random factor, but rather that this is the most plausible explanation of the evidence, which admits other, less likely explanations.\n\nThomas Bayes attempted to provide a logic that could handle varying degrees of confidence; as such, Bayesian probability is an attempt to recast the representation of probabilistic statements as an expression of the degree of confidence by which the beliefs they express are held.\n\nThough probability initially had somewhat mundane motivations, its modern influence and use is widespread ranging from evidence-based medicine, through Six sigma, all the way to the Probabilistically checkable proof and the String theory landscape.\n\nThe first attempt at mathematical rigour in the field of probability, championed by Pierre-Simon Laplace, is now known as the classical definition. Developed from studies of games of chance (such as rolling dice) it states that probability is shared equally between all the possible outcomes, provided these outcomes can be deemed equally likely. (3.1)\n\nThis can be represented mathematically as follows:\nIf a random experiment can result in N mutually exclusive and equally likely outcomes and if N of these outcomes result in the occurrence of the event A, the probability of A is defined by\n\nThere are two clear limitations to the classical definition. Firstly, it is applicable only to situations in which there is only a 'finite' number of possible outcomes. But some important random experiments, such as tossing a coin until it rises heads, give rise to an infinite set of outcomes. And secondly, you need to determine in advance that all the possible outcomes are equally likely without relying on the notion of probability to avoid circularity—for instance, by symmetry considerations.\n\nFrequentists posit that the probability of an event is its relative frequency over time, (3.4) i.e., its relative frequency of occurrence after repeating a process a large number of times under similar conditions. This is also known as aleatory probability. The events are assumed to be governed by some random physical phenomena, which are either phenomena that are predictable, in principle, with sufficient information (see determinism); or phenomena which are essentially unpredictable. Examples of the first kind include tossing dice or spinning a roulette wheel; an example of the second kind is radioactive decay. In the case of tossing a fair coin, frequentists say that the probability of getting a heads is 1/2, not because there are two equally likely outcomes but because repeated series of large numbers of trials demonstrate that the empirical frequency converges to the limit 1/2 as the number of trials goes to infinity.\n\nIf we denote by formula_2 the number of occurrences of an event formula_3 in formula_4 trials, then if formula_5 we say that \"formula_6.\n\nThe frequentist view has its own problems. It is of course impossible to actually perform an infinity of repetitions of a random experiment to determine the probability of an event. But if only a finite number of repetitions of the process are performed, different relative frequencies will appear in different series of trials. If these relative frequencies are to define the probability, the probability will be slightly different every time it is measured. But the real probability should be the same every time. If we acknowledge the fact that we only can measure a probability with some error of measurement attached, we still get into problems as the error of measurement can only be expressed as a probability, the very concept we are trying to define. This renders even the frequency definition circular; see for example “What is the Chance of an Earthquake?” \n\nSubjectivists, also known as Bayesians or followers of epistemic probability, give the notion of probability a subjective status by regarding it as a measure of the 'degree of belief' of the individual assessing the uncertainty of a particular situation. Epistemic or subjective probability is sometimes called credence, as opposed to the term chance for a propensity probability.\n\nSome examples of epistemic probability are to assign a probability to the proposition that a proposed law of physics is true, and to determine how probable it is that a suspect committed a crime, based on the evidence presented.\n\nGambling odds don't reflect the bookies' belief in a likely winner, so much as the other bettors' belief, because the bettors are actually betting against one another. The odds are set based on how many people have bet on a possible winner, so that even if the high odds players always win, the bookies will always make their percentages anyway.\n\nThe use of Bayesian probability raises the philosophical debate as to whether it can contribute valid justifications of belief.\n\nBayesians point to the work of Ramsey (p 182) and de Finetti (p 103) as proving that subjective beliefs must follow the laws of probability if they are to be coherent. Evidence casts doubt that humans will have coherent beliefs.\n\nThe use of Bayesian probability involves specifying a prior probability. This may be obtained from consideration of whether the required prior probability is greater or lesser than a reference probability associated with an urn model or a thought experiment. The issue is that for a given problem, multiple thought experiments could apply, and choosing one is a matter of judgement: different people may assign different prior probabilities, known as the reference class problem.\nThe \"sunrise problem\" provides an example.\n\nPropensity theorists think of probability as a physical propensity, or disposition, or tendency of a given type of physical situation to yield an outcome of a certain kind or to yield a long run relative frequency of such an outcome. This kind of objective probability is sometimes called 'chance'.\n\nPropensities, or chances, are not relative frequencies, but purported causes of the observed stable relative frequencies. Propensities are invoked to explain why repeating a certain kind of experiment will generate given outcome types at persistent rates, which are known as propensities or chances. Frequentists are unable to take this approach, since relative frequencies do not exist for single tosses of a coin, but only for large ensembles or collectives (see \"single case possible\" in the table above). In contrast, a propensitist is able to use the law of large numbers to explain the behaviour of long-run frequencies. This law, which is a consequence of the axioms of probability, says that if (for example) a coin is tossed repeatedly many times, in such a way that its probability of landing heads is the same on each toss, and the outcomes are probabilistically independent, then the relative frequency of heads will be close to the probability of heads on each single toss. This law allows that stable long-run frequencies are a manifestation of invariant \"single-case\" probabilities. In addition to explaining the emergence of stable relative frequencies, the idea of propensity is motivated by the desire to make sense of single-case probability attributions in quantum mechanics, such as the probability of decay of a particular atom at a particular time.\n\nThe main challenge facing propensity theories is to say exactly what propensity means. (And then, of course, to show that propensity thus defined has the required properties.) At present, unfortunately, none of the well-recognised accounts of propensity comes close to meeting this challenge.\n\nA propensity theory of probability was given by Charles Sanders Peirce. A later propensity theory was proposed by philosopher Karl Popper, who had only slight acquaintance with the writings of C. S. Peirce, however. Popper noted that the outcome of a physical experiment is produced by a certain set of \"generating conditions\". When we repeat an experiment, as the saying goes, we really perform another experiment with a (more or less) similar set of generating conditions. To say that a set of generating conditions has propensity \"p\" of producing the outcome \"E\" means that those exact conditions, if repeated indefinitely, would produce an outcome sequence in which \"E\" occurred with limiting relative frequency \"p\". For Popper then, a deterministic experiment would have propensity 0 or 1 for each outcome, since those generating conditions would have same outcome on each trial. In other words, non-trivial propensities (those that differ from 0 and 1) only exist for genuinely indeterministic experiments.\n\nA number of other philosophers, including David Miller and Donald A. Gillies, have proposed propensity theories somewhat similar to Popper's.\n\nOther propensity theorists (e.g. Ronald Giere) do not explicitly define propensities at all, but rather see propensity as defined by the theoretical role it plays in science. They argued, for example, that physical magnitudes such as electrical charge cannot be explicitly defined either, in terms of more basic things, but only in terms of what they do (such as attracting and repelling other electrical charges). In a similar way, propensity is whatever fills the various roles that physical probability plays in science.\n\nWhat roles does physical probability play in science? What are its properties? One central property of chance is that, when known, it constrains rational belief to take the same numerical value. David Lewis called this the \"Principal Principle\", (3.3 & 3.5) a term that philosophers have mostly adopted. For example, suppose you are certain that a particular biased coin has propensity 0.32 to land heads every time it is tossed. What is then the correct price for a gamble that pays $1 if the coin lands heads, and nothing otherwise? According to the Principal Principle, the fair price is 32 cents.\n\nIt is widely recognized that the term \"probability\" is sometimes used in contexts where it has nothing to do with physical randomness. Consider, for example, the claim that the extinction of the dinosaurs was probably caused by a large meteorite hitting the earth. Statements such as \"Hypothesis H is probably true\" have been interpreted to mean that the (presently available) empirical evidence (E, say) supports H to a high degree. This degree of support of H by E has been called the logical probability of H given E, or the epistemic probability of H given E, or the inductive probability of H given E.\n\nThe differences between these interpretations are rather small, and may seem inconsequential. One of the main points of disagreement lies in the relation between probability and belief. Logical probabilities are conceived (for example in Keynes' Treatise on Probability) to be objective, logical relations between propositions (or sentences), and hence not to depend in any way upon belief. They are degrees of (partial) entailment, or degrees of logical consequence, not degrees of belief. (They do, nevertheless, dictate proper degrees of belief, as is discussed below.) Frank P. Ramsey, on the other hand, was skeptical about the existence of such objective logical relations and argued that (evidential) probability is \"the logic of partial belief\". (p 157) In other words, Ramsey held that epistemic probabilities simply \"are\" degrees of rational belief, rather than being logical relations that merely \"constrain\" degrees of rational belief.\n\nAnother point of disagreement concerns the \"uniqueness\" of evidential probability, relative to a given state of knowledge. Rudolf Carnap held, for example, that logical principles always determine a unique logical probability for any statement, relative to any body of evidence. Ramsey, by contrast, thought that while degrees of belief are subject to some rational constraints (such as, but not limited to, the axioms of probability) these constraints usually do not determine a unique value. Rational people, in other words, may differ somewhat in their degrees of belief, even if they all have the same information.\n\nAn alternative account of probability emphasizes the role of \"prediction\" – predicting future observations on the basis of past observations, not on unobservable parameters. In its modern form, it is mainly in the Bayesian vein. This was the main function of probability before the 20th century,\nbut fell out of favor compared to the parametric approach, which modeled phenomena as a physical system that was observed with error, such as in celestial mechanics.\n\nThe modern predictive approach was pioneered by Bruno de Finetti, with the central idea of exchangeability – that future observations should behave like past observations. This view came to the attention of the Anglophone world with the 1974 translation of de Finetti's book, and has\nsince been propounded by such statisticians as Seymour Geisser.\n\nThe mathematics of probability can be developed on an entirely axiomatic basis that is independent of any interpretation: see the articles on probability theory and probability axioms for a detailed treatment.\n\n\n\n"}
{"id": "2827829", "url": "https://en.wikipedia.org/wiki?curid=2827829", "title": "Rossport Five", "text": "Rossport Five\n\nThe Rossport Five () are Willie Corduff, brothers Philip and Vincent McGrath, Micheál Ó Seighin and James Brendan Philbin, from Kilcommon parish, Erris, County Mayo, Ireland. \n\nJustice Finnegan, President of the High Court of the Republic of Ireland, jailed the five on 29 June 2005 by for civil contempt of court after refusing to obey a temporary court injunction forbidding them to interfere with work being undertaken by Shell on their land. The committal order was sought by Shell who intended to build a high pressure raw gas pipeline across land in Rossport to pipe gas from the offshore Corrib Gas Field. Three of the five men own land in Rossport: Vincent McGrath and Ó Seighin were brought to court along with them as they had assisted in blocking the Shell workers. About thirty others who had done the same were not charged.\n\nThere were protests all over Ireland during the period of the men's imprisonment, with filling stations of Shell, and its junior partner Statoil, being picketed and blockaded by both political activists and ordinary members of the public. The protests were driven by the Shell to Sea campaign (then TD Jerry Cowley liaised with the men in prison) which took its name following a meeting with Burren campaigners in January 2005 Shell to Sea. Defending his company's stance, Shell Ireland's CEO Andy Pyle said: \"The fact is that we've gone through a process, and we have five people who don't like the outcome.\" All Shell sites around Rossport and Bellinaboy were blockaded by the men's neighbours, preventing work. Local TD Michael Ring said that Ireland was now a \"dictatorship within a democracy\". In an unusual move by the Irish judiciary, the men were told that a judge would be on hand at any time of day or night if they wanted to purge their contempt, by promising they would no longer hinder Shell employees.\n\nThe men were released from Cloverhill Prison on 30 September 2005, after 94 days, when Shell applied to the High Court to have the injunction lifted. This came after intense media and political scrutiny of the case.\n\nThe five men and their supporters have continued to campaign on the issue. In September 2006, a TNS/MRBI poll of adults in Mayo, commissioned by RTÉ, showed 66% supported the stance taken by the five men in their defiance of the court order, 20% did not.\n\nIn December 2006,an account of events leading up their imprisonment was published by the Rossport Five. There was a second printing of the book (Our Story,The Rossport 5)in January 2007.\n\nIn April 2007, Willie Corduff won the Goldman Environmental Prize on behalf of Europe. No government representative was present, though representatives were present from other countries for their winners.\n\nIn April 2008, Vincent McGrath, Mícheál Ó Seighin, his wife Caitlín and son-in-law John Monaghan, among a few others, split from Shell to Sea and set upPobal Chill Chomáin (PCC), a parochial grouping concerned primarily with health and safety issues. PCC have not held a public meeting in the parish or elsewhere since early 2011 and are, to all intents and purposes, defunct.\n\n"}
{"id": "26691", "url": "https://en.wikipedia.org/wiki?curid=26691", "title": "Set (mathematics)", "text": "Set (mathematics)\n\nIn mathematics, a set is a collection of distinct objects, considered as an object in its own right. For example, the numbers 2, 4, and 6 are distinct objects when considered separately, but when they are considered collectively they form a single set of size three, written {2,4,6}. The concept of a set is one of the most fundamental in mathematics. Developed at the end of the 19th century, set theory is now a ubiquitous part of mathematics, and can be used as a foundation from which nearly all of mathematics can be derived. In mathematics education, elementary topics from set theory such as Venn diagrams are taught at a young age, while more advanced concepts are taught as part of a university degree.\n\nThe German word \"Menge\", rendered as \"set\" in English, was coined by Bernard Bolzano in his work \"The Paradoxes of the Infinite\".\n\nA set is a well-defined collection of distinct objects. The objects that make up a set (also known as the set's \"elements\" or \"members\") can be anything: numbers, people, letters of the alphabet, other sets, and so on. Georg Cantor, one of the founders of set theory, gave the following definition of a set at the beginning of his \"Beiträge zur Begründung der transfiniten Mengenlehre\":\n\nSets are conventionally denoted with capital letters. Sets \"A\" and \"B\" are equal if and only if they have precisely the same elements.\n\nFor technical reasons, Cantor's definition turned out to be inadequate; today, in contexts where more rigor is required, one can use axiomatic set theory, in which the notion of a \"set\" is taken as a primitive notion and the properties of sets are defined by a collection of axioms. The most basic properties are that a set can have elements, and that two sets are equal (one and the same) if and only if every element of each set is an element of the other; this property is called the \"extensionality of sets\".\n\nThere are two ways of describing, or specifying the members of, a set. One way is by intensional definition, using a rule or semantic description:\n\nThe second way is by extension – that is, listing each member of the set. An extensional definition is denoted by enclosing the list of members in curly brackets:\n\nOne often has the choice of specifying a set either intensionally or extensionally. In the examples above, for instance, \"A\" = \"C\" and \"B\" = \"D\".\n\nIn an extensional definition, a set member can be listed two or more times, for example, {11, 6, 6}. However, per extensionality, two definitions of sets which differ only in that one of the definitions lists members multiple times define the same set. Hence, the set {11, 6, 6} is identical to the set {11, 6}. Moreover, the order in which the elements of a set are listed is irrelevant (unlike for a sequence or tuple). We can illustrate these two important points with an example:\n\nFor sets with many elements, the enumeration of members can be abbreviated. For instance, the set of the first thousand positive integers may be specified extensionally as\n\nwhere the ellipsis (\"...\") indicates that the list continues in the obvious way.\n\nThe notation with braces may also be used in an intensional specification of a set. In this usage, the braces have the meaning \"the set of all ...\". So, \"E\" = {playing card suits} is the set whose four members are A more general form of this is set-builder notation, through which, for instance, the set \"F\" of the twenty smallest integers that are four less than perfect square can be denoted\n\nIn this notation, the colon (\":\") means \"such that\", and the description can be interpreted as \"\"F\" is the set of all numbers of the form \"n\" − 4, such that \"n\" is a whole number in the range from 0 to 19 inclusive.\" Sometimes the vertical bar (\"|\") is used instead of the colon.\n\nIf \"B\" is a set and \"x\" is one of the objects of \"B\", this is denoted \"x\" ∈ \"B\", and is read as \"x belongs to B\", or \"x is an element of B\". If \"y\" is not a member of \"B\" then this is written as \"y\" ∉ \"B\", and is read as \"y does not belong to B\".\n\nFor example, with respect to the sets \"A\" = {1,2,3,4}, \"B\" = {blue, white, red}, and \"F\" = {\"n\" − 4 : \"n\" is an integer; and 0 ≤ \"n\" ≤ 19} defined above,\n\nIf every member of set \"A\" is also a member of set \"B\", then \"A\" is said to be a \"subset\" of \"B\", written \"A\" ⊆ \"B\" (also pronounced \"A is contained in B\"). Equivalently, we can write \"B\" ⊇ \"A\", read as \"B is a superset of A\", \"B includes A\", or \"B contains A\". The relationship between sets established by ⊆ is called \"inclusion\" or \"containment\".\n\nIf \"A\" is a subset of, but not equal to, \"B\", then \"A\" is called a \"proper subset\" of \"B\", written \"A\" ⊊ \"B\" (\"A is a proper subset of B\") or \"B\" ⊋ \"A\" (\"B is a proper superset of A\").\n\nThe expressions \"A\" ⊂ \"B\" and \"B\" ⊃ \"A\" are used differently by different authors; some authors use them to mean the same as \"A\" ⊆ \"B\" (respectively \"B\" ⊇ \"A\"), whereas others use them to mean the same as \"A\" ⊊ \"B\" (respectively \"B\" ⊋ \"A\").\n\nExamples:\n\nThe empty set is a subset of every set and every set is a subset of itself:\n\nEvery set is a subset of the universal set:\n\nAn obvious but useful identity, which can often be used to show that two seemingly different sets are equal:\n\nA partition of a set \"S\" is a set of nonempty subsets of \"S\" such that every element \"x\" in \"S\" is in exactly one of these subsets.\n\nThe power set of a set \"S\" is the set of all subsets of \"S\". The power set contains \"S\" itself and the empty set because these are both subsets of \"S\". For example, the power set of the set {1, 2, 3} is \n"}
{"id": "561681", "url": "https://en.wikipedia.org/wiki?curid=561681", "title": "Simran", "text": "Simran\n\nSimran (, ) is a Punjabi word derived from the Sanskrit word (\"smaraṇa\", \"the act of remembrance, reminiscence, and recollection\") which leads to the realization of what may be the highest aspect and purpose in one's life. It is the continuous remembrance of the finest aspect of the self, and/or the continuous remembrance (or feeling) of God, thus used for introducing spirituality. This state is maintained continuously while carrying out the worldly works outside.\n\n\"Simran\" is a commonly used term as a verb in Gurmukhi, which refers to 'meditating' of the Nām, the ultimate feeling of Ultimate. Sikhism is a distinct contemporary faith, whereby the Realization of God can be realized purely through individual devotion, without subjection to rites and rituals that have recently become the business of avaricious priests and that would impede the pure search for truth in Sikhism.\n\nIt says in the Guru Granth Sahib that through Simran one is purified and attains salvation or 'mukti'. This is because 'si-mar' means 'to die over', thus indicating to death of ego, allowing truth ultimate truth or sat to appear.\n\nOn page 202 of the Guru Granth Sahib, Guruji writes:\n\nThis hymn teaches a person who wishes to gain from this human life, one must attain a higher spiritual state by become free of attachment by realizing that all that is, is empty. Thereby, merit is acquired by devoutly repeating, comprehending and living by the sacred word every day so as to progressively reveal the divine and ultimate truth to the person who earnestly seeks it:\n\nGuru Ram Das says in \"Sarang ki var\" (Guru Granth Sahib, 1242):\n\nIn Sant Mat the word Simran is used for the spiritual practice of repeating the mantra given by the Satguru during initiation. The mantra itself is also called Simran. Simran repetition is done during meditation and also outside it, however this mantra is later dropped in favor of real feeling of self or the God, which happens due to breaking out of monotony through Jap. Thus mantra is used only till the point, monotony and previously formed patterns are broken. After it pure Simran is carried by the sadhak.\n\n"}
{"id": "38209210", "url": "https://en.wikipedia.org/wiki?curid=38209210", "title": "Solar bus", "text": "Solar bus\n\nA solar bus or solar-charged bus is a bus which is powered exclusively or mainly by solar energy. A solar-powered bus service is referred to as a solar bus service. The use of the term \"solar bus\" normally implies that solar energy is used not only for powering electric equipment on the bus, but also for the propulsion of the vehicle.\n\nExisting solar buses are battery electric buses or (in the case of hybrid solar buses) hybrid buses equipped with batteries which are recharged from solar (or other) power sources; a launch of a solar bus service often goes hand in hand with investments for large-scale installations of stationary solar panels with photovoltaic cells. Similarly as other solar vehicles, many solar buses have photovoltaic cells contained in solar panels on the vehicle's roof which convert the sun's energy directly into electric energy to be used by the engine.\n\nThe introduction of solar buses and other green vehicles for purposes of public transport forms a part of sustainable transport schemes.\n\nThe distinction between a solar-only electric bus and an electro-solar bus is fluid, as the distinction depends on the actual usage: whether the bus is recharged from solar or other power sources.\n\nSolar-only bus services involve recharging the bus from solar energy, usually from solar panel-covered bus station canopies. The concept is similar to that of solar parking plot for cars and bicycles, where vehicles can re-charge while parked. The need for recharging poses constraints on the run and standstill times of the bus. The implementation of a solar bus service benefits from an optimization of over-all requirements for the specific bus service.\n\nElectro-solar buses are powered additionally from electric power transmitted from power plants; hybrid solar buses may be equipped with hybrid engines.\n\nOpen air low-speed electric shuttle sightseeing buses equipped with a solar panel-covered roof are produced in series and are commercially available. According to the producers, the solar panels save energy and prolong the battery life cycle.\n\nThe Tindo solar battery-charged bus (\"Tindo\", Kaurna word for \"sun\") is an experimental battery electric vehicle that operates in Adelaide, Australia. It is the world's first solar bus, operating since 2007. It uses 100% solar power, is equipped with a regenerative braking system and air conditioning and can carry up to 40 persons, 25 of whom are seated. The bus itself is not equipped with solar panels. It receives electric power from a photovoltaic system on Adelaide's central bus station. Hailed as the world's first bus service powered exclusively by solar power, the bus service connects Adelaide City and North Adelaide as part of Adelaide City's sustainable transport agenda. The Tindo is part of the 98A and 98C bus service (until recently known as the Adelaide Connector) which is offered as free public transport. \n\nWithin the Chinese government's program for clean transport sector, China's first solar hybrid buses were put in operation in July 2012 in the city of Qiqihar. Its engine is powered by lithium-ion batteries which are fed by solar panels installed on the bus roof. It is claimed that each bus consumes 0.6 to 0.7 kilowatt-hours of electricity per kilometer and can transport up to 100 persons, and that the use of solar panels prolongs the batteries' lifetime by 35 percent.\n\nAustria's first solar-powered bus was put in operation in the village of Perchtoldsdorf. Its powertrain, operating strategy, and design specification were specifically optimized in view of its planned regular service routes. It has been in trial operation since autumn 2011.\n\nThe tribrid bus is a hybrid electric bus developed by the University of Glamorgan, Wales, for use as student transport between the University’s different campuses. It is powered by hydrogen fuel or solar cells, batteries and ultracapacitors.\n\nThe first Solar Bus in the UK launched in Brighton April 2017. Following a marathon six week effort from hundreds of local people, The Big Lemon and Brighton Energy Coop's joint Solar Bus project has won funding from the M&S Community Energy Fund to cover the roof of The Big Lemon’s bus depot in solar panels to power the new electric buses on clean green renewable energy. The bus was named by one of the Solar Roof partners, Viper IT Solutions and is called \"Om Shanti\".\n\nThe 120 solar panels will generate 30,000kW of electricity – the equivalent of 1.8 million boiled kettles. With no emissions, the Solar Buses will reduce noxious gases in some of Brighton and Hove’s most polluted areas and will power the 52 route between Woodingdean and Brighton on 100% renewable energy.\n\nThe Solar Bus project was one of 199 different applications to the scheme, 125 of which were shortlisted. These were put to public vote for six weeks during September and October and the voting process also included the option to donate to the project via the Crowdfunding platform.\n\nThe Solar Bus project was one of 19 regional winners, with 1549 votes, 170 pledges, and a total of £13,325 raised through crowdfunding, almost half the total amount of £28,798 raised through the scheme nationally. The project will benefit from £12,500 funding from M&S Energy which, together with the £13,325 crowdfunding donations will fund the solar array on the roof of the bus depot.\n\nThe Kayoola Solar Bus is a 35-seater electric solar bus with zero tailpipe emissions, a range of 80 km, with latent range extension from the real-time charging enabled by the roof mounted solar panels. The development of the Kayoola Solar Bus Concept represents the commitment of the Kiira Motors Project to championing the progressive development of local capacity for Vehicle Technology Innovation, a key ingredient for institutionalizing a sustainable Vehicle Manufacturing Industry in Uganda. \n\nThe Solar Buzz is a 14-seater US Electricar bus, made in 1994, that has been repurposed as the world's first truly solar bus in Truth or Consequences, New Mexico in 2011. The Buzz has 2 KW in homemade solar panels on the roof, 40 golf cart batteries, 2 electric motors, and has no tailpipe. The power required to go one mile is the same as the power required to make a pot of coffee: around 700 Wh. The Solar Buzz is an IntraCity bus approved by the state Public Regulation Commission (#56817) in 2015 and is a commercial daytime private shuttle service with a US$3 Fare, operated by Hot Springs Transit, LLC. Hot Springs Transit provides transit service to the 6100 person population of Truth or Consequences.\n\nSolar panels are also used for powering electronic devices of the bus such as heating and air conditioning, even in buses with non-solar-powered engine. In the US, such buses are advertised to meet anti-idling regulations in several states.\n\nRefitting existing non-existing vehicles with photovoltaic panels that feed the original battery with additional electric power has been shown to have the potential for making a substantial contribution to CO2 emission mitigation and to the reduction of pollution. The thus transformed buses are however not solar buses in the strict sense of the word, as they do not use solar energy for propulsion. The use of buses in public transport implies frequent stops with the opening and closing of doors, which influence the way the energy of the battery is used.\n\nIn principle also trolleybuses or other non-autonomous electric buses or alternately powered buses such as fuel cell buses or dual-mode buses could be used for solar bus services, provided the origin of all or most of the energy used for propulsing the bus would be solar energy. In practice however such systems also draw on other sources of energy, at least also other renewable energy sources such as wind energy. An example is that the city of Hamburg, Germany, received the 2011 European Green Capital Award for, among others, its fuel cell bus service that is claimed to be the world's largest hydrogen-powered bus fleet and is intended to use hydrogen generated from solar and wind energy.\n\n"}
{"id": "1913978", "url": "https://en.wikipedia.org/wiki?curid=1913978", "title": "Solow–Swan model", "text": "Solow–Swan model\n\nThe Solow–Swan model is an economic model of long-run economic growth set within the framework of neoclassical economics. It attempts to explain long-run economic growth by looking at capital accumulation, labor or population growth, and increases in productivity, commonly referred to as technological progress. At its core is a neoclassical (aggregate) production function, often specified to be of Cobb–Douglas type, which enables the model \"to make contact with microeconomics\". The model was developed independently by Robert Solow and Trevor Swan in 1956, and superseded the Keynesian Harrod–Domar model.\n\nMathematically, the Solow–Swan model is a nonlinear system consisting of a single ordinary differential equation that models the evolution of the \"per capita\" stock of capital. Due to its particularly attractive mathematical characteristics, Solow–Swan proved to be a convenient starting point for various extensions. For instance, in 1965, David Cass and Tjalling Koopmans integrated Frank Ramsey's analysis of consumer optimization, thereby endogenizing the saving rate, to create what is now known as the Ramsey–Cass–Koopmans model.\n\nThe neo-classical model was an extension to the 1946 Harrod–Domar model that included a new term: productivity growth. Important contributions to the model came from the work done by Solow and by Swan in 1956, who independently developed relatively simple growth models. Solow's model fitted available data on US economic growth with some success. In 1987 Solow was awarded the Nobel Prize in Economics for his work. Today, economists use Solow's sources-of-growth accounting to estimate the separate effects on economic growth of technological change, capital, and labor.\n\nSolow extended the Harrod–Domar model by adding labor as a factor of production and capital-output ratios that are not fixed as they are in the Harrod–Domar model. These refinements allow increasing capital intensity to be distinguished from technological progress. Solow sees the fixed proportions production function as a \"crucial assumption\" to the instability results in the Harrod-Domar model. His own work expands upon this by exploring the implications of alternative specifications, namely the Cobb-Douglass and the more general Constant Elasticity of Substitution. Although this has become the canonical and celebrated story in the history of economics, featured in many economic textbooks, recent reappraisal of Harrod's work has contested it. One central criticism is that Harrod's original piece was neither mainly concerned with economic growth nor did he explicitly use a fixed proportions production function.\n\nThe standard Solow model predicts that in the long run, economies converge to their steady state equilibrium and that permanent growth is achievable only through technological progress. Both shifts in saving and in populational growth cause only level effects in the long-run (i.e. in the absolute value of real income per capita).\n\nAn interesting implication of Solow's model is that poor countries should grow faster and eventually catch-up to richer countries. This convergence could be explained by:\nBaumol attempted to verify this empirically and found a very strong correlation between a countries' output growth over a long period of time (1870 to 1979) and its initial wealth. His findings were later contested by DeLong who claimed that both the non-randomness of the sampled countries, and potential for significant measurement errors for estimates of real income per capita in 1870, biased Baumol's findings. DeLong concludes that there is little evidence to support the convergence theory.\n\nThe key assumption of the neoclassical growth model is that capital is subject to diminishing returns in a closed economy.\n\n→Given a fixed stock of labor, the impact on output of the last unit of capital accumulated will always be less than the one before.\n\n→Assuming for simplicity no technological progress or labor force growth, diminishing returns implies that at some point the amount of new capital produced is only just enough to make up for the amount of existing capital lost due to depreciation. At this point, because of the assumptions of no technological progress or labor force growth, we can see the economy ceases to grow.\n\n→Assuming non-zero rates of labor growth complicate matters somewhat, but the basic logic still applies – in the short-run, the rate of growth slows as diminishing returns take effect and the economy converges to a constant \"steady-state\" rate of growth (that is, \"no\" economic growth per-capita).\n\n→Including non-zero technological progress is very similar to the assumption of non-zero workforce growth, in terms of \"effective labor\": a new steady state is reached with constant output per \"worker-hour required for a unit of output\". However, in this case, per-capita output grows at the rate of technological progress in the \"steady-state\" (that is, the rate of productivity growth).\n\nIn the Solow–Swan model the unexplained change in the growth of output after accounting for the effect of capital accumulation is called the Solow residual. This residual measures the exogenous increase in total factor productivity (TFP) during a particular time period. The increase in TFP is often attributed entirely to technological progress, but it also includes any permanent improvement in the efficiency with which factors of production are combined over time. Implicitly TFP growth includes any permanent productivity improvements that result from improved management practices in the private or public sectors of the economy. Paradoxically, even though TFP growth is exogenous in the model, it cannot be observed, so it can only be estimated in conjunction with the simultaneous estimate of the effect of capital accumulation on growth during a particular time period.\n\nThe model can be reformulated in slightly different ways using different productivity assumptions, or different measurement metrics:\n\nIn a growing economy, capital is accumulated faster than people are born, so the denominator in the growth function under the MFP calculation is growing faster than in the ALP calculation. Hence, MFP growth is almost always lower than ALP growth. (Therefore, measuring in ALP terms increases the apparent capital deepening effect.) MFP is measured by the \"Solow residual\", not ALP.\n\nThe textbook Solow–Swan model is set in continuous-time world with no government or international trade. A single good (output) is produced using two factors of production, labor (formula_1) and capital (formula_2) in an aggregate production function that satisfies the Inada conditions, which imply that the elasticity of substitution must be asymptotically equal to one.\n\nwhere formula_4 denotes time, formula_5 is the elasticity of output with respect to capital, and formula_6 represents total production. formula_7 refers to labor-augmenting technology or “knowledge”, thus formula_8 represents effective labor. All factors of production are fully employed, and initial values formula_9, formula_10, and formula_11 are given. The number of workers, i.e. labor, as well as the level of technology grow exogenously at rates formula_12 and formula_13, respectively:\n\nThe number of effective units of labor, formula_16, therefore grows at rate formula_17. Meanwhile, the stock of capital depreciates over time at a constant rate formula_18. However, only a fraction of the output (formula_19 with formula_20) is consumed, leaving a saved share formula_21 for investment:\n\nwhere formula_23 is shorthand for formula_24, the derivative with respect to time. Derivative with respect to time means that it is the change in capital stock—output that is neither consumed nor used to replace worn-out old capital goods is net investment.\n\nSince the production function formula_25 has constant returns to scale, it can be written as output per effective unit of labour:\n\nThe main interest of the model is the dynamics of capital intensity formula_27, the capital stock per unit of effective labour. Its behaviour over time is given by the key equation of the Solow–Swan model:\n\nThe first term, formula_29, is the actual investment per unit of effective labour: the fraction formula_30 of the output per unit of effective labour formula_31 that is saved and invested. The second term, formula_32, is the “break-even investment”: the amount of investment that must be invested to prevent formula_27 from falling. The equation implies that formula_34 converges to a steady-state value of formula_35, defined by formula_36, at which there is neither an increase nor a decrease of capital intensity:\n\nat which the stock of capital formula_2 and effective labour formula_8 are growing at rate formula_40. By assumption of constant returns, output formula_41 is also growing at that rate. In essence, the Solow–Swan model predicts that an economy will converge to a balanced-growth equilibrium, regardless of its starting point. In this situation, the growth of output per worker is determined solely by the rate of technological progress.\n\nSince, by definition, formula_42, at the equilibrium formula_35 we have\nTherefore, at the equilibrium, the capital/output ratio depends only on the saving, growth, and depreciation rates. This is the Solow–Swan model's version of the golden rule saving rate.\n\nSince formula_45, at any time formula_46 the marginal product of capital formula_47 in the Solow–Swan model is inversely related to the capital/labor ratio.\n\nIf productivity formula_49 is the same across countries, then countries with less capital per worker formula_50 have a higher marginal product, which would provide a higher return on capital investment. As a consequence, the model predicts that in a world of open market economies and global financial capital, investment will flow from rich countries to poor countries, until capital/worker formula_50 and income/worker formula_52 equalize across countries.\n\nSince the marginal product of physical capital is not higher in poor countries than in rich countries, the implication is that productivity is lower in poor countries. The basic Solow model cannot explain why productivity is lower in these countries. Lucas suggested that lower levels of human capital in poor countries could explain the lower productivity.\n\nIf one equates the marginal product of capital formula_53 with the rate of return formula_54 (such approximation is often used in neoclassical economics), then, for our choice of the production function\nso that formula_56 is the fraction of income appropriated by capital. Thus, Solow–Swan model assumes from the beginning that the labor-capital split of income remains constant.\n\nN. Gregory Mankiw, David Romer, and David Weil created a human capital augmented version of the Solow–Swan model that can explain the failure of international investment to flow to poor countries. In this model output and the marginal product of capital (K) are lower in poor countries because they have less human capital than rich countries.\n\nSimilar to the textbook Solow–Swan model, the production function is of Cobb–Douglas type:\nwhere formula_58 is the stock of human capital, which depreciates at the same rate formula_18 as physical capital. For simplicity, they assume the same function of accumulation for both types of capital. Like in Solow–Swan, a fraction of the outcome, formula_60, is saved each period, but in this case split up and invested partly in physical and partly in human capital, such that formula_61. Therefore, there are two fundamental dynamic equations in this model:\nThe balanced (or steady-state) equilibrium growth path is determined by formula_64, which means formula_65 and formula_66. Solving for the steady-state level of formula_27 and formula_68 yields:\nIn the steady state, formula_71.\n\nKlenow and Rodriguez-Clare cast doubt on the validity of the augmented model because Mankiw, Romer, and Weil's estimates of formula_72 did not seem consistent with accepted estimates of the effect of increases in schooling on workers' salaries. Though the estimated model explained 78% of variation in income across countries, the estimates of formula_72 implied that human capital's external effects on national income are greater than its direct effect on workers' salaries.\n\nTheodore Breton provided an insight that reconciled the large effect of human capital from schooling in the Mankiw, Romer and Weil model with the smaller effect of schooling on workers' salaries. He demonstrated that the mathematical properties of the model include significant external effects between the factors of production, because human capital and physical capital are multiplicative factors of production. The external effect of human capital on the productivity of physical capital is evident in the marginal product of physical capital:\n\nHe showed that the large estimates of the effect of human capital in cross-country estimates of the model are consistent with the smaller effect typically found on workers' salaries when the external effects of human capital on physical capital and labor are taken into account. This insight significantly strengthens the case for the Mankiw, Romer, and Weil version of the Solow–Swan model. Most analyses criticizing this model fail to account for the pecuniary external effects of both types of capital inherent in the model.\n\nThe exogenous rate of TFP (total factor productivity) growth in the Solow–Swan model is the residual after accounting for capital accumulation. The Mankiw, Romer, and Weil model provide a lower estimate of the TFP (residual) than the basic Solow–Swan model because the addition of human capital to the model enables capital accumulation to explain more of the variation in income across countries. In the basic model, the TFP residual includes the effect of human capital because human capital is not included as a factor of production.\n\nThe Solow–Swan model augmented with human capital predicts that the income levels of poor countries will tend to catch up with or converge towards the income levels of rich countries if the poor countries have similar savings rates for both physical capital and human capital as a share of output, a process known as conditional convergence. However, savings rates vary widely across countries. In particular, since considerable financing constraints exist for investment in schooling, savings rates for human capital are likely to vary as a function of cultural and ideological characteristics in each country.\n\nSince the 1950s, output/worker in rich and poor countries generally has not converged, but those poor countries that have greatly raised their savings rates have experienced the income convergence predicted by the Solow–Swan model. As an example, output/worker in Japan, a country which was once relatively poor, has converged to the level of the rich countries. Japan experienced high growth rates after it raised its savings rates in the 1950s and 1960s, and it has experienced slowing growth of output/worker since its savings rates stabilized around 1970, as predicted by the model.\n\nThe per-capita income levels of the southern states of the United States have tended to converge to the levels in the Northern states. The observed convergence in these states is also consistent with the conditional convergence concept. Whether absolute convergence between countries or regions occurs depends on whether they have similar characteristics, such as:\n\nAdditional evidence for conditional convergence comes from multivariate, cross-country regressions.\n\nIf productivity growth were associated only with high technology then the introduction of information technology should have led to a noticeable productivity acceleration over the past twenty years; but it has not: \"see\": Solow computer paradox. Instead, world productivity appears to have increased relatively steadily since the 19th century.\n\nEconometric analysis on Singapore and the other \"East Asian Tigers\" has produced the surprising result that although output per worker has been rising, almost none of their rapid growth had been due to rising per-capita productivity (they have a low \"Solow residual\").\n\n\n\n"}
{"id": "2389884", "url": "https://en.wikipedia.org/wiki?curid=2389884", "title": "Somatic experiencing", "text": "Somatic experiencing\n\nSomatic experiencing is a form of alternative therapy aimed at relieving the symptoms of post-traumatic stress disorder (PTSD) and other mental and physical trauma-related health problems by focusing on the client's perceived body sensations (or somatic experiences). It was created by trauma therapist Peter A. Levine.\n\nSessions are normally done in person, and involve a client tracking his or her own experience. Practitioners are often mental health practitioners such as social workers, psychologists, marriage and family therapists (MFTs) or psychotherapists, but may also be nurses, physicians, bodyworkers, physical therapists, chaplains, clergy, or members of other professions. Certified practitioners complete a three-year training course (216 hours of instruction) and must complete 18 hours of case consultations and 12 hours of personal sessions. Somatic Experiencing is used for shock trauma in the short term and for developmental trauma as an adjunct to psychotherapy that may span years.\n\nSomatic Experiencing attempts to promote awareness and release physical tension that remains in the aftermath of trauma.\n\nAnother element of Somatic Experiencing therapy is \"pendulation\", the movement between regulation and dysregulation. The client is helped to move to a state where he or she is dysregulated (i.e. is aroused or frozen, demonstrated by physical symptoms such as pain or numbness) and then iteratively helped to return to a state of regulation. The goal is to allow the client to resolve the physical and mental difficulties caused by the trauma, and thereby to be able to respond appropriately to everyday situations.\n\n\"Resources\" are defined as anything that helps the client's autonomic nervous system return to a regulated state. This might be the memory of someone close to them, a physical item that might ground them in the present moment, or other supportive elements that minimize distress. In the face of arousal, \"discharge\" is facilitated to allow the client's body to return to a regulated state. Discharge may be in the form of tears, a warm sensation, unconscious movement, the ability to breathe easily again, or other responses which demonstrate the autonomic nervous system returning to its baseline. The intention of this process is to reinforce the client's inherent capacity to self-regulate.\nSomatic experiencing is used for both shock trauma and developmental trauma. Shock trauma is loosely defined as a single-episode traumatic event such as a car accident, natural disaster such as an earthquake, battlefield incident, physical attack, etc. Developmental trauma refers to various kinds of psychological damage that occur during child development when a child has insufficient or detrimental attention from the primary caregivers.\n\n"}
{"id": "1763396", "url": "https://en.wikipedia.org/wiki?curid=1763396", "title": "Tensor density", "text": "Tensor density\n\nIn differential geometry, a tensor density or relative tensor is a generalization of the tensor field concept. A tensor density transforms as a tensor field when passing from one coordinate system to another (see tensor field), except that it is additionally multiplied or \"weighted\" by a power \"W\" of the Jacobian determinant of the coordinate transition function or its absolute value. A distinction is made among (authentic) tensor densities, pseudotensor densities, even tensor densities and odd tensor densities. Sometimes tensor densities with a negative weight \"W\" are called tensor capacity. A tensor density can also be regarded as a section of the tensor product of a tensor bundle with a density bundle.\n\nSome authors classify tensor densities into the two types called (authentic) tensor densities and pseudotensor densities in this article. Other authors classify them differently, into the types called even tensor densities and odd tensor densities. When a tensor density weight is an integer there is an equivalence between these approaches that depends upon whether the integer is even or odd.\n\nNote that these classifications elucidate the different ways that tensor densities may transform somewhat pathologically under orientation-\"reversing\" coordinate transformations. Regardless of their classifications into these types, there is only one way that tensor densities transform under orientation-\"preserving\" coordinate transformations.\n\nIn this article we have chosen the convention that assigns a weight of +2 to the determinant of the metric tensor expressed with covariant indices. With this choice, classical densities, like charge density, will be represented by tensor densities of weight +1. Some authors use a sign convention for weights that is the negation of that presented here.\n\nFor example, a mixed rank-two (authentic) tensor density of weight \"W\" transforms as:\n\nwhere formula_2 is the rank-two tensor density in the formula_3 coordinate system, formula_4 is the transformed tensor density in the formula_5 coordinate system; and we use the Jacobian determinant. Because the determinant can be negative, which it is for an orientation-reversing coordinate transformation, this formula is applicable only when \"W\" is an integer. (However, see even and odd tensor densities below.)\n\nWe say that a tensor density is a pseudotensor density when there is an additional sign flip under an orientation-reversing coordinate transformation. A mixed rank-two pseudotensor density of weight \"W\" transforms as\nwhere sgn( ) is a function that returns +1 when its argument is positive or −1 when its argument is negative.\n\nThe transformations for even and odd tensor densities have the benefit of being well defined even when \"W\" is not an integer. Thus one can speak of, say, an odd tensor density of weight +2 or an even tensor density of weight −1/2.\n\nWhen \"W\" is an even integer the above formula for an (authentic) tensor density can be rewritten as\n\nSimilarly, when \"W\" is an odd integer the formula for an (authentic) tensor density can be rewritten as\n\nA tensor density of any type that has weight zero is also called an absolute tensor. An (even) authentic tensor density of weight zero is also called an ordinary tensor.\n\nIf a weight is not specified but the word \"relative\" or \"density\" is used in a context where a specific weight is needed, it is usually assumed that the weight is +1.\n\n\nIf formula_9 is a non-singular matrix and a rank-two tensor density of weight \"W\" with covariant indices then its matrix inverse will be a rank-two tensor density of weight −\"W\" with contravariant indices. Similar statements apply when the two indices are contravariant or are mixed covariant and contravariant.\n\nIf formula_9 is a rank-two tensor density of weight \"W\" with covariant indices then the matrix determinant formula_11 will have weight , where \"N\" is the number of space-time dimensions. If formula_12 is a rank-two tensor density of weight \"W\" with contravariant indices then the matrix determinant formula_13 will have weight . The matrix determinant formula_14 will have weight \"NW\".\n\nAny non-singular ordinary tensor formula_15 transforms as\nwhere the right-hand side can be viewed as the product of three matrices. Taking the determinant of both sides of the equation (using that the determinant of a matrix product is the product of the determinants), dividing both sides by formula_17, and taking their square root gives\n\nWhen the tensor \"T\" is the metric tensor, formula_19, and formula_20 is a locally inertial coordinate system where formula_21diag(−1,+1,+1,+1), the Minkowski metric, then formula_22 −1 and so\nwhere formula_24 is the determinant of the metric tensor formula_25.\n\nConsequently, an even tensor density, formula_26, of weight \"W\", can be written in the form\n\nwhere formula_28 is an ordinary tensor. In a locally inertial coordinate system, where formula_29, it will be the case that formula_26 and formula_28 will be represented with the same numbers.\n\nWhen using the metric connection (Levi-Civita connection), the covariant derivative of an even tensor density is defined as\n\nFor an arbitrary connection, the covariant derivative is defined by adding an extra term, namely\n\nto the expression that would be appropriate for the covariant derivative of an ordinary tensor.\n\nEquivalently, the product rule is obeyed\nwhere, for the metric connection, the covariant derivative of any function of formula_35 is always zero,\n\nThe expression formula_37 is a scalar density. By the convention of this article it has a weight of +1. \n\nThe density of electric current formula_38 (\"e.g.\", formula_39 is the amount of electric charge crossing the 3-volume element formula_40 divided by that element — do not use the metric in this calculation) is a contravariant vector density of weight +1. It is often written as formula_41 or formula_42, where formula_43 and the differential form formula_44 are absolute tensors, and where formula_45 is the Levi-Civita symbol; see below.\n\nThe density of Lorentz force formula_46 (\"i.e.\", the linear momentum transferred from the electromagnetic field to matter within a 4-volume element formula_47 divided by that element — do not use the metric in this calculation) is a covariant vector density of weight +1. \n\nIn \"N\"-dimensional space-time, the Levi-Civita symbol may be regarded as either a rank-\"N\" covariant (odd) authentic tensor density of weight −1 (ε) or a rank-\"N\" contravariant (odd) authentic tensor density of weight +1 (ε). Notice that the Levi-Civita symbol (so regarded) does \"not\" obey the usual convention for raising or lowering of indices with the metric tensor. That is, it is true that\nbut in general relativity, where formula_49 is always negative, this is never equal to formula_50.\n\nThe determinant of the metric tensor,\nis an (even) authentic scalar density of weight +2.\n\n\n"}
{"id": "11251634", "url": "https://en.wikipedia.org/wiki?curid=11251634", "title": "The Behavior of Organisms", "text": "The Behavior of Organisms\n\nThe Behavior of Organisms is B.F. Skinner's first book and was published in May 1938 as a volume of the Century Psychology Series. It set out the parameters for the discipline that would come to be called the experimental analysis of behavior (EAB) and Behavior Analysis. This book was reviewed in 1939 by Ernest R. Hilgard. Skinner looks at science behavior and how the analysis of behavior produces data which can be studied, rather than acquiring data through a conceptual or neural process. In the book, behavior is classified either as respondent or operant behavior, where respondent behavior is caused by an observable stimulus and operant behavior is where there is no observable stimulus for a behavior. The behavior is studied in depth with rats and the feeding responses they exhibit.\n\n\n"}
{"id": "3324864", "url": "https://en.wikipedia.org/wiki?curid=3324864", "title": "The Mosquito", "text": "The Mosquito\n\nThe Mosquito or Mosquito alarm is an electronic device used to deter loitering by young people by emitting sound at high frequency, in some versions so it can be heard mostly by younger people. The devices have attracted controversy on the basis of human rights and discrimination concerns.\n\nThe device is marketed as a safety and security tool for preventing youths from congregating in specific areas. As such, it is promoted to reduce anti-social behaviour, such as loitering, vandalism, drug use, drug distribution, and violence. In the UK, over 3,000 have been sold, mainly for use outside shops and near transport hubs. The device is also sold in Australia, France, Denmark, Italy, Spain, Germany, Switzerland, Canada and the USA.\n\nCritics say that it discriminates against young people and infringes their human rights, while supporters argue that making the Mosquito illegal would infringe the rights of shopkeepers who suffer business losses when \"unruly teenagers\" drive away their customers. Mosquito distributors have said that they keep standards to ensure that the device is not abused, and Howard Stapleton who invented the device has asked European governments to legislate guidelines governing its use.\n\nThe newest version of the device, launched late in 2008, has two frequency settings, one of approximately 17.4 kHz that can generally be heard only by young people, and another at 8 kHz that can be heard by most people. The maximum potential output sound pressure level is stated by the manufacturer to be 108 decibels (dB), and the manufacturer's product specification furthermore states that the sound can typically be heard by people below 25 years of age. The ability to hear high frequencies deteriorates in most humans with age (a condition known as presbycusis), typically observable by the age of 18.\n\nThe Mosquito machine was invented by Howard Stapleton in 2005, and was originally tested in Barry, South Wales, where it was successful in reducing teenagers loitering near a grocery store. The idea was born after he was irritated by a factory noise when he was a child. The push to create the product was when Mr. Stapleton's 17-year-old daughter went to the store to buy milk and was harassed by a group of 12 to 15-year-olds. Using his children as test subjects, he determined the frequency of \"The Mosquito.\"\n\nThe Mosquito was released to the mainstream market in 2005, through Stapleton's company Compound Security Solutions. The current device has two settings: the high frequency sound targeted at youth, and another that can be heard by everyone. The range of the sound is with the sound baffle, and without. It requires a 24-volt DC or 15-volt AC power supply.\n\nA device installed in a Spar shop in Caerleon Road in Newport, South Wales was banned after three months by the Newport Community Safety Partnership, a partnership set up to meet the requirements of the Crime and Disorder Act 1998, with members including Newport City Council, Gwent Police, Newport Local Health Board, South Wales Fire Service, representatives of Customs and Excise, and the Welsh Assembly Government. Despite the ban, another Spar shop in Newport installed the device. A Newport Community Safety Partnership spokesman said: \"Any view expressed by the Partnership does not stop any business or private company from purchasing these devices. They must ensure these systems comply with the law.\"\n\nIn February 2008, in response to a national campaign launched by the Children’s Commissioner for England, Liberty, and the National Youth Agency, the government issued a statement insisting that \"'Mosquito alarms are not banned and the government has no plans to ban them\".\n\nThe sound was made into a mobile phone ringtone, which could not be heard by teachers if the phone rang during a class. Mobile phone speakers are capable of producing frequencies above 20 kHz. This ringtone became informally known as \"Teen Buzz\" or \"the Mosquito ringtone\" and has since been sold commercially.\n\nThe Mosquito won the Ig Nobel for Peace in 2006. The Ig Nobels celebrate the quirkier side of serious scientific endeavour, honouring \"achievements that first make people laugh, and then make them think\".\n\nThe German Federal Institute for Occupational Safety and Health stated in a report on The Mosquito, entitled \"Use of ultrasonic noise channels not entirely safe\":\n\nIn a United Kingdom survey of the relevant studies of adults exposed to high frequency sound in an occupational context for the Health and Safety Executive (HSE) in 2001, it was concluded that the studies were inadequate to establish guidelines for safe exposure. The Mosquito's manufacturer relies on these inconclusive adult studies to justify the safety of the device.\n\nThe National Autistic Society said in 2008 that it was \"extremely concerned\" about possible harmful effects of the devices upon people with autism. Since autism causes auditory hypersensitivity, individuals with this disability can have more intense reactions to this sound, especially if they are also under 25. Since autism can also affect communication skills, some individuals may not be able to communicate their discomfort to caregivers. A supermarket in Longridge, England, removed a mosquito device in 2008 after a campaign by a 19-year-old Paul Brookfield, who has autism. Brookfield contended that the device was causing him pain due to his disability.\n\nOther disabilities may be exacerbated by the device. In March 2009, a child who had recently undergone ear surgery reported that the device set off her tinnitus, causing significant pain. \n\nThe Mosquito has received support and endorsements from municipalities, school districts, property management companies, convenience stores and other organisations. Rochdale Safer Communities Partnership said the borough was committed to the continued use of the Mosquito:\n\n\"We feel totally justified in deploying Mosquito devices in the borough of Rochdale to give the community respite in cases of acute anti-social behaviour and youth nuisance,\" she said. \"We use the devices when there are large groups of young people making life a problem for residents and businesses, as we are very keen not to let problems of anti-social behaviour escalate.\"\n\nThe Association of Convenience Stores (ACS) also supports the usage of the device, and so does British Retail Consortium (BRC), stating that \"Not all young people are involved in violence, but given that some retail staff are facing a level of insolence [from teenagers] they have to have the option of doing what they can to protect themselves. They are entitled to discourage threatening groups from hanging around or in their shops.\"\n\nAt the Maple Ridge, Pitt Meadows and Vancouver west side school districts in British Columbia, Canada, the device has been credited with lowering exterior vandalism at one school by about 40%.\n\nOpposition categorises it as an indiscriminate sonic weapon which succeeds only in demonising children and young people and may breach their human rights. The National Youth Rights Association has met with some success fighting the devices in the United States.\n\nA UK campaign called \"Buzz off\" is calling for The Mosquito to be banned. Shami Chakrabarti, director of Liberty, has claimed that the sound is \"untested [and] unregulated\" and that it can be a \"sonic weapon directed against children and young people.\" Albert Aynsley-Green, the former Children’s Commissioner for England, criticised the devices for indiscriminately targeting all children and babies regardless of their behaviour. He described such measures as \"demonising children and young people\", and creating a \"dangerous and widening divide\" between the young and the old.\n\nThe device was singled out for criticism in a joint report by children's commissioners for all parts of the UK, which formed part of a United Nations review of standards in the UK. A report for the Council of Europe called for a ban in 2010, suggesting use of the Mosquito may breach human rights law.\n\nIn January 2011, the device was banned on all Council and Partnership buildings in Sheffield following a successful campaign led by the then Member of UK Youth Parliament for Sheffield, Harrison Carter. Sheffield is the largest city in the country with such a ban in place. It was recognised by the UK Government in their Positive For Youth document, published by the Department for Education in January 2012. This strategy paper acts to set out a new-approach to cross government policy for young people aged 13–19. Although mentioned in the document, a national ban of the mosquito device was not in the Coalition Agreement, and is not part of current Government policy.\n\nIn July 2014, a new campaign was started in order to get a mosquito device removed from the public library in Milford Haven, Pembrokeshire, and is ongoing.\n\nIn 2016, a shopping centre in Queensland, Australia removed the device after two years of campaigning by a local lawyer, due to it discriminating young people. It had been installed in the centre for 10 years.\n\nCampaigners and authorities in many countries have stated that they believe the device to breach human rights and may even constitute assault. Liberty has expressed concern that the device may violate sections 8 and 14 of the European Convention on Human Rights, and the United Kingdom's Human Rights Act 1998.\n\nThe Committee on Culture, Science and Education of the Parliamentary Assembly of the Council of Europe prepared a report stating that this device violates many articles of both the European Convention on Human Rights and the United Nations Convention on the Rights of the Child, and should be banned in Europe, because it is often \"painful\" and causes \"degrading and discriminatory consequences for young people\". In September 2008, Baroness Sarah Ludford MEP moved a motion to the European Parliament to ban the use of the Mosquito. It failed to get enough signatures from MEPs to proceed to a full debate.\n\nIn Belgium, a resolution was passed by the House in June 2008 asking the government to take all necessary measures to prohibit the use of devices like the Mosquito on Belgian territory. The legality of the device has also been tested in the French courts, where the Tribunal de Grande Instance de Saint-Brieuc prohibited the use of The Mosquito within its municipality, and ordered a private individual to pay €2,000 compensation after operating the device outside their house.\n\nUnder Ireland's Non-Fatal Offences Against the Person Act, 1997, anyone who \"directly or indirectly applies force to or causes an impact on the body of another... without the consent of the other\" (force including \"application of [any] form of energy\"), is guilty of committing assault. This issue has been raised in relation to the Mosquito device by Ireland's Ombudsman for Children following legal advice provided by Youth Work Ireland, but has yet to be tested in the Irish courts.\n\nIn the United Kingdom, the manufacturers claim that the tones are broadcast at 75 dB, meaning that the product falls within the government's auditory-safety limits, although the German news source Heise reported the device emits 85 dB., and websites selling the Mosquito claim it can go up to 95dB with a special chip, which in reality allows the Mosquito to reach 104dB. The government of the United Kingdom issued a statement in 2008 stating that \"'Mosquito alarms are not banned and the government has no plans to ban them\". The English county of Kent has chosen not to allow the usage of the Mosquito on council-owned buildings.\n\n\n"}
{"id": "11630973", "url": "https://en.wikipedia.org/wiki?curid=11630973", "title": "Torsion-free abelian group", "text": "Torsion-free abelian group\n\nIn mathematics, specifically in abstract algebra, a torsion-free abelian group is an abelian group which has no non-trivial torsion elements; that is, a group in which the group operation is commutative and the identity element is the only element with finite order. That is, multiples of any element other than the identity element generate an infinite number of distinct elements of the group.\n\nAn abelian group formula_1 is said to be torsion-free if no element other than the identity formula_2 is of finite order. Compare this notion to that of a torsion group where every element of the group is of finite order. \n\nA natural example of a torsion-free group is formula_3, as only the integer 0 can be added to itself finitely many times to reach 0.\n\n\n\n"}
{"id": "169942", "url": "https://en.wikipedia.org/wiki?curid=169942", "title": "Troy weight", "text": "Troy weight\n\nTroy weight is a system of units of mass customarily used for precious metals and gemstones. One troy ounce (abbreviated \"t oz\" or \"oz t\") is equal to , (or about 1.0971 oz. avoirdupois, the \"avoirdupois\" ounce being the most common definition of an \"ounce\" in the US). There are only 12 troy ounces per troy pound, rather than the 16 ounces per pound found in the more common avoirdupois system. The avoirdupois pound has 7000 grains whereas the troy pound has only 5760 grains (i.e. 12 × 480 grains). Both systems use the same grain defined by the international yard and pound agreement of 1959 as . Therefore, the troy ounce is , compared with the avoirdupois ounce, which is . The troy ounce, then, is about 10% heavier (ratio 192/175) than the avoirdupois ounce. Although troy ounces are still used to weigh gold, silver, and gemstones, troy weight is no longer used in most other applications. One troy ounce of gold is denoted with the ISO 4217 currency code XAU, while one troy ounce of silver is denoted as XAG.\n\nTroy weight probably takes its name from the French market town of Troyes in France where English merchants traded at least as early as the early 9th century. The name \"troy\" is first attested in 1390, describing the weight of a platter, in an account of the travels in Europe of the Earl of Derby.\n\nCharles Moore Watson (1844–1916) proposes an alternative etymology: \"The Assize of Weights and Measures\" (also known as \"Tractatus de Ponderibus et Mensuris\"), one of the statutes of uncertain date from the reign of either Henry III or Edward I, thus before 1307, specifies \"troni ponderacionem\"—which the Public Record Commissioners translate as \"troy weight\". The word \"troni\" refers to markets. Watson finds the dialect word \"troi\", meaning a balance in Wright's Dialect Dictionary. Troy weight referred to the tower system; the earliest reference to the modern troy weights is in 1414.\n\nMany aspects of the troy weight system were indirectly derived from the Roman monetary system. The Romans used bronze bars of varying weights as currency. An (\"heavy bronze\") weighed one pound. One twelfth of an was called an , or in English, an \"ounce\". Before the adoption of the metric system, many systems of troy weights were in use in various parts of Europe, among them Holland troy, Paris troy, etc. Their values varied from one another by up to several percentage points. Troy weights were first used in England in the 15th century, and were made official for gold and silver in 1527. The British Imperial system of weights and measures (also known as Imperial units) was established in 1824, prior to which the troy weight system was a subset of pre-Imperial English units.\n\nThe troy ounce in use today is essentially the same as the British Imperial troy ounce (1824–1971), adopted as an official weight standard for United States coinage by Act of Congress on May 19, 1828.\nThe British Imperial troy ounce (known more commonly simply as the imperial troy ounce) was based on, and virtually identical with, the pre-1824 British troy ounce and the pre-1707 English troy ounce. (1824 was the year the British Imperial system of weights and measures was adopted, 1707 was the year of the Act of Union which created the Kingdom of Great Britain.) Troy ounces have been used in England since about 1400 and the English troy ounce was officially adopted for coinage in 1527. Before that time, various sorts of troy ounces were in use on the continent.\n\nThe troy ounce and grain were also part of the apothecaries' system. This was long used in medicine, but has now been largely replaced by the metric system (milligrams).\n\nThe only troy weight in widespread use today is the British Imperial troy ounce and its American counterpart. Both are currently based on a grain of 0.06479891 gram (exact, by definition), with 480 grains to a troy ounce (compared with grains for an ounce avoirdupois).\n\nThe British Empire abolished the 12-ounce troy pound in the 19th century, though it has been retained (although rarely used) in the American system.\n\nThe origin of the troy weight system is unknown. Although the name probably comes from the Champagne fairs at Troyes, in northeastern France, the units themselves may be of more northern origin. English troy weights were nearly identical to the troy weight system of Bremen. (The Bremen troy ounce had a mass of 480.8 British Imperial grains.)\n\nAn alternative suggestion is that the weights come from the Muslim domains by way of the Gold Dirhem (47.966 British Imperial grains), in the manner that King Offa's weights were derived from the silver Dirhem (about 45.0 British grains).\n\nAccording to Watson, troy relates to a dialect word troi (balance). Then troy weight is a style of weighing, like auncel or bismar weights, or other kindred methods. The troy weight then refers to weighing of small precious or potent goods, such as bullion and medicines.\n\nTroy ounces are still often used in precious metal markets in countries that otherwise use International System of Units (SI), except in East Asia. The People's Bank of China, in particular, which has never historically used troy measurements, has begun issuing Gold Pandas minted according to SI weights.\n\nThe troy pound is 5 760 grains (≈ 373.24 g, 12 oz t), while an avoirdupois pound is approximately 21.53% heavier at 7 000 grains (≈ 453.59 g).\n\nOne troy ounce (oz t) is equal to grams.\nAlso equal to avoirdupois ounces, exactly , or about 10% larger.\n\nThe pennyweight symbol is \"dwt\". There are 24 grains in 1 dwt, and 20 dwt in one troy ounce. Because there were 12 troy ounces in the old troy pound, there would have been 240 pennyweights to the pound—the basis of the fact that the old British pound sterling of currency contained 240 pence. (However, prior to 1526, English pound sterling was based on the tower pound, which is of a troy pound.) The \"d\" in \"dwt\" stands for \"denarius\", the ancient Roman coin that equates loosely to a penny. The symbol \"d\" for penny can be recognized in the notation for British pre-decimal pennies, in which pounds, shillings, and pence were indicated using the symbols \"£\", \"s\", and \"d\", respectively. For example, \"£6 11s 8d\" indicated six pounds, eleven shillings, and eight pence.\n\nMint weights, also known as \"moneyers' weights\" were legalised by Act of Parliament dated 17 July 1649 entitled \"An Act touching the monies and coins of England\". A grain is 20 mites, a mite is 24 droits, a droit is 20 perits, a perit is 24 blanks.\n\nIn Scotland, the Incorporation of Goldsmiths of the City of Edinburgh used a system in multiples of sixteen. (\"See Assay-Master's Accounts, 1681–1702, on loan from the Incorporation to the National Archives of Scotland\".) Thus, there were 16 drops to the troy ounce, 16 ounces to the troy pound, and 16 pounds to the troy stone. The Scots had several other ways of measuring precious metals and gems, but this was the common usage for gold and silver.\n\nThe Pound was 7716 BI grains, but after the union, rounded to 7680 BI grains. This divides to 16 ounces, each of 16 drops, each of 30 grains. The rounding makes the ounce and grain equal to the English standard.\n\nThe Dutch troy system is based on a Mark, of 8 Ounces, the ounce of 20 Engels (pennyweight), the Engel of 32 As. The mark was rated as 3798 Grains, English troy, or 246.084 metric grams. The divisions are identical to the tower system.\n\nThe troy system was used in the apothecaries' system, but with different further subdivisions.\n\nKing Offa's currency reform replaced the sceat with the silver penny. This coin was derived from half of a silver dirhem. The weights were then derived by a count of coins, by a mix of Charlemagne and Roman systems. A shilling was set to twelve pence, an ounce to twenty pence, and a pound to twelve ounces or twenty shillings. The penny was quite a lot of money, so weight by coins was not a general practice.\n\nLater kings debased the coin, both in weight and fineness. The original pound divided was the tower pound of 5400 grains, but a later pound of 5760 grains displaced it. Where once 240 pence made a tower pound (and 256 make a troy pound), by the time of the United Kingdom Weights and Measures Act of 1824, a troy pound gives 792 silver pence, still minted as such as Maundy Money.\n\nSterling originally referred to the Norman silver penny of the late 11th century. The coin was minted to a fineness of 11 oz, 2 dwt (in the pound), or 925 Millesimal fineness.\n\n"}
{"id": "52160921", "url": "https://en.wikipedia.org/wiki?curid=52160921", "title": "Value trumping", "text": "Value trumping\n\nValue trumping is the \"precedence of certain cultural values over others\" in certain contexts, whereas the same value may not be trumping other values under different circumstances. The term is usually applied to cultural or group values, where the members adapt to specific circumstances.\n\n"}
{"id": "145043", "url": "https://en.wikipedia.org/wiki?curid=145043", "title": "Vice", "text": "Vice\n\nVice is a practice, behaviour, or habit generally considered immoral, sinful, criminal, rude, taboo, depraved, or degrading in the associated society. In more minor usage, vice can refer to a fault, a negative character trait, a defect, an infirmity, or a bad or unhealthy habit (such as an addiction to smoking). Vices are usually associated with a transgression in a person's character or temperament rather than their morality. Synonyms for vice include fault, sin, depravity, iniquity, wickedness, and corruption.\n\nThe opposite of vice is virtue.\n\nThe modern English term that best captures its original meaning is the word \"vicious\", which means \"full of vice\". In this sense, the word \"vice\" comes from the Latin word \"vitium\", meaning \"failing or defect\".\n\nDepending on the country or jurisdiction, vice crimes may or may not be treated as a separate category in the criminal codes. Even in jurisdictions where vice is not explicitly delineated in the legal code, the term \"vice\" is often used in law enforcement and judicial systems as an umbrella for crimes involving activities that are considered inherently immoral, regardless of the legality or objective harm involved.\n\nIn the United Kingdom, the term \"vice\" is commonly used in law and law enforcement to refer to criminal offences related to prostitution and pornography. In the United States, the term is also used to refer to crimes related to drugs, alcohol, and gambling.\n\nA vice squad, also called a vice unit or a morality squad, is (though not always) a police division, whose focus is to restrain or suppress moral crimes. Though what is considered or accepted as a \"moral\" crime by society often varies considerably according to local laws or customs between nations, countries, or states, it often includes activities such as gambling, narcotics, pornography, and illegal sales of alcoholic beverages. Vice squads do not concentrate on crimes like fraud and murder.\n\nReligious police, for example islamic religious police units or sharia police in certain parts of the Arab-speaking world, are morality squads that also monitors for example dress codes, observance of store-closures during prayer time, consumption of unlawful beverages or foods, unrelated males and females socializing, and homosexual behavior.\n\nIn the Sarvastivadin tradition of Buddhism, there are 108 defilements, or vices, which are prohibited. These are subdivided into 10 bonds and 98 proclivities. The 10 bonds are the following:\n\nChristians believe there are two kinds of vice:\n\nThe first kind of vice, though sinful, is believed less serious than the second. Vices recognized as spiritual by Christians include blasphemy (holiness betrayed), apostasy (faith betrayed), despair (hope betrayed), hatred (love betrayed), and indifference (scripturally, a \"hardened heart\"). Christian theologians have reasoned that the most destructive vice equates to a certain type of pride or the complete idolatry of the self. It is argued that through this vice, which is essentially competitive, all the worst evils come into being. In Christian theology, it originally led to the Fall of Man, and, as a purely diabolical spiritual vice, it outweighs anything else often condemned by the Church.\n\nThe Roman Catholic Church distinguishes between vice, which is a habit of sin, and the sin itself, which is an individual morally wrong act. Note that in Roman Catholicism, the word \"sin\" also refers to the state that befalls one upon committing a morally wrong act. In this section, the word always means the sinful act. It is the sin, and not the vice, that deprives one of God's sanctifying grace and renders one deserving of God's punishment. Thomas Aquinas taught that \"absolutely speaking, the sin surpasses the vice in wickedness\". On the other hand, even after a person's sins have been forgiven, the underlying habit (the vice) may remain. Just as vice was created in the first place by repeatedly yielding to the temptation to sin, so vice may be removed only by repeatedly resisting temptation and performing virtuous acts; the more entrenched the vice, the more time and effort needed to remove it. Saint Thomas Aquinas says that following rehabilitation and the acquisition of virtues, the vice does not persist as a habit, but rather as a mere disposition, and one that is in the process of being eliminated. Medieval illuminated manuscripts circulated with colorful schemas for developing proper attitudes, with scriptural allusions modelled on nature: the tree of virtues as blossoming flowers or vices bearing sterile fruit, The Renaissance writer Pietro Bembo is credited with reaffirming and promoting the Christian perfection of classical humanism. Deriving all from love (or the lack thereof) his schemas were added as supplements in the newly invented technology of printing by Aldus Manutius in his editions of Dante's Divine Comedy dating from early in the 16th century.\n\nThe poet Dante Alighieri listed the following seven deadly vices, associating them structurally as flaws in the soul's inherent capacity for goodness as made in the Divine Image yet perverted by the Fall:\n\nThe first three terraces of purgatory expiate the sins which can be considered to arise from love perverted, that is, sins which arise from the heart of the sinner being set upon something which is wrong in the eyes of God. Those being purged here must have their love set upon the right path. The fourth terrace of purgatory expiates the sins which can be considered to arise from love defective, that is, love which, although directed towards the correct subjects is too weak to drive the sinner to act as they should. Those being purged here must have their love strengthened so as to drive them correctly. The fifth, sixth, and seventh terraces of purgatory expiate the sins which can be considered to arise from love excessive, that is, love which although directed towards ends which God considers good is directed towards them too much for the sinner to gain bliss from them, and also so that the sinner is distracted from the love of other things of which God approves. Their love must be cooled to a more sensible level.\n\nThe Qur'an and many other Islamic religious writings provide prohibitions against acts that are seen as immoral.\n\nIbn abi Dunya, a 9th-century scholar and tutor to the caliphs, described seven \"censures\" (prohibitions against vices) in his writings:\n\n\nIn Sanskrit काम (lust) क्रोध (anger) लोभ (greed) मद(pride) मोह (temptation) मत्सर (jealousy)\n\n\n\n\n"}
{"id": "1436668", "url": "https://en.wikipedia.org/wiki?curid=1436668", "title": "Voigt notation", "text": "Voigt notation\n\nIn mathematics, Voigt notation or Voigt form in multilinear algebra is a way to represent a symmetric tensor by reducing its order. There are a few variants and associated names for this idea: Mandel notation, Mandel–Voigt notation and Nye notation are others found. Kelvin notation is a revival by Helbig of old ideas of Lord Kelvin. The differences here lie in certain weights attached to the selected entries of the tensor. Nomenclature may vary according to what is traditional in the field of application.\n\nFor example, a 2×2 symmetric tensor X has only three distinct elements, the two on the diagonal and the other being off-diagonal. Thus it can be expressed as the vector\n\nAs another example:\n\nThe stress tensor (in matrix notation) is given as\n\nIn Voigt notation it is simplified to a 6-dimensional vector:\n\nThe strain tensor, similar in nature to the stress tensor—both are symmetric second-order tensors --, is given in matrix form as\n\nIts representation in Voigt notation is\nwhere formula_6, formula_7, and formula_8 are engineering shear strains.\n\nThe benefit of using different representations for stress and strain is that the scalar invariance\nis preserved.\n\nLikewise, a three-dimensional symmetric fourth-order tensor can be reduced to a 6×6 matrix.\n\nA simple mnemonic rule for memorizing Voigt notation is as follows:\n\n\nVoigt indexes are numbered consecutively from the starting point to the end (in the example, the numbers in blue).\n\nFor a symmetric tensor of second rank\n\nonly six components are distinct, the three on the diagonal and the others being off-diagonal. \nThus it can be expressed, in Mandel notation, as the vector\n\nThe main advantage of Mandel notation is to allow the use of the same conventional operations used with vectors,\nfor example:\n\nA symmetric tensor of rank four satisfying formula_13 and formula_14 has 81 components in three-dimensional space, but only 36 \ncomponents are distinct. Thus, in Mandel notation, it can be expressed as\n\nThe notation is named after physicist Woldemar Voigt. It is useful, for example, in calculations involving constitutive models to simulate materials, such as the generalized Hooke's law, as well as finite element analysis, and Diffusion MRI.\n\nHooke's law has a symmetric fourth-order stiffness tensor with 81 components (3×3×3×3), but because the application of such a rank-4 tensor to a symmetric rank-2 tensor must yield another symmetric rank-2 tensor, not all of the 81 elements are independent. Voigt notation enables such a rank-4 tensor to be represented by a 6×6 matrix. However, Voigt's form does not preserve the sum of the squares, which in the case of Hooke's law has geometric significance. This explains why weights are introduced (to make the mapping an isometry).\n\nA discussion of invariance of Voigt's notation and Mandel's notation be found in Helnwein (2001).\n\n"}
{"id": "45754", "url": "https://en.wikipedia.org/wiki?curid=45754", "title": "Where Mathematics Comes From", "text": "Where Mathematics Comes From\n\nWhere Mathematics Comes From: How the Embodied Mind Brings Mathematics into Being (hereinafter \"WMCF\") is a book by George Lakoff, a cognitive linguist, and Rafael E. Núñez, a psychologist. Published in 2000, \"WMCF\" seeks to found a cognitive science of mathematics, a theory of embodied mathematics based on conceptual metaphor.\n\nMathematics makes up that part of the human conceptual system that is special in the following way:\n\nNikolay Lobachevsky said \"There is no branch of mathematics, however abstract, which may not some day be applied to phenomena of the real world.\" A common type of conceptual blending process would seem to apply to the entire mathematical procession.\n\nLakoff and Núñez's avowed purpose is to begin laying the foundations for a truly scientific understanding of mathematics, one grounded in processes common to all human cognition. They find that four distinct but related processes metaphorically structure basic arithmetic: object collection, object construction, using a measuring stick, and moving along a path.\n\n\"WMCF\" builds on earlier books by Lakoff (1987) and Lakoff and Johnson (1980, 1999), which analyze such concepts of metaphor and image schemata from second-generation cognitive science. Some of the concepts in these earlier books, such as the interesting technical ideas in Lakoff (1987), are absent from \"WMCF\".\n\nLakoff and Núñez hold that mathematics results from the human cognitive apparatus and must therefore be understood in cognitive terms. \"WMCF\" advocates (and includes some examples of) a \"cognitive idea analysis\" of mathematics which analyzes mathematical ideas in terms of the human experiences, metaphors, generalizations, and other cognitive mechanisms giving rise to them. A standard mathematical education does not develop such idea analysis techniques because it does not pursue considerations of A) what structures of the mind allow it to do mathematics or B) the philosophy of mathematics.\n\nLakoff and Núñez start by reviewing the psychological literature, concluding that human beings appear to have an innate ability, called subitizing, to count, add, and subtract up to about 4 or 5. They document this conclusion by reviewing the literature, published in recent decades, describing experiments with infant subjects. For example, infants quickly become excited or curious when presented with \"impossible\" situations, such as having three toys appear when only two were initially present.\n\nThe authors argue that mathematics goes far beyond this very elementary level due to a large number of metaphorical constructions. For example, the Pythagorean position that all is number, and the associated crisis of confidence that came about with the discovery of the irrationality of the square root of two, arises solely from a metaphorical relation between the length of the diagonal of a square, and the possible numbers of objects.\n\nMuch of \"WMCF\" deals with the important concepts of infinity and of limit processes, seeking to explain how finite humans living in a finite world could ultimately conceive of the actual infinite. Thus much of \"WMCF\" is, in effect, a study of the epistemological foundations of the calculus. Lakoff and Núñez conclude that while the potential infinite is not metaphorical, the actual infinite is. Moreover, they deem all manifestations of actual infinity to be instances of what they call the \"Basic Metaphor of Infinity\", as represented by the ever-increasing sequence 1, 2, 3, ...\n\n\"WMCF\" emphatically rejects the Platonistic philosophy of mathematics. They emphasize that all we know and can ever know is \"human mathematics\", the mathematics arising from the human intellect. The question of whether there is a \"transcendent\" mathematics independent of human thought is a meaningless question, like asking if colors are transcendent of human thought—colors are only varying wavelengths of light, it is our interpretation of physical stimuli that make them colors.\n\n\"WMCF\" (p. 81) likewise criticizes the emphasis mathematicians place on the concept of closure. Lakoff and Núñez argue that the expectation of closure is an artifact of the human mind's ability to relate fundamentally different concepts via metaphor.\n\n\"WMCF\" concerns itself mainly with proposing and establishing an alternative view of mathematics, one grounding the field in the realities of human biology and experience. It is not a work of technical mathematics or philosophy. Lakoff and Núñez are not the first to argue that conventional approaches to the philosophy of mathematics are flawed. For example, they do not seem all that familiar with the content of Davis and Hersh (1981), even though the book warmly acknowledges Hersh's support.\n\nLakoff and Núñez cite Saunders Mac Lane (the inventor, with Samuel Eilenberg, of category theory) in support of their position. \"Mathematics, Form and Function\" (1986), an overview of mathematics intended for philosophers, proposes that mathematical concepts are ultimately grounded in ordinary human activities, mostly interactions with the physical world.\n\nEducators have taken some interest in what \"WMCF\" suggests about how mathematics is learned, and why students find some elementary concepts more difficult than others.\n\nConceptual metaphors described in \"WMCF\", in addition to the Basic Metaphor of Infinity, include:\nMathematical reasoning requires variables ranging over some universe of discourse, so that we can reason about generalities rather than merely about particulars. \"WMCF\" argues that reasoning with such variables implicitly relies on what it terms the Fundamental Metonymy of Algebra.\n\n\"WMCF\" (p. 151) includes the following example of what the authors term \"metaphorical ambiguity.\" Take the set formula_1 Then recall two bits of standard terminology from elementary set theory:\nBy (1), \"A\" is the set {1,2}. But (1) and (2) together say that \"A\" is also the ordered pair (0,1). Both statements cannot be correct; the ordered pair (0,1) and the unordered pair {1,2} are fully distinct concepts. Lakoff and Johnson (1999) term this situation \"metaphorically ambiguous.\" This simple example calls into question any Platonistic foundations for mathematics.\n\nWhile (1) and (2) above are admittedly canonical, especially within the consensus set theory known as the Zermelo–Fraenkel axiomatization, \"WMCF\" does not let on that they are but one of several definitions that have been proposed since the dawning of set theory. For example, Frege, \"Principia Mathematica\", and New Foundations (a body of axiomatic set theory begun by Quine in 1937) define cardinals and ordinals as equivalence classes under the relations of equinumerosity and similarity, so that this conundrum does not arise. In Quinian set theory, \"A\" is simply an instance of the number 2. For technical reasons, defining the ordered pair as in (2) above is awkward in Quinian set theory. Two solutions have been proposed:\n\nThe \"Romance of Mathematics\" is \"WMCF\"s light-hearted term for a perennial philosophical viewpoint about mathematics which the authors describe and then dismiss as an intellectual myth:\n\nIt is very much an open question whether \"WMCF\" will eventually prove to be the start of a new school in the philosophy of mathematics. Hence the main value of \"WMCF\" so far may be a critical one: its critique of Platonism and romanticism in mathematics.\n\nMany working mathematicians resist the approach and conclusions of Lakoff and Núñez. Reviews by mathematicians of \"WMCF\" in professional journals, while often respectful of its focus on conceptual strategies and metaphors as paths for understanding mathematics, have taken exception to some of the \"WMCF\"s philosophical arguments on the grounds that mathematical statements have lasting 'objective' meanings. For example, Fermat's last theorem means exactly what it meant when Fermat initially proposed it 1664. Other reviewers have pointed out that multiple conceptual strategies can be employed in connection with the same mathematically defined term, often by the same person (a point that is compatible with the view that we routinely understand the 'same' concept with different metaphors). The metaphor and the conceptual strategy are not the same as the formal definition which mathematicians employ. However, \"WMCF\" points out that formal definitions are built using words and symbols that have meaning only in terms of human experience.\n\nCritiques of \"WMCF\" include the humorous:\nand the physically informed:\n\nLakoff made his reputation by linking linguistics to cognitive science and the analysis of metaphor. Núñez, educated in Switzerland, is a product of Jean Piaget's school of cognitive psychology as a basis for logic and mathematics. Núñez has thought much about the foundations of real analysis, the real and complex numbers, and the Basic Metaphor of Infinity. These topics, however, worthy though they be, form part of the superstructure of mathematics. Cognitive science should take more interest in the foundations of mathematics. And indeed, the authors do pay a fair bit of attention early on to logic, Boolean algebra and the Zermelo–Fraenkel axioms, even lingering a bit over group theory. But neither author is well-trained in logic (there is no index entry for \"quantifier\" or \"quantification\"), the philosophy of set theory, the axiomatic method, metamathematics, and model theory. Nor does \"WMCF\" say enough about the derivation of number systems (the Peano axioms go unmentioned), abstract algebra, equivalence and order relations, mereology, topology, and geometry.\n\nLakoff and Núñez tend to dismiss the negative opinions mathematicians have expressed about \"WMCF\", because their critics do not appreciate the insights of cognitive science. Lakoff and Núñez maintain that their argument can only be understood using the discoveries of recent decades about the way human brains process language and meaning. They argue that any arguments or criticisms that are not grounded in this understanding cannot address the content of the book.\n\nIt has been pointed out that it is not at all clear that \"WMCF\" establishes that the claim \"intelligent alien life would have mathematical ability\" is a myth. To do this, it would be required to show that intelligence and mathematical ability are separable, and this has not been done. On Earth, intelligence and mathematical ability seem to go hand in hand in all life-forms, as pointed out by Keith Devlin among others. The authors of \"WMCF\" have not explained how this situation would (or even could) be different anywhere else.\n\nLakoff and Núñez also appear not to appreciate the extent to which intuitionists and constructivists have anticipated their attack on the Romance of (Platonic) Mathematics. Brouwer, the founder of the intuitionist/constructivist point of view, in his dissertation \"On the Foundation of Mathematics\", argued that mathematics was a mental construction, a free creation of the mind and totally independent of logic and language. He goes on to upbraid the formalists for building verbal structures that are studied without intuitive interpretation. Symbolic language should not be confused with mathematics; it reflects, but does not contain, mathematical reality.\n\n\"WMCF\" (pp. 378–79) concludes with some key points, a number of which follow. Mathematics arises from our bodies and brains, our everyday experiences, and the concerns of human societies and cultures. It is:\n\nThe cognitive approach to formal systems, as described and implemented in \"WMCF\", need not be confined to mathematics, but should also prove fruitful when applied to formal logic, and to formal philosophy such as Edward Zalta's theory of abstract objects. Lakoff and Johnson (1999) fruitfully employ the cognitive approach to rethink a good deal of the philosophy of mind, epistemology, metaphysics, and the history of ideas.\n\n\n\n"}
{"id": "34426925", "url": "https://en.wikipedia.org/wiki?curid=34426925", "title": "Zone diagram", "text": "Zone diagram\n\nA zone diagram is a certain geometric object which a variation on the notion of Voronoi diagram. It was introduced by Tetsuo Asano, Jiri Matousek, and Takeshi Tokuyama in 2007.\n\nFormally, it is a fixed point of a certain function. Its existence or uniqueness are not clear in advance and have been established only in specific cases. Its computation is not obvious too.\n\nConsider a group of formula_1 different points formula_2 in the Euclidean plane. Each point is called a site. When we speak about the Voronoi diagram induced by these sites, we associate to the site formula_3 the set formula_4 of all points in the plane whose distance to the given site formula_3 is not greater to their distance to any other site formula_6. The collection formula_7 of these regions is the Voronoi diagram associated with these sites, and it induces a decomposition of the plane into regions: the Voronoi regions (Voronoi cells).\n\nIn a zone diagram the region associated with the site formula_8 is defined a little bit differently: instead of associating it the set of all points whose distance to formula_8 is not greater than their distance to the other sites, we associate to formula_8 the set formula_11 of all points in the plane whose distance to formula_8 is not greater than their distance to any other region. Formally,\n\nHere formula_14 denotes the euclidean distance between the points formula_15 and formula_16 and formula_17 is the distance between the point formula_18 and the set formula_19. In addition, formula_20 since we consider the plane. The tuple formula_7 is the zone diagram associated with the sites.\n\nThe problem with this definition is that it seems circular: in order to know formula_11 we should know formula_23 for each index formula_24 but each such formula_23 is defined in terms of formula_4. On a second thought, we see that actually the tuple formula_7 is a solution of the following system of equations:\n\nRigorously, a zone diagram is any solution of this system, if such a solution exists. This definition can be extended without essentially any change to higher dimensions, to sites which are not necessarily points, to infinitely many sites, etc.\n\nIn some settings, such as the one described above, a zone diagram can be interpreted as a certain equilibrium between mutually hostile kingdoms. In a discrete setting it can be interpreted as a stable configuration in a certain combinatorial game.\n\nLet formula_29 be a metric space and let formula_30 be a set of at least 2 elements (indices), possibly infinite. Given a tuple formula_31 of nonempty subsets of formula_32, called the sites, a zone diagram with respect to this tuple is a tuple formula_33 of subsets of formula_32 such that for all formula_35 the following equation is satisfied:\n\nThe system of equations which defines the zone diagram can be represented as a fixed point of a function defined on a product space. Indeed, for each index formula_37 let formula_38 be the set of all nonempty subsets of formula_39.\nLet\n\nand let formula_41 be the function defined by formula_42, where formula_43 and\n\nThen formula_45 is a zone diagram if and only if it is a fixed point of Dom, that is, formula_46. Viewing zone diagrams as fixed points is useful since in some settings known tools or approaches from fixed point theory can be used for investigating them and\nderiving relevant properties (existence, etc.).\n\nFollowing the pioneering work of Asano et al. (existence and uniqueness of the zone diagram in the euclidean plane with respect to finitely many point sites), several existence or existence and uniqueness results have been published. As of 2012, the most general results which have been published are:\n\n\nMore information is needed.\n\nIn addition to Voronoi diagrams, zone diagrams are closely related to other geometric objects such as double zone diagrams, trisectors, k-sectors, mollified zone diagrams and as a result may be used for solving problems related to robot motion and VLSI design.\n\n"}
