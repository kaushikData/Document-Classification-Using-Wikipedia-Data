{"id": "6982923", "url": "https://en.wikipedia.org/wiki?curid=6982923", "title": "Adaptive mutation", "text": "Adaptive mutation\n\nAdaptive mutation is a controversial evolutionary theory. It posits that mutations, or genetic changes, are much less random and more purposeful than traditional evolution. There have been a wide variety of experiments trying to prove (or disprove) the idea of adaptive mutation, at least in microorganisms.\n\nThe most widely accepted theory of evolution states that organisms will diversify based on natural selection where changes caused by mutations improve the organism's chance of survival. Adaptive mutation states that rather than mutations and evolution being random, they are in response to specific stresses. In other words, the mutations that occur are more beneficial and specific to the given stress, instead of random and not a response to anything in particular. It should be noted that the term stress refers to any change in the environment, such as temperature, nutrients, population size, etc. Tests with microorganisms have found that for adaptive mutation, more of the mutations observed after a given stress were more effective at dealing with the stress than chance alone would suggest is possible. This theory of adaptive mutation was first brought to academic attention in the 1980s by John Cairns.\n\nAdaptive mutation is a very controversial topic so there have been many experiments to prove or discredit the theory. Three major experiments are the SOS response and responses to starvation in \"Escherichia coli\", and testing for revertants of a tryptophan auxotroph of \"Saccharomyces cerevisiae\", or yeast.\n\nThis experiment is different from the others in one small way: this experiment is concerned with the pathways leading to an adaptive mutation while the others tested the changing environment microorganisms were exposed to.\n\nSimply put, the SOS response in \"E. coli\" is a response to DNA damage that must be repaired. The normal cell cycle is put on hold and mutagenesis may begin. This means that mutations will occur to try to fix the damage. This hypermutation, or increased rate of change, response has to have some regulatory process, and some key molecules in this process are RecA, and LexA. These are proteins and act as stoplights for this and other processes. They also appear to be the main contributors to adaptive mutation in \"E. coli\". Changes in presence of one or the other was shown to affect the SOS response, which in turn affected how the cells were able to process lactose, which should not be confused with the lactose starvation experiment. The key point to understand here is that LexA and RecA both were required for adaptive mutation to occur, and without the SOS response adaptive mutation would not be possible.\n\nThe \"E. coli\" strain FC40 has a high rate of mutation, and so is useful for studies, such as for adaptive mutation. Due to a frameshift mutation, a change in the sequence that causes the DNA to code for something different, FC40 is unable to process lactose. When placed in a lactose-rich medium, it has been found that 20% of the cells mutated from Lac- (could not process lactose) to Lac+, meaning they could now utilize the lactose in their environment. The responses to stress are not in current DNA, but the change is made during DNA replication through recombination and the replication process itself, meaning that the adaptive mutation occurs in the current bacteria and will be inherited by the next generations because the mutation becomes part of the genetic code in the bacteria. This is particularly obvious in a study by Cairns, which demonstrated that even after moving \"E. coli\" back to a medium with minimal levels of lactose, Lac+ mutants continued to be produced as a response to the previous environment. This would not be possible if adaptive mutation was not at work because natural selection would not favor this mutation in the new environment.\n\nAlthough there are many genes involved in adaptive mutation, RecG, a protein, was found to have an effect on adaptive mutation. By itself, RecG was found to not necessarily lead to a mutational phenotype. However, it was found to inhibit the appearance of revertants (cells that appeared normally, as opposed to those with the mutations being studied) in wild-type cells. On the other hand, RecG mutants were key to the expression of RecA-dependent mutations, which were a major portion of study in the SOS response experiments, such as the ability to utilize lactose.\n\nDr. von Borstel, in the 1970s, conducted experiments similar to the Lactose Starvation experiment with yeast, specifically \"Saccharomyces cerevisiae\". He tested for tryptophan auxotroph revertants. A tryptophan auxotroph cannot make tryptophan for itself, but wild-type cells can and so a revertant will revert to the normal state of being able to produce tryptophan. He found that when yeast colonies were moved from a tryptophan-rich medium to a minimal one, revertants continued to appear for several days. The degree to which revertants were observed in yeast was not as high as with bacteria. Other scientists have conducted similar experiments, such as Hall who tested histidine revertants, or Steele & Jinks-Robertson who tested lysine. These experiments demonstrate how recombination and DNA replication are necessary for adaptive mutation. However, in lysine-tested cells, recombination continued to occur even without selection for it. Steele and Jinks-Robertson concluded that recombination occurred in all circumstances, adaptive or otherwise, while mutations were present only when they were beneficial and adaptive.\n\nAlthough the production of mutations during selection was not as vigorous as observed with bacteria, these studies are convincing. As mentioned above, a subsequent study adds even more weight to the results with \"lys2\". Steele & Jinks-Robertson found that LYS prototrophs due to interchromosomal recombination events also continue to arise in nondividing cells, but in this case, the production of recombinants continued whether there was selection for them or not. Thus, mutation occurred in stationary phase only when it was adaptive, but recombination occurred whether it was adaptive or not.\n\nDelayed appearance of mutants has also been reported for \"Candida albicans.\" With long exposure to sublethal concentrations of heavy metals, colonies of resistant cells began to appear after 5–10 days and continued to appear for 1–2 weeks thereafter. These resistances could have resulted from gene amplification, although the phenotypes were stable during a short period of nonselective growth. However, revertants of two auxotrophies also appeared with similar kinetics. None of these events in \"Candida albicans\" have, as yet, been shown to be specific to the selection imposed.\n"}
{"id": "50211648", "url": "https://en.wikipedia.org/wiki?curid=50211648", "title": "Ahdname of Milodraž", "text": "Ahdname of Milodraž\n\nThe Ahdname of Milodraž (/Милодрашка ахднама), also called the Ahdname of Fojnica (Фојничка ахднама/\"Fojnička ahdnama\"), was the \"ahdname\" issued on 28 May 1463 (or 1464) by the Ottoman sultan Mehmed the Conqueror to Bosnian Franciscans, represented by Anđeo Zvizdović.\n\nAccording to Bosnian Franciscan tradition, Mehmed was preparing to depart following the Ottoman conquest of Bosnia when Anđeo Zvizdović came to meet him in the Ottoman military camp in Milodraž. Led in by Mehmed's soldiers, Zvizdović drew the Sultan's attention to the exodus of Catholics from the newly conquered country. The friar specifically pointed to the necessity of maintaining the merchants, craftsmen and miners, and so succeeded in receiving Mehmed's solemn promise of religious tolerance. The Franciscans of Bosna Argentina recognized Mehmed as their sovereign, and in return he promised that \"the Bosnian priests shall have freedom and protection, and may return to and settle the lands in the Empire in their monasteries without consternation. No-one is to attack them, nor threaten their lives, property or churches.\" Its form and content, as well as Mehmed's personal oath, resemble that of an international treaty.\n\nThe rights expressed in the Ahdname of Milodraž were reiterated by all subsequent Ottoman sultans, but the Franciscans were nevertheless in a difficult position with the local authorities. Although they were loyal to the Ottoman regime, the local government often suspected them of aiding the Catholic Habsburg Empire, the Ottoman Empire's greatest enemy. Bosnian Franciscans used the Ahdname of Milodraž not only in relations with the Muslim authorities, but also to protect themselves from the ambitions of Eastern Orthodox clergy when the latter claimed the right to collect tax from them too on the basis of an earlier \"firman\". The Ahdname of Milodraž is often said to have enabled the survival of Roman Catholicism in Bosnia and Herzegovina.\n\nThe Ahdname of Milodraž has been preserved in transcripts; an \"ahdname\" was likely also issued to the Franciscans of Srebrenica in 1462, but it has been entirely lost. The absence of the original document led some historians to describe the Ahdname of Milodraž as a forgery. Its historicity was only confirmed in the mid-20th century.\n"}
{"id": "29214621", "url": "https://en.wikipedia.org/wiki?curid=29214621", "title": "Ashta Nayika", "text": "Ashta Nayika\n\nThe Ashta-Nayika is a collective name for eight types of \"nayika\"s or heroines as classified by Bharata in his Sanskrit treatise on performing arts - \"Natya Shastra\". The eight nayikas represent eight different states (\"avastha\") in relationship to her hero or \"nayaka\". As archetypal states of the romantic heroine, it has been used as theme in Indian painting, literature, sculpture as well as Indian classical dance and music.\n\nAs per Ashta Nayika, there are eight nayikas.\n\nThe Ashta-Nayika classification (\"nayika-bheda\") first appears in \"Natya Shastra\" (24.210-11), a key Sanskrit treatise on Indian performing arts, authored by Bharata (dated between 2nd century BC and 2nd century AD). The classification is detailed in later works like the \"Dasarupaka\" (10th century), \"Sahityadarpana\" (14th century) and various other treatises on poetics as well as erotic \"Kamashastra\" texts like Kuttanimata (8th-9th century) based on courtesans, \"Panchasayaka\", \"Anangaranga\" and \"Smaradipika\". Keshavadasa's \"Rasikapriya\" (16th century) in Hindi, also elaborates on the Ashta-nayika.\n\nThe Ashta-Nayika have been illustrated in Indian painting, literature, sculpture as well as Indian classical dance. Notable medieval paintings that depict the Ashta nayika are the Ragamala paintings, as those from the Bundi school of painting.\n\nA famous example in Indian literature is Jayadeva's \"Gita Govinda\" (12th century) as well as in the Vaishnava poet Banamali's compositions, Radha dons the roles of the various nayikas while with her nayaka is the god Krishna.\n\nThe Ashta-Nayika is a central theme in \"Pahari embroidery\" used to decorate the \"Chamba Rumal\", especially produced in Chamba, Himachal Pradesh. The Ashta Nayika are usually portrayed in eight panels on the Rumal.\n\nIn Indian (Hindustani) classical music, the eternal love between Radha and Krishna is represented through the consciousness of Radha as the leitmotif that dominates the lyrics. Especially the semi-classical genre of Thumri imbibes the myriad moods of Radha as Ashta Nayika consumed by passionate love for Krishna.\n\nThe \"Natya Shastra\" describes the nayikas in the following order: Vasakasajja, Virahotkanthita, Svadhinabhartruka, Kalahantarita, Khandita, Vipralabdha, Proshitabhartruka and Abhisarika. The nayikas are further classified in two varieties of the \"shringara rasa\", the \"rasa\" related to love: Sambhoga (love in meeting) and Vipralambha (love in separation). Vasakasajja, Svadhinabhartruka and Abhisarika are associated with Sambhoga; the others with Vipralambha.\n\nIn the \"Shringara Prakasha\", Bhoja relates the various nayakas and nayikas with musical \"raga\"s and \"ragini\"s (a female raga). Somanatha's \"Ragavibodha\" (1609) and Damodara's \"Sangitadarpana\" (c. 1625) continue this trend.\n\nVasakasajja (\"one dressed up for union\") or Vasakasajjika is waiting for her lover returning from a long journey. She is depicted in her bed-chamber filled with lotus leaves and garlands. She is dressing herself for the union with her lover and \"eager with expectation of love's pleasure\". Her beauty is compared by Kesavadasa to Rati - the Hindu love goddess, waiting for her husband, the love god Kamadeva. A Vasakasajja sculpture is found in the Lakshmana Temple in Khajuraho and the National Museum, Delhi.\n\nThe \"Ragavibodha\" associates the raginis Bhupali and Todi with Vasakasajja.\n\nVirahotkanthita (\"One distressed by separation\") or Utka (as described by Keshavadasa) is the distressed heroine pining for her lover, who, due to his preoccupation, fails to return home. She is depicted waiting for him, sitting or standing on a bed or out in the pavilion.\n\nThe \"Ragavibodha\" identifies the raginis Mukhari, Pauravi and Turushkatodi with the Virahotkanthita, while the \"Sangitadarpana\" names Patamanjari in this category.\nSvadhinabhartruka (\"one having her husband in subjection\") or Svadhinapatika(as named by Keshavadasa) is the woman who is loved by her husband and controls him. He is subjugated by her intense love and pleasing qualities.He is devoted and faithful to her\n\nMany raginis like Malashri, Travanika, Ramakriti, Jaitashri and Purvi are associated with Svadhinabhartruka.\n\nKalahantarita (\"one separated by quarrel\") or Abhisandhita (as named by Keshavadasa) is a heroine separated from her lover due to a quarrel or jealousy or her own arrogance. Her lover is usually depicted leaving her apartment disheartened, while she too becomes heartsick and repentant without him. In other portrayals, she is depicted refusing the advances of her lover or refusing a wine cup from him. In Gita Govinda, Radha is also portrayed as Kalahantarita in an instance.\nKhandita (\"one enraged with her lover\") is an enraged heroine, whose lover had promised her to spend the night with her, but instead comes to her house the next morning after spending the night with another woman. She is depicted offended, rebuking her lover for his infidelity.\n\nIn the \"Sangitadarpana\", the ragini Varati represents the Khandita Nayika.\n\nVipralabdha (\"one deceived by her lover\"), , is a deceived heroine, who waited for her lover the whole night. She is depicted throwing away her jewellery as her lover did not keep his promise. This happens when a lover meets a Khandita and promises a tryst and breaks his promise.\n\nThe \"Sangitadarpana\" associates Vipralabdha with the ragini Bhupali. However, the \"Ragavibodha\" presents the raginis Varati and Velavati as Vipralabdhas.\n\nProshitabhartruka (\"one with a sojourning husband\") or Proshitapatika (as named by Keshavadasa) is the woman whose husband has gone away from her for some business and does not return on the appointed day. She is depicted seated mourning, surrounded by her maids, but refusing to be consoled.\n\nThe \"Ragavibodha\" describes the raginis Dhanashri and Kamodi as Proshitabhartrukas.\n\nAbhisarika (\"one who moves\") is a heroine, who sets aside her modesty and moves out of her home to secretly meet her lover. She is depicted at the door of her house and on her way to the tryst, defying all kinds of difficulties like the storm, snakes and dangers of the forest. In art, Abhisarika is portrayed often in hurry towards her destination.\n\nThe raginis Bahuli and Saurashtri are described having the traits of the daring Abhisarika.\n"}
{"id": "8934226", "url": "https://en.wikipedia.org/wiki?curid=8934226", "title": "Basic limiting principle", "text": "Basic limiting principle\n\nA Basic Limiting Principle (B.L.P.) is a general principle that limits our explanations metaphysically or epistemologically, and which normally goes unquestioned or even unnoticed in our everyday or scientific thinking. The term was introduced by the philosopher C. D. Broad in his 1949 paper \"The Relevance of Psychical research to Philosophy\":\n\n\"There are certain limiting principles which we unhesitatingly take for granted as the framework within which all our practical activities and our scientific theories are confined. Some of these seem to be self-evident. Others are so overwhelmingly supported by all the empirical facts which fall within the range of ordinary experience and the scientific elaborations of it (including under this heading orthodox psychology) that it hardly enters our heads to question them. Let us call these Basic Limiting Principles.\"\n\nBroad offers nine examples of B.L.P.s, including the principle that there can be no backward causation, that there can be no action at a distance, and that one cannot perceive physical events or material things directly, unmediated by sensations.\n\n"}
{"id": "5653710", "url": "https://en.wikipedia.org/wiki?curid=5653710", "title": "Biquandle", "text": "Biquandle\n\nIn mathematics, biquandles and biracks are generalizations of quandles and racks. Whereas the hinterland of quandles and racks is the theory of classical knots, that of the bi-versions, is the theory of virtual knots.\n\nBiquandles and biracks have two binary operations on a set formula_1 written formula_2 and formula_3. These satisfy the following three axioms:\n\n1. formula_4\n\n2. formula_5\n\n3. formula_6\n\nThese identities appeared in 1992 in reference [FRS] where the object was called a species.\n\nThe superscript and subscript notation is useful here because it dispenses with the need for brackets. For example\nif we write formula_7 for formula_3 and formula_9 for formula_2 then the\nthree axioms above become\n\n1. formula_11\n\n2. formula_12\n\n3. formula_13\n\nFor other notations see racks and quandles.\n\nIf in addition the two operations are invertible, that is given formula_14 in the set formula_1 there are unique formula_16 in the set formula_1 such that formula_18 and formula_19 then the set formula_1 together with the two operations define a birack.\n\nFor example if formula_1, with the operation formula_2, is a rack then it is a birack if we define the other operation to be the identity, formula_23.\n\nFor a birack the function formula_24 can be defined by\n\nThen\n\n1. formula_26 is a bijection\n\n2. formula_27\n\nIn the second condition, formula_28 and formula_29 are defined by formula_30 and formula_31. This condition is sometimes known as the set-theoretic Yang-Baxter equation.\n\nTo see that 1. is true note that formula_32 defined by\n\nis the inverse to \n\nTo see that 2. is true let us follow the progress of the triple formula_35 under formula_36. So \n\nOn the other hand, formula_38. Its progress under formula_39 is\n\nAny formula_26 satisfying 1. 2. is said to be a \"switch\" (precursor of biquandles and biracks).\n\nExamples of switches are the identity, the \"twist\" formula_42 and formula_43 where formula_2 is the operation of a rack.\n\nA switch will define a birack if the operations are invertible. Note that the identity switch does not do this.\n\nA biquandle is a birack which satisfies some additional structure, as described by Nelson and Rische. The axioms of a biquandle are \"minimal\" in the sense that they are the weakest restrictions that can be placed on the two binary operations while making the biquandle of a virtual knot invariant under Reidemeister moves.\n\n"}
{"id": "56664942", "url": "https://en.wikipedia.org/wiki?curid=56664942", "title": "Cabang Atas", "text": "Cabang Atas\n\nThe Cabang Atas — literally 'highest branch' in Malay — was the traditional Chinese establishment or gentry of colonial Indonesia. As a privileged social class, they exerted a powerful influence on the political, economic and social life of pre-revolutionary Indonesia, in particular on its local Chinese community. \n\nIn older literature, the Cabang Atas is referred to as the baba bangsawan (Malay for 'Chinese gentry').\n\nThe phrase 'Cabang Atas' was first used by the colonial Indonesian historian Liem Thian Joe in his book \"Riwajat Semarang\" (published in 1933). The term refers to a small group of old gentry families that dominated the Dutch colonial institution of the Chinese officership (see 'Kapitan Cina'); this was colonial Indonesia's equivalent of the Chinese mandarinate. As a class, they intermarried to maintain their political and economic power, owned extensive agricultural landholdings and monopolised the colonial government's lucrative revenue farms.\n\nThe oldest families of the Cabang Atas traced their roots in Indonesia back to early Chinese allies and compradores of the Dutch East India Company, in a period that lasted until the latter's bankruptcy in 1799. Many of these Chinese magnates — such as Souw Beng Kong, first \"Kapitein der Chinezen\" of Batavia (1580-1644); or the sons of Han Siong Kong (1673-1743), founder of the Han family of Lasem — played an instrumental role in establishing Dutch colonial rule in Indonesia in the seventeenth and eighteenth centuries. Some families came of gentry stock in China, but many more started off as successful merchant families. They shared some common traits with the scholar-gentry of Imperial China, but accumulated much greater dynastic wealth thanks partly to the protection of Dutch colonial law.\n\nThe foundation of their political power was their near-hereditary control of the bureaucratic posts of \"Majoor\", \"Kapitein\" and \"Luitenant der Chinezen\". This gave them a high degree of political and legal jurisdiction over the local Chinese community. By colonial Indonesian tradition, descendants of Chinese officers bore the hereditary title \"Sia\".\n\nIn addition, most families of the Cabang Atas owned particuliere landrijen or private domains in the \"Ommelanden\" (rural hinterland) of Batavia (now Jakarta); or appanage leaseholds in the Javanese princely states. This gave them significant seigniorial powers over the indigenous peasants living on their landholdings, but also earned them much enmity and resentment. \n\nThe economic foundation of the Cabang Atas, as pointed out by the American historian James R. Rush, was their monopolistic control of the colonial government's revenue farms, in particular the highly lucrative opium farms. These farms were auctioned off with much fanfare and ceremony at the local colonial administrator's residence to the highest bidders, and were most frequently won by members of the Cabang Atas or others allied to, or backed, by them. Menghong Chen highlights, however, that among some more established Cabang Atas families, commercial activities as represented by the revenue farms were looked down upon, hence a gradual shift towards landownership and agriculture. In any case, the accumulation of great fortunes among Cabang Atas families received the protection of Dutch colonial law. This legal certainty gave a firm basis to the creation of long-lasting bureaucratic and landowning dynasties of great wealth in colonial Indonesia that were not as common in pre-revolutionary China.\n\nEthnically and culturally, families of the Cabang Atas were overwhelmingly creolised 'Peranakan Chinese'. There was extensive intermarriage between Cabang Atas families in order to consolidate their political power and influence, as well as estates and fortunes. Social mobility, however, was possible; Cabang Atas families sometimes took in successful \"totok\", or newly arrived, settlers as sons-in-law. As cited by the historian Ong Hok Ham, notable examples included the late nineteenth-century, \"totok\" businessman Oei Tjie Sien (1835–1900), who married a middle-class Peranakan woman; and the latter's Peranakan son Oei Tiong Ham, Majoor der Chinezen (1866–1924), who firmly sealed the family's social ascent by marrying into the Cabang Atas and by his eventual elevation to the Chinese officership.\n\nIn the early twentieth century, in keeping with their so-called 'Ethical Policy', the Dutch colonial authorities made a concerted effort to appoint government officials, including Chinese officers, based on merit rather than family background. Some of these candidates came from outside traditional Cabang Atas families, including totok appointees, such as Tjong A Fie, \"Majoor der Chinezen\" (1860–1921) in Medan and Lie Hin Liam, \"Luitenant der Chinezen\" in Tangerang and Khoe A Fan, \"Luitenant der Chinezen\" in Batavia. Nonetheless, descendants of the Cabang Atas continued to feature prominently in the officership until the end of colonial rule: for example, Han Tjiong Khing, the last Majoor der Chinezen of Surabaya, was a direct descendant of Han Bwee Kong, the city's first Dutch-appointed Kapitein der Chinezen. \n\nBeyond the Chinese officership, members of the Cabang Atas took a leading role in the emerging modernization movement and modern politics of the late colonial period. The influential Confucian and educational organization Tiong Hoa Hwee Koan, founded in 1900, was headed for many decades by its founding President, Phoa Keng Hek Sia, scion of a Cabang Atas family, and dominated by others of Phoa's class and background. The aim of the organization was to renew and purify the practice of Confucianism in the Dutch East Indies, and to introduce modern educational opportunities to the colony's Chinese subjects.\n\nPolitically, the Cabang Atas was mainly associated with Chung Hwa Hui or CHH, a modern political party that was seen as the mouthpiece of the colonial Chinese establishment. CHH's chairman was none other than Majoor Han Tjiong Khing's distant cousin, the Dutch-educated landlord H. H. Kan, a doyen of the Cabang Atas and landowning gentry of Batavia. CHH representatives in Indonesia's first legislature, the Volksraad, were largely scions of the Cabang Atas: presided by Kan, they included Jo Heng Kam, Luitenant der Chinezen, Loa Sek Hie and Han Tiauw Tjong. Due to their largely establishment background, progressive elements dubbed CHH's parliamentary arm as the 'Packard group' after the expensive cars many of them used.\n\nDue to their close proximity to Dutch colonial authorities, many families of the Cabang Atas were early adopters of the Dutch language and many European cultural and social mores. European education and westernisation among the Cabang Atas began in the second half of the nineteenth century, and became the norm by the beginning of the twentieth century. While tying them ever closer to the colonial authorities, the European outlook of the class put them at odds with the overwhelming majority of the Chinese-Indonesian population they had traditionally led. \n\nAlready attacked for their perceived Dutch sympathies in the late colonial period, the Cabang Atas bore the brunt of the Indonesian Revolution from 1945 until 1949. The end of Dutch colonial rule in 1950 saw the exile and emigration of many families of the Cabang Atas. The turbulent early decades of Indonesian independence also ensured an end to their centuries-long dominating and privileged position in Indonesian political, economic and social life.\n\n"}
{"id": "18246352", "url": "https://en.wikipedia.org/wiki?curid=18246352", "title": "Capital call", "text": "Capital call\n\nA capital call (also known as a draw down or a capital commitment) is a legal right of an investment firm or an insurance firm to demand a portion of the money promised to it by an investor. A capital call fund would be the money that had been committed to the fund. The capital call is the act of actually transferring the promised funds to the investment target. A capital call agreement defines capital call terms.\n\nFor example, when an investor buys into a real estate fund, that fund's managers may wait some time before using the investor's money to buy real estate, either because they are waiting for real estate prices to be favorable, or because they are researching new deals. When they are ready to buy real estate, the fund managers issue a capital call, requiring investors who have committed money to the fund to transfer that money over.\n\nThe fund might also borrow funds instead of using the investor's money. This allows the fund to benefit from leverage. The financing of the real estate purchase is realized through borrowing from banks. When the fund has reached a certain level of return, capital calls are issued and the borrowing is paid off.\n\n"}
{"id": "13285352", "url": "https://en.wikipedia.org/wiki?curid=13285352", "title": "Chilling requirement", "text": "Chilling requirement\n\nThe chilling requirement of a fruit is the minimum period of cold weather after which a fruit-bearing tree will blossom. It is often expressed in chill hours, which can be calculated in different ways, all of which essentially involve adding up the total amount of time in a winter spent at certain temperatures.\n\nSome bulbs have chilling requirements to bloom, and some seeds have chilling requirements to sprout.\n\nBiologically, the chilling requirement is a way of ensuring that vernalization occurs.\n\nA \"chilling unit\" in agriculture is a metric of a plant's exposure to chilling temperatures. Chilling temperatures extend from freezing point to, depending on the model, or even . Stone fruit trees and certain other plants of temperate climate develop next year's buds in the summer. In the autumn the buds become dormant, and the switch to proper, healthy dormancy is triggered by a certain minimum exposure to chilling temperatures. Lack of such exposure results in delayed and substandard foliation, flowering and fruiting. One chilling unit, in the simplest models, is equal to one hour's exposure to the chilling temperature; these units are summed up for a whole season. Advanced models assign different weights to different temperature bands.\n\nAccording to Fishman, chilling in trees acts in two stages. The first is reversible: chilling helps to build up the precursor to dormancy, but the process can be easily reversed with a rise in temperature. After the level of precursor reaches a certain threshold, dormancy becomes irreversible and will not be affected by short-term warm temperature peaks. Apples have the highest chilling requirements of all fruit trees, followed by apricots and, lastly, peaches. Apple cultivars have a diverse range of permissible minimum chilling: most have been bred for temperate weather, but \"Gala\" and \"Fuji\" can be successfully grown in subtropical Bakersfield, California.\n\nPeach cultivars in Texas range in their requirements from 100 chilling units (\"Florida Grande\" cultivar, zoned for low chill regions) to 1,000 units (\"Surecrop\", zoned for high chill regions). Planting a low-chilling cultivar in a high-chill region risks loss of a year's harvest when an early bloom is hit by a spring frost. A high-chilling cultivar planted in a low-chill region will, quite likely, never fruit at all. A four-year study of \"Ruston Red\" Alabama peach, which has a threshold of 850 chilling units, demonstrated that a seasonal chilling deficiency of less than 50 units has no effect on harvest. Deficiency of 50 to 100 units may result in loss of up to 50% of expected harvest. Deficiency of 250 hours and more is a sure loss of practically whole harvest; the few fruit will be of very poor quality and have no market value. Rest-breaking agents (e.g. hydrogen cyanamide, trade name \"BudPro or Dormex\"), applied in spring, can partially mitigate the effects of insufficient chilling. BudPro can substitute for up to 300 hours of chilling, but an excessive spraying and timing error can easily damage the buds.\nOther products such as Dormex use stabilizing compounds.\n\nChilling of orange trees has two effects. First, it increases production of carotenoids and decreases chlorophyll content of the fruit, improving their appearance and, ultimately, their market value. Second, the \"quasi-dormancy\" experienced by orange trees triggers concentrated flowering in spring, as opposed to more or less uniform round-the-year flowering and fruiting in warmer climates.\n\nBiennial plants like cabbage, sugar beet, celery and carrots need chilling to develop second-year flowering buds. Excessive chilling in the early stages of a sugar beet seedling, on the contrary, may trigger undesired growth of a flowering stem (bolting) in its first year. This phenomenon has been offset by breeding sugar beet cultivars with a higher minimum chilling threshold. Such cultivars can be seeded earlier than normal without the risk of bolting.\n\nAll models require hourly recording of temperatures. The simplest model assigns one chilling unit for every full hour at temperatures below . A slightly more sophisticated model excludes freezing temperatures, which do not contribute to proper dormancy cycle, and counts only hours with temperatures between and .\nThe Utah model assigns different weight to different temperature bands; a full unit per hour is assigned only to temperatures between and . Maximum effect is achieved at . Temperatures between and (the threshold between chilling and warm weather) have zero weight, and higher temperature have negative weights: they reduce the beneficial effects of an already accumulated chilling hours.\n\nSouthwick et al. wrote that neither of these models is accurate enough to account for application of rest-breaking agents widely used in modern farming. They advocated the use of a dynamic model tailored to the two-stage explanation of dormancy.\n\n"}
{"id": "32126830", "url": "https://en.wikipedia.org/wiki?curid=32126830", "title": "Danish bicycle VIN-system", "text": "Danish bicycle VIN-system\n\nThe Danish bicycle VIN-system is a system introduced in 1942 by the Danish government, providing all bicycles in Denmark with a unique code. The code is a combination of letters and digits embedded into the bicycle frame and consists of a manufacturer code, a serial number, and construction year code.\n\nSince 1948, it has been illegal to sell bicycle frames in Denmark without an embedded VIN. Because of this, insurance companies in Denmark will not pay indemnities for stolen bicycles without a VIN.\n\nBy default, the VIN is to be engraved into the seat tube or the down tube, but if these are made of such a material that hinders this, it may alternately be put on the bottom bracket shell. In very special instances, and only with the approval of the Danish National Police Commissioner’s Office, it may be applied by other means and on other locations.\n\nThe bicycle VIN is constructed of three elements: a letter-block, a digit-block and a letter-block:\n\nThe manufacturers-code, is a 1, 2, 3 or 4-letter block. If the code starts with a W, it is an imported bicycle frame. The second block is the frame serial number from that manufacturer. This shall have at least one digit, but can otherwise have any number of digits. The third block is a single letter code that identifies the production year. Certain limits regarding letters are instated for the VIN:\n\nIn the examples above, the first bicycle is a bicycle imported by FDB, with serial number \"1234\" and made in either 1963, 1984 or 2005. The second bike is a SCO bicycle, with serial number \"57\", made in 1942, 1964, 1985 or 2006.\n\nThis table shows examples of manufacturer codes, but is in no way to be considered a complete listing.\n\n"}
{"id": "412326", "url": "https://en.wikipedia.org/wiki?curid=412326", "title": "Democratic peace theory", "text": "Democratic peace theory\n\nDemocratic peace theory is a theory which posits that democracies are hesitant to engage in armed conflict with other identified democracies. In contrast to theories explaining war engagement, it is a \"theory of peace\" outlining motives that dissuade state-sponsored violence.\n\nSome theorists prefer terms such as \"mutual democratic pacifism\" or \"inter-democracy nonaggression hypothesis\" so as to clarify that a state of peace is not singular to democracies, but rather that it is easily sustained between democratic nations.\n\nAmong proponents of the democratic peace theory, several factors are held as motivating peace between democratic states:\n\nThose who dispute this theory often do so on grounds that it conflates correlation with causation, and that the academic definitions of 'democracy' and 'war' can be manipulated so as to manufacture an artificial trend.\n\nThough the democratic peace theory was not rigorously or scientifically studied until the 1960s, the basic principles of the concept had been argued as early as the 1700s in the works of philosopher Immanuel Kant and political theorist Thomas Paine. Kant foreshadowed the theory in his essay \"\" written in 1795, although he thought that a world with only constitutional republics was only one of several necessary conditions for a perpetual peace. Kant's theory was that a majority of the people would never vote to go to war, unless in self-defense. Therefore, if all nations were republics, it would end war, because there would be no aggressors. In earlier but less cited works, Thomas Paine made similar or stronger claims about the peaceful nature of republics. Paine wrote in \"Common Sense\" in 1776: \"The Republics of Europe are all (and we may say always) in peace.\" Paine argued that kings would go to war out of pride in situations where republics would not. French historian and social scientist Alexis de Tocqueville also argued, in \"Democracy in America\" (1835–1840), that democratic nations were less likely to wage war.\n\nDean Babst, a criminologist, was the first to do statistical research on this topic. His academic paper supporting the theory was published in 1964 in \"Wisconsin Sociologist\"; he published a slightly more popularized version, in 1972, in the trade journal \"Industrial Research\". Both versions initially received little attention.\n\nMelvin Small and J. David Singer (1976: 50–69) responded; they found an absence of wars between democratic states with two \"marginal exceptions\", but denied that this pattern had statistical significance. This paper was published in the \"Jerusalem Journal of International Relations\" which finally brought more widespread attention to the theory, and started the academic debate. A 1983 paper by political scientist Michael W. Doyle contributed further to popularizing the theory. Rudolph J. Rummel was another early researcher and drew considerable lay attention to the subject in his later works.\n\nMaoz & Abdolali (1989) extended the research to lesser conflicts than wars. Bremer (1992) and Maoz & Russett (1993) found the correlation between democracy and peacefulness remained significant after controlling for many possible confounding variables. This moved the theory into the mainstream of social science. Supporters of realism in international relations and others responded by raising many new objections. Other researchers attempted more systematic explanations of how democracy might cause peace (Köchler 1995), and of how democracy might also affect other aspects of foreign relations such as alliances and collaboration (Ray 2003).\n\nThere have been numerous further studies in the field since these pioneering works. Most studies have found some form of democratic peace exists, although neither methodological disputes nor doubtful cases are entirely resolved (Kinsella 2005).\n\nResearch on the democratic peace theory has to define \"democracy\" and \"peace\" (or, more often, \"war\").\n\nDemocracies have been defined differently by different theorists and researchers; this accounts for some of the variations in their findings. Some examples:\n\nSmall and Singer (1976) define democracy as a nation that (1) holds periodic elections in which the opposition parties are as free to run as government parties, (2) allows at least 10% of the adult population to vote, and (3) has a parliament that either controls or enjoys parity with the executive branch of the government.\n\nDoyle (1983) requires (1) that \"liberal regimes\" have market or private property economics, (2) they have policies that are internally sovereign, (3) they have citizens with juridical rights, and (4) they have representative governments. Either 30% of the adult males were able to vote or it was possible for every man to acquire voting rights as by attaining enough property. He allows greater power to hereditary monarchs than other researchers; for example, he counts the rule of Louis-Philippe of France as a liberal regime.\n\nRay (1995) requires that at least 50% of the adult population is allowed to vote and that there has been at least one peaceful, constitutional transfer of executive power from one independent political party to another by means of an election. This definition excludes long periods often viewed as democratic. For example, the United States until 1800, India from independence until 1979, and Japan until 1993 were all under one-party rule, and thus would not be counted under this definition (Ray 1995, p. 100).\n\nRummel (1997) states that \"By democracy is meant liberal democracy, where those who hold power are elected in competitive elections with a secret ballot and wide franchise (loosely understood as including at least 2/3 of adult males); where there is freedom of speech, religion, and organization; and a constitutional framework of law to which the government is subordinate and that guarantees equal rights.\"\n\nThe above definitions are binary, classifying nations into either democracies or non-democracies. Many researchers have instead used more finely grained scales. One example is the Polity data series which scores each state on two scales, one for democracy and one for autocracy, for each year since 1800; as well as several others. The use of the Polity Data has varied. Some researchers have done correlations between the democracy scale and belligerence; others have treated it as a binary classification by (as its maker does) calling all states with a high democracy score and a low autocracy score democracies; yet others have used the difference of the two scores, sometimes again making this into a binary classification (Gleditsch 1992).\n\nSeveral researchers have observed that many of the possible exceptions to the democratic peace have occurred when at least one of the involved democracies was very young. Many of them have therefore added a qualifier, typically stating that the peacefulness apply to democracies older than three years (Doyle 1983, Russett 1993, Rummel 1997, Weart 1998). Rummel (1997) argues that this is enough time for \"democratic procedures to be accepted, and democratic culture to settle in.\" Additionally, this may allow for other states to actually come to the recognition of the state as a democracy.\n\nMansfield and Snyder (2002, 2005), while agreeing that there have been no wars between mature liberal democracies, state that countries in transition to democracy are especially likely to be involved in wars. They find that democratizing countries are even more warlike than stable democracies, stable autocracies or even countries in transition towards autocracy. So, they suggest caution in eliminating these wars from the analysis, because this might hide a negative aspect of the process of democratization. A reanalysis of the earlier study's statistical results emphasizes that the above relationship between democratization and war can only be said to hold for those democratizing countries where the executive lacks sufficient power, independence, and institutional strength. A review cites several other studies finding that the increase in the risk of war in democratizing countries happens only if many or most of the surrounding nations are undemocratic. If wars between young democracies are included in the analysis, several studies and reviews still find enough evidence supporting the stronger claim that all democracies, whether young or established, go into war with one another less frequently (Ray 1998), (Ray 2003), , while some do not .\n\nQuantitative research on international wars usually define war as a military conflict with more than 1000 killed in battle in one year. This is the definition used in the Correlates of War Project which has also supplied the data for many studies on war. It turns out that most of the military conflicts in question fall clearly above or below this threshold (Ray 1995, p. 103).\n\nSome researchers have used different definitions. For example, Weart (1998) defines war as more than 200 battle deaths. Russett (1993, p. 50), when looking at Ancient Greece, only requires some real battle engagement, involving on both sides forces under state authorization.\n\nMilitarized Interstate Disputes (MIDs), in the Correlates of War Project classification, are lesser conflicts than wars. Such a conflict may be no more than military display of force with no battle deaths. MIDs and wars together are \"militarized interstate conflicts\" or MICs. MIDs include the conflicts that precede a war; so the difference between MIDs and MICs may be less than it appears.\n\nStatistical analysis and concerns about degrees of freedom are the primary reasons for using MID's instead of actual wars. Wars are relatively rare. An average ratio of 30 MIDs to one war provides a richer statistical environment for analysis.\n\nMost research is regarding the \"dyadic\" peace, that democracies do not fight one another. Very few researchers have supported the \"monadic\" peace, that democracies are more peaceful in general. There are some recent papers that find a slight monadic effect. Müller and Wolff (2004), in listing them, agree \"that democracies on average might be slightly, but not strongly, less warlike than other states,\" but general \"monadic explanations is neither necessary nor convincing.\" They note that democracies have varied greatly in their belligerence against non-democracies.\n\nSome scholars support the democratic peace on probabilistic grounds: since many wars have been fought since democracies first arose, we might expect a proportionate number of wars to have occurred between democracies, if democracies fought each other as freely as other pairs of states; but proponents of democratic peace theory claim that the number is much less than might be expected. However, opponents of the theory argue this is mistaken and claim there are numerous examples of wars between democracies.\n\nHistorically, cases commonly cited as exceptions include the Sicilian Expedition, the Spanish–American War, and more recently the Kargil War. Doyle (1983) cites the Paquisha War and the Lebanese air force's intervention in the Six-Day War. The total number of cases suggested in the literature is at least 50. The data set Bremer (1993) was using showed one exception, the French-Thai War of 1940; Gleditsch (1995) sees the (somewhat technical) state of war between Finland and UK during World War II, as a special case, which should probably be treated separately: an incidental state of war between democracies during large multi-polar wars (Gowa 1999; Maoz 1997, p. 165). However, the UK did bomb Finland, implying the war was not only on paper. Page Fortna (2004) discusses the 1974 Turkish invasion of Cyprus and the Kargil War as exceptions, finding the latter to be the most significant. However, the status of these countries as being truly democratic is a matter of debate. For instance, in Spain in 1898, two parties alternated in the government in a controlled process known as \"el turno pacífico\", and the caciques, powerful local figures, were used to manipulate election results, and as a result resentment of the system slowly built up over time and important nationalist movements as well as unions started to form. Similarly, the Turkish intervention in Cyprus occurred only after the Cypriot elected government was abolished in a coup sponsored by the military government of Greece.\n\nLimiting the theory to only truly stable and genuine democracies leads to a very restrictive set of highly prosperous nations with little incentive in armed conflict that might harm their economies, in which the theory might be expected to hold virtually by definition.\n\nOne advocate of the democratic peace explains that his reason to choose a definition of democracy sufficiently restrictive to exclude \"all\" wars between democracies are what \"might be disparagingly termed \"public relations\"\": students and politicians will be more impressed by such a claim than by claims that wars between democracies are less likely.\n\nOne problem with the research on wars is that, as the Realist Mearsheimer (1990, p. 50) put it, \"democracies have been few in number over the past two centuries, and thus there have been few opportunities where democracies were in a position to fight one another\". Democracies have been very rare until recently. Even looser definitions of democracy, such as Doyle's, find only a dozen democracies before the late nineteenth century, and many of them short-lived or with limited franchise (Doyle 1983), (Doyle 1997, p. 261). Freedom House finds no independent state with universal suffrage in 1900.\n\nWayman (1998), a supporter of the theory, states that \"If we rely solely on whether there has been an inter-democratic war, it is going to take many more decades of peace to build our confidence in the stability of the democratic peace\".\n\nMany researchers have reacted to this limitation by studying lesser conflicts instead, since they have been far more common. There have been many more MIDs than wars; the Correlates of War Project counts several thousand during the last two centuries. A review lists many studies that have reported that democratic pairs of states are less likely to be involved in MIDs than other pairs of states.\n\nAnother study finds that after both states have become democratic, there is a decreasing probability for MIDs within a year and this decreases almost to zero within five years.\n\nWhen examining the inter-liberal MIDs in more detail, one study finds that they are less likely to involve third parties, and that the target of the hostility is less likely to reciprocate, if the target reciprocates the response is usually proportional to the provocation, and the disputes are less likely to cause any loss of life. The most common action was \"Seizure of Material or Personnel\".\n\nStudies find that the probability that disputes between states will be resolved peacefully is positively affected by the degree of democracy exhibited by the lesser democratic state involved in that dispute. Disputes between democratic states are significantly shorter than disputes involving at least one undemocratic state. Democratic states are more likely to be amenable to third party mediation when they are involved in disputes with each other .\n\nIn international crises that include the threat or use of military force, one study finds that if the parties are democracies, then relative military strength has no effect on who wins. This is different from when nondemocracies are involved. These results are the same also if the conflicting parties are formal allies . Similarly, a study of the behavior of states that joined ongoing militarized disputes reports that power is important only to autocracies: democracies do not seem to base their alignment on the power of the sides in the dispute .\n\nAccording to a 2017 review study, \"there is enough evidence to conclude that democracy does cause peace at least between democracies, that the observed correlation between democracy and peace is not spurious.\"\n\nMost studies have looked only at who is involved in the conflicts and ignored the question of who initiated the conflict. In many conflicts both sides argue that the other side was initiator. Several researchers, as described in (Gleditsch, Christiansen & Hegre 2004), have argued that studying conflict initiation is of limited value, because existing data about conflict initiation may be especially unreliable. Even so, several studies have examined this. Reiter and Stam (2003) argue that autocracies initiate conflicts against democracies more frequently than democracies do against autocracies. Quackenbush and Rudy (2006), while confirming Reiter and Stam's results, find that democracies initiate wars against nondemocracies more frequently than nondemocracies do to each other. Several following studies (Peceny & Beer 2003), (Peceny & Butler 2004), (Lai & Slater 2006) have studied how different types of autocracies with different institutions vary regarding conflict initiation. Personalistic and military dictatorships may be particularly prone to conflict initiation, as compared to other types of autocracy such as one party states, but also more likely to be targeted in a war having other initiators.\n\nOne 2017 study found that democracies are no less likely to settle border disputes peacefully than non-democracies.\n\nMost of this article discusses research on relations between states. However, there is also evidence that democracies have less internal systematic violence. For instance, one study finds that the most democratic and the most authoritarian states have few civil wars, and intermediate regimes the most. The probability for a civil war is also increased by political change, regardless whether toward greater democracy or greater autocracy. Intermediate regimes continue to be the most prone to civil war, regardless of the time since the political change. In the long run, since intermediate regimes are less stable than autocracies, which in turn are less stable than democracies, durable democracy is the most probable end-point of the process of democratization . Abadie (2004) study finds that the most democratic nations have the least terrorism. Harff (2003) finds that genocide and politicide are rare in democracies. Rummel (1997) finds that the more democratic a regime, the less its democide. He finds that democide has killed six times as many people as battles.\n\nDavenport and Armstrong (2004) lists several other studies and states: \"Repeatedly, democratic political systems have been found to decrease political bans, censorship, torture, disappearances and mass killing, doing so in a linear fashion across diverse measurements, methodologies, time periods, countries, and contexts.\" It concludes: \"Across measures and methodological techniques, it is found that below a certain level, democracy has no impact on human rights violations, but above this level democracy influences repression in a negative and roughly linear manner.\" Davenport and Armstrong (2003) states that thirty years worth of statistical research has revealed that only two variables decrease human rights violations: political democracy and economic development.\n\nAbulof and Ogen add a caveat, focusing on the contemporary Middle East and North Africa (MENA). Statistically, a MENA democracy makes a country more prone to both the onset and incidence of civil war, and the more democratic a MENA state is, the more likely it is to experience violent intrastate strife. Moreover, anocracies do not seem to be predisposed to civil war, either worldwide or in MENA. Looking for causality beyond correlation, they suggest that democracy’s pacifying effect is partly mediated through societal subscription to self-determination and popular sovereignty. This may turn “democratizing nationalism” to a long-term prerequisite, not just an immediate hindrance, to peace and democracy.\n\nThese theories have traditionally been categorized into two groups: explanations that focus on democratic norms and explanations that focus on democratic\npolitical structures , . Note that they usually are meant to be explanations for little violence between democracies, not for a low level of internal violence in democracies.\n\nSeveral of these mechanisms may also apply to countries of similar systems. The book \"Never at War\" finds evidence for an oligarchic peace. One example is the Polish–Lithuanian Commonwealth, in which the Sejm resisted and vetoed most royal proposals for war, like those of Władysław IV Vasa.\n\nOne example from the first group is that liberal democratic culture may make the leaders accustomed to negotiation and compromise , (Müller & Wolff 2004). Another that a belief in human rights may make people in democracies reluctant to go to war, especially against other democracies. The decline in colonialism, also by democracies, may be related to a change in perception of non-European peoples and their rights .\n\nBruce Russett (1993, p. 5–11, 35, 59–62, 73–4) also argues that the democratic culture affects the way leaders resolve conflicts. In addition, he holds that a social norm emerged toward the end of the nineteenth century; that democracies should not fight each other, which strengthened when the democratic culture and the degree of democracy increased, for example by widening the franchise. Increasing democratic stability allowed partners in foreign affairs to perceive a nation as reliably democratic. The alliances between democracies during the two World Wars and the Cold War also strengthened the norms. He sees less effective traces of this norm in Greek antiquity.\n\nHans Köchler (1995) relates the question of transnational democracy to empowering the individual citizen by involving him, through procedures of direct democracy, in a country's international affairs, and he calls for the restructuring of the United Nations Organization according to democratic norms. He refers in particular to the Swiss practice of participatory democracy.\n\nMousseau (2000, 2005) argues that it is market-oriented development that creates the norms and values that explain both democracy and the peace. In less developed countries individuals often depend on social networks that impose conformity to in-group norms and beliefs, and loyalty to group leaders. When jobs are plentiful on the market, in contrast, as in market-oriented developed countries, individuals depend on a strong state that enforces contracts equally. Cognitive routines emerge of abiding by state law rather than group leaders, and, as in contracts, tolerating differences among individuals. Voters in marketplace democracies thus accept only impartial ‘liberal’ governments, and constrain leaders to pursue their interests in securing equal access to global markets and in resisting those who distort such access with force. Marketplace democracies thus share common foreign policy interests in the supremacy—and predictability—of international law over brute power politics, and equal and open global trade over closed trade and imperial preferences. When disputes do originate between marketplace democracies, they are less likely than others to escalate to violence because both states, even the stronger one, perceive greater long-term interests in the supremacy of law over power politics.\n\nThe case for institutional constraints goes back to Kant (1795), who wrote:\n\nDemocracy thus gives influence to those most likely to be killed or wounded in wars, and their relatives and friends (and to those who pay the bulk of the war taxes) Russett (1993, p. 30). This monadic theory must, however, explain why democracies do attack non-democratic states. One explanation is that these democracies were threatened or otherwise were provoked by the non-democratic states. Doyle (1997, p. 272) argued that the absence of a monadic peace is only to be expected: the same ideologies that cause liberal states to be at peace with each other inspire idealistic wars with the illiberal, whether to defend oppressed foreign minorities or avenge countrymen settled abroad. Doyle also notes (p. 292) liberal states do conduct covert operations against each other; the covert nature of the operation, however, prevents the publicity otherwise characteristic of a free state from applying to the question\n\nStudies show that democratic states are more likely than autocratic states to win the wars. One explanation is that democracies, for internal political and economic reasons, have greater resources. This might mean that democratic leaders are unlikely to select other democratic states as targets because they perceive them to be particularly formidable opponents. One study finds that interstate wars have important impacts on the fate of political regimes, and that the probability that a political leader will fall from power in the wake of a lost war is particularly high in democratic states .\n\nAs described in , several studies have argued that liberal leaders face institutionalized constraints that impede their capacity to mobilize the state’s resources for war without the consent of a broad spectrum of interests. Survey results that compare the attitudes of citizens and elites in the Soviet successor states are consistent with this argument . Moreover, these constraints are readily apparent to other states and cannot be manipulated by leaders. Thus, democracies send credible signals to other states of an aversion to using force. These signals allow democratic states to avoid conflicts with one another, but they may attract aggression from nondemocratic states. Democracies may be pressured to respond to such aggression—perhaps even preemptively—through the use of force. Also as described in , studies have argued that when democratic leaders do choose to escalate international crises, their threats are taken as highly credible, since there must be a relatively large public opinion for these actions. In disputes between liberal states, the credibility of their bargaining signals allows them to negotiate a peaceful settlement before mobilization.\n\nAn explanation based on game theory similar to the last two above is that the participation of the public and the open debate send clear and reliable information regarding the intentions of democracies to other states. In contrast, it is difficult to know the intentions of nondemocratic leaders, what effect concessions will have, and if promises will be kept. Thus there will be mistrust and unwillingness to make concessions if at least one of the parties in a dispute is a nondemocracy .\n\nThe risk factors for certain types of state have, however, changed since Kant's time. In the quote above, Kant points to the lack of popular support for war – first that the populace will directly or indirectly suffer in the event of war – as a reason why republics will not tend to go to war. The number of American troops killed or maimed versus the number of Iraqi soldiers and civilians maimed and killed in the American-Iraqi conflict is indicative. This may explain the relatively great willingness of democratic states to attack weak opponents: the Iraq war was, initially at least, highly popular in the United States. The case of the Vietnam War might, nonetheless, indicate a tipping point where publics may no longer accept continuing attrition of their soldiers (even while remaining relatively indifferent to the much higher loss of life on the part of the populations attacked).\n\nColeman (2002) uses economic cost-benefit analysis to reach conclusions similar to Kant's. Coleman examines the polar cases of autocracy and liberal democracy. In both cases, the costs of war are assumed to be borne by the people. In autocracy, the autocrat receives the entire benefits of war, while in a liberal democracy the benefits are dispersed among the people. Since the net benefit to an autocrat exceeds the net benefit to a citizen of a liberal democracy, the autocrat is more likely to go to war. The disparity of benefits and costs can be so high that an autocrat can launch a welfare-destroying war when his net benefit exceeds the total cost of war. Contrarily, the net benefit of the same war to an individual in a liberal democracy can be negative so that he would not choose to go to war. This disincentive to war is increased between liberal democracies through their establishment of linkages, political and economic, that further raise the costs of war between them. Therefore, liberal democracies are less likely to go war, especially against each other. Coleman further distinguishes between offensive and defensive wars and finds that liberal democracies are less likely to fight defensive wars that may have already begun due to excessive discounting of future costs.\n\nThere are several logically distinguishable classes of criticism. Note that they usually apply to no wars or few MIDs between democracies, not to little systematic violence in established democracies. (But see List of wars between democracies.)\n\nOne study has argued that there have been as many wars between democracies as one would expect between any other couple of states. However, its authors include wars between young and dubious democracies, and very small wars.\n\nOthers , , state that, although there may be some evidence for democratic peace, the data sample or the time span may be too small to assess any definitive conclusions. For example, Gowa finds evidence for democratic peace to be insignificant before 1939, because of the too small number of democracies, and offers an alternate explanation for the following period (see the section on Realist Explanations). Gowa's use of statistics has been criticized, with several other studies and reviews finding different or opposing results , . However, this can be seen as the longest-lasting criticism to the theory; as noted earlier, also some supporters agree that the statistical sample for assessing its validity is limited or scarce, at least if only full-scale wars are considered.\n\nAccording to one study, which uses a rather restrictive definition of democracy and war, there were no wars between jointly democratic couples of states in the period from 1816 to 1992. Assuming a purely random distribution of wars between states, regardless of their democratic character, the predicted number of conflicts between democracies would be around ten. So, Ray argues that the evidence is statistically significant, but that it is still conceivable that, in the future, even a small number of inter-democratic wars would cancel out such evidence.\n\nDouglas M. Gibler and Andrew Owsiak in their study argued peace almost always comes before democracy and that states do not develop democracy until all border disputes have been settled. These studies indicate that there is strong evidence that peace causes democracy but little evidence that democracy causes peace. Azar Gat (2017) argues that it is not democracy in itself that leads to peace but other aspects of modernization, such as economic prosperity and lower population growth.\n\nThe hypothesis that peace causes democracy is supported by psychological and cultural theories. Christian Welzel's human empowerment theory posits that existential security leads to emancipative cultural values and support for a democratic political organization. This is in agreement with theories based on evolutionary psychology.\n\nSeveral studies fail to confirm that democracies are less likely to wage war than autocracies if wars against non-democracies are included.\n\nSome authors criticize the definition of democracy by arguing that states continually reinterpret other states' regime types as a consequence of their own objective interests and motives, such as economic and security concerns . For example, one study reports that Germany was considered a democratic state by Western opinion leaders at the end of the 19th century; yet in the years preceding World War I, when its relations with the United States, France and Britain started deteriorating, Germany was gradually reinterpreted as an autocratic state, in absence of any actual regime change. Shimmin moves a similar criticism regarding the western perception of Milosevic's Serbia between 1989 and 1999. Rummel replies to this criticism by stating that, in general, studies on democratic peace do not focus on other countries' perceptions of democracy; and in the specific case of Serbia, by arguing that the limited credit accorded by western democracies to Milosevic in the early '90s did not amount to a recognition of democracy, but only to the perception that possible alternative leaders could be even worse.\n\nSome democratic peace researchers have been criticized for \"post hoc\" reclassifying some specific conflicts as non-wars or political systems as non-democracies without checking and correcting the whole data set used similarly. Supporters and opponents of the democratic peace agree that this is bad use of statistics, even if a plausible case can be made for the correction (Bremer 1992), (Gleditsch 1995), (Gowa 1999). A military affairs columnist of the newspaper \"Asia Times\" has summarized the above criticism in a journalist's fashion describing the theory as subject to the no true Scotsman problem: exceptions are explained away as not being between \"real\" democracies or \"real\" wars.\n\nSome democratic peace researchers require that the executive result from a substantively contested election. This may be a restrictive definition: For example, the National Archives of the United States notes that \"For all intents and purposes, George Washington was unopposed for election as President, both in 1789 and 1792\". (Under the original provisions for the Electoral College, there was no distinction between votes for President and Vice-President: each elector was required to vote for two distinct candidates, with the runner-up to be Vice-President. Every elector cast one of his votes for Washington, John Adams received a majority of the other votes; there were several other candidates: so the election for Vice President was contested.)\n\nSpiro (1994) made several other criticisms of the statistical methods used. Russett (1995) and a series of papers described by Ray (2003) responded to this, for example with different methodology.\n\nSometimes the datasets used have also been criticized. For example, some authors have criticized the Correlates of War data for not including civilian deaths in the battle deaths count, especially in civil wars . Weeks and Cohen (2006) argue that most fishing disputes, which include no deaths and generally very limited threats of violence, should be excluded even from the list of military disputes. Gleditsch (2004) made several criticisms to the Correlates of War data set, and produced a revised set of data. Maoz and Russett (1993) made several criticisms to the Polity I and II data sets, which have mostly been addressed in later versions. These criticisms are generally considered minor issues.\n\nThe most comprehensive critique points out that \"democracy\" is rarely defined, never refers to substantive democracy, is unclear about causation, has been refuted in more than 100 studies, fails to account for some 200 deviant cases, and has been promoted ideologically to justify one country seeking to expand democracy abroad (Haas 2014). Most studies treat the complex concept of \"democracy\" is a bivariate variable rather than attempting to dimensionalize the concept. Studies also fail to take into account the fact that there are dozens of types of democracy, so the results are meaningless unless articulated to a particular type of democracy or claimed to be true for all types, such as consociational or economic democracy, with disparate datasets.\n\nThe peacefulness may have various limitations and qualifiers and may not actually mean very much in the real world.\n\nDemocratic peace researchers do in general not count as wars conflicts which do not kill a thousand on the battlefield; thus they exclude for example the bloodless Cod Wars. However, as noted earlier, research has also found a peacefulness between democracies when looking at lesser conflicts.\n\nDemocracies were involved in more colonial and imperialistic wars than other states during the 1816–1945 period. On the other hand, this relation disappears if controlling for factors like power and number of colonies. Liberal democracies have less of these wars than other states after 1945. This might be related to changes in the perception of non-European peoples, as embodied in the Universal Declaration of Human Rights (Ravlo & Glieditsch 2000).\n\nRelated to this is the human rights violations committed against native people, sometimes by liberal democracies. One response is that many of the worst crimes were committed by nondemocracies, like in the European colonies before the nineteenth century, in King Leopold II of Belgium's privately owned Congo Free State, and in Joseph Stalin's Soviet Union. The United Kingdom abolished slavery in British territory in 1833, immediately after the Reform Act 1832 had significantly enlarged the franchise. (Of course, the abolition of the slave trade had been enacted in 1807; and many DPT supporters would deny that the UK was a liberal democracy in 1833 when examining interstate wars.)\n\nHermann and Kegley (1995) argue that interventions between democracies are more likely to happen than projected by an expected model. They further argue (1996) that democracies are more likely to intervene in other liberal states than against countries that are non-democracies. Finally, they argue that these interventions between democracies have been increasing over time and that the world can expect more of these interventions in the future. The methodology used has been criticized and more recent studies have found opposing results (Gleditsch, Christiansen & Hegre 2004).\n\nRummel argues that the continuing increase in democracy worldwide will soon lead to an end to wars and democide, possibly around or even before the middle of this century. The fall of Communism and the increase in the number of democratic states were accompanied by a sudden and dramatic decline in total warfare, interstate wars, ethnic wars, revolutionary wars, and the number of refugees and displaced persons. One report claims that the two main causes of this decline in warfare are the end of the Cold War itself and decolonization; but also claims that the three Kantian factors have contributed materially.\n\nEconomic historians Joel Mokyr and Hans-Joachim Voth argue that democratic states may have been more vulnerable to conquest because the rulers in those states were too heavily constrained. Absolutist rulers in other states could however operate more effectively.\n\nDemocratic peace theory is a well established research field with more than a hundred authors having published articles about it. Several peer-reviewed studies mention in their introduction that most researchers accept the theory as an empirical fact.\n\nImre Lakatos suggested that what he called a \"progressive research program\" is better than a \"degenerative\" one when it can explain the same phenomena as the \"degenerative\" one, but is also characterized by growth of its research field and the discovery of important novel facts. In contrast, the supporters of the \"degenerative\" program do not make important new empirical discoveries, but instead mostly apply adjustments to their theory in order to defend it from competitors. Some researchers argue that democratic peace theory is now the \"progressive\" program in international relations. According to these authors, the theory can explain the empirical phenomena previously explained by the earlier dominant research program, realism in international relations; in addition, the initial statement that democracies do not, or rarely, wage war on one another, has been followed by a rapidly growing literature on novel empirical regularities. , , .\n\nOther examples are several studies finding that democracies are more likely to ally with one another than with other states, forming alliances which are likely to last longer than alliances involving nondemocracies ; several studies including showing that democracies conduct diplomacy differently and in a more conciliatory way compared to nondemocracies; one study finding that democracies with proportional representation are in general more peaceful regardless of the nature of the other party involved in a relationship ; and another study reporting that proportional representation system and decentralized territorial autonomy is positively associated with lasting peace in postconflict societies .\n\nMany democracies become non-democratic by war, as being aggressed or as aggressor (quickly after a coup), sometimes the coup leader worked to provoke that war.\n\nSchmitt (1922) wrote on how to overrule a Constitution: \"Sovereign is he who decides on the exception.\"\n\nSchmitt (1927) again on the need for internal (and foreign) enemies because they are useful to persuade the people not to trust anyone more than the Leader: “As long as the state is a political entity this requirement for internal peace compels it in critical situations to decide also upon the domestic enemy. Every state provides, therefore, some kind of formula for the declaration of an internal enemy.” Whatever opposition will be pictured and intended as the actual foreign enemy's puppet.\n\nOne general criticism motivating research of different explanations is that actually the theory cannot claim that \"democracy causes peace\", because the evidence for democracies being, in general, more peaceful is very slight or non existent; it only can support the claim that \"\"joint\" democracy causes peace\". According to Rosato (2003), this casts doubts on whether democracy is actually the cause because, if so, a monadic effect would be expected.\n\nPerhaps the simplest explanation to such perceived anomaly (but not the one the Realist Rosato prefers, see the section on Realist explanations below) is that democracies are not peaceful to each other because they are democratic, but rather because they are \"similar\". This line of thought started with several independent observations of an \"Autocratic Peace\" effect, a reduced probability of war (obviously no author claims its absence) between states which are both non-democratic, or both highly so , , This has led to the hypothesis that democratic peace emerges as a particular case when analyzing a subset of states which are, in fact, similar . Or, that similarity in general does not solely affect the probability of war, but only coherence of strong political regimes such as full democracies and stark autocracies.\n\nAutocratic peace and the explanation based on political similarity is a relatively recent development, and opinions about its value are varied. Henderson (2002) builds a model considering political similarity, geographic distance and economic interdependence as its main variables, and concludes that democratic peace is a statistical artifact which disappears when the above variables are taken into account. Werner (2000) finds a conflict reducing effect from political similarity in general, but with democratic dyads being particularly peaceful, and noting some differences in behavior between democratic and autocratic dyads with respect to alliances and power evaluation. Beck, King and Zeng (2004) use neural networks to show two distinct low probability zones, corresponding to high democracy and high autocracy. Petersen (2004) uses a different statistical model and finds that autocratic peace is not statistically significant, and that the effect attributed to similarity is mostly driven by the pacifying effect of joint democracy. Ray (2005) similarly disputes the weight of the argument on logical grounds, claiming that statistical analysis on \"political similarity\" uses a main variable which is an extension of \"joint democracy\" by linguistic redefinition, and so it is expected that the war reducing effects are carried on in the new analysis. Bennett (2006) builds a direct statistical model based on a triadic classification of states into \"democratic\", \"autocratic\" and \"mixed\". He finds that autocratic dyads have a 35% reduced chance of going into any type of armed conflict with respect to a reference mixed dyad. Democratic dyads have a 55% reduced chance. This effect gets stronger when looking at more severe conflicts; for wars (more than 1000 battle deaths), he estimates democratic dyads to have an 82% lower risk than autocratic dyads. He concludes that autocratic peace exists, but democratic peace is clearly stronger. However, he finds no relevant pacifying effect of political similarity, except at the extremes of the scale.\n\nTo summarize a rather complex picture, there are no less than four possible stances on the value of this criticism:\n\nThe capitalist peace, or capitalist peace theory, posits that according to a given criteria for economic development (capitalism), developed economies have not engaged in war with each other, and rarely enter into low-level disputes. These theories have been proposed as an explanation for the democratic peace by accounting for both democracy and the peace among democratic nations. The exact nature of the causality depends upon both the proposed variable and the measure of the indicator for the concept used.\n\nA majority of researchers on the determinants of democracy agree that economic development is a primary factor which allows the formation of a stable and healthy democracy (Hegre, 2003; Weede, 2004). Thus, some researchers have argued that economic development also plays a factor in the establishment of peace.\n\nMousseau argues that a culture of contracting in advanced market-oriented economies may cause both democracy and peace (2000; 2002; 2003; 2005). These studies indicate that democracy, alone, is an unlikely cause of the democratic peace. A low level of market-oriented economic development may hinder development of liberal institutions and values. Hegre (2000) and Souva (2003) confirmed these expectations. Mousseau (2005) finds that democracy is a significant factor only when both democracies have levels of economic development well above the global median. In fact, the poorest 21% of the democracies studied, and the poorest 4–5% of current democracies, are significantly \"more\" likely than other kinds of countries to fight each other. Mousseau, Hegre & Oneal (2003) confirm that if at least one of the democracies involved has a very low level of economic development, democracy is ineffective in preventing war; however, they find that when also controlling for trade, 91% of all the democratic pairs had high enough development for the pacifying effect of democracy to be important during the 1885–1992 period and all in 1992. The difference in results of Mousseau (2005) and Mousseau, Hegre & Oneal (2003) may be due to sampling: Mousseau (2005) observed only neighboring states where poor countries actually can fight each other. In fact, fully 89% of militarized conflicts between less developed countries from 1920 and 2000 were among directly contiguous neighbors (Mousseau 2005:68–69). He argues that it is not likely that the results can be explained by trade: Because developed states have large economies, they do not have high levels of trade interdependence (2005:70 and footnote 5; Mousseau, Hegre & Oneal 2003:283). In fact, the correlation of developed democracy with trade interdependence is a scant 0.06 (Pearson's \"r\" – considered substantively no correlation by statisticians)(2005:77).\n\nBoth World Wars were fought between countries which can be considered economically developed. Mousseau argues that both Germany and Japan – like the USSR during the Cold War and Saudi Arabia today – had state-managed economies and thus lacked his market norms (Mousseau 2002–03:29). Hegre (2003) finds that democracy is correlated with civil peace only for developed countries, and for countries with high levels of literacy. Conversely, the risk of civil war decreases with development only for democratic countries.\n\nGartzke (2005) argues that economic freedom (a quite different concept from Mousseau's market norms) or financial dependence (2007) explains the developed democratic peace, and these countries may be weak on these dimensions too. Rummel (2005) criticizes Gartzke's methodology and argues that his results are invalid.\n\nSeveral studies find that democracy, more trade causing greater economic interdependence, and membership in more intergovernmental organizations reduce the risk of war. This is often called the Kantian peace theory since it is similar to Kant's earlier theory about a perpetual peace; it is often also called \"liberal peace\" theory, especially when one focuses on the effects of trade and democracy. (The theory that free trade can cause peace is quite old and referred to as Cobdenism.) Many researchers agree that these variables positively affect each other but each has a separate pacifying effect. For example, in countries exchanging a substantial amount of trade, economic interest groups may exist that oppose a reciprocal disruptive war, but in democracy such groups may have more power, and the political leaders be more likely to accept their requests. (Russett & Oneal 2001), , . Weede (2004) argues that the pacifying effect of free trade and economic interdependence may be more important than that of democracy, because the former affects peace both directly and indirectly, by producing economic development and ultimately, democracy. Weede also lists some other authors supporting this view. However, some recent studies find no effect from trade but only from democracy , .\n\nNone of the authors listed argues that free trade alone causes peace. Even so, the issue of whether free trade or democracy is more important in maintaining peace may have potentially significant practical consequences, for example on evaluating the effectiveness of applying economic sanctions and restrictions to autocratic countries.\n\nIt was Michael Doyle (1983, 1997) who reintroduced Kant's three articles into democratic peace theory. He argued that a pacific union of liberal states has been growing for the past two centuries. He denies that a pair of states will be peaceful simply because they are both liberal democracies; if that were enough, liberal states would not be aggressive towards weak non-liberal states (as the history of American relations with Mexico shows they are). Rather, liberal democracy is a necessary condition for international organization and hospitality (which are Kant's other two articles)—and all three are sufficient to produce peace. Other Kantians have not repeated Doyle's argument that all three in the triad must be present, instead stating that all three reduce the risk of war.\n\nImmanuel Wallerstein has argued that it is the global capitalist system that creates shared interests among the dominant parties, thus inhibiting potentially harmful belligerence.\n\nNegri and Hardt take a similar stance, arguing that the intertwined network of interests in the global capitalism leads to the decline of individual nation states, and the rise of a global \"Empire\" which has no outside, and no external enemies. As a result, they write, \"The era of imperialist, interimperialist, and anti-imperialist wars is over. (...) we have entered the era of minor and internal conflicts. Every imperial war is a civil war, a police action.\" (Hardt & Negri 2000).\n\nMany studies, as those discussed in , , , supporting the theory have controlled for many possible alternative causes of the peace. Examples of factors controlled for are geographic distance, geographic contiguity, power status, alliance ties, militarization, economic wealth and economic growth, power ratio, and political stability. These studies have often found very different results depending on methodology and included variables, which has caused criticism. It should be noted that DPT does not state democracy is the only thing affecting the risk of military conflict. Many of the mentioned studies have found that other factors are also important. However, a common thread in most results is an emphasis on the relationship between democracy and peace. \n\nSeveral studies have also controlled for the possibility of reverse causality from peace to democracy. For example, one study supports the theory of simultaneous causation, finding that dyads involved in wars are likely to experience a decrease in joint democracy, which in turn increases the probability of further war. So they argue that disputes between democratizing or democratic states should be resolved externally at a very early stage, in order to stabilize the system. Another study finds that peace does not spread democracy, but spreading democracy is likely to spread peace. A different kind of reverse causation lies in the suggestion that impending war could destroy or decrease democracy, because the preparation for war might include political restrictions, which may be the cause for the findings of democratic peace. However, this hypothesis has been statistically tested in a study whose authors find, depending on the definition of the pre-war period, no such effect or a very slight one. So, they find this explanation unlikely. Note also that this explanation would predict a monadic effect, although weaker than the dyadic one.\n\nWeart (1998) argues that the peacefulness appears and disappears rapidly when democracy appears and disappears. This in his view makes it unlikely that variables that change more slowly are the explanation. Weart, however, has been criticized for not offering any quantitative analysis supporting his claims (Ray, 2000).\n\nWars tend very strongly to be between neighboring states. Gleditsch (1995) showed that the average distance between democracies is about 8000 miles, the same as the average distance between all states. He believes that the effect of distance in preventing war, modified by the democratic peace, explains the incidence of war as fully as it can be explained.\n\nSupporters of realism in international relations in general argue that not democracy or its absence, but considerations and evaluations of power, cause peace or war. Specifically, many realist critics claim that the effect ascribed to democratic, or liberal, peace, is in fact due to alliance ties between democratic states which in turn are caused, one way or another, by realist factors.\n\nFor example, Farber and Gowa (1995) find evidence for peace between democracies to be statistically significant only in the period from 1945 on, and consider such peace an artifact of the Cold War, when the threat from the communist states forced democracies to ally with one another. Mearsheimer (1990) offers a similar analysis of the Anglo-American peace before 1945, caused by the German threat. Spiro (1994) finds several instances of wars between democracies, arguing that evidence in favor of the theory might be not so vast as other authors report, and claims that the remaining evidence consists of peace between allied states with shared objectives. He acknowledges that democratic states might have a somewhat greater tendency to ally with one another, and regards this as the only real effect of democratic peace. Rosato (2003) argues that most of the significant evidence for democratic peace has been observed after World War II; and that it has happened within a broad alliance, which can be identified with NATO and its satellite nations, imposed and maintained by American dominance (see Pax Americana). One of the main points in Rosato's argument is that, although never engaged in open war with another liberal democracy during the Cold War, the United States intervened openly or covertly in the political affairs of democratic states several times, for example in the Chilean coup of 1973, the 1953 coup in Iran and 1954 coup in Guatemala; in Rosato's view, these interventions show the United States' determination to maintain an \"imperial peace\".\n\nThe most direct counter arguments to such criticisms have been studies finding peace between democracies to be significant even when controlling for \"common interests\" as reflected in alliance ties , . Regarding specific issues, Ray (1998) objects that explanations based on the Cold War should predict that the Communist bloc would be at peace within itself also, but exceptions include the Soviet Invasion of Afghanistan, the Cambodian-Vietnamese War, and the Sino-Vietnamese War. Ray also argues that the external threat did not prevent conflicts in the Western bloc when at least one of the involved states was a nondemocracy, such as the Turkish Invasion of Cyprus (against Greek Junta supported Cypriot Greeks), the Falklands War, and the Football War. Also, one study notes that the explanation \"goes increasingly stale as the post-Cold War world accumulates an increasing number of peaceful dyad-years between democracies\". Rosato's argument about American dominance has also been criticized for not giving supporting statistical evidence (Slantchev, Alexandrova & Gartzke 2005).\n\nSome realist authors also criticize in detail the explanations first by supporters of democratic peace, pointing to supposed inconsistencies or weaknesses.\n\nRosato (2003) criticizes most explanations to how democracy might cause peace. Arguments based on normative constraints, he argues, are not consistent with the fact that democracies do go to war no less than other states, thus violating norms preventing war; for the same reason he refutes arguments based on the importance of public opinion. Regarding explanations based on greater accountability of leaders, he finds that historically autocratic leaders have been removed or punished more often than democratic leaders when they get involved in costly wars. Finally, he also criticizes the arguments that democracies treat each other with trust and respect even during crises; and that democracy might be slow to mobilize its composite and diverse groups and opinions, hindering the start of a war, drawing support from other authors. Another realist, Layne (1994), analyzes the crises and brinkmanship that took place between non-allied democratic great powers, during the relatively brief period when such existed. He finds no evidence either of institutional or cultural constraints against war; indeed, there was popular sentiment in favor of war on both sides. Instead, in all cases, one side concluded that it could not afford to risk that war at that time, and made the necessary concessions.\n\nRosato's objections have been criticized for claimed logical and methodological errors, and for being contradicted by existing statistical research (Kinsella 2005). Russett (1995) replies to Layne by re-examining some of the crises studied in his article, and reaching different conclusions; Russett argues that perceptions of democracy prevented escalation, or played a major role in doing so. Also, a recent study (Gelpi & Griesdorf 2001) finds that, while in general the outcome of international disputes is highly influenced by the contenders' relative military strength, this is not true if both contenders are democratic states; in this case the authors find the outcome of the crisis to be independent of the military capabilities of contenders, which is contrary to realist expectations. Finally, both the realist criticisms here described ignore new possible explanations, like the game-theoretic one discussed below.\n\nA different kind of realist criticism (see for a discussion) stresses the role of nuclear weapons in maintaining peace. In realist terms, this means that, in the case of disputes between nuclear powers, respective evaluation of power might be irrelevant because of Mutual assured destruction preventing both sides from foreseeing what could be reasonably called a \"victory\". The 1999 Kargil War between India and Pakistan has been cited as a counterexample to this argument (Page Fortna, 2004).\n\nSome supporters of the democratic peace do not deny that realist factors are also important (Russett 1995). Research supporting the theory has also shown that factors such as alliance ties and major power status influence interstate conflict behavior .\n\nThe democratic peace theory has been extremely divisive among political scientists. It is rooted in the idealist and classical liberalist traditions and is opposed to the previously dominant theory of realism. However, democratic peace theory has come to be more widely accepted and has in some democracies effected policy change. \n\nIn the United States, presidents from both major parties have expressed support for the theory. In his 1994 State of the Union address, then-President Bill Clinton, a member of the Democratic Party, said: \"Ultimately, the best strategy to ensure our security and to build a durable peace is to support the advance of democracy elsewhere. Democracies don't attack each other.\" In a 2004 press conference, then-President George W. Bush, a member of the Republican Party, said: \"And the reason why I'm so strong on democracy is democracies don't go to war with each other. And the reason why is the people of most societies don't like war, and they understand what war means... I've got great faith in democracies to promote peace. And that's why I'm such a strong believer that the way forward in the Middle East, the broader Middle East, is to promote democracy.\"\n\nIn a 1999 speech, Chris Patten, the then-European Commissioner for External Relations, said: \"Inevitable because the EU was formed partly to protect liberal values, so it is hardly surprising that we should think it appropriate to speak out. But it is also sensible for strategic reasons. Free societies tend not to fight one another or to be bad neighbours.\" The \"A Secure Europe in a Better World, European Security Strategy\" states: \"The best protection for our security is a world of well-governed democratic states.\" Tony Blair has also claimed the theory is correct.\n\nSome fear that the democratic peace theory may be used to justify wars against nondemocracies in order to bring lasting peace, in a \"democratic crusade\" (Chan 1997, p. 59). Woodrow Wilson in 1917 asked Congress to declare war against Imperial Germany, citing Germany's sinking of American ships due to unrestricted submarine warfare and the Zimmermann telegram, but also stating that \"A steadfast concert for peace can never be maintained except by a partnership of democratic nations\" and \"The world must be made safe for democracy.\" R. J. Rummel is a notable proponent of war for the purpose of spreading democracy, based on this theory.\n\nSome point out that the democratic peace theory has been used to justify the 2003 Iraq War, others argue that this justification was used only after the war had already started (Russett 2005). Furthermore, Weede (2004) has argued that the justification is extremely weak, because forcibly democratizing a country completely surrounded by non-democracies, most of which are full autocracies, as Iraq was, is at least as likely to increase the risk of war as it is to decrease it (some studies show that dyads formed by one democracy and one autocracy are the most warlike, and several find that the risk of war is greatly increased in democratizing countries surrounded by nondemocracies). According to Weede, if the United States and its allies wanted to adopt a rationale strategy of forced democratization based on democratic peace, which he still does not recommend, it would be best to start intervening in countries which border with at least one or two stable democracies, and expand gradually. Also, research shows that attempts to create democracies by using external force has often failed. Gleditsch, Christiansen and Hegre (2004) argue that forced democratization by interventionism may initially have partial success, but often create an unstable democratizing country, which can have dangerous consequences in the long run. Those attempts which had a permanent and stable success, like democratization in Austria, West Germany and Japan after World War II, mostly involved countries which had an advanced economic and social structure already, and implied a drastic change of the whole political culture. Supporting internal democratic movements and using diplomacy may be far more successful and less costly. Thus, the theory and related research, if they were correctly understood, may actually be an argument against a democratic crusade (Weart 1998), (Owen 2005), (Russett 2005).\n\nMichael Haas has written perhaps the most trenchant critique of a hidden normative agenda (Haas 1997). Among the points raised: Due to sampling manipulation, the research creates the impression that democracies can justifiably fight nondemocracies, snuff out budding democracies, or even impose democracy. And due to sloppy definitions, there is no concern that democracies continue undemocratic practices yet remain in the sample as if pristine democracies.\n\nThis criticism is confirmed by David Keen(2006) who finds that almost all historical attempts to impose democracy by violent means have failed.\n\nAccording to Azar Gat's \"War in Human Civilization\", there are several related and independent factors that contribute to democratic societies being more peaceful than other forms of governments:\n\nThere is significant debate over whether the lack of any major European general wars since 1945, is due to cooperation and integration of liberal-democratic European states themselves (as in the European Union or Franco-German cooperation), an enforced peace due to intervention of the Soviet Union and the United States until 1989 and the United States alone thereafter, or a combination of both.\n\nThe debate over this theory was thrust in the public eye, when the 2012 Nobel Peace Prize was awarded to the European Union, for its role in creating peace in Europe.\n\n\n\n"}
{"id": "2214599", "url": "https://en.wikipedia.org/wiki?curid=2214599", "title": "Direct reference theory", "text": "Direct reference theory\n\nA direct reference theory (also called referentialism or referential realism) is a theory of language that claims that the meaning of a word or expression lies in what it points out in the world. The object denoted by a word is called its referent. Criticisms of this position are often associated with Ludwig Wittgenstein.\n\nIn the 19th century, mathematician and philosopher Gottlob Frege argued against it, and contrasted it with mediated reference theory. In 1953, with his \"Philosophical Investigations\", Wittgenstein argued against referentialism, famously saying that \"the meaning of a word is its use.\" Direct reference theory is a position typically associated with logical positivism and analytical philosophy. Logical positivist philosophers in particular have significantly devoted their efforts in countering positions of the like of Wittgenstein's, and they aim at creating a \"perfectly descriptive language\" purified from ambiguities and confusions.\n\nThe philosopher John Stuart Mill was one of the earliest modern advocates of a direct reference theory beginning in 1843. In his \"A System of Logic\" Mill introduced a distinction between what he called \"connotation\" and \"denotation.\" Connotation is a relation between a name (singular or general) and one or more attributes. For example, ‘widow’ denotes widows and connotes the attributes of being female, and of having been married to someone now dead. If a name is connotative, it denotes what it denotes in virtue of object or objects having the attributes the name connotes. Connotation thus determines denotation. The same object can, on the other hand, be denoted with several names with different connotations. A name can have connotation but no denotation. Connotation of a name, if it has one, can be taken to be its meaning in Mill.\n\nAccording to Mill, most individual concrete names are connotative, but some, namely proper names, are not. In other words, proper names do not have meaning. All general terms, on the other hand, are according to Mill connotative. In sum, Mill’s overall picture resembles very much the description theory of reference, though his take on proper names is an exception.\n\nSaul Kripke, a proponent of direct reference theory, in his \"Naming and Necessity\" dubbed mediated reference theory the \"Frege–Russell view\" and criticized it (see below). Subsequent scholarship refuted the claim that Bertrand Russell's views on reference theory were the same as Gottlob Frege's, since Russell was also a proponent of direct reference theory.\n\nSaul Kripke defended direct reference theory when applied to proper names. Kripke claims that proper names do not have any \"senses\" at all, because senses only offer contingent facts about things. Ruth Barcan Marcus advanced a theory of direct reference for proper names at a symposium in which Quine and Kripke were participants: published in \"Synthese\", 1961 with Discussion in \"Synthese\" 1962. She called directly referring proper names \"tags\" (see tag theory of names). Kripke urged such a theory in 1971 and thereafter. He called such directly referring proper names \"rigid designators\".\n\nKripke articulated this view using the formal apparatus of possible worlds. The possible worlds thought-experiment first takes the subject, and then tries to imagine the subject in other possible worlds. Taking George W. Bush, for example. First (1) the thought-experiment must state that the name \"George W. Bush\" is the name used to describe the particular individual man that is typically meant. Then (2), the experimenter must imagine the possible states of affairs that reality could have been - where Bush was not president, or went into a different career, was never born at all, etc. When this is done, it becomes obvious that the phrase \"President of the United States in 2004\" does not necessarily describe George W. Bush, because it is not necessarily true in all possible worlds; it only contingently describes him. By contrast, for instance, the word \"apple\" will always describe the same things across all possible worlds, because of premise (1). So use of the word \"apple\" to describe apples is true in all possible worlds.\n\nTerms that are true across all possible worlds in this way are called \"rigid designators\".\n\n"}
{"id": "40258", "url": "https://en.wikipedia.org/wiki?curid=40258", "title": "Harmony", "text": "Harmony\n\nIn music, harmony considers the process by which the composition of individual sounds, or superpositions of sounds, is analysed by hearing. Usually, this means simultaneously occurring frequencies, pitches (tones, notes), or chords.\n\nThe study of harmony involves chords and their construction and chord progressions and the principles of connection that govern them.\n\nHarmony is often said to refer to the \"vertical\" aspect of music, as distinguished from melodic line, or the \"horizontal\" aspect.\n\nCounterpoint, which refers to the relationship between melodic lines, and polyphony, which refers to the simultaneous sounding of separate independent voices, are thus sometimes distinguished from harmony.\n\nIn popular and jazz harmony, chords are named by their root plus various terms and characters indicating their qualities. In many types of music, notably baroque, romantic, modern, and jazz, chords are often augmented with \"tensions\". A tension is an additional chord member that creates a relatively dissonant interval in relation to the bass.\n\nTypically, in the classical common practice period a dissonant chord (chord with tension) \"resolves\" to a consonant chord. Harmonization usually sounds pleasant to the ear when there is a balance between the consonant and dissonant sounds. In simple words, that occurs when there is a balance between \"tense\" and \"relaxed\" moments.\n\nThe term \"harmony\" derives from the Greek \"harmonia\", meaning \"joint, agreement, concord\", from the verb \"harmozō\", \"(Ι) fit together, join\". In the past, \"harmony\" often referred to the whole field of music, while \"music\" referred to the arts in general. In Ancient Greece, the term defined the combination of contrasted elements: a higher and lower note. Nevertheless, it is unclear whether the simultaneous sounding of notes was part of ancient Greek musical practice; \"harmonía\" may have merely provided a system of classification of the relationships between different pitches. In the Middle Ages the term was used to describe two pitches sounding in combination, and in the Renaissance the concept was expanded to denote three pitches sounding together. Aristoxenus wrote a work entitled \"Harmonika Stoicheia\", which is thought the first work in European history written on the subject of harmony.\nIt was not until the publication of Rameau's \"Traité de l'harmonie\" (\"Treatise on Harmony\") in 1722 that any text discussing musical practice made use of the term in the title, although that work is not the earliest record of theoretical discussion of the topic. The underlying principle behind these texts is that harmony sanctions harmoniousness (sounds that please) by conforming to certain pre-established compositional principles.\n\nCurrent dictionary definitions, while attempting to give concise descriptions, often highlight the ambiguity of the term in modern use. Ambiguities tend to arise from either aesthetic considerations (for example the view that only pleasing concords may be harmonious) or from the point of view of musical texture (distinguishing between harmonic (simultaneously sounding pitches) and \"contrapuntal\" (successively sounding tones). In the words of Arnold Whittall:\n\nThe view that modern tonal harmony in Western music began in about 1600 is commonplace in music theory. This is usually accounted for by the replacement of horizontal (or contrapuntal) composition, common in the music of the Renaissance, with a new emphasis on the vertical element of composed music. Modern theorists, however, tend to see this as an unsatisfactory generalisation. According to Carl Dahlhaus:\n\nDescriptions and definitions of harmony and harmonic practice may show bias towards European (or Western) musical traditions. For example, South Asian art music (Hindustani and Carnatic music) is frequently cited as placing little emphasis on what is perceived in western practice as conventional harmony; the underlying harmonic foundation for most South Asian music is the drone, a held open fifth interval (or fourth interval) that does not alter in pitch throughout the course of a composition. Pitch simultaneity in particular is rarely a major consideration. Nevertheless, many other considerations of pitch are relevant to the music, its theory and its structure, such as the complex system of Rāgas, which combines both melodic and modal considerations and codifications within it.\n\nSo, intricate pitch combinations that sound simultaneously do occur in Indian classical music—but they are rarely studied as teleological harmonic or contrapuntal progressions—as with notated Western music. This contrasting emphasis (with regard to Indian music in particular) manifests itself in the different methods of performance adopted: in Indian Music improvisation takes a major role in the structural framework of a piece, whereas in Western Music improvisation has been uncommon since the end of the 19th century. Where it does occur in Western music (or has in the past), the improvisation either embellishes pre-notated music or draws from musical models previously established in notated compositions, and therefore uses familiar harmonic schemes.\n\nNevertheless, emphasis on the precomposed in European art music and the written theory surrounding it shows considerable cultural bias. The \"Grove Dictionary of Music and Musicians\" (Oxford University Press) identifies this clearly:\n\nYet the evolution of harmonic practice and language itself, in Western art music, is and was facilitated by this process of prior composition, which permitted the study and analysis by theorists and composers of individual pre-constructed works in which pitches (and to some extent rhythms) remained unchanged regardless of the nature of the performance.\n\nSome traditions of Western music performance, composition, and theory have specific rules of harmony. These rules are often described as based on natural properties such as Pythagorean tuning's law whole number ratios (\"harmoniousness\" being inherent in the ratios either perceptually or in themselves) or harmonics and resonances (\"harmoniousness\" being inherent in the quality of sound), with the allowable pitches and harmonies gaining their beauty or simplicity from their closeness to those properties. This model provides that the minor seventh and (major) ninth are not dissonant (i.e., are consonant).\n\nEarly Western religious music often features parallel perfect intervals; these intervals would preserve the clarity of the original plainsong. These works were created and performed in cathedrals, and made use of the resonant modes of their respective cathedrals to create harmonies. As polyphony developed, however, the use of parallel intervals was slowly replaced by the English style of consonance that used thirds and sixths. The English style was considered to have a sweeter sound, and was better suited to polyphony in that it offered greater linear flexibility in part-writing. Early music also forbade usage of the tritone, due to its dissonance, and composers often went to considerable lengths, via musica ficta, to avoid using it. In the newer triadic harmonic system, however, the tritone became permissible, as the standardization of functional dissonance made its use in dominant chords desirable.\n\nMost harmony comes from two or more notes sounding simultaneously—but a work can imply harmony with only one melodic line by using arpeggios or hocket. Many pieces from the baroque period for solo string instruments—such as Bach's Sonatas and partitas for solo violin and cello—convey subtle harmony through inference rather than full chordal structures. These works create a sense of harmonies by using arpeggiated chords and implied bass lines. The implied basslines are created with low notes of short duration that many listeners perceive as being the bass note of a chord.\n\nCarl Dahlhaus (1990) distinguishes between \"coordinate\" and \"subordinate harmony\". \"Subordinate harmony\" is the hierarchical tonality or tonal harmony well known today. \"Coordinate harmony\" is the older Medieval and Renaissance \"tonalité ancienne\", \"The term is meant to signify that sonorities are linked one after the other without giving rise to the impression of a goal-directed development. A first chord forms a 'progression' with a second chord, and a second with a third. But the former chord progression is independent of the later one and vice versa.\" Coordinate harmony follows direct (adjacent) relationships rather than indirect as in subordinate. Interval cycles create symmetrical harmonies, which have been extensively used by the composers Alban Berg, George Perle, Arnold Schoenberg, Béla Bartók, and Edgard Varèse's \"Density 21.5\".\n\nClose harmony and open harmony use close position and open position chords, respectively. See: voicing (music) and close and open harmony.\n\nOther types of harmony are based upon the intervals of the chords used in that harmony. Most chords in western music are based on \"tertian\" harmony, or chords built with the interval of thirds. In the chord C Major7, C–E is a major third; E–G is a minor third; and G to B is a major third. Other types of harmony consist of quartal and quintal harmony.\n\nA unison is considered a harmonic interval, just like a fifth or a third, but is unique in that it is two identical notes produced together. The unison, as a component of harmony, is important, especially in orchestration. In pop music, unison singing is usually called \"doubling\", a technique The Beatles used in many of their earlier recordings. As a type of harmony, singing in unison or playing the same notes, often using different musical instruments, at the same time is commonly called monophonic harmonization.\n\nAn interval is the relationship between two separate musical pitches. For example, in the melody \"Twinkle Twinkle Little Star\", the first two notes (the first \"twinkle\") and the second two notes (the second \"twinkle\") are at the interval of one fifth. What this means is that if the first two notes were the pitch C, the second two notes would be the pitch \"G\"—four scale notes, or seven chromatic notes (a perfect fifth), above it.\n\nThe following are common intervals:\n\nTherefore, the combination of notes with their specific intervals—a chord—creates harmony. For example, in a C chord, there are three notes: C, E, and G. The note C is the root. The notes E and G provide harmony, and in a G7 (G dominant 7th) chord, the root G with each subsequent note (in this case B, D and F) provide the harmony.\nIn the musical scale, there are twelve pitches. Each pitch is referred to as a \"degree\" of the scale. The names A, B, C, D, E, F, and G are insignificant. The intervals, however, are not. Here is an example:\n\nAs can be seen, no note always corresponds to a certain degree of the scale. The \"tonic\", or 1st-degree note, can be any of the 12 notes (pitch classes) of the chromatic scale. All the other notes fall into place. For example, when C is the tonic, the fourth degree or subdominant is F. When D is the tonic, the fourth degree is G. While the note names remain constant, they may refer to different scale degrees, implying different intervals with respect to the tonic. The great power of this fact is that any musical work can be played or sung in any key. It is the same piece of music, as long as the intervals are the same—thus transposing the melody into the corresponding key. When the intervals surpass the perfect Octave (12 semitones), these intervals are called \"compound intervals\", which include particularly the 9th, 11th, and 13th Intervals—widely used in jazz and blues Music.\n\nCompound Intervals are formed and named as follows:\n\nThe reason the two numbers don't \"add\" correctly is that one note is counted twice. Apart from this categorization, intervals can also be divided into consonant and dissonant. As explained in the following paragraphs, consonant intervals produce a sensation of relaxation and dissonant intervals a sensation of tension. In tonal music, the term consonant also means \"brings resolution\" (to some degree at least, whereas dissonance \"requires resolution\").\n\nThe consonant intervals are considered the perfect unison, octave, fifth, fourth and major and minor third and sixth, and their compound forms. An interval is referred to as \"perfect\" when the harmonic relationship is found in the natural overtone series (namely, the unison 1:1, octave 2:1, fifth 3:2, and fourth 4:3). The other basic intervals (second, third, sixth, and seventh) are called \"imperfect\" because the harmonic relationships are not found mathematically exact in the overtone series. In classical music the perfect fourth above the bass may be considered dissonant when its function is contrapuntal. Other intervals, the second and the seventh (and their compound forms) are considered Dissonant and require resolution (of the produced tension) and usually preparation (depending on the music style).\n\nNote that the effect of dissonance is perceived relatively within musical context: for example, a major seventh interval alone (i.e., C up to B) may be perceived as dissonant, but the same interval as part of a major seventh chord may sound relatively consonant. A tritone (the interval of the fourth step to the seventh step of the major scale, i.e., F to B) sounds very dissonant alone, but less so within the context of a dominant seventh chord (G7 or D7 in that example).\n\nIn the Western tradition, in music after the seventeenth century, harmony is manipulated using chords, which are combinations of pitch classes. In tertian harmony, so named after the interval of a third, the members of chords are found and named by stacking intervals of the third, starting with the \"root\", then the \"third\" above the root, and the \"fifth\" above the root (which is a third above the third), etc. (Note that chord members are named after their interval above the root.) Dyads, the simplest chords, contain only two members (see power chords).\n\nA chord with three members is called a triad because it has three members, not because it is necessarily built in thirds (see Quartal and quintal harmony for chords built with other intervals). Depending on the size of the intervals being stacked, different qualities of chords are formed. In popular and jazz harmony, chords are named by their root plus various terms and characters indicating their qualities. To keep the nomenclature as simple as possible, some defaults are accepted (not tabulated here). For example, the chord members C, E, and G, form a C Major triad, called by default simply a C chord. In an A chord (pronounced A-flat), the members are A, C, and E.\n\nIn many types of music, notably baroque, romantic, modern and jazz, chords are often augmented with \"tensions\". A tension is an additional chord member that creates a relatively dissonant interval in relation to the bass. Following the tertian practice of building chords by stacking thirds, the simplest first tension is added to a triad by stacking on top of the existing root, third, and fifth, another third above the fifth, giving a new, potentially dissonant member the interval of a seventh away from the root and therefore called the \"seventh\" of the chord, and producing a four-note chord, called a \"seventh chord\".\n\nDepending on the widths of the individual thirds stacked to build the chord, the interval between the root and the seventh of the chord may be major, minor, or diminished. (The interval of an augmented seventh reproduces the root, and is therefore left out of the chordal nomenclature.) The nomenclature allows that, by default, \"C7\" indicates a chord with a root, third, fifth, and seventh spelled C, E, G, and B. Other types of seventh chords must be named more explicitly, such as \"C Major 7\" (spelled C, E, G, B), \"C augmented 7\" (here the word augmented applies to the fifth, not the seventh, spelled C, E, G, B), etc. (For a more complete exposition of nomenclature see Chord (music).)\n\nContinuing to stack thirds on top of a seventh chord produces extensions, and brings in the \"extended tensions\" or \"upper tensions\" (those more than an octave above the root when stacked in thirds), the ninths, elevenths, and thirteenths. This creates the chords named after them. (Note that except for dyads and triads, tertian chord types are named for the interval of the largest size and magnitude in use in the stack, not for the number of chord members : thus a ninth chord has five members \"[tonic, 3rd, 5th, 7th, 9th]\", not nine.) Extensions beyond the thirteenth reproduce existing chord members and are (usually) left out of the nomenclature. Complex harmonies based on extended chords are found in abundance in jazz, late-romantic music, modern orchestral works, film music, etc.\n\nTypically, in the classical Common practice period a dissonant chord (chord with tension) \"resolves\" to a consonant chord. Harmonization usually sounds pleasant to the ear when there is a balance between the consonant and dissonant sounds. In simple words, that occurs when there is a balance between \"tense\" and \"relaxed\" moments. For this reason, usually tension is 'prepared' and then 'resolved', where preparing tension means to place a series of consonant chords that lead smoothly to the dissonant chord. In this way the composer ensures introducing tension smoothly, without disturbing the listener. Once the piece reaches its sub-climax, the listener needs a moment of relaxation to clear up the tension, which is obtained by playing a consonant chord that resolves the tension of the previous chords. The clearing of this tension usually sounds pleasant to the listener, though this is not always the case in late-nineteenth century music, such as Tristan und Isolde by Richard Wagner.\n\nHarmony is based on consonance, a concept whose definition has changed various times during the history of Western music. In a psychological approach, consonance is a continuous variable. Consonance can vary across a wide range. A chord may sound consonant for various reasons.\n\nOne is lack of perceptual roughness. Roughness happens when partials (frequency components) lie within a critical bandwidth, which is a measure of the ear's ability to separate different frequencies. Critical bandwidth lies between 2 and 3 semitones at high frequencies and becomes larger at lower frequencies. The roughness of two simultaneous harmonic complex tones depends on the amplitudes of the harmonics and the interval between the tones. The roughest interval in the chromatic scale is the minor second and its inversion the major seventh. For typical spectral envelopes in the central range, the second roughest interval is the major second and minor seventh, followed by the tritone, the minor third (major sixth), the major third (minor sixth) and the perfect fourth (fifth).\n\nThe second reason is perceptual fusion. A chord fuses in perception if its overall spectrum is similar to a harmonic series. According to this definition a major triad fuses better than a minor triad and a major-minor seventh chord fuses better than a major-major seventh or minor-minor seventh. These differences may not be readily apparent in tempered contexts but can explain why major triads are generally more prevalent than minor triads and major-minor sevenths generally more prevalent than other sevenths (in spite of the dissonance of the tritone interval) in mainstream tonal music. Of course these comparisons depend on style.\n\nThe third reason is familiarity. Chords that have often been heard in musical contexts tend to sound more consonant. This principle explains the gradual historical increase in harmonic complexity of Western music. For example, around 1600 unprepared seventh chords gradually became familiar and were therefore gradually perceived as more consonant.\n\nWestern music is based on major and minor triads. The reason why these chords are so central is that they are consonant in terms of both fusion and lack of roughness. They fuse because they include the perfect fourth/fifth interval. They lack roughness because they lack major and minor second intervals. No other combination of three tones in the chromatic scale satisfies these criteria.\n\nPost-nineteenth century music has evolved in the way that tension may be less often prepared and less formally structured than in Baroque or Classical periods, thus producing new styles such as post-Romantic harmony, impressionism, pantonality, jazz and blues, where dissonance may not be prepared in the way seen in \"common practice era\" harmony. In a jazz or blues song, the tonic chord that opens a tune may be a dominant seventh chord. A jazz song may end on what in Classical music is a quite dissonant chord, such as an altered dominant chord with a sharpened eleventh note.\n\n\n\n\n"}
{"id": "50020", "url": "https://en.wikipedia.org/wiki?curid=50020", "title": "Hope", "text": "Hope\n\nHope is an optimistic state of mind that is based on an expectation of positive outcomes with respect to events and circumstances in one's life or the world at large.\nAs a verb, its definitions include: \"expect with confidence\" and \"to cherish a desire with anticipation.\"\n\nAmong its opposites are , hopelessness, and despair.\n\nProfessor of Psychology Barbara Fredrickson argues that hope comes into its own when crisis looms, opening us to new creative possibilities. Frederickson argues that with great need comes an unusually wide range of ideas, as well as such positive emotions as happiness and joy, courage, and empowerment, drawn from four different areas of one’s self: from a cognitive, psychological, social, or physical perspective. Hopeful people are \"like the little engine that could, [because] they keep telling themselves \"I think I can, I think I can\". Such positive thinking bears fruit when based on a realistic sense of optimism, not on a naive \"false hope\".\n\nThe psychologist Charles R. Snyder linked hope to the existence of a goal, combined with a determined plan for reaching that goal: Alfred Adler had similarly argued for the centrality of goal-seeking in human psychology, as too had philosophical anthropologists like Ernst Bloch. Snyder also stressed the link between hope and mental willpower, as well as the need for realistic perception of goals, arguing that the difference between hope and optimism was that the former included practical pathways to an improved future. D. W. Winnicott saw a child's antisocial behavior as expressing an unconscious hope for management by the wider society, when containment within the immediate family had failed. Object relations theory similarly sees the analytic transference as motivated in part by an unconscious hope that past conflicts and traumas can be dealt with anew.\n\nAs a specialist in positive psychology, Snyder studied how hope and forgiveness can impact several aspects of life such as health, work, education, and personal meaning. He postulated that there are three main things that make up hopeful thinking:\n\n\nIn other words, hope was defined as the perceived capability to derive pathways to desired goals and motivate oneself via agency thinking to use those pathways.\n\nSnyder argues that individuals who are able to realize these three components and develop a belief in their ability are hopeful people who can establish clear goals, imagine multiple workable pathways toward those goals, and persevere, even when obstacles get in their way. \n\nSnyder proposed a \"Hope Scale\" which considered that a person's determination to achieve their goal is their measured hope. Snyder differentiates between adult-measured hope and child-measured hope. The Adult Hope Scale by Snyder contains 12 questions; 4 measuring 'pathways thinking', 4 measuring 'agency thinking', and 4 that are simply fillers. Each subject responds to each question using an 8-point scale. Fibel and Hale measure hope by combining Snyder's Hope Scale with their own Generalized Expectancy for Success Scale (GESS) to empirically measure hope. Snyder regarded that psychotherapy can help focus attention on one's goals, drawing on tacit knowledge of how to reach them. Similarly, there is an \"outlook\" and a \"grasp of reality\" to hope, distinguishing No Hope, Lost Hope, False Hope and Real Hope, which differ in terms of viewpoint and realism.\n\nOf the countless models that examine the importance of hope in an individual’s life, there are two major theories that have gained a significant amount of recognition in the field of psychology. One of these theories, developed by Charles R. Snyder, argues that hope should be viewed as a cognitive skill that demonstrates an individual’s ability to maintain drive in the pursuit of a particular goal. This model reasons that an individual’s ability to be hopeful depends on two types of thinking: agency thinking and pathway thinking. Agency thinking refers to an individual’s determination to achieve their goals despite possible obstacles, while pathway thinking refers to the ways in which an individual believes they can achieve these personal goals.\n\nSnyder’s theory uses hope as a mechanism that is most often seen in psychotherapy. In these instances, the therapist helps their client overcome barriers that have prevented them from achieving goals. The therapist would then help the client set realistic and relevant personal goals (i.e. \"I am going to find something I am passionate about and that makes me feel good about myself\"), and would help them remain hopeful of their ability to achieve these goals, and suggest the correct pathways to do so.\n\nWhereas Snyder’s theory focuses on hope as a mechanism to overcome an individual’s lack of motivation to achieve goals, the other major theory developed by K.A Herth deals more specifically with an individual’s future goals as they relate to coping with illnesses. Herth views hope as \"a motivational and cognitive attribute that is theoretically necessary to initiate and sustain action toward goal attainment\". Establishing realistic and attainable goals in this situation is more difficult, as the individual most likely does not have direct control over the future of their health. Instead, Herth suggests that the goals should be concerned with how the individual is going to personally deal with the illness—\"Instead of drinking to ease the pain of my illness, I am going to surround myself with friends and family\".\n\nWhile the nature of the goals in Snyder’s model differ with those in Herth’s model, they both view hope as a way to maintain personal motivation, which ultimately will result in a greater sense of optimism.\n\nHope, and more specifically, particularized hope, has been shown to be an important part of the recovery process from illness; it has strong psychological benefits for patients, helping them to cope more effectively with their disease. For example, hope motivates people to pursue healthy behaviors for recovery, such as eating fruits and vegetables, quitting smoking, and engaging in regular physical activity. This not only helps to enhance people’s recovery from illnesses, but also helps prevent illness from developing in the first place. Patients who maintain high levels of hope have an improved prognosis for life-threatening illness and an enhanced quality of life. Belief and expectation, which are key elements of hope, block pain in patients suffering from chronic illness by releasing endorphins and mimicking the effects of morphine. Consequently, through this process, belief and expectation can set off a chain reaction in the body that can make recovery from chronic illness more likely. This chain reaction is especially evident with studies demonstrating the placebo effect, a situation when hope is the only variable aiding in these patients’ recovery.\n\nOverall, studies have demonstrated that maintaining a sense of hope during a period of recovery from illness is beneficial. A sense of hopelessness during the recovery period has, in many instances, resulted in adverse health conditions for the patient (i.e. depression and anxiety following the recovery process). Additionally, having a greater amount of hope before and during cognitive therapy has led to decreased PTSD-related depression symptoms in war veterans. Hope has also been found to be associated with more positive perceptions of subjective health. However, reviews of research literature have noted that the connections between hope and symptom severity in other mental health disorders are less clear, such as in cases of individuals with schizophrenia.\n\nThe inclusion of hope in treatment programs has potential in both physical and mental health settings. Hope as a mechanism for improved treatment has been studied in the contexts of PTSD, chronic physical illness, and terminal illness, among other disorders and ailments. Within mental health practice, clinicians have suggested using hope interventions as a supplement to more traditional cognitive behavioral therapies. In terms of support for physical illness, research suggests that hope can encourage the release of endorphins and enkephalins, which help to block pain.\n\nThere are two main arguments based on judgement against those who are advocates of using hope to help treat severe illnesses. The first of which is that if physicians have too much hope, they may aggressively treat the patient. The physician will hold on to a small shred of hope that the patient may get better. Thus, this causes them to try methods that are costly and may have many side effects. One physician noted that she regretted having hope for her patient; it resulted in her patient suffering through three more years of pain that the patient would not have endured if the physician had realized recovery was infeasible.\n\nThe second argument is the division between hope and wishing. Those that are hopeful are actively trying to investigate the best path of action while taking into consideration the obstacles. Research has shown though that many of those who have \"hope\" are wishfully thinking and passively going through the motions, as if they are in denial about their actual circumstances. Being in denial and having too much hope may negatively impact both the patient and the physician.\n\nThe impact that hope can have on a patient’s recovery process is strongly supported through both empirical research and theoretical approaches. Also always have hope. However, reviews of literature also maintain that more longitudinal and methodologically-sound research is needed to establish which hope interventions are actually the most effective, and in what setting (i.e. chronic illness vs. terminal illness).\n\nIn the matter of globalization, hope is focused on economic and social empowerment.\n\nFocusing on parts of Asia, hope has taken on a secular or materialistic form in relation to the pursuit of economic growth. Primary examples are the rise of the economies of China and India, correlating with the notion of Chindia. A secondary relevant example is the increased use of contemporary architecture in rising economies, such as the building of the Shanghai World Financial Center, Burj Khalifa and Taipei 101, which has given rise to a prevailing hope within the countries of origin. In chaotic environments hope is transcended without cultural boundaries, Syrian refugee children are supported by UNESCO's education project through creative education and psycho-social assistance. Other inter-cultural support for instilling hope involve food culture, disengaging refugees from trauma through immersing them in their rich cultural past.\n\nRobert Mattox, a social activist and futurist, proposed in 2012 a social change theory based on the hope phenomenon in relation to leadership.\nLarry Stout postulated in 2006 that certain conditions must exist before even the most talented leaders can lead change. Given such conditions, Mattox proposes a change management theory around hope, suggesting that a leader can lead change and shape culture within a community or organization by creating a \"hopescape\" and by harnessing the hope system.\nA classic reference to hope which has entered modern language is the concept that \"Hope springs eternal\" taken from Alexander Pope's \"Essay on Man\", the phrase reading \"Hope springs eternal in the human breast, Man never is, but always \"to be\" blest:\" Another popular reference, \"Hope is the thing with feathers,\" is from a poem by Emily Dickinson.\n\nHope can be used as an artistic plot device and is often a motivating force for change in dynamic characters. A commonly understood reference from western popular culture is the subtitle \"A New Hope\" from the original first installment (now considered Episode IV) in the \"Star Wars\" science fiction space opera. The subtitle refers to one of the lead characters, Luke Skywalker, who is expected in the future to allow good to triumph over evil within the plot of the films.\n\nContemporary philosopher Richard Rorty understands hope as more than goal setting, rather as a metanarrative, a story that serves as a promise or reason for expecting a better future. Rorty as postmodernist believes past meta–narratives, including the Christian story, utilitarianism, and Marxism have proved false hopes; that theory cannot offer social hope; and that liberal man must learn to live without a consensual theory of social hope. Rorty says a new document of promise is needed for social hope to exist again.\n\nThe swallow has been a symbol of hope, in Aesop's fables and numerous other historic literature. It symbolizes hope, in part because it is among the first birds to appear at the end of winter and the start of spring. Other symbols of hope include the anchor and the dove.\n\nElpis (Hope) appears in ancient Greek mythology with the story of Zeus and Prometheus. Prometheus stole fire from the god Zeus, which infuriated the supreme god. In turn, Zeus created a box that contained all manners of evil, unbeknownst to the receiver of the box. Pandora opened the box after being warned not to, and unleashed a multitude of harmful spirits that inflicted plagues, diseases, and illnesses on mankind. Spirits of greed, envy, hatred, mistrust, sorrow, anger, revenge, lust, and despair scattered far and wide looking for humans to torment. Inside the box, however, there was also an unreleased healing spirit named Hope. From ancient times, people have recognized that a spirit of hope had the power to heal afflictions and helps them bear times of great suffering, illnesses, disasters, loss, and pain caused by the malevolent spirits and events. In Hesiod's \"Works and Days\", the personification of hope is named Elpis.\n\nNorse mythology however considered Hope (\"Vön\") to be the slobber dripping from the mouth of Fenris Wolf: their concept of courage rated most highly a cheerful bravery in the \"absence\" of hope.\n\nHope is a key concept in most major world religions, often signifying the \"hoper\" believes an individual or a collective group will reach a concept of heaven. Depending on the religion, hope can be seen as a prerequisite for and/or byproduct of spiritual attainment.\n\nHope is one of the three theological virtues of the Christian religion, alongside faith and love. \"Hope\" in the Holy Bible means \"a strong and confident expectation\" of future reward (see Titus 1:2). In modern terms, hope is akin to trust and a confident expectation\". Paul the Apostle argued that hope was a source of salvation for Christians: \"For in hope we have been saved...if we hope for what we do not see, with perseverance we wait eagerly for it\" (see Romans 8:25).\n\nAccording to the \"Holman Bible Dictionary\", hope is a \"[t]rustful expectation...the anticipation of a favorable outcome under God's guidance.\"\" In \"The Pilgrim's Progress\", it is Hopeful who comforts Christian in Doubting Castle; while conversely at the entrance to Dante's Hell were the words, \"Lay down all hope, you that go in by me\".\n\nIn historic literature of Hinduism, hope is referred to with \"Pratidhi\" (Sanskrit: प्रतिधी), or \"Apêksh\" (Sanskrit: अपेक्ष). It is discussed with the concepts of desire and wish. In Vedic philosophy, \"karma\" was linked to ritual sacrifices (\"yajna\"), hope and success linked to correct performance of these rituals. In Vishnu Smriti, the image of hope, morals and work is represented as the virtuous man who rides in a chariot directed by his hopeful mind to his desired wishes, drawn by his five senses, who keeps the chariot on the path of the virtuous, and thus is not distracted by the wrongs such as wrath, greed, and other vices.\n\nIn the centuries that followed, the concept of \"karma\" changed from sacramental rituals to actual human action that builds and serves society and human existence–a philosophy epitomized in the Bhagavad Gita. Hope, in the structure of beliefs and motivations, is a long-term \"karmic\" concept. In Hindu belief, actions have consequences, and while one’s effort and work may or may not bear near term fruits, it will serve the good, that the journey of one’s diligent efforts (karma) and how one pursues the journey, sooner or later leads to bliss and moksha.\n\n"}
{"id": "1814231", "url": "https://en.wikipedia.org/wiki?curid=1814231", "title": "Iki (aesthetics)", "text": "Iki (aesthetics)\n\nIki (いき, English: roughly \"chic, stylish\") is a concept in aesthetics, the basis of which is thought to have formed among urbane commoners (chōnin) in Edo in the Tokugawa period. \"Iki\" is sometimes misunderstood as simply \"anything Japanese\" overseas, but it is actually a specific aesthetic ideal, distinct from more ethereal notions of transcendence or poverty. As such, samurai, for example, would typically, as a class, be considered devoid of \"iki\" (see \"yabo\"). At the same time, individual warriors are often depicted in contemporary popular imagination as embodying the \"iki\" ideals of a clear, stylish manner and blunt, unwavering directness. The term became widespread in modern intellectual circles through the book \"The Structure of Iki\" (1930) by Kuki Shūzō.\n\n\"Iki\", having emerged from the worldly Japanese merchant class, may appear in some ways a more contemporary expression of Japanese aesthetics than concepts such as \"wabi-sabi\". The term is commonly used in conversation and writing, but is not necessarily exclusive of other categories of beauty.\n\n\"Iki\" is an expression of simplicity, sophistication, spontaneity, and originality. It is ephemeral, romantic, straightforward, measured, audacious, smart, and lacking in detrimental self-consciousness.\n\n\"Iki\" is not overly refined, pretentious, complicated, showy, slick, coquettish, or, generally, cute. At the same time, \"iki\" may exhibit any of those traits in a smart, direct, and unabashed manner.\n\n\"Iki\" may signify a personal trait, or artificial phenomena exhibiting human will or consciousness. \"Iki\" is not used to describe natural phenomena, but may be expressed in human appreciation of natural beauty, or in the nature of human beings. Murakami Haruki (b. 1949), who writes in a clear, unflinching style—at turns sentimental, fantastic, and surreal—is described as embodying \"iki\". In contrast, Kawabata Yasunari (1899-1972) writes in a more poetic vein, with a closer focus on the interior \"complex\" of his characters, while situations and surroundings exhibit a kind of \"wabi-sabi\". That said, stylistic differences may tend to distract from a similar emotional subjectivity. Indeed, \"iki\" is strongly tied to stylistic tendencies.\n\nThe indefinite ideal of tsū (通) can be said to reference a highly cultivated but not necessarily solemn sensibility. The \"iki\"/\"tsu\" sensibility resists being construed within the context of overly specific rules about what could be considered as vulgar or uncouth.\n\n\"Iki\" and \"tsu\" are considered synonymous in some situations, but \"tsu\" exclusively refers to persons, while \"iki\" can also refer to situations/objects. In both ideals, the property of refinement is not academic in nature. \"Tsu\" sometimes involves excessive obsession and cultural (but not academic) pedantry, and in this case, it differs from \"iki\", which will not be obsessive. \"Tsu\" is used, for example, for knowing how to properly appreciate (eat) Japanese cuisines (sushi, tempura, soba etc.). \"Tsu\" (and some \"iki\"-style) can be transferred from person to person in form of \"tips.\" As \"tsu\" is more focused in knowledge, it may be considered superficial from \"iki\" point of view, since \"iki\" cannot be easily attained by learning.\n\n\"Yabo\" (野暮) is the antonym of \"iki\". \"Busui\" (無粋), literally \"non-\"iki\",\" is synonymous to \"yabo\".\n\nIn the Kamigata or Kansai area, the ideal of \"sui\" is prevalent. \"Sui\" is also represented by the kanji \"粋\". The sense of \"sui\" is similar to \"iki\" but not identical, reflecting various regional differences. The contexts of their usages are also different.\n\n\n"}
{"id": "38182080", "url": "https://en.wikipedia.org/wiki?curid=38182080", "title": "Index of Freedom in the World", "text": "Index of Freedom in the World\n\nThe Index of Freedom in the World is an index of civil liberties published in late 2012 by Canada's Fraser Institute, Germany's Liberales Institut, and the U.S. Cato Institute. The index is the predecessor of the Human Freedom Index, which was been annually published since 2015. The coauthors of both indexes are Ian Vásquez and Tanja Porčnik (née Štumberger).\n\nThe index is based on measures of freedom of speech, freedom of religion, individual economic choice, freedom of association, freedom of assembly, violence and crimes, freedom of movement, LGBT rights, and women's rights. Other components of the Freedom Index include human trafficking, sexual violence, female genital mutilation, homicide, and adoption by homosexuals.\n\nThe index rates countries on a scale from 10 (freest) to 0 (least free). In 2012, the freest countries were New Zealand (8.7), the Netherlands (8.5), and Hong Kong (8.3). Least free were Zimbabwe (3.4), Burma (3.7), and Pakistan (4.5). The components on which the index is based can be divided into economic freedoms and other personal freedoms. Highest ranking in economic freedoms were Hong Kong (9.0) and Singapore (8.8). Highest ranking in personal freedoms were the Netherlands (9.5) and Uruguay (9.4).\n\nThe Freedom Index does not measure democracy, but it does measure freedom of speech and media, press killings, political imprisonment, etc. According to the report, democracy may be the form of government that best protects freedom, but democracy may both increase and reduce freedom. Nevertheless, democracy strongly correlates with freedom (7.9), as measured by the Economist Intelligence Unit's Democracy Index and the Freedom Index.\n\nThe Freedom Index is included as part of the book \"Towards a Worldwide Index of Human Freedom\", written by 13 academics and economists from Canada (Fraser Institute), the United States (Cato Institute, Emory University), Germany (Liberales Institut, Goethe-University Frankfurt am Main), and Russia (Institute of Economic Analysis). Among other claims, the report argues that the criminalization of and the war on drugs have restricted many components of freedom.\n\n\n"}
{"id": "45586", "url": "https://en.wikipedia.org/wiki?curid=45586", "title": "Indifference curve", "text": "Indifference curve\n\nIn economics, an indifference curve connects points on a graph representing different quantities of two goods, points between which a consumer is \"indifferent\". That is, the consumer has no preference for one combination or bundle of goods over a different combination on the same curve. One can also refer to each point on the indifference curve as rendering the same level of utility (satisfaction) for the consumer. In other words, an indifference curve is the locus of various points showing different combinations of two goods providing equal utility to the consumer. Utility is then a device to represent preferences rather than something from which preferences come. The main use of indifference curves is in the representation of potentially observable demand patterns for individual consumers over commodity bundles.\n\nThere are infinitely many indifference curves: one passes through each combination. A collection of (selected) indifference curves, illustrated graphically, is referred to as an indifference map.\n\nThe theory of indifference curves was developed by Francis Ysidro Edgeworth, who explained in his 1881 book the mathematics needed for their drawing; later on, Vilfredo Pareto was the first author to actually draw these curves, in his 1906 book. The theory can be derived from William Stanley Jevons' ordinal utility theory, which posits that individuals can always rank any consumption bundles by order of preference.\n\nA graph of indifference curves for several utility levels of an individual consumer is called an indifference map. Points yielding different utility levels are each associated with distinct indifference curves and these indifference curves on the indifference map are like contour lines on a topographical graph. Each point on the curve represents the same elevation. If you move \"off\" an indifference curve traveling in a northeast direction (assuming positive marginal utility for the goods) you are essentially climbing a mound of utility. The higher you go the greater the level of utility. The non-satiation requirement means that you will never reach the \"top,\" or a \"bliss point,\" a consumption bundle that is preferred to all others.\n\nIndifference curves are typically represented to be:\n\nIt also implies that the commodities are good rather than bad. Examples of bad commodities can be disease, pollution etc. because we always desire less of such things.\nConsumer theory uses indifference curves and budget constraints to generate consumer demand curves. For a single consumer, this is a relatively simple process. First, let one good be an example market e.g., carrots, and let the other be a composite of all other goods. Budget constraints give a straight line on the indifference map showing all the possible distributions between the two goods; the point of maximum utility is then the point at which an indifference curve is tangent to the budget line (illustrated). This follows from common sense: if the market values a good more than the household, the household will sell it; if the market values a good less than the household, the household will buy it. The process then continues until the market's and household's marginal rates of substitution are equal. Now, if the price of carrots were to change, and the price of all other goods were to remain constant, the gradient of the budget line would also change, leading to a different point of tangency and a different quantity demanded. These price / quantity combinations can then be used to deduce a full demand curve. A line connecting all points of tangency between the indifference curve and the budget constraint is called the expansion path.\nIn Figure 1, the consumer would rather be on \"I\" than \"I\", and would rather be on \"I\" than \"I\", but does not care where he/she is on a given indifference curve. The slope of an indifference curve (in absolute value), known by economists as the marginal rate of substitution, shows the rate at which consumers are willing to give up one good in exchange for more of the other good. For \"most\" goods the marginal rate of substitution is not constant so their indifference curves are curved. The curves are convex to the origin, describing the negative substitution effect. As price rises for a fixed money income, the consumer seeks the less expensive substitute at a lower indifference curve. The substitution effect is reinforced through the income effect of lower real income (Beattie-LaFrance). An example of a utility function that generates indifference curves of this kind is the Cobb–Douglas function formula_1. The negative slope of the indifference curve incorporates the willingness of the consumer to make trade offs.\n\nIf two goods are perfect substitutes then the indifference curves will have a constant slope since the consumer would be willing to switch between at a fixed ratio. The marginal rate of substitution between perfect substitutes is likewise constant. An example of a utility function that is associated with indifference curves like these would be formula_2.\n\nIf two goods are perfect complements then the indifference curves will be L-shaped. Examples of perfect complements include left shoes compared to right shoes: the consumer is no better off having several right shoes if she has only one left shoe - additional right shoes have zero marginal utility without more left shoes, so bundles of goods differing only in the number of right shoes they include - however many - are equally preferred. The marginal rate of substitution is either zero or infinite. An example of the type of utility function that has an indifference map like that above is the Leontief function: formula_3.\n\nThe different shapes of the curves imply different responses to a change in price as shown from demand analysis in consumer theory. The results will only be stated here. A price-budget-line change that kept a consumer in equilibrium on the same indifference curve:\n\nChoice theory formally represents consumers by a preference relation, and use this representation to derive indifference curves showing combinations of equal preference to the consumer.\n\nLet\nIn the language of the example above, the set formula_4 is made of combinations of apples and bananas. The symbol formula_5 is one such combination, such as 1 apple and 4 bananas and formula_6 is another combination such as 2 apples and 2 bananas.\n\nA preference relation, denoted formula_11, is a binary relation define on the set formula_4.\n\nThe statement\nis described as 'formula_5 is weakly preferred to formula_6.' That is, formula_5 is at least as good as formula_6 (in preference satisfaction).\n\nThe statement\nis described as 'formula_5 is weakly preferred to formula_6, and formula_6 is weakly preferred to formula_5.' That is, one is \"indifferent\" to the choice of formula_5 or formula_6, meaning not that they are unwanted but that they are equally good in satisfying preferences.\n\nThe statement\nis described as 'formula_5 is weakly preferred to formula_6, but formula_6 is not weakly preferred to formula_5.' One says that 'formula_5 is strictly preferred to formula_6.'\n\nThe preference relation formula_11 is complete if all pairs formula_33 can be ranked. The relation is a transitive relation if whenever formula_13 and formula_35 then formula_36.\n\nFor any element formula_37, the corresponding indifference curve, formula_38 is made up of all elements of formula_4 which are indifferent to formula_40. Formally,\n\nformula_41.\n\nIn the example above, an element formula_5 of the set formula_4 is made of two numbers: The number of apples, call it formula_44 and the number of bananas, call it formula_45\n\nIn utility theory, the utility function of an agent is a function that ranks \"all\" pairs of consumption bundles by order of preference (\"completeness\") such that any set of three or more bundles forms a transitive relation. This means that for each bundle formula_46 there is a unique relation, formula_47, representing the utility (satisfaction) relation associated with formula_46. The relation formula_49 is called the utility function. The range of the function is a set of real numbers. The actual values of the function have no importance. Only the ranking of those values has content for the theory. More precisely, if formula_50, then the bundle formula_46 is described as at least as good as the bundle formula_52. If formula_53, the bundle formula_46 is described as strictly preferred to the bundle formula_52.\n\nConsider a particular bundle formula_56 and take the total derivative of formula_47 about this point:\n\nor, without loss of generality,\n\nwhere formula_60 is the partial derivative of formula_47 with respect to its first argument, evaluated at formula_46. (Likewise for formula_63)\n\nThe indifference curve through formula_56 must deliver at each bundle on the curve the same utility level as bundle formula_56. That is, when preferences are represented by a utility function, the indifference curves are the level curves of the utility function. Therefore, if one is to change the quantity of formula_66 by formula_67, without moving off the indifference curve, one must also change the quantity of formula_68 by an amount formula_69 such that, in the end, there is no change in \"U\":\nThus, the ratio of marginal utilities gives the absolute value of the slope of the indifference curve at point formula_56. This ratio is called the marginal rate of substitution between formula_66 and formula_68.\n\nIf the utility function is of the form formula_75 then the marginal utility of formula_66 is formula_77 and the marginal utility of formula_68 is formula_79. The slope of the indifference curve is, therefore,\nObserve that the slope does not depend on formula_66 or formula_68: the indifference curves are straight lines.\n\nIf the utility function is of the form formula_83 the marginal utility of formula_66 is formula_85 and the marginal utility of formula_68 is formula_87.Where formula_88. The slope of the indifference curve, and therefore the negative of the marginal rate of substitution, is then\n\nA general CES (Constant Elasticity of Substitution) form is\nwhere formula_91 and formula_92. (The Cobb–Douglas is a special case of the CES utility, with formula_93.) The marginal utilities are given by\nand\nTherefore, along an indifference curve,\nThese examples might be useful for modelling individual or aggregate demand.\n\nAs used in biology, the indifference curve is a model for how animals 'decide' whether to perform a particular behavior, based on changes in two variables which can increase in intensity, one along the x-axis and the other along the y-axis. For example, the x-axis may measure the quantity of food available while the y-axis measures the risk involved in obtaining it. The indifference curve is drawn to predict the animal's behavior at various levels of risk and food availability.\n\nIndifference curve inherit the critics on the utility.\n\nHerbert Hovenkamp (1991) has argued that the presence of an endowment effect has significant implications for law and economics, particularly in regard to welfare economics. He argues that the presence of an endowment effect indicates that a person has no indifference curve (see however Hanemann, 1991) rendering the neoclassical tools of welfare analysis useless, concluding that courts should instead use WTA as a measure of value. Fischel (1995) however, raises the counterpoint that using WTA as a measure of value would deter the development of a nation's infrastructure and economic growth.\n\n\n"}
{"id": "6537184", "url": "https://en.wikipedia.org/wiki?curid=6537184", "title": "Indriya", "text": "Indriya\n\nIndriya (literally \"belonging to or agreeable to Indra\") is the Sanskrit and Pali term for physical strength or ability in general, and for the senses more specifically.\nIn Buddhism, the term refers to multiple intrapsychic processes and is generally translated as \"faculty\" or, in specific contexts, as \"spiritual faculty\" or \"controlling principle.\" \nThe term literally means \"belonging to Indra,\" chief deity in the Rig Veda and lord of the Trāyastriṃśa heaven (also known as Śakra or Sakka in Buddhism) hence connoting supremacy, dominance and control, attested in the general meaning of \"power, strength\" from the \"Rig Veda\".\n\nIn Buddhism, depending on the context, \"indriya\" traditionally refers to one of the following groups of faculties:\n\nIn the Pali Canon's Sutta Pitaka, \"indriya\" is frequently encountered in the context of the \"five spiritual faculties\" (Pali: \"\"):\nTogether, this set of five faculties is one of the seven sets of qualities lauded by the Buddha as conducive to Enlightenment.\n\nSN 48.10 is one of several discourses that characterizes these spiritual faculties in the following manner:\n\nIn SN 48.51, the Buddha declares that, of these five faculties, wisdom is the \"chief\" (\"agga\").\n\nIn AN 6.55, the Buddha counsels a discouraged monk, Sona, to balance or \"tune\" his spiritual faculties as one would a musical instrument:\n\nRelatedly, the Visuddhimagga and other post-canonical Pali commentaries caution against one spiritual faculty overpowering and inhibiting the other four faculties, and thus generally recommend modifying the overpowering faculty with the investigation of states (see \"dhamma vicaya\") or the development of tranquillity (\"samatha\"). Moreover, these commentaries especially recommend that the five spiritual faculties be developed in counterbalancing dyads:\n\nThe commentator Buddhaghosa adds:\n\nIn SN 48.43, the Buddha declares that the five spiritual faculties are the Five Powers and vice versa. He uses the metaphor of a stream passing by a mid-stream island; the island creates two streams, but the streams can also be seen as one and the same. The Pali commentaries remark that these five qualities are \"faculties\" when used to control their spheres of influence, and are \"powers\" when unshakeable by opposing forces.\n\nIn the Sutta Pitaka, six sensory faculties are referenced in a manner similar to the six sense bases. These faculties consist of the five senses with the addition of \"mind\" or \"thought\" (manas).\nThe first five of these faculties are sometimes referenced as the five material faculties (e.g., \"\").\n\nIn the Abhidhamma Pitaka, the notion of \"indriya\" is expanded to the twenty-two \"phenomenological faculties\" or \"controlling powers\" (Pali: \"\") which are:\n\nAccording to the post-canonical Visuddhimagga, the 22 faculties along with such constructs as the aggregates, sense bases, Four Noble Truths and Dependent Origination are the \"soil\" of wisdom (\"\").\n\nAt times in the Pali Canon, different discourses or Abhidhammic passages will refer to different subsets of the 22 phenomenological faculties. Thus, for instance, in the Abhidhamma there are references to the \"eightfold form-faculty\" (\"\") which includes the first five sensory faculties (eye, ear, nose, tongue and body faculties) plus the three physical faculties (femininity, masculinity and vitality).\n\n\n"}
{"id": "44527033", "url": "https://en.wikipedia.org/wiki?curid=44527033", "title": "International Business Communication Standards", "text": "International Business Communication Standards\n\nThe International Business Communication Standards (IBCS) are practical proposals for the design of business communication published for free use under a Creative-Commons-Lizenz (CC BY-SA). In most cases, applying IBCS means the proper conceptual, perceptual and semantic design of charts and tables.\n\nBusiness Communication meets the IBCS Standards if it complies with the three rule sets comprising the three pillars of IBCS:\n\nIBCS Notation is the designation for the semantic rule set suggested by IBCS. IBCS Notation covers the unification of terminology (e.g. words, abbreviations, and number formats), descriptions (e.g. messages, titles, legends, and labels), dimensions (e.g. measures, scenarios, and time periods), analyses (e.g. scenario analyses and time series analyses), and indicators (e.g. highlighting indicators and scaling indicators).\n\nThe review and further development of the IBCS is an ongoing process controlled by the IBCS Association. The IBCS Association is a non-profit organization that publishes the Standards for free and engages in extensive consultation and discussion prior to issuing new versions. This includes worldwide solicitation for public comment. Release of IBCS Version 1.0: The active members accepted the released Version 1.0 of the IBCS Standards at the General Assembly of June 18, 2015 in Amsterdam. Current themes of the further development of the IBCS standards were discussed at the Annual Conference in Warsaw of June 3, 2016. The version 1.1 of the standards were confirmed by the active members at the Annual Conference in Barcelona of June 1, 2017. More than 80 professionals of 12 counties attended the Annual Conference.\nThe Annual Conference in London of June 8, 2018 took place at the Headquarters to The Institute of Chartered Accountants in England and Wales.\n\n\n"}
{"id": "14018719", "url": "https://en.wikipedia.org/wiki?curid=14018719", "title": "Intra-organismic perspective", "text": "Intra-organismic perspective\n\nIntra-organismic perspective is a theory developed by Bowlby that states infants have an innate desire to develop attachments to other individuals. This desire is also thought to have been a result of natural selection.\n"}
{"id": "3252724", "url": "https://en.wikipedia.org/wiki?curid=3252724", "title": "Intuition pump", "text": "Intuition pump\n\nAn intuition pump is a thought experiment structured to allow the thinker to use their intuition to develop an answer to a problem.\n\nThe term was coined by Daniel Dennett. In \"Consciousness Explained\", he uses the term to describe John Searle's Chinese room thought experiment, characterizing it as designed to elicit intuitive but incorrect answers by formulating the description in such a way that important implications of the experiment would be difficult to imagine and tend to be ignored.\n\nIn the case of the Chinese room argument, Dennett considers the intuitive notion that a person manipulating symbols seems inadequate to constitute any form of consciousness, and says that this notion ignores the requirements of memory, recall, emotion, world knowledge, and rationality that the system would actually need to pass such a test. \"Searle does not deny that programs can have all this structure, of course\", Dennett says. \"He simply discourages us from attending to it. But if we are to do a good job imagining the case, we are not only entitled but obliged to imagine that the program Searle is hand-simulating has all this structure—and more, if only we can imagine it. But then it is no longer obvious, I trust, that there is no genuine understanding of the joke going on.\"\n\nIn his 1984 book, \"Elbow Room\", Dennett used the term in a positive sense to describe thought experiments which facilitate the understanding of or reasoning about complex subjects by harnessing intuition:\n\n"}
{"id": "6747501", "url": "https://en.wikipedia.org/wiki?curid=6747501", "title": "JGraph", "text": "JGraph\n\nJGraph is a graph drawing open source software component available for the Java, C#, and JavaScript languages. It was started as a pure Java language software project by Gaudenz Alder and as a University project in 2000 at ETH Zurich, Switzerland.\n\nThe original design for JGraph was to make it an architectural extension of for the Java language.\n\nThe initial public release of JGraph was in May 2002. After the JGraph 5.x branch the project took the version numbering from \"mxGraph\", which is the JavaScript version of the library, since the two projects shared API by version. The Java branch was renamed as JGraphX and started again from release 1.0. The summarized history of JGraphX since public release is available in the library change log.\n\n\n\n"}
{"id": "45481573", "url": "https://en.wikipedia.org/wiki?curid=45481573", "title": "Jatismara", "text": "Jatismara\n\nJātismara (Sanskrit: जातिस्मर) means - recollecting a former existence or birth. Such recollection is believed to be a talent which great saints possessed or cultivated.\n\nIn the Buddhist Nikāya and Āgama literature there is reference to \"jātismara\" as first of the three \"vidyās\" ('sciences'), as the fourth of the five \"abhijñās\" ('superknowledges') and as the eighth of the ten \"tathagātadaśabala\" (powers of a \"tathagata\"); it is listed as a faculty connected with the higher stages of meditation as a yogic attainment through control of the body and purity of body and conduct, as the result of abiding in a particular \"samādhi\". The Mahayāna Buddhist literature refers to \"jātismara\" not as an individual’s meditational development but as effected by a Bodhisattva for improving religious life, or as a religious gain, as an \"anuśamsa\" ('blessing') through a third kind of non-meditational activity but connected with the sacred texts and with \"dhāranīs\".\n\nAccording to the Naradiya Purana observance of Ekadashi Vrata ('fast') can make a sinless person a \"Jātismara\". The \"Jātismara Vrata\" requires the fasting person to remain silent till the moon rises. The Vishnu Purana speaks of Shavya who was born a \"jātismara\"-daughter of the king of Kāshi.\n\nBhagavata Purana (III.xxvi.30) tells us that memory is a characteristic of intelligence, that \"maya\" clouds intelligence and causes false identification (III.xxxi.20), that a person when born is bereft of memory (III.xxxi.23) when all wisdom gained in the past birth(s) is lost (III.xxxi.24). Memory is the link between body and soul. And, the Mahabharata speaks about the place the four seas meet bathing where one has immunity from misfortune; bathing then in \"Jatismara\" with pure mind and senses one acquires the recollections of his former life.\n"}
{"id": "21335877", "url": "https://en.wikipedia.org/wiki?curid=21335877", "title": "Jointness (psychodynamics)", "text": "Jointness (psychodynamics)\n\nJointness is a term (R. Solan 1991) in psychoanalysis and psychodynamic theory, describing a new look at normal object relation that takes place from the beginning of life. Till nowadays symbiosis (propounded by Margaret Mahler 1968, 1975) is the common term for a normal object relation, while Ronnie Solan emphasizes that symbiosis represents impairment in object relation.\n\nJointness is defined as a dynamic process representing an emotional system for attachment and for communication between separate individuals who jointly approach each other in a third, joint, virtual space. Jointness represents an encounter between mother and infant, psychotherapist and patient, or any partners experiencing simultaneously mutual intimacy, while concomitantly safeguarding separateness.\n\nThe newborn, very early in life, perceives the other, even his mother, as a \"not-I\" (it indicates a psychic process to safeguard one's own self), and is attached to the \"not-I mother\" by an intimate acquaintance with her through his/her senses. When both, mother and baby, devote themselves to intimacy, that temporarily blurs the boundaries between them, separateness can be safeguarded. As a result, baby might gradually develop his/her own boundaries and acknowledge those of his or her object and might invest their own innate abilities to participate in human interactions and enjoy relationships (\"motivational systems,\" Emde, 1988).\n\nThe development of this basic process of jointness between baby and mother depends on mother's capacity to tolerate separateness. It is the mother who imprints the quality and the intensity of the rapprochement-separation balancing process in their relationship, while both of them are fully invested in each other.\n\nThe unique jointness and the unique communication, in a unique psychic virtual space are created by the sharing of interests (emotional or cognitive), and by the mutual investment of partners in a joint phenomenon, object, or idea, meaningful to both. All vital human communication represents both the separateness of the two (or more) individuals and their joining in a third virtual space. Thus, \"jointness\" elicits the triadic (triangulation) object relations (mother's space – \"virtual transitional space\" – baby's space).\n\nIn this type of transitional space baby and mother, lovers, or partners of a common task, are jointly determine the extent of rapprochement between themselves, the extent of safeguarding separateness and also the moment to separate. Each of them is sensorily attentive to the strangeness and the separateness of the “non-I” that the other represents for him. Such a dynamic process of jointness, represents healthy development from birth, paves the way to a sense of individuation and culminates to establish the valuable communication with others relating to their otherness, while preserving separateness and self integrity.\n\nIt is important to distinguish between jointness and symbiosis. Both may commence with the beginning of life; they may seem similar, and yet they are widely different experiences. In symbiosis, baby and mother behave and function as though they were \"an omnipotent system - a dual unity within one common boundary\" (Margaret Mahler, 1968, p. 201). Partners of symbiosis can be fully satisfied as long as there is no hint of separateness. Jointness, on the other hand, represents both the separateness of the two (or more) individuals and their joining in a third virtual space.\n\nThe development of this basic process (between baby and mother) in symbiosis depends on mother's inability to endure separateness while both of them are fully invested in each other. It is the mother who imprints on their encounter her need to contain herself with her baby in one unit and to prevent the encouragement of separateness in favor of the boundaries of their unity. Both partners will be motivated, through life, by a powerful need for merging and they will remain almost addicted to finding another object to merge with and attach their symbiotic needs, even at the expense of sacrificing their individuation, their true-self and their self-esteem. Such an encounter fosters only a dyadic relation where the \"transitional virtual space\" between them is missing.\n\nHence, symbiosis is a dyadic pathological process, even from the beginning of life that results in self fragility, narcissistic disturbances and in an immature personality; while jointness represents a triadic healthy development that depends on healthy narcissism and generates separation-individuation, communication and relationship.\n\n\n"}
{"id": "2301309", "url": "https://en.wikipedia.org/wiki?curid=2301309", "title": "KMA (art)", "text": "KMA (art)\n\nKMA is a collaboration between media artists Kit Monkman and Tom Wexler (UK). KMA's work is primarily focused on the use of projected light to transform spaces and the interactions of people within those spaces. \n\nThe idea of people gathering after dark to enact and / or watch a drama or ritual lies deep inside us and our ancestral history. It is surely one of the oldest, simplest and most essential of human responses to our fate.\n\nKMA’s work seeks to explore this impulse in the context of the modern city. By combining sophisticated interactive technologies with an emotional narrative the work choreographs pedestrian’s movement; it builds, sustains, and develops complex, physically networked, relations between the body, the individual, the crowd, and the city.\n\nKMA are best known for large scale public interactive works that use projected light and motion tracking technology to create immersive digital 'playgrounds' in existing public spaces. 'Flock' - based on 3 sections of Tchaikovsky's Swan Lake - typifies this approach, and was presented at Trafalgar Square, London as a co-commission by the Institute of Contemporary Arts and The Royal Opera House, Covent Garden.\n\nCongregation (2010) was commissioned to represent the UK Pavilion at the Shanghai World Expo.\n\n\n\n"}
{"id": "16831", "url": "https://en.wikipedia.org/wiki?curid=16831", "title": "Kick", "text": "Kick\n\nA kick is a physical strike using the leg, in unison usually with an area of the knee or lower using the foot, heel, tibia, ball of the foot, blade of the foot, toes or knee (the latter is also known as a knee strike). This type of attack is used frequently by hooved animals as well as humans in the context of stand-up fighting. Kicks play a significant role in many forms of martial arts, such as savate, taekwondo, MMA, sikaran, karate, Pankration, Kung Fu, Vovinam, kickboxing, Muay Thai, Yaw-Yan, capoeira, silat, and kalaripayattu.\n\nKicking is also prominent from its use in many sports, especially those called football. The best known of these sports is association football, also known as soccer.\n\nThe English verb to kick appears only in the late 14th century, apparently as a loan from Old Norse, originally in the sense of a hooved animal delivering strikes with his hind legs; the oldest use is Biblical.\n\nKicks as an act of human aggression have likely existed worldwide since prehistory. However, high kicks, aiming above the waist or to the head appear to have originated from Asian martial arts. Such kicks were introduced to the west in the 19th century with early hybrid martial arts inspired by Asian styles such as Bartitsu and Savate. Practice of high kicks became more universal in the second half of the 20th century with the more widespread development of hybrid styles such as kickboxing and eventually mixed martial arts.\n\nThe history of the high kick in Asian martial arts is difficult to trace. It appears to be prevalent in all traditional forms of Indochinese kickboxing, but these cannot be traced with any technical detail to pre-modern times. For example, Muay Boran or \"ancient boxing\" in Thailand was developed under Rama V (r. 1868-1910). While it is known that earlier forms of \"boxing\" existed during the Ayutthaya Kingdom, the details regarding these techniques are unclear. Some stances that look like low kicks, but not high kicks, are visible in the Shaolin temple frescoes, dated to the 17th century. The \"Mahabharata\" (4.13), an Indian epic compiled at some point before the 5th century AD, describes an unarmed hand-to-hand battle, including the sentence \"and they gave each other violent kicks\" (without providing any further detail).\n\nAs the human leg is longer and stronger than the arm, kicks are generally used to keep an opponent at a distance, surprise him or her with their range, and inflict substantial damage. On the other hand, stance is very important in any combat system, and any attempt to deliver a kick will necessarily compromise one's stability of stance. The application of kicks is thus a question of the tradeoff between the power that can be delivered vs. the cost incurred to balance. Since combat situations are fluid, understanding this tradeoff and making the appropriate decision to adjust to each moment is key.\n\nKicks are commonly directed against helpless or downed targets, while for more general self-defense applications, the consensus is that simple kicks aimed at vulnerable targets below the chest may be highly efficient, but should be executed with a degree of care. Self-defense experts, such as author and teacher Marc Macyoung, claim that kicks should be aimed no higher than the waist/stomach. Thus, the fighter should not compromise their balance while delivering a kick, and retract the leg properly to avoid grappling. It is often recommended to build and drill simple combinations that involve attacking different levels of an opponent. A common example would be distracting an opponent's focus via a fake jab, following up with a powerful attack at the opponent's legs and punching.\n\nFurther, since low kicks are inherently quicker and harder to see and dodge in general they are often emphasized in a street fight scenario.\n\nThe utility of high kicks (above chest level) has been debated.\n\nProponents have viewed that some high front snap kicks are effective for striking the face or throat, particularly against charging opponents, and flying kicks can be effective to scare off attackers. Martial arts systems that utilize high kicks also emphasize training of very efficient and technically perfected forms of kicks, include recovery techniques in the event of a miss or block, and will employ a wide repertoire of kicks adapted to specific situations.\n\nDetractors have asserted that the flying/jumping kicks performed in synthesis styles are primarily performed for conditioning or aesthetic reasons while the high kicks as practiced in sport martial arts are privileged due to specialized tournament rules, such as limiting the contest to stand-up fighting, or reducing the penalty resulting from a failed attempt at delivering a kick.\n\nAlthough kicks can result in an easy takedown for the opponent if they are caught or the resulting imbalance is exploited, kicks to all parts of the body are very present in mixed martial arts, with some fighters employing them sporadically, while others, like Lyoto Machida, Edson Barboza and Donald Cerrone rely heavily on their use and have multiple knockouts by kicks on their resumee.\n\nDelivering a front kick involves raising the knee and foot of the striking leg to the desired height and extending the leg to contact the target. The actual strike is usually delivered by the ball of the foot for a forward kick or the top of the toes for an upward kick. Taekwondo practitioners utilize both the heel and ball of the foot for striking. Various combat systems teach 'general' front kick using the heel or whole foot when footwear is on. Depending of fighter's tactical needs, a front kick may involve more or less body motion. Thrusting one's hips is a common method of increasing both reach and power of the kick. The front kick is typically executed with the upper body straight and balanced. Front kicks are typically aimed at targets below the chest: stomach, thighs, groin, knees or lower. Highly skilled martial artists are often capable of striking head-level targets with front kick.\n\nThis is the most commonly used kick in kickboxing due to its power and ease of use. In most styles, the instep is used to strike, though most Karate styles would allow the shin as official technique for a street fight. To execute, the attacker swings their leg sideways in a circular motion, kicking the opponent's side with the front of the leg, usually with the instep, ball of the foot, toe, or shin. Also performable is a 360-degree kick in which the attacker performs a full circle with their leg, in which the striking surface is generally either the instep, shin or ball of the foot.\n\nThere are many variations of the roundhouse kick based on various chambering of the cocked leg (small, or full, or universal or no chambering) or various footwork possibilities (rear-leg, front-leg, hopping, switch, oblique, dropping, ground spin-back or full 360 spin-back). An important variation is the downward roundhouse kick, nicknamed the Brazilian Kick from recent K-1 use: A more pronounced twist of the hips allows for a downward end of the trajectory of the kick that is very deceiving.\n\nDue to its power, the roundhouse kick may also be performed at low level against targets, such as the knees, calf, or even thigh, since attacking leg muscles will often cripple an opponent's mobility.\n\nThe side kick refers to a kick that is delivered sideways in relation to the body of the person kicking. It is one of the most adaptable kicks, useful as both an offensive move and as a defensive counter to a blitzing opponent. There are two areas that are commonly used as impact points in sidekicks: the heel of the foot or the outer edge of the foot. The heel is more suited to hard targets such as the ribs, stomach, jaw, temple and chest. However, when executing a side kick with the heel, the toes should be pulled back so that they only make contact the heel and not with the whole foot. If a person hits with the arch or the ball of the foot, it can injure the foot or break an ankle. A standard sidekick is performed by first chambering the kicking leg diagonally across the body, then extending the leg in a linear fashion toward the target, while flexing the abdominals.\n\nAnother way of doing the side kick is to make it an end result of a faked roundhouse. This technique is considered antiquated, and used only after an opponent is persuaded to believe it is a roundhouse, and then led to believe that closing the distance is best for an upper body attack, which plays into the tactical position and relative requirement of this version of the side kick. In Korean, \"yeop chagi\". In Okinawan te fighting, it is sometimes called a \"dragon kick\". Some have called this side kick a \"twist kick\" due to its roundhouse like origins. This side kick begins as would a roundhouse kick however the practitioner allows the heel to move towards the center of the body. The kick is then directed outward from a cross-leg chamber so that the final destination of the kick is a target to the side, rather than one that is directly ahead.\n\nAlso referred to as a donkey kick, mule kick, or turning back kick. This kick is directed backwards, keeping the kicking leg close to the standing leg and using the heel as a striking surface. In wushu, this kick is called the \"half-moon\" kick but involves the slight arching of the back and a higher lift of the leg to give a larger curvature. It is often used to strike opponents by surprise when facing away from them.\n\nThese are often complicated variations of basic kicks, either with a different target or combined with another move, such as jumping.\n\nIn Japanese, \"kakato-geri\" or \"kakato-otoshi\"; in Korean, \"doki bal chagi\" or \"naeryeo chagi\" or \"chikka chagi\". In Chinese, \"pigua tui\" or \"xiapi tui\".\n\nAn axe kick, also known as a \"hammer kick\" or \"stretch kick\", is characterized by a straightened leg with the heel descending onto an opponent like the blade of an axe. It begins with one foot rising upward as in a crescent kick. The upward arc motion is stopped, and then the attacking foot is lowered so as to strike the target from above. The arc can be performed in either an inward (counter-clockwise) or outward (clockwise) fashion.\n\nA well-known proponent of the axe kick was Andy Hug, the Swiss Kyokushinkai Karateka who won the 1996 K-1 Grand Prix.\n\nThe butterfly kick is done by doing a large circular motion with both feet in succession, making the combatant airborne. There are many variations of this kick. The kick may look like a slanted aerial cartwheel, and at the same time, the body spins horizontally in a circle. It begins as a jump with one leg while kicking with the other, then move the kicking leg down and the jumping leg up into a kick, landing with the first kicking leg, all while spinning. This kick involves also the arching the back backwards when airborne to give a horizontal body with high angled legs to the horizontal. It may also resemble a jumping spin roundhouse kick (developed by James 'Two Screens' Perkins) into a spinning hook kick, all in one jump and one spin although the difference is that both legs should remain in the air at the same time for a considerable amount of time.\n\nFirst practiced in Chinese martial arts, the butterfly kick, or \"xuan zi\", is widely viewed as ineffective for actual combat. However, its original purpose was to evade an opponent's floor sweep and flip to the antagonist's exposed side or it may be used as a double aerial kick to an opponent standing off to the side. It is now widely used in demonstrative wushu forms (taolu) as a symbol of difficulty. Also note the similarity in execution when compared to an ice skating maneuver known as a flying camel spin (aka: Button Camel).\n\nThis strike is a low roundhouse kick that hits the backside of the calf with the shin. While a Calf Kick sacrifices range in comparison to a standard low roundhouse kick to the thigh, the Calf Kick can't be checked with a knee or grabbed with an arm making it a safer kick for a striker in MMA matches vs opponents capable of checking low kicks or grapplers looking for takedown opportunities.\n\nIn Japanese, \"mikazuki Geri\"; in Korean, \"bandal chagi\" (반달 차기).\n\nThe crescent kick, also referred to as a 'swing' kick, has some similarities to a hook kick, and is sometimes practised as an off-target front snap kick. The leg is bent like the front kick, but the knee is pointed at a target to the left or right of the true target. The energy from the snap is then redirected, whipping the leg into an arc and hitting the target from the side. This is useful for getting inside defenses and striking the side of the head or for knocking down hands to follow up with a close attack. In many styles of T'ai chi ch'uan and Kalaripayattu, crescent kicks are taught as tripping techniques. When training for crescent kicks, it is common to keep the knee extended to increase the difficulty. This also increases the momentum of the foot and can generate more force, though it takes longer to build up the speed.\n\nThe inward/inner/inside crescent hits with the inside edge of the foot. Its arch is clockwise for the left leg and counter-clockwise for the right leg. Force is generated by both legs' hip adduction. The inward variant has also been called a \"hangetsu geri\" (Half moon kick) in karate and is employed to \"wipe\" an opponents hand off of one's wrist. It can quickly be followed up by a low side-blade kick to the knee of the offender.\n\nThe outward/outer/outside crescent hits with the 'blade', the outside edge of the foot. Its path is counter-clockwise for the left leg and clockwise for the right leg, and force is generated by both legs' hip abduction. This is similar to a rising side kick, only with the kicking leg's hip flexed so that the line of force travels parallel to the ground from front to side rather than straight up, beginning and ending at the side.\n\nIn Korean, \"huryeo chagi\" (후려 차기) or \"golcho chagi\".\n\nThe hook kick strikes with the heel from the side (or flat of the foot in sparring). It is executed similar to a side kick. However, the kick is intentionally aimed slightly off target in the direction of the kicking foot's toes. At full extension, the knee is bent and the foot snapped to the side, impacting the target with the heel. In Taekwondo it is often used at the resulting miss of a short slide side kick to the head, but is considered a very high level technique in said circumstance. Practitioners of jeet kune do frequently use the term \"heel hook kick\" or \"sweep kick\". It is known as Gancho in Capoeira.\n\nThere are many variations of the hook kick, generally based on different footworks: rear- or front-leg, oblique or half-pivot, dropping, spin-back and more. The hook kick can be delivered with a near-straight leg at impact, or with a hooked finish (Kake in Japanese Karate) where the leg bends before impact to catch the target from behind. An important variation is the downward hook kick, delivered as a regular or a spin-back kick, in which the end of the trajectory is diagonally downwards for a surprise effect or following an evading opponent.\n\nThe hook kick is mainly used to strike the jaw area of an opponent, but is also highly effective in the temple region.\n\nIn Japanese, ; in Korean, \"bandae dollyo chagi\" (반대 돌려 차기), \"dwit hu ryo chagi\", \"nakkio mom dollyo chagi\" or \"parryo chagi\".\n\nThis kick is also known as a \"heel kick\", \"reverse turning kick\", \"reverse round kick\", \"spinning hook kick\", \"spin kick\", or \"wheel kick\". A low reverse roundhouse is also known as a \"Sweep Kick\". This kick traditionally uses the heel to strike with. The kicking leg comes from around the kicker's back and remains straight, unlike a reverse hooking kick. See above for more on hook kicks. Variations exist for low, middle and high height. Spinning and leaping variations of the kick are also popular, and are often showcased in film and television media. Edson Barboza executed the first wheel kick for a knockout in the UFC at UFC 142:Aldo vs. Mendes. He knocked out Terry Etim 3:23 into the third round of their fight. Mickie James uses this move calling it \"Mick-Kick\"\n\nA different kick that is similarly named also exists. It is literally a roundhouse kick performed by turning as if for a \"back straight kick\" and executing a roundhouse kick. It is known as a \"Reverse Roundhouse Kick\" because the kicker turns in the opposite, or \"reverse\", direction before the kick is executed. This kick strikes with the ball of the foot for power or the top of the foot for range. The kick was exhibited by Bruce Lee on numerous occasions in his films Enter the Dragon, Fist of Fury and The Big Boss. Bill Wallace was also a great user of this kick, as seen in his fight with Bill Briggs, where he KO'd his opponent with the clocked 60 mph kick. The Jump Spin Hook Kick was popularized in the mid-eighties by Steven Ho in open martial art competitions.\n\nIn Olympic format (sport) taekwondo, this technique is performed using the balls of the feet, and in a manner similar to a back thrust, rather than the circular technique adopted in other styles/Martial Arts.\n\nA flying kick, in martial arts, is a general description of kicks that involve a running start, jump, then a kick in mid-air. Compared to a regular kick, the user is able to achieve greater momentum from the run at the start. Flying kicks are not to be mistaken for jumping kicks, which are similar maneuvers. A jumping kick is very similar to a flying kick, except that it lacks the running start and the user simply jumps and kicks from a stationary position. Flying kicks are often derived from the basic kicks. Some of the more commonly known flying kicks are the: flying side kick, flying back kick and the flying roundhouse kick, as well as the flying reverse roundhouse kick. Flying kicks are commonly practiced in Taekwondo, Karate, Wushu, and Muay Thai for fitness, exhibitions and competition. It is known as \"tobi geri\" in Japanese martial arts, and \"twyo chagi\" in Taekwondo.\n\nSeveral kicks may be called a scissor kick, involving swinging out the legs to kick multiple targets or using the legs to take down an opponent.\n\nThe popularized version of a scissor kick is, while lying down, or jumping, the kicker brings both legs to both sides of the opponent's legs or to their body and head, then brings both in as a take down (as the name states, leg motions are like that of a pair of scissors).\n\nThe scissor kick in Taekwondo is called \"kawi chagi\". In Capoeira it is called tesoura (scissors).\n\nScissor kicks and other variants are also commonly applied in Vovinam.\n\nA spinning heel kick is where the artist turns his/her body 360 degrees before landing the heel or the ball of his/her foot on the target. It is found in Muay Thai and is known in Capoeira as armada.\n\nThe vertical kick involves bringing the knee forward and across the chest, then swinging the hip while extending the kicking leg outward, striking with the outside (\"sword\") edge of the foot. It can deliver a considerable amount of power. This is called a \"yoko geri keage\" in karate.\n\nIn Taekwondo, the vertical kick is called \"sewo chagi\", and can be performed as either an inward (\"anuro\") or outward (\"bakuro\") kick.\n\nIn Japanese karate, the term \"ren geri\" is used for several kicks performed in succession. Old karate did not promote the use of the legs for weapons as much as modern karate does, seeing them as being too open for countering. However, in modern sport karate (non-traditional) competitions, the ability to use multiple kicks without setting the foot down has become a viable option, not only for effectiveness but also for stylish aesthetics.\n\nIn Taekwon-Do, three types of multiple kick are distinguished:\n\nDouble kick (\"i-jung chagi\") - two kicks of the same type executed in succession by the same foot in the same direction.\n\nConsecutive kick (\"yonsok chagi\") - two or more kicks executed in succession by the same foot but in different directions, or with different attacking tools.\n\nCombination kick (\"honhap chagi\") - two or more kicks executed in succession by both feet.\n\nOne such Multiple Kick commonly seen in Taekwon-Do, is a slightly complex Side Kick where a High Side Kick is followed by a Low Side Kick which is in turn followed by a more powerful Side Kick. This combination is done rapidly and is meant not for multiple targets but for a single one. The Multiple Kick usually targets the face, thigh, and chest, but in turn can be a multiple chest attack which is useful for knocking the breath out of an attacker. The Multiple Kick is usually done in the \"second\" style described in the Side Kick article which \"involves shooting the leg forward as in a front kick and then pivoting and turning so\" to actually deliver a side kick. That style \"has far less power but is much faster and more deceptive\", which is what the Multiple Kick was designed for. The Multiple Kick, unlike some Side Kicks or \"side blade kicks\", never uses the outer edge of the foot; it is intended solely for the heel to be used as the impact point. Depending on the strength and skill of the attacker and the attacked, the combination can be highly effective or highly ineffective when compared to more pragmatic attacks. In some encounters with highly trained and conditioned fighters, multiple side-kicks have seen disastrous results against the abs of their targets.\n\nThe Showtime kick gained notability after being used by mixed martial artist Anthony Pettis, during his fight gainst Benson Henderson on December 16, at WEC 53 for the WEC Lightweight Championship.In the fifth round Pettis ran up the cage, jumped off the cage, then landed a switch kick while airborne. Sports reporters later named this the Showtime Kick.. The kick was also used by mixed martial artists: Zabit Magomedsharipov and others. The kick was featured in the movie Here Comes the Boom.\n\n\n"}
{"id": "44357613", "url": "https://en.wikipedia.org/wiki?curid=44357613", "title": "Källén function", "text": "Källén function\n\nThe Källén function, also known as triangle function, is a polynomial function in three variables, which appears in geometry and particle physics. In the latter field it is usually denoted by the symbol formula_1. It is named after the theoretical physicist Gunnar Källén, who introduced it as a short-hand in his textbook \"Elementary Particle Physics\".\n\nThe function is given by a quadratic polynomial in three variables\n\nIn geometry the function describes the area formula_3 of a triangle with side lengths formula_4:\nSee also Heron's formula.\n\nThe function appears naturally in the Kinematics of relativistic particles, e.g. when expressing the energy and momentum components in the center of mass frame by Mandelstam variables.\n\nThe function is (obviously) symmetric in permutations of its arguments, as well as independent of a common sign flip of its arguments:\n\nIf formula_7 the polynomial factorizes into two factors\n\nIf formula_9 the polynomial factorizes into four factors\n\nIts most condensed form is\n"}
{"id": "51474451", "url": "https://en.wikipedia.org/wiki?curid=51474451", "title": "Menger space", "text": "Menger space\n\nIn mathematics, a Menger space is a topological space that satisfies a certain a basic selection principle that generalizes σ-compactness. A Menger space is a space in which for every sequence of open covers formula_1 of the space there are finite sets formula_2 such that the family formula_3 covers the space.\n\nIn 1924, Karl Menger \nintroduced the following basis property for metric spaces: \nEvery basis of the topology contains a countable family of sets with vanishing \ndiameters that covers the space. Soon thereafter, \nWitold Hurewicz \nobserved that Menger's basis property can be reformulated to the above form using sequences of open covers.\n\nMenger conjectured that in ZFC every Menger metric space is σ-compact. \nFremlin and Miller \nproved that Menger's conjecture is false, by showing that there is,\nin ZFC, a set of real numbers that is Menger but not σ-compact. \nThe Fremlin-Miller proof was dichotomic, and the set witnessing the failure\nof the conjecture heavily depends on whether a certain (undecidable) axiom\nholds or not.\n\nBartoszyński and Tsaban\ngave a uniform ZFC example of a Menger subset of the real line that is not σ-compact.\n\nFor subsets of the real line, the Menger property can be characterized using continuous functions into the Baire space formula_4.\nFor functions formula_5, write formula_6 if formula_7 for all but finitely many natural numbers formula_8. A subset formula_9 of formula_4 is dominating if for each function formula_11 there is a function formula_12 such that formula_13. Hurewicz proved that a subset of the real line is Menger iff every continuous image of that space into the Baire space is not dominating. In particular, every subset of the real line of cardinality less than the dominating number formula_14 is Menger.\n\nThe cardinality of Bartoszyński and Tsaban's counter-example to Menger's conjecture is\nformula_14.\n\n"}
{"id": "51783272", "url": "https://en.wikipedia.org/wiki?curid=51783272", "title": "No-no paradox", "text": "No-no paradox\n\nThe No–no paradox is a distinctive paradox belonging to the family of the semantic paradoxes (like the Liar paradox). It derives its name from the fact that it consists of two sentences each simply denying what the other says.\n\nA variation on the paradox occurs already in Thomas Bradwardine’s \"Insolubilia\". The paradox itself appears as the eighth sophism of chapter 8 of John Buridan’s \"Sophismata\". Although the paradox has gone largely unnoticed even in the course of the 20th-century revival of the semantic paradoxes, it has recently been rediscovered (and dubbed with its current name) by the US philosopher Roy Sorensen, and is now appreciated for the distinctive difficulties it presents.\n\nThe notion of truth seems to be governed by the naive schema:\n\n(where we use single quotes to refer to the linguistic expression inside the quotes). Consider however the two sentences:\n\nReasoning in classical logic, there are four possibilities concerning (N) and (N):\n\n\nYet, possibilities 1. and 2. are ruled out by the instances of (T) for (N) and (N). To wit, possibility 1. is ruled out because, if (N) is true, then, by (T), (N) is not true; possibility 2. is ruled out because, if (N) is not true, then, by (T), (N) is true. It would then seem that either of possibilities 3. and 4. should obtain. Yet, both of those possibilities would also seem repugnant, as, on each of them, two perfectly symmetrical sentences would mysteriously diverge in truth value.\n\nGenerally speaking, the paradox instantiates the problem of determining the status of ungrounded sentences that are not inconsistent. More in particular, the paradox presents the challenge of expanding one’s favourite theory of truth with further principles which either express the symmetry intuition against possibilities 3. and 4. or make them acceptable in spite of their intuitive repugnancy. Because (N) and (N) do not lead to inconsistency, a certain strand in the discussion of the paradox has been willing to assume both the relevant instances of (T) and classical logic, thereby deriving the conclusion that either possibility 3. or possibility 4. holds. Such conclusion has in turn been taken to have momentous consequences for certain influential philosophical theses. Consider, for example, the thesis of truthmaker maximalism:\n\nIf, as per possibilities 3. and 4., one of (N) or (N) is true and the other one is not true, then, given the symmetry between the two sentences, it might seem that there is nothing that makes true whichever of the two is in fact true. If so, (TM) would fail. These and similar conclusions have however been contested by other philosophers on the grounds that, as evidenced by Curry's paradox, joint reliance on (T) and classical logic might be problematic even when it does not lead to inconsistency.\n"}
{"id": "38053200", "url": "https://en.wikipedia.org/wiki?curid=38053200", "title": "Oikeiôsis", "text": "Oikeiôsis\n\nIn Stoic ethics, oikeiôsis (, ) is a technical term variously translated as \"appropriation,\" \"orientation,\" \"familiarization,\" \"affinity,\" \"affiliation,\" and \"endearment.\" \"Oikeiôsis\" signifies the perception of something as one’s own, as belonging to oneself. The theory of \"oikeiôsis\" can be traced back to the work of the first Stoic philosopher, Zeno of Citium.\nThe Stoic philosopher Hierocles saw it as the basis for all animal impulses as well as human ethical action. According to Porphyry, \"those who followed Zeno stated that \"oikeiôsis\" is the beginning of justice\".\n\n\"Oikeiôsis\" is rooted in the word \"oikos\" (οἶκος). Oikos is the word for household, house, or family, and can be seen in modern English words like economics and ecology (Greek oiko- to Classical Latin oeco- to Medieval Latin eco-). Similarly, the term \"Oikeiotes\" denotes the sense of belonging, the opposite of alienation. The term invokes the sense of being \"at home\", of belonging to and by extension becoming \"familiarized\" with something.\n\nIn his \"Elements of Ethics\" (), the philosopher Hierocles began his account of \"oikeiôsis\" by looking at the beginning of the life of animals. In the initial stage of perception, an animal is only aware of their bodies and sensations as \"belonging to itself\", this awareness is the \"proton oikeion\", the \"first thing that is one's own and familiar\". This self-awareness is continuous as well as dependent on the perception of external objects. This is why according to Hierocles, children are afraid of the dark, because their weak sense of self fears death in the absence of external entities. Hierocles argued that the impulse of self-preservation arises out of \"oikeiôsis\": \"an animal, when it has received the first perception of itself, immediately becomes its own and familiar to itself and to its constitution\". In perceiving itself and becoming familiar to itself, an animal finds value in itself and its own well-being.\n\nHierocles divided the many forms of \"Oikeiôsis\" as internal and external. Internal forms of \"oikeiôsis\" included appropriation of the self as well as of one's constitution, external forms included familiarization with other people and an orientation towards external goods. \"Oikeiôsis\" is the basis for Hierocles' theory of \"appropriate acts\" () because it is in \"accordance with nature\" since animals use appropriation to project themselves externally and thus care for others (such as their offspring). Stoics see these acts as a duty because, according to Cicero, \"all duties derive from principles of nature\". In Hierocles' other ethical work, \"On Appropriate acts\" (of which only fragments survive), he outlined a theory of duty based on concentric circles. Beginning with the self and then our immediate family, Hierocles outlined how humans can extend their \"oikeiôsis\" towards other human beings in widening circles, such as our ethnos and eventually the entire human race. The distance from the center acts as a standard by which we may measure the strength of our ties and therefore our duties towards other people. Hierocles argued that there was an ethical need for a \"contraction of circles\", to reduce the distance between the circles as much as possible and therefore increase our familiarization with all of mankind (while still retaining the strongest affinity within our immediate circle).\n"}
{"id": "2690589", "url": "https://en.wikipedia.org/wiki?curid=2690589", "title": "Philosophical interpretation of classical physics", "text": "Philosophical interpretation of classical physics\n\nClassical \"Newtonian\" physics has, formally, been replaced by quantum mechanics on the small scale and relativity on the large scale. Because most humans continue to think in terms of the kind of events we perceive in the human scale of daily life, it became necessary to provide a new philosophical interpretation of classical physics. Classical mechanics worked extremely well within its domain of observation but made inaccurate predictions at very small scale - atomic scale systems - and when objects moved very fast or were very massive. Viewed through the lens of quantum mechanics or relativity, we can now see that classical physics, imported from the world of our everyday experience, includes notions for which there is no actual evidence. For example, one commonly held idea is that there exists one absolute time shared by all observers. Another is the idea that electrons are discrete entities like miniature planets that circle the nucleus in definite orbits.\n\nThe correspondence principle says that classical accounts are approximations to quantum mechanics that are for all practical purposes equivalent to quantum mechanics when dealing with macro-scale events.\n\nVarious problems occur if classical mechanics is used to describe quantum systems, such as the ultraviolet catastrophe in black body radiation, the Gibbs paradox, and the lack of a zero point for entropy.\n\nSince classical physics corresponds more closely to ordinary language than modern physics does, this subject is also a part of the philosophical interpretation of ordinary language, which has other aspects, as well.\n\nIn classical mechanics it is assumed that given properties - speed or mass of a particle; temperature of a gas, etc. - can in principle be measured to any degree of accuracy desired.\n\nStudy of the problem of measurement in quantum mechanics has shown that measurement of any object involves interactions between the measuring apparatus and that object that inevitably affect it in some way; at the scale of particles this effect is necessarily large. On the everyday macroscopic scale the effect can be made small.\n\nFurthermore, the classical idealization of a property simply being \"measured\" ignores the fact that measurement of a property - temperature of a gas by thermometer, say - involves a pre-existing account of the behavior of the measuring device. When effort was devoted to working out the operational definitions involved in precisely determining position and momentum of micro-scale entities, physicists were required perforce to provide such an account for measuring devices to be used at that scale. The key thought experiment in this regard is known as Heisenberg's microscope.\n\nThe problem for the individual is how to properly characterize a part of reality of which one has no direct sense experience. Our inquiries into the quantum domain find most pertinent whatever it is that happens in between the events by means of which we obtain our only information. Our accounts of the quantum domain are based on interactions of macro domain instruments and sense organs with physical events, and those interactions give us some but not all of the information we seek. We then seek to derive further information from series of those experiments in an indirect way. \n\nOne interpretation of this conundrum is given by Werner Heisenberg in his 1958 book, \"Physics and Philosophy,\"p. 144f:\nWe can say that physics is a part of science and as such aims at a description and understanding of nature. Any kind of understanding, scientific or not, depends on our language, on the communication of ideas. Every description of phenomena, of experiments and their results, rests upon language as the only means of communication. The words of this language represent the concepts of daily life, which in the scientific language of physics may be refined to the concepts of classical physics. These concepts are the only tools for an unambiguous communication about events, about the setting up of experiments, and about their results. If therefore the atomic physicist is asked to give a description of what really happens in his experiments, the words \"description\" and \"really\" and \"happens\" can only refer to the concepts of daily life or of classical physics. As soon as the physicist gave up this basis he would lose the means of unambiguous communication and could not continue in his science. Therefore, any statement about what has \"actually happened\" is a statement in terms of the classical concepts and -- because of thermodynamics and of the uncertainty relations -- by its very nature incomplete with respect to the details of the atomic events involved. The demand to \"describe what happens\" in the quantum-theoretical process between two successive observations is a contradiction \"in adjecto,\" since the word \"describe\" refers to the use of the classical concepts, while these concepts cannot be applied in the space between the observations; they can only be applied at the points of observation.\n\nBoth quantum mechanics and special relativity begin their divergence from classical mechanics by insisting on the primacy of observations and a refusal to admit unobservable entities. Thus special relativity rejects the absolute simultaneity assumed by classical mechanics; and quantum mechanics does not permit one to speak of properties of the system (exact position, say) other than those that can be connected to macro scale observations. Position and momentum are not things waiting for us to discover; rather, they are the results that are obtained by performing certain procedures.\n\n\n\n"}
{"id": "77933", "url": "https://en.wikipedia.org/wiki?curid=77933", "title": "Pitch (music)", "text": "Pitch (music)\n\nPitch is a perceptual property of sounds that allows their ordering on a frequency-related scale,\nor more commonly, pitch is the quality that makes it possible to judge sounds as \"higher\" and \"lower\" in the sense associated with musical melodies. \nPitch can be determined only in sounds that have a frequency that is clear and stable enough to distinguish from noise. \nPitch is a major auditory attribute of musical tones, along with duration, loudness, and timbre.\n\nPitch may be quantified as a frequency, but pitch is not a purely objective physical property; it is a subjective psychoacoustical attribute of sound. Historically, the study of pitch and pitch perception has been a central problem in psychoacoustics, and has been instrumental in forming and testing theories of sound representation, processing, and perception in the auditory system.\n\nPitch is an auditory sensation in which a listener assigns musical tones to relative positions on a musical scale based primarily on their perception of the frequency of vibration. Pitch is closely related to frequency, but the two are not equivalent. Frequency is an objective, scientific attribute that can be measured. Pitch is each person's \"subjective perception\" of a sound wave, which cannot be directly measured. However, this does not necessarily mean that most people won't agree on which notes are higher and lower.\n\nThe oscillations of sound waves can often be characterized in terms of \"frequency\". \"Pitches\" are usually associated with, and thus quantified as, \"frequencies\" (in cycles per second, or hertz), by comparing the sounds being assessed against sounds with pure tones (ones with periodic, sinusoidal waveforms). Complex and aperiodic sound waves can often be assigned a \"pitch\" by this method.\n\nAccording to the American National Standards Institute, pitch is the auditory attribute of sound according to which sounds can be ordered on a scale from low to high. Since pitch is such a close proxy for frequency, it is almost entirely determined by how quickly the sound wave is making the air vibrate and has almost nothing to do with the intensity, or amplitude, of the wave. That is, \"high\" pitch means very rapid oscillation, and \"low\" pitch corresponds to slower oscillation. Despite that, the idiom relating vertical height to sound pitch is shared by most languages. At least in English, it is just one of many deep conceptual metaphors that involve up/down. The exact etymological history of the musical sense of high and low pitch is still unclear. There is evidence that humans do actually perceive that the source of a sound is slightly higher or lower in vertical space when the sound frequency is increased or reduced.\n\nIn most cases, the pitch of complex sounds such as speech and musical notes corresponds very nearly to the repetition rate of periodic or nearly-periodic sounds, or to the reciprocal of the time interval between repeating similar events in the sound waveform.\n\nThe pitch of complex tones can be ambiguous, meaning that two or more different pitches can be perceived, depending upon the observer. When the actual fundamental frequency can be precisely determined through physical measurement, it may differ from the perceived pitch because of overtones, also known as upper partials, harmonic or otherwise. A complex tone composed of two sine waves of 1000 and 1200 Hz may sometimes be heard as up to three pitches: two spectral pitches at 1000 and 1200 Hz, derived from the physical frequencies of the pure tones, and the combination tone at 200 Hz, corresponding to the repetition rate of the waveform. In a situation like this, the percept at 200 Hz is commonly referred to as the missing fundamental, which is often the greatest common divisor of the frequencies present.\n\nPitch depends to a lesser degree on the sound pressure level (loudness, volume) of the tone, especially at frequencies below 1,000 Hz and above 2,000 Hz. The pitch of lower tones gets lower as sound pressure increases. For instance, a tone of 200 Hz that is very loud seems one semitone lower in pitch than if it is just barely audible. Above 2,000 Hz, the pitch gets higher as the sound gets louder. These results were obtained in the pioneering works by S.Stevens and W.Snow . Later investigations, i.e. by A.Cohen, had shown that in most cases the apparent pitch shifts were not significantly different from pitch‐matching errors. When averaged, the remaining shifts followed the directions of Stevens' curves but were small (2% or less by frequency, i.e. not more than a semitone)\n\nTheories of pitch perception try to explain how the physical sound and specific physiology of the auditory system work together to yield the experience of pitch. In general, pitch perception theories can be divided into place coding and temporal coding. Place theory holds that the perception of pitch is determined by the place of maximum excitation on the basilar membrane.\n\nA place code, taking advantage of the tonotopy in the auditory system, must be in effect for the perception of high frequencies, since neurons have an upper limit on how fast they can phase-lock their action potentials. However, a purely place-based theory cannot account for the accuracy of pitch perception in the low and middle frequency ranges.\n\nTemporal theories offer an alternative that appeals to the temporal structure of action potentials, mostly the phase-locking and mode-locking of action potentials to frequencies in a stimulus. The precise way this temporal structure helps code for pitch at higher levels is still debated, but the processing seems to be based on an autocorrelation of action potentials in the auditory nerve. However, it has long been noted that a neural mechanism that may accomplish a delay—a necessary operation of a true autocorrelation—has not been found. At least one model shows that a temporal delay is unnecessary to produce an autocorrelation model of pitch perception, appealing to phase shifts between cochlear filters; however, earlier work has shown that certain sounds with a prominent peak in their autocorrelation function do not elicit a corresponding pitch percept, and that certain sounds without a peak in their autocorrelation function nevertheless elicit a pitch. To be a more complete model, autocorrelation must therefore apply to signals that represent the output of the cochlea, as via auditory-nerve interspike-interval histograms. Some theories of pitch perception hold that pitch has inherent octave ambiguities, and therefore is best decomposed into a pitch \"chroma\", a periodic value around the octave, like the note names in western music—and a pitch \"height\", which may be ambiguous, that indicates the octave the pitch is in.\n\nThe \"just-noticeable difference (jnd)\" (the threshold at which a change is perceived) depends on the tone's frequency content. Below 500 Hz, the jnd is about 3 Hz for sine waves, and 1 Hz for complex tones; above 1000 Hz, the jnd for sine waves is about 0.6% (about 10 cents).\nThe jnd is typically tested by playing two tones in quick succession with the listener asked if there was a difference in their pitches. The jnd becomes smaller if the two tones are played simultaneously as the listener is then able to discern beat frequencies. The total number of perceptible pitch steps in the range of human hearing is about 1,400; the total number of notes in the equal-tempered scale, from 16 to 16,000 Hz, is 120.\n\nThe relative perception of pitch can be fooled, resulting in \"aural illusions\". There are several of these, such as the tritone paradox, but most notably the Shepard scale, where a continuous or discrete sequence of specially formed tones can be made to sound as if the sequence continues ascending or descending forever.\n\nNot all musical instruments make notes with a clear pitch. The unpitched percussion instrument (a class of percussion instrument) does not produce particular pitches. A sound or note of definite pitch is one where a listener can possibly (or relatively easily) discern the pitch. Sounds with definite pitch have harmonic frequency spectra or close to harmonic spectra.\n\nA sound generated on any instrument produces many modes of vibration that occur simultaneously. A listener hears numerous frequencies at once. The vibration with the lowest frequency is called the \"fundamental frequency\"; the other frequencies are \"overtones\".\n\"Harmonics\" are an important class of overtones with frequencies that are integer multiples of the fundamental. Whether or not the higher frequencies are integer multiples, they are collectively called the partials, referring to the different parts that make up the total spectrum.\n\nA sound or note of indefinite pitch is one that a listener finds impossible or relatively difficult to identify as to pitch. Sounds with indefinite pitch do not have harmonic spectra or have altered harmonic spectra—a characteristic known as inharmonicity.\n\nIt is still possible for two sounds of indefinite pitch to clearly be higher or lower than one another. For instance, a snare drum sounds higher pitched than a bass drum though both have indefinite pitch, because its sound contains higher frequencies. In other words, it is possible and often easy to roughly discern the relative pitches of two sounds of indefinite pitch, but sounds of indefinite pitch do not neatly correspond to any specific pitch.\nA special type of pitch often occurs in free nature when sound reaches the ear of an observer directly from the source, and also after reflecting off a sound-reflecting surface. This phenomenon is called \"repetition pitch\", because the addition of a true repetition of the original sound to itself is the basic prerequisite.\n\nA pitch standard (also Concert pitch) is the conventional pitch reference a group of musical instruments are tuned to for a performance. Concert pitch may vary from ensemble to ensemble, and has varied widely over musical history.\n\nStandard pitch is a more widely accepted convention. The A above middle C is usually set at 440 Hz (often written as \"A = 440 Hz\" or sometimes \"A440\"), although other frequencies, such as 442 Hz, are also often used as variants. Another standard pitch, the so-called \"Baroque pitch\", has been set in the 20th century as A = 415 Hz—approximately an equal-tempered semitone lower than A440 to facilitate transposition.\n\nTransposing instruments have their origin in the variety of pitch standards. In modern times, they conventionally have their parts transposed into different keys from voices and other instruments (and even from each other). As a result, musicians need a way to refer to a particular pitch in an unambiguous manner when talking to each other.\n\nFor example, the most common type of clarinet or trumpet, when playing a note written in their part as C, sounds a pitch that is called B on a non-transposing instrument like a violin (which indicates that at one time these wind instruments played at a standard pitch a tone lower than violin pitch). To refer to that pitch unambiguously, a musician calls it \"concert B\", meaning, \"...the pitch that someone playing a non-transposing instrument like a violin calls B.\"\n\nPitches are labeled using:\nFor example, one might refer to the A above middle C as \"a′\", \"A\", or \"440 Hz\". In standard Western equal temperament, the notion of pitch is insensitive to \"spelling\": the description \"G double sharp\" refers to the same pitch as \"A\"; in other temperaments, these may be distinct pitches.\nHuman perception of musical intervals is approximately logarithmic with respect to fundamental frequency: the perceived interval between the pitches \"A220\" and \"A440\" is the same as the perceived interval between the pitches \"A440\" and \"A880\". Motivated by this logarithmic perception, music theorists sometimes represent pitches using a numerical scale based on the logarithm of fundamental frequency. For example, one can adopt the widely used MIDI standard to map fundamental frequency, \"f\", to a real number, \"p\", as follows\nThis creates a linear pitch space in which octaves have size 12, semitones (the distance between adjacent keys on the piano keyboard) have size 1, and A440 is assigned the number 69. (See Frequencies of notes.) Distance in this space corresponds to musical intervals as understood by musicians. An equal-tempered semitone is subdivided into 100 cents. The system is flexible enough to include \"microtones\" not found on standard piano keyboards. For example, the pitch halfway between C (60) and C (61) can be labeled 60.5.\n\nThe following table shows frequencies in Hz for notes in various octaves, named according to the \"German method\" of octave nomenclature:\n\nThe relative pitches of individual notes in a scale may be determined by one of a number of tuning systems. In the west, the twelve-note chromatic scale is the most common method of organization, with equal temperament now the most widely used method of tuning that scale. In it, the pitch ratio between any two successive notes of the scale is exactly the twelfth root of two (or about 1.05946). In well-tempered systems (as used in the time of Johann Sebastian Bach, for example), different methods of musical tuning were used. Almost all of these systems have one interval in common, the octave, where the pitch of one note is double the frequency of another. For example, if the A above middle C is 440 Hz, the A an octave above that is .\n\nIn atonal, twelve tone, or musical set theory a \"pitch\" is a specific frequency while a pitch class is all the octaves of a frequency. In many analytic discussions of atonal and post-tonal music, pitches are named with integers because of octave and enharmonic equivalency (for example, in a serial system, C and D are considered the same pitch, while C and C are functionally the same, one octave apart).\n\nDiscrete pitches, rather than continuously variable pitches, are virtually universal, with exceptions including \"tumbling strains\" and \"indeterminate-pitch chants\". Gliding pitches are used in most cultures, but are related to the discrete pitches they reference or embellish.\n\n\n"}
{"id": "1297768", "url": "https://en.wikipedia.org/wiki?curid=1297768", "title": "Political repression", "text": "Political repression\n\nPolitical repression is the persecution of an individual or group within society for political reasons, particularly for the purpose of restricting or preventing their ability to take part in the political life of a society thereby reducing their standing among their fellow citizens.\n\nPolitical repression is sometimes used synonymously with the term political discrimination (also known as politicism). It often is manifested through discriminatory policies, such as human rights violations, surveillance abuse, police brutality, imprisonment, involuntary settlement, stripping of citizen's rights, lustration and violent action or terror such as the murder, summary executions, torture, forced disappearance and other extrajudicial punishment of political activists, dissidents, or general population. Political repression can also be reinforced by means outside of written policy, such as by public and private media ownership and by self-censorship within the public.\n\nWhere political repression is sanctioned and organised by the state, it may constitute state terrorism, genocide, politicide or crimes against humanity. Systemic and violent political repression is a typical feature of dictatorships, totalitarian states and similar regimes. Acts of political repression may be carried out by secret police forces, army, paramilitary groups or death squads. Repressive activities have also been found within democratic contexts as well. This can even include setting up situations where the death of the target of repression is the end result.\n\nIf political repression is not carried out with the approval of the state, a section of government may still be responsible. An example is the FBI COINTELPRO operations in the United States between 1956 and 1971.\n\nIn some states, \"repression\" can be an official term used in legislation or the names of government institutions. For example, the Soviet Union had a legal policy of repression of political opposition defined in the penal code and Cuba under Fulgencio Batista had a secret police agency officially named the \"Bureau for the Repression of Communist Activities\".\n\nPolitical conflict strongly increases the likelihood of state repression. This is arguably the most robust finding in social science research on political repression – civil wars are a strong predictor of repressive activity, as are other forms of challenges from non-government actors. States so often engage in repressive behaviors in times of civil conflict that the relationship between these two phenomena has been termed the \"Law of Coercive Responsiveness\". When their authority or legitimacy is threatened, regimes respond by overtly or covertly suppressing dissidents to eliminate the behavioral threat. State repression subsequently affects dissident mobilization, though the direction of this effect is still an open question. Some strong evidence suggests that repression suppresses dissident mobilization by reducing the capacity of challengers to organize, yet it is also feasible that challengers can leverage state repressive behavior to spur mobilization among sympathizers by framing repression as a new grievance against the state.\n\nPolitical repression is often accompanied by violence, which might be legal or illegal according to domestic law. Violence can both eliminate political opposition directly by killing opposition members, or indirectly by instilling fear.\nPolitical repression is sometimes synonymous with political, ideological, religious and social discrimination and intolerance. This intolerance is manifested through discriminatory policies, human rights violations, police brutality, imprisonment, extermination, exile, extortion, terrorism, extrajudicial killing, summary execution, torture, forced disappearance and other punishments against political activists, dissidents, and population in general.\n\nWhen political repression is sanctioned and organized by the state, situations of state terrorism, genocide and crimes against humanity can be reached. Systematic and violent political repression is a typical feature of dictatorships, totalitarianisms and similar regimes. In these regimes, acts of political repression can be carried out by the police and secret police, the army, paramilitary groups and death squads. Sometimes regimes considered democratic exercise political repression and state terrorism to other states as part of their security policy.\n\nArticles\nJournals\nBooks\n"}
{"id": "35793286", "url": "https://en.wikipedia.org/wiki?curid=35793286", "title": "Power diagram", "text": "Power diagram\n\nIn computational geometry, a power diagram, also called a Laguerre–Voronoi diagram, Dirichlet cell complex, radical Voronoi tesselation or a sectional Dirichlet tesselation, is a partition of the Euclidean plane into polygonal cells defined from a set of circles. The cell for a given circle \"C\" consists of all the points for which the power distance to \"C\" is smaller than the power distance to the other circles. The power diagram is a form of generalized Voronoi diagram, and coincides with the Voronoi diagram of the circle centers in the case that all the circles have equal radii.\n\nIf \"C\" is a circle and \"P\" is a point outside \"C\", then the power of \"P\" with respect to \"C\" is the square of the length of a line segment from \"P\" to a point \"T\" of tangency with \"C\". Equivalently, if \"P\" has distance \"d\" from the center of the circle, and the circle has radius \"r\", then (by the Pythagorean theorem) the power is \"d\" − \"r\". The same formula \"d\" − \"r\" may be extended to all points in the plane, regardless of whether they are inside or outside of \"C\": points on \"C\" have zero power, and points inside \"C\" have negative power.\n\nThe power diagram of a set of \"n\" circles \"C\" is a partition of the plane into \"n\" regions \"R\" (called cells), such that a point \"P\" belongs to \"R\" whenever circle \"C\" is the circle minimizing the power of \"P\".\nIn the case \"n\" = 2, the power diagram consists of two halfplanes, separated by a line called the radical axis or chordale of the two circles. Along the radical axis, both circles have equal power. More generally, in any power diagram, each cell \"R\" is a convex polygon, the intersection of the halfspaces bounded by the radical axes of circle \"C\" with each other circle. Triples of cells meet at vertices of the diagram, which are the radical centers of the three circles whose cells meet at the vertex.\n\nThe power diagram may be seen as a weighted form of the Voronoi diagram of a set of point sites, a partition of the plane into cells within which one of the sites is closer than all the other sites. Other forms of weighted Voronoi diagram include the additively weighted Voronoi diagram, in which each site has a weight that is added to its distance before comparing it to the distances to the other sites, and the multiplicatively weighted Voronoi diagram, in which the weight of a site is multiplied by its distance before comparing it to the distances to the other sites. In contrast, in the power diagram, we may view each circle center as a site, and each circle's squared radius as a weight that is subtracted from the squared distance before comparing it to other squared distances. In the case that all the circle radii are equal, this subtraction makes no difference to the comparison, and the power diagram coincides with the Voronoi diagram.\n\nA planar power diagram may also be interpreted as a planar cross-section of an unweighted three-dimensional Voronoi diagram. In this interpretation, the set of circle centers in the cross-section plane are the perpendicular projections of the three-dimensional Voronoi sites, and the squared radius of each circle is a constant \"K\" minus the squared distance of the corresponding site from the cross-section plane, where \"K\" is chosen large enough to make all these radii positive.\n\nLike the Voronoi diagram, the power diagram may be generalized to Euclidean spaces of any dimension. The power diagram of \"n\" spheres in \"d\" dimensions is combinatorially equivalent to the intersection of a set of \"n\" upward-facing halfspaces in \"d\" + 1 dimensions, and vice versa.\n\nTwo-dimensional power diagrams may be constructed by an algorithm that runs in time O(\"n\" log \"n\"). More generally, because of the equivalence with higher-dimensional halfspace intersections, \"d\"-dimensional power diagrams (for \"d\" > 2) may be constructed by an algorithm that runs in time formula_1.\n\nThe power diagram may be used as part of an efficient algorithm for computing the volume of a union of spheres. Intersecting each sphere with its power diagram cell gives its contribution to the total union, from which the volume may be computed in time proportional to the complexity of the power diagram.\n\nOther applications of power diagrams include data structures for testing whether a point belongs to a union of disks, algorithms for constructing the boundary of a union of disks, and algorithms for finding the closest two balls in a set of balls.\n\n traces the definition of the power distance to the work of 19th-century mathematicians Edmond Laguerre and Georgy Voronoy. defined power diagrams and used them to show that the boundary of a union of \"n\" circular disks can always be illuminated from at most 2\"n\" point light sources. Power diagrams have appeared in the literature under other names including the \"Laguerre–Voronoi diagram\", \"Dirichlet cell complex\", \"radical Voronoi tesselation\" and \"sectional Dirichlet tesselation\".\n"}
{"id": "76176", "url": "https://en.wikipedia.org/wiki?curid=76176", "title": "Pride", "text": "Pride\n\nPride is an inwardly directed emotion that carries two antithetical meanings. With a negative connotation \"pride\" refers to a foolishly and irrationally corrupt sense of one's personal value, status or accomplishments, used synonymously with hubris. With a positive connotation, \"pride\" refers to a humble and content sense of attachment toward one's own or another's choices and actions, or toward a whole group of people, and is a product of praise, independent self-reflection, and a fulfilled feeling of belonging.\n\nIn Judaism, pride is called the root of all evil.\n\nPhilosophers and social psychologists have noted that pride is a complex secondary emotion which requires the development of a sense of self and the mastery of relevant conceptual distinctions (e.g. that pride is distinct from happiness and joy) through language-based interaction with others. Some social psychologists identify the nonverbal expression of pride as a means of sending a functional, automatically perceived signal of high social status. In contrast, pride could also be defined as a lowly disagreement with the truth. One definition of pride in the former sense comes from St. Augustine: \"the love of one's own excellence\". A similar definition comes from Meher Baba: \"Pride is the specific feeling through which egoism manifests.\"\n\nPride is sometimes viewed as corrupt or as a vice, sometimes as proper or as a virtue. While some philosophers such as Aristotle (and George Bernard Shaw) consider pride (but not hubris) a profound virtue, some world religions consider pride's fraudulent form a sin, such as is expressed in Proverbs 11:2 of the Hebrew Bible. When viewed as a virtue, pride in one's abilities is known as virtuous pride, greatness of soul or magnanimity, but when viewed as a vice it is often known to be self-idolatry, sadistic contempt, vanity or vainglory. Pride can also manifest itself as a high opinion of one's nation (national pride) and ethnicity (ethnic pride).\n\n\"Proud\" comes from late Old English \"prut\", probably from Old French \"prud\" \"brave, valiant\" (11th century) (which became \"preux\" in French), from Late Latin term \"prodis\" \"useful\", which is compared with the Latin \"prodesse\" \"be of use\". The sense of \"having a high opinion of oneself\", not in French, may reflect the Anglo-Saxons' opinion of the Norman knights who called themselves \"proud\".\n\nAristotle identified pride (\"megalopsuchia\", variously translated as proper pride, greatness of soul and magnanimity) as the crown of the virtues, distinguishing it from vanity, temperance, and humility, thus:\nHe concludes then that\nBy contrast, Aristotle defined the vice of hubris as follows:\nThus, although pride and hubris are often deemed the same thing, for Aristotle and many philosophers hubris is altogether an entirely different thing from pride.\n\nIn psychological terms, positive pride is \"a pleasant, sometimes exhilarating, emotion that results from a positive self-evaluation\". It was added by Tracy et al. to the University of California, Davis, Set of Emotion Expressions (UCDSEE) in 2009, as one of three \"self-conscious\" emotions known to have recognizable expressions (along with embarrassment and shame).\n\nThe term \"fiero\" was coined by Italian psychologist Isabella Poggi to describe the pride experienced and expressed in the moments following a personal triumph over adversity. Facial expressions and gestures that demonstrate pride can involve a lifting of the chin, smiles, or arms on hips to demonstrate victory. Individuals may implicitly grant status to others based solely on their expressions of pride, even in cases in which they wish to avoid doing so. Indeed, some studies show that the nonverbal expression of pride conveys a message that is automatically perceived by others about a person's high social status in a group.\n\nBehaviorally, pride can also be expressed by adopting an expanded posture in which the head is tilted back and the arms extended out from the body. This postural display is innate as it is shown in congenitally blind individuals who have lacked the opportunity to see it in others.\n\nA common understanding of pride is that it results from self-directed satisfaction with meeting the personal goals; for example, Weiner et al. have posited that positive performance outcomes elicit pride in an individual when the event is appraised as having been caused by him alone. Moreover, Oveis et al. conceptualize pride as a display of the strong self that promotes feelings of similarity to strong others, as well as differentiation from weak others. Seen in this light, pride can be conceptualized as a hierarchy-enhancing emotion, as its experience and display helps rid negotiations of conflict.\nPride involves exhilarated pleasure and a feeling of accomplishment. It is related to \"more positive behaviors and outcomes in the area where the individual is proud\" (Weiner, 1985). Pride is generally associated with positive social behaviors such as helping others and outward promotion. Along with hope, it is also often described as an emotion that facilitates performance attainment, as it can help trigger and sustain focused and appetitive effort to prepare for upcoming evaluative events. It may also help enhance the quality and flexibility of the effort expended (Fredrickson, 2001). According to Bagozzi et al., pride can have the positive benefits of enhancing creativity, productivity, and altruism. For instance, it has been found that in terms of school achievement, pride is associated with a higher GPA in low neighborhood socioeconomic environments, whereas in more advantaged neighborhoods, pride is associated with a lower GPA.\n\nIn the field of economic psychology, pride is conceptualized in a spectrum ranging from \"proper pride\", associated with genuine achievements, and \"false pride\", which can be maladaptive or even pathological. Lea et al. have examined the role of pride in various economic situations and claim that in all cases pride is involved because economic decisions are not taken in isolation from one another, but are linked together by the selfhood of the people who take them. Understood in this way, pride is an emotional state that works to ensure that people take financial decisions that are in their long-term interests, even when in the short term they would appear irrational.\n\nExaggerated self-esteem is called \"pride\". Classical Christian theology views pride as being the result of high self-esteem, and thus high self-esteem was viewed as the primary human problem, but beginning in the 20th century, \"humanistic psychology\" diagnosed the primary human problem as low self-esteem stemming from a lack of belief in one's \"true worth\". Carl Rogers observed that most people \"regard themselves as worthless and unlovable.\" Thus, they lack self-esteem.\n\nTerry Cooper conceptualized in 2003 excessive pride (along with low self-esteem) as an important paradigm in describing the human condition. He examines and compares the Augustinian-Niebuhrian conviction that pride is primary, the feminist concept of pride as being absent in the experience of women, the humanistic psychology position that pride does not adequately account for anyone's experience, and the humanistic psychology idea that if pride emerges, it is always a false front designed to protect an undervalued self.\n\nHe considers that the work of certain neo-Freudian psychoanalysts, namely Karen Horney, offers promise in dealing with what he calls a \"deadlock between the overvalued and undervalued self\" (Cooper, 112–3).\nCooper refers to their work in describing the connection between religious and psychological pride as well as sin to describe how a neurotic pride system underlies an appearance of self-contempt and low self-esteem:\n\nThe \"idealized self,\" the \"tyranny of the should,\" the \"pride system\" and the nature of self-hate all point toward the intertwined relationship between neurotic pride and self-contempt. Understanding how a neurotic pride system underlies an appearance of self-contempt and low self-esteem. (Cooper, 112–3).\nThus, hubris, which is an exaggerated form of self-esteem, is sometimes actually a lie used to cover the lack of self-esteem the committer of pride feels deeply down.\n\nIn the King James Bible, those people exhibiting excess pride are labeled with the somewhat archaic term, \"Haughty\".\n\nHubris itself is associated with more intra-individual negative outcomes and is commonly related to expressions of aggression and hostility (Tangney, 1999). As one might expect, Hubris is not necessarily associated with high self-esteem but with highly fluctuating or variable self-esteem. Excessive feelings of hubris have a tendency to create conflict and sometimes terminating close relationships, which has led it to be understood as one of the few emotions with no clear positive or adaptive functions (Rhodwalt, et al.).\n\nSeveral studies by UC Davis psychologist Cynthia Picket about group pride, have shown that groups that boast, gloat or denigrate others tend to become a group with low social status or to be vulnerable to threats from other groups. Suggesting that \"hubristic, pompous displays of group pride might actually be a sign of group insecurity as opposed to a sign of strength,\" she states that those that express pride by being filled with humility whilst focusing on members' efforts and hard work tend to achieve high social standing in both the adult public and personal eyes.\n\nIn Germany, \"national pride\" (\"Nationalstolz\") is often associated with the former Nazi regime. Strong displays of national pride are therefore considered poor taste by many Germans. There is an ongoing public debate about the issue of German patriotism. The World Cup in 2006, held in Germany, saw a wave of patriotism sweep the country in a manner not seen for many years. Although many were hesitant to show such blatant support as the hanging of the national flag from windows, as the team progressed through the tournament, so too did the level of support across the nation. By the time the semi-final against Italy came around, the level of national pride and unity was at its highest throughout the tournament, and the hosting of the World Cup is seen to have been a great success for Germany as a nation. After the World Cup, however, the subject of patriotism became again as difficult as it had been before.\n\nAsian pride in modern slang refers mostly to those of East Asian descent, though it can include anyone of Asian descent. Asian pride was originally fragmented, as Asian nations have had long conflicts with each other, examples are the old Japanese and Chinese religious beliefs of their individual superiority. Asian pride emerged prominently during European colonialism. At one time, Europeans controlled 85% of the world's land through colonialism, resulting in anti-Western feelings among Asian nations. Today, some Asians still look upon European involvement in their affairs with suspicion. In contrast, Asian empires are prominent and are proudly remembered by adherents to Asian Pride.\n\nThere is an emerging discourse of Chinese pride which unfolds complex histories and maps of privileges and empowerments. In a deeper sense, it is a strategic positioning, aligned with approaches such as \"Asia as method\", to invite more diverse resistances in language, culture, and practices, in challenging colonial, imperial dominations, and being critical of Eurocentric epistemologies. In more specific cases, it examines the Sinophone circulations of power relations connecting the transnational to the local, for example, a particular set of Chinese-Canadian relations between China's increasing industrial materiality and output in which pride becomes an expansionist reach and mobilization of capital, Canada's active interests in tapping into Asian and Chinese labours, markets, and industrial productions, and the intersected cultural politics of 'Chinese-ness' in an East Pacific British Columbia city where 'Chinese' has been tagged as a majority-minority.\n\nBlack pride is a slogan used primarily in the United States to raise awareness for a black racial identity. The slogan has been used by African Americans of sub-Saharan African origin to denote a feeling of self-confidence, self-respect, celebrating one's heritage, and being proud of one's personal worth.\n\n\"White pride\" is a slogan used primarily in the United States for a white race identity.\n\nMad pride refers to a worldwide movement and philosophy that individuals with mental illnesses should be proud of their 'mad' identity. Mad Pride advocates mutual support and rallies in support of rights for people with mental illness. The Mad Pride movement aims to reclaim the word mad as a self-descriptor.\n\n\"Gay pride\" refers to a worldwide movement and philosophy asserting that lesbian, gay, bisexual, and transgender (LGBT) individuals should be proud of their sexual orientation and gender identity. LGBT pride advocates work for equal \"rights and benefits\" for LGBT people. The movement has three main premises: that people should be proud of their sexual orientation and gender identity, that sexual diversity is a gift, and that sexual orientation and gender identity are inherent and cannot be intentionally altered.\n\nThe word \"pride\" is used in this case an antonym for \"shame\". \"Pride\" in this sense is an affirmation of one's self and the community as a whole. The modern \"gay pride\" movement began after the Stonewall riots of the late 1960s. In June 1970, the first pride parade in the United States commemorated the one-year anniversary of the Stonewall riots—the nearly week-long uprising between New York City youth and police officers following a raid of Stonewall Inn.\n\nIn conventional parlance, vanity sometimes is used in a positive sense to refer to a rational concern for one's personal appearance, attractiveness and dress and is thus not the same as pride. However, it also refers to an excessive or irrational belief in one's own abilities or attractiveness \"in the eyes of others\" and may in so far be compared to pride. The term Vanity originates from the Latin word \"vanitas\" meaning \"emptiness\", \"untruthfulness\", \"futility\", \"foolishness\" and \"empty pride\". Here \"empty pride\" means a fake pride, in the sense of vainglory, unjustified by one's own achievements and actions, but sought by pretense and appeals to superficial characteristics.\n\nIn many religions, vanity is considered a form of self-idolatry, in which one rejects God for the sake of one's own image, and thereby becomes divorced from the graces of God. The stories of Lucifer and Narcissus (who gave us the term narcissism), and others, attend to a pernicious aspect of vanity. In Western art, vanity was often symbolized by a peacock, and in Biblical terms, by the Whore of Babylon. During the Renaissance, vanity was invariably represented as a naked woman, sometimes seated or reclining on a couch. She attends to her hair with comb and mirror. The mirror is sometimes held by a demon or a putto. Other symbols of vanity include jewels, gold coins, a purse, and often by the figure of death himself.\nOften we find an inscription on a scroll that reads \"Omnia Vanitas\" (\"All is Vanity\"), a quote from the Latin translation of the Book of Ecclesiastes. Although that phrase, itself depicted in a type of still life, vanitas, originally referred not to obsession with one's appearance, but to the ultimate fruitlessness of man's efforts in this world, the phrase summarizes the complete preoccupation of the subject of the picture.\n\n\"The artist invites us to pay lip-service to condemning her\", writes Edwin Mullins, \"while offering us full permission to drool over her. She admires herself in the glass, while we treat the picture that purports to incriminate her as another kind of glass—a window—through which we peer and secretly desire her.\" The theme of the recumbent woman often merged artistically with the non-allegorical one of a reclining Venus.\n\nIn his table of the seven deadly sins, Hieronymus Bosch depicts a bourgeois woman admiring herself in a mirror held up by a devil. Behind her is an open jewelry box. A painting attributed to Nicolas Tournier, which hangs in the Ashmolean Museum, is \"An Allegory of Justice and Vanity\". A young woman holds a balance, symbolizing justice; she does not look at the mirror or the skull on the table before her. Vermeer's famous painting \"Girl with a Pearl Earring\" is sometimes believed to depict the sin of vanity, as the young girl has adorned herself before a glass without further positive allegorical attributes. \"All is Vanity\", by Charles Allan Gilbert (1873–1929), carries on this theme. An optical illusion, the painting depicts what appears to be a large grinning skull. Upon closer examination, it reveals itself to be a young woman gazing at her reflection in the mirror.\nSuch artistic works served to warn viewers of the ephemeral nature of youthful beauty, as well as the brevity of human life and the inevitability of death.\n\n\n"}
{"id": "53368524", "url": "https://en.wikipedia.org/wiki?curid=53368524", "title": "Privacy in education", "text": "Privacy in education\n\nPrivacy in education refers to the broad area of ideologies, practices, and legislation that involve the privacy rights of individuals in the education system. Concepts that are commonly associated with privacy in education include the expectation of privacy, the Family Educational Rights and Privacy Act (FERPA), the Fourth Amendment, and the Health Insurance Portability and Accountability Act of 1996 (HIPAA). The majority of privacy in education concerns are prevalent to the protection of student data, such as educational records and other personal information, both inside and outside the traditional classroom setting as well as the privacy and confidentiality of medical records. Many scholars are engaging in an academic discussion that covers the scope of students’ privacy rights, from student in K-12 and even higher education, and the management of student data in an age of rapid access and dissemination of information. \n\n\"Expectation of privacy,\" similar to the \"right to privacy,\" is a phrase that describes the natural desire of humans to maintain their sense of privacy. There is currently no legal definition in the American law that explicitly grants humans the right to privacy. Oftentimes, the Fourth Amendment is utilized by people in court cases to defend themselves from actions that involve certain infringements upon their privacy, such as searches that require warrants. However, over the years, the U.S. Supreme Court has found it difficult to determine an impartial non-biased meaning for \"expectation of privacy\" because there are too many subjective variables to consider.\n\nIn line with the general meaning of \"expectation of privacy,\" student expectation of privacy refers to a student's inherent right to privacy in the school system. Examples of student expectation of privacy, especially in the pre-collegiate levels, include the protection of a student's academic record from being viewed by anyone other than the academic instructor, the student's parents or guardians, and the students themselves. There have been many legal cases regarding the privacy concerns of pupils' academic records, for example the Owasso Independent School District v. Falvo that was handled by the U.S. Supreme Court in 2002. This particular case began in October 1998 when Kristja J. Falvo filed a lawsuit against the Owasso Independent School District on the premise that the grading practice employed in her children's classroom, peer grading, was a violation of the Fourteenth Amendment and of FERPA. Additionally, peer grading embarrassed her children in front of their peers, which could be interpreted as a violation of a student's expectation of privacy in the classroom. When the case reached the Tenth Circuit Court of Appeals, it was ruled in October 2000 that peer grading was not a violation of the constitutional Fourteenth Amendment but was, in fact, a violation of FERPA. The Tenth Circuit judges reasoned so by closely interpreting the statue in FERPA about privacy protection of \"educational records.\" Since it was agreed upon that teacher grade books were considered \"educational records,\" the Tenth Circuit decided that anything that went into these grade books, including student grades written on student work, were also considered \"educational records\" and thus subject to FERPA's privacy protection policies. After the court decision was released, peer grading practices were banned in school districts within the Tenth Circuit regional borders.\n\nMany citizens and scholars were opposed to the Tenth Circuit's decision, and the case ultimately reached the U.S. Supreme Court in 2001. In 2002, the Supreme Court Justices officially ruled that peer grading was not a violation of FERPA. They reasoned that student grades on student work were not considered \"educational records\" until the teacher had physically recorded the grades into a grade book. Thus, peer grading returned as a common grading practice in classrooms across the United States.\n\nOther examples of student expectation of privacy include the right of children to withhold personal information from teachers within in a traditional classroom setting. Such topic remains as a contentious educational privacy concern in the classroom. Some argue that teachers should know more information about students in order to help support them in their academic endeavors. Others argue that teachers should refrain from prying into the personal lives of children because, like adults, children should have the right to privacy and determine the amount of information that they reveal to teachers. Still others make the claim that children are too young to make this decision for themselves and should consult their parents prior to revealing anything personal to their teachers.\n\nStudent educational records, according to the FERPA statue, is defined as \"those records, files, documents, and other materials which--(i) contain information directly related to a student; and (ii) are maintained by an educational agency or institution or by a person acting for such agency or institution.\"\n\nThe Family Educational Rights and Privacy Act of 1974 was propelled by New York Senator James Buckley who promoted the importance of protection and privacy of educational records of students who attended primary and secondary schooling. As a federal legislation, FERPA grants students under the age of 18 and their parents the right to manage their educational records, which may include data about their academic performance, medical information, behavioral analysis, and more. Thus, when parents or guardians feel that their children's educational records have been exposed to the public in some way, they are able to file claims against the school district as a violation of FERPA. \n\nFor students over the age of 18, particularly those enrolled in postsecondary education institutions, FERPA can be vague and unclear about the disclosure of educational records to parents. Since individuals aged 18 and over are recognized as adults by the law, FERPA separates students in postsecondary education and their parents in terms of access to educational records such as health and drug records. According to Baker, many controversial issues may arise. For example, Baker writes that \"FERPA regulation 99.31(a)(8) allows disclosure to parents without the student's prior written authorization if the student is 'dependent' on the parents as defined by the Internal Revenue Code.\" However, there may be circumstances under which financially dependent students would not want their private records accessed by their parents. Additionally, FERPA authorizes the release to parents or guardians of information about recorded drug or alcohol use by students under the age of 21. If the underage and illegal drug or alcohol use resulted in legitimate concerns and disciplinary action, school administrators can notify and disclose details to parents without the students' permission.\n\nAs the Common Core State Standards are being developed and implemented in schools across the United States, some academics are bringing awareness to the potential privacy concerns of student data and educational records. According to Stacie Hunt, the Common Core system creates a large student database that keeps track of student performance and information from pre-K to college and even beyond. Federal government and other agencies are able to gain access to this database, analyze student data records, and sell pertinent information to the schools and districts overall. This creates a privacy concern about student data being spread and utilized by third-parties without the students', or their parents', explicit permission.\n\nEducational technology (\"edtech\") is an emerging field in the area of education. According to Dylan Peterson, \"edtech represents a broad category of educational products and services used in schools and by private individuals.\" Privacy concerns surround the fact that large amounts of data of each student using edtech are collected and stored in a wide database that can be accessed by schools. This data may be personal to the students and they may not wish for others to view it. However, edtech companies claim that without storing as much student data as possible, it would be more difficult to create programs that effectively address the students' educational needs.\n\nTo help improve programs that benefit the educational experience of students with disabilities, some scholars propose a digitalized database of student records that are updated in real time. This is so that educators can keep track of important information about students with various disabilities and monitor their academic performances for educational purposes. While these scholars support the idea of disclosure of data of students with disabilities, they make it very clear that strong legislation and policies must be enforced to protect the privacy of such data. It is currently a debate whether or not digitalized student data that can be accessed by many parties is really beneficial to support individuals with disabilities in school. Some are not comfortable with the idea that the data is updated frequently and disseminated widely, while others believe that the acquisition of such data is necessary to improve programs and enrich educational experiences.\n\nSince the 1970's, the commonly held perspective was that the right to privacy was an evaluation of individual worth. Further, technology, even before the World Wide Web, was perceived as having potentially negative effects such as allowing information to be breached. Yet, there were not many violations warranted that would urge legislation to act and shift their attention to protecting privacy in education or primarily individual privacy in general. Technology was also viewed as a source to uncover values, behaviors, motives and thoughts but at the same time, many thought that only qualified professionals had access to personal data. However, specifically in higher education, there was a perspective that individuals were susceptible to having their information breached. Thus, the role of education in the 1970's was viewed as one that safeguarded its students and staff to ensure privacy and prevent data from being breached given the technology that existed. \n\nIn many public and private colleges and universities across the United States, enrolled students typically live on campus in college-operated dormitories. Since students live in these dormitories for the duration of about one year, many personalize and consider their dormitory rooms as their personal and personalized living space. However, these dormitories are property owned by colleges and often students must waive their right to privacy for college representatives to conduct searches for safety purposes. While some believe that dormitory searches are effective for maintaining a safe community on campus, others believe that these searches are a violation of student privacy. Many cases have been filed by students in which they were caught with illegal substances in their dormitories during searches that these students felt were an invasion of their private living spaces. Additionally, only public or government-related searches with warrants are protected under the Fourth Amendment. Oftentimes college dormitory searches would be regarded as private searches and thus undergo lawsuits that claim a violation of the Fourth Amendment. For example, in the case \"Morale v. Grigel\", a Resident Assistant at the New Hampshire Technical Institute searched a student's room on multiple occasions even though the student was not present in the room at all times. Once the Resident Assistant found marijuana in the room, the student was arrested for possessing illegal substances on college campus property. The student then filed a lawsuit against the Resident Assistant on the grounds that the searches were private and thus violated his Fourth Amendment right to protection from private searches. However, the court concluded that the Resident Assistant's employment status rendered him as a government agent and thus his searches were conducted on behalf of the college, a government-related institution.\n\nWithin social networks, or particular websites that enable the sharing of information as well as communication among individuals, there exists a level of privacy that students prefer to keep their personal information or social life private from school personnel or faculty in order to avoid context collapse. \n\nTechnology enables the creation of one's own social presence through informal settings as well as formal settings that allow for the connection of student and instructors for academic purposes. Students in higher education using Facebook typically censor or block their information from instructors. Thus, Facebook is primarily used to interact with friends and family rather than instructors. By students blocking their information, they believe that they avoid context collapse that may cause confusion of who the person really is. Typically, students prefer to be present in the classroom and have their social life private from formal settings. \n\nWith the improvement of technology, more data has become available within higher education. Administrators are then able to learn more about students in order to implement forms of improving student's success. Through learning analytics, which is defined as the focus on \"students and their learning behaviors, gathering data from course management and student information systems in order to improve student success,\" administrators are able to obtain real-time empirical data such as insights and responses of student's learning processes. \n\nYet the privacy issues arise in how student data is collected, stored, analyzed and presented to stakeholders. There arises ethical issues of \"location and interpretation of data; informed consent, privacy, and de-identification of data; and classification and management of data.\" \n\nStudents believe that data about them is elaborate and personalized and at the same time hold a conservative view about learning analytics. Learning analytics helps obtain real time data for higher education learning processes but, at the same time may hinder the development of students such as critical thinking and autonomous learning. It is not as simple as saying that learning analytics will benefit students and thus increase their success and retention rates. This is such because procedures to regulate access are put in place while at the same time bias and lack of validity and comprehension affect the ability to obtain data that will then be used for the benefit of students.\n\nAs of 2017, there has been over 30 data breaches since 2005. The susceptibility to breaches creates threats to institutional research (IR) professionals who store and manage student data within the regulatory structure that controls data management. Further than this, student information is then brought to light which can threaten them as well. As vast amounts of data continues to be actively collected, potential breaching through hacking, physical theft, and by vendors becomes more probable. \n\nPreventative Measures Those who study the implications of data breaches emphasize that data should be kept to a minimum and that steps should be taken in order to see who can be trusted to regulate this information in order to keep data private and not accessible to all employees. They also talk about investing in educating employees about what can and cannot be done with data. Further, they state that institutions should use the resources available at their own college/university in order to most effectively implement policies and procedures to keep data private. Exercising caution against third-party vendors that help with data is advised and further that there should be a contract established that 1) defines who exactly who will be working with data, 2) makes clear that data is sensitive and thus should be handled with care, 3) and includes security procedures that describe the exact responsibility of the vendor just in case data is breached. Writers who investigate data breaches in higher education advise that research professionals should understand that data breaches are bound to happen and that it is better to implement policies and take preventative measures in the first place to ensure the security of data. \n\nFamily Educational Rights and Privacy Act of 1974 (FERPA)\n\nThe Family Education Rights and Privacy Act of 1974 limits the “disclosure of certain information contained in a student’s education record to third parties” which includes parents if the student has not given consent. Third parties can be parents, family, another institution (mental health providers), or pursuants of a subpoena or court order (law enforcement). It gives colleges and universities the right to \"inspect and review\" educational records that can disclosed if 1) consent is given by the student, 2) if the information falls within the definition of \"directory information\" (information that is not considered harmful such as name, major and address), 3) if the information is of \"legitimate educational interest\" (if an official needs to review the education records in order to fulfill their responsibilities within the University), 4) the student is tax dependent, 5) if it regards drugs or alcohol violations, 6) if it involves serious conduct violations, and 7) when it involves health or safety emergencies \n\nFor example, if a student in a residence hall is diagnosed with a contagious disease (measles), has a serious eating disorder, has suicidal ideation, binge drinks heavily, or has erratic and angry behaviors. Furthermore, information can be released if it entails disciplinary information such as a student who is an “alleged perpetrator of a crime of violence or a non-forcible sex offense.\" However, there have been cases where troubled students remain in college, without the college or university advising parents about their \"strange\" behavior, which resulted in students to take their own lives. The cases of Jain v. Iowa\", Shin v. Massachusetts Institute of Technology\" and Mahoney v. Allegheny College exemplify this issue. Nonetheless, according to FERPA, disclosures are considered to be made in “good faith based upon the available facts.”Educational records are covered by FERPA. They are not just academic records, class schedules or transcripts but also financial records, disciplinary records, \"disability accommodation records, photographs, e-mails, and electronic database records.\" Official documentation is needed to fall under FERPA even if this entails a personal experience or observation.What is not covered under FERPA are: law enforcement records, treatment records, and sole possession records and instead fall under other laws or considerations. \n\nIn Loco Parentis \n\nDue to the influence of FERPA, there has been a shift from \"in loco parentis\", to in sin parentibus, and back to in loco parentis. \"In sine parentibus\" means \"without parents\" meanwhile in loco parentis means \"in place of the parent.\" Thus, as represented by FERPA, the shift to in loco parentis within higher education is the act of the school taking over the legal responsibility of parents. This means that college authorities stand in place of parents.\n\nThe role of FERPA is to enhance student achievement through greater parent involvement as well as protecting the private interests of students. Yet, the shift toward in loco parentis also comes with concerns related to educational records. More specifically, there is a concern about the extent to which large and powerful institutions obtain information to their advantage such as data that is gathered by researchers and policymakers. On the other hand there are concerns that relate to the university itself disclosing information. For example, under FERPA, the school can disclose information about students to parents if it includes alcohol and drug related incidents any time if they are under 21. Because of reasons like these, there is a concern that there may be \"systematic disclosure policies\" that become out of control and thus harm student rights and privacy. \n\nPrivacy vs. Confidentiality\n\nWithin student records there is a difference between what is privacy and what is confidentiality. Privacy is more of a legal concept and is defined as the \"right of a person to withhold himself and his property from public scrutiny if he so chooses.\" Thus privacy gives the individual the right to be let alone which means that the university itself does not have the right to pry into student's personal affairs or reveal student information unless there are explicit and valid reasons in doing so or permission has been granted by the student. Yet even giving permission does not mean that the student has given permission to have all of their information revealed from then on but rather than permission is granted within the particular circumstance.\n\nOn the other hand, confidentiality means that files and records of students are not authorized to be disclosed to third parties such as not disclosing information that is received in confidence from a patient and physician. Given this, authors who focus on confidentiality ask questions such as:1) Does the communication originate in confidence?2) Is the element of confidentiality essential to the full and satisfactory maintenance of the relationship between the parties?3) Is the relationship one that must be fostered??4) Will there result an injury that is greater than any benefits which may be gained from that disclosure?If the answers are \"yes\" then the university may be legally bound to not disclose information unless it overrides the interest. \n\nThe terms privacy and confidentiality arise when it comes to medical records. \n\nHealth Insurance Portability and Accountability Act of 1996\n\nThe Health Insurance Portability and Accountability Act of 1996 provides privacy to data that is related to medical or mental health records that are legally more restrictive than FERPA in regards to confidentiality. HIPAA includes provisions that “intend to facilitate the creation of a national system for the electronic transmission and exchange of medical record information” such as access to information that is individually identifiable like health plans and health care. The act “defined protected health information so as to exclude individually identifiable health information that is included in education records covered by FERPA and that is in treatment records that are exempted from FERPA.” The difference between educational records and treatment records is that treatment records fall under federal and state law while educational records fall under FERPA. Nevertheless, the documentation of patient and caregiver is confidential meaning that medical records will not be disclosed unless consent is given or there is a belief that disclosing records is crucial. Furthermore, generally, health care providers do not disclose information unless they meet a standard that falls above the required FERPA health or safety exception, or consent is given, and thus is limited in providing information within the constraints of the confidentiality among patient and provider. \n\nIntegration of Mental and Physical Records \n\nIn some instances, college campuses have begun to integrate physical and mental health needs of patients. This means that medical records are becoming more shared among physicians as well as counselors or psychologists that work with students. Yet, medical providers, separately, have the obligation to withhold confidential information as an ethical duty and state privacy regulations. For example, health providers such as counselors, also have the obligation to be confidential and not disclose private information. However, as medical providers move towards integrated care, such that mental and physical records are shared amongst themselves, there arises a confidentiality challenge that may lead to college students to fall behind in school. Since confidentiality is compromised as information is disclosed among providers that use this method of continuity care, less students utilize therapy because they refuse to disclose private information that can then be shared with others. This simultaneously fuels the stigma towards college counseling. Thus, as more information becomes disclosed, less college students seek counseling due to lack of confidentiality as medical records of patients are disclosed between medical providers, when legally the obligation of these medical providers is to abide and guarantee privacy and confidentiality by withholding patient's information unless under specific circumstances.\n\nFurther, outside of sharing information among medical providers, there is also the issue of sharing information with researchers. They claim that medical records are difficult to access but when they are, it opens up the door for research. Yet, at the same time it opens the door to privacy and confidentiality risks.\n\nElectronic Health Records Since technology continues to revolutionize, medical records have become accessible as electronic health records. This allows information to be shared more easily but appears to create a challenge for stigma management and disclosing information during medical appointments.An in-depth interview study called \"Negotiating stigma in health care: disclosure and the role of electronic health records\" was made that took into account sexual minority men (gay, bisexual, and other men who have sex with men) in the U.S. to seek how they viewed electronic health records. What the study found is that there were concerns of privacy in terms of how the electronic aspect creates a barrier to be open and talk about seemingly confidential information as well as how it may challenge the right to confidentiality and privacy. On the other hand, the study also found that electronic health records may benefit by improving communication among providers when sharing information and further, to provide better care especially after the Health Information Technology for Economic and Clinical and Health Act invested billions in the adoption of electronic health records to improve the quality of care. The study concludes that technology may enhance medical care yet at the same time fuel the stigma that seeking medical help is bad and thus would hinder patients to make appointments, to attend counseling with certain providers, or disclose personal information such as sexual identity and HIV status that they believe will be shared to others without their consent. \n\nState Laws \n\nFederal regulations allow states to place their own regulations, to either increase or decrease the requirements for disclosure, but states who do are few. \n\n\nIn 1996 the state of Minnesota placed a law regarding medical records that appeared to be more stringent than HIPAA. Minnesota law attempted to obtain a \"written general authorization for such release from the patient\" as a form to impede the activities of researchers or providers to share information without given consent. Hospitals in Minnesota even made brochures that highlight patient's rights to confidentiality and that they can give consent in writing if they allow for their medical records to be released outside of the facility. Thus, the law required health care providers to obtain a written consent and authorization from patients in order for medical records to be released and used for research. However, researchers themselves campaigned against the law and the law was not successful in enforcing the right that patients have to refuse their information to be released. Meanwhile, the patients themselves wanted information as to what information is being used within their medical records.\n\nAs of 2006, under Minnesota's state rights, individuals have the right to: see and get a copy of their medical records, have information added to their medical records in order to make them accurate, file a complaint, and importantly, sue in state court for violations of their rights under state law. \n\n\nThe Massachusetts state law imposed the requirement that a person has the right against the unreasonable interference of privacy and states that the superior court shall have jurisdiction to enforce a person's right and thus must award damages if need be. According to the law there are strict privacy protections classified as medical. Records are considered educational records unless there exists there need be heightened confidentiality such as child abuse, AIDS, substance abuse, immigration status, pregnancy and abortion. Further, it is considered a medical record if a school-based clinic is under “operation of an outside entity or by a physician under any employment arrangement” and considered educational records if not considered for heightened confidentiality. \n\nIf an individual believes that their right to privacy they have the right to file a complaint with the Officer for Civil Rights, U.S. Department of Health and Human services against health care providers, with the Massachusetts Board of Registration in Medicine against doctors and with the Department of Public Health against hospitals. \n\n\nThe Confidentiality of Medical Information Act (CMIA) is a California state law that includes more information than HIPAA in regards to medical records. The main function is to protect confidentiality of identifiable medical information obtained by an individual's health care provider. It applies to licenses providers such as physicians and nurses. It prohibit medical providers to disclose medical information without obtaining authorization first and that any medical information about an individual is preserved in confidentiality by anyone who comes in contact with it. An individual whose confidentiality is not respected may obtain $1,000 and the amount of actual damages and for the person or entity that discloses confidential information is liable for an administrative fine. \n\nStudents With Disabilities \n\nTension on campus arises because as of the event of 9/11 some people on campus are fearful or overreact in demanding to know which students have conduct records or a disability accommodation. There is a tension of whether the information will be used to discriminate or treat students unfairly. Nevertheless, the distribution of this information is not limited by FERPA among school officials as long as the disclosure is done due to \"legitimate educational interests.\" \n\nForeign Students \n\nThe event of 9/11 impacted the release of information of students with visas and leads to the questioning of the responsibility and obligation of universities to report foreign student's information. Foreign individuals granted the opportunity to study in the U.S. for a period of time are given one of three visas: F-1 for academic studying, J-1 for exchange visitors, and M-1 for vocational training. \n\nHowever, the government claims that there are no accurate records of the 547,000 individuals holding student status (as of 2003). Meanwhile, universities are supposed to report information the information of F-1 and M-1 students to the Immigration and Naturalization Service (INS) such as their name, date and place of birth, current address, student status, degree program, field of study, etc. For those with J-1 visas, the sponsoring organization is to report information such as the individuals activities and compliance. Yet, if they do not necessarily report the information they are at least required to keep track of their foreign student's information.\n\nImportantly, regulations are not addressed in regards to how FERPA applies. The school may release information if the student is no longer enrolled if it needs to comply with judicial order, if it lawfully issues a subpoena, or if there are \"specific and articulable facts” that show that a student’s education record may contain information relevant to investigation or prosecution. Information can also be disclosed if it includes the protection of health or safety of students, especially if it is to “protect the health and safety of Americans.” Further, students who were issued I-20A or I-20M forms (F-1 and M-1 students) or DS-2019 forms (J-1 students) automatically grant consent to any information needed to determine immigrant’s status or release information that's related to the individual’s compliance with the Exchange Visitor Program. Yet, this information is stated to be only given to certain organizations such as the INS or the Department of State. \n\nLibrarians \n\n\nLibrarians themselves take part in protecting the right of library users privacy. Typically, the library itself aims to protect user's information primarily regarding what they do when they use technology, such as using computers to surf the web. According to Michael Zimmer in 2014, 95 percent of librarians agree or strongly agree individuals should control their personal information and many agree that there are threats to the privacy of their users. A survey conducted by the Office for Intellectual Freedom which obtained over 1,000 responses of librarians and library professionals found is that the Library Bill of Rights is honored which believe that everyone is entitled to \"freedom of access, freedom to read texts and view images, and freedom of thought and expression\" The Librarian's Code of Ethics and the adoption of the Privacy Act of 1974 also illuminate not just on a librarian level but also on a federal level that privacy is to be protected. \n\nOther non-governmental acts that protect the right to privacy and thus limit the information that can be collected are:\n\nGramm-Leach-Bliley Act\n\nHealth Insurance Portability and Accountability Act of 1996\n\nCredit Reporting Act\n\nFurther, the Library 2.0 tools and services enhance what the user can do but at the same time track, collect and retain data that then may affect individuals especially since the recent dominance of social media. Yet, because of librarian's belief of protecting the rights of users, they take their own initiative in protecting user's information by destroying access logs daily, posting warning signs, and teaching users about privacy issues. This is especially done in order for information to not be obtained outside of legal restrictions.\n\nSpecifically, the Livingston Lord Library (LLL) of Minnesota State University's mission is to support both cultural and academic experiences and to encourage lifelong learning. Thus, their particular library provides resources that allows individuals to enhance their knowledge and skills. At the same time, they work to maintain their image of believing in confidentiality such that people can exercise their First Amendment right. Yet, there is not specific documentation as of 2007 that displays what privacy is to them. Nevertheless, there are examples of librarians exerting effort to ensure confidentiality and privacy by protecting their user's information.\n\nCampus Privacy Officers (CPO's) are individuals within the institution who have the institutional responsibility for anything that regards privacy, they make sure that privacy is upheld within higher education. Yet they are relatively new in the United States but nevertheless have been growing since 2002. Their role or function in higher education is:\n\n“sustaining an environment where faculty and student are free to inquire, experiment, discover, speak, and participate in discourse is without intimidation, protecting against and responding to modern-day cybersecurity threats, protecting the interests of individuals and assuring they have appropriate influence over data about themselves, pursuing opportunities for use of data in medical treatment, research, and student success, and enabling shared governance” \n\nTheir activities include maintaining: data privacy policies, notices, personal data inventory, governance structure and to respond to both complaints and requests from individuals, among other tasks. \n\nA few of the major issues that CPO's focus on are :\n\n\nUniversity of California Berkeley \n\nIn 1964 students at UC Berkeley protested against the ban that prevented them from engaging in political activity on campus. FBI Director J.Edgar Hoover got involved because he thought that the Free Speech Movement had to do with Communism that aimed to disrupt Capitalism and thus U.S. government. Particularly, Seth Rosenfield's book \"Subersives: The FBI's War on Student Radicals, and Reagan's Rise to Power\" demonstrates how Hoover investigated the movement and specifically student activists such as Mario Savio through \"'intense surveillance and harassment.'\" Further, when Clark Kerr, former president of Berkeley and then vice chancellor of the University of California system, lifted the ban on political engagement and further against \"Communist speakers\" the FBI targeted him and tried to get him fired. Hoover had ordered agents to find information about Kerr and leak it to the Board of Regents in order to show that Kerr was not fulfilling his role as president and thus had to be fired. Essentially, what it points to is that in the 1960's the FBI took on the role of trying to eliminate Communism within the UC Berkeley campus by investigating particular individuals in order to see if they were really Communist or in the case of Kerr to fire them for lifting the ban on political engagement. Some say that this is a breach in privacy because the FBI surveilled and investigated individuals without their consent. Others say that it was needed in order to make sure that no Communist activity was taking place particularly on the Berkeley campus due to the Free Speech Movement. \n\nPrinceton University\n\nIn 2002 Princeton University’s admission staff accessed a Yale University website used to inform applicants that they have been admitted. The act of Yale University accessing private information was brought to light. As a result, Yale University responded that it would improve their website with additional security to prevent another breach. Meanwhile, Princeton University responded by announcing the resignation of the top Princeton admissions official. Some say that acts like these raise the issues of student record privacy in the digital world. \n\nUniversity of Oregon \n\nIn 2015 a woman who said she was raped by three basketball players sued the University of Oregon for disclosing her mental health records to an attorney. This case gave rise to employees from the counseling center to write an open letter to the university community as a form to show that they were disturbed by the university's actions. Yet, officials argue that because the women claimed to have emotional distress then the university had the right to her medical records under FERPA. An attorney named Steve McDonald argued that HIPAA did not apply in this case. Meanwhile, Lynn Daggett, a FERPA specialist, stated that the university has the right to get access to student medical records, especially if entails the need for legal defense. This led to Denise Horn, a U.S. Department of Education representative of the time, to write a statement addressing that higher education institutions should comply with FERPA but also respect the expectation of confidentiality between patient and counselor/therapist. \n"}
{"id": "32920094", "url": "https://en.wikipedia.org/wiki?curid=32920094", "title": "Prussian Reform Movement (1806–1815)", "text": "Prussian Reform Movement (1806–1815)\n\nThe Prussian Reform Movement was a series of constitutional, administrative, social and economic reforms early in the nineteenth-century Kingdom of Prussia. They are sometimes known as the Stein-Hardenberg Reforms, for Karl Freiherr vom Stein and Karl August Fürst von Hardenberg, their main initiators. Before the Second World War, German historians, such as Heinrich von Treitschke, saw the reforms as the first steps towards the unification of Germany and the foundation of the German Empire.\n\nThe reforms were a reaction to the defeat of the Prussians by Napoleon I at Jena-Auerstedt in 1806, leading to the second Treaty of Tilsit, in which Prussia lost about half its territory and was forced to make massive tribute payments to France. To make those payments, it needed to rationalize its administration. Prussia's defeat and subjection also demonstrated the weaknesses of its absolute monarchy model of statehood and excluded it from the great powers of Europe.\n\nTo become a great power again, it initiated reforms from 1807 onwards, based on Enlightenment ideas and in line with reforms in other European nations. They led to the reorganization of Prussia's government and administration and changes in its agricultural trade regulations, including the abolition of serfdom and allowing peasants to become landowners. In industry, the reforms aimed to encourage competition by suppressing the monopoly of guilds. The administration was decentralised and the nobility's power reduced. There were also parallel military reforms led by Gerhard von Scharnhorst, August Neidhardt von Gneisenau and Hermann von Boyen and educational reforms headed by Wilhelm von Humboldt. Gneisenau made it clear that all these reforms were part of a single programme when he stated that Prussia had to put its foundations in \"the three-faced primacy of arms, knowledge and the constitution\".\n\nIt is harder to ascertain when the reforms ended – in the fields of the constitution and internal politics in particular, the year 1819 marked a turning point, with Restoration tendencies gaining the upper hand over constitutional ones. Though the reforms undoubtedly modernised Prussia, their successes were mixed, with results that were against the reformers' original wishes. The agricultural reforms freed some peasants, but the liberalisation of landholding condemned many of them to poverty. The nobility saw its privileges reduced but its overall position reinforced.\n\nIn 1803, German Mediatisation profoundly changed the political and administrative map of Germany. Favourable to mid-ranking states and to Prussia, the reorganisation reinforced French influence. In 1805, the Third Coalition formed in the hope of stopping French domination of Europe from advancing further, but the coalition's armies were defeated at Austerlitz in December 1805. Triumphant, Napoleon I continued working towards dismantling the Holy Roman Empire. On 12 July 1806, he detached 16 German states from it to form the Confederation of the Rhine under French influence. On 6 August the same year, Francis I of Austria was forced to renounce his title of emperor, and the Empire had to be dissolved.\n\nFrench influence reached as far as the Prussian frontier by the time Frederick William III of Prussia realised the situation. Encouraged by the United Kingdom, Prussia broke off its neutrality (in force since 1795) and repudiated the 1795 Peace of Basel, joined the Fourth Coalition and entered the war against France. Prussia mobilised its troops on 9 August 1806 but two months later was defeated at Jena-Auerstedt. Prussia was on the verge of collapse, and three days after the defeat Frederick William III issued posters appealing to the inhabitants of his capital Berlin to remain calm. Ten days later, Napoleon entered Berlin.\n\nThe war ended on 7 July 1807 with the first Treaty of Tilsit between Napoleon and Alexander I of Russia. Two days later, Napoleon signed a second Treaty of Tilsit with Prussia, removing half its territory and forcing Prussia's king to recognise Jérôme Bonaparte as sovereign of the newly created Kingdom of Westphalia, to which Napoleon annexed the Prussian territories west of the river Elbe. Prussia had had 9 million inhabitants in 1805, of which it lost 4.55 million in the treaty. It was also forced to pay 120 million francs to France in war indemnities and fund a French occupying force of 150,000 troops.\n\nThe biting defeat of 1806 was not only the result of poor decisions and Napoleon's military genius, but also a reflection on Prussia's poor internal structures. In the 18th century the Prussian state had been the model of enlightened despotism for the rest of Germany. To the west and south, there was no single state or alliance that could challenge it. Yet in the era of Frederick II of Prussia it was a country oriented towards reform, beginning with the abolition of torture in 1740.\nThe economic reforms of the second half of the 18th century were based on a mercantilist logic. They had to allow Prussia a certain degree of self-sufficiency and give it sufficient surpluses for export. Joseph Rovan emphasises that Economic development also had to fund and support the military. Prussia's infrastructure was developed in the form of canals, roads and factories. Roads connected its outlying regions to its centre, the Oder, Warta and Noteć marshes were reclaimed and farmed and apple-growing was developed.\n\nHowever, industry remained very limited, with heavy state-control. Trades were organised into monopolistic guilds and fiscal and customs laws were complex and inefficient. After the defeat of 1806, funding the occupation force and the war indemnities put Prussia's economy under pressure. As in the 18th century, the early 19th century reforms aimed to create budgetary margins, notably in their efforts towards economic development.\n\nFrederick II of Prussia favoured both economic and political reform. His government worked on the first codification of Prussia's laws – the 19,000 paragraph \"Allgemeines Landrecht für die preußischen Staaten\" (literally the \"General legal code for the Prussia states\"). Article 22 indicated that all his subjects were equal before the law: . However, Frederick died in 1786 leaving the code incomplete and was succeeded by Frederick William II of Prussia, who extended the same administrative structure and the same civil servants.\n\nThe absolutist system started to re-solidify under the obscurantist influence of Johann Christoph von Wöllner, financial privy councillor to Frederick William II. The reforms stalled, especially in the field of modernising society. The editing of the \"Allgemeines Landrecht\" was completed in 1792, but the French Revolution led to opposition to it, especially from the nobility. It was then withdrawn from circulation for revision and did not come back into force until 1794. Its aims included linking the state and middle class society to the law and to civil rights, but at the same time it retained and confirmed the whole structure of the Ancien Régime. Serfdom, for example, was abolished in Prussia's royal domains but not in the estates of the great landowners east of the river Elbe. The nobility also held onto its position in the army and administration.\n\nIn 1797 Frederick William III succeeded his father Frederick William II, but at the time of his accession he found society dominated by the old guard, apart from the \"Allgemeines Landrecht\" promulgated in 1794. His own idea of the state was absolutist and he considered that the state had to be in the hands of the sovereign. Before 1806, several observers and high-level civil servants such as Heinrich Friedrich Karl vom Stein and Karl August von Hardenberg underlined the fact that the Prussia state needed restructuring. As Minister of Finances and the Economy, Stein put some reforms in place, such as standardising the price of salt (then a state monopoly) and partially abolishing the export-import taxes between the kingdom's territories. In April 1806, he published \"Darstellung der fehlerhaften Organisation des Kabinetts und der Notwendigkeit der Bildung einer Ministerialkonferenz\" (literally \"Exposé on the imperfect organisation of the cabinet and on the necessity of forming a ministerial conference\"). In it, he wrote: .\n\nPrussia's war against Napoleon revealed the gaps in its state organisation. Pro-war and a strong critic of his sovereign's policies, Stein was dismissed in January 1807 after the defeat by France. However, Frederick William III saw that the Prussian state and Prussian society could only survive if they began to reform. After the treaty of Tislsit, he recalled Stein as a minister on 10 July 1807 with the backing of Hardenberg and Napoleon, the latter of whom saw in Stein a supporter of France. Queen Louise of Mecklenburg-Strelitz also supported Stein's re-appointment – indeed, she was more in favour of reform than her husband and was its main initiator. Aided by Stein, Hardenberg and others, she had convinced her husband to mobilise in 1806 and in 1807 she had even met with Napoleon to demand that he review the hard conditions imposed in the treaty. Hardenberg wrote the same year: \n\nStein set certain conditions for his taking the job, among which was that the cabinets system should be abolished. In its place, ministers had to win their right to power by speaking to the king directly. After this condition had been satisfied, Stein took up his role and was thus directly responsible for the civil administration as well as exercising a controlling role over the other areas. Frederick William III still showed little inclination to engage in reforms and hesitated for a long while. The reformers thus had to expend much effort convincing the king. In this situation, it was within the bureaucracy and the army that the reformers had to fight the hardest against the nobility and the conservative and restorationist forces. The idealist philosophy of Immanuel Kant thus had a great influence on the reformers – Stein and Hardenberg each produced a treatise describing their ideas in 1807.\n\nAfter his recall, Stein retired to his lands in Nassau. In 1807, he published the \"Nassauer Denkschrift\", whose main argument was the reform of the administration. In contrast to the reforms in the states of the Confederation of the Rhine, Stein's approach was traditionalist and above all anti-Enlightenment, focussing instead on critiquing absolutism. Stein followed English models such as the Glorious Revolution of 1688 and was sceptical of a centralised and militarised bureaucracy, favouring a decentralised and collegiate administration. With his collaborators, he followed (in his own words) a \"policy of defensive modernisation, not with Napoleon but against him\".\n\nAccording to Stein, the administration should be split up by field and no longer by geographical area. Thus the administration had to be divided into two branches – the public revenue branch and the top-level state-policy branch (\"oberste Staatsbehörde\"). One of the main aims of this concept was to rationalise the state financial system to raise the money to meet its war indemnities under the treaty of Tilsit. Rationalising the state finances would allow the state to raise revenue but limit losses due to poor administrative organisation.\n\nStein was an anti-absolutist and an anti-statist, suspicious of bureaucracy and central government. For him, civil servants were only men paid to carry out their task with \"indifference\" and \"fear of innovation\". Above all, he set out to decentralise and form a collegiate state. Stein thus gave more autonomy to the provinces, Kreise and towns. Thanks to the different posts he had previously held, Stein, realised that he had to harmonise the government of the provinces. He had recourse to the old corporative constitution, as he had experienced in Westphalia. The landowner, according to Stein, was the keystone to local self-government – \"If the landowner is excluded from all participation in the provincial administration, then the link which links him to the fatherland remains unused\".\n\nHowever, it was not only functional considerations which played a role for Stein. He felt he first had to educate the people in politics and provincial self-government was one of the most useful things in this area. On landowners' participation in the provincial administration, he wrote In his reform projects, Stein tried to reform a political system without losing sight of the Prussian unity shaken by the defeat of 1806.\n\nStein and Hardenberg not only made a mark on later policy but also represented two different approaches to politics, with Hardenberg more steeped in Enlightenment ideas. He took the principles of the French Revolution and the suggestions created by Napoleon's practical policy on board more deeply than Stein. Hardenberg was a statist who aspired to reinforce the state through a dense and centralised administration. Nevertheless, these differences only represented a certain change of tendency among the reformers. The initiatives put in place were very much things of their own time, despite the latter umbrella concept of the 'Stein-Hardenberg reforms'.\n\nThe \"Rigaer Denkschrift\" was published the same year as Stein's work and presented on 12 September 1807. It bore the title 'On the reorganisation of the Prussian state'. Previously living in Riga, Hardenberg had been summoned in July by the king of Prussia under pressure from Napoleon. Hardenberg developed ideas on the overall organisation of the Prussian state that were different from those of his fellow reformers. The main editors of the \"Rigaer Denkschrift\" were Barthold Georg Niebuhr, an expert financier, Karl vom Stein zum Altenstein, a future minister of Finances and Heinrich Theodor von Schön. These three men concluded that the Revolution had given France a new impetus: \"All the sleeping forces were re-awakened, the misery and weakness, the old prejudices and the shortcomings were destroyed.\" Thus, in their view, Prussia had to follow France's example: \nThe authors thus favoured a revolution \"im guten Sinn\" or \"in the right sense\", which historians later described as \"revolution from above\". Sovereigns and their ministers thus put in place reforms to gain all the advantages of a revolution without any of the disadvantages, such as losing their power or suffering from setbacks or outbreaks of violence.\n\nAs in Stein's Denkschrift, the \"Rigaer Denkschrift\" favours reviving national spirit to work with the nation and the administration. Hardenberg also sought to define society's three classes – the nobility, the middle class and the peasants. For him, the peasants took part in \"the most numerous and most important but nevertheless the most neglected and belittled class in the state\" and added that \"the peasant class has to become the main object of our attention\". Hardenberg also sought to underline the principal of merit which he felt had to rule in society, by affirming \"no task in the state, without exception, is for this or that class but is open to merit and to skill and to the ability of all classes\".\n\nWithin fourteen months of his appointment, Stein put in place or prepared the most important reforms. The major financial crisis caused by the requirements of Tilsit forced Stein into a radical austerity policy, harnessing the state's machinery to raise the required indemnities. The success of the reforms begun by Stein was the result of a discussion already going on within the upper bureaucracy and Stein's role in putting them in place was variable – he was almost never, for example, involved in questions of detail. Many of the reforms were drafted by others among his collaborators, such as Heinrich Theodor von Schön in the case of the October decree. However, Stein was responsible for presenting the reforms to the king and to other forces opposed to them, such as the nobility.\nDuring Stein's short period of office, decisive laws were promulgated, even if the organizational law on state administration was not published until 1808 (i.e. after Stein's fall). It was during Stein's time in office that the edict of October 1807 and the cities' organizational reforms (\"Städteordnung\") of 1808 were put into effect. After a short term of office by Karl vom Stein zum Altenstein, Hardenberg regained control of policy. From 1810, he bore the title of \"Staatskanzler\", retaining it until 1822. Thanks to him, land reform was completed via the Edicts of Regulation (\"Regulierungsedikten\") of 1811 and 1816 as well as the \"Ablöseordnung\" (literally the \"redemption decree\") of 1821. He also pushed through the reforms of trades such as the edict on professional tax of 2 November 1810 and the law on policing trades (\"Gewerbepolizeigesetz\") of 1811. In 1818 he reformed the customs laws, abolishing internal taxes. As for social reform, the edict of emancipation was promulgated in 1812 for Jewish citizens. Despite the different initial situations and aims pursued, similar reforms were carried out in the states of the Confederation of the Rhine, except for the military and educational reforms. The Restoration tendencies put a stop to the reformist policy in Prussia around 1819 or 1820.\n\nThe reforms which were to be put in place were essentially a synthesis between historic and progressive concepts. Their aim was to replace the absolutist state structures which had become outdated. The state would have to offer its citizens the possibility of becoming involved in public affairs on the basis of personal freedom and equality before the law. The government's main policy aim was to make it possible to liberate Prussian territory from French occupation and return the kingdom to great-power status through modernising domestic policy.\n\nThe Prussian subject had to become an active citizen of the state thanks to the introduction of self-government to the provinces, districts (\"kreise\") and towns. National sentiment had to be awakened as Stein foresaw in his Nassau work, but a citizen's duties were in some ways more important than his or her rights. Moreover, Stein's concept of self-government rested on a class-based society. A compromise between corporative aspects and a modern representative system was put in place. The old divisions into the three estates of nobility, clergy and bourgeoisie were replaced by divisions into nobility, bourgeoisie and peasants. The right to vote also had to be expanded, particularly to free peasants, which would be one of the bases for the freeing of the peasants in 1807.\n\nThe new organisation of power in the countryside and reform of industry were factors in the liberalisation of the Prussian economy. In this respect, the Prussian reforms went much further than those in the states of the Confederation of the Rhine and were much more successful. The 1806 financial crisis, intensified by the indemnities, the occupation costs and other war costs, gave the necessary impetus for these changes – in all, Prussia had to pay France 120 million francs. The freeing of the peasants, the industrial reforms and the other measures removed economic barriers and imposed free competition. The Prussian reforms relied on the economic liberalism of Adam Smith (as propounded by Heinrich Theodor von Schön and Christian Jakob Kraus) more heavily than the south German reformers. The Prussian reformers did not actively seek to encourage Prussian industry, which was then under-developed, but to remedy the crisis in the agricultural economy.\n\nThe reformers' top priority was to reorganise the administration and the state. Before 1806, there was not really a single Prussian state, but a multiplicity of states and provinces, mostly only held together by the single person of the king himself. There was no unified administration – instead there were the two parallel structures of decentralised administrations (each responsible for all portfolios within a single given territory) and a centralised administration (responsible for a single portfolio across the whole of Prussia). This double structure made any coordinated action difficult. The government also had no overview of Prussia's economic situation and its government ministers had little clout faced with the king's cabinet, where they had less power than the king's private political councillors.\n\nThe start of the Stein era saw the unification of the Prussian state, with the old system of cabinets being abolished. A ministry of state (\"Staatsministerium\") was introduced on 16 December 1808 in place of a top-level administration poorly defined as the \"Generaldirektorium\". This reform was completed in 1810. Now the administration was ruled according to the principle of portfolios. The \"Staatsministerium\" included five major ministries – minister of the interior, minister for foreign affairs, minister for finances, minister for justice and minister of war, all responsible to the king alone. These modifications could not take full effect, however, until a more effective statist model of leadership was created. This was done by replacing Prussian absolutism with a double domination of king and bureaucracy, in which the ministers had an important role, reducing the king's influence and meaning he could now only reign through his ministers' actions. In Stein's era, the \"Staatsministerium\" was organised in a collegiate way without a prime minister – that post was set up under Hardenberg, who received the title of \"Staatskanzler\" or State Chancellor in June 1810 along with control over the ministers' relations with the king.\n\nThe role of the head of state was also considerably modified. From 1808, Prussia was divided into districts. The different governments of these districts were set up according to the principle of portfolios, as with the national ministers of state. Each region was given an \"Oberpräsident\" for the first time, directly subordinate to the national ministers and with the role of stimulating public affairs. Their rôle, which even went as far as putting up sanitary cordons in the event of an epidemic, was similar to that of French prefects – that is, to represent regional interests to the central government. The post was abolished in 1810 but revived in 1815 to play an important part in political life. It was in this context that justice and administration were separated once and for all. On the establishment of administrative acts, the people concerned thus had a right of appeal. Nevertheless, there was no judicial control on the administration. Aiming to reduce any influence on the administration, this was reinforced by different administrative acts. The organisation that the reformers put in place served as a model for other German states and to major businesses.\n\nIn parallel to the \"Staatsministerium\", Stein planned the creation of a \"Staatsrat\" or Privy Council. However, he had had no opportunity to set up a correctly functioning one by 1808 and it was Hardenberg who set it up in 1810. The text of the relevant law stated: The members of the Council of State had to be current ministers or former ministers, high-level civil servants, princes of the royal house or figures nominated by the king. A commission was also formed to function as a kind of parliament, with major legislative rights. As a bastion of the bureaucracy, the Council of State had to prevent any return to absolutism or any moves to reinforce the interests of the Ancien Régime. The Council of State also had to subroge all laws and administrative and constitutional procedures.\n\nIn the same way as the towns' self-government, Hardenberg foresaw the establishment of a national representative body made up of corporative and representative elements. The first assembly of notable figures was held in 1811 and the second in 1812. These were made up of a corporative base of 18 aristocratic landowners, 12 urban property owners and nine representatives from among the peasants. This corporative composition was based partly on the traditional conception of society and partly on practical and fiscal considerations – in order to be able to pay its war indemnities to France, the Prussian state had to have massive recourse to credit contracts issued by the aristocrats and to gain credit in foreign countries the different states had to offer themselves as guarantors.\n\nAfter the summoning of the provisional assemblies, it quickly became clear that their deputies' first priority was not the state's interests but more defending their own classes' interests. The nobility saw the reforms as trying to reduce their privileges and so blocked them in the assemblies, led by figures such as Friedrich August von der Marwitz and Friedrich Ludwig Karl Fink von Finkenstein. Their resistance went so far that the cabinet even resorted to imprisoning them at Spandau. The historian Reinhart Koselleck has argued that the establishment of a corporative national representative body prevented all later reforms. At the end of the reforming period, the districts and the provincial representative bodies (such as the \"Provinziallandtage\") remained based on corporative principles. Prussia was prevented from forming a true representative national body, with considerable consequences on the internal development of Prussia and the German Confederation. Thus, while the states of the Confederation of the Rhine located in southern Germany became constitutional states, Prussia remained without a parliament until 1848.\n\nPrior to the reforms the Prussian towns east of the river Elbe were under the state's direct control, with any surviving instances of self-government retaining their names and forms but none of their power. Stein's reform of the towns used this former tradition of self-government. All rights specific to a certain city were abolished and all the cities put under the same structures and rule – this even came to be the case for their courts and police. Self-government was at the centre of the town reforms of 1808, with the towns now no longer subject to the state and their citizens given a duty to participate in the towns' political life. This was the strongest indication of Stein's rejection of a centralised bureaucracy – self-government had to awaken its citizens' interest in public affairs in order to benefit the whole Prussian state.\n\nThe \"Städteordnung\" (Municipal Ordinance) of 1808 defined a citizen (or at least a citizen in the sense of the inhabitant of a town or city) as \"a citizen or member of an urban community which possesses the right of citizenship in a town\". The municipal councillors were representatives of the town and not of an order or estate. These councillors could be elected by all landowning citizens with a taxable revenue of at least 15 taler. A councillor's main task was to participate in the election of a municipal council or \"Magistrat\", headed by a mayor. The election of the mayor and the members of the council had to be ratified by the central government. Different officials were put in place to carry out administrative portfolios. The council managed the municipal budget and the town also managed its own police.\n\nDespite some democratic elements, the town administrations retained large corporative elements – the groups were differentiated according to their estates and only citizens had full rights. Only landowners and industrial property-owners had a right to citizenship, though it was in principle also open to other people, such as \"Eximierten\" (bourgeois people, mostly those in state service) or \"Schutzverwandten\" (members of the lower classes without full citizenship rights). The costs linked to a citizen's octroi dissuaded many people. It was only the new reform of 1831 which replaced the 1808 assemblies of \"Bürger\" (citizens) with assemblies of inhabitants. Until the Vormärz, self-government in the towns was in the hands of artisans and established businessmen. In the cities and large towns, the citizens with full rights and their families represented around a third of the total population. Resistance by the nobility prevented these reforms from also being set up in the countryside. These reforms were a step towards modern civic self-government.\n\nTax reform was a central problem for the reformers, notably due to the war indemnities imposed by Napoleon, and these difficulties marked Hardenberg's early reforms. He managed to avoid state bankruptcy and inflation by increasing taxes or selling off lands. These severe financial problems led to a wholesale fiscal reform. Taxes were standardised right across Prussia, principally by replacing the wide variety of minor taxes with main taxes. The reformers also tried to introduce equal taxation for all citizens, thus bringing them into conflict with aristocratic privileges. On 27 October 1810, the king proclaimed in his \"Finanzedikt\":\nExcises were raised the following year on appeals.\n\nIn 1819, excise (originally only raised by the towns) was suppressed and replaced with a tax on the consumption of beer, wine, gin and tobacco. In the industrial sphere, several taxes were replaced with a progressively spread-out professional tax. Other innovations were an income tax and a tax on wealth based on a tax evaluation carried out by the taxpayer. 1820 saw protests against a tax on classes, the tax being defined by the taxpayer's position in society. This tax on classes was an intermediate form between poll tax and income tax. The towns had the possibility of retaining the tax on cattle and cereal crops. The results for fiscal policy remain controversial. The nobility was not affected by the taxes as the reformers had originally planned, so much so that they did not managed to put in place a 'foncier' tax also including the nobility. The poorest suffered most as a result of these measures.\nIt was only after the end of the Napoleonic Wars and after the territorial reorganisation of Europe at the Congress of Vienna that Prussia's customs duties were reformed. At the Congress Prussia regained its western territories, leading to economic competition between the industrialised part of these territories such as the Rhine Province, the Province of Westphalia and the territories in Saxony on the one hand and the essentially agricultural territories to the east of the Elbe on the other. Customs policy was also very disparate. Thus, in 1817, there were 57 customs tariffs on 3,000 goods passing from the historic western territories to the Prussian heartland, with the taxes in the heartland not yet having spread to the formerly French-dominated western provinces.\n\nThis was one of the factors that made customs reform vital. That reform occurred on 26 May 1818, with the establishment of a compromise between the interest of the major landowners practicing free-exchange and those of the still-weak industrial economy asking for protectionist custom duties. They therefore only took on what would now be called a tax for protecting internal markets from foreign competition and customs duties for haulage were lifted. The mercantile policy instituted by Frederick II thus came to an end. Export bans were lifted. The customs laws and duties put in place by the reformers proved so simple and effective over time that they served as a model for taxation in other German states for around fifty years and that their basic principles remained in place under the German Empire. The Prussian customs policy was one of the important factors in the creation of the Deutscher Zollverein in the 1830s.\n\nAgriculture was reformed across Europe at this time, though in different ways and in different phases. The usefulness of existing agricultural methods came into doubt and so the Ancien Régime's and Holy Roman Empire's agricultural structures were abolished. Peasants were freed and became landowners; and services and corvées were abolished. Private landownership also led to the breakdown of common lands – that is, to the usage of woods and meadows 'in common'. These communal lands were mostly given to lords in return for lands acquired by the peasants. Some meadow reforms had already taken place in some parts of Prussia before 1806, such as the freeing of the peasants on royal lands in the 18th century, though this freeing only fully came into force in 1807.\n\nThe landowning nobility successfully managed to oppose similar changes. The government had to confront aristocratic resistance even to the pre-1806 reforms, which became considerable. The \"Gesindeordnung\" of 1810 was certainly notable progress for servants compared to that proposed in the \"Allgemeines Landrecht\", but still remained conservative and favourable to the nobility. The nobility's opposition to this also led to several privileges being saved from abolition. The rights of the police and the courts were controlled more strongly by the state, but not totally abolished like religious and scholarly patrongage, hunting rights and fiscal privileges. Unlike the reforms in the Kingdom of Bavaria, the nobles were not asked to justify their rank. The reformers made compromises, but the nobility were unable to block the major changes brought by the reforms' central points.\n\nThe freeing of the peasants marked the start of the Prussian reforms. The kingdom's modernisation began by modernising its base, that is, its peasants and its agriculture. At the start of the 19th century, 80% of the German population lived in the countryside. The edict of 9 October 1807, one of the central reforms, liberated the peasants and was signed only five days after Stein's appointment on von Schön's suggestion. The October edict began the process of abolishing serfdom and its hereditary character. The first peasants to be freed were those working on the domains in the \"Reichsritter\" and on 11 November 1810 at the latest, all the Prussian serfs were declared free: However, though serfdom was abolished, corvées were not – the October edict said nothing on the latter subject.\n\nThe October edict authorised all Prussian citizens to acquire property and choose their profession, including the nobles, who until then could not take on jobs reserved for the bourgeoisie: The principle of \"dérogeance\" disappeared.\n\nThe peasants were allowed to travel freely and set up home in the towns and no longer had to buy their freedom or pay for its with domestic service. The peasants no longer had to ask their lord's permission to marry – this freedom in marriage led to a rising birth rate and population in the countryside. The freeing of the peasants, however, was also to their disadvantage – lordly domains were liberalised and major landowners were allowed to buy peasants' farms (the latter practice having been illegal previously). The lords no longer had an obligation to provide housing for any of their former serfs who became invalids or too old to work. This all led to the formation of an economic class made up of bourgeois and noble entrepreneurs who opposed the bourgeoisie.\n\nAfter the reformers freed the peasants, they were faced with other problems, such as the abolition of corvées and the establishment of properties. According to the \"Allgemeines Landrecht\", these problems could only be solved by compensating the financiers. The need to legally put in place a \"revolution from above\" slowed down the reforms.\n\nThe edict of regulation of 1811 solved the problem by making all peasants the owners of the farms they farmed. In place of buying back these lands (which was financially impossible), the peasants were obliged to compensate their former lords by handing over between a third and a half of the farmed lands. To avoid splitting up the lands and leaving areas that were too small to viably farm, in 1816 the buy-back of these lands was limited to major landowners. The smaller ones remained excluded from allodial title. Other duties linked to serfdom, such as that to provide domestic service and the payment of authorisation taxes on getting married, were abolished without compensation. As for corvées and services in kind, the peasants had to buy back from their lords for 25% of their value.\nThe practical compensations in Prussia were without doubt advantageous compared to the reforms put in place in the states of the Confederation of the Rhine. In effect, they allowed the process of reform to be accelerated. Nevertheless, the 12,000 lordly estates in Prussia saw their area increase to reach around 1.5 million Morgen (around 38,000 hectares), mostly made up of common lands, of which only 14% returned to the peasants, with the rest going to the lords. Many of the minor peasants thus lost their means of subsistence and most could only sell their indebted lands to their lords and become agricultural workers. Some jachère lands were made farmable, but their cultivation remained questionable due to their poor soil quality. The measures put in place by the reformers did have some financial success, however, with Prussia's cultivated land rising from 7.3 to 12.46 million hectares in 1848 and production raised by 40%.\n\nIn the territories east of the Elbe, the agricultural reforms had major social consequences. Due to the growth of lordly estates, the number of lordly families rose greatly, right up until the second half of the 19th century. The number of exploited lands remained the same. A very important lower social class was also created. According to region and the rights in force, the number of agricultural day workers and servants rose 2.5 times. The number of minor landowners, known as \"Kätner\" after their homes (known as \"Kotten\"), tripled or even quadrupled. Many of them were dependent on another job. Ernst Rudolf Huber, professor of public law, judged that the agricultural reforms were \n\nThe reformers aspired to free individual forces in the industrial sphere just as in the agricultural one, in their devotion to the theories of Adam Smith. To free these forces, they had to get rid of guilds and an economic policy based on mercantilism. To encourage free competition also meant the suppression of all limitations on competition.\n\nIt was in this context that the freedom of industry (\"Gewerbefreiheit\") was introduced in 1810–1811. To set up an industry, one had to acquire a licence, but even so there were exceptions, such as doctors, pharmacists and hotelliers. The guilds lost their monopoly role and their economic privileges. They were not abolished, but membership of them was now voluntary, not compulsory as it had been in the past. State control over the economy also disappeared, to give way to a free choice of profession and free competition. The reform of industry unlocked the economy and gave it a new impetus. There was no longer any legal difference in the industrial sphere between the town and the countryside. Only mining remained as an exception until the 1860s.\n\nOriginally planned to encourage rural industry, the freedom of industry became the central condition for Prussian economic renewal on an industrial base. As had happened with the nobility, the citizens of the towns arose unsuccessfully opposed the reforms. Their immediate results were contradictory—early on, non-guild competition was weak, but after a period of adaptation the number of non-guild artisans rose significantly. However, in the countryside, the burdens of the artisans and other industries rose considerably. This rise in the number of artisans was not accompanied by a similar growth in the rest of the population. The number of master-craftsmen rose too, but master-craftsmen remained poor due to the strong competition. During the Vormärz, tailors, cobblers, carpenters and weavers were the main over-subscribed trades. The rise in the lower classes in the countryside accentuated the 'social question and would be one of the causes of the 1848 Revolution.\n\nBy the Edict of Emancipation of 11 March 1812, Jews gained the same rights and duties as other citizens:\n\nArticle 8 of the Edict allowed Jews to own land and take up municipal and university posts. The Jews were free to practise their religion and their traditions were protected. Nevertheless, unlike the reforms in the Kingdom of Westphalia, the Edict of Emancipation in Prussia did have some limitations – Jews could not become army officers or have any government or legal role, but were still required to do military service.\n\nEven if some traditionalists opposed the Edict of Emancipation, it proved a major step towards Jewish emancipation in the German states during the 19th century. The judicial situation in Prussia was significantly better than that in most of southern and eastern Germany, making it an attractive destination for Jewish immigration.\n\nFor the reformers, the reform of the Prussian education system (\"Bildung\") was a key reform. All the other reforms relied on creating a new type of citizen who had to be capable of proving themselves responsible and the reformers were convinced that the nation had to be educated and made to grow up. Unlike the state reforms, which still contained corporative elements, the \"Bildungsreform\" was conceived outside all class structures. Wilhelm von Humboldt was the main figure behind the educational reform. From 1808, he was in charge of the department of religion and education within the ministry of the interior. Like Stein, Humboldt was only in his post for a short time, but was able to put in place the main elements of his reforms.\n\nHumboldt developed his ideas in July 1809 in his treatise \"Über die mit dem Königsberger Schulwesen vorzunehmende Reformen\" (\"On reforms to execute with the teaching in Königsberg\"). In place of a wide variety of religious, private, municipal and corporative educational institutions, he suggested setting up a school system divided into \"Volksschule\" (people's schools), \"Gymnasiums\" and universities. Humboldt defined the characteristics of each stage in education. Elementary teaching \"truly only need be occupied with language, numbers and measures, and remain linked to the mother tongue being given that nature is indifferent in its design\". For the second stage, that of being taught in school, Humboldt wrote \"The aim of being taught in school is to exercise [a pupil's] ability and to acquire knowledge without which scientific understanding and ability are impossible. Finally, he stated that university had to train a student in research and allow him to understanding \"the unity of science\". From 1812, a university entry had to obtain the \"Abitur\". The state controlled all the schools, but even so it strictly imposed compulsory education and controlled exams. To enter the civil service, performance criteria were set up. Education and performance replaced social origin.\n\nWilhelm von Humboldt backed a new humanism. Unlike the utilitarian teaching of the Enlightenment, which wished to transmit useful knowledge for practical life, Humboldt desired a general formation of man. From then students had to study antiquity and ancient languages to develop themselves intellectually. Not only would they acquire this humanistic knowledge, they would also acquire other knowledge necessary for other jobs. The state would not seek to form citizens at all costs to serve it, but it did not entirely let go of that aim:\n\nIn paying professors better and improving their training, the quality of teaching in the \"Volksschule\"s was improved. The newly founded \"gymnasia\" offered a humanist education to ready pupils for university studies. In parallel \"Realschule\"s were set up to train men in manual trades. Some schools for officer cadets were allowed to remain. Despite stricter state influence and control, the religious authorities retained their role in inspecting schools.\n\nIn Humboldt's thinking, university represented the crowning glory of intellectual education and the expression of the ideal of freedom between teaching and research held an important place in it. German universities of the time were mostly mediocre. For Humboldt, \"the state must treat its universities neither as gymnasia nor as specialist schools and must not serve its Academy as a technical or scientific deputation. Together, they must [...] demand nothing of them which does not give it profit immediately and simply\".\n\nStudents, in his view, had to learn to think autonomously and work in a scientific way by taking part in research. The foundation of Berlin University served as a model. It was opened in 1810 and the great men of the era taught there – Johann Gottlieb Fichte, the physician Christoph Wilhelm Hufeland, the historian Barthold Georg Niebuhr and the jurist Friedrich Carl von Savigny.\n\nIn practice, the educational reforms' results were different from what Humboldt had expected. Putting in place his ideal of philological education excluded the lower classes of society and allied the educational system to the restorationist tendencies. The major cost of education rendered the reforms in this area ineffective. The reformers had hoped that people would rise through the social scale thanks to education, but this did not happen so well as they had hoped.\n\nUnlike the reforms in the states of the Confederation of the Rhine, the Prussian policy was aimed against French supremacy right from the start. Also, the Prussian military reforms were much more profound than those in the south German states. They were instigated by a group of officers which had formed after the defeats of 1806 and notably included Scharnhorst, Gneisenau, Boyen, Grolman and Clausewitz.\n\nChief of staff since 1806, Scharnhorst became head of the military reorganisation commission set up by Frederick William III in July 1807. For him, every citizen was a born defender of the state. His main aim was to drive out the French occupiers. In close contact with Stein, Scharnhorst managed to convince the king that the military needed reform. Like the civil administration, the military organisation was simplified, via the creation of a Prussian ministry of war and of an army staff on 25 December 1808. Scharnhorst was at the head of the new ministry and he aimed his reforms at removing the obstacles between army and society and at making the army ground itself in the citizens' patriotism.\n\nThe experiences of 1806 showed that the old organisation of the Prussian army was no longer a match for the might of the French army. Compared to the French defensive tactics, Prussian tactics were too immobile. Its officers treated their soldiers as objects and punished them severely – one of the most severe punishments, the \"Spießrutenlaufen\", consisted of making a soldier pass between two ranks of men and be beaten by them. The French instead had compulsory military service and the Prussian army's adoption of it was the centre of Prussia's military reforms.\nFrederick William III hesitated about the military reforms, the officer corps and nobility resisted them and even the bourgeoisie remained sceptical. The start of the German campaign of 1813 was the key factor. On 9 February 1813 a decree replaced the previous conscription system with an obligation to serve by canton \"Kantonpflichtigkeit\"), and this new system had to last for the whole war. Thus it looked to restore the pride and position of the common soldier in adapting army discipline to civil law. The punishments and in particular the 'schlague' (consisting of a soldier being beaten) were abolished. The social differences had to disappear. The Treaty of Tilsit had reduced the Prussian army to 42,000 men, but Scharnhorst put in place the \"Krümper system\", which consisted of training a number of soldiers by making them turn without ever exceeding the numbers authorised by the Treaty. Between 30,000 and 150,000 supplementary men were also trained – the training system changed several times and so it is difficult to work out precise numbers. Compulsory military service was ordered by Frederick William III on 27 May 1814 then fixed by a military law on 3 September the same year:\n\nThe officer corps was also reformed and several officers dismissed. The nobility's privilege was abolished and a career as an officer was opened up to the bourgeois. The aristocrats disliked this and protested, as with Ludwig Yorck von Wartenburg. In practice a system of co-opting of officers was put in place which gave generally favoured nobles, even if there remained some (albeit minor) bourgeois influence. Starting with the regiment of chasseurs on campaign, chasseur and protection units were set up. It was Yorck von Wartenburg who from June 1808 occupied on their training. In the officer corps, it was now the terms of service not the number of years served which determined promotion. The Prussian Academy of War also provided better officer training than before – dissolved after the defeat at Jena, it had been refounded in 1810 by Scharnhorst.\n\nStarting in 1813–1814 with the line infantry troops, we also see the Landwehr, which served as reserve troops to defend Prussia itself. It was independent in organisation and had its own units and its own officers. In the \"Kreise\" (districts), commissions organised troops in which the bourgeois could become officers. The reformers' idea of unifying the people and the army seems to have succeeded. Volunteer chasseur detachments (\"freiwillige Jägerdetachements\") were also formed as reinforcements.\n\nThe reforms are sometimes named after their leaders Stein and Hardenberg, but they were also the fruit of a collaboration between experts, each with his own speciality. One of these experts was Heinrich Theodor von Schön – born in 1773, he had studied law at Königsberg university to follow a career in political sciences. In 1793 he entered Prussian service. Nine years later, he became financial councillor to the \"Generaldirektorium\". When the Prussian government fled to Königsberg after its defeat at Jena, he followed Stein there. It was there that he brought to bear his expertise on serfdom and it was his treatise that would help Stein write the October edict. Unlike Stein, Schön backed a greater liberalisation of landowning – for him, economic profitability had to take first priority, even if this was to the peasants' disadvantage. From 1816, Schön became \"Oberpräsident\", a post he held for around 40 years, and devoted himself to the economic and social life of the provinces which he governed.\nSchön also took part in editing the \"Rigaer Denkschrift\". In 1806 he travelled with a group of civil servants that had gathered around the just-dismissed Hardenberg – the group also included Karl vom Stein zum Altenstein, Friedrich August von Stägemann and Barthold Georg Niebuhr. Niebuhr had studied law, philosophy and history at the university of Kiel between 1794 and 1796. In 1804 he was made head of the Danish national bank. His reputation as a financial expert quickly spread to Prussia. On 19 June 1806, Niebuhr and his family left for Riga with other civil servants to work with Hardenberg when he was dismissed. On 11 December 1809, he was made financial councillor and section chief for state debt. In 1810, he edited a note to the king in which he expressed strong doubts on whether a financial plan put in place by Hardenberg could be realised. Its tone he employed was so strong that the king disavowed him and so Niebuhr retired from politics.\n\nThe three other civil servants present at Riga – Karl vom Stein zum Altenstein, Wilhelm Anton von Klewitz and Friedrich August von Stägemann – also played important rôles in the reforms. Altenstein became high financial councillor in the \"Generaldirektorium\". When Stein was dismissed in 1807, Altenstein and the minister of finances Friedrich Ferdinand Alexander zu Dohna-Schlobitten put in place the state reform conceived by Stein. In 1810, Klewitz and Theodor von Schön edited the \"Verordnung über die veränderte Staatsverfassung aller obersten Staatsbehörden\" (\"Decree on the new constitution of all the high portfolios of state\"). Other collaborators took part in the reforms, such as Johann Gottfried Frey (chief of police in Königsberg and the real author of the \"Städteordnung\"), Friedrich Leopold Reichsfreiherr von Schrötter (who collaborated with Stein on the \"Städteordnung\"), Christian Peter Wilhelm Beuth (in Prussian service since 1801, who had collaborated with Hardenberg on the fiscal and industrial laws) and Christian Friedrich Scharnweber (who had some influence on Hardenberg).\n\nFrom 1806 onwards isolated uprisings occurred in Germany and the German-speaking countries. On 26 August 1806 the bookseller Johann Philipp Palm was shot for publishing an anti-Napoleon pamphlet, to a strong public outcry. In 1809, Andreas Hofer launched an insurrection in the Tyrol, but met the same fate as Palm. Anti-Napoleonic feeling developed little by little, with Germans feeling their spirits weighed down by the French occupation and Prussia still paying huge indemnities to the French. When Napoleon's 1812 invasion of Russia met with disaster, it lit a glimmer of hope in Germany and above all in Prussia. On 30 December 1812, Yorck von Wartenburg signed the convention of Tauroggen, by which Prussia in effect turned against Napoleon and repudiated the Treaty of Tilsit.\n\nOn 13 March 1813 Frederick William III made his 'An Mein Volk' speech, making an appeal:\nThe following 27 March Prussia declared war on France and the following 16–19 October saw the beginning of the end for Napoleonic power with the battle of Leipzig. On 1 October 1815 the Congress of Vienna opened and at it Harbenberg represented the victorious Kingdom of Prussia.\n\nIn late 19th century historiography, the Prussian reforms and the \"revolution from above\" were considered by Heinrich von Treitschke and others to be the first step in the foundation of the German Empire on the basis of a 'small-Germany' solution. For Friedrich Meinecke, the reforms put in place the conditions necessary for the future evolution of Prussia and Germany. For a long time, under the influence of Leopold von Ranke, the era of reforms was presented first and foremost as a story of the deeds and destinies of \"great men\", as shown by the large number of biographies written about the reformers – Hans Delbrück wrote about Gneisenau and Meinecke about Boyen, for example.\n\nIndeed, it was the military reforms which first gained the researchers' interest. It was only with the biography of Max Lehmann that Stein's life and actions were analysed. Unlike Stein, the biographers paid little attention to Hardenberg. Despite the significant differences between Stein and Hardenberg, historiography saw a fundamental continuity between their approaches that made them one single unified policy.\n\nSome authors, such as Otto Hintze, underlined the role of reform programmes such as the \"Allgemeines Landrecht\" of 1794. One such continuity confirmed the theory that the reformers were already a distinct group before the reforms occurred. Thomas Nipperdey resumed the debate by writing that there had been reform plans before the disaster of 1806, but that those behind them had lacked the energy to put them into force and also lacked internal cohesion. As for the agricultural reforms, the works of Georg Friedrich Knapp aroused a controversy at the end of the 19th century – he criticised the reform policy, stating that it favoured the aristocrats' interests and not the peasants' interests. He held Adam Smith's liberal interest responsible for the evolution of certain problems. Research later led to a global critique which could not be maintained. After all, the peasants' properties were developed, even if the lands they gained were most often revealed to be poor soil.\n\nToday, the industrial reforms' success is also critiqued in a more nuanced way. They are considered not to have been the immediate reason for the artisans' misery, instead taken as the reduced influence of the legislation on their development. The historian Barbara Vogel tried to address an overall concept of agricultural and industrial approaches and to describe them as a \"bureaucratic strategy of modernisation\". When industrial development was taken into account, the policy of reforms is seen to certainly be centred on the encouragement of rural industry in the historic Prussian territories, thus allowing the onset of Prussia's industrial revolution.\n\nReinhart Koselleck tried to give a general interpretation of the reform policy in view of the 1848 revolution, in his work \"Preußen zwischen Reform und Revolution\" (\"Prussia between Reform and Revolution\"). He distinguished three different processes. The \"Allgemeines Landrecht\" represented – at the time of its publication – a reaction to social problems, but remained attached to corporative elements. Koselleck saw the birth of an administrative state during the reform era and during the reinforcement of the administration between 1815 and 1825 as an anticipation of the later constitution. However, in his view, the following decades saw the political and social movement suppressed by the bureaucracy. After the end of the reform period, Koselleck underlined that there was a rupture in the equilibrium between the high level civil servants and the bourgeois of the 'Bildungsbürgertum' who could not become civil servants. According to him, the bureaucracy represented the general interest against the individual interest and no national representative body was set up for fear of seeing the reforming movement stopped.\nThe historian Hans Rosenberg and later the representatives of the Bielefeld School supported the theory that the end of the process which would have led to a constitution in Prussia was one of the reasons for the end to its democratisation and for the \"Sonderweg\". Hans-Jürgen Puhle, professor at Frankfurt University, even held the Prussian regime to be \"in the long term programmed for its own destruction\". Other writers more orientated towards historicism such as Thomas Nipperdey underlined the divergence between the reformers' intentions and the unexpected later results of the reforms.\n\nSeveral decades ago, the Prussian reforms from 1807 to 1819 lost their central position in historical study of 19th-century Germany. One contributing factor to this decline is that the reforms in the states of the Confederation of the Rhine were considered as similar by several historians. Another is that the Prussian regions – dynamic in industry and society – belonged to the French sphere of influence directly or indirectly until the end of the Napoleonic era.\n\nSeveral statues of the reformers were set up, especially of Stein. In 1870 a statue of Stein by Hermann Schievelbein was put up on the Dönhoffplatz in Berlin. Around its base can be read \"To minister Baron vom Stein. The recognition of the fatherland.\". A statue of Hardenberg by Martin Götze was also put up beside it in 1907. Stein's statue now stands in front of the Landtag of Prussia in Berlin.\n\nOne of the most important monuments to the reformers is that in the Heumarkt in Cologne, made up of an equestrian statue of Frederick William III by Gustav Blaeser on a base surrounded by statues of important figures of the era, including several Prussian reformers: Stein, Hardenberg, Gneisenau, Scharnhorst, Humboldt, Schön, Niebuhr and Beuth. The monument's design process had been launched in 1857 and it was inaugurated on 26 September 1878, with a medal marking the occasion bearing William I of Germany and his wife on the obverse and the monument and the phrase \"To king Frederick William III, the Rhine states recognise him\" on the reverse. The monument recalled the Berlin equestrian statue of Frederick the Great, designed by Christian Daniel Rauch, master of Blaeser.\n\nStein featured on commemorative stamps in 1957 and 2007 and Humboldt in 1952, whilst several streets are now named after the reformers, especially in Berlin, which has a Humboldtstraße, a Hardenbergstraße, a Freiherr-vom-Stein-Straße, a Niebuhrstraße, a Gneisenaustraße and a Scharnhorststraße.\n\n\n\n\n"}
{"id": "24449039", "url": "https://en.wikipedia.org/wiki?curid=24449039", "title": "Refinement type", "text": "Refinement type\n\nIn type theory, a refinement type is a type endowed with a predicate which is assumed to hold for any element of the refined type. Refinement types can express preconditions when used as function arguments or postconditions when used as return types: for instance, the type of a function which accepts natural numbers and returns natural numbers greater than 5 may be written as formula_1. Refinement types are thus related to behavioral subtyping.\n"}
{"id": "38025273", "url": "https://en.wikipedia.org/wiki?curid=38025273", "title": "Role-taking theory", "text": "Role-taking theory\n\nRole-taking theory, or social perspective taking, is the sociological theory that one of the most important factors in facilitating social cognition in children is the growing ability to understand others’ feelings and perspectives, an ability that emerges as a result of general cognitive growth. Part of this process requires that children come to realize that others’ views may differ from their own. Role-taking ability involves understanding the cognitive and affective (i.e. relating to moods, emotions, and attitudes) aspects of another person's point of view and differs from perceptual perspective taking, which is the ability to recognize another person's visual point of view of the environment. Furthermore, albeit some mixed evidence on the issue, role taking and perceptual perspective taking seem to be functionally and developmentally independent of each other.\n\nRobert Selman is noted for emphasizing the importance of this theory within the field of cognitive development. He argues that a matured role taking ability allows us to better appreciate how our actions will affect others, and if we fail to develop the ability to role take, we will be forced to erroneously judge that others are behaving solely as a result of external factors. One of Selman's principal additions to the theory has been an empirically supported developmental theory of role taking ability.\n\nSocial cognitive research on children's thoughts about others’ perspectives, feelings, and behaviors has emerged as one of the largest areas of research in the field. Role taking theory can provide a theoretical foundation upon which this research can rest and be guided by and has relations and applications to numerous other theories and topics.\n\nSelman developed his developmental theory of role taking ability based on four sources. The first is the work of Feffer and Feffer and Gourevitch, which related role taking ability to Piaget's theory of social decentering and developed a projective test to assess children's ability to decenter as they mature. The second is the research of Flavell, which studied children's growing abilities to judge other people's conceptual and perceptual perspectives. The third is the developmental ideas of differentiation, whereupon one learns to distinguish his/her perspective from the perspectives of others, and integration, the ability to relate one's perspective to the perspectives of others. The final source of influence comes from Selman's own previous research where he assessed children's ability to describe the different perspectives of characters in a story.\n\nOne example of Selman's stories is that of Holly and her father. Children are told about Holly, an avid 8-year-old tree climber. One day, Holly falls off a tree, but does not hurt herself. Holly's father sees this and makes Holly promise that she will stop climbing trees, and Holly promises. Later, however, Holly and her friends meet Shawn, a boy whose kitten is stuck in a tree. Holly is the only one amongst her friends who can climb trees well enough to save Shawn's kitten, who may fall at any moment, but she remembers the promise she made with her father. Selman then goes on to ask children about the perspectives of Holly and her father, and each stage is associated with typical responses.\n\nLevel 0: Egocentric Role Taking (ages 3–6, roughly)\n\nThis stage is characterized by two lacking abilities. The first is the failure to distinguish perspectives (differentiation). More specifically, the child is unable to distinguish between his perspective, including his perspective on why a social action occurred, and that of others. [1] The second ability the child lacks is relating perspectives (integration).\n\nIn the Holly dilemma, children tend to respond that Holly will save the kitten and that the father will not mind Holly's disobedience because he will be happy and he likes kittens. In actuality, the child is displaying his/her inability to separate his/her liking for kittens from the perspectives of Holly and her father.\n\nLevel 1: Subjective role taking (ages 6–8, roughly) \n\nChildren now recognize that they and others in a situation may have different information available to them, and thus may differ in their views. In other words, children have matured in differentiation. The child still significantly lacks integration ability, however: he/she cannot understand that his views are influenced by the views of others, and vice versa, ad infinitum. In addition, the child believes that the sole reason for differing social perspectives is because of different information, and nothing else.\n\nIn the Holly dilemma, when asked if the father would be angry if he found out that Holly climbed the tree again, children might respond, “If he didn’t know why she climbed the tree, he would be angry. But if he knew why she did it, he would realize that she had a good reason,” not recognizing that the father may still be angry, regardless of her wanting to save the kitten, because of his own values, such as his concern for his daughter's safety.\n\nLevel 2: Self-reflective role taking (ages 8–10, roughly) \n\nThe child's differentiation ability matures at this age enough so that he/she understands that people can also differ in their social perspectives because of their particularly held and differing values and set of purposes. In turn, the child is able to better put him/herself in the position of another person. In terms of integration, the child can now understand that others think about his/her point of view too. This allows the child to predict how the other person might react to the child's behaviour. What is still lacking, however, is for the child to be able to consider another person's point of view and another person's point of view of the child simultaneously.\n\nIn the Holly dilemma, when children are asked if Holly will climb the tree, they will typically respond, “Yes. She knows that her father will understand why she did it.” This indicates the child is considering the father's perspective in light of Holly's perspective; however, when asked if the father would want Holly to climb the tree, children typically respond that he would not. This shows that the child is solely considering the father's point of view and his worry for Holly's safety.\n\nLevel 3: Mutual role taking (ages 10–12, roughly) \n\nIn this stage, the child can now differentiate his/her own perspective from the viewpoint likely for the average member of the group. In addition, the child can take the view of a detached third-person and view a situation from that perspective. In terms of integration, the child can now simultaneously consider his/her view of others and others’ view of the child, and the consequences of this feedback loop of perspectives in terms of behaviour and cognition.\n\nIn describing the result of the Holly dilemma, the child may take the perceptive of a detached third party, responding that “Holly wanted to get the kitten because she likes kittens, but she knew that she wasn’t supposed to climb trees. Holly’s father knew that Holly had been told not to climb trees, but he couldn’t have known about [the kitten].” \n\nLevel 4: Societal role taking (ages 12–15+, roughly) \n\nThe adolescent now considers others’ perspectives with reference to the social environment and culture the other person comes from, assuming that the other person will believe and act in accord to their society's norms and values.\n\nWhen asked if Holly deserves to be punished for her transgression, adolescents typically respond that Holly should not as her father should understand that we need to humanely treat animals.\n\nThree studies have been conducted to assess Selman's theory, and all three have shown support for his developmental outline of role taking ability progression. Selman conducted the first study of his own theory using 60 middle-class children from ages 4 to 6. In this experiment, the children were asked to predict and explain their predictions about another child's behaviour in a certain situation. The child participants were given situational information not available to the child they were making behavioural and cognitive predictions about. Results implied a stage progression of role taking ability as a function of age, as theorized by Selman.\n\nIn a second assessment of the theory, Selman and Byrne interviewed 40 children, ages 4, 6, 8, and 10, on two socio-moral dilemmas. Children were required to discuss the perspectives of different characters in each dilemma, and results again showed that role taking ability progressed through stages as a function of age.\n\nThe third study assessing Selman's theory was a 5-year longitudinal study of 41 male children on their role taking ability. Results showed that 40 of the 41 children interviewed followed the stages as outlined by Selman and none skipped over a stage.\n\nJean Piaget stressed the importance of play in children, especially play that involves role taking. He believed that role taking play in children promotes a more mature social understanding by teaching children to take on the roles of others, allowing them to understand that different people can have differing perspectives. In addition, Piaget argued that good solutions to interpersonal conflicts involve compromise which arises out of our ability to consider the points of view of others.\nTwo of Piaget's fundamental concepts have primarily influenced role taking theory. The first is egocentrism, the mode of thinking that characterizes preoperational thinking, which is the child's failure to consider the world from other points of view. The second is decentration, the mode of thinking that characterizes operational thinking, which is the child's growing ability to perceive the world with more than one perspective in mind. In Piagetian theory, these concepts were used to describe solely cognitive development, but they have been applied in role taking theory to the social domain.\n\nEvidence that Piaget's cognitive theories can be applied to the interpersonal aspects of role taking theory comes from two sources. The first is empirical evidence that children's ability to role take is correlated to their IQ and performance on Piagetian tests. Secondly, the two theories have been conceptually linked in that Selman's role taking stages correspond to Piaget's cognitive development stages, where preoperational children are at level 1 or 2, concrete operators are at level 3 or 4, and formal operators are at level 4 or 5 of Selman's stages. Given this relation, Feffer and Feffer and Gourevitch have argued that social role taking is an extension of decentering in the social sphere. Selman has argued this same point, also noting that the growth of role taking ability is brought on by the child's decreased egocentrism as he/she ages.\n\nLawrence Kohlberg argued that higher moral development requires role taking ability. For instance, Kohlberg's conventional level of morality (between ages 9 and 13, roughly), involves moral stereotyping, empathy-based morality, alertness to and behaviour guided by predicted evaluations by others, and identifying with authority, all of which require role taking.\n\nSelman tested 60 children, ages 8 to 10, on Kohlberg's moral-judgment measure and two role taking tasks. He found that the development of role taking, within this age range, related to the progression into Kohlberg's conventional moral stage. A retest a year later confirmed Kohlberg's argument, and in general, it was shown that higher moral development at the conventional stage requires children's achieved ability at this age to reciprocally deal with their own and others’ perspectives.\nMason and Gibbs found that moral judgment development, as measured by Kohlberg's theory, consistently related to role taking opportunities experienced after childhood in adolescence and adulthood. This finding supported Kohlberg's view that moral progress beyond his third stage necessitates contact with other perspectives, namely those of entire cultures or political groups, which individuals are likely to encounter as they become adolescents and adults and thus meet many different people in school and the workplace.\n\nKohlberg and Piaget both emphasized that role taking ability facilitates moral development. Kohlberg argued that cognitive and role taking development are required but not sufficient for moral development. In turn, he maintained that Piaget's cognitive developmental stages underlie Selman's role taking stages, which are subsequently fundamental to his own moral developmental stages. This predicts that cognition develops first, followed by the corresponding role taking stage, and finally the corresponding moral stage, and never the other way around.\n\nConceptually, the three processes have been tied together by Walker. His reasoning is that cognitive development involves the progressive understanding of the environment as it is. Role taking is a step upon this, which is the recognition that people each have their own subjective interpretation of the environment, including how they think about and behave towards other people. Moral development, the final step, is the grasping of how people should think and behave towards one another.\n\nEvidence in support of this view comes first from three reviews which showed moderate correlations between Selman's role taking theory, Piaget's cognitive developmental stages, and Kohlberg's moral developmental stages. More evidence comes from Walker and Richards's finding that moral development to Kohlberg's stage 4 occurred only for those who already had early basic formal operations according to Piaget's developmental theory, and not for those in an earlier stage. Similarly, Paolitto's attempts to stimulate moral development worked only for subjects who already attained the corresponding role taking stage. Previous research has also shown that short role taking treatments, such as exposing subjects to the role taking reasoning of subjects in one stage higher, can lead to moral development. In more general demonstrations of this argument, Faust and Arbuthnot and other researchers have shown that moral development is most probable for subjects with higher cognitive development.\n\nIn a direct investigation of Kohlberg's necessary but not sufficient argument, Walker tested the hypothesis that only children who had attained both beginning formal operations and role taking stage 3 could progress to Kohlberg's moral stage 3. 146 grade 4-7 children participated in this study, and the results strongly supported the hypothesis, given that only children who had the beginning formal substage of cognitive development and role taking stage 3 progressed to moral stage 3. Further support came from the study's demonstration that a short role playing treatment stimulated progress in moral reasoning in a 6-week follow-up retest. Krebs and Gilmore also directly tested Kohlberg's necessary but not sufficient argument of moral development in 51 children, ages 5–14, for the first three stages of cognitive, role taking, and moral development. Results generally supported Kohlberg's view, but not as strongly, given that it was only demonstrated that cognitive development is a prerequisite for role taking development, and not for moral development. Based on these results, researchers have suggested that moral education programs underlain by Kohlberg's theory must first ensure that the prerequisite cognitive and role taking abilities have developed.\n\nRole taking ability and prosocial behaviours and feelings have been argued to be related. Evidence for this claim has been found from many sources. Underwood and Moore have found that perceptual, affective, and cognitive perspective taking are positively correlated with prosocial behaviour. Children trained to improve their role taking ability subsequently become more generous, more cooperative, and more apprehensive to the needs of others in comparison to children who received no role taking training. Research has also shown that people who are good at role taking have greater ability to sympathize with others. Overall, the picture is clear: prosocial behaviour is related to role taking ability development and social deviance is linked to egocentrism.\n\nTo study one reason for the link between role taking ability and prosociality, grade 2 children found to be either high or low in role taking were instructed to teach two kindergartners on an arts and crafts task. 16 measures of prosocial behaviour were scored, and high and low role takers diverged on 8 of the measures, including several helping measures, providing options, and social problem solving. Analysis of the results showed that low role takers helped less than high role takers not because of a lack of wanting to help, but because of their poorer ability in interpreting social cues indicating the need for help. In other words, low role takers tended to only be able to recognize problems when they were made plainly obvious.\n\nRole taking has also been related to empathy. Batson had participants listen to an interview of a woman going through hardship. He then instructed participants to imagine how she feels, or, to imagine how they would feel in her situation, and found that both conditions produced feelings of empathy. Schoenrade has found the same result, where imagining how a person in distress feels or how one would feel in that person's situation produces feelings of empathy.\n\nFinally, many theorists, including Mead, Piaget, Asch, Heider, Deutsch, Madsen, and Kohlberg have theorized a relationship between cooperation and role taking ability. In one study, children's predisposition to cooperate was shown to strongly correlate with their affective role taking ability. Other researchers have also shown an indirect relationship between cooperation and role taking capacity.\n\nA child's ability to function in social relationships has been found to depend partially on his/her role-taking ability. For instance, researchers found that children poor in role-taking ability had greater difficulty in forming and sustaining social relationships, as well as receiving lower peer nominations. Davis found that role taking ability was positively correlated with social understanding. In general, progress in role-taking ability has shown to be beneficial for one's personal and interpersonal life.\n\nBetter functioning in the interpersonal domain is particularly shown in the relation between role-taking ability and social problem solving ability. Role playing has been shown to improve male teenagers’ handling of social problem tasks. Gehlbach found a similar supporting result, demonstrating that adolescents with better role taking abilities had superior ability in conflict resolution. Many other researchers have also found that role taking ability development positively affects interpersonal problem solving skills. Additionally, role taking can promote better social functioning in the interpersonal domain through smoothening social interactions by improving behavioural mimicking ability.\n\nTraining children on role taking ability can improve interpersonal functioning as well. In one study, preschoolers were made to role play interpersonal conflicts using puppets. Their task at the end was to discuss alternative solutions to the problems and how each solution would affect each character. Over the 10 weeks that the preschoolers participated in this role playing, their solutions became less aggressive and their classroom adjustment became better. Moreover, the use of role reversal in interpersonal problem situations has been found to stimulate cooperation, help participants better understand each other and each other's arguments and position, elicit new interpretations of the situation, change attitudes about the problem, and improve perceptions about the other person's efforts at solving the issue, willingness to compromise and cooperate, and trustworthiness. As a result of this research, it has been suggested that one way to improve cooperative skills is to improve affective role taking abilities. \nRole taking can also work to decrease prejudice and stereotyping. Importantly, the decrease in prejudice and stereotyping occurs for both the target individual and the target's group. In addition, role taking ability has been demonstrated to decrease social aggression.\n\nChildren with ADHD struggle in their social environments, but the social-cognitive reasons for this are unknown. Several studies have indicated a difference between children with and without ADHD on their role taking ability, wherein children with ADHD have lower role taking ability, lower role taking use, and slower role taking development than children without ADHD. Given these results, it has been suggested that children with ADHD be trained on role taking to improve their social skills, including their often comorbid oppositional and conduct problems.\n\nThe relationship between childhood and adolescent delinquency and role taking is considerable. Burack found that maltreated children and adolescents with behavioural problems exhibited egocentrism at higher levels than non-maltreated children and adolescents who had progressed faster and more expectedly in their role taking development. Chandler found that chronically delinquent boys exhibited lower role taking abilities so much so that their role taking ability was comparable to the role taking ability scores of non-delinquent children nearly half their age. In turn, one-third of the delinquent boys in this study were assigned to a treatment program designed to improve role taking skills. Post-treatment measures demonstrated that the program successfully induced role taking ability progress in this group, and an 18-month follow-up assessment found a nearly 50% decrease in delinquent behaviours following these progresses in role taking skills. The same has been found for delinquent girls. Chalmers and Townsend trained delinquent girls, ages 10–16, on role taking skills over 15 sessions, following which the girls demonstrated improved understanding of interpersonal situations and problems, greater empathy, more acceptance of individual differences, and exhibited more prosociality in the classroom. The overall picture, then, is that role taking training can help delinquent youth and youth with conduct disorders as they lag behind in role taking ability.\n\nSeveral researchers have argued that the deficits in the social lives, communication ability, and imagination of autistic children are a result of their deficiencies in role taking. It is believed that autistic children's inability to role take prevents them from developing a theory of mind. Indeed, role taking has been described as the theory of mind in action. Failing to role take and failing to develop a theory of mind may lead autistic children to use only their own understanding of a situation to predict others’ behaviour, resulting in deficits in social understanding.\n\nIn support, two studies found shortcomings in role-taking ability in autistic children in comparison to controls. Another study found that lower ability in role taking related significantly with the lower social competency in autistic children. In particular, the autistic children in the study could not focus concurrently on different cognitions required for successful role taking and proficient social interaction. More specifically, Dawson and Fernald found that conceptual role-taking related most to the social deficits and severity of autism experienced by autistic children, while affective role taking was related only to the severity of autism.\n\nThe main criticism of Selman's role-taking theory is that it focuses too much on the effect of cognitive development on role-taking ability and social cognition, thereby overlooking the non-cognitive factors that affect children's abilities in these domains. For instance, social experiences, such as disagreements between close friends, have been found to foster role taking skills and social cognitive growth. In addition, parental influence amongst sibling conflicts matters, as mothers who act as mediators to help solve sibling disagreements have been found to promote role taking skills and social cognitive maturation.\n"}
{"id": "589516", "url": "https://en.wikipedia.org/wiki?curid=589516", "title": "Sexual maturity", "text": "Sexual maturity\n\nSexual maturity is the capability of an organism to reproduce. It may be considered synonymous with adulthood, but, in humans, puberty encompasses the process of sexual maturation and adulthood is based on cultural definitions. \n\nMost multicellular organisms are unable to sexually reproduce at birth (or germination), and depending on the species, it may be days, weeks, or years until their bodies are able to do so. Also, certain cues may cause the organism to become sexually mature. They may be external, such as drought, or internal, such as percentage of body fat (such internal cues are not to be confused with hormones which directly produce sexual maturity).\n\nSexual maturity is brought about by a maturing of the reproductive organs and the production of gametes. It may also be accompanied by a growth spurt or other physical changes which distinguish the immature organism from its adult form. These are termed secondary sex characteristics, and often represent an increase in sexual dimorphism. For example, before puberty, human children have flat chests, but adult females have generally larger breasts than adult males. However, there are exceptions such as obesity and hormone imbalances such as gynecomastia.\n\nAfter sexual maturity is achieved, it is possible for some organisms to become infertile, or even to change their sex. Some organisms are hermaphrodites and may or may not be able to produce viable offspring. Also, while in many organisms sexual maturity is strongly linked to age, many other factors are involved, and it is possible for some to display most or all of the characteristics of the adult form without being sexually mature. Conversely it is also possible for the \"immature\" form of an organism to reproduce. This is called \"progenesis\", in which sexual development occurs faster than other physiological development (in contrast, the term \"neoteny\" refers to when non-sexual development is slowed - but the result is the same, the retention of juvenile characteristics into adulthood).\n"}
{"id": "27271259", "url": "https://en.wikipedia.org/wiki?curid=27271259", "title": "Space mirror (climate engineering)", "text": "Space mirror (climate engineering)\n\nThe use of space mirrors as an anti-global warming measure is a proposed technology for climate change mitigation by deflection of sunlight. It was one of a series of proposals for controlling global warming made to the United States government in 2001.\n\nAt the \"Response Options to Rapid or Severe Climate Change\" round-table meeting organized by the President's Climate Change Technology Program in September 2001 to gather ideas for averting climate change, one of the proposals was to station one or more wire-mesh \"mirrors\" in orbit to deflect sunlight back into space or to filter it. The idea was proposed by Lowell Wood, a senior staff scientist at Lawrence Livermore National Laboratory, who calculated that deflecting 1% of sunlight would restore climatic stability, and that that would require either a single mirror in area or several smaller ones. Wood had been researching the idea for more than ten years but considered it so infeasible that it should only be a back-up plan for solving the global warming problem. \n\nIn January 2007, \"The Guardian\" reported that the US government was recommending that research on sunlight deflection, including space mirrors, be continued as \"insurance\" and that the next United Nations Report on Climate Change advocate such a strategy. In addition to the space mirror, suggested sunlight-reducing techniques included launching thousands of highly reflective balloons and pumping sulphate droplets into the upper atmosphere to emulate volcanic emissions.\n\nSpace mirrors were first considered in the 1980s as a way to cool the climate of Venus. James Early, also at Livermore, in 1989 proposed using a \"space shade\" 2,000 kilometers in diameter orbiting at Lagrangian Point L1. He estimated the cost at between one and ten trillion US dollars and suggested manufacturing it on the moon using moon rock. \n\nUsing space mirrors as a space sunshade to reduce the impact of sunlight falls into the category of geoengineering: deliberately modifying the earth's climate. At a conference on the topic organized by Daniel Schrag of Harvard University and David Keith of the University of Calgary in November 2007, the consensus was that it was worth studying such ideas further despite their high cost, the doubtful feasibility of some including the space mirror, and the risk of their distracting attention from reduction of greenhouse gas emissions.\n"}
{"id": "1335297", "url": "https://en.wikipedia.org/wiki?curid=1335297", "title": "Spaceship Earth", "text": "Spaceship Earth\n\nSpaceship Earth or Spacecraft Earth is a world view encouraging everyone on Earth to act as a harmonious crew working toward the greater good.\n\nThe earliest known use is a passage in Henry George's best known work, \"Progress and Poverty\" (1879).\nFrom book IV, chapter 2: \nIt is a well-provisioned ship, this on which we sail through space. If the bread and beef above decks seem to grow scarce, we but open a hatch and there is a new supply, of which before we never dreamed. And very great command over the services of others comes to those who as the hatches are opened are permitted to say, \"This is mine!\"\n\nGeorge Orwell later paraphrases Henry George in \"The Road to Wigan Pier\":\n\nThe world is a raft sailing through space with, potentially, plenty of provisions for everybody; the idea that we must all cooperate and see to it that everyone does his fair share of the work and gets his fair share of the provisions seems so blatantly obvious that one would say that no one could possibly fail to accept it unless he had some corrupt motive for clinging to the present system.\n\nIn 1965 Adlai Stevenson made a famous speech to the UN in which he said:\n\nWe travel together, passengers on a little space ship, dependent on its vulnerable reserves of air and soil; all committed for our safety to its security and peace; preserved from annihilation only by the care, the work, and, I will say, the love we give our fragile craft. We cannot maintain it half fortunate, half miserable, half confident, half despairing, half slave—to the ancient enemies of man—half free in a liberation of resources undreamed of until this day. No craft, no crew can travel safely with such vast contradictions. On their resolution depends the survival of us all.\n\nThe following year, \"Spaceship Earth\" became the title of a book by a friend of Stevenson's, the internationally influential economist Barbara Ward.\n\nAlso in 1966, Kenneth E. Boulding, who was influenced by reading Henry George, used the phrase in the title of an essay, \"The Economics of the Coming Spaceship Earth\". Boulding described the past open economy of apparently illimitable resources, which he said he was tempted to call the \"cowboy economy\", and continued: \"The closed economy of the future might similarly be called the 'spaceman' economy, in which the earth has become a single spaceship, without unlimited reservoirs of anything, either for extraction or for pollution, and in which, therefore, man must find his place in a cyclical ecological system\".\n\nThe phrase was also popularized by Buckminster Fuller, who published a book in 1968 under the title of \"Operating Manual for Spaceship Earth\". This quotation, referring to fossil fuels, reflects his approach: \n…we can make all of humanity successful through science's world-engulfing industrial evolution provided that we are not so foolish as to continue to exhaust in a split second of astronomical history the orderly energy savings of billions of years' energy conservation aboard our Spaceship Earth. These energy savings have been put into our Spaceship's life-regeneration-guaranteeing bank account for use only in self-starter functions.\n\nUnited Nations Secretary-General U Thant spoke of Spaceship Earth on Earth Day March 21, 1971 at the ceremony of the ringing of the Japanese Peace Bell: \"May there only be peaceful and cheerful Earth Days to come for our beautiful Spaceship Earth as it continues to spin and circle in frigid space with its warm and fragile cargo of animate life.\"\n\nSpaceship Earth is the name given to the 50 m diameter geodesic sphere that greets visitors at the entrance of Walt Disney World's Epcot theme park. Housed within the sphere is a dark ride that serves to explore the history of communications and promote Epcot's founding principles, \"[a] belief and pride in man's ability to shape a world that offers hope to people everywhere.\" A previous incarnation of the ride, narrated by actor Jeremy Irons and revised in 2008, was explicit in its message:\n\nLike a grand and miraculous spaceship, our planet has sailed through the universe of time, and for a brief moment, we have been among its many passengers….We now have the ability and the responsibility to build new bridges of acceptance and co-operation between us, to create a better world for ourselves and our children as we continue our amazing journey aboard Spaceship Earth.\n\nDavid Deutsch has pointed out that the picture of Earth as a friendly \"spaceship\" habitat is difficult to defend even in metaphorical sense. The Earth environment is harsh and survival is constant struggle for life, including whole species extinction. Humans wouldn't be able to live in most of the areas where they are living now without knowledge necessary to build life-support systems such as houses, heating, water supply, etc.\n\nThe term \"Spaceship Earth\" is frequently used on the labels of Emanuel Bronner's products to refer to the Earth.\n\n"}
{"id": "9261359", "url": "https://en.wikipedia.org/wiki?curid=9261359", "title": "Species problem", "text": "Species problem\n\nThe species problem is the set of questions that arises when biologists attempt to define what a species is. Such a definition is called a species concept; there are at least 26 recognized species concepts. A species concept that works well for sexually reproducing organisms such as birds is useless for species that reproduce asexually, such as bacteria. The scientific study of the species problem has been called microtaxonomy.\n\nOne common, but sometimes difficult, question is how best to decide which species an organism belongs to, because reproductively isolated groups may not be readily recognizable, and cryptic species may be present. There is a continuum from \"reproductive isolation\" with no interbreeding, to \"panmixis\", unlimited interbreeding. Populations can move forward or backwards along this continuum, at any point meeting the criteria for one or another species concept, and failing others.\n\nMany of the debates on species touch on philosophical issues, such as nominalism and realism, and on issues of language and cognition.\n\nThe current meaning of the phrase \"species problem\" is quite different from what Charles Darwin and others meant by it during the 19th and early 20th centuries. For Darwin, the species problem was the question of how new species arose. Darwin was however one of the first people to question how well-defined species are, given that they constantly change.\n\nThe idea that one organism reproduces by giving birth to a similar organism, or producing seeds that grow to a similar organism, goes back to the earliest days of farming. While people tended to think of this as a relatively stable process, many thought that change was possible. The term \"species\" was just used as a term for a sort or kind of organism, until in 1686 John Ray introduced the biological concept that species were distinguished by always producing the same species, and this was fixed and permanent, though considerable variation was possible within a species. Carolus Linnaeus (1707–1778) formalized the taxonomic rank of species, and devised the two part naming system of binomial nomenclature that we use today. However, this did not prevent disagreements on the best way to identify species.\n\nThe history of definitions of the term \"species\" reveal that the seeds of the modern species debate were alive and growing long before Darwin.\n\n\"The traditional view, which was developed by Cain, Mayr and Hull in the mid-twentieth century, claims that until the ‘Origin of species’ by Charles Darwin both philosophy and biology considered species as invariable natural kinds with essential features. This ‘essentialism story’ was adopted by many authors, but questioned from the beginning by a minority … when Aristotle and the early naturalists wrote about the essences of species, they meant essential ‘functions’, not essential ‘properties’. Richards pointed out [Richard A. Richards, The Species Problem: A Philosophical Analysis, Cambridge University Press, 2010] that Linnaeus saw species as eternally fixed in his very first publication from 1735, but only a few years later he discovered hybridization as a modus for speciation.\n\nCharles Darwin's famous book \"On the Origin of Species\" (1859) offered an explanation as to how species evolve, given enough time. Although Darwin did not provide details on how species can split into two, he viewed speciation as a gradual process. If Darwin was correct, then, when new \"incipient species\" are forming, there must be a period of time when they are not yet distinct enough to be recognized as species. Darwin's theory suggested that there was often not going to be an objective fact of the matter, on whether there were one or two species.\n\nDarwin's book triggered a crisis of uncertainty for some biologists over the objectivity of species, and some came to wonder whether individual species could be objectively real — i.e. have an existence that is independent of the human observer.\n\nIn the 1920s and 1930s, Mendel's theory of inheritance and Darwin's theory of evolution by natural selection were joined in what was called the modern synthesis. This conjunction of theories also had a large impact on how biologists think about species. Edward Poulton anticipated many ideas on species that today are well accepted, and that were later more fully developed by Theodosius Dobzhansky and Ernst Mayr, two of the architects of the modern synthesis. Dobzhansky's 1937 book articulated the genetic processes that occur when incipient species are beginning to diverge. In particular, Dobzhansky described the critical role, for the formation of new species, of the evolution of reproductive isolation.\n\nErnst Mayr's 1942 book was a turning point for the species problem. In it, he wrote about how different investigators approach species identification, and he characterized their approaches as species concepts. He argued for what came to be called the \"Biological Species Concept\" (BSC), that a species consists of populations of organisms that can reproduce with one another and that are reproductively isolated from other populations, though he was not the first to define \"species\" on the basis of reproductive compatibility. For example, Mayr discusses how Buffon proposed this kind of definition of \"species\" in 1753.\nTheodosius Dobzhansky was a contemporary of Mayr and the author of a classic book about the evolutionary origins of reproductive barriers between species, published a few years before Mayr's. Many biologists credit Dobzhansky and Mayr jointly for emphasizing reproductive isolation.\n\nAfter Mayr's book, some two dozen species concepts were introduced. Some, such as the Phylogenetic Species Concept (PSC), were designed to be more useful than the BSC for describing species. Many authors have professed to \"solve\" or \"dissolve\" the species problem. Some have argued that the species problem is too multidimensional to be \"solved\" by any one concept. Since the 1990s, others have argued that concepts intended to help describe species have not helped to resolve the species problem. Although Mayr promoted the BSC for use in systematics, some systematists have criticized it as not operational. For others, the BSC is the preferred definition of species. Many geneticists who work on speciation prefer the BSC because it emphasizes the role of reproductive isolation. It has been argued that the BSC is a natural consequence of the effect of sexual reproduction on the dynamics of natural selection.\n\nRealism, in the context of the species problem, is the philosophical position that species are real mind-independent entities, natural kinds. Mayr, a proponent of realism, attempted to demonstrate species exist as natural, extra-mental categories. He showed for example that the New Guinean tribesman classify 136 species of birds, which Western ornithologists came to independently recognize:\n\nMayr's argument however has been criticized:\n\nAnother position of realism is that natural kinds are demarcated by the world itself by having a unique property that is shared by all the members of a species, and none outside the group. In other words, a natural kind possesses an essential or intrinsic feature (“essence”) that is self-individuating and non-arbitrary. This notion has been heavily criticized as essentialist, but modern realists have argued that while biological natural kinds have essences, these need not be fixed and are prone to change through speciation. According to Mayr (1957) reproductive isolation or interbreeding \"supplies an objective yardstick, a completely non-arbitrary criterion” and \"describing a presence or absence relationship makes this species concept non-arbitrary\". The BSC defines species as \"groups of actually or potentially interbreeding natural populations, which are reproductively isolated from other such groups\". From this perspective, each species is based on a property (reproductive isolation) that is shared by all the organisms in the species that objectively distinguishes them.\n\nSome philosophical variants of nominalism propose that species are just names that people have assigned to groups of creatures but where the lines between species get drawn does not reflect any fundamental underlying biological cut-off point. In this view, the kinds of things that people have given names to, do not reflect any underlying reality. It then follows that species do not exist outside the mind, because species are just named abstractions. If species are not real, then it would not be sensible to talk about \"the origin of a species\" or the \"evolution of a species\". As recently at least as the 1950s, some authors adopted this view and wrote of species as not being real.\n\nA counterpoint to the nominalist views in regard to species, was raised by Michael Ghiselin who argued that an individual species is not a type, but rather an actual individual, an actual entity. This idea comes from thinking of a species as an evolving dynamic population. If viewed as an entity, a species would exist regardless of whether or not people have observed it and whether or not it has been given a name.\n\nA popular alternative view, pragmatism, espoused by philosophers such as Philip Kitcher and John Dupre states while species do not exist in the sense of natural kinds, they are \"conceptually\" real and exist for convenience and for practical applications. For example, regardless of which definition of species one uses, one can still quantitatively compare species diversity across regions or decades, as long as the definition is held constant within a study. This has practical importance in advancing biodiversity science and environmental science.\n\nThe nominalist critique of the view that kinds of things exist, raises for consideration the role that humans play in the species problem. For example, Haldane suggested that species are just mental abstractions.\n\nSeveral authors have noted the similarity between \"species\", as a word of ambiguous meaning, and points made by Wittgenstein on family resemblance concepts and the indeterminacy of language.\n\nJody Hey described the species problem as a result of two conflicting motivations by biologists:\n\nUnder the first view, species appear to us as typical natural kinds, but when biologists turn to understand species evolutionarily they are revealed as changeable and without sharp boundaries. Hey argued that it is unrealistic to expect that one definition of \"species\" is going to serve the need for categorization and still reflect the changeable realities of evolving species.\n\nMany approaches to the species problem have attempted to develop one single common conception of what species are and of how they should be identified. It is thought that, if such a monistic description of species could be developed and agreed upon, then the species problem would be solved. In contrast, some authors have argued for pluralism, claiming that biologists cannot have just one shared concept of species, and that they should accept multiple, seemingly incompatible ideas about species. David Hull however argued that pluralist proposals were unlikely to actually solve the species problem.\n\n\"No term is more difficult to define than \"species,\" and on no point are zoologists more divided than as to what should be understood by this word.\" Nicholson (1872).\n\n\"Of late, the futility of attempts to find a universally valid criterion for distinguishing species has come to be fairly generally, if reluctantly, recognized\" Dobzhansky (1937).\n\n\"The concept of a species is a concession to our linguistic habits and neurological mechanisms\" Haldane (1956).\n\n\"The species problem is the long-standing failure of biologists to agree on how we should identify species and how we should define the word 'species'.\" Hey (2001).\n\n\"First, the species problem is not primarily an empirical one, but it is rather fraught with philosophical questions that require — but cannot be settled by — empirical evidence.\" Pigliucci (2003).\n\n\"An important aspect of any species definition whether in neontology or palaeontology is that any statement that particular individuals (or fragmentary specimens) belong to a certain species is an hypothesis (not a fact)\" Bonde (1977).\n\n\"We show that although discrete phenotypic clusters exist in most <nowiki>[plant]</nowiki> genera (> 80%), the correspondence of taxonomic species to these clusters is poor (< 60%) and no different between plants and animals. ... Contrary to conventional wisdom, plant species are more likely than animal species to represent reproductively independent lineages.\" Rieseberg et al. (2006).\n\n\n"}
{"id": "15378826", "url": "https://en.wikipedia.org/wiki?curid=15378826", "title": "Sāvakabuddha", "text": "Sāvakabuddha\n\nSāvakabuddha is a Pali term used rarely in Theravada Buddhism to refer to an enlightened disciple of a Buddha. Such enlightened disciples obtained by hearing the dhamma as initially taught by a sammasambuddha. A sāvakabuddha is distinguished from a sammasambuddha and a paccekabuddha. The standard designation for such a person is \"arhat\".\n\nBuddhas are supposed to reach by their own efforts and insights. A sāvakabuddha might also lead others to enlightenment, but cannot teach the dhamma in a time or world where it has been forgotten, because they depend upon a tradition that stretches back to a sammasambuddha.\n\nThe term savakabuddha is used in Theravadin commentaries but does not occur in the scriptures of the Pāli Canon.\n\nSāvaka means \"one who hears\" – a person who follows the path to enlightenment by means of hearing the instructions of others. Lay persons, who take special vows, are called sāvakas.\n"}
{"id": "957529", "url": "https://en.wikipedia.org/wiki?curid=957529", "title": "The Ancestor's Tale", "text": "The Ancestor's Tale\n\nThe Ancestor's Tale: A Pilgrimage to the Dawn of Life is a 2004 popular science book by Richard Dawkins, with contributions from Dawkins' research assistant Yan Wong. It follows the path of humans backwards through evolutionary history, meeting humanity's cousins as they converge on common ancestors. Dawkins' longest book to date, it was nominated for the 2005 Aventis Prize for Science Books.\n\nRichard Dawkins starts the tale by talking about history. He claims that evolution rhymes and patterns recur. He talks about our universe that has its own remarkable set of laws and constants which are capable of generating us and other organisms living on this planet. Not only it is capable of generating organisms, it is capable of evolving them too. \nHe claims that biological evolution has no privilege line of descent and no designated end. Evolution has arrived at many millions of interim ends and organisms are still evolving. He believes that evolution is directional, progressive and even predictable.\nHe also talks about how homo sapiens tend to think that they are more evolved than other, but that's not true, all the other species have gone through evolution too. They just have inherited different traits that helped them survive through natural selection. Dawkins claims that all species are equal.\nHe uses backward chronology, instead of forward chronology because in backward chronology, no matter where you start, you end up celebrating the unity of life. While going forward just extols diversity. In a backward chronology, the ancestors of any set of species must eventually meet at a particular geological moment. The last common ancestor is the one that they all share which he calls \"Concestor\". The oldest concestor is the grand ancestor of all surviving life forms on this planet.\nThere is a single concestor of all surviving life forms and its evidence is that all that have ever been examined share the same genetic code and the genetic code is too complex to have been invented twice. There is no sign of other independent origins of life and if new ones arise, they would probably be eaten by bacteria. \nThis book is a pilgrimage to discover human ancestors and as it progresses, it meets other pilgrims (organisms) who join humans in order as the book reaches the common ancestor that human share with them. Human only passes 40 rendezvous before hitting the origin of life itself. In each rendezvous, we find one particular ancestor, the concestor which has the same labeling number as the rendezvous. All the creatures in this tale are alive except for 2 classes. The two exceptions are Dodo and Handyman. \nDawkin’s book’s structure is inspired by Geoffrey Chaucer's The Canterbury Tales.\nAt each rendezvous point, Dawkins recounts tales concerning the cousin animals which are about to join the band of pilgrims. Every newly recruited species, genus or family has its own peculiar features, often ones that are relevant to human anatomy or otherwise interesting for humans. For instance, Dawkins discusses why the axolotl never needs to grow up, how new species come about, how hard it is to classify animals, and why our fish-like ancestors moved to the land. These peculiar features are studied and analysed using a newly introduced tool or method from evolutionary biology, carefully woven into a tale to illustrate how the Darwinian theory of evolution explains all diversity in nature.\n\nEven though the book is best read sequentially, every chapter can also be read independently as a self-contained tale with an emphasis on a particular aspect of modern biology. As a whole, the book elaborates on all major topics in evolution.\n\nDawkins also tells personal stories about his childhood and time at university. He talks with fondness about a tiny bushbaby he kept as a child in Malawi (Nyasaland). He described his surprise when he learned that the closest living relatives to the hippos are the whales.\n\nThe book was produced in two hardback versions: a British one with extensive colour illustrations (by Weidenfeld & Nicolson), and an American one with a reduced number of black-and-white illustrations (by Houghton Mifflin). Paperback versions and an abridged audio version (narrated by Dawkins and his wife Lalla Ward) have also been published.\n\nThe book is dedicated to Dawkins' friend and mentor, population geneticist John Maynard Smith, who died shortly before the book went to press.\n\nDawkins uses the term \"concestor\"—coined by Nicky Warren—for the most recent common ancestor at each rendezvous point. At each rendezvous point, we meet the concestor of ourselves and the listed species or collection of species. This does not mean that the concestor was much like those creatures; after the \"rendezvous\", our fellow \"pilgrims\" have had as much time to evolve and change as we have. Only creatures alive at the time of the book's writing join us at each rendezvous point. Except for a few special cases, numerous extinct species and families such as the non-avian dinosaurs are excluded from the pilgrimage.\n\nNote: From the lancelets onward, Dawkins only provides dates under duress stating that, \"dating becomes so difficult and controversial that my courage fails me\".\n\nIn what Dawkins calls the \"Great Historic Rendezvous\", he describes the significantly important event of endosymbiosis, which results in the beginnings of eukaryotic cells. In his estimates, this occurred in two or three steps, roughly two billion years ago. Firstly, bacteria, perhaps related to \"Rickettsia\", entered proto-protozoan cells. For one reason or another, the bacteria were not digested and did not kill the cell. The cell offered protection to the bacteria, and the bacteria provided energy to the cell, resulting in a mutualistic symbiotic relationship. This is the speculated origin of mitochondria. Subsequently, photosynthetic bacteria (thought to be related to cyanobacteria) entered some, but not all, of these mitochondria-containing cells. These ancient bacteria evolved to become chloroplasts, and the cells became the Plant and Algal lineages. Meanwhile, the cells which this second endosymbiotic relationship did not occur in went on to form the Kingdoms Fungi and Animalia, as well as various Protozoa.\n\nChloroplasts and mitochondria have their own genomes, and they replicate independent of the cell in which they live. Dawkins acknowledges how the endosymbiotic theory proposed by Lynn Margulis is now virtually universally accepted.\n\n\"The Ancestor's Tale\" was popular when first published. Carl Zimmer of New York Times stated that the book is one of the best to understand Evolutionary trees.\n\n\n\n\n"}
{"id": "40602553", "url": "https://en.wikipedia.org/wiki?curid=40602553", "title": "The Morals of Chess", "text": "The Morals of Chess\n\nThe Morals of Chess is an essay on chess by the American intellectual Benjamin Franklin, which was first published in \"The Columbian Magazine\" in December 1786. \n\nFranklin, who was one of the Founding Fathers of the United States, played chess from at least 1733. Evidence suggests that he was an above-average player, who, however, did not reach the top level. He outlined the essay around 1732, but did not publish it until 1786. After a short prologue in which Franklin details the history of chess he gets to the main part of his essay. He compares chess to life and writes that foresight, circumspection and caution can be learnt from the game. After describing the effects chess can have on one's perception of life he describes a set of moral rules that a chess player should hold, including to not cheat and not disturb the opponent. Franklin suggests that the opponent be told about mistakes he makes, for example if he would lose a piece.\n\nThe essay is one of the first texts about chess that was published in the United States; it appeared in the first chess-related book that was published in Russia in 1791. It still is widely reproduced, especially on the Internet. In 1999 Franklin was inducted into the U.S. Chess Hall of Fame.\n"}
{"id": "18871460", "url": "https://en.wikipedia.org/wiki?curid=18871460", "title": "Van Kampen diagram", "text": "Van Kampen diagram\n\nIn the mathematical area of geometric group theory, a van Kampen diagram (sometimes also called a Lyndon–van Kampen diagram ) is a planar diagram used to represent the fact that a particular word in the generators of a group given by a group presentation represents the identity element in that group.\n\nThe notion of a van Kampen diagram was introduced by Egbert van Kampen in 1933. This paper appeared in the same issue of American Journal of Mathematics as another paper of van Kampen, where he proved what is now known as the Seifert–van Kampen theorem. The main result of the paper on van Kampen diagrams, now known as the \"van Kampen lemma\" can be deduced from the Seifert–van Kampen theorem by applying the latter to the presentation complex of a group. However, van Kampen did not notice it at the time and this fact was only made explicit much later (see, e.g.). Van Kampen diagrams remained an underutilized tool in group theory for about thirty years, until the advent of the small cancellation theory in the 1960s, where van Kampen diagrams play a central role. Currently van Kampen diagrams are a standard tool in geometric group theory. They are used, in particular, for the study of isoperimetric functions in groups, and their various generalizations such as isodiametric functions, filling length functions, and so on.\n\nThe definitions and notations below largely follow Lyndon and Schupp.\n\nLet \nbe a group presentation where all \"r\"∈\"R\" are cyclically reduced words in the free group \"F\"(\"A\"). The alphabet \"A\" and the set of defining relations \"R\" are often assumed to be finite, which corresponds to a finite group presentation, but this assumption is not necessary for the general definition of a van Kampen diagram. Let \"R\" be the \"symmetrized closure\" of \"R\", that is, let \"R\" be obtained from \"R\" by adding all cyclic permutations of elements of \"R\" and of their inverses.\n\nA van Kampen diagram over the presentation (†) is a planar finite cell complex formula_2, given with a specific embedding formula_3 with the following additional data and satisfying the following additional properties:\n\n\nThus the 1-skeleton of formula_2 is a finite connected planar graph \"Γ\" embedded in formula_9 and the two-cells of formula_2 are precisely the bounded complementary regions for this graph.\n\nBy the choice of \"R\" Condition 4 is equivalent to requiring that for each region of formula_2 there is some boundary vertex of that region and some choice of direction (clockwise or counter-clockwise) such that the boundary label of the region read from that vertex and in that direction is freely reduced and belongs to \"R\".\n\nA van Kampen diagram formula_2 also has the \"boundary cycle\", denoted formula_13, which is an edge-path in the graph \"Γ\" corresponding to going around formula_2 once in the clockwise direction along the boundary of the unbounded complementary region of \"Γ\", starting and ending at the base-vertex of formula_2. The label of that boundary cycle is a word \"w\" in the alphabet \"A\" ∪ \"A\" (which is not necessarily freely reduced) that is called the \"boundary label\" of formula_2.\n\n\nIn general, a van Kampen diagram has a \"cactus-like\" structure where one or more disk-components joined by (possibly degenerate) arcs, see the figure below:\n\nThe following figure shows an example of a van Kampen diagram for the free abelian group of rank two\n\nThe boundary label of this diagram is the word\nThe area of this diagram is equal to 8.\n\nA key basic result in the theory is the so-called \"van Kampen lemma\" which states the following:\n\n\nFirst observe that for an element \"w\" ∈ \"F\"(\"A\") we have \"w\" = 1 in \"G\" if and only if \"w\" belongs to the normal closure of \"R\" in \"F\"(\"A\") that is, if and only if w can be represented as\n\nwhere \"n\" ≥ 0 and where \"s\" ∈ \"R\" for \"i\" = 1, ..., \"n\".\n\nPart 1 of van Kampen's lemma is proved by induction on the area of formula_2. The inductive step consists in \"peeling\" off one of the boundary regions of formula_2 to get a van Kampen diagram formula_36 with boundary cycle \"w\"' and observing that in \"F\"(\"A\") we have \nwhere \"s\"∈\"R\" is the boundary cycle of the region that was removed to get formula_36 from formula_2.\n\nThe proof of part 2 of van Kampen's lemma is more involved. First, it is easy to see that if \"w\" is freely reduced and \"w\" = 1 in \"G\" there exists some van Kampen diagram formula_40 with boundary label \"w\" such that \"w\" = \"w\" in \"F\"(\"A\") (after possibly freely reducing \"w\"). Namely consider a representation of \"w\" of the form (♠) above. Then make formula_40 to be a wedge of \"n\" \"lollipops\" with \"stems\" labeled by \"u\" and with the \"candys\" (2-cells) labelled by \"s\". Then the boundary label of formula_40 is a word \"w\" such that \"w\" = \"w\" in \"F\"(\"A\"). However, it is possible that the word \"w\" is not freely reduced. One then starts performing \"folding\" moves to get a sequence of van Kampen diagrams formula_43 by making their boundary labels more and more freely reduced and making sure that at each step the boundary label of each diagram in the sequence is equal to \"w\" in \"F\"(\"A\"). The sequence terminates in a finite number of steps with a van Kampen diagram formula_44 whose boundary label is freely reduced and thus equal to \"w\" as a word. The diagram formula_44 may not be reduced. If that happens, we can remove the reduction pairs from this diagram by a simple surgery operation without affecting the boundary label. Eventually this produces a reduced van Kampen diagram formula_2 whose boundary cycle is freely reduced and equal to \"w\".\n\nMoreover, the above proof shows that the conclusion of van Kampen's lemma can be strengthened as follows. Part 1 can be strengthened to say that if formula_2 is a van Kampen diagram of area \"n\" with boundary label \"w\" then there exists a representation (♠) for \"w\" as a product in \"F\"(\"A\") of exactly \"n\" conjugates of elements of \"R\". Part 2 can be strengthened to say that if \"w\" is freely reduced and admits a representation (♠) as a product in \"F\"(\"A\") of \"n\" conjugates of elements of \"R\" then there exists a reduced van Kampen diagram with boundary label \"w\" and of area \"at most\" \"n\".\n\nLet \"w\" ∈ \"F\"(\"A\") be such that \"w\" = 1 in \"G\". Then the \"area\" of \"w\", denoted Area(\"w\"), is defined as the minimum of the areas of all van Kampen diagrams with boundary labels \"w\" (van Kampen's lemma says that at least one such diagram exists).\n\nOne can show that the area of \"w\" can be equivalently defined as the smallest \"n\"≥0 such that there exists a representation (♠) expressing \"w\" as a product in \"F\"(\"A\") of \"n\" conjugates of the defining relators.\n\nA nonnegative monotone nondecreasing function \"f\"(\"n\") is said to be an \"isoperimetric function\" for presentation (†) if for every freely reduced word \"w\" such that \"w\" = 1 in \"G\" we have\n\nwhere |\"w\"| is the length of the word \"w\".\n\nSuppose now that the alphabet \"A\" in (†) is finite.\nThen the \"Dehn function\" of (†) is defined as\n"}
{"id": "55442629", "url": "https://en.wikipedia.org/wiki?curid=55442629", "title": "Vicarious embarrassment", "text": "Vicarious embarrassment\n\nVicarious embarrassment (also known as secondhand, empathetic, or third party embarrassment) is the feeling of embarrassment from observing the embarrassing actions of another person. Unlike general embarrassment, vicarious embarrassment is not caused by participating in an embarrassing event, but instead it's caused by witnessing (either verbally or visually) another person's experience an embarrassing event. These emotions can be perceived as pro-social, and some say they can be seen as motives for following socially and culturally acceptable behavior.\n\nVicarious embarrassment (German: fremdscham.) is often seen as an opposite to schadenfreude, which is the feeling of pleasure or satisfaction at misfortune, humiliation or embarrassment of another person.\n\nVicarious embarrassment is different from an emotional contagion, which is when a person unconsciously mimics the emotions that others are experiencing. An emotional contagion is experienced by both people making it a shared emotion. Vicarious embarrassment often occurs even when the individual experiencing the embarrassing event might not be aware of the implications. For an act to be considered an emotional contagion, more than one person must be affected by the emotion, but in vicarious emotions, it is only necessary that the observer experience the emotion. Furthermore, vicarious embarrassment can be experienced even when the observer is completely isolated.\n\nVicarious embarrassment, like other vicarious emotions, presents symptoms that reflect the original emotion. However, unlike shared emotions, the experience of embarrassment for the observer is dependent on how they normally experience embarrassment. Individuals who experience social anxiety in their own life may experience the familiar symptoms of blushing, excess sweating, trembling, palpitations, and nausea. Other, less severe symptoms may include cringing, looking away, or general discomfort.\n\nVicarious embarrassment, also known as empathetic embarrassment, is intrinsically linked to empathy. Empathy is the ability to understand the feelings of another and is considered a highly reinforcing emotion to promote selflessness, prosocial behavior, and group emotion, whereas a lack of empathy is related to antisocial behavior. During an embarrassing situation, the observer empathizes with the victim of embarrassment, assuming the feeling of embarrassment. People who have more empathy are more likely to be susceptible to vicarious embarrassment. The capacity recognize emotions is probably innate as it may be achieved unconsciously. Yet it can be trained and achieved with various degrees of intensity or accuracy.\n\nPsychological projection is a theory in psychology and psychoanalysis in which humans defend themselves against undesirable emotions by denying their existence in themselves while attributing them to others. Projection is a considered a normal, common process in everyday life. Vicarious embarrassment and other vicarious emotions, however, work in the reverse, a process called self projection. The undesirable emotion is experienced in another person, and the observer projects what they interpret as the appropriate response the event onto themselves. For example, someone who lies easily might feel vicariously embarrassed if they self projecting the experience of someone getting caught in a bad lie.\n\nEmbarrassing situations often arise in social situations, as the result of failing to meet a social expectation, and is used to help learn what has been deemed culturally appropriate. While embarrassment isolates the victim based on a cultural bias, vicarious embarrassment is used to promote prosocial behavior between the victim and the observer.\n\nEmbarrassing situations have been used for a long time in situational comedy, sketch comedy, dramatic irony, and practical jokes. Traditionally, laugh tracks were used to help cue the audience to laugh at appropriate times. But as laugh tracks were removed from sitcoms, embarrassing situations on television were now accompanied by silence, creating a genre known as cringe comedy In addition, many critically acclaimed sitcom television shows, such as American television series \"The Office\". \n"}
{"id": "424964", "url": "https://en.wikipedia.org/wiki?curid=424964", "title": "Water clock", "text": "Water clock\n\nA water clock or clepsydra (Greek κλεψύδρα from κλέπτειν \"kleptein\", 'to steal'; ὕδωρ \"hydor\", 'water') is any timepiece in which time is measured by the regulated flow of liquid into (inflow type) or out from (outflow type) a vessel where the amount is then measured.\n\nWater clocks are one of the oldest time-measuring instruments. Where and when they were first invented is not known, and given their great antiquity it may never be. The bowl-shaped outflow is the simplest form of a water clock and is known to have existed in Babylon and in Egypt around the 16th century BCE. Other regions of the world, including India and China, also have early evidence of water clocks, but the earliest dates are less certain. Some authors, however, claim that water clocks appeared in China as early as 4000 BCE.\n\nSome modern timepieces are called \"water clocks\" but work differently from the ancient ones. Their timekeeping is governed by a pendulum, but they use water for other purposes, such as providing the power needed to drive the clock by using a water wheel or something similar, or by having water in their displays.\n\nThe Greeks and Romans advanced water clock design to include the inflow clepsydra with an early feedback system, gearing, and escapement mechanism, which were connected to fanciful automata and resulted in improved accuracy. Further advances were made in Byzantium, Syria and Mesopotamia, where increasingly accurate water clocks incorporated complex segmental and epicyclic gearing, water wheels, and programmability, advances which eventually made their way to Europe. Independently, the Chinese developed their own advanced water clocks, incorporating gears, escapement mechanisms, and water wheels, passing their ideas on to Korea and Japan .\n\nSome water clock designs were developed independently and some knowledge was transferred through the spread of trade. These early water clocks were calibrated with a sundial. While never reaching a level of accuracy comparable to today's standards of timekeeping, the water clock was the most accurate and commonly used timekeeping device for millennia, until it was replaced by more accurate pendulum clocks in 17th-century Europe.\n\nA water clock uses a flow of water to measure time. If viscosity is neglected, the physical principle required to study such clocks is Torricelli's law. There are two types of water clocks: inflow and outflow. In an outflow water clock, a container is filled with water, and the water is drained slowly and evenly out of the container. This container has markings that are used to show the passage of time. As the water leaves the container, an observer can see where the water is level with the lines and tell how much time has passed. An inflow water clock works in basically the same way, except instead of flowing out of the container, the water is filling up the marked container. As the container fills, the observer can see where the water meets the lines and tell how much time has passed.\n\nIn ancient China, as well as throughout East Asia, water clocks were very important in the study of astronomy and astrology. The oldest written reference dates the use of the water-clock in China to the 6th century BCE. From about 200 BCE onwards, the outflow clepsydra was replaced almost everywhere in China by the inflow type with an indicator-rod borne on a float. The Han dynasty philosopher and politician Huan Tan (40 BCE – 30 CE), a Secretary at the Court in charge of clepsydrae, wrote that he had to compare clepsydrae with sundials because of how temperature and humidity affected their accuracy, demonstrating that the effects of evaporation, as well as of temperature on the speed at which water flows, were known at this time. In 976, the Song dynasty military engineer and astronomer Zhang Sixun addressed the problem of the water in clepsydrae freezing in cold weather by using liquid mercury instead. Again, instead of using water, the early Ming Dynasty engineer Zhan Xiyuan (c. 1360-1380) created a sand-driven wheel clock, improved upon by Zhou Shuxue (c. 1530-1558).\n\nThe use of clepsydrae to drive mechanisms illustrating astronomical phenomena began with the Han Dynasty polymath Zhang Heng (78-139) in 117, who also employed a waterwheel. Zhang Heng was the first in China to add an extra compensating tank between the reservoir and the inflow vessel, which solved the problem of the falling pressure head in the reservoir tank. Zhang's ingenuity led to the creation by the Tang dynasty mathematician and engineer Yi Xing (683–727) and Liang Lingzan in 725 of a clock driven by a waterwheel linkwork escapement mechanism. The same mechanism would be used by the Song dynasty polymath Su Song (1020–1101) in 1088 to power his astronomical clock tower, as well as a chain drive. Su Song's clock tower, over tall, possessed a bronze power-driven armillary sphere for observations, an automatically rotating celestial globe, and five front panels with doors that permitted the viewing of changing mannequins which rang bells or gongs, and held tablets indicating the hour or other special times of the day. In the 2000s, in Beijing's Drum Tower an outflow clepsydra is operational and displayed for tourists. It is connected to automata so that every quarter-hour a small brass statue of a man claps his cymbals.\n\nN. Kameswara Rao suggested that pots excavated from Mohenjo daro may have been used as water clocks. They are tapered at the bottom, have a hole on the side, and are similar to the utensil used to perform abhishekam (pour holy water) on shivalingam.\nN. Narahari Achar and Subhash Kak suggest that the use of the water clock in ancient India is mentioned in the Atharvaveda from the 2nd millennium BCE.\n\"Ghati\" or \"Kapala\" (clepsydra or water clock) is referred to in Jyotisha Vedanga, where the amount of water that measures a nadika (24 minutes) is mentioned. A more developed form of the clepsydra is described in chapter xiii, 23 of the \"Suryasiddhanta.\"\nAt Nalanda, a Buddhist university, four hours a day and four hours at night were measured by a water clock, which consisted of a copper bowl holding two large floats in a larger bowl filled with water. The bowl was filled with water from a small hole at its bottom; it sank when completely filled and was marked by the beating of a drum at daytime. The amount of water added varied with the seasons and this clock was operated by the students of the university.\nThe description of a water clock in astrologer Varahimira's \"Pancasiddhantika\" (505) adds further detail to the account given in the \"Suryasiddhanta\". The description given by mathematician Brahmagupta in his work \"Brahmasphutasiddhanta\" matches with that given in the \"Suryasiddhanta\". Astronomer Lallacharya describes this instrument in detail. In practice, the dimensions were determined by experiment.\n\nIn Babylon, water clocks were of the outflow type and were cylindrical in shape. Use of the water clock as an aid to astronomical calculations dates back to the Old Babylonian period (\"c.\" 2000 BCE–\"c.\" 1600 BCE). While there are no surviving water clocks from the Mesopotamian region, most evidence of their existence comes from writings on clay tablets. Two collections of tablets, for example, are the \"Enuma-Anu-Enlil\" (1600–1200 BCE) and the \"MUL.APIN\" (7th century BCE). In these tablets, water clocks are used in reference to payment of the night and day watches (guards).\n\nThese clocks were unique, as they did not have an indicator such as hands (as are typically used today) or grooved notches (as were used in Egypt). Instead, these clocks measured time \"by the weight of water flowing from\" it. The volume was measured in capacity units called \"qa\". The weight, \"mana\" (the Greek unit for about one pound), is the weight of water in a water clock.\n\nIn Babylonian times, time was measured with temporal hours. So, as seasons changed, so did the length of a day. \"To define the length of a 'night watch' at the summer solstice, one had to pour two mana of water into a cylindrical clepsydra; its emptying indicated the end of the watch. One-sixth of a mana had to be added each succeeding half-month. At equinox, three mana had to be emptied in order to correspond to one watch, and four mana were emptied for each watch of the winter solstitial night.\"\n\nThe oldest water clock of which there is physical evidence dates to \"c.\" 1417-1379 BCE, during the reign of Amenhotep III where it was used in the Temple of Amen-Re at Karnak. The oldest documentation of the water clock is the tomb inscription of the 16th century BCE Egyptian court official Amenemhet, which identifies him as its inventor. These simple water clocks, which were of the outflow type, were stone vessels with sloping sides that allowed water to drip at a nearly constant rate from a small hole near the bottom. There were twelve separate columns with consistently spaced markings on the inside to measure the passage of \"hours\" as the water level reached them. The columns were for each of the twelve months to allow for the variations of the seasonal hours. These clocks were used by priests to determine the time at night so that the temple rites and sacrifices could be performed at the correct hour. These clocks may have been used in daylight as well.\n\nAccording to Callisthenes, the Persians were using water clocks in 328 BC to ensure a just and exact distribution of water from qanats to their shareholders for agricultural irrigation. The use of water clocks in Iran, especially in Zibad, dates back to 500BC. Later they were also used to determine the exact holy days of pre-Islamic religions, such as the \"Nowruz\", \"Chelah\", or \"Yaldā\" - the shortest, longest, and equal-length days and nights of the years. The water clocks used in Iran were one of the most practical ancient tools for timing the yearly calendar.\nThe water clock, or \"Fenjaan\", was the most accurate and commonly used timekeeping device for calculating the amount or the time that a farmer must take water from a qanat or well for irrigation, until it was replaced by more accurate current clocks. Persian water clocks were a practical and useful tool for the qanat's shareholders to calculate the length of time they could divert water to their farm. The qanat(Kariz) was the only water source for agriculture and irrigation so a just and fair water distribution was very important. Therefore, a very fair and clever old person was elected to be the manager of the water clock(MirAab), and at least two full-time managers were needed to control and observe the number of fenjaans and announce the exact time during the days and nights.\n\nThe Fenjaan consisted of a large pot full of water and a bowl with a small hole in the center. When the bowl became full of water, it would sink into the pot, and the manager would empty the bowl and again put it on the top of the water in the pot. He would record the number of times the bowl sank by putting small stones into a jar. The place where the clock was situated, and its managers, were collectively known as \"khaneh Fenjaan\". Usually this would be the top floor of a public-house, with west- and east-facing windows to show the time of Sunset and Sunrise. There was also another time-keeping tool named a \"staryab\" or astrolabe, but it was mostly used for superstitious beliefs and was not practical for use as a farmers' calendar. The Zeebad Gonabad water clock was in use until 1965 when it was substituted by modern clocks.\n\nThe word \"clepsydra\" comes from the Greek meaning \"water thief\". The Greeks considerably advanced the water clock by tackling the problem of the diminishing flow. They introduced several types of the inflow clepsydra, one of which included the earliest feedback control system. Ctesibius invented an indicator system typical for later clocks such as the dial and pointer. The Roman engineer Vitruvius described early alarm clocks, working with gongs or trumpets. A commonly used water clock was the simple outflow clepsydra. This small earthenware vessel had a hole in its side near the base. In both Greek and Roman times, this type of clepsydra was used in courts for allocating periods of time to speakers. In important cases, such as when a person's life was at stake, it was filled completely, but for more minor cases, only partially. If proceedings were interrupted for any reason, such as to examine documents, the hole in the clepsydra was stopped with wax until the speaker was able to resume his pleading.\n\nJust northeast of the entrance to the Acropolis of Athens there was a famous natural spring named Clepsydra. It is mentioned by Aristophanes in \"Lysistrata\" (lines 910-913) and other ancient literary sources. A fountain house was build on the site ca. 470-460 BCE. It was of simple rectangular construction with a draw-basin and paved court.\n\nIn the 4th century BCE, the clepsydra is known to have been used as a stop-watch for imposing a time limit on clients' visits in Athenian brothels. Slightly later, in the early 3rd century BCE, the Hellenistic physician Herophilos employed a portable clepsydra on his house visits in Alexandria for measuring his patients' pulse-beats. By comparing the rate by age group with empirically obtained data sets, he was able to determine the intensity of the disorder.\n\nBetween 270 BCE and 500 CE, Hellenistic (Ctesibius, Hero of Alexandria, Archimedes) and Roman horologists and astronomers were developing more elaborate mechanized water clocks. The added complexity was aimed at regulating the flow and at providing fancier displays of the passage of time. For example, some water clocks rang bells and gongs, while others opened doors and windows to show figurines of people, or moved pointers, and dials. Some even displayed astrological models of the universe. The 3rd century BCE engineer Philo of Byzantium referred in his works to water clocks already fitted with an escapement mechanism, the earliest known of its kind.\n\nThe biggest achievement of the invention of clepsydrae during this time, however, was by Ctesibius with his incorporation of gears and a dial indicator to automatically show the time as the lengths of the days changed throughout the year, because of the temporal timekeeping used during his day. Also, a Greek astronomer, Andronicus of Cyrrhus, supervised the construction of his Horologion, known today as the Tower of the Winds, in the Athens marketplace (or agora) in the first half of the 1st century BCE. This octagonal clocktower showed scholars and shoppers both sundials and mechanical hour indicators. It featured a 24-hour mechanized clepsydra and indicators for the eight winds from which the tower got its name, and it displayed the seasons of the year and astrological dates and periods.\n\nIn the medieval Islamic world (632-1280), the use of water clocks has its roots from Archimedes during the rise of Alexandria in Egypt and continues on through Byzantium. The water clocks by Persian engineer Al-Jazari, however, are credited for going \"well beyond anything\" that had preceded them. In al-Jazari's 1206 treatise, he describes one of his water clocks, the elephant clock. The clock recorded the passage of temporal hours, which meant that the rate of flow had to be changed daily to match the uneven length of days throughout the year. To accomplish this, the clock had two tanks, the top tank was connected to the time indicating mechanisms and the bottom was connected to the flow control regulator. Basically, at daybreak the tap was opened and water flowed from the top tank to the bottom tank via a float regulator that maintained a constant pressure in the receiving tank.\n\nThe most sophisticated water-powered astronomical clock was Al-Jazari's castle clock, considered by some to be an early example of a programmable analog computer, in 1206. It was a complex device that was about high, and had multiple functions alongside timekeeping. It included a display of the zodiac and the solar and lunar orbits, and a pointer in the shape of the crescent moon which traveled across the top of a gateway, moved by a hidden cart and causing automatic doors to open, each revealing a mannequin, every hour. It was possible to re-program the length of day and night in order to account for the changing lengths of day and night throughout the year, and it also featured five musician automata who automatically play music when moved by levers operated by a hidden camshaft attached to a water wheel. Other components of the castle clock included a main reservoir with a float, a float chamber and flow regulator, plate and valve trough, two pulleys, crescent disc displaying the zodiac, and two falcon automata dropping balls into vases.\n\nThe first water clocks to employ complex segmental and epicyclic gearing was invented earlier by the Arab engineer Ibn Khalaf al-Muradi in Islamic Iberia c. 1000. His water clocks were driven by water wheels, as was also the case for several Chinese water clocks in the 11th century. Comparable water clocks were built in Damascus and Fez. The latter (Dar al-Magana) remains until today and its mechanism has been reconstructed. The first European clock to employ these complex gears was the astronomical clock created by Giovanni de Dondi in c. 1365. Like the Chinese, Arab engineers at the time also developed an escapement mechanism which they employed in some of their water clocks. The escapement mechanism was in the form of a constant-head system, while heavy floats were used as weights.\n\nIn 1434 during the Choson (or Joseon) Dynasty, Chang Yongsil (or Jang Young Sil) (장영실 in Korean), Palace Guard and later Chief Court Engineer, constructed the Jagyeongnu (self-striking water clock or striking clepsydra) for King Sejong. What made the Jagyeongnu self-striking (or automatic) was the use of jack-work mechanisms, by which three wooden figures (jacks) struck objects to signal the time. This innovation no longer required the reliance of human workers, known as \"rooster men\", to constantly replenish it. By 1554, the water clock spread from Korea to Japan. Water clocks were used and improved upon throughout Asia well into the 15th century.\n\nOnly a few modern water clocks exist today. In 1979, French scientist Bernard Gitton began creating his Time-Flow Clocks, which are a modern-day approach to the historical version. His unique glass tube designs can be found in over 30 locations throughout the world, including one at NEMO Science Museum in Amsterdam, Europa-Center's The Clock of Flowing Time in Berlin, Centre Commercial Milenis in Guadeloupe, the Giant Water Clock at The Children's Museum of Indianapolis in Indianapolis, Indiana, the Abbotsford International Airport (formerly at Sevenoaks Shopping Centre) in Abbotsford, British Columbia, and the Shopping Iguatemi in São Paulo and Porto Alegre, Brazil.\n\nGitton's design relies on gravity powering multiple siphons in the same principle as the Pythagorean cup; for example, after the water level in the minute or hour display tubes is reached, an overflow tube starts to act as a siphon and thus empties the display tube. Actual time keeping is done by a calibrated pendulum powered by a water stream piped from the clock's reservoir. The pendulum has a carefully constructed container attached to it; this measures the water that is then poured into the display system. This means that strictly speaking these are not water clocks. The water is used to power the pendulum and to show the time in the display system. There are other modern designs of water clocks, including the Royal Gorge water clock in Colorado, the Woodgrove Mall in Nanaimo, British Columbia, and the Hornsby Water Clock in Sydney, Australia.\n\nWhen viscosity can be neglected, the outflow rate of the water is governed by Torricelli's law, or more generally, by Bernoulli's principle. Viscosity will dominate the outflow rate if the water flows out through a nozzle that is sufficiently long and thin, as given by the Hagen–Poiseuille equation. Approximately, the flow rate is for such design inversely proportional to the viscosity, which depends on the temperature. Liquids generally become less viscous as the temperature increases. In the case of water, the viscosity varies by a factor of about seven between zero and 100 degrees Celsius. Thus, a water clock with such a nozzle would run about seven times faster at 100 °C than at 0 °C. Water is about 25 percent more viscous at 20 °C than at 30 °C, and a variation in temperature of one degree Celsius, in this \"room temperature\" range, produces a change of viscosity of about two percent. Therefore, a water clock with such a nozzle that keeps good time at some given temperature would gain or lose about half an hour per day if it were one degree Celsius warmer or cooler. To make it keep time within one minute per day would require its temperature to be controlled within °C (about ° Fahrenheit). There is no evidence that this was done in antiquity, so ancient water clocks with sufficiently thin and long nozzles (unlike the modern pendulum-controlled one described above) cannot have been reliably accurate by modern standards. Note, however, that while modern timepieces may not be reset for long periods, water clocks were likely reset every day, when refilled, based on a sundial, so the cumulative error would not have been great.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "164631", "url": "https://en.wikipedia.org/wiki?curid=164631", "title": "Wavenumber–frequency diagram", "text": "Wavenumber–frequency diagram\n\nA wavenumber–frequency diagram is a plot displaying the relationship between the wavenumber (spatial frequency) and the frequency (temporal frequency) of certain phenomena. Usually frequencies are placed on the vertical axis, while wavenumbers are placed on the horizontal axis.\n\nIn the atmospheric sciences, these plots are a common way to visualize atmospheric waves.\n\nIn the geosciences, especially seismic data analysis, these plots also called \"f\"–\"k\" plot, in which energy density within a given time interval is contoured on a frequency-versus-wavenumber basis. They are used to examine the direction and apparent velocity of seismic waves and in velocity filter design.\n\nIn general, the relationship between wavelength formula_1, frequency formula_2, and the phase velocity formula_3 of a sinusoidal wave is:\n\nUsing the wavenumber (formula_5) and angular frequency (formula_6) notation, the previous equation can be rewritten as\n\nOn the other hand, the group velocity is equal to the slope of the wavenumber–frequency diagram:\n\nAnalyzing such relationships in detail often yields information on the physical properties of the medium, such as density, composition, etc.\n\n"}
