{"id": "4689919", "url": "https://en.wikipedia.org/wiki?curid=4689919", "title": "And-inverter graph", "text": "And-inverter graph\n\nAn and-inverter graph (AIG) is a directed, acyclic graph that represents a structural implementation of the logical functionality of a circuit or network. An AIG consists of two-input nodes representing logical conjunction, terminal nodes labeled with variable names, and edges optionally containing markers indicating logical negation. This representation of a logic function is rarely structurally efficient for large circuits, but is an efficient representation for manipulation of boolean functions. Typically, the abstract graph is represented as a data structure in software.\nConversion from the network of logic gates to AIGs is fast and scalable. It only requires that every gate be expressed in terms of AND gates and inverters. This conversion does not lead to unpredictable increase in memory use and runtime. This makes the AIG an efficient representation in comparison with either the binary decision diagram (BDD) or the \"sum-of-product\" (ΣoΠ) form, that is, the canonical form in Boolean algebra known as the disjunctive normal form (DNF). The BDD and DNF may also be viewed as circuits, but they involve formal constraints that deprive them of scalability. For example, ΣoΠs are circuits with at most two levels while BDDs are canonical, that is, they require that input variables be evaluated in the same order on all paths.\n\nCircuits composed of simple gates, including AIGs, are an \"ancient\" research topic. The interest in AIGs started with Alan Turing's seminal 1948 paper on neural networks, in which he described a randomized trainable network of NAND gates. Interest continued through the late 1950s and continued in the 1970s when various local transformations have been developed. These transformations were implemented in several\nlogic synthesis and verification systems, such as Darringer et al. and Smith et al., which reduce circuits to improve area and delay during synthesis, or to speed up formal equivalence checking. Several important techniques were discovered early at IBM, such as combining and reusing multi-input logic expressions and subexpressions, now known as structural hashing.\n\nRecently there has been a renewed interest in AIGs as a functional representation for a variety of tasks in synthesis and verification. That is because representations popular in the 1990s (such as BDDs) have reached their limits of scalability in many of their applications. Another important development was the recent emergence of much more efficient boolean satisfiability (SAT) solvers. When coupled with \"AIGs\" as the circuit representation, they lead to remarkable speedups in solving a wide variety of boolean problems.\n\nAIGs found successful use in diverse EDA applications. A well-tuned combination of \"AIGs\" and boolean satisfiability made an impact on formal verification, including both model checking and equivalence checking. Another recent work shows that efficient circuit compression techniques can be developed using AIGs. There is a growing understanding that logic and physical synthesis problems can be solved using simulation and boolean satisfiability to compute functional properties (such as symmetries) and node flexibilities (such as don't-care terms, resubstitutions, and SPFDs). Mishchenko et al. shows that AIGs are a promising \"unifying\" representation, which can bridge logic synthesis, technology mapping, physical synthesis, and formal verification. This is, to a large extent, due to the simple and uniform structure of AIGs, which allow rewriting, simulation, mapping, placement, and verification to share the same data structure.\n\nIn addition to combinational logic, AIGs have also been applied to sequential logic and sequential transformations. Specifically, the method of structural hashing was extended to work for AIGs with memory elements (such as D-type flip-flops with an initial state,\nwhich, in general, can be unknown) resulting in a data structure that is specifically tailored for applications related to retiming.\n\nOngoing research includes implementing a modern logic synthesis system completely based on AIGs. The prototype called ABC features an AIG package, several AIG-based synthesis and equivalence-checking techniques, as well as an experimental implementation of sequential synthesis. One such technique combines technology mapping and retiming in a single optimization step. These optimizations can be implemented using networks composed of arbitrary gates, but the use of AIGs makes them more scalable and easier to implement.\n\n\n\"This article is adapted from a column in the ACM SIGDA e-newsletter by Alan Mishchenko <br>\nOriginal text is available here.\"\n"}
{"id": "4333171", "url": "https://en.wikipedia.org/wiki?curid=4333171", "title": "Archaeological looting in Iraq", "text": "Archaeological looting in Iraq\n\nArchaeological looting in Iraq took place on the aftermath of the US-led invasion of Iraq in 2003. The chaos following war has provided the opportunity to pillage everything that is not nailed down. The period between April 8, 2003 when the staff vacated National Museum of Iraq and April 16, 2003 when US forces arrived in sufficient numbers to “restore some semblance of order.” Some 15,000 cultural artifacts disappeared in that time.\n\nLooting of ancient artifacts has a long tradition. As early as 1884, laws passed in Mesopotamia about moving and destroying antiquities. By the end of WW1, British-occupied Mesopotamia had created protections for archeological sites where looting was beginning to become a problem. They established an absolute prohibition on exporting antiquities. The British Museum was responsible for the sites and museums across Iraq during this time period. Gertrude Bell, well known for drawing the Iraq borders, excavated many sites around Iraq and created what is now the National Museum of Iraq. \n\nBy the mid 1920s the black market for antiquities was growing and looting began in all sites where antiquities could be found. After Iraq was independent of Britain the absolute ban on antiquity exports was lifted. Until the mid 1970s Iraq was one of very few countries to not prohibit external trade in antiquities. This made Iraq attractive to looters and black market collectors from around the globe. The result of the first Gulf War was that at least 4000 artifacts were looted from Iraq sites. Uprisings that followed the war also resulted in 9 of 13 regional museums being looted and burned. This was just a preview for what would once again happen after the 2003 war. Since the 2003 invasion of Iraq, archaeological looting has become an even greater problem. Though some sites, such as Ur and Nippur, were officially protected by US and Coalition forces, most were not. \n\nSaddam Hussein treasured his national heritage immensely and acted to defend these sites and the artifacts within them. Hussein came into power in 1979 as the fifth president of Iraq. He believed that the past of Iraq was important to his national campaign and his regime actually doubled the national budget for archeology and heritage creating museums and protecting sites all over Iraq. It wasn’t until his party the Ba'ath Party was under pressure in the 1990s that looting become a large problem once again for Iraq. By 2000 looting had become so rampant that the workers of the sites were even looting their own workplaces. With the fall of Saddam's government on 9 April 2003, archaeological sites were left completely open to looting.\n\nBefore the 2003 invasion by US and Coalition forces, the US government created a post-war plan for Iraq. The US has been heavily criticized in the media and academic writings for not adequately planning protections for culture and antiquities. According to Lawrence Rothfield, former director of the Cultural Policy Center at the University of Chicago and associate professor of English and comparative literature, this looting of the National Museum of Iraq and of hundreds of archaeological sites around the country was not prevented. At the time of war planning it was Secretary of Defense Donald Rumsfeld who decided on a fast invasion with fewer troops, resulting in inadequate protection of buildings and cultural sites. \n\nAmerican troops and commanders did not prioritize security for cultural sites around Iraq. Peacekeeping was seen as a lesser job than physically fighting in combat and President Bush’s suspension of former president Clinton’s policies for peacekeeping not only backed up this thought but also made the US’s duties to restore public order unclear. American troops in Iraq didn’t trust Iraqi power of any kind meaning that instead of using and training Iraqi police, the US military took matters of security and policing into their own hands. Essentially the US would act as peacekeepers to train a national army and police force. Special Forces teams would work with regional warlords to keep control of their territories. Allowing warlords to police their own areas has been credited with being a disastrous plan for the archaeological sites in particular. \n\nArthur Houghton had an interest and some expertise in cultural heritage and was one of the first to wonder what the pre-war plan was for Iraqi culture. He had worked in the State Department as a Foreign Service officer, as an international policy analyst for the White house and also served at an acting curator for the Getty Museum. In late spring 2002, Houghton was approached by Ashton Hawkins, former Executive Vice President and Counsel to the Trustees of the Metropolitan Museum, and was asked to find out what was being done by officials to secure heritage sites in the upcoming war in Iraq. Houghton could find no one designated with the task of protection and preservation of culture in Iraq. \n\nThere had been a secret Future of Iraq Project since October 2001, with clearance from the Pentagon. However, even under this Project no specific person had taken up responsibility of culture. Even archaeological organizations in the US hadn’t noticed the issue until late 2002. Likewise when the US Agency for Cultural Development (USAID) met with estimated 150 NGO’s not one brought up protection of cultural heritage. UNESCO had in fact, after the Gulf War in 1991, attempted to go into Iraq and assess the damage to cultural sites but they were not allowed to enter the country. UNESCO then focused, for the next decade, on reconstruction after the fact rather than prevention measures.\n\nWithin the US military, Civil Affairs (CA) forces were important to the protection of culture and, as they were mostly reservists, included experts in a variety of areas including archaeology. The plan was to spread the expertise among fighting forces in order to warn them of cultural sites in the area. However, CA was left out of pre-war planning until January 2003, when it was too late to be of any real significant help. The CA had to prioritize the small amount of CA troops to what they thought was necessary, which inevitably wasn’t culture. The CA did, however, pull the only two archaeologists in Civil Affairs to be on a culture team, Maj. Chris Varhola and Capt. William Sumner. These two men, however, in the end were sent to other places when the conflict began. Varhola was needed to prepare for the refugee crises that never arrived and Sumner was reassigned to guard a zoo after pushing his advisor too hard on antiquities issues. Any protection of culture, sites or buildings was stopped due to the priorities of other matters. Essentially no one who had archaeological expertise was senior enough to get anything done. \n\nAnother branch of the US government that had interest in culture was the Foreign Area Offices (FAO). Unfortunately though, they were focused on customs and attitudes rather than archaeological sites. Something that was accomplished was the creation of a no-strike list created by Maj. Varhola just like two archaeologists before him had done during the 1991 Gulf War which had a great outcome of saving antiquities from bombing. \n\nOne piece of international law that is important for this conflict is the 1954 Hague Convention for the Protection of cultural Property in the Event of Armed Conflict, this Convention states that parties in conflict must “under take to prohibit, prevent and, if necessary, put a stop to any form of theft, pillage or misappropriation of, and any act of vandalism directed against, cultural property.” This provision was constructed for the parties actually in combat within the war and not civilians within their own state. As the upcoming years would prove, there are exceptions to this Convention and they would result in Americans firing on the Iraq National Museum. \n\nBy fall 2002, post-war planning was sporadic and improvised. The cultural planning aspect needed to have leadership that it never got. Deputy Assistant under the Security of Defense, Joseph Collins, recalls some forces spent more time working on projects that ended up not being needed like a refugee crises plan. He says he can’t remember if there was even an organizational plans to solve specific issues. \n\nThe first known effort by cultural interests to contact US officials was October 2002. After a meeting of powerful players in culture, Houghton sent a letter asking for departments to tell forces to avoid damaging monuments, soldiers were to respect the integrity of sites, and lastly to work quickly to get the antiquities services in Iraq up and running again. Following this, the Archaeological Institute of America (AIA) also sent a similar letter to the Pentagon in December 2002 asking for governments to take action to prevent looting in the aftermath of the war. As 2002 came to an end the media and government were only broadcasting the good done by the troops in not destroying cultural heritage themselves but not on the looting done by people in Iraq and the Americans duty to protect the antiquities.\n\nWhen the looting of the National Iraq Museum became known, experts from around the globe started planning to remedy the situation. McGuire Gibson, one of the leading archaeologists and experts on Mesopotamia explained to the Office of Reconstruction and Humanitarian Assistance (ORHA) that the looted museum artifacts were only a small part of what archaeological digs around the country held. Perhaps 25 thousand of an estimated half million sites were registered. ORHA had no resources to address this problem. Gibson had suggested helicopter surveys to determine the scale of looted sites. By April 24, 2003, looting had taken place in Umma, Umm al-Hafriyat, Umm al-Aqarib, Bismaya, Larsa, and Bad-tibira, most of were unguarded. Most looting was by workers once employed by the now disbanded State Board of Antiquities and Heritage. A local tribe was guarding the World Heritage Site of Hatra although others were unsupervised. \n\nBy May 2003, international work began on the already looted museum but not on other sites. The US military conducted a raid in May on Umma where they found hundreds of trenches with many looters all over the site. On May 7, the Bush Administration replaced Gen. Jay Garner with L. Paul Bremer who was given more power and banned high-ranking Ba’ath Party members from government jobs and disbanded the remains of the Iraqi army. Any guards at archeological sites were unpaid for months and they were not allowed to carry guns. Now, instead of dealing with civilian looters, these unarmed guards were dealing with large mobs of armed people. \n\nAt the end of May 2003, it finally became clear how badly the sites were looted when a trip sponsored by National Geographic went out to assess the damage. There was a northern and southern team to assess the post-conflict damage by land. They found the famous sites such as Babylon, Hatra, Nimrud and Ur were under US military control. Lesser known sites were completely unguarded and the responsible Civil Affairs teams didn’t even know where they were. Every place the National Geographic team saw, except one that was guarded by barbed wire, had been damaged. \n\nGibson was part of the northern National Geographic team and he sent a report to the White House science advisor John Marburger. Other archeological experts in both the US and Britain were waiting for invitations to go to Iraq and help. After Gibson’s report they were given invitations to create a team in Iraq. \n\nEarly July 2003, UNESCO revealed that looting was still happening at sites across the country. Other military services such as Japanese and Dutch troops offered assistance but were ignored. In July 8, a new security guard force known as the Iraqi Facility Protection Service (FPS) was established to protect sites all around the country in co-operation with the US military. A week later, the State Department announced it was forming a group to assist in the rebuilding of Iraq’s cultural heritage. \n\nThese announcements had no effect on the looting and illegal export of artifacts. McGuire Gibson on September 11, 2003 wrote to a military geographer, \"The continuing destruction of sites all over southern Iraq and the theft of thousands of artifacts every week, with no visible effort on part of the US authorities, makes the question of ethical behaviour by museums pointless. Your unit of the Pentagon is capable of demonstrating the location and expansion of illegal digging. Are you at least doing that much?\"\n\nIt is impossible to discover exactly how much destruction to archeological sites has happened since the 2003 invasion. As late as 2004, US military maps still did not show archeological sites. Archeologist Elizabeth Stone purchased satellite images of the seven thousand square kilometers in Iraq that contain many known sites. She counted 1,837 new holes by comparing 2001-2002 with 2003 images. Looters concentrated on sites that had the most marketable artifacts. Estimates of the number of looted artifacts from 2003-2005 are from 400,000 to 600,000 items. This number is 30-40 times greater than the number of artifacts stolen from the museum. Britain alone between 2004-2006 seized 3-4 tons of plundered artifacts. \n\nSome artifacts have been recovered by accident. An archeologist was watching a home decorating show when he saw a second century stone head from Hatra sitting on the decorator’s mantle. The illegal black market for goods became so saturated that prices in the market were going down after 2003 according to an antiquities researcher specializing in illicit dealings. \n\nRevenue from looted antiquities is estimated by the Archeological Institute of America to amount to between $10 to $20 million annually. Terrorist and rebel groups have a long history of using stolen artifacts to finance their operations. \n\nBy the end of 2003, 1,900 Iraqi antiquities had been confiscated from bordering countries: 1,450 in Jordan, 36 in Syria, 38 in Kuwait and 18 in Saudi Arabia. \n\nMarine Corps reservist Matthew Boulay witnessed illicit trading even on U.S. military bases. Flea markets authorized by the camp commanders included a booth with antiquities for $20, $40 or $100 each. Boulay asked Gibson and was told that these artifacts were real. Gibson asked Boulay to ask the base commander to stop these sales. When Boulay informed his platoon commander he got a “cease and desist” order forbidding any more emails about the issue to anyone. \n\nOther institutes from America and around the world have contributed to protect sites in Iraq but this is still not enough. The US started teaching military personnel headed for Iraq the importance of cultural heritage and site preservation, but not how to stop civilian looting. Donny George who was an employee at the Iraq National Museum was appointed to Director of Museums in 2004, and by summer 2006, a force of 1,400 guards were situated at sites around the country.\n\n\n\n\n"}
{"id": "18364057", "url": "https://en.wikipedia.org/wiki?curid=18364057", "title": "Architecture criticism", "text": "Architecture criticism\n\nArchitecture criticism is the critique of architecture. Everyday criticism relates to published or broadcast critiques of buildings, whether completed or not, both in terms of news and other criteria. In many cases, criticism amounts to an assessment of the architect's success in meeting his or her own aims and objectives and those of others. The assessment may consider the subject from the perspective of some wider context, which may involve planning, social or aesthetic issues. It may also take a polemical position reflecting the critic's own values. At the most accessible extreme, architectural criticism is a branch of lifestyle journalism, especially in the case of high-end residential projects.\n\nMost major national newspapers in developed countries cover the arts in some form. Architectural criticism may be included as a part of their arts coverage, in a real estate section or a \"Home & Style\" supplement. In the USA, reviews are published in specialist magazines ranging from the popular (e.g. Architectural Digest, Wallpaper) to specialist magazines for design professionals (e.g. Architectural Review, Detail). As with other forms of criticism, technical language is used to a varying extent to convey impressions and views precisely.\n\nLewis Mumford wrote extensively on architecture in the nineteen thirties, forties and fifties at \"The New Yorker\". Ada Louise Huxtable was the first full-time architecture critic working for an American daily newspaper when \"The New York Times\" gave her the role in 1963. John Betjeman, a co-founder of the Victorian Society, wrote and broadcast from the 1950s to 1970s, principally covering historical rather than new buildings, but contributing to a trend for criticism to expand into radio and then television. Charles, Prince of Wales, is outspoken in his criticism of modern architecture, memorably describing a proposed extension to the National Gallery in London as a \"monstrous carbuncle on the face of a much-loved friend\".\n\nThe critic's task is to assess how successful the architect and others involved with the project have been in meeting both the criteria the project set out to meet and those that the critic himself feels to be important. Specific criteria include:\n\n\nContemporary critics working for major newspapers include:\n\nList of architecture magazines\n\n\n\n"}
{"id": "3561143", "url": "https://en.wikipedia.org/wiki?curid=3561143", "title": "Armen Terzian", "text": "Armen Terzian\n\nArmen Terzian (1915–1989) was an American-Armenian American football official in the National Football League (NFL) from to . He was a field judge and wore uniform number 23.\n\nTerzian officiated an NFC Divisional Playoff game in December at Metropolitan Stadium between the Minnesota Vikings and the visiting Dallas Cowboys. Following a late Hail Mary touchdown pass by the Cowboys to take the lead, an angry fan threw a whiskey bottle that hit Terzian in the forehead, rendering him momentarily unconscious. He did not require stitches, but had to wear a large white bandage around his forehead for the final few seconds remaining in the game.\n\nThe following season, he officiated Super Bowl XI, also a Vikings' loss.\n\n"}
{"id": "51162639", "url": "https://en.wikipedia.org/wiki?curid=51162639", "title": "Breast Tax", "text": "Breast Tax\n\nThe Breast Tax (\"Mulakkaram\" or \"mula-karam\" in Malayalam) was a tax imposed on the untouchable (outcaste) women in Travancore (in present-day Kerala state of India), until 1924. The untouchable (\"avarna\") women were expected to pay the government a tax on their breasts, as soon as they started developing breasts. The outcaste men had to pay a similar tax, called \"tala-karam\", on their heads. Those belonging to a varna (\"savarna\"), from Brahmins to the Shudras, were exempted from the taxes.\n\nThe outcaste women were not allowed to wear clothes covering their upper body in the public. The law resulted from Travancore's tradition, in which the breast was bared as a symbol of respect to higher-status people. For example, the Nairs bared their breasts to the upper-caste Nambudiri Brahmins, while the Brahmins bared their breasts only to the images of the deities. The people of the even lower castes, such as Nadars (or Shanars), were not allowed cover their breasts at all. With the spread of Christianity in the 19th century, the Christian converts among the Nadar women started covering their upper body, and gradually, even the Hindu Nadar women adopted this practice. After a series of protests (see Channar revolt), the Nadar women were granted the right to cover their breasts in 1859.\n\n"}
{"id": "2831995", "url": "https://en.wikipedia.org/wiki?curid=2831995", "title": "Burgess (title)", "text": "Burgess (title)\n\nBurgess originally meant a freeman of a borough (England, Wales, Ireland) or burgh (Scotland). It later came to mean an elected or unelected official of a municipality, or the representative of a borough in the English House of Commons.\n\nThe term was also used in some of the original American colonies. In the Colony of Virginia, a \"burgess\" was a member of the legislative body, which was termed the \"House of Burgesses\".\n\nIt was derived in Middle English and Middle Scots from the Old French word \"burgeis\", simply meaning \"an inhabitant of a town\" (cf. \"burgeis\" or \"burges\" respectively). The Old French word \"burgeis\" is derived from \"bourg\", meaning a market town or medieval village, itself derived from Late Latin \"burgus\", meaning \"fortress\" or \"wall\". In effect, the reference was to the north-west European medieval and renaissance merchant class which tended to set up their storefronts along the outside of the city wall, where traffic through the gates was an advantage and safety in event of an attack was easily accessible. The right to seek shelter within a burg was known as the \"right of burgess\".\n\nThe term was close in meaning to the Germanic term \"burgher\", a formally defined class in medieval German cities (Middle Dutch \"burgher\", Dutch \"burger\" and German \"Bürger\"). It is also linguistically close to the French term \"Bourgeois\", which evolved from \"burgeis\". \n\nThe original version of the well-known English folk song \"Greensleeves\" includes the following:\n\n<poem>Thy purse and eke thy gay guilt knives,\nthy pincase gallant to the eye:\nNo better wore the Burgesse wives,\nand yet thou wouldst not love me.</poem>\n\nThis clearly implies that at the time when it was composed (late 16th to early 17th century) a burgess was proverbial as being able to provide his wife with beautiful and expensive clothes.\n\n"}
{"id": "613557", "url": "https://en.wikipedia.org/wiki?curid=613557", "title": "Calculus of constructions", "text": "Calculus of constructions\n\nIn mathematical logic and computer science, the calculus of constructions (CoC) is a type theory created by Thierry Coquand. It can serve as both a typed programming language and as constructive foundation for mathematics. For this second reason, the CoC and its variants have been the basis for Coq and other proof assistants.\n\nSome of its variants include the calculus of inductive constructions (which adds inductive types),\nthe calculus of (co)inductive constructions (which adds coinduction), \nand the predicative calculus of inductive constructions (which removes some impredicativity).\n\nThe CoC is a higher-order typed lambda calculus, initially developed by Thierry Coquand. It is well known for being at the top of Barendregt's lambda cube. It is possible within CoC to define functions from integers to integers, as well as integers to types, types to types, and types to integers.\n\nThe CoC is strongly normalizing, although, by Gödel's incompleteness theorem, it is impossible to prove this property within the CoC since it implies inconsistency.\n\nThe CoC has been developed alongside the Coq proof assistant. As features were added (or possible liabilities removed) to the theory, they became available in Coq.\n\nVariants of the CoC are used in other proof assistants, such as Matita.\n\nThe Calculus of Constructions can be considered an extension of the Curry–Howard isomorphism. The Curry–Howard isomorphism associates a term in the simply typed lambda calculus with each natural-deduction proof in intuitionistic propositional logic. The Calculus of Constructions extends this isomorphism to proofs in the full intuitionistic predicate calculus, which includes proofs of quantified statements (which we will also call \"propositions\").\n\nA \"term\" in the calculus of constructions is constructed using the following rules:\n\n\nIn other words, the term syntax, in BNF, is then:\n\nThe calculus of constructions has five kinds of objects:\n\nThe calculus of constructions allows proving typing judgments:\n\nWhich can be read as the implication\n\nThe valid judgments for the calculus of constructions are derivable from a set of inference rules. In the following, we use formula_7 to mean a sequence of type assignments \nformula_8, and we use K to mean either P or T. We shall write formula_9 to mean the result of substituting the term \nformula_10 for the free variable formula_11 in \nthe term formula_6.\n\nAn inference rule is written in the form\n\nwhich means\n\n1. formula_16\n\n3. formula_17\n\n4. formula_18\n\n5. formula_19\n\nThe calculus of constructions has very few basic operators: the only logical operator for forming propositions is formula_20. However, this one operator is sufficient to define all the other logical operators:\n\nThe basic data types used in computer science can be defined\nwithin the Calculus of Constructions:\n\n\nNote that Booleans and Naturals are defined in the same way as in Church encoding. However additional problems raise from propositional extensionality and proof irrelevance .\n\n\n"}
{"id": "7844595", "url": "https://en.wikipedia.org/wiki?curid=7844595", "title": "Center-of-momentum frame", "text": "Center-of-momentum frame\n\nIn physics, the center-of-momentum frame (also zero-momentum frame or COM frame) of a system is the unique (up to velocity but not origin) inertial frame in which the total momentum of the system vanishes. The \"center of momentum\" of a system is not a location (but a collection of relative momenta/velocities). Thus \"center of momentum\" means \"center-of-momentum frame\" and is a short form of this phrase.\n\nA special case of the center-of-momentum frame is the center-of-mass frame: an inertial frame in which the center of mass (which is a physical point) remains at the origin. In all COM frames, the center of mass is at rest, but it is not necessarily at the origin of the coordinate system.\n\nIn special relativity, the COM frame is necessarily unique only when the system is isolated.\n\nThe center of momentum frame is defined as the inertial frame in which the sum over the linear momentum of each particle vanishes. Let \"S\" denote the laboratory reference system and \"S\"′ denote the center-of-momentum reference frame. Using a galilean transformation, the particle velocity in \"S\"′ is\n\nformula_1\n\nwhere \nformula_2\n\nis the velocity of the mass center. The total momentum in the center-of-momentum system then vanishes:\n\nformula_3\n\nAlso, the total energy of the system is the \"minimal energy\" as seen from all inertial reference frames.\n\nIn relativity, COM frame exists for an isolated massive system. This is a consequence of . In the COM frame the total energy of the system is the \"rest energy\", and this quantity (when divided by the factor \"c\", where \"c\" is the speed of light) gives the rest mass (invariant mass) of the system:\n\nThe invariant mass of the system is given in any inertial frame by the relativistic invariant relation\n\nbut for zero momentum the momentum term (\"p\"/\"c\") vanishes and thus the total energy coincides with the rest energy.\n\nSystems that have nonzero energy but zero rest mass (such as photons moving in a single direction, or equivalently, plane electromagnetic waves) do not have COM frames, because there is no frame in which they have zero net momentum. Due to the invariance of the speed of light, such massless systems must travel at the speed of light in any frame, and therefore always possess a net momentum magnitude that is equal to their energy divided by the speed of light:\n\nAn example of the usage of this frame is given below – in a two-body collision, not necessarily elastic (where \"kinetic energy\" is conserved). The COM frame can be used to find the momentum of the particles much easier than in a lab frame: the frame where the measurement or calculation is done. The situation is analyzed using Galilean transformations and conservation of momentum (for generality, rather than kinetic energies alone), for two particles of mass \"m\" and \"m\", moving at initial velocities (before collision) u and u respectively. The transformations are applied to take the velocity of the frame from the velocity of each particle from the lab frame (unprimed quantities) to the COM frame (primed quantities):\n\nwhere V is the velocity of the COM frame. Since V is the velocity of the COM, i.e. the time derivative of the COM location R (position of the center of mass of the system):\n\n\\frac\n"}
{"id": "19365282", "url": "https://en.wikipedia.org/wiki?curid=19365282", "title": "Cluster diagram", "text": "Cluster diagram\n\nA Cluster diagram or \"clustering diagram\" is a general type of diagram, which represents some kind of cluster. A cluster in general is a group or bunch of several discrete items that are close to each other.\n\nThe cluster diagram figures a cluster, such as a network diagram figures a network, a flow diagram a process or movement of objects, and a tree diagram an abstract tree. But all these diagrams can be considered interconnected: A network diagram can be seen as a special orderly arranged kind of cluster diagram. A cluster diagram is a mesh kind of network diagram. A flow diagram can be seen as a line type of network diagram, and a tree diagram a tree type of network diagram.\n\nSpecific types of cluster diagrams are:\n\n<br>\n\n\n<br>\n\n\n<br>\n\n\n\n"}
{"id": "35993402", "url": "https://en.wikipedia.org/wiki?curid=35993402", "title": "Conceptual space", "text": "Conceptual space\n\nA conceptual space is a geometric structure that represents a number of quality dimensions, which denote basic features by which concepts and objects can be compared, such as weight, color, taste, temperature, pitch, and the three ordinary spatial dimensions. In a conceptual space, \"points\" denote objects, and \"regions\" denote concepts. The theory of conceptual spaces is a theory about concept learning first proposed by Peter Gärdenfors. It is motivated by notions such as conceptual similarity and prototype theory.\n\nThe theory also puts forward the notion that \"natural\" categories are convex regions in conceptual spaces. In that if formula_1 and formula_2 are elements of a category, and if formula_3 is between formula_1 and formula_2, then formula_3 is also likely to belong to the category. The notion of concept convexity allow the interpretation of the focal points of regions as category prototypes. In the more general formulations of the theory, concepts are defined in terms conceptual similarity to their prototypes.\n\n"}
{"id": "412185", "url": "https://en.wikipedia.org/wiki?curid=412185", "title": "Critical rationalism", "text": "Critical rationalism\n\nCritical rationalism is an epistemological philosophy advanced by Karl Popper. Popper wrote about critical rationalism in his works: \"The Logic of Scientific Discovery\", \"The Open Society and its Enemies\", \"Conjectures and Refutations\", \"The Myth of the Framework\", and \"Unended Quest\". Ernest Gellner is another notable proponent of this approach.\n\nCritical rationalists hold that scientific theories and any other claims to knowledge can and should be rationally criticized, and (if they have empirical content) can and should be subjected to tests which may falsify them. Thus claims to knowledge may be contrastingly and normatively evaluated. They are either falsifiable and thus empirical (in a very broad sense), or not falsifiable and thus non-empirical. Those claims to knowledge that are potentially falsifiable can then be admitted to the body of empirical science, and then further differentiated according to whether they are retained or are later actually falsified. If retained, further differentiation may be made on the basis of how much subjection to criticism they have received, how severe such criticism has been, and how probable the theory is, with the least probable theory that still withstands attempts to falsify it being the one to be preferred. That it is the \"least\" probable theory that is to be preferred is one of the contrasting differences between critical rationalism and classical views on science, such as positivism, who hold that one should instead accept the \"most\" probable theory. (The least probable theory is the one with the highest information content and most open to future falsification.)\nCritical Rationalism as a discourse positioned itself against what its proponents took to be epistemologically relativist philosophies, particularly post-modernist or sociological approaches to knowledge. Critical rationalism has it that knowledge is objective (in the sense of being embodied in various substrates and in the sense of not being reducible to what humans individually \"know\"), and also that truth is objective (exists independently of social mediation or individual perception, but is \"really real\").\n\nHowever, this contrastive, critical approach to objective knowledge is quite different from more traditional views that also hold knowledge to be objective. (These include the classical rationalism of the Enlightenment, the verificationism of the logical positivists, or approaches to science based on induction, a supposed form of logical inference which critical rationalists reject, in line with David Hume.) For criticism is all that can be done when attempting to differentiate claims to knowledge, according to the critical rationalist. Reason is the organon of criticism, not of support; of tentative refutation, not of proof.\n\nSupposed positive evidence (such as the provision of \"good reasons\" for a claim, or its having been \"corroborated\" by making successful predictions) actually does nothing to bolster, support, or prove a claim, belief, or theory.\n\nIn this sense, critical rationalism turns the normal understanding of a traditional rationalist, and a realist, on its head. Especially the view that a theory is better if it is less likely to be true is in direct opposition to the traditional positivistic view, which holds that one should seek for theories that have a high probability. Popper notes that this \"may illustrate Schopenhauer's remark that the solution of a problem often first looks like a paradox and later like a truism\". Even a highly unlikely theory that conflicts current observation (and is thus false, like \"all swans are white\") must be considered to be better than one which fits observations perfectly, but is highly probable (like \"all swans have a color\"). This insight is the crucial difference between naive falsificationism and critical rationalism. The lower probability theory is favoured by critical rationalism because the higher the informative content of a theory the lower will be its probability, for the more information a statement contains, the greater will be the number of ways in which it may turn out to be false. The rationale behind this is simply to make it as easy as possible to find out whether the theory is false so that it can be replaced by one that is closer to the truth. It is not meant as a concession to justificatory epistemology, like assuming a theory to be \"justifiable\" by asserting that it is highly unlikely and yet fits observation.\n\nCritical rationalism rejects the classical position that knowledge is justified true belief; it instead holds the exact opposite: That, in general, knowledge is unjustified untrue unbelief. It is unjustified because of the non-existence of good reasons. It is untrue, because it usually contains errors that sometimes remain unnoticed for hundreds of years. And it is not belief either, because scientific knowledge, or the knowledge needed to build a plane, is contained in no single person's mind. It is only available as the content of books.\n\nWilliam Warren Bartley compared critical rationalism to the very general philosophical approach to knowledge which he called justificationism, the view that scientific theories can be justified. Most justificationists do not know that they are justificationists. Justificationism is what Popper called a \"subjectivist\" view of truth, in which the question of whether some statement is true, is confused with the question of whether it can be justified (established, proven, verified, warranted, made well-founded, made reliable, grounded, supported, legitimated, based on evidence) in some way.\n\nAccording to Bartley, some justificationists are positive about this mistake. They are naïve rationalists, and thinking that their knowledge can indeed be founded, in principle, it may be deemed certain to some degree, and rational.\n\nOther justificationists are negative about these mistakes. They are epistemological relativists, and think (rightly, according to the critical rationalist) that you cannot \"find\" knowledge, that there is no source of epistemological absolutism. But they conclude (wrongly, according to the critical rationalist) that there is therefore no rationality, and no objective distinction to be made between the true and the false.\n\nBy dissolving justificationism itself, the critical rationalist (a proponent of non-justificationism) regards knowledge and rationality, reason and science, as neither foundational nor infallible, but nevertheless does not think we must therefore all be relativists. Knowledge and truth still exist, just not in the way we thought.\n\nThe rejection of \"positivist\" approaches to knowledge occurs due to various pitfalls that positivism falls into.\n\n1. The naïve empiricism of induction was shown to be illogical by Hume. A thousand observations of some event A coinciding with some event B does not allow one to logically infer that all A events coincide with B events. According to the critical rationalist, if there is a sense in which humans accrue knowledge positively by experience, it is only by pivoting observations off existing conjectural theories pertinent to the observations, or off underlying cognitive schemas which unconsciously handle perceptions and use them to generate new theories. But these new theories advanced in response to perceived particulars are not \"logically\" \"induced\" from them. These new theories may be wrong. The myth that we \"induce\" theories from particulars is persistent because \"when\" we do this we are often successful, but this is due to the advanced state of our evolved tendencies. If we were really \"inducting\" theories from particulars, it would be inductively logical to claim that the sun sets \"because\" I get up in the morning, or that all buses must have drivers in them (if you've never seen an empty bus).\n\n2. Popper and David Miller showed in 1983 that evidence supposed to partly support a hypothesis can, in fact, only be neutral to, or even be counter-supportive of the hypothesis.\n\n3. Related to the point above, David Miller, attacks the use of \"good reasons\" in general (including evidence supposed to support the excess content of a hypothesis). He argues that good reasons are neither attainable, nor even desirable. Basically, Miller asserts that all arguments purporting to give valid support for a claim are either circular or question-begging. That is, if one provides a valid deductive argument (an inference from premises to a conclusion) for a given claim, then the content of the claim must already be contained within the premises of the argument (if it is not, then the argument is ampliative and so is invalid). Therefore, the claim is already presupposed by the premises, and is no more \"supported\" than are the assumptions upon which the claim rests, i.e. begging the question.\n\n\n\n\n"}
{"id": "17216986", "url": "https://en.wikipedia.org/wiki?curid=17216986", "title": "Cyberformance", "text": "Cyberformance\n\nCyberformance refers to live theatrical performances in which remote participants are enabled to work together in real time through the medium of the internet, employing technologies such as chat applications or purpose-built, multiuser, real-time collaborative software (for example, UpStage, Visitors Studio, the Waterwheel Tap, MOOs, and other platforms). Cyberformance is also known as online performance, networked performance, telematic performance, and digital theatre; there is as yet no consensus on which term should be preferred, but cyberformance has the advantage of compactness. For example, it is commonly employed by users of the UpStage platform to designate a special type of Performance art activity taking place in a cyber-artistic environment.\n\nCyberformance can be created and presented entirely online, for a distributed online audience who participate via internet-connected computers anywhere in the world, or it can be presented to a proximal audience (such as in a physical theatre or gallery venue) with some or all of the performers appearing via the internet; or it can be a hybrid of the two approaches, with both remote and proximal audiences and/or performers.\n\nThe term 'cyberformance' (a portmanteau word blending 'cyberspace' with 'performance') was coined by the net artist and curator Helen Varley Jamieson. She states that the invention of this term in 2000 \"came out of the need to find a word that avoided the polarisation of virtual and real, and the need for a new term (rather than 'online performance' or 'virtual theatre') for a new genre\". Jamieson traces the history of cyberformance back to the \"Satellite Arts Project\" of 1977, when interactive art pioneers Kit Galloway and Sherrie Rabinowitz used live video mixing to create what they called \"a performance space with no geographic boundaries\".\n\nOnline performances or virtual theatre has taken place in a number of the virtual environments that have emerged since the 1980s, including the multi-user virtual environments known as MUDs and MOOs in the 1970s, internet chat spaces (e.g. Internet Relay Chat, or IRC) in the 1980s, the Palace graphical chatroom in the 1990s, and UpStage, Visitors Studio, Second Life, Waterwheel Tap and other platforms in the 2000s. Notable cyberformance groups and projects thus far include:\n\n\nCyberformance differs from digital performance, which refers to any kind of digitally mediated performance, including those with no significant networked element. In some cases cyberformance may be considered a subset of net art; however, many cyberformance artists use what is termed 'mixed reality' or 'mixed space' for their work, linking physical, virtual, and cyber spaces in manifold ingenious ways. The internet is often a subject and inspiration of the work as well as being the central enabling technology.\n\nCyberformers often work with the dual identities afforded by avatars, exploiting the gap between online persona and offline self. They can also take advantage of the ease of switching between avatars in a way unavailable to 'proximal' actors. However cyberformance has its own unique problems, including unstable technology and \"real life\" interruptions.\n\n\n"}
{"id": "1316878", "url": "https://en.wikipedia.org/wiki?curid=1316878", "title": "Destructive dilemma", "text": "Destructive dilemma\n\nDestructive dilemma is the name of a valid rule of inference of propositional logic. It is the inference that, if \"P\" implies \"Q\" and \"R\" implies \"S\" and either \"Q\" is false or \"S\" is false, then either \"P\" or \"R\" must be false. In sum, if two conditionals are true, but one of their consequents is false, then one of their antecedents has to be false. \"Destructive dilemma\" is the disjunctive version of \"modus tollens\". The disjunctive version of \"modus ponens\" is the constructive dilemma. The rule can be stated:\n\nwhere the rule is that wherever instances of \"formula_2\", \"formula_3\", and \"formula_4\" appear on lines of a proof, \"formula_5\" can be placed on a subsequent line.\n\nThe \"destructive dilemma\" rule may be written in sequent notation:\n\nwhere formula_7 is a metalogical symbol meaning that formula_8 is a syntactic consequence of formula_2, formula_3, and formula_4 in some logical system;\n\nand expressed as a truth-functional tautology or theorem of propositional logic:\n\nwhere formula_13, formula_14, formula_15 and formula_16 are propositions expressed in some formal system.\n\nThe validity of this argument structure can be shown by using both conditional proof (CP) and reductio ad absurdum (RAA) in the following way:\n\n\n"}
{"id": "5884870", "url": "https://en.wikipedia.org/wiki?curid=5884870", "title": "Detention (imprisonment)", "text": "Detention (imprisonment)\n\nDetention is the process whereby a state or private citizen lawfully holds a person by removing his or her freedom or liberty at that time. This can be due to (pending) criminal charges preferred against the individual pursuant to a prosecution or to protect a person or property. Being detained does not always result in being taken to a particular area (generally called a detention centre), either for interrogation or as punishment for a crime (see prison).\n\nThe term can also be used in reference to the holding of property for the same reasons. The process of detainment may or may not have been preceded or followed with an arrest. \n\nDetainee is a term used by certain governments and their armed forces to refer to individuals held in custody, such as those it does not classify and treat as either prisoners of war or suspects in criminal cases. It is used to refer to \"any person captured or otherwise detained by an armed force.\" More generally, it means \"someone held in custody.\" The prisoners in Guantánamo Bay are referred to as \"detainees\".\n\nArticle 9 of the Universal Declaration of Human Rights provides that \"[n]o one shall be subjected to arbitrary arrest, detention or exile.\" In wars between nations, treatment of detainees is governed by the provisions of the Fourth Geneva Convention.\n\nAny form of imprisonment where a person's freedom of liberty is removed can be classed as detention, although the term is often associated with persons who are being held without warrant or charge before any have been raised. Being detained for the purposes of a drugs search is tantamount to a temporary arrest, as it is not yet known whether charges can be brought against an individual, pending the outcome of the search. The term 'detained' often refers to the \"immediacy\" when someone has their liberty deprived, often before an arrest or pre-arrest procedure has yet been followed. For example, a shoplifter being pursued and restrained, but not yet informed he/she is under arrest or read his rights would be classed as 'detained'.\n\nThe detention of suspects is the process of keeping a person who has been arrested in a police-cell, remand prison or other detention centre before trial or sentencing.\n\nThe length of detention of suspected terrorists, with the justification of taking an action that would aid counter-terrorism, varies according to country or situation, as well as the laws which regulate it.\n\nThe Terrorism Act 2006 in the United Kingdom lengthened the 14-day limit for detention without an arrest warrant or an indictment from the Terrorism Act 2000 to 28 days. A controversial Government proposal for an extension to 90 days was rejected by the House of Commons. English criminal law requires the detainer/arrestor to have reasonable grounds to suspect (reasonable suspicion) when detaining (or arresting) someone.\n\nIndefinite detention of an individual occurs frequently in wartime under the laws of war. This has been applied notably by the United States after the September 11, 2001 attacks. Before the Combatant Status Review Tribunals, created for reviewing the status of the Guantanamo detainees, the United States has argued that it is engaged in a legally recognizable armed conflict to which the laws of war apply, and that it therefore may hold captured al Qaeda and Taliban operatives throughout the duration of that conflict, without granting them a criminal trial.\n\nThe U.S. military regulates treatment of detainees in the manual , last revised in 1997.\n\nThe term \"unlawful combatant\" came into public awareness during and after the War in Afghanistan (2001–present), as the U.S. detained members of the Taliban and al-Qaeda captured in that war, and determined them to be unlawful combatants. This had generated considerable debate around the globe. The U.S. government refers to these captured enemy combatants as \"detainees\" because they did not qualify as prisoners of war under the definition found in the Geneva Conventions.\n\nUnder the Obama administration the term enemy combatants was also removed from the lexicon and further defined under the 2010 Defense Omnibus Bill:\n\nSection 948b. Military commissions generally: (a) Purpose-This chapter establishes procedures governing the use of military commissions to try alien unprivileged enemy belligerents for violations of the law of war and other offenses triable by military commission.\n\n\n\n"}
{"id": "2867114", "url": "https://en.wikipedia.org/wiki?curid=2867114", "title": "Doreen Kimura", "text": "Doreen Kimura\n\nDoreen Kimura (born Doreen Goebel 1933 in Winnipeg, Manitoba – February 27, 2013) was a Canadian psychologist who was professor at the University of Western Ontario and professor emeritus at Simon Fraser University. She was the founding president of the Society for Academic Freedom and Scholarship.\n\nKimura held a PhD in psychobiology. Her interests included the relationship between sex and cognition (see sex and intelligence) and promoting academic freedom.\n\nWhile some criticized Lawrence Summers' claims that differences in male-female representation in the sciences could be due to innate ability, Kimura supported him. She was a critic of affirmative action, arguing that it is demeaning to women. She also supported the concept of the biological origin of differences in cognitive ability between males and females (see also nature versus nurture).\n\nAccording to the CISG's (Canadian Inter-Organizational Steering Group for Speech-Language Pathology and Audiology) 'Canadian Guidelines on Auditory Processing Disorder in Children and Adults: Assessment and Intervention' (December 2012), \"In 1961, Doreen Kimura proposed a theory that would attempt to explain dichotic listening abilities in humans. As a testament to her theory, her views on dichotic processing of auditory information recently celebrated a 50th anniversary.\"\n\nKimura was the mother of Charlotte Thistle, grandmother of Ella Archer, and sister of Shelagh Derouin and Amber Harvey.\n\n\n"}
{"id": "11274", "url": "https://en.wikipedia.org/wiki?curid=11274", "title": "Elementary particle", "text": "Elementary particle\n\nIn particle physics, an elementary particle or fundamental particle is a with no substructure, thus not composed of other particles. Particles currently thought to be elementary include the fundamental fermions (quarks, leptons, antiquarks, and antileptons), which generally are \"matter particles\" and \"antimatter particles\", as well as the fundamental bosons (gauge bosons and the Higgs boson), which generally are \"force particles\" that mediate interactions among fermions. A particle containing two or more elementary particles is a \"composite particle\".\n\nEveryday matter is composed of atoms, once presumed to be matter's elementary particles—\"atom\" meaning \"unable to cut\" in Greek—although the atom's existence remained controversial until about 1910, as some leading physicists regarded molecules as mathematical illusions, and matter as ultimately composed of energy. Soon, subatomic constituents of the atom were identified. As the 1930s opened, the electron and the proton had been observed, along with the photon, the particle of electromagnetic radiation. At that time, the recent advent of quantum mechanics was radically altering the conception of particles, as a single particle could seemingly span a field as would a wave, a paradox still eluding satisfactory explanation.\n\nVia quantum theory, protons and neutrons were found to contain quarks—up quarks and down quarks—now considered elementary particles. And within a molecule, the electron's three degrees of freedom (charge, spin, orbital) can separate via the wavefunction into three quasiparticles (holon, spinon, orbiton). Yet a free electron—which is not orbiting an atomic nucleus and lacks orbital motion—appears unsplittable and remains regarded as an elementary particle.\n\nAround 1980, an elementary particle's status as indeed elementary—an \"ultimate constituent\" of substance—was mostly discarded for a more practical outlook, embodied in particle physics' Standard Model, what's known as science's most experimentally successful theory. Many elaborations upon and theories beyond the Standard Model, including the popular supersymmetry, double the number of elementary particles by hypothesizing that each known particle associates with a \"shadow\" partner far more massive, although all such superpartners remain undiscovered. Meanwhile, an elementary boson mediating gravitation—the graviton—remains hypothetical.\n\nAll elementary particles are—depending on their \"spin\"—either bosons or fermions. These are differentiated via the spin–statistics theorem of quantum statistics. Particles of \"half-integer\" spin exhibit Fermi–Dirac statistics and are fermions. Particles of \"integer\" spin, in other words full-integer, exhibit Bose–Einstein statistics and are bosons.\n\nIn the Standard Model, elementary particles are represented for predictive utility as point particles. Though extremely successful, the Standard Model is limited to the microcosm by its omission of gravitation and has some parameters arbitrarily added but unexplained.\n\nAccording to the current models of big bang nucleosynthesis, the primordial composition of visible matter of the universe should be about 75% hydrogen and 25% helium-4 (in mass). Neutrons are made up of one up and two down quarks, while protons are made of two up and one down quark. Since the other common elementary particles (such as electrons, neutrinos, or weak bosons) are so light or so rare when compared to atomic nuclei, we can neglect their mass contribution to the observable universe's total mass. Therefore, one can conclude that most of the visible mass of the universe consists of protons and neutrons, which, like all baryons, in turn consist of up quarks and down quarks.\n\nSome estimates imply that there are roughly baryons (almost entirely protons and neutrons) in the observable universe.\n\nThe number of protons in the observable universe is called the Eddington number.\n\nIn terms of number of particles, some estimates imply that nearly all the matter, excluding dark matter, occurs in neutrinos, and that roughly elementary particles of matter exist in the visible universe, mostly neutrinos. Other estimates imply that roughly elementary particles exist in the visible universe (not including dark matter), mostly photons and other massless force carriers.\n\nThe Standard Model of particle physics contains 12 flavors of elementary fermions, plus their corresponding antiparticles, as well as elementary bosons that mediate the forces and the Higgs boson, which was reported on July 4, 2012, as having been likely detected by the two main experiments at the Large Hadron Collider (ATLAS and CMS). However, the Standard Model is widely considered to be a provisional theory rather than a truly fundamental one, since it is not known if it is compatible with Einstein's general relativity. There may be hypothetical elementary particles not described by the Standard Model, such as the graviton, the particle that would carry the gravitational force, and sparticles, supersymmetric partners of the ordinary particles.\n\nThe 12 fundamental fermions are divided into 3 generations of 4 particles each. Half of the fermions are leptons, three of which have an electric charge of −1, called the electron (), the muon (), and the tau (), the other three leptons are neutrinos (, , ), which are the only elementary fermions with no electric or color charge. The remaining six particles are quarks (discussed below).\n\nThe following table lists current measured masses and mass estimates for all the fermions, using the same scale of measure: millions of electron-volts (MeV). For example, the most accurately known quark mass is of the top quark () at 172.7 GeV/c² or 172 700 MeV/c², estimated using the On-shell scheme.\n\nEstimates of the values of quark masses depend on the version of quantum chromodynamics used to describe quark interactions. Quarks are always confined in an envelope of gluons which confer vastly greater mass to the mesons and baryons where quarks occur, so values for quark masses cannot be measured directly. Since their masses are so small compared to the effective mass of the surrounding gluons, slight differences in the calculation make large differences in the masses.\n\nThere are also 12 fundamental fermionic antiparticles that correspond to these 12 particles. For example, the antielectron (positron) \"\" is the electron's antiparticle and has an electric charge of +1.\n\nIsolated quarks and antiquarks have never been detected, a fact explained by confinement. Every quark carries one of three color charges of the strong interaction; antiquarks similarly carry anticolor. Color-charged particles interact via gluon exchange in the same way that charged particles interact via photon exchange. However, gluons are themselves color-charged, resulting in an amplification of the strong force as color-charged particles are separated. Unlike the electromagnetic force, which diminishes as charged particles separate, color-charged particles feel increasing force.\n\nHowever, color-charged particles may combine to form color neutral composite particles called hadrons. A quark may pair up with an antiquark: the quark has a color and the antiquark has the corresponding anticolor. The color and anticolor cancel out, forming a color neutral meson. Alternatively, three quarks can exist together, one quark being \"red\", another \"blue\", another \"green\". These three colored quarks together form a color-neutral baryon. Symmetrically, three antiquarks with the colors \"antired\", \"antiblue\" and \"antigreen\" can form a color-neutral antibaryon.\n\nQuarks also carry fractional electric charges, but, since they are confined within hadrons whose charges are all integral, fractional charges have never been isolated. Note that quarks have electric charges of either + or −, whereas antiquarks have corresponding electric charges of either − or +.\n\nEvidence for the existence of quarks comes from deep inelastic scattering: firing electrons at nuclei to determine the distribution of charge within nucleons (which are baryons). If the charge is uniform, the electric field around the proton should be uniform and the electron should scatter elastically. Low-energy electrons do scatter in this way, but, above a particular energy, the protons deflect some electrons through large angles. The recoiling electron has much less energy and a jet of particles is emitted. This inelastic scattering suggests that the charge in the proton is not uniform but split among smaller charged particles: quarks.\n\nIn the Standard Model, vector (spin-1) bosons (gluons, photons, and the W and Z bosons) mediate forces, whereas the Higgs boson (spin-0) is responsible for the intrinsic mass of particles. Bosons differ from fermions in the fact that multiple bosons can occupy the same quantum state (Pauli exclusion principle). Also, bosons can be either elementary, like photons, or a combination, like mesons. The spin of bosons are integers instead of half integers.\n\nGluons mediate the strong interaction, which join quarks and thereby form hadrons, which are either baryons (three quarks) or mesons (one quark and one antiquark). Protons and neutrons are baryons, joined by gluons to form the atomic nucleus. Like quarks, gluons exhibit color and anticolor—unrelated to the concept of visual color—sometimes in combinations, altogether eight variations of gluons.\n\nThere are three weak gauge bosons: \"W\", \"W\", and \"Z\"; these mediate the weak interaction. The W bosons are known for their mediation in nuclear decay. The \"W\" converts a neutron into a proton then decay into an electron and electron antineutrino pair. The \"Z\" does not convert charge but rather changes momentum and is the only mechanism for elastically scattering neutrinos. The weak gauge bosons were discovered due to momentum change in electrons from neutrino-Z exchange. The massless photon mediates the electromagnetic interaction. These four gauge bosons form the electroweak interaction among elementary particles.\n\nAlthough the weak and electromagnetic forces appear quite different to us at everyday energies, the two forces are theorized to unify as a single electroweak force at high energies. This prediction was clearly confirmed by measurements of cross-sections for high-energy electron-proton scattering at the HERA collider at DESY. The differences at low energies is a consequence of the high masses of the \"W\" and \"Z\" bosons, which in turn are a consequence of the Higgs mechanism. Through the process of spontaneous symmetry breaking, the Higgs selects a special direction in electroweak space that causes three electroweak particles to become very heavy (the weak bosons) and one to remain massless (the photon). On 4 July 2012, after many years of experimentally searching for evidence of its existence, the Higgs boson was announced to have been observed at CERN's Large Hadron Collider. Peter Higgs who first posited the existence of the Higgs boson was present at the announcement. The Higgs boson is believed to have a mass of approximately 125 GeV. The statistical significance of this discovery was reported as 5-sigma, which implies a certainty of roughly 99.99994%. In particle physics, this is the level of significance required to officially label experimental observations as a discovery. Research into the properties of the newly discovered particle continues.\n\nThe graviton is a hypothetical elementary spin-2 particle proposed to mediate gravitation. While it remains undiscovered due to the difficulty inherent in its detection, it is sometimes included in tables of elementary particles. The conventional graviton is massless, although there exist models containing massive Kaluza–Klein gravitons.\n\nAlthough experimental evidence overwhelmingly confirms the predictions derived from the Standard Model, some of its parameters were added arbitrarily, not determined by a particular explanation, which remain mysteries, for instance the hierarchy problem. Theories beyond the Standard Model attempt to resolve these shortcomings.\n\nOne extension of the Standard Model attempts to combine the electroweak interaction with the strong interaction into a single 'grand unified theory' (GUT). Such a force would be spontaneously broken into the three forces by a Higgs-like mechanism. The most dramatic prediction of grand unification is the existence of X and Y bosons, which cause proton decay. However, the non-observation of proton decay at the Super-Kamiokande neutrino observatory rules out the simplest GUTs, including SU(5) and SO(10).\n\nSupersymmetry extends the Standard Model by adding another class of symmetries to the Lagrangian. These symmetries exchange fermionic particles with bosonic ones. Such a symmetry predicts the existence of supersymmetric particles, abbreviated as \"sparticles\", which include the sleptons, squarks, neutralinos, and charginos. Each particle in the Standard Model would have a superpartner whose spin differs by from the ordinary particle. Due to the breaking of supersymmetry, the sparticles are much heavier than their ordinary counterparts; they are so heavy that existing particle colliders would not be powerful enough to produce them. However, some physicists believe that sparticles will be detected by the Large Hadron Collider at CERN.\n\nString theory is a model of physics where all \"particles\" that make up matter are composed of strings (measuring at the Planck length) that exist in an 11-dimensional (according to M-theory, the leading version) or 12-dimensional (according to F-theory) universe. These strings vibrate at different frequencies that determine mass, electric charge, color charge, and spin. A string can be open (a line) or closed in a loop (a one-dimensional sphere, like a circle). As a string moves through space it sweeps out something called a \"world sheet\". String theory predicts 1- to 10-branes (a 1-brane being a string and a 10-brane being a 10-dimensional object) that prevent tears in the \"fabric\" of space using the uncertainty principle (e.g., the electron orbiting a hydrogen atom has the probability, albeit small, that it could be anywhere else in the universe at any given moment).\n\nString theory proposes that our universe is merely a 4-brane, inside which exist the 3 space dimensions and the 1 time dimension that we observe. The remaining 7 theoretical dimensions either are very tiny and curled up (and too small to be macroscopically accessible) or simply do not/cannot exist in our universe (because they exist in a grander scheme called the \"multiverse\" outside our known universe).\n\nSome predictions of the string theory include existence of extremely massive counterparts of ordinary particles due to vibrational excitations of the fundamental string and existence of a massless spin-2 particle behaving like the graviton.\n\nTechnicolor theories try to modify the Standard Model in a minimal way by introducing a new QCD-like interaction. This means one adds a new theory of so-called Techniquarks, interacting via so called Technigluons. The main idea is that the Higgs-Boson is not an elementary particle but a bound state of these objects.\n\nAccording to preon theory there are one or more orders of particles more fundamental than those (or most of those) found in the Standard Model. The most fundamental of these are normally called preons, which is derived from \"pre-quarks\". In essence, preon theory tries to do for the Standard Model what the Standard Model did for the particle zoo that came before it. Most models assume that almost everything in the Standard Model can be explained in terms of three to half a dozen more fundamental particles and the rules that govern their interactions. Interest in preons has waned since the simplest models were experimentally ruled out in the 1980s.\n\nAccelerons are the hypothetical subatomic particles that integrally link the newfound mass of the neutrino to the dark energy conjectured to be accelerating the expansion of the universe.\n\nIn theory, neutrinos are influenced by a new force resulting from their interactions with accelerons. Dark energy results as the universe tries to pull neutrinos apart.\n\n\n\nThe most important address about the current experimental and theoretical knowledge about elementary particle physics is the Particle Data Group, where different international institutions collect all experimental data and give short reviews over the contemporary theoretical understanding.\n\nother pages are:\n\n"}
{"id": "11617", "url": "https://en.wikipedia.org/wiki?curid=11617", "title": "Feynman diagram", "text": "Feynman diagram\n\nIn theoretical physics, Feynman diagrams are pictorial representations of the mathematical expressions describing the behavior of subatomic particles. The scheme is named after its inventor, American physicist Richard Feynman, and was first introduced in 1948. The interaction of sub-atomic particles can be complex and difficult to understand intuitively. Feynman diagrams give a simple visualization of what would otherwise be an arcane and abstract formula. As David Kaiser writes, \"since the middle of the 20th century, theoretical physicists have increasingly turned to this tool to help them undertake critical calculations\", and so \"Feynman diagrams have revolutionized nearly every aspect of theoretical physics\". While the diagrams are applied primarily to quantum field theory, they can also be used in other fields, such as solid-state theory.\n\nFeynman used Ernst Stueckelberg's interpretation of the positron as if it were an electron moving backward in time. Thus, antiparticles are represented as moving backward along the time axis in Feynman diagrams.\n\nThe calculation of probability amplitudes in theoretical particle physics requires the use of rather large and complicated integrals over a large number of variables. These integrals do, however, have a regular structure, and may be represented graphically as Feynman diagrams.\n\nA Feynman diagram is a contribution of a particular class of particle paths, which join and split as described by the diagram. More precisely, and technically, a Feynman diagram is a graphical representation of a perturbative contribution to the transition amplitude or correlation function of a quantum mechanical or statistical field theory. Within the canonical formulation of quantum field theory, a Feynman diagram represents a term in the Wick's expansion of the perturbative -matrix. Alternatively, the path integral formulation of quantum field theory represents the transition amplitude as a weighted sum of all possible histories of the system from the initial to the final state, in terms of either particles or fields. The transition amplitude is then given as the matrix element of the -matrix between the initial and the final states of the quantum system.\n\nWhen calculating scattering cross-sections in particle physics, the interaction between particles can be described by starting from a free field that describes the incoming and outgoing particles, and including an interaction Hamiltonian to describe how the particles deflect one another. The amplitude for scattering is the sum of each possible interaction history over all possible intermediate particle states. The number of times the interaction Hamiltonian acts is the order of the perturbation expansion, and the time-dependent perturbation theory for fields is known as the Dyson series. When the intermediate states at intermediate times are energy eigenstates (collections of particles with a definite momentum) the series is called old-fashioned perturbation theory.\n\nThe Dyson series can be alternatively rewritten as a sum over Feynman diagrams, where at each vertex both the energy and momentum are conserved, but where the length of the energy-momentum four-vector is not necessarily equal to the mass. The Feynman diagrams are much easier to keep track of than “old-fashioned” terms, because the old-fashioned way treats the particle and antiparticle contributions as separate. Each Feynman diagram is the sum of exponentially many old-fashioned terms, because each internal line can separately represent either a particle or an antiparticle. In a non-relativistic theory, there are no antiparticles and there is no doubling, so each Feynman diagram includes only one term.\n\nFeynman gave a prescription for calculating the amplitude (the Feynman rules, below) for any given diagram from a field theory Lagrangian. Each internal line corresponds to a factor of the virtual particle's propagator; each vertex where lines meet gives a factor derived from an interaction term in the Lagrangian, and incoming and outgoing lines carry an energy, momentum, and spin.\n\nIn addition to their value as a mathematical tool, Feynman diagrams provide deep physical insight into the nature of particle interactions. Particles interact in every way available; in fact, intermediate virtual particles are allowed to propagate faster than light. The probability of each final state is then obtained by summing over all such possibilities. This is closely tied to the functional integral formulation of quantum mechanics, also invented by Feynman—see path integral formulation.\n\nThe naïve application of such calculations often produces diagrams whose amplitudes are infinite, because the short-distance particle interactions require a careful limiting procedure, to include particle self-interactions. The technique of renormalization, suggested by Ernst Stueckelberg and Hans Bethe and implemented by Dyson, Feynman, Schwinger, and Tomonaga compensates for this effect and eliminates the troublesome infinities. After renormalization, calculations using Feynman diagrams match experimental results with very high accuracy.\n\nFeynman diagram and path integral methods are also used in statistical mechanics and can even be applied to classical mechanics.\n\nMurray Gell-Mann always referred to Feynman diagrams as Stueckelberg diagrams, after a Swiss physicist, Ernst Stueckelberg, who devised a similar notation many years earlier. Stueckelberg was motivated by the need for a manifestly covariant formalism for quantum field theory, but did not provide as automated a way to handle symmetry factors and loops, although he was first to find the correct physical interpretation in terms of forward and backward in time particle paths, all without the path-integral. \n\nHistorically, as a book-keeping device of covariant perturbation theory, the graphs were called Feynman–Dyson diagrams or Dyson graphs, because the path integral was unfamiliar when they were introduced, and Freeman Dyson's derivation from old-fashioned perturbation theory was easier to follow for physicists trained in earlier methods. Feynman had to lobby hard for the diagrams, which confused the establishment physicists trained in equations and graphs.\n\nIn their presentations of fundamental interactions, written from the particle physics perspective, Gerard 't Hooft and Martinus Veltman gave good arguments for taking the original, non-regularized Feynman diagrams as the most succinct representation of our present knowledge about the physics of quantum scattering of fundamental particles. Their motivations are consistent with the convictions of James Daniel Bjorken and Sidney Drell:\n\nThe Feynman graphs and rules of calculation summarize quantum field theory in a form in close contact with the experimental numbers one wants to understand. Although the statement of the theory in terms of graphs may imply perturbation theory, use of graphical methods in the many-body problem shows that this formalism is flexible enough to deal with phenomena of nonperturbative characters … Some modification of the Feynman rules of calculation may well outlive the elaborate mathematical structure of local canonical quantum field theory …\n\nSo far there are no opposing opinions. In quantum field theories the Feynman diagrams are obtained from Lagrangian by Feynman rules.\n\nDimensional regularization is a method for regularizing integrals in the evaluation of Feynman diagrams; it assigns values to them that are meromorphic functions of an auxiliary complex parameter , called the dimension. Dimensional regularization writes a Feynman integral as an integral depending on the spacetime dimension and spacetime points.\n\nA Feynman diagram is a representation of quantum field theory processes in terms of particle interactions. The particles are represented by the lines of the diagram, which can be squiggly or straight, with an arrow or without, depending on the type of particle. A point where lines connect to other lines is a \"vertex\", and this is where the particles meet and interact: by emitting or absorbing new particles, deflecting one another, or changing type.\n\nThere are three different types of lines: \"internal lines\" connect two vertices, \"incoming lines\" extend from \"the past\" to a vertex and represent an initial state, and \"outgoing lines\" extend from a vertex to \"the future\" and represent the final state (the latter two are also known as \"external lines\"). Traditionally, the bottom of the diagram is the past and the top the future; other times, the past is to the left and the future to the right. When calculating correlation functions instead of scattering amplitudes, there is no past and future and all the lines are internal. The particles then begin and end on little x's, which represent the positions of the operators whose correlation is being calculated.\n\nFeynman diagrams are a pictorial representation of a contribution to the total amplitude for a process that can happen in several different ways. When a group of incoming particles are to scatter off each other, the process can be thought of as one where the particles travel over all possible paths, including paths that go backward in time.\n\nFeynman diagrams are often confused with spacetime diagrams and bubble chamber images because they all describe particle scattering. Feynman diagrams are graphs that represent the interaction of particles rather than the physical position of the particle during a scattering process. Unlike a bubble chamber picture, only the sum of all the Feynman diagrams represent any given particle interaction; particles do not choose a particular diagram each time they interact. The law of summation is in accord with the principle of superposition—every diagram contributes to the total amplitude for the process.\n\nA Feynman diagram represents a perturbative contribution to the amplitude of a quantum transition from some initial quantum state to some final quantum state.\n\nFor example, in the process of electron-positron annihilation the initial state is one electron and one positron, the final state: two photons.\n\nThe initial state is often assumed to be at the left of the diagram and the final state at the right (although other conventions are also used quite often).\n\nA Feynman diagram consists of points, called vertices, and lines attached to the vertices.\n\nThe particles in the initial state are depicted by lines sticking out in the direction of the initial state (e.g., to the left), the particles in the final state are represented by lines sticking out in the direction of the final state (e.g., to the right).\n\nIn QED there are two types of particles: matter particles such as electrons or positrons (called fermions) and exchange particles (called gauge bosons). They are represented in Feynman diagrams as follows:\n\nIn QED a vertex always has three lines attached to it: one bosonic line, one fermionic line with arrow toward the vertex, and one fermionic line with arrow away from the vertex.\n\nThe vertices might be connected by a bosonic or fermionic propagator. A bosonic propagator is represented by a wavy line connecting two vertices (•~•). A fermionic propagator is represented by a solid line (with an arrow in one or another direction) connecting two vertices, (•←•).\n\nThe number of vertices gives the order of the term in the perturbation series expansion of the transition amplitude.\n\nThe electron–positron annihilation interaction:\n\nhas a contribution from the second order Feynman diagram shown adjacent:\n\nIn the initial state (at the bottom; early time) there is one electron (e) and one positron (e) and in the final state (at the top; late time) there are two photons (γ).\n\nThe probability amplitude for a transition of a quantum system from the initial state to the final state is given by the matrix element\n\nwhere is the -matrix.\n\nIn the canonical quantum field theory the -matrix is represented within the interaction picture by the perturbation series in the powers of the interaction Lagrangian,\n\nwhere is the interaction Lagrangian and signifies the time-ordered product of operators.\n\nA Feynman diagram is a graphical representation of a term in the Wick's expansion of the time-ordered product in the th order term of the -matrix,\n\nwhere signifies the normal-product of the operators and (±) takes care of the possible sign change when commuting the fermionic operators to bring them together for a contraction (a propagator).\n\nThe diagrams are drawn according to the Feynman rules, which depend upon the interaction Lagrangian. For the QED interaction Lagrangian\ndescribing the interaction of a fermionic field with a bosonic gauge field , the Feynman rules can be formulated in coordinate space as follows:\n\n\nThe second order perturbation term in the -matrix is\n\nThe Wick's expansion of the integrand gives (among others) the following term\n\nwhere\n\nis the electromagnetic contraction (propagator) in the Feynman gauge. This term is represented by the Feynman diagram at the right. This diagram gives contributions to the following processes:\n\nAnother interesting term in the expansion is\n\nwhere\n\nis the fermionic contraction (propagator).\n\nIn a path integral, the field Lagrangian, integrated over all possible field histories, defines the probability amplitude to go from one field configuration to another. In order to make sense, the field theory should have a well-defined ground state, and the integral should be performed a little bit rotated into imaginary time, i.e. a Wick rotation.\n\nA simple example is the free relativistic scalar field in dimensions, whose action integral is:\n\nThe probability amplitude for a process is:\n\nwhere and are space-like hypersurfaces that define the boundary conditions. The collection of all the on the starting hypersurface give the initial value of the field, analogous to the starting position for a point particle, and the field values at each point of the final hypersurface defines the final field value, which is allowed to vary, giving a different amplitude to end up at different values. This is the field-to-field transition amplitude.\n\nThe path integral gives the expectation value of operators between the initial and final state:\n\nand in the limit that A and B recede to the infinite past and the infinite future, the only contribution that matters is from the ground state (this is only rigorously true if the path-integral is defined slightly rotated into imaginary time). The path integral can be thought of as analogous to a probability distribution, and it is convenient to define it so that multiplying by a constant doesn't change anything:\n\nThe normalization factor on the bottom is called the \"partition function\" for the field, and it coincides with the statistical mechanical partition function at zero temperature when rotated into imaginary time.\n\nThe initial-to-final amplitudes are ill-defined if one thinks of the continuum limit right from the beginning, because the fluctuations in the field can become unbounded. So the path-integral can be thought of as on a discrete square lattice, with lattice spacing and the limit should be taken carefully. If the final results do not depend on the shape of the lattice or the value of , then the continuum limit exists.\n\nOn a lattice, (i), the field can be expanded in Fourier modes:\n\nHere the integration domain is over restricted to a cube of side length , so that large values of are not allowed. It is important to note that the -measure contains the factors of 2 from Fourier transforms, this is the best standard convention for -integrals in QFT. The lattice means that fluctuations at large are not allowed to contribute right away, they only start to contribute in the limit . Sometimes, instead of a lattice, the field modes are just cut off at high values of instead.\n\nIt is also convenient from time to time to consider the space-time volume to be finite, so that the modes are also a lattice. This is not strictly as necessary as the space-lattice limit, because interactions in are not localized, but it is convenient for keeping track of the factors in front of the -integrals and the momentum-conserving delta functions that will arise.\n\nOn a lattice, (ii), the action needs to be discretized:\n\nwhere is a pair of nearest lattice neighbors and . The discretization should be thought of as defining what the derivative means.\n\nIn terms of the lattice Fourier modes, the action can be written:\nFor near zero this is:\n\nNow we have the continuum Fourier transform of the original action. In finite volume, the quantity is not infinitesimal, but becomes the volume of a box made by neighboring Fourier modes, or .\n\nThe field is real-valued, so the Fourier transform obeys:\n\nIn terms of real and imaginary parts, the real part of is an even function of , while the imaginary part is odd. The Fourier transform avoids double-counting, so that it can be written:\n\nover an integration domain that integrates over each pair exactly once.\n\nFor a complex scalar field with action\n\nthe Fourier transform is unconstrained:\n\nand the integral is over all .\n\nIntegrating over all different values of is equivalent to integrating over all Fourier modes, because taking a Fourier transform is a unitary linear transformation of field coordinates. When you change coordinates in a multidimensional integral by a linear transformation, the value of the new integral is given by the determinant of the transformation matrix. If\n\nthen\n\nIf is a rotation, then\nso that , and the sign depends on whether the rotation includes a reflection or not.\n\nThe matrix that changes coordinates from to can be read off from the definition of a Fourier transform.\n\nand the Fourier inversion theorem tells you the inverse:\n\nwhich is the complex conjugate-transpose, up to factors of 2. On a finite volume lattice, the determinant is nonzero and independent of the field values.\n\nand the path integral is a separate factor at each value of .\n\nThe factor is the infinitesimal volume of a discrete cell in -space, in a square lattice box\nwhere is the side-length of the box. Each separate factor is an oscillatory Gaussian, and the width of the Gaussian diverges as the volume goes to infinity.\n\nIn imaginary time, the \"Euclidean action\" becomes positive definite, and can be interpreted as a probability distribution. The probability of a field having values is\n\nThe expectation value of the field is the statistical expectation value of the field when chosen according to the probability distribution:\n\nSince the probability of is a product, the value of at each separate value of is independently Gaussian distributed. The variance of the Gaussian is , which is formally infinite, but that just means that the fluctuations are unbounded in infinite volume. In any finite volume, the integral is replaced by a discrete sum, and the variance of the integral is .\n\nThe path integral defines a probabilistic algorithm to generate a Euclidean scalar field configuration. Randomly pick the real and imaginary parts of each Fourier mode at wavenumber to be a Gaussian random variable with variance . This generates a configuration at random, and the Fourier transform gives . For real scalar fields, the algorithm must generate only one of each pair , and make the second the complex conjugate of the first.\n\nTo find any correlation function, generate a field again and again by this procedure, and find the statistical average:\n\nwhere is the number of configurations, and the sum is of the product of the field values on each configuration. The Euclidean correlation function is just the same as the correlation function in statistics or statistical mechanics. The quantum mechanical correlation functions are an analytic continuation of the Euclidean correlation functions.\n\nFor free fields with a quadratic action, the probability distribution is a high-dimensional Gaussian, and the statistical average is given by an explicit formula. But the Monte Carlo method also works well for bosonic interacting field theories where there is no closed form for the correlation functions.\n\nEach mode is independently Gaussian distributed. The expectation of field modes is easy to calculate:\n\nfor , since then the two Gaussian random variables are independent and both have zero mean.\n\nin finite volume , when the two -values coincide, since this is the variance of the Gaussian. In the infinite volume limit,\n\nStrictly speaking, this is an approximation: the lattice propagator is:\n\nBut near , for field fluctuations long compared to the lattice spacing, the two forms coincide.\n\nIt is important to emphasize that the delta functions contain factors of 2, so that they cancel out the 2 factors in the measure for integrals.\n\nwhere is the ordinary one-dimensional Dirac delta function. This convention for delta-functions is not universal—some authors keep the factors of 2 in the delta functions (and in the -integration) explicit.\n\nThe form of the propagator can be more easily found by using the equation of motion for the field. From the Lagrangian, the equation of motion is:\n\nand in an expectation value, this says:\n\nWhere the derivatives act on , and the identity is true everywhere except when and coincide, and the operator order matters. The form of the singularity can be understood from the canonical commutation relations to be a delta-function. Defining the (Euclidean) \"Feynman propagator\" as the Fourier transform of the time-ordered two-point function (the one that comes from the path-integral):\n\nSo that:\n\nIf the equations of motion are linear, the propagator will always be the reciprocal of the quadratic-form matrix that defines the free Lagrangian, since this gives the equations of motion. This is also easy to see directly from the path integral. The factor of disappears in the Euclidean theory.\n\nBecause each field mode is an independent Gaussian, the expectation values for the product of many field modes obeys \"Wick's theorem\":\n\nis zero unless the field modes coincide in pairs. This means that it is zero for an odd number of , and for an even number of , it is equal to a contribution from each pair separately, with a delta function.\n\nwhere the sum is over each partition of the field modes into pairs, and the product is over the pairs. For example,\n\nAn interpretation of Wick's theorem is that each field insertion can be thought of as a dangling line, and the expectation value is calculated by linking up the lines in pairs, putting a delta function factor that ensures that the momentum of each partner in the pair is equal, and dividing by the propagator.\n\nThere is a subtle point left before Wick's theorem is proved—what if more than two of the phis have the same momentum? If it's an odd number, the integral is zero; negative values cancel with the positive values. But if the number is even, the integral is positive. The previous demonstration assumed that the phis would only match up in pairs.\n\nBut the theorem is correct even when arbitrarily many of the are equal, and this is a notable property of Gaussian integration:\n\nDividing by ,\n\nIf Wick's theorem were correct, the higher moments would be given by all possible pairings of a list of different :\n\nwhere the are all the same variable, the index is just to keep track of the number of ways to pair them. The first can be paired with others, leaving . The next unpaired can be paired with different leaving , and so on. This means that Wick's theorem, uncorrected, says that the expectation value of should be:\n\nand this is in fact the correct answer. So Wick's theorem holds no matter how many of the momenta of the internal variables coincide.\n\nInteractions are represented by higher order contributions, since quadratic contributions are always Gaussian. The simplest interaction is the quartic self-interaction, with an action:\n\nThe reason for the combinatorial factor 4! will be clear soon. Writing the action in terms of the lattice (or continuum) Fourier modes:\n\nWhere is the free action, whose correlation functions are given by Wick's theorem. The exponential of in the path integral can be expanded in powers of , giving a series of corrections to the free action.\n\nThe path integral for the interacting action is then a power series of corrections to the free action. The term represented by should be thought of as four half-lines, one for each factor of . The half-lines meet at a vertex, which contributes a delta-function that ensures that the sum of the momenta are all equal.\n\nTo compute a correlation function in the interacting theory, there is a contribution from the terms now. For example, the path-integral for the four-field correlator:\n\nwhich in the free field was only nonzero when the momenta were equal in pairs, is now nonzero for all values of . The momenta of the insertions can now match up with the momenta of the s in the expansion. The insertions should also be thought of as half-lines, four in this case, which carry a momentum , but one that is not integrated.\n\nThe lowest-order contribution comes from the first nontrivial term in the Taylor expansion of the action. Wick's theorem requires that the momenta in the half-lines, the factors in , should match up with the momenta of the external half-lines in pairs. The new contribution is equal to:\n\nThe 4! inside is canceled because there are exactly 4! ways to match the half-lines in to the external half-lines. Each of these different ways of matching the half-lines together in pairs contributes exactly once, regardless of the values of , by Wick's theorem.\n\nThe expansion of the action in powers of gives a series of terms with progressively higher number of s. The contribution from the term with exactly s is called th order.\n\nThe th order terms has:\n\nBy Wick's theorem, each pair of half-lines must be paired together to make a \"line\", and this line gives a factor of\n\nwhich multiplies the contribution. This means that the two half-lines that make a line are forced to have equal and opposite momentum. The line itself should be labelled by an arrow, drawn parallel to the line, and labeled by the momentum in the line . The half-line at the tail end of the arrow carries momentum , while the half-line at the head-end carries momentum . If one of the two half-lines is external, this kills the integral over the internal , since it forces the internal to be equal to the external . If both are internal, the integral over remains.\n\nThe diagrams that are formed by linking the half-lines in the s with the external half-lines, representing insertions, are the Feynman diagrams of this theory. Each line carries a factor of , the propagator, and either goes from vertex to vertex, or ends at an insertion. If it is internal, it is integrated over. At each vertex, the total incoming is equal to the total outgoing .\n\nThe number of ways of making a diagram by joining half-lines into lines almost completely cancels the factorial factors coming from the Taylor series of the exponential and the 4! at each vertex.\n\nA forest diagram is one where all the internal lines have momentum that is completely determined by the external lines and the condition that the incoming and outgoing momentum are equal at each vertex. The contribution of these diagrams is a product of propagators, without any integration. A tree diagram is a connected forest diagram.\n\nAn example of a tree diagram is the one where each of four external lines end on an . Another is when three external lines end on an , and the remaining half-line joins up with another , and the remaining half-lines of this run off to external lines. These are all also forest diagrams (as every tree is a forest); an example of a forest that is not a tree is when eight external lines end on two s.\n\nIt is easy to verify that in all these cases, the momenta on all the internal lines is determined by the external momenta and the condition of momentum conservation in each vertex.\n\nA diagram that is not a forest diagram is called a \"loop\" diagram, and an example is one where two lines of an are joined to external lines, while the remaining two lines are joined to each other. The two lines joined to each other can have any momentum at all, since they both enter and leave the same vertex. A more complicated example is one where two s are joined to each other by matching the legs one to the other. This diagram has no external lines at all.\n\nThe reason loop diagrams are called loop diagrams is because the number of -integrals that are left undetermined by momentum conservation is equal to the number of independent closed loops in the diagram, where independent loops are counted as in homology theory. The homology is real-valued (actually valued), the value associated with each line is the momentum. The boundary operator takes each line to the sum of the end-vertices with a positive sign at the head and a negative sign at the tail. The condition that the momentum is conserved is exactly the condition that the boundary of the -valued weighted graph is zero.\n\nA set of valid -values can be arbitrarily redefined whenever there is a closed loop. A closed loop is a cyclical path of adjacent vertices that never revisits the same vertex. Such a cycle can be thought of as the boundary of a hypothetical 2-cell. The -labellings of a graph that conserve momentum (i.e. which has zero boundary) up to redefinitions of (i.e. up to boundaries of 2-cells) define the first homology of a graph. The number of independent momenta that are not determined is then equal to the number of independent homology loops. For many graphs, this is equal to the number of loops as counted in the most intuitive way.\n\nThe number of ways to form a given Feynman diagram by joining together half-lines is large, and by Wick's theorem, each way of pairing up the half-lines contributes equally. Often, this completely cancels the factorials in the denominator of each term, but the cancellation is sometimes incomplete.\n\nThe uncancelled denominator is called the \"symmetry factor\" of the diagram. The contribution of each diagram to the correlation function must be divided by its symmetry factor.\n\nFor example, consider the Feynman diagram formed from two external lines joined to one , and the remaining two half-lines in the joined to each other. There are 4 × 3 ways to join the external half-lines to the , and then there is only one way to join the two remaining lines to each other. The comes divided by , but the number of ways to link up the half lines to make the diagram is only 4 × 3, so the contribution of this diagram is divided by two.\n\nFor another example, consider the diagram formed by joining all the half-lines of one to all the half-lines of another . This diagram is called a \"vacuum bubble\", because it does not link up to any external lines. There are 4! ways to form this diagram, but the denominator includes a 2! (from the expansion of the exponential, there are two s) and two factors of 4!. The contribution is multiplied by = .\n\nAnother example is the Feynman diagram formed from two s where each links up to two external lines, and the remaining two half-lines of each are joined to each other. The number of ways to link an to two external lines is 4 × 3, and either could link up to either pair, giving an additional factor of 2. The remaining two half-lines in the two s can be linked to each other in two ways, so that the total number of ways to form the diagram is , while the denominator is . The total symmetry factor is 2, and the contribution of this diagram is divided by 2.\n\nThe symmetry factor theorem gives the symmetry factor for a general diagram: the contribution of each Feynman diagram must be divided by the order of its group of automorphisms, the number of symmetries that it has.\n\nAn automorphism of a Feynman graph is a permutation of the lines and a permutation of the vertices with the following properties:\n\n\nThis theorem has an interpretation in terms of particle-paths: when identical particles are present, the integral over all intermediate particles must not double-count states that differ only by interchanging identical particles.\n\nProof: To prove this theorem, label all the internal and external lines of a diagram with a unique name. Then form the diagram by linking a half-line to a name and then to the other half line.\n\nNow count the number of ways to form the named diagram. Each permutation of the s gives a different pattern of linking names to half-lines, and this is a factor of . Each permutation of the half-lines in a single gives a factor of 4!. So a named diagram can be formed in exactly as many ways as the denominator of the Feynman expansion.\n\nBut the number of unnamed diagrams is smaller than the number of named diagram by the order of the automorphism group of the graph.\n\nRoughly speaking, a Feynman diagram is called \"connected\" if all vertices and propagator lines are linked by a sequence of vertices and propagators of the diagram itself. If one views it as an undirected graph it is connected. The remarkable relevance of such diagrams in QFTs is due to the fact that they are sufficient to determine the quantum partition function . More precisely, connected Feynman diagrams determine\n\nTo see this, one should recall that\n\nwith constructed from some (arbitrary) Feynman diagram that can be thought to consist of several connected components . If one encounters (identical) copies of a component within the Feynman diagram one has to include a \"symmetry factor\" . However, in the end each contribution of a Feynman diagram to the partition function has the generic form\n\nwhere labels the (infinitely) many connected Feynman diagrams possible.\n\nA scheme to successively create such contributions from the to is obtained by\n\nand therefore yields\n\nTo establish the \"normalization\" one simply calculates all connected \"vacuum diagrams\", i.e., the diagrams without any \"sources\" (sometimes referred to as \"external legs\" of a Feynman diagram).\n\nAn immediate consequence of the linked-cluster theorem is that all vacuum bubbles, diagrams without external lines, cancel when calculating correlation functions. A correlation function is given by a ratio of path-integrals:\n\nThe top is the sum over all Feynman diagrams, including disconnected diagrams that do not link up to external lines at all. In terms of the connected diagrams, the numerator includes the same contributions of vacuum bubbles as the denominator:\n\nWhere the sum over diagrams includes only those diagrams each of whose connected components end on at least one external line. The vacuum bubbles are the same whatever the external lines, and give an overall multiplicative factor. The denominator is the sum over all vacuum bubbles, and dividing gets rid of the second factor.\n\nThe vacuum bubbles then are only useful for determining itself, which from the definition of the path integral is equal to:\n\nwhere is the energy density in the vacuum. Each vacuum bubble contains a factor of zeroing the total at each vertex, and when there are no external lines, this contains a factor of , because the momentum conservation is over-enforced. In finite volume, this factor can be identified as the total volume of space time. Dividing by the volume, the remaining integral for the vacuum bubble has an interpretation: it is a contribution to the energy density of the vacuum.\n\nCorrelation functions are the sum of the connected Feynman diagrams, but the formalism treats the connected and disconnected diagrams differently. Internal lines end on vertices, while external lines go off to insertions. Introducing \"sources\" unifies the formalism, by making new vertices where one line can end.\n\nSources are external fields, fields that contribute to the action, but are not dynamical variables. A scalar field source is another scalar field that contributes a term to the (Lorentz) Lagrangian:\n\nIn the Feynman expansion, this contributes H terms with one half-line ending on a vertex. Lines in a Feynman diagram can now end either on an vertex, or on an vertex, and only one line enters an vertex. The Feynman rule for an vertex is that a line from an with momentum gets a factor of .\n\nThe sum of the connected diagrams in the presence of sources includes a term for each connected diagram in the absence of sources, except now the diagrams can end on the source. Traditionally, a source is represented by a little \"×\" with one line extending out, exactly as an insertion.\n\nwhere is the connected diagram with external lines carrying momentum as indicated. The sum is over all connected diagrams, as before.\n\nThe field is not dynamical, which means that there is no path integral over : is just a parameter in the Lagrangian, which varies from point to point. The path integral for the field is:\n\nand it is a function of the values of at every point. One way to interpret this expression is that it is taking the Fourier transform in field space. If there is a probability density on , the Fourier transform of the probability density is:\n\nThe Fourier transform is the expectation of an oscillatory exponential. The path integral in the presence of a source is:\n\nwhich, on a lattice, is the product of an oscillatory exponential for each field value:\n\nThe fourier transform of a delta-function is a constant, which gives a formal expression for a delta function:\n\nThis tells you what a field delta function looks like in a path-integral. For two scalar fields and ,\n\nwhich integrates over the Fourier transform coordinate, over . This expression is useful for formally changing field coordinates in the path integral, much as a delta function is used to change coordinates in an ordinary multi-dimensional integral.\n\nThe partition function is now a function of the field , and the physical partition function is the value when is the zero function:\n\nThe correlation functions are derivatives of the path integral with respect to the source:\n\nIn Euclidean space, source contributions to the action can still appear with a factor of , so that they still do a Fourier transform.\n\nThe field path integral can be extended to the Fermi case, but only if the notion of integration is expanded. A Grassmann integral of a free Fermi field is a high-dimensional determinant or Pfaffian, which defines the new type of Gaussian integration appropriate for Fermi fields.\n\nThe two fundamental formulas of Grassmann integration are:\n\nwhere is an arbitrary matrix and are independent Grassmann variables for each index , and\n\nwhere is an antisymmetric matrix, is a collection of Grassmann variables, and the is to prevent double-counting (since ).\n\nIn matrix notation, where and are Grassmann-valued row vectors, and are Grassmann-valued column vectors, and is a real-valued matrix:\n\nwhere the last equality is a consequence of the translation invariance of the Grassmann integral. The Grassmann variables are external sources for , and differentiating with respect to pulls down factors of .\n\nagain, in a schematic matrix notation. The meaning of the formula above is that the derivative with respect to the appropriate component of and gives the matrix element of . This is exactly analogous to the bosonic path integration formula for a Gaussian integral of a complex bosonic field:\n\nSo that the propagator is the inverse of the matrix in the quadratic part of the action in both the Bose and Fermi case.\n\nFor real Grassmann fields, for Majorana fermions, the path integral a Pfaffian times a source quadratic form, and the formulas give the square root of the determinant, just as they do for real Bosonic fields. The propagator is still the inverse of the quadratic part.\n\nThe free Dirac Lagrangian:\n\nformally gives the equations of motion and the anticommutation relations of the Dirac field, just as the Klein Gordon Lagrangian in an ordinary path integral gives the equations of motion and commutation relations of the scalar field. By using the spatial Fourier transform of the Dirac field as a new basis for the Grassmann algebra, the quadratic part of the Dirac action becomes simple to invert:\n\nThe propagator is the inverse of the matrix linking and , since different values of do not mix together.\n\nThe analog of Wick's theorem matches and in pairs:\n\nwhere S is the sign of the permutation that reorders the sequence of and to put the ones that are paired up to make the delta-functions next to each other, with the coming right before the . Since a pair is a commuting element of the Grassmann algebra, it doesn't matter what order the pairs are in. If more than one pair have the same , the integral is zero, and it is easy to check that the sum over pairings gives zero in this case (there are always an even number of them). This is the Grassmann analog of the higher Gaussian moments that completed the Bosonic Wick's theorem earlier.\n\nThe rules for spin- Dirac particles are as follows: The propagator is the inverse of the Dirac operator, the lines have arrows just as for a complex scalar field, and the diagram acquires an overall factor of −1 for each closed Fermi loop. If there are an odd number of Fermi loops, the diagram changes sign. Historically, the −1 rule was very difficult for Feynman to discover. He discovered it after a long process of trial and error, since he lacked a proper theory of Grassmann integration.\n\nThe rule follows from the observation that the number of Fermi lines at a vertex is always even. Each term in the Lagrangian must always be Bosonic. A Fermi loop is counted by following Fermionic lines until one comes back to the starting point, then removing those lines from the diagram. Repeating this process eventually erases all the Fermionic lines: this is the Euler algorithm to 2-color a graph, which works whenever each vertex has even degree. Note that the number of steps in the Euler algorithm is only equal to the number of independent Fermionic homology cycles in the common special case that all terms in the Lagrangian are exactly quadratic in the Fermi fields, so that each vertex has exactly two Fermionic lines. When there are four-Fermi interactions (like in the Fermi effective theory of the weak nuclear interactions) there are more -integrals than Fermi loops. In this case, the counting rule should apply the Euler algorithm by pairing up the Fermi lines at each vertex into pairs that together form a bosonic factor of the term in the Lagrangian, and when entering a vertex by one line, the algorithm should always leave with the partner line.\n\nTo clarify and prove the rule, consider a Feynman diagram formed from vertices, terms in the Lagrangian, with Fermion fields. The full term is Bosonic, it is a commuting element of the Grassmann algebra, so the order in which the vertices appear is not important. The Fermi lines are linked into loops, and when traversing the loop, one can reorder the vertex terms one after the other as one goes around without any sign cost. The exception is when you return to the starting point, and the final half-line must be joined with the unlinked first half-line. This requires one permutation to move the last to go in front of the first , and this gives the sign.\n\nThis rule is the only visible effect of the exclusion principle in internal lines. When there are external lines, the amplitudes are antisymmetric when two Fermi insertions for identical particles are interchanged. This is automatic in the source formalism, because the sources for Fermi fields are themselves Grassmann valued.\n\nThe naive propagator for photons is infinite, since the Lagrangian for the A-field is:\n\nThe quadratic form defining the propagator is non-invertible. The reason is the gauge invariance of the field; adding a gradient to does not change the physics.\n\nTo fix this problem, one needs to fix a gauge. The most convenient way is to demand that the divergence of is some function , whose value is random from point to point. It does no harm to integrate over the values of , since it only determines the choice of gauge. This procedure inserts the following factor into the path integral for :\n\nThe first factor, the delta function, fixes the gauge. The second factor sums over different values of that are inequivalent gauge fixings. This is simply\n\nThe additional contribution from gauge-fixing cancels the second half of the free Lagrangian, giving the Feynman Lagrangian:\n\nwhich is just like four independent free scalar fields, one for each component of . The Feynman propagator is:\n\nThe one difference is that the sign of one propagator is wrong in the Lorentz case: the timelike component has an opposite sign propagator. This means that these particle states have negative norm—they are not physical states. In the case of photons, it is easy to show by diagram methods that these states are not physical—their contribution cancels with longitudinal photons to only leave two physical photon polarization contributions for any value of .\n\nIf the averaging over is done with a coefficient different from , the two terms don't cancel completely. This gives a covariant Lagrangian with a coefficient formula_90, which does not affect anything:\n\nand the covariant propagator for QED is:\n\nTo find the Feynman rules for non-Abelian gauge fields, the procedure that performs the gauge fixing must be carefully corrected to account for a change of variables in the path-integral.\n\nThe gauge fixing factor has an extra determinant from popping the delta function:\n\nTo find the form of the determinant, consider first a simple two-dimensional integral of a function that depends only on , not on the angle . Inserting an integral over :\n\nThe derivative-factor ensures that popping the delta function in removes the integral. Exchanging the order of integration,\n\nbut now the delta-function can be popped in ,\n\nThe integral over just gives an overall factor of 2, while the rate of change of with a change in is just , so this exercise reproduces the standard formula for polar integration of a radial function:\n\nIn the path-integral for a nonabelian gauge field, the analogous manipulation is:\n\nThe factor in front is the volume of the gauge group, and it contributes a constant, which can be discarded. The remaining integral is over the gauge fixed action.\n\nTo get a covariant gauge, the gauge fixing condition is the same as in the Abelian case:\n\nWhose variation under an infinitesimal gauge transformation is given by:\n\nwhere is the adjoint valued element of the Lie algebra at every point that performs the infinitesimal gauge transformation. This adds the Faddeev Popov determinant to the action:\n\nwhich can be rewritten as a Grassmann integral by introducing ghost fields:\n\nThe determinant is independent of , so the path-integral over can give the Feynman propagator (or a covariant propagator) by choosing the measure for as in the abelian case. The full gauge fixed action is then the Yang Mills action in Feynman gauge with an additional ghost action:\n\nThe diagrams are derived from this action. The propagator for the spin-1 fields has the usual Feynman form. There are vertices of degree 3 with momentum factors whose couplings are the structure constants, and vertices of degree 4 whose couplings are products of structure constants. There are additional ghost loops, which cancel out timelike and longitudinal states in loops.\n\nIn the Abelian case, the determinant for covariant gauges does not depend on , so the ghosts do not contribute to the connected diagrams.\n\nFeynman diagrams were originally discovered by Feynman, by trial and error, as a way to represent the contribution to the S-matrix from different classes of particle trajectories.\n\nThe Euclidean scalar propagator has a suggestive representation:\n\nThe meaning of this identity (which is an elementary integration) is made clearer by Fourier transforming to real space.\n\nThe contribution at any one value of to the propagator is a Gaussian of width . The total propagation function from 0 to is a weighted sum over all proper times of a normalized Gaussian, the probability of ending up at after a random walk of time .\n\nThe path-integral representation for the propagator is then:\n\nwhich is a path-integral rewrite of the Schwinger representation.\n\nThe Schwinger representation is both useful for making manifest the particle aspect of the propagator, and for symmetrizing denominators of loop diagrams.\n\nThe Schwinger representation has an immediate practical application to loop diagrams. For example, for the diagram in the theory formed by joining two s together in two half-lines, and making the remaining lines external, the integral over the internal propagators in the loop is:\n\nHere one line carries momentum and the other . The asymmetry can be fixed by putting everything in the Schwinger representation.\n\nNow the exponent mostly depends on ,\n\nexcept for the asymmetrical little bit. Defining the variable and , the variable goes from 0 to , while goes from 0 to 1. The variable is the total proper time for the loop, while parametrizes the fraction of the proper time on the top of the loop versus the bottom.\n\nThe Jacobian for this transformation of variables is easy to work out from the identities:\n\nand \"wedging\" gives\n\nThis allows the integral to be evaluated explicitly:\n\nleaving only the -integral. This method, invented by Schwinger but usually attributed to Feynman, is called \"combining denominator\". Abstractly, it is the elementary identity:\n\nBut this form does not provide the physical motivation for introducing ; is the proportion of proper time on one of the legs of the loop.\n\nOnce the denominators are combined, a shift in to symmetrizes everything:\n\nThis form shows that the moment that is more negative than four times the mass of the particle in the loop, which happens in a physical region of Lorentz space, the integral has a cut. This is exactly when the external momentum can create physical particles.\n\nWhen the loop has more vertices, there are more denominators to combine:\n\nThe general rule follows from the Schwinger prescription for denominators:\n\nThe integral over the Schwinger parameters can be split up as before into an integral over the total proper time and an integral over the fraction of the proper time in all but the first segment of the loop for . The are positive and add up to less than 1, so that the integral is over an -dimensional simplex.\n\nThe Jacobian for the coordinate transformation can be worked out as before:\n\nWedging all these equations together, one obtains\n\nThis gives the integral:\n\nwhere the simplex is the region defined by the conditions\nas well as\nPerforming the integral gives the general prescription for combining denominators:\n\nSince the numerator of the integrand is not involved, the same prescription works for any loop, no matter what the spins are carried by the legs. The interpretation of the parameters is that they are the fraction of the total proper time spent on each leg.\n\nThe correlation functions of a quantum field theory describe the scattering of particles. The definition of \"particle\" in relativistic field theory is not self-evident, because if you try to determine the position so that the uncertainty is less than the compton wavelength, the uncertainty in energy is large enough to produce more particles and antiparticles of the same type from the vacuum. This means that the notion of a single-particle state is to some extent incompatible with the notion of an object localized in space.\n\nIn the 1930s, Wigner gave a mathematical definition for single-particle states: they are a collection of states that form an irreducible representation of the Poincaré group. Single particle states describe an object with a finite mass, a well defined momentum, and a spin. This definition is fine for protons and neutrons, electrons and photons, but it excludes quarks, which are permanently confined, so the modern point of view is more accommodating: a particle is anything whose interaction can be described in terms of Feynman diagrams, which have an interpretation as a sum over particle trajectories.\n\nA field operator can act to produce a one-particle state from the vacuum, which means that the field operator produces a superposition of Wigner particle states. In the free field theory, the field produces one particle states only. But when there are interactions, the field operator can also produce 3-particle, 5-particle (if there is no +/− symmetry also 2, 4, 6 particle) states too. To compute the scattering amplitude for single particle states only requires a careful limit, sending the fields to infinity and integrating over space to get rid of the higher-order corrections.\n\nThe relation between scattering and correlation functions is the LSZ-theorem: The scattering amplitude for particles to go to particles in a scattering event is the given by the sum of the Feynman diagrams that go into the correlation function for field insertions, leaving out the propagators for the external legs.\n\nFor example, for the interaction of the previous section, the order contribution to the (Lorentz) correlation function is:\n\nStripping off the external propagators, that is, removing the factors of , gives the invariant scattering amplitude :\n\nwhich is a constant, independent of the incoming and outgoing momentum. The interpretation of the scattering amplitude is that the sum of over all possible final states is the probability for the scattering event. The normalization of the single-particle states must be chosen carefully, however, to ensure that is a relativistic invariant.\n\nNon-relativistic single particle states are labeled by the momentum , and they are chosen to have the same norm at every value of . This is because the nonrelativistic unit operator on single particle states is:\n\nIn relativity, the integral over the -states for a particle of mass m integrates over a hyperbola in space defined by the energy–momentum relation:\n\nIf the integral weighs each point equally, the measure is not Lorentz-invariant. The invariant measure integrates over all values of and , restricting to the hyperbola with a Lorentz-invariant delta function:\n\nSo the normalized -states are different from the relativistically normalized -states by a factor of\n\nThe invariant amplitude is then the probability amplitude for relativistically normalized incoming states to become relativistically normalized outgoing states.\n\nFor nonrelativistic values of , the relativistic normalization is the same as the nonrelativistic normalization (up to a constant factor ). In this limit, the invariant scattering amplitude is still constant. The particles created by the field scatter in all directions with equal amplitude.\n\nThe nonrelativistic potential, which scatters in all directions with an equal amplitude (in the Born approximation), is one whose Fourier transform is constant—a delta-function potential. The lowest order scattering of the theory reveals the non-relativistic interpretation of this theory—it describes a collection of particles with a delta-function repulsion. Two such particles have an aversion to occupying the same point at the same time.\n\nThinking of Feynman diagrams as a perturbation series, nonperturbative effects like tunneling do not show up, because any effect that goes to zero faster than any polynomial does not affect the Taylor series. Even bound states are absent, since at any finite order particles are only exchanged a finite number of times, and to make a bound state, the binding force must last forever.\n\nBut this point of view is misleading, because the diagrams not only describe scattering, but they also are a representation of the short-distance field theory correlations. They encode not only asymptotic processes like particle scattering, they also describe the multiplication rules for fields, the operator product expansion. Nonperturbative tunneling processes involve field configurations that on average get big when the coupling constant gets small, but each configuration is a coherent superposition of particles whose local interactions are described by Feynman diagrams. When the coupling is small, these become collective processes that involve large numbers of particles, but where the interactions between each of the particles is simple.\n\nThis means that nonperturbative effects show up asymptotically in resummations of infinite classes of diagrams, and these diagrams can be locally simple. The graphs determine the local equations of motion, while the allowed large-scale configurations describe non-perturbative physics. But because Feynman propagators are nonlocal in time, translating a field process to a coherent particle language is not completely intuitive, and has only been explicitly worked out in certain special cases. In the case of nonrelativistic bound states, the Bethe–Salpeter equation describes the class of diagrams to include to describe a relativistic atom. For quantum chromodynamics, the Shifman Vainshtein Zakharov sum rules describe non-perturbatively excited long-wavelength field modes in particle language, but only in a phenomenological way.\n\nThe number of Feynman diagrams at high orders of perturbation theory is very large, because there are as many diagrams as there are graphs with a given number of nodes. Nonperturbative effects leave a signature on the way in which the number of diagrams and resummations diverge at high order. It is only because non-perturbative effects appear in hidden form in diagrams that it was possible to analyze nonperturbative effects in string theory, where in many cases a Feynman description is the only one available.\n\n\n\n\n"}
{"id": "42802325", "url": "https://en.wikipedia.org/wiki?curid=42802325", "title": "HHole", "text": "HHole\n\nHHole for Mannheim (2006-∞) is a permanent multimedia installation in the \"Athene-Trakt\" of the created by NatHalie Braun Barends.\n\nThe Kunsthalle is under reconstruction. Following a court decision of the Landgericht Mannheim, the installation will likely not be rebuilt afterwards.\n\nThe Landgericht Mannheim decided that the right of the owner of the building (even if against the signed loan contracts) overrides the copyright and the property of the artist, to allow the city to build the new €70 millions museum. It is to appreciate that the artist, in order to protect the fundamental right of artistic freedom, property right and copyright will go on to the next instance.\n\nHHole for Mannheim (2006-∞) has been conceived as a permanent, conceptual, site specific, developing artwork in progress.\nIt consists of seven holes which pass through the Athene Trakt that unites the old (Billing-Bau) and the new building (Mitzlaff-Bau) of the Mannheim Kunsthalle.\n\nThe natural light flows from the topmost hole above the museum rooftop, through the holes in the floors below, meeting the light projected upwards from a Gobo projector located at the lowest floor. At the rooftop, a custom designed laser light (also used by astronomers to point at stars), projects into the universe the artist's symbol of the HMap. The natural and the artificial lights meet again symbolically at the antipodes of the museum, close to New Zealand at 49° 28′ 56.68″ S, 171° 31′ 29.63″ E.\n\nSeven rooms contain the seven holes, starting from above: Infinite Room, Phoenix Room, Reflection Room, Silence Room, Treasure Room, Ground Room, and Earth Room.\n\nEach room hosts different installations, with videos, HHole specially designed acrylic furniture, waterfall, living tree, pool, and telephone to connect each level.\n\n"}
{"id": "43703721", "url": "https://en.wikipedia.org/wiki?curid=43703721", "title": "Higher-order theories of consciousness", "text": "Higher-order theories of consciousness\n\nHigher-order theories of consciousness postulate that consciousness consists in perceptions or thoughts about first-order mental states. In particular, phenomenal consciousness is thought to be higher-order representation of perceptual or quasi-perceptual contents, such as visual images.\n\nHigher-order theories are distinguished from other cognitive/representational accounts of consciousness which suggest that merely first-order mentality of certain sorts constitutes consciousness.\n\nHigher-order theory can account for the distinction between unconscious and conscious brain processing. Both types of mental operations involve first-order manipulations, and according to higher-order theory, what makes cognition conscious is a higher-order observation of the first-order processing.\n\nIn neuroscience terms, higher-order theory is motivated by the distinction between first-order information in early sensory regions versus higher-order representations in prefrontal and parietal cortices.\n\nAlso called inner-sense theory, this version of higher-order theory proposes that phenomenal consciousness consists not in immediate sensations but in higher-level sensing of those sensations. Or put another way:\nOne motivation for this approach is that it accounts for phenomenal consciousness absent beliefs or behaviors associated with those experiences—so that, e.g., someone could feel pain without necessarily exhibiting functional reactions to pain.\n\nDavid Rosenthal is a foremost advocate of this view. It claims that a mental state is conscious when it's the subject of a higher-order thought (HOT). Phenomenal consciousness in particular corresponds to certain kinds of mental states (e.g., visual inputs) that are the subjects of HOTs. Rosenthal excludes the special case in which one learns about one's lower-order states by conscious deduction. For instance, if psychoanalysis could reveal one's unconscious motives, this would not suddenly make them conscious.\n\nThe dispositionalist mirrors the actualist view except that the first-order mental state needn't actually be thought about—it only needs to be available to potentially be thought about.\n\nWhile actualist accounts would seem to require immense higher-order computation on all first-order percepts, dispositionalist accounts do not; they merely require \"availability\" of first-order information. Such availability could come from, e.g., global broadcasting as in the Global Workspace Theory.\n\nSelf-representational higher-order theories consider the higher-order state to be \"constitutive\" or \"internal\" to its first-order state. This may be either because\nAn example of the second, \"part-whole\" self-representational theory is Vincent Picciuto's \"quotational theory of consciousness\" in which consciousness consists of \"mentally quoting\" a first-order perception.\n\nIn this theory, higher-level processing determines that a first-order representation is reliable.\n\nSimilar to a HOT view, this theory proposes that the brain \"learns\" when there exists a trustworthy lower-level representation.\n\nHigher-order theories originated in philosophy, but they have also gained some scientific defenders. Here are some examples of evidence supporting higher-order views:\n\nEdmund Rolls defends a higher-order account of consciousness. He argues that consciousness consists in higher-order thoughts allowing one to monitor and correct errors and \"that the brain systems that are required for consciousness and language are similar.\" Qualia like pain become conscious when \"they enter into a specialized linguistic symbol-manipulation system, which is part of a higher-order thought system\" that helps with, inter alia, \"flexible planning of actions.\"\n\nScott Sturgeon argues against inner-sense theory on the grounds that it could give rise to disorders in which, e.g., one has a first-order perception of red which mis-triggers a second-order sense of \"looks orange\". But we don't see any such disorders in neurology. More generally, inner-sense and actualist views face the \"targetless higher-order representation problem\" in which there might be, e.g., a higher-order experience/thought about perceiving red without a corresponding first-order redness percept. One reply is that this is no more a problem for higher-order theories than for other neuroscientific theories of consciousness, which also involve many layers of processing that could theoretically be inconsistent.\n\nPeter Carruthers points out that inner sense of or actualist HOTs about first-order percepts might greatly increase the computing power required to consciously process stimuli, for not only does one need to have a perception, but one then needs to have another (perhaps highly detailed) perception or thought about that perception.\n\nThe \"rock objection\" notes that thinking about a rock doesn't cause the rock to \"light up\" with consciousness, so why does thinking about a first-order percept cause it to light up? Higher-order theorists reply that first-order states need to be mental states, which rocks are not.\n"}
{"id": "32004102", "url": "https://en.wikipedia.org/wiki?curid=32004102", "title": "Hilde Lindemann", "text": "Hilde Lindemann\n\nHilde Lindemann (also \"Hilde Lindemann Nelson\") is an American philosophy professor and bioethicist currently teaching at Michigan State University. Lindemann earned her B.A. in German language and literature in 1969 at the University of Georgia. Lindemann also earned her M.A. in theatre history and dramatic literature, in 1972, at the University of Georgia. Lindemann began her career as a copyeditor for several universities (Interview at 3AM Magazine). She then moved on to a job at the Hastings Center in New York City, an institute focused on bioethics research, and co-authored book \"The Patient in the Family\" before deciding to earn a Ph.D. in philosophy at Fordham University in 2000. Previously, she taught at the University of Tennessee and Vassar College and served as the associate editor of the Hastings Center Report (1990–95). Lindemann currently teaches courses on feminist philosophy, identity and agency, naturalized bioethics, and narrative approaches to bioethics at Michigan State University.\n\nLindemann's work primarily focuses on feminist bioethics, the ethics of families, feminist ethics, and the social construction of identities. She is the former editor of \"\" and was also coeditor, with Sara Ruddick and Margaret Urban Walker, of the Feminist Constructions series for Rowman & Littlefield. In addition, she coedited, with James Lindemann Nelson, a series on Reflective Bioethics for Routledge. Lindemann is a Hastings Center Fellow, a member of the advisory board for the Women’s Bioethics Project (2006–), and was the president of the American Society for Bioethics and Humanities (2008–2009).\n\nHilde Lindemann is a narrative ethicist. A narrative approach uses stories and relationships between people in specific cases, as well as generalizable examples, for moral contexts and discussion (Gotlib).\n\nTwo of her books, \"Holding and Letting Go: The Social Practice of Personal Identities\" and \"Alzheimer's: Answers to Hard Questions for Families,\" co-authored by her partner James Lindemann Nelson, have various reviews that summarize philosophical theories and ethics demonstrated in her works.\n\nThe review of \"Holding and Letting Go: The Social Practice of Personal Identities\" and \"Alzheimer's Answer to Hard Questions for Families\" explains that Lindemann adopts a non-obscure, story-related approach to make readers think about realistic situations: \"Only when we see ethical lives as diachronically and interpersonally structured and as embedded in narratively rich contexts can ethical reflection take hold in actual persons' lives\" (Christman).\n\nThe review of the co-authored book, \"Alzheimer's: Answers to Hard Questions for Families\", also demonstrates that Lindemann employs case studies and life experiences for the patients and caregivers to convey ideas in her work (Moody). This method of provoking thought is aimed at a wide general audience of people who are not necessarily ethics scholars.\n\n\nLindemann has published numerous peer-reviewed articles in journals such as \"The Journal of Medical Ethics,\" \"The American Journal of Bioethics,\" \"The Hastings Center Report,\" \"Metaphilosophy,\" and \"Hypatia.\" Her books include \"Holding and Letting Go: The Social Practice of Personal Identities,\" \"An Invitation to Feminist Ethics,\" \"Damaged Identities, Narrative Repair,\" \"Alzheimer’s: Answers to Hard Questions for Families,\" and \"The Patient in the Family.\" Lindemann has also edited five collections: \"Feminism and Families;\" \"Stories and Their Limits: Narrative Approaches to Bioethics;\" \"Rights, Recognition, and Responsibility: Feminist Ethics and Social Theory;\" \"Meaning and Medicine: A Reader in the Philosophy of Medicine;\" and, with Marian Verkerk and Margaret Urban Walker, \"Naturalized Bioethics\" (Cambridge 2008). Her most recent book, \"Holding and Letting Go: The Social Practice of Personal Identities,\" was published by Oxford University Press in 2014.\n\nIn addition to being named a Hastings Center Fellow and having been elected President of the American Society of Bioethics and Humanities, Lindemann was also awarded a NWO (Netherlands Organization for Scientific Research) grant (2004–2008), a National Endowment for the Humanities grant, and several grants from the University of Tennessee including the Haines-Morris grant. Lindemann has also received a Distinguished Service Award from the American Society of Bioethics and is both a Fulbright scholar (1969) and a Woodrow Wilson fellow (1969).\n\n\n\n Most recently, Hilde Lindemann served as President-Elect (2007–2008) and President (2008–2009) for the American Society of Bioethics and Humanities. She was elected a Hastings Center Fellow in October 2004.\n\nLindemann has received two NWO (Netherlands Organization for Scientific Research) grants, one of €30,000, plus €3,000 each from Michigan State University, Newcastle University, Uppsala University, and Lübeck University to build a network on the ethics of families and care (2013–2016). The other NWO grant (with Marian Verkerk and Margaret Urban Walker) of €25,000, plus the equivalent of €3,500 each from Michigan State University and Arizona State University, for an international collaboration to produce a bioethics whose moral epistemology and psychology are naturalized and whose ethical focus is on practices of responsibility (2004–2008).\n\nShe has also won awards such as the American Society of Bioethics and Humanities Distinguished Service Award (2003) and National Endowment for the Humanities grant to conduct a Summer Seminar for College and University Teachers, entitled “Bioethics in Particular,” $87,000. Project Co-Director (1999–2000). Lindemann was named a Fulbright scholar (1969) and a Woodrow Wilson fellow (1969).\n\n\n"}
{"id": "24315796", "url": "https://en.wikipedia.org/wiki?curid=24315796", "title": "Human dynamics", "text": "Human dynamics\n\nHuman dynamics refer to a branch of complex systems research in statistical physics such as the movement of crowds and queues and other systems of complex human interactions including statistical modelling of human networks, including interactions over communications networks.\n\nHuman Dynamics as a branch of statistical physics: Its main goal is to understand human behavior using methods originally developed in statistical physics. Research in this area started to gain momentum in 2005 after the publication of A.-L. Barabási's seminal paper \"The origin of bursts and heavy tails in human dynamics.\" that introduced a queuing model that was alleged to be capable of explaining the long tailed distribution of inter event times that naturally occur in human activity.\n\nThis paper spurred a burst of activity in this new area leading to not only further theoretical development of the Barabasi model, its experimental verification in several different activities and the beginning of interest in using proxy tools, such as web server logs., cell phone records and even the rate at which registration to a major international conference occurs and the distance and rate people around the globe commute from home to work.\n\nIn recent years there has been a growing appetite for access to new data sources that might prove useful in quantifying and understanding human behavior both at the individual and collective scales.\n\nThe term \"Human Dynamics\" or \"Human Dynamics as Personality Dynamics\" has also been used to describe a technique aimed at education and team building which has been subject to some skepticism, having been described in a Dutch newspaper as a personality course with esoteric (occult) roots.\n\nSense Networks\n\n"}
{"id": "232844", "url": "https://en.wikipedia.org/wiki?curid=232844", "title": "Huzita–Hatori axioms", "text": "Huzita–Hatori axioms\n\nThe Huzita–Hatori axioms or Huzita–Justin axioms are a set of rules related to the mathematical principles of paper folding, describing the operations that can be made when folding a piece of paper. The axioms assume that the operations are completed on a plane (i.e. a perfect piece of paper), and that all folds are linear. These are not a minimal set of axioms but rather the complete set of possible single folds.\n\nThe axioms were first discovered by Jacques Justin in 1986. Axioms 1 through 6 were rediscovered by Japanese-Italian mathematician Humiaki Huzita and reported at \"the First International Conference on Origami in Education and Therapy\" in 1991. Axioms 1 though 5 were rediscovered by Auckly and Cleveland in 1995. Axiom 7 was rediscovered by Koshiro Hatori in 2001; Robert J. Lang also found axiom 7.\n\nThe first 6 axioms are known as Huzita's axioms. Axiom 7 was discovered by Koshiro Hatori. Jacques Justin and Robert J. Lang also found axiom 7. The axioms are as follows:\n\n\nAxiom 5 may have 0, 1, or 2 solutions, while Axiom 6 may have 0, 1, 2, or 3 solutions. In this way, the resulting geometries of origami are stronger than the geometries of compass and straightedge, where the maximum number of solutions an axiom has is 2. Thus compass and straightedge geometry solves second-degree equations, while origami geometry, or origametry, can solve third-degree equations, and solve problems such as angle trisection and doubling of the cube. The construction of the fold guaranteed by Axiom 6 requires \"sliding\" the paper, or neusis, which is not allowed in classical compass and straightedge constructions. Use of neusis together with a compass and straightedge does allow trisection of an arbitrary angle.\n\nGiven two points \"p\" and \"p\", there is a unique fold that passes through both of them.\n\nIn parametric form, the equation for the line that passes through the two points is :\n\nGiven two points \"p\" and \"p\", there is a unique fold that places \"p\" onto \"p\".\n\nThis is equivalent to finding the perpendicular bisector of the line segment \"p\"\"p\". This can be done in four steps:\n\n\nGiven two lines \"l\" and \"l\", there is a fold that places \"l\" onto \"l\".\n\nThis is equivalent to finding a bisector of the angle between \"l\" and \"l\". Let \"p\" and \"p\" be any two points on \"l\", and let \"q\" and \"q\" be any two points on \"l\". Also, let u and v be the unit direction vectors of \"l\" and \"l\", respectively; that is:\n\nIf the two lines are not parallel, their point of intersection is:\n\nwhere\n\nThe direction of one of the bisectors is then:\n\nAnd the parametric equation of the fold is:\n\nA second bisector also exists, perpendicular to the first and passing through \"p\". Folding along this second bisector will also achieve the desired result of placing \"l\" onto \"l\". It may not be possible to perform one or the other of these folds, depending on the location of the intersection point.\n\nIf the two lines are parallel, they have no point of intersection. The fold must be the line midway between \"l\" and \"l\" and parallel to them.\n\nGiven a point \"p\" and a line \"l\", there is a unique fold perpendicular to \"l\" that passes through point \"p\".\n\nThis is equivalent to finding a perpendicular to \"l\" that passes through \"p\". If we find some vector v that is perpendicular to the line \"l\", then the parametric equation of the fold is:\n\nGiven two points \"p\" and \"p\" and a line \"l\", there is a fold that places \"p\" onto \"l\" and passes through \"p\".\n\nThis axiom is equivalent to finding the intersection of a line with a circle, so it may have 0, 1, or 2 solutions. The line is defined by \"l\", and the circle has its center at \"p\", and a radius equal to the distance from \"p\" to \"p\". If the line does not intersect the circle, there are no solutions. If the line is tangent to the circle, there is one solution, and if the line intersects the circle in two places, there are two solutions.\n\nIf we know two points on the line, (\"x\", \"y\") and (\"x\", \"y\"), then the line can be expressed parametrically as:\n\nLet the circle be defined by its center at \"p\"=(\"x\", \"y\"), with radius formula_13. Then the circle can be expressed as:\n\nIn order to determine the points of intersection of the line with the circle, we substitute the \"x\" and \"y\" components of the equations for the line into the equation for the circle, giving:\n\nOr, simplified:\n\nwhere:\n\nThen we simply solve the quadratic equation:\n\nIf the discriminant \"b\" − 4\"ac\" < 0, there are no solutions. The circle does not intersect or touch the line. If the discriminant is equal to 0, then there is a single solution, where the line is tangent to the circle. And if the discriminant is greater than 0, there are two solutions, representing the two points of intersection. Let us call the solutions \"d\" and \"d\", if they exist. We have 0, 1, or 2 line segments:\n\nA fold \"F\"(\"s\") perpendicular to \"m\" through its midpoint will place \"p\" on the line at location \"d\". Similarly, a fold \"F\"(\"s\") perpendicular to \"m\" through its midpoint will place \"p\" on the line at location \"d\". The application of Axiom 2 easily accomplishes this. The parametric equations of the folds are thus:\n\nGiven two points \"p\" and \"p\" and two lines \"l\" and \"l\", there is a fold that places \"p\" onto \"l\" and \"p\" onto \"l\".\n\nThis axiom is equivalent to finding a line simultaneously tangent to two parabolas, and can be considered equivalent to solving a third-degree equation as there are in general three solutions. The two parabolas have foci at \"p\" and \"p\", respectively, with directrices defined by \"l\" and \"l\", respectively.\n\nThis fold is called the Beloch fold after Margharita P. Beloch, who in 1936 showed using it that origami can be used to solve general cubic equations.\n\nGiven one point \"p\" and two lines \"l\" and \"l\", there is a fold that places \"p\" onto \"l\" and is perpendicular to \"l\".\n\nThis axiom was originally discovered by Jacques Justin in 1989 but was overlooked and was rediscovered by Koshiro Hatori in 2002. Robert J. Lang has proven that this list of axioms completes the axioms of origami.\n\nSubsets of the axioms can be used to construct different sets of numbers. The first three can be used with three given points not on a line to do what Alperin calls Thalian constructions.\n\nThe first four axioms with two given points define a system weaker than compass and straightedge constructions: every shape that can be folded with those axioms can be constructed with compass and straightedge, but some things can be constructed by compass and straightedge that cannot be folded with those axioms. The numbers that can be constructed are called the origami or pythagorean numbers, if the distance between the two given points is 1 then the constructible points are all of the form formula_24 where formula_25 and formula_26 are Pythagorean numbers. The Pythagorean numbers are given by the smallest field containing the rational numbers and formula_27 whenever formula_25 is such a number.\n\nAdding the fifth axiom gives the Euclidean numbers, that is the points constructible by compass and straightedge construction.\n\nAdding the neusis axiom 6, all compass-straightedge constructions, and more, can be made. In particular, the constructible regular polygons with these axioms are those with formula_29 sides, where formula_30 is a product of distinct Pierpont primes. Compass-straightedge constructions allow only those with formula_31 sides, where formula_32 is a product of distinct Fermat primes. (Fermat primes are a subset of Pierpont primes.)\n\nThe seventh axiom does not allow construction of further axioms. The seven axioms give all the single-fold constructions that can be done rather than being a minimal set of axioms.\n\nThe existence of an eighth axiom was claimed by Lucero in 2017, which may be stated as: there is a fold along a given line \"l\". The new axiom was found after enumerating all possible incidences between constructible points and lines on a plane. Although it does not create a new line, it is nevertheless needed in actual paper folding when it is required to fold a layer of paper along a line marked on the layer immediately below.\n\n"}
{"id": "464877", "url": "https://en.wikipedia.org/wiki?curid=464877", "title": "Information management", "text": "Information management\n\nInformation management (IM) concerns a cycle of organizational activity: the acquisition of information from one or more sources, the custodianship and the distribution of that information to those who need it, and its ultimate disposition through archiving or deletion.\n\nThis cycle of organisational involvement with information involves a variety of stakeholders, including those who are responsible for assuring the quality, accessibility and utility of acquired information; those who are responsible for its safe storage and disposal; and those who need it for decision making. Stakeholders might have rights to originate, change, distribute or delete information according to organisational information management policies.\n\nInformation management embraces all the generic concepts of management, including the planning, organizing, structuring, processing, controlling, evaluation and reporting of information activities, all of which is needed in order to meet the needs of those with organisational roles or functions that depend on information. These generic concepts allow the information to be presented to the audience or the correct group of people. After individuals are able to put that information to use, it then gains more value.\n\nInformation management is closely related to, and overlaps with, the management of \"data\", \"systems\", \"technology\", \"processes\" and – where the availability of information is critical to organisational success – \"strategy\". This broad view of the realm of information management contrasts with the earlier, more traditional view, that the life cycle of managing information is an operational matter that requires specific procedures, organisational capabilities and standards that deal with information as a product or a service.\n\nIn the 1970s, the management of information largely concerned matters closer to what would now be called data management: punched cards, magnetic tapes and other record-keeping media, involving a life cycle of such formats requiring origination, distribution, backup, maintenance and disposal. At this time the huge potential of information technology began to be recognised: for example a single chip storing a whole book, or electronic mail moving messages instantly around the world, remarkable ideas at the time. With the proliferation of information technology and the extending reach of information systems in the 1980s and 1990s, information management took on a new form. Progressive businesses such as British Petroleum transformed the vocabulary of what was then \"IT management\", so that “systems analysts” became “business analysts”, “monopoly supply” became a mixture of “insourcing” and “outsourcing”, and the large IT function was transformed into “lean teams” that began to allow some agility in the processes that harness information for business benefit. The scope of senior management interest in information at British Petroleum extended from the creation of value through improved business processes, based upon the effective management of information, permitting the implementation of appropriate information systems (or “applications”) that were operated on IT infrastructure that was outsourced. In this way, information management was no longer a simple job that could be performed by anyone who had nothing else to do, it became highly strategic and a matter for senior management attention. An understanding of the technologies involved, an ability to manage information systems projects and business change well, and a willingness to align technology and business strategies all became necessary.\n\nIn the transitional period leading up to the strategic view of information management, Venkatraman (a strong advocate of this transition and transformation, proffered a simple arrangement of ideas that succinctly brought together the managements of data, information, and knowledge (see the figure)) argued that:\n\nThis is often referred to as the DIKAR model: Data, Information, Knowledge, Action and Result, it gives a strong clue as to the layers involved in aligning technology and organisational strategies, and it can be seen as a pivotal moment in changing attitudes to information management. The recognition that information management is an investment that must deliver meaningful results is important to all modern organisations that depend on information and good decision-making for their success.\n\nIt is commonly believed that good information management is crucial to the smooth working of organisations, and although there is no commonly accepted theory of information management \"per se\", behavioural and organisational theories help. Following the behavioural science theory of management, mainly developed at Carnegie Mellon University and prominently supported by March and Simon, most of what goes on in modern organizations is actually information handling and decision making. One crucial factor in information handling and decision making is an individual's ability to process information and to make decisions under limitations that might derive from the context: a person's age, the situational complexity, or a lack of requisite quality in the information that is at hand – all of which is exacerbated by the rapid advance of technology and the new kinds of system that it enables, especially as the social web emerges as a phenomenon that business cannot ignore. And yet, well before there was any general recognition of the importance of information management in organisations, March and Simon argued that organizations have to be considered as cooperative systems, with a high level of information processing and a vast need for decision making at various levels. Instead of using the model of the \"economic man\", as advocated in classical theory they proposed \"administrative man\" as an alternative, based on their argumentation about the cognitive limits of rationality. Additionally they proposed the notion of satisficing, which entails searching through the available alternatives until an acceptability threshold is met - another idea that still has currency.\n\nIn addition to the organisational factors mentioned by March and Simon, there are other issues that stem from economic and environmental dynamics. There is the cost of collecting and evaluating the information needed to take a decision, including the time and effort required. The transaction cost associated with information processes can be high. In particular, established organizational rules and procedures can prevent the taking of the most appropriate decision, leading to sub-optimum outcomes \n. This is an issue that has been presented as a major problem with bureaucratic organizations that lose the economies of strategic change because of entrenched attitudes.\n\nAccording to the Carnegie Mellon School an organization's ability to process information is at the core of organizational and managerial competency, and an organization's strategies must be designed to improve information processing capability and as information systems that provide that capability became formalised and automated, competencies were severely tested at many levels. It was recognised that organisations needed to be able to learn and adapt in ways that were never so evident before and academics began to organise and publish definitive works concerning the strategic management of information, and information systems. Concurrently, the ideas of business process management and knowledge management although much of the optimistic early thinking about business process redesign has since been discredited in the information management literature. In the strategic studies field, it is considered of the highest priority the understanding of the information environment, conceived as the aggregate of individuals, organizations, and systems that collect, process, disseminate, or act on information. This environment consists of three interrelated dimensions which continuously interact with individuals, organizations, and systems. These dimensions are the physical, informational, and cognitive.\n\nVenkatraman has provided a simple view of the requisite capabilities of an organisation that wants to manage information well – the DIKAR model (see above). He also worked with others to understand how technology and business strategies could be appropriately aligned in order to identify specific capabilities that are needed. This work was paralleled by other writers in the world of consulting, practice and academia.\n\nBytheway has collected and organised basic tools and techniques for information management in a single volume. At the heart of his view of information management is a portfolio model that takes account of the surging interest in external sources of information and the need to organise un-structured information external so as to make it useful (see the figure).\n\nSuch an information portfolio as this shows how information can be gathered and usefully organised, in four stages:\n\nStage 1: Taking advantage of public information: recognise and adopt well-structured external schemes of reference data, such as post codes, weather data, GPS positioning data and travel timetables, exemplified in the personal computing press.\n\nStage 2: Tagging the noise on the world wide web: use existing schemes such as post codes and GPS data or more typically by adding “tags”, or construct a formal ontology that provides structure. Shirky provides an overview of these two approaches.\n\nStage 3: Sifting and analysing: in the wider world the generalised ontologies that are under development extend to hundreds of entities and hundreds of relations between them and provide the means to elicit meaning from large volumes of data. Structured data in databases works best when that structure reflects a higher-level information model – an ontology, or an entity-relationship model.\n\nStage 4: Structuring and archiving: with the large volume of data available from sources such as the social web and from the miniature telemetry systems used in personal health management, new ways to archive and then trawl data for meaningful information. Map-reduce methods, originating from functional programming, are a more recent way of eliciting information from large archival datasets that is becoming interesting to regular businesses that have very large data resources to work with, but it requires advanced multi-processor resources.\n\nThe Information Management Body of Knowledge was made available on the world wide web in 2004 \nand sets out to show that the required management competencies to derive real benefits from an investment in information are complex and multi-layered. The framework model that is the basis for understanding competencies comprises six “knowledge” areas and four “process” areas:\nThe IMBOK is based on the argument that there are six areas of required management competency, two of which (“business process management” and “business information management”) are very closely related.\n\n\nEven with full capability and competency within the six knowledge areas, it is argued that things can still go wrong. The problem lies in the migration of ideas and information management value from one area of competency to another. Summarising what Bytheway explains in some detail (and supported by selected secondary references):\n\n\nThere are always many ways to see a business, and the information management viewpoint is only one way. It is important to remember that other areas of business activity will also contribute to strategy – it is not only good information management that moves a business forwards. Corporate governance, human resource management, product development and marketing will all have an important role to play in strategic ways, and we must not see one domain of activity alone as the sole source of strategic success. On the other hand, corporate governance, human resource management, product development and marketing are all dependent on effective information management, and so in the final analysis our competency to manage information well, on the broad basis that is offered here, can be said to be predominant.\n\nOrganizations are often confronted with many information management challenges and issues at the operational level, especially when organisational change is engendered. The novelty of new systems architectures and a lack of experience with new styles of information management requires a level of organisational change management that is notoriously difficult to deliver. As a result of a general organisational reluctance to change, to enable new forms of information management, there might be (for example): a shortfall in the requisite resources, a failure to acknowledge new classes of information and the new procedures that use them, a lack of support from senior management leading to a loss of strategic vision, and even political manoeuvring that undermines the operation of the whole organisation. However, the implementation of new forms of information management should normally lead to operational benefits.\n\nIn early work, taking an information processing view of organisation design, Jay Galbraith has identified five tactical areas to increase information processing capacity and reduce the need for information processing.\n\n\nThe lateral relations concept leads to an organizational form that is different from the simple hierarchy, the “matrix organization”. This brings together the vertical (hierarchical) view of an organisation and the horizontal (product or project) view of the work that it does visible to the outside world. The creation of a matrix organization is one management response to a persistent fluidity of external demand, avoiding multifarious and spurious responses to episodic demands that tend to be dealt with individually.\n\n\n"}
{"id": "15143713", "url": "https://en.wikipedia.org/wiki?curid=15143713", "title": "Integrated assessment modelling", "text": "Integrated assessment modelling\n\nIntegrated assessment modelling (IAM) or integrated modelling (IM) is a type of scientific modelling often used by the environmental sciences and environmental policy analysis. The modelling is integrated because environmental problems do not respect the borders between academic disciplines. Integrated assessment models therefore integrate knowledge from two or more domains into a single framework. Integrated modelling is referred to as assessment because the activity aims to generate useful information for policy making, rather than to advance knowledge for knowledge's sake. Integrated assessment modelling is that part of integrated assessment that relies on the use of numerical models.\n\nIntegrated assessment modelling has a long history, and scholars disagree on the first precedent. However, it became recognizable as a sub- or inter-discipline in the late 1980s with a focus on understanding and regulating acidification. Integrated assessment modelling was further developed in the area of climate change, inter alias in the context of the Energy Modeling Forum.\n\nNotable centres of integrated assessment modelling are IIASA, MIT, Netherlands Environmental Assessment Agency, and International Futures. Notable scholars are Barry B. Hughes, Bill Nordhaus, Robert Mendelsohn, Rich Richels, Michael Schlesinger, Stephen Schneider, Richard Tol, John Weyant, and Gary Yohe.\n\nWhile the Intergovernmental Panel on Climate Change (IPCC) relies heavily on integrated modeling, IPCC Fifth Assessment Report lead author Thomas Bruckner cautions against overinterpreting the results from such modeling. With regard to future technology choice, Bruckner observes that \"The \"quantitative deployment\" of nuclear power or CCS technologies that has been found by the IPCC authors in the different mitigation scenarios, for instance, occasionally has been (mis)interpreted as an IPCC \"recommendation for their factual usage\".\" Bruckner continues \"The scale of deployment of low-carbon technologies shown in [IPCC] diagrams... reveals neither the \"possible contribution\" of the respective energy technology to GHG mitigation nor the \"required contribution\"... The possible contribution (without considering costs) might be higher whereas the strictly required one could be lower.\n\nEconomist Robert Pindyck argues that IAMs are problematic and \"close to useless as tools for policy analysis\". He argues that \"IAM-based analyses of climate policy create a perception of knowledge and precision that is illusory, and can fool policy-makers into thinking that the forecasts the models generate have some kind of scientific legitimacy\".\n\n\n"}
{"id": "12047861", "url": "https://en.wikipedia.org/wiki?curid=12047861", "title": "Israel Democracy Institute", "text": "Israel Democracy Institute\n\nIsrael Democracy Institute (IDI; ), established in 1991, is an independent center of research and action dedicated to strengthening the foundations of Israeli democracy. It is located in Jerusalem, Israel.\n\nThe Israel Democracy Institute was founded in 1991 by Arye Carmon, the founding president, and Mr. Bernard Marcus.\n\nIDI works to bolster the values and institutions of Israel as a Jewish and democratic state. A non-partisan think-and-do tank, the institute harnesses rigorous applied research to influence policy, legislation and public opinion. The institute partners with government, policy and decision makers, civil service and society, to improve the functioning of the government and its institutions, confront security threats while preserving civil liberties, and foster solidarity within Israeli society. Israel recognized the positive impact of IDI's research and recommendations by conferring upon the institute its most prestigious award, the Israel Prize for Lifetime Achievement.\n\nIn 2014, Yohanan Plesner was appointed president of the Institute. He replaced Arik Carmon, who left to concentrate on research after a 22-year term of service.\n\nThe Israel Democracy Institute contains the following centers:\n\nEngages with the core issues of Israeli democracy, and with the erosion in public commitment to substantive democracy and its values. The Center focuses on bolstering the commitment of public institutions and decision makers to the protection of human rights, to the principles of freedom and equality, and to the rule of law. The research and implementation work carried out at the Center concentrates on failures in implementing the principle of the separation of powers, and on maintaining the independence of the state's structural institutions, including the courts, the Knesset, the government, regulatory bodies, the state prosecutor and the attorney general, local government, and the media.\n\nAddresses the question of how to balance the particularistic Jewish characteristics of the State of Israel (nationality and religion) with its liberal-democratic characteristics. The Center is also concerned with renewing the conceptual foundations of Jewish-democratic Zionism for our times, and with the internal and external challenges to the state's Jewish identity. The Center's researchers are also engaged in developing ways for Israel's Haredi population to be integrated into the general Israeli economy and society.\n\nPromotes reforms to the political system, the civil service, and the labor market in Israel, with the aim of improving the functioning of these systems and increasing public trust in them.\n\nMaps the challenges for democratic societies posed by the fight against terror, and examines how a balance can be maintained between answering security needs and protecting human rights.\n\nAn independent research center that has operated under the auspices of the Israel Democracy Institute since 1998, engaged in collecting, analyzing, and maintaining empirical data relating to Israeli society and politics. The Center's comprehensive survey database contains data from more than a thousand public opinion surveys conducted since 1967 on a range of political, social, and media-related topics. The Center is headed by Prof. Tamar Hermann. Among the more noteworthy publications issued by the Guttman Center in recent years are: Religious? National!, a survey of attitudes and behaviors in the national-religious sector in Israel; the annual Israeli Democracy Index; and the monthly Peace Index produced in partnership with Tel-Aviv University.\n\nThe Eli Hurvitz Conference on Economy and Society, run by the Israel Democracy Institute, is Israel's leading economic conference. Since 1993, the conference has served as the meeting place for public discourse and professional knowledge relating to economy and society in Israel. Its main goal is to improve decision-making processes in government, so as to ensure better social and economic policies that will benefit all Israelis.\n\nThe Israeli Democracy Index provides an annual assessment of the quality of democracy in Israel. It is prepared on the basis of a large-scale survey conducted every year since 2004 with a representative sample of the Israeli population, comprising around 1,500 interviewees aged 18 and above. The survey is designed to identify trends in Israeli society on substantial issues relating to the fulfillment of democratic values and goals, and to the functioning of government systems and elected officials. The Index contains an analysis of the survey results, and aims to enrich public discourse on the state of democracy in Israel by giving access to a broad database of relevant information.\n\nAlongside this general assessment, the Index also focuses each year on a particular topic connected to Israeli democracy. The subjects chosen so far include: opinions of Israeli youth (2004); the media in Israeli democracy (2005); tenth anniversary of the Rabin assassination (2005); political parties in Israel (2006); cohesion in a fractured society (2007); civil society in Israel (2008); twentieth anniversary of mass immigration from the Soviet Union (2009); democratic principles in Israel in practice (2010); the social protest, a year on (2012); civilian opinions on the socioeconomic situation (2014); the Rabin assassination, twenty years on (2015); and opinions in Haredi society (2016).\n\nThe Institute's Outstanding Parliamentarian Award seeks to encourage high-quality parliamentary activity and to recognize those who perform it.\n\nThe award is given annually to two Knesset members, one from the governing coalition and one from the opposition, based on a comprehensive review of several fixed criteria used to determine good parliamentary performance. First, a database is created of the activity of all Knesset members (excluding ministers, deputy ministers, the Speaker of the Knesset, and the previous year's winners of the award) across a range of parameters: attendance at committee meetings, private legislation passed during that year's Knesset session, queries submitted, agenda item proposals debated in the plenum, brief and full-length speeches delivered at plenary sessions, commissioning of research studies from the Knesset Research and Information Center, and plenary attendance, as well as reprimands and punishments received from the Knesset Ethics Committee. Next, the award committee, chaired by retired Justice Theodore Orr, convenes to discuss the list of candidates, and selects the honorees based on the data provided as well as on additional criteria, such as the broader public influence and importance of the candidates’ parliamentary work.\n\nThe Outstanding Parliamentarian Award helps improve both the quality of Knesset members’ work and the public perception of the Knesset as Israel's legislative body.\n\nThe Forum for Political Reform in Israel was founded by Arik Carmon, driven by “a sense of the need to launch processes that will bring about a significant change in the capabilities and functioning of the parliament in general, and in particular, of those who hold the levers of power,” as he described it. The forum was chaired by former Supreme Court President Meir Shamgar, and its members included academics, retired justices, leading public figures, and businesspeople.\n\nThe forum examined the expected impact of reforms to the electoral, governmental, and party-political systems proposed by the Israel Democracy Institute, and formulated practical proposals for legislation and public policy accordingly.\n\nIn 2015, the Israel Democracy Institute published an updated plan for political and electoral reform, proposing structural changes to the political system that would result in the formation of two major blocks of political parties providing clear alternatives for government.\n\nParliament is an online journal exploring topics related to governance, law, and society from both local and international perspectives. The first issue was published in print in 1993, and up until 2007 the journal contained articles on such subjects as elections in Israel and around the world, Israeli society, constitution and basic laws, religion and state, security and the armed forces, political participation, political parties, the European Union, and many others. In 2007, the format of the journal was changed so that each issue is dedicated to a single main topic. The journal was also taken online, and is no longer published in a print format.\n\nThis is a unique scholarship program for outstanding doctoral research students in the field of human rights and Judaism, which seeks to create an intellectual foundation for shaping the human rights discourse in a Jewish nation state.\n\nThis is the first publication in Israel to provide a comprehensive anthology of existing quantitative data about Haredi society in Israel. It aims to provide a full and up-to-date picture of Haredi society based on statistical information and data analysis, covering a range of subject areas including demographics, education, welfare, standard of living, employment, voting patterns in Knesset elections, and lifestyles.\n\nThe Annual Statistical Report on Ultra-Orthodox Society in Israel is produced by a team of researchers from the Jerusalem Institute for Policy Research and the Israel Democracy Institute.\n\nThe Ministerial Committee on Legislation is one of Israel's most important ministerial committees. Chaired by the Minister of Justice, it meets weekly to decide the governing coalition's position on all proposed legislation, whether government-sponsored or private members’ bills.\n\nEvery week, the Israel Democracy Institute submits a legal opinion on the bills being discussed by the committee, which it also shares with relevant Knesset committees. The opinion is written based on the areas of research expertise of the Institute's research fellows.\n\n\"Security Clearance\" is an online forum for analyzing issues relating to both national security and democracy. It aims to contribute to the public-academic debate on national security issues from a range of perspectives, including law, ethics, and social sciences.\n\nThe blog is edited by researchers at the Israel Democracy Institute, but is open to a diverse range of views, and to writers from a variety of fields related to national security who seek to participate in serious, open, and fruitful discussions. The views expressed on the blog are strictly those of the writers themselves, and do not represent the institutions or organizations to which they belong, or the views of the blog editors or the Israel Democracy Institute.\n\nThis comparative tool, developed by the Israel Democracy Institute, provides an overview of Israel's elections since the founding of the state, and of the political parties that participated in each. The information provided by party includes the parties’ political platform, main leaders, the number of votes and Knesset seats won at each election, and their representation in government. The information on elections includes a description of the major issues that formed the backdrop to each, and the political context in which they were held. Additionally, full lists of candidates are provided, including those parties that did not reach the electoral threshold.\n\nThis tool provides in-depth political information on 50 democratic countries, including Israel. This information includes basic data on each country (population, area, language), current leaders, system of government, electoral system, type of current government, and structure of legislative authority.\n\nIn addition, it allows users to view countries’ rankings according to a number of comparative indexes, such as freedom of the press, perception of corruption, percentage of female members of parliament, GINI inequality index, and more. Users can compare multiple countries on a particular index, or compare multiple index rankings for a particular country.\n\nIn 2001, the Israel Democracy Institute established the Constitution by Consensus public council, chaired by former Supreme Court President Meir Shamgar, with the aim of designing a constitution for Israel that would enjoy broad public support. The council had around 100 members, including ministers and Knesset members; secular, national religious, and Haredi Jews; Arabs; right-wingers and left-wingers; and academics, jurists, religious leaders, and public figures. Over the course of five years, this public council examined the diverse range of views relating to constitutional issues in Israel, and sought to formulate the agreements and compromises that would be necessary to create a consensual constitution. The council's discussions formed the basis and inspiration for the Israel Democracy Institute's proposal for a constitution by consensus.\n\nIDI is headed by former MK Yohanan Plesner. Its leadership includes two Vice Presidents of Research: Prof. Mordechai Kremnitzer, Professor Emeritus of International Law at Hebrew University, and Prof. Yedidia Stern, a Professor of Law at Bar-Ilan University and former dean of its law school. Senior Research Fellows of the Institute include Admiral Ami (Amichay) Ayalon, Professors Hanoch Dagan, Momi Dahan, Yuval Feldman, Tamar Hermann, Shahar Lifshitz, Yotam Margalit, Gideon Rahat, Eli Shaltiel, and Yuval Shany, and Mr. Doron H. Cohen. Past Senior Fellows include Prof. Avi Ben Bassat, Prof. Ruth Gavison, Adv. Dan Meridor, Prof. David Nachmias, Prof. Eyal Naveh, Prof. Aviezer Ravitzky, and Prof. Anita Shapira. The late Prof. Asher Arian headed the Institute's program on political reform and directed its Guttman Center for Applied Social Research. In 2013, Dan Landau replaced Rabbi Itshak Levi as head of IDI's Policy Implementation division. Former U.S. Secretary of State George P. Shultz serves as the Chairman of the Institute's International Advisory Council. Mr. Amir Elstein is the Chairman of the Board. Rabbi Dr. Benjamin (Benny) Lau heads the Institute's Human Rights and Judaism in Action project, which has published several contemporary responsa on people with disabilities and Jewish law.\nIn 2018, Yuval Shany was Vice President for research.\n\nThe Israel Democracy Institute's publications department publishes books and policy studies written by the Institute's research fellows, as well as proceedings of the conferences and study days organized by the Institute. These publications aim to enrich Israeli public discourse about the issues of most importance for Israeli democracy. It also publishes the online journal HaAyin HaShevi'it (\"The 7th eye\").\n\n\nIn 2009, IDI was awarded the Israel Prize for its \"lifetime achievement and special contribution to society and the State of Israel.\n\nIn the University of Pennsylvania's \"2014 Global Go To Think Tanks Report\", IDI was ranked the twenty-third best think thank in the Middle East and North Africa.\n\nThe Israel Democracy Institute is funded by charitable donations, which totaled $8,989,254 in 2017. The Institute's main funder is the Jewish-American billionaire Bernard Marcus.\n\nThe Institute's budget is published yearly in its annual report.\n\n\n"}
{"id": "22635773", "url": "https://en.wikipedia.org/wiki?curid=22635773", "title": "James J. P. McShane", "text": "James J. P. McShane\n\nJames Joseph Patrick McShane (March 17, 1909 - December 23, 1968) was the former chief United States Marshal during part of the Civil Rights Movement. \n\nHe was born in 1909 in New York City. \n\nAs head of the Executive Office for U.S. Marshals, he supervised federal agents during the Freedom Rides of 1961, but is most known for his role in leading the federal agents who escorted James Meredith, the first African American student at University of Mississippi in 1962. \n\nIn November of 1962 McShane was indicted by a Lafayette County Mississippi grand jury on charges that he \"did incite a riot\" in relation to his decision to fire tear gas into crowds of violent protesters during the Ole Miss riot of 1962. McShane subsequently gave himself up for arrest, was processed, and released. The US District Court found that McShane acted under orders and statutory authority and \"had reasonable cause to believe ... that the use of tear gas ... was a proper measure to be taken\". The court granted summary judgment in favor of McShane. \n\nBefore being a U.S. Marshal, he worked for the Senate's select McClellan Committee, was chief of security and the personal bodyguard for President John F. Kennedy and a New York City Police Department homicide detective.\n\nHe died on December 23, 1968, from pneumonia in Alexandria, Virginia.\n"}
{"id": "2680258", "url": "https://en.wikipedia.org/wiki?curid=2680258", "title": "Jewish Autonomism", "text": "Jewish Autonomism\n\nJewish Autonomism was a non-Zionist political movement that emerged in Eastern Europe in the late 19th and early 20th century. One of its major proponents was the historian and activist Simon Dubnow, who also called his ideology folkism.\nThe Autonomists believed that the future survival of the Jews as a nation depends on their spiritual and cultural strength, in developing \"spiritual nationhood\" and in viability of Jewish diaspora as long as Jewish communities maintain self-rule, and rejected assimilation. Autonomists often stressed the vitality of modern Yiddish culture.\n\nVarious concepts of the Autonomism were adopted in the platforms of the Folkspartei, the Sejmists and socialist Jewish parties such as the Bund.\n\nSome groups blended Autonomism with Zionism: they favored Jewish self-rule in the diaspora until diaspora Jews make Aliyah to their national homeland in Zion.\n\nThe movement's beliefs were similar to those of the Austromarxists, who advocated national personal autonomy within the multinational Austro-Hungarian empire, and cultural pluralists in America, such as Randolph Bourne and Horace Kallen.\n\nIn 1941, Simon Dubnow was one of thousands of Jews murdered in the Rumbula massacre. After the Holocaust, the notion of Autonomism practically disappeared from Jewish philosophy.\n\nIt is not connected to the contemporary political movement autonomism.\n\n\n"}
{"id": "42530390", "url": "https://en.wikipedia.org/wiki?curid=42530390", "title": "King Canute and the tide", "text": "King Canute and the tide\n\nThe story of King Canute and the tide is an apocryphal anecdote illustrating the piety or humility of King Canute the Great, recorded in the 12th century by Henry of Huntingdon.\n\nIn the story, Canute demonstrates to his flattering courtiers that he has no control over the elements (the incoming tide), explaining that secular power is vain compared to the supreme power of God.\nThe episode is frequently alluded to in contexts where the futility of \"trying to stop the tide\" of an inexorable event is pointed out, but usually misrepresenting Canute as believing he had supernatural powers, when Huntingdon's story in fact relates the opposite.\n\nHenry of Huntingdon tells the story as one of three examples of Canute's \"graceful and magnificent\" behaviour (outside of his bravery in warfare), the other two being his arrangement of the marriage of his daughter to the later Holy Roman Emperor, and the negotiation of a reduction in tolls on the roads across Gaul to Rome at the imperial coronation of 1027.\n\nIn Huntingdon's account, Canute set his throne by the sea shore and commanded the incoming tide to halt and not wet his feet and robes. Yet \"continuing to rise as usual [the tide] dashed over his feet and legs without respect to his royal person. Then the king leapt backwards, saying: 'Let all men know how empty and worthless is the power of kings, for there is none worthy of the name, but He whom heaven, earth, and sea obey by eternal laws. He then hung his gold crown on a crucifix, and never wore it again \"to the honour of God the almighty King\".\n\nLater historians repeated the story, most of them adjusting it to have Canute more clearly aware that the tides would not obey him, and staging the scene to rebuke the flattery of his courtiers.\nThere are also earlier parallels in Celtic stories of men who commanded the tides, namely Saint Illtud of Glamorgan, Maelgwn, king of Gwynedd, and Tuirbe, of Tuirbe's Strand in Brittany.\n\nProverbial reference to the legend in modern journalism or politics usually casts the story in terms of \"Canute's arrogance\" of \"attempting to stop the tide\". \nIt was cited, for example, by Stacy Head as typifying the New Orleans city council's response to Hurricane Katrina (2005), or by Mark Stephens in reference to Ryan Giggs as \"the King Canute of football\" for his attempts of stopping \"the unstoppable tide of information\" on the internet in the 2011 British privacy injunctions controversy. That is a misrepresentation of Huntingdon's account, whose Canute uses the tide to demonstrate his inability to control the elements and his deference to the greater authority of God.\n\nTheodore Dalrymple refers to the story, without misattributing motives of arrogance to Canute, in the context of the British reaction to the Ukraine crisis (2014), saying \nWarren Burger, the Chief Justice of the United States, mentions Canute in the 1980 decision \"Diamond v. Chakrabarty\" (447 U.S. 303), stating the denial of a patent for a micro-organism \"is not likely to put an end to genetic research\". Burger likens doing so to Canute commanding the tides.\n\nThe contemporary \"Encomium Emmae\" has no mention of the episode, which has been taken as indicating its ahistoricity, as it would seem that so pious a dedication might have been recorded there, since the same source gives an \"eye-witness account of his lavish gifts to the monasteries and poor of St Omer when on the way to Rome, and of the tears and breast-beating which accompanied them\". \nGoscelin, writing later in the 11th century, instead has Canute place his crown on a crucifix at Winchester one Easter, with no mention of the sea, and \"with the explanation that the king of kings was more worthy of it than he\". Nevertheless, there may be a \"basis of fact, in a planned act of piety\" behind this story. \nOn the other hand, Malcolm Godden says the story is simply \"a 12th Century legend... and those 12th Century historians were always making up stories about kings from Anglo-Saxon times\".\n\nThe site of the episode is often identified as Thorney Island (now known as Westminster), where Canute set up a royal palace during his reign over London. \nConflictingly, a sign on Southampton city centre's Canute Road reads, \"Near this spot AD 1028 Canute reproved his courtiers\". Bosham in West Sussex also claims to be the site of this episode, as does Gainsborough in Lincolnshire. As Gainsborough is inland, if the story is true then Canute would have been trying to turn back the tidal bore known as the aegir.\n\n\n"}
{"id": "1719638", "url": "https://en.wikipedia.org/wiki?curid=1719638", "title": "Law of specific nerve energies", "text": "Law of specific nerve energies\n\nThe law of specific nerve energies, first proposed by Johannes Peter Müller in 1835, is that the nature of perception is defined by the pathway over which the sensory information is carried. Hence, the origin of the sensation is not important. Therefore, the difference in perception of seeing, hearing, and touch are not caused by differences in the stimuli themselves but by the different nervous structures that these stimuli excite. For example, pressing on the eye elicits sensations of flashes of light because the neurons in the retina send a signal to the occipital lobe. Despite the sensory input's being mechanical, the experience is visual.\n\nHere is Müller's statement of the law, from \"Handbuch der Physiologie des Menschen für Vorlesungen\", 2nd Ed., translated by Edwin Clarke and Charles Donald O'Malley:\n\nAs the above quotation shows, Müller's law seems to differ from the modern statement of the law in one key way. Müller attributed the quality of an experience to some specific quality of the energy in the nerves. For example, the visual experience from light shining into the eye, or from a poke in the eye, arises from some special quality of the energy carried by optic nerve, and the auditory experience from sound coming into the ear, or from electrical stimulation of the cochlea, arises from some different, special quality of the energy carried by the auditory nerve. In 1912, Lord Edgar Douglas Adrian showed that all neurons carry the same energy, electrical energy in the form of action potentials. That means that the quality of an experience depends on the part of the brain to which nerves deliver their action potentials (e.g., light from nerves arriving at the visual cortex and sound from nerves arriving at the auditory cortex).\n\nIn 1945, Roger Sperry showed that it is the location in the brain to which nerves attach that determines experience. He studied amphibians whose optic nerves cross completely, so that the left eye connects to the right side of the brain and the right eye connects to the left side of the brain. He was able to cut the optic nerves and cause them to regrow on the opposite side of the brain so that the left eye now connected to the left side of the brain and the right eye connected to the right side of the brain. He then showed that these animals made the opposite movements from the ones they would have made before the operation. For example, before the operation, the animal would move to the left to get away from a large object approaching from the right. After the operation, the animal would move to the right in response to the same large object approaching from the right. Sperry showed similar results in other animals including mammals (rats), this work contributing to his Nobel Prize in 1981.\n\n"}
{"id": "43830010", "url": "https://en.wikipedia.org/wiki?curid=43830010", "title": "Law on the languages of the peoples of the Republic of Bashkortostan", "text": "Law on the languages of the peoples of the Republic of Bashkortostan\n\nThe Law on the languages of the peoples of the Republic of Bashkortostan is a law aimed at protecting and preserving the languages of the peoples of Bashkortostan.\n\nAccording to the law, all the signs and indicators should be in Bashkir and Russian.\n\nSubstantive provisions of the law include the following sections.\n\nIn the Republic of Bashkortostan recognized equality of languages. Equality of languages - the totality of the rights of peoples and the person on the conservation and comprehensive development of the native language, freedom of choice and use the language of communication.\n\nWriting the names of geographical objects and drawing inscriptions, road and other signs along with the state language of the Republic of Bashkortostan can be done in the languages of the Bashkortostan in the territories where they are concentrated.\n\nViolation of the regulations of the Bashkortostan on the use of languages of the Bashkortostan when placing road signs, names (names) of settlements, streets - shall entail the imposition of an administrative fine on officials in the amount of three thousand to five thousand rubles; for legal entities - from eight thousand to ten thousand (Article 2.3 of the Code of the Bashkortostan on administrative offenses).\n\nViolation of the regulations of the Bashkortostan on the use of the languages of the Bashkortostan, signage at registration organizations in the Republic of Bashkortostan - shall entail the imposition of an administrative fine on officials in the amount of four thousand to five thousand rubles; for legal entities - from ten thousand to fifteen thousand rubles.\n\n"}
{"id": "689617", "url": "https://en.wikipedia.org/wiki?curid=689617", "title": "Metahuman", "text": "Metahuman\n\nIn DC Comics' DC Universe, a metahuman is a human with superpowers. The term is roughly synonymous with both \"mutant\" and \"mutate\" in the Marvel Universe and \"posthuman\" in the Wildstorm and Ultimate Marvel Universes. In DC Comics, the term is used loosely in most instances to refer to any human-like being with extranormal powers and abilities, be they cosmic, mutant, science, mystic, skill or tech in nature. A significant portion of these are normal human beings born with a genetic variant called the \"metagene\", which causes them to gain powers and abilities during freak accidents or times of intense psychological distress.\n\nThe term as a referent to superheroes began in 1986 by author George R. R. Martin, first in the \"Superworld\" role playing system, and then later in his \"Wild Cards\" series of novels.\n\nThe term was first used by a fictitious race of extraterrestrials known as the Dominators when they appeared in DC Comics' \"Invasion!\" mini-series. The Dominators use this term to refer to any human native of the planet Earth with \"fictional superhuman abilities\". The prefix \"\"meta-\" simply means \"beyond\"\", denoting persons and abilities beyond human limits. Metahuman may also relate to an individual who has exceeded what is known as \"The Current Potential\", meaning one's ability to move matter with mind. (See Telekinesis).\n\nBefore the White Martians arrived on Earth, Lord Vimana, the Vimanian overlord from the Xenobrood mini-series, claimed credit for the creation of the human race both normal and metahuman, due to their introduction of superpowered alien genetic matter into human germline DNA. The Vimanians in the series forced their super powered worker drones to mate with humanity's ancestors \"Australopithecus afarensis\" (3 million years ago), and later \"Homo erectus\" (1.5 million years ago) in order to create a race of superpowered slaves.\n\nThe \"Invasion!\" mini-series provided a concept for why humans in the DC Universe would survive catastrophic events and develop superpowers. One of the Dominators discovered that select members of the human race had a \"biological variant,\" which he called the meta-gene (also spelled \"metagene\"). This gene often lay dormant until an instant of extraordinary physical and emotional stress activates it. A \"spontaneous chromosomal combustion\" then takes place, as the metagene takes the source of the biostress – be it chemical, radioactive or whatever – and turns the potential catastrophe into a catalyst for \"genetic change,\" resulting in metahuman abilities. It should also be noted that DC does not use the \"metagene concept\" as a solid editorial rule, and few writers explicitly reference the metagene when explaining a character's origin.\n\nDC also has characters born with superhuman abilities, suggesting the metagene can activate spontaneously and without any prior appearance in the ancestry. One well-known example involves Dinah Laurel Lance, the second Black Canary. Although her mother (Dinah Drake Lance, the original Black Canary) was a superhero, neither she nor her husband Larry Lance were born with any known metagenes. However, Dinah Laurel \"was\" born with a metagene, the infamous ultrasonic scream known as the Canary Cry.\n\nThe prefix \"meta-\", in this context, simply means \"beyond\"—as in \"metastable\", which is beyond regular stability and ready to collapse at the slightest disruption, or \"metamorphosis\", which is the state of going beyond a single shape. In the DC comic mini-series \"Invasion!\", the Dominators point out that the Meta-gene is contained inside every cell of the human body.\n\nIn the DC Comics universe, metahuman criminals are incarcerated in special metahuman prisons, like the prison built on Alcatraz Island, which is outfitted not only with provisions to hold criminals whose powers are science and technology-based, but even mystical dampeners to hold villains (including \"Homo magi\") whose powers are magic based. Prisoners in this facility are tagged with nanobyte tracers injected into their bloodstream that allow them to be located wherever they are.\n\nIt is possible for individuals skilled in science and biology to manipulate, dampen or modify the activities of the metagene. During the Final Crisis while the Dominators were devised a \"Gene Bomb\" able to accelerate the metagene activity to the point of cellular and physical instabilities, an \"anti-metagene virus\" was spread as a last-ditch weapon in the invaded Checkmate quarters. This \"metavirus\" has the opposite effects of the Gene Bomb, curbing and shutting down the metagene and stripping the metahumans of their powers for an unspecified amount of time.\n\nThe genetic potential for a future metagene was discovered in ancient Homo sapien's DNA (500,000 - 250,000 years ago) by the White Martian race. The White Martians performed experiments on these primitive humans, changing how the metahuman phenotype was expressed by the metagene.\n\nDue to their experiments, they altered the destiny of the human race. Whereas before, evolution would have eventually made mankind into a race of superhumans similar to the Daxamites and Kryptonians, now only a select few humans would develop metahuman powers. As punishment for this, the group of renegades known as the Hyperclan was exiled to the \"Still Zone\", a version of the Phantom Zone.\n\nThe White Martians also created a metavirus, a metagene that could be passed from host to host via touch. This metavirus was responsible for the empowerment of the very first Son of Vulcan. From that time onwards, the Sons of Vulcan passed the metavirus down in an unbroken line, sworn to hunt and kill White Martians.\n\nThe terms \"meta\" and \"metahuman\" do not refer only to humans born with biological variants. Superman and Martian Manhunter (aliens) as well as Wonder Woman (a near-goddess) and Aquaman (an Atlantean) are referred to in many instances as \"metahumans.\" It can refer to anyone with extraordinary powers, no matter the origins and including those not born with such power. According to \"Countdown to Infinite Crisis\", roughly 1.3 million metahumans live on Earth, 99.5% of whom are considered \"nuisance-level\" (such as kids who can bend spoons with their mind and the old lady \"who keeps hitting at Powerball\"). The other 0.5% are what Checkmate and the OMACs consider alpha, beta and gamma level threats. For example, Superman and Wonder Woman are categorized as alpha level, while Metamorpho is considered a beta level and Ratcatcher is considered a gamma level.\n\nThe \"52\" mini-series introduced a toxic mutagen called the Exo-gene (also referred to as the Exogene). It is a toxic gene therapy treatment created by LexCorp for the Everyman Project which creates metahuman abilities in compatible non-metahumans. It first appeared in \"52\" #4 with the first announcement of the Everyman Project in \"52\" #8. The project was controversial, creating unstable heroes that gave Luthor an \"off switch\" for their powers, creating countless mid-flight deaths.\n\nIn the short-lived DC/Marvel Comics crossover \"Amalgam Comics\" event, in the \"JLX\" series (combining \"Justice League\" and Marvel's \"X-Men\"), metahumans are replaced with metamutants (a portmanteau of metahumans and Marvel's mutants) who are said to carry a 'metamutant gene'.\n\nIn animated versions of the DC universe, the term metahuman is used in the animated series \"Static Shock\".\n\nOn the television series \"Birds of Prey\", metahumans included heroines Huntress and Dinah Lance. New Gotham has a thriving metahuman underground, mostly made of metahumans who are trying to live their own lives, although a self-hating metahuman, Claude Morton (Joe Flanigan), tries to convince the police that all metahumans are evil. In \"Birds of Prey\", metahumans are treated seemingly as a race or species; the Huntress is described as being \"half-metahuman\" on her mother's side.\n\nOn the television series \"Smallville\", metahumans can occur naturally. However, the majority are the result of exposure to kryptonite, which in the \"Smallville\" universe can turn people into superpowered \"meteor freaks\", often with psychotic side effects. For many seasons of \"Smallville\", all superpowered people other than Kryptonians were so-called meteor freaks, but as the show went on it began to explore further corners of the DC universe. Non-kryptonite metahumans include the \"Smallville\" versions of Aquaman, the Flash, Black Canary, and Zatanna.\n\nOn the animated series \"Young Justice\", the alien antagonists known as the Kroloteans have frequently used the term and have even researched into the discovery of a \"metagene\" by abducting and testing on random humans. The alien reach conduct similar experiments and kidnap a cadre of teen runaways to test for the metagene, leading several of these individuals to develop superpowers. In the episode \"Runaways,\" a S.T.A.R. Labs scientist surmises that the gene is \"opportunistic\" in as much as it causes its user to develop powers seemingly based on their personal experiences or surroundings.\n\nIn the Arrowverse family of live-action shows, \"metahuman\" is used more narrowly than in comics, typically referring to a human being who has powers, often acquired following some kind of accident.\n\nIn the television series \"Gotham\", Professor Hugo Strange experiments with dead (and alive) bodies of criminals, Arkham Asylum patients, and civilians under the orders of the Court of Owls. There, Strange gives his victims superhuman abilities such as shapeshifting (Clayface), mind control (Fish Mooney) and super strength (Azrael). By the end of Season 2, Strange's victims escape and wreak havoc over the city. Throughout the series, the metahumans are commonly referred to as Strange's Monsters, simply Monsters (an allusion to Batman & the Monster Men), or the Freaks from Indian Hill.\n\nIn the television series \"Black Lightning\", Jefferson Pierce / Black Lightning and his daughters Anissa and Jennifer are metahumans. The A.S.A. are tracking young metahumans with abilities. As the series progresses, it is revealed that decades ago, because of the city of Freeland's prevailing racial and political conflicts, the A.S.A. developed a substance supposedly as a suppressant to turn its citizens docile in the interest of controlling them but failed. Instead, it turned out to be a mutagen which transforms some of its citizens into metahumans, including Jefferson Pierce and his daughters who inherited his metagene after his fatherhood. Under Martin Procter, the agency illegally developed an addictive derivative of the drug called Green Light, which is distributed as a recreational drug in hopes to create more metahumans for Procter's agendas. Because the metahumans are dying from unstable mutations, Procter seeks to capture Black Lightning and his offspring because they are the only known stable specimens of the agency's drug. After Procter's death, the A.S.A.'s experiments are exposed and Freeland's metahumans' origin has become a public knowledge. However, it ignites an anti-metahuman bigotry amongst the people in Freeland.\n\n\n\n"}
{"id": "285549", "url": "https://en.wikipedia.org/wiki?curid=285549", "title": "Milton Diamond", "text": "Milton Diamond\n\nMilton Diamond (born March 6, 1934 in New York City) is a Professor Emeritus of anatomy and reproductive biology at the University of Hawaiʻi at Mānoa. After a career in the study of human sexuality, Diamond retired from the University in December 2009 but continues with his research and writing.\n\nMilton Diamond graduated from the City College of New York with a B.S. in biophysics in 1955, after which he spent three years in the Army as an engineering officer, stationed in Japan. On returning to the United States, he attended graduate school at University of Kansas from 1958–1962 and earned a Ph.D. in anatomy and psychology from that University. His first job was teaching at the University of Louisville, School of Medicine where he simultaneously completed two years toward an M.D., passing his Basic Medicine Boards, and in 1967 he moved to Hawaii to take up a post at the recently established John A. Burns School of Medicine. Milton Diamond had a long running feud with the psychologist Dr. John Money. In the early seventies, Diamond and Money were attending a conference on transgenderism in Dubrovnik. According to the book \"As Nature Made Him: The Boy Raised As a Girl\" (p. 174) at this conference Money initiated a loud and aggressive argument with Diamond. One witness claims that Money punched Diamond; however, Diamond himself said that he could not recall any physical contact during this encounter.\n\nDiamond is known for following up the case of David Reimer, a boy raised as a girl after a botched circumcision. This case, which Diamond renamed that of \"John/Joan\" to protect Reimer’s privacy, has become one of the most cited cases in the literature of psychiatry, anthropology, women's studies, child development, and biology of gender. With the cooperation of H. Keith Sigmundson, who had been Reimer’s supervising psychiatrist, Diamond tracked down the adult Reimer and found that John Money’s sex reassignment of Reimer had failed. Diamond was the first to alert physicians that the model of how to treat infants with intersex conditions that Reimer's case proposed was faulty. \n\nDiamond recommended that physicians do no surgery on intersexed infants without their informed consent, assign such infants in the gender to which they will probably best adjust, and refrain from adding shame, stigma and secrecy to the issue, by assisting intersexual people to meet and associate with others of like condition. Diamond similarly encouraged considering the intersex condition as a difference of sex development, not as a disorder.\n\nDiamond has written extensively about abortion and family planning, pornography, intersexuality, transsexuality, and other sex- and reproduction-related issues for professional sex and legal journals, as well as lay periodicals. He is frequently interviewed for public media and legal matters, and often serves as an expert in court proceedings, and is known for his research on the origins and development of sexual identity. Although he has retired from teaching, he continues to research and consult concerning transsexuality, intersexuality and pornography.\n\nDiamond was based at the John A. Burns School of Medicine at the University of Hawai'i at Mānoa, from 1967. He was appointed Professor of Anatomy and Reproductive Biology in 1971, and from 1985 until his retirement he was Director of the Pacific Center for Sex and Society within the School of Medicine.\n\nIn 1999 Diamond was appointed as President of the International Academy of Sex Research, and in 2001/02 as President of the Society for the Scientific Study of Sexuality.\n\nThe awards Diamond have received include:\n\n\n"}
{"id": "25782137", "url": "https://en.wikipedia.org/wiki?curid=25782137", "title": "Mircea Florian", "text": "Mircea Florian\n\nMircea Florian (; April 1, 1888 – October 31, 1960) was a Romanian philosopher and translator. Active mainly during the interwar period, he was noted as one of the leading proponents of rationalism, opposing it to the \"Trăirist\" philosophy of Nae Ionescu. His work, comprising some 20 books, shows Florian as a disciple of centrists and rationalists such as Constantin Rădulescu-Motru and Titu Maiorescu.\n\nActive in independent social democratic politics, the philosopher became a political prisoner under the communist regime. It was during his time in jail that Florian conceived his philosophical system, published after his death in the treatise \"Recesivitatea ca structură a lumii\" (\"Recessivity as World Structure\"). In 1990, he was made a posthumous member of the Romanian Academy.\n\nBorn in Bucharest, Florian graduated from the Faculty of Letters and Philosophy at the local university, where he became a disciple of Rădulescu-Motru and P. P. Negulescu. He afterward took his Ph.D. at the University of Greifswald, in the German Empire, with a thesis on Henri Bergson's notion of time. In later years, he found employment as a Bucharest University assistant and substitute professor, lecturing in the History of Philosophy. He was a Docent from 1916.\n\nDuring World War I, Florian served in the Romanian Land Forces and was taken prisoner by the Germans. He was transported to an internment camp at Krefeld, alongside figures such as Alexandru D. Strurdza and Ilie Moscovici. Eventually, Florian was freed by his captors and allowed to lecture at King Carol Foundation in Bucharest, under a German occupation government. As Florian later indicated, this was made possible by the intercession of a Germanophile scholar, Alexandru Tzigara-Samurcaș, who vouched for him, and by the protection of Constantin Giurescu. Following the November Armistice, which reinstated the pro-Allied government, Florian, Tzigara-Samurcaș and Rădulescu-Motru were all subject to an official inquiry, and accused of being collaborators. The University Commissions created for this task were largely ineffective, and, among the incriminated, Florian was one of few who presented himself for questioning.\n\nIn the wake of the war, Florian was in contact with \"Ideea Europeană\", Rădulescu-Motru's magazine, and went on its conference tour, alongside in various cities by, among others, Nae Ionescu, Cora Irineu, Octav Onicescu, Virgil Bărbat, and Emanoil Bucuța. However, in later years, he remained largely cut off from his public: said to have been shy in delivering his lectures, he led a private life, and dedicated himself, almost entirely, to research.\n\nHe wrote a large body of works over a short time, including such titles as: \"Îndrumare în filosofie\" (\"Philosophical Companion\"), \"Rostul și utilitatea filosofiei\" (\"The Purpose and Use of Philosophy\"), \"Știință și raționalism\" (\"Science and Rationalism\"), \"Cosmologia elenă\" (\"Hellenic Cosmology\"), \"Antinomiile credinței\" (\"The Antinomies of Faith\"), \"Kant și criticismul până la Fichte\" (\"Kant and the Critical Method before Fichte\"), \"Cunoaștere și existență\" (\"Knowledge and Being\"), \"Reconstrucție filosofică\" (\"Philosophical Reconstruction\"), \"Metafizică și artă\" (\"Metaphysics and Art\"), \"Misticism și credință\" (\"Mysticism and Faith\"). Although absorbed by his academic work, Florian affiliated with social democracy, and was a member of the Social Democratic Party, the Constantin Titel Petrescu wing.\n\nFlorian's philosophy developed from ideas common to both Rădulescu-Motru and the 19th-century thinker Titu Maiorescu, herald of Romania's moderate and critical approach to philosophy. Florian is therefore ranked among the third-generation \"Maiorescans\", and seen as reactivating the spirit of Maiorescu's literary club \"Junimea\". After World War I, the \"Junimist\" legacy came in direct contradiction with Nae Ionescu's critique of rationalism, which was growing in popularity and lending its support to the far right's causes. Florian's steady opposition to Ionescu, in both concept and method, has been described as the \"dualism\" of interwar Romanian philosophy.\n\nBy the time of World War II, Mircea Florian was still pursuing a debate with the two schools of irrationalism, promoted by Lucian Blaga and Constantin Noica. Granted a full professorship in 1940, he was presented for Romanian Academy membership by his mentor Rădulescu-Motru, but the proposal failed to gather support. Blaga gave poor reviews to his work in \"Saeculum\" magazine, and, in one (disputed) interpretation, may have portrayed him as the unknown adversary in the virulent lampoon \"Săpunul filozofic\" (\"Philosophic Soap\", 1943). According to literary historian Z. Ornea, Florian's relative lack of exposure is unfair, since his works may rank better than those of either Noica or Blaga. Florian was also an adversary of official fascism, before and during the Ion Antonescu dictatorship. Like Grigore T. Popa, Constantin I. Parhon, Alexandru Rosetti, Mihai Ralea, and several other academics, he was in contact with the underground Romanian Communist Party and the Union of Patriots, and, as such, kept under close surveillance by the \"Siguranța Statului\" agents.\n\nFlorian's stance, and especially his commitment to independent social democracy, made him a suspect upon the establishment of Romania's communist regime. As noted by researcher Victor Frunză, Florian and Alexandru Claudian made a \"high sacrifice\" when they refused to give in to \"blackmail\" and would not join the \"Workers' Party\" (as the Communist Party styled itself upon its absorption of Social Democratic sympathizers). In 1948, he was stripped of his university chair. Placed under constant surveillance by the Securitate, Romania's new secret police, he was soon after arrested. During his eight-month-long imprisonment without trial, he had the revelation on \"recessivity as world structure\". The new system evidenced that Florian had come to criticize some of the basic assumptions in Western philosophy, and conceiving of the world through the teachings of genetics. His system divided existence alongside its two, equal but alternating, attributes: the dominant trait tempered by the recessive (albeit not degraded) one; violence to love, rational to irrational, nationalism to supranationalism.\n\nThe imprisonment is said to have been a grueling experience: allegedly, his wife Angela no longer recognized him upon his return to the family home. A while after, Florian was partly reintegrated into academia, and assigned a researcher's position at the Institute of Philosophy. There, he dedicated himself to translating Aristotle's \"Organon\", while in committing his \"Recesivitatea\" to paper in his spare time.\n\n\"Recesivitatea\" was only published 23 years after Florian's death, with Editura Eminescu. Reportedly, the text had suffered cuts and interventions by communist censors. Florian's role was reconsidered mainly after the Romanian Revolution of 1989, when he was made a posthumous member of the Academy. His full work was recovered for the public and reviewed by philosopher Mircea Flonta, in a 1998 volume of essays.\n\n"}
{"id": "41175367", "url": "https://en.wikipedia.org/wiki?curid=41175367", "title": "Natural scene perception", "text": "Natural scene perception\n\nNatural scene perception refers to the process by which an agent (such as a human being) visually takes in and interprets scenes that it typically encounters in natural modes of operation (e.g. busy streets, meadows, living rooms). This process has been modeled in several different ways that are guided by different concepts.\n\nOne major dividing line between theories that explain natural scene perception is the role of attention. Some theories maintain the need for focused attention, while others claim that focused attention is not involved.\n\nFocused attention played a partial role in early models of natural scene perception. Such models involved two stages of visual processing. According to these models, the first stage is attention free and registers low level features such as brightness gradients, motion and orientation in a parallel manner. Meanwhile, the second stage requires focused attention. It registers high-level object descriptions, has limited capacity and operates serially. These models have been empirically informed by studies demonstrating change blindness, inattentional blindness and attentional blink. Such studies show that when one's visual focused attention is engaged by a task, significant changes in one's environment that are not directly pertinent to the task can escape awareness. It was generally thought that natural scene perception was similarly susceptible to change blindness, inattentional blindness and attentional blink, and that these psychological phenomena occurred because engaging in a task diverts attentional resources that would otherwise be used for natural scene perception. \n\nThe attention-free hypothesis soon emerged to challenge early models. The initial basis for the attention-free hypothesis was the finding that in visual search, basic visual features of objects immediately and automatically pop out to the person doing the visual search. Further experiments seemed to support this: Potter (as cited by Evans & Treisman, 2005) showed that high-order representations can be accessed rapidly from natural scenes presented at rates of up to 10 per second. Additionally, Thorpe, Fize & Marlot (as cited by Evans & Treisman) discovered that humans and primates can categorize natural images (i.e. of animals in everyday indoor and outdoor scenes) rapidly and accurately even after brief exposures. The basic idea in these studies is that exposure to each individual scene is too brief for attentional processes to occur, yet human beings are able to interpret and categorize these scenes.\n\nWeaker versions of the attention-free hypothesis have also been targeted at specific components of the natural scene perception process instead of the process as a whole. Kihara & Takeda (2012) limit their claim to saying that it is the integration of spatial frequency-based information in natural scenes (a sub-process of natural scene perception) that is attention free. This claim is based on a study of theirs which used attention-demanding tasks to examine participants' abilities to accurately categorize images that were filtered to have a wide range of spatial frequencies. The logic behind this experiment was that if integration of visual information across spatial frequencies (measured by the categorization task) is preattentive, then attention-demanding tasks should not affect performance in the categorization task. This was indeed found to be the case. \n\nA recent study by Cohen, Alvarez & Nakayama (2011) calls into question the validity of evidence supporting the attention-free hypothesis. They found that participants did display inattentional blindness while doing certain kinds of multiple-object tracking (MOT) and rapid serial visual presentation (RSVP) tasks. Furthermore, Cohen et al. found that participants' natural scene perception was impaired under dual-task conditions, but that this dual-task impairment happened only when participants' primary task was sufficiently demanding. The authors concluded that previous studies showing the absence of a need for focused attention did not use tasks that were demanding enough to fully engage attention.\n\nIn the Cohen et al. study, the MOT task involved viewing eight black moving discs presented against a changing background that consisted of randomly colored checkerboard masks. Four of these discs were picked out and participants were instructed to track these four discs. The RSVP task involved viewing a stream of letters and digits presented against a series of changing checkerboards, and counting the number of times a digit was presented. In both experiments, the critical trial involved a natural scene suddenly replacing the second last checkerboard, and participants were immediately afterwards asked whether they had noticed anything different, as well as presented with six questions to determine whether they had categorized the scene. The dual-task condition simply involved participants performing the MOT task mentioned above and a scene-classification task simultaneously. The authors varied the difficulty of the task (i.e. how demanding the task was) by increasing or decreasing the speed of the moving discs.\n\nThese are some of the models that have been proposed for the purpose of explaining natural scene perception.\n\nEvans & Treisman (2005) proposed a hypothesis that humans rapidly detect disjunctive sets of unbound features of target categories in a parallel manner, and then use these features to discriminate between scenes that do or do not contain the target without necessarily fully identifying it. An example of such a feature would be outstretched wings that can be used to tell whether or not a bird is in a picture, even before the system has identified an object as a bird. Evans & Treisman propose that natural scene perception involves a first pass through the visual processing hierarchy up to the nodes in a visual identification network, and then optional revisiting of earlier levels for more detailed analysis. During the 'first pass' stage, the system forms a global representation of the natural scene that includes the layout of global boundaries and potential objects. During the 'revisiting' stage, focused attention is employed to select local objects of interest in a serial manner, and then bind their features to their representations.\n\nThis hypothesis is consistent with the results of their study in which participants were instructed to detect animal targets in RSVP sequences, and then report their identities and locations. While participants were able to detect the targets in most trials, they were often subsequently unable to identify or localize them. Furthermore, when two targets were presented in quick succession, participants displayed a significant attentional blink when required to identify the targets, but the attentional blink was mostly eliminated among participants only required to only detect them. Evans & Treisman explain these results by with the hypothesis that the attentional blink occurs because the identification stage requires attentional resources, while the detection stage does not.\n\nUltra-rapid visual categorization is a model proposing an automatic feedforward mechanism that forms high-level object representations in parallel without focused attention. In this model, the mechanism cannot be sped up by training. Evidence for a feedforward mechanism can be found in studies that have shown that many neurons are already highly selective at the beginning of a visual response, thus suggesting that feedback mechanisms are not required for response selectivity to increase. Furthermore, recent fMRI and ERP studies have shown that masked visual stimuli that participants do not consciously perceive can significantly modulate activity in the motor system, thus suggesting somewhat sophisticated visual processing.\nVanRullen (2006) ran simulations showing that the feedforward propagation of one wave of spikes through high-level neurons, generated in response to a stimulus, could be enough for crude recognition and categorization that occurs in 150 ms or less.\n\nXu & Chun (2009) propose the neural-object file theory, which posits that the human visual system initially selects a fixed number of roughly four objects from a crowded scene based on their spatial information (object individuation) before encoding their details (object identification). Under this framework, object individuation is generally controlled by the inferior intra-parietal sulcus (IPS), while object identification involves the superior IPS and higher-level visual areas. At the object individuation stage, object representations are coarse and contain minimal feature information. However, once these object representations (or object-files, to use the theory's language) have been 'set up' during the object individuation stage they can be elaborated on over time during the object identification stage, during which additional featural and identity information is received.\n\nThe neural-object file theory deals with the issue of attention by proposing two different processing systems. One of them tracks the overall hierarchical structure of the visual display and is attention-free, while the other processes current objects of attentional selection. The current hypothesis is that the parahippocampal place area (PPA) plays a role in shifting visual attention to different parts of a scene and incorporating information from multiple frames in order to form an integrated representation of the scene.\n\nThe separation between object individuation and identification in the neural object-file theory is supported by evidence such as that from Xu's & Chun's fMRI study (as cited in Xu & Chun, 2009). In this study, they examined posterior brain mechanisms that supported visual short-term memory (VSTM). The fMRI showed that representations in the inferior IPS were fixed to roughly four objects regardless of object complexity, but representations in the superior IPS and lateral occipital complex (LOC) varied according to complexity.\n"}
{"id": "6501264", "url": "https://en.wikipedia.org/wiki?curid=6501264", "title": "New Haven Black Panther trials", "text": "New Haven Black Panther trials\n\nIn 1970 there was a series of criminal prosecutions in New Haven, Connecticut against various members of the Black Panther Party. The charges ranged from criminal conspiracy to felony murder. All indictments stemmed from the murder of 19-year-old Alex Rackley in the early hours of May 21, 1969. The trials became a rallying-point for the American Left, and marked a decline in public support, even among the black community, for the Black Panther Party.\n\nOn May 19, 1969, members of the Black Panther Party kidnapped fellow Panther Alex Rackley, who had fallen under suspicion of informing for the FBI. He was held captive at the New Haven Panther headquarters on Orchard Street, where he was tortured and interrogated for two days until he confessed. His interrogation was tape recorded by the Panthers. During that time, national party chairman Bobby Seale visited New Haven and spoke on the campus of Yale University for the Yale Black Ensemble Theater Company. After his speech, Seale briefly stopped by the headquarters where Rackley was being held captive, though it was never proven that he went inside or knew about Rackley's treatment.\n\nEarly the next day, three Panthers - Warren Kimbro, Lonnie McLucas, and national Panther \"Field Marshal\" George Sams, Jr. - drove Rackley to the nearby town of Middlefield, Connecticut. Kimbro shot Rackley once in the head and McLucas shot him once in the chest. They dumped his corpse in the Coginchaug River, where it was discovered the next day.\n\nPolice raided the Panther headquarters, eventually arresting nine New Haven area Black Panthers (in addition to two juveniles). McLucas and Sams were captured later.\n\nSams and Kimbro confessed to the murder, and agreed to testify against McLucas in exchange for a reduction in sentence. Sams also implicated Seale in the killing, telling his interrogators that while visiting the Panther headquarters on the night of his speech, Seale had directly ordered him to murder Rackley. In all, nine defendants were indicted on charges related to the case. In the heated political rhetoric of the day, these defendants were referred to as the \"New Haven Nine\", a deliberate allusion to other \"cause-celebre\" defendants like the \"Chicago Seven\".\n\nThe first trial was that of Lonnie McLucas, the only person who physically took part in the killing who refused to plead guilty. In fact, McLucas had confessed to shooting Rackley, but since some of the charges brought against him made him eligible for the death penalty, a not-guilty plea was the only logical trial strategy.\n\nJury selection began in May 1970. The case and trial were already a national \"cause célèbre\" among critics of the Nixon administration, and especially among those hostile to the actions of the FBI. Under the Bureau's then-secret \"Counter-Intelligence Program\" (COINTELPRO), FBI director J. Edgar Hoover had ordered his agents to disrupt, discredit, or otherwise neutralize radical groups like the Panthers. Hostility between groups organizing political dissent and the Bureau was, by the time of the trials, at a fever pitch. Hostility from the left was also directed at the two Panthers cooperating with the prosecutors. Sams in particular, never widely popular in the movement, was accused of being an informant, and lying to implicate Seale to hide his own complicity with the FBI.\n\nBeginning with the pretrial proceedings, tens of thousands of supporters of the Panthers arrived in New Haven individually and in organized groups. They were housed and fed by community organizations and by sympathetic Yale students in their dormitory rooms. The Yale college dining halls provided basic meals for everyone. Protesters met daily \"en masse\" on the New Haven Green across the street from the Courthouse (and one hundred yards from Yale's main gate) to hear protest speakers. Among the speakers were Jean Genet, Benjamin Spock, Abbie Hoffman, Jerry Rubin, and John Froines (an assistant professor of chemistry at the University of Oregon). Teach-ins and other events were also held in the colleges themselves.\n\nTowards midnight on May 1, two bombs exploded in Yale's Ingalls Rink, where a concert was being held in conjunction with the protests. Although the rink was damaged, no one was injured, and no culprit was identified.\n\nYale chaplain William Sloane Coffin stated, \"All of us conspired to bring on this tragedy by law enforcement agencies by their illegal acts against the Panthers, and the rest of us by our immoral silence in front of these acts,\" while Yale President Kingman Brewster Jr. issued the statement, \"I personally want to say that I'm appalled and ashamed that things should have come to such a pass that I am skeptical of the ability of Black revolutionaries to achieve a fair trial anywhere in the U.S.\" Brewster's generally sympathetic tone enraged many of the university's older, more conservative alumni, heightening tensions within the school community.\n\nAs tensions mounted, Yale officials sought to avoid deeper unrest and to deflect the real possibility of riots or violent student demonstrations. Sam Chauncey has been credited with winning tactical management on behalf of the administration to quell anxiety among law enforcement and New Haven's citizens, while Kurt Schmoke, a future Rhodes Scholar, mayor of Baltimore, MD and Dean of Howard University School of Law, has received kudos as undergraduate spokesman to the faculty during some of the protest's tensest moments. Ralph Dawson, a classmate of Schmoke's, figured prominently as moderator of the Black Student Alliance at Yale (BSAY).\n\nIn the end, compromises between the administration and the students quashed the possibility of violence. While Yale (and many other colleges) went \"on strike\" from May Day until the end of the term, like most schools it was not actually \"shut down\". Classes were made \"voluntarily optional\" for the time and students were graded \"Pass/Fail\" for the work done up to then.\n\nMcLucas's trial set new records for the scale of judicial proceedings in Connecticut. It was the first in Connecticut to have metal detectors installed at the courtroom doors; jury selection took six weeks, a Connecticut record, and the jury deliberated for six days, another Connecticut record. Despite impassioned accusations from protesters that McLucas was being railroaded into the electric chair by a \"racist jury,\" the jurors (ten white, two black) acquitted him on the most severe charges, convicting him instead on the sole charge of conspiracy to commit murder. His defense attorney declared, \"The judge was fair, the jury was fair, and, in this case, a black revolutionary was given a fair trial.\" McLucas was sentenced to twelve to fifteen years in prison. His two collaborators in the murder, who had pleaded to second degree murder, were released after four years.\n\nIn October, 1970, Bobby Seale went on trial alongside Ericka Huggins, founder of the New Haven chapter. This trial was an even larger undertaking, involving a full four months of jury selection. Seale's attorney emphasized that it was only Sams' testimony that tied Seale to Rackley's murder. Huggins's voice was heard on a tape recording of the victim's interrogation, played for the jury. Her lawyer argued that Huggins had been acting under duress. The jury was unable to reach a verdict, deadlocked 11 to 1 for Seale's acquittal and 10 to 2 for Huggins' acquittal. On May 25, 1971 Judge Harold Mulvey stunned courtroom spectators by dismissing the charges against Huggins and Seale saying, \"I find it impossible to believe that an unbiased jury could be selected without superhuman efforts -- efforts which this court, the state and these defendants should not be called upon either to make or to endure\".\n\nBoth the Panthers and the FBI suffered damage to their reputations, after the public exposure of their most unsavory activities.\n\nIn 1971, a group of left-wing radicals calling themselves the Citizens' Commission to Investigate the FBI burglarized an FBI field office in Media, Pennsylvania, which was found to be spying on Swarthmore College students and faculty. Among the materials stolen in this break-in were documents revealing the nature of the COINTELPRO program. Within the year, Director Hoover declared that the centralized COINTELPRO was over.\n\nFor the Panthers, the Seale trial may have been the height of their national exposure and their popularity among the broader left-wing movement. A string of violent confrontations with law-enforcement, along with the trials and convictions of national party leaders that followed, left the movement spent and adrift, and by the mid-1970s it was largely inactive.\n\nThe trial surfaced again in the news in 2000, when former first lady Hillary Clinton ran for U.S. Senate in the state of New York. Anti-Clinton activists discovered that during the trials, Clinton (then a Yale law student named Hillary Rodham) volunteered to monitor the trial for violations of civil rights, for the American Civil Liberties Union. Widely circulated mass e-mails erroneously ascribed to Clinton responsibility for \"getting the defendants off,\" and also blamed the future head of the Clinton U.S. Justice Department's Civil Rights Division, Bill Lann Lee, who was a Yale undergraduate at the time. Although both were much too junior to have had any role in the actual legal defense, according to John Elvin of the conservative newsmagazine \"Insight on the News\", \"Insight reviewed biographies of Hillary Clinton by Milton, [David] Brock and Roger Morris for this story and lengthy selections from such other biographies as Barbara Olson's Hell to Pay. Together, relying on primary and other firsthand sources, they unquestionably back David Horowitz's contention that Hillary was a campus leader during the Panther protests\"; Lee apparently played no prominent role in any protests.\n\nIn 2006, Kelly Moye revealed that he was a police informant recruited to infiltrate the Panthers by Nick Pastore, head of New Haven Police's Intelligence Division. George Sams held a gun to Moye's head and demanded he turn over his car. Rackley was driven to his death in Moye's car. The police followed Moye's car as it left toward the murder destination but lost sight of the car prior to the murder. The State Police discovered Rackley's body the next day. This epilogue was detailed in an article \"After 37 Years, Spy Comes In From Cold\" by Paul Bass, author of Murder In The Model City: The Black Panthers, Yale and the Redemption of a Killer.\n\nDetective Nick Pastore, who arrested Seale and brought him to New Haven to stand trial, went on to become New Haven's Chief of Police, widely renowned for his successful policy of community policing, and now heads a nonprofit advocacy group in Washington, DC named Criminal Justice Policy. Thirty one years later, when Seale returned to New Haven to speak at the Yale Repertory Theatre, Pastore decided to attend and even presented Seale with a pink porcelain pig and a hug, congratulating him for continuing \"the struggle\".\n\n\n"}
{"id": "333677", "url": "https://en.wikipedia.org/wiki?curid=333677", "title": "Nowhere continuous function", "text": "Nowhere continuous function\n\nIn mathematics, a nowhere continuous function, also called an everywhere discontinuous function, is a function that is not continuous at any point of its domain. If \"f\" is a function from real numbers to real numbers, then \"f\" is nowhere continuous if for each point \"x\" there is an such that for each we can find a point \"y\" such that and . Therefore, no matter how close we get to any fixed point, there are even closer points at which the function takes not-nearby values.\n\nMore general definitions of this kind of function can be obtained, by replacing the absolute value by the distance function in a metric space, or by using the definition of continuity in a topological space.\n\nOne example of such a function is the indicator function of the rational numbers, also known as the Dirichlet function, named after German mathematician Peter Gustav Lejeune Dirichlet. This function is denoted as \"I\" and has domain and codomain both equal to the real numbers. \"I\"(\"x\") equals 1 if \"x\" is a rational number and 0 if \"x\" is not rational. If we look at this function in the vicinity of some number \"y\", there are two cases: \nIn less rigorous terms, between any two irrationals, there is a rational, and vice versa.\n\nThe Dirichlet function can be constructed as the double pointwise limit of a sequence of continuous functions, as follows:\n\nfor integer \"j\" and \"k\".\n\nThis shows that the Dirichlet function is a Baire class 2 function. It cannot be a Baire class 1 function because a Baire class 1 function can only be discontinuous on a meagre set.\n\nIn general, if \"E\" is any subset of a topological space \"X\" such that both \"E\" and the complement of \"E\" are dense in \"X\", then the real-valued function which takes the value 1 on \"E\" and 0 on the complement of \"E\" will be nowhere continuous. Functions of this type were originally investigated by Peter Gustav Lejeune Dirichlet.\n\nA real function \"f\" is nowhere continuous if its natural hyperreal extension has the property that every \"x\" is infinitely close to a \"y\" such that the difference is appreciable (i.e., not infinitesimal).\n\n\n"}
{"id": "3361957", "url": "https://en.wikipedia.org/wiki?curid=3361957", "title": "Poison pen letter", "text": "Poison pen letter\n\nA poison pen letter is a letter or note containing unpleasant, abusive, or malicious statements or accusations about the recipient or a third party. It is usually sent anonymously. In the term \"poison pen\", the word poison is used figuratively, rather than literally. Poison pen letters are usually composed and sent to upset the recipient. They differ from blackmail, which is intended to obtain something from the recipient. In contrast, poison pen letters are purely malicious.\n\nIn the United Kingdom, Section 1 of the Malicious Communications Act 1988 covers most cases of poison pen letters.\n\n"}
{"id": "29929800", "url": "https://en.wikipedia.org/wiki?curid=29929800", "title": "Prudence (given name)", "text": "Prudence (given name)\n\nPrudence is a feminine given name. The name is a Medieval form of the Latin \"Prudentia\", meaning prudence, i.e. good judgment. \n\nThe usual diminutive or short form is Pru or Prue. These may also be short for the unrelated name Prunella, which means plum.\n\n\n"}
{"id": "4031859", "url": "https://en.wikipedia.org/wiki?curid=4031859", "title": "Q–Q plot", "text": "Q–Q plot\n\nIn statistics, a Q–Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other. First, the set of intervals for the quantiles is chosen. A point on the plot corresponds to one of the quantiles of the second distribution (-coordinate) plotted against the same quantile of the first distribution (-coordinate). Thus the line is a parametric curve with the parameter which is the number of the interval for the quantile.\n\nIf the two distributions being compared are similar, the points in the Q–Q plot will approximately lie on the line . If the distributions are linearly related, the points in the Q–Q plot will approximately lie on a line, but not necessarily on the line . Q–Q plots can also be used as a graphical means of estimating parameters in a location-scale family of distributions.\n\nA Q–Q plot is used to compare the shapes of distributions, providing a graphical view of how properties such as location, scale, and skewness are similar or different in the two distributions. Q–Q plots can be used to compare collections of data, or theoretical distributions. The use of Q–Q plots to compare two samples of data can be viewed as a non-parametric approach to comparing their underlying distributions. A Q–Q plot is generally a more powerful approach to do this than the common technique of comparing histograms of the two samples, but requires more skill to interpret. Q–Q plots are commonly used to compare a data set to a theoretical model. This can provide an assessment of \"goodness of fit\" that is graphical, rather than reducing to a numerical summary. Q–Q plots are also used to compare two theoretical distributions to each other. Since Q–Q plots compare distributions, there is no need for the values to be observed as pairs, as in a scatter plot, or even for the numbers of values in the two groups being compared to be equal.\n\nThe term \"probability plot\" sometimes refers specifically to a Q–Q plot, sometimes to a more general class of plots, and sometimes to the less commonly used P–P plot. The probability plot correlation coefficient plot (PPCC plot) is a quantity derived from the idea of Q–Q plots, which measures the agreement of a fitted distribution with observed data and which is sometimes used as a means of fitting a distribution to data.\n\nA Q–Q plot is a plot of the quantiles of two distributions against each other, or a plot based on estimates of the quantiles. The pattern of points in the plot is used to compare the two distributions.\n\nThe main step in constructing a Q–Q plot is calculating or estimating the quantiles to be plotted. If one or both of the axes in a Q–Q plot is based on a theoretical distribution with a continuous cumulative distribution function (CDF), all quantiles are uniquely defined and can be obtained by inverting the CDF. If a theoretical probability distribution with a discontinuous CDF is one of the two distributions being compared, some of the quantiles may not be defined, so an interpolated quantile may be plotted. If the Q–Q plot is based on data, there are multiple quantile estimators in use. Rules for forming Q–Q plots when quantiles must be estimated or interpolated are called plotting positions.\n\nA simple case is where one has two data sets of the same size. In that case, to make the Q–Q plot, one orders each set in increasing order, then pairs off and plots the corresponding values. A more complicated construction is the case where two data sets of different sizes are being compared. To construct the Q–Q plot in this case, it is necessary to use an interpolated quantile estimate so that quantiles corresponding to the same underlying probability can be constructed.\n\nMore abstractly, given two cumulative probability distribution functions and , with associated quantile functions and (the inverse function of the CDF is the quantile function), the Q–Q plot draws the -th quantile of against the -th quantile of for a range of values of . Thus, the Q–Q plot is a parametric curve indexed over [0,1] with values in the real plane .\n\nThe points plotted in a Q–Q plot are always non-decreasing when viewed from left to right. If the two distributions being compared are identical, the Q–Q plot follows the 45° line . If the two distributions agree after linearly transforming the values in one of the distributions, then the Q–Q plot follows some line, but not necessarily the line . If the general trend of the Q–Q plot is flatter than the line , the distribution plotted on the horizontal axis is more dispersed than the distribution plotted on the vertical axis. Conversely, if the general trend of the Q–Q plot is steeper than the line , the distribution plotted on the vertical axis is more dispersed than the distribution plotted on the horizontal axis. Q–Q plots are often arced, or \"S\" shaped, indicating that one of the distributions is more skewed than the other, or that one of the distributions has heavier tails than the other.\n\nAlthough a Q–Q plot is based on quantiles, in a standard Q–Q plot it is not possible to determine which point in the Q–Q plot determines a given quantile. For example, it is not possible to determine the median of either of the two distributions being compared by inspecting the Q–Q plot. Some Q–Q plots indicate the deciles to make determinations such as this possible.\n\nThe intercept and slope of a linear regression between the quantiles gives a measure of the relative location and relative scale of the samples. If the median of the distribution plotted on the horizontal axis is 0, the intercept of a regression line is a measure of location, and the slope is a measure of scale. The distance between medians is another measure of relative location reflected in a Q–Q plot. The \"probability plot correlation coefficient\" (PPCC plot) is the correlation coefficient between the paired sample quantiles. The closer the correlation coefficient is to one, the closer the distributions are to being shifted, scaled versions of each other. For distributions with a single shape parameter, the probability plot correlation coefficient plot provides a method for estimating the shape parameter – one simply computes the correlation coefficient for different values of the shape parameter, and uses the one with the best fit, just as if one were comparing distributions of different types.\n\nAnother common use of Q–Q plots is to compare the distribution of a sample to a theoretical distribution, such as the standard normal distribution , as in a normal probability plot. As in the case when comparing two samples of data, one orders the data (formally, computes the order statistics), then plots them against certain quantiles of the theoretical distribution.\n\nThe choice of quantiles from a theoretical distribution can depend upon context and purpose. One choice, given a sample of size , is for , as these are the quantiles that the sampling distribution realizes. The last of these, , corresponds to the 100th percentile – the maximum value of the theoretical distribution, which is sometimes infinite. Other choices are the use of , or instead to space the points evenly in the uniform distribution, using .\n\nMany other choices have been suggested, both formal and heuristic, based on theory or simulations relevant in context. The following subsections discuss some of these. A narrower question is choosing a maximum (estimation of a population maximum), known as the German tank problem, for which similar \"sample maximum, plus a gap\" solutions exist, most simply . A more formal application of this uniformization of spacing occurs in maximum spacing estimation of parameters.\n\nThe approach equals that of plotting the points according to the probability that the last of () randomly drawn values will not exceed the -th smallest of the first randomly drawn values.\n\nIn using a normal probability plot, the quantiles one uses are the rankits, the quantile of the expected value of the order statistic of a standard normal distribution.\n\nMore generally, Shapiro–Wilk test uses the expected values of the order statistics of the given distribution; the resulting plot and line yields the generalized least squares estimate for location and scale (from the intercept and slope of the fitted line).\nAlthough this is not too important for the normal distribution (the location and scale are estimated by the mean and standard deviation, respectively), it can be useful for many other distributions.\n\nHowever, this requires calculating the expected values of the order statistic, which may be difficult if the distribution is not normal.\n\nAlternatively, one may use estimates of the \"median\" of the order statistics, which one can compute based on estimates of the median of the order statistics of a uniform distribution and the quantile function of the distribution; this was suggested by .\n\nThis can be easily generated for any distribution for which the quantile function can be computed, but conversely the resulting estimates of location and scale are no longer precisely the least squares estimates, though these only differ significantly for small.\n\nFor the quantiles of the comparison distribution typically the formula is used.\nSeveral different formulas have been used or proposed as affine symmetrical plotting positions. Such formulas have the form for some value of in the range from 0 to 1/2, which gives a range between and .\n\nOther expressions include:\nFor large sample size, , there is little difference between these various expressions.\n\nThe order statistic medians are the medians of the order statistics of the distribution. These can be expressed in terms of the quantile function and the order statistic medians for the continuous uniform distribution by:\nwhere are the uniform order statistic medians and is the quantile function for the desired distribution. The quantile function is the inverse of the cumulative distribution function (probability that is less than or equal to some value). That is, given a probability, we want the corresponding quantile of the cumulative distribution function.\n\nJames J. Filliben uses the following estimates for the uniform order statistic medians:\nThe reason for this estimate is that the order statistic medians do not have a simple form.\n\n\n"}
{"id": "6294571", "url": "https://en.wikipedia.org/wiki?curid=6294571", "title": "Rank-dependent expected utility", "text": "Rank-dependent expected utility\n\nThe rank-dependent expected utility model (originally called anticipated utility) is a generalized expected utility model of choice under uncertainty, designed to explain the behaviour observed in the Allais paradox, as well as for the observation that many people both purchase lottery tickets (implying risk-loving preferences) and insure against losses (implying risk aversion).\n\nA natural explanation of these observations is that individuals overweight low-probability events such as winning the lottery, or suffering a disastrous insurable loss. In the Allais paradox, individuals appear to forgo the chance of a very large gain to avoid a one per cent chance of missing out on an otherwise certain large gain, but are less risk averse when offered the chance of reducing an 11 per cent chance of loss to 10 per cent.\n\nA number of attempts were made to model preferences incorporating probability theory, most notably the original version of prospect theory, presented by Daniel Kahneman and Amos Tversky (1979). However, all such models involved violations of first-order stochastic dominance. In prospect theory, violations of dominance were avoided by the introduction of an 'editing' operation, but this gave rise to violations of transitivity.\n\nThe crucial idea of rank-dependent expected utility was to overweigh only unlikely extreme outcomes, rather than all unlikely events. Formalising this insight required transformations to be applied to the cumulative probability distribution function, rather than to individual probabilities (Quiggin, 1982, 1993).\n\nThe central idea of rank-dependent weightings was then incorporated by Daniel Kahneman and Amos Tversky into prospect theory, and the resulting model was referred to as cumulative prospect theory (Tversky & Kahneman, 1992).\n\nAs the name implies, the rank-dependent model is applied to the increasing rearrangement formula_1 of formula_2 which satisfies formula_3.\n\nformula_4\nwhere formula_5 and formula_6 is a probability weight such that \nformula_7\n\nfor a transformation function formula_8 with formula_9, formula_10.\n\nNote that \nformula_11\nso that the decision weights sum to 1.\n\n\n"}
{"id": "5694714", "url": "https://en.wikipedia.org/wiki?curid=5694714", "title": "Sanctity of life", "text": "Sanctity of life\n\nIn religion and ethics, the inviolability or sanctity of life is a principle of implied protection regarding aspects of sentient life that are said to be holy, sacred, or otherwise of such value that they are not to be violated. This can be applied to both animals and humans, for instance in religions that practice Ahimsa, as both are seen as holy and worthy of life.\n\nThe concept of inviolability is an important tie between the ethics of religion and the ethics of law, as each seeks justification for its principles as based on both purity and natural concept, as well as in universality of application.\n\nThe phrase \"sanctity of life\" refers to the idea that human life is sacred, holy, and precious, argued mainly by the pro-life side in political and moral debates over such controversial issues as abortion, contraception, euthanasia, embryonic stem-cell research, and the \"right to die\" in the United States, Canada, United Kingdom and other English-speaking countries. (Comparable phrases are also used in other languages.) Although the phrase was used primarily in the 19th century in Protestant discourse, after World War II the phrase has been used in Catholic moral theology and, following \"Roe v. Wade\", Evangelical Christian moral rhetoric.\n\nThe sanctity of life principle, which is often contrasted with the \"quality of life\" to some extent, is the basis of all Catholic teaching about the fifth commandment in the Ten Commandments. \n\nIn Western thought, sanctity of life is usually applied solely to the human species (anthropocentrism, sometimes called dominionism), in marked contrast to many schools of Eastern philosophy, which often hold that all animal life is sacred―in some cases to such a degree that, for example, practitioners of Jainism carry brushes with which to sweep insects from their path, lest they inadvertently tread upon them. George Carlin, American social critic and author, challenged this viewpoint in his 1996 album and HBO special \"Back in Town\".\n\n\n"}
{"id": "19016743", "url": "https://en.wikipedia.org/wiki?curid=19016743", "title": "Sandwich class", "text": "Sandwich class\n\nThe sandwich class is an informal term used in Singapore and Hong Kong to refer to the middle class. \n\nVery generally, the sandwich class consists of lower-middle-class people who feel \"squeezed\" — although they are not poor, they are not able to achieve their aspirations as people with a higher income. \n\nIn Hong Kong, this comprises families with an income of US$20,000–40,000 per year. Per capita income is typically around US$10,000 per year in Hong Kong, so this places them far above the average family in the territory. However, given very high real estate prices, it is nowhere near enough for them to afford a private residence. Hence, they are \"sandwiched\" between the large population who truly need public assistance, and the smaller number of people who can afford private residences and other luxury goods.\n\n"}
{"id": "27802972", "url": "https://en.wikipedia.org/wiki?curid=27802972", "title": "Scatterplot smoothing", "text": "Scatterplot smoothing\n\nIn statistics, several scatterplot smoothing methods are available to fit a function through the points of a scatterplot to best represent the relationship between the variables.\n\nScatterplots may be smoothed by fitting a line to the data points in a diagram. This line attempts to display the non-random component of the association between the variables in a 2D scatter plot. Smoothing attempts to separate the non-random behaviour in the data from the random fluctuations, removing or reducing these fluctuations, and allows prediction of the response based value of the explanatory variable.\n\nSmoothing is normally accomplished by using any one of the techniques mentioned below.\n\n\nThe smoothing curve is chosen so as to provide the best fit in some sense, often defined as the fit that results in the minimum sum of the squared errors (a least squares criterion).\n\n"}
{"id": "889172", "url": "https://en.wikipedia.org/wiki?curid=889172", "title": "Selective perception", "text": "Selective perception\n\nSelective perception is the tendency not to notice and more quickly forget stimuli that cause emotional discomfort and contradict our prior beliefs. For example, a teacher may have a favorite student because they are biased by in-group favoritism. The teacher ignores the student's poor attainment. Conversely, they might not notice the progress of their least favorite student.\n\nSelective perception is the process by which individuals perceive what they want to in media messages while ignoring opposing viewpoints. It is a broad term to identify the behavior all people exhibit to tend to \"see things\" based on their particular frame of reference. It also describes how we categorize and interpret sensory information in a way that favors one category or interpretation over another. In other words, selective perception is a form of bias because we interpret information in a way that is congruent with our existing values and beliefs. Psychologists believe this process occurs automatically.\n\nSelective perception may refer to any number of cognitive biases in psychology related to the way expectations affect perception. Human judgment and decision making is distorted by an array of cognitive, perceptual and motivational biases, and people tend not to recognise their own bias, though they tend to easily recognise (and even overestimate) the operation of bias in human judgment by others. One of the reasons this might occur might be because people are simply bombarded with too much stimuli every day to pay equal attention to everything, therefore, they pick and choose according to their own needs.\n\nTo understand when and why a particular region of a scene is selected, studies observed and described the eye movements of individuals as they go about performing specific tasks. In this case, vision was an active process that integrated scene properties with specific, goal-oriented oculomotor behaviour.\n\nSeveral other studies have shown that students who were told they were consuming alcoholic beverages (which in fact were non-alcoholic) perceived themselves as being \"drunk\", exhibited fewer physiological symptoms of social stress, and drove a simulated car similarly to other subjects who had actually consumed alcohol. The result is somewhat similar to the placebo effect.\n\nIn one classic study on this subject related to the hostile media effect (which is itself an example of selective perception), viewers watched a filmstrip of a particularly violent Princeton-Dartmouth American football game. Princeton viewers reported seeing nearly twice as many rule infractions committed by the Dartmouth team than did Dartmouth viewers. One Dartmouth alumnus did not see any infractions committed by the Dartmouth side and erroneously assumed he had been sent only part of the film, sending word requesting the rest.\n\nSelective perception is also an issue for advertisers, as consumers may engage with some ads and not others based on their pre-existing beliefs about the brand. \n\nSeymour Smith, a prominent advertising researcher, found evidence for selective perception in advertising research in the early 1960s, and he defined it to be \"a procedure by which people let in, or screen out, advertising material they have an opportunity to see or hear. They do so because of their attitudes, beliefs, usage preferences and habits, conditioning, etc.\" People who like, buy, or are considering buying a brand are more likely to notice advertising than are those who are neutral toward the brand. This fact has repercussions within the field of advertising research because any post-advertising analysis that examines the differences in attitudes or buying behavior among those aware versus those unaware of advertising is flawed unless pre-existing differences are controlled for. Advertising research methods that utilize a longitudinal design are arguably better equipped to control for selective perception. \n\nSelective perceptions are of two types:\n\n\n"}
{"id": "4420230", "url": "https://en.wikipedia.org/wiki?curid=4420230", "title": "Self-pity", "text": "Self-pity\n\nSelf-pity is a psychological state of mind.\n\nThough the primary focus of self-pity is on the self and one's own emotions that are within, it also has a strong interpersonal component. Being an interpersonal emotion is directing the emotional feeling or response toward others with the goal of attracting attention, empathy or help. However, some who are dealing with self-pity usually look outside of themselves for the source of their problems which only leads to a downward spiral of issues.\n\n"}
{"id": "500163", "url": "https://en.wikipedia.org/wiki?curid=500163", "title": "Self-replicating spacecraft", "text": "Self-replicating spacecraft\n\nThe idea of self-replicating spacecraft has been applied – in theory – to several distinct \"tasks\". The particular variant of this idea applied to the idea of space exploration is known as a von Neumann probe. Other variants include the Berserker and an automated terraforming seeder ship.\n\nIn theory, a self-replicating spacecraft could be sent to a neighbouring planetary system, where it would seek out raw materials (extracted from asteroids, moons, gas giants, etc.) to create replicas of itself. These replicas would then be sent out to other planetary systems. The original \"parent\" probe could then pursue its primary purpose within the star system. This mission varies widely depending on the variant of self-replicating starship proposed.\n\nGiven this pattern, and its similarity to the reproduction patterns of bacteria, it has been pointed out that von Neumann machines might be considered a form of life. In his short story, \"Lungfish\" (see Self-replicating machines in fiction), David Brin touches on this idea, pointing out that self-replicating machines launched by different species might actually compete with one another (in a Darwinistic fashion) for raw material, or even have conflicting missions. Given enough variety of \"species\" they might even form a type of ecology, or – should they also have a form of artificial intelligence – a society. They may even mutate with untold thousands of \"generations\".\n\nThe first quantitative engineering analysis of such a spacecraft was published in 1980 by Robert Freitas, in which the non-replicating Project Daedalus design was modified to include all subsystems necessary for self-replication. The design's strategy was to use the probe to deliver a \"seed\" factory with a mass of about 443 tons to a distant site, have the seed factory replicate many copies of itself there to increase its total manufacturing capacity, over a 500-year period, and then use the resulting automated industrial complex to construct more probes with a single seed factory on board each.\n\nIt has been theorized that a self-replicating starship utilizing relatively conventional theoretical methods of interstellar travel (i.e., no exotic faster-than-light propulsion, and speeds limited to an \"average cruising speed\" of 0.1c.) could spread throughout a galaxy the size of the Milky Way in as little as half a million years.\n\nIn 1981, Frank Tipler put forth an argument that extraterrestrial intelligences do not exist, based on the absence of von Neumann probes. Given even a moderate rate of replication and the history of the galaxy, such probes should already be common throughout space and thus, we should have already encountered them. Because we have not, this shows that extraterrestrial intelligences do not exist. This is thus a resolution to the Fermi paradox – that is, the question of why we have not \"already\" encountered extraterrestrial intelligence if it is common throughout the universe.\n\nA response came from Carl Sagan and William Newman. Now known as \"Sagan's Response\", it pointed out that in fact Tipler had underestimated the rate of replication, and that von Neumann probes should have already started to consume most of the mass in the galaxy. Any intelligent race would therefore, Sagan and Newman reasoned, not design von Neumann probes in the first place, and would try to destroy any von Neumann probes found as soon as they were detected. As Robert Freitas has pointed out, the assumed capacity of von Neumann probes described by both sides of the debate are unlikely in reality, and more modestly reproducing systems are unlikely to be observable in their effects on our Solar System or the Galaxy as a whole.\n\nAnother objection to the prevalence of von Neumann probes is that civilizations of the type that could potentially create such devices may have inherently short lifetimes, and self-destruct before so advanced a stage is reached, through such events as biological or nuclear warfare, nanoterrorism, resource exhaustion, ecological catastrophe, or pandemics.\n\nSimple workarounds exist to avoid the over-replication scenario. Radio transmitters, or other means of wireless communication, could be used by probes programmed not to replicate beyond a certain density (such as five probes per cubic parsec) or arbitrary limit (such as ten million within one century), analogous to the Hayflick limit in cell reproduction. One problem with this defence against uncontrolled replication is that it would only require a single probe to malfunction and begin unrestricted reproduction for the entire approach to fail – essentially a technological cancer – unless each probe also has the ability to detect such malfunction in its neighbours and implements a seek and destroy protocol (which in turn could lead to probe-on-probe space wars if faulty probes first managed to multiply to high numbers before they were found by sound ones, which could then well have programming to replicate to matching numbers so as to manage the infestation). Another workaround is based on the need for spacecraft heating during long interstellar travel. The use of plutonium as a thermal source would limit the ability to self-replicate. The spacecraft would have no programming to make more plutonium even if it found the required raw materials. Another is to program the spacecraft with a clear understanding of the dangers of uncontrolled replication.\n\nThe details of the mission of self-replicating starships can vary widely from proposal to proposal, and the only common trait is the self-replicating nature.\n\nA von Neumann probe is a spacecraft capable of replicating itself. The concept is named after Hungarian American mathematician and physicist John von Neumann, who rigorously studied the concept of self-replicating machines that he called \"Universal Assemblers\" and which are often referred to as \"von Neumann machines\". While von Neumann never applied his work to the idea of spacecraft, theoreticians since then have done so.\n\nIf a self-replicating probe finds evidence of primitive life (or a primitive, low-level culture) it might be programmed to lie dormant, silently observe, attempt to make contact (this variant is known as a Bracewell probe), or even interfere with or guide the evolution of life in some way.\n\nPhysicist Paul Davies of Arizona State University has even raised the possibility of a probe resting on our own Moon, having arrived at some point in Earth's ancient prehistory and remained to monitor Earth, which is very reminiscent of Arthur C. Clarke's \"The Sentinel\" and the Stanley Kubrick .\n\nA variant idea on the interstellar von Neumann probe idea is that of the \"Astrochicken\", proposed by Freeman Dyson. While it has the common traits of self-replication, exploration, and communication with its \"home base\", Dyson conceived the Astrochicken to explore and operate within our own planetary system, and not explore interstellar space.\n\nOxford-based philosopher Nick Bostrom discusses the idea that future powerful superintelligences will create efficient cost-effective space travel and interstellar Von Neumann probes.\n\nAnders Sandberg and Stuart Armstrong argued that launching the colonization of the entire reachable universe through self-replicating probes is well within the capabilities of a star-spanning civilization, and proposed a theoretical approach for achieving it in 32 years, by mining planet Mercury for resources and constructing a Dyson Swarm around the Sun.\n\nA variant of the self-replicating starship is the \"Berserker\". Unlike the benign probe concept, Berserkers are programmed to seek out and exterminate lifeforms and life-bearing exoplanets whenever they are encountered.\n\nThe name is derived from the \"Berserker\" series of novels by Fred Saberhagen which describe a war between humanity and such machines. Saberhagen points out (through one of his characters) that the Berserker warships in his novels are not von Neumann machines themselves, but the larger complex of Berserker machines – including automated shipyards – \"do\" constitute a von Neumann machine. This again brings up the concept of an ecology of von Neumann machines, or even a von Neumann hive entity.\n\nIt is speculated in fiction that Berserkers could be created and launched by a xenophobic civilization (see \"Anvil of Stars\", by Greg Bear, in the section In fiction below) or could theoretically \"mutate\" from a more benign probe. For instance, a von Neumann ship designed for terraforming processes – mining a planet's surface and adjusting its atmosphere to more human-friendly conditions – might malfunction and attack inhabited planets, killing their inhabitants in the process of changing the planetary environment, and then self-replicate and dispatch more ships to attack other planets.\n\nYet another variant on the idea of the self-replicating starship is that of the seeder ship. Such starships might store the genetic patterns of lifeforms from their home world, perhaps even of the species which created it. Upon finding a habitable exoplanet, or even one that might be terraformed, it would try to replicate such lifeforms – either from stored embryos or from stored information using molecular nanotechnology to build zygotes with varying genetic information from local raw materials.\n\nSuch ships might be terraforming vessels, preparing colony worlds for later colonization by other vessels, or – should they be programmed to recreate, raise, and educate individuals of the species that created it – self-replicating colonizers themselves. Seeder ships would be a suitable alternative to Generation ships as a way to colonize worlds too distant to travel to in one lifetime.\n\n\n\n\n\n"}
{"id": "29181", "url": "https://en.wikipedia.org/wiki?curid=29181", "title": "Spherical coordinate system", "text": "Spherical coordinate system\n\nIn mathematics, a spherical coordinate system is a coordinate system for three-dimensional space where the position of a point is specified by three numbers: the \"radial distance\" of that point from a fixed origin, its \"polar angle\" measured from a fixed zenith direction, and the \"azimuth angle\" of its orthogonal projection on a reference plane that passes through the origin and is orthogonal to the zenith, measured from a fixed reference direction on that plane. It can be seen as the three-dimensional version of the polar coordinate system. \n\nThe radial distance is also called the \"radius\" or \"radial coordinate\". The polar angle may be called \"colatitude\", \"zenith angle\", \"normal angle\", or \"inclination angle\".\n\nThe use of symbols and the order of the coordinates differs between sources. In one system frequently encountered in physics (, , ) gives the radial distance, polar angle, and azimuthal angle, whereas in another system used in many mathematics books gives the radial distance, azimuthal angle, and polar angle. In both systems is often used instead of . Other conventions are also used, so great care needs to be taken to check which one is being used. \n\nA number of different spherical coordinate systems following other conventions are used outside mathematics. In a geographical coordinate system positions are measured in latitude, longitude and height or altitude. There are a number of different celestial coordinate systems based on different fundamental planes and with different terms for the various coordinates. The spherical coordinate systems used in mathematics normally use radians rather than degrees and measure the azimuthal angle counterclockwise from the -axis to the -axis rather than clockwise from north (0°) to east (+90°) like the horizontal coordinate system. The polar angle is often replaced by the \"elevation angle\" measured from the reference plane. Elevation angle of zero is at the horizon.\n\nThe spherical coordinate system generalizes the two-dimensional polar coordinate system. It can also be extended to higher-dimensional spaces and is then referred to as a hyperspherical coordinate system.\n\nTo define a spherical coordinate system, one must choose two orthogonal directions, the \"zenith\" and the \"azimuth reference\", and an \"origin\" point in space. These choices determine a reference plane that contains the origin and is perpendicular to the zenith. The spherical coordinates of a point are then defined as follows:\n\n\nThe sign of the azimuth is determined by choosing what is a \"positive\" sense of turning about the zenith. This choice is arbitrary, and is part of the coordinate system's definition.\n\nThe \"elevation\" angle is 90 degrees ( radians) minus the inclination angle.\n\nIf the inclination is zero or 180 degrees ( radians), the azimuth is arbitrary. If the radius is zero, both azimuth and inclination are arbitrary.\n\nIn linear algebra, the vector from the origin to the point is often called the \"position vector\" of \"P\".\n\nSeveral different conventions exist for representing the three coordinates, and for the order in which they should be written. The use of to denote radial distance, inclination (or elevation), and azimuth, respectively, is common practice in physics, and is specified by ISO standard 80000-2:2009, and earlier in ISO 31-11 (1992).\n\nHowever, some authors (including mathematicians) use for inclination (or elevation) and for azimuth, which \"provides a logical extension of the usual polar coordinates notation\".<ref name=\"http://mathworld.wolfram.com/SphericalCoordinates.html\"></ref> Some authors may also list the azimuth before the inclination (or elevation), and/or use (rho) instead of for radial distance. Some combinations of these choices result in a left-handed coordinate system. The standard convention conflicts with the usual notation for the two-dimensional polar coordinates, where is often used for the azimuth. It may also conflict with the notation used for three-dimensional cylindrical coordinates.\nThe angles are typically measured in degrees (°) or radians (rad), where 360° = 2 rad. Degrees are most common in geography, astronomy, and engineering, whereas radians are commonly used in mathematics and theoretical physics. The unit for radial distance is usually determined by the context.\n\nWhen the system is used for physical three-space, it is customary to use positive sign for azimuth angles that are measured in the counter-clockwise sense from the reference direction on the reference plane, as seen from the zenith side of the plane. This convention is used, in particular, for geographical coordinates, where the \"zenith\" direction is north and positive azimuth (longitude) angles are measured eastwards from some prime meridian.\n\nAny spherical coordinate triplet specifies a single point of three-dimensional space. On the other hand, every point has infinitely many equivalent spherical coordinates. One can add or subtract any number of full turns to either angular measure without changing the angles themselves, and therefore without changing the point. It is also convenient, in many contexts, to allow negative radial distances, with the convention that is equivalent to for any , , and . Moreover, is equivalent to .\n\nIf it is necessary to define a unique set of spherical coordinates for each point, one must restrict their ranges. A common choice is:\n\nHowever, the azimuth is often restricted to the interval , or in radians, instead of . This is the standard convention for geographic longitude.\n\nThe range for inclination is equivalent to for elevation (latitude).\n\nEven with these restrictions, if is 0° or 180° (elevation is 90° or −90°) then the azimuth angle is arbitrary; and if is zero, both azimuth and inclination/elevation are arbitrary. To make the coordinates unique, one can use the convention that in these cases the arbitrary coordinates are zero.\n\nTo plot a dot from its spherical coordinates , where is inclination, move units from the origin in the zenith direction, rotate by about the origin towards the azimuth reference direction, and rotate by about the zenith in the proper direction.\n\nThe geographic coordinate system uses the azimuth and elevation of the spherical coordinate system to express locations on Earth, calling them respectively longitude and latitude. Just as the two-dimensional Cartesian coordinate system is useful on the plane, a two-dimensional spherical coordinate system is useful on the surface of a sphere. In this system, the sphere is taken as a unit sphere, so the radius is unity and can generally be ignored. This simplification can also be very useful when dealing with objects such as rotational matrices.\n\nSpherical coordinates are useful in analyzing systems that have some degree of symmetry about a point, such as volume integrals inside a sphere, the potential energy field surrounding a concentrated mass or charge, or global weather simulation in a planet's atmosphere. A sphere that has the Cartesian equation has the simple equation in spherical coordinates.\n\nTwo important partial differential equations that arise in many physical problems, Laplace's equation and the Helmholtz equation, allow a separation of variables in spherical coordinates. The angular portions of the solutions to such equations take the form of spherical harmonics.\n\nAnother application is ergonomic design, where is the arm length of a stationary person and the angles describe the direction of the arm as it reaches out.\nThree dimensional modeling of loudspeaker output patterns can be used to predict their performance. A number of polar plots are required, taken at a wide selection of frequencies, as the pattern changes greatly with frequency. Polar plots help to show that many loudspeakers tend toward omnidirectionality at lower frequencies.\n\nThe spherical coordinate system is also commonly used in 3D game development to rotate the camera around the player's position.\n\nTo a first approximation, the geographic coordinate system uses elevation angle (latitude) in degrees north of the equator plane, in the range , instead of inclination. Latitude is either geocentric latitude, measured at the Earth's center and designated variously by or geodetic latitude, measured by the observer's local vertical, and commonly designated . The azimuth angle (longitude), commonly denoted by , is measured in degrees east or west from some conventional reference meridian (most commonly the IERS Reference Meridian), so its domain is . For positions on the Earth or other solid celestial body, the reference plane is usually taken to be the plane perpendicular to the axis of rotation. \n\nThe polar angle, which is 90° minus the latitude and ranges from 0 to 180°, is called colatitude in geography.\n\nInstead of the radial distance, geographers commonly use altitude above or below some reference surface, which may be the sea level or \"mean\" surface level for planets without liquid oceans. The radial distance can be computed from the altitude by adding the mean radius of the planet's reference surface, which is approximately for Earth.\n\nHowever, modern geographical coordinate systems are quite complex, and the positions implied by these simple formulae may be wrong by several kilometers. The precise standard meanings of latitude, longitude and altitude are currently defined by the World Geodetic System (WGS), and take into account the flattening of the Earth at the poles (about ) and many other details.\n\nIn astronomy there are a series of spherical coordinate systems that measure the elevation angle from different fundamental planes. These reference planes are the observer's horizon, the celestial equator (defined by Earth's rotation), the plane of the ecliptic (defined by Earth's orbit around the Sun), the plane of the earth terminator (normal to the instantaneous direction to the Sun), and the galactic equator (defined by the rotation of the Milky Way).\n\nAs the spherical coordinate system is only one of many three-dimensional coordinate systems, there exist equations for converting coordinates between the spherical coordinate system and others.\n\nThe spherical coordinates of a point in the ISO convention (i.e. for physics: \"radius\" , \"inclination\" , \"azimuth\" ) can be obtained from its Cartesian coordinates by the formulae\n\nThe inverse tangent denoted in must be suitably defined, taking into account the correct quadrant of . See the article on atan2.\n\nAlternatively, the conversion can be considered as two sequential rectangular to polar conversions: the first in the Cartesian -plane from to , where is the projection of onto the -plane, and the second in the Cartesian -plane from to . The correct quadrants for and are implied by the correctness of the planar rectangular to polar conversions.\n\nThese formulae assume that the two systems have the same origin, that the spherical reference plane is the Cartesian -plane, that is inclination from the direction, and that the azimuth angles are measured from the Cartesian -axis (so that the -axis has ). If \"θ\" measures elevation from the reference plane instead of inclination from the zenith the arccos above becomes an arcsin, and the and below become switched.\n\nConversely, the Cartesian coordinates may be retrieved from the spherical coordinates (\"radius\" , \"inclination\" , \"azimuth\" ), where , , , by:\n\nCylindrical coordinates (\"radius\" , \"azimuth\" , \"elevation\" ) may be converted into spherical coordinates (\"radius\" , \"inclination\" , \"azimuth\" ), by the formulas\n\nConversely, the spherical coordinates may be converted into cylindrical coordinates by the formulae\n\nThese formulae assume that the two systems have the same origin and same reference plane, measure the azimuth angle in the same sense from the same axis, and that the spherical angle is inclination from the cylindrical -axis.\n\nThe following equations assume that is inclination from the (polar) axis (ambiguous since , , and are mutually normal):\n\nThe line element for an infinitesimal displacement from to is\n\nwhere\nare the local orthogonal unit vectors in the directions of increasing , , and , respectively,\nand , , and are the unit vectors in Cartesian coordinates.\nThe general form of the formula to prove the differential line element, is \n\nformula_7\n\nthat is, the change in formula_8 is decomposed into individual changes corresponding to changes in the individual coordinates. To apply this to the present case, you need to calculate how formula_8 changes with each of the coordinates. With the conventions being used, we have\n\nformula_10\n\nThus\n\nformula_11\n\nThen the desired coefficients are the magnitudes of these vectors:\n\nformula_12 \nThe surface element spanning from to and to on a spherical surface at (constant) radius is\n\nThus the differential solid angle is\n\nThe surface element in a surface of polar angle constant (a cone with vertex the origin) is\n\nThe surface element in a surface of azimuth constant (a vertical half-plane) is\n\nThe volume element spanning from to , to , and to is (determinant of the Jacobian matrix of partial derivatives):\n\nThus, for example, a function can be integrated over every point in ℝ by the triple integral\n\nThe del operator in this system leads to the following expressions for gradient, divergence, curl and Laplacian:\n\nIn spherical coordinates the position of a point is written\n\nIts velocity is then\n\nand its acceleration is\n\nIn the case of a constant or , this reduces to vector calculus in polar coordinates.\n\n\n\n"}
{"id": "10658990", "url": "https://en.wikipedia.org/wiki?curid=10658990", "title": "Symbolic chickens", "text": "Symbolic chickens\n\nChickens have been widely used as national symbols, and as mascots for clubs, businesses, and other associations.\n\n"}
{"id": "21775229", "url": "https://en.wikipedia.org/wiki?curid=21775229", "title": "Tableau économique", "text": "Tableau économique\n\nThe Tableau économique () or \"Economic Table\" is an economic model first described by French economist François Quesnay in 1758, which laid the foundation of the Physiocratic school of economics.\n\nQuesnay believed that trade and industry were not sources of wealth, and instead in his 1758 manuscript \"Tableau économique\" (Economic Table) argued that agricultural surpluses, by flowing through the economy in the form of rent, wages, and purchases were the real economic movers.\nThe model Quesnay created consisted of three economic movers. The \"Proprietary\" class consisted of only landowners. The \"Productive\" class consisted of all agricultural laborers. The \"Sterile\" class is made up of artisans and merchants. The flow of production and/or cash between the three classes started with the Proprietary class because they own the land and they buy from both of the other classes. The process has these steps (consult Figure 1).\n\n\nThe Tableau shows the reason why the Physiocrats disagreed with Cantillon about exporting food. The economy produces a surplus of food, and neither the farmer nor the artisan can afford to consume more than a subsistence level of food. The landlord is assumed to be consuming at a level of satiation; therefore, he cannot consume any more. Since food cannot be stored easily, it is necessary to sell it to someone who can use it. This is where the merchant provides value.\n\nThe merchant is not a source of wealth, however. The Physiocrats believed that “neither industry nor commerce generates wealth.” A “plausible explanation is that the Physiocrats developed their theory in light of the actual situation of the French economy…” France was an absolute monarchy with the land owners constituting 6-8% of the population and owning 50% of the land. (5, p. 859) Agriculture contributes 80% of the country’s wealth, and the non-land owning segment of the population “practises a subsistence agriculture that produces the essential minimum, with virtually all income being absorbed by food requirements.” Additionally, exports consisted mostly of agricultural-based products, e.g. wine. Given the massive effect of agriculture on France’s economy, it was more likely they would develop an economic model that used it to the king’s advantage.\n\nThe Physiocrats are at the beginning of the anti-mercantilist movement. Quesnay’s argument against industry and international trade as alternatives to his doctrine is twofold. First, industry produces no gain in wealth; therefore, redirecting labor from agriculture to industry will in effect decrease the nation’s overall wealth. Additionally, population expands to fill available land and food supply; therefore, population must go down if the use of land does not produce food. Second, the basic premise of the Mercantilists is that a country must export more than it imports to gain wealth, but that assumes it has more of a tradeable resource than it needs for internal consumption. France did not have a colony with the ability to produce finished or semi-finished goods like England (e.g. India) or Holland (e.g. North America, Africa, South America). Its main colonial presence was in the Caribbean, southern North America, and southeast Asia, and like France, the colonies had agricultural-based economies. The only good which France had in enough excess to export was food; therefore, international trade based on industrial production would not yield as much wealth.\n\nQuesnay was not anti-industry, however. He was just realistic in his assessment that France was not in good position to incubate a strong industrial market. His argument was that artisans and manufacturers would come to France only in proportion to the size of the internal market for their goods. Quesnay believed “a country should concentrate on manufacturing only to the extent that the local availability of raw materials and suitable labor enabled it to have a cost advantage over its overseas competitors.” Anything above that amount should be purchased through trade.\n\nThe tableau économique is credited as the \"first precise formulation\" of interdependent systems in economics and the origin of the theory of the multiplier in economics. An analogous table is used in the theory of money creation under fractional-reserve banking by relending of deposits, leading to the money multiplier.\n\nThe wage-fund doctrine was derived from the tableau, then later rejected.\n\n\n"}
