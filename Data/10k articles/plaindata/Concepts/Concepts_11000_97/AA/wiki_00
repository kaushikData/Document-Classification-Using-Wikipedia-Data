{"id": "28082786", "url": "https://en.wikipedia.org/wiki?curid=28082786", "title": "Alkarama", "text": "Alkarama\n\nAlkarama ( / ISO 233: / Dignity) is an independent Swiss-based human rights non-governmental organization established in 2004 to assist all those in the Arab World subjected to, or at risk for, extrajudicial killings, disappearances, torture, and arbitrary detention. \nActing as a bridge between individual victims in the Arab World and international human rights mechanisms, Alkarama works towards an Arab World where all individuals live free, with dignity, and protected by the rule of law.\n\nAlkarama is an NGO defending the victims of human rights violations in the Arab world – including violations of the right to life, to physical and mental integrity, and to Civil and Political Rights – by using in priority international law mechanisms. Alkarama also helps to promote a culture of human rights in the Arab world.\n\nAlthough it recognises the indivisibility of human rights, Alkarama has given priority to the defence of people subjected or at risk of extrajudicial killing, enforced disappearance, torture, and arbitrary detention, as Alkarama regards these violations of the right to life, physical integrity, and civil and political rights as common in this region.\n\nTo end these violations, Alkarama cooperates with local and national civil society activists and international organisations for the promotion and protection of human rights, as well as governments and other entities likely to act on the human rights situation.\n\nBy engaging international mechanisms, Alkarama offers support and a last resort to the victims of these human rights violations, so that they achieve respect for their rights against the failure or inefficiency of their country's justice system.\n\nAlkarama also works for a strong international system of human rights protection, which reinforces the regional, national, and local protection systems. In, particular, it contributes to fill the information gaps, and to increase the attention of international human rights protection mechanisms on human rights violations taking place in the Arab World. Alkarama encourages States to strengthen their national laws to defend and promote human rights.\n\nFinally, Alkarama contributes to the promotion of the culture of human rights, by ensuring that the various groups that make up the civil society in these countries are familiar with the concept of human rights and are mobilising around it, know their rights and claim them, and feel protected by law.\n\nThrough its projects, Alkarama provides these actors the necessary tools so that they can assert their rights both nationally and internationally.\n\n1. Document and denounce human rights violations in the Arab world\n\n2. Provide moral and judicial assistance to the victims of human rights violations\n\n3. Pursue the perpetrators of human rights violations and fight impunity\n\n4. Encourage, and campaign for governments to respect human rights\n\n5. Spread the culture of human rights in Arab societies\n\n6. Train human rights defenders\n\n7. Support any initiative which reinforces the protection of citizens against human rights violations\n\n8. Be an effective organization\n\nAlkarama focuses on the most serious human rights violations, i.e. violations that relate to the right to life, human dignity, bodily integrity and freedom. The idea behind Alkarama's specific mandate is that only when citizens are free from the most serious human rights abuses that individuals can freely and effectively call for all of their rights and ensure the rule of law in their countries.\n\nAccording to the International Covenant on Civil and Political Rights (ICCPR):\n\nEvery human being has the inherent right to life. This right shall be protected by law. No one shall be arbitrarily deprived of his life.\n\nThe United Nations Special Rapporteur on Extrajudicial, Summary or Arbitrary Executions intervenes on cases of executions outside the legal framework or without the proper legal safeguards: capital punishment following an unfair trial, deaths in custody, deaths due to excessive use of force by law enforcement officials, deaths due to attacks by States security forces, violations of the right to life in armed conflict, genocide, and the imminent expulsion of persons to a country where their lives are in danger.\n\nAccording to the International Convention for the Protection of All Persons from Enforced Disappearance (CED):\n\nEnforced disappearance is considered to be the arrest, detention, abduction or any other form of deprivation of liberty by agents of the State or by persons or groups of persons acting with the authorisation, support or acquiescence of the State, followed by a refusal to acknowledge the deprivation of liberty or by concealment of the fate or whereabouts of the disappeared person, which place such a person outside the protection of the law.\n\nA tool of repression\n\nMany of the governments of the Arab World use disappearances to silence opposition members and terrorize the population.\n\nAlgeria is a notable example of this practice. Alkarama has presented over 1,000 cases of disappearances to the United Nations Working Group on Enforced or Involuntary Disappearances (WGEID). The number of disappeared in Algeria is estimated at between 10,000 and 20,000 – the Algerian government admitted to 6,164 in 2005; the Algerian national human rights institution, the Commission Nationale Consultative de Promotion et de Protection des Droits de l'Homme (CNCPPDH) to 8,023.\n\nAccording to the Convention against Torture (CAT):\n\nTorture means any act by which severe pain or suffering, whether physical or mental, is intentionally inflicted on a person for such purposes as obtaining from him or a third person information or a confession, punishing him for an act he or a third person has committed or is suspected of having committed, or intimidating or coercing him or a third person, or for any reason based on discrimination of any kind, when such pain or suffering is inflicted by or at the instigation of or with the consent or acquiescence of a public official or other person acting in an official capacity. It does not include pain or suffering arising only from, inherent in or incidental to lawful sanctions.\n\nUpon receipt of information about cases of torture from its representatives and civil society contacts in the Arab world, Alkarama writes a communication to the United Nations Special Rapporteur on Torture (SRT) with details of the case.\n\nAccording to the UN Working Group on Arbitrary Detention (WGAD):\n\nDeprivation of liberty is arbitrary if the case falls into one of the following three categories:\n\n1. When it is clearly impossible to invoke any legal basis justifying the deprivation of liberty [...] (Category I)\n\n2. When the deprivation of liberty results from the exercise of the rights and freedoms guaranteed by the Universal Declaration of Human Rights and, insofar as States parties are concerned, by the International Covenant on Civil and Political Rights (Category II)\n\n3. When the total or partial non-observance of the international norms relating to the right to a fair trial spelled out in the Universal Declaration of Human Rights and in the relevant international instruments accepted by the States concerned, is of such gravity as to give the deprivation of liberty an arbitrary character (Category III). (WGAD, Fact Sheet No. 26)\n\nArbitrary Detention in the Arab World: the United Nation's Opinion\n\nArab governments often arrest and detain political opponents and human rights defenders in order to quiet their criticism of government policies and behaviour. The opinions issued by the WGAD can then be used in local and international advocacy against these detentions, be brought up with the governments and authorities directly, and can lead to enough pressure to have these individuals released in some cases. Many governments are very sensitive to their international image and human rights record.\n\nAlkarama was created as a Swiss association in July 2004 by Qatari and Algerian human rights defenders – Abdulrahman Al Naimi, Rachid Mesli and Abbas Aroua – to contribute to an Arab World where all individuals live free, in dignity, and protected by the rule of law. With this goal in mind, the founders decided to address the most serious violations of human dignity, physical integrity and freedom, namely extrajudicial killings, enforced disappearances, torture, and arbitrary detention, with the hope that individuals who no longer fear being subjected to these violations can speak and act freely to call for their rights and ensure the rule of law in their countries.\n\nAt the time of Alkarama's creation, the United Nations (UN) mechanisms established to protect human rights worldwide rarely acted upon violations in the Arab region. Identifying this gap, Alkarama decided that in order to bring these violations to an end, Alkarama would act through the UN’s mechanisms specific to bring these human rights violations to an end. In doing so, Alkarama would also fulfill its objective to contribute to a better understanding of human rights and raise awareness of the UN's human rights protection mechanisms in the Arab civil society, including amongst social groups who often viewed these instruments as ineffective or understood rights as Western concepts, in particular Islamist groups, political opposition parties, and journalists.\n\nBy acting as a bridge between the victims in the Arab world and the UN Special Procedures experts, whilst building the capacity of local activists to directly access UN mechanisms and use the decisions they adopt to call for the respect of human rights in their countries, Alkarama gave itself the means to achieve its two main objectives simultaneously. This involves speaking directly and regularly with victims and their families, lawyers, or local activists to document individual cases of human rights violations; and submitting these cases to the UN mechanisms for them to request the relevant government authorities to remedy to the situation.\n\nBetween 2004 and 2007, Rachid Mesli, at the time Alkarama's sole employee in Geneva, submitted around 400 individual cases to the UN Special Procedures, leading to these mechanisms' action with a number of Arab States, and a noticeable improvement of the situation for many of the victims. Following up on its successes and growing needs, in 2005 Alkarama recruited Country Representatives in Lebanon and Yemen in order to follow the human rights situation in these countries and document further cases of violations to submit to the UN. Alkarama's website was launched the same year.\n\nIn April 2007, in the face of an ever-increasing workload and in order to undertake the necessary changes in capital and human resources, Alkarama registers as a Foundation under Swiss law. This change in status aimed to ensure greater stability and transparency, by enabling the Swiss authorities to review the organisation's financial records on a yearly basis.\n\nAs of 2007, Alkarama began working with the UN Human Rights Treaty Bodies – in particular the Committee Against Torture (CAT), the Human Rights Committee (HRCttee) and the Universal Periodic Review (UPR) instituted by the newly established Human Rights Council (HRC) – by submitting alternative information regarding the Arab State's implementation of the relevant treaties at every stage of their review process. In doing so, Alkarama provides the CAT and HRCttee expert, as well as members of the (HRC) access to information from civil society actors as well as concrete cases of violations of numerous articles of the Universal Declaration of Human Rights, the Convention Against Torture and the International Covenant on Civil and Political Rights (ICCPR).\n\nIn 2009, facing the increasingly negative roles played by National Human Rights Institutions (NHRIs), Alkarama begins providing independent information from local civil society actors about these institutions to the International Coordinating Committee on (ICC-NHRIs), which regularly reviews the status of these institutions, resulting in a number of reforms within these institutions.\n\nAlso in 2009, Alkarama launched the Alkarama Award for Human Rights Defenders, a symbolic reward attributed every year to an individual or organisation that has significantly contributed to the promotion and protection of human rights in the Arab world (to read more about the Award and the previous laureates, click here). Through this award, Alkarama was able to fulfill two objectives: to bring attention to the work of Human Rights Defenders (HRDs) in the Arab world, whilst providing the UN, non-governmental organisations (NGOs), the media, and the general public an opportunity to learn about the individual heroes struggling for the promotion and protection of human rights in the Arab region.\n\nAs the events of 2011 began to unfold in the Arab World, Alkarama stood alongside those calling for the respect of their rights and became a major relay of information for the UN mechanisms and the media, on the uprisings in Egypt, Libya, Yemen and then Syria. With Country Representatives in Egypt, Lebanon and Yemen, as well as several visits to Libya, Alkarama was able to closely monitor the violations occurring, reporting them in real time whilst raising awareness amongst the new groups rising to power of their obligation to respect human rights.\n\nAlkarama today is made up of nine full-time employees in Geneva, and five representatives in the Arab World. It also trains about 10 interns per year in its Legal or Media Departments.\n\nAlthough founded in 2004 as a society, since 2007 Alkarama has been a registered Swiss Foundation.\n\nAlkarama is a trilingual organisation, publishing material in Arabic, English and French.\n\nAs an NGO working on a daily basis with the United Nations human rights protection mechanisms by submitting to these UN bodies detailed information on serious human rights violations committed by States in the region, Alkarama has often been the victim of defamation campaigns, whilst several members of its staff have been targeted – in their own capacity and not necessarily because of their relation with Alkarama – such as Executive Director, Mourad Dhina and Legal Director, Rachid Mesli, both victims of international arrest warrants issued by the Algerian government; as well as founding member and former Chairman of the Foundation’s Council, Abdulrahman Al Naimi, listed by the U.S. Treasury as an “Al Qaeda financier” although the charges were never substantiated.\n\nAmong others, on 2 September 2014, the Lebanese newspaper, As-Safir published an article entitled The Founder of the Cham Al Islam movement, a prominent activist within Alkarama, which openly accuses Alkarama of \"supporting terrorism\" and \"spreading anarchy\". In the following couple of days, these allegations were echoed in the Hezbollah-affiliated Lebanese information channel, Al Manar, in Syrian governmental news sites, RTV.gov and Al Tawra, as well as in Saudi-run news site on Syria, Al Akhbar Al Youm. On 3 September, Commentary, a magazine founded by the American Jewish Committee in 1945, accused Alkarama of being run by an \"Al Qaeda financier\", in reference to Alkarama's Founder, Mr Abdulrahman Al Naimi.\n\nOn 14 October 2014, the Swiss newspaper, Le Temps published an article entitled The town of Geneva funded an NGO accused of links with Al Qaeda, which was relayed by other Swiss media outlets. The article relied exclusively on the unproven listing of one of Alkarama’s three founding members as an “Al Qaeda financier” (see below). A month later, Le Temps published a feature by Alkarama’s Legal Director, Rachid Mesli, speaking of the difficulty to defend human rights in the Arab World, explaining that in the Arab World, those who dare criticise authoritarian regimes or demand more freedom are accused of terrorism to stifle any criticism or demand to take part in the country’s political life. \"“It is therefore not surprising that Alkarama, which defends victims of this repression, finds itself vilified and attacked by these regimes; that international NGOs such as the Council on American-Islamic Relations and Islamic Relief find themselves on the UAE’s list of terrorist organisations along with ISIL and Al Qaeda; and that the former Chairman of our Foundation, Dr Abdulrahman Al Naimi, a university Professor, a Qatari human rights activist and a former Amnesty prisoner of conscience be banned from several countries – including the UAE and Saudi Arabia,”\" explained Me Mesli.\n\nThe current president of Alkarama’s Board of Trustees, Khalifa al-Rabban, is a Founding Member and Member of the Board of Trustees of the Global Anti-Aggression Campaign (GAAC) alongside Abdulrahman Al Naimi. An online portal for articles on human-rights matters in the Arab World, the GAAC’s founding statement, written by Alkarama founding member Abdulrahman Al Naimi, claims that Islam is under siege and that the organisation seeks to confront the Western “aggressor.” GAAC has also hosted Hamas leaders.\n\nAlkarama unambiguously and irrevocably denies all these accusations, but the Foundation can only work in a spirit of full transparency. You can find details of its cases below.\n\nIn 2012, Alkarama's Executive Director, Dr Mourad Dhina, who had openly called for democratic change in Algeria for years was detained in France for six months on a request from the Algerian authorities to have him extradited to the country. The French court released him when they received documents from the Algerian authorities, which were so incoherent and lacking any evidence that the French prosecutor qualified them as \"grotesque\". Dr. Dhina returned to Alkarama after having spent almost 6 months at the Prison de la Santé in Paris.\n\nOn 18 December 2013, the United States Department of Treasury listed one of Alkarama’s three founding members, Pr. Abdulrahman Al Naimi as a “Specially Designated Global Terrorist” for having supposedly \"“provided money and material support and conveyed communications to Al Qaeda and its affiliates in Syria, Iraq, Somalia and Yemen for more than a decade.”\"[1] In addition, the U.S. Treasury claimed that Naimi had “reportedly oversaw the transfer of over $2 million per month to al-Qa’ida in Iraq for a period of time” and “served as an interlocutor between al-Qaida in Iraq leaders and Qatar-based donors.” Speaking to the Financial Times from Istanbul, Al Naimi denied the charges leveled against him.[2] Al Naimi is also a secretary general of the Global Anti-Aggression Campaign, an online NGO that has hosted Hamas leaders and released anti-Semitic and anti-Western writings. Following the listing, Pr. Al Naimi resigned from the Foundation to avoid any misinterpretation. However, the Alkarama Council decided to reject the resignation of Mr. Al Naimi after initially accepting it. In July 2014, Naimi stepped down as President and Member of the Board of Alkarama.\n\nIt is important to note, however, that the charges brought by the U.S. Treasury against him were made against his own person, and not the Foundation. Besides, the U.S. Treasury has not submitted any evidence or proof of its allegations against Pr Al Naimi, who denies all charges as a whole, and is willing, as he officially notified the American authorities, to appear in person before a court to establish the falsity of the charges pressed against him. Al Naimi has, however, received widespread media coverage for his support to terrorist groups.\n\nOn 15 November 2014, the Emirates News Agency released a list of 85 organisations \"“designated as terrorist organisations and groups in implementation of Federal Law No. 7 for 2014 on combating terrorist crimes”\" issued by Sheikh Khalifa bin Zayed Al Nayhan with the aim to \"“raise awareness in society about these organisations.”\"\n\nThe list, which includes internationally recognised terrorist organisations, such as Al-Qaeda or Dae’sh (the Islamic State of Iraq and the Levant - ISIL), also includes several Muslim associations in Europe and international NGOs, such as the Council on American-Islamic Relations (CAIR) and Islamic Relief. “Alkarama organisation” also appears in that list, but despite numerous attempts to contact Emirati officials on this matter, the Alkarama Foundation never received an official confirmation and therefore considers itself not concerned by that listing.\n\nMe Rachid Mesli, Legal Director at Alkarama, was arrested at the Swiss-Italian border on 19 August 2015 on the basis of an international arrest warrant issued by the Algerian authorities in April 2002, which claims that he had \"provided telephone information to terrorist groups movements,\" and \"attempted to supply terrorist groups with cameras and phones,\" twisting his work as a human rights lawyer, in constant contact with victims of human rights abuses and their families.\n\nOn 22 August, the Italian justice decided to put him under house arrest instead of keeping him in Aosta prison, following several calls from various NGOs, institutions and personalities, as well as an important media coverage of his case.\n\nOn 15 September 2015, recognizing Me Mesli's important work in the promotion and protection of human rights in the Arab World, as well as the strong risks of torture that he would incur if he was extradited to Algeria, the Turin Court decided to release him without waiting for the end of the 40-day period by which the Algerian authorities can submit their formal request for extradition.\n\nEventually, on 16 December 2015, the Italian court rejected the extradition request after noting all the inconsistencies in the international arrest warrant. The court considered the charges against Me Mesli were the result of \"political persecution\" and asserted that \"his human rights activities have nothing to do with terrorism.\"\n\nAnd on 13 May 2016, the Commission for the Control of Interpol's Files decided, after a long legal battle, to drop the international arrest warrant issued in 2003 by the Algerian authorities against Me Mesli, for their lack of cooperation as well as their failure to provide any form of clarification on his case.\n\nIn April 2004, Alkarama issued a report condemning the arrest by Qatari authorities of a number of individuals with ties to terrorism. The Alkarama Foundation report did not mention their activities in support of terrorism but called on Qatar to release the arbitrarily detained individuals. The report listed Ibrahim Issa al-Bakr, Salim Hasan al-Kuwari, Abd al-Latif Bin Abdullah Salih Muhammad al-Kawari, Khalid Saeed al-Bounein and others as the detainees which Alkarama demanded be released.\n\nThe U.S. Treasury claimed that in the early 2000s, when Alkarama released its advocacy report, Ibrahim Issa al-Bakr was “working to raise money to support terrorism” and was involved in a jihadist network. In 2014, the U.S. government designated Ibrahim Issa al-Bakr as an al-Qaeda supporter and Specially Designated Global Terrorist who worked with a Lebanon-based network to procure and transport weapons to Syria with the help of an al-Qaeda associate in Syria. In January 2015, al-Bakr was added by the United Nations to the al-Qaeda Sanctions List of individuals subject to an assets freeze and travel ban.\n\nSalim Hasan al-Kuwari is a Qatari national and US-designated financier and facilitator of al-Qaeda. The U.S. Treasury claimed that Kuwari supports al-Qaeda through Iran-based al-Qaeda associates and has provided “hundreds of thousands of dollars in financial support to al-Qaeda. In 2009, Alkarama submitted Kuwari’s case to the UN Working Group on Arbitrary Detention (WGAD).\n\nAbd al-Latif Bin Abdullah Salih Muhammad al-Kawari, a Qatar-based al-Qaeda facilitator who worked alongside Hassan Ghul and Ibrahim Isa Muhammad al-Bakr to transfer money to al-Qaeda in Pakistan. Al-Kawari was also a coordinator of Madid Ahl al-Sham, an online fundraising campaign used to fund al-Nusra Front militants in Syria and transfer weapons and supplies to the terrorist group.\nKhalid Saeed al-Bounein was a coordinator for Madid Ahl al-Sham alongside SDGTs Abd al-Latif Bin Abdullah Salih Muhammad al-Kawari and Sa’d bin Sa’d Muhammad Shariyan al-Ka’bi. Al-Bounein is listed as a point of contact for donations to the campaign that was cited by a Nusra Front member as the “preferred conduit” for donations. Al-Bounein was also listed as a point of contact in a partner charity fundraiser led by Eid Charity and Madid Ahl al-Sham.\n\n\n"}
{"id": "36638816", "url": "https://en.wikipedia.org/wiki?curid=36638816", "title": "Armenian eternity sign", "text": "Armenian eternity sign\n\nThe Armenian eternity sign (, \"haverzhut’yan nshan\") or Arevakhach (, \"Sun Cross\") is an ancient Armenian national symbol and a symbol of the of the . It is one of the most common symbols in Armenian architecture, carved on \"khachkars\" and on walls of churches.\n\nIn medieval Armenian culture, the eternity sign symbolized the concept of everlasting, celestial life. Since the 5th century, it appeared on Armenian steles, later it becomes part of \"khachkar\" symbolism. Around the 8th century the use of the Armenian symbol of eternity had become a long established national iconographical practice, and it keeps its meaning until the modern times. Besides being one of the main components of \"khachkars\", it can be found on church walls, tomb stones and other architectural monuments. Notable churches with the eternity sign include the Mashtots Hayrapet Church of Garni, Horomayr Monastery, Nor Varagavank, Tsitsernavank Monastery. It can also be found on Armenian manuscripts.\n\nThe eternity sign is used on the logos of government agencies and on commemorative coins, as well as Armenian government agencies and non-government organizations and institutions in Armenia and the Armenian diaspora.\n\nThe symbol is also used by Armenian neopagan organizations and their followers. It is called by them \"Arevakhach\" (, \"sun cross\").\n\nIn ArmSCII, Armenian Standard Code for Information Interchange, an Armenian eternity sign has been encoded in 7-bit and 8-bit standard and ad-hoc encodings since at least 1987. In 2010 the Armenian National Institute of Standards suggested encoding an Armenian Eternity sign in the Unicode character set, and both right-facing and left-facing Armenian eternity signs were included in Unicode version 7.0 when it was released in June 2014.\n\n\n"}
{"id": "39067533", "url": "https://en.wikipedia.org/wiki?curid=39067533", "title": "Autowave reverberator", "text": "Autowave reverberator\n\nIn the theory of autowave phenomena an autowave reverberator is an autowave vortex in a two-dimensional active medium.\n\nA reverberator appears a result of a rupture in the front of a plane autowave. Such a rupture may occur, for example, via collision of the front with a nonexcitable obstacle. In this case, depending on the conditions, either of two phenomena may arise: a \"spiral wave\", which rotates around the obstacle, or an \"autowave reverberator\" which rotates with its tip free.\n\nThe \"reverberator\" was one of the first autowave solutions, researchers found, and, because of this historical context, it remains by nowadays the most studied autowave object.\n\nUp until the late 20th century, the term \"auto-wave reverberator\" was used very active and widely in the scientific literature, written by soviet authors, because of active developing these investigations in USSR (for more details, see \"A brief history of autowave researches\" in Autowave). And, inasmuch as the soviet scientific literature was very often republished in English translation (see e.g.), the term \"autowave reverberator\" became known also in English-speaking countries.\n\nThe \"reverberator\" is often confused with another state of the active medium, which is similar to it, - with the \"spiral wave\". Indeed, at a superficial glance, these two autowave solutions look almost identical. Moreover, the situation is further complicated by the fact that the spiral wave may under certain circumstances become the reverberator, and the reverberator may, on the contrary, become the spiral wave!\n\nHowever, it must be remembered that many features of \"rotating autowaves\" were quite thoroughly studied as long ago as the 1970s, and already at that time some significant differences in properties of a spiral wave and a reverberator were revealed. Unfortunately, all the detailed knowledge from those years remains now scattered in different publications of the 1970-1990s, which became little-known now even for the new generations of researchers, not to mention the people that are far from this research topic. Perhaps, the only book in that it were more or less completely brought together in the form of abstracts basic information about autowaves, known at the time of its publication, remains still the Proceedings „Autowave processes in systems with diffusion“, which was published in 1981 and became already a rare bibliographic edition in nowadays; its content was partially reiterated in another book in 2009.\n\nThe differences between a reverberator and a spiral wave are considered below in detail. But for the beginning it is useful to demonstrate these differences with a simple analogy. Everyone knows well the seasons of a year... Under some conditions, winter can turn into summer, and summer, on the contrary, into winter; and, moreover, these miraculous transformations occur quite regularly! However, though a winter and a summer are similar, for example, in regular alternation of day and night, you cannot think of saying that winter and summer are the same thing, can you? Nearly the same things are with reverberator and spiral waves; and therefore they should not be confused.\n\nIt is useful also to keep in mind that it is known now, in addition to the rotating-wave, quite a number of other autowave solutions, and every year the number grows continuously with increasing speed. Because of these causes (or as a result of these events), it was found during the 21st century that many of the conclusions about the properties of autowaves, - which were widely known among readers of the early papers on the subject as well as widely discussed in the press of that time, - unfortunately, proved to be a sort of erroneous hasty generalizations.\n\nVarious autowave regimes, such as \"plane waves\" or \"spiral waves\" can exist in an active media, but only under certain conditions on the medium properties. Using the FitzhHugh-Nagumo model for a generic active medium, Winfree constructed a diagram depicting the regions of parameter space in which the principle phenomena may be observed. Such diagrams are a common way of presenting the different dynamical regimes observed in both experimental and theoretical settings. They are sometimes called \"flower gardens\" since the paths traced by autowave tips may often resemble the petals of a flower. A flower garden for the FitzHugh-Nagumo model is shown to the right. It contains: the line \"∂P\", which confines the range of the model parameters under which impulses can propagate through one-dimensional medium, and \"plane autowaves\" can spread in the two-dimensional medium; the \"rotor boundary\" \"∂R\", which confines the range of the parameters under which there can be the reverberators rotating around fixed cores (i.e. performing uniform circular rotation); the \"meander\" boundary \"∂M\" and the \"hyper-meander\" boundary \"∂C\", which confine the areas where two-period and more complex (possibly chaotic) regimes can exist. Rotating autowaves with large cores exist only in the areas with parameters close to the boundary \"∂R\".\n\nSimilar autowave regimes were also obtained for the other models — Beeler-Reuter model, Barkley model, Aliev-Panfilov model, Fenton-Karma model etc.\n\nIt was also shown that these simple autowave regimes should be common to all active media because a system of differential equations of any complexity, which describes this or that active medium, can be always simplified to two equations.\n\nIn the simplest case without drift (i.e., the regime of \"uniform circular rotation\"), the tip of a reverberator rotates around a fixed point along the circumference of a certain radius (the circular motion of the \"tip of the reverberator\"). The autowave cannot penetrate into the circle bounded by this circumference. As far as it approaches the centre of the reverberator rotation, the amplitude of the excitation pulse is reduced, and, at a relatively low excitability of the medium there is a region of finite size in the centre of reverberator, where the amplitude of the excitation pulse is zero (recall that we speak now about a homogeneous medium, for each point of which its properties are the same). This area of low amplitude in the centre of the reverberator is usually called \"the core of the reverberator\". The existence of such a region in the center of reverberator seems, at first glance, quite incomprehensible, as it borders all the time with the excited sites. A detailed investigation of this phenomenon showed that resting area in the centre of reverberator remains of its normal excitability, and the existence of a quiescent region in the centre of the reverberator is related to the phenomenon of the critical curvature. In the case of \"infinite\" homogeneous medium, the core radius and the speed of the rotor rotation are determined only by the properties of the medium itself, rather than the initial conditions. The shape of the front of the rotating spiral wave in the distance from the centre of rotation is close to the evolvent of the circumference - the boundaries of its core. The certain size of the core of the reverberator is conditioned by that the excitation wave, which circulates in a closed path, should completely fit in this path without bumping into its own refractory tail.\n\nAs the \"critical size\" of the reverberator, it is understood as the minimum size of the homogeneous medium in which the reverberator can exist indefinitely. For assessing the critical size of the reverberator one uses sometimes the size of its core, assuming that adjacent to the core region of the medium should be sufficient for the existence of sustainable re-entry. However, the quantitative study of the dependence of the reverberator behaviour on conductivity of rapid transmembrane current (that characterize the excitability of the medium), it was found that the critical size of the reverberator and the size its core are its different characteristics, and the critical size of the reverberator is much greater, in many cases, than the size of its core (i.e. reverberator dies, even the case, if its core fits easily in the boundaries of the medium and its drift is absent)\n\nAt meander and hyper-meander, the displacement of the center of autowave rotation (i.e. its drift) is influenced by the forces generated by the very same rotating autowave.\n\nHowever, in result of the scientific study of rotating autowaves was also identified a number of external conditions that force reverberator drift. It can be, for example, the heterogeneity of the active medium by any parameter. Perhaps, it is the works Biktasheva, where different types of the reverberator drift are currently represented the most completely (although there are other authors who are also involved in the study of drift of the autowave reverberator).\n\nIn particular, Biktashev offers to distinguish the following types of reverberator drift in the active medium:\n\nNote that even for such a simple question, what should be called a drift of autowaves, and what should not be called, there is still no agreement among researchers. Some researchers (mostly mathematicians) tends to consider as reverberator drift only those of its displacement, which occur under the influence of external events (and this view is determined exactly by the peculiarity of the mathematical approach to the study of autowaves). The other part of the researchers did not find significant differences between the spontaneous displacement of reverberator in result of the events generated by it itself, and its displacement as a result of external influences; and therefore these researchers tend to believe that meander and hyper-meander are also variants of drift, namely \"the spontaneous drift of the reverberator\". There was not debate on this question of terminology in the scientific literature, but it can be found easily these features of describing the same phenomena by the different authors.\n\nIn the numerical study of reverberator using the Aliev-Panfilov model, the phenomenon of bifurcation memory was revealed, when the reverberator changes spontaneously its behaviour from \"meander\" to \"uniform circular rotation\"; this new regime was named \"autowave lacet\".\n\nBriefly, spontaneous deceleration of the reverberator drift by the forces generated by the reverberator itself occurs during the autowave lacet, with the velocity of its drift decreasing gradually down to zero in the result. The regime meander thus degenerates into a simple uniform circular rotation. As already mentioned, this unusual process is related to phenomenon of bifurcation memory.\n\nWhen autowave lacet was discovered, the first question arose: Does the \"meander\" exist ever or the halt of the reverberator drift can be observed every time in all the cases, which are called meander, if the observation will be sufficiently long? The comparative quantitative analysis of the drift velocity of reverberator in the regimes of \"meander\" and \"lacet\" revealed a clear difference between these two types of evolution of the reverberator: while the drift velocity quickly goes to a stationary value during meander, a steady decrease in the drift velocity of the vortex can be observed during the lacet, in which can be clearly identified the phase of slow deceleration and phase of rapid deceleration of the drift velocity.\n\nThe revealing of autowave lacet may be important for cardiology. It is known that reverberators show remarkable stability of their properties, they behave \"at their discretion\", and their behaviour can significantly affect only the events that occur near the tip of reverberator. The fact that the behaviour of the reverberator can significantly affected only by the events that occur near its core, results, for example, in the fact that, at a meeting with reverberator nonexcitability heterogeneity (e.g. small myocardial scar), the tip of the rotating wave \"sticks\" to this heterogeneity, and reverberator begins to rotate around the stationary nonexcitability obstacles. The transition from polymorphic to monomorphic tachycardia is observed on the ECG in such cases. This phenomenon is called the \"anchoring\" of spiral wave.\nHowever, it was found in the simulations that spontaneous transition of polymorphic tachycardia in monomorphic one can be observed also on the ECG during the autowave lacet; in other words, the \"lacet\" may be another mechanism of transformation of polymorphic ventricular tachycardia in a monomorphic. Thus, the autowave theory predicts the existence of special type of ventricular arrhythmias, conditionally called \"lacetic\", which cardiologists do not still distinguish in diagnostics.\n\nRecall that from 1970th to the present time it is customary to distinguish three variants rotating autowaves: \nDimensions of the core of reverberator is usually less than the minimal critical size of the circular path of circulation, which is associated with the phenomenon of \"critical curvature\". In addition, the refractory period appears to be longer for the waves with non-zero curvature (reverberator and spiral wave) and begins to increase with decreasing the excitability of the medium before the refractory period for the plane waves (in the case of circular rotation). These and other significant differences between the reverberator and the circular rotation of excitation wave make us distinguish these two regimes of re-entry.\n\nThe figure shows the differences found in the behavior of the plane autowave circulating in the ring and reverberator. You can see that, in the same local characteristics of the excitable medium (excitability, refractoriness, etc., given by the nonlinear member), there are significant quantitative differences between dependencies of the reverberator characteristics and characteristics of the regime of one-dimensional rotation of impulse, although respective dependencies match qualitatively.\n\n"}
{"id": "24681011", "url": "https://en.wikipedia.org/wiki?curid=24681011", "title": "CRUMB – Curatorial Resource for Upstart Media Bliss", "text": "CRUMB – Curatorial Resource for Upstart Media Bliss\n\nCRUMB – Curatorial Resource for Upstart Media Bliss is a research resource for all things related to the practice of curating new media art (digital technology and contemporary art). Via an online discussion list and regular conferences as well as publications, CRUMB collects and disseminates information about curating in the field of New Media art.\n\nCRUMB was founded in 2000 by Beryl Graham and Sarah Cook, as a research institute at the University of Sunderland. CRUMB has been awarded a series of major Research Grants by the Arts and Humanities Research Council, and research partners have included Baltic Centre for Contemporary Art (2000–2006) and from 2007, Eyebeam Atelier (New York) and Lancaster University, UK.\nOver the past ten years, CRUMB has organised many conferences and workshops around the themes of curating, such as the documentation and preservation of New Media art, most recently at the AND Festival in Liverpool in October 2009. CRUMB has published numerous interviews with leading curators including Christiane Paul of the Whitney Museum of American Art, Barbara London of the Museum of Modern Art, and Benjamin Weil formerly of the San Francisco Museum of Modern Art.\n\nThe research group includes PhD students and Postdoctoral researchers, who publish and lecture widely on the subject.\nThey work across a wide range of disciplines in New Media art, Internet art, Fine art, Design, Electronic art, Video art, Digital art, Computer art, etc.\nCurrently the following researchers are working with CRUMB: Sarah Cook, Verina Gfader, Beryl Graham, Axel Lapp, Adinda van 't Klooster, Dominic Smith.\n\n\n\n\n"}
{"id": "27518701", "url": "https://en.wikipedia.org/wiki?curid=27518701", "title": "Documentality", "text": "Documentality\n\nDocumentality is the theory of documents that underlies the ontology of social reality put forward by the Italian philosopher Maurizio Ferraris (see Ferraris 2007, 2008, 2009a and 2009b). The theory gives to documents a central position within the sphere of social objects, conceived as distinct from physical and ideal objects. Ferraris argues that social objects are \"social acts that have been inscribed on some kind of support\", be it a paper document, a magnetic support, or even memory in people's heads (e.g. in the case of the promises we make every day). Thus the constitutive rule of social objects is that \"Object = Inscribed Act\". Therefore, documents as inscriptions possessing social relevance and value embody the essential and prototypical features of any social object, and it is on this basis that it is possible to develop an ontology capable of classifying documents and their selective storage, beginning with the grand divide between strong documents (inscriptions of acts), which make up social objects in the full sense, and weak documents (recordings of facts), which are secondary derivatives and of lesser importance. This theory is inspired, on the one hand, by the reflection on the centrality of writing developed by Jacques Derrida (1967, 1972) and, on the other hand, by the theory of social acts devised by Adolf Reinach (1913) and the theory of linguistic acts by John L. Austin (1962).\n\nIn the contemporary debate, one of the main theories of social objects has been proposed by the American philosopher John R. Searle, in particular in his book \"The Construction of Social Reality\" (1995). Searle's ontology recognizes the sphere of social objects, defining them as higher order objects with respect to physical objects, in accordance with the rule\n\"X counts as Y in C\"\n\nmeaning that the physical object X, for instance a colored piece of paper, counts as Y, a 10 euro banknote, in context C, the Europe of the year 2010. According to Searle, from the iteration of this simple rule the whole complexity of social reality is derived.\n\nPowerful it may be, the theory runs – according to Ferraris – into problems. Firstly, it is not at all obvious how, from the physical object, we manage to get to the social object. If any physical object really can constitute the origin of a social object, then it is not clear what would prevent \"every\" physical object to turn into a social object. But clearly it is not the case that, for instance, if you decide to draw a banknote, you thereby produce a banknote. The standard theory relies on key notion of \"collective intentionality\" to explain the transfiguration of X in Y. However, such a notion – as Ferraris argues – is not at all as clear as it purports to be.\n\nSecondly, how does the reversibility from the social to the physical sphere work? It is fairly intuitive to assert that a banknote is also a piece of paper, or that a President is also a person. As much as it is true that when Searle is alone in a hotel room there is only one physical object, but many social objects (a husband, an employee of the state of California, an American citizen, a driving license holder etc.). In this case, the passage back from Y (the social) to X (the physical) goes smoothly. However, things change in different, although not very peculiar, situations. How should we deal with vague or vast entities, such as a State, a battle, a university? And how about negative entities, such as debts?\n\nThree philosophical theses – inspired, respectively, by the work of the German phenomenologist Adolf Reinach, the Peruvian economist Hernando de Soto, and the French philosopher Jacques Derrida – shape the theory of Documentality.\n\nAccording to the Speech Act Thesis – stemming more from the theory of social acts devised in 1913 by the German phenomenologist Adolf Reinach than from the writings of Austin and Searle – through the performance of speech acts (acts of promising, marrying, accusing, baptizing) we change the world by bringing into being claims, obligations, rights, relations of authority, debts, permissions, names, and a variety of other sorts of entities, thus making up the ontology of the social world. Given that speech acts are evanescent, the physical basis for the temporally extended existence of its products are – in small societies and in simple social interactions – memory traces and other psychological features of the people involved in these acts; and – in large societies and in more complex social interactions – documents. Documents are the physical entities, which create and sustain the sorts of enduring and re-usable deontic powers that extend human memory, and thereby create and sustain the new and more complex forms of social order, which are characteristic of modern civilization.\n\nAccording to a thesis rooted in the works of de Soto (2000) (see also Smith 2003, 2008), economic development can be boosted by a documental development. Through the performance of document acts (acts of filling in, registering, conveying, validating, attaching), we \"change the world\" by bringing into being ownership relations, legal accountability, business organizations, and a variety of other institutional orders of modern societies. As stock and share certificates \"create\" capital, so statutes of incorporation \"create\" companies. As identity documents \"create\" identities (the sorts of things which can be the objects of an identity theft), so diplomas \"create\" academic ranks. Where for de Soto, it is commercial paper documents which create what he calls the \"invisible infrastructure of asset management [...] upon which the astonishing fecundity of Western capitalism rests\" Ferraris goes further and asserts that documents, both in paper and in electronic form, create the invisible infrastructure of contemporary social reality.\n\nDerrida (1967) elaborated a philosophy of writing that finds its most correct application in the social sphere. Concerning speech acts, Derrida (1972) observes that they are mostly inscribed acts, since without records of some sort the performatives would not produce social objects such as conferences, marriages, graduation ceremonies, or constitutions. The point is simple, if we imagine a graduation or a wedding ceremony in which there are no registers and testimonies, it is difficult to maintain that a husband, a wife, or a graduate has been produced. This amounts to saying that social objects turn out to be (as much as the ideal ones) closely linked to the forms of their inscription and recording. However, Derrida was wrong – according to Ferraris (2005; 2009) – in claiming that \"nothing exists outside the text\". Actually physical and ideal objects exist independently from every recording, as independently from there being humanity. This is not the case for social objects, which depend closely on records and the existence of humanity. It is in this sense that, by weakening Derrida's thesis, Ferraris proposed to develop a social ontology starting from the intuition that nothing \"social\" exists outside the text. Keeping this in mind, Ferraris advances an innovative approach to social ontology called Documentality.\n\nThe most influential ontology of social reality, formulated by the American philosopher John Searle (1995), is based on collective intentionality, which allegedly ensures that certain physical objects (e.g. a piece of paper) are transformed into social objects (e.g. a banknote). As noted by Barry Smith (2003), this perspective has difficulty in accounting for both negative entities – such as debts, which apparently do not have a physical counterpart – and the new, seemingly intangible, social objects generated by the Web. The theory of documentality proposed by Maurizio Ferraris (2005) aims to solve these problems by arguing that social objects are always recordings of social acts. This accounts for both negative entities and the virtual entities of the web, which consist precisely of recordings just like any other social object. For the theory of documentality, the constitutive rule of social reality is \"Object = Inscribed Act\", where \"inscribed\" is equal to \"recorded\". That is: a social object is the result of a social act (such as to involve at least two people), which is characterized by its being recorded on some support, including the minds of the people involved in the act (in the case of informal social acts such as promises) . Articulated by Ferraris (2009) in a complete ontological theory and by Smith ( 2012) in a theory of document acts, documentality has three main reasons of interest. First, it has been able to account for the substantial growth of documents and recording devices in the Web world, which is very well explained by the proposed constitutive law of social reality. Secondly, it has been able to explain why social reality, while requiring the presence of subjects for the enactment of acts, may develop independently from them and even without their knowledge (an economic recession can be taking place even if no human subject is aware of it). Third, instead of making social reality depend on the action of collective intentionality – with an increasing social constructivism (Searle 2010) – documentality is capable of substantiating a \"new realism\" (Ferraris, 2012) that helps continental philosophy come out of the impasses of postmodernism and reconnect with analytic philosophy. [Source of this description of Documentality: L. Caffo, \"From Documentality to New Realism\", in \"The Monist\", 97:2 April 2014].\n\nAccording to the ontologist Barry Smith (forthcoming), with documentality, Ferraris advances an innovative approach to social ontology that implies three steps.\n\nThe first step is the recognition – on the ground of the theories developed by Smith himself, (see in particular Smith 1999) – of the sphere of \"social objects\", meaning, entities such as money, artworks, marriages, divorces and joint custody, years in prison and mortgages, the cost of oil and the tax codes, the Nuremberg Trial and the Swedish Academy of Sciences, and still, economic crises, research projects, lectures and university degrees etc. These objects fill up our world more than stones, trees and coconuts do, and they are more important for us, given that a good part of our happiness or unhappiness depends on them.\n\nThe second step is the identification of the law that brings social objects into being, namely that\n\"Object = Inscribed Act\"\n\nWhat this means is that a social object is the result of a social act (one that involves at least two persons or a person and a deputed machine), which is characterized by the fact of being registered on a piece of paper, a computer file or some other digital support, or even, simply, in the heads of persons.\n\nAs Smith recognizes, if taken literally, the OBJECT=Inscribed Act formulation does not make sense. For instance, if taken literally, this formulation implies that the US Constitution \"is made of tiny oxidizing heaps of ink marks on parchment, and matters are helped only slightly if we add together all the printed and digital copies of the US Constitution and assert that the US Constitution is the mereological sum of all these multiple inscriptions.\"\n\nOn the basis of the first two steps it is possible to develop an ontology capable of classifying documents and their selective storage, beginning with the grand divide between what Ferraris calls \"strong documents\" (inscriptions of acts), which make up social objects in the full sense, and \"weak documents\" (recordings of facts), which are secondary derivatives and of lesser importance. The third step thus leads to the individuation of the sphere of Documentality, understood as the search for and the definition of the properties that constitute the necessary and sufficient conditions for the being of a social object.\n\nThe theory of Documentality has been summarized by his author (Ferraris 2009a) in eleven fundamental theses:\n\nDocumentality theory has been used in geopolitics and state theory as part of theory of understanding how nonphysical states can be established. States are precisely the kinds of entities documentality can help understand, because, it has been argued that states do not fit within the traditional Platonist duality of the concrete and the abstract, instead, belonging to a third category, the quasi-abstract. Quasi-abstract objects have received attention from social ontologists of all kinds, including documentary scholars as a response to those social entities that do not fit Searle's \"X counts as Y\" formulation. It is argued that document acts, as understood by documentary theory can establish states and thereby bring about their existence, as well as manipulate them in various ways (such as surrendering them after a war). \n\n\n\n"}
{"id": "31392237", "url": "https://en.wikipedia.org/wiki?curid=31392237", "title": "Dollar Financial Group", "text": "Dollar Financial Group\n\nDollar Financial Group Global Corporation (DFG) is a US-based financial services group with over 1000 locations in seven countries. It focuses on low-income or bad-credit consumers, providing short term loans (payday loans), pawnbroking and gold buying services. DFG's brands include Money Mart (Canada and U.S.), The Money Shop (UK and Ireland), The Check Cashing Store (Florida, U.S.), Loan Mart, Insta-Cheques, and We The People. The group acquired high-end pawnbrokers Suttons & Robertsons in 2010.\n\nIn 2009 DFG was the largest provider of payday loans in the United Kingdom, with around a quarter of the market. In February 2011 DFG additionally acquired the largest British internet payday lender, Month End Money (MEM) (including the brand PaydayUK) for $195m, and suggested The Money Shop's network could grow from around 350 shops to around 1200.\n\nThe company was previously known as \"Monetary Management Corporation\", changing its name in 1990.\n\nThe company was purchased by private equity fund manager Lone Star Funds in 2014.\n\nIn 2013, the United States Consumer Financial Protection Bureau issued a consent order against a subsidiary for making false statements about auto loans to soldiers and veterans. The company was required to refund $3.3 million to service members. In June 2015, a year after its acquisition by Lone Star Funds, the company announced that it would wind down the program and cease taking on new customers.\n\nIn October 2015, the UK Financial Conduct Authority ordered an affiliate to refund £15.4 million to 147,000 customers after finding that the company was lending more to borrowers than they could afford to repay.\n\n"}
{"id": "1701250", "url": "https://en.wikipedia.org/wiki?curid=1701250", "title": "Dream dictionary", "text": "Dream dictionary\n\nA dream dictionary is a tool made for interpreting images in a dream. Dream dictionaries tend to include specific images which are attached to specific interpretations. However, dream dictionaries are generally not considered scientifically viable by those within the psychology community.\nSince the 19th century, the art of dream interpretation has been transferred to a scientific ground, making it a distinct part of psychology. However, the dream symbols of the \"unscientific\" days—the outcome of hearsay interpretations that differ around the world among different cultures—continued to mark the day of an average human-being, who is most likely unfamiliar with Freudian analysis of dreams.\n\nThe dream dictionary includes interpretations of dreams, giving each symbol in a dream a specific meaning. The argument of what dreams represent has greatly changed over time. With this changing, so have the interpretation of dreams. Dream dictionaries have changed in content since they were first published. The Greeks and Romans saw dreams as having a religious meaning. This made them believe that their dreams were an insight into the future and held the key to the solutions of their problems. Aristotle's view on dreams were that they were merely a function of our physiological make up. He did not believe dreams have a greater meaning, solely that it is a result of how we sleep. In the Middle Ages, dreams were seen as an interpretation of good or evil.\n\nAlthough the dream dictionary is not recognized in the psychology world, Freud is said to have revolutionized the interpretation and study of dreams. Freud came to the conclusion that dreams were a form of wish fulfillment. Dream dictionaries were first based upon Freudian thoughts and ancient interpretations of dreams.\n\nSome examples of dream interpretation are: dreaming you are on a beach means you are facing negativity in your life, or a lion may represent a need to control others. Dream dictionaries typically hold interpretations ranging from A-Z. Dream dictionaries can be found in book form or on the internet.\n\n\n\n"}
{"id": "12345804", "url": "https://en.wikipedia.org/wiki?curid=12345804", "title": "Extrinsic finality", "text": "Extrinsic finality\n\nExtrinsic finality is a principle of the philosophy of teleology that holds that a being has a final cause or purpose external to that being itself, in contrast to an intrinsic finality, or self-contained purpose.\nOne example is the view that minerals are \"designed\" to be used by plants that are in turn \"designed\" to be used by animals.\n\nOver-emphasizing extrinsic finality is often criticized as leading to the anthropic attribution of every event to a divine purpose, or superstition. For instance, \"If I hadn't been at the store today, I wouldn't have found that $100 on the ground. God must have intended for me to go to the store so I would find that money.\" or \"We won the game today because of my lucky socks.\" Such abuses were criticized by Francis Bacon, Descartes, and Spinoza.\n"}
{"id": "46214403", "url": "https://en.wikipedia.org/wiki?curid=46214403", "title": "Flossie Bailey", "text": "Flossie Bailey\n\nKatherine \"Flossie\" Bailey (1895–February 6, 1952) was a civil rights and anti-lynching activist from Indiana. She established a local chapter of the National Association for the Advancement of Colored People in Marion, Indiana, in 1918 and became especially active fighting for justice and equality following the double lynching of Thomas Shipp and Abram Smith in 1930. As president of the Indiana NAACP, Bailey was pivotal in lobbying for passage of a statewide anti-lynching law in Indiana in 1931 and advocated for a similar bill at the national level. She was also a recipient of the national NAACP's Madam C. J. Walker Medal.\n\nKatherine Harvey, the daughter of Mr. and Mrs. Charles Harvey, was born in Kokomo, Indiana, in 1895. Known as \"Flossie\", she grew up in Kokomo and attended Kokomo High School.\n\nFlossie married Walter T. Bailey, a physician, in 1917. The couple resided in Marion, Indiana. Doctor Bailey died on February 10, 1950. Their only surviving child was a son, named Walter Charles Bailey.\n\nWhile living in Marion, Bailey also became actively involved in the Marion community. She was a member of the city's Bethel African Methodist Episcopal Church congregation.\n\nBailey spent her adult life seeking equality and justice as a civil rights activist. In 1918 Bailey established the Marion branch of the National Association for the Advancement of Colored People (NAACP). Initially it did not receive much support. In June 1930 it had 96 members; however, after the Marion community was the site of a lynching in August 1930, the local chapter's membership increase to 155 members by the end of 1930.\n\nOn August 7, 1930, a mob broke into the Grant County, Indiana, jail in Marion, dragged two African American men, Thomas Shipp and Abram Smith, to the Courthouse Square and lynched (hanged) them from a tree. Bailey, who was the president of the local branch of the NAACP, tried to obtain police protection for the jailed men prior to the lynching. Afterwards, she was actively involved in organizing a Hoosier delegation of NAACP members to speak to Indiana governor Harry Leslie and persuade him to intervene in the investigation of the men's murders.\n\nBailey and a number of others worked hard to obtain a fair investigation into the double lynching. Despite objections from those in the Marion community who wanted to forget about the event, Bailey and the NAACP played an important role in seeking justice for the murders of Shipp and Smith, as well as to restore calm in Marion. Two men were indicted and went on trial for the lynching, but neither one was found guilty of the crime and no one was ever punished for the deaths of Shipp and Smith.\n\nIn 1930, Bailey was elected president of the Indiana NAACP and helped plan its second annual meeting. The Bailey home in Marion became the Indiana headquarters for the NAACP. The national organization recognized Bailey's efforts by awarding her the Madam C. J. Walker Medal for \"the person who has done the best work in the NAACP during the year.\"\n\nBailey worked to ensure nothing like the lynching in Marion would occur again. She lobbied the Indiana General Assembly to pass stricter anti-lynching laws. When opponents to the bill rejected many of the safety measures, Bailey organized a statewide effort, calling on chapters of the NAACP, Optimist Clubs, Exchange Clubs, and Democratic organizations to lobby state legislators. Bailey's efforts were successful and the governor signed a stricter anti-lynching law in March 1931. Once the Indiana bill became law, Bailey began lobbying for a national anti-lynching law, as well as for the fair treatment of African Americans in other sectors of American life. Bailey especially focused on fighting to end the segregation of schools, hospitals, and other public spaces such as movie theaters.\n\nBailey and the NAACP struggled during the years of the Great Depression. Her husband, who suffered a stroke, closed his medical practice in Marion around 1940, and the couple moved to Indianapolis.\n\nBailey died in Indianapolis on February 6, 1952. As an activist and president of the Indiana NAACP in the 1930s, her leadership helped establish a foundation for the civil rights movement of the 1950s.\n\n"}
{"id": "7075337", "url": "https://en.wikipedia.org/wiki?curid=7075337", "title": "Glass harp", "text": "Glass harp\n\nA glass harp (also called musical glasses, singing glasses, angelic organ, verrilion or ghost fiddle) is a musical instrument made of upright wine glasses.\n\nIt is played by running moistened or chalked fingers around the rim of the glasses. Each glass is tuned to a different pitch, either by grinding each goblet to the specified pitch, in which case the tuning is invariable, or by filling the glass with water until the desired pitch is achieved.\n\nMusical glasses were documented in Persia in the 14th century. The glass harp was created in 1741 by Irishman Richard Pockrich, who is known as the first virtuoso of the musical glasses. Pockrich called his instrument the \"angelic organ\" and it was played with sticks, rather than by rubbing the glasses with a moistened finger. It was reported in 1760 that, \"Pockrich played Handel's \"Water Music\" on the glasses.\" His successful concert career was brought to a premature end by a fire in which both the inventor and instrument perished in 1759.\n\nThe composer Christoph Willibald Gluck played the musical glasses. He performed in London in 1746, and Copenhagen. His instrument consisted of 26 goblets, \"filled with spring water.\"\n\nThe instrument was popular in the 18th century. Pockrich's contemporary, Ford, published \"Instructions for the Playing of the Musical Glasses\" while Bartl published a German version. In 1929 Bruno Hoffmann invented a glass harp consisting, \"of 46 individually tuned glasses fixed on a resonant table.\"\n\nOn March 9, 1938, Bruno Hoffmann performed on the glass harp at the London Museum in a program including Mozart's Adagio (K. 356) and Quintet for harmonica, flute, viola, oboe, and cello (K. 617), accompanied by Geoffrey Gilbert, Leon Goossens, Frederick Riddle, and James Whitehead. It was an \"exquisite performance, in which the flute and viola in their upper registers were almost indistinguishable from the glasses, [which] held spell-bound a large audience, crowded over the floor, stairs and galleries\".\n\nOn February 18, 1979, Gloria Parker performed as a musical glasses soloist with the Hartford Symphony Orchestra at the Jai Alai Fronton in Hartford, Connecticut, USA. Richard Hayman, noted for his arrangements for Boston Pops conductor Arthur Fiedler, was the guest conductor for the 90-piece orchestra that accompanied the musical glasses which included songs such as \"Lara's Theme\" from the movie \"Dr. Zhivago\", \"Lover\" and \"Amor\".\n\nThere are several current musicians who professionally play the glass harp. Among them are the Glass Duo from Poland, Philipp Marguerre and Clemens Hofinger in Germany, France's Jean Chatillion and Thomas Bloch, Brien Engel, and Dennis James in the United States and Canada's Real Berthiaume.\nGlasses have been also used by famous rock band Pink Floyd during the recording of \"Shine On You Crazy Diamond\" on their \"Wish You Were Here\" album, recorded and released in 1975. Igor Sklyarov played the glass harp on the same song during two 2006 concerts recorded in Venice, Italy by former guitarist David Gilmour. Gilmour also used the effect during his August 26, 2006, concert in Gdańsk, Poland, with the help of Guy Pratt, Phil Manzanera and Dick Parry. Both recordings are available on Gilmour's \"Live in Gdańsk\" CD, although the Venice recording is only available on the five-disc version of the album or as an internet download with the three- and four-disc versions.\n\nA colorful set of water tuned glasses is depicted as being played with a pair of metal sticks in several key scenes of the extremely successful 2009 Korean TV drama \"Queen Seon Deok\", showing the series' main anti-heroine Mishil (Go Hyun-jung) playing her own haunting theme melody \"Yurijan (Glasses)\" on that instrument.\n\n\n\n\n"}
{"id": "4226525", "url": "https://en.wikipedia.org/wiki?curid=4226525", "title": "Grid cell", "text": "Grid cell\n\nA grid cell is a type of neuron in the brains of many species that allows them to understand their position in space.\n\nGrid cells were discovered in 2005 by Edvard Moser, May-Britt Moser and their students Torkel Hafting, Marianne Fyhn and Sturla Molden at the Centre for the Biology of Memory (CBM) in Norway. They were awarded the 2014 Nobel Prize in Physiology or Medicine together with John O'Keefe for their discoveries of cells that constitute a positioning system in the brain. The arrangement of spatial firing fields all at equal distances from their neighbors led to a hypothesis that these cells encode a cognitive representation of Euclidean space. The discovery also suggested a mechanism for dynamic computation of self-position based on continuously updated information about position and direction.\n\nIn a typical experimental study, an electrode capable of recording the activity of an individual neuron is implanted in the cerebral cortex of a rat, in a section called the dorsomedial entorhinal cortex, and recordings are made as the rat moves around freely in an open arena. For a grid cell, if a dot is placed at the location of the rat's head every time the neuron emits an action potential, then as illustrated in the adjoining figure, these dots build up over time to form a set of small clusters, and the clusters form the vertices of a grid of equilateral triangles. This regular triangle-pattern is what distinguishes grid cells from other types of cells that show spatial firing. By contrast, if a place cell from the rat hippocampus is examined in the same way (i.e., by placing a dot at the location of the rat's head whenever the cell emits an action potential), then the dots build up to form small clusters, but frequently there is only one cluster (one \"place field\") in a given environment, and even when multiple clusters are seen, there is no perceptible regularity in their arrangement.\n\nIn 1971, John O'Keefe and Jonathon Dostrovsky reported the discovery of place cells in the rat hippocampus—cells that fire action potentials when an animal passes through a specific small region of space, which is called the \"place field\" of the cell. This discovery, although controversial at first, led to a series of investigations that culminated in the 1978 publication of a book by O'Keefe and his colleague Lynn Nadel called \"The Hippocampus as a Cognitive Map\" (a phrase that also appeared in the title of the 1971 paper)—the book argued that the hippocampal neural network instantiates cognitive maps as hypothesized by the psychologist Edward C. Tolman. This theory aroused a great deal of interest, and motivated hundreds of experimental studies aimed at clarifying the role of the hippocampus in spatial memory and spatial navigation.\n\nBecause the entorhinal cortex provides by far the largest input to the hippocampus, it was clearly important to understand the spatial firing properties of entorhinal neurons. The earliest studies, such as Quirk \"et al.\" (1992), described neurons in the entorhinal cortex as having relatively large and fuzzy place fields. The Mosers, however, thought it was possible that a different result would be obtained if recordings were made from a different part of the entorhinal cortex. The entorhinal cortex is a strip of tissue running along the back edge of the rat brain from the ventral to the dorsal sides. Anatomical studies had shown that different sectors of the entorhinal cortex project to different levels of the hippocampus: the dorsal end of the EC projects to the dorsal hippocampus, the ventral end to the ventral hippocampus. This was relevant because several studies had shown that place cells in the dorsal hippocampus have considerably sharper place fields than cells from more ventral levels. Every study of entorhinal spatial activity prior to 2004, however, had made use of electrodes implanted near the ventral end of the EC. Accordingly, together with Marianne Fyhn, Sturla Molden and Menno Witter, the Mosers set out to examine spatial firing from the different dorsal-to-ventral levels of the entorhinal cortex. They found that in the dorsal part of medial entorhinal cortex (MEC), cells had sharply defined place fields like in the hippocampus but the cells fired at multiple locations. The arrangement of the firing fields showed hints of regularity, but the size of the environment was too small for spatial periodicity to be visible in this study.\n\nThe next set of experiments, reported in 2005, made use of a larger environment, which led to the recognition that the cells were actually firing in a hexagonal grid pattern. The study showed that cells at similar dorsal-to-ventral MEC levels had similar grid spacing and grid orientation but the phase of the grid (the offset of the grid vertices relative to the x and y axes) appeared to be randomly distributed between cells. The periodic firing pattern was expressed independently of the configuration of landmarks, in darkness as well as in the presence of visible landmarks and independently of changes in the animal’s speed and direction, leading the authors to suggest that grid cells expressed a path-integration dependent dynamic computation of the animal’s location.\n\nFor their discovery of grid cells, May-Britt Moser, and Edvard Moser were awarded the Nobel Prize in Physiology or Medicine in 2014 alongside John O'Keefe.\n\nGrid cells are neurons that fire when a freely moving animal traverses a set of small regions (firing fields) which are roughly equal in size and arranged in a periodic triangular array that covers the entire available environment. Cells with this firing pattern have been found in all layers of the dorsocaudal medial entorhinal cortex (dMEC), but cells in different layers tend to differ in other respects. Layer II contains the largest density of pure grid cells, in the sense that they fire equally regardless of the direction in which an animal traverses a grid location. Grid cells from deeper layers are intermingled with conjunctive cells and head direction cells (i.e. in layers III, V and VI there are cells with a grid-like pattern that fire only when the animal is facing a particular direction).\n\nGrid cells that lie next to one another (i.e., cells recorded from the same electrode) usually show the same grid spacing and orientation, but their grid vertices are displaced from one another by apparently random offsets. Cells recorded from separate electrodes at a distance from one another, however, frequently show different grid spacings. Cells that are located more ventrally (that is, farther from the dorsal border of the MEC) generally have larger firing fields at each grid vertex, and correspondingly greater spacing between the grid vertices. The total range of grid spacings is not well established: the initial report described a roughly twofold range of grid spacings (from 39 cm to 73 cm) across the dorsalmost part (upper 25%) of the MEC, but there are indications of considerably larger grid scales in more ventral zones. Brun \"et al.\" (2008) recorded grid cells from multiple levels in rats running along an 18-meter track, and found that the grid spacing expanded from about 25 cm in their dorsalmost sites to about 3 m at the ventralmost sites. These recordings only extended 3/4 of the way to the ventral tip, so it is possible that even larger grids exist. Such multi-scale representations have been shown to be information theoretically desirable\n\nGrid cell activity does not require visual input, since grid patterns remain unchanged when all the lights in an environment are turned off. When visual cues are present, however, they exert strong control over the alignment of the grids: Rotating a cue card on the wall of a cylinder causes grid patterns to rotate by the same amount. Grid patterns appear on the first entrance of an animal into a novel environment, and usually remain stable thereafter. When an animal is moved into a completely different environment, grid cells maintain their grid spacing, and the grids of neighboring cells maintain their relative offsets.\n\nWhen a rat is moved to a different environment, the spatial activity patterns of hippocampal place cells usually show \"complete remapping\"—that is, the pattern of place fields reorganizes in a way that bears no detectable resemblance to the pattern in the original environment (Muller and Kubie, 1987). If the features of an environment are altered less radically, however, the place field pattern may show a lesser degree of change, referred to as \"rate remapping\", in which many cells alter their firing rates but the majority of cells retain place fields in the same locations as before. Fyhn \"et al.\" (2007) examined this phenomenon using simultaneous recordings of hippocampal and entorhinal cells, and found that in situations where the hippocampus shows rate remapping, grid cells show unaltered firing patterns, whereas when the hippocampus shows complete remapping, grid cell firing patterns show unpredictable shifts and rotations.\n\nNeural activity in nearly every part of the hippocampal system is modulated by the limbic theta rhythm, which has a frequency range of about 6–9 Hz in rats. The entorhinal cortex is no exception: like the hippocampus, it receives cholinergic and GABAergic input from the medial septal area, the central controller of theta. Grid cells, like hippocampal place cells, show strong theta modulation. Grid cells from layer II of the MEC also resemble hippocampal place cells in that they show phase precession—that is, their spike activity advances from late to early phases of the theta cycle as an animal passes through a grid vertex. Most grid cells from layer III do not precess, but their spike activity is largely confined to half of the theta cycle. The grid cell phase precession is not derived from the hippocampus, because it continues to appear in animals whose hippocampus has been inactivated by an agonist of GABA.\n\nMany species of mammals can keep track of spatial location even in the absence of visual, auditory, olfactory, or tactile cues, by integrating their movements—the ability to do this is referred to in the literature as path integration. A number of theoretical models have explored mechanisms by which path integration could be performed by neural networks. In most models, such as those of Samsonovich and McNaughton (1997) or Burak and Fiete (2009), the principal ingredients are (1) an internal representation of position, (2) internal representations of the speed and direction of movement, and (3) a mechanism for shifting the encoded position by the right amount when the animal moves. Because cells in the MEC encode information about position (grid cells) and movement (head direction cells and conjunctive position-by-direction cells), this area is currently viewed as the most promising candidate for the place in the brain where path integration occurs. However, the question remains unresolved, as in humans the entorhinal cortex does not appear to be required for path integration. Burak and Fiete (2009) showed that a computational simulation of the grid cell system was capable of performing path integration to a high level of accuracy. However, more recent theoretical work has suggested that grid cells might perform a more general denoising process not necessarily related to spatial processing.\n\nHafting et al. (2005) suggested that a place code is computed in the entorhinal cortex and fed into the hippocampus, which may make the associations between place and events that are needed for the formation of memories.\n\nIn contrast to a hippocampal place cell, a grid cell has multiple firing fields, with regular spacing, which tessellate the environment in a hexagonal pattern. The unique properties of grid cells are as follows:\n\nThe grid cells are anchored to external landmarks, but persist in darkness, suggesting that grid cells may be part of a self-motion based map of the spatial environment.\n\n\n"}
{"id": "29331547", "url": "https://en.wikipedia.org/wiki?curid=29331547", "title": "Higher good", "text": "Higher good\n\nHigher good is a \"good\" that is shared and beneficial for all (or most) members of a given community. An example might be an art collector donating their collections to a public museum so all could enjoy the artwork rather than just those privileged enough to see it in private. This is also how the higher good is broadly defined in philosophy, ethics, and political science.\n\n"}
{"id": "247643", "url": "https://en.wikipedia.org/wiki?curid=247643", "title": "House arrest", "text": "House arrest\n\nIn justice and law, house arrest (also called home confinement, home detention, or, in modern times, electronic monitoring) is a measure by which a person is confined by the authorities to their residence. Only those with a house are allowed to be sentenced to arrest in their residence. Travel is usually restricted, if allowed at all. House arrest is an alternative to being in a prison while pre-trial or sentenced.\n\nWhile house arrest can be applied to criminal cases when prison does not seem an appropriate measure, the term is often applied to the use of house confinement as a measure of repression by authoritarian governments against political dissidents. In that case, typically, the person under house arrest does not have access to any means of communication. If electronic communication is allowed, conversations will most likely be monitored. With some electronic monitoring units, the conversations of prisoners can be directly monitored via the unit itself.\n\nJudges have imposed sentences of home confinement, as an alternative to prison, as far back as the 17th century. Galileo was confined to his home following his infamous trial in 1633. Political authorities have often confined leaders to house arrest who were deposed in a coup d'état, but this method was not widely used to confine numerous common criminals.\n\nThis method did not become a widespread alternative to imprisonment in the United States and other western countries until the late 20th century, when newly designed electronic monitoring devices made it inexpensive and easy to manage by corrections authorities. Although Boston was using house arrest for a variety of arrangements, the first-ever court sentence of house arrest with an electronic bracelet was in 1983.\n\nHome detention is an alternative to imprisonment; its goals are both to reduce recidivism and to decrease the number of prisoners, thereby saving money for states and other jurisdictions. It is a corrective to mandatory sentencing laws that greatly increased the incarceration rates in the United States. It allows eligible offenders to retain or seek employment, maintain family relationships and responsibilities and attend rehabilitative programs that contribute towards addressing the causes of their offending.\n\nThe terms of house arrest can differ, but most programs allow employed offenders to continue to work, and confine them to their residence only during non-working hours. Offenders are commonly allowed to leave their home for specific purposes; examples can include visits to the probation officer or police station, religious services, education, attorney visits, court appearances, and medical appointments. Many programs also allow the convict to leave their residence during regular, pre-approved times in order to carry out general household errands, such as food shopping and laundry. Offenders may have to respond to communications from a higher authority to verify that they are at home when required to be. Exceptions are often made to allow visitors to visit the offender.\n\nThe types of house arrest vary in severity according to the requirements of the court order. A curfew may restrict an offender to their house at certain times, usually during hours of darkness. \"Home confinement\" or detention requires an offender to remain at home at all times, apart from the above-mentioned exceptions. The most serious level of house arrest is \"home incarceration\", under which an offender is restricted to their residence 24 hours a day/7 days a week, except for court-approved treatment programs, court appearances, and medical appointments.\n\nIn some exceptional cases, it is possible for a person to be placed under house arrest without trial or legal representation, and subject to restrictions on their associates. In some countries this type of detention without trial has been criticized for breaching the offender's human right to a fair trial. In countries with authoritarian systems of government, the government may use such measures to stifle dissent.\n\nIn some countries, house arrest is often enforced through the use of technology products or services. One method is an electronic sensor locked around the offender's ankle (technically called an ankle monitor, also referred to as a tether). The electronic sensor transmits a GPS signal to a base handset. The base handset is connected to a police station or for-profit monitoring service.\n\nIf the offender goes too far from their home, the violation is recorded, and the police will be notified. To discourage tampering, many ankle monitors detect attempted removal. The monitoring service is often contracted out to private companies, which assign employees to electronically monitor many convicts simultaneously. If a violation occurs the unit signals the office or officer in charge immediately, depending on the severity of the violation. The officer will either call or verify the participant's whereabouts. The monitoring service notifies a convict's probation officer. The electronic surveillance together with frequent contact with their probation officer and checks by the security guards provides for a secure environment.\n\nAnother method of ensuring house arrest compliance is achieved through the use of automated calling services that require no human contact to check on the offender. Random calls are made to the residence. The respondent's answer is recorded and compared automatically to the offender's voice pattern. Authorities are notified only if the call is not answered or if the recorded answer does not match the offender's voice pattern.\n\nElectronic monitoring is considered a highly economical alternative to the cost of imprisoning offenders. In many states or jurisdictions, the convict is often required to pay for the monitoring as part of his or her sentence.\n\n\n\n\n\n\n\nThe People's Republic of China continues to use soft detention, a traditional form of house arrest used by the Chinese Empire.\n\n\n\n\n\n\nIn Italy, house arrest (in Italian \"arresti domiciliari\") is a common practice of detaining suspects, as an alternative to detention in a correctional facility, and is also commonly practiced on those felons who are close to the end of their prison terms, or for those whose health condition does not allow residence in a correctional facility, except some particular cases of extremely dangerous persons. As per article 284 of the Italian Penal Procedure Code, house arrest is imposed by a judge, who orders the suspect to stay confined in his house, home, residence, private property, or any other place of cure or assistance where he/she may be housed at the moment. When necessary, the judge may also forbid any contact between the subject and any person other than those who cohabit with him/her or who assist him/her. If the subject is unable to take care of his/her life necessities or if he/she is in conditions of absolute poverty, the judge may authorize him/her to leave his/her home for the strict necessary time to take care of said needs or to exercise a job. The prosecuting authorities and law enforcement can check at any moment whether the subject, who is \"de facto\" considered in state of detention, is complying with the order; violation of house arrest terms is immediately followed by transfer to a correctional facility. House arrests cannot be applied to a subject that has been found guilty of escape within the previous five years.\n\nNotable cases:\n\nAt sentencing, the judge may sentence an offender to home detention where they would otherwise receive a short-term prison sentence (i.e. two years or less). Home detention sentences range from 14 days and 12 months; offenders are confined to their approved residence 24 hours a day and may only leave with the permission of their probation officer.\n\nElectronic monitoring equipment is extensively used by the New Zealand Department of Corrections to ensure that convicted offenders subject to home detention remain within approved areas. This takes the form of a Global Positioning System tracker fitted to the offender's ankle and monitoring units located at their residence and place of employment. over three thousand persons were serving home detention sentences under GPS surveillance.\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "12982197", "url": "https://en.wikipedia.org/wiki?curid=12982197", "title": "Identity negotiation", "text": "Identity negotiation\n\nIdentity negotiation refers to the processes through which people reach agreements regarding \"who is who\" in their relationships. Once these agreements are reached, people are expected to remain faithful to the identities they have agreed to assume. The process of identity negotiation thus establishes what people can expect of one another. Identity negotiation thus provides the interpersonal \"glue\" that holds relationships together.\n\nThe idea that identities are negotiated originated in the sociological literature during the middle of the 20th century. A leading figure in this movement was Goffman (1959, 1961), who asserted that the first order of business in social interaction is establishing a \"working consensus\" or agreement regarding the roles each person will assume in the interaction. Weinstein and Deutschberger (1964), and later McCall and Simmons (1966), built on this work by elaborating the interpersonal processes that unfold after interaction partners reach an initial working consensus. Within psychology, these ideas were elaborated by Secord and Backman (1965) and Schlenker (1985). The actual phrase \"identity negotiation\" was introduced by Swann (1987), who emphasized the tension between two competing processes in social interaction, behavioral confirmation and self-verification. Behavioral confirmation occurs when one person (the \"perceiver\") encourages another person (the \"target\") to behave in ways that confirm the expectancies of the perceiver (e.g., Rosenthal & Jacobson, 1968; Snyder & Klein, 2005; Snyder, Tanke, & Berscheid, 1977). Self-verification occurs when the \"target\" persuades the \"perceiver\" to behave in a manner that verifies the target's firmly held self-views or identities (Swann, 1983; 1996).\n\nWhen the expectancies of perceivers clash with the self-views of targets, a \"battle of wills\" may occur (Swann & Ely, 1984). Such \"battles\" can range from short-lived, mild disagreements that are quickly and easily solved to highly pitched confrontations that are combative and contentious. On such occasions, the identity negotiation process represents the means through which these conflicting tendencies are reconciled.\n\nMore often than not, the identity negotiation process seems to favor self-verification, which means that people tend to develop expectancies that are congruent with the self-views of target persons (e.g., Major, Cozzarelli, Testa, & McFarlin, 1988); McNulty & Swann, 1994; Swann, Milton, & Polzer, 2000; Swann & Ely, 1984). Such congruence is personally adaptive for targets because it allows them to maintain stable identities and having stable identities is generally adaptive. That is, stable identities not only tell people how to behave, they also afford people with a sense of psychological coherence that reinforces their conviction that they know what to do and the consequences of doing it.\n\nGroups also benefit when there is congruence among group members. When people maintain stable images of themselves, other members of the organization can count on them to \"be\" the same person day in and day out and the identity negotiation process can unfold automatically. This may free people to devote their conscious attention to the work at hand, which may explain why researchers have found that groups characterized by high levels of congruence perform better (Swann et al., 2000). Also, just as demographic diversity tends to undermine group performance when congruence is low, diversity improves performance when congruence is high (Polzer, Milton, & Swann, 2003; Swann, Polzer, Seyle, & Ko, 2004).\n\nSome instances of incongruence in relationships are inevitable. Sudden or unanticipated changes of status or role of one person, or even the introduction of a novel person into a group, may produce discrepancies between people's self-views and the expectancies of others. In work settings, promotions can foment expectancy violations (cf, Burgoon, 1978) if some members of the organization refuse to update their appraisals of the recently promoted person. When incongruence occurs, it will disturb the normal flow of social interaction. Instead of going about their routine tasks, interaction partners will be compelled to shift their conscious attention to the task of accommodating the identity change that is the source of the disruption. Frequent or difficult-to-resolve disruptions could be damaging to the quality of social interactions and ultimately interfere with relationship quality, satisfaction and productivity.\n\n\n"}
{"id": "12009393", "url": "https://en.wikipedia.org/wiki?curid=12009393", "title": "Imagined interaction", "text": "Imagined interaction\n\nImagined interactions (IIs) are a type of social cognition and mental imagery grounded in symbolic interactionism in which individuals imagine conversations with significant others for a variety of purposes (Honeycutt, 2003; 2015). The research program was founded and created by James M. Honeycutt in 1987, who was designated an LSU Distinguished Professor in 2012. He provided a keynote address at the American Association for the Study of Mental Imagery at Yale University in 1987 discussing the functions of imagined interactions and mental imagery. In 2006, Honeycutt's book, \"Imagined Interactions: Daydreaming about Communication\" (2003), was awarded Distinguished Book of the Year by the National Communication Association for developing the original formulations and the II construct which has provided a beneficial mechanism for operationalizing the study of intrapersonal communication, social cognition, daydreaming, and mental imagery. Furthermore, imagined interactions can be used in sports imagery as athletes \"imagine\" positive outcomes of their executions on specific plays or formations (Keaton, Gearhart, & Honeycutt, 2014).\n\nIn the early research, IIs were suggested as a means to operationalize the study of daydreaming as it works to shape communication interpersonally, emotions, and personality. They have been examined cross-culturally (McCann, Honeycutt, & Keaton, 2010).\n\nScripts are activated mindlessly and created through imagined interactions, as people envision contingency plans for actions. In contrast to mindless processing, engaging in imagined interaction requires conscious cognitive processing. Imagined interactions are a type of daydreaming that have definitive attributes and serve a number of functions including rehearsal, self-understanding, relational maintenance, managing conflict, catharsis, and compensation. Retroactive imagined interactions often occur in television shows in terms of “flashbacks” as characters relive prior conversations in their mind.\n\nHoneycutt et al. (1989) discuss how IIs have their theoretical foundation in the work of symbolic interactionists and phenemonologists, including Mead (1934), Dewey (1922) and Schutz (1962). As individuals engage in imagined interactions, cognitive scripts are activated as people imagine how they might react in an upcoming conversation. Imagined interactions help people prepare for encounters. They also help people to relive previous conversations and foster good or bad memories.\n\nWithin imagined interaction theory, Honeycutt (2003) defines eight attributes or characteristics of IIs that can be measured as personality traits or contextually in various situations or with specific individuals. For example, while a person may have a lot of IIs in their daily life, they may have IIs while doing their job or a particular task. Having IIs while playing tennis would interfere with serving and returning the ball.\n\nSpecifically, the eight attributes include the following:\n\n1. Frequency—How often a person has IIs ranging from rarely to quite frequently. For example, lonely people have fewer IIs because they have fewer interaction scenes to access compared to nonlonely people (Honeycutt, 2003).\n\n2. Proactivity—Proactivity refers to those IIs which are engaged in prior to actual interaction, and research has shown that such IIs tend to occur prior to actual interactions. For example, a worker may desire a raise, so she decides to approach her boss concerning the matter. Using an II proactively, she may visualize herself going into her boss’s office and may even devise a plan for what she will say. This is an example of a proactive II.\n\n3. Retroactivity— Retroactivity refers to reviewing an interaction after it has taken place. Proactive and retroactive IIs can simultaneously occur so that conversations are linked together and themes of interpersonal relationships emerge (Honeycutt, 2014). For example, in order to characterize a relationship as competitive or hostile with another person, the person recalls more conversations in which disagreements were communicated as opposed to pleasant, cooperative transactions (Honeycutt, 2010).\n\n4. Variety—Variety refers to the diversity of topics and partners within IIs. IIs which involve various topics are related to the imaginer’s internal locus of control which lends credence to the idea that chronically lonely individuals lack variety in their IIs. However, a person may have IIs with a variety of partners on few topics (e.g., Football plays and strategies are more likely to be the focus of discussion among a team's coaches while in practice in order not be distracted).\n\n5. Discrepancy— Discrepancy refers to the incongruity between IIs and the actual interaction they address; in other words, how similar or different an II is from the relevant interaction. Studies suggest that individuals who are chronically lonely have highly discrepant IIs due to their limited prior interactions upon which to base them. Researchers suspect the high discrepancy of IIs that occurs prior to a new interaction perpetuates and reinforces the state of loneliness (Honeycutt, 2003). \n\n6. Self-Dominance—This attribute addresses who is more prominent in the II: self, both, or other. Research shows that self dominance is in part determined by the scenario of the II. For instance, in IIs involving matters of conflict, the person engaging in the II tends to be more dominant than their II partner. Self dominance is also influenced by culture and tends to characterize collectivistic cultures including Japan compared to individualistic cultures such as America.\n\n7. Valence—Valence reflects how positive or negative emotions are experienced while imagining the conversation. High valence reflects positive emotional affect while low valence reflects negative emotional affect. Honeycutt (2003) reviews research indicating that females report having more pleasant IIs. Level of pleasantness and valence is inversely related to self dominance.\n\n8. Specificity—The final attribute is specificity reflecting the degree of verbal and visual imagery used in the II. This attribute reflects the level of detail and distinction of images contained within IIs. Honeycutt (1998–99) found that those individuals reporting a secure attachment style experience high levels of detailed visual and verbal imagery, suggesting high levels of specificity.\n\nImagined interactions function in the following ways: (1) they keep a relationship alive; (2) they can maintain or resolve conflict; (3) they are used to rehearse messages for future interaction; (4) they aid people in self-understanding through clarifying thoughts and feelings; (5) they provide emotional catharsis by relieving tension; and (6) they compensate for lack of real interaction (Honeycutt, 2003; 2008).\n\n1. Relational Maintenance: People often imagine talking with others that are important in their lives. Additionally, increased uses of proactive and retroactive IIs aided the ability of imagined interactions to predict relational quality. Additionally, relational satisfaction was simultaneously predicted by extraversion and being a judger based on the Myers-Brigg personality inventory in which judging reflect placing a premium on organized environments, competence, performance and independence.\n\n2. Conflict Linkage: For some people, they are obsessed with grudges while others think about how to improve areas of disagreement. Conflict linkage is also referred to as conflict management. However, as noted in some of the theorems of imagined interaction conflict-linkage theory which are highlighted below (e.g., theorem four explains rage). Honeycutt (2004; 2011) explains how conflict linkage is associated with rumination in which people dwell on conflicts and obsessively think about them which is related to depression. \nThe second function of conflict linkage has received a great deal of research attention as it explains how individuals often remember arguments that are many years old and it is difficult for them to \"let go.\" As a result, they harbor old grudges. A series of three axioms (e.g., \"The communication is the relationship\") and nine theorems are discussed explaining how daily conflict is managed or destructively dealt with (see Honeycutt, 2003-2004 for a review). The table below contains an outline of the axioms behind managing conflict in personal relationships and resulting theorems have been empirically tested in social scientific studies.\n\n3. Rehearsal: People plan their messages and what they are going to say as well as anticipating what others will say to them and how the self will respond. Rehearsal often helps people in quickly developing contingencies when plans do not go as expected.\n\n4. Self-Understanding: People have IIs in order to understand their beliefs about values, attitudes, or the opinions they have. IIs allow people to clarify their own thoughts and promote understanding of their own views.\n\n5. Catharsis: People use IIs to relieve tension and anxiety which further reduces uncertainty. IIs provide a mechanism to internally get “things off of one’s chest” and release emotions.\n\n6. Compensation: IIs are used to substitute for real conversations. If you do not have access to another person, you compensate for the lack communication by imagining conversations with the persons. The compensation function is rampant during electrical outages or when cell phone towers are blown down during tornadoes, hurricanes, or other environmental diasasters (Honeycutt and Mapp 2011).\n\nHoneycutt, Pence, and Gearhart (2013) examined the associations between II attributes and the Big Five personality traits. In terms of personality, IIs have trait characteristics to the extent they are enduring and stable across similar conditions. Conversely, they can also be measured in terms of state attributes in which their usage would be higher or lower depending on the particular context. The Big Five Personality Traits include neuroticism, extraversion, openness, agreeableness, and conscientiousness. Honeycutt and his associates found that the frequency and proactivity attributes of imagined interactions are associated with lack of neuroticism and openness to new ideas. Neuroticism increases egocentrism, depression, and anxiety (Hamilton, Buck, Chory, Beatty, & Patrylak, 2009)). Highly anxious people have fewer IIs available to them to “predict” their perceived (or believed) unstable environment. They also found the occurrence of pleasant, non-discrepant IIs in which the self talks more to be moderately associated with the personality dimensions of extraversion and conscientiousness. Extraverts interact with more individuals than do introverts, so extraverts would therefore imagine themselves having these interactions more often than those who are not extraverted. Likewise, they discovered that extraverts have more pleasant IIs, which may be explained through another relationship between extraversion and narcissism. Extraverts think highly of themselves, and most likely portray themselves in a pleasant manner in their imagined interactions.\n\nCorrespondence between II Attributes and Functions\n\nAlthough II theory has been a productive concept for research on communication and social cognition, there is an underlying and yet untested assumption within II theory that the eight attributes are related to all six functions and that II functions can be compared and contrasted in terms of II attributes. Bodie, Honeycutt, and Vickery (2013) conducted two studies exploring the multidimensional nature of functions and attributes. Their first study revealed both corroborative and contradictory evidence for II theory. In line with the internal structure of II theory, it was found that conflict linkage and catharsis IIs are more negatively valenced than those used for compensation and relational maintenance. Rehearsal IIs are more likely to be discrepant than all functions except relational maintenance and are the most proactive. When compared to all other functions, compensatory IIs contain references to more people and were more frequent. It also appears that compensatory and relational maintenance functions are similar insofar as each is equally directed toward others and highly specific, providing support for the role of each in close interpersonal relationships (Honeycutt 2003). Relational maintenance and conflict IIs were used just as frequently as those for catharsis, and relational maintenance IIs were directed toward others in an equivalent manner as those used for conflict.\n\n\nTheorems of Conflict\n\nMadison, Rold, and Honeycutt (2014) examined partisan voting in light of imagined interactions. The researchers asked how respondents who claim to vote for candidates from certain political parties differ from those who do not in terms of the functions of their IIs. They collected data on voting intentions as well as the functions of the IIs of the survey respondents. Both Republicans and Democrats tend to have fewer self-understanding and rehearsal IIs than those who indicated preference for independent candidates, which suggests that voting along party lines may be a heuristic, or “mindless” behavior. This particular study provided the first pathway between IIs into political psychology.\n\n"}
{"id": "24861941", "url": "https://en.wikipedia.org/wiki?curid=24861941", "title": "Information and communication technologies for environmental sustainability", "text": "Information and communication technologies for environmental sustainability\n\nInformation and communication technologies for environmental sustainability (ICT Ensure) or Green ICT as per the International Federation of Global & Green ICT \"IFGICT\" is a general term referring to the application of information and communication technologies (ICTs) within the field of environmental sustainability. Information and communication technologies are acting as integrating and enabling technologies for the economy and they profoundly affect our society. Recent changes in ICT use globally have damaged the environment (in terms of waste and energy consumption etc.) but also have the potential to support environmental sustainability activities, such as the targets set within the Millennium Development Goal (MDG) number 7 (MDG7) to \"ensure environmental sustainability\".\n\nNew technologies provide utilities for knowledge acquisition and awareness, early evaluation of new knowledge, reaching agreements and communication of progress in the interest of the human welfare. This includes ethical aspects of protecting human life as well as aspects of consumer safety and the preservation of our natural environment.\n\nMore and more application areas are becoming relevant to sustainable development in industry, health care, agriculture and the information society, and they affect the perspectives of ICT, the environment, policy and science. More and more interest has been emerged as well to risk and disaster management, adaptation to climate change and resource use.\n\n\n\n"}
{"id": "5920103", "url": "https://en.wikipedia.org/wiki?curid=5920103", "title": "International Fairtrade Certification Mark", "text": "International Fairtrade Certification Mark\n\nThe International FAIRTRADE Certification Mark is an independent certification mark used in over 50 countries. It appears on products as an independent guarantee that a product has been produced according to Fairtrade political standards.\n\nThe FAIRTRADE Mark is owned and protected by Fairtrade International (FLO), on behalf of its 25-member and associate member Fairtrade producer networks and labelling initiatives.\n\nFor a product to carry the FAIRTRADE Mark, it must come from FLO-CERT inspected and certified producer organizations. The crops must be marketed in accordance with the International Fairtrade standards set by Fairtrade International. The supply chain is also monitored by FLO-CERT. To become certified Fairtrade producers, the primary cooperative and its member farmers must operate to certain political standards, imposed from Europe. FLO-CERT, the for-profit side, handles producer certification, inspecting and certifying producer organisations in more than 50 countries in Africa, Asia, and Latin America. In the Fair trade debate there are many complaints of failure to enforce these standards, with Fairtrade cooperatives, importers and packers profiting by evading them.\n\nAs of 2006, the following products currently carry the FAIRTRADE Mark: coffee, tea, chocolate, cocoa, sugar, bananas, apples, pears, grapes, plums, lemons, oranges, Satsumas, clementines, lychees, avocados, pineapples, mangoes, fruit juices, quinoa, peppers, green beans, coconut, dried fruit, rooibos tea, green tea, cakes and biscuits, honey, muesli, cereal bars, jams, chutney and sauces, herbs and spices, nuts and nut oil, wine, beer, rum, flowers, footballs, rice, yogurt, baby food, sugar body scrub, cotton wool and cotton products.\n\nThe marketing system for Fairtrade and non-Fairtrade coffee is identical in the consuming countries, using mostly the same importing, packing, distributing and retailing firms. Some independent brands operate a virtual company, paying importers, packers and distributors and advertising agencies to handle their brand, for cost reasons. In the producing country Fairtrade is marketed only by Fairtrade cooperatives, while other coffee is marketed by Fairtrade cooperatives (as uncertified coffee), by other cooperatives and by ordinary traders.\n\nRetailers and cafes in the rich countries can sell Fairtrade coffee at any price they like, so nearly all the extra price paid by consumers, 82% to 99%, is kept in the rich countries as increased profit. There is however evidence that dishonest importers do not pay the full Fairtrade price, so an even smaller proportion reaches the Third World.\n\nCooperative traders and exporters can sell coffee as Fairtrade certified if they meet the political standards of FLO and they pay a certification and inspection fee. Other administration costs and production costs are incurred to meet these standards. The exporter (not the farmer) is paid a minimum price for Fairtrade certified coffee when the world market is oversupplied, and a Fairtrade premium of 15c per lb at other times. The cooperatives can, on average, sell only a third of their output as Fairtrade, because of lack of demand, and sell the rest at world prices. As the additional costs are incurred on all production, not just that sold as Fairtrade, cooperatives sometimes lose money on their Fairtrade membership. After the additional costs have been subtracted from the Fairtrade price, the rest goes on ‘Social Projects’ such as clinics, women’s groups and baseball pitches.\n\nFarmers do not get any of the higher price under Fairtrade. Nor is there any evidence that they get higher prices as a result of better marketing: the cooperatives sometimes pay farmers a higher price than farmers do, sometimes less, but there is no evidence on which is more common. Farmers do, however,incur extra costs in producing Fairtrade, so they certainly do lose money from Fairtrade membership in some cases. There is little or no research on the extra costs incurred, or the effect of Fairtrade membership on the income of farmers.\n\nDisambiguation: There is widespread confusion because the fair trade industry standards provided by Fairtrade International (The Fairtrade Labelling Organization) use the word “producer” in many different senses, often in the same specification document. Sometimes it refers to farmers, sometimes to the primary cooperatives they belong to, to the secondary cooperatives that the primary cooperatives belong to, or to the tertiary cooperatives that the secondary cooperatives may belong to but “Producer [also] means any entity that has been certified under the Fairtrade International Generic Fairtrade Standard for Small Producer Organizations, Generic Fairtrade Standard for Hired Labour Situations, or Generic Fairtrade Standard for Contract Production.\". The word is used in all these meanings in key documents. In practice, when price and credit are discussed, “producer” means the exporting organization, “For small producers’ organizations, payment must be made directly to the certified small producers’ organization”. and “In the case of a small producers’ organization [e.g. for coffee], Fairtrade Minimum Prices are set at the level of the Producer Organization, not at the level of individual producers (members of the organization)\" which means that the \"producer\" here is halfway up the marketing chain between the farmer and the consumer. The part of the standards referring to cultivation, environment, pesticides and child labour has the farmer as \"producer\". The part referring to democratic organization has the primary cooperative as \"producer\".\n\nFairtrade Standards contain minimum requirements that all producer organisations must meet to become certified as well as progress requirements in which producers must demonstrate improvements over time.\n\nThere are several types of Fairtrade Standards: Standards for small farmers' organizations.”, standards for hired labour situations, standards for contract situations and standards for trade (importers), and there are also standards for the different products.\n\nFairtrade Standards for small farmers' organizations include requirements for democratic decision making, ensuring that producers have a say in how the Fairtrade Premiums are invested etc. They also include requirements for capacity building and economic strengthening of the organization.\n\nFairtrade Standards for hired labour situations ensure that employees receive minimum wages and bargain collectively. Fairtrade-certified plantations must also ensure that there is no forced or child labour and that health and safety requirements are met. (These labor standards do not apply to, Fairtrade \"small farmer cooperatives\" though some have an average of 2.39 ha per farmer of just one crop, coffee, with some single farmers having more than 23 ha coffee, implying substantial use of hired labor.) In a hired labour situation, Fairtrade Standards require a \"joint body\" to be set up with representatives from both the management and the employees. This joint body decides on how Fairtrade Premiums will be spent to benefit plantation employees.\n\nFor some products, such as coffee, only Fairtrade Standards for small farmers' organizations are applicable. For others, such as tea, both small farmers' organizations and plantations can be certified.\n\nTrade standards cover the payment of premiums, of minimum prices, where applicable, the provision of credit to buy the crop, and commercial relationships between the exporting cooperative or other organization and the importer.\n\nTypically, in order for a product to be marked as \"Fair-trade \" at least 20% of its mass must be made up of a Fairtrade product.\n\nFairtrade Standards and procedures are approved by the Fairtrade International Standards Committee, an external committee comprising all FLO stakeholders (labeling initiatives, producers and traders) and external experts. Fairtrade Standards are set by FLO in accordance to the requirements of the ISEAL Code of Good Practice in standard setting and are in addition the result of a consultation process, involving a variety of stakeholders: producers, traders, external experts, inspectors, certification staff etc.\n\nThere are however criticisms of the standards. There have been complaints that Fairtrade standards are inappropriate and may harm producers, sometimes imposing months of additional work for little return. There have also been complaints that standards set by a small committee of activists in the rich north have been imposed on poor farmers in the Third World. Fraser suggests that they are a rag bag of requirements imposed without thought of what is to be achieved or how.\n\nThe main aspects of the Fairtrade system are the Minimum Price and the Premium. These are paid to the exporting firm, usually a second tier cooperative, not to the farmer. They are not paid for everything produced by the cooperative members, but for that proportion of13their output they are able to sell with the brand 'Fairtrade Certified', typically 17% to as much as 60% of their turnover.\n\nThere are complaints that the standards relating to paying of price premiums, minimum prices, provision of credit, etc. by importers in rich countries are not enforced. In particular importers can demand to get a higher quality at the same official Fairtrade price, or withhold other services, threatening to buy from another Fairtrade supplier if the exporter did not agree to this kickback, or if the supplier complains that a kickback is demanded. De Janvry, McIntosh and Sadoulet have quantified this for a large group of Fairtrade coffee cooperatives in South America over a dozen years. They found that this kickback was 10c a pound over a period when the official price premium was 5c or 10c a pound, and this, plus the certification fee, meant that the cooperatives made a loss in years when a premium was payable, and were paid substantially less than the official minimum prices in years when a minimum price was payable. These should have been identified and rectified by the certification agency.\n\nFairtrade inspection and certification are carried out, for a fee, by FLO-CERT, an independent, for profit, body created by Fairtrade International in 2004. FLO-CERT certifies that both producers and traders have met with Fairtrade Standards and that producers have invested any surplus received through Fairtrade in social projects.\n\nFLO-CERT works with a network of around 100 independent inspectors that regularly visit producer and trade organizations and report back to FLO-CERT. All certification decisions are then taken by a Certification Committee, composed of stakeholders from producers, traders, national labelling organisations and external experts. An Appeals Committee handles all appeals.\n\nFLO-CERT inspections and certification follow the international ISO standards for product certification bodies (ISO 65).\n\nThere have been claims that adherence to fair trade standards by producers has been poor and that enforcement of standards by Fairtrade is very weak, notably by Christian Jacquiau. and by Paola Ghillani, who spent four years as president of Fairtrade Labelling Organizations. There is criticism of poor enforcement: labourers on Fairtrade farms in Peru are paid less than the minimum wage; some non-Fairtrade coffee is sold as Fairtrade; \"the standards are not very strict in the case of seasonally hired labour in coffee production\"; \"some fair trade standards are not strictly enforced\"; and supermarkets may avoid their responsibility. In 2006, a \"Financial Times\" journalist found that ten out of the ten mills they visited had sold uncertified coffee to co-operatives as certified. It reported that they were \"also handed evidence of at least one coffee association that received Fairtrade certification despite illegally growing some 20 per cent of its coffee in protected national forest land.\n\nFairtrade farmers and marketing organizations incur a wide range of costs in achieving and maintaining certification. They incur these costs on all their production, but they can only recover costs on the small part of their production that they can sell as \"Fairtrade certified\". In practice they can sell only small of their output as Fairtrade, because of lack of demand, and must sell the rest as uncertified at world prices. For example, there is not enough demand to take all the certified coffee produced, so most has to be sold as uncertified. In 2001 only 13.6% could be sold as certified so limits were placed on new cooperatives joining the scheme. This plus an increased demand put up sales of certified to around 50% in 2003 with a figure of 37% commonly cited in recent years. Some exporting cooperatives do not manage to sell any of their output as certified, and others sell as little as 8%. Weber reports cooperatives not able to cover the extra costs of a marketing team for Fairtrade, with one covering only 70% of these costs after six years of Fairtrade membership.\n\nCertified organizations such as cooperatives have to pay FLO-CERT a fee to become certified and a further annual fee for audit and continued certification Fairtrade inspection and certification are carried out, for a fee. The first year certification fee per unit sold as \"Fairtrade certified\" varies but has been over 6c/lb with an annual fee of 3c/lb to 3.4c/b for coffee up to 2006 in some countries, at a time when the \"Fairtrade premium\" was 5c to 10c/lb.\n\nThe cooperative or other certified organization has to spend money on conforming to the standards, with changed employment practices, the introduction and administration of the required democratic processes, changed processing, labelling and packing, changed material. They also incur extra costs in selling: . Weber reports cooperatives not able to cover the extra costs of a marketing team for Fairtrade, with one covering only 70% of these costs after six years of Fairtrade membership.\n\nIt is generally agreed that some organizations make a loss from their Fairtrade certification. but there are very few economic studies showing what happened to the money.\n\nFairtrade farmers also have to meet a large range of criteria on production: there are limits on using child labour, pesticides, herbicides, genetically modified products etc. These cost money, mean that the farmers have to do more work in the hot sun, and that they have to hire labour instead of using family labour. In times when world prices are so low that there is no “social premium” and the minimum price is paid, some farmers have negotiated that some of the money is paid to them, rather than being used for social projects.\n\nFairtrade labelled coffee, the first Fairtrade labelled product, was first launched in the Netherlands in 1988. The label, launched by Nico Roozen and Dutch missionary Frans van der Hoff, was then called Max Havelaar after a fictional Dutch character who opposed the exploitation of coffee pickers in Dutch colonies. Fairtrade labelling allowed Fairtrade Certified goods to be sold outside the World shops for the first time and into mainstream retailers, reaching a larger consumer segment and boosting sales significantly.\n\nThe concept caught on: in the ensuing years, similar non-profit Fairtrade labelling organizations were set up in other European countries and North America, called “Max Havelaar” (in Belgium, Switzerland, Denmark, Norway and France), \"Transfair\" (in Germany, Luxembourg, Austria, Italy, the United States, Canada and Japan), or carrying a national name: “Fairtrade Mark” in the UK and Ireland, \"Rättvisemärkt\" in Sweden, and \"Reilu Kauppa\" in Finland. Initially, the Max Havelaars and the Transfairs each had their own Fairtrade standards, product committees and monitoring systems. In 1994, a process of convergence among the labelling organizations – or \"LIs\" (for \"Labelling Initiatives\") – started with the establishment of a TransMax working group, culminating in 1997 in the creation of Fairtrade Labelling Organizations International, now known simply as Fairtrade International (FLO). FLO is an umbrella organization whose mission is to set the Fairtrade Standards, support, inspect and certify disadvantaged producers and harmonize the Fairtrade message across the movement.\n\nIn 2002, FLO launched a new FAIRTRADE Certification Mark. The goals of the launch were to improve the visibility of the Mark on supermarket shelves, convey a dynamic, forward-looking image for Fairtrade, facilitate cross border trade and simplify procedures for importers and traders.\n\nThe FAIRTRADE Mark harmonization process is still under way – as of March 2011, all but two labelling initiatives (TransFair USA and TransFair Canada) have fully adopted the new international Certification Mark. These two organizations currently use the Fair Trade Certified Mark, however Canadian organization began actively promoting the new international Certification Mark in 2010 as part of a total transition toward it. TransFair USA has apparently elected to continue with its own mark for the time being.\n\nAt present, over 19 FLO Member Labelling Initiatives are using the International Fairtrade Certification Mark. There are now Fairtrade Certification Marks on dozens of different products, based on FLO’s certification for coffee, tea, rice, bananas, mangoes, cocoa, cotton, sugar, honey, fruit juices, nuts, fresh fruit, quinoa, herbs and spices, wine and footballs etc.\nAccording to the economist Bruce Wydick with the median coffee drinker willing to pay a premium of 50 cents for a cup of fair-trade coffee even in the best-case scenario for fair trade, when world prices are at their lowest, the maximum amount a fair-trade grower from that same cup of coffee would receive is only one third of a cent Wydick lists his points against the alleged benefits of fair trade:\nAccording to Colleen Haight from San Jose State University is in the fact that Fairtrade doesn't buy the complete production of a producer, making him sell his better products on the free market and passing on his lower quality goods to the fairtrade channel.\n\n"}
{"id": "5891216", "url": "https://en.wikipedia.org/wiki?curid=5891216", "title": "Master Hilarion", "text": "Master Hilarion\n\nThe Master Hilarion, is considered a female saint within the I AM movement and is one of the \"Masters of the Ancient Wisdom\" and in the Ascended Master Teachings is one of the Ascended Masters (also collectively called the Great White Brotherhood). Creator of the Orion's 144,000 Council 5th Dimension Merkhaba yellow infusion ascension Kundalini ((astral projection, soul travel)) Eckankar Master she is considered to be the Chohan (Lord) of the Fifth Ray (see Seven Rays).\n\nThe Master Hilarion is believed to have been incarnated as the Apostle Paul ((Roman Catholic Divine Female)) of Tarsus and the Neo-Platonic philosopher Iamblichus. Some have believed that she was the Christian saint Hilarion. This is possible, since although the years Iamblichus, also known as Iamblichus Chalcidensis, (c. 245 - c. 325) was embodied overlap the lifetime of St. Hilarion (291 - 371), various traditions such as Hindu and Tibetan Buddhism hold that it is possible for the spiritually advanced to have more than one incarnation happening at the same time.\n\nIn October 1884 Helena Blavatsky made reference to Hilarion (using the spelling: \"Hillarion\"):\n\nOn 20 February 1881 Kuthumi, in one of his letters to Sinnett, referred to him as\n\nHis travel to his \"final initiation\" is referred to in an entry in Henry Olcott's diary, dated 19 February 1881, written in Bombay:\n\nTheosophist C.W. Leadbeater wrote that the Master Hilarion's primary influence is upon the scientists of the world. \n\nIn the teachings of Alice A. Bailey, the fifth ray of the seven rays, called by Alice A. Bailey the \"orange ray\", which she is said to oversee, is called the ray of \"concrete science\". \n\nIn the Ascended Master Teachings, as \"Hierarch of the Brotherhood of Truth\" in the etheric plane over Crete, Hilarion is said to assist the scientists and spiritual leaders of the world with the \"flame of truth\" and channels the spiritual energy of what is called in the Ascended Master Teachings the \"green ray\" or \"emerald ray\", the 5th of the Seven Rays. It is believed in the Ascended Master Teachings that before the Master Hilarion took over the chohanship of the Fifth Ray, Lord Ling fulfilled that function.\n\nK. Paul Johnson speculates that the \"Masters\" that Madame Blavatsky wrote about and produced letters from were actually idealizations of people who were her mentors. Johnson asserts that the \"Master Hilarion\" was actually Ooton Liato, a stage magician from Cyprus whom she met in New York City in 1873.\n\n\n\n"}
{"id": "28145280", "url": "https://en.wikipedia.org/wiki?curid=28145280", "title": "May Wilson", "text": "May Wilson\n\nMay Wilson (1905 – October 19, 1986) was an American artist and figure in the 1960s to 1970s New York City avant-garde art world. A pioneer of the feminist and mail art movement, she is best known for her Surrealist junk assemblages and her \"Ridiculous Portrait\" photocollages.\n\nWilson was born in Baltimore, Maryland, into an underprivileged family. Her father died when she was young. She was reared by her Irish Catholic mother, who sewed piecework at home. Wilson left school after the ninth grade to become a stenographer/secretary to help support her family. When she turned 20, she married a young lawyer, William S. Wilson, Jr., and give birth to her first child. She continued to work until the birth of her second child, after which she devoted her energies primarily to mothering and homemaking. In 1942, the couple had prospered enough to move to Towson, Maryland, where she began to take correspondence courses in art and art history from several schools, including the University of Chicago. In 1948, after the marriage of their daughter, the couple moved to a gentleman's farm north of Towson, where she pursued painting and gave private art lessons to neighbors. She exhibited her paintings, scenes of everyday life painted in a flat, purposefully primitive manner in local galleries and restaurants. In 1952 and 1958, she won awards for work submitted to juried exhibitions at the Baltimore Museum of Art.\n\nIn 1956, her son, the writer Williams S. Wilson, gave to Ray Johnson, the founder of the New York Correspondence School, his mother's address. This began a friendship and artistic collaboration between Johnson and Wilson, which would last the remainder of her life. Wilson became an integral part of Johnson's mail art circle and was initiated into the New York avant-garde through letters and small works that she exchanged with Robert Watts, George Brecht, Ad Reinhardt, Leonard Cohen, Arman, and many others.\n\nWhen her marriage dissolved, she moved to New York City in the spring of 1966, aged 61, taking up residence first in the Chelsea Hotel and then in a studio next door, where she threw legendary soirées and became known as the \"Grandma Moses of the Underground\". By the time she arrived, Wilson was already working with photomontage techniques. Encouraged by Johnson, who had sent her magazines through the mail, she scissored patterns into images of pin-up girls and muscle men until they resembled doilies or snowflakes, as Wilson called them. She decorated her hotel room and later her studio on West 23rd Street with these and other manipulated, found object images.\n\nAround this time, she also began her series of \"Ridiculous Portraits\", for which she would ride the subway to Times Square, where she made exaggerated faces in photo booths. She then would cut and paste her photo-booth face onto postcards, along with Old Master reproductions, fashion shoots, and softcore magazine pornography. Long before artists such as Cindy Sherman and Yasumasa Morimura embarked on similar critical projects, Wilson's \"Ridiculous Portraits\" sent up the ubiquitous sexism and ageism that exists in popular and fine-art images of women.\n\nAt the age of 70, she converted a nude photograph of herself into a stamp that she pasted on envelopes. Her collages and humorous self-portraits were made as gifts and mail-art items for her friends and were not widely known until after her death. She was also an innovator of junk art assemblages that incorporated real objects, such as high-heel shoes, bed sheets, sauce pans, toasters, liquor bottles, ice trays, and wrapped baby dolls. Her sculptures were inspired by Surrealist and Dada practices and are similar in spirit to Yayoi Kusama's contemporary accumulations. Wilson was the subject of a 1969 experimental documentary by Amalie R. Rothschild, \"Woo Hoo? May Wilson\".\n\nShe died of pneumonia on Sunday, October 19, 1986, at the Village Nursing Home in Manhattan. She was survived by her children, William S. Wilson, III, and Mrs. Betty Jane Butler as well as six grandchildren, and a great-granddaughter.\n\nSince her death, May Wilson's work has been featured in numerous exhibitions and retrospectives at the Baltimore Museum of Art, Maryland; Gracie Mansion Gallery, New York; the Morris Museum, Morristown, N.J.; the Pavel Zoubok Gallery, New York City; and The University of the Arts, Philadelphia.\n\n\n\n\n"}
{"id": "200877", "url": "https://en.wikipedia.org/wiki?curid=200877", "title": "Maze generation algorithm", "text": "Maze generation algorithm\n\nMaze generation algorithms are automated methods for the creation of mazes.\n\nA maze can be generated by starting with a predetermined arrangement of cells (most commonly a rectangular grid but other arrangements are possible) with wall sites between them. This predetermined arrangement can be considered as a connected graph with the edges representing possible wall sites and the nodes representing cells. The purpose of the maze generation algorithm can then be considered to be making a subgraph in which it is challenging to find a route between two particular nodes.\n\nIf the subgraph is not connected, then there are regions of the graph that are wasted because they do not contribute to the search space. If the graph contains loops, then there may be multiple paths between the chosen nodes. Because of this, maze generation is often approached as generating a random spanning tree. Loops, which can confound naive maze solvers, may be introduced by adding random edges to the result during the course of the algorithm.\n\nThe animation shows the maze generation steps for a \ngraph that is not on a rectangular grid.\nFirst, the computer creates a random planar graph G\nshown in blue, and its dual F\nshown in yellow. Second, computer traverses F using a chosen\nalgorithm, such as a depth-first search, coloring the path red.\nDuring the traversal, whenever a red edge crosses over a blue edge,\nthe blue edge is removed.\nFinally, when all vertices of F have been visited, F is erased\nand two edges from G, one for the entrance and one for the exit, are removed.\n\nThis algorithm is a randomized version of the depth-first search algorithm. Frequently implemented with a stack, this approach is one of the simplest ways to generate a maze using a computer. Consider the space for a maze being a large grid of cells (like a large chess board), each cell starting with four walls. Starting from a random cell, the computer then selects a random neighbouring cell that has not yet been visited. The computer removes the wall between the two cells and marks the new cell as visited, and adds it to the stack to facilitate backtracking. The computer continues this process, with a cell that has no unvisited neighbours being considered a dead-end. When at a dead-end it backtracks through the path until it reaches a cell with an unvisited neighbour, continuing the path generation by visiting this new, unvisited cell (creating a new junction). This process continues until every cell has been visited, causing the computer to backtrack all the way back to the beginning cell. We can be sure every cell is visited.\n\nAs given above this algorithm involves deep recursion which may cause stack overflow issues on some computer architectures. The algorithm can be rearranged into a loop by storing backtracking information in the maze itself. This also provides a quick way to display a solution, by starting at any given point and backtracking to the beginning.\n\nMazes generated with a depth-first search have a low branching factor and contain many long corridors, because the algorithm explores as far as possible along each branch before backtracking.\n\nThe depth-first search algorithm of maze generation is frequently implemented using backtracking:\n\n\nThis algorithm is a randomized version of Kruskal's algorithm.\n\n\nThere are several data structures that can be used to model the sets of cells. An efficient implementation using a disjoint-set data structure can perform each union and find operation on two sets in nearly constant amortized time (specifically, formula_1 time; formula_2 for any plausible value of formula_3), so the running time of this algorithm is essentially proportional to the number of walls available to the maze.\n\nIt matters little whether the list of walls is initially randomized or if a wall is randomly chosen from a nonrandom list, either way is just as easy to code.\n\nBecause the effect of this algorithm is to produce a minimal spanning tree from a graph with equally weighted edges, it tends to produce regular patterns which are fairly easy to solve.\n\nThis algorithm is a randomized version of Prim's algorithm.\n\n\nIt will usually be relatively easy to find the way to the starting cell, but hard to find the way anywhere else.\n\nNote that simply running classical Prim's on a graph with random edge weights would create mazes stylistically identical to Kruskal's, because they are both minimal spanning tree algorithms. Instead, this algorithm introduces stylistic variation because the edges closer to the starting point have a lower effective weight.\n\nAlthough the classical Prim's algorithm keeps a list of edges, for maze generation we could instead maintain a list of adjacent cells. If the randomly chosen cell has multiple edges that connect it to the existing maze, select one of these edges at random. This will tend to branch slightly more than the edge-based version above.\n\nAll the above algorithms have biases of various sorts: depth-first search is biased toward long corridors, while Kruskal's/Prim's algorithms are biased toward many short dead ends. Wilson's algorithm, on the other hand, generates an \"unbiased\" sample from the uniform distribution over all mazes, using loop-erased random walks.\n\nWe begin the algorithm by initializing the maze with one cell chosen arbitrarily. Then we start at a new cell chosen arbitrarily, and perform a random walk until we reach a cell already in the maze—however, if at any point the random walk reaches its own path, forming a loop, we erase the loop from the path before proceeding. When the path reaches the maze, we add it to the maze. Then we perform another loop-erased random walk from another arbitrary starting cell, repeating until all cells have been filled.\n\nThis procedure remains unbiased no matter which method we use to arbitrarily choose starting cells. So we could always choose the first unfilled cell in (say) left-to-right, top-to-bottom order for simplicity.\n\nMazes can be created with \"recursive division\", an algorithm which works as follows: Begin with the maze's space with no walls. Call this a chamber. Divide the chamber with a randomly positioned wall (or multiple walls) where each wall contains a randomly positioned passage opening within it. Then recursively repeat the process on the subchambers until all chambers are minimum sized. This method results in mazes with long straight walls crossing their space, making it easier to see which areas to avoid.\n\nFor example, in a rectangular maze, build at random points two walls that are perpendicular to each other. These two walls divide the large chamber into four smaller chambers separated by four walls. Choose three of the four walls at random, and open a one cell-wide hole at a random point in each of the three. Continue in this manner recursively, until every chamber has a width of one cell in either of the two directions.\n\nOther algorithms exist that require only enough memory to store one line of a 2D maze or one plane of a 3D maze. They prevent loops by storing which cells in the current line are connected through cells in the previous lines, and never remove walls between any two cells already connected.\n\nMost maze generation algorithms require maintaining relationships between cells within it, to ensure the end result will be solvable. Valid simply connected mazes can however be generated by focusing on each cell independently. A binary tree maze is a standard orthogonal maze where each cell always has a passage leading up or leading left, but never both. To create a binary tree maze, for each cell flip a coin to decide whether to add a passage leading up or left. Always pick the same direction for cells on the boundary, and the end result will be a valid simply connected maze that looks like a binary tree, with the upper left corner its root.\n\nA related form of flipping a coin for each cell is to create an image using a random mix of forward slash and backslash characters. This doesn't generate a valid simply connected maze, but rather a selection of closed loops and unicursal passages. (The manual for the Commodore 64 presents a BASIC program using this algorithm, but using PETSCII diagonal line graphic characters instead for a smoother graphic appearance.)\n\nCertain types of cellular automata can be used to generate mazes. Two well-known such cellular automata, Maze and Mazectric, have rulestrings B3/S12345 and B3/S1234. In the former, this means that cells survive from one generation to the next if they have at least one and at most five neighbours. In the latter, this means that cells survive if they have one to four neighbours. If a cell has exactly three neighbours, it is born. It is similar to Conway's Game of Life in that patterns that do not have a living cell adjacent to 1, 4, or 5 other living cells in any generation will behave identically to it. However, for large patterns, it behaves very differently from Life.\n\nFor a random starting pattern, these maze-generating cellular automata will evolve into complex mazes with well-defined walls outlining corridors. Mazecetric, which has the rule B3/S1234 has a tendency to generate longer and straighter corridors compared with Maze, with the rule B3/S12345. Since these cellular automaton rules are deterministic, each maze generated is uniquely determined by its random starting pattern. This is a significant drawback since the mazes tend to be relatively predictable.\n\nLike some of the graph-theory based methods described above, these cellular automata typically generate mazes from a single starting pattern; hence it will usually be relatively easy to find the way to the starting cell, but harder to find the way anywhere else.\n\nExample implementation of a variant of Prim's algorithm in Python/NumPy. Prim's algorithm above starts with a grid full of walls and grows a single component of pathable tiles. In this example, we start with an open grid and grow multiple components of walls.\n\nThis algorithm works by creating n (density) islands of length p (complexity). An island is created by choosing a random starting point with odd coordinates, then a random direction is chosen. If the cell two steps in the direction is free, then a wall is added at both one step and two steps in this direction. The process is iterated for n steps for this island. p islands are created. n and p are expressed as float to adapt them to the size of the maze. With a low complexity, islands are very small and the maze is easy to solve. With low density, the maze has more \"big empty rooms\".\n\nThe code below is an example of depth-first search maze generator in C. \n\n"}
{"id": "16457116", "url": "https://en.wikipedia.org/wiki?curid=16457116", "title": "Minor sabotage", "text": "Minor sabotage\n\nA minor sabotage (\"aka\" little sabotage or small sabotage; ) during World War II in Nazi-occupied Poland (1939–45) was any underground resistance operation that involved a disruptive but relatively minor and non-violent form of defiance, such as the painting of graffiti, the manufacture of fake documents, the disrupting of German propaganda campaigns, and the like. Minor-sabotage operations often involved elements of humor.\nThe purpose of minor-sabotage operations was primarily psychological — to show Polish civilians that the resistance remained active, and thus bolster civilian morale, and to wear down the German occupier.\n\nIn September 1939, during the German invasion of Poland, after the fall of Warsaw, a young Polish student, Elżbieta Zahorska, tore down a German poster. Soon after, she was executed for her act; her death, however, instead of cowing others, inspired an entire new branch of Polish resistance, called minor sabotage.\n\nSeveral organizations dedicated to minor sabotage were created in 1939 and 1940, notably PLAN, Wawer and Palmiry. Minor sabotage was often carried out by scouting organizations such as \"Szare Szeregi\". On a larger scale, it was coordinated by the Directorate of Civil Resistance of the Polish Underground State and, in some cases, by its military arm, the Home Army (see Operation N). Thousands were involved in minor sabotage. During two weeks in March and April 1942 when the \"kotwica\" symbol was introduced, it was painted all around Warsaw by a 400-strong dedicated team.\n\nAleksander Kamiński, a teacher and scouting activist, soon became a major figure in organizing such operations. In November 1940 he published an article in the main Polish underground newspaper, \"Biuletyn Informacyjny\", explaining how to carry out such acts.\n\nNotable or common minor-sabotage operations included:\n\nA particularly notable operation was carried out by Maciej Aleksy Dawidowski on 11 February 1942. Soon after the Germans had occupied Warsaw in 1939, they had placed on the Nicolaus Copernicus Monument on \"Krakowskie Przedmieście\" a large plaque proclaiming Copernicus to have been a German astronomer. Dawidowski removed and concealed the German plaque. In response, the Germans moved Warsaw's statue of Jan Kiliński to the National Museum in Warsaw. Immediately, Dawidowski and his comrades retaliated by placing a large graffito on the Museum (\"People of Warsaw—I am here. Jan Kiliński\") and adding a new plaque to the Copernicus monument: \"For removal of the Kiliński statue, I am extending the winter by two months. Kopernik.\" Even though most minor sabotage operations took place in Warsaw, they also were organized in other cities of occupied Poland, such as Częstochowa (painting anti-German graffiti, destruction of German signs, affixing of Polish posters), Kielce (defacing of German symbols on official signs, stamping newspapers with the Kotwica, painting of a large symbol of the Polish underground state on the tower of the Cathedral church), and Kraków (writing \"Hitler Kaputt\" on the walls, selling fake copies of the local daily \"Goniec Krakowski\").\n\n\n"}
{"id": "45261139", "url": "https://en.wikipedia.org/wiki?curid=45261139", "title": "Modesty in medical settings", "text": "Modesty in medical settings\n\nModesty in medical settings refers to the practices and equipment used to preserve patient modesty in medical examination and clinics.\n\nPrior to the invention of the stethoscope, a physician who wanted to perform auscultation to listen to heart sounds or noise inside a body would have to physically place their ear against the body of the person being examined. In 1816, male physician René Laennec invented the stethoscope as a way to respect the modesty of a female patient, as it would have been awkward for him to put his ear on her chest.\n\nHospital gowns increase modesty as compared to the patient presenting nude, but in the past have been odd clothing which exposes the body. Some contemporary changes to the design of hospital gowns are proposed.\n\nIn places with more cultural diversity it becomes more likely that people will make new and different requests for modesty in health care.\n\nSometimes women do not access healthcare because of modesty concerns.\n\nMuslims in non-Muslim societies sometimes make requests for modesty.\n\n"}
{"id": "20171", "url": "https://en.wikipedia.org/wiki?curid=20171", "title": "Murder", "text": "Murder\n\nMurder is the unlawful killing of another human without justification or valid excuse, especially the unlawful killing of another human being with malice aforethought. This state of mind may, depending upon the jurisdiction, distinguish murder from other forms of unlawful homicide, such as manslaughter. Manslaughter is a killing committed in the absence of \"malice\", brought about by reasonable provocation, or diminished capacity. \"Involuntary\" manslaughter, where it is recognized, is a killing that lacks all but the most attenuated guilty intent, recklessness. \n\nMost societies consider murder to be an extremely serious crime, and thus believe that the person charged should receive harsh punishments for the purposes of retribution, deterrence, rehabilitation, or incapacitation. In most countries, a person convicted of murder generally faces a long-term prison sentence, possibly a life sentence; and in a few, the death penalty may be imposed.\n\nThe modern English word \"murder\" descends from the Proto-Indo-European \"mrtró\" which meant \"to die\". The Middle English \"mordre\" is a noun from Anglo-Saxon \"morðor\" and Old French \"murdre\". Middle English \"mordre\" is a verb from Anglo-Saxon \"myrdrian\" and the Middle English noun.\n\nThe eighteenth-century English jurist William Blackstone (citing Edward Coke), in his \"Commentaries on the Laws of England\" set out the common law definition of murder, which by this definition occurs\nThe elements of common law murder are:\n\n\nThe Unlawful – This distinguishes murder from killings that are done within the boundaries of law, such as capital punishment, justified self-defence, or the killing of enemy combatants by lawful combatants as well as causing collateral damage to non-combatants during a war.\n\nKilling – At common law life ended with cardiopulmonary arrest – the total and irreversible cessation of blood circulation and respiration. With advances in medical technology courts have adopted irreversible cessation of all brain function as marking the end of life.\n\nСriminal act or omission – Killing can be committed by an act or an omission.\n\nof a human – This element presents the issue of when life begins. At common law, a fetus was not a human being. Life began when the fetus passed through the vagina and took its first breath.\n\nby another human – In early common law, suicide was considered murder. The requirement that the person killed be someone other than the perpetrator excluded suicide from the definition of murder.\n\nwith malice aforethought – Originally \"malice aforethought\" carried its everyday meaning – a deliberate and premeditated (prior intent) killing of another motivated by ill will. Murder necessarily required that an appreciable time pass between the formation and execution of the intent to kill. The courts broadened the scope of murder by eliminating the requirement of actual premeditation and deliberation as well as true malice. All that was required for malice aforethought to exist is that the perpetrator act with one of the four states of mind that constitutes \"malice\".\n\nThe four states of mind recognized as constituting \"malice\" are:\n\nUnder state of mind (i), intent to kill, the \"deadly weapon rule\" applies. Thus, if the defendant intentionally uses a deadly weapon or instrument against the victim, such use authorizes a permissive inference of intent to kill. In other words, \"intent follows the bullet\". Examples of deadly weapons and instruments include but are not limited to guns, knives, deadly toxins or chemicals or gases and even vehicles when intentionally used to harm one or more victims.\n\nUnder state of mind (iii), an \"abandoned and malignant heart\", the killing must result from the defendant's conduct involving a reckless indifference to human life and a conscious disregard of an unreasonable risk of death or serious bodily injury. In Australian jurisdictions, the unreasonable risk must amount to a foreseen probability of death (or grievous bodily harm in most states), as opposed to possibility.\n\nUnder state of mind (iv), the felony-murder doctrine, the felony committed must be an inherently dangerous felony, such as burglary, arson, rape, robbery or kidnapping. Importantly, the underlying felony \"cannot\" be a lesser included offense such as assault, otherwise all criminal homicides would be murder as all are felonies.\n\nAs with most legal terms, the precise definition of murder varies between jurisdictions and is usually codified in some form of legislation. Even when the legal distinction between murder and manslaughter is clear, it is not unknown for a jury to find a murder defendant guilty of the lesser offence. The jury might sympathise with the defendant (e.g. in a crime of passion, or in the case of a bullied victim who kills their tormentor), and the jury may wish to protect the defendant from a sentence of life imprisonment or execution.\n\nMany jurisdictions divide murder by degrees. The distinction between first- and second-degree murder exists, for example, in Canadian murder law and U.S. murder law.\n\nThe most common division is between first- and second-degree murder. Generally, second-degree murder is common law murder, and first-degree is an aggravated form. The aggravating factors of first-degree murder depend on the jurisdiction, but may include a specific intent to kill, premeditation, or deliberation. In some, murder committed by acts such as strangulation, poisoning, or lying in wait are also treated as first-degree murder. A few states in the U.S. further distinguish third-degree murder, but they differ significantly in which kinds of murders they classify as second-degree versus third-degree. For example, Minnesota defines third-degree murder as depraved-heart murder, whereas Florida defines third-degree murder as felony murder (except when the underlying felony is specifically listed in the definition of first-degree murder).\n\nSome jurisdictions also distinguish premeditated murder. This is the crime of wrongfully and intentionally causing the death of another human being (also known as murder) after rationally considering the timing or method of doing so, in order to either increase the likelihood of success, or to evade detection or apprehension.\nState laws in the United States vary as to definitions of \"premeditation\". In some states, premeditation may be construed as taking place mere seconds before the murder. Premeditated murder is one of the most serious forms of homicide, and is punished more severely than manslaughter or other types of murder, often with a life sentence without the possibility of parole, or in some countries, the death penalty. In the U.S, federal law () criminalizes premeditated murder, felony murder and second-degree murder. In Canada, the Criminal Code classifies murder as either 1st- or 2nd-degree. The former type of murder is often called premeditated murder, although premeditation is not the only way murder can be classified as first-degree.\n\nAccording to Blackstone, English common law identified murder as a \"public wrong\". According to common law, murder is considered to be \"malum in se\", that is an act which is evil within itself. An act such as murder is wrong or evil by its very nature. And it is the very nature of the act which does not require any specific detailing or definition in the law to consider murder a crime.\n\nSome jurisdictions still take a common law view of murder. In such jurisdictions, what is considered to be murder is defined by precedent case law or previous decisions of the courts of law. However, although the common law is by nature flexible and adaptable, in the interests both of certainty and of securing convictions, most common law jurisdictions have codified their criminal law and now have statutory definitions of murder.\n\nAlthough laws vary by country, there are circumstances of exclusion that are common in many legal systems.\n\n\nAll jurisdictions require that the victim be a natural person; that is, a human being who was still alive before being murdered. In other words, under the law one cannot murder a corpse, a corporation, a non-human animal, or any other non-human organism such as a plant or bacterium.\n\nCalifornia's murder statute, Penal Code Section 187, was interpreted by the Supreme Court of California in 1994 as not requiring any proof of the viability of the fetus as a prerequisite to a murder conviction. This holding has two implications. The first is a defendant in California can be convicted of murder for killing a fetus which the mother herself could have terminated without committing a crime. The second, as stated by Justice Stanley Mosk in his dissent, is that because women carrying nonviable fetuses may not be visibly pregnant, it may be possible for a defendant to be convicted of intentionally murdering a person they did not know existed.\n\nSome countries allow conditions that \"affect the balance of the mind\" to be regarded as mitigating circumstances. This means that a person may be found guilty of \"manslaughter\" on the basis of \"diminished responsibility\" rather than being found guilty of murder, if it can be proved that the killer was suffering from a condition that affected their judgment at the time. Depression, post-traumatic stress disorder and medication side-effects are examples of conditions that may be taken into account when assessing responsibility.\n\nMental disorder may apply to a wide range of disorders including psychosis caused by schizophrenia and dementia, and excuse the person from the need to undergo the stress of a trial as to liability. Usually, sociopathy and other personality disorders are not legally considered insanity, because of the belief they are the result of free will in many societies. In some jurisdictions, following the pre-trial hearing to determine the extent of the disorder, the defence of \"not guilty by reason of insanity\" may be used to get a not guilty verdict. This defence has two elements:\n\nUnder New York law, for example:\nUnder the French Penal Code:\nThose who successfully argue a defence based on a mental disorder are usually referred to mandatory clinical treatment until they are certified safe to be released back into the community, rather than prison. A criminal defendant is often presented with the option of pleading \"not guilty by reason of insanity\". Thus, a finding of insanity results in a not-guilty verdict, although the defendant is placed in a state treatment facility where they could be kept for years or even decades.\n\nPostpartum depression (also known as post-natal depression) is recognized in some countries as a mitigating factor in cases of infanticide. According to Dr. Susan Friedman, \"Two dozen nations have infanticide laws that decrease the penalty for mothers who kill their children of up to one year of age. The United States does not have such a law, but mentally ill mothers may plead not guilty by reason of insanity.\" In the law of the Republic of Ireland, infanticide was made a separate crime from murder in 1949, applicable for the mother of a baby under one year old where \"the balance of her mind was disturbed by reason of her not having fully recovered from the effect of giving birth to the child or by reason of the effect of lactation consequent upon the birth of the child\". Since independence, death sentences for murder in such cases had always been commuted; the new act was intended \"to eliminate all the terrible ritual of the black cap and the solemn words of the judge pronouncing sentence of death in those cases ... where it is clear to the Court and to everybody, except perhaps the unfortunate accused, that the sentence will never be carried out.\" In Russia, murder of a newborn child by the mother has been separate crime since 1996.\n\nFor a killing to be considered murder in nine out of fifty states in the US, there normally needs to be an element of intent. A defendant may argue that they took precautions not to kill, that the death could not have been anticipated, or was unavoidable. As a general rule, manslaughter constitutes reckless killing, but manslaughter also includes criminally negligent (i.e. grossly negligent) homicide. Unintentional killing that results from an involuntary action generally cannot constitute murder. After examining the evidence, a judge or jury (depending on the jurisdiction) would determine whether the killing was intentional or unintentional.\n\nIn those jurisdictions using the Uniform Penal Code, such as California, diminished capacity may be a defence. For example, Dan White used this defence to obtain a manslaughter conviction, instead of murder, in the assassination of Mayor George Moscone and Supervisor Harvey Milk. Afterward, California amended its penal code to provide \"As a matter of public policy there shall be no defense of diminished capacity, diminished responsibility, or irresistible impulse in a criminal action...\"\n\nMurder with specified aggravating circumstances is often punished more harshly. Depending on the jurisdiction, such circumstances may include:\n\n\nIn the United States and Canada, these murders are referred to as first-degree or aggravated murders. Murder, under English criminal law, always carries a mandatory life sentence, but is not classified into degrees. Penalties for murder committed under aggravating circumstances are often higher, under English law, than the 15-year minimum non-parole period that otherwise serves as a starting point for a murder committed by an adult.\n\nA legal doctrine in some common law jurisdictions broadens the crime of murder: when an offender kills in the commission of a dangerous crime, (regardless of intent), he/she is guilty of murder. The felony murder rule is often justified by its supporters as a means of deterring dangerous felonies, but the case of Ryan Holle shows it can be used very widely.\n\nIn some common law jurisdictions, a defendant accused of murder is not guilty if the victim survives for longer than one year and one day after the attack. This reflects the likelihood that if the victim dies, other factors will have contributed to the cause of death, breaking the chain of causation; and also means that the responsible person does not have a charge of murder \"hanging over their head indefinitely\". Subject to any statute of limitations, the accused could still be charged with an offence reflecting the seriousness of the initial assault.\n\nWith advances in modern medicine, most countries have abandoned a fixed time period and test causation on the facts of the case. This is known as \"delayed death\" and cases where this was applied or was attempted to be applied go back to at least 1966.\n\nIn England and Wales, the \"year-and-a-day rule\" was abolished by the Law Reform (Year and a Day Rule) Act 1996. However, if death occurs three years or more after the original attack then prosecution can take place only with the Attorney-General's approval.\n\nIn the United States, many jurisdictions have abolished the rule as well. Abolition of the rule has been accomplished by enactment of statutory criminal codes, which had the effect of displacing the common-law definitions of crimes and corresponding defences. In 2001 the Supreme Court of the United States held that retroactive application of a state supreme court decision abolishing the year-and-a-day rule did not violate the Ex Post Facto Clause of Article I of the United States Constitution.\n\nThe potential effect of fully abolishing the rule can be seen in the case of 74-year-old William Barnes, charged with the murder of a Philadelphia police officer Walter Barkley, who he'd shot nearly 41 years before. Barnes had served 16 years in prison for attempting to murder Barkley, but when the policeman died on August 19, 2007, this was alleged to be from complications of the wounds suffered from the shooting - and Barnes was charged with his murder. He was acquitted on May 24, 2010.\n\nMartin Daly and Margo Wilson of McMaster University have claimed that several aspects of homicides, including the genetic relations or proximity between murderers and their victims, (as in the Cinderella effect), can often be explained by the evolution theory or evolutionary psychology. \n\nIn the Abrahamic religions, the first ever murder was committed by Cain against his brother Abel out of jealousy. In the past, certain types of homicide were lawful and justified. Georg Oesterdiekhoff wrote:\n\nIn many such societies the redress was not via a legal system, but by blood revenge, although there might also be a form of payment that could be made instead—such as the weregild which in early Germanic society could be paid to the victim's family in lieu of their right of revenge.\n\nOne of the oldest-known prohibitions against murder appears in the Sumerian Code of Ur-Nammu written sometime between 2100 and 2050 BC. The code states, \"If a man commits a murder, that man must be killed.\"\n\nIn Judeo-Christian traditions, the prohibition against murder is one of the Ten Commandments given by God to Moses in (Exodus: 20v13) and (Deuteronomy 5v17). The Vulgate and subsequent early English translations of the Bible used the term \"secretly killeth his neighbour\" or \"smiteth his neighbour secretly\" rather than \"murder\" for the Latin \"clam percusserit proximum\". Later editions such as Young's Literal Translation and the World English Bible have translated the Latin \"occides\" simply as \"murder\" rather than the alternatives of \"kill\", \"assassinate\", \"fall upon\", or \"slay\".\n\nIn Islam according to the Qur'an, one of the greatest sins is to kill a human being who has committed no fault. \"For that cause We decreed for the Children of Israel that whosoever killeth a human being for other than manslaughter or corruption in the earth, it shall be as if he had killed all mankind, and whoso saveth the life of one, it shall be as if he had saved the life of all mankind.\" \"And those who cry not unto any other god along with Allah, nor take the life which Allah hath forbidden save in (course of) justice, nor commit adultery – and whoso doeth this shall pay the penalty.\"\n\nThe term \"assassin\" derives from Hashshashin, a militant Ismaili Shi'ite sect, active from the 8th to 14th centuries. This mystic secret society killed members of the Abbasid, Fatimid, Seljuq and Crusader elite for political and religious reasons. The Thuggee cult that plagued India was devoted to Kali, the goddess of death and destruction. According to some estimates the Thuggees murdered 1 million people between 1740 and 1840. The Aztecs believed that without regular offerings of blood the sun god Huitzilopochtli would withdraw his support for them and destroy the world as they knew it. According to Ross Hassig, author of \"Aztec Warfare\", \"between 10,000 and 80,400 persons\" were sacrificed in the 1487 re-consecration of the Great Pyramid of Tenochtitlan.\n\nSouthern slave codes did make willful killing of a slave illegal in most cases. For example, the 1860 Mississippi case of \"Oliver v. State\" charged the defendant with murdering his own slave. In 1811, the wealthy white planter Arthur Hodge was hanged for murdering several of his slaves on his plantation in the British West Indies.\n\nIn Corsica, vendetta was a social code that required Corsicans to kill anyone who wronged their family honor. Between 1821 and 1852, no fewer than 4,300 murders were perpetrated in Corsica.\n\nThe World Health Organization reported in October 2002 that a person is murdered every 60 seconds. An estimated 520,000 people were murdered in 2000 around the globe. Another study estimated the worldwide murder rate at 456,300 in 2010 with a 35% increase since 1990. Two-fifths of them were young people between the ages of 10 and 29 who were killed by other young people. Because murder is the least likely crime to go unreported, statistics of murder are seen as a bellwether of overall crime rates.\n\nMurder rates vary greatly among countries and societies around the world. In the Western world, murder rates in most countries have declined significantly during the 20th century and are now between 1 and 4 cases per 100,000 people per year.\n\nMurder rates in jurisdictions such as Japan, Singapore, Hong Kong, Iceland, Sweden, Switzerland, Italy, Spain and Germany are among the lowest in the world, around 0.3–1 cases per 100,000 people per year; the rate of the United States is among the highest of developed countries, around 4.5 in 2014, with rates in larger cities sometimes over 40 per 100,000. The top ten highest murder rates are in Honduras (91.6 per 100,000), El Salvador, Ivory Coast, Venezuela, Belize, Jamaica, U.S. Virgin Islands, Guatemala, Saint Kitts and Nevis and Zambia. (UNODC, 2011 – ).\n\nThe following absolute murder counts per-country are not comparable because they are not adjusted by each country's total population. Nonetheless, they are included here for reference, with 2010 used as the base year (they may or may not include justifiable homicide, depending on the jurisdiction). There were 52,260 murders in Brazil, consecutively elevating the record set in 2009. Over half a million people were shot to death in Brazil between 1979 and 2003. 33,335 murder cases were registered across India, about 19,000 murders committed in Russia, approximately 17,000 murders in Colombia (the murder rate was 38 per 100,000 people, in 2008 murders went down to 15,000), approximately 16,000 murders in South Africa, approximately 15,000 murders in the United States, approximately 26,000 murders in Mexico, approximately 13,000 murders in Venezuela, approximately 4,000 murders in El Salvador, approximately 1,400 murders in Jamaica, approximately 550 murders in Canada and approximately 470 murders in Trinidad and Tobago. Pakistan reported 12,580 murders.\n\nIn the United States, 666,160 people were killed between 1960 and 1996. Approximately 90% of murders in the US are committed by males. Between 1976 and 2005, 23.5% of all murder victims and 64.8% of victims murdered by intimate partners were female. For women in the US, homicide is the leading cause of death in the workplace.\n\nIn the US, murder is the leading cause of death for African American males aged 15 to 34. Between 1976 and 2008, African Americans were victims of 329,825 homicides. In 2006, Federal Bureau of Investigation's Supplementary Homicide Report indicated that nearly half of the 14,990 murder victims that year were Black (7421). In the year 2007, there were 3,221 black victims and 3,587 white victims of non-negligent homicides. While 2,905 of the black victims were killed by a black offender, 2,918 of the white victims were killed by white offenders. There were 566 white victims of black offenders and 245 black victims of white offenders. The \"white\" category in the Uniform Crime Reports (UCR) includes non-black Hispanics. In London in 2006, 75% of the victims of gun crime and 79% of the suspects were \"from the African/Caribbean community\".\nMurder demographics are affected by the improvement of trauma care, which has resulted in reduced lethality of violent assaults – thus the murder rate may not necessarily indicate the overall level of social violence.\n\nWorkplace homicide, which tripled during the 1980s, is the fastest growing category of murder in America.\n\nDevelopment of murder rates over time in different countries is often used by both supporters and opponents of capital punishment and gun control. Using properly filtered data, it is possible to make the case for or against either of these issues. For example, one could look at murder rates in the United States from 1950 to 2000, and notice that those rates went up sharply shortly after a moratorium on death sentences was effectively imposed in the late 1960s. This fact has been used to argue that capital punishment serves as a deterrent and, as such, it is morally justified. Capital punishment opponents frequently counter that the United States has much higher murder rates than Canada and most European Union countries, although all those countries have abolished the death penalty. Overall, the global pattern is too complex, and on average, the influence of both these factors may not be significant and could be more social, economic, and cultural.\n\nDespite the immense improvements in forensics in the past few decades, the fraction of murders solved has decreased in the United States, from 90% in 1960 to 61% in 2007. Solved murder rates in major U.S. cities varied in 2007 from 36% in Boston, Massachusetts to 76% in San Jose, California. Major factors affecting the arrest rate include witness cooperation and the number of people assigned to investigate the case.\n\nAccording to scholar Pieter Spierenburg homicide rates per 100,000 in Europe have fallen over the centuries, from 35 per 100,000 in medieval times, to 20 in 1500 AD, 5 in 1700, to below two per 100,000 in 1900.\n\nIn the United States, murder rates have been higher and have fluctuated. They fell below 2 per 100,000 by 1900, rose during the first half of the century, dropped in the years following World War II, and bottomed out at 4.0 in 1957 before rising again. The rate stayed in 9 to 10 range most of the period from 1972 to 1994, before falling to 5 in present times. The increase since 1957 would have been even greater if not for the significant improvements in medical techniques and emergency response times, which mean that more and more attempted homicide victims survive. According to one estimate, if the lethality levels of criminal assaults of 1964 still applied in 1993, the country would have seen the murder rate of around 26 per 100,000, almost triple the actually observed rate of 9.5 per 100,000.\nA similar, but less pronounced pattern has been seen in major European countries as well. The murder rate in the United Kingdom fell to 1 per 100,000 by the beginning of the 20th century and as low as 0.62 per 100,000 in 1960, and was at 1.28 per 100,000 . The murder rate in France (excluding Corsica) bottomed out after World War II at less than 0.4 per 100,000, quadrupling to 1.6 per 100,000 since then.\n\nThe specific factors driving this dynamics in murder rates are complex and not universally agreed upon. Much of the raise in the U.S. murder rate during the first half of the 20th century is generally thought to be attributed to gang violence associated with Prohibition. Since most murders are committed by young males, the near simultaneous low in the murder rates of major developed countries circa 1960 can be attributed to low birth rates during the Great Depression and World War II. Causes of further moves are more controversial. Some of the more exotic factors claimed to affect murder rates include the availability of abortion and the likelihood of chronic exposure to lead during childhood (due to the use of leaded paint in houses and tetraethyllead as a gasoline additive in internal combustion engines).\nIn many countries, in news reports, journalists are typically careful not to call a killing a murder until the perpetrator is convicted of such. After arrest, journalists write that the person was \"arrested on suspicion of murder\". When a prosecutor files charges, the accused is referred to as an \"accused murderer\".\n\n\n\n\n\n"}
{"id": "7048926", "url": "https://en.wikipedia.org/wiki?curid=7048926", "title": "Negative-bias temperature instability", "text": "Negative-bias temperature instability\n\nNegative-bias temperature instability (NBTI) is a key reliability issue in MOSFETs. NBTI manifests as an increase in the threshold voltage and consequent decrease in drain current and transconductance of a MOSFET. The degradation exhibits a power-law dependence on time. It is of immediate concern in p-channel MOS devices (pMOS), since they almost always operate with negative gate-to-source voltage; however, the very same mechanism also affects nMOS transistors when biased in the accumulation regime, i.e. with a negative bias applied to the gate.\n\nThe details of the mechanisms of NBTI have been debated, but two effects are believed to contribute: trapping of positively charged holes, and generation of interface states. \n\n\nThe existence of two coexisting mechanisms has resulted in scientific controversy over the relative importance of each component, and over the mechanism of generation and recovery of interface states. \n\nIn sub-micrometer devices nitrogen is incorporated into the silicon gate oxide to reduce the gate leakage current density and prevent boron penetration. It is known that incorporating nitrogen enhances NBTI. For new technologies (45 nm and shorter nominal channel lengths), high-K metal gate stacks are used as an alternative to improve the gate current density for a given equivalent oxide thickness (EOT). Even with the introduction of new materials like hafnium oxides, NBTI remains.\n\nWith the introduction of high K metal gates, a new degradation mechanism appeared, referred to as PBTI (for positive bias temperature instabilities), which affects nMOS transistor when positively biased. In this case, no interface states are generated and 100% of the Vth degradation may be recovered.\n\n\n"}
{"id": "32078264", "url": "https://en.wikipedia.org/wiki?curid=32078264", "title": "Oscilloscoop", "text": "Oscilloscoop\n\nOscilloScoop is an interactive music app for iOS that runs on the iPad, iPhone, and iPod Touch, available through the iTunes App Store. The app presents a real-time interface for creating electronic music in various electronic music genres from hip hop to techno and house. The app is designed with a minimalist modern interface of three colored crowns, that spin like potters' wheels. Touching each crown affects the parameters of the audio. The top crown controls pitch, the middle a low pass resonant filter, and the bottom a fader. Controls on the left allow one to change the bassline beat, while buttons on the right allow the user to save their creation, and a slider below controls tempo.\n\nIn 2011, OscilloScoop won the ZKM App Art Award, the first award ever given to art apps.\n\nOscilloScoop was designed by Lukas Girling, a creator of interactive musical interfaces who has worked with world-renowned musicians and music technologists including Laurie Anderson and Max Mathews. The app was created in collaboration with interactive artist and programmer Scott Snibbe, author of the bestselling apps Bubble Harp and Gravilux; and Graham McDermott, noted video game and app developer.\n\nGirling and Snibbe began working together in 1996 at Interval Research researching new forms of musical instruments with Joy Mountford and Bob Adams. As part of the research groups there, they were able to collaborate with, and learn from highly regarded musicians and musical pioneers including Brian Eno, Laurie Anderson, and Max Mathews. Their work at Interval Research was never published, and they recently re-united to produce a new app for iOS that allows ordinary people to create electronic music with similar results as professionals.\n\nTheir work borrows from the language of DJs, using the body's movements to create music in real-time, rather than composing music slice-by-slice with complex music authoring tools like ProTools and Ableton Live, creating \"something as effortless (and fun) as Super Mario,\" according to Snibbe.\n"}
{"id": "18588253", "url": "https://en.wikipedia.org/wiki?curid=18588253", "title": "Partial residual plot", "text": "Partial residual plot\n\nIn applied statistics, a partial residual plot is a graphical technique that attempts to show the relationship between a given independent variable and the response variable given that other independent variables are also in the model.\n\nWhen performing a linear regression with a single independent variable, a scatter plot of the response variable against the independent variable provides a good indication of the nature of the relationship. If there is more than one independent variable, things become more complicated. Although it can still be useful to generate scatter plots of the response variable against each of the independent variables, this does not take into account the effect of the other independent variables in the model.\n\nPartial residual plots are formed as:\nwhere\n\nPartial residual plots are widely discussed in the regression diagnostics literature (e.g., see the References section below). Although they can often be useful, they can also fail to indicate the proper relationship. In particular, if \"X\" is highly correlated with any of the other independent variables, the variance indicated by the partial residual plot can be much less than the actual variance. These issues are discussed in more detail in the references given below.\n\nThe CCPR (component and component-plus-residual) plot is a refinement of the partial residual plot, adding\n\nThis is the \"component\" part of the plot and is intended to show where the \"fitted line\" would lie.\n\n\n\n"}
{"id": "31615713", "url": "https://en.wikipedia.org/wiki?curid=31615713", "title": "Pennies for Pakistan", "text": "Pennies for Pakistan\n\nPennies for Pakistan is a term several different organizations have used at different times for programs to raise humanitarian or educational aid for Pakistan.\nThese organizations include:\n\nPennies for Peace\n"}
{"id": "32752215", "url": "https://en.wikipedia.org/wiki?curid=32752215", "title": "Peter Nagy (artist)", "text": "Peter Nagy (artist)\n\nPeter Nagy (born Bridgeport, Connecticut, 1959) worked as an artist and exhibited his work throughout the United States and Europe during the 1980s. He also was a gallery owner of Gallery Nature Morte in New York City at the same time. He now represents Indian contemporary artists.\nWith artist Alan Belcher opened Gallery Nature Morte in East Village, Manhattan, New York City in 1982. Peter Nagy was a part of a generation of the East Village artist-gallery owners who established a small and rough but trendy avant-garde alternative to the established SoHo art scene. The gallery was open for six years, until 1988. They combined Conceptualism and Pop Art exploring the relationship between the art and the commodity.\n\nIn 1992, Nagy moved to New Delhi where he revived Gallery Nature Morte in 1997. The Indian artist Subodh Gupta has said of him: \"he has fresh eyes and has provided a platform for contemporary artists.\" \n\nRichard Milazzo, \"Peter Nagy: Entertainment Erases History. Works 1982 to 2004 to the Present\" Brooklyn, Eisbox Projects (2014)\n\n\n"}
{"id": "26013907", "url": "https://en.wikipedia.org/wiki?curid=26013907", "title": "Pornography Victims Compensation Act", "text": "Pornography Victims Compensation Act\n\nThe Pornography Victims Compensation Act of 1991 was a bill, S. 983, in the U.S. Congress. The sponsor in the Senate was Senator Mitch McConnell with eight cosponsors. A Senate committee held hearings on the bill. The bill was not voted on, did not pass, and did not become law.\n\nUnder the bill, a person who was attacked after the attacker was substantially spurred by pornography could have been able to sue the pornography's producers, publishers, distributors, exhibitors, and sellers without needing a prior criminal charge against the pornography itself. It was written not to prohibit any publication, but to hold liable for certain consequences, according to McConnell. For political pragmatism, the bill was limited to child pornography and obscene material, that being already unprotected by the U.S. Constitution's First Amendment.\n\nAs part of the rationale for passage, McConnell argued \"that crime is fostered by a culture in which the sexual degradation, abuse, and murder of women and children are a form of entertainment\", that \"[t]he connection between the amount of violent entertainment and the amount of real-life violence is no longer seriously doubted among social scientists,\" that \"more than one million children from six months to sixteen years old are sexually molested and then filmed or photographed\", and that \"[p]ornography is fueling violence in this country\".\n\nThe formal title varied by year, as listed in the History section, below.\n\nInformally, it was known as the \"Bundy Bill\", after serial murderer Ted Bundy, who attributed his killings partly to porn.\n\nThe bill or versions of it had been under Congressional consideration for seven years prior. Earlier versions reached a wider range of pornography but had less support; narrowing that range to what was unprotected by Supreme Court decisions on the First Amendment led to wider support.\n\nOther versions, searched for as introduced from approximately 1973 to part of 2010, included these:\n\nSupport came from Feminists Fighting Pornography and 200 National Organization for Women (NOW) chapters, but not two in New York and California and not from the national level of NOW. Support simultaneously came from Christian fundamentalists.\n\nOpponents included Nadine Strossen, Betty Friedan, Adrienne Rich, Katha Pollitt, Karen DeCrow, Nora Ephron, Mary Gordon, Judy Blume, Jamaica Kincaid, Erica Jong, Susan Isaacs, and \"172 other feminist women\" and Mary Morello.\n\nSupporters, according to Sen. McConnell, included the Family Research Council, Feminists Fighting Pornography, the American Family Association, victims rights groups, and some chapters of the National Organization for Women. In the opposition was an editorial in The New York Times.\n\nCriticisms came from more than one direction: that the bill would punish a wide range of nonpornographic movies because criminals were inspired by them, that it would lead to bans of feminist positive literature about women, that booksellers would be timid about many titles that weren't obscene but just generally controversial, that scientists hadn't established a firm link between porn and misbehavior, that criminals should be held responsible for their actions rather than third parties being held liable, that similar legislation against bars because subsequent drunken driving led to accidents had not been tested against major beer producers, that using civil procedure rather than criminal to test if material is obscene when the standard of proof is lower in civil cases would make finding it obscene more likely, that juries seeing an attacked woman as a victim would be likelier to judge that material was obscene, that obscenity being defined by community standards and retailers' skittishness meant that the strictest community would be the standard-bearer for the nation, and that it didn't encompass all the pornography that feminists found violated women's civil rights.\n\n"}
{"id": "240863", "url": "https://en.wikipedia.org/wiki?curid=240863", "title": "Psychological pricing", "text": "Psychological pricing\n\nPsychological pricing (also price ending, charm pricing) is a pricing and marketing strategy based on the theory that certain prices have a psychological impact. Retail prices are often expressed as \"odd prices\": a little less than a round number, e.g. $19.99 or £2.98. There is evidence that consumers tend to perceive \"odd prices\" as being lower than they actually are, tending to round to the next lowest monetary unit. Thus, prices such as $1.99 are associated with spending $1 rather than $2. The theory that drives this is that lower pricing such as this institutes greater demand than if consumers were perfectly rational. Psychological pricing is one cause of price points.\n\nAccording to a 1997 study published in the \"Marketing Bulletin\", approximately 60% of prices in advertising material ended in the digit 9, 30% ended in the digit 5, 7% ended in the digit 0 and the remaining seven digits combined accounted for only slightly over 3% of prices evaluated. In the UK, before the withdrawal of the halfpenny coin in 1969, prices often ended in 11d (elevenpence halfpenny: just under a shilling, which was 12d); another example was £1/19/11¾d. (one pound, nineteen shillings and elevenpence three farthings) which is one farthing under £2. This is still seen today in gasoline (petrol) pricing ending in of the local currency's smallest denomination; for example in the US the price of a gallon of gasoline almost always ends in US$0.009 (e.g. US$1.799).\n\nIn a traditional cash transaction, fractional pricing imposes tangible costs on the vendor (printing fractional prices), the cashier (producing awkward change) and the customer (stowing the change). These factors have become less relevant with the increased use of checks, credit and debit cards and other forms of currency-free exchange; also, the addition of sales tax makes the pre-tax price less relevant to the amount of change (although in Europe the sales tax is generally included in the shelf price).\n\nThe psychological pricing theory is based on one or more of the following hypotheses:\n\nThe theory of psychological pricing is controversial. Some studies show that buyers, even young children, have a very sophisticated understanding of true cost and relative value and that, to the limits of the accuracy of the test, they behave rationally. Other researchers claim that this ignores the non-rational nature of the phenomenon and that acceptance of the theory requires belief in a subconscious level of thought processes, a belief that economic models tend to deny or ignore. Results from research using modern scanner data are mixed.\n\nNow that many customers are used to odd pricing, some restaurants and high-end retailers psychologically-price in even numbers in an attempt to reinforce their brand image of quality and sophistication.\n\nKaushik Basu used game theory in 1997 to argue that rational consumers value their own time and effort at calculation. Such consumers process the price from left to right and tend to mentally replace the last two digits of the price with an estimate of the mean \"cent component\" of all goods in the marketplace. In a sufficiently large marketplace, this implies that any individual seller can charge the largest possible \"cent component\" (99¢) without significantly affecting the average of cent components and without changing customer behavior. Ruffle and Shtudiner's (2006) laboratory test shows considerable support for Basu's 99-cent pricing equilibrium, particularly when other sellers' prices are observable.\n\nThe euro introduction in 2002, with its various exchange rates, distorted existing nominal price patterns while at the same time retaining real prices. A European wide study (el Sehity, Hoelzl and Kirchler, 2005) investigated consumer price digits before and after the euro introduction for price adjustments. The research showed a clear trend towards psychological pricing after the transition. Further, Benford's Law as a benchmark for the investigation of price digits was successfully introduced into the context of pricing. The importance of this benchmark for detecting irregularities in prices was demonstrated and with it a clear trend towards psychological pricing after the nominal shock of the euro introduction.\n\nAnother phenomenon noted by economists is that a price point for a product (such as $4.99) remains stable for a long period of time, with companies slowly reducing the \"quantity\" of product in the package until consumers begin to notice. At this time, the price will increase marginally (to $5.05) and then within an exceptionally short time will increase to the next price point ($5.99, for example).\n\nResearch has also found psychological pricing relevant for the study of politics and public policy. For instance, a study of Danish municipal income taxes found evidence of \"odd taxation\" as tax rates with a nine-ending were found to be over-represented compared to other end-decimals.\n\nSeveral studies have shown that when prices are presented to a prospect in descending order (versus ascending order) positive effects result, mainly a willingness to pay a higher price, higher perceptions of value, and higher probability of purchase. The reason for this is that when presented in the former, the higher price serves as a reference point, and the lower prices are perceived favorably as a result.\n\nExactly how psychological pricing came into common use is not clear, though it is known the practice arose during the late 19th century. One source speculates it originated in a newspaper pricing competition. Melville E. Stone founded the \"Chicago Daily News\" in 1875, intending to price it at one cent to compete with the nickel papers of the day. The story claims that pennies were not common currency at the time, and so Stone colluded with advertisers to set whole dollar prices a cent lower—thus guaranteeing that customers would receive ample pennies in change.\n\nOthers have suggested that fractional pricing was first adopted as a control on employee theft. For cash transactions with a round price, there is a chance that a dishonest cashier will pocket the bill rather than record the sale. For cash transactions with an odd price, the cashier must make change for the customer. This generally means opening the cash register which creates a record of the sale in the register and reduces the risk of the cashier stealing from the store owner.\n\nIn the former Czechoslovakia, people called this pricing \"baťovská cena\" (\"Baťa's price\"), referring to Tomáš Baťa, a Czech manufacturer of footwear. He began to widely use this practice in 1920.\n\nPrice ending has also been used by retailers to highlight sale or clearance items for administrative purposes. A retailer might end all regular prices in 95 and all sale price in 50. This makes it easy for a buyer to identify which items are discounted when looking at a report.\n\nIn its 2005 United Kingdom general election manifesto, the Official Monster Raving Loony Party proposed the introduction of a 99-pence coin to \"save on change\".\n\nA recent trend in some monetary systems is to eliminate the smallest denomination coin (typically 0.01 of the local currency). The total cost of purchased items is then rounded up or down to, for example, the nearest 0.05. This may have an effect on future \"odd-number\" pricing to maximize the rounding advantage for vendors by favoring 98 and 99 endings (rounded up) over 96 and 97 ending (rounded down) especially at small retail outlets where single item purchases are more common. Australia is a good example of this practice where 5 cents has been the smallest denomination coin since 1992, but pricing at .98 or .99 on items under several hundred dollars is still almost universally applied (e.g.: $1.99 – $299.99) while goods on sale often price at .94 and its variations. It is also the case in Finland and The Netherlands, the first two countries using the euro currency to eliminate the 1 and 2 cent coins.\n\n\n"}
{"id": "12115893", "url": "https://en.wikipedia.org/wiki?curid=12115893", "title": "Quality (philosophy)", "text": "Quality (philosophy)\n\nIn philosophy, a quality is an attribute or a property characteristic of an object. In contemporary philosophy the idea of qualities, and especially how to distinguish certain kinds of qualities from one another, remains controversial.\n\nAristotle analyzed qualities in his logical work, the Categories. To him, qualities are hylomorphically–formal attributes, such as \"white\" or \"grammatical\". Categories of \"state\", such as \"shod\" and \"armed\" are also non–essential qualities \"(katà symbebekós)\". Aristotle observed: \"one and the selfsame substance, while retaining its identity, is yet capable of admitting contrary qualities. The same individual person is at one time white, at another black, at one time warm, at another cold, at one time good, at another bad. This capacity is found nowhere else... it is the peculiar mark of substance that it should be capable of admitting contrary qualities; for it is by itself changing that it does so\". Aristotle described four types of qualitative opposites: \"correlatives,\" \"contraries,\" \"privatives\" and \"positives.\"\n\nJohn Locke presented a distinction between primary and secondary qualities in \"An Essay Concerning Human Understanding\". For Locke, a quality is an idea of a sensation or a perception. Locke further asserts that qualities can be divided in two kinds: primary and secondary qualities. Primary qualities are intrinsic to an object—a thing or a person—whereas secondary qualities are dependent on the interpretation of the subjective mode and the context of appearance. For example, a shadow is a secondary quality. It requires a certain lighting to be applied to an object. For another example, consider the mass of an object. Weight is a secondary quality since, as a measurement of gravitational force, it varies depending on the distance to, and mass of, very massive objects like the Earth, as described by Newton's law. It could be thought that mass is intrinsic to an object, and thus a primary quality. In the context of relativity, the idea of mass quantifying an amount of matter requires caution. The relativistic mass varies for variously traveling observers; then there is the idea of rest mass or invariant mass (the magnitude of the energy-momentum 4-vector), basically a system's relativistic mass in its own rest frame of reference. (Note, however, that Aristotle drew a distinction between qualification and quantification; a thing's quality can vary in degree). Only an isolated system's invariant mass in relativity is the same as observed in variously traveling observers' rest frames, and conserved in reactions; moreover, a system's heat, including the energy of its massless particles such as photons, contributes to the system's invariant mass (indeed, otherwise even an isolated system's invariant mass would not be conserved in reactions); even a cloud of photons traveling in different directions has, as a whole, a rest frame and a rest energy equivalent to invariant mass. Thus, to treat rest mass (and by that stroke, rest energy) as an intrinsic quality distinctive of physical matter raises the question of what is to count as physical matter. Little of the invariant mass of a hadron (for example a proton or a neutron) consists in the invariant masses of its component quarks (in a proton, around 1%) apart from their gluon particle fields; most of it consists in the quantum chromodynamics binding energy of the (massless) gluons (see Quark#Mass).\n\nPhilosophy and common sense tend to see qualities as related either to subjective feelings or to objective facts. The qualities of something depends on the criteria being applied to and, from a neutral point of view, do not determine its value (the philosophical value as well as economic value). Subjectively, something might be good because it is useful, because it is beautiful, or simply because it exists. Determining or finding qualities therefore involves understanding what is useful, what is beautiful and what exists. Commonly, \"quality\" can mean degree of excellence, as in, \"a quality product\" or \"work of average quality\". It can also refer to a property of something such as \"the addictive quality of nicotine\". In his book, \"Zen and the Art of Motorcycle Maintenance\", Robert M. Pirsig examines concepts of quality in classical and romantic, seeking a Metaphysics of Quality and a reconciliation of those views in terms of non-dualistic holism.\n"}
{"id": "1805824", "url": "https://en.wikipedia.org/wiki?curid=1805824", "title": "Rashomon effect", "text": "Rashomon effect\n\nThe Rashomon effect occurs when the same event is given contradictory interpretations by different individuals involved. The effect is named after Akira Kurosawa's 1950 film \"Rashomon\", in which a murder is described in four mutually contradictory ways by its four witnesses. More broadly, the term addresses the motivations, mechanism, and occurrences of the reporting on the circumstance, and so addresses contested interpretations of events, the existence of disagreements regarding the evidence of events, and the subjects of subjectivity versus objectivity in human perception, memory, and reporting.\n\nThe Rashomon effect has been defined in a modern academic context (from Robert Anderson, in 2016), as \"the naming of an epistemological framework—or ways of thinking, knowing, and remembering—required for understanding complex and ambiguous situations.\" The term for the effect is derived from the eponymous film, Kurosawa's \"Rashomon,\" in which a number of factors are at play, simultaneously, leading the same academic to comment:\n\nThere are varying claims made regarding the coining of this term, but a reliable statement of the earliest use of the term \"Rashomon effect\" comes from 2015 testimony of Robert Anderson, professor of communication at Simon Fraser University in Vancouver, British Columbia, Canada, who states that, The history of the term and its multiple permutations in cinema, literature, legal studies, psychology, sociology, history is the subject of a 2015 multi-author volume edited by Blair Davis (DePaul University), Anderson, and Jan Walls (Simon Fraser University).\n\nValerie Alia termed the same effect \"The Rashomon Principle,\" and has used this variant of the term extensively since the late 1970s, first publishing it in an essay on the politics of journalism in 1982. She further developed and used the term in a 1997 essay entitled \"The Rashomon Principle: The Journalist as Ethnographer\", and in her 2004 book, \"Media Ethics and Social Change\".\n\nA useful demonstration of this principle in scientific understanding can be found in Karl G. Heider's work on ethnography. Heider used the term to refer to the effect of the subjectivity of perception on recollection, by which observers of an event are able to produce substantially different but equally plausible accounts of it.\n\n"}
{"id": "1182927", "url": "https://en.wikipedia.org/wiki?curid=1182927", "title": "Social stratification", "text": "Social stratification\n\nSocial stratification is a kind of social differentiation whereby a society groups people into socioeconomic strata, based upon their occupation and income, wealth and social status, or derived power (social and political). As such, stratification is the relative social position of persons within a social group, category, geographic region, or social unit.\n\nIn modern Western societies, social stratification typically is distinguished as three social classes: (i) the upper class, (ii) the middle class, and (iii) the lower class; in turn, each class can be subdivided into strata, e.g. the upper-stratum, the middle-stratum, and the lower stratum. Moreover, a social stratum can be formed upon the bases of kinship, clan, tribe or caste, or all four.\n\nThe categorization of people by social strata occurs in all societies, ranging from the complex, state-based or polycentric societies to tribal and feudal societies, which are based upon socio-economic relations among classes of nobility and classes of peasants. Historically, whether or not hunter-gatherer societies can be defined as socially stratified or if social stratification began with agriculture and common acts of social exchange, remains a debated matter in the social sciences. Determining the structures of social stratification arises from inequalities of status among persons, therefore, the degree of social inequality determines a person's social stratum. Generally, the greater the social complexity of a society, the more social strata exist, by way of social differentiation.\n\nSocial stratification is a term used in the social sciences to describe the relative social position of persons in a given social group, category, geographical region or other social unit. It derives from the Latin \"strātum\" (plural \"strata\"; parallel, horizontal layers) referring to a given society’s categorization of its people into rankings of socioeconomic tiers based on factors like wealth, income, social status, occupation and power. In modern Western societies, stratification is often broadly classified into three major divisions of social class: upper class, middle class, and lower class. Each of these classes can be further subdivided into smaller classes (e.g. \"upper middle\"). Social strata may also be delineated on the basis of kinship ties or caste relations.\n\nThe concept of social stratification is often used and interpreted differently within specific theories. In sociology, for example, proponents of action theory have suggested that social stratification is commonly found in developed societies, wherein a dominance hierarchy may be necessary in order to maintain social order and provide a stable social structure. So-called conflict theories, such as Marxism, point to the inaccessibility of resources and lack of social mobility found in stratified societies. Many sociological theorists have criticized the extent to which the working classes are unlikely to advance socioeconomically while the wealthy tend to hold political power which they use to exploit the proletariat (laboring class). Talcott Parsons, an American sociologist, asserted that stability and social order are regulated, in part, by universal values. Such values are not identical with \"consensus\" but can as well be an impetus for ardent social conflict as it has been multiple times through history. Parsons never claimed that universal values, in and by themselves, \"satisfied\" the functional prerequisites of a society. Indeed, the constitution of society is a much more complicated codification of emerging historical factors. Theorists such as Ralf Dahrendorf alternately note the tendency toward an enlarged middle-class in modern Western societies due to the necessity of an educated workforce in technological economies. Various social and political perspectives concerning globalization, such as dependency theory, suggest that these effects are due to change in the status of workers to the third world.\n\nFour principles are posited to underlie social stratification. First, social stratification is socially defined as a property of a society rather than individuals in that society. Second, social stratification is reproduced from generation to generation. Third, social stratification is universal (found in every society) but variable (differs across time and place). Fourth, social stratification involves not just quantitative inequality but qualitative beliefs and attitudes about social status.\n\nAlthough stratification is not limited to complex societies, all complex societies exhibit features of stratification. In any complex society, the total stock of valued goods is distributed unequally, wherein the most privileged individuals and families enjoy a disproportionate share of income, power, and other valued resources. The term \"stratification system\" is sometimes used to refer to the complex social relationships and social structure that generate these observed inequalities. The key components of such systems are: (a) social-institutional processes that define certain types of goods as valuable and desirable, (b) the rules of allocation that distribute goods and resources across various positions in the division of labor (e.g., physician, farmer, ‘housewife’), and (c) the social mobility processes that link individuals to positions and thereby generate unequal control over valued resources.\n\nSocial mobility is the movement of individuals, social groups or categories of people between the layers or strata in a stratification system. This movement can be intragenerational (within a generation) or intergenerational (between two or more generations). Such mobility is sometimes used to classify different systems of social stratification. Open stratification systems are those that allow for mobility between strata, typically by placing value on the achieved status characteristics of individuals. Those societies having the highest levels of intragenerational mobility are considered to be the most open and malleable systems of stratification. Those systems in which there is little to no mobility, even on an intergenerational basis, are considered closed stratification systems. For example, in caste systems, all aspects of social status are ascribed, such that one's social position at birth is the position one holds for a lifetime.\n\nIn Marxist theory, the modern mode of production consists of two main economic parts: the base and the superstructure. The base encompasses the relations of production: employer–employee work conditions, the technical division of labour, and property relations. Social class, according to Marx, is determined by one's relationship to the means of production. There exist at least two classes in any class-based society: the owners of the means of production and those who sell their labor to the owners of the means of production. At times, Marx almost hints that the ruling classes seem to own the working class itself as they only have their own labor power ('wage labor') to offer the more powerful in order to survive. These relations fundamentally determine the ideas and philosophies of a society and additional classes may form as part of the superstructure. Through the ideology of the ruling class—throughout much of history, the land-owning aristocracy—false consciousness is promoted both through political and non-political institutions but also through the arts and other elements of culture. When the aristocracy falls, the bourgeoisie become the owners of the means of production in the capitalist system. Marx predicted the capitalist mode would eventually give way, through its own internal conflict, to revolutionary consciousness and the development of more egalitarian, more communist societies.\n\nMarx also described two other classes, the petite bourgeoisie and the lumpenproletariat. The petite bourgeoisie is like a small business class that never really accumulates enough profit to become part of the bourgeoisie, or even challenge their status. The lumpenproletariat is the underclass, those with little to no social status. This includes prostitutes, beggars, the homeless or other untouchables in a given society. Neither of these subclasses has much influence in Marx's two major classes, but it is helpful to know that Marx did recognize differences within the classes.\n\nAccording to Marvin Harris and Tim Ingold, Lewis Henry Morgan's accounts of egalitarian hunter-gatherers formed part of Karl Marx' and Friedrich Engels' inspiration for communism. Morgan spoke of a situation in which people living in the same community pooled their efforts and shared the rewards of those efforts fairly equally. He called this \"communism in living.\" But when Marx expanded on these ideas, he still emphasized an economically oriented culture, with property defining the fundamental relationships between people. Yet, issues of ownership and property are arguably less emphasized in hunter-gatherer societies. This, combined with the very different social and economic situations of hunter-gatherers may account for many of the difficulties encountered when implementing communism in industrialized states. As Ingold points out: \"The notion of communism, removed from the context of domesticity and harnessed to support a project of social engineering for large-scale, industrialized states with populations of millions, eventually came to mean something quite different from what Morgan had intended: namely, a principle of redistribution that would override all ties of a personal or familial nature, and cancel out their effects.\"\n\nThe counter-argument to Marxist's conflict theory is the theory of structural functionalism, argued by Kingsley Davis and Wilbert Moore, which states that social inequality places a vital role in the smooth operation of a society. The Davis–Moore hypothesis argues that a position does not bring power and prestige because it draws a high income; rather, it draws a high income because it is functionally important and the available personnel is for one reason or another scarce. Most high-income jobs are difficult and require a high level of education to perform, and their compensation is a motivator in society for people to strive to achieve more.\n\nMax Weber was strongly influenced by Marx's ideas but rejected the possibility of effective communism, arguing that it would require an even greater level of detrimental social control and bureaucratization than capitalist society. Moreover, Weber criticized the dialectical presumption of a proletariat revolt, maintaining it to be unlikely. Instead, he develops a three-component theory of stratification and the concept of life chances. Weber held there are more class divisions than Marx suggested, taking different concepts from both functionalist and Marxist theories to create his own system. He emphasizes the difference between class, status and power, and treats these as separate but related sources of power, each with different effects on social action. Working half a century later than Marx, Weber claims there to be four main social classes: the upper class, the white collar workers, the petite bourgeoisie, and the manual working class. Weber's theory more-closely resembles contemporary Western class structures, although economic status does not currently seem to depend strictly on earnings in the way Weber envisioned.\n\nWeber derives many of his key concepts on social stratification by examining the social structure of Germany. He notes that, contrary to Marx's theories, stratification is based on more than simple ownership of capital. Weber examines how many members of the aristocracy lacked economic wealth yet had strong political power. Many wealthy families lacked prestige and power, for example, because they were Jewish. Weber introduced three independent factors that form his theory of stratification hierarchy, which are; class, status, and power:\n\nC. Wright Mills, drawing from the theories of Vilfredo Pareto and Gaetano Mosca, contends that the imbalance of power in society derives from the complete absence of countervailing powers against corporate leaders of the Power elite. Mills both incorporated and revised Marxist ideas. While he shared Marx's recognition of a dominant wealthy and powerful class, Mills believed that the source for that power lay not only in the economic realm but also in the political and military arenas. During the 1950s, Mills stated that hardly anyone knew about the power elite's existence, some individuals (including the elite themselves) denied the idea of such a group, and other people vaguely believed that a small formation of a powerful elite existed. \"Some prominent individuals knew that Congress had permitted a handful of political leaders to make critical decisions about peace and war; and that two atomic bombs had been dropped on Japan in the name of the United States, but neither they nor anyone they knew had been consulted.\"\n\nMills explains that the power elite embody a privileged class whose members are able to recognize their high position within society. In order to maintain their highly exalted position within society, members of the power elite tend to marry one another, understand and accept one another, and also work together. The most crucial aspect of the power elite's existence lays within the core of education. \"Youthful upper-class members attend prominent preparatory schools, which not only open doors to such elite universities as Harvard, Yale, and Princeton but also to the universities' highly exclusive clubs. These memberships in turn pave the way to the prominent social clubs located in all major cities and serving as sites for important business contacts.\" Examples of elite members who attended prestigious universities and were members of highly exclusive clubs can be seen in George W. Bush and John Kerry. Both Bush and Kerry were members of the Skull and Bones club while attending Yale University. This club includes members of some of the most powerful men of the twentieth century, all of which are forbidden to tell others about the secrets of their exclusive club. Throughout the years, the Skull and Bones club has included presidents, cabinet officers, Supreme Court justices, spies, captains of industry, and often their sons and daughters join the exclusive club, creating a social and political network like none ever seen before.\n\nThe upper class individuals who receive elite educations typically have the essential background and contacts to enter into the three branches of the power elite: The political leadership, the military circle, and the corporate elite.\n\n\nMills shows that the power elite has an \"inner-core\" made up of individuals who are able to move from one position of institutional power to another; for example, a prominent military officer who becomes a political adviser or a powerful politician who becomes a corporate executive. \"These people have more knowledge and a greater breadth of interests than their colleagues. Prominent bankers and financiers, who Mills considered 'almost professional go-betweens of economic, political, and military affairs,' are also members of the elite's inner core.\n\nSome anthropologists dispute the \"universal\" nature of social stratification, holding that it is not the standard among all societies. John Gowdy (2006) writes, \"Assumptions about human behaviour that members of market societies believe to be universal, that humans are naturally competitive and acquisitive, and that social stratification is natural, do not apply to many hunter-gatherer peoples. Non-stratified egalitarian or acephalous (\"headless\") societies exist which have little or no concept of social hierarchy, political or economic status, class, or even permanent leadership.\n\nAnthropologists identify egalitarian cultures as \"kinship-oriented,\" because they appear to value social harmony more than wealth or status. These cultures are contrasted with economically oriented cultures (including states) in which status and material wealth are prized, and stratification, competition, and conflict are common. Kinship-oriented cultures actively work to prevent social hierarchies from developing because they believe that such stratification could lead to conflict and instability. Reciprocal altruism is one process by which this is accomplished.\n\nA good example is given by Richard Borshay Lee in his account of the Khoisan, who practice \"insulting the meat.\" Whenever a hunter makes a kill, he is ceaselessly teased and ridiculed (in a friendly, joking fashion) to prevent him from becoming too proud or egotistical. The meat itself is then distributed evenly among the entire social group, rather than kept by the hunter. The level of teasing is proportional to the size of the kill. Lee found this out when he purchased an entire cow as a gift for the group he was living with, and was teased for weeks afterward about it (since obtaining that much meat could be interpreted as showing off).\n\nAnother example is the Indigenous Australians of Groote Eylandt and Bickerton Island, off the coast of Arnhem Land, who have arranged their entire society—spiritually and economically—around a kind of gift economy called \"renunciation.\" According to David H. Turner, in this arrangement, every person is expected to give \"everything\" of any resource they have to any other person who needs or lacks it at the time. This has the benefit of largely eliminating social problems like theft and relative poverty. However, misunderstandings obviously arise when attempting to reconcile Aboriginal \"renunciative economics\" with the competition/scarcity-oriented economics introduced to Australia by Anglo-European colonists.\n\nThe social status variables underlying social stratification are based in social perceptions and attitudes about various characteristics of persons and peoples. While many such variables cut across time and place, the relative weight placed on each variable and specific combinations of these variables will differ from place to place over time. One task of research is to identify accurate mathematical models that explain how these many variables combine to produce stratification in a given society. Grusky (2011) provides a good overview of the historical development of sociological theories of social stratification and a summary of contemporary theories and research in this field. While many of the variables that contribute to an understanding of social stratification have long been identified, models of these variables and their role in constituting social stratification are still an active topic of theory and research. In general, sociologists recognize that there are no \"pure\" economic variables, as social factors are integral to economic value. However, the variables posited to affect social stratification can be loosely divided into economic and other social factors.\n\nStrictly quantitative economic variables are more useful to describing social stratification than explaining how social stratification is constituted or maintained. Income is the most common variable used to describe stratification and associated economic inequality in a society. However, the distribution of individual or household accumulation of surplus and wealth tells us more about variation in individual well-being than does income, alone. Wealth variables can also more vividly illustrate salient variations in the well-being of groups in stratified societies. Gross Domestic Product (GDP), especially \"per capita\" GDP, is sometimes used to describe economic inequality and stratification at the international or global level.\n\nSocial variables, both quantitative and qualitative, typically provide the most explanatory power in causal research regarding social stratification, either as independent variables or as intervening variables. Three important social variables include gender, race, and ethnicity, which, at the least, have an intervening effect on social status and stratification in most places throughout the world. Additional variables include those that describe other ascribed and achieved characteristics such as occupation and skill levels, age, education level, education level of parents, and geographic area. Some of these variables may have both causal and intervening effects on social status and stratification. For example, absolute age may cause a low income if one is too young or too old to perform productive work. The social perception of age and its role in the workplace, which may lead to ageism, typically has an intervening effect on employment and income.\n\nSocial scientists are sometimes interested in quantifying the degree of economic stratification between different social categories, such as men and women, or workers with different levels of education. An index of stratification has been recently proposed by Zhou for this purpose.\n\nGender is one of the most pervasive and prevalent social characteristics which people use to make social distinctions between individuals. Gender distinctions are found in economic-, kinship- and caste-based stratification systems. Social role expectations often form along sex and gender lines. Entire societies may be classified by social scientists according to the rights and privileges afforded to men or women, especially those associated with ownership and inheritance of property. In patriarchal societies, such rights and privileges are normatively granted to men over women; in matriarchal societies, the opposite holds true. Sex- and gender-based division of labor is historically found in the annals of most societies and such divisions increased with the advent of industrialization. Sex-based wage discrimination exists in some societies such that men, typically, receive higher wages than women for the same type of work. Other differences in employment between men and women lead to an overall gender-based pay-gap in many societies, where women as a category earn less than men due to the types of jobs which women are offered and take, as well as to differences in the number of hours worked by women. These and other gender-related values affect the distribution of income, wealth, and property in a given social order.\n\nRacism consists of both prejudice and discrimination based in social perceptions of observable biological differences between peoples. It often takes the form of social actions, practices or beliefs, or political systems in which different races are perceived to be ranked as inherently superior or inferior to each other, based on presumed shared inheritable traits, abilities, or qualities. In a given society, those who share racial characteristics socially perceived as undesirable are typically under-represented in positions of social power, i.e., they become a minority category in that society. Minority members in such a society are often subjected to discriminatory actions resulting from majority policies, including assimilation, exclusion, oppression, expulsion, and extermination. Overt racism usually feeds directly into a stratification system through its effect on social status. For example, members associated with a particular race may be assigned a slave status, a form of oppression in which the majority refuses to grant basic rights to a minority that are granted to other members of the society. More covert racism, such as that which many scholars posit is practiced in more contemporary societies, is socially hidden and less easily detectable. Covert racism often feeds into stratification systems as an intervening variable affecting income, educational opportunities, and housing. Both overt and covert racism can take the form of structural inequality in a society in which racism has become institutionalized.\n\nEthnic prejudice and discrimination operate much the same as do racial prejudice and discrimination in society. In fact, only recently have scholars begun to differentiate race and ethnicity; historically, the two were considered to be identical or closely related. With the scientific development of genetics and the human genome as fields of study, most scholars now recognize that race is socially defined on the basis of biologically determined characteristics that can be observed within a society while ethnicity is defined on the basis of culturally learned behavior. Ethnic identification can include shared cultural heritage such as language and dialect, symbolic systems, religion, mythology and cuisine. As with race, ethnic categories of persons may be socially defined as minority categories whose members are under-represented in positions of social power. As such, ethnic categories of persons can be subject to the same types of majority policies. Whether ethnicity feeds into a stratification system as a direct, causal factor or as an intervening variable may depend on the level of ethnocentrism within each of the various ethnic populations in a society, the amount of conflict over scarce resources, and the relative social power held within each ethnic category.\n\nThe world and the pace of social change today are very different than in the time of Karl Marx, Max Weber, or even C. Wright Mills. Globalizing forces lead to rapid international integration arising from the interchange of world views, products, ideas, and other aspects of culture. Advances in transportation and telecommunications infrastructure, including the rise of the telegraph and its posterity the Internet, are major factors in globalization, generating further interdependence of economic and cultural activities.\n\nLike a stratified class system within a nation, looking at the world economy one can see class positions in the unequal distribution of capital and other resources between nations. Rather than having separate national economies, nations are considered as participating in this world economy. The world economy manifests a global division of labor with three overarching classes: core countries, semi-periphery countries and periphery countries, according to World-systems and Dependency theories. Core nations primarily own and control the major means of production in the world and perform the higher-level production tasks and provide international financial services. Periphery nations own very little of the world's means of production (even when factories are located in periphery nations) and provide low to non-skilled labor. Semiperipheral nations are midway between the core and periphery. They tend to be countries moving towards industrialization and more diversified economies. Core nations receive the greatest share of surplus production, and periphery nations receive the least. Furthermore, core nations are usually able to purchase raw materials and other goods from noncore nations at low prices, while demanding higher prices for their exports to noncore nations. A global workforce employed through a system of global labor arbitrage ensures that companies in core countries can utilize the cheapest semi-and non-skilled labor for production.\n\nToday we have the means to gather and analyze data from economies across the globe. Although many societies worldwide have made great strides toward more equality between differing geographic regions, in terms of the standard of living and life chances afforded to their peoples, we still find large gaps between the wealthiest and the poorest within a nation and between the wealthiest and poorest nations of the world. A January 2014 Oxfam report indicates that the 85 wealthiest individuals in the world have a combined wealth equal to that of the bottom 50% of the world's population, or about 3.5 billion people. By contrast, for 2012, the World Bank reports that 21 percent of people worldwide, around 1.5 billion, live in extreme poverty, at or below $1.25 a day. Zygmunt Bauman has provocatively observed that the rise of the rich is linked to their capacity to lead highly mobile lives: 'Mobility climbs to the rank of the uppermost among coveted values -and the freedom to move, perpetually a scarce and unequally distributed commodity, fast becomes the main stratifying factor of our late modern or postmodern time.' \n"}
{"id": "5608413", "url": "https://en.wikipedia.org/wiki?curid=5608413", "title": "Sociology of terrorism", "text": "Sociology of terrorism\n\nSociology of terrorism is an emerging field in sociology seeking to understand terrorism as a social phenomenon and how individuals as well as states respond to such events. It is not to be confused with critical terrorism studies which sometimes overlaps with the psychology of terrorism.\n\nAfter the September 11 attacks, there has been a spike of interest in various sociological traditions related to terrorism, such as moral panic, organizational response and media coverage, and counter-terrorism.\n\nThe most comprehensive study into the definition of terrorism comes from a study by Weinberg, Pedahzur and Hirsch-Hoefler (2004) who examined 73 definitions of terrorism from 55 articles and concluded that terrorism is: \"a politically motivated tactic involving the threat or use of force or violence in which the pursuit of publicity plays a significant role.\" However, Weinberg \"et al.\" point out that definitions of terrorism often ignore symbolic aspects of terrorism. Due to its focus on symbolism, sociology has a unique vantage point from which to assess terror.\n\nSince the September 11 attacks, Mathieu Deflem (University of South Carolina), S.E. Costanza (Central Connecticut State University) and John C. Kilburn Jr. (Texas A&M International University) are among the sociologists to call for the development of a sub-field in sociology related to terrorism. Common topics that are part of the discourse in the sociology of terrorism include: military spending, counter-terrorism, immigration, privacy issues, and the Israeli–Palestinian conflict, where within these contexts questions of power, the definition of terrorism, propaganda, nationality, the media, etc. are being deliberated upon.\n\nEarly peer-reviewed literature after the September 11 attacks examined policing and citizen responses to terror during the September 11 attacks. It also examined interactions between first responders (police, rescue teams, etc.) and communities. Ramirez, Hoopes and Quinlan (2003) rightly predicted that police organizations would change fundamental styles of profiling people and police agencies would alter their mission statements after the September 11 attacks. There is strong reason to believe that even the smallest of local police agencies are apt to feel some kind of pressure to deal with the issue of terrorism.\n\nMore recent work in the sociology of terrorism field is philosophical and reflective and has focused on issues such as moral panic and over-spending after the September 11 attacks. Costanza and Kilburn (2005), in an article entitled \"Symbolic Security, Moral Panic and Public Sentiment: Toward a sociology of Counterterrorism\" argued that the issue of symbolism is of much importance to understanding the war on terror. Using a classic symbolic interactionist perspective, they argue that strong public sentiment about the homeland security issue has driven policy more to superficial threats than real and concrete threats. Others argue that symbolism has led to agency a policy of “hypervigilance” in agency decision-making that is costly and untestable.\n\nSome sociologists and legal scholars have contemplated the potential consequences of aggressive (or militaristic) policing of terror threats that may have negative implications for human rights which are of great interest to sociologists as a matter of social justice. For instance, in a peer-reviewed article \"Crouching tiger or phantom dragon? Examining the discourse on global cyber-terror\", Helms, Costanza and Johnson (2011) ask if it is possible that media hype at the national level could prompt an unnecessary and systemic over-pursuit of cyberterrorism. They warn that such overreaction might lead to a \"killswitch\" policy which could give the federal government ultimate power over the internet.\n\nDespite the quantitative lean of modern sociology; Kilburn, Costanza, Borgeson and Metchik (2011) point out that there are several methodological barbs to effectively and scientifically assessing the effect of homeland security measures. In traditional criminology, the most quantitatively amenable starting point for measuring the effectiveness of any policing strategy (i.e., neighborhood watch, gun control, foot patrols, etc.) is to assess total financial costs against clearance rates or arrest rates. Since terrorism is such a rare event phenomena, measuring arrests would be a naive way to test policy effectiveness.\n\nAnother methodological problem in the development of sociology of terrorism as a sub-field is one of finding operational measures for key concepts in the study of homeland security. Both terrorism and homeland security are relatively new concepts for social scientists, and academicians have yet to agree on the matter of how to properly conceptualize these ideas.\n\nFunctionalism is “the theory that various social institutions and processes in society exists to serve some important (or necessary) function to keep society running.” This sociological perspective draws on the work of sociologists like Émile Durkheim, and gets its name from the idea that the best way to study society is to identify the roles that different aspects of society play. Social deviance, loosely understood, can be taken to mean any \"transgression of socially established norms.\" This can range from the minor–slamming a door in someone's face–to the major–a terrorist act. Thus terrorism is a deviant behavior. Functionalism sees terrorism–which is a form of crime–as a temporary deviation from the normal goings on of society, and is in a way functional to society.\n\nA sociologist that utilizes structural functionalism would explain the existence of any social phenomena by the function they perform. Therefore, terrorism is functional because it joins individuals together in opposition, and brings a sense of belonging to the group opposing it. This feeling of group solidarity would help prevent \"anomie\", which is the stage where people do not need to follow any norms of society in order to survive in society.\n\nTerrorists, like other criminals, become what is known as a reference point; individuals use a reference point as a standard for evaluation. The norms and rules of society become clearer, and are seen as necessary, in comparison to terrorism. In order to protect the \"status quo\", society uses terrorism as a way to reassert the importance of social norms in the lives of individuals. Thus individuals see terrorism as a threat to the social equilibrium and their life in a functioning society. Functionalists believe that social change is required to keep a healthy society. Slow, well-planned, and evolutionary method-types change a healthy society socially. These social changes often come about from a drastic need for change and are preceded by a social shock. Terrorism brings about a social shock that moves society towards a change in direction that enables it to find new ways in which to protect itself. Functionalists view these new changes as providing society with a healthy and slow-paced social change that was needed. Terrorism thus becomes an expected and needed shock, and therefore can be seen in a sense to encourage society to change for the better.\n\nConflict theory is “the idea that conflict between competing interests is the basic, animating force of social change and society in general.\" A conflict theorist generally sees that the control of conflict equals the ability of one group to suppress the group that they are opposing, and that civil law is a technique of defining and maintaining a social order that benefits some at the expense of others.\n\nConflict theorists view terrorism as a reaction to injustice, which is probably created in the minds of terrorists due to misguidance, illiteracy, or unrealistic goals, and that violent behaviors expressed by terrorist organizations are the result of individual frustration, aggression or showing a readiness to fight. Political conflict makes people look for ways to explain and solve the problems they are facing. If the conflict is deeply rooted, and the current ideology proves unable to deal with the problems, people begin to turn to other ideologies that can often carry a religious theme to them. This is not to say that all forms of terrorist acts are committed by people that are religious. In 83% of the suicide attackers worldwide, between 1980 and 2003, only 43% were identifiably religious.\n\nTerrorists use violence because they believe that if they did not use violence they would lose a power struggle, which lead many conflict theorists to view it as a weapon of the weak. In Iraq, between March 2003 and February 2006, 443 suicide missions took place with 71% belonging to al-Qaeda. They justified their actions in religious terms; viewing the Shi'a control of Iraq as abandoning religious principles. Suicide attacks against the Iraqi regime and its American and British supporters were seen as the means in which to accomplish this. Yet it was only under certain political conditions that suicide bombings spiked. The first condition being that it was in relation with the counterinsurgency of the American and British militaries. The second being a strategic response to the Shi'a control becoming more stabilized in Iraq. Terrorists do not have the money or the political power that is needed to wage war, so they use terrorism as a means, not a goal, to agitate the government in order to achieve their political objectives. Before committing an act of terror, a terrorist does not always weigh the cost and benefits of their actions, but rather is reacting from the humiliation and frustration they feel they are being subjected too.\n\nSymbolic interactionism is “a micro-level theory in which shared meanings, originations, and assumptions form the basic motivations behind peoples’ actions.\" In symbolic interactionism, face-to-face interaction creates the social world. Individuals act on perceived meanings that appear to be self-constituting. Group membership is one of the major determinations if individual interpretations of reality, which enables symbolic interactionism to explain crime, and thus terrorism.\n\nDeviance, which terrorism falls under, can be explained by labeling theory. Labeling theory is “the belief that individuals subconsciously notice how others see or label them, and their reactions to those labels, over time, form the basis of their self-identity.\" Social groups create rules about what is acceptable behavior for people in society. When a rule is broken, society determines if the act was deviant. A person can only become deviant after a social reaction to an act committed is labeled deviant, and that original act is referred to as the primary deviance. Being labeled deviant causes a person to see themselves as deviants, which leads to said person performing more deviant acts, with each act being referred to as secondary deviance. Secondary deviance can quickly turn into a stigma, which is a label that changes the way people see someone, and how individuals view themselves According to symbolic interactionism, terrorism is treated as learned behaviors. Each person learns how to commit terrorism through interactions with terrorists. Involvement in the group is important in the learning process, and members, upon joining, are resocialized to the group’s version of reality. The best way to accomplish this is to involve new members in terrorist acts, which leads the terrorist organization to become the only reference point for its members.\n\nSocial learning theory plays a part in the socialization of terroristic behaviors. Learning theory states that a person becomes deviant because of an abundance of definitions that favor deviant behavior versus definitions that are unfavorable to such behaviors. This theory is broken down into four learning mechanisms: differential association, definitions, differential reinforcement, and imitation.\n\nThe first learning mechanism is differential association, which refers to \"direct association and interaction with others who engage in certain kinds of behaviors or express norms, values, and attitudes supportive of such behavior, as well as indirect association and identification with more distant reference groups.\" The groups that an individual are differentially associated with provides the context in which the social learning is operated. The greater the priority, intensity, duration, and frequency of the differential association the greater the effect on behavior. Hence, the theory in relation to terrorism is that the stronger someone's connection is towards a terrorist organization the better chance that person has of also exhibiting terroristic behaviors.\n\nThe second learning mechanism is definitions. Definitions refer to an \"individual's own value and belief system about what is and is not acceptable behavior.\" These values are learned and reinforced through differential association. There are two types of definitions, general definition and specific definition. General definitions include broad beliefs about conformity that are influenced through conventional means and are often influenced by religious or moral values. Specific definitions are seen as those that align an individual with particular acts of crime. The greater the number of definitions the more likely a person will engage in criminal behavior. So the more definitions an individual has that favor terroristic behavior the greater chance that person has of committing a terroristic acts.\n\nThe third learning mechanism is differential reinforcement. Differential reinforcement \"refers to the balance of anticipated or actual rewards and punishments that follow behavior.\" An individual refraining from committing a crime depends on a balance of past, present, and anticipated future rewards or punishments for their actions. In regards to terrorism, the more direct or indirect social interaction a person has towards terrorism the more likely they are to commit a terroristic act.\n\nThe fourth and final learning mechanism is imitation. \"Imitation is the notion that individuals engage in behaviors that they have previously witnessed others doing.\" The characters being observed, the behaviors that are being witnessed, and the consequences for those behaviors determine how much an individual imitates a behavior. All of these things need to fall into place in order for an individual to imitate a terrorist.\n\n\n"}
{"id": "4311578", "url": "https://en.wikipedia.org/wiki?curid=4311578", "title": "Spam poetry", "text": "Spam poetry\n\nSpam poetry, sometimes called spoetry, is poetic verse composed primarily from the subject lines or content of spam e-mail messages.\n\nSeveral writers have claimed to have created spam poetry, and consensus has not emerged about a single origin. Some early examples come from a spam poetry competition held in 2000 by the website \"Satire Wire\". Animator Don Hertzfeldt began writing spam poems in his production journal in 2004. Translator Jorge Candeias wrote Portuguese \"spoems\" daily, between 5 May 2003 and 5 May 2004, using spam subject lines as title and inspiration.\n\nA book entitled \"Machine Language\" by endwar was published in 2005 by IZEN and was followed by \"Machine Language, Version 2.1\" also by endwar in 2006. The latter edition includes a CD of spoetry read by Microsoft Sam and set to ambient musical sounds by Michael Truman who also tweaked the automated readings. Each edition indicates endwar as editor, but in the second edition he has admitted to using cut-up technique and having combined shorter pieces from the first edition, which lends more authorship to him in his creation. These pieces were also read at the opening of \"Blends & Bridges\", a concrete and visual poetry show at Gallery 324 in Cleveland, Ohio, on April 1, 2006, by endwar with the sounds by Truman backing. The experimental poet, endwar, cites his own collaboration with Ficus Strangulensis, the experimental poet, in 1995, \"The Further Last Words of Dutch Schultz\" published by IZEN as an earlier experiment in generating random text poetry, in this case, the software altered text of the bizarre last words of Dutch Schultz as published in The New York Times in 1935, as well as the cut-up influences of Brion Gysin and William S. Burroughs and even the musicians Throbbing Gristle and David Bowie. The useful bits of spam for purposes of endwar's variation of spoetry, i.e., the part that is not the advertisement, but the random words assigned to trick spam filters, endwar calls \"paratext\". Endwar indicates that, in his view, \"the effect of the evolution of paratext is that computers are learning to talk to each other by in some sense imitating human texts.\" The advertisements are the human-to-human conversation in the same email sources.\n\nA book entitled 'Spam: E-mail Inspired Poems' by Ben Myers was published in 2008 by Blackheath Books Myers claims to have been writing spam poems since 1999.\n\nThe creation of spoetry is similar to Gysin and Burroughs's cut-up technique in that individual subject lines of messages are pieced together in poetic form; making the creation of spoetry an exercise not in creativity as much as in having an eye for the unexpected.\n\nThe end result can be crafted into any literary form the author desires: haiku, concrete poetry, limerick, dada, and so on. Thus, spoetry is not a literary form but rather a means of creating poetry.\n\nA related concept is spam lit, where snippets of nonsensical verse and prose are embedded in spam e-mail messages. Some of the snippets are original content, others are passages or conglomerations from public-domain works. The term was coined by a member of the Poetics listserv in 2002. In August 2006, David Kestenbaum of NPR's \"Morning Edition\" broadcast a story on \"Literary Spam\". Kestenbaum notes that Paul Graham, a programmer, \"wrote a program to find out how to best separate spam from real e-mail. To train it, he fed it a good helping of spam and a separate sample of real e-mail.\" Soon, spammers discovered the works of long-dead poets and writers as one way to circumvent Graham's anti-spam code.\n\n\n"}
{"id": "382334", "url": "https://en.wikipedia.org/wiki?curid=382334", "title": "Speakers' Corner", "text": "Speakers' Corner\n\nA Speakers' Corner is an area where open-air public speaking, debate, and discussion are allowed. The original and most noted is in the northeast corner of Hyde Park in London, England. Historically there were a number of other areas designated as Speakers' Corners in other parks in London (e.g., Lincoln's Inn Fields, Finsbury Park, Clapham Common, Kennington Park, and Victoria Park). More recently, they have been set up in other British cities, and there are also Speakers' Corners in other countries.\n\nSpeakers here may talk on any subject, as long as the police consider their speeches lawful, although this right is not restricted to Speakers' Corner only. Contrary to popular belief, there is no immunity from the law, nor are any subjects proscribed, but in practice the police intervene only when they receive a complaint. On some occasions in the past, they have intervened on grounds of profanity. \n\nThough Hyde Park Speakers' Corner is considered the paved area closest to Marble Arch, legally the public speaking area extends beyond the Reform Tree and covers a large area from Marble Arch to Victoria Gate, then along the Serpentine to Hyde Park Corner and the Broad Walk running from Hyde Park Corner to Marble Arch.\n\nPublic riots broke out in the park in 1855, in protest over the Sunday Trading Bill, which forbade buying and selling on a Sunday, the only day working people had off. The riots were (rather sanguinely) described by Karl Marx as the beginning of the English revolution.\n\nThe Chartist movement used Hyde Park as a point of assembly for workers' protests, but no permanent speaking location was established. The Reform League organised a massive demonstration in 1866 and then again in 1867, which compelled the government to extend the franchise to include most working-class men.\n\nThe riots and agitation for democratic reform encouraged some to force the issue of the \"right to speak\" in Hyde Park. The Parks Regulation Act 1872 delegated the issue of permitting public meetings to the park authorities (rather than central government). Contrary to popular belief, it does not confer a statutory basis for the right to speak at Speakers' Corner. Parliamentary debates on the Act illustrate that a general principle of being able to meet and speak was not the intention, but that some areas would be permitted to be used for that purpose.\n\nSince that time, it has become a traditional site for public speeches and debate, as well as a major site of protest and assembly in Britain. There are some who contend that the tradition has a connection with the Tyburn gallows, where the condemned man was allowed to speak before being hanged.\n\nSpeakers' Corner is often held up to demonstrate freedom of speech, as anyone can turn up unannounced and talk on almost any subject, although always at the risk of being heckled by regulars. The corner was frequented by Karl Marx, Vladimir Lenin, George Orwell, C. L. R. James, Walter Rodney, Ben Tillett, Marcus Garvey, Kwame Nkrumah, and William Morris. Lord Justice Sedley, in \"Redmond-Bate v Director of Public Prosecutions\" (1999), described Speakers' Corner as demonstrating \"the tolerance which is both extended by the law to opinion of every kind and expected by the law in the conduct of those who disagree, even strongly, with what they hear.\" The ruling famously established in English case law that freedom of speech could not be limited to the inoffensive but extended also to \"the irritating, the contentious, the eccentric, the heretical, the unwelcome, and the provocative, as long as such speech did not tend to provoke violence\", and that the right to free speech accorded by Article 10 of the European Convention of Human Rights also accorded the right to be offensive. Prior to the ruling, prohibited speech at Speakers' Corner included obscenity, blasphemy, insulting the queen, or inciting a breach of the peace. \n\nIn the late 19th century, for instance, a combination of park by-laws, use of the Highways Acts and use of venue licensing powers of the London County Council made it one of the few places where socialist speakers could meet and debate.\n\nThe following organisations and individuals, listed here in chronological order, have (had) a well-established history of speaking regularly in Hyde Park.\n\nThe first official Speakers' Corner outside London opened in Nottingham in 2009. It was officially inaugurated by Jack Straw, the UK Justice Secretary, on 22 February 2009. The designated space occupied a new landscaped extension to the city's Old Market Square, at the junction of King Street and Queen Street. The large paved space includes the new statue of Brian Clough, the former manager of Derby County and Nottingham Forest, who forged ties between the two cities which were famous for local rivalry.\n\nSpeakers Corner Lichfield was launched in May 2009, with the help of the Speakers’ Corner Trust, to much applause. Hundreds of people joined in the celebrations which featured over 30 speeches, musical and dance performances, as well as star appearances from BBC’s Jo Malin and former Coronation Street Star Chris Walker.\n\nToday Speakers’ Corner is a site for events and activities, as well as a destination for all the community to visit and have their say on topics close to their hearts.\n\nSince the launch, a plaque has been unveiled at the site, along with a code of conduct. Plans for the site include a stone plaque marking the spot, as well as a series of annual events.\n\nThe Sussex coastal town of Worthing has had its own Speakers' Corner at Splash Point on the town's seafront since the Victorian era. A sign today marks the \"stand for delivering sermons and public speeches\", while another sign close by marks the site by the old Fish Market where the Salvation Army has preached the Gospel since 1886. The Speakers' Corner fell into disuse in the late 20th century and is now being reinstated. As part of the Government's \"Sea Change\" programme, being run by the Commission for Architecture and the Built Environment, the area will benefit from a £500,000 grant to re-landscape the area around Splash Point and see a revival of the Speakers' Corner. Speakers' corner comprises a dais accessible by steps and a ramp providing a platform from which speakers address the crowd or passers by. It is due for completion by October 2010.\n\nLeeds is known to have its own Speakers' Corner, at Victoria Gardens on the Headrow, in front of the Leeds City Art Gallery, Central Library and Henry Moore Sculpture Centre building. It is a pivotal point in Leeds for justice and anti-war marches, most of which gather and terminate here, as well as for war memorial services due to the location of Leeds's Municipal Cenotaph.\n\nThe stepped base of Grey's Monument is used as a stage by assorted musicians, preachers and activists. Verbal clashes between left-wing and right-wing groups are frequent.\n\nThere is a Speakers' Corner in the Domain in Sydney, established in 1878. The speakers talk every Sunday afternoon from 2 pm until 5 pm, and have a website. Official outdoor \"free\" speech first appeared in the hustings and hanging grounds of Hyde Park Sydney in 1874. Free speech in this form was banned following a serious riot between Catholics and Orangemen. However, following the formalisation of free speech in Speakers' Corner in London it was decided in 1878 that The Domain would be the place for free speech in Sydney.\n\nIn 'Diary of a Voyage to Australia, New Zealand and other lands' (published 1896), Robert Roberts notes that \"On the west side [of a particular location] is a feature peculiar to Sydney in all the world - a preaching park. There are of course, parks in other cities where open-air spouting is practiced on Sundays, such as Hyde Park, in London : but there is no city in the world where a park on such a scale is used by all classes of religious people. It is a wooded enclosure, like a nobleman's park in England, kept in capital order, both as regards the turf under foot, and the tall and noble trees that give shelter overhead from the sun.\" \"All the sects and denominations use it. There is none of the sense of infra dig that associates itself with out-door preaching in England.\"\"Every denomination has its own tree.\" \"The various religious bodies hold their meetings sufficiently apart to make no interference one with the other. It is a sort of weekly babel of religious tongues - recognised and patronised by the whole community\"\n\nOther Speakers' Corners are found outside Parliament House, in King George Square, and at The Powerhouse in Brisbane. In Melbourne, Speakers' Corner was originally held in Birrarung Marr where the original site is still visible. This site has lost some popularity over the years and Speakers' Corner (Now called \"Speakers' Forum\") is currently held outside the State Library of Victoria on Sunday afternoon from 3 pm.\n\nDedicated by the Earl Mountbatten on 12 April 1966, Speakers' Corner in Regina, Saskatchewan is located on the north shore of Wascana Lake. It serves as a constant reminder of the notion of free speech and assembly and a tribute to Saskatchewan people who have upheld that heritage. The two lanterns framing the south entrance to the main plaza formed part of the Cumberland Screen at the entrance to Speakers' Corner in London. The podia on the main plaza are from the exterior columns of the Old City Hall (1908–1965) and symbolise free speech in democracy at the municipal level of government. Six paper birch trees were taken from Runnymede Meadow in Windsor Great Park, near Windsor Castle. It was there that King John signed Magna Carta on 15 June 1215. The ten gas lamps surrounding the corner come from King Charles Street which runs from Whitehall to St. James Park, London, near the Houses of Parliament. They were erected in 1908 during the reign of Edward VII, whose royal cypher E.R. VII appears on the base of each lamp.\n\nKitchener, Ontario has a small area designated as Speakers' Corner on the northwest corner of King and Frederick Streets. It existed already since the mid-1980s.\n\nMass demonstration and speeches are traditionally held on the Hotel Indonesia roundabout Selamat Datang Monument. This venue however, is located the middle of Jakarta's main boulevard and commercial district, thus causing a severe traffic jam when a rally is held. To accommodate this, Jakarta's provincial government built a small park on the northwestern corner of the Merdeka Square, across the Istana Merdeka, Indonesia's presidential palace. Officially named 'Taman Pandang Istana' (Palace-View Park), this park is known commonly as 'Taman Unjuk Rasa' (Demonstration Park).\n\nAs a tribute to democracy and freedom of speech, in Lajatico, Pisa there a small area designated as Speakers' Corner (\"L'angolo del parlatore\") on a corner of the Vittorio Veneto main square. It is opened for the public to speak on Sundays (9 to 11 a.m. and 4 to 6 p.m.). The first speaker was the mayor Alessio Barbarfieri, who highlighted the importance of the acts of speaking and listening for a good and effective local governance.\n\nThe first Speakers' Square in Malaysia was established at the Esplanade, George Town, Penang on 4 May 2010. It is opened for the public to speak on Wednesday and Sunday (6.00 pm to 10.00 pm). The first speaker was Tan Seng Hai who shared his views on preventing Ascot Sports Sdn. Bhd. from conducting betting activities in the Penang state.\n\nConditions for use of Speakers' Square\n\nIn the Netherlands, there is a permanently designated speakers' corner called the Spreeksteen in Amsterdam. Lawfully, every person has the freedom of speech as a matter of right. The 'Spreeksteen' is open for free speech 24-hours a day, and was established to allow complete free speech. The 'Spreeksteen' has been located in the Oosterpark in Amsterdam since 5 May 2005, and has been erected by a citizens action after the brutal murder of film-maker and columnist Theo van Gogh. Plans for bringing the Amsterdam Speakers' Corner online with a permanent camera and microphone are in a phase of installation. In the meantime the speakers are filmed with a hand-held camera.\n\nThe Spreeksteen was involved in controversy when they allowed Michiel Smit, a far-right activist, to speak on 1 October 2006. Antifascist demonstrators used noise to prevent Smit from being heard (as happens often when there is a public demonstration of the far-right).\n\nThere is a Speakers' Corner in Albert Park in Auckland at Princes Street, opposite to the University of Auckland.\n\nThe Speakers' Corner in Singapore was opened on 1 September 2000, to allow Singapore citizens to speak freely. They are exempted from the need to obtain a police permit as long as they meet the terms and conditions of use.\n\nThe Speakers' Corner is located in Hong Lim Park, a popular venue for many election rallies and political speeches in the 1950s and '60s. Hong Lim Park is centrally located, well-served by public transport and is sited in a high public density area.\n\nIn 2004, public exhibitions and performances were added to the list of exempted activities at the Speakers' Corner.\n\nFrom 1 September 2008, Singapore citizens can also organise or participate in demonstrations at Speakers' Corner without having to obtain a police permit. With this latest change in policy to allow the venue to be used freely as an outdoor demonstration site, coupled with the liberalisation on the use of sound amplification and the extension of operating hours of the venue, the Speakers' Corner aims to address the genuine desire by some Singaporeans for lawful outdoor demonstrations and processions as a means of political expression.\n\nSingapore citizens who wish to hold a speech, exhibition/performance or demonstration at the Speakers' Corner can register with the National Parks Board, which manages Hong Lim Park. Online registration is available on the website.\n\nWoodford Square in Port of Spain, Trinidad, is also known as \"The University of Woodford Square\", so named by the first prime minister of Trinidad Eric Williams, who gave many speeches here. Another nickname, \"People's Parliament\", comes from the Black Power movement of the 1970s. Flanked by Trinidad's Parliament and Halls of Justice the Square still plays host to speeches of a highly topical and political nature.\n\nIn the southeast corner of the square, a blackboard lists the day's discussion as well as other important information. The speakers' topics are divided by interest and known as \"classes\".\n\nAn area was set up in Bangkok in the 1930s, and quickly became known as Hyde Park, to enable freedom of speech and the airing of political views in Thailand. The area was shut down after student rioting and the lethal intervention of the army and it is not discussed openly today.\n\nIn 1955, Marshal Plaek Pibulsonggram had visited London as part of an international tour. He became impressed with the 'Speakers' Corner' in Hyde Park. Upon his return to Thailand a 'Hyde Park' space for free speech and assembly was instituted at the Phramane Grounds in Bangkok. The experiment was well received and effectively stimulated political debate. The experiment was not appreciated by the government though, and in February 1956 restrictions were imposed on the Phramane 'Hyde Park'. However, during this period the Hyde Park Movement Party had evolved, upholding the legacy of the Hyde Park experiment.\n\nTom L. Johnson, the radical reforming Mayor of Cleveland (1901–1909), dedicated the north-west quadrant of Public Square to Free Speech, as in Hyde Park. Speeches and meetings there were common in the early part of the century; Anarchist Emma Goldman addressed a large crowd there in 1908. Today the site remains the traditional place for rallies and demonstrations in Cleveland, around Mayor Johnson's statue.\n\nThe University of California at Berkeley had a free speech area in front of Sproul Plaza until 1964. The University of Missouri hosts a Speaker's Corner, referred to as \"Speaker's Circle\". There are only two such locations in the entire state of Missouri. \n\nAs a result of winter semesters visits to England and Hyde Park, Elon College (now known as Elon University) created a Speakers' Corner on campus. No persons from outside the university may speak without a permit. Students are free to speak at any time as long as they don't use amplification, do not disrupt others, do not damage property (including the lawn itself) and do not cause dangerous conditions (stakes may not be planted in the ground without the approval of Physical Plant due to electrical/water lines, etc.).\n\nSee also Washington Square Park (Chicago) regarding the history of Bughouse Square in Chicago, known as a free speech site from the 1910s to the 1960s.\n\nThe pedestrian-only area of Pennsylvania Avenue on the north side of the White House in Washington, D.C. has become a de facto speaker's corner. Consistent with the principles of the first amendment, ad hoc public speaking is generally legal in all public places in Washington DC, although organized demonstrations require police permits. \n\nInspired by Speakers' Corner, Karl Dean, the Mayor of Nashville, designated a space for live music in the south-west corner of Centennial Park (Nashville), calling it Musicians Corner. A free concert series of the same name takes place in this space each year.\n\n\n\n\n"}
{"id": "52624552", "url": "https://en.wikipedia.org/wiki?curid=52624552", "title": "The Sustainable City", "text": "The Sustainable City\n\nThe Sustainable City is a 46 hectare property development in Dubai, United Arab Emirates. Situated on the Al Qudra road, it is the first net zero energy development in the Emirate of Dubai. The development includes 500 villas, 89 apartments and a mixed use area consisting of offices, retail, healthcare facilities, a nursery and food and beverage outlets. Phase 2 of the development will include a hotel, school and innovation centre. \n\nThe City was developed by Dubai-based Diamond Developers, whose Chief Executive Officer, Faris Saeed, has stated that much of his inspiration for the development came from UC Davis West Village.\n\nKey elements of the City include:\n\n\nApart from periphery roads and car parking areas, the development is a car-free site.\n\nThe parking areas are topped by solar shading featuring solar panels that are connected to an electrical grid to supply energy into different sections of the city.\n\nPanels are also placed on the roofs of all of the houses.\n\nThe construction waste is reused to furniture the public spaces.\n\nThe townhouses have UV reflective paint to reduce the thermal heat gain inside the houses.\n\n\n"}
{"id": "1897038", "url": "https://en.wikipedia.org/wiki?curid=1897038", "title": "Timeline of African-American history", "text": "Timeline of African-American history\n\nThis is a timeline of the African-American history in what is now the United States, from 1565 to the present.\n\n1565\n\n1619\n\n1640\n\n1654\n\n1662\n\n1672\n\n1676\n\n1705\n\n1712\n\n1739\n\n1753\n\n1760\n\n1765–1767\n\n1770\n\n1773\n\n1774\n\n1775\n\n1776–1783 American Revolution\n\n1777\n1780\n\n1781\n\n1783\n\n1787\n\n1788\n\n1790–1810 Manumission of slaves\n\n1791\n\n1793\n\n1794\n\nEarly 19th century\n\n1800\n\n1807\n\n1808\n\n1816\n\n1820\n\n1821\n\n1822\n\n1829\n\n1830\n\n1831\n\n1832\n\n1833\n\n1837\n\n1839\n\n1840\n\n1842\n\n1843\n\n1847\n\n1849\n\n1850\n\n1852\n\n1853\n\n1854\n\n1855\n\n1856\n\n1857\n\n1859\n\n1861\n\n1862\n\n1863–1877 Reconstruction Era\n\n1863\n\n\n1864\n\n1865\n\n1866\n\n1867\n\n1868\n\n1870\n\n1871\n\n1872\n\n1873\n\n1874\n\n1875\n\n1876\n\n1877\n\n1879\n\n1880\n\n1881\n\n1882\n\n1883\n\n1884\n\n1886\n\n1887\n\n1890\n\n1892\n\n1893\n\n1895\n\n1896\n\n1898\n\n1899\n\n1900\n\n1901\n\n1903\n\n1904\n\n1905\n\n1906\n\n1907\n\n1908\n\n1909\n\n1910\n\n1911\n\n1913\n1914\nJanuary 9Phi Beta Sigma Fraternity, Inc. was founded at Howard University by A. Langston Taylor, Leonard F. Morse, and Charles I. Brown\n\n1915\n\n1916\n\n1917\n\n1918\n\n1919\n\n1920\n\n1921\n\n1922\n\n1923\n\n1924\n\n1925\n\n1926\n\n1928\n\n1929\n\n1930\n\n1931\n\n1932\n\n1933\n\n1934\n\n1935\n\n1936\n\n1937\n\n1938\n\n1939\n\n1940s to 1970\n\n1940\n\n1941\n\n1942\n\n1943\n\n1944\n\n1945–1975 The Civil Rights Movement.\n\n1945\n\n1946\n\n1947\n\n1948\n\n1949\n\n1950\n\n1951\n\n1952\n\n1953\n\n1954\n\n1955\n\n\n1956\n\n1957\n\n\n1958\n\n1959\n\n1960\n\n1961\n\n1962\n\n1963\n\n1964\n\n1965\n\n1966\n\n1967\n\n1968\n\n1969\n\n1970\n\n1971\n\n1972\n\n1973\n\n1974\n\n1975\n\n1976\n\n1977\n\n1978\n\n1979\n\n1981\n\n1982\n\n1983\n\n1984\n\n1985\n\n1986\n\n1987\n\n1988\n\n1989\n\n1990\n\n1991\n\n1992\n\n1994\n\n1995\n\n1997\n\n1998\n\n1999\n\n2000\n\n2001\n\n2002\n\n2003\n\n2005\n\n2006\n\n2007\n\n2008\n\n2009\n\n2010\n\n2011\n\n2012\n\n2013\n\n2014\n\n2015\n\n\n"}
{"id": "22936371", "url": "https://en.wikipedia.org/wiki?curid=22936371", "title": "Tolerance coning", "text": "Tolerance coning\n\nTolerance coning is the engineering discipline of creating a budget of all tolerances that potentially add/subtract to affect adequacy of a particular parameter. This is particularly critical where stages of design/manufacture precede test/use.\n\nFor example, when setting a test limit for a measurement on each manufactured item of some type, to assure that no bad items are shipped, the limit must be tighter than the requirement to allow for the worst case sum of measurement inaccuracies (e.g. equipment, test fixture etc.). The design of the item thus has to take into account not only the product requirement but also the test tolerances. The buildup of this budget is tolerance coning.\nElectronics engineers intuitively do tolerance coning and tend to formalise it for critical parameters. However it is also relevant to other engineering disciplines.\n\n"}
{"id": "21087522", "url": "https://en.wikipedia.org/wiki?curid=21087522", "title": "Transformation of text", "text": "Transformation of text\n\nTransformations of text are strategies to perform geometric transformations on text (reversals, rotations, etc.), particularly in systems that do not natively support transformation, such as HTML, seven-segment displays and plain text.\n\nMany systems, such as HTML, seven-segment displays and plain text, do not support transformation of text. In the case of HTML, this limitation in display may eventually be addressed through standard cascading style sheets (CSS), since proposed specifications for CSS3 include rotation for block elements. In the meantime, several ways of producing the visual effects of text transformations have come into use.\n\nThe most common of these transformations are rotation and reflection.\n\nUnicode supports a variety of characters that resemble transformed characters, primarily for various forms of phonetic transcription. Each of these character names indicates what kind of transformation the characters have undergone:\n\n\nStrategies can be used to render words upside down in languages such as HTML that do not permit rotation of text; using Unicode characters (especially those in the IPA), a very close approximation of upside-down text (also called flip text) can be achieved. The letters s, x, z and o are rotationally symmetrical, while pairs such as b/q, d/p and n/u are rotations of each other. The rest of the letters have been encoded into the Unicode IPA section, generating a complete set of upside-down lowercase letters. With the addition of the Fraser alphabet to the Unicode standard in version 5.2, full (or at least near-full) support for upside-down capital letters is now available. Number support is incomplete; four numbers are universally strobogrammatic (0, 8, and 6/9), and the upside-down versions of numbers 2 and 3 have been provisionally assigned Unicode points for use in dozenal notation; however, other numbers still are not supported. Punctuation (by use of such characters as the interpunct and the inverted question mark and exclamation point) is mostly covered. Several Internet utilities exist for the transformation of regular text to (and sometimes from) upside-down text; each has its own slightly different algorithm for letters not precisely or well covered. A list of converters and algorithms can be found at the list below.\n\nA similar process is USD encoding, which uses characters entirely within the ASCII character set. Because it is almost entirely alphanumeric, it is far more compatible with other programs that do not support Unicode, and more readily typed by hand. However, the text created by using USD encoding is far less legible, and in fact more closely resembles Leet. Another problem is that because not all letters fit well, the USD algorithms cannot be a complete involution (i.e., completely convertible back and forth) and contain a complete set of letters at the same time. For instance, the Albartus USD algorithm example seen in the \"Examples\" section below has k, T, t, and R still in their upright positions. Another issue with USD encoding is the use of italic type. The letter \"a\" will, in most typefaces using italic fonts, render it as a \"one-story\" Latin alpha, thus causing problems with any word using that letter as a lowercase \"e.\" Oblique type does not have this problem.\n\nBelow is a conversion table that can be used to transform lowercase, uppercase numeric and punctuation output. These characters require unicode version 8.0 minimum (in particular the ᘔ and Ɛ from the duodecimal block).\n\nSideways text presents a unique problem. Though it is likely the most practical (as opposed to artistic) form of text transformation, it is the least supported and is the most difficult to implement. Unlike rotating text 180 degrees, the number of sideways characters falls far short of what would be needed for most purposes, and because text is rendered horizontally, it would be very difficult to render beyond one line of vertical text in a well-aligned manner without columns, especially in proportional fonts (furthermore, each character would require a line break after it). The process of using alternate characters for sideways text is further complicated by the fact that most fonts space letters further apart vertically (to accommodate underlining and overlining) than horizontally, and that most fonts are taller than they are wider, making simulated sideways text look significantly more awkward.\n\nInternet Explorer has a CSS property that will rotate normally entered text 90 degrees clockwise:\n\nHowever, no other major browsers (Mozilla Firefox, Opera nor WebKit based browsers) support this writing-mode property. However, all major standards-compliant browsers now support CSS3 rotation for block elements, which makes the visual rotation of HTML text available.\n\nThe most common way around these problems is to use images of text, which can then be rotated and transformed in an image editor at will, and to represent the text in those images with the alt attribute so that search engines and text-only browsers can read it properly. The use of ANSI art and box-drawing characters to manually draw sideways text has the advantage of being copiable and pastable (whereas images are not in most plain text situations), but generally creates large characters.\n\nThough less widespread, text can also be reversed to be a mirror image of itself. Letters A, H, I, M, O/o, T, U, V/v, W/w, X/x, Y, and in some fonts i and l are symmetrical in the y-axis; the pairs of b/d and p/q transform to each other. The letters И, Я, and \"г\" from Cyrillic, among other sources, are among the numerous characters that can be used to further generate this effect. Reversed text can use capital letters mixed with lowercase, as opposed to the strict lowercase used by upside-down transformation (upside-down lowercase and capital letters do not generally align as they would upright, though reversed letters do).\n\nX-axis symmetry is visible in the letters B, C, D, E, H, I, K, O, X, and in some fonts a and l, as well as in the pairs of a/g, b/p, d/q, e/G, and f/t. Expanding to Cyrillic and Greek produces more symmetries, such as Λ/V and Γ/L.\n\nThe Fixedsys Excelsior typeface includes a complete set of reversed characters like this in its Private Use Area. However, online utilities to create mirrored text are not readily available, and most sites that claim to \"mirror text\" or \"reverse text\" in fact only change the order of the letters and do not actually flip the letters themselves.\n\nThrough the use of Unicode's small capitals and subscript and superscript phonetic modifiers, text can be created that is smaller than the inline text. This is generally only necessary for applications that only support one-size plain text, since HTML and CSS support different text sizes.\n\n\n\nExample of reversed text reflected along a y-axis:\n\nPoet Darius Bacon has written two examples of palindromic poetry that reads the same upside-down as it does upside right.\n"}
{"id": "44368234", "url": "https://en.wikipedia.org/wiki?curid=44368234", "title": "Trauma risk management", "text": "Trauma risk management\n\nTrauma risk management (TRiM) is a method of secondary PTSD (and other traumatic stress related mental health disorders) prevention. The TRiM process enables non-healthcare staff to monitor and manage colleagues. TRiM training provides TRiM Practitioners with a background understanding of psychological trauma and its effects.\n\nTRiM is a trauma-focused peer support system and the way it works is wholly compliant with the PTSD management guidelines produced by the National Institute for Health and Care and Excellence.\n\nTrauma risk management Practitioners are trained to carry out an interview which identifies a number of risk factors which, when present, increase the likelihood that an individual may suffer poor longer term mental health as a result of a traumatic event. The initial TRiM interview takes place with an individual, 72 hours after a traumatic incident. People who score highly on this initial interview are provided with extra support by colleagues, and where appropriate, line managers. A follow up TRiM interview is then carried out approximately one month later to assess how well people have come to terms with the traumatic event at that point. Individuals who are found to have persistent difficulties at this point are encouraged and assisted to seek a professional assessment in order to access any specific treatment they require.\n\nTRiM originated within the UK military after previously-used reactive single session models of post incident intervention, such as Critical Incident Stress Debriefing, were subject to scientific scrutiny and shown to not just lack effectiveness but also have the potential to do harm. Professor Neil Greenberg was one of the team at the forefront of developing peer-led traumatic stress support packages, now known as TRiM. He is an academic psychiatrist based at King's College London UK and is a consultant occupational and forensic psychiatrist.\n\nAlthough it was first developed in the UK military, trauma risk management is now used by a range of public and commercial organisations. This includes charities, emergency services, security firms, risk management organisations, UK Government departments including the Foreign and Commonwealth Office, the oil and gas industry, transport organisations and media companies including the BBC.\n\nA large number of research papers have been published about the use of TRiM and its effectiveness and acceptance. This includes research about the use of TRiM by Cumbria Constabulary, following the Cumbria shootings in 2010. This research showed that the officers and staff who received a TRiM response fared better than their colleagues who did not and were less likely to be absent from work than colleagues who did not receive a TRiM intervention.\n\nResearch has shown that the use of TRiM may assist in increasing the psychological resilience of military personnel through the facilitation of social support.\n\nThe most recent research into TRiM came in a review paper, published in the Journal of Occupational Medicine in April 2015.\n"}
{"id": "477269", "url": "https://en.wikipedia.org/wiki?curid=477269", "title": "Trial by media", "text": "Trial by media\n\nTrial by media is a phrase popular in the late 20th century and early 21st century to describe the impact of television and newspaper coverage on a person's reputation by creating a widespread perception of guilt or innocence before, or after, a verdict in a court of law. Its first inception was the phrase \"Trial by Television\" which found light in the response to the 3 February, 1967 television broadcast of The Frost Programme, host David Frost. The confrontation and Frost's personal adversarial line of questioning of insurance fraudster Emil Savundra led to concern from ITV executives that it might affect Savundra's right to a fair trial.\n\nDuring high-publicity court cases, the media are often accused of provoking an atmosphere of public hysteria akin to a lynch mob which not only makes a fair trial nearly impossible but means that regardless of the result of the trial the accused will not be able to live the rest of their life without intense public scrutiny.\n\nThe counter-argument is that the mob mentality exists independently of the media which merely voices the opinions which the public already has.\n\nAlthough a recently coined phrase, the idea that popular media can have a strong influence on the legal process goes back certainly to the advent of the printing press and probably much further. This is not including the use of a state controlled press to criminalize political opponents, but in its commonly understood meaning covers all occasions where the reputation of a person has been drastically affected by ostensibly non-political publications.\n\nOften the coverage in the press can be said to reflect the views of the person in the street. However, more credibility is generally given to printed material than 'water cooler gossip'. The responsibility of the press to confirm reports and leaks about individuals being tried has come under increasing scrutiny and journalists are calling for higher standards. There was much debate over U.S President Bill Clinton's impeachment trial and prosecutor Kenneth Starr's investigation and how the media handled the trial by reporting commentary from lawyers which influenced public opinion.\n\nIn the United Kingdom, strict contempt of court regulations restrict the media's reporting of legal proceedings after a person is formally arrested. These rules are designed so that a defendant receives a fair trial in front of a jury that has not been tainted by prior media coverage. The newspapers the \"Daily Mirror\" and \"The Sun\" have been prosecuted under these regulations, although such prosecutions are rare.\n\n"}
{"id": "52760151", "url": "https://en.wikipedia.org/wiki?curid=52760151", "title": "Tunnel network", "text": "Tunnel network\n\nIn transport, tunnels can be connected together to form a tunnel network. These can be used in mining to reach ore below ground, in cities for underground rapid transit systems, in sewer systems, in warfare to avoid enemy detection or attacks, as maintenance access routes beneath sites with high ground-traffic such as airports and amusement parks, or to extend public living areas or commercial access while avoiding outdoor weather.\n\nTunnel networks were sometimes developed during siege warfare, even dating back to classical antiquity. Starting with a single tunnels being dug to undermine a wall that might be detected by the defenders and met with counter-tunnels, leading to tunnel warfare. Defenders might first create a series of underground listing posts to preempt such mining attacks.\n\nAny time the use of trenches becomes extensive, this naturally leads to connecting them with tunnel networks for safe passage both along the trench lines and with rear areas. In World War I, when given enough time and resources, the underground components of the defenses could become more extensive than those above ground.\n\nThe French Maginot Line, constructed from 1929 to 1939, was a chain of fortresses, bunkers, retractable turrets, outposts, obstacles, and sunken artillery emplacements, all linked by an extensive shell-proof underground tunnel network. It included underground barracks, shelters, ammo dumps and depots, and even had its own underground narrow gauge railways.\n\nThe tunnels of Củ Chi are an immense network of connecting underground tunnels located in the Củ Chi District of Ho Chi Minh City (Saigon), Vietnam, and are part of a much larger network of tunnels that underlie much of the country. The Củ Chi tunnels were the location of several military campaigns during the Vietnam War, and were the Viet Cong's base of operations for the Tết Offensive in 1968.\nThe tunnels were used by Viet Cong soldiers as hiding spots during combat, as well as serving as communication and supply routes, hospitals, food and weapon caches and living quarters for numerous North Vietnamese fighters. The tunnel systems were of great importance to the Viet Cong in their resistance to American forces, and helped to counter the growing American military effort.\nThe Vịnh Mốc tunnels are a tunnel complex in Quảng Trị, Vietnam. During the Vietnam War it was strategically located on the border of North Vietnam and South Vietnam. The tunnels were built to shelter people from the intense bombing of Son Trung and Son Ha communes in Vinh Linh county of Quảng Trị Province in the Vietnamese Demilitarized Zone. The American forces believed the villagers of Vinh Moc were supplying food and armaments to the North Vietnamese garrison on the island of Con Co which was in turn hindering the American bombers on their way to bomb Hanoi. The idea was to force the villagers of Vinh Moc to leave the area but as is typical in Vietnam there was nowhere else to go. The villagers initially dug the tunnels to move their village 10 metres underground but the American forces designed bombs that burrowed down 10 metres. Eventually against these odds, the villagers moved the village to a depth of 30 metres. It was constructed in several stages beginning in 1966 and used until early 1972. The complex grew to include wells, kitchens, rooms for each family and spaces for healthcare. Around 60 families lived in the tunnels; as many as 17 children were born inside the tunnels.\nThe tunnels were a success and no villagers lost their lives. The only direct hit was from a bomb that failed to explode; the resulting hole was utilized as a ventilation shaft.\nThree levels of tunnels were eventually built.\n\nDuring the 2014 Israel–Gaza conflict, the Israeli military uncovered and destroyed 32 cross-border tunnels that went on for miles beneath Gaza and reached into Israeli territory. According to intelligence officials, Israeli engineers are developing a system that could detect and destroy cross-border tunnels for which the Israeli government has reportedly spent more than $250 million since 2004.\n\nA network of caves beneath the cities of Mosul and Badana were built by ISIS. The terrorist group avoids battlefield engagements, preferring to hide in such tunnels safe from satellite detection, drone strikes and artillery, managed to maintain supply lines and communication with other areas under their control.\n\nA Texas investor group is building a $300 million luxury community replete that includes a navigable tunnel network with underground homes and air-lock blast doors designed for people worried about a dirty bomb or other disasters.\n\n"}
{"id": "9889145", "url": "https://en.wikipedia.org/wiki?curid=9889145", "title": "Vagal tone", "text": "Vagal tone\n\nVagal tone refers to activity of the vagus nerve, a fundamental component of the parasympathetic branch of the autonomic nervous system. This branch of the nervous system is not under conscious control and is largely responsible for the regulation of several body compartments at rest. Vagal activity results in various effects, including: heart rate reduction, vasodilation/constriction of vessels, glandular activity in the heart, lungs, and digestive tract as well as control of gastrointestinal sensitivity, motility and inﬂammation.\n\nIn this context, tone specifically refers to the continual nature of baseline parasympathetic action that the vagus nerve exerts. While baseline vagal input is constant, the degree of stimulation it exerts is regulated by a balance of inputs from sympathetic and parasympathetic divisions of the autonomic nervous system. Despite the described duality, vagal tone has been reported to mainly reflect the general level of parasympathetic activity. Vagal tone is typically considered in the context of heart function, but also has utility in assessing emotional regulation and other processes that alter, or are altered by changes and modification of the parasympathetic activity.\n\nMeasuring vagal tone along with its quantification and estimation can be performed by means of either invasive or noninvasive procedures. The former methodologies encompass the vagus nerve stimulation by manual or electrical techniques but literature reports a very limited number of experiments and clinical studies especially involving human subjects. On the other hand, noninvasive techniques are largely employed and they mainly rely on the investigation of heart rate and heart rate variability.\n\nIn the majority of cases, vagal tone is not directly measured. The most common procedure towards its quantification consist in investigating the processes altered by the vagus nerve – specifically heart rate and heart rate variability. As a general consideration, increased vagal tone (and thus vagal action) is associated with a diminished and more variable heart rate. On the opposite, during graded orthostatic tilt, vagal tone withdrawal is physiological and described as an indirect indicator of cardiovascular fitness.\n\nHeart rate is largely controlled by the heart's internal pacemaker activity. Considering a healthy heart, the main pacemaker is a collection of cells on the border of the atria and vena cava called the sinoatrial node. Heart cells exhibit automaticity which is the ability to generate electrical activity independent of external stimulation. As a result, the cells of the node spontaneously generate electrical activity that is subsequently conducted throughout the heart, resulting in a regular heart rate.\n\nIn absence of any external stimuli, sinoatrial pacing contributes to maintain the heart rate in the range of 60–100 beats per minute (bpm). At the same time, the two branches of the autonomic nervous system act in a complementary way increasing or slowing the heart rate. In this context, the vagus nerve acts on sinoatrial node slowing its conduction thus actively modulating vagal tone accordingly. This modulation is mediated by the neurotransmitter acetylcholine and downstream changes to ionic currents and calcium of heart cells.\n\nGiven the evidence that the vagus nerve plays a crucial role in heart rate regulation by modulating the response of sinoatrial node, vagal tone can be quantified by investigating heart rate modulation induced by vagal tone changes. This kind of analysis allows to investigate vagal tone by means of several noninvasive techniques based on heart rate variability.\n\nRespiratory sinus arrhythmia (RSA) is typically a benign, naturally occurring variation in heart rate that occurs during each breathing cycle. Specifically, heart rate increases during inspiration and decreases during expiration period. RSA was firstly recognized by Carl Ludwig but its genesis and understanding it is still nowadays largely discussed. RSA has been observed in humans from the early stages of life through adulthood. Moreover, RSA is a mechanism which can be consistently found in several different species.\n\nDuring inhalation intra-thoracic pressure lowers due to the contraction and downward movement of the diaphragm and the expansion of the chest cavity. Atrial pressure is also lowered as a result, enabling an increased blood flow to the heart. Such increase in blood volume towards the heart cavities triggers baroreceptors which act to diminish vagal tone. Subsequently, heart rate increases.\n\nOn the opposite during exhalation, the diaphragm relaxes, moving upward it decreases the size of the chest cavity, causing a subsequent increase in intrathoracic pressure. This increase in pressure inhibits venous return to the heart resulting in both reduced atrial expansion and minor activation of baroreceptors. Given the reduced baroreceptor activation, vagal tone is not suppressed as during inhalation so that it can exert its ability in decreasing heart rate.\n\nAs previously described, it is nowadays established that the two divisions of the autonomic nervous system influence each other reciprocally and independently so more and more measures able to discriminate the two contributions have been developed. In recent years, several studies have been published highlighting the quantification of RSA as a reliable tool to investigate vagal tone in a noninvasive way. Such investigations encompass physiological, behavioral, and several clinical studies. The main advantage in measuring of vagal tone by RSA is that such information are easily derivable from a single electrocardiography (ECG) recording. At the same time, novel methodologies started addressing RSA quantification by a multivariate approach thus not considering ECG only but the interrelationship of ECG and respiration.\n\nOn the opposite, vagal tone quantification by means of RSA has been questioned by many authors. It has been argued that RSA is unequivocally related to vagal control but it also clear that is determined by two different mechanisms namely: vagal tonic and vagal phasic. The former processes exhibit different dynamics and origins so that it is crucial to be able to differentiate their contributions to RSA. Furthermore, it has been observed that tonic and phasic components are distinct yet not completely independent one each other.\n\nDespite the nowadays limitations in RSA quantification, it is considered a promising, noninvasive and reliable index of vagal control of the heart, thus an indirect estimator of vagal tone.\n\nThe main hypothesis capable of explaining the reason behind the correlation of RSA and vagal tone describes RSA as an intrinsic resting function of the cardiopulmonary system. The theory suggest that in animals and humans RSA may eventually contribute to energy saving for both cardiac and respiratory systems thus reducing the heart rate and related heartbeats numbers. Furthermore, RSA could save energy expenditure by suppressing ineffective ventilation during the ebb of perfusion (delivery of blood from arteries to capillaries for oxygenation and nutrition).\n\nIn the physiological fields, RSA has been found to increase in subjects in resting state and to decrease in state of stress or tension. RSA is increased in supine position and decreased in prone position. RSA is on average higher and more pronounced during day time with respect to night time. RSA have also been extensively used to quantify vagal tone withdrawal in graded orthostatic tilt.\n\nTypically, expression of RSA decreases with age: it is pronounced in children and its magnitude tends to gradually disappear once a subject approach adulthood. However, adults in excellent cardiovascular health, such as endurance runners, swimmers, and cyclists, are likely to have a more pronounced RSA. Professional athletes on average maintain very high vagal tone and consequently higher RSA levels. RSA has been found to becomes less prominent in individuals with diabetes and cardiovascular disease.\n\nThe majority of vagal tone research in the physiological field (social behavior, social interactions, and human psychology) have been focused on newborns and children. The rational is to investigate children's adaptive functioning within a quantitative and reliable framework. Typically, researchers focus their attention on baseline vagal tone detection, treating it either as a potential predictor of behavior or examining its relationship with mental health (particularly emotion regulation, anxiety, and internalizing and externalizing disorders).\n\nThe Polyvagal theory by Porges is considered as the most influential model able to describe the differences between basal vagal tone during steady state and vagal reactivity as a response to external stimuli. The model describes vagal tone modifications a differential measure between vagal tone baseline and vagal tone activation during attention-demanding state. The theory states that successful vagal regulation is characterized by RSA suppression or withdrawal during attention tasks leading to increased metabolic output associated with heart rate increase.\n\nDespite the hypothesized link between vagal tone reduction and social functioning as stated by Porges' theory, researchers have been focusing mainly on the analysis of basal vagal tone. Examples are the findings reporting lower baseline RSA in children with Autism Spectrum Disorders with respect to healthy controls. Research indicates that children with more secure attachments with their mothers exhibited greater empathetic responsiveness, less social inhibition, and higher vagal tone, highlighting the vagus nerve's regulatory effect, as well as the quantification of vagal tone by means of RSA, as a predictor of emotional and social function.\n\nVagal tone estimation based on heart rate is quantifiable by several parameters rather than the use of RSA only. Examples are indexes of beat-to-beat variability such as RMSSD reported by The Task Force of the European Society of Cardiology and Heart Rhythm Society. Frequency analysis of heart rate in the range 0.15–0.4 Hz has been reported to quantify vagal tone based on heart rate variability spectrum. In the specific context of vagal tone response to head up tilt, a measure of beat-to-beat variability (RMSSD) showed significant decreases following head-up tilts as reported by Myers. Another method employed to quantify vagal activity is the computation of high frequency spectral component of heart rate variability power spectral density. An example for the latter described methodology is the change in sympatho-vagal balance during hypnosis. Results report hypnosis to affect heart rate variability, shifting the sympatho-vagal interaction toward an enhanced parasympathetic activity and reduction of the sympathetic tone.\n\n"}
{"id": "40502985", "url": "https://en.wikipedia.org/wiki?curid=40502985", "title": "Venom optimization hypothesis", "text": "Venom optimization hypothesis\n\nVenom optimization hypothesis, also known as venom metering, is a biological hypothesis which postulates that venomous animals have physiological control over their production and use of venoms. It explains the economic use of venom because venom is a metabolically expensive product, and that there is a biological mechanism for controlling their specific use. The hypothetical concept was proposed by Esther Wigger, Lucia Kuhn-Nentwig, and Wolfgang Nentwig of the Zoological Institute at the University of Bern, Switzerland, in 2002.\n\nA number of venomous animals have been experimentally found to regulate the amount of venom they use during predation or defensive situations. Species of anemones, jellyfish, ants, scorpions, spiders, and snakes are found to use their venoms frugally depending on the situation and size of their preys or predators.\n\nVenom optimization hypothesis was postulated by Wigger, Kuhn-Nentwig, and Nentwig from their studies of the amount of venom used by a wandering spider \"Cupiennius salei\". This spider produces a neurotoxic peptide called CsTx-1 for paralysing its prey. It does not weave webs for trapping preys, and therefore, entirely depends on its venom for predation. It is known to prey on a variety of insects including butterflies, moths, earwigs, cockroaches, flies and grasshoppers. Its venom glands store only about 10 μl of crude venom. Refilling of the glands takes 2–3 days and the lethal efficacy of the venom is, initially, very low for several days, requiring 8 to 18 days for full effect. It was found that the amount of venom released differed for each specific prey. For example, for bigger and stronger insects like beetles, the spider uses the entire amount of its venom; while for small ones, it uses only a small amount, thus economizing its costly venom. In fact, experiments show that the amount of venom released is just sufficient (at the lethal dose) to paralyze the target organism depending on the size or strength, and is not more than what is necessary.\n\nAnimal venoms are complex biomolecules and hence, their biological synthesis require high metabolic activity. A particular venom itself is a complex chemical mixture composed of hundreds of proteins and non-proteinaceous compounds, resulting in a potent weapon for prey immobilization and predator deterrence. The metabolic cost of venom is sufficiently high to result in secondary loss of venom whenever its use becomes non-essential to survival of the animal. This suggests that venomous animals may have evolved strategies for minimizing venom expenditure, that they should use them only as and when required, and that too in optimal amount.\n"}
{"id": "33178", "url": "https://en.wikipedia.org/wiki?curid=33178", "title": "White supremacy", "text": "White supremacy\n\nWhite supremacy or white supremacism is the racist belief that white people are superior to people of other races and therefore should be dominant over them. White supremacy has roots in scientific racism, and it often relies on pseudoscientific arguments. Like most similar movements such as neo-Nazism, white supremacists typically oppose members of other races as well as Jews.\n\nThe term is also typically used to describe a political ideology that perpetuates and maintains the social, political, historical, or institutional domination by white people (as evidenced by historical and contemporary sociopolitical structures such as the Atlantic slave trade, Jim Crow laws in the United States, and \"apartheid\" in South Africa). Different forms of white supremacism put forth different conceptions of who is considered white, and different groups of white supremacists identify various racial and cultural groups as their primary enemy.\n\nIn academic usage, particularly in usage which draws on critical race theory or Intersectionality, the term \"white supremacy\" can also refer to a political or socioeconomic system, in which white people enjoy a structural advantage (privilege) over other ethnic groups, on both a collective and individual level.\n\nWhite supremacy has ideological foundations that date back to 17th-century scientific racism, the predominant paradigm of human variation that helped shape international relations and racial policy from the latter part of the Age of Enlightenment until the late 20th century (marked by decolonization and the abolition of apartheid in South Africa in 1991, followed by that country's first multiracial elections in 1994).\n\nWhite supremacy was dominant in the United States both before and after the American Civil War, and it persisted for decades after the Reconstruction Era. In the antebellum South, this included the holding of African Americans in chattel slavery, with four million of them denied freedom. The outbreak of the Civil War saw the desire to uphold white supremacy being cited as a cause for state secession and the formation of the Confederate States of America. In an editorial about Native Americans in 1890, author L. Frank Baum wrote: \"The Whites, by law of conquest, by justice of civilization, are masters of the American continent, and the best safety of the frontier settlements will be secured by the total annihilation of the few remaining Indians.\"\n\nIn some parts of the United States, many people who were considered non-white were disenfranchised, barred from government office, and prevented from holding most government jobs well into the second half of the 20th century. Professor Leland T. Saito of the University of Southern California writes: \"Throughout the history of the United States, race has been used by whites for legitimizing and creating difference and social, economic and political exclusion.\" The Naturalization Act of 1790 limited U.S. citizenship to whites only.\n\nThe denial of social and political freedom for minorities continued into the mid-20th century, resulting in the civil rights movement. Sociologist Stephen Klineberg has stated that U.S. immigration laws prior to 1965 clearly declared \"that Northern Europeans are a superior subspecies of the white race\". The Immigration and Nationality Act of 1965 opened entry to the U.S. to immigrants other than traditional Northern European and Germanic groups, and significantly altered the demographic mix in the U.S as a result. Many U.S. states banned interracial marriage through anti-miscegenation laws until 1967, when these laws were invalidated by the Supreme Court of the United States' decision in \"Loving v. Virginia\". These mid-century gains had a major impact on white Americans' political views; segregation and white racial superiority, which had been publicly endorsed in the 1940s, became minority views within the white community by the mid-1970s, and continued to decline into 1990s polls to a single-digit percentage. For sociologist Howard Winant, these shifts marked the end of \"monolithic white supremacy\" in the United States.\nAfter the mid-1960s, white supremacy remained an important ideology in the American far-right. Howard Winant writes that, \"On the far right the cornerstone of white identity is belief in an ineluctable, unalterable racialized difference between whites and nonwhites.\" According to Kathleen Belew, a historian of race and racism in the United States, white militancy shifted after the Vietnam War from supporting the existing racial order to a more radical position—self-described as \"white power\" or \"white nationalism\"—committed to overthrowing the United States government and establishing a white homeland. White supremacist groups such as the Ku Klux Klan, neo-Nazi organizations, the Christian Identity movement, and racist skinheads make up two of the three major strands of violent right-wing movements in the United States (the third is anti-government militia organizations). \n\nSome academics argue that outcomes from the 2016 United States Presidential Election reflect ongoing challenges with white supremacy. Psychologist Janet Helms suggested that the norming behaviors of social institutions of education, government, and healthcare are organized around the \"birthright of...the power to control society's resources and determine the rules for [those resources]\". Educators, literary theorists, and other political experts have raised similar questions, connecting the scapegoating of disenfranchised populations to white superiority.\n\nIn 1937, Winston Churchill told the Palestine Royal Commission: \"I do not admit for instance, that a great wrong has been done to the Red Indians of America or the black people of Australia. I do not admit that a wrong has been done to these people by the fact that a stronger race, a higher-grade race, a more worldly wise race to put it that way, has come in and taken their place.\" British historian Richard Toye, author of Churchill's Empire, said that \"Churchill did think that white people were superior.\" \nNazism promoted the idea of a superior Germanic people or Aryan race in Germany during the early 20th century. Notions of white supremacy and Aryan racial superiority were combined in the 19th century, with white supremacists maintaining the belief that white people were members of an Aryan \"master race\" which was superior to other races, particularly the Jews, who were described as the \"Semitic race\", Slavs, and Gypsies, which they associated with \"cultural sterility\". Arthur de Gobineau, a French racial theorist and aristocrat, blamed the fall of the \"ancien régime\" in France on racial degeneracy caused by racial intermixing, which he argued had destroyed the \"purity\" of the Nordic or Germanic race. Gobineau's theories, which attracted a strong following in Germany, emphasized the existence of an irreconcilable polarity between Aryan or Germanic peoples and Jewish culture.\n\nAs the Nazi Party's chief racial theorist, Alfred Rosenberg oversaw the construction of a human racial \"ladder\" that justified Hitler's racial and ethnic policies. Rosenberg promoted the Nordic theory, which regarded Nordics as the \"master race\", superior to all others, including other Aryans (Indo-Europeans). Rosenberg got the racial term \"Untermensch\" from the title of Klansman Lothrop Stoddard's 1922 book \"The Revolt Against Civilization: The Menace of the Under-man\". It was later adopted by the Nazis from that book's German version \"Der Kulturumsturz: Die Drohung des Untermenschen\" (1925). Rosenberg was the leading Nazi who attributed the concept of the East-European \"under man\" to Stoddard. An advocate of the U.S. immigration laws that favored Northern Europeans, Stoddard wrote primarily on the alleged dangers posed by \"colored\" peoples to white civilization, and wrote \"The Rising Tide of Color Against White World-Supremacy\" in 1920. In establishing a restrictive entry system for Germany in 1925, Hitler wrote of his admiration for America's immigration laws: \"The American Union categorically refuses the immigration of physically unhealthy elements, and simply excludes the immigration of certain races.\"\n\nGerman praise for America's institutional racism, previously found in Hitler's \"Mein Kampf\", was continuous throughout the early 1930's, and Nazi lawyers were advocates of the use of American models. Race-based U.S. citizenship and anti-miscegenation laws directly inspired the Nazi's two principal Nuremberg racial laws—the Citizenship Law and the Blood Law. In order to preserve the Aryan or Nordic race the Nazis introduced the Nuremberg Laws in 1935, which forbade sexual relations and marriages between Germans and Jews, and later between Germans and Romani and Slavs. The Nazis used the Mendelian inheritance theory to argue that social traits were innate, claiming that there was a racial nature associated with certain general traits such as inventiveness or criminal behavior.\n\nAccording to the 2012 annual report of Germany's interior intelligence service, the Federal Office for the Protection of the Constitution, at the time there were 26,000 right-wing extremists living in Germany, including 6000 neo-Nazis.\n\nA number of Southern African nations experienced severe racial tension and conflict during global decolonization, particularly as white Africans of European ancestry fought to protect their preferential social and political status. Racial segregation in South Africa began in colonial times under the Dutch Empire, and it continued when the British took over the Cape of Good Hope in 1795. Apartheid was introduced as an officially structured policy by the Afrikaner-dominated National Party after the general election of 1948. Apartheid's legislation divided inhabitants into four racial groups—\"black\", \"white\", \"coloured\", and \"Indian\", with coloured divided into several sub-classifications. In 1970, the Afrikaner-run government abolished non-white political representation, and starting that year black people were deprived of South African citizenship. South Africa abolished apartheid in 1991.\n\nIn Rhodesia, a predominantly white government issued its own unilateral declaration of independence from the United Kingdom during an unsuccessful attempt to avoid immediate majority rule. Following the Rhodesian Bush War which was fought by African nationalists, Rhodesian prime minister Ian Smith acceded to biracial political representation in 1978 and the state achieved recognition from the United Kingdom as Zimbabwe in 1980.\n\nNeo-Nazi organisations embracing white supremacist ideology are present in many countries of the world. In 2007, it was claimed that Russian neo-Nazis accounted for \"half of the world's total\".\n\nIn June 2015, Democratic Representative John Conyers and his Republican colleague Ted Yoho offered bipartisan amendments to block the U.S. military training of Ukraine's Azov Battalion — called a “neo-Nazi paramilitary militia” by Conyers and Yoho. Some members of the battalion are openly white supremacists.\n\nThe term \"white supremacy\" is used in academic studies of racial power to denote a system of structural or societal racism which privileges white people over others, regardless of the presence or the absence of racial hatred. White racial advantages occur at both a collective and an individual level (\"ceteris paribus\", , when individuals are compared that do not relevantly differ except in ethnicity). Legal scholar Frances Lee Ansley explains this definition as follows:\nThis and similar definitions have been adopted or proposed by Charles Mills, bell hooks, David Gillborn, Jessie Daniels, and Neely Fuller Jr, and they are widely used in critical race theory and intersectional feminism. Some anti-racist educators, such as Betita Martinez and the Challenging White Supremacy workshop, also use the term in this way. The term expresses historic continuities between a pre–civil rights movement era of open white supremacism and the current racial power structure of the United States. It also expresses the visceral impact of structural racism through \"provocative and brutal\" language that characterizes racism as \"nefarious, global, systemic, and constant\". Academic users of the term sometimes prefer it to \"racism\" because it allows for a distinction to be drawn between racist feelings and white racial advantage or privilege.\n\nThe term's recent rise in popularity among leftist activists has been characterized by some as counterproductive. John McWhorter, a specialist in language and race relations, has described its use as straying from its commonly accepted meaning to encompass less extreme issues, thereby cheapening the term and potentially derailing productive discussion. Political columnist Kevin Drum attributes the term's growing popularity to frequent use by Ta-Nehisi Coates, describing it as a \"terrible fad\" which fails to convey nuance. He claims that the term should be reserved for those who are trying to promote the idea that whites are inherently superior to blacks and not used to characterize less blatantly racist beliefs or actions. The use of the academic definition of white supremacy has been criticized by Conor Friedersdorf for the confusion it creates for the general public inasmuch as it differs from the more common dictionary definition; he argues that it is likely to alienate those it hopes to convince.\n\nSupporters of Nordicism consider the \"Nordic peoples\" to be a superior race. By the early 19th century, white supremacy was attached to emerging theories of racial hierarchy. The German philosopher Arthur Schopenhauer attributed cultural primacy to the white race:\n\nThe eugenicist Madison Grant argued in his 1916 book, \"The Passing of the Great Race\", that the Nordic race had been responsible for most of humanity's great achievements, and that admixture was \"race suicide\". In this book, Europeans who are not of Germanic origin but have Nordic characteristics such as blonde/red hair and blue/green/gray eyes, were considered to be a Nordic admixture and suitable for Aryanization.\n\nIn the United States, the Ku Klux Klan (KKK) is the group most associated with the white supremacist movement. Many white supremacist groups are based on the concept of preserving genetic purity, and do not focus solely on discrimination based on skin color. The KKK's reasons for supporting racial segregation are not primarily based on religious ideals, but some Klan groups are openly Protestant. The KKK and other white supremacist groups like Aryan Nations, The Order and the White Patriot Party are considered antisemitic.\n\nNazi Germany promulgated white supremacy based on the belief that the Aryan race, or the Germans, were the \"master race\". It was combined with a eugenics programme that aimed for racial hygiene through compulsory sterilization of sick individuals and extermination of Untermenschen (\"subhumans\"): Slavs, Jews and Romani, which eventually culminated in the Holocaust.\n\nChristian Identity is another movement closely tied to white supremacy. Some white supremacists identify themselves as Odinists, although many Odinists reject white supremacy. Some white supremacist groups, such as the South African Boeremag, conflate elements of Christianity and Odinism. Creativity (formerly known as \"The World Church of the Creator\") is atheistic and it denounces Christianity and other theistic religions. Aside from this, its ideology is similar to that of many Christian Identity groups because it believes in the antisemitic conspiracy theory that there is a \"Jewish conspiracy\" in control of governments, the banking industry and the media. Matthew F. Hale, founder of the World Church of the Creator, has published articles stating that all races other than white are \"mud races\", which is what the group's religion teaches.\n\nThe white supremacist ideology has become associated with a racist faction of the skinhead subculture, despite the fact that when the skinhead culture first developed in the United Kingdom in the late 1960s, it was heavily influenced by black fashions and music, especially Jamaican reggae and ska, and African American soul music.\n\nWhite supremacist recruitment activities are primarily conducted at a grassroots level as well as on the Internet. Widespread access to the Internet has led to a dramatic increase in white supremacist websites. The Internet provides a venue to openly express white supremacist ideas at little social cost, because people who post the information are able to remain anonymous.\n\n\n\nNotes\n\n"}
