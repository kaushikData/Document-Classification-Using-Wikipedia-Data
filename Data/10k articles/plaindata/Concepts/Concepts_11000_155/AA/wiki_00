{"id": "2135139", "url": "https://en.wikipedia.org/wiki?curid=2135139", "title": "After Virtue", "text": "After Virtue\n\nAfter Virtue is a book on moral philosophy by Alasdair MacIntyre. MacIntyre provides a bleak view of the state of modern moral discourse, regarding it as failing to be rational, and failing to admit to being irrational. He claims that older forms of moral discourse were in better shape, particularly singling out Aristotle's moral philosophy as an exemplar. \"After Virtue\" is among the most important texts in the recent revival of virtue ethics.\n\nThe book was first published in 1981 and has since gone through two subsequent editions, which have added to, but not changed, the original text. The second edition, published in 1984, adds a postscript replying to critics of the first edition; the third edition, published in 2007, contains a new prologue entitled \"\"After Virtue\" After a Quarter of a Century\".\n\nMacIntyre holds that \"After Virtue\" makes seven central claims. It begins with an allegory suggestive of the premise of the science-fiction novel \"A Canticle for Leibowitz\": a world where all sciences have been dismantled quickly and almost entirely. MacIntyre asks what the sciences would look like if they were re-assembled from the remnants of scientific knowledge that survived the catastrophe. \n\nHe claims that the new sciences, though superficially similar to the old, would in fact be devoid of real scientific content, because the key suppositions and attitudes would not be present. \"The hypothesis which I wish to advance,\" he continues, \"is that in the actual world which we inhabit the language of morality is in the same state of grave disorder as the language of natural science in the imaginary world which I described.\" Specifically, MacIntyre applies this hypothesis to advance the notion that the moral structures that emerged from the Enlightenment were philosophically doomed from the start because they were formed using the aforementioned incoherent language of morality. MacIntyre claims that this failure encompasses the work of many significant Enlightenment and post-Enlightenment moral philosophers, including Søren Kierkegaard, Karl Marx, Immanuel Kant, and David Hume. These philosophers \"fail because of certain shared characteristics deriving from their highly specific historical background.\" That background is the Enlightenment's abandonment of Aristotelianism, and in particular the Aristotelian concept of teleology. \n\nAncient and medieval ethics, argues MacIntyre, relied wholly on the teleological idea that human life had a proper end or character, and that human beings could not reach this natural end without preparation. Renaissance science rejected Aristotle's teleological physics as an incorrect and unnecessary account, which led Renaissance philosophy to make a similar rejection in the realm of ethics. But shorn of teleology, ethics as a body of knowledge was expurgated of its central content, and only remained as, essentially, a vocabulary list with few definitions and no context. With such an incomplete framework on which to base their moral understanding, the philosophers of the Enlightenment and their successors were doomed from the beginning.\n\nMacIntyre illustrates this point through an example of a people who, he argues, experienced a similar incoherence in their own moral and ethical tradition: the Polynesian people of the South Pacific and their taboos. King Kamehameha II removed the taboos of the people in order to modernize their society and met little if any resistance. The Polynesians had no issue with abandoning their long-standing cultural traditions and MacIntyre claims this is because the taboos, though once meaningful to the islanders, had been shorn over the centuries of their underlying spiritual and didactic purpose, becoming a set of arbitrary prohibitions. The fact that Kamehameha II could abolish them so easily and without opposition is evidence, MacIntyre argues, of their incoherence. A similar incoherence, he argues, bedevils the ethical project since the Enlightenment.\n\nAnother reason MacIntyre gives for the doomed nature of the Enlightenment is the fact that it ascribed moral agency to the individual. He claims this made morality no more than one man's opinion and, thus, philosophy became a forum of inexplicably subjective rules and principles. The failure of the Enlightenment Project, because of the abandonment of a teleological structure, is shown by the inadequacy of moral emotivism, which MacIntyre believes accurately reflects the state of modern morality.\n\nMacIntyre offers a critique of Friedrich Nietzsche, whom he calls the \"King Kamehameha II of the European tradition,\" in reference to the Polynesian allegory above. MacIntyre explains that, \"Nietzschean man, the Übermensch, [is] the man who transcends, finds his good nowhere in the social world to date, but only that in himself which dictates his own new law and his own new table of the virtues.\" Although he disagreed with Nietzsche's inegalitarian and elitist view of humankind, he acknowledged the validity of Nietzsche's critique of Enlightenment morality as an explanation of the latter's degeneration into emotivism, and that, like Kamehameha II, Nietzsche had identified the moral imperatives of his time as arbitrary and incoherent in demanding their abolition. \n\nThe nineteenth-century critic who has most lastingly and profoundly influenced MacIntyre is not Nietzsche but Marx—indeed, \"After Virtue\" originates in MacIntyre's plans to write a book repairing the moral weaknesses of Marxism. His critique of capitalism, and its associated liberal ideology and bureaucratic state (including what, in \"After Virtue\", he condemned as the state capitalism of the USSR) is not expressed in traditional Marxist terms. Instead, it is written as a defence of ordinary social \"practices\", and of the \"goods internal to practices\". Pursuit of these helps to give narrative structure and intelligibility to our lives, but these goods must be defended against their corruption by \"institutions\", which pursue such \"external goods\" as money, power and status (chapters 14-15).\n\nMacIntyre seeks to find an alternative to Nietzsche's philosophy and eventually concludes that only classic Aristotelian thought can hope to save Western humanity. While Nietzsche seems to include the Aristotelian ethics and politics in his attack on the \"degenerate disguises of the will to power,\" MacIntyre claims that this cannot be done due to important differences between the structure and assumptions of Aristotelian and post-Enlightenment philosophy. These include:\n\nMacIntyre opposes Nietzsche's return to the aristocratic ethics of Homeric Greece with the teleological approach to ethics pioneered by Aristotle. Nietzsche's critique of Enlightenment moral theory does not work against a teleological ethics. For MacIntyre, \"Nietzsche replaces the fictions of the Enlightenment individualism, of which he is so contemptuous, with a set of individualist fictions of his own.\" Nietzsche's Uebermensch, his solution to the lies of the Enlightenment, exposes the failure of the Enlightenment's epistemological project and of its search for a subjective yet universal morality. Nietzsche neglects the role of society in the formation and understanding of tradition and morality, and \"Nietzsche's great man cannot enter into relationships mediated by appeal to shared standards or virtues or goods; he is his own only moral authority and his relationships to others have to be exercises of that authority... it will be to condemn oneself to that moral solipsism which constitutes Nietzschean greatness.\"\n\n\"After Virtue\" ends by posing the question 'Nietzsche or Aristotle?', although MacIntyre acknowledges that the book does not give sufficient grounds for a definitive answer that it is Aristotle, not Nietzsche, who points to the best solution for the problems that the book has diagnosed. Those grounds are set out in MacIntyre's subsequent works, in which he elaborates a sophisticated revision of the philosophical tradition of Aristotelianism.\n\nIn the end, however, MacIntyre tells us that we are waiting not for Godot but for Benedict of Nursia. MacIntyre criticizes individualist political philosophy, such as John Rawls' \"A Theory of Justice\" and Robert Nozick's \"Anarchy, State, and Utopia\". To MacIntyre, morals and virtues can only be comprehended through their relation to the community in which they come from. Whereas Rawls tells us to conceive of justice through abstracting ourselves from who we are (through the veil of ignorance, for example) MacIntyre disagrees. Running throughout \"After Virtue\" is the belief that in order to comprehend who we are, we must understand where we come from.\n\nGeorge Scialabba found \"After Virtue\" to be a strong critique of modernity, but claimed that MacIntyre \"faltered\" at the conclusion of the argument, when he sketched the features of what virtuous life should be like in the conditions of modernity. In particular, Scialabba objected to MacIntyre's claim that the good life for human beings consists in contemplating the good life for human beings; Scialabba found this insufficient and anticlimactic. Scialabba also argued that, although he appreciated MacIntyre's insistence on participation in community life as the best defense against the perils of modernity, this insistence was not justified with any discussion of how community life can be reconciled with the critical spirit that Scialabba finds to be one of the great achievements of modernity and of the philosophical enterprise.\n\nIn a review for \"Political Theory\", William E. Connolly argues that MacIntyre sees Nietzsche as \"the adversary to be defeated, but Nietzsche's voice is not heard clearly\". Connolly objects that MacIntyre's defense of virtue does not take into account Nietzsche's critique; MacIntyre also fails to build an account of \"telos\" that does not draw on biology in the way MacIntyre wanted to avoid—such a theory doesn't account for the fact that we are embodied.\n\nAnthony Ellis, in the journal \"Philosophy\", argued that MacIntyre's positive philosophical project is not explained as well as it could have been: it is \"of daunting opacity, though tantalizingly interesting\" but not given enough space in the book. Ellis also states that the discussion of Rawls and Nozick in \"After Virtue\" \"is slight and assertive\".\n\nIn the \"Review of Metaphysics\", Christos Evangeliou said that if the reader \"had expected to find in this book concretely how a revived Aristotelian tradition is supposed to work in order to shape ethically and rationally the irrational and disorderly modern world\", they \"may be a little disappointed in their expectations\".\n\nFrancis Wheen included a brief critique of \"After Virtue\" in his own book \"How Mumbo-Jumbo Conquered the World\". The thrust of Wheen's book was a defense of the principles of the Enlightenment against various strands of irrationalism, and Wheen identified \"After Virtue\" and MacIntyre as constituting one such strand. In general, critics identified Wheen's disparagement of MacIntyre as one of \"Mumbo-Jumbo\" few missteps. While MacIntyre certainly is a fierce critic of the Enlightenment, Wheen refused to engage MacIntyre's case but dismissed his work on the grounds that any objection to the Enlightenment qualified as \"mumbo-jumbo\" \"ex hypothesi\".\n\n\n"}
{"id": "929468", "url": "https://en.wikipedia.org/wiki?curid=929468", "title": "Aleksandr Dugin", "text": "Aleksandr Dugin\n\nAleksandr Gelyevich Dugin (; born 7 January 1962) is a Russian philosopher, political analyst, and strategist known for his fascist views.\n\nHe has close ties with the Kremlin and the Russian military, having served as an advisor to State Duma speaker Gennadiy Seleznyov and key member of the ruling United Russia party Sergei Naryshkin. However, commentators dispute his influence: in the words of journalist Alexander Nevzorov, \"if we had had Sergey Kurginyan and Dugin instead of Putin, there would have been hell for all of us to pay; they would have unleashed a European and World War without a shadow of a doubt, without considering consequences at all.\" But \"Dugin and Kurginyan do not have the slightest impact on what is going on in the Kremlin and do not even get coaching there\". Dugin was the leading organizer of the National Bolshevik Party, National Bolshevik Front, and Eurasia Party. He is the author of more than 30 books, among them \"Foundations of Geopolitics\" (1997) and \"The Fourth Political Theory\" (2009).\n\nDugin was born in Moscow, into the family of a colonel-general in the Soviet military intelligence and candidate of law Geliy Alexandrovich Dugin and his wife Galina, a doctor and candidate of medicine. In 1979, he entered the Moscow Aviation Institute, but did not graduate due to his association with thinking contrary to the ongoing regime, which caused him to be expelled from the institute.\n\nA self-taught polyglot, he read avidly in his fields of interest, from Middle Age and Renaissance European authors to Oriental Studies, translating into Russian from English, French, and German, and also the writings of Italian fascist Julius Evola and French author René Guénon. Religious Traditionalism, Conservatism, Hermeticism and Poetry were topics followed closely by Dugin and a group of philosophers/thinkers also interested in Traditionalism: Geydar Dzhemal, Evgeniy Golovin, Yuri Mamleev, Vladimir Stepanov, and Sergey Jigalkin.\n\nHe graduated in Economics and Management at Novocherkassk Meliorative State Academy in the Rostov Oblast in 1999, defending his post-graduate degree in Philosophy at Rostov-on-Don with the dissertation \"The Evolution of Paradigmatic Foundations of Science\" in 2000, and the PhD in the Faculty of Sociology at the same University in 2004 with the theme \"The Transformation of the Political Structures and Institutions in the Process of Modernization of the Civil Society\". In 1998, Dugin, Geydar Dzhemal, and Evgeniy Golovin created a study center based on their long time shared interests called the New University project.\n\nDugin in the 1980s was a dissident and an anti-communist. Dugin worked as a journalist before becoming involved in politics just before the fall of communism. In 1988 he and his friend Geydar Dzhemal joined the nationalist group Pamyat. He helped to write the political program for the newly refounded Communist Party of the Russian Federation under the leadership of Gennady Zyuganov.\n\nIn his 1997 article \"Fascism – Borderless and Red\", Dugin proclaimed the arrival of a \"genuine, true, radically revolutionary and consistent, fascist fascism\" in Russia. He believes that it was \"by no means the racist and chauvinist aspects of National Socialism that determined the nature of its ideology. The excesses of this ideology in Germany are a matter exclusively of the Germans ... while Russian fascism is a combination of natural national conservatism with a passionate desire for true changes.\" \"Waffen-SS and especially the scientific sector of this organization, Ahnenerbe,\" was \"an intellectual oasis in the framework of the National Socialist regime\", according to him.\"\n\nDugin soon began publishing his own journal entitled \"Elementy\", which initially began by praising Franco-Belgian Jean-François Thiriart, supporter of a Europe \"from Dublin to Vladivostok\". Consistently glorifying both Tsarist and Stalinist Russia, \"Elementy\" also revealed Dugin's admiration for Julius Evola. Dugin also collaborated with the weekly journal \"Den\" (The Day), a bastion of Russian anti-Cosmopolitanism previously directed by Alexander Prokhanov.\n\nDugin was amongst the earliest members of the National Bolshevik Party (NBP) and convinced Eduard Limonov to enter the political arena in 1994. A part of hard-line nationalist NBP members, supported by Dugin, split off to form the more right-wing, anti-liberal, anti-left, anti-Kasparov aggressive nationalist organization, National Bolshevik Front. After breaking with Limonov, he became close to Yevgeny Primakov and later to Vladimir Putin's circle.\n\nDugin claims to be disapproving of liberalism and the West, particularly American hegemony. His assertions show that he likes Stalin and the Soviet Union: \"We are on the side of Stalin and the Soviet Union\". He calls himself a conservative and says, \"We, conservatives, want a strong, solid State, want order and healthy family, positive values, the reinforcing of the importance of religion and the Church in society\". He adds, \"We want patriotic radio, TV, patriotic experts, patriotic clubs. We want the media that expresses national interests\".\n\nThe Eurasia Party, later Eurasia Movement, was officially recognized by the Ministry of Justice on 31 May 2001. The Eurasia Party claims support by some military circles and by leaders of the Orthodox Christian faith in Russia, and the party hopes to play a key role in attempts to resolve the Chechen problem, with the objective of setting the stage for Dugin's dream of a Russian strategic alliance with European and Middle Eastern states, primarily Iran. Dugin's ideas, particularly those on \"a Turkic-Slavic alliance in the Eurasian sphere\" have recently become popular among certain nationalistic circles in Turkey, most notably among alleged members of the Ergenekon network, which is the subject of a high-profile trial (on charges of conspiracy). Dugin's Eurasianist ideology has also been linked to his adherence to the doctrines of the Traditionalist School. (Dugin's Traditionalist beliefs are the subject of a book length study by J. Heiser, \"The American Empire Should Be Destroyed—Aleksandr Dugin and the Perils of Immanentized Eschatology.\") Dugin also advocates for a Russo-Arab alliance.\n\nThe reborn Russia, according to Dugin's concept, is said by Charles Clover of the \"Financial Times\" to be a slightly remade version of the Soviet Union with echoes of \"Nineteen Eighty-Four\" by George Orwell, where Eurasia was one of three continent-sized super states including Eastasia and Oceania as the other two and was participating in endless war between them.\n\nIn the Eurasian public discourse sphere, the totalitarian communist policy deployed in over three decades of works by various international groups that are part of the movement, is \"a version of reintegration of the post-Soviet space into a \"Eurasian\" sphere of influence for Russia\". The North-American program \"works with a wide range of partners from all sectors of civil society\" and \"is advanced through grant making, advocacy and research, regional initiatives, and close engagement\".\n\nHe has criticized the \"Euro-Atlantic\" involvement in the 2004 Ukrainian presidential election as a scheme to create a \"cordon sanitaire\" around Russia, much like the French and British attempt post-World War I.\n\nIn 2005, Dugin founded the Eurasian Youth Union of Russia as the youth wing of the International Eurasia Movement.\n\nUkraine gave Dugin a five-year entry ban, starting in June 2006, and Kiev declared him a \"persona non grata\" in 2007. His Eurasian Youth Union was banned in Ukraine. In 2007, the Security Service of Ukraine identified persons of the Eurasian Youth Union who committed : they climbed up the mountain of Hoverla, imitated sawing down the details of the construction in the form of the small coat of arms of Ukraine by tools brought with them and painted the emblem of the Eurasian Youth Union on the memorial symbol of the Constitution of Ukraine. He was deported back to Russia when he arrived at Simferopol International Airport in June 2007.\n\nBefore war broke out between Russia and Georgia in 2008, Dugin visited South Ossetia and predicted, \"Our troops will occupy the Georgian capital Tbilisi, the entire country, and perhaps even Ukraine and the Crimean Peninsula, which is historically part of Russia, anyway.\" Afterwards he said Russia should \"not stop at liberating South Ossetia but should move further,\" and \"we have to do something similar in Ukraine.\" In 2008, Dugin stated that Russia should repeat the Georgian scenario in Ukraine, namely attack it. In September 2008, after the Russian-Georgian war, he did not hide his anger towards Putin, who \"dared not drop the other shoe\" and \"restore the Empire.\"\n\nDugin was baptized at the age of six in the Russian Orthodox church of Michurinsk by his great-grandmother Elena Mikhailovna Kargaltseva. Since 1999, he formally embraced a branch of the Old Believers, a Russian religious movement which rejected the 1652–1666 reforms of the official Russian Orthodox Church. Dugin's Eurasian philosophy owes much to Traditional Integralism and \"Nouvelle Droite\" movements, and as such it resonates with Neopaganism, a category which in this context means the movement of Slavic Native Faith (Rodnovery), especially in the forms of Anastasianism and Ynglism. Dugin's Eurasianism is often cited as belonging to the same spectrum of these movements, as well as also having influences from Hermetic, Gnostic and Eastern traditions. He himself calls to rely upon \"Eastern theology and mystical currents\" for the development of the Fourth Political Theory.\n\nAccording to Marlene Laruelle, his adherence to the Old Believers allows him to stand between Paganism and Orthodox Christianity without formally adopting either of them. His choice is not paradoxical, since, according to him — in the wake of René Guénon —, Russian Orthodoxy and especially the Old Believers have preserved an esoteric and initiatory character which was utterly lost in Western Christianity. As such, the Russian Orthodox tradition may be merged with Neopaganism and may host \"Neopaganism's nationalist force, which anchors it in the Russian soil, and separates it from the two other Christian confessions\".\n\nIn the early 1990s Dugin's work at the National Bolshevik Front included research into the roots of national movements and the activities of supporting esoteric groups in the first half of the 20th century. Partnering Christian Bouchet, a then-member of the French OTO, and building on the national-fascist and migratory-integrative interest groups in Asia and Europe, they contribute in bringing international politics closer to Russia's Eurasian geopolitical concept.\n\nAleksandr Dugin supports Putin and his foreign policies but has opposed Russian governments due to their economic policies. His 2007 quote, \"There are no more opponents of Putin's course and, if there are, they are mentally ill and need to be sent off for clinical examination. Putin is everywhere, Putin is everything, Putin is absolute, and Putin is indispensable\" – was voted number two in flattery by readers of \"Kommersant\".\n\nIn the Kremlin, Dugin represents the \"war party\", a division within the leadership over Ukraine. Dugin is seen as an author of Putin's initiative for the annexation of Crimea by the Russian Federation. He considered the war between Russia and Ukraine to be inevitable and appealed for Putin to start military intervention in eastern Ukraine. Dugin said, \"The Russian Renaissance can only stop by Kiev.\" During the 2014 pro-Russian conflict in Ukraine, Dugin was in regular contact with pro-Russian separatist insurgents. He described his position as \"unconditionally pro-DPR and pro-LPR\". A Skype video call posted on YouTube showed Dugin providing instructions to separatists of South and Eastern Ukraine as well as advising Ekaterina Gubareva, whose husband Pavel Gubarev declared himself a local governor and after that was arrested by the Security Service of Ukraine. On 31 March 2014, Oleg Bahtiyarov, a member of the Eurasia Youth Union of Russia founded by Dugin, was arrested. He had trained a group of about 200 people to seize parliament and another government building, according to the Security Service of Ukraine. Dugin also developed links with far-right and far-left political parties in the European Union, including Syriza in Greece, Ataka in Bulgaria, the Freedom Party of Austria, and Front National in France, to influence EU policy on Ukraine and Russia.\n\nDugin stated he was disappointed in Russian President Vladimir Putin, saying that Putin did not aid the pro-Russian insurgents in Ukraine after the Ukrainian Army's early July 2014 offensive. In August 2014, Dugin called for a \"genocide\" of Ukrainians.\n\nHalya Coynash of the Kharkiv Human Rights Protection Group said that the influence of Dugin's \"Eurasian ideology\" on events in eastern Ukraine and on Russia's invasion of the Crimea was beyond any doubt. According to Vincent Jauvert, Dugin's radical ideology today became the basis for the internal and foreign policy of the Russian authorities. So Dugin is worth listening to, in order to understand to which fate the Kremlin is leading its country and the whole of Europe. However, according to Alexander Nevzorov, if we had had Kurginyan and Dugin instead of Putin, there would have been hell for all of us to pay, they would have unleashed a European and World War without a shadow of a doubt, without considering consequences at all. But Dugin and Kurginyan do not have the slightest impact on what is going on in the Kremlin and do not even get coaching there. In another publication, Nevzorov said, \"Beliefs are only proven by being under bullets, another prescription does not exist. I do not understand why Milonov and Dugin are not there yet.\"\n\nOn 10 October 2014, Dugin said, \"Only after restoring the Greater Russia that is the Eurasian Union, we can become a credible global player. Now these processes slowed down very much. The Ukrainian maidan was the response of the West to the advance of the Russian integration.\" He described the Euromaidan as a coup d'état carried out not by the Ukrainians but by the United States: \"America wishes to wage the war against Russia not by its own hands but by the hands of the Ukrainians. Promising to wink at up to 10 thousand victims among the peaceful population of Ukraine and actually demanding the victims, the United States led to this war. The United States carried out the coup d'état during the maidan for the purpose of this war. The United States raised neo-Nazis Russophobes to the power for the purpose of this war.\" Dugin said Russia is the major driving force for the current events in Ukraine, \"Russia insists on its sovereignty, its liberty, responds to challenges thrown down to it, for example, in Ukraine. Russia is attempting to integrate the post-Soviet space ...\" As Israeli political scientist Vyacheslav Likhachov states, \"If one seriously takes the fact that such a person as Alexander Dugin is the ideologist of the imperial dash for the West, then one can establish that Russia is not going to stop as far as the Atlantic Ocean.\"\n\nIn the 2014 article by Dmitry Bykov \"Why TV, Alexander Dugin and Galina Pyshnyak crucified a boy\", Channel One Russia's use of the aired story by Dugin and Pyshnyak about the allegedly crucified boy as a pretext for escalating the conflict was compared to the case of Beilis. On 9 July 2014, Dugin on his Facebook account wrote a story that a 6 year old child was allegedly nailed down to an advertisement board and shot to death before his father's eyes. On 16 July 2014, \"Novaya Gazeta\" provided a videotape of its correspondent Eugen Feldman walking along the main square in Sloviansk, asking local old women if they had heard of the murder of the child. They said such an event did not take place. The website Change.org hosted a petition of citizens who demanded \"a comprehensive investigation with identification for all persons involved in the fabrication of the plot.\"\n\nOn 2 October 2014, Dugin described the situation in Donbass: \"The humanitarian crisis has long since been raging on the territory of Novorossiya. Already up to a million, if not more, refugees are in the Russian Federation. A large part of the inhabitants of the DPR and the LPR simply moved abroad.\" In the end of October 2014, Dugin advised the separatists to establish dictatorship in Novorossiya until they win in the confrontation.\n\nThe typical rhetoric about the fifth column as foreign agents is used by Dugin for political accusations in many publications. In his 2014 interview published by \"Vzglyad\" and \"Komsomolskaya Pravda\", he says, \"A huge struggle is being conducted. And, of course, Europe has its own fifth column, its own Bolotnaya Square-minded people. And if we have them sitting idly and doing nasty things on Dozhd, Europe is indeed dominated and ruled by the fifth column in full swing. This is the same American riffraff ...\" He sees the United States standing behind all the scenes, including the Russian fifth column, according to his statement, \"The danger of our fifth column is not that they are strong, they are absolutely paltry, but that they are hired by the greatest 'godfather' of the modern world—by the United States. That is why they are effective, they work, they are listened to, they get away with anything because they have the world power standing behind them.\" He sees the American embassy as the center for funding and guiding the fifth column and asserts, \"We know that the fifth column receives money and instructions from the American embassy.\"\n\nAccording to Dugin, the fifth column promoted the breakup of the Soviet Union as a land continental construction, seized power under Boris Yeltsin, and headed Russia as the ruling politico-economic and cultural elite until the 2000s; the fifth column is the regime of liberal reformers of the 1990s and includes former Russian oligarchs Vladimir Gusinsky, Boris Berezovsky, former government officials Mikhail Kasyanov, Boris Nemtsov, Vladimir Ryzhkov, artistic, cultural, and media workers, the Echo of Moscow, the Russian State University for the Humanities, the highest ranks of the National Research University Higher School of Economics, a significant part of teachers of the Moscow State Institute of International Relations, and a minority part of teachers of the Moscow State University. Dugin proposes to deprive the fifth column of Russian citizenship and deport the group from Russia: \"I believe it is necessary to deport the fifth column and deprive them of their citizenship.\" However, in 2007, Dugin argued, \"There are no longer opponents of Putin's policy, and if there are, they are mentally ill and should be sent to prophylactic health examination.\" In 2014, Dugin in an interview to \"Der Spiegel\" confirmed that he considers the opponents of Putin to be mentally ill.\n\nIn one of his publications, Dugin introduced the term \"the sixth column\" and defined it as \"the fifth column which just pretends to be something different\", those who are in favor of Putin, but demand that he stand for liberal values (as opposed to the liberal fifth column, which is specifically against Putin). During the 2014 Russian military intervention in Ukraine, Dugin said that all the Russian sixth column stood up staunchly for Ukrainian oligarch Rinat Akhmetov. As he asserts, \"We need to struggle against the fifth and sixth columns.\"\n\nRussian-American artist Mihail Chemiakin says Dugin is inventing \"the sixth column\". \"Soon, probably, there would already be the seventh one as well. \"The fifth column\" is understandable. That is we, intelligentsia, lousy, dirty, who read Camus. And \"the sixth column\", in his opinion, is more dangerous, because that is the personal entourage of Vladimir Putin. But he is naive and understands nothing. And as for Dugin, he can tell him who to shoot to death and who to imprison. Maybe, Kudrin and maybe, Medvedev ...\"\n\nAccording to Dugin, the whole Internet should be banned: \"I think that Internet as such, as a phenomenon is worth prohibiting because it gives nobody anything good.\" In June 2012, Dugin said in a lecture that chemistry and physics are demonic sciences, and that all Orthodox Russians need to unite around the President of the Russian Federation in the last battle between good and evil, following the example of Iran and North Korea. He added, \"If we want to liberate ourselves from the West, it is needed to liberate ourselves from textbooks on physics and chemistry.\"\n\nDugin has characterized his position on the Ukrainian conflict as \"firm opposition to the Junta and Ukrainian Nazism that are annihilating peaceful civilians\" as well as unacceptance of liberalism and American hegemony.\n\nDuring the conflict in Ukraine, Dugin also lost his post as Head of the Department of Sociology of International Relations of Moscow State University. In 2014, a petition entitled \"We demand the dismissal of MSU Sociology Faculty Professor A. G. Dugin!\" was signed by over 10,000 people and sent to the MSU rector Viktor Sadovnichiy. The petition was started after Dugin in an interview expressed his opinion on how to deal with Ukrainians (\"Kill them, kill them, kill them. There should not be any more conversations. As a professor, I consider it so.\"). Dugin claimed to have been fired from this post; the university claimed the offer of a department chairmanship resulted from a technical error and that he would remain a professor under contract until September 2014. Dugin wrote the statement of resignation from the faculty because it was necessary to be reappointed to the Moscow State University, but the appointment did not happen, so as a result he is no longer a staff member of the faculty and a staff member of the Moscow State University.\n\nAt the end of October 2014, Sergey Kurginyan, the leader of all-Russian movement Essence of Time, sued Dugin because Dugin called Kurginyan a traitor and accused him of destroying DPR deputy foreign minister Proselkov in the following words: \"Possessed supporters of traitor Kurginyan, who as now it turns out is working for oligarchs, Yukos and Israel, declared Sasha Proselkov \"main enemy\" after he along with Gubarev did not allow denigrating Russian hero Igor Strelkov with impunity.\" Dugin added: \"Kurginyan crossed a red line: the ideological controversy is one thing, the physical destruction of Russian patriots is another one.\"\n\nOn 11 March 2015, the United States Department of the Treasury added Dugin, as well as his Eurasian Youth Union, to its list of Russian citizens who are sanctioned as a result of their involvement in the Ukrainian crisis. In June 2015, Canada added Dugin to its list of sanctioned individuals.\n\nHe is a character of \"Yevraziyskoye\" (\"Something Eurasian\"), a poem by Dmitry Bykov.\n\nSeveral of Dugin's books have been published by the publishing house Arktos, an English-language publisher for Traditionalist and New Right books, which specializes in works by prominent fascists and neo-Nazis. \n\n\n\n"}
{"id": "25137399", "url": "https://en.wikipedia.org/wiki?curid=25137399", "title": "Alphanumeric grid", "text": "Alphanumeric grid\n\nAn alphanumeric grid (also known as atlas grid) is a simple coordinate system on a grid in which each cell is identified by a combination of a letter and a number.\n\nAn advantage over numeric coordinates, which use two numbers instead of a number and a letter to refer to a grid cell, is that there can be no confusion over which coordinate refers to which direction. As an easy example, one could think about battleship; simply match the number at the top to the number on the bottom, then follow the two lines until they meet in a spot.\nAlgebraic chess notation uses an alphanumeric grid to refer to the squares of a chessboard.\n"}
{"id": "4567548", "url": "https://en.wikipedia.org/wiki?curid=4567548", "title": "Anderson's rule", "text": "Anderson's rule\n\nAnderson's rule is used for the construction of energy band diagrams of the heterojunction between two semiconductor materials. Anderson's rule states that when constructing an energy band diagram, the vacuum levels of the two semiconductors on either side of the heterojunction should be aligned (at the same energy).\n\nIt is also referred to as the electron affinity rule, and is closely related to the Schottky-Mott rule for metal-semiconductor junctions. \n\nAnderson's rule was first described by R. L. Anderson in 1960.\n\nOnce the vacuum levels are aligned it is possible to use the electron affinity and band gap values for each semiconductor to calculate the conduction band and valence band offsets. The electron affinity (usually given by the symbol formula_1 in solid state physics) gives the energy difference between the lower edge of the conduction band and the vacuum level of the semiconductor. The band gap (usually given the symbol formula_2) gives the energy difference between the lower edge of the conduction band and the upper edge of the valence band. Each semiconductor has different electron affinity and band gap values. For semiconductor alloys it may be necessary to use Vegard's law to calculate these values.\n\nOnce the relative positions of the conduction and valence bands for both semiconductors are known, Anderson's rule allows the calculation of the band offsets of both the valence band (formula_3) and the conduction band (formula_4).\nAfter applying Anderson's rule and discovering the bands' alignment at the junction, Poisson’s equation can then be used to calculate the shape of the band bending in the two semiconductors.\n\nConsider a heterojunction between semiconductor 1 and semiconductor 2. Suppose the conduction band of semiconductor 2 is closer to the vacuum level than that of semiconductor 1. The conduction band offset would then be given by the difference in electron affinity (energy from upper conducting band to vacuum level) of the two semiconductors:\n\nNext, suppose that the band gap of semiconductor 2 is large enough that the valence band of semiconductor 1 lies at a higher energy than that of semiconductor 2. Then the valence band offset is given by:\n\nIn real semiconductor heterojunctions, Anderson's rule fails to predict actual band offsets.\nIn Anderson's idealized model the materials are assumed to behave as they would in the limit of a large vacuum separation, yet where the vacuum separation is taken to zero.\nIt is that assumption that involves the use of the vacuum electron affinity parameter, even in a solidly filled junction where there is no vacuum.\nMuch like with the Schottky-Mott rule, Anderson's rule ignores the real chemical bonding effects that occur with a small or nonexistent vacuum separation: interface states which may have a very large electrical polarization and defect states, dislocations and other perturbations caused by imperfect crystal lattice matches.\n\nTo try to improve the accuracy of Anderson's rule, various models have been proposed.\nThe \"common anion rule\" guesses that, since the valence band is related to anionic states, materials with the same anions should have very small valence band offsets.\nTersoff proposed the presence of a dipole layer due to induced gap states, by analogy to the metal-induced gap states in a metal-semiconductor junction.\nPractically, heuristic corrections to Anderson's rule have found success in specific systems, such as the \"60:40 rule\" used for the GaAs/AlGaAs system.\n"}
{"id": "3051043", "url": "https://en.wikipedia.org/wiki?curid=3051043", "title": "Aponia", "text": "Aponia\n\n\"Aponia\" () means the absence of pain, and was regarded by the Epicureans to be the height of bodily pleasure.\n\nAs with the other Hellenistic schools of philosophy, the Epicureans believed that the goal of human life is happiness. The Epicureans defined pleasure as the absence of pain (mental and physical), and hence pleasure can only increase until the point in which pain is absent. Beyond this, pleasure cannot increase further, and indeed one cannot rationally seek bodily pleasure beyond the state of \"aponia\". For Epicurus, \"aponia\" was one of the static (\"katastematic\") pleasures, that is, a pleasure one has when there is no want or pain to be removed. To achieve such a state, one has to experience kinetic pleasures, that is, a pleasure one has when want or pain is being removed.\n\n\n"}
{"id": "2337609", "url": "https://en.wikipedia.org/wiki?curid=2337609", "title": "Black pride", "text": "Black pride\n\nBlack pride is a movement in response to dominant white cultures and ideologies that encourages black people to celebrate black culture and embrace their African heritage. In the United States, it was a direct response to white racism especially during the Civil Rights Movement. Related movements include black power, black nationalism, Black Panthers and Afrocentrism.\n\nBlack pride is a major theme in some works of African American popular musicians. Civil Rights Movement era songs such as The Impressions's hit songs \"We're a Winner\" and \"Keep on Pushing\" and James Brown's \"Say It Loud – I'm Black and I'm Proud\" celebrated black pride. Beyoncé's half-time performance at Super Bowl 50, which included homages to Malcolm X and the Black Panthers, has been described by the media as a display of black pride.\n\nBeauty standards are a major theme of black pride. Black pride was represented in slogans such as \"black is beautiful\" which challenged white beauty standards. Prior to the black pride movement, the majority of black people straightened their hair or wore wigs. The return to natural hair styles such as the afro, cornrows, and dreadlocks were seen as expressions of black pride.\n\nIn the 1960s to 1970s, kente cloth and the Black Panthers uniform were worn in the U.S. as expressions of black pride. Headscarves were sometimes worn by Nation of Islam and other Black Muslim Movement members as an expression of black pride and a symbol of faith. Other women used scarves with African prints to cover their hair.\n\nMaxine Leeds Craig argues that all-black beauty pageants such as Miss Black America were institutionalized forms of black pride created in response to exclusion from white beauty pageants.\n\nThe black pride movement is very prevalent in Brazil, especially throughout the poorer population, and it is found in the Brazilian funk music genre that began arose in the late 1960s, as also in funk carioca, that emerged in the late 1980s. Both the origin of Brazilian funk and funk carioca reflects Brazilian black resistance. Ethnomusicologist George Yúdice states that youths were engaging black culture mediated by a U.S. culture industry met with many arguments against their susceptibility to cultural colonization. Although it borrows some ingredients from hip hop, its style still remains unique to Brazil (mainly Rio de Janeiro and also São Paulo).\n\nBlack pride has been a central theme of the originally Jamaican Rastafari movement since the second half of the 20th century. It has been described as \"a rock in the face of expressions of white superiority.\"\n\nThe slogan has been used in the United States by African Americans to celebrate heritage and personal pride. The black pride movement is closely linked with the developments of the civil rights movement and Black Power movement which opposed the conditions of the United States' segregated society, and lobbied for better treatment for people of all races.\n\n"}
{"id": "26135912", "url": "https://en.wikipedia.org/wiki?curid=26135912", "title": "Bomb-making instructions on the internet", "text": "Bomb-making instructions on the internet\n\nThe availability of bomb-making instruction on the Internet has been a cause célèbre amongst lawmakers and politicians anxious to curb the Internet frontier by censoring certain types of information deemed \"dangerous\" which is available online. \"Simple\" examples of explosives created from cheap, readily available ingredients are given.\n\nThe Federal Bureau of Investigation reports that there were 1,699 criminal bombings in 1989 and 3,163 in 1994.\n\nSupporters of digital rights argue that managers of Internet traffic do not have a right to deep packet inspection, the automated system of analyzing what information is being transmitted, for example refusing to deliver a packet with the words \"bomb instructions\" and alerting authorities to the ISP that requested the information. They suggest that \"we never seem to hear\" about how the same instructions, including those for building nuclear devices, have been available in public libraries for decades without calls for censorship. In the late 19th century, Johann Most compiled Austrian military documents into a booklet demonstrating the use of explosives and distributed it at anarchist picnics without repercussion.\n\nMike Godwin, then of the Electronic Frontier Foundation, claimed that journalists have played a key role in linking the creation of \"bombs\" with \"the Internet\" in the public conscious.\n\nCritics of the prosecution of Sherman Austin, an American anarchist charged with publishing instructions on the Internet, have pointed out that the Wikipedia article on Molotov cocktails contains more detailed instructions on the construction of homemade explosives, than Austin's website did.\n\nMost American websites offering bomb-making instructions would not face civil liability, since \"Hess v. Indiana\" and \"Waller v. Osbourne\" determined that free speech restrictions can only be applied if the goal was \"producing imminent lawless conduct\" among a single target group – which is not the case for a website available to a large swath of the population – making the situation comparable to music advocating violence or suicide in its lyrics.\n\nIn 1986, prior to the widespread use of the Internet, police investigated the sharing of a computer print-out from a digital manual titled the \"Complete Book of Explosives\" written by a group calling itself \"Phoenix Force\", as students shared the list with classmates and experimented with building many of the bombs it listed.\n\nAfter the 1995 Oklahoma City bombing, anonymous usenet posts criticised the construction of the bomb, and offered suggestions on how to overcome the failure of the bomb to do its maximum intended damage. On March 23, 1996, the full text of the \"Terrorist Handbook\" was published online, including instructions on building the bomb used in the bombing, with the suggested upgrades. When Mohammed Usman Saddique was arrested in 2006, he was charged with \"possessing a document or record containing information of a kind likely to be useful to a person committing or preparing an act of terrorism\" for having a copy of the manual on CD-ROM.\n\nAlso in 1994, a thread was made on the National Rifle Association's bulletin board by a user named \"Warmaster\" that detailed how to make bombs out of baby food jars.\n\nA 1996 copy of the left-wing online German magazine \"Radikal\" hosted on a Dutch server provided detailed instructions of how to sabotage railroad lines. In March of that year, a New South Wales MP called for legislation regarding internet access for youth, following reports of a boy injuring himself while trying to follow a bomb recipe online.\n\nThrough 1998, the common view of the instructions was that they were used by curious youth anxious to build explosives simply as a dangerous experiment \"with no intention of hurting anybody\".\n\nControversy over the availability of this information on the internet started as a result of the Columbine High School Shooting. Also, police claim that they found printed copies of bomb-making instructions downloaded from the Internet in the bedroom of Anthony \"T.J.\" Solomon, the perpetrator of the 1999 Heritage High School shooting.\n\nAlso in 1999, David Copeland planted nail bombs in London, killing 3 people and injuring 139, based on techniques discussed in \"The Terrorist's Handbook\" and \"How to Make Bombs: Part Two\", which he had downloaded from the internet.\n\nThe militant anti-abortion movement Army of God also provided information on constructing bombs in preparation for anti-abortion violence on their website.\n\nIn 2001, journalists discovered that al-Qaeda members in Afghanistan had been using the internet to learn bomb-making techniques.\n\nIn Finland in 2002, \"RC\" discussed bomb-making techniques on the internet on a Finnish website whose moderator displayed a picture of his own face on Osama bin Laden's body, and then RC set off a bomb that killed seven people, including himself.\n\nIn 2002, New Zealander Bruce Simpson published \" The Low Cost Cruise Missile; A looming threat?\" showing readers how they could construct a cruise missile for under $5,000.\n\nIn 2003, Jeremy Parker of the Southern Knights of the Ku Klux Klan posted detailed bomb instructions on the internet in response to Martin Luther King Jr. Day, stating \"sure would hate to see anything happen\". \n\nThe report \"How to Bomb Thy Neighbor: Hamas Offers Online 'Academy'\" describes a Hamas online interactive 14-lesson course for Muslims on bomb-making, as part of a campaign to increase the number of bomb-makers.\n\nIn 2004, a Palestinian group posted an online video showing the proper construction of suicide vests, hoping to support the Iraqi insurgency.\n\nThe 2004 Madrid train bombers, who killed 191 people and wounded 1,800, downloaded their bomb-making instructions from the internet.\n\nThe Canadian Saad Khalid admitted that he had downloaded bomb-making materials online in 2006, leading to the 2006 Toronto terrorism case.\n\nBritish student Isa Ibrahim made a suicide vest bomb using instructions he found online. He planned on exploding the device at a mall. He was sentenced in July 2009 to a minimum of ten years in jail.\n\nNajibullah Zazi, an al-Qaeda member who pleaded guilty in February 2010 to a plot to bomb the New York City Subway system, searched online for information on how to build a bomb and where to buy the parts.\n\nIn 1995, Dianne Feinstein produced a bill to the United States Senate making it illegal to distribute bomb-making information, punishable by a $250,000 fine and 20 years' imprisonment. Two years later, the body voted 94–0 in favor of implementing it. Although it was frequently said to be in response to Timothy McVeigh's Oklahoma bombing, he had actually used two traditional hard-copy books titled \"Homemade C-4, A Recipe for Survival\" and \"Ragnar's Big Book of Homemade Weapons and Improvised Explosives\". Critics later pointed out both books were still for sale at Amazon.com, suggesting that legislators were not concerned about the true dissemination of such information. When lawsuits erupted over DeCSS technology available over the Internet, allowing users to \"crack\" DVD encryption, the founders questioned why bomb-making instructions were legal, while software cracks that simply cost corporations money were not.\n\nIn 2004, German authorities forced a seventeen-year-old to shut down his Web site, entitled \"Der Abarisch Sturm\" after he posted detailed instructions of how to build a bomb. That year, French police also arrested a computer student in Alfortville who claimed he had posted similar instructions \"for fun.\"\n\nA 2007 attempt by the European Commission to suppress bomb-making websites by making ISPs criminally liable for allowing a user to view such a page was ridiculed by \"The Register\" as \"fantastically ignorant of internet realities\"\n\nWeb sites offering advice on construction explosives are labelled as \"Refused Classification\" in Australia, as it is deemed to violate \"all acceptable community standards\".\n\n"}
{"id": "36059298", "url": "https://en.wikipedia.org/wiki?curid=36059298", "title": "Carola Stabe", "text": "Carola Stabe\n\nCarola Stabe is a former dissident and civil rights activist in East Germany GDR. She is the founder and leader of the environmental group ARGUS in Potsdam, Germany. She initiated the GDR- wide opposition network of environmental groups, which later converged in the Grüne Liga.\n\nStabe was born in 1955 in Templin, Mecklenburg. Her father, Dr. Siegfried Stabe, was detained by the Soviets in 1945. Later, he became a member of the Socialist Unity Party of Germany, a school principal and a representative in the East German People's Chamber. In 1975, during her studies of History and Russian at Humboldt University of Berlin, she was arrested because of a publication about the Soviet Gulags. From 1977 on, she worked as a teacher in Potsdam.\n\nIn April 1988, she initiated the founding of the group ARGUS (Consortium for Environmental Protection and Urban Design), which she led until December, 1989. In the summer of 1988, she began to build a network of environmental groups. As her base she used the Cultural Association of the GDR, the only place where assembly was legal, under the scrutiny of the Stasi. In April 1988 she organized the first GDR- wide meeting of environmental groups at the Cultural Association, together with Matthias Platzeck and other members of the ARGUS group in Potsdam. In June 1989, she organized, the 1. Potsdamer Pfingsbergfest, a public gathering at which the Stasi counted 3000 visitors from all over the GDR, together with Wieland Eschenburg and Matthias Platzeck. She used administrative loopholes to circumvent a prohibition of the assembly. During the gathering, oppositional groups distributed information leaflets.\n\nA poster by graphic artist Bob Bahra and an information leaflet published by ARGUS helped to stop the demolition of Potsdam’s baroque city center, which had been planned by the leadership of the SED (Socialist Unity Party of Germany). Because of her active role at Pfingstbergfest and other events that were critical of the system, Stabe was dismissed from the teaching profession in July 1989, for reasons of “counterrevolutionary activities”.\n\nOn October 7, 1989 she organized, together with Matthias Platzeck, the second GDR-wide meeting. 124 representatives of environmental groups followed an ARGUS invitation to Potsdam’s Cultural Association. The GDR leadership could not forbid the meeting. Stabe had announced it as a festivity for the 40th anniversary of the GDR. On the evening of October 7, representatives of the environmental groups signed a declaration, expressing the need for political changes and human rights in the GDR. The declaration was sent to the press in East- and West Berlin. During the meeting on October 7, a small group of dissidents around Stabe and Platzeck planted the idea for an environmental organization that would exist independently from the Cultural Association. In October 1989, the founding appeal for a “Green League” (Grüne Liga) was drafted. On November 15, 1989, Carola Stabe, Matthias Platzeck and members of the ARGUS group organized the 1. Potsdamer Umweltnacht an event at Potsdam’s Karl Liebknecht Stadium. Here, they made their call for he founding of the Grüne Liga public, in front of 3,000 people. From the middle of October 1989 on, Staube took part in talks towards the creation of a Green Party of the GDR.\n\nFollowing the fall of the Berlin Wall on November 9, 1989, she became a founding member of the GDR’s Green Party on November 24, 1989. Simultaneously, she worked in the speakers’ council of the Grüne Liga to foster the collaboration between both groups.\n\nOn December 5, 1989, she took part in the occupation of the Stasi headquarters in Potsdam. With a group of fellow dissidents she founded the Rat der Volkskontrolle, which organized the transition to a democratic system in Potsdam. At the Zentraler Runder Tisch (Central Round Table) in Berlin and at the Round Table in Potsdam she represented the Grüne Liga. She partook in the organization of the founding congresses of the Grüne Liga, as well as the Green Party. Her election into the federal board was prevented by the wrongful allegation that she had worked for the Stasi. Henry Schramm, the person who made the allegation, was later exposed as an agent of the Stasi, posing as a dissident in the opposition movement.\n\nFrom December 7 to April 30, 1990, she headed the office of Grüne Liga in Potsdam and later in Berlin. From May 1990 to July 1992 she was the executive director of Grüne Liga e. V. She supported the creation of environmental centers in the GDR’s 14 district cities and realized projects to build environmental counseling. She helped to prepare the United Nations Conference on Environment and Development in Brasil for the Federal Republic of Germany. In September 1994 she was politically and professionally rehabilitated and started to work in Brandenburg against right-wing extremism, for democratic participation and for a critical assessment of GDR history. In 2005, with Bob Bahra, she founded the Forum zur kritischen Auseinandersetzung mit der DDR-Geschichte (Forum for the Critical Exploration of GDR History). Since 2006, she works with Stefan Roloff on the production of films and art projects dealing with the history of the GDR. In 2011 she founded the Gemeinschaft der Verfolgten des DDR-Systems (Association of the Persecuted of the GDR System), together with Bob Bahra, Sibylle Schönemann and Birgit Willschütz.\n\n\n"}
{"id": "17257716", "url": "https://en.wikipedia.org/wiki?curid=17257716", "title": "Chain of events", "text": "Chain of events\n\nA chain of events is a number of actions and their effects that are contiguous and linked together that results in a particular outcome. In the physical sciences, chain reactions are a primary example. \n\n\"Determinism\" is the philosophical proposition that every event, including human cognition and behaviour, decision and action, is causally determined by an unbroken \"chain of events\". With numerous historical debates, many varieties and philosophical positions on the subject of determinism exist from traditions throughout the world.\n\nIn value theory, it is the amount of cause and effects of the chain of events before generating intrinsic value that separates high and low grades of instrumental value. The \"chain of events duration\" is the time it takes to reach the terminal event. In value theory this is generally the intrinsic value (also called terminal value). It is contrasted with ethic value duration, which is the time that an object has any value intensity.\n\nA \"fabric of events\" is an expansion of the \"chain of events\", emphasizing that chains of events are intertwined with each other, as a fabric.\n\nIn accident analysis - for example, in the analysis of aviation accidents - a chain of events (or error chain) consists of the contributing factors leading to an undesired outcome.\n\n"}
{"id": "5738671", "url": "https://en.wikipedia.org/wiki?curid=5738671", "title": "Chronocentrism", "text": "Chronocentrism\n\nChronocentrism is the assumption that certain time periods (typically the present) are better, more important, or a more significant frame of reference than other time periods, either past or future.\n\nChronocentrism (from the Greek \"chrono-\" meaning \"time\") was coined by sociologist Jib Fowles in an article in the journal \"Futures\" in February, 1974. Fowles described chronocentrism as \"the belief that one's own times are paramount, that other periods pale in comparison\". More recently, it has been defined as \"the egotism that one's own generation is poised on the very cusp of history\". The term had been used earlier in a study about attitudes to ageing in the workplace. Chronocentricity: \"...only seeing the value of one's own age cohort...described the tendency for younger managers to hold negative perceptions of the abilities or other work-related competencies of older employees.\" This type of discrimination is a form of ageism.\n\nChronocentrism as ethnocentrism is the perceiving and judging of a culture's historical values in terms of the standards of one's own time period.\n\nThe Long Now Foundation is an organization that encourages the use of 5-digit years, e.g. \"02016\" instead of \"2016,\" to help emphasize how early the present time is in their vision of the timeline of humanity. The use of two-digit years before Y2K was an example of chronocentrism (in the early years of computing, the years 2000 and 1899 were believed to be too far in the future or the past, and thus of less importance than being able to save two digits in computerizing and typing out years).\n\nThe \"Copernican time principle\" is a temporal analog of the Copernican principle for space, which states that no spatial location is any more or less special of a frame of reference than any other spatial location (i.e., that our physical universe has no center). Some authors have extended this to also include that no point in time is any more or less special than any other point in time (e.g., in outdated steady-state theories), though this cannot be universally applied (e.g., the Big-Bang singularity is a special point in time that can be logically used as a frame of reference to date later events.)\n\n"}
{"id": "3106301", "url": "https://en.wikipedia.org/wiki?curid=3106301", "title": "Cortical homunculus", "text": "Cortical homunculus\n\nA cortical homunculus is a distorted representation of the human body, based on a neurological \"map\" of the areas and proportions of the human brain dedicated to processing motor functions, or sensory functions, for different parts of the body. The word \"homunculus\" is Latin for \"little man\", and was a term used in alchemy and folklore long before scientific literature began using it. A cortical homunculus, or \"cortex man\", illustrates the concept of heuristically representing the body lying within the brain. Nerve fibres from the spinal cord terminate in various areas of the parietal lobe in the cerebral cortex, which forms a representational map of the body.\n\nA \"motor\" homunculus represents a map of brain areas dedicated to \"motor\" processing for different anatomical divisions of the body. The primary motor cortex is located in the precentral gyrus, and handles signals coming from the premotor area of the frontal lobes.\n\nA \"sensory\" homunculus represents a map of brain areas dedicated to \"sensory\" processing for different anatomical divisions of the body. The primary sensory cortex is located in the postcentral gyrus, and handles signals coming from the thalamus.\n\nThese signals are transmitted on from the gyri to the brain stem and spinal cord via corresponding nerves.\n\nAlong the length of the primary motor and sensory cortices, the areas specializing in different parts of the body are arranged in an orderly manner, although ordered differently than one might expect. The toes are represented at the top of the cerebral hemisphere (or more accurately, \"the upper end\", since the cortex curls inwards and down at the top), and then as one moves down the hemisphere, progressively higher parts of the body are represented, assuming a body that's faceless and has arms raised. Going further down the cortex, the different areas of the face are represented, in approximately top-to-bottom order, rather than bottom-to-top as before. The homunculus is split in half, with motor and sensory representations for the left side of the body on the right side of the brain, and vice versa.\n\nThe amount of cortex devoted to any given body region is not proportional to that body region's surface area or volume, but rather to how richly innervated that region is. Areas of the body with more complex and/or more numerous sensory or motor connections are represented as larger in the homunculus, while those with less complex and/or less numerous connections are represented as smaller. The resulting image is that of a distorted human body, with disproportionately huge hands, lips, and face.\n\nIn the sensory homunculus, below the areas handling sensation for the teeth, gums, jaw, tongue, and pharynx lies an area for intra-abdominal sensation. At the very top end of the primary sensory cortex, beyond the area for the toes, it has traditionally been believed that the sensory neural networks for the genitals occur. However, more recent research has suggested that there may be two different cortical areas for the genitals, possibly differentiated by one dealing with erogenous stimulation and the other dealing with non-erogenous stimulation.\n\nDr. Wilder Penfield and his co-investigators Edwin Boldrey and Theodore Rasmussen are considered to be the originators of the sensory and motor homunculi. They were not the first scientists to attempt to objectify human brain function by means of a homunculus. However, they were the first to differentiate between sensory and motor function and to map the two across the brain separately, resulting in two different homunculi. In addition, their drawings and later drawings derived from theirs became perhaps the most famous conceptual maps in modern neuroscience because they compellingly illustrated the data at a single glance.\n\nPenfield first conceived of his homunculi as a thought experiment, and went so far as to envision an imaginary world in which the homunculi lived, which he referred to as \"if\". He and his colleagues went on to experiment with electrical stimulation of different brain areas of patients undergoing open brain surgery to control epilepsy, and were thus able to produce the topographical brain maps and their corresponding homunculi.\n\nMore recent studies have improved this understanding of somatotopic arrangement using techniques such as functional magnetic resonance imaging (fMRI).\n\nPenfield referred to his creations as \"grotesque creatures\" due to their strange-looking proportions. For example, the sensory nerves arriving from the hands terminate over large areas of the brain, resulting in the hands of the homunculus being correspondingly large. In contrast, the nerves emanating from the torso or arms cover a much smaller area, thus the torso and arms of the homunculus look comparatively small and weak.\n\nPenfield's homunculi are usually shown as 2-D diagrams. This is an oversimplification, as it cannot fully show the data set Penfield collected from his brain surgery patients. Rather than the sharp delineation between different body areas shown in the drawings, there is actually significant overlap between neighboring regions. The simplification suggests that lesions of the motor cortex will give rise to specific deficits in specific muscles. However, this is a misconception, as lesions produce deficits in groups of synergistic muscles. This finding suggests that the motor cortex functions in terms of overall movements as coordinated groups of individual motions.\n\nThe sensorimotor homunculi can also be represented as 3-D figures (such as the sensory homunculus sculpted by Sharon Price-James shown from different angles below), which can make it easier for laymen to understand the ratios between the different body regions' levels of motor or sensory innervation. However, these 3-D models do not illustrate which areas of the brain are associated with which parts of the body.\n\n\n"}
{"id": "54727095", "url": "https://en.wikipedia.org/wiki?curid=54727095", "title": "Cumulative accuracy profile", "text": "Cumulative accuracy profile\n\nThe cumulative accuracy profile (CAP) is used in data science to visualize the discriminative power of a model. The CAP of a model represents the cumulative number of positive outcomes along the \"y\"-axis versus the corresponding cumulative number of a classifying parameter along the \"x\"-axis. The CAP is distinct from the receiver operating characteristic (ROC), which plots the true-positive rate against the false-positive rate.\n\nAn example is a model that predicts whether a product is bought (positive outcome) by each individual from a group of people (classifying parameter) based on factors such as their gender, age, income etc. If group members would be contacted at random, the cumulative number of products sold would rise linearly toward a maximum value corresponding to the total number of buyers within the group. This distribution is called the \"random\" CAP. A perfect prediction, on the other hand, determines exactly which group members will buy the product, such that the maximum number of products sold will be reached with a minimum number of calls. This produces a steep line on the CAP curve that stays flat once the maximum is reached (contacting all other group members will not lead to more products sold), which is the \"perfect\" CAP.\n\nA successful model predicts the likelihood of individuals purchasing the product and ranks these probabilities to produce a list of potential customers to be contacted first. The resulting cumulative number of sold products will increase rapidly and eventually flatten out to the given maximum as more group members are contacted. This results in a distribution that lies between the random and the perfect CAP curves.\n\nThe CAP can be used to evaluate a model by comparing the curve to the perfect CAP in which the maximum number of positive outcomes is achieved directly and to the random CAP in which the positive outcomes are distributed equally. A good model will have a CAP between the perfect CAP and the random CAP with a better model tending to the perfect CAP.\n\nThe accuracy ratio (AR) is defined as the ratio of the area between the model CAP and the random CAP and the area between the perfect CAP and the random CAP. For a successful model the AR has values between zero and one, with a higher value for a stronger model.\n\nAnother indication of the model strength is given by the cumulative number of positive outcomes at 50% of the classifying parameter. For a successful model this value should lie between 50% and 100% of the maximum, with a higher percentage for stronger models.\n\nThe CAP and the ROC are both commonly used by banks and regulators to analyze the discriminatory ability of rating systems that evaluate the credit risks \n"}
{"id": "14534297", "url": "https://en.wikipedia.org/wiki?curid=14534297", "title": "Deviance (sociology)", "text": "Deviance (sociology)\n\nIn sociology, deviance describes an action or behavior that violates social norms, including a formally enacted rule (e.g., crime), as well as informal violations of social norms (e.g., rejecting folkways and mores). Although deviance may have a negative connotation, the violation of social norms is not always a negative action; positive deviation exists in some situations. Although a norm is violated, a behavior can still be classified as positive or acceptable. \n\nSocial norms differ from culture to culture. For example, a deviant act can be committed in one society but may be normal for another society.\n\nDeviance is relative to the place where it was committed or to the time the act took place. Killing another human is considered wrong, except when governments permit it during warfare or for self defense. There are two types of major deviant actions, \"mala in se\" or \"mala prohibits types.\"\n\nDeviant acts can be assertions of individuality and identity, and thus as rebellion against group norms of the dominant culture and in favor of a sub-culture.\n\nDeviance affirms cultural values and norms. It also clarifies moral boundaries, promotes social unity by creating an us/them dichotomy, encourages social change, and provides jobs to control deviance. \"Certain factors of personality are theoretically and empirically related to workplace deviance, such as work environment, and individual differences.\"\n\nThree broad sociological classes exist that describe deviant behavior, namely, structural functionalism, symbolic interaction and conflict theory.\n\nSocial integration is the attachment to groups and institutions, while social regulation is the adherence to the norms and values of society. Those who are very integrated fall under the category of \"altruism\" and those who are not very integrated fall under \"egotism.\" Similarly, those who are very regulated fall under \"fatalism\" and those who are very unregulated fall under \"anomie\".\nDurkheim's theory attributes social deviance to extremes of the dimensions of the social bond. Altruistic suicide (death for the good of the group), egoistic suicide (death for the removal of the self-due to or justified by the lack of ties to others), and anomic suicide (death due to the confounding of self-interest and societal norms) are the three forms of suicide that can happen due to extremes. Likewise, individuals may commit crimes for the good of an individual's group, for the self-due to or justified by lack of ties, or because the societal norms that place the individual in check no longer have power due to society's corruption.\n\nDurkheim (1858–1917) claimed that deviance was in fact a normal and necessary part of social organization. When he studied deviance he stated four important functions of deviance.\n\n\nRobert K. Merton discussed deviance in terms of goals and means as part of his strain/anomie theory. Where Durkheim states that anomie is the confounding of social norms, Merton goes further and states that anomie is the state in which social goals and the legitimate means to achieve them do not correspond. He postulated that an individual's response to societal expectations and the means by which the individual pursued those goals were useful in understanding deviance. Specifically, he viewed collective action as motivated by strain, stress, or frustration in a body of individuals that arises from a disconnection between the society's \"goals\" and the popularly used \"means\" to achieve those goals. Often, non-routine collective behavior (rioting, rebellion, etc.) is said to map onto economic explanations and causes by way of strain. These two dimensions determine the adaptation to society according to the cultural goals, which are the society's perceptions about the ideal life, and to the institutionalized means, which are the legitimate means through which an individual may aspire to the cultural goals.\n\nMerton described 5 types of deviance in terms of the acceptance or rejection of social goals and the institutionalized means of achieving them:\n\n1. Innovation is a response due to the strain generated by our culture's emphasis on wealth and the lack of opportunities to get rich, which causes people to be \"innovators\" by engaging in stealing and selling drugs. Innovators accept society's goals, but reject socially acceptable means of achieving them. (e.g.: monetary success is gained through crime). Merton claims that innovators are mostly those who have been socialised with similar world views to conformists, but who have been denied the opportunities they need to be able to legitimately achieve society's goals.\n\n2. Conformists accept society's goals and the socially acceptable means of achieving them (e.g.: monetary success is gained through hard work). Merton claims that conformists are mostly middle-class people in middle class jobs who have been able to access the opportunities in society such as a better education to achieve monetary success through hard work.\n\n3. Ritualism refers to the inability to reach a cultural goal thus embracing the rules to the point where the people in question lose sight of their larger goals in order to feel respectable. Ritualists reject society's goals, but accept society's institutionalised means. Ritualists are most commonly found in dead-end, repetitive jobs, where they are unable to achieve society's goals but still adhere to society's means of achievement and social norms.\n\n4. Retreatism is the rejection of both cultural goals and means, letting the person in question \"drop out\". Retreatists reject the society's goals and the legitimate means to achieve them. Merton sees them as true deviants, as they commit acts of deviance to achieve things that do not always go along with society's values.\n\n5. Rebellion is somewhat similar to retreatism, because the people in question also reject both the cultural goals and means, but they go one step further to a \"counterculture\" that supports other social orders that already exist (rule breaking). Rebels reject society's goals and legitimate means to achieve them, and instead creates new goals and means to replace those of society, creating not only new goals to achieve but also new ways to achieve these goals that other rebels will find acceptable.\n\nSymbolic Interaction, refers to the patterns of communication, interpretation and adjustment between individuals. Both the verbal and nonverbal responses that a listener then delivers are similarly constructed in expectation of how the original speaker will react. The ongoing process is like the game of charades, only it’s a full-fledged conversation.\n\nThe term \"symbolic interactionism\" has come into use as a label for a relatively distinctive approach to the study of human life and human conduct. (Blumer, 1969). With Symbolic interactionism, reality is seen as social, developed interaction with others. Most symbolic interactionists believe a physical reality does indeed exist by an individual's social definitions, and that social definitions do develop in part or relation to something “real.” People thus do not respond to this reality directly, but rather to the social understanding of reality. Humans therefore exist in three realities: a physical objective reality, a social reality, and a unique. A unique is described as a third reality created out of the social reality, a private interpretation of the reality that is shown to the person by others (Charon, 2007). Both individuals and society cannot be separated far from each other for two reasons. One, being that both are created through social interaction, and two, one cannot be understood in terms without the other. Behavior is not defined by forces from the environment such as drives, or instincts, but rather by a reflective, socially understood meaning of both the internal and external incentives that are currently presented (Meltzer et al., 1975).\n\nHerbert Blumer (1969) set out three basic premises of the perspective:\n\n\nIn his differential association theory, Edwin Sutherland posited that criminals learn criminal and deviant behaviors and that deviance is not inherently a part of a particular individual's nature. When an individual's significant others engage in deviant and/or criminal behavior, criminal behavior will be learned as a result to this exposure. Also, he argues that criminal behavior is learned in the same way that all other behaviors are learned, meaning that the acquisition of criminal knowledge is not unique compared to the learning of other behaviors.\n\nSutherland outlined some very basic points in his theory, including the idea that the learning comes from the interactions between individuals and groups, using communication of symbols and ideas. When the symbols and ideas about deviation are much more favorable than unfavorable, the individual tends to take a favorable view upon deviance and will resort to more of these behaviors.\n\nCriminal behavior (motivations and technical knowledge), as with any other sort of behavior, is learned. Some basic assumptions include:\n\n\nOne example of this would be gang activity in inner city communities. Sutherland would feel that because a certain individual's primary influential peers are in a gang environment, it is through interaction with them that one may become involved in crime.\n\nGresham Sykes and David Matza's neutralization theory explains how deviants justify their deviant behaviors by providing alternative definitions of their actions and by providing explanations, to themselves and others, for the lack of guilt for actions in particular situations.\n\nThere are five types of neutralization:\n\n\nFrank Tannenbaum and Howard S. Becker created and developed the labeling theory, which is a core facet of symbolic interactionism, and often referred to as Tannenbaum's \"dramatization of evil\". Becker believed that \"social groups create deviance by making the rules whose infraction constitutes deviance\".\n\nLabeling is a process of social reaction by the \"social audience\", (stereotyping) the people in society exposed to, judging and accordingly defining (labeling) someone's behavior as deviant or otherwise. It has been characterized as the \"invention, selection, manipulation of beliefs which define conduct in a negative way and the selection of people into these categories [...]\"\n\nLabeling theory, consequently, suggests that deviance is caused by the deviant's being labeled as morally inferior, the deviant's internalizing the label and finally the deviant's acting according to that specific label (in other words, you label the \"deviant\" and they act accordingly). As time goes by, the \"deviant\" takes on traits that constitute deviance by committing such deviations as conform to the label (so you as the audience have the power to not label them and you have the power to stop the deviance before it ever occurs by not labeling them). Individual and societal preoccupation with the label, in other words, leads the deviant individual to follow a self-fulfilling prophecy of abidance to the ascribed label.\n\nThis theory, while very much symbolically interactionist, also has elements of conflict theory, as the dominant group has the power to decide what is deviant and acceptable, and enjoys the power behind the labeling process. An example of this is a prison system that labels people convicted of theft, and because of this they start to view themselves as by definition thieves, incapable of changing. \"From this point of view,\" as Howard S. Becker has written,\n\nDeviance is \"not\" a quality of the act the person commits, but rather a consequence of the application by others of rules and sanctions to an \"offender\". The deviant is one to whom the label has successfully been applied; deviant behavior is behavior that people so label.\n\nIn other words, \"Behavior only becomes deviant or criminal if defined and interfered as such by specific people in [a] specific situation.\" It is important to note the salient fact that society is not always correct in its labeling, often falsely identifying and misrepresenting people as deviants, or attributing to them characteristics which they do not have. In legal terms, people are often wrongly accused, yet many of them must live with the ensuant stigma (or conviction) for the rest of their lives.\n\nOn a similar note, society often employs double standards, with some sectors of society enjoying favouritism. Certain behaviors in one group are seen to be perfectly acceptable, or can be easily overlooked, but in another are seen, by the same audiences, as abominable.\n\nThe medicalization of deviance, \"the transformation of moral and legal deviance into a medical condition\", is an important shift that has transformed the way society views deviance. The labelling theory helps to explain this shift, as behaviour that used to be judged morally are now being transformed into an objective clinical diagnosis. For example, people with drug addictions are considered \"sick\" instead of \"bad\".\n\nEdwin Lemert developed the idea of primary and secondary deviation as a way to explain the process of labeling. Primary deviance is any general deviance before the deviant is labeled as such in a particular way. Secondary deviance is any action that takes place after primary deviance as a reaction to the institutional identification of the person as a deviant.\n\nWhen an actor commits a crime (primary deviance), however mild, the institution will bring social penalties down on the actor. However, punishment does not necessarily stop crime, so the actor might commit the same primary deviance again, bringing even harsher reactions from the institutions. At this point, the actor will start to resent the institution, while the institution brings harsher and harsher repression. Eventually, the whole community will stigmatize the actor as a deviant and the actor will not be able to tolerate this, but will ultimately accept his or her role as a criminal, and will commit criminal acts that fit the role of a criminal.\n\nPrimary And Secondary Deviation is what causes people to become harder criminals. Primary deviance is the time when the person is labeled deviant through confession or reporting. Secondary deviance is deviance before and after the primary deviance. Retrospective labeling happens when the deviant recognizes his acts as deviant prior to the primary deviance, while prospective labeling is when the deviant recognizes future acts as deviant.\nThe steps to becoming a criminal are:\n\n\nControl theory advances the proposition that weak bonds between the individual and society free people to deviate. By contrast, strong bonds make deviance costly. This theory asks why people refrain from deviant or criminal behavior, instead of why people commit deviant or criminal behavior, according to Travis Hirschi. The control theory developed when norms emerge to deter deviant behavior. Without this \"control\", deviant behavior would happen more often. This leads to conformity and groups. People will conform to a group when they believe they have more to gain from conformity than by deviance. If a strong bond is achieved there will be less chance of deviance than if a weak bond has occurred. Hirschi argued a person follows the norms because they have a bond to society. The bond consists of four positively correlated factors: opportunity, attachment, belief, and involvement. When any of these bonds are weakened or broken one is more likely to act in defiance. Michael Gottfredson and Travis Hirschi in 1990 founded their Self-Control Theory. It stated that acts of force and fraud are undertaken in the pursuit of self-interest and self-control. A deviant act is based on a criminals own self-control of themselves.\n\nContainment theory is considered by researchers such as Walter C. Reckless to be part of the control theory because it also revolves around the thoughts that stop individuals from engaging in crime. Reckless studied the unfinished approaches meant to explain the reasoning behind delinquency and crime. He recognized that societal disorganization is included in the study of delinquency and crime under social deviance, leading him to claim that the majority of those who live in unstable areas tend not to have criminal tendencies in comparison those who live in middle-class areas. This claim opens up more possible approaches to social disorganization, and proves that the already implemented theories are in need or a deeper connection to further explore ideas of crime and delinquency. These observations brought Reckless to ask questions such as, \"Why do some persons break through the tottering (social) controls and others do not? Why do rare cases in well-integrated society break through the lines of strong controls?\" Reckless asserted that the intercommunication between self-control and social controls are partly responsible for the development of delinquent thoughts. Social disorganization was not related to a particular environment, but instead was involved in the deterioration of an individuals social controls. The containment theory is the idea that everyone possesses mental and social safeguards which protect the individual from committing acts of deviancy. Containment depends on the individuals ability to separate inner and outer controls for normative behavior.\n\nMore contemporary control theorists such as Robert Crutchfield take the theory into a new light, suggesting labor market experiences not only affect the attitudes and the \"stakes\" of individual workers, but can also affect the development of their children's views toward conformity and cause involvement in delinquency. This is an ongoing study as he has found a significant relationship between parental labor market involvement and children's delinquency, but has not empirically demonstrated the mediating role of parents' or children's attitude. In a study conducted by Tim Wadsworth, the relationship between parent's employment and children's delinquency, which was previously suggested by Crutchfield (1993), was shown empirically for the first time. The findings from this study supported the idea that the relationship between socioeconomic status and delinquency might be better understood if the quality of employment and its role as an informal social control is closely examined.\n\nIn sociology, conflict theory states that society or an organization functions so that each individual participant and its groups struggle to maximize their benefits, which inevitably contributes to social change such as political changes and revolutions. Deviant behaviors are actions that do not go along with the social institutions as what cause deviance. The institution's ability to change norms, wealth or status comes into conflict with the individual. The legal rights of poor folks might be ignored, middle class are also accept; they side with the elites rather than the poor, thinking they might rise to the top by supporting the status quo. Conflict theory is based upon the view that the fundamental causes of crime are the social and economic forces operating within society. However, it explains white-collar crime less well.\n\nThis theory also states that the powerful define crime. This raises the question: for whom is this theory functional? In this theory, laws are instruments of oppression: tough on the powerless and less tough on the powerful.\n\nMarx did not write about deviant behavior but he wrote about alienation amongst the proletariat—as well as between the proletariat and the finished product—which causes conflict, and thus deviant behavior.\n\nMany Marxist writers have the theory of the capitalist state in their arguments. For example, Steven Spitzer utilized the theory of bourgeois control over social junk and social dynamite; George Rusche was known to present analysis of different punishments correlated to the social capacity and infrastructure for labor. He theorized that throughout history, when more labor is needed, the severity of punishments decreases and the tolerance for deviant behavior increases. Jock Young, another Marxist writer, presented the idea that the modern world did not approve of diversity, but was not afraid of social conflict. The late modern world, however, is very tolerant of diversity. But is extremely afraid of social conflicts, which is an explanation given for the political correctness movement. The late modern society easily accepts difference, but it labels those that it does not want as deviant and relentlessly punishes and persecutes.\n\nMichel Foucault believed that torture had been phased out from modern society due to the dispersion of power; there was no need any more for the wrath of the state on a deviant individual. Rather, the modern state receives praise for its fairness and dispersion of power which, instead of controlling each individual, controls the mass.\n\nHe also theorized that institutions control people through the use of discipline.\"Race and ethnicity could be relevant to an understanding of prison rule breaking if inmates bring their ecologically structured beliefs regarding legal authority, crime and deviance into the institutional environment.\" For example, the modern prison (more specifically the panopticon) is a template for these institutions because it controls its inmates by the perfect use of discipline.\n\nFoucault theorizes that, in a sense, the postmodern society is characterized by the lack of free will on the part of individuals. Institutions of knowledge, norms, and values, are simply in place to categorize and control humans.\n\nPraveen Attri claims genetic reasons to be largely responsible for social deviance. The Italian school of criminology contends that biological factors may contribute to crime and deviance. Cesare Lombroso was among the first to research and develop the Theory of Biological Deviance which states that some people are genetically predisposed to criminal behavior. He believed that criminals were a product of earlier genetic forms. The main influence of his research was Charles Darwin and his Theory of Evolution. Lombroso theorized that people were born criminals or in other words, less evolved humans who were biologically more related to our more primitive and animalistic urges. From his research, Lombroso took Darwin's Theory and looked at primitive times himself in regards to deviant behaviors. He found that the skeletons that he studied mostly had low foreheads and protruding jaws. These characteristics resembled primitive beings such as Homo Neanderthalensis. He stated that little could be done to cure born criminals because their characteristics were biologically inherited. Over time, most of his research was disproved. His research was refuted by Pearson and Charles Goring. They discovered that Lombroso had not researched enough skeletons to make his research thorough enough. When Pearson and Goring researched skeletons on their own they tested many more and found that the bone structure had no relevance in deviant behavior. The statistical study that Charles Goring published on this research is called \"The English Convict\".\n\nThe Classical school of criminology comes from the works of Cesare Beccaria and Jeremy Bentham. Beccaria assumed a utilitarian view of society along with a social contract theory of the state. He argued that the role of the state was to maximize the greatest possible utility to the maximum number of people and to minimize those actions that harm the society. He argued that deviants commit deviant acts (which are harmful to the society) because of the utility it gives to the private individual. If the state were to match the pain of punishments with the utility of various deviant behaviors, the deviant would no longer have any incentive to commit deviant acts. (Note that Beccaria argued for \"just\" punishment; as raising the severity of punishments without regard to logical measurement of utility would cause increasing degrees of social harm once it reached a certain point.)\n\nCross-cultural communication is a field of study that looks at how people from different cultural backgrounds endeavor to communicate. All cultures make use of nonverbal communication but its meaning varies across cultures. In one particular country, a non-verbal sign may stand for one thing, and mean something else in another culture or country. The relation of cross-cultural communication with deviance is that a sign may be offensive to one in one culture and mean something completely appropriate in another. This is an important field of study because as educators, business employees, or any other form of career that consists of communicating with ones from other cultures, you need to understand non-verbal signs and their meanings, so you avoid offensive conversation or misleading conversation. Below is a list of non-verbal gestures that are appropriate in one country, and that would be considered deviant in another.\nThese are just a few non-verbal cross-cultural communication signs of which one should be aware. Cross-Cultural communication can make or break a business deal, or even prevent an educator from offending a student. Different cultures have different methods of communication, so it is important to understand the cultures of others.\n\nShaving of heads after death of a family member is more common in some African cultures.\n\nProponents of the theory of a Southern culture of honor hold that violent behavior which would be considered criminal in most of the United States, may be considered a justifiable response to insult in a Southern culture of honor.\n\nTaboo is a strong social form of behavior considered deviant by a majority. To speak of it publicly is condemned, and therefore, almost entirely avoided. The term “taboo” comes from the Tongan word “tapu” meaning \"under prohibition\", \"not allowed\", or \"forbidden\". Some forms of taboo are prohibited under law and transgressions may lead to severe penalties. Other forms of taboo result in shame, disrespect and humiliation. Taboo is not universal but does occur in the majority of societies. Some of the examples include murder, rape, incest, or child molestation.\n\nHoward Becker, a labeling theorist, identified four different types of deviant behavior labels which are given as:\n\n\nPolice: The police maintain public order by enforcing the law. Police use personal discretion in deciding whether and how to handle a situation. Research suggests that police are more likely to make an arrest if the offence is serious, if bystanders are present, or if the suspect is of a visible minority.\n\nCourts: Courts rely on an adversarial process in which attorneys-one representing the defendant and one representing the Crown-present their cases in the presence of a judge who monitors legal procedures. In practice, courts resolve most cases through plea bargaining. Though efficient, this method puts less powerful people at a disadvantage.\n\nPunishment: There are four jurisdictions for punishment: retribution, deterrence, rehabilitation, societal protection. Community-based corrections include probation and parole. These programs lower the cost of supervising people convicted of crimes and reduce prison overcrowding but have not been shown to reduce recidivism.\n\nMany works of literature offer allegories illustrating the conflict between character and society, in which the character does not conform to the society's norms and is subsequently alienated, ostracized, socially sanctioned, discriminated against or persecuted.\n\n\nA Conceptual Overview of Deviance and Its Implication to Mental Health: a Bio Psychosocial Perspective\n\nSocial Monitoring Matters for Deterring Social Deviance in Stable but Not Mobile Socio-Ecological Contexts\n\nThe Impact of Social Structures on Deviant Behaviors: The Study of 402 High Risk Street Drug Users in Iran\n"}
{"id": "18978563", "url": "https://en.wikipedia.org/wiki?curid=18978563", "title": "Domestic violence", "text": "Domestic violence\n\nDomestic violence (also named domestic abuse or family violence) is violence or other abuse by one person against another in a domestic setting, such as in marriage or cohabitation. It may be termed \"intimate partner violence\" when committed by a spouse or partner in an intimate relationship against the other spouse or partner, and can take place in heterosexual or same-sex relationships, or between former spouses or partners. Domestic violence can also involve violence against children, parents, or the elderly. It takes a number of forms, including physical, verbal, emotional, economic, religious, reproductive, and sexual abuse, which can range from subtle, coercive forms to marital rape and to violent physical abuse such as choking, beating, female genital mutilation, and acid throwing that results in disfigurement or death. Domestic murders include stoning, bride burning, honor killings, and dowry deaths.\n\nGlobally, the victims of domestic violence are overwhelmingly women, and women tend to experience more severe forms of violence. They are also likelier than men to use intimate partner violence in self-defense. In some countries, domestic violence is often seen as justified, particularly in cases of actual or suspected infidelity on the part of the woman, and is legally permitted. Research has established that there exists a direct and significant correlation between a country's level of gender equality and rates of domestic violence, where countries with less gender equality experience higher rates of domestic violence. Domestic violence is among the most underreported crimes worldwide for both men and women. Due to social stigmas regarding male victimization, men face an increased likelihood of being overlooked by healthcare providers.\n\nDomestic violence often occurs when the abuser believes that abuse is an entitlement, acceptable, justified, or unlikely to be reported. It may produce an intergenerational cycle of abuse in children and other family members, who may feel that such violence is acceptable or condoned. Many people do not recognize themselves as abusers or victims because they may consider their experiences as family conflicts that got out of control. Awareness, perception, definition and documentation of domestic violence differs widely from country to country. Domestic violence often happens in the context of forced or child marriage.\n\nIn abusive relationships, there may be a cycle of abuse during which tensions rise and an act of violence is committed, followed by a period of reconciliation and calm. Victims of domestic violence may be trapped in domestic violent situations through isolation, power and control, traumatic bonding to the abuser, cultural acceptance, lack of financial resources, fear, shame, or to protect children. As a result of abuse, victims may experience physical disabilities, dysregulated aggression, chronic health problems, mental illness, limited finances, and poor ability to create healthy relationships. Victims may experience severe psychological disorders, such as post-traumatic stress disorder. Children who live in a household with violence often show psychological problems from an early age, such as avoidance, hypervigilance to threats, and dysregulated aggression which may contribute to vicarious traumatization.\n\nThe first known use of the term domestic violence in a modern context, meaning violence in the home, was in an address to the Parliament of the United Kingdom by Jack Ashley in 1973. The term previously referred primarily to civil unrest, violence from within a country as opposed to violence perpetrated by a foreign power.\n\nTraditionally, domestic violence (DV) was mostly associated with physical violence. Terms such as \"wife abuse\", \"wife beating\", and \"wife\" \"battering\" were used, but have declined in popularity due to efforts to include unmarried partners, abuse other than physical, female perpetrators, and same-sex relationships. Domestic violence is now commonly defined broadly to include \"all acts of physical, sexual, psychological or economic violence\" that may be committed by a family member or intimate partner.\n\nThe term \"intimate partner violence\" is often used synonymously with \"domestic abuse\" or \"domestic violence\", but it specifically refers to violence occurring within a couple relationship (i.e., marriage, cohabitation, or non-cohabitating intimate partners). To these, the World Health Organization (WHO) adds controlling behaviors as a form of abuse. Intimate partner violence has been observed in opposite and same-sex relationships, and in the former instance by both men against women and women against men. \"Family violence\" is a broader term, often used to include child abuse, elder abuse, and other violent acts between family members.\nIn 1993, The United Nations Declaration on the Elimination of Violence Against Women defined domestic violence as: Physical, sexual and psychological violence occurring in the family, including battering, sexual abuse of female children in the household, dowry-related violence, marital rape, female genital mutilation and other traditional practices harmful to women, non-spousal violence and violence related to exploitation.\n\nPrior to the mid-1800s, most legal systems viewed wife beating as a valid exercise of a husband's authority over his wife. One exception, however, was the 1641 Body of Liberties of the Massachusetts Bay colonists, which declared that a married woman should be \"free from bodilie correction or stripes by her husband.\"\n\nPolitical agitation and the first-wave feminist movement during the 19th century led to changes in both popular opinion and legislation regarding domestic violence within the United Kingdom, the United States and other countries. In 1850, Tennessee became the first state in the United States to explicitly outlaw wife beating. Other states soon followed. In 1878, the UK Matrimonial Causes Act made it possible for women in the UK to seek legal separation from an abusive husband. By the end of the 1870s, most courts in the United States had rejected a claimed right of husbands to physically discipline their wives. By the early 20th century, it was common for police to intervene in cases of domestic violence in the United States, but arrests remained rare.\n\nIn most legal systems around the world, domestic violence has been addressed only from the 1990s onwards; indeed, before the late-20th century, in most countries there was very little protection, in law or in practice, against DV. In 1993, the UN published \"Strategies for Confronting Domestic Violence: A Resource Manual\". This publication urged countries around the world to treat DV as a criminal act, stated that the right to a private family life does not include the right to abuse family members, and acknowledged that, at the time of its writing, most legal systems considered DV to be largely outside the scope of the law, describing the situation at that time as follows: \"Physical discipline of children is allowed and, indeed, encouraged in many legal systems and a large number of countries allow moderate physical chastisement of a wife or, if they do not do so now, have done so within the last 100 years. Again, most legal systems fail to criminalize circumstances where a wife is forced to have sexual relations with her husband against her will. [...] Indeed, in the case of violence against wives, there is a widespread belief that women provoke, can tolerate or even enjoy a certain level of violence from their spouses.\"\nIn recent decades, there has been a call for the end of legal impunity for domestic violence, an impunity often based on the idea that such acts are private. The Istanbul Convention is the first legally binding instrument in Europe dealing with domestic violence and violence against women. The convention seeks to put an end to the toleration, in law or in practice, of violence against women and DV. In its explanatory report it acknowledges the long tradition of European countries of ignoring, \"de jure\" or \"de facto\", these forms of violence. At para 219, it states: \"There are many examples from past practice in Council of Europe member states that show that exceptions to the prosecution of such cases were made, either in law or in practice, if victim and perpetrator were, for example, married to each other or had been in a relationship. The most prominent example is rape within marriage, which for a long time had not been recognised as rape because of the relationship between victim and perpetrator.\"\n\nThere has been increased attention given to specific forms of domestic violence, such as honor killings, dowry deaths, and forced marriages. India has, in recent decades, made efforts to curtail dowry violence: the Protection of Women from Domestic Violence Act (PWDVA) was enacted in 2005, following years of advocacy and activism by the women's organizations. Crimes of passion in Latin America, a region which has a history of treating such killings with extreme leniency, have also come to international attention. In 2002, Widney Brown, advocacy director for Human Rights Watch, argued that there are similarities between the dynamics of crimes of passion and honor killings, stating that: \"crimes of passion have a similar dynamic [to honor killings] in that the women are killed by male family members and the crimes are perceived as excusable or understandable\".\n\nHistorically, children had few protections from violence by their parents, and in many parts of the world, this is still the case. For example, in Ancient Rome, a father could legally kill his children. Many cultures have allowed fathers to sell their children into slavery. Child sacrifice was also a common practice. Child maltreatment began to garner mainstream attention with the publication of \"The Battered Child Syndrome\" by pediatric psychiatrist C. Henry Kempe. Prior to this, injuries to children—even repeated bone fractures—were not commonly recognized as the results of intentional trauma. Instead, physicians often looked for undiagnosed bone diseases or accepted parents' accounts of accidental mishaps such as falls or assaults by neighborhood bullies.\n\nNot all domestic violence is equivalent. Differences in frequency, severity, purpose, and outcome are all significant. Domestic violence can take many forms, including physical aggression or assault (hitting, kicking, biting, shoving, restraining, slapping, throwing objects, beating up, etc.), or threats thereof; sexual abuse; controlling or domineering; intimidation; stalking; passive/covert abuse (e.g., neglect); and economic deprivation. It can also mean endangerment, criminal coercion, kidnapping, unlawful imprisonment, trespassing, and harassment.\n\nPhysical abuse is that involving contact intended to cause fear, pain, injury, other physical suffering or bodily harm. In the context of coercive control, physical abuse is to control the victim. The dynamics of physical abuse in a relationship are often complex. Physical violence can be the culmination of other abusive behavior, such as threats, intimidation, and restriction of victim self-determination through isolation, manipulation and other limitations of personal freedom. Denying medical care, sleep deprivation, and forced drug or alcohol use, are also forms of physical abuse. It can also include inflicting physical injury onto other targets, such as children or pets, in order to cause emotional harm to the victim.\n\nStrangulation in the context of DV has received significant attention. It is now recognized as one of the most lethal forms of DV; yet, because of the lack of external injuries, and the lack of social awareness and medical training in regard to it, strangulation has often been a hidden problem. As a result, in recent years, many US states have enacted specific laws against strangulation.\n\nHomicide as a result of domestic violence makes up a greater proportion of female homicides than it does male homicides. More than 50% of female homicides are committed by former or current intimate partners in the US. In the United Kingdom, 37 percent of murdered women were killed by an intimate partner compared to 6 percent for men. Between 40 and 70 percent of women murdered in Canada, Australia, South Africa, Israel and the United States were killed by an intimate partner. The World Health Organization states that globally, about 38% of female homicides are committed by an intimate partner.\n\nDuring pregnancy, a woman is at higher risk to be abused or long-standing abuse may change in severity, causing negative health effects to the mother and fetus. Pregnancy can also lead to a hiatus of domestic violence when the abuser does not want to harm the unborn child. The risk of domestic violence for women who have been pregnant is greatest immediately after childbirth.\nAcid attacks, are an extreme form of violence in which acid is thrown at the victims, usually their faces, resulting in extensive damage including long-term blindness and permanent scarring. These are commonly a form of revenge against a woman for rejecting a marriage proposal or sexual advance.\n\nIn the Middle East and other parts of the world, planned domestic homicides, or honor killings, are carried out due to the belief of the perpetrators that the victim has brought dishonor upon the family or community. According to Human Rights Watch, honor killings are generally performed against women for \"refusing to enter into an arranged marriage, being the victim of a sexual assault, seeking a divorce\" or being accused of committing adultery. In some parts of the world, where there is a strong social expectation for a woman to be a virgin prior to marriage, a bride may be subjected to extreme violence, including an honor killing, if she is deemed not to be a virgin on her wedding night due to the absence of blood.\n\nBride burning or dowry killing is a form of domestic violence in which a newly married woman is killed at home by her husband or husband's family due to their dissatisfaction over the dowry provided by her family. The act is often a result of demands for more or prolonged dowry after the marriage. Dowry violence is most common in South Asia, especially in India. In 2011, the National Crime Records Bureau reported 8,618 dowry deaths in India, but unofficial figures estimate at least three times this amount.\n\nSexual abuse, is defined by World Health Organization as any sexual act, attempt to obtain a sexual act, unwanted sexual comments or advances, or acts to traffic, or otherwise directed, against a person’s sexuality using coercion. It also includes obligatory inspections for virginity and female genital mutilation. Aside from initiation of the sexual act through physical force, sexual abuse occurs if a person is verbally pressured into consenting, unable to understand the nature or condition of the act, unable to decline participation, or unable to communicate unwillingness to engage in the sexual act. This could be because of underage immaturity, illness, disability, or the influence of alcohol or other drugs, or due to intimidation or pressure.\n\nIn many cultures, victims of rape are considered to have brought 'dishonour' or 'disgrace' to their families and face severe familial violence, including honor killings. This is especially the case if the victim becomes pregnant.\n\nFemale genital mutilation is defined by WHO as \"all procedures that involve partial or total removal of the external female genitalia, or other injury to the female genital organs for non-medical reasons.\" This procedure has been performed on more than 125 million females alive today, and it is concentrated in 29 countries in Africa and Middle East.\n\nIncest, or sexual contact between an adult and a child, is one form of familial sexual violence. In some cultures, there are ritualized forms of child sexual abuse taking place with the knowledge and consent of the family, where the child is induced to engage in sexual acts with adults, possibly in exchange for money or goods. For instance, in Malawi some parents arrange for an older man, often called \"hyena\", to have sex with their daughters as a form of initiation. The Council of Europe Convention on the Protection of Children against Sexual Exploitation and Sexual Abuse was the first international treaty to address child sexual abuse occurring within the home or family.\n\nReproductive coercion (also called \"coerced reproduction\") are threats or acts of violence against a partner's reproductive rights, health and decision-making; and includes a collection of behaviors intended to pressure or coerce a partner into becoming pregnant or ending a pregnancy. Reproductive coercion is associated with forced sex, fear of or inability to make contraceptive decision, fear of violence after refusing sex, and abusive partner interference with access to healthcare.\n\nIn some cultures, marriage imposes a social obligation for women to reproduce. In northern Ghana, for example, payment of bride price signifies a woman's requirement to bear children, and women using birth control face threats of violence and reprisals.\nWHO includes forced marriage, cohabitation, and pregnancy including wife inheritance within its definition of sexual violence. Wife inheritance, or levirate marriage, is a type of marriage in which the brother of a deceased man is obliged to marry his widow, and the widow is obliged to marry her deceased husband's brother.\n\nMarital rape is non-consensual penetration perpetrated against a spouse. It is under-reported, under-prosecuted, and legal in many countries, due in part to the belief that through marriage, a woman gives irrevocable consent for her husband to have sex with her when he wishes. In Lebanon, for instance, while discussing a proposed law that would criminalize marital rape, Sheik Ahmad Al-Kurdi, a judge in the Sunni religious court, said that the law \"could lead to the imprisonment of the man where in reality he is exercising the least of his marital rights.\" Feminists have worked systematically since the 1960s to criminalize marital rape internationally. In 2006, a study by the United Nations found that marital rape was a prosecutable offense in at least 104 countries Once widely condoned or ignored by law and society, marital rape is now repudiated by international conventions and increasingly criminalized. The countries which ratified the Council of Europe Convention on preventing and combating violence against women and domestic violence, the first legally binding instrument in Europe in the field of violence against women, are bound by its provisions to ensure that non-consensual sexual acts committed against a spouse or partner are illegal. The convention came into force in August 2014.\n\nEmotional abuse (or psychological abuse) is a pattern of behavior that threatens, intimidates, dehumanizes or systematically undermines self-worth. According to the Istanbul Convention, psychological violence is \"the intentional conduct of seriously impairing a person’s psychological integrity through coercion or threats\".\n\nEmotional abuse includes minimising, threats, isolation, public humiliation, unrelenting criticism, constant personal devaluation, repeated stonewalling and gaslighting. Stalking is a common form of psychological intimidation, and is most often perpetrated by former or current intimate partners. Victims tend to feel their partner has nearly total control over them, greatly affecting the power dynamic in a relationship, empowering the perpetrator, and disempowering the victim. Victims often suffer from depression, putting them at increased risk of eating disorders, suicide, and drug and alcohol abuse.\n\nEconomic abuse (or financial abuse) is a form of abuse when one intimate partner has control over the other partner's access to economic resources. Marital assets are used as a means of control. Economic abuse may involve preventing a spouse from resource acquisition, limiting what the victim may use, or by otherwise exploiting economic resources of the victim. Economic abuse diminishes the victim's capacity to support themselves, increasing dependence on the perpetrator, including reduced access to education, employment, career advancement, and assets acquirement. Forcing or pressuring a family member to sign documents, to sell things, or to change a will are forms of economic abuse.\n\nA victim may be put on an allowance, allowing close monitoring of money is spent, preventing spending without perpetrator consent, leading to the accumulation of debt or depletion of the victim's savings. Disagreement about money spent can result in retaliation with additional physical, sexual or emotional abuse. In parts of the world where women depend on husbands' income in order to survive (due to lack of opportunities for female employment and lack of state welfare) economic abuse can have very severe consequences. Abusive relations have been associated with malnutrition among both mothers and children. In India, for example, the withholding of food is a documented form of family abuse.\n\nDomestic violence occurs across the world, in various cultures, and affects people of all economic statuses; however, indicators of lower socioeconomic status (such as unemployment and low income) have been shown to be risk factors for higher levels of domestic violence in several studies.\n\nThere continues to be some debate regarding gender differences with relation to domestic violence. Limitations of methodology, such as the conflict tactics scale, that fail to capture injury, homicide, and sexual violence rates, context (e.g., motivations, fear), disparate sampling procedures, respondent reluctance to self-report, and differences in operationalization all pose challenges to existing research. Normalization of domestic violence in those who experience covert forms of abuse, or have been abused by multiple partners, for long periods of time, reduces the likelihood of recognizing, and therefore reporting, domestic violence. Many organizations have made efforts to use gender-neutral terms when referring to perpetration and victimization. For example, using broader terms like \"family violence\" rather than \"violence against women\".\n\nFindings indicate that the main or a primary motive for female-on-male intimate partner violence (IPV) is self-defense or other self-protection (such as emotional health). Sherry Hamby states that males' self-reports of victimization are unreliable, as they consistently underreport their own violence perpetration. Hamby also reports that both men and women use IPV for coercive control. Coercive control is when one person uses a variety of IPV tactics to control and dominate the other, with little empathy; victims often resist with physical violence. It is generally perpetrated by men against women, and is the most likely of the types to cause trauma bonding and require medical services. A 2010 systematic review of the literature on women's perpetration of IPV found that anger, self-defense and retaliation were common motivations but that distinguishing between self-defense and retaliation was difficult. Family violence research by Murray A. Straus concluded that most IPV perpetrated by women against men is not motivated by self-defense. This has been criticized by scholars for using narrow definitions of self-defense. A 2011 review by researcher Chan Ko Ling from the University of Hong Kong found that perpetration of minor partner violence was equal for both men and women but more severe partner violence was far likelier to be perpetrated by men. His analysis found that men were more likely to beat up, choke or strangle their partners while women were more likely to throw objects, slap, kick, bite, punch, or hit with an object.\n\nResearchers have also found different outcomes for men and women in response to intimate partner violence. A 2012 review from the journal \"Psychology of Violence\" found that women suffered disproportionately as a result of intimate partner violence, especially in terms of injuries, fear, and posttraumatic stress disorder. The review also found that 70% of female victims in one study were \"very frightened\" in response to IPV from their partners, but 85% of male victims reported \"no fear\", and that IPV mediated the satisfaction of the relationship for women but not for men. Hamberger's (2005) review found that men tend to respond to female partner-initiated IPV with laughter and amusement. Researchers report that male violence causes great fear, \"fear is the force that provides battering with its power\" and \"injuries help sustain the fear.\"\n\nA 2013 review examined studies from five continents and the correlation between a country's level of gender inequality and rates of domestic violence. The authors found that when partner abuse is defined broadly to include emotional abuse, any kind of hitting, and who hits first, partner abuse is relatively even. They also stated if one examines who is physically harmed and how seriously, expresses more fear, and experiences subsequent psychological problems, domestic violence is significantly gendered toward women as victims.\n\nLaws on domestic violence vary by country. While it is generally outlawed in the Western world, this is not the case in many developing countries. For instance, in 2010, the United Arab Emirates's Supreme Court ruled that a man has the right to physically discipline his wife and children as long as he does not leave physical marks. The social acceptability of domestic violence also differs by country. While in most developed countries domestic violence is considered unacceptable by most people, in many regions of the world the views are different: according to a UNICEF survey, the percentage of women aged 15–49 who think that a husband is justified in hitting or beating his wife under certain circumstances is, for example: 90% in Afghanistan and Jordan, 87% in Mali, 86% in Guinea and Timor-Leste, 81% in Laos, 80% in Central African Republic. Refusing to submit to a husband's wishes is a common reason given for justification of violence in developing countries: for instance 62.4% of women in Tajikistan justify wife beating if the wife goes out without telling the husband; 68% if she argues with him; 47.9% if she refuses to have sex with him.\n\nThe United Nations Population Fund found violence against women and girls to be one of the most prevalent human rights violations worldwide, stating that \"one in three women will experience physical or sexual abuse in her lifetime.\" Violence against women tends to be less prevalent in developed Western nations, and more normalized in the developing world.\n\nWife beating was made illegal nationally in the United States by 1920. Although the exact rates are disputed, there is a large body of cross-cultural evidence that women are subjected to domestic violence significantly more often than men. In addition, there is broad consensus that women are more often subjected to severe forms of abuse and are more likely to be injured by an abusive partner, and this is exacerbated by economic or social dependence.\n\nThe United Nations Declaration on the Elimination of Violence against Women (1993) states that \"violence against women is a manifestation of historically unequal power relations between men and women, which has led to domination over and discrimination against women by men and to the prevention of the full advancement of women, and that violence against women is one of the crucial social mechanisms by which women are forced into a subordinate position compared with men\".\nThe Declaration on the Elimination of Violence against Women classifies violence against women into three categories: that occurring in the family (DV), that occurring within the general community, and that perpetrated or condoned by the State.\n\nThe Inter-American Convention on the Prevention, Punishment, and Eradication of Violence against Women defines violence against women as \"any act or conduct, based on gender, which causes death or physical, sexual or psychological harm or suffering to women, whether in the public or the private sphere\". Similarly with the Declaration on the Elimination of Violence against Women, it classifies violence against women into three categories; one of which being DV – defined as violence against women which takes place \"within the family or domestic unit or within any other interpersonal relationship, whether or not the perpetrator shares or has shared the same residence with the woman\".\nThe Maputo Protocol adopted a broader definition, defining violence against women as: \"all acts perpetrated against women which cause or could cause them physical, sexual, psychological, and economic harm, including the threat to take such acts; or to undertake the imposition of arbitrary restrictions on or deprivation of fundamental freedoms in private or public life in peace time and during situations of armed conflicts or of war\".\n\nThe Istanbul Convention states: \"\"violence against women\" is understood as a violation of human rights \"and a form of discrimination against women\" (...)\". (Article 3 – Definitions). In the landmark case of \"Opuz v Turkey\", the European Court of Human Rights held for the first time that gender-based domestic violence is a form of discrimination under the European Convention.\n\nAccording to one study, the percentage of women who have reported being physically abused by an intimate partner vary from 69% to 10% depending on the country. In the United States, it is estimated that intimate partner violence accounts for 15% of all violent crime. The latest research (2017) by the CDC found that over half of all female homicides are committed by intimate partners, 98 percent of whom are men.\n\nFemicide is usually defined as the gender-based killing of women by men, although the exact definitions vary. Femicides often occur in the context of DV, such as honor killings or dowry killings. For statistical purposes, femicide is often defined as any killing of a woman. The top countries by rate of femicide are El Salvador, Jamaica, Guatemala, South Africa and Russia (data from 2004–09). However, in El Salvador and Colombia, which have a very high rate of femicide, only three percent of all femicides are committed by a current or former intimate partner, while in Cyprus, France, and Portugal former and current partners are responsible for more than 80% of all cases of femicide.\n\nDomestic violence against men includes physical, emotional and sexual forms of abuse, including mutual violence. Male domestic violence victims may be reluctant to get help for various reasons. One study investigated whether women who assaulted their male partners were more likely to avoid arrest even when the male contacts police, and found that, \"police are particularly unlikely to arrest women who assault their male partners.\" The reason being that they \"assume that the man can protect himself from his female partner and that a woman's violence is not dangerous unless she assaults someone other than her partner\". Another study concluded there is \"some support for qualitative research suggesting that court personnel are responsive to the gendered asymmetry of intimate partner violence, and may view female intimate violence perpetrators more as victims than offenders.\"\n\nAmong adolescents, researchers have primarily focused on heterosexual Caucasian populations. The literature indicates that rates are similar for the number of girls and boys in heterosexual relationships who report experiencing intimate partner violence (IPV), or that girls in heterosexual relationships are more likely than their male counterparts to report perpetrating IPV. Ely et al. stated that, unlike domestic violence in general, equal rates of IPV perpetration is a unique characteristic with regard to adolescent dating violence, and that this is \"perhaps because the period of adolescence, a special developmental state, is accompanied by sexual characteristics that are distinctly different from the characteristics of adult.\" Wekerle and Wolfe theorized that \"a mutually coercive and violent dynamic may form during adolescence, a time when males and females are more equal on a physical level\" and that this \"physical equality allows girls to assert more power through physical violence than is possible for an adult female attacked by a fully physically mature man.\" Sherry Hamby stated that horseplay and joking among adolescents and young adults is common and that \"a small but growing body of research indicates that females may be more likely to include this sort of joking around in responses to IPV questionnaires than males.\" \n\nWhile the general literature indicates that adolescent boys and girls engage in IPV at about equal rates, females are more likely to use less dangerous forms of physical violence (e.g. pushing, pinching, slapping, scratching or kicking), while males are more likely to punch, strangle, beat, burn, or threaten with weapons. Males are also more likely to use sexual aggression, although both genders are equally likely to pressure their partner into sexual activities. In addition, females are four times more likely to respond as having experienced rape and are more likely to suffer fatal injuries inflicted by their partner, or to need psychological help as a result of the abuse. Females are more likely to consider IPV a serious problem than are their male counterparts, who are more likely to disregard female-perpetrated IPV. Along with form, motivations for violence also vary by gender: females are likely to perpetrate violence in self-defense, while males are likely to perpetrate violence to exert power or control. The self-defense aspect is supported by findings that previous victimization is a stronger predictor of perpetration in females than in males. Other research indicates that boys who have been abused in childhood by a family member are more prone to IPV perpetration, while girls who have been abused in childhood by a family member are prone to lack empathy and self-efficacy; but the risks for the likelihood of IPV perpetration and victimization among adolescents vary and are not well understood. Hamby's 2018 literature review of 33 studies, using a scale that rules out the false positives of horseplay and joking, indicates that males report perpetrating significantly more violence than females.\n\nThere is a strong link between domestic violence and child abuse. Since domestic violence is a pattern of behavior, these incidences may increase in severity and frequency, resulting in an increased probability the children themselves will become victims. The estimated overlap between domestic violence and child abuse ranges from 30 to 50 percent.\n\nToday, corporal punishment of children by their parents remains legal in most countries, but in Western countries that still allow the practice there are strict limits on what is permitted. The first country to outlaw parental corporal punishment was Sweden (parents' right to spank their own children was first removed in 1966), and it was explicitly prohibited by law from July 1979. As of 2016, parental corporal punishment is banned in 51 countries.\n\nHistorically, domestic violence has been seen as a heterosexual family issue and little interest has been directed at violence in same-sex relationships, but domestic violence can occur in same-sex relationships as well. The \"Encyclopedia of Victimology and Crime Prevention\" states, \"For several methodological reasons – nonrandom sampling procedures and self-selection factors, among others – it is not possible to assess the extent of same-sex domestic violence. Studies on abuse between gay male or lesbian partners usually rely on small convenience samples such as lesbian or gay male members of an association.\"\n\nA 1999 analysis of nineteen studies of partner abuse concluded that \"[r]esearch suggests that lesbians and gay men are just as likely to abuse their partners as heterosexual men.\" In 2011, the Centers for Disease Control and Prevention released the 2010 results of their National Intimate Partner and Sexual Violence Survey and report that 44% of lesbian women, 61% of bisexual women, and 35% of heterosexual women experienced domestic violence in their lifetime. This same report states that 26% of gay men, 37% of bisexual men, and 29% of heterosexual men experienced domestic violence in their lifetime. A 2013 study showed that 40.4% of self-identified lesbians and 56.9% of bisexual women have reported being victims of partner violence. In 2014, national surveys indicated that anywhere from 25–50% of gay and bisexual males have experienced physical violence from a partner.\nSome sources state that gay and lesbian couples experience domestic violence at the same frequency as heterosexual couples, while other sources state domestic violence among gay, lesbian, and bisexual individuals might be higher than among heterosexual individuals, that gay, lesbian, and bisexual individuals are less likely to report domestic violence that has occurred in their intimate relationships than heterosexual couples are, or that lesbian couples experience domestic violence less than heterosexual couples do. One study focusing on Hispanic men indicated that gay men are less likely to have been perpetrators or victims of domestic violence than heterosexual men but that bisexual men are more likely to have been both. By contrast, some researchers commonly assume that lesbian couples experience domestic violence at the same rate as heterosexual couples, and have been more cautious when reporting domestic violence among gay male couples.\nGay and lesbian relationships have been identified as a risk factor for abuse in certain populations. LGBT people in some parts of the world have very little legal protection from DV, because engaging in homosexual acts is itself prohibited by the \"sodomy laws\" of those jurisdictions (as of 2014, same-sex sexual acts are punishable by imprisonment in 70 countries and by death in another 5 countries) and these legal prohibitions prevent LGBT victims of DV from reporting the abuse to authorities. In the face of the 2003 Supreme Court decision, 13 US states have refused to remove sodomy laws from legislation as of 2013.\n\nPeople in same-sex relationships face special obstacles in dealing with the issues that some researchers have labeled \"the double closet\". A 1997 Canadian study by Mark W. Lehman suggests similarities include frequency (approximately one in every four couples); manifestations (emotional, physical, financial, etc.); co-existent situations (unemployment, substance abuse, low self-esteem); victims' reactions (fear, feelings of helplessness, hypervigilance); and reasons for staying (love, can work it out, things will change, denial). Studies conducted by Emory University in 2014 identified 24 trigger for partner violence through web-based surveys, ranging from drugs and alcohol to safe-sex discussions. A general theme of power and control seems to underlie abuse in both heterosexual and homosexual relationships.\n\nAt the same time, significant differences, unique issues, and deceptive myths are typically present. Lehman, regarding his 1997 survey, points to added discrimination and fears that gay and lesbian individuals may face. This includes potential dismissal by police and some social services, a lack of support from peers, fear of attracting negative stigma toward the gay community, the impact of HIV/AIDS status in keeping partners together (due to health care insurance/access, or guilt), threat of outing, and encountering supportive services that are targeted, or structured for the needs of heterosexual women, and may not meet the needs of gay men or lesbians. This service structure can make LGBTQ victims feel even more isolated and misunderstood than they may already because of their minority status. Lehman, however, stated that \"due to the limited number of returned responses and non-random sampling methodology the findings of this work are not generalizable beyond the sample\" of 32 initial respondents and final 10 who completed the more in-depth survey. Particularly, sexual stressors and HIV/AIDS status have emerged as significant differences in same-sex partner violence.\n\nDV is among the most underreported crimes worldwide for both men and women. A 2011 review article by intimate partner violence researcher Ko Ling Chan found men tended to under-report their own perpetration of domestic violence while women were more likely to under-report their victimization and over-estimate their own violence perpetration. Financial or familial dependence, normalization of violence, and self-blaming were found to reduce the likelihood of self-reporting victimization in women. By contrast, fear and avoidance of legal consequences, the tendency to blame their partner, and a narrative focus on their own needs and emotions reduced the likelihood of self-reporting perpetration in men.\n\nA 2014 study conducted across the 28 member states of the European Union found that only 14% of women reported their most serious incident of intimate partner violence to the police. A 2009 report on DV in Northern Ireland found that \"under-reporting is a concern and domestic abuse is the least likely of all violent crimes to be reported to the police\".\n\nMen face additional gender related barriers in reporting, due to social stigmas regarding male victimization, and an increased likelihood of being overlooked by healthcare providers.\n\nSocial views on domestic violence vary from person to person, and from region to region, but in many places outside the West, the concept is very poorly understood. This is because in most of these countries the relation between the husband and wife is not considered one of equals, but instead one in which the wife must submit herself to the husband. This is codified in the laws of some countries – for example, in Yemen, marriage regulations state that a wife must obey her husband and must not leave home without his permission.\n\nAccording to \"Violence against Women in Families and Relationships\", \"Globally, wife-beating is seen as justified in some circumstances by a majority of the population in various countries, most commonly in situations of actual or suspected infidelity by wives or their \"disobedience\" toward a husband or partner.\" These violent acts against a wife are often not considered a form of abuse by society (both men and women) but are considered to have been provoked by the behavior of the wife, who is seen as being at fault. While beatings of wives are often a response to \"inappropriate\" behaviors, in many places extreme acts such as honor killings are approved by a high section of the society. In one survey, 33.4% of teenagers in Jordan's capital city, Amman, approved of honor killings. This survey was carried in the capital of Jordan, which is much more liberal than other parts of the country; the researchers said that \"We would expect that in the more rural and traditional parts of Jordan, support for honor killings would be even higher\".\n\nIn a 2012 news story, \"The Washington Post\" reported, \"The Reuters TrustLaw group named India one of the worst countries in the world for women this year, partly because domestic violence there is often seen as deserved. A 2012 report by UNICEF found that 57 percent of Indian boys and 53 percent of girls between the ages of 15 and 19 think wife-beating is justified.\"\n\nIn conservative cultures, a wife dressing in attire deemed insufficiently modest can suffer serious violence at the hands of her husband or relatives, with such violent responses seen as appropriate by most of the society: in a survey, 62.8% of women in Afghanistan said that a husband is justified in beating his wife if she wears inappropriate clothes.\n\nAccording to Antonia Parvanova, one of the difficulties of dealing legally with the issue of DV is that men in many male dominated societies do not understand that inflicting violence against their wives is against the law. She said, referring to a case that occurred in Bulgaria, \"A husband was tried for severely beating his wife and when the judge asked him if he understood what he did and if he's sorry, the husband said \"But she's my wife\". He doesn't even understand that he has no right to beat her.\" UNFPA writes that: \n\"In some developing countries, practices that subjugate and harm women – such as wife-beating, killings in the name of honour, female genital mutilation/cutting and dowry deaths – are condoned as being part of the natural order of things\".\n\nStrong views among the population in certain societies that reconciliation is more appropriate than punishment in cases of domestic violence are also another cause of legal impunity; a study found that 64% of public officials in Colombia said that if it were in their hands to solve a case of intimate partner violence, the action they would take would be to encourage the parties to reconcile.\n\nVictim blaming is also prevalent in many societies, including in Western countries: a 2010 Eurobarometer poll found that 52% of respondents agreed with the assertion that the \"provocative behaviour of women\" was a cause of violence against women; with respondents in Cyprus, Denmark, Estonia, Finland, Latvia, Lithuania, Malta and Slovenia being most likely to agree with the assertion (more than 70% in each of these countries).\n\nThere is controversy regarding the influence of religion on domestic violence. According to \"Domestic Violence Cross Cultural Perspective\": \"No religion sanctions violence against women\", but there are some religious scriptures that have been \"taken out of context\" to support discrimination against women within a community.\n\nJudaism, Christianity and Islam have traditionally supported male-dominant households and \"socially sanctioned violence against women has been persistent since ancient times.\"\n\nThe Catholic Church has been criticized for opposing divorce, and therefore trapping victims of violence in abusive marriages.\n\nAt the same time, religious leaders can play an important role in preventing and treating domestic violence, when they provide abusers with guidance and treatment option information, and offer their support to those who have been subject to abuse.\n\nViews on the influence of religion on domestic violence differ. While some authors, such as Phyllis Chesler, argue that Islam is connected to violence against women, especially in the form of honor killings, others, such as Tahira Shahid Khan, a professor specializing in women's issues at the Aga Khan University in Pakistan, argue that it is the domination of men and inferior status of women in society that lead to these acts, not the religion itself. Public (such as through the media) and political discourse debating the relation between Islam, immigration, and violence against women is highly controversial in many Western countries.\n\nLocal customs and traditions are often responsible for maintaining certain forms of DV. Such customs and traditions include son preference (the desire of a family to have a boy and not a girl, which is strongly prevalent in parts of Asia), which can lead to abuse and neglect of girl children by disappointed family members; child and forced marriages; dowry; the hierarchic caste system which stigmatizes \"lower castes\" and \"untouchables\", leading to discrimination and restricted opportunities of the females and thus making them more vulnerable to abuse; strict dress codes for women that may be enforced through violence by family members; strong requirement of female virginity before the wedding and violence related to non-conforming women and girls; taboos about menstruation leading to females being isolated and shunned during the time of menstruation; female genital mutilation (FGM); ideologies of marital 'conjugal rights' to sex which justify marital rape; the importance given to 'family honor'.\n\nAccording to a 2003 report by Human Rights Watch, \"Customs such as the payment of 'bride price' (payment made by a man to the family of a woman he wishes to marry), whereby a man essentially purchases his wife’s sexual favors and reproductive capacity, underscore men’s socially sanctioned entitlement to dictate the terms of sex, and to use force to do so.\"\n\nIn recent years, there has been progress in the area of addressing customary practices that endanger women, with laws being enacted in several countries. The Inter-African Committee on Traditional Practices Affecting the Health of Women and Children is an NGO which works on changing social values, raising consciousness, and enacting laws against harmful traditions which affect the health of women and children in Africa. Laws were also enacted in some countries; for example the 2004 Criminal Code of Ethiopia has a chapter on harmful traditional practices – \"Chapter III – Crimes committed against life, person and health through harmful traditional practices\". In addition, the Council of Europe adopted a convention which addresses domestic violence and violence against women, and calls for the states which ratify it to create and fully adjudicate laws against acts of violence previously condoned by traditional, culture, custom, in the name of honor, or to correct what is deemed unacceptable behavior. The United Nations created the \"Handbook on effective police responses to violence against women\" to provide guidelines to address and manage violence through the creation of effective laws, law enforcement policies and practices and community activities to break down societal norms that condone violence, criminalize it and create effect support systems for survivors of violence.\n\nIn cultures where the police and legal authorities have a reputation of corruption and abusive practices, victims of DV are often reluctant to turn to formal help.\n\nA forced marriage is a marriage where one or both participants are married without their freely given consent. In many parts of the world, it is often difficult to draw a line between 'forced' and 'consensual' marriage: in many cultures (especially in South Asia, the Middle East and parts of Africa), marriages are prearranged, often as soon a girl is born; the idea of a girl going against the wishes of her family and choosing herself her own future husband is not socially accepted – there is no need to use threats or violence to force the marriage, the future bride will submit because she simply has no other choice. As in the case of child marriage, the customs of dowry and bride price contribute to this phenomenon. A child marriage is a marriage where one or both parties are younger than 18.\n\nForced and child marriages are associated with a high rate of domestic violence. These types of marriages are related to violence both in regard to the spousal violence perpetrated inside marriage, and in regard to the violence related to the customs and traditions of these marriage: violence and trafficking related to the payment of dowry and bride price, honor killings for refusing the marriage.\n\nUNFPA states, \"Despite near-universal commitments to end child marriage, one in three girls in developing countries (excluding China) will probably be married before they are 18. One out of nine girls will be married before their 15th birthday.\" UNFPA estimates, \"Over 67 million women 20–24 year old in 2010 had been married as girls, half of which were in Asia, and one-fifth in Africa.\" UNFPA says that, \"In the next decade 14.2 million girls under 18 will be married every year; this translates into 39,000 girls married each day and this will rise to an average of 15.1 million girls a year, starting in 2021 until 2030, if present trends continue.\" \n\nThe World Health Organization has stated that women in abusive relations are at significantly higher risk of HIV/AIDS. WHO states that women in violent relations have difficulty negotiating safer sex with their partners, are often forced to have sex, and find it difficult to ask for appropriate testing when they think they may be infected with HIV. A decade of cross-sectional research from Rwanda, Tanzania, South Africa, and India, has consistently found women who have experienced partner violence to be more likely to be infected with HIV. The WHO stated that:\nThere is a compelling case to end intimate partner violence both in its own right as well as to reduce women and girls vulnerability to HIV/AIDS. The evidence on the linkages between violence against women and HIV/AIDS highlights that there are direct and indirect mechanisms by which the two interact.\n\nSame-sex relationships are similarly affected by HIV/AIDS status in domestic violence. Research by Heintz and Melendez found that same-sex individuals may have difficulty breaching the topic of safe-sex for reasons such as \"decreased perception of control over sex, fear of violence, and unequal power distributions...\" Of those who reported violence in the study, about 50% reported forced sexual experiences, of which only half reported the use of safe sex measures. Barriers to safer-sex included fear of abuse, and deception in safe-sex practices. Heintz and Melendez's research ultimately concluded that sexual assault/abuse in same-sex relationships provides a major concern for HIV/AIDS infection as it decreases instances of safe-sex. Furthermore, these incidents create additional fear and stigma surrounding safe-sex conversations and knowing ones STD status.\n\nLack of adequate legislation which criminalizes domestic violence, or, alternatively legislation which prohibits consensual behaviors, may hinder the progress in regard to reducing the incidence of DV. Amnesty International’s Secretary General has stated that: \"It is unbelievable that in the twenty-first century some countries are condoning child marriage and marital rape while others are outlawing abortion, sex outside marriage and same-sex sexual activity – even punishable by death.\" According to WHO, \"one of the most common forms of violence against women is that performed by a husband or male partner.\" The WHO notes that such violence is often ignored because often \"legal systems and cultural norms do not treat as a crime, but rather as a 'private' family matter, or a normal part of life.\" The criminalization of adultery has been cited as inciting violence against women, as these prohibitions are often meant, in law or in practice, to control women's and not men's behavior; and are used to rationalize acts of violence against women. According to High Commissioner for Human Rights Navi Pillay: \"Some have argued, and continue to argue, that family violence is placed outside the conceptual framework of international human rights. However, under international laws and standards, there is a clear State responsibility to uphold women’s rights and ensure freedom from discrimination, which includes the responsibility to prevent, protect and provide redress – regardless of sex, and regardless of a person’s status in the family.\"\n\nThe ability of victims of domestic violence to leave the relationship is crucial for preventing further abuse. In traditional communities, divorced women often feel rejected and ostracized. In order to avoid this stigma, many women prefer to remain in the marriage and endure the abuse.\n\nDiscriminatory marriage and divorce laws can also play a role in the proliferation of the practice. According to Rashida Manjoo, a United Nations special rapporteur on violence against women: in many countries a woman’s access to property hinges on her relationship to a man. When she separates from her husband or when he dies, she risks losing her home, land, household goods and other property. Failure to ensure equal property rights upon separation or divorce discourages women from leaving violent marriages, as women may be forced to choose between violence at home and destitution in the street.\n\nThe legal inability to obtain a divorce is also a factor in the proliferation of domestic violence. In some cultures where marriages are arranged between families, a woman who attempts a separation or divorce without the consent of her husband and extended family or relatives may risk being subjected to \"honor\"- based violence.\n\nThe custom of bride price also makes leaving a marriage more difficult: if a wife want to leave, the husband may demand back the bride price from her family.\n\nIn advanced nations like the United Kingdom domestic violence victims may have difficulties getting alternative housing which can force them to stay in the abusive relationship.\n\nMany domestic violence victims delay leaving the abuser because they have pets and are afraid of what will happen to the pets if they leave. Safehouses need to be more accepting of pets, many refuse to accept pets.\n\nThe way the individual rights of a family member versus the rights of the family as a unit are balanced vary significantly in different societies. This may influence the degree to which a government may be willing to investigate family incidents. In some cultures, individual members of the family are expected to sacrifice almost completely their own interests in favor of the interests of the family as a whole. What is viewed as an undue expression of personal autonomy is condemned as unacceptable. In these cultures the family predominates over the individual, and where this interacts with cultures of honor, individualistic choice that may damage the family reputation in the community may result in extreme punishment, such as honor killings.\n\nIn some countries, the immigration policy is tied to whether the person desiring citizenship is married to his/her sponsor. This can lead to persons being trapped in violent relations – such persons may risk deportation if they attempt to separate (they may be accused of having entered into a sham marriage). Often the women come from cultures where they will suffer disgrace from their families if they abandon their marriage and return home, and so they prefer to stay married, therefore remaining locked in a cycle of abuse.\n\nDomestic violence may happen in immigrant communities, and often there is little awareness in these communities of the laws and policies of the host country. A study among first generation South Asians in the UK found that they had little knowledge about what constituted criminal behavior under the English law. The researchers found that \"There was certainly no awareness that there could be rape within a marriage\". A study in Australia showed that among the immigrant women sampled who were abused by partners and did not report it, 16.7% did not know DV was illegal, while 18.8% did not know that they could get protection.\n\nOf the most important factors in domestic violence is a belief that abuse, whether physical or verbal, is acceptable. Other factors include substance abuse, unemployment, mental health problems, lack of coping skills, isolation, and excessive dependence on the abuser.\n\nLenore E. Walker presented the model of a cycle of abuse which consists of four phases. First, there is a buildup to abuse when tension rises until a domestic violence incident ensues. During the reconciliation stage, the abuser may be kind and loving and then there is a period of calm. When the situation is calm, the abused person may be hopeful that the situation will change. Then, tensions begin to build, and the cycle starts again.\n\nA common aspect among abusers is that they witnessed abuse in their childhood, in other words they were participants in a chain of intergenerational cycles of domestic violence. That does not mean, conversely, that if a child witnesses or is subject to violence that they will become abusers. Understanding and breaking the intergenerational abuse patterns may do more to reduce domestic violence than other remedies for managing the abuse.\n\nResponses that focus on children suggest that experiences throughout life influence an individual’s propensity to engage in family violence (either as a victim or as a perpetrator). Researchers supporting this theory suggest it is useful to think of three sources of domestic violence: childhood socialization, previous experiences in couple relationships during adolescence, and levels of strain in a person's current life. People who observe their parents abusing each other, or who were themselves abused may incorporate abuse into their behaviour within relationships that they establish as adults.\n\nResearch indicates that the more children are physically punished, the more likely they will be as adults to act violently towards family members, including intimate partners. Children who are spanked more as children are more likely as adults to approve of hitting a partner, and also experience more marital conflict and feelings of anger in general. A number of studies have found physical punishment to be associated with \"higher levels of aggression against parents, siblings, peers and spouses\", even when controlling for other factors. While these associations do not prove a causal relationship, a number of longitudinal studies suggest that the experience of physical punishment does have a direct causal effect on later aggressive behaviors. Such research has shown that corporal punishment of children (e.g. smacking, slapping, or spanking) predicts weaker internalisation of values such as empathy, altruism, and resistance to temptation, along with more antisocial behavior, including dating violence.\n\nIn some patrilineal societies around the world, a young bride moves with the family of her husband. As a new girl in the home, she starts as having the lowest (or among the lowest) position in the family, and is often subjected to violence and abuse, and is, in particular, strongly controlled by the parents-in-law: with the arrival of the daughter-in-law in the family, the mother-in-law's status is elevated and she now has (often for the first time in her life) substantial power over someone else, and \"This family system itself tends to produce a cycle of violence in which the formerly abused bride becomes the abusing mother-in-law to her new daughter-in-law\". Amnesty International writes that, in Tajikistan, \"it is almost an initiation ritual for the mother-in-law to put her daughter-in-law through the same torments she went through herself as a young wife.\"\n\nThese factors include genetics and brain dysfunction and are studied by neuroscience. Psychological theories focus on personality traits and mental characteristics of the offender. Personality traits include sudden bursts of anger, poor impulse control, and poor self-esteem. Various theories suggest that psychopathology is a factor, and that abuse experienced as a child leads some people to be more violent as adults. Correlation has been found between juvenile delinquency and domestic violence in adulthood.\n\nStudies have found high incidence of psychopathology among domestic abusers. For instance, some research suggests that about 80% of both court-referred and self-referred men in these domestic violence studies exhibited diagnosable psychopathology, typically personality disorders. \"The estimate of personality disorders in the general population would be more in the 15–20% range [...] As violence becomes more severe and chronic in the relationship, the likelihood of psychopathology in these men approaches 100%.\"\n\nDutton has suggested a psychological profile of men who abuse their wives, arguing that they have borderline personalities that are developed early in life. However, these psychological theories are disputed: Gelles suggests that psychological theories are limited, and points out that other researchers have found that only 10% (or less) fit this psychological profile. He argues that social factors are important, while personality traits, mental illness, or psychopathy are lesser factors.\n\nAn evolutionary psychological explanation of domestic violence is that it represents male attempts to control female reproduction and ensure sexual exclusivity. Violence related to extramarital relations is seen as justified in certain parts of the world. For instance, a survey in Diyarbakir, Turkey, found that, when asked the appropriate punishment for a woman who has committed adultery, 37% of respondents said she should be killed, while 21% said her nose or ears should be cut off. Similar feelings may at times be generated in a situations where one partner is more financially successful.\n\nSocial theories look at external factors in the offender's environment, such as family structure, stress, social learning, and includes rational choice theories.\n\nSocial learning theory suggests that people learn from observing and modeling after others' behavior. With positive reinforcement, the behavior continues. If one observes violent behavior, one is more likely to imitate it. If there are no negative consequences (e. g. victim accepts the violence, with submission), then the behavior will likely continue.\n\nResource theory was suggested by William Goode (1971). Women who are most dependent on the spouse for economic well being (e.g. homemakers/housewives, women with handicaps, the unemployed), and are the primary caregiver to their children, fear the increased financial burden if they leave their marriage. Dependency means that they have fewer options and few resources to help them cope with or change their spouse's behavior.\n\nCouples that share power equally experience lower incidence of conflict, and when conflict does arise, are less likely to resort to violence. If one spouse desires control and power in the relationship, the spouse may resort to abuse This may include coercion and threats, intimidation, emotional abuse, economic abuse, isolation, making light of the situation and blaming the spouse, using children (threatening to take them away), and behaving as \"master of the castle\".\n\nStress may be increased when a person is living in a family situation, with increased pressures. Social stresses, due to inadequate finances or other such problems in a family may further increase tensions. Violence is not always caused by stress, but may be one way that some people respond to stress. Families and couples in poverty may be more likely to experience domestic violence, due to increased stress and conflicts about finances and other aspects. Some speculate that poverty may hinder a man's ability to live up to his idea of \"successful manhood\", thus he fears losing honor and respect. Theory suggests that when he is unable to economically support his wife, and maintain control, he may turn to misogyny, substance abuse, and crime as ways to express masculinity.\n\nSame-sex relationships may experience similar social stressors. Additionally, violence in same-sex relationships has been linked to internalized homophobia, which contributed to low self-esteem and anger in both perpetrator and victim. Internalized homophobia also appears to be a barrier in victims seeking help. Similarly, heterosexism can play a key role in domestic violence in the LGBT community. As a social ideology that implies \"heterosexuality is normative, morally superior, and better than [homosexuality],\" heterosexism can hinder services and lead to an unhealthy self-image in sexual minorities. Heterosexism in legal and medical institutions can be seen in instances of discrimination, biases, and insensitivity toward sexual orientation. For example, as of 2006, seven states explicitly denied LGBT individuals the ability to apply for protective orders, proliferating ideas of LGBT subjugation, which is tied to feelings of anger and powerlessness.\n\nPower and control in abusive relationships is the way that abusers exert physical, sexual and other forms of abuse to gain control within relationships.\n\nA causalist view of domestic violence is that it is a strategy to gain or maintain power and control over the victim. This view is in alignment with Bancroft's \"cost-benefit\" theory that abuse rewards the perpetrator in ways other than, or in addition to, simply exercising power over his or her target(s). He cites evidence in support of his argument that, in most cases, abusers are quite capable of exercising control over themselves, but choose not to do so for various reasons.\n\nSometimes, one person seeks complete power and control over their partner and uses different ways to achieve this, including resorting to physical violence. The perpetrator attempts to control all aspects of the victim's life, such as their social, personal, professional and financial decisions.\n\nQuestions of power and control are integral to the widely utilized Duluth Domestic Abuse Intervention Project. They developed a \"Power and Control Wheel\" to illustrate this: it has power and control at the center, surrounded by spokes (techniques used), the titles of which include: coercion and threats, intimidation, emotional abuse, isolation, minimizing, denying and blaming, using children, economic abuse, and privilege.\n\nCritics of this model argue that it ignores research linking domestic violence to substance abuse and psychological problems. Some modern research into the patterns in DV has found that women are more likely to be physically abusive towards their partner in relationships in which only one partner is violent,<ref name=\"10.2105/AJPH.2005.079020\"></ref> which draws the effectiveness of using concepts like male privilege to treat domestic violence into question. Some modern research into predictors of injury from domestic violence suggests that the strongest predictor of injury by domestic violence is participation in reciprocal domestic violence.\n\nNonsubordination theory, sometimes called dominance theory, is an area of feminist legal theory that focuses on the power differential between men and women. Nonsubordination theory takes the position that society, and more especially men in society, use sex differences between men and women to perpetuate this power imbalance. Unlike other topics within feminist legal theory, nonsubordination theory focuses specifically on certain sexual behaviors, including control of women’s sexuality, sexual harassment, pornography, and violence against women generally. Catharine MacKinnon argues that nonsubordination theory best addresses these particular issues because they affect “almost exclusively” women. MacKinnon advocates for nonsubordination theory over other theories, like formal equality, substantive equality, and difference theory, because sexual violence and other forms of violence against women are not a question of “sameness and difference,” but rather are best viewed as “more central inequalities” for women. Though nonsubordination theory has been discussed at great length in evaluating various forms of sexual violence against women, it also serves as a basis for understanding domestic violence and why it occurs. Nonsubordination theory tackles the issue of domestic violence as a subset of the broader problem of violence against women because domestic violence victims are overwhelmingly female.\n\nProponents of nonsubordination theory propose several reasons why it works best to explain domestic violence. First, there are certain recurring patterns in domestic violence that indicate it is not the result of intense anger or arguments, but rather is a form of subordination. This is evidenced in part by the fact that domestic violence victims are typically abused in a variety of situations and by a variety of means. For example, victims are sometimes beaten after they have been sleeping or have been separated from the batterer, and often the abuse takes on a financial or emotional form in addition to physical abuse. Supporters of nonsubordination theory use these examples to dispel the notion that battering is always the result of heat of the moment anger or intense arguments. Also, batterers often employ manipulative and deliberate tactics when abusing their victims, which can “rang[e] from searching for and destroying a treasured object of hers to striking her in areas of her body that do not show bruises (e.g. her scalp) or in areas where she would be embarrassed to show others her bruises.” These behaviors can be even more useful to a batterer when the batterer and the victim share children, because the batterer often controls the family’s financial assets, making the victim less likely to leave if it would put her children at risk.\n\nProfessor Martha Mahoney, of the University of Miami School of Law, also points to the notion of “separation assault”—a phenomenon where a batterer further assaults a victim who is attempting or has attempted to leave an abusive relationship—as additional evidence that domestic violence is used to subordinate victims to their batterers. A batterer’s unwillingness to allow the victim to leave the relationship substantiates the idea that violence is being used to force the victim to continue to fulfill the batterer’s wishes that she obey him. Nonsubordination theorists argue that all of these actions—the variety of abusive behaviors and settings, exploiting the victim’s children, and assault upon separation—suggest a larger problem than merely an inability to properly manage anger, though anger may be a byproduct of these behaviors. The purpose of these actions is to keep the victim, and sometimes the entire family, subordinate to the batterer, according to nonsubordination theory.\n\nA second rationale for using nonsubordination theory to explain domestic violence, beyond the variety of tactics used by abusers, is that the frequency with which domestic violence occurs overpowers the idea that it is merely the result of a batterer’s anger. Professor Mahoney explains that because of the sensationalism generated in media coverage of “big” or particularly horrific domestic violence cases, it is difficult for people to conceptualize how frequently domestic violence happens in society. However, domestic violence is a regular occurrence experienced by up to one half of people in the United States, and an overwhelming number of victims are female. The sheer number of domestic violence victims in the United States suggests that domestic violence is not merely the result of intimate partners who cannot control their anger. Nonsubordination theory contends that it is the batterer’s desire to subordinate the victim, not his uncontainable anger, which explains the frequency of domestic violence. Nonsubordination theorists argue that other forms of feminist legal theory do not offer any explanation for the phenomenon of domestic violence generally or the frequency with which it occurs.\n\nCritics of nonsubordination theory complain that it offers no solutions to the problems it points out. For example, proponents of nonsubordination theory criticize certain approaches that have been taken to address domestic violence in the legal system, such as mandatory arrest or prosecution policies. These policies take discretion away from law enforcement by forcing police officers to arrest suspected domestic violence offenders and prosecutors to prosecute those cases. There is a lot of discourse surrounding mandatory arrest. Opponents argue that it undermines a victim's autonomy, discourages the empowerment of women by discounting other resources available and puts victims at more risk for domestic abuse. States that have implemented mandatory arrest laws have 60% higher homicide rates which have been shown to be consistent with the decline in reporting rates. Advocates of these policies contend that the criminal justice system is sometimes the only way to reach victims of domestic violence, and that if an offender knows he will be arrested, it will deter future domestic violence conduct. People who endorse nonsubordination theory argue that these policies only serve to further subordinate women by forcing them to take a certain course of action, thus compounding the trauma they experienced during the abuse. However, nonsubordination theory itself offers no better or more appropriate solutions, which is why some scholars argue that other forms of feminist legal theory are more appropriate to address issues of domestic and sexual violence. Sociologist Andrew Glover critiques non-subordination theory on the grounds that domestic violence occurs among homosexual couples at the same rate as heterosexual couples. So it cannot be caused by, or exist to perpetuate, power differentials between the sexes. \n\n3.3 million children witness domestic violence each year in the US. There has been an increase in acknowledgment that a child who is exposed to domestic abuse during their upbringing will suffer developmental and psychological damage. During the mid 1990s, the Adverse Childhood Experiences study (ACE) found that children who were exposed to domestic violence and other forms of abuse had a higher risk of developing mental and physical health problems. Because of the awareness of domestic violence that some children have to face, it also generally impacts how the child develops emotionally, socially, behaviorally as well as cognitively.\n\nSome emotional and behavioral problems that can result due to domestic violence include increased aggressiveness, anxiety, and changes in how a child socializes with friends, family, and authorities. Depression, emotional insecurity, and mental health disorders can follow due to traumatic experiences. Problems with attitude and cognition in schools can start developing, along with a lack of skills such as problem-solving. Correlation has been found between the experience of abuse and neglect in childhood and perpetrating domestic violence and sexual abuse in adulthood.\n\nAdditionally, in some cases the abuser will purposely abuse the mother or father in front of the child to cause a ripple effect, hurting two victims simultaneously. Children may intervene when they witness severe violence against a parent, which can place a child at greater risk for injury or death. It has been found that children who witness mother-assault are more likely to exhibit symptoms of post-traumatic stress disorder (PTSD). Consequences to these children are likely to be more severe if their assaulted mother develops post-traumatic stress disorder (PTSD) and does not seek treatment due to her difficulty in assisting her child with processing his or her own experience of witnessing the domestic violence.\n\nBruises, broken bones, head injuries, lacerations, and internal bleeding are some of the acute effects of a domestic violence incident that require medical attention and hospitalization. Some chronic health conditions that have been linked to victims of domestic violence are arthritis, irritable bowel syndrome, chronic pain, pelvic pain, ulcers, and migraines. Victims who are pregnant during a domestic violence relationship experience greater risk of miscarriage, pre-term labor, and injury to or death of the fetus.\n\nNew research illustrates that there are strong associations between exposure to domestic violence and abuse in all their forms and higher rates of many chronic conditions. The strongest evidence comes from the Adverse Childhood Experiences study which shows correlations between exposure to abuse or neglect and higher rates in adulthood of chronic conditions, high risk health behaviors and shortened life span. Evidence of the association between physical health and violence against women has been accumulating since the early 1990s.\n\nAmong victims who are still living with their perpetrators high amounts of stress, fear, and anxiety are commonly reported. Depression is also common, as victims are made to feel guilty for ‘provoking’ the abuse and are frequently subjected to intense criticism. It is reported that 60% of victims meet the diagnostic criteria for depression, either during or after termination of the relationship, and have a greatly increased risk of suicide. Those who are battered either emotionally or physically often are also depressed because of a feeling of worthlessness. These feelings often persist long-term and it is suggested that many receive therapy for it because of the heightened risk of suicide and other traumatic symptoms.\n\nIn addition to depression, victims of domestic violence also commonly experience long-term anxiety and panic, and are likely to meet the diagnostic criteria for Generalized Anxiety Disorder and Panic Disorder. The most commonly referenced psychological effect of domestic violence is Post-Traumatic Stress Disorder (PTSD). PTSD (as experienced by victims) is characterized by flashbacks, intrusive images, exaggerated startle response, nightmares, and avoidance of triggers that are associated with the abuse. Studies have indicated that it is important to consider the effect of domestic violence and its psychophysiologic sequelae on women who are mothers of infants and young children. Several studies have shown that maternal interpersonal violence-related posttraumatic stress disorder (PTSD) can, despite traumatized mother's best efforts, interfere with their child's response to the domestic violence and other traumatic events.\n\nOnce victims leave their perpetrator, they can be stunned with the reality of the extent to which the abuse has taken away their autonomy. Due to economic abuse and isolation, the victim usually has very little money of their own and few people on whom they can rely when seeking help. This has been shown to be one of the greatest obstacles facing victims of DV, and the strongest factor that can discourage them from leaving their perpetrators.\n\nIn addition to lacking financial resources, victims of DV often lack specialized skills, education, and training that are necessary to find gainful employment, and also may have several children to support. In 2003, thirty-six major US cities cited DV as one of the primary causes of homelessness in their areas. It has also been reported that one out of every three homeless women are homeless due to having left a DV relationship. If a victim is able to secure rental housing, it is likely that her apartment complex will have \"zero tolerance\" policies for crime; these policies can cause them to face eviction even if they are the victim (not the perpetrator) of violence. While the number of shelters and community resources available to DV victims has grown tremendously, these agencies often have few employees and hundreds of victims seeking assistance which causes many victims to remain without the assistance they need.\n\nWomen and children experiencing domestic violence undergo occupational apartheid; they are typically denied access to desired occupations. Abusive partners may limit occupations and create an occupationally void environment which reinforces feelings of low self-worth and poor self-efficacy in ability to satisfactorily perform everyday tasks. In addition, work is impacted by functional losses, ability to maintain necessary employment skills, and ability to function within the work place. Oftentimes the victims are very isolated from other relationships as well such as having few to no friends, this is another method of control for the abuser.\n\nAn analysis in the US showed that 106 of the 771 officer killings between 1996 and 2009 occurred during domestic violence interventions. Of these, 51% were defined as unprovoked or as ambushes, taking place before officers had made contact with suspects. Another 40% occurred after contact and the remainder took place during tactical situations (those involving hostages and attempts to overcome barricades). The FBI's LEOKA system grouped officer domestic violence response deaths into the category of disturbances, along with \"bar fights, gang matters, and persons brandishing weapons,\" which may have given rise to a misperception of the risks involved.\n\nDue to the gravity and intensity of hearing victims’ stories of abuse, professionals (social workers, police, counselors, therapists, advocates, medical professionals) are at risk themselves for secondary or vicarious trauma (VT), which causes the responder to experience trauma symptoms similar to the original victim after hearing about the victim’s experiences with abuse. Research has demonstrated that professionals who experience vicarious trauma show signs of exaggerated startle response, hypervigilance, nightmares, and intrusive thoughts although they have not experienced a trauma personally and do not qualify for a clinical diagnosis of PTSD.\n\nManagement of domestic violence may take place through medical services, law enforcement, counseling, and other forms of prevention and intervention. Participants in domestic violence may require medical treatment, such as examination by a family physician, other primary care provider, or emergency room physicians.\n\nCounseling is another means of managing the effects of domestic violence. For the victim of abuse, counseling may include an assessment of the presence, extent and types of abuse. A lethality assessment is a tool that can assist in determining the best course of treatment for a client, as well as helping the client to recognize dangerous behaviors and more subtle abuse in their relationship. In a study of victims of attempted domestic violence-related homicide, only about one-half of the participants recognized that their perpetrator was capable of killing them, as many domestic violence victims minimize the true seriousness of their situation. Another important component is safety planning, which allows the victim to plan for dangerous situations they may encounter, and is effective regardless of their decision on whether remain with their perpetrator.\n\nCounseling may be used by offenders to minimize the risk of future domestic violence, or to stop the violence and repair the harm it has caused. Most commonly, to date, convicted or self-referring offenders undertake programmes for perpetrators of intimate partner violence. These are delivered in a group format, one or two hours per week, over a set time period. Programme facilitators guide participants through a curriculum of adult-education style modules, which draw on a variety of therapeutic approaches, but predominantly cognitive behavioural therapy and psycho-education. A debate on the effectiveness of these programmes is on-going. While some (ex-) partners of offenders have experienced improvements in their situation, others have not, and there also appears to be a risk of doing harm. Along with using group work, there are other approaches that incorporate individual and conjoint conversations to help stop the violence and restore the victims' safety and respect.\n\nPrevention and intervention includes ways to prevent domestic violence by offering safe shelter, crisis intervention, advocacy, and education and prevention programs. Community screening for domestic violence can be more systematic in cases of animal abuse, healthcare settings, emergency departments, behavioral health settings and court systems. Tools are being developed to facilitate domestic violence screening such as mobile apps. The Duluth Model or Domestic Abuse Intervention Project is a program developed to reduce domestic violence against women, which is the first multi-disciplinary program designed to address the issue of domestic violence by coordinating the actions of a variety of agencies dealing with domestic conflict.\n\nDomestic violence hotlines offer advice, support and referral services to those in abusive relationships.\n\nThere exist several strategies that are being used to attempt to prevent or reduce DV. It is important to assess the effectiveness of a strategy that is being implemented.\n\nReforming the legislation in order to ensure that domestic violence falls under the scope of the law is important. This may imply repealing existing laws which discriminate against women: according to the WHO, \"when the law allows husbands to physically discipline wives, implementing a programme to prevent intimate partner violence may have little impact\". Marriage laws are also important, \"They [women] should also be able to enter freely into a marriage or to leave it, to obtain financial credit, and to own and administer property.\" Abolishing or restricting the offering and receiving of dowry and bride price and scrutinizing the impact of these transactions on the legislative decisions regarding DV is also important. UN Women has stated that the legislation should ensure that \"a perpetrator of domestic violence, including marital rape, cannot use the fact that he paid bride price as a defence to a domestic violence charge\".\n\nGender norms that promote the inferiority of women may lead to the abuse of women by intimate partners. The WHO writes that, \"Dismantling hierarchical constructions of masculinity and femininity predicated on the control of women, and eliminating the structural factors that support inequalities are likely to make a significant contribution to preventing intimate partner and sexual violence\".\n\nAccording to the Centers for Disease Control and Prevention, \"A key strategy in preventing domestic violence is the promotion of respectful, nonviolent relationships through individual, community, and societal level change.\"\nEarly intervention programs, such as school-based programs to prevent dating violence are also effective. Children who grow up in violent homes may be led to believe that such behavior is a normal part of life, therefore it is important to challenge such attitudes when they are present among these children.\n\n\nIn Australia, domestic violence refers to occurrences of violence in domestic settings between people in intimate relationships. The term can be altered by each state's legislation and can broaden the spectrum of domestic violence, such as in Victoria, where family-like relationships and witnessing any type of violence in the family is defined as a family violence incident.\n\n\n"}
{"id": "65888", "url": "https://en.wikipedia.org/wiki?curid=65888", "title": "Electromagnetic induction", "text": "Electromagnetic induction\n\nElectromagnetic or magnetic induction is the production of an electromotive force (i.e., voltage) across an electrical conductor in a changing magnetic field.\n\nMichael Faraday is generally credited with the discovery of induction in 1831, and James Clerk Maxwell mathematically described it as Faraday's law of induction. Lenz's law describes the direction of the induced field. Faraday's law was later generalized to become the Maxwell–Faraday equation, one of the four Maxwell equations in his theory of electromagnetism.\n\nElectromagnetic induction has found many applications, including electrical components such as inductors and transformers, and devices such as electric motors and generators.\n\nElectromagnetic induction was discovered by Michael Faraday, published in 1831. It was discovered independently by Joseph Henry in 1832.\n\nIn Faraday's first experimental demonstration (August 29, 1831), he wrapped two wires around opposite sides of an iron ring or \"torus\" (an arrangement similar to a modern toroidal transformer). Based on his understanding of electromagnets, he expected that, when current started to flow in one wire, a sort of wave would travel through the ring and cause some electrical effect on the opposite side. He plugged one wire into a galvanometer, and watched it as he connected the other wire to a battery. He saw a transient current, which he called a \"wave of electricity\", when he connected the wire to the battery and another when he disconnected it. This induction was due to the change in magnetic flux that occurred when the battery was connected and disconnected. Within two months, Faraday found several other manifestations of electromagnetic induction. For example, he saw transient currents when he quickly slid a bar magnet in and out of a coil of wires, and he generated a steady (DC) current by rotating a copper disk near the bar magnet with a sliding electrical lead (\"Faraday's disk\").\n\nFaraday explained electromagnetic induction using a concept he called lines of force. However, scientists at the time widely rejected his theoretical ideas, mainly because they were not formulated mathematically. An exception was James Clerk Maxwell, who used Faraday's ideas as the basis of his quantitative electromagnetic theory. In Maxwell's model, the time varying aspect of electromagnetic induction is expressed as a differential equation, which Oliver Heaviside referred to as Faraday's law even though it is slightly different from Faraday's original formulation and does not describe motional EMF. Heaviside's version (see Maxwell–Faraday equation below) is the form recognized today in the group of equations known as Maxwell's equations.\n\nIn 1834 Heinrich Lenz formulated the law named after him to describe the \"flux through the circuit\". Lenz's law gives the direction of the induced EMF and current resulting from electromagnetic induction.\n\nFaraday's law of induction makes use of the magnetic flux Φ through a region of space enclosed by a wire loop. The magnetic flux is defined by a surface integral:\nwhere \"dA is an element of the surface Σ enclosed by the wire loop, B is the magnetic field. The dot product B·\"dA corresponds to an infinitesimal amount of magnetic flux. In more visual terms, the magnetic flux through the wire loop is proportional to the number of magnetic flux lines that pass through the loop.\n\nWhen the flux through the surface changes, Faraday's law of induction says that the wire loop acquires an electromotive force (EMF). The most widespread version of this law states that the induced electromotive force in any closed circuit is equal to the rate of change of the magnetic flux enclosed by the circuit:\nwhere formula_3 is the EMF and Φ is the magnetic flux. The direction of the electromotive force is given by Lenz's law which states that an induced current will flow in the direction that will oppose the change which produced it. This is due to the negative sign in the previous equation. To increase the generated EMF, a common approach is to exploit flux linkage by creating a tightly wound coil of wire, composed of \"N\" identical turns, each with the same magnetic flux going through them. The resulting EMF is then \"N\" times that of one single wire.\n\nGenerating an EMF through a variation of the magnetic flux through the surface of a wire loop can be achieved in several ways:\n\nIn general, the relation between the EMF formula_5 in a wire loop encircling a surface Σ, and the electric field E in the wire is given by\nwhere \"d\"ℓ is an element of contour of the surface Σ, combining this with the definition of flux\n\nwe can write the integral form of the Maxwell–Faraday equation\n\nIt is one of the four Maxwell's equations, and therefore plays a fundamental role in the theory of classical electromagnetism.\n\nFaraday's law describes two different phenomena: the \"motional EMF\" generated by a magnetic force on a moving wire (see Lorentz force), and the \"transformer EMF\" this is generated by an electric force due to a changing magnetic field (due to the differential form of the Maxwell–Faraday equation). James Clerk Maxwell drew attention to the separate physical phenomena in 1861. This is believed to be a unique example in physics of where such a fundamental law is invoked to explain two such different phenomena.\n\nEinstein noticed that the two situations both corresponded to a relative movement between a conductor and a magnet, and the outcome was unaffected by which one was moving. This was one of the principal paths that led him to develop special relativity.\n\nThe principles of electromagnetic induction are applied in many devices and systems, including:\n\nThe EMF generated by Faraday's law of induction due to relative movement of a circuit and a magnetic field is the phenomenon underlying electrical generators. When a permanent magnet is moved relative to a conductor, or vice versa, an electromotive force is created. If the wire is connected through an electrical load, current will flow, and thus electrical energy is generated, converting the mechanical energy of motion to electrical energy. For example, the \"drum generator\" is based upon the figure to the bottom-right. A different implementation of this idea is the Faraday's disc, shown in simplified form on the right.\n\nIn the Faraday's disc example, the disc is rotated in a uniform magnetic field perpendicular to the disc, causing a current to flow in the radial arm due to the Lorentz force. Mechanical work is necessary to drive this current. When the generated current flows through the conducting rim, a magnetic field is generated by this current through Ampère's circuital law (labelled \"induced B\" in the figure). The rim thus becomes an electromagnet that resists rotation of the disc (an example of Lenz's law). On the far side of the figure, the return current flows from the rotating arm through the far side of the rim to the bottom brush. The B-field induced by this return current opposes the applied B-field, tending to \"decrease\" the flux through that side of the circuit, opposing the \"increase\" in flux due to rotation. On the near side of the figure, the return current flows from the rotating arm through the near side of the rim to the bottom brush. The induced B-field \"increases\" the flux on this side of the circuit, opposing the \"decrease\" in flux due to rotation. Thus, both sides of the circuit generate an EMF opposing the rotation. The energy required to keep the disc moving, despite this reactive force, is exactly equal to the electrical energy generated (plus energy wasted due to friction, Joule heating, and other inefficiencies). This behavior is common to all generators converting mechanical energy to electrical energy.\n\nWhen the electric current in a loop of wire changes, the changing current creates a changing magnetic field. A second wire in reach of this magnetic field will experience this change in magnetic field as a change in its coupled magnetic flux, \"d\" Φ / \"d t\". Therefore, an electromotive force is set up in the second loop called the induced EMF or transformer EMF. If the two ends of this loop are connected through an electrical load, current will flow.\n\nA current clamp is a type of transformer with a split core which can be spread apart and clipped onto a wire or coil to either measure the current in it or, in reverse, to induce a voltage. Unlike conventional instruments the clamp does not make electrical contact with the conductor or require it to be disconnected during attachment of the clamp.\n\nFaraday's law is used for measuring the flow of electrically conductive liquids and slurries. Such instruments are called magnetic flow meters. The induced voltage ℇ generated in the magnetic field \"B\" due to a conductive liquid moving at velocity \"v\" is thus given by:\n\nwhere ℓ is the distance between electrodes in the magnetic flow meter.\n\nConductors (of finite dimensions) moving through a uniform magnetic field, or stationary within a changing magnetic field, will have currents induced within them. These induced eddy currents can be undesirable, since they dissipate energy in the resistance of the conductor.\nThere are a number of methods employed to control these undesirable inductive effects.\n\nEddy currents occur when a solid metallic mass is rotated in a magnetic field, because the outer portion of the metal cuts more lines of force than the inner portion, hence the induced electromotive force not being uniform, tends to set up currents between the points of greatest and least potential. Eddy currents consume a considerable amount of energy and often cause a harmful rise in temperature.\nOnly five laminations or plates are shown in this example, so as to show the subdivision of the eddy currents. In practical use, the number of laminations or punchings ranges from 40 to 66 per inch, and brings the eddy current loss down to about one percent. While the plates can be separated by insulation, the voltage is so low that the natural rust/oxide coating of the plates is enough to prevent current flow across the laminations.\nThis is a rotor approximately 20mm in diameter from a DC motor used in a Note the laminations of the electromagnet pole pieces, used to limit parasitic inductive losses.\n\nIn this illustration, a solid copper bar conductor on a rotating armature is just passing under the tip of the pole piece N of the field magnet. Note the uneven distribution of the lines of force across the copper bar. The magnetic field is more concentrated and thus stronger on the left edge of the copper bar (a,b) while the field is weaker on the right edge (c,d). Since the two edges of the bar move with the same velocity, this difference in field strength across the bar creates whorls or current eddies within the copper bar.\n\nHigh current power-frequency devices, such as electric motors, generators and transformers, use multiple small conductors in parallel to break up the eddy flows that can form within large solid conductors. The same principle is applied to transformers used at higher than power frequency, for example, those used in switch-mode power supplies and the intermediate frequency coupling transformers of radio receivers.\nNotes\nReferences\n\n"}
{"id": "992706", "url": "https://en.wikipedia.org/wiki?curid=992706", "title": "Endowment (philosophy)", "text": "Endowment (philosophy)\n\nEndowment (in philosophy) refers to the innate capacities of an individual, group, or institution. An individual's \"natural endowment\" can be abilities, such as intelligence or strength, given at birth. An individual's \"social endowment\" can be abilities attributed to the individual's position within a social hierarchy. \n\nAccording to Stephen R. Covey, author of The Seven Habits of Highly Effective People, there are seven human endowments. They are listed below.\n\n1) Self-awareness or self-knowledge\n2) Imagination and conscience \n3) Volition or will power\n\n4) An abundance mentality\n\n5) Courage and consideration\n\n6) Creativity\n\n7) Self-renewal\n\n\n"}
{"id": "105979", "url": "https://en.wikipedia.org/wiki?curid=105979", "title": "Exclusive or", "text": "Exclusive or\n\nExclusive or or exclusive disjunction is a logical operation that outputs true only when inputs differ (one is true, the other is false).\n\nIt is symbolized by the prefix operator J and by the infix operators XOR (), EOR, EXOR, ⊻, ⩒, ⩛, ⊕, ↮, and ≢. The negation of XOR is logical biconditional, which outputs true only when both inputs are the same.\n\nIt gains the name \"exclusive or\" because the meaning of \"or\" is ambiguous when both operands are true; the exclusive or operator \"excludes\" that case. This is sometimes thought of as \"one or the other but not both\". This could be written as \"A or B, but not, A and B\".\n\nMore generally, XOR is true only when an odd number of inputs are true. A chain of XORs—\"a\" XOR \"b\" XOR \"c\" XOR \"d\" (and so on)—is true whenever an odd number of the inputs are true and is false whenever an even number of inputs are true.\n\nThe truth table of A XOR B shows that it outputs true whenever the inputs differ:\n\n\nExclusive disjunction essentially means 'either one, but not both nor none'. In other words, the statement is true if and only if one is true and the other is false. For example, if two horses are racing, then one of the two will win the race, but not both of them. The exclusive disjunction formula_1, also denoted by formula_2 ⩛ formula_3 or formula_4, can be expressed in terms of the logical conjunction (\"logical and\", formula_5), the disjunction (\"logical or\", formula_6), and the negation (formula_7) as follows:\n\nThe exclusive disjunction formula_1 can also be expressed in the following way:\n\nThis representation of XOR may be found useful when constructing a circuit or network, because it has only one formula_7 operation and small number of formula_5 and formula_6 operations. A proof of this identity is given below:\n\nIt is sometimes useful to write formula_1 in the following way:\nor:\n\nThis equivalence can be established by applying De Morgan's laws twice to the fourth line of the above proof.\n\nThe exclusive or is also equivalent to the negation of a logical biconditional, by the rules of material implication (a material conditional is equivalent to the disjunction of the negation of its antecedent and its consequence) and material equivalence.\n\nIn summary, we have, in mathematical and in engineering notation:\n\nAlthough the operators formula_5 (conjunction) and formula_6 (disjunction) are very useful in logic systems, they fail a more generalizable structure in the following way:\n\nThe systems formula_21 and formula_22 are monoids, but neither is a group. This unfortunately prevents the combination of these two systems into larger structures, such as a mathematical ring.\n\nHowever, the system using exclusive or formula_23 is an abelian group. The combination of operators formula_5 and formula_25 over elements formula_26 produce the well-known field formula_27. This field can represent any logic obtainable with the system formula_28 and has the added benefit of the arsenal of algebraic analysis tools for fields.\n\nMore specifically, if one associates formula_29 with 0 and formula_30 with 1, one can interpret the logical \"AND\" operation as multiplication on formula_27 and the \"XOR\" operation as addition on formula_27:\n\nUsing this basis to describe a boolean system is referred to as algebraic normal form.\n\nThe Oxford English Dictionary explains \"either ... or\" as follows:\nThe exclusive-or explicitly states \"one or the other, but not neither nor both.\" However, the mapping correspondence between formal Boolean operators and natural language conjunctions is far from simple or one-to-one, and has been studied for decades in linguistics and analytic philosophy.\n\nFollowing this kind of common-sense intuition about \"or\", it is sometimes argued that in many natural languages, English included, the word \"or\" has an \"exclusive\" sense. The exclusive disjunction of a pair of propositions, (\"p\", \"q\"), is supposed to mean that \"p\" is true or \"q\" is true, but not both. For example, it might be argued that the normal intention of a statement like \"You may have coffee, or you may have tea\" is to stipulate that exactly one of the conditions can be true. Certainly under some circumstances a sentence like this example should be taken as forbidding the possibility of one's accepting both options. Even so, there is good reason to suppose that this sort of sentence is not disjunctive at all. If all we know about some disjunction is that it is true overall, we cannot be sure which of its disjuncts is true. For example, if a woman has been told that her friend is either at the snack bar or on the tennis court, she cannot validly infer that he is on the tennis court. But if her waiter tells her that she may have coffee or she may have tea, she can validly infer that she may have tea. Nothing classically thought of as a disjunction has this property. This is so even given that she might reasonably take her waiter as having denied her the possibility of having both coffee and tea.\n\nIn English, the construct \"either ... or\" is usually used to indicate exclusive or and \"or\" generally used for inclusive. But in Spanish, the word \"\"o\" (or) can be used in the form \"p o q\" (exclusive) or the form \"o p o q\"\" (inclusive). Some may contend that any binary or other n-ary exclusive \"or\" is true if and only if it has an odd number of true inputs (this is not, however, the only reasonable definition; for example, digital xor gates with multiple inputs typically do not use that definition), and that there is no conjunction in English that has this general property. For example, Barrett and Stenner contend in the 1971 article \"The Myth of the Exclusive 'Or (Mind, 80 (317), 116–121) that no author has produced an example of an English or-sentence that appears to be false because both of its inputs are true, and brush off or-sentences such as \"The light bulb is either on or off\" as reflecting particular facts about the world rather than the nature of the word \"or\". However, the \"barber paradox\"—Everybody in town shaves himself or is shaved by the barber, who shaves the barber? -- would not be paradoxical if \"or\" could not be exclusive (although a purist could say that \"either\" is required in the statement of the paradox).\n\nWhether these examples can be considered \"natural language\" is another question. Certainly when one sees a menu stating \"Lunch special: sandwich and soup or salad\" (parsed as \"sandwich and (soup or salad)\" according to common usage in the restaurant trade), one would not expect to be permitted to order both soup and salad. Nor would one expect to order neither soup nor salad, because that belies the nature of the \"special\", that ordering the two items together is cheaper than ordering them a la carte. Similarly, a lunch special consisting of one meat, French fries or mashed potatoes and vegetable would consist of three items, only one of which would be a form of potato. If one wanted to have meat and both kinds of potatoes, one would ask if it were possible to substitute a second order of potatoes for the vegetable. And, one would not expect to be permitted to have both types of potato and vegetable, because the result would be a vegetable plate rather than a meat plate.\n\nThe symbol used for exclusive disjunction varies from one field of application to the next, and even depends on the properties being emphasized in a given context of discussion. In addition to the abbreviation \"XOR\", any of the following symbols may also be seen:\n\nIf using binary values for true (1) and false (0), then \"exclusive or\" works exactly like addition modulo 2.\n\nExclusive disjunction is often used for bitwise operations. Examples:\n\nAs noted above, since exclusive disjunction is identical to addition modulo 2, the bitwise exclusive disjunction of two \"n\"-bit strings is identical to the standard vector of addition in the vector space formula_38.\n\nIn computer science, exclusive disjunction has several uses:\n\nIn logical circuits, a simple adder can be made with an XOR gate to add the numbers, and a series of AND, OR and NOT gates to create the carry output.\n\nOn some computer architectures, it is more efficient to store a zero in a register by XOR-ing the register with itself (bits XOR-ed with themselves are always zero) instead of loading and storing the value zero.\n\nIn simple threshold activated neural networks, modeling the XOR function requires a second layer because XOR is not a linearly separable function.\n\nExclusive-or is sometimes used as a simple mixing function in cryptography, for example, with one-time pad or Feistel network systems.\n\nExclusive-or is also heavily used in block ciphers such as AES (Rijndael) or Serpent and in block cipher implementation (CBC, CFB, OFB or CTR).\n\nSimilarly, XOR can be used in generating entropy pools for hardware random number generators. The XOR operation preserves randomness, meaning that a random bit XORed with a non-random bit will result in a random bit. Multiple sources of potentially random data can be combined using XOR, and the unpredictability of the output is guaranteed to be at least as good as the best individual source.\n\nXOR is used in RAID 3–6 for creating parity information. For example, RAID can \"back up\" bytes and from two (or more) hard drives by XORing the just mentioned bytes, resulting in () and writing it to another drive. Under this method, if any one of the three hard drives are lost, the lost byte can be re-created by XORing bytes from the remaining drives. For instance, if the drive containing is lost, and can be XORed to recover the lost byte.\n\nXOR is also used to detect an overflow in the result of a signed binary arithmetic operation. If the leftmost retained bit of the result is not the same as the infinite number of digits to the left, then that means overflow occurred. XORing those two bits will give a \"1\" if there is an overflow.\n\nXOR can be used to swap two numeric variables in computers, using the XOR swap algorithm; however this is regarded as more of a curiosity and not encouraged in practice.\n\nXOR linked lists leverage XOR properties in order to save space to represent doubly linked list data structures.\n\nIn computer graphics, XOR-based drawing methods are often used to manage such items as bounding boxes and cursors on systems without alpha channels or overlay planes.\n\nApart from the ASCII codes, the operator is encoded at and , both in block Mathematical Operators.\n\n"}
{"id": "192355", "url": "https://en.wikipedia.org/wiki?curid=192355", "title": "Functionalism (philosophy of mind)", "text": "Functionalism (philosophy of mind)\n\nFunctionalism is a view in the theory of the mind. It states that mental states (beliefs, desires, being in pain, etc.) are constituted solely by their functional role – that is, they have causal relations to other mental states, numerous sensory inputs, and behavioral outputs. Functionalism developed largely as an alternative to the identity theory of mind and behaviorism.\n\nFunctionalism is a theoretical level between the physical implementation and behavioral output. Therefore, it is different from its predecessors of Cartesian dualism (advocating independent mental and physical substances) and Skinnerian behaviorism and physicalism (declaring only physical substances) because it is only concerned with the effective functions of the brain, through its organization or its \"software programs\".\n\nSince mental states are identified by a functional role, they are said to be realized on multiple levels; in other words, they are able to be manifested in various systems, even perhaps computers, so long as the system performs the appropriate functions. While computers are physical devices with electronic substrate that perform computations on inputs to give outputs, so brains are physical devices with neural substrate that perform computations on inputs which produce behaviors.\n\nAn important part of some accounts of functionalism is the idea of multiple realizability. Since, according to standard functionalist theories, mental states are the corresponding functional role, mental states can be sufficiently explained without taking into account the underlying physical medium (e.g. the brain, neurons, etc.) that realizes such states; one need only take into account the higher-level functions in the cognitive system. Since mental states are not limited to a particular medium, they can be realized in multiple ways, including, theoretically, within non-biological systems, such as computers. In other words, a silicon-based machine could, in principle, have the same sort of mental life that a human being has, provided that its cognitive system realized the proper functional roles. Thus, mental states are individuated much like a valve; a valve can be made of plastic or metal or whatever material, as long as it performs the proper function (say, controlling the flow of liquid through a tube by blocking and unblocking its pathway).\n\nHowever, there have been some functionalist theories that combine with the identity theory of mind, which deny multiple realizability. Such \"Functional Specification Theories\" (FSTs) (Levin, § 3.4), as they are called, were most notably developed by David Lewis and David Malet Armstrong. According to FSTs, mental states are the particular \"realizers\" of the functional role, not the functional role itself. The mental state of belief, for example, just is whatever brain or neurological process that realizes the appropriate belief function. Thus, unlike standard versions of functionalism (often called \"Functional State Identity Theories\"), FSTs do not allow for the multiple realizability of mental states, because the fact that mental states are realized by brain states is essential. What often drives this view is the belief that if we were to encounter an alien race with a cognitive system composed of significantly different material from humans' (e.g., silicon-based) but performed the same functions as human mental states (e.g., they tend to yell \"Yowzass!\" when poked with sharp objects, etc.) then we would say that their type of mental state is perhaps similar to ours, but too different to say it's the same. For some, this may be a disadvantage to FSTs. Indeed, one of Hilary Putnam's arguments for his version of functionalism relied on the intuition that such alien creatures would have the same mental states as humans do, and that the multiple realizability of standard functionalism makes it a better theory of mind.\n\nThe broad position of \"functionalism\" can be articulated in many different varieties. The first formulation of a functionalist theory of mind was put forth by Hilary Putnam in the 1960s. This formulation, which is now called machine-state functionalism, or just machine functionalism, was inspired by the analogies which Putnam and others noted between the mind and the theoretical \"machines\" or computers capable of computing any given algorithm which were developed by Alan Turing (called Turing machines). It should be noted that Putnam himself, by the mid-1970s, had begun questioning this position. The beginning of his opposition to machine-state functionalism can be read about in his Twin Earth thought experiment.\n\nIn non-technical terms, a Turing machine is not a physical object, but rather an abstract machine built upon a mathematical model. Typically, a Turing Machine has a horizontal tape divided into rectangular cells arranged from left to right. The tape itself is infinite in length, and each cell may contain a symbol. The symbols used for any given \"machine\" can vary. The machine has a \"read-write head\" that scans cells and moves in left and right directions. The action of the machine is determined by the symbol in the cell being scanned and a table of transition rules that serve as the machine's programming. Because of the infinite tape, a traditional Turing Machine has an infinite amount of time to compute any particular function or any number of functions. In the below example, each cell is either blank (\"B\") or has a \"1\" written on it. These are the inputs to the machine. The possible outputs are:\n\n\nAn extremely simple example of a Turing machine which\nwrites out the sequence '111' after scanning three blank squares and then stops as specified by the following machine table:\n\nThis table states that if the machine is in state one and scans a blank square (\"B\"), it will print a \"1\" and remain in state one. If it is in state one and reads a \"1\", it will move one square to the right and also go into state two. If it is in state two and reads a \"B\", it will print a \"1\" and stay in state two. If it is in state two and reads a \"1\", it will move one square to the right and go into state three. If it is in state three and reads a \"B\", it prints a \"1\" and remains in state three. Finally, if it is in state three and reads a \"1\", then it will stay in state three.\n\nThe essential point to consider here is the \"nature of the states\" of the Turing machine. Each state can be defined exclusively in terms of its relations to the other states as well as inputs and outputs. State one, for example, is simply the state in which the machine, if it reads a \"B\", writes a \"1\" and stays in that state, and in which, if it reads a \"1\", it moves one square to the right and goes into a different state. This is the functional definition of state one; it is its causal role in the overall system. The details of how it accomplishes what it accomplishes and of its material constitution are completely irrelevant. \n\nThe above point is critical to an understanding of machine-state functionalism. Since Turing machines are not required to be physical systems, \"anything capable of going through a succession of states in time can be a Turing machine\". Because biological organisms “go through a succession of states in time”, any such organisms could also be equivalent to Turing machines. \n\nAccording to machine-state functionalism, the nature of a mental state is just like the nature of the Turing machine states described above. If one can show the rational functioning and computing skills of these machines to be comparable to the rational functioning and computing skills of human beings, it follows that Turing machine behavior closely resembles that of human beings. Therefore, it is not a particular physical-chemical composition responsible for the particular machine or mental state, it is the programming rules which produce the effects that are responsible. To put it another way, any rational preference is due to the rules being followed, not to the specific material composition of the agent.\n\nA second form of functionalism is based on the rejection of behaviorist theories in psychology and their replacement with empirical cognitive models of the mind. This view is most closely associated with Jerry Fodor and Zenon Pylyshyn and has been labeled psycho-functionalism.\n\nThe fundamental idea of psycho-functionalism is that psychology is an irreducibly complex science and that the terms that we use to describe the entities and properties of the mind in our best psychological theories cannot be redefined in terms of simple behavioral dispositions, and further, that such a redefinition would not be desirable or salient were it achievable. Psychofunctionalists view psychology as employing the same sorts of irreducibly teleological or purposive explanations as the biological sciences. Thus, for example, the function or role of the heart is to pump blood, that of the kidney is to filter it and to maintain certain chemical balances and so on—this is what accounts for the purposes of scientific explanation and taxonomy. There may be an infinite variety of physical realizations for all of the mechanisms, but what is important is only their role in the overall biological theory. In an analogous manner, the role of mental states, such as belief and desire, is determined by the functional or causal role that is designated for them within our best \"scientific\" psychological theory. If some mental state which is postulated by folk psychology (e.g. hysteria) is determined not to have any fundamental role in cognitive psychological explanation, then that particular state may be considered not to exist .\nOn the other hand, if it turns out that there are states which theoretical cognitive psychology posits as necessary for explanation of human behavior but which are not foreseen by ordinary folk psychological language, then these entities or states exist.\n\nA third form of functionalism is concerned with the meanings of theoretical terms in general. This view is most closely associated with David Lewis and is often referred to as analytic functionalism or conceptual functionalism. The basic idea of analytic functionalism is that theoretical terms are implicitly defined by the theories in whose formulation they occur and not by intrinsic properties of the phonemes they comprise. In the case of ordinary language terms, such as \"belief\", \"desire\", or \"hunger\", the idea is that such terms get their meanings from our common-sense \"folk psychological\" theories about them, but that such conceptualizations are not sufficient to withstand the rigor imposed by materialistic theories of reality and causality. Such terms are subject to conceptual analyses which take something like the following form:\n\nFor example, the state of pain is \"caused\" by sitting on a tack and \"causes\" loud cries, and higher order mental states of anger and resentment directed at the careless person who left a tack lying around. These sorts of functional definitions in terms of causal roles are claimed to be \"analytic\" and \"a priori\" truths about the submental states and the (largely fictitious) propositional attitudes they describe. Hence, its proponents are known as \"analytic\" or \"conceptual\" functionalists. The essential difference between analytic and psychofunctionalism is that the latter emphasizes the importance of laboratory observation and experimentation in the determination of which mental state terms and concepts are genuine and which functional identifications may be considered to be genuinely contingent and \"a posteriori\" identities. The former, on the other hand, claims that such identities are necessary and not subject to empirical scientific investigation.\n\nHomuncular functionalism was developed largely by Daniel Dennett and has been advocated by William Lycan. It arose in response to the challenges that Ned Block's China Brain (a.k.a. Chinese nation) and John Searle's Chinese room thought experiments presented for the more traditional forms of functionalism (see below under \"Criticism\"). In attempting to overcome the conceptual difficulties that arose from the idea of a nation full of Chinese people wired together, each person working as a single neuron to produce in the wired-together whole the functional mental states of an individual mind, many functionalists simply bit the bullet, so to speak, and argued that such a Chinese nation would indeed possess all of the qualitative and intentional properties of a mind; i.e. it would become a sort of systemic or collective mind with propositional attitudes and other mental characteristics. Whatever the worth of this latter hypothesis, it was immediately objected that it entailed an unacceptable sort of mind-mind supervenience: the \"systemic\" mind which somehow emerged at the higher-level must necessarily supervene on the individual minds of each individual member of the Chinese nation, to stick to Block's formulation. But this would seem to put into serious doubt, if not directly contradict, the fundamental idea of the supervenience thesis: there can be no change in the mental realm without some change in the underlying physical substratum. This can be easily seen if we label the set of mental facts that occur at the higher-level \"M1\" and the set of mental facts that occur at the lower-level \"M2\". Given the transitivity of supervenience, if \"M1\" supervenes on \"M2\", and \"M2\" supervenes on \"P\" (physical base), then \"M1\" and \"M2\" both supervene on \"P\", even though they are (allegedly) totally different sets of mental facts.\n\nSince mind-mind supervenience seemed to have become acceptable in functionalist circles, it seemed to some that the only way to resolve the puzzle was to postulate the existence of an entire hierarchical series of mind levels (analogous to homunculi) which became less and less sophisticated in terms of functional organization and physical composition all the way down to the level of the physico-mechanical neuron or group of neurons. The homunculi at each level, on this view, have authentic mental properties but become simpler and less intelligent as one works one's way down the hierarchy.\n\nMechanistic functionalism, originally formulated and defended by Gualtiero Piccinini and Carl Gillett independently, augments previous functionalist accounts of mental states by maintaining that any psychological explanation must be rendered in mechanistic terms. That is, instead of mental states receiving a purely functional explanation in terms of their relations to other mental states, like those listed above, functions are seen as playing only a part—the other part being played by structures— of the explanation of a given mental state.\nA mechanistic explanation involves decomposing a given system, in this case a mental system, into its component physical parts, their activities or functions, and their combined organizational relations. On this account the mind remains a functional system, but one that is understood mechanistically. This account remains a sort of functionalism because functional relations are still essential to mental states, but it is mechanistic because the functional relations are always manifestations of concrete structures—albeit structures understood at a certain level of abstraction. Functions are individuated and explained either in terms of the contributions they make to the given system or in teleological terms. If the functions are understood in teleological terms, then they may be characterized either etiologically or non-etiologically. \nMechanistic functionalism leads functionalism away from the traditional functionalist autonomy of psychology from neuroscience and towards integrating psychology and neuroscience. By providing an applicable framework for merging traditional psychological models with neurological data, mechanistic functionalism may be understood as reconciling the functionalist theory of mind with neurological accounts of how the brain actually works. This is due to the fact that mechanistic explanations of function attempt to provide an account of how functional states (mental states) are physically realized through neurological mechanisms.\n\nThere is much confusion about the sort of relationship that is claimed to exist (or not exist) between the general thesis of functionalism and physicalism. It has often been claimed that functionalism somehow \"disproves\" or falsifies physicalism \"tout court\" (i.e. without further explanation or description). On the other hand, most philosophers of mind who are functionalists claim to be physicalists—indeed, some of them, such as David Lewis, have claimed to be strict reductionist-type physicalists.\n\nFunctionalism is fundamentally what Ned Block has called a broadly metaphysical thesis as opposed to a narrowly ontological one. That is, functionalism is not so much concerned with \"what there is\" than with what it is that characterizes a certain type of mental state, e.g. pain, as the type of state that it is. Previous attempts to answer the mind-body problem have all tried to resolve it by answering \"both\" questions: dualism says there are two substances and that mental states are characterized by their immateriality; behaviorism claimed that there was one substance and that mental states were behavioral disposition; physicalism asserted the existence of just one substance and characterized the mental states as physical states (as in \"pain = C-fiber firings\").\n\nOn this understanding, type physicalism can be seen as incompatible with functionalism, since it claims that what characterizes mental states (e.g. pain) is that they are physical in nature, while functionalism says that what characterizes pain is its functional/causal role and its relationship with yelling \"ouch\", etc. However, any weaker sort of physicalism which makes the simple ontological claim that everything that exists is made up of physical matter is perfectly compatible with functionalism. Moreover, most functionalists who are physicalists require that the properties that are quantified over in functional definitions be physical properties. Hence, they \"are\" physicalists, even though the general thesis of functionalism itself does not commit them to being so.\n\nIn the case of David Lewis, there is a distinction in the concepts of \"having pain\" (a rigid designator true of the same things in all possible worlds) and just \"pain\" (a non-rigid designator). Pain, for Lewis, stands for something like the definite description \"the state with the causal role x\". The referent of the description in humans is a type of brain state to be determined by science. The referent among silicon-based life forms is something else. The referent of the description among angels is some immaterial, non-physical state. For Lewis, therefore, \"local\" type-physical reductions are possible and compatible with conceptual functionalism. (See also Lewis's mad pain and Martian pain.) There seems to be some confusion between types and tokens that needs to be cleared up in the functionalist analysis.\n\nNed Block argues against the functionalist proposal of multiple realizability, where hardware implementation is irrelevant because only the functional level is important. The \"China brain\" or \"Chinese nation\" thought experiment involves supposing that the entire nation of China systematically organizes itself to operate just like a brain, with each individual acting as a neuron. (The tremendous difference in speed of operation of each unit is not addressed.). According to functionalism, so long as the people are performing the proper functional roles, with the proper causal relations between inputs and outputs, the system will be a real mind, with mental states, consciousness, and so on. However, Block argues, this is patently absurd, so there must be something wrong with the thesis of functionalism since it would allow this to be a legitimate description of a mind.\n\nSome functionalists believe China would have qualia but that due to the size it is impossible to imagine China being conscious. Indeed, it may be the case that we are constrained by our theory of mind and will never be able to understand what Chinese-nation consciousness is like. Therefore, if functionalism is true either qualia will exist across all hardware or will not exist at all but are illusory.\n\nThe Chinese room argument by John Searle is a direct attack on the claim that thought can be represented as a set of functions. The thought experiment asserts that it is possible to mimic intelligent action without any interpretation or understanding through the use of a purely functional system. In short, Searle describes a person who only speaks English who is in a room with only Chinese symbols in baskets and a rule book in English for moving the symbols around. The person is then ordered by people outside of the room to follow the rule book for sending certain symbols out of the room when given certain symbols. Further suppose that the people outside of the room are Chinese speakers and are communicating with the person inside via the Chinese symbols. According to Searle, it would be absurd to claim that the English speaker inside knows Chinese simply based on these syntactic processes. This thought experiment attempts to show that systems which operate merely on syntactic processes (inputs and outputs, based on algorithms) cannot realize any semantics (meaning) or intentionality (aboutness). Thus, Searle attacks the idea that thought can be equated with following a set of syntactic rules; that is, functionalism is an insufficient theory of the mind.\n\nAs noted above, in connection with Block's Chinese nation, many functionalists responded to Searle's thought experiment by suggesting that there was a form of mental activity going on at a higher level than the man in the Chinese room could comprehend (the so-called \"system reply\"); that is, the system does know Chinese. Of course, Searle responds that there is nothing more than syntax going on at the higher-level as well, so this reply is subject to the same initial problems. Furthermore, Searle suggests the man in the room could simply memorize the rules and symbol relations. Again, though he would convincingly mimic communication, he would be aware only of the symbols and rules, not of the meaning behind them.\n\nAnother main criticism of functionalism is the inverted spectrum or inverted qualia scenario, most specifically proposed as an objection to functionalism by Ned Block. This thought experiment involves supposing that there is a person, call her Jane, that is born with a condition which makes her see the opposite spectrum of light that is normally perceived. Unlike normal people, Jane sees the color violet as yellow, orange as blue, and so forth. So, suppose, for example, that you and Jane are looking at the same orange. While you perceive the fruit as colored orange, Jane sees it as colored blue. However, when asked what color the piece of fruit is, both you and Jane will report \"orange\". In fact, one can see that all of your behavioral as well as functional relations to colors will be the same. Jane will, for example, properly obey traffic signs just as any other person would, even though this involves the color perception. Therefore, the argument goes, since there can be two people who are functionally identical, yet have different mental states (differing in their qualitative or phenomenological aspects), functionalism is not robust enough to explain individual differences in qualia.\n\nDavid Chalmers tries to show that even though mental content cannot be fully accounted for in functional terms, there is nevertheless a \"nomological correlation\" between mental states and functional states in this world. A silicon-based robot, for example, whose functional profile matched our own, would \"have\" to be fully conscious. His argument for this claim takes the form of a \"reductio ad absurdum\". The general idea is that since it would be very unlikely for a conscious human being to experience a change in its qualia which it utterly fails to notice, mental content and functional profile appear to be inextricably bound together, at least in the human case. If the subject's qualia were to change, we would expect the subject to notice, and therefore his functional profile to follow suit. A similar argument is applied to the notion of \"absent\" qualia. In this case, Chalmers argues that it would be very unlikely for a subject to experience a fading of his qualia which he fails to notice and respond to. This, coupled with the independent assertion that a conscious being's functional profile just could be maintained, irrespective of its experiential state, leads to the conclusion that the subject of these experiments would remain fully conscious. The problem with this argument, however, as Brian G. Crabb (2005) has observed, is that it begs the central question: How could Chalmers \"know\" that functional profile can be preserved, for example while the conscious subject's brain is being supplanted with a silicon substitute, unless he already assumes that the subject's possibly changing qualia would not be a determining factor? And while changing or fading qualia in a conscious subject might force changes in its functional profile, this tells us nothing about the case of a permanently inverted or unconscious robot. A subject with inverted qualia from birth would have nothing to notice or adjust to. Similarly, an unconscious functional simulacrum of ourselves (a zombie) would have no experiential changes to notice or adjust to. Consequently, Crabb argues, Chalmers' \"fading qualia\" and \"dancing qualia\" arguments fail to establish that cases of permanently inverted or absent qualia are nomologically impossible.\n\nA related critique of the inverted spectrum argument is that it assumes that mental states (differing in their qualitative or phenomenological aspects) can be independent of the functional relations in the brain. Thus, it begs the question of functional mental states: its assumption denies the possibility of functionalism itself, without offering any independent justification for doing so. (Functionalism says that mental states are produced by the functional relations in the brain.) This same type of problem—that there is no argument, just an antithetical assumption at their base—can also be said of both the Chinese room and the Chinese nation arguments. Notice, however, that Crabb's response to Chalmers does not commit this fallacy: His point is the more restricted observation that \"even if\" inverted or absent qualia turn out to be nomologically impossible, and it is perfectly possible that we might subsequently discover this fact by other means, Chalmers' argument fails to demonstrate that they are impossible.\n\nThe Twin Earth thought experiment, introduced by Hilary Putnam, is responsible for one of the main arguments used against functionalism, although it was originally intended as an argument against semantic internalism. The thought experiment is simple and runs as follows. Imagine a Twin Earth which is identical to Earth in every way but one: water does not have the chemical structure H₂O, but rather some other structure, say XYZ. It is critical, however, to note that XYZ on Twin Earth is still called \"water\" and exhibits all the same macro-level properties that H₂O exhibits on Earth (i.e., XYZ is also a clear drinkable liquid that is in lakes, rivers, and so on). Since these worlds are identical in every way except in the underlying chemical structure of water, you and your Twin Earth doppelgänger see exactly the same things, meet exactly the same people, have exactly the same jobs, behave exactly the same way, and so on. In other words, since you share the same inputs, outputs, and relations between other mental states, you are functional duplicates. So, for example, you both believe that water is wet. However, the content of your mental state of believing that water is wet differs from your duplicate's because your belief is of H₂O, while your duplicate's is of XYZ. Therefore, so the argument goes, since two people can be functionally identical, yet have different mental states, functionalism cannot sufficiently account for all mental states.\n\nMost defenders of functionalism initially responded to this argument by attempting to maintain a sharp distinction between internal and external content. The internal contents of propositional attitudes, for example, would consist exclusively in those aspects of them which have no relation with the external world \"and\" which bear the necessary functional/causal properties that allow for relations with other internal mental states. Since no one has yet been able to formulate a clear basis or justification for the existence of such a distinction in mental contents, however, this idea has generally been abandoned in favor of externalist \"causal theories of mental contents\" (also known as informational semantics). Such a position is represented, for example, by Jerry Fodor's account of an \"asymmetric causal theory\" of mental content. This view simply entails the modification of functionalism to include within its scope a very broad interpretation of input and outputs to include the objects that are the causes of mental representations in the external world.\n\nThe twin earth argument hinges on the assumption that experience with an imitation water would cause a different mental state than experience with natural water. However, since no one would notice the difference between the two waters, this assumption is likely false. Further, this basic assumption is directly antithetical to functionalism; and, thereby, the twin earth argument does not constitute a genuine argument: as this assumption entails a flat denial of functionalism itself (which would say that the two waters would not produce different mental states, because the functional relationships would remain unchanged).\n\nAnother common criticism of functionalism is that it implies a radical form of semantic holism. Block and Fodor referred to this as the \"damn/darn problem\". The difference between saying \"damn\" or \"darn\" when one smashes one's finger with a hammer can be mentally significant. But since these outputs are, according to functionalism, related to many (if not all) internal mental states, two people who experience the same pain and react with different outputs must share little (perhaps nothing) in common in any of their mental states. But this is counterintuitive; it seems clear that two people share something significant in their mental states of being in pain if they both smash their finger with a hammer, whether or not they utter the same word when they cry out in pain.\n\nAnother possible solution to this problem is to adopt a moderate (or molecularist) form of holism. But even if this succeeds in the case of pain, in the case of beliefs and meaning, it faces the difficulty of formulating a distinction between relevant and non-relevant contents (which can be difficult to do without invoking an analytic–synthetic distinction, as many seek to avoid).\n\nAccording to Ned Block, if functionalism is to avoid the chauvinism of type-physicalism, it becomes overly liberal in \"ascribing mental properties to things that do not in fact have them\". As an example, he proposes that the economy of Bolivia might be organized such that the economic states, inputs, and outputs would be isomorphic to a person under some bizarre mapping from mental to economic variables.\n\nHilary Putnam, John Searle, and others have offered further arguments that functionalism is trivial, i.e. that the internal structures functionalism tries to discuss turn out to be present everywhere, so that either functionalism turns out to reduce to behaviorism, or to complete triviality and therefore a form of panpsychism. These arguments typically use the assumption that physics leads to a progression of unique states, and that functionalist realization is present whenever there is a mapping from the proposed set of mental states to physical states of the system. Given that the states of a physical system are always at least slightly unique, such a mapping will always exist, so any system is a mind. Formulations of functionalism which stipulate absolute requirements on interaction with external objects (external to the functional account, meaning not defined functionally) are reduced to behaviorism instead of absolute triviality, because the input-output behavior is still required.\n\nPeter Godfrey-Smith has argued further that such formulations can still be reduced to triviality if they accept a somewhat innocent-seeming additional assumption. The assumption is that adding a \"transducer layer\", that is, an input-output system, to an object should not change whether that object has mental states. The transducer layer is restricted to producing behavior according to a simple mapping, such as a lookup table, from inputs to actions on the system, and from the state of the system to outputs. However, since the system will be in unique states at each moment and at each possible input, such a mapping will always exist so there will be a transducer layer which will produce whatever physical behavior is desired.\n\nGodfrey-Smith believes that these problems can be addressed using causality, but that it may be necessary to posit a continuum between objects being minds and not being minds rather than an absolute distinction. Furthermore, constraining the mappings seems to require either consideration of the external behavior as in behaviorism, or discussion of the internal structure of the realization as in identity theory; and though multiple realizability does not seem to be lost, the functionalist claim of the autonomy of high-level functional description becomes questionable.\n\n\n"}
{"id": "26052076", "url": "https://en.wikipedia.org/wiki?curid=26052076", "title": "Fungal behavior", "text": "Fungal behavior\n\nFungi are capable of a variety of behaviors. Nearly all secrete chemicals, and some of these chemicals act as pheromones to communicate with other individuals. Many of the most dramatic examples involve mechanisms to get fungal spores dispersed to new environments. In mushrooms, spores are propelled into the air space between the gills, where they are free to fall down and can then be carried by air currents. Other fungi shoot spores aimed at openings in their surroundings, sometimes reaching distances over a meter.\n\nFungi such as Phycomyces blakesleeanus employ a variety of sensory mechanisms to avoid obstacles as their fruiting body grows, growing against gravity, toward light (even on the darkest night), into wind, and away from physical obstacles (probably using a mechanism of chemical sounding). \n\nOther fungi form constricting rings or adhesive knobs that trap nematodes, which the fungus then digests. \n\nOne hormone that is used by many fungi is Cyclic adenosine monophosphate (cAMP).\n\n"}
{"id": "38964905", "url": "https://en.wikipedia.org/wiki?curid=38964905", "title": "Gondi writing", "text": "Gondi writing\n\nGondi has typically been written in Devanagari script or Telugu script, but native scripts are in existence. A Gond by the name of Munshi Mangal Singh Masaram designed a Brahmi-based script in 1918, and very recently, a native script that dates up to 1750 has been discovered by a group of researchers from the University of Hyderabad.\n\nNonetheless, most Gonds are illiterate and do not use any script. The Gunjala Gondi Lipi has witnessed a surge in prominence, and well-supported efforts are being undertaken in villages of northern Andhra Pradesh to widen its usage.\n\nIn 1918, Munshi Mangal Singh Masaram, a Gond from Balaghat district of Madhya Pradesh, designed a script for Gondi based on Brahmi characters found in other descendant Indian scripts. However, this script is not widely used, even though a few publications have been made available by his followers and supporters.\n\nMasaram Gondi script was added to the Unicode Standard in June, 2017 with the release of version 10.0.\n\nThe Unicode block for Masaram Gondi is U+11D00–U+11D5F:\n\nThis script is the subject of ongoing linguistic and historical research. Discovered manuscripts have been dated up to 1750, and discuss information from as early as the 6th-7th centuries. Much of the information reveals independence initiatives by the Gond Rajas and encounters with the British. Also, the names of the days of the week, the months, the Gond festivals have been discovered in this Gondi script. \n\nGunjala Gondi lipi was added to the Unicode Standard in June, 2018 with the release of version 11.0. \n\nThe Unicode block for Gunjala Gondi is U+11D60–U+11DAF:\n\n\n "}
{"id": "2918999", "url": "https://en.wikipedia.org/wiki?curid=2918999", "title": "Gunshot residue", "text": "Gunshot residue\n\nGunshot residue (GSR), also known as cartridge discharge residue (CDR), \"gunfire residue\" (GFR), or firearm discharge residue (FDR), is residue deposited on the hands and clothes of someone who discharges a firearm. It is principally composed of burnt and unburnt particles from the explosive primer, the propellant—and possibly fragments of the bullet, cartridge case, and the firearm.\n\nLaw enforcement investigators test the clothing and skin of people for GSR to determine if they were near a gun when it discharged. Gunshot residue can travel over 3–5 feet (0.9–1.5 meters) from the gun. At the farthest distance, only a few trace particles may be present.\n\nIn 1971 John Boehm presented some micrographs of GSR particles found during the examination of bullet entrance holes using a scanning electron microscope. If the scanning electron microscope is equipped with an energy-dispersive X-ray spectroscopy detector, the chemical elements present in such particles, mainly lead, antimony and barium, can be identified.\n\nIn 1979 Wolten et al. proposed a classification of GSR based on composition, morphology, and size. Four compositions were considered \"characteristic\":\nThe authors proposed some rules about chemical elements that could also be present in these particles.\n\nWallace and McQuillan published a new classification of the GSR particles in 1984. They labeled as \"unique\" particles those that contain lead, antimony, and barium, or that contain antimony and barium. Wallace and McQuillan also maintained that these particles could contain only some chemical elements.\n\nIn the latest ASTM Standard Guide for GSR analysis by Scanning Electron Microscopy/Energy Dispersive X-ray Spectrometry (SEM-EDX) particles containing lead, antimony and barium, and respecting some rules related to the morphology and to the presence of other elements are considered characteristic of GSR. The most definitive method to determine whether a particle is characteristic of or consistent with GSR is by its elemental profile. An approach to the identification of particles characteristic of or consistent with GSR is to compare the elemental profile of the recovered particulate with that collected from case-specific known source items, such as the recovered weapon, Cartridge cases or victim-related items whenever necessary. This approach was called ‘‘case by case’’ by Romolo and Margot in an article published in 2001. In 2010 Dalby et al. published the latest review on the subject and concluded that the adoption of a \"case by case\" approach to GSR analysis must be seen as preferable, in agreement with Romolo and Margot.\n\nIn light of similar particles produced from extraneous sources, both Mosher et al. (1998) and Grima et al. (2012) presented evidence of pyrotechnic particles that can be mistakenly identified as GSR. Both publications highlight that certain markers of exclusion and reference to the general population of collected particulate can help the expert in designating GSR-similar particles as firework-sourced.\n\nParticle analysis by scanning electron microscope equipped with an energy-dispersive X-ray spectroscopy detector is the most powerful forensic tool that investigators can use to determine a subject's proximity to a discharging firearm or contact with a surface exposed to GSR (firearm, spent cartridge case, target hole). Test accuracy requires procedures that avoid secondary gunshot residue transfer from police officers onto subjects or items to be tested, and that avoid contamination in the laboratory.\n\nThe two main groups of specialists currently active on gunshot residue analysis are the Scientific Working Group for Gunshot Residue (SWGGSR) based in USA and the ENFSI EWG Firearms/GSR Working Group based in Europe.\n\nA positive result for GSR from SEM-EDX analysis can mean many things. Mainly it indicates that the person sampled was either in the vicinity of a gun when it was fired, handled a gun after it was fired, or touched something that was around the gun when it was fired. (For example: When a person goes to the aid of a victim of a gunshot wound, some GSR particles can transfer from the victim.)\n\nA negative result can mean that the person was nowhere near the gun when it was fired, or that they were near it but not close enough for GSR to land on them, or it can mean that the GSR deposited on them wore off. GSR is the consistency of flour and typically only stays on the hands of a living person for 4–6 hours. Wiping the hands on anything, even putting them in and out of pockets can transfer GSR off the hands. Victims don't always get GSR on them; even suicide victims can test negative for GSR.\n\nIf the ammunition used was specifically tagged in some way by special elements, it is possible to know the cartridge used to produce the GSR. Inference about the source of GSR can be based on the examination of the particles found on a suspect and the population of particles found on the victim, in the firearm or in the cartridge case, as suggested by the ASTM Standard Guide for GSR analysis by Scanning Electron Microscopy/Energy Dispersive X-ray Spectrometry. Advanced analytical techniques such as Ion Beam Analysis (IBA), carried out after Scanning Electron Microscopy, can support further information allowing to infer about the source of GSR particles. Christopher et al. showed as the grouping behaviour of different makes of ammunition can be determined using multivariate analysis. Bullets can be matched back to a gun using comparative ballistics.\n\nOrganic gunshot residue can be analysed by analytical techniques such as chromatography, capillary electrophoresis, and mass spectrometry.\n\n\n\n"}
{"id": "23664973", "url": "https://en.wikipedia.org/wiki?curid=23664973", "title": "Higraph", "text": "Higraph\n\nA higraph is a diagramming object that formalizes relations into a visual structure, it was developed by David Harel in 1988. Higraphs extend mathematical graphs by including notions of depth and orthogonality. In particular, nodes in a higraph can contain other nodes inside them, creating a hierarchy. The idea was initially developed for applications to databases, knowledge representation, and the behavioral specification of complex concurrent systems using the higraph-based language of statecharts.\n\nHigraphs are widely used in industrial applications like UML. Recently they have been used by philosophers to formally study the use of diagrams in mathematical proofs and reasoning.\n\n"}
{"id": "1007812", "url": "https://en.wikipedia.org/wiki?curid=1007812", "title": "Ignoramus et ignorabimus", "text": "Ignoramus et ignorabimus\n\nThe Latin maxim ignoramus et ignorabimus, meaning \"we do not know and will not know\", represents the idea that scientific knowledge is limited. It was publicized, in this sense, by Emil du Bois-Reymond, a German physiologist, in his publication \"Über die Grenzen des Naturerkennens\" (\"On the limits of our understanding of nature\") of 1872.\n\nEmil du Bois-Reymond used the phrase \"ignoramus et ignorabimus\" while discussing what he termed seven \"world riddles\", in a famous 1880 speech before the Prussian Academy of Sciences.\n\nHe defined seven \"world riddles\", of which three, he declared, neither science nor philosophy could ever explain, because they are \"transcendent\". Of the riddles, he considered the following transcendental and declared of them \"ignoramus et ignorabimus:\"\n\"1. the ultimate nature of matter and force, 2. the origin of motion, ... 5. the origin of simple sensations, a quite transcendent question.\"\n\nDavid Hilbert, a widely-respected German mathematician, suggested that such a conceptualization of human knowledge and ability is too pessimistic, and that by considering questions unsolvable, we limit our understanding.\n\nIn 1900, during an address to the International Congress of Mathematicians in Paris, Hilbert suggested that answers to problems of mathematics are possible with human effort. He declared, \"In mathematics there is no \"ignorabimus\".\", and he worked with other formalists to establish foundations for mathematics during the early 20th century.\n\nOn 8 September 1930, Hilbert elaborated his opinion in a celebrated address to the Society of German Scientists and Physicians, in Königsberg:\n\nAnswers to some of Hilbert's Program of 23 problems were found during the 20th century. Some have been answered definitively; some have not yet been solved; a few have been shown to be impossible to answer with mathematical rigor. \n\nDuring 1931, Gödel's incompleteness theorems showed that answers to some mathematical questions cannot be answered in the manner we would usually prefer.\n\nThe sociologist Wolf Lepenies has discussed the \"ignorabimus\" with the opinion that du Bois-Reymond was not really pessimistic about science:\n\nThis is in a discussion of Friedrich Wolters, one of the members of the literary group \"George-Kreis\". Lepenies comments that Wolters misunderstood the degree of pessimism being expressed about science, but well understood the implication that scientists themselves could be trusted with self-criticism.\n\n"}
{"id": "51240989", "url": "https://en.wikipedia.org/wiki?curid=51240989", "title": "International Year of Volunteers", "text": "International Year of Volunteers\n\nInternational Year of Volunteers was designated for 2001 by the United Nations General Assembly. The initiative aimed at increased recognition, facilitation, networking and promotion of volunteering, to highlight the achievements of the millions of volunteers worldwide who devote their time to serving others, and to encourage more people globally to engage in volunteering.\n\nThe concept for a United Nations year to recognize volunteerism first emerged within the UN system at a 1996 policy forum in Japan by UNV and United Nations University (UNU). A February 1997 proposal of the Government of Japan was transmitted through the UN Secretary General be placed on the agenda of the Economic and Social Council (ECOSOC) in July 1997. ECOSOC, in its resolution 1977/44 of 22 July 1997, recommended to the UN General Assembly that it adopt the resolution proclaiming 2001 the International Year of Volunteers. The UN General Assembly, in its 52nd session on 20 November 1997 in Resolution 52/17, co-sponsored by 123 countries, passed the ECOSOC resolution, thereby proclaiming 2001 as the International Year of Volunteers. The United Nations Volunteers programme (UNV), part of the United Nations Development Programme (UNDP), was designated in the resolution as the international focal point.\n\nObjectives of IYV were:\n\nSharon Capeling-Alakija was the Executive Coordinator of United Nations Volunteers during IYV 2001. As the international focal point, UNV, based in Bonn, Germany, took the lead in all organizing and promotion regarding the year internationally.\n\nThe \"www.iyv2001.org\" website, launched in December 1998 by UNV, provided resources for United Nations organizations, non-governmental organizations and governments to recognize the year in some way. The resources provided by the site included:\n\nThe IYV logo was a creation and volunteer contribution from Argentine designer Sandra Rojas, and was provided in the six official UN languages as well as a composite logo that combines all six in one.\n\nUN Secretary-General Kofi Annan opened IYV in November 2000 in New York. Other speakers at the event included Capeling-Alakija; Felipe VI of Spain (then Prince of Asturias]]; Nadia Comaneci, Olympic Gold Medalist; representatives from the governments of Japan, Uganda and Brazil; Dr. Astrid Heiberg, then President of the International Federation of Red Cross and Red Crescent Societies (IFRC); Anita Roddick, then CEO of The Body Shop; Bernard Kouchner, then of UNMIK in Kosovo; Sergio Vieira de Mello, then of UNTAET in East Timor; UN Volunteers from the Philippines and Nigeria; and representatives from NetAid. Annan also appointed the former President of Ghana, Jerry Rawlings, as the first IYV Eminent Person, to help raise the profile of millions of volunteers working for peace and development around the world. Other IYV Eminent Persons named for the year included Crown Prince Felipe, Roddick, and the former Executive Director of the United Nations Population Fund (UNFPA), Dr. Nafis Sadik.\n\nAt the Word Volunteer Conference in Amsterdam, in honor of IYV, the International Association for Volunteer Effort re-issued a revised Universal Declaration on Volunteering, first issued in 1990 during the World Volunteer Conference in Paris. \"This Declaration supports the right of every woman, man and child to associate freely and to volunteer regardless of their cultural and ethnic origin, religion, age, gender, and physical, social or economic condition.\"\n\nIndividual countries also created their own national IYV websites, and events were held around the world to promote the goals of IYV, such as the official launch event in New York City.\n\nIn the USA, the Points of Light Foundation and the Association of Junior Leagues International (AJLI) partnered to convene and lead the IYV USA Steering Committee. Members of the committee included the National Council of Volunteer Centers, National Council on Workplace Volunteering, Association for Volunteer Administration, National Parents and Teachers Association and Make A Difference Day. The USA's IYV web site, \"www.iyv2001us.org\", was launched in September 2000.\n\nOn 29 March 2001, the United Nations Postal Administration (UNPA) issued a set of six commemorative stamps and a souvenir card in honor of IYV.\n\nFour IYV 2001 Stamps were issued by the Bhutan Post on June 15.\n\nCharles, Prince of Wales unveiled the design of a 10-cent Canadian circulation coin commemorating IYV in April 2001. The coin went into circulation later that year. It was produced by Royal Canadian Mint and adapted from a photo provided by the March of Dimes.\n\nAccording to a press release by UNV in October 2001, lawmakers in France, Germany, Spain and the UK passed pro-volunteer legislation in 2001, and new laws were proposed in Argentina, Colombia, Ecuador, Madagascar, Mozambique, Nepal, Senegal and Tanzania. Also for the year, UNV and the Independent Sector released \"Measuring Volunteering: A Practical Toolkit,\" a survey guide to measure the economic contribution of volunteering. The press release says the toolkit was adapted for use in Botswana, China, Lao PDR, Kazakhstan and Mongolia, and that research on volunteering had begun in Cambodia, Madagascar, Namibia, Sri Lanka and Tanzania.\n\nSolicited by UNV, musicians from around the world donated and wrote songs about volunteerism, submitting them to UNV for possible inclusion on the IYV2001 website and in a CD. Chosen songs were offered via the website to download for free. UNV also offered a CD with 27 songs in nine languages, by musicians from 18 countries, in celebration of IYV. The CD featured Jamaican reggae star Tony Rebel, who donated the song, \"Not all about money\", and Portugal's Paulo de Carvalho, who donated the song \"Vai e faz\" (Go and Do). Most of the songs praised the virtues of volunteers, but the last song, by Dave Greenfield of Canada, was meant to provoke political debate and question social issues about volunteers.\n\nThe United Colors of Benetton's communication campaign for autumn 2001 was produced in collaboration with UNV and honored IYV. Instead of professional models, the campaign featured photos of volunteers, such as a tattooed former member of a street gang who volunteered in anti-violence activities, a young lawyer volunteering to promote and defend human rights, a trans-gendered person who volunteered to distribute condoms amongst prostitutes, and an elderly tap dancer who entertained residents in homes for the elderly. The campaign was promoted throughout Europe, the United States, South America and parts of Asia, in newspapers, magazines, and billboards. A special issue of Benetton's magazine \"Colors\" was published for the campaign in November 2001.\n\nThe final event for the year by UNV, the International Symposium on Volunteering, was 18–21 November 2001 in Geneva, Switzerland. The event was attended by representatives from most of the IYV national committees, and provided workshops to discuss the outcomes of IYV 2001.\n\nThen Pope John Paul II issued a statement on December 5, 2001, noting, \"At the end of the year, that the United Nations dedicated to Volunteer work, I wish to express my heartfelt appreciation for your constant dedication, in every part of the world, in going to meet those who live in poverty. Whether you work individually or gathered together in special associations, you represent for children, the elderly, the sick, people in difficulty, refugees and the persecuted a ray of hope that pierces the darkness of solitude and encourages them to overcome the temptations of violence and egotism.\"\n\nThe General Assembly marked the closing of IYV on December 5, 2001 by adopting a resolution on recommendations for volunteer action, commending the ongoing contributions of all volunteers to society, and encouraging all people to become more engaged in voluntary activities. The assembly also decided that two plenary meetings at its fifty-seventh session on 5 December 2002 should be devoted to the outcome of the IYV and its follow-up. Specific recommendations on ways governments and the UN system could support volunteering were contained in an annex to the resolution.\n\nWorldwide volunteer activities of the Vienna based NGO community were presented at the Vienna International Centre on 11 December 2001 to mark IYV 2001 and the International Day of Human Rights, jointly organized by the United Nations Information Service Vienna (UNIS) and CONGO (Conference of NGOs in consultative relationship with the UN).\n\nThe European Year of Volunteering (EYV) was in 2011, launched by the European Commission to celebrate the volunteerism efforts of Europeans and chosen to correspond with the 10th anniversary of IYV. It was originally an initiative of the EYV 2011 Alliance, an informal gathering of volunteer networks, such as Caritas, the International Committee of the Red Cross and European Youth Forum. The web site address was \"www.europa.eu/volunteering\", and the site is now archived on the European Commission website.\n\nIn 2011, ten years after IYV, under General Assembly Resolution 63/153 (2008) the United Nations IYV+10 was recognized and promoted by UNV to renew the goals of the original IYV and to encourage people’s contributions to peace and the Millennium Development Goals through volunteering. National and international UN Volunteers, as well as online volunteers, were recruited to support work on IYV+10, strengthen links with stakeholders and partners at the national level, and monitor initiatives developed during the Year. Activities were promoted via the UNV web site www.worldvolunteerweb.org. During the first half of 2011, a series of five regional consultation meetings were organized in Ecuador (28–20 March), Turkey (18–19 April), the Philippines (3–4 May), and Senegal (30–31 May for Francophone Africa and 6–7 June for Anglophone Africa). These events brought together hundreds of stakeholders from civil society, the private sector, national authorities and academia, UNV Field Units and other United Nations entities. A global conference on ‘Volunteering for a sustainable future’ was held 15–17 September in Budapest in partnership with the International Federation of Red Cross and Red Crescent Societies (IFRC) and the International Association for Volunteer Effort (IAVE). The conference brought together UN member states and volunteer-involving organizations, plus partners from academia and the private sector. The 64th Annual UN DPI/NGO Conference was held from 3–5 September hosted by the German government and the city of Bonn. The event theme was ‘Sustainable Societies, Responsive Citizens’.\n\n\n"}
{"id": "3025983", "url": "https://en.wikipedia.org/wiki?curid=3025983", "title": "Late bloomer", "text": "Late bloomer\n\nA late bloomer is a person whose talents or capabilities are not visible to others until later than usual. The term is used metaphorically to describe a child or adolescent who develops slower than others in their age group, but eventually catches up and in some cases overtakes their peers, or an adult whose talent or genius in a particular field only appears later in life than is normal – in some cases only in old age.\n\nThis article discusses late-blooming children, adolescents, and adults.\n\n\"Late Bloomer\" is commonly used to refer to young children who develop skills such as language, reading, or social interaction later than others of their age.\n\nThere are many theories of the way in which children develop, proposed by authorities such as Urie Bronfenbrenner, Jerome Bruner, Erik Erikson, Jerome Kagan, Lawrence Kohlberg, Jean Piaget, and Lev Vygotsky. Although they disagree about how stages of development should be defined, and about the primary influences on development, they agree that a child's development can be measured as a predictable series of advances in physical, intellectual and social skills which almost always occur in the same sequence, although the rate may vary from one child to another.\n\nWhen a child falls behind their peers at some stage of development, their teacher may perceive that the child is \"backward\". There is strong evidence that this perception may become self-fulfilling: although the child catches up, the teacher may continue to rate their performance poorly, imposing a long-term handicap. Thomas Edison's mind often wandered and his teacher was overheard calling him \"addled.\" This ended Edison's three months of official schooling. His mother then home schooled him. Edison may have had some form of Attention-deficit hyperactivity disorder (ADHD), which the American Psychiatric Institute says affects about 3 – 5% of children.\n\nA notable example of a child who overcame early developmental problems is Albert Einstein, who suffered from speech difficulties as a young child. Other late-talking children who became highly-successful engineers, mathematicians, and scientists include the physicists Richard Feynman and Edward Teller. Neuroscientist Steven Pinker postulates that a certain form of language delay may in fact be associated with exceptional and innate-analytical prowess in some individuals.\n\nDyslexia is a learning disability that may affect 3–10% of children. It is thought to be the result of a genetically inherited neurological difference from \"normal\" children, and has been diagnosed in people of all levels of intelligence. Studies indicate that 20% to 35% of U.S. and British entrepreneurs have the condition: by definition, late bloomers. Researchers theorise that dyslexic entrepreneurs may attain success by delegating responsibilities and excelling at verbal communication. Richard Branson, known for his Virgin brand of over 360 companies is a notable example, as is Charles R. Schwab the founder and CEO of the Charles Schwab Corporation. Pablo Picasso, Tom Cruise, and Whoopi Goldberg are other examples of dyslexics, considered \"slow\" as children.\n\nThe autism spectrum of psychological conditions affects about 0.6% of children, characterized by widespread abnormalities of social interactions and communication, severely-restricted interests, and highly-repetitive behavior. Notable individuals with autism spectrum disorders include Tim Page, a Pulitzer Prize-winning critic and author and Vernon L. Smith, a Nobel Laureate in economics.\n\nDuring adolescence a child goes through physical and mental changes that lead to them becoming an adult. Adolescence is usually considered to start with the first stages of puberty and to continue until physical growth is complete, although the World Health Organization defines adolescence simply as the period between ages 10 and 20. There is a wide range of normal ages, but generally girls begin the process of puberty between the ages of 9 to 14, reaching adult height and reproductive maturity within 4 years, while boys usually start between the ages of 10 to 17, and continue to grow for about 6 years after the first visible pubertal changes. Adolescence is often a period of turbulent emotions and mood swings combined with rapid, intellectual development.\n\n\"Late bloomer\" can refer to children who suffer from delayed puberty, who are late in reaching their full height. W. B. Yeats (age 30), Pierre Trudeau (age at least 28), Mark Twain (age 34), and Johann von Goethe (age 39) are all \"late bloomers\" in this last sense.\n\nIn most public educational systems, children and adolescents of the same age are put in the same classes. Because of the wide variance in the onset of adolescence, this means that one class may include individuals who have not yet started puberty, others who are sexually mature but not fully grown and yet others who are effectively adult. During this period, there is a high risk of an adolescent dropping out of formal education (due most commonly to laziness, intellectual boredom, bullying, or rebellion) without having achieved their full learning potential. The term \"late-bloomer\" may refer to such an individual who develops serious intellectual interests in their 20s or 30s and enrols in college, where he or she performs particularly well and subsequently establishes a professional career.\n\nA late blooming adult is a person who does not discover their talents and abilities until later than normally expected. In certain cases retirement may lead to this discovery.\n\nAlthough there is a common perception that intellectual development peaks in a young adult and then slowly declines with increasing age, this may be simplistic. Although the ability to form new memories and concepts may indeed diminish, the older person has the advantage of accumulated knowledge, associations between concepts, and mental techniques that may give them an advantage in some fields.\n\nSome notable examples of late bloomers in different fields follow.\n\nIt was common for many actors to not get their big 'break' into the film industry until their late twenties or well into their thirties. Meryl Streep did not graduate Yale School of Drama until the age of twenty seven. The actor Alan Rickman did not begin his career until he was twenty eight, having operated a graphic-design company before then. He did not get his first real break into theatre until he was in his forties. Danny Aiello did not start acting until he was forty. Peg Phillips might be one of the best examples as she first pursued acting as a professional after her retirement from accounting; she started acting professionally in her late sixties. Although not a noteworthy actress, Clara Peller might be noted for having an even later start in entertainment, in her eighties. Richard Farnsworth became an actor after forty years as a stunt man, although he had had a few small uncredited roles when younger. Rodney Dangerfield was an actor/comedian who did not really start until he was forty two. He had done clubs when he was younger, but stopped in order to work as a salesman. Zelda Rubinstein was forty eight before she had her first role, a minor part in \"Under the Rainbow\", but is more known for her \"debut\" in the \"Poltergeist\" film series starting the following year. Chicago native, Chi McBride, best known for the role as the principal in the series \"Boston Public\", only got into acting when he was thirty one. Danny Glover had a brief stint in the career of politics before he had involved himself in acting at twenty eight. BAFTA winning British actress Liz Smith did not become a professional actress until the age of fifty. Kathryn Joosten also got a late start, beginning acting at age forty two in community theater. Television star Judd Hirsch from \"Taxi\" and character actor Bill Cobbs became active at the age of thirty six. George Wendt who played Norm on \"Cheers\" became active at the age of thirty two. Brian Dennehy had dreams of stage and screen at an early age, but chose to first pursue other interests such as service in the U.S. Marine Corps prior to becoming active at the age of thirty eight. Irish actor Brendan Gleeson, who appeared as Mad Eye Moody in the \"Harry Potter\" films and alongside Colin Farrell in \"In Bruges\", started acting professionally at thirty four, having previous work as a school teacher. The Indian (Bengali) actor Paran Bandopadhyay is another late bloomer, who started his acting career in television and films at the age of sixty, after retiring from his government job. Jerry Doyle, of \"Babylon 5\" fame, did not start acting until he was thirty six after working as a stockbroker and pilot. Sylvester Stallone was thirty when he wrote and starred in the first \"Rocky\". All throughout his life, he has pushed his body through rigorous training routines for his film roles. Most notably at age forty three, he developed his now-famous \"Rambo 3\" physique which got him named as \"body of the '80s\". The veteran Indian (Malayalam) actor Sathyan started his career at the age of forty, after resigning from Police Service. He later came to be known as one of the greatest actors ever in Malayalam film industry.\n\nIn art \"late bloomers\" are most often associated with naïve art. This term is used for untrained artists so fits those who start late in life without artistic training. Hence the classic late bloomer is Grandma Moses whose painting career began in her seventies after abandoning a career in embroidery because of arthritis. An even older example is Bill Traylor who started drawing at age 83. Another painter who started late in life is Alfred Wallis, who began painting after his wife's death in his 60s. Mary Delany produced her \"paper mosaiks [sic]\" from the age of 71 to 88. Then there is Carmen Herrera, who did have artistic training, but who sold her first artwork in 2004 when she was 89 years old, after six decades of private painting.\n\nIn business Irene Wells Pennington became best known in her nineties when she helped straighten out irregularities in her husband's oil business after he went senile in his own 90s. Colonel Sanders began his franchise in his sixties and can also be deemed a late in life financial success. In his mid-50s Taikichiro Mori founded the business that made him, for a year or two, the richest man in the world. He came from a merchant family, but had been a business professor before his 50s.\n\nJapanese dancer and choreographer Kazuo Ohno did not undertake formal dance lessons until his late twenties and was 43 years old when he performed his first recital at Kanda Kyoritsu Hall in Tokyo in 1949. A decade later, he and colleague Tatsumi Hijikata would achieve worldwide acclaim as the nucleus of the Butoh dance movement. Martha Graham dancer David Zurak took his first dance class at the age of 23 and built a successful career in New York City.\n\nIn professional sports, an athlete's career usually ends in the mid-to-late 30s, so a player who breaks through in their late 20s/early 30s would be considered a late bloomer. One such example is Kurt Warner, who entered the NFL at age 28, and went on to become a two-time MVP and Super Bowl champion. Baseball pitcher Randy Johnson, who made his Major League debut at 25, but didn't reach superstar status until he was 30, might also be considered a late bloomer, Former NBA star, Hakeem Olajuwon did not touch a basketball until he was 15, but his athleticism and fundamentals from the sports, football and handball, helped him advance as one of the greatest bigmen to ever play in the NBA.\n\nIn shooting there have been two figures of note whose accomplishments occurred in their sixties or later. Joshua Millner of Britain was 61 when he won his Olympic gold medal in Free rifle, 1,000 yards. Swedish marksman Oscar Swahn won two Olympic gold medals at the age of 60 and one at the age of 64. He won his last medal, silver, at 72 making him the oldest medalist. In athletics Philip Rabinowitz set a sprinting record for centenarians.\n\nJonah Barrington, a squash player, overcame alcoholism to later become a 6 times British Open Squash champion, and was regarded as one of the fittest men on the planet.\n\nHeavyweight champions Ken Norton and Rocky Marciano did not take up boxing until their twenties, but both enjoyed successful careers at the highest level of competition.\n\nBaseball player Josh Hamilton, a former number one overall draft pick, did not make his major league debut until the age of 26 due to years of serious drug and alcohol abuse. He has since been an all star several times and won the 2010 American League MVP award.\n\nFootball player Didier Drogba did not sign a professional contract with a club until the age of 21. It was not until the age of 26 when he joined Chelsea when he showed his real talent as a world-class footballer. Drogba scored the equaliser, and then the winning penalty, in the 2012 UEFA Champions League final aged 34.\n\nIndian hockey legend Dhyan Chand did not play any hockey in his life till he joined the Indian Army.\n\nCricketer Dirk Nannes who once was played for Delhi Daredevils ahead of the great fast bowler Glenn McGrath made his first class debut at 29.\n\nTim Thomas, an American professional ice hockey goaltender played for several years in the minor leagues and Europe, before making it to the NHL at age 28, with the Boston Bruins. He finally emerged as the Bruins' starting goaltender at age 32. Thomas is a two-time winner of the Vezina Trophy (2009 and 2011) as the league's best goaltender, and was a member of Team USA in the 2010 Winter Olympics in Vancouver. Thomas won the Conn Smythe Trophy as the most valuable player in the 2011 Stanley Cup playoffs. He became the oldest player in league history to win the Conn Smythe at age 37. \n\nFor professional tennis, Angelique Kerber, Li Na, Stan Wawrinka, Francesca Schiavone, Flavia Pennetta, Jana Novotna, Goran Ivanišević and Andrés Gómez are famous late bloomers who won their first Grand Slam Singles titles after age 28.\n\nGeorge Green, working as a miller and with no formal education in mathematics, published his famous \"An Essay on the Application of Mathematical Analysis to the Theories of Electricity and Magnetism\" in 1828, at the age of 35.\n\nAlexandre-Théophile Vandermonde started to study mathematics at 35, and began to publish in this field the same year.\n\nEugène Ehrhart started publishing in mathematics in his 40s, and finished his PhD thesis at the age of 60.\n\nMarjorie Rice, an amateur mathematician with no formal education in mathematics beyond high school, did not begin studying tessellations until December 1975; as she was born in 1923, this means she was either 51 or 52 when she began, depending on her birthday. She developed her own system of notation and used it to discover three new types of tessellating pentagons and over sixty distinct tessellations by pentagons by 1977.\n\nCaspar Wessel published his only mathematics paper at the age of 54.\n\nRoger Apéry proved Apéry's theorem at the age of 63.\n\nMusical ability is inherent in almost all people, to a greater or lesser extent. However, those who develop it to a high level are generally encouraged to play an instrument or to sing at an early age. Late bloomers in music are generally composers or artists who became prominent later in life, but had displayed musical ability much earlier.\n\nAnton Bruckner is an example of a musical late bloomer. Although he played church organ some in his twenties he did not become a composer until his 40s. Singer K. T. Oslin released her first album at age 47 which was a major country music success. Al Jarreau is also an example, who released his first album at age 35. AERIA Recording Artist Colie Brice released his 10th solo album Late Bloomer at 39. Elliott Carter did not achieve compositional maturity until his Cello Sonata (1948), when he was 40. César Franck and Leoš Janáček also matured late as composers: Franck at 56, with his \"Symphony no. 1 in D\"; and Janáček at 50, with \"Jenůfa\"\" (1904). Iannis Xenakis did not even begin studying composition until 30, with Messiaen. Leonard Cohen did not release his first album until he was 32 years old. Malayalam singer Chandralekha did not start her singing career until the age of 35, when she was shot to fame by a YouTube video featuring her.\n\nThough many filmmakers begin directing in their late 20s or early 30s, many of the most notable directors in film history waited until their mid-to-late-30s to direct their first feature. These directors include Nicholas Ray, Alain Resnais, Edward Yang, Michael Mann, Frank Tashlin, Robert Aldrich, Satyajit Ray, Anthony Mann, Terry Gilliam, Jerry Lewis, Tsai Ming-liang, Don Siegel, Melvin Van Peebles, Gaspar Noé, Lloyd Bacon, Alexander Kluge, Mrinal Sen, Jean-Marie Straub, Ida Lupino, Sam Mendes, Alexander Payne, Ang Lee and Jacques Rivette. David Mamet directed his first feature at 40, having already found success and been awarded a Pulitzer Prize as a playwright. Éric Rohmer directed his first feature film at 39, though he didn't become a full-time filmmaker until he was in his late 40s. \n\nMany notable directors started even later: Robert Bresson, Jacques Tati, and Takeshi Kitano directed their first features at 42; Maurice Pialat at 43; Michael Haneke at 47; Jim Sheridan at 40; and his peer and collaborator Terry George at 46. Yevgeni Bauer at 48. Clint Eastwood, the oldest person to win the Academy Award for Best Director, directed his first film at 41.\n\nOne of the most shining examples of late bloomers in filmmaking is the Portuguese director Manoel de Oliveira. Born in 1908, he worked sporadically in filmmaking from the 1930s. He completed his first feature film in 1941 called \"Aniki-Bobo\". Due to circumstances beyond his control (difficulty in financing, having to deal with his family's business), he did not complete his second feature film until 1971 (when he turned 63). 2 years later, he completed his third feature film, \"Benilde or the Virgin Mother\" (1973). Five years later, he made his breakthrough film (originally commissioned by Portuguese TV) called \"Doomed Love\". After his critically acclaimed film \"Francisca\" (1981), he became a full-time filmmaker (at the age of 73).\n\nIt is common for politicians to achieve prominence late in life, often after a career in business, law or academia. For example, in the United States Congress of January 2009, of 540 elected officials, 215 had worked in the legal profession, and 189 had worked in private sector business. The average age of senators was 62. Also, Donald Trump was the first U.S. President to reach the age of seventy prior to his election to the presidency and first to reach seventy years of age before entering office, as well as the first U.S. President to assume the office without any prior military or political experience.\n\nSome highly successful politicians come from more unusual backgrounds.\n\nVáclav Havel, born in 1936, was a playwright and writer with an interest in human rights. He became the voice of the opposition in Czechoslovakia in the 1980s and President of Czechoslovakia at age 53 after the collapse of the communist regime in 1989. Ronald Reagan, a former actor, union leader, and corporate spokesman, was first elected to public office at 55 when he became Governor of California and remains the oldest man to have served as U.S. President. Melchora Aquino was an uneducated Filipino peasant woman, the mother of six children, who became an activist in the fight to gain independence from Spain. Known as the Grand Woman of the revolution, she was 84 when the Philippine Revolution broke out in 1896. Silas C. Swallow was a minister who became a Prohibition Party activist in his sixties. Marjory Stoneman Douglas's career might also fit. Her first environmental work of note occurred when she was almost 60, at 78 she founded \"Friends of the Everglades\", and she continued until she was over age 100.\n\nThe great proponent of Gaudiya Vaishnavism A. C. Bhaktivedanta Swami Prabhupada founded the International Society for Krishna Consciousness, or the Hare Krishna movement, in 1966 at the age of 70. Within the final twenty years of his life Prabhupada translated over sixty volumes of classic Vedic scriptures (such as the Bhagavad Gita and Bhagavata Purana) into the English language.\n\nMany writers have published their first major work late in life. Mary Wesley might be a classic example. She wrote two children's books in her late fifties, but her writing career did not gain note until her first novel at 70, written after the death of her husband. \nAt the age of 74, Norman Maclean published his first and only novel, the 1976 best-selling book \"A River Runs Through It\", which fictionalizes Maclean's memories of the early twentieth century in Montana. Harriet Doerr published her first novel at age 74, and went on to great praise. A possibly more well-known example might be Laura Ingalls Wilder. She became a columnist in her forties, but did not publish her first novel in the \"Little House\" series of children's books until her sixties. Charles Bukowski wrote his first novel in 1971, when he was 51 years old.\n\nMemoirist and novelist Flora Thompson was first published in her thirties but is most famous for the semi-autobiographical \"Lark Rise to Candleford\" trilogy, the first volume of which was published when she was 63. Frank McCourt didn't publish his first book \"Angela's Ashes\", which he later won the Pulitzer Prize for, until he was 66. Children's author Mary Alice Fontenot wrote her first book at 51 and wrote almost thirty additional books, publishing multiple volumes in her eighties and nineties. Kenneth Grahame was born in 1859 and joined the Bank of England in 1879, rising through the ranks to become its secretary. Although he had written various short stories while working at the bank, it was only after his retirement in 1908 that he published his masterpiece and final work \"The Wind in the Willows\". Penelope Fitzgerald launched her literary career in 1975, at the age of 58, when she published a biography of the Pre-Raphaelite artist Edward Burne-Jones. She won the Booker Prize for 1979 with \"Offshore\", and in 2012, \"The Observer\" named her final novel, \"The Blue Flower\", as one of \"the ten best historical novels\".\n\nRichard Adams's first novel, the bestseller \"Watership Down\", was published when he was in his fifties. The Marquis de Sade published his first novel, \"Justine\", after turning 51. Raymond Chandler published his first short story at 45, and his first novel, \"The Big Sleep\" at 51. Paul Torday published his debut novel \"Salmon Fishing in the Yemen\" at the age of 59, after a career in the engineering industry. Jean Rhys is best known for her novel \"Wide Sargasso Sea\", which was published in October 1966, when she was 76.\n\nAron Ettore Schmitz published his first novel \"Senilità\" in his 38th year; however it was not until he published \"Zeno's Conscience\" that he made a breakthrough, aged 61. Even this was self-published.\n\nIn other areas of writing, historian Gerda Lerner, born in 1920, published her first book, \"The Grimke Sisters from South Carolina,\" in 1967, when she was either 47 or 46 depending on her birthday. Poet Wallace Stevens started poetry late in life after years as an insurance salesman and executive. Although he was first published at 38, his \"canonical works\" came out in his fifties. In philosophy Mary Midgley had her first book when she was 56. Edmond Hoyle wrote a booklet on whist in his late sixties. To avoid unauthorized copies he wrote the copyrighted \"A Short Treatise on the Game of Whist\" at age 70.\n\nThe Indian writer and polymath Nirad C. Chaudhuri wrote his autobiography \"The Autobiography of an Unknown Indian\" at the age of 54. He wrote a sequel to it \"Thy Hand, Great Anarch!\" at the age of 90. He published his next work (and his final work) \"Three Horsemen of the New Apocalypse\" at the age of 100.\n\n"}
{"id": "39821179", "url": "https://en.wikipedia.org/wiki?curid=39821179", "title": "Lecherous millionaire", "text": "Lecherous millionaire\n\nThe lecherous millionaire is a thought experiment devised by Joel Feinberg to illustrate questions concerning coercion. It presents a scenario in which a millionaire offers to pay for medical care for a woman's ill child on the condition that she has sexual relations with him. While the millionaire is making an offer, he nevertheless seems to be coercing the woman.\n\n\n"}
{"id": "3732314", "url": "https://en.wikipedia.org/wiki?curid=3732314", "title": "Multiplicity (philosophy)", "text": "Multiplicity (philosophy)\n\nMultiplicity () is an assertion that there is more than one geo-historical trajectory. It is a philosophical concept developed by Edmund Husserl and Henri Bergson from Riemann's description of the mathematical concept. It forms an important part of the philosophy of Gilles Deleuze, particularly in his collaboration with Félix Guattari, \"Capitalism and Schizophrenia\" (1972–80). In his \"Foucault\" (1986), Deleuze describes Michel Foucault's \"The Archaeology of Knowledge\" (1969) as \"the most decisive step yet taken in the theory-practice of multiplicities.\"\n\nThe philosopher Jonathan Roffe describes Deleuze's concept of Multiplicity as follows: \"A multiplicity is, in the most basic sense, a complex structure that does not reference a prior unity. Multiplicities are not parts of a greater whole that have been fragmented, and they cannot be considered manifold expressions of a single concept or transcendent unity. On these grounds, Deleuze opposes the dyad One/Many, in all of its forms, with multiplicity. Further, he insists that the crucial point is to consider multiplicity in its substantive form – a multiplicity – rather than as an adjective – as multiplicity of something. Everything for Deleuze is a multiplicity in this fashion.\"\n\nDeleuze argues in his commentary \"Bergsonism\" (1966) that the notion of multiplicity forms a central part of Bergson's critique of philosophical negativity and the dialectical method. The theory of multiplicities, he explains, must be distinguished from traditional philosophical problems of \"the One and the Multiple.\" By opposing \"the One and the Multiple,\" dialectical philosophy claims \"to reconstruct the real,\" but this claim is false, Bergson argues, since it \"involves abstract concepts that are much too general.\"\n\nInstead of referring to \"the Multiple in general\", Bergson's theory of multiplicities distinguishes between two types of multiplicity: continuous multiplicities and discrete multiplicities (a distinction that he developed from Riemann). The features of this distinction may be tabulated as follows:\n\n\n"}
{"id": "1261240", "url": "https://en.wikipedia.org/wiki?curid=1261240", "title": "Nitya-samsarins", "text": "Nitya-samsarins\n\nIn Dvaita theology, Nitya-samsarins, as classified by Shri Madhvacharya, are souls which are eternally transmigrating.\n\nMadhva divides souls into three classes: one class of souls which qualify for liberation (Mukti-yogyas), another subject to eternal rebirth or eternal transmigration (Nitya-samsarins), and a third class that is eventually condemned to eternal hell or Andhatamas (Tamo-yogyas).\n\nNitya-samsarins delight only in worldly values and feel no need for ethical and spiritual life. By reaping the fruits of their own actions, they pass through continuous births and deaths eternally.\n"}
{"id": "648470", "url": "https://en.wikipedia.org/wiki?curid=648470", "title": "Oppression", "text": "Oppression\n\nOppression can refer to an authoritarian regime controlling its citizens via state control of politics, the monetary system, media, and the military; denying people any meaningful human or civil rights; and terrorizing the populace through harsh, unjust punishment, and a hidden network of obsequious informants reporting to a vicious secret police force. \n\nOppression also refers to a less overtly malicious pattern of subjugation, although in many ways this social oppression represents a particularly insidious and ruthlessly effective form of manipulation and control. In this instance, the subordination and injustices do not afflict everyone—instead it targets specific groups of people for restrictions, ridicule, and marginalization. No universally accepted term has yet emerged to describe this variety of oppression, although some scholars will parse the multiplicity of factors into a handful of categories, e.g., social (or sociocultural) oppression; institutional (or legal) oppression; and economic oppression.\n\nThe word \"oppress \"comes from the Latin \"oppressus\", past participle of \"opprimere\", (\"to press against\", \"to squeeze\", \"to suffocate\"). Thus, when authoritarian governments use oppression to subjugate the people, they want their citizenry to feel that \"pressing down\", and to live in fear that if they displease the authorities they will, in a metaphorical sense, be \"squeezed\" and \"suffocated\", e.g., thrown in a dank, dark, state prison or summarily executed. Such governments oppress the people using restriction, control, terror, hopelessness, and despair. The tyrant's tools of oppression include, for example, extremely harsh punishments for \"unpatriotic\" statements; developing a loyal, guileful secret police force; prohibiting freedom of assembly, freedom of speech, and freedom of the press; controlling the monetary system and economy; and imprisoning or killing activists or other leaders who might pose a threat to their power.\n\nOppression also refers to a more insidious type of manipulation and control, in this instance involving the subjugation and marginalization of specific groups of people within a country or society, such as: girls and women, boys and men, people of color, religious communities, citizens in poverty, LGBT people, youth and children, and many more. This socioeconomic, cultural, political, legal, and institutional oppression (hereinafter, \"social oppression\") probably occurs in every country, culture, and society, including the most advanced democracies, such as the United States, Japan, Costa Rica, Sweden, and Canada.\n\nA single, widely accepted definition of social oppression does not yet exist, although there are commonalities. Taylor (2016) defined (social) oppression in this way:\n\nOppression is a form of injustice that occurs when one social group is subordinated while another is privileged, and oppression is maintained by a variety of different mechanisms including social norms, stereotypes and institutional rules. A key feature of oppression is that it is perpetrated by and affects social groups. ... [Oppression] occurs when a particular social group is unjustly subordinated, and where that subordination is not necessarily deliberate but instead results from a complex network of social restrictions, ranging from laws and institutions to implicit biases and stereotypes. In such cases, there may be no deliberate attempt to subordinate the relevant group, but the group is nonetheless unjustly subordinated by this network of social constraints. \n\nHarvey (1999) suggested the term \"civilized oppression\", which he introduced as follows:\n\nIt is harder still to become aware of what I call 'civilized Oppression,' that involves neither physical violence nor the use of law. Yet these subtle forms are by far the most prevalent in Western industrialized societies. This work will focus on issues that are common to such subtle oppression in several different contexts (such as racism, classism, and sexism) ... Analyzing what is involved in civilized oppression includes analyzing the kinds of mechanisms used, the power relations at work, the systems controlling perceptions and information, the kinds of harms inflicted on the victims, and the reasons why this oppression is so hard to see even by contributing agents.\n\nResearch and theory development on social oppression has advanced apace since the 1980s with the publication of seminal books and articles, and the cross-pollination of ideas and discussion among diverse disciplines, such as: feminism, sociology, psychology, philosophy, and political science. Nonetheless, more fully understanding the problem remains an extremely complicated challenge for scholars. Improved understanding will likely involve, for example, comprehending more completely the historical antecedents of current social oppression; the commonalities (and lack thereof) among the various social groups damaged by social oppression (and the individual human beings who make up those groups); and the complex interplay between and amongst sociocultural, political, economic, psychological, and legal forces that cause and support oppression.\n\nSocial oppression is when a single group in society takes advantage of, and exercises power over, another group using dominance and subordination. This results in the socially supported mistreatment and exploitation of a group of individuals by those with relative power. In a social group setting, oppression may be based on many ideas, such as poverty, gender, class, race, or other categories. Oppression by institution, or systematic oppression, is when the laws of a place create unequal treatment of a specific social identity group or groups. Another example of social oppression is when a specific social group is denied access to education that may hinder their lives in later life. Economic oppression is the divide between two classes of society. These were once determined by factors such slavery, property rights, disenfranchisement, and forced displacement of livelihood. Each divide yielded various treatments and attitudes towards each group.\n\nSocial oppression derives from power dynamics and imbalances related to the social location of a group or individual. Social location, as defined by Lynn Weber, is \"an individual's or a group's social 'place' in the race, class, gender and sexuality hierarchies, as well as in other critical social hierarchies such as age, ethnicity, and nation\". An individual's social location often determines how they will be perceived and treated by others in society. Three elements shape whether a group or individual can exercise power: the power to design or manipulate the rules and regulations, the capacity to win competitions through the exercise of political or economic force, and the ability to write and document social and political history.. There are four predominant social hierarchies, race, class, gender and sexuality, that contribute to social oppression.\n\nWeber, among some other political theorists, argues that oppression persists because most individuals fail to recognize it; that is, discrimination is often not visible to those who are not in the midst of it. Privilege refers to a sociopolitical immunity one group has over others derived from particular societal benefits. Many of the groups who have privilege over gender, race, or sexuality, for example, can be unaware of the power their privilege holds. These inequalities further perpetuate themselves because those who are oppressed rarely have access to resources that would allow them to escape their maltreatment. This can lead to internalized oppression, where subordinate groups essentially give up the fight to get access to equality, and accept their fate as a non-dominant group.\n\nThe first social hierarchy is race or racial oppression, which is defined as: \" ... burdening a specific race with unjust or cruel restraints or impositions. Racial oppression may be social, systematic, institutionalized, or internalized. Social forms of racial oppression include exploitation and mistreatment that is socially supported.\" United States history consists of five primary forms of racial oppression including genocide and geographical displacement, slavery, second-class citizenship, non-citizen labor, and diffuse racial discrimination. \n\nThe first, primary form of racial oppression—genocide and geographical displacement—in the US context refers to Western Europe and settlers taking over an Indigenous population's land. Many Indigenous people, commonly known today as Native Americans, were relocated to Indian Reservations or killed during wars fought over the land. The second form of racial oppression, slavery, refers to Africans being taken from their homeland and sold as property to white Americans. Racial oppression, particularly in the Southern United States, was a significant part of daily life and routine in which African-Americans worked on plantations and did other labor without pay and the freedom to leave their workplace. The third form of racial oppression, second-class citizenship, refers to some categories of citizens having fewer rights than others. Second-class citizenship became a pivotal form of racial oppression in the United States following the Civil War, as African-Americans who were formerly enslaved continued to be considered unequal to white citizens, and had no voting rights. Moreover, immigrants and foreign workers in the US are also treated like second-class citizens, with fewer rights than people born in the US. The fourth form of racial oppression in American history, non-citizen labor, refers to the linkage of race and legal citizenship status. During the middle of the 19th century, some categories of immigrants, such as Mexicans and Chinese, were sought as physical laborers, but were nonetheless denied legal access to citizenship status. The last form of racial oppression in American history is diffuse discrimination. This form of racial oppression refers to discriminatory actions that are not directly backed by the legal powers of the state, but take place in widespread everyday social interactions. This can include employers not hiring or promoting someone on the basis of race, landlords only renting to people of certain racial groups, salespeople treating customers differently based on race, and racialized groups having access only to impoverished schools. Even after the civil rights legislation abolishing segregation, racial oppression is still a reality in the United States. According to Robert Blauner, author of \"Racial Oppression in America\", \"racial groups and racial oppression are central features of the American social dynamic\".\nThe second social hierarchy, class oppression, sometimes referred to as classism, can be defined as prejudice and discrimination based on social class. Class is an unspoken social ranking based on income, wealth, education, status, and power. A class is a large group of people who share similar economic or social positions based on their income, wealth, property ownership, job status, education, skills, and power in the economic and political sphere. The most commonly used class categories include: upper class, middle class, working class, and poor class. A majority of people in the United States self-identify in surveys as middle class, despite vast differences in income and status. Class is also experienced differently depending on race, gender, ethnicity, global location, disability, and more. Class oppression of the poor and working class can lead to deprivation of basic needs and a feeling of inferiority to higher-class people, as well as shame towards one's traditional class, race, gender, or ethnic heritage. In the United States, class has become racialized leaving the greater percentage of people of color living in poverty. Since class oppression is universal among the majority class in American society, at times it can seem invisible, however, it is a relevant issue that causes suffering for many.\n\nThe third social hierarchy is gender oppression, which is instituted through gender norms society has adopted. In some cultures today, gender norms suggest that masculinity and femininity are opposite genders, however it is an unequal binary pair, with masculinity being dominant and femininity being subordinate. Gender as such is not natural but socially constructed, and gendered power differences provide social mechanisms that benefit masculinity. \"Many have argued that cultural practices concerning gender norms of child care, housework, appearance, and career impose an unfair burden on women and as such are oppressive.\" According to feminist Barbara Cattunar, women have always been \"subjected to many forms of oppression, backed up by religious texts which insist upon women's inferiority and subjugation\". Femininity has always been looked down upon, perpetuated by socially constructed stereotypes, which has affected women's societal status and opportunity. In current society, sources like the media further impose gendered oppression as they shape societal views. Females in pop-culture are objectified and sexualized, which can be understood as degrading to women by depicting them as sex objects with little regard for their character, political views, cultural contributions, creativity or intellect. Feminism, or struggles for women's cultural, political and economic equality, has challenged gender oppression. Gender oppression also takes place against trans, gender-non-conforming, gender queer, or non-binary individuals who do not identify with binary categories of masculine/feminine or male/female.\n\nYoung people are a commonly, yet rarely acknowledged, oppressed demographic. Minors are denied many democratic and human rights, including the rights to vote, marry, and give sexual consent. Society as a whole also tends to discriminate against young people and view them as inferior.\n\nThe fourth social hierarchy is sexuality oppression or heterosexism. Dominant societal views with respect to sexuality, and sex partner selection, have formed a sexuality hierarchy oppressing people who do not conform to heteronormativity. Heteronormativity is an underlying assumption that everyone in society is heterosexual, and those who are not are treated as different or even abnormal by society, excluded, oppressed, and sometimes subject to violence. Heterosexism also derives from societal views of the nuclear family which is presumed to be heterosexual, and dominated or controlled by the male partner. Social actions by oppressed groups such as LGBTQI movements have organized to create social change.\n\nReligious persecution is the systematic mistreatment of an individual because of their religious beliefs. According to Iris Young oppression can be divided into different categories such as powerlessness, exploitation, and violence. The first category of powerlessness in regards to religious persecution is when a group of people that follow one religion have less power than the dominant religious followers. An example of powerlessness would be during the 17th century when the pilgrims, wanting to escape the Church of England came to what is now called the United States. The pilgrims created their own religion of Protestantism, and after doing so they eventually passed laws to keep other religions from prospering. The Protestants used their power of legislature to oppress the other religions in the United States. The second category of oppression: exploitation, has been seen in many different forms around the world when it comes to religion. The definition of exploitation is the action or fact of treating someone unfairly in order to benefit from their work. For example during the American Civil War, white Americans used Chinese immigrants in order to build the transcontinental railroads. During this time it was common for the Chinese immigrants to follow the religions of Buddhism, Taoism, and Confucianism, because of this the Chinese were looked at as different and not equal to the white Americans. Due to this view it led them to unequal pay, and many hardships during their time working on the railroad. The third category that can be seen in religious persecution is violence. According to the Merriam Webster dictionary violence is \"the use of physical force so as to injure, abuse, damage, or destroy\". An example of violence in regards to religious persecution is hate crimes that occur in the United States against Muslims. Sense September 11th, 2001 hate crimes against people of the Muslim faith have greatly increased. One incident occurred on August 5th, 2017 when three men bombed a Mosque because they felt that Muslims \"'push their beliefs on everyone else'\". This violence happens to not only muslims but other religions as well.\n\nAddressing social oppression on both a macro and micro level, feminist Patricia Hill Collins discusses her \"matrix of domination\". The matrix of domination discusses the interrelated nature of four domains of power, including the structural, disciplinary, hegemonic, and interpersonal domains. Each of these spheres works to sustain current inequalities that are faced by marginalized, excluded or oppressed groups. The structural, disciplinary and hegemonic domains all operate on a macro level, creating social oppression through macro structures such as education, or the criminal justice system, which play out in the interpersonal sphere of everyday life through micro-oppressions.\n\nStandpoint theory can help us to understand the interpersonal domain. Standpoint theory deals with an individual's social location in that each person will have a very different perspective based on where they are positioned in society. For instance, a white male living in America will have a very different take on an issue such as abortion than a black female living in Africa. Each will have different knowledge claims and experiences that will have shaped how they perceive abortion. Standpoint theory is often used to expose the powerful social locations of those speaking, to justify claims of knowledge through closer experience of an issue, and to deconstruct the construction of knowledge of oppression by oppressors.\n\n\"Institutional Oppression occurs when established laws, customs, and practices systemically reflect and produce inequities based on one's membership in targeted social identity groups. If oppressive consequences accrue to institutional laws, customs, or practices, the institution is oppressive whether or not the individuals maintaining those practices have oppressive intentions.\"\nInstitutionalized oppression allows for government organizations and their employees to systematically favor specific groups of people based upon group identity. Dating back to colonization, the United States implemented the institution of slavery where Africans were brought to the United States to be a source of free labor to expand the cotton and tobacco industry. Implementing these systems by the United States government was justified through religious grounding where \"servants [were] bought and established as inheritable property\".\n\nAlthough the thirteenth, fourteenth, and fifteenth amendments freed African Americans, gave them citizenship, and provided them the right to vote, institutions such as some police departments continue to use oppressive systems against minorities. They train their officers to profile individuals based upon their racial heritage, and to exert excessive force to restrain them. Racial profiling and police brutality are \"employed to control a population thought to be undesirable, undeserving, and under punished by established law\". In both situations, police officers \"rely on legal authority to exonerate their extralegal use of force; both respond to perceived threats and fears aroused by out-groups, especially— but not exclusively— racial minorities\". For example, \"blacks are: approximately four times more likely to be targeted for police use of force than their white counterparts; arrested and convicted for drug-related criminal activities at higher rates than their overall representation in the U.S. population; and are more likely to fear unlawful and harsh treatment by law enforcement officials\". The International Association of Chiefs of Police collected data from police departments between the years 1995 and 2000 and found that 83% of incidents involving use-of-force against subjects of different races than the officer executing it involved a white officer and a black subject.\n\nInstitutionalized oppression is not only experienced by people of racial minorities, but can also affect those in the LGBT community. Oppression of the LGBT community in the United States dates back to President Eisenhower's presidency where he passed Executive Order 10450 in April 1953 which permitted non-binary sexual behaviors to be investigated by federal agencies. As a result of this order, \"More than 800 federal employees resigned or were terminated in the two years following because their files linked them in some way with homosexuality.\"\n\nOppression of the LGBT community continues today through some religious systems and their believers' justifications of discrimination based upon their own freedom of religious belief. States such as Arizona and Kansas passed laws in 2014 giving religious-based businesses \"the right to refuse service to LGBT customers\". The proposal of the Employment Non-Discrimination Act (EDNA) offers full protection of LGBT workers from job discrimination; however, the act does not offer protection against religious-based corporations and businesses, ultimately allowing the LGBT community to be discriminated against in environments such as churches and religious-based hospitals. The LGBT community is further oppressed by the United States government with the passage of the First Amendment Defense Act which states, \"Protecting religious freedom from Government intrusion is a Government interest of the highest order.\" This act essentially allows for institutions of any kind—schools, businesses, hospitals—to deny service to people based upon their sexuality because it goes against a religious belief.\n\nThe term economic oppression changes in meaning and significance over time, depending on its contextual application. In today's context, economic oppression may take several forms, including, but not limited to: the practice of bonded labour in some parts of India, serfdom, forced labour, low wages, denial of equal opportunity, and practicing employment discrimination, and economic discrimination based on sex, nationality, race, and religion.\n\nAnn Cudd describes the main forces of economic oppression as oppressive economic systems and direct and indirect forces. Even though capitalism and socialism are not inherently oppressive, they \"lend themselves to oppression in characteristic ways\". She defines direct forces of economic oppression as \"restrictions on opportunities that are applied from the outside on the oppressed, including enslavement, segregation, employment discrimination, group-based harassment, opportunity inequality, neocolonialism, and governmental corruption\". This allows for a dominant social group to maintain and maximize its wealth through the intentional exploitation of economically inferior subordinates. With indirect forces (also known as oppression by choice), \"the oppressed are co-opted into making individual choices that add to their own oppression\". The oppressed are faced with having to decide to go against their social good, and even against their own good. If they choose otherwise, they have to choose against their interests, which may lead to resentment by their group.\n\nAn example of direct forces of economic oppression is employment discrimination in the form of the gender pay gap. Restrictions on women's access to and participation in the workforce like the wage gap is an \"inequality most identified with industrialized nations with nominal equal opportunity laws; legal and cultural restrictions on access to education and jobs, inequities most identified with developing nations; and unequal access to capital, variable but identified as a difficulty in both industrialized and developing nations\". In the United States, the median weekly earnings for women were 82 percent of the median weekly earnings for men in 2016. Some argue women are prevented from achieving complete gender equality in the workplace because of the \"ideal-worker norm,\" which \"defines the committed worker as someone who works full-time and full force for forty years straight,\" a situation designed for the male sex. \n\nWomen, in contrast, are still expected to fulfill the caretaker role and take time off for domestic needs such as pregnancy and ill family members, preventing them from conforming to the \"ideal-worker norm\". With the current norm in place, women are forced to juggle full-time jobs and family care at home. Others believe that this difference in wage earnings is likely due to the supply and demand for women in the market because of family obligations. Eber and Weichselbaumer argue that \"over time, raw wage differentials worldwide have fallen substantially. Most of this decrease is due to better labor market endowments of females\".\n\nIndirect economic oppression is exemplified when individuals work abroad to support their families. Outsourced employees, working abroad generally little to no bargaining power not only with their employers, but with immigration authorities as well. They could be forced to accept low wages and work in poor living conditions. And by working abroad, an outsourced employee contributes to the economy of a foreign country instead of their own. Veltman and Piper describe the effects of outsourcing on female laborers abroad:\nHer work may be oppressive first in respects of being heteronomous: she may enter work under conditions of constraint; her work may bear no part of reflectively held life goals; and she may not even have the: freedom of bodily movement at work. Her work may also fail to permit a meaningful measure of economic independence or to help her support herself or her family, which she identifies as the very purpose of her working.\n\nBy deciding to work abroad, laborers are \"reinforcing the forces of economic oppression that presented them with such poor options\".\n\nAlthough a relatively modern form of resistance, feminism's origins can be traced back to the events leading up to the introduction of the Equal Rights Amendment (ERA) in 1923. While the ERA was created to address the need for equal protection under the law between men and women in the workplace, it spurred increased feminism that has come to represent the search for equal opportunity and respect for women in patriarchal societies, across all social, cultural, and political spheres. Demonstrations and marches have been a popular medium of support, with the January 21, 2017, Women's March's replication in major cities across the world drawing tens of thousands of supporters. Feminists' main talking points consist of women's reproductive rights, the closing of the pay gap between men and women, the glass ceiling and workplace discrimination, and the intersectionality of feminism with other major issues such as African-American rights, immigration freedoms, and gun violence.\n\nResistance to oppression has been linked to a moral obligation, an act deemed necessary for the preservation of self and society. Still, resistance to oppression has been largely overlooked in terms of the amount of research and number of studies completed on the topic, and therefore, is often largely misinterpreted as \"lawlessness, belligerence, envy, or laziness\". Over the last two centuries, resistance movements have risen that specifically aim to oppose, analyze, and counter various types of oppression, as well as to increase public awareness and support of groups marginalized and disadvantaged by systematic oppression. Late 20th century resistance movements such as liberation theology and anarchism set the stage for mass critiques of, and resistance to, forms of social and institutionalized oppression that have been subtly enforced and reinforced over time. Resistance movements of the 21st century have furthered the missions of activists across the world, and movements such as liberalism, Black Lives Matter (related: Blue Lives Matter, All Lives Matter) and feminism (related: Meninism) are some of the most prominent examples of resistance to oppression today.\n\nCudd, Ann E. (2006). \"Analyzing oppression\". Oxford University Press US. .\n\nDeutsch, M. (2006). A framework for thinking about oppression and its change. \"Social Justice Research, 19\"(1), 7–41. doi:10.1007/s11211-006-9998-3\n\nGil, David G. (2013). \"Confronting injustice and oppression: Concepts and strategies for social workers (2nd ed.)\". New York City, NY: Columbia University Press. OCLC 846740522\n\nHarvey, J. (1999). \"Civilized oppression\". Lanham, MD: Rowman & Littlefield. \n\nMarin, Mara (2017). \"Connected by commitment: Oppression and our responsibility to undermine it\". New York City, NY: Oxford University Press. OCLC 989519441\n\nNoël, Lise (1989). \"L'Intolérance. Une problématique générale\" (Intolerance: a general survey). Montréal (Québec), Canada: Boréal. . OCLC 20723090.\n\nOpotow, S. (1990). Moral exclusion and injustice: an introduction. \"Journal of Social Issues, 46\"(1), 1–20. doi:10.1111/j.1540-4560.1990.tb00268.x\n\nYoung, Iris (1990). \"Justice and the politics of difference\" (2011 reissue; foreward by Danielle Allen). Princeton, NJ: Princeton University Press. OCLC 778811811\n\nYoung-Bruehl, Elisabeth (1996). \"The anatomy of prejudices\". Cambridge, MA: Harvard University Press. . OCLC 442469051.\n\n"}
{"id": "17144791", "url": "https://en.wikipedia.org/wiki?curid=17144791", "title": "Pattern-oriented modeling", "text": "Pattern-oriented modeling\n\nPattern-oriented modeling (POM) is an approach to bottom-up complex systems analysis that was developed to model complex ecological and agent-based systems. A goal of POM is to make ecological modeling more rigorous and comprehensive.\n\nA traditional ecosystem model attempts to approximate the real system as closely as possible. POM proponents posit that an ecosystem is so information-rich that an ecosystem model will inevitably either leave out relevant information or become over-parameterized and lose predictive power. Through a focus on only the relevant patterns in the real system, POM offers a meaningful alternative to the traditional approach. \n\nAn attempt to mimic the scientific method, POM requires the researcher to begin with a pattern found in the real system, posit hypotheses to explain the pattern, and then develop predictions that can be tested. A model used to determine the original pattern may not be used to test the researcher’s predictions. Through this focus on the pattern, the model can be constructed to include only information relevant to the question at hand.\n\nPOM is also characterized by an effort to identify the appropriate temporal and spatial scale at which to study a pattern, and to avoid the assumption that a single process might explain a pattern at multiple temporal or spatial scales. It does, however, offer the opportunity to look explicitly at how processes at multiple scales might be driving a particular pattern.\n\nA look at the trade-offs between model complexity and payoff can be considered in the framework of the Medawar zone. The model is considered too simple if it addresses a single problem (e.g., the explanation behind a single pattern), whereas it will be considered too complex if it incorporates all the available biological data. The Medawar zone, where the payoff in what is learned is greatest, is at an intermediate level of model complexity.\n\nPattern-oriented modeling has been used to test a priori hypotheses on how herdsman decide which farmers to contract with when grazing their cattle. Herdsman behavior followed the pattern predicted by a 'friend' rather than 'cost' priority hypothesis.\n"}
{"id": "8420435", "url": "https://en.wikipedia.org/wiki?curid=8420435", "title": "Perajurit Tanah Air", "text": "Perajurit Tanah Air\n\nPerajurit Tanah Air or Inilah Barisan Kita is a patriotic Malaysian national song. Composed by Saiful Bahari. The song is remarkable for its impartial, egalitarian lyrics, which extols the soldiers' love and duty to the motherland without explicit reference to any particular race or religion.\n\n\"Perajurit Tanah Air\" found renewed popularity among Malaysians in the wake of the 2013 Lahad Datu standoff.\n\n\n"}
{"id": "1845896", "url": "https://en.wikipedia.org/wiki?curid=1845896", "title": "Personally identifiable information", "text": "Personally identifiable information\n\nPersonal information, described in United States legal fields as either personally identifiable information (PII), or sensitive personal information (SPI), as used in information security and privacy laws, is information that can be used on its own or with other information to identify, contact, or locate a single person, or to identify an individual in context. The abbreviation PII is widely accepted in the U.S. context, but the phrase it abbreviates has four common variants based on \"personal\" / \"personally\", and \"identifiable\" / \"identifying\". Not all are equivalent, and for legal purposes the effective definitions vary depending on the jurisdiction and the purposes for which the term is being used. (In other countries with privacy protection laws derived from the OECD privacy principles, the term used is more often \"personal information\", which may be somewhat broader: in Australia's \"Privacy Act \"1988 (Cth) \"personal information\" also includes information from which the person's identity is \"reasonably ascertainable\", potentially covering some information not covered by PII.)\n\nUnder European and other data protection regimes, which centre primarily around the General Data Protection Regulation, the term \"personal data\" is significantly broader, and determines the scope of the regulatory regime.\n\nNIST Special Publication 800-122 defines PII as \"any information about an individual maintained by an agency, including (1) any information that can be used to distinguish or trace an individual's identity, such as name, social security number, date and place of birth, mother's maiden name, or biometric records; and (2) any other information that is linked or linkable to an individual, such as medical, educational, financial, and employment information.\" So, for example, a user's IP address is not classed as PII on its own, but is classified as linked PII.\n\nThe concept of PII has become prevalent as information technology and the Internet have made it easier to collect PII leading to a profitable market in collecting and reselling PII. PII can also be exploited by criminals to stalk or steal the identity of a person, or to aid in the planning of criminal acts. As a response to these threats, many website privacy policies specifically address the gathering of PII, and lawmakers have enacted a series of legislation to limit the distribution and accessibility of PII.\n\nHowever, PII is a legal concept, not a technical concept, and as noted, it is not utilised in all jurisdictions. Because of the versatility and power of modern re-identification algorithms, the absence of PII data does not mean that the remaining data does not identify individuals. While some attributes may not be uniquely identifying on their own, any attribute can be potentially identifying in combination with others. These attributes have been referred to as quasi-identifiers or pseudo-identifiers. While such data may not constitute PII in the United States, it is highly likely to remain personal data under European data protection law.\n\nThe following data, often used for the express purpose of distinguishing individual identity, clearly classify as PII under the definition used by the National Institute of Standards and Technology (described in detail below):\n\n\nThe following are less often used to distinguish individual identity, because they are traits shared by many people.\nHowever, they are potentially PII, because they may be combined with other personal information to identify an individual.\n\n\nWhen a person wishes to remain anonymous, descriptions of them will often employ several of the above, such as \"a 34-year-old white male who works at Target\". Note that information can still be \"private\", in the sense that a person may not wish for it to become publicly known, without being personally identifiable. Moreover, sometimes multiple pieces of information, none sufficient by itself to uniquely identify an individual, may uniquely identify a person when combined; this is one reason that multiple pieces of evidence are usually presented at criminal trials. It has been shown that, in 1990, 87% of the population of the United States could be uniquely identified by gender, ZIP code, and full date of birth.\n\nIn hacker and Internet slang, the practice of finding and releasing such information is called \"doxing\". It is sometimes used to deter collaboration with law enforcement. On occasion, the doxing can trigger an arrest, particularly if law enforcement agencies suspect that the \"doxed\" individual may panic and disappear.\n\nThe U.S. government used the term \"personally identifiable\" in 2007 in a memorandum from the Executive Office of the President, Office of Management and Budget (OMB), and that usage now appears in US standards such as the NIST \"Guide to Protecting the Confidentiality of Personally Identifiable Information\" (SP 800-122). The OMB memorandum defines PII as follows:\n\nA term similar to PII, \"personal data\" is defined in EU directive 95/46/EC, for the purposes of the directive:\n\nHowever, in the EU rules, there has been a clearer notion that the data subject can potentially be identified through additional processing of other attributes—quasi- or pseudo-identifiers. In an early draft of the EU General Data Protection Regulation, this was formalized in Article 4, but was omitted in the final version: a \"data subject\" is one \"who can be identified, directly or indirectly, by means reasonably likely to be used by the controller or by any other natural or legal person\". The GDPR became enforceable on 25 May 2018.\n\nAnother term similar to PII, \"personal information\" is defined in a section of the California data breach notification law, SB1386:\n\nThe concept of information combination given in the SB1386 definition is key to correctly distinguishing PII, as defined by OMB, from \"personal information\", as defined by SB1386. Information, such as a name, that lacks context cannot be said to be SB1386 \"personal information\", but it must be said to be PII as defined by OMB. For example, the name John Smith has no meaning in the current context and is therefore not SB1386 \"personal information\", but it is PII. A Social Security Number (SSN) without a name or some other associated identity or context information is not SB1386 \"personal information\", but it is PII. For example, the SSN 078-05-1120 by itself is PII, but it is not SB1386 \"personal information\". However the combination of a valid name with the correct SSN is SB1386 \"personal information\".\n\nThe combination of a name with a context may also be considered PII; for example, if a person's name is on a list of patients for an HIV clinic. However, it is not necessary for the name to be combined with a context in order for it to be PII. The reason for this distinction is that bits of information such as names, although they may not be sufficient by themselves to make an identification, may later be combined with other information to identify persons and expose them to harm.\n\nAccording to the OMB, it is not always the case that PII is \"sensitive\", and context may be taken into account in deciding whether certain PII is or is not sensitive.\n\nIn Australia, the Privacy Act 1988 deals with the protection of individual privacy, using the OECD Privacy Principles from the 1980s to set up a broad, principles-based regulatory model (unlike in the US, where coverage is generally not based on broad principles but on specific technologies, business practices or data items). Section 6 has the relevant definition. The critical detail is that the definition of 'personal information' also applies to where the individual can be indirectly identified:\n\nThis raises the question of reasonableness: assume it is theoretically possible to identify a person from core information which say does NOT include a simple name and address, but does contain clues which could be pursued to ascertain who it relates to. Just how much extra effort or difficulty would such a step need before we could clearly say that the identity could NOT be \"reasonably ascertained\" from it?\n\nIt appears that this definition is significantly broader than the Californian example given above, and thus that Australian privacy law, while in some respects weakly enforced, may cover a broader category of data and information than in some US law.\nIn particular, online behavioral advertising businesses based in the US but surreptitiously collecting information from people in other countries in the form of cookies, bugs, trackers and the like may find that their preference to avoid the implications of wanting to build a psychographic profile of a particular person using the rubric of 'we don't collect personal information' may find that this does not make sense under a broader definition like that in the Australian Privacy Act.\n\n\nEuropean data protection law does not utilize the concept of PII, and its scope is instead determined by non-synonymous, wider concept of \"personal data\".\n\nFurther examples can be found on the EU privacy website.\n\n\nThe twelve Information Privacy Principles of the Privacy Act 1993 apply.\n\nThe Federal Act on Data Protection of 19 June 1992 (in force since 1993) has set up a strict protection of privacy by prohibiting virtually any processing of personal data which is not expressly authorized by the data subjects. The protection is subject to the authority of the Federal Data Protection and Information Commissioner.\n\nAdditionally, any person may ask in writing a company (managing data files) the correction or deletion of any personal data. The company must respond within thirty days.\n\nThe Privacy Act of 1974 (Pub.L. 93–579, 88 Stat. 1896, enacted December 31, 1974, 5 U.S.C. § 552a), a United States federal law, establishes a Code of Fair Information Practice that governs the collection, maintenance, use, and dissemination of personally identifiable information about individuals that is maintained in systems of records by federal agencies.\n\nOne of the primary focuses of the Health Insurance Portability and Accountability Act (HIPAA), is to protect a patient's Protected Health Information (PHI), which is similar to PII. The U.S. Senate proposed the Privacy Act of 2005, which attempted to strictly limit the display, purchase, or sale of PII without the person's consent. Similarly, the (proposed) Anti-Phishing Act of 2005 attempted to prevent the acquiring of PII through phishing.\n\nU.S. lawmakers have paid special attention to the social security number because it can be easily used to commit identity theft. The (proposed) Social Security Number Protection Act of 2005 and (proposed) Identity Theft Prevention Act of 2005 each sought to limit the distribution of an individual's social security number.\n\n\n\nIn forensics, particularly the identification and prosecution of criminals, personally identifiable information is critical in establishing evidence in criminal procedure. Criminals may go to great trouble to avoid leaving any PII, such as by:\n\n\n\nIn some professions, it is dangerous for a person's identity to become known, because this information might be exploited violently by their enemies; for example, their enemies might hunt them down or kidnap loved ones to force them to cooperate. For this reason, the United States Department of Defense (DoD) has strict policies controlling release of PII of DoD personnel. Many intelligence agencies have similar policies, sometimes to the point where employees do not disclose to their friends that they work for the agency.\n\nSimilar identity protection concerns exist for witness protection programs, women's shelters, and victims of domestic violence and other threats.\n\n\n"}
{"id": "102140", "url": "https://en.wikipedia.org/wiki?curid=102140", "title": "Perturbation theory", "text": "Perturbation theory\n\nPerturbation theory comprises mathematical methods for finding an approximate solution to a problem, by starting from the exact solution of a related, simpler problem. A critical feature of the technique is a middle step that breaks the problem into \"solvable\" and \"perturbation\" parts. Perturbation theory is applicable if the problem at hand cannot be solved exactly, but can be formulated by adding a \"small\" term to the mathematical description of the exactly solvable problem.\n\nPerturbation theory leads to an expression for the desired solution in terms of a formal power series in some \"small\" parameter – known as a perturbation series – that quantifies the deviation from the exactly solvable problem. The leading term in this power series is the solution of the exactly solvable problem, while further terms describe the deviation in the solution, due to the deviation from the initial problem. Formally, we have for the approximation to the full solution , a series in the small parameter (here called ), like the following:\n\nIn this example, would be the known solution to the exactly solvable initial problem and represent the higher-order terms which may be found iteratively by some systematic procedure. For small these higher-order terms in the series become successively smaller.\n\nAn approximate \"perturbation solution\" is obtained by truncating the series, usually by keeping only the first two terms, the initial solution and the \"first-order\" perturbation correction\n\nPerturbation theory is closely related to methods used in numerical analysis. The earliest use of what would now be called \"perturbation theory\" was to deal with the otherwise unsolvable mathematical problems of celestial mechanics: for example the orbit of the Moon, which moves noticeably differently from a simple Keplerian ellipse because of the competing gravitation of the Earth and the Sun.\n\nPerturbation methods start with a simplified form of the original problem, which is \"simple enough\" to be solved exactly. In celestial mechanics, this is usually a Keplerian ellipse. Under non-relativistic gravity, an ellipse is exactly correct when there are only two gravitating bodies (say, the Earth and the Moon) but not quite correct when there are three or more objects (say, the Earth, Moon, Sun, and the rest of the solar system) and not quite correct when the gravitational interaction is stated using formulations from General relativity.\n\nThe solved, but simplified problem is then \"perturbed\" to make the conditions that the perturbed solution actually satisfies closer to the formula in the original problem, such as including the gravitational attraction of a third body (the Sun). Typically, the \"conditions\" that represent reality are a formula (or several) that specifically express some physical law, like Newton's second law, the force-acceleration equation,\n\nIn the case of the example, the force is calculated based on the number of gravitationally relevant bodies; the acceleration is obtained, using calculus, from the path of the Moon in its orbit. Both of these come in two forms: approximate values for force and acceleration, which result from simplifications, and hypothetical exact values for force and acceleration, which would require the complete answer to calculate.\n\nThe slight changes that result from accommodating the perturbation, which themselves may have been simplified yet again, are used as corrections to the approximate solution. Because of simplifications introduced along every step of the way, the corrections are never perfect, and the conditions met by the corrected solution do not perfectly match the equation demanded by reality. However, even only one cycle of corrections often provides an excellent approximate answer to what the real solution should be.\n\nThere is no requirement to stop at only one cycle of corrections. A partially corrected solution can be re-used as the new starting point for yet another cycle of perturbations and corrections. In principle, cycles of finding increasingly better corrections could go on indefinitely. In practice, one typically stops at one or two cycles of corrections. The usual difficulty with the method is that the corrections progressively make the new solutions very much more complicated, so each cycle is much more difficult to manage than the previous cycle of corrections. Isaac Newton is reported to have said, regarding the problem of the Moon's orbit, that \"It causeth my head to ache.\"\n\nThis general procedure is a widely used mathematical tool in advanced sciences and engineering: start with a simplified problem and gradually add corrections that make the formula that the corrected problem becomes a closer and closer match to the original formula.\n\nExamples for the \"mathematical description\" are: an algebraic equation, a differential equation (e.g., the equations of motion or a wave equation), a free energy (in statistical mechanics), radiative transfer, a Hamiltonian operator (in quantum mechanics).\n\nExamples for the kind of solution to be found perturbatively: the solution of the equation (e.g., the trajectory of a particle), the statistical average of some physical quantity (e.g., average magnetization), the ground state energy of a quantum mechanical problem.\n\nExamples for the exactly solvable problems to start with: linear equations, including linear equations of motion (harmonic oscillator, linear wave equation), statistical or quantum-mechanical systems of non-interacting particles (or in general, Hamiltonians or free energies containing only terms quadratic in all degrees of freedom).\n\nExamples of \"perturbations\" to deal with: Nonlinear contributions to the equations of motion, interactions between particles, terms of higher powers in the Hamiltonian/Free Energy.\n\nFor physical problems involving interactions between particles, the terms of the perturbation series may be displayed (and manipulated) using Feynman diagrams.\n\nPerturbation theory was first devised to solve otherwise intractable problems in the calculation of the motions of planets in the solar system. For instance, Newton's law of universal gravitation explained the gravitation between two astronomical bodies, but when a third body is added, the problem was, \"How does each body pull on each?\" Newton's equation only allowed the mass of two bodies to be analyzed. The gradually increasing accuracy of astronomical observations led to incremental demands in the accuracy of solutions to Newton's gravitational equations, which led several notable 18th and 19th century mathematicians, such as Lagrange and Laplace, to extend and generalize the methods of perturbation theory. These well-developed perturbation methods were adopted and adapted to solve new problems arising during the development of quantum mechanics in 20th century atomic and subatomic physics. Paul Dirac developed perturbation theory in 1927 to evaluate when a particle would be emitted in radioactive elements. It was later named Fermi's golden rule.\n\nSince the planets are very remote from each other, and since their mass is small as compared to the mass of the Sun, the gravitational forces between the planets can be neglected, and the planetary motion is considered, to a first approximation, as taking place along Kepler's orbits, which are defined by the equations of the two-body problem, the two bodies being the planet and the Sun.\n\nSince astronomic data came to be known with much greater accuracy, it became necessary to consider how the motion of a planet around the Sun is affected by other planets. This was the origin of the three-body problem; thus, in studying the system Moon–Earth–Sun the mass ratio between the Moon and the Earth was chosen as the small parameter. Lagrange and Laplace were the first to advance the view that the constants which describe the motion of a planet around the Sun are \"perturbed\", as it were, by the motion of other planets and vary as a function of time; hence the name \"perturbation theory\".\n\nPerturbation theory was investigated by the classical scholars—Laplace, Poisson, Gauss—as a result of which the computations could be performed with a very high accuracy. The discovery of the planet Neptune in 1848 by Urbain Le Verrier, based on the deviations in motion of the planet Uranus (he sent the coordinates to Johann Gottfried Galle who successfully observed Neptune through his telescope), represented a triumph of perturbation theory.\n\nThe standard exposition of perturbation theory is given in terms of the \"order\" to which the perturbation is carried out: first-order perturbation theory or second-order perturbation theory, and whether the perturbed states are degenerate, which requires singular perturbation. In the singular case extra care must be taken, and the theory is slightly more elaborate.\n\nMany of the ab initio quantum chemistry methods use perturbation theory directly or are closely related methods. Implicit perturbation theory works with the complete Hamiltonian from the very beginning and never specifies a perturbation operator as such. Møller–Plesset perturbation theory uses the difference between the Hartree–Fock Hamiltonian and the exact non-relativistic Hamiltonian as the perturbation. The zero-order energy is the sum of orbital energies. The first-order energy is the Hartree–Fock energy and electron correlation is included at second-order or higher. Calculations to second, third or fourth order are very common and the code is included in most ab initio quantum chemistry programs. A related but more accurate method is the coupled cluster method.\n\n\n"}
{"id": "25262679", "url": "https://en.wikipedia.org/wiki?curid=25262679", "title": "Phonemic imagery", "text": "Phonemic imagery\n\nPhonemic imagery refers to the processing of thoughts as words rather than as symbols or other images. It is sometimes referred to as the equivalent of inner speech or covert speech, and sometimes considered as a third phenomenon, separate from but similar to these other forms of internal speech.\n\nPhonemic imagery is a part of the philosophy of consciousness rather than linguistics as it is considered an internal phenomenon of consciousness observed through reflection rather than amenable to empirical observation.\n"}
{"id": "1168486", "url": "https://en.wikipedia.org/wiki?curid=1168486", "title": "Plücker coordinates", "text": "Plücker coordinates\n\nIn geometry, Plücker coordinates, introduced by Julius Plücker in the 19th century, are a way to assign six homogeneous coordinates to each line in projective 3-space, P. Because they satisfy a quadratic constraint, they establish a one-to-one correspondence between the 4-dimensional space of lines in P and points on a quadric in P (projective 5-space). A predecessor and special case of Grassmann coordinates (which describe \"k\"-dimensional linear subspaces, or \"flats\", in an \"n\"-dimensional Euclidean space), Plücker coordinates arise naturally in geometric algebra. They have proved useful for computer graphics, and also can be extended to coordinates for the screws and wrenches in the theory of kinematics used for robot control.\n\nA line \"L\" in 3-dimensional Euclidean space is determined by two distinct points that it contains, or by two distinct planes that contain it. Consider the first case, with points x = (\"x\",\"x\",\"x\") and y = (\"y\",\"y\",\"y\"). The vector displacement from x to y is nonzero because the points are distinct, and represents the \"direction\" of the line. That is, every displacement between points on \"L\" is a scalar multiple of d = y − x. If a physical particle of unit mass were to move from x to y, it would have a moment about the origin. The geometric equivalent is a vector whose direction is perpendicular to the plane containing \"L\" and the origin, and whose length equals twice the area of the triangle formed by the displacement and the origin. Treating the points as displacements from the origin, the moment is m = x×y, where \"×\" denotes the vector cross product. For a fixed line, \"L\", the area of the triangle is proportional to the length of the segment between x and y, considered as the base of the triangle; it is not changed by sliding the base along the line, parallel to itself. By definition the moment vector is perpendicular to every displacement along the line, so d•m = 0, where \"•\" denotes the vector dot product.\n\nAlthough neither d nor m alone is sufficient to determine \"L\", together the pair does so uniquely, up to a common (nonzero) scalar multiple which depends on the distance between x and y. That is, the coordinates\n\nmay be considered homogeneous coordinates for \"L\", in the sense that all pairs (\"λd:\"λm), for \"λ\" ≠ 0, can be produced by points on \"L\" and only \"L\", and any such pair determines a unique line so long as d is not zero and d•m = 0. Furthermore, this approach extends to include points, lines, and a plane \"at infinity\", in the sense of projective geometry.\n\nAlternatively, let the equations for points x of two distinct planes containing \"L\" be\n\nThen their respective planes are perpendicular to vectors a and b, and the direction of \"L\" must be perpendicular to both. Hence we may set d = a×b, which is nonzero because a and b are neither zero nor parallel (the planes being distinct and intersecting). If point x satisfies both plane equations, then it also satisfies the linear combination\n\nThat is, m = \"a\" b − \"b\" a is a vector perpendicular to displacements to points on \"L\" from the origin; it is, in fact, a moment consistent with the d previously defined from a and b.\n\nproof：need to show m = \"a\" b − \"b\" a = r×d = r× a×b, let a•a = b•b = 1\n\npoint B is the origin. the line is pass point D and orthogonal to the plane, the 2 plane pass CD and DE both orthogonal to the plane, BD is diameter\n\nangle BCE = BDE = BGF, so points D,G,E,H on a circle, and angle GHG is right angle, FG orthogonal to BD, so 4 points C, D, H, F on a circle, and \n\nwhen ||r|| = 0, the line is the one pass origin with direction d; if ||r|| > 0, the line is with direction d, the plane including the origin and the line has normal vector m, the line is tangent to a circle on the plane centered origin and with radius ||\"r\"|| at point r.\n\nAlthough the usual algebraic definition tends to obscure the relationship, (d:m) are the Plücker coordinates of \"L\".\n\nIn a 3-dimensional projective space P, let \"L\" be a line through distinct points x and y with homogeneous coordinates (\"x\":\"x\":\"x\":\"x\") and (\"y\":\"y\":\"y\":\"y\").\nThe Plücker coordinates \"p\" are defined as follows:\nThis implies \"p\" = 0 and \"p\" = −\"p\", reducing the possibilities to only six (4 choose 2) independent quantities. The sixtuple\n\nis uniquely determined by \"L\" up to a common nonzero scale factor. Furthermore, not all six components can be zero.\nThus the Plücker coordinates of \"L\" may be considered as homogeneous coordinates of a point in a 5-dimensional projective space, as suggested by the colon notation.\n\nTo see these facts, let \"M\" be the 4×2 matrix with the point coordinates as columns.\n\nThe Plücker coordinate \"p\" is the determinant of rows \"i\" and \"j\" of \"M\".\nBecause x and y are distinct points, the columns of \"M\" are linearly independent; \"M\" has rank 2. Let \"M′\" be a second matrix, with columns x′ and y′ a different pair of distinct points on \"L\". Then the columns of \"M′\" are linear combinations of the columns of \"M\"; so for some 2×2 nonsingular matrix Λ,\n\nIn particular, rows \"i\" and \"j\" of \"M′\" and \"M\" are related by\n\nTherefore, the determinant of the left side 2×2 matrix equals the product of the determinants of the right side 2×2 matrices, the latter of which is a fixed scalar, det Λ. Furthermore, all six 2×2 subdeterminants in \"M\" cannot be zero because the rank of \"M\" is 2.\n\nDenote the set of all lines (linear images of P) in P by \"G\". We thus have a map:\nwhere\n\nAlternatively, a line can be described as the intersection of two planes. Let \"L\" \nbe a line contained in distinct planes a and b with homogeneous coefficients (\"a\":\"a\":\"a\":\"a\") and (\"b\":\"b\":\"b\":\"b\"), respectively. (The first plane equation is ∑ \"a\"\"x\"=0, for example.) The dual Plücker coordinate \"p\" is\n\nDual coordinates are convenient in some computations, and they are equivalent to primary coordinates:\nHere, equality between the two vectors in homogeneous coordinates means that the numbers on the right side are equal to the numbers on the left side up to some common scaling factor formula_8. Specifically, let (\"i\",\"j\",\"k\",\"ℓ\") be an even permutation of (0,1,2,3); then\n\nTo relate back to the geometric intuition, take \"x\" = 0 as the plane at infinity; thus the coordinates of points \"not\" at infinity can be normalized so that \"x\" = 1. Then \"M\" becomes\n\nand setting x = (\"x\",\"x\",\"x\") and y = (\"y\",\"y\",\"y\"), we have d = (\"p\",\"p\",\"p\") and m = (\"p\",\"p\",\"p\").\n\nDually, we have d = (\"p\",\"p\",\"p\") and m = (\"p\",\"p\",\"p\").\n\nIf the point z = (\"z\":\"z\":\"z\":\"z\") lies on \"L\", then the columns of\n\nare linearly dependent, so that the rank of this larger matrix is still 2. This implies that all 3×3 submatrices have determinant zero, generating four (4 choose 3) plane equations, such as\n\nThe four possible planes obtained are as follows.\n\nUsing dual coordinates, and letting (\"a\":\"a\":\"a\":\"a\") be the line coefficients, each of these is simply \"a\" = \"p\", or\n\nEach Plücker coordinate appears in two of the four equations, each time multiplying a different variable; and as at least one of the coordinates is nonzero, we are guaranteed non-vacuous equations for two distinct planes intersecting in \"L\". Thus the Plücker coordinates of a line determine that line uniquely, and the map α is an injection.\n\nThe image of α is not the complete set of points in P; the Plücker coordinates of a line \"L\" satisfy the quadratic Plücker relation\n\nFor proof, write this homogeneous polynomial as determinants and use Laplace expansion (in reverse).\n\nSince both 3×3 determinants have duplicate columns, the right hand side is identically zero.\n\nAnother proof may be done like this:\nSince vector\n\nis perpendicular to vector\n\n(see above), the scalar product of \"d\" and \"m\" must be zero! q.e.d.\n\nLetting (\"x\":\"x\":\"x\":\"x\") be the point coordinates, four possible points on a line each have coordinates \"x\" = \"p\", for \"j\" = 0…3. Some of these possible points may be inadmissible because all coordinates are zero, but since at least one Plücker coordinate is nonzero, at least two distinct points are guaranteed.\n\nIf (\"q\":\"q\":\"q\":\"q\":\"q\":\"q\") are the homogeneous coordinates of a point in P, without loss of generality assume that \"q\" is nonzero. Then the matrix\n\nhas rank 2, and so its columns are distinct points defining a line \"L\". When the P coordinates, \"q\", satisfy the quadratic Plücker relation, they are the Plücker coordinates of \"L\". To see this, first normalize \"q\" to 1. Then we immediately have that for the Plücker coordinates computed from \"M\", \"p\" = \"q\", except for\n\nBut if the \"q\" satisfy the Plücker relation \"q\"+\"q\"\"q\"+\"q\"\"q\" = 0, then \"p\" = \"q\", completing the set of identities.\n\nConsequently, α is a surjection onto the algebraic variety consisting of the set of zeros of the quadratic polynomial\n\nAnd since α is also an injection, the lines in P are thus in bijective correspondence with the points of this quadric in P, called the Plücker quadric or Klein quadric.\n\nPlücker coordinates allow concise solutions to problems of line geometry in 3-dimensional space, especially those involving incidence.\n\nTwo lines in P are either skew or coplanar, and in the latter case they are either coincident or intersect in a unique point. If \"p\" and \"p\"′ are the Plücker coordinates of two lines, then they are coplanar precisely when d⋅m′+m⋅d′ = 0, as shown by\n\nWhen the lines are skew, the sign of the result indicates the sense of crossing: positive if a right-handed screw takes \"L\" into \"L\"′, else negative.\n\nThe quadratic Plücker relation essentially states that a line is coplanar with itself.\n\nIn the event that two lines are coplanar but not parallel, their common plane has equation\n\nwhere x = (\"x\",\"x\",\"x\").\n\nThe slightest perturbation will destroy the existence of a common plane, and near-parallelism of the lines will cause numeric difficulties in finding such a plane even if it does exist.\n\nDually, two coplanar lines, neither of which contains the origin, have common point\n\nTo handle lines not meeting this restriction, see the references.\n\nGiven a plane with equation\n\nor more concisely 0 = \"a\"\"x\"+a•x; and given a line not in it with Plücker coordinates (d:m), then their point of intersection is\n\nThe point coordinates, (\"x\":\"x\":\"x\":\"x\"), can also be expressed in terms of Plücker coordinates as\n\nDually, given a point (\"y\":y) and a line not containing it, their common plane has equation\n\nThe plane coordinates, (\"a\":\"a\":\"a\":\"a\"), can also be expressed in terms of dual Plücker coordinates as\n\nBecause the Klein quadric is in P, it contains linear subspaces of dimensions one and two (but no higher). These correspond to one- and two-parameter families of lines in P.\n\nFor example, suppose \"L\" and \"L\"′ are distinct lines in P determined by points x, y and x′, y′, respectively. Linear combinations of their determining points give linear combinations of their Plücker coordinates, generating a one-parameter family of lines containing \"L\" and \"L\"′. This corresponds to a one-dimensional linear subspace belonging to the Klein quadric.\n\nIf three distinct and non-parallel lines are coplanar; their linear combinations generate a two-parameter family of lines, all the lines in the plane. This corresponds to a two-dimensional linear subspace belonging to the Klein quadric.\n\nIf three distinct and non-coplanar lines intersect in a point, their linear combinations generate a two-parameter family of lines, all the lines through the point. This also corresponds to a two-dimensional linear subspace belonging to the Klein quadric.\n\nA ruled surface is a family of lines that is not necessarily linear. It corresponds to a curve on the Klein quadric. For example, a hyperboloid of one sheet is a quadric surface in P ruled by two different families of lines, one line of each passing through each point of the surface; each family corresponds under the Plücker map to a conic section within the Klein quadric in P.\n\nDuring the nineteenth century, \"line geometry\" was studied intensively. In terms of the bijection given above, this is a description of the intrinsic geometry of the Klein quadric.\n\nLine geometry is extensively used in ray tracing application where the geometry and intersections of rays need to be calculated in 3D. An implementation is described in\nIntroduction to Plücker Coordinates written for the Ray Tracing forum by Thouis Jones.\n\n\n"}
{"id": "20611212", "url": "https://en.wikipedia.org/wiki?curid=20611212", "title": "Rule of Law in Armed Conflicts Project", "text": "Rule of Law in Armed Conflicts Project\n\nThe Rule of Law in Armed Conflicts Project (RULAC Project) is an initiative of the Geneva Academy of International Humanitarian Law and Human Rights to support the application and implementation of the international law of armed conflict.\n\nThrough a global database and analysis, the RULAC Project has as its aim an assessment of the implementation by states of the law applicable in armed conflicts: \n\nThe project will ultimately cover all member states of the United Nations and parties to the Geneva Conventions as well as contested territories, whether they are in situation of armed conflict or not. Indeed, certain international rules must be implemented during peacetime or are relevant in post-conflict situations, in particular those relating to the repression of international crimes. In addition, the rules regarding the fight against terrorism, also to be covered by the website, are applicable to states that are not necessarily in a situation of armed conflict.\n\nThe website is divided into three parts. The homepage offers a small description of the applicable law and addresses the main legal issues in that area, for example the legal qualification of conflicts or the applicability of international law to non-state armed groups. The website then offers for each country the relevant texts and documents dealing with the national and international legal framework (national legislation and case law, resolutions of intergovernmental organizations, treaty, etc.). Finally, the website offers a legal analysis that, on one hand, qualifies the conflict under international humanitarian law and on the other hand, determines the applicable law. This part of the website, certainly the most delicate in juridical and political terms, is particular to the RULAC Project.\n\nThe RULAC Project should prove to be a precious source of information for government officials, journalists and more widely for any person interested by the respect of the law in war.\n\nThe first three Geneva Conventions were revised, expanded, and replaced, and the fourth one was added, in 1949.\n\n\nIn addition, there are three additional amendment protocols to the Geneva Convention:\n\nICRC, What is International Humanitarian Law?, Fact Sheet, 2004.\n\nICRC, Basic Rules of the 1949 Geneva Conventions and 1977 Additional Protocols, 1988\n\nICRC, study of customary international humanitarian law.\n\n\n"}
{"id": "1439959", "url": "https://en.wikipedia.org/wiki?curid=1439959", "title": "Seven virtues", "text": "Seven virtues\n\nThe \"Catechism of the Catholic Church\" defines virtue as \"a habitual and firm disposition to do the good.\" Traditionally, the seven Christian virtues or heavenly virtues combine the four classical cardinal virtues of prudence, justice, temperance and courage (or fortitude) with the three theological virtues of faith, hope and charity. These were adopted by the Church Fathers as the seven virtues.\n\nThe Greek philosophers Aristotle and Plato, regarded temperance, wisdom, justice, and courage as the four most desirable character traits. The Book of Wisdom is one of the seven Sapiential Books included in the Septuagint. Wisdom 8:7 states that the fruits of Wisdom \"...are virtues; For she teaches moderation and prudence, justice and fortitude, and nothing in life is more useful for men than these.\"\n\nThe moral virtues are attitudes, dispositions, and good habits that govern one's actions, passions, and conduct according to reason; and are acquired by human effort. Immanuel Kant said, \"Virtue is the moral strength of the will in obeying the dictates of duty\". The cardinal virtues are prudence, justice, fortitude, and temperance.\n\n\nPhilosophers recognized the interrelatedness of the virtues such that courage without prudence risks becoming mere foolhardiness. Aquinas found an interconnection of practical wisdom (\"prudentia\") and moral virtue. This is frequently termed \"the Unity of the Virtues.\" Aquinas also argued that it not only matters what a person does but how the person does it. The person must aim at a good end and also make a right choice about the means to that end. The moral virtues direct the person to aim at a good end, but to ensure that the person make the right choices about the means to a good end, one needs practical wisdom.\n\nThe traditional understanding of the differences in the natures of Cardinal and Theological virtues, is that the latter are not fully accessible to humans in their natural state without assistance from God. \"All virtues have as their final scope to dispose man to acts conducive to his true happiness. The happiness, however, of which man is capable is twofold, namely, natural, which is attainable by man's natural powers, and supernatural, which exceeds the capacity of unaided human nature. Since, therefore, merely natural principles of human action are inadequate to a supernatural end, it is necessary that man be endowed with supernatural powers to enable him to attain his final destiny. Now these supernatural principles are nothing else than the theological virtues.\"\n\nA list of seven virtues that oppose the seven deadly sins appeared later in an epic poem titled \"Psychomachia\", or \"Battle/Contest of the Soul\". Written by Aurelius Clemens Prudentius, a Christian governor who died around 410 AD, it entails the battle between good virtues and evil vices. The enormous popularity of this work in the Middle Ages helped to spread the concept of holy virtue throughout Europe.\n\nAfter Pope Gregory released his list of seven deadly sins in 590 AD, the seven virtues became identified as chastity, temperance, charity, diligence, patience, kindness, and humility. Practicing them is said to protect one against temptation from the seven deadly sins.\n\nIt should be noted, however, that these seven virtues do not correspond to the seven heavenly virtues arrived at by combining the cardinal and theological virtues. Furthermore, efforts in the Middle Ages to set the seven heavenly virtues in direct opposition to the seven capital sins are both uncommon and beset with difficulties. “[T]reatises exclusively concentrating on both septenaries are actually quite rare.” and “examples of late medieval catalogues of virtues and vices which extend or upset the double heptad can be easily multiplied.” And there are problems with this parallelism.\n\n"}
{"id": "16845636", "url": "https://en.wikipedia.org/wiki?curid=16845636", "title": "Size functor", "text": "Size functor\n\nGiven a size pair formula_1 where formula_2 is a manifold of dimension\nformula_3 and formula_4 is an arbitrary real continuous function defined\non it, the formula_5-th \" size functor\", with formula_6, denoted\nby formula_7, is the functor in formula_8, where formula_9 is the category of ordered real numbers, and formula_10 is the category of Abelian groups, defined in the following way. For formula_11, setting formula_12, formula_13, formula_14 equal to the inclusion from formula_15 into formula_16, and formula_17 equal to the morphism in formula_9 from formula_19 to formula_20,\n\n\nIn other words, the size functor studies the\nprocess of the birth and death of homology classes as the lower level set changes.\nWhen formula_2 is smooth and compact and formula_4 is a Morse function, the functor formula_26 can be\ndescribed by oriented trees, called formula_27 − trees.\n\nThe concept of size functor was introduced as an extension to homology theory and category theory of the idea of size function. The main motivation for introducing the size functor originated by the observation that the size function formula_28 can be seen as the rank\nof the image of formula_29.\n\nThe concept of size functor is strictly related to the concept of persistent homology group\nstudied in persistent homology. It is worth to point out that the formula_5-th persistent homology group coincides with the image of the homomorphism formula_31.\n\n"}
{"id": "26858533", "url": "https://en.wikipedia.org/wiki?curid=26858533", "title": "Species inquirenda", "text": "Species inquirenda\n\nIn biological classification, a species inquirenda is a species of doubtful identity requiring further investigation. The use of the term in English-language biological literature dates back to at least the early nineteenth century.\n\n"}
{"id": "29238", "url": "https://en.wikipedia.org/wiki?curid=29238", "title": "Systems theory", "text": "Systems theory\n\nSystems theory is the interdisciplinary study of systems. A system is a cohesive conglomeration of interrelated and interdependent parts that is either natural or man-made. Every system is delineated by its spatial and temporal boundaries, surrounded and influenced by its environment, described by its structure and purpose or nature and expressed in its functioning. In terms of its effects, a system can be more than the sum of its parts if it expresses synergy or emergent behavior. Changing one part of the system usually affects other parts and the whole system, with predictable patterns of behavior. For systems that are self-learning and self-adapting, the positive growth and adaptation depend upon how well the system is adjusted with its environment. Some systems function mainly to support other systems by aiding in the maintenance of the other system to prevent failure. The goal of systems theory is systematically discovering a system's dynamics, constraints, conditions and elucidating principles (purpose, measure, methods, tools, etc.) that can be discerned and applied to systems at every level of nesting, and in every field for achieving optimized equifinality.\n\nGeneral systems theory is about broadly applicable concepts and principles, as opposed to concepts and principles applicable to one domain of knowledge. It distinguishes dynamic or active systems from static or passive systems. Active systems are activity structures or components that interact in behaviours and processes. Passive systems are structures and components that are being processed. E.g. a program is passive when it is a disc file and active when it runs in memory. The field is related to systems thinking and systems engineering.\n\n\nThe term \"general systems theory\" originates from Bertalanffy's general systems theory (GST). His ideas were adopted by others including Kenneth E. Boulding, William Ross Ashby and Anatol Rapoport working in mathematics, psychology, biology, game theory and social network analysis.\n\nSociological systems thinking started earlier, in the 19th century. Stichweh states: \"... Since its beginnings the social sciences were an important part of the establishment of systems theory... the two most influential suggestions were the comprehensive sociological versions of systems theory which were proposed by Talcott Parsons since the 1950s and by Niklas Luhmann since the 1970s.\" References include Parsons' action theory and Luhmann's social systems theory.\n\nContemporary ideas from systems theory have grown with diverse areas, exemplified by the work of biologist Ludwig von Bertalanffy, linguist Béla H. Bánáthy, sociologist Talcott Parsons, ecological systems with Howard T. Odum, Eugene Odum and Fritjof Capra, organizational theory and management with individuals such as Peter Senge, interdisciplinary study with areas like Human Resource Development from the work of Richard A. Swanson, and insights from educators such as Debora Hammond and Alfonso Montuori. As a transdisciplinary, interdisciplinary and multiperspectival domain, the area brings together principles and concepts from ontology, philosophy of science, physics, computer science, biology and engineering as well as geography, sociology, political science, psychotherapy (within family systems therapy) and economics among others. Systems theory thus serves as a bridge for interdisciplinary dialogue between autonomous areas of study as well as within the area of systems science itself.\n\nIn this respect, with the possibility of misinterpretations, von Bertalanffy believed a general theory of systems \"should be an important regulative device in science\", to guard against superficial analogies that \"are useless in science and harmful in their practical consequences\". Others remain closer to the direct systems concepts developed by the original theorists. For example, Ilya Prigogine, of the Center for Complex Quantum Systems at the University of Texas, Austin, has studied emergent properties, suggesting that they offer analogues for living systems. The theories of autopoiesis of Francisco Varela and Humberto Maturana represent further developments in this field. Important names in contemporary systems science include Russell Ackoff, Ruzena Bajcsy, Béla H. Bánáthy, Gregory Bateson, Anthony Stafford Beer, Peter Checkland, Barbara Grosz, Brian Wilson, Robert L. Flood, Allenna Leonard, Radhika Nagpal, Fritjof Capra, Warren McCulloch, Kathleen Carley, Michael C. Jackson, Katia Sycara, and Edgar Morin among others.\n\nWith the modern foundations for a general theory of systems following World War I, Ervin Laszlo, in the preface for Bertalanffy's book: \"Perspectives on General System Theory\", points out that the translation of \"general system theory\" from German into English has \"wrought a certain amount of havoc\":\n\nA system in this frame of reference can contain regularly interacting or interrelating groups of activities. For example, in noting the influence in organizational psychology as the field evolved from \"an individually oriented industrial psychology to a systems and developmentally oriented organizational psychology\", some theorists recognize that organizations have complex social systems; separating the parts from the whole reduces the overall effectiveness of organizations. This difference, from conventional models that center on individuals, structures, departments and units, separates in part from the whole, instead of recognizing the interdependence between groups of individuals, structures and processes that enable an organization to function. Laszlo explains that the new systems view of organized complexity went \"one step beyond the Newtonian view of organized simplicity\" which reduced the parts from the whole, or understood the whole without relation to the parts. The relationship between organisations and their environments can be seen as the foremost source of complexity and interdependence. In most cases, the whole has properties that cannot be known from analysis of the constituent elements in isolation. Béla H. Bánáthy, who argued—along with the founders of the systems society—that \"the benefit of humankind\" is the purpose of science, has made significant and far-reaching contributions to the area of systems theory. For the Primer Group at ISSS, Bánáthy defines a perspective that iterates this view:\n\nSimilar ideas are found in learning theories that developed from the same fundamental concepts, emphasising how understanding results from knowing concepts both in part and as a whole. In fact, Bertalanffy's organismic psychology paralleled the learning theory of Jean Piaget. Some consider interdisciplinary perspectives critical in breaking away from industrial age models and thinking, wherein history represents history and math represents math, while the arts and sciences specialization remain separate and many treat teaching as behaviorist conditioning. The contemporary work of Peter Senge provides detailed discussion of the commonplace critique of educational systems grounded in conventional assumptions about learning, including the problems with fragmented knowledge and lack of holistic learning from the \"machine-age thinking\" that became a \"model of school separated from daily life\". In this way some systems theorists attempt to provide alternatives to, and evolved ideation from orthodox theories which have grounds in classical assumptions, including individuals such as Max Weber and Émile Durkheim in sociology and Frederick Winslow Taylor in scientific management. The theorists sought holistic methods by developing systems concepts that could integrate with different areas.\n\nSome may view the contradiction of reductionism in conventional theory (which has as its subject a single part) as simply an example of changing assumptions. The emphasis with systems theory shifts from parts to the organization of parts, recognizing interactions of the parts as not static and constant but dynamic processes. Some questioned the conventional closed systems with the development of open systems perspectives. The shift originated from absolute and universal authoritative principles and knowledge to relative and general conceptual and perceptual knowledge and still remains in the tradition of theorists that sought to provide means to organize human life. In other words, theorists rethought the preceding history of ideas; they did not lose them. Mechanistic thinking was particularly critiqued, especially the industrial-age mechanistic metaphor for the mind from interpretations of Newtonian mechanics by Enlightenment philosophers and later psychologists that laid the foundations of modern organizational theory and management by the late 19th century.\n\nSystem dynamics is an approach to understanding the nonlinear behaviour of complex systems over time using stocks, flows, internal feedback loops, and time delays.\n\nSystems biology is a movement that draws on several trends in bioscience research. Proponents describe systems biology as a biology-based inter-disciplinary study field that focuses on complex interactions in biological systems, claiming that it uses a new perspective (holism instead of reduction). Particularly from the year 2000 onwards, the biosciences use the term widely and in a variety of contexts. An often stated ambition of systems biology is the modelling and discovery of emergent properties which represents properties of a system whose theoretical description requires the only possible useful techniques to fall under the remit of systems biology. It is thought that Ludwig von Bertalanffy may have created the term systems biology in 1928.\n\nSystems ecology is an interdisciplinary field of ecology, a subset of Earth system science, that takes a holistic approach to the study of ecological systems, especially ecosystems. Systems ecology can be seen as an application of general systems theory to ecology. Central to the systems ecology approach is the idea that an ecosystem is a complex system exhibiting emergent properties. Systems ecology focuses on interactions and transactions within and between biological and ecological systems, and is especially concerned with the way the functioning of ecosystems can be influenced by human interventions. It uses and extends concepts from thermodynamics and develops other macroscopic descriptions of complex systems.\n\nSystems engineering is an interdisciplinary approach and means for enabling the realisation and deployment of successful systems. It can be viewed as the application of engineering techniques to the engineering of systems, as well as the application of a systems approach to engineering efforts. Systems engineering integrates other disciplines and specialty groups into a team effort, forming a structured development process that proceeds from concept to production to operation and disposal. Systems engineering considers both the business and the technical needs of all customers, with the goal of providing a quality product that meets the user's needs.\n\nSystems psychology is a branch of psychology that studies human behaviour and experience in complex systems. It received inspiration from systems theory and systems thinking, as well as the basics of theoretical work from Roger Barker, Gregory Bateson, Humberto Maturana and others. It makes an approach in psychology in which groups and individuals receive consideration as systems in homeostasis. Systems psychology \"includes the domain of engineering psychology, but in addition seems more concerned with societal systems and with the study of motivational, affective, cognitive and group behavior that holds the name engineering psychology.\" In systems psychology, \"characteristics of organizational behaviour, for example individual needs, rewards, expectations, and attributes of the people interacting with the systems, considers this process in order to create an effective system\".\n\nWhether considering the first systems of written communication with Sumerian cuneiform to Mayan numerals, or the feats of engineering with the Egyptian pyramids, systems thinking can date back to antiquity. Differentiated from Western rationalist traditions of philosophy, C. West Churchman often identified with the I Ching as a systems approach sharing a frame of reference similar to pre-Socratic philosophy and Heraclitus. Von Bertalanffy traced systems concepts to the philosophy of G.W. Leibniz and Nicholas of Cusa's \"coincidentia oppositorum\". While modern systems can seem considerably more complicated, today's systems may embed themselves in history.\n\nFigures like James Joule and Sadi Carnot represent an important step to introduce the \"systems approach\" into the (rationalist) hard sciences of the 19th century, also known as the energy transformation. Then, the thermodynamics of this century, by Rudolf Clausius, Josiah Gibbs and others, established the \"system\" reference model as a formal scientific object.\n\nThe Society for General Systems Research specifically catalyzed systems theory as an area of study, which developed following the World Wars from the work of Ludwig von Bertalanffy, Anatol Rapoport, Kenneth E. Boulding, William Ross Ashby, Margaret Mead, Gregory Bateson, C. West Churchman and others in the 1950s, had specifically catalyzed by collaboration in. Cognizant of advances in science that questioned classical assumptions in the organizational sciences, Bertalanffy's idea to develop a theory of systems began as early as the interwar period, publishing \"An Outline for General Systems Theory\" in the \"British Journal for the Philosophy of Science\", Vol 1, No. 2, by 1950. Where assumptions in Western science from Greek thought with Plato and Aristotle to Newton's \"Principia\" have historically influenced all areas from the hard to social sciences (see David Easton's seminal development of the \"political system\" as an analytical construct), the original theorists explored the implications of twentieth century advances in terms of systems.\n\nPeople have studied subjects like complexity, self-organization, connectionism and adaptive systems in the 1940s and 1950s. In fields like cybernetics, researchers such as Norbert Wiener, William Ross Ashby, John von Neumann and Heinz von Foerster, examined complex systems mathematically. John von Neumann discovered cellular automata and self-reproducing systems, again with only pencil and paper. Aleksandr Lyapunov and Jules Henri Poincaré worked on the foundations of chaos theory without any computer at all. At the same time Howard T. Odum, known as a radiation ecologist, recognized that the study of general systems required a language that could depict energetics, thermodynamics and kinetics at any system scale. Odum developed a general system, or universal language, based on the circuit language of electronics, to fulfill this role, known as the Energy Systems Language. Between 1929-1951, Robert Maynard Hutchins at the University of Chicago had undertaken efforts to encourage innovation and interdisciplinary research in the social sciences, aided by the Ford Foundation with the interdisciplinary Division of the Social Sciences established in 1931. Numerous scholars had actively engaged in these ideas before (Tectology by Alexander Bogdanov, published in 1912-1917, is a remarkable example), but in 1937, von Bertalanffy presented the general theory of systems at a conference at the University of Chicago.\n\nThe systems view was based on several fundamental ideas. First, all phenomena can be viewed as a web of relationships among elements, or a system. Second, all systems, whether electrical, biological, or social, have common patterns, behaviors, and properties that the observer can analyze and use to develop greater insight into the behavior of complex phenomena and to move closer toward a unity of the sciences. System philosophy, methodology and application are complementary to this science. By 1956, theorists established the Society for General Systems Research, which they renamed the International Society for Systems Science in 1988. The Cold War affected the research project for systems theory in ways that sorely disappointed many of the seminal theorists. Some began to recognize that theories defined in association with systems theory had deviated from the initial General Systems Theory (GST) view. The economist Kenneth Boulding, an early researcher in systems theory, had concerns over the manipulation of systems concepts. Boulding concluded from the effects of the Cold War that abuses of power always prove consequential and that systems theory might address such issues. Since the end of the Cold War, a renewed interest in systems theory emerged, combined with efforts to strengthen an ethical view on the subject.\n\nMany early systems theorists aimed at finding a general systems theory that could explain all systems in all fields of science. The term goes back to Bertalanffy's book titled \"General System theory: Foundations, Development, Applications\" from 1968. He developed the \"allgemeine Systemlehre\" (general systems theory) first via lectures beginning in 1937 and then via publications beginning in 1946.\n\nVon Bertalanffy's objective was to bring together under one heading the organismic science he had observed in his work as a biologist. His desire was to use the word \"system\" for those principles that are common to systems in general. In GST, he writes:\n\nErvin Laszlo in the preface of von Bertalanffy's book \"Perspectives on General System Theory\":\n\nLudwig von Bertalanffy outlines systems inquiry into three major domains: Philosophy, Science, and Technology. In his work with the Primer Group, Béla H. Bánáthy generalized the domains into four integratable domains of systemic inquiry:\n\nThese operate in a recursive relationship, he explained. Integrating Philosophy and Theory as Knowledge, and Method and Application as action, Systems Inquiry then is knowledgeable action.\n\nCybernetics is the study of the communication and control of regulatory feedback both in living and lifeless systems (organisms, organizations, machines), and in combinations of those. Its focus is how anything (digital, mechanical or biological) controls its behavior, processes information, reacts to information, and changes or can be changed to better accomplish those three primary tasks.\n\nThe terms \"systems theory\" and \"cybernetics\" have been widely used as synonyms. Some authors use the term \"cybernetic\" systems to denote a proper subset of the class of general systems, namely those systems that include feedback loops. However Gordon Pask's differences of eternal interacting actor loops (that produce finite products) makes general systems a proper subset of cybernetics. According to Jackson (2000), von Bertalanffy promoted an embryonic form of general system theory (GST) as early as the 1920s and 1930s but it was not until the early 1950s it became more widely known in scientific circles.\n\nThreads of cybernetics began in the late 1800s that led toward the publishing of seminal works (e.g., Wiener's \"Cybernetics\" in 1948 and von Bertalanffy's \"General Systems Theory\" in 1968). Cybernetics arose more from engineering fields and GST from biology. If anything it appears that although the two probably mutually influenced each other, cybernetics had the greater influence. Von Bertalanffy (1969) specifically makes the point of distinguishing between the areas in noting the influence of cybernetics: \"Systems theory is frequently identified with cybernetics and control theory. This again is incorrect. Cybernetics as the theory of control mechanisms in technology and nature is founded on the concepts of information and feedback, but as part of a general theory of systems;\" then reiterates: \"the model is of wide application but should not be identified with 'systems theory' in general\", and that \"warning is necessary against its incautious expansion to fields for which its concepts are not made.\" (17-23). Jackson (2000) also claims von Bertalanffy was informed by Alexander Bogdanov's three volume \"Tectology\" that was published in Russia between 1912 and 1917, and was translated into German in 1928. He also states it is clear to Gorelik (1975) that the \"conceptual part\" of general system theory (GST) had first been put in place by Bogdanov. The similar position is held by Mattessich (1978) and Capra (1996). Ludwig von Bertalanffy never even mentioned Bogdanov in his works, which Capra (1996) finds \"surprising\".\n\nCybernetics, catastrophe theory, chaos theory and complexity theory have the common goal to explain complex systems that consist of a large number of mutually interacting and interrelated parts in terms of those interactions. Cellular automata (CA), neural networks (NN), artificial intelligence (AI), and artificial life (ALife) are related fields, but they do not try to describe general (universal) complex (singular) systems. The best context to compare the different \"C\"-Theories about complex systems is historical, which emphasizes different tools and methodologies, from pure mathematics in the beginning to pure computer science now. Since the beginning of chaos theory when Edward Lorenz accidentally discovered a strange attractor with his computer, computers have become an indispensable source of information. One could not imagine the study of complex systems without the use of computers today.\n\nComplex adaptive systems (CAS) are special cases of complex systems. They are \"complex\" in that they are diverse and composed of multiple, interconnected elements; they are \"adaptive\" in that they have the capacity to change and learn from experience. In contrast to control systems in which negative feedback dampens and reverses disequilibria, CAS are often subject to positive feedback, which magnifies and perpetuates changes, converting local irregularities into global features. Another mechanism, Dual-phase evolution arises when connections between elements repeatedly change, shifting the system between phases of variation and selection that reshape the system. Differently from Beer Management Cybernetics, Cultural Agency Theory (CAT) provides a modelling approach to explore predefined contexts and can be adapted to reflect those contexts.\n\nThe term \"complex adaptive system\" was coined at the interdisciplinary Santa Fe Institute (SFI), by John H. Holland, Murray Gell-Mann and others. An alternative conception of complex adaptive (and learning) systems, methodologically at the interface between natural and social science, has been presented by Kristo Ivanov in terms of hypersystems. This concept intends to offer a theoretical basis for understanding and implementing participation of \"users\", decisions makers, designers and affected actors, in the development or maintenance of self-learning systems.\n\n\n\n\nOrganizations\n"}
{"id": "11599908", "url": "https://en.wikipedia.org/wiki?curid=11599908", "title": "Tonalli", "text": "Tonalli\n\nTonalli /to(ː)nalli/ (see also: Tonal) plays a multiplicity of roles; acting as a day sign, body part, and a symbol of the sun’s warmth. Ancient Nahuatl people believed that it was located in the hair and the fontanel area of one’s skull, and that the tonalli provided the “vigor and energy for growth and development”. It often overlaps with the force of teyolía which was often considered both an animating force (soul) and the physical heart in various Mesoamerican cultures.\n\nThe root “tona” acts as a verb to mean \"to irradiate or make warm with sun”.\n\nIn the Ancient Nahuatl belief, the tonalli is bestowed upon a child in utero by the aged deities known as Ometecuhtli and Omecihuatl, or the “Lord and Lady of Duality”. The implementation of tonalli is conducted through a process known as Fire Drilling.\n\nIt is believed that the old deities, Ometecuhtli and Omecihuatl transferred tonalli to human fetuses by “simultaneously breath[ing] the tonalli into the child and ignit[ing] a fire in its chest”. This Fire Drilling process involves an upright wooden piece being twirled rapidly on a flat base. It produces heat through friction, although this seemingly simple instrument requires considerable skill to make anything but smoke. The fire maker blows on an ignited spark to fan it into a vigorous flame, and the breathing (or blowing air) and friction in the chest animate an infant. The Franciscan friars connected this idea of Fire Drilling, namely, the conception of tonalli as breath, to Christianity as the infusion of breath into the body recalls the beginning of Genesis, where God the Father breathes life into Adam.\n\nThe Nahuatl people of Mesoamerica believed that the soul comprised three entities: Tonalli, Teyolía, and Ihíyotl, three souls in the body. Tonalli is located in fontanel area of the skull. Teyolía is located in the heart and Ihíyotl is in the liver. Each of these souls has its own functions and protective deities. But there are important differences. The Tonalli is the soul that enters and leaves the body. In Atla in the northern Sierra de Puebla, the inhabitants believe this is the soul that travels while you sleep at night, and then comes back. This is the soul that leaves and comes back every time you sneeze, or whenever you yawn, or even when you are startled. The Nahua believed that it was not good to sneeze and keep talking, because it causes your tonalli to leave and once your Tonalli leaves, you have to wait for a period of time before it returns. At that moment, anything can enter your body. However, the soul of the heart (Teyolía) and the soul of the liver (Ihiíyotl) only leave your body when you die; those two souls will exit only at the exact moment of your death.\n\nAlong with the heart, the ancient Mexica took blood from sacrificed prisoners and offered its fructifying force to the gods. On public and private ritual occasions, people drew blood from their ears, tongues, or calves, splattered it on pieces of native paper, and gave it to the spirits as tokens of thanks for benefits received or as requests for future favors. The gods could be coaxed or rewarded, less by the physical fluid itself than by the tonalli (life force) it carried and transferred to them. Among most modern Nahuatl speakers, the state of the life force may even be determined from the movement of blood in the body, whether this movement is experienced as a tic, a pulse, or a muscular movement. When blood drains from its proper course, the person loses his/her life force, and, essentially, soul.\n\nTonalli was highly valued in society and sought after in warfare and ritual sacrifice. The hair that covered the head, especially the fontanel area, was a major receptacle of tonalli, and it was believed that hair prevented the tonalli from leaving the body. During times of war, when a warrior would take a prisoner captive, the warrior would often seize the captive by the hair (the fontanel area). It was believed that the fortitude and valor of a warrior resided, in part, in the hair, and there are many pictorial scenes showing Aztec warriors grabbing the hair of enemies. The hair of warriors captured in battle was kept by the captors in order to increase their tonalli. The severed heads of enemy warriors were a supreme prize for the city, which gained more tonalli through the ceremonial use of heads. The tonalli embedded within skulls of enemy warriors and captives were also offered as gifts to the gods in the temple complexes. This was believed to be an offering to the gods in the form of a type of debt repayment.\n\nThe essence of tonalli was a force that could transcend the limits of the human body. Parts of the tonalli could reside outside the body in objects and animals. For example, tlacopatli beads were often left in the temple and represented a substitute for a child unable to go to the temple school, due to age restrictions. These beads served to contain the tonalli and do penitence for the underaged child. This points to the belief of the physicality of the soul and the embodiment of the tonalli. Stones were also used as repositories of the soul in a different sense. As tonalli was considered to be an indication of destiny or fate, the possession of gemstones served as direct evidence of those with positive fates despite low births and the auspicious destinies of members of the elite class.\nThe tonalli operated within a complex that involved the god of the birth date and the human. The soul of the individuality of a person, resided inside; but the god of the tonalli resided outside The co-essence of tonalli in a human body is subject to the power of an external god/time that could lodge in a person’s body. The dynamics of this joint essence relationship made it necessary for humans to implore each god of birth dates for internal strength, health and good fortune.\n\nTonalli, along with \"teyolia\" and \"ihiyotl\", was believed to direct the physiological process of the human body. It gave a person character, and was highly valued by the family and sought after in warfare and ritual sacrifice. It was believed that tonalli could be taken from a human body and either offered to the gods as a form of debt payment or acquired by the ritual person who touched the physical entity in which they resided. The concept of tonalli was not only limited to human beings. It infiltrated animals, gods, plants, and objects used in rituals.\n\nThe tonalli also determines the sign under which a person is born and informs fortune, character, and name. Tonalli conveyed astrological signs and names through birthdays, and in the Mexica divinatory system, a person’s birthday fell on one of the 260 name days in a special calendar. Individuals followed the path or code of conduct demanded by the tonalli and the day sign. This calendar was notable because it was used solely for divination and celebrating rituals in the deities’ honor. The calendrical name of a given person transmitted a character and fate to both men and women. A person born on the first day of the 260-day cycle, would be named One Crocodile and was given a positive character that would bring about authority, wealth, and fame. It is important to note that inauspicious day signs could be ameliorated through rituals such as first baths, and life events.\n\nIt was believed that individuals possess free will within the constraints imposed by their tonalli. One is born with either favorable or unfavorable tonalli and with a corresponding predetermined character. While this places certain constraints upon what one may accomplish, one freely chooses what to make of one's tonalli within these limits. Someone born with favorable tonalli may squander it through improper action; someone with unfavorable tonalli may neutralize its adverse effects through knowledge of the sacred calendar and careful selection of actions.\n\nAs an animating force within both the human and spiritual world, retaining tonalli provides consciousness and personality. The concept of soul loss was inherent to the ancient Central Mexican understanding of aging and death. The loss of tonalli, in its various stages, is known by states in which the person suffers slowed, impaired, or complete loss of consciousness. As the understanding of tonalli is reliant upon conceptions of warmth, heat, and the sun, the absence of tonalli is felt as fluctuations in internal temperature. The inability to re-implant the life force leads to a decline in health and ultimately death. The modern Nahua and the Mexica have identified the tonalli’s departure as the cause of illnesses with the same general and observable symptoms as death and dying.\n\n"}
{"id": "31830628", "url": "https://en.wikipedia.org/wiki?curid=31830628", "title": "Type II Partnerships", "text": "Type II Partnerships\n\nType II partnerships were developed at the Johannesburg World Summit on Sustainable Development in 2002. Arising in opposition to the state-centred eco-governmentality of previous approaches to sustainable development policy, the partnerships facilitate the inclusion of private and civil actors into the management of sustainable development. The partnerships are employed alongside traditional intergovernmental mechanisms in order to effectively implement the United Nations' Agenda 21 and Millennium Development Goals, particularly at sub-national level. Although widely acknowledged as one of the most innovative and effective developments in global environmental governance in recent years, the partnerships have faced criticism due to fears of a lack of accountability, and the risk that they may exacerbate inequalities of power between Northern and Southern states. Despite these reservations, there is a general consensus among state and non-governmental actors that Type II partnerships are a significantly progressive step in global environmental governance in general, and sustainable development discourse in particular.\n\nFirst proposed at the Johannesburg World Summit on Sustainable Development in 2002, Type II partnerships are characterised by collaborations between national or sub-national governments, private sector actors and civil society actors, who form voluntary transnational agreements in order to meet specific sustainable development goals. The Johannesburg negotiations also produced so-called Type I outcomes referred to under the umbrella of a Global Deal, a series of legally binding intergovernmental commitments designed to aid states in the implementation of sustainable development goals. However, during the discussions preceding the summit, a growing consensus emerged among the actors involved that traditional intergovernmental relations were no longer sufficient in the management of sustainable development, and consequently the talks began to incorporate suggestions for increasingly decentralised and participatory approaches. Considered to be one of the most innovative and celebrated outcomes of the 2002 summit, the partnerships were created as a means by which to further implement the sustainable development goals set out in the Agenda 21 action plan, particularly those objectives aimed at the local and regional level, as traditional Type I intergovernmental strategies were deemed unlikely to effectively achieve lower level implementation of the Agenda 21 plan.\n\nThe Johannesburg negotiations concluded that Type II partnerships must meet seven key criteria: i) they should be voluntary and based on shared responsibility, ii) they must complement, rather than substitute, intergovernmental sustainable development strategies, and must meet the agreed outcomes of the Johannesburg summit, iii) they must consist of a range of multi-level stakeholders, preferably within a given area of work, iv) they must ensure transparency and accountability, v) they must produce tangible results, vi) the partnership must be new, and adequate funding must be available, and vii) a follow-up process must be developed. If these requirements were successfully fulfilled, it was hoped that Type II partnerships could create a fundamental shift in sustainable development discourse, leading to an increasingly participatory, bottom-up method of governing the issue.\n\nFollowing the Johannesburg summit, the United Nations Commission for Sustainable Development was granted responsibility for the management of Type II partnerships as its mandate and focus were considered most appropriate to the supervision of the partnerships. The UNCSD was created following the 1992 Rio summit with the sole mandate of overseeing the implementation of Agenda 21 and the Rio Declaration on Environment and Development, a focus which made the UNCSD highly amenable to the management of Type II agreements. The UNCSD supervises the 300+ Type II partnerships formed as a result of the summit, ensuring that the partnerships continue to implement the sustainable development goals agreed at Johannesburg.\n\nAn example of one of the larger partnerships supervised by the UNCSD is the Global Water Partnership, a network of over 2,300 global partners composed of organisations including UN agencies, governments of developed and developing countries, development banks, research institutions, NGOs and private actors. The organisation aims to diffuse information regarding water management to stakeholders at all levels, and assists countries in the development of water management strategies at the local, national and global level, increasing the capacity of developing nations to manage water supplies in the long term.\n\n\"“This Summit will be remembered not for the treaties, commitments, or eloquent declarations it produced, but for the first stirrings of a new way of governing the global commons, the beginnings of a shift from the stiff formal waltz of traditional diplomacy to the jazzier dance of improvisational solution oriented partnerships that may include non-government organizations, willing governments and other stakeholders.”\" World Resources Institute, 2002\n\nThe dominance of Type II partnerships as a primary outcome of the Johannesburg summit represented a fundamental shift in the governing of sustainable development; a transition from the top-down, government-centred method favoured by the Brundtland Report and at the 1992 Rio summit, to a collaborative, multi-stakeholder approach which acknowledged the importance of the economic and social expertise of non-governmental actors in sustainable development initiatives. Immediately prior to the summit, then- UN Secretary General Kofi Annan predicted that whilst governments would be responsible for the creation of a common plan of action for sustainable development, the most significant and powerful agents of change to emerge from the Johannesburg negotiations would be Type II partnerships, through which the UN hoped to harness the technological, financial and scientific resources of partners involved in the agreements, reinvigorating the organisation's pursuit of sustainable development.\n\nThe UN guidelines for Type II partnerships specified that the agreements should be complementary to, not an alternative to, intergovernmental action plans for sustainable development. As opposed to developing the partnerships as a method of 'governing without government', the agreements were designed to govern alongside traditional government approaches. Rather than treating Type II partnerships as a panacea for sustainable development, it was hoped that such participatory multi-stakeholder governance mechanisms would increase the flexibility and enhance the implementation of sustainable development policy in collaboration with states and international organisations. After the Johannesburg summit, the concept of environmental governance was no longer understood as a legalistic function performed solely by governments, but rather as a collaborative, informal approach to the management of environmental issues, involving both state and non-governmental actors. This new understanding demonstrates that the changing approach to sustainable development which arose from the Johannesburg summit influenced a much wider shift in global environmental governance.\n\nWithin a Foucauldian context, the Type I and Type II arguments presented in the summit negotiations represent two fundamentally opposing rationalities of government. The interventionist, state-centric approach to sustainable development favoured by advocates of the Global Deal represents a rationality of government which Foucault identified as bio-politics; the application of political power in an attempt to control or modify life processes. Such an approach to the pursuit of sustainable development offers little opportunity for participation by private or civil actors, in direct contrast to the multi-stakeholder premise of Type II partnerships. The eco-governmental, disciplinary ideas conveyed in the Global Deal suggest that the Type I approach typifies the centralised command-and-control method favoured by traditional government, whereas the decentralised, voluntary nature of Type II partnerships demonstrates an advanced liberal governmentality which empowers non-state actors to undertake responsibility for the governing of sustainable development, an approach which is representative of the participatory, multi-stakeholder methods by which governance is characterised. The eventual dominance of Type II partnerships in the outcomes of the Johannesburg summit therefore symbolises a wider shift in the understanding of the purposes, ends and means of government in relation to sustainable development and environmental governance.\n\nHowever, Mert questions the compatibility of partnerships with the global governance of sustainable development. The partnerships represented a point of intersection between three previously separate political discourses; participatory democracy, private governance, and sustainable development, altering the dynamics of global environmental governance processes. The shift towards voluntary mechanisms as opposed to international regulation could prove problematic as legally binding frameworks are sometimes the most appropriate solution to the governance of environmental problems. A hegemonic approach to sustainable development discourse could demonstrate greater effectiveness in the management of the issue than a fragmented, partnership-driven approach which could lead to inconsistent and conflicting management of such a global issue.\n\nType II partnerships exemplify a growing reliance on public-private-civil co-operation in environmental governance. The architects of the summit placed an emphasis on discussions which would encourage the creation of multi-stakeholder partnerships with the objective of fulfilling UN sustainable development goals, acknowledging that traditional intergovernmental agreements were inadequate to sufficiently promote sustainable development. Furthermore, state actors were notoriously unwilling to improve international environmental co-operation prior to the Johannesburg summit, leaving those who sought a positive outcome to the WSSD to search for alternative solutions which incorporated a broader variety of actors. Type II partnerships emerged as the dominant outcome of the Johannesburg summit, highlighting their importance as agents of change in the achievement of sustainable development. The partnerships were considered by advocates to be representative of a new era of environmental governance, characterised by collaborative decision-making and shared responsibility between public, private and civil actors in the management of transnational public issues.\n\nThe development of Type II partnerships marked the emergence of a broader phenomenon within environmental governance- the introduction of transnational governance networks in the management of global environmental issues. Transnational governance networks combine actors from the public, private and civil sectors in the pursuit of common practices and ideas. The role of networks of private and civil actors in transboundary communication is not novel to the academic community; however, the emphasis on transnational public-private-civil networks as mechanisms for achieving sustainable development during the Johannesburg negotiations led to recognition of the ability of such networks to integrate private and civil actors into the global environmental governance process.\n\nTransnational governance networks address a number of shortcomings in traditional state-centred approaches to the management of transboundary issues such as sustainable development. They can diffuse information to the public perhaps more effectively than governments or international organisations, particularly when such information requires a degree of technical expertise to deliver, such as the transfer of specialised knowledge from the private sector to industry groups regarding sustainable business practices. They can also facilitate the implementation of global management strategies at the local level, and they may potentially close the participation gap in global environmental governance by involving private and civil actors in decision-making processes.\n\nThe Johannesburg summit represented a further shift in the governing of sustainable development; rather than considering environmental issues in isolation, as had previously been common practice within sustainable development policy, the Johannesburg negotiations concluded that a reframing of sustainable development discourse, which re-conceptualised sustainable development as a dynamic interaction between three interdependent pillars- society, environment and economy- was necessary in order to pursue a more holistic ideal of sustainable development. This reframing of sustainable development required Type II partnerships to address a broader concept of sustainable development, and consequently objectives such as poverty alleviation and community involvement feature alongside environmental issues in the objectives of the partnerships.\n\nAlthough designed to incorporate a broader range of social, environmental and economic perspectives into the environmental governance process and to facilitate the inclusion of actors from all levels into decision-making, the extent to which imbalances of power between actors involved in the partnerships affects their implementation has provoked concern among their critics.\n\nBrinkerhoff and Brinkerhoff theorised that an effective partnership must fulfil two essential criteria: mutuality- interdependence and equality between partners, and organisational identity- the equal maintenance of each partner’s missions and goals. In the event of a partnership between Northern and Southern actors, for example, the North will inevitably contribute greater financial and material resources to the partnership than the South, creating a power inequality which may enable the North to assume control of the partnership and impairing the mutuality necessary for the partnership to function successfully. This concern was reflected by a number of developing nations who formed a coalition to lobby against the development of Type II partnerships, fearing that the partnerships would award too much authority over sustainable development to the global North, whilst simultaneously reducing the responsibility of industrialised nations to develop and implement legally binding intergovernmental management strategies.\n\nIn order to maintain mutuality, it is therefore essential that the definition of a contribution within Type II partnerships is extended beyond financial and material resources, and includes knowledge, skills and other relevant strengths which can be incorporated to redress the balance of power within the partnership.\n\nCritics of Type II partnerships have expressed concern that the initiative is simply a means by which to deflect accountability for sustainable development management from states and international organisations. The United States, a nation infamously opposed to state-led environmental governance as evidenced by their withdrawal from the Kyoto protocol, strongly supported the development of Type II partnerships whilst continuing to oppose Type I outcomes, leading to concern that some nations may view Type II partnerships as an opportunity to deflect attention from a lack of state-level progress in the management of sustainable development. An emphasis on Type II partnerships, it is argued, could therefore be exploited by nations who wish to avoid accountability for the management of sustainable development and environmental issues, transferring responsibility for such issues to private actors who are less accountable to the needs of those affected by the problem in question.\n\nThe ensuring of accountability and transparency is a key criterion of Type II partnerships; however, the diverse multi-stakeholder composition of the partnerships negates the use of traditional accountability methods, such as the introduction of a centralised authority charged with maintaining the accountability of the partners involved in Type II agreements. Bäckstrand suggests that a pluralistic system of accountability, incorporating market and reputational accountability measures such as financial sanctions and naming and shaming, could improve the accountability of the actors involved in Type II partnerships by providing more flexible methods of ensuring accountability which can be adapted to the nature of the actor in question.\n\nThe Type II partnerships developed at the Johannesburg summit demonstrated a paradigm-shifting impact upon sustainable development discourse and the conceptualisation of global environmental governance. By addressing the limitations of the state-centric, top-down method which typified environmental governance prior to Johannesburg and facilitating the participation of private and civil actors in the governing of sustainable development, the partnerships became emblematic of the transition from command-and-control government to the informal, participatory governance mechanisms by which global environmental governance is now classified. Furthermore, the partnerships exemplify the use of transnational governance networks as a mechanism by which to implement environmental policy at local and regional level. Such factors led the World Resources Institute to declare the partnerships representative of a ‘new era’ of environmental governance.\n\nHowever, considering the flaws inherent in the partnerships, it is crucial that advocates of the agreements resist the temptation to consider them a magic bullet with which every shortcoming of a centralised approach to environmental governance can be addressed. Although advantageous in terms of increased flexibility and effective lower-level implementation of policies, partnerships lack the internal and external accountability of intergovernmental strategies, and may intensify power inequalities between the industrialised North and developing South. The decision of the UN to introduce Type II partnerships as complementary governance mechanism is therefore the most appropriate application of the partnerships, as the dynamic interaction between intergovernmental strategies and voluntary public-private-civil partnerships can potentially produce a far greater positive impact upon global environmental governance than the sum of its parts.\n\n"}
{"id": "1006035", "url": "https://en.wikipedia.org/wiki?curid=1006035", "title": "Unix time", "text": "Unix time\n\nUnix time (also known as POSIX time or UNIX Epoch time) is a system for describing a point in time. It is the number of seconds that have elapsed since 00:00:00 Coordinated Universal Time (UTC), Thursday, 1 January 1970, minus leap seconds. Every day is treated as if it contains exactly 86400 seconds, so leap seconds are to be subtracted since the Epoch. It is used widely in Unix-like and many other operating systems and file formats. However, Unix time is not a true representation of UTC, as a leap second in UTC shares the same Unix time as the second which came before it. Unix time may be checked on most Unix systems by typing on the command line.\n\nOn systems where the representation of Unix time is as a signed 32-bit number, the representation will end after the completion of (2 − 1) seconds from 00:00:00 on 1 January 1970, which will happen at 3:14:08 on 19 January 2038 UTC, although exactly how many seconds that is from now is not known because of unpredictable leap seconds. This is referred to as the \"Year 2038 problem\" where the 32-bit signed Unix time will overflow and will take the actual count to negative.\n\nTwo layers of encoding make up Unix time. The first layer encodes a point in time as a scalar real number which represents the number of seconds that have passed since the beginning of 00:00:00 UTC Thursday, 1 January 1970. The second layer encodes that number as a sequence of bits or decimal digits.\n\nAs is standard with UTC, this article labels days using the Gregorian calendar, and counts times within each day in hours, minutes, and seconds. Some of the examples also show International Atomic Time (TAI), another time scheme, which uses the same seconds and is displayed in the same format as UTC, but in which every day is exactly seconds long, gradually losing synchronization with the Earth's rotation at a rate of roughly one second per year.\n\nUnix time is a single signed number which increments every second, without requiring the calculations to determine year, month, day of month, hour and minute required for intelligibility to humans.\n\nThe \"Unix epoch\" is the time 00:00:00UTC on 1 January 1970. There is a problem with this definition, in that UTC did not exist in its current form until 1972; this issue is discussed below. For brevity, the remainder of this section uses ISO 8601 date and time format, in which the Unix epoch is 1970-01-01T00:00:00Z.\n\nThe Unix time number is zero at the Unix epoch, and increases by exactly per day since the epoch. Thus 2004-09-16T00:00:00Z, days after the epoch, is represented by the Unix time number × = . This can be extended backwards from the epoch too, using negative numbers; thus 1957-10-04T00:00:00Z, days before the epoch, is represented by the Unix time number × = .\n\nWithin each day, the Unix time number is calculated as in the preceding paragraph at midnight UTC (00:00:00Z), and increases by exactly 1 per second since midnight. Thus 2004-09-16T17:55:43.54Z, seconds since midnight on the first day in the example above, is represented by the Unix time number + = . On dates before the epoch the number still increases, thus becoming less negative, as time moves forward.\n\nBecause Unix time is based on the Unix epoch, it is sometimes referred to as \"epoch time\".\n\nThe above scheme means that on a normal UTC day, which has a duration of seconds, the Unix time number changes in a continuous manner across midnight. For example, at the end of the day used in the examples above, the time representations progress as follows:\n\nWhen a leap second occurs, the UTC day is not exactly seconds long and the Unix time number (which always increases by exactly each day) experiences a discontinuity. At the end of a day with a negative leap second, which has not yet occurred, the Unix time number would jump up by 1 to the start of the next day. During a positive leap second at the end of a day, which occurs about every year and a half on average, the Unix time number increases continuously into the next day during the leap second and then at the end of the leap second jumps back by 1 (returning to the start of the next day). For example, this is what happened on strictly conforming POSIX.1 systems at the end of 1998:\n\nUnix time numbers are repeated in the second immediately following a positive leap second. The Unix time number is thus ambiguous: it can refer either to the instant in the middle of the leap second, or to the instant one second later, half a second after midnight UTC. In the theoretical case when a negative leap second occurs, no ambiguity is caused, but instead there is a range of Unix time numbers that do not refer to any point in time at all.\n\nA Unix clock is often implemented with a different type of positive leap second handling associated with the Network Time Protocol (NTP). This yields a system that does not conform to the POSIX standard. See the section below concerning NTP for details.\n\nWhen dealing with periods that do not encompass a UTC leap second, the difference between two Unix time numbers is equal to the duration in seconds of the period between the corresponding points in time. This is a common computational technique. However, where leap seconds occur, such calculations give the wrong answer. In applications where this level of accuracy is required, it is necessary to consult a table of leap seconds when dealing with Unix times, and it is often preferable to use a different time encoding that does not suffer from this problem.\n\nA Unix time number is easily converted back into UTC by taking the quotient and modulus of the Unix time number, modulo . The quotient is the number of days since the epoch, and the modulus is the number of seconds since midnight UTC on that day. If given a Unix time number that is ambiguous due to a positive leap second, this algorithm interprets it as the time just after midnight. It never generates a time that is during a leap second. If given a Unix time number that is invalid due to a negative leap second, it generates an equally invalid UTC time. If these conditions are significant, it is necessary to consult a table of leap seconds to detect them.\n\nCommonly a Mills-style Unix clock is implemented with leap second handling not synchronous with the change of the Unix time number. The time number initially decreases where a leap should have occurred, and then it leaps to the correct time 1 second after the leap. This makes implementation easier, and is described by Mills' paper. This is what happens across a positive leap second:\n\nThis can be decoded properly by paying attention to the leap second state variable, which unambiguously indicates whether the leap has been performed yet. The state variable change is synchronous with the leap.\n\nA similar situation arises with a negative leap second, where the second that is skipped is slightly too late. Very briefly the system shows a nominally impossible time number, but this can be detected by the TIME_DEL state and corrected.\n\nIn this type of system the Unix time number violates POSIX around both types of leap second. Collecting the leap second state variable along with the time number allows for unambiguous decoding, so the correct POSIX time number can be generated if desired, or the full UTC time can be stored in a more suitable format.\n\nThe decoding logic required to cope with this style of Unix clock would also correctly decode a hypothetical POSIX-conforming clock using the same interface. This would be achieved by indicating the TIME_INS state during the entirety of an inserted leap second, then indicating TIME_WAIT during the entirety of the following second while repeating the seconds count. This requires synchronous leap second handling. This is probably the best way to express UTC time in Unix clock form, via a Unix interface, when the underlying clock is fundamentally untroubled by leap seconds.\n\nAnother, much rarer, non-conforming variant of Unix time keeping involves encoding TAI rather than UTC; some Linux systems are configured this way. Because TAI has no leap seconds, and every TAI day is exactly 86400 seconds long, this encoding is actually a pure linear count of seconds elapsed since 1970-01-01T00:00:00TAI. This makes time interval arithmetic much easier. Time values from these systems do not suffer the ambiguity that strictly conforming POSIX systems or NTP-driven systems have.\n\nIn these systems it is necessary to consult a table of leap seconds to correctly convert between UTC and the pseudo-Unix-time representation. This resembles the manner in which time zone tables must be consulted to convert to and from civil time; the IANA time zone database includes leap second information, and the sample code available from the same source uses that information to convert between TAI-based time stamps and local time. Conversion also runs into definitional problems prior to the 1972 commencement of the current form of UTC (see section UTC basis below).\n\nThis TAI-based system, despite its superficial resemblance, is not Unix time. It encodes times with values that differ by several seconds from the POSIX time values, and does not have the simple mathematical relationship to UTC that is mandated by POSIX.\n\nA Unix time number can be represented in any form capable of representing numbers. In some applications the number is simply represented textually as a string of decimal digits, raising only trivial additional problems. However, certain binary representations of Unix times are particularly significant.\n\nThe Unix time_t data type that represents a point in time is, on many platforms, a signed integer, traditionally of 32bits (but see below), directly encoding the Unix time number as described in the preceding section. Being 32 bits means that it covers a range of about 136 years in total. The minimum representable time is Friday 1901-12-13, and the maximum representable time is Tuesday 2038-01-19. One second after 03:14:07UTC 2038-01-19 this representation will overflow. This milestone is anticipated with a mixture of amusement and dread—see year 2038 problem.\n\nIn some newer operating systems, time_t has been widened to 64 bits. This expands the times representable by approximately 293 billion years in both directions, which is over twenty times the present age of the universe per direction.\n\nThere was originally some controversy over whether the Unix time_t should be signed or unsigned. If unsigned, its range in the future would be doubled, postponing the 32-bit overflow (by 68 years). However, it would then be incapable of representing times prior to the epoch. The consensus is for time_t to be signed, and this is the usual practice. The software development platform for version 6 of the QNX operating system has an unsigned 32-bit time_t, though older releases used a signed type.\n\nThe POSIX and Open Group Unix specifications include the C standard library, which includes the time types and functions defined in the codice_1 header file. The ISO C standard states that time_t must be an arithmetic type, but does not mandate any specific type or encoding for it. POSIX requires time_t to be an integer type, but does not mandate that it be signed or unsigned.\n\nUnix has no tradition of directly representing non-integer Unix time numbers as binary fractions. Instead, times with sub-second precision are represented using composite data types that consist of two integers, the first being a time_t (the integral part of the Unix time), and the second being the fractional part of the time number in millionths (in struct timeval) or billionths (in struct timespec). These structures provide a decimal-based fixed-point data format, which is useful for some applications, and trivial to convert for others.\n\nThe present form of UTC, with leap seconds, is defined only from 1 January 1972 onwards. Prior to that, since 1 January 1961 there was an older form of UTC in which not only were there occasional time steps, which were by non-integer numbers of seconds, but also the UTC second was slightly longer than the SI second, and periodically changed to continuously approximate the Earth's rotation. Prior to 1961 there was no UTC, and prior to 1958 there was no widespread atomic timekeeping; in these eras, some approximation of GMT (based directly on the Earth's rotation) was used instead of an atomic timescale.\n\nThe precise definition of Unix time as an encoding of UTC is only uncontroversial when applied to the present form of UTC. The Unix epoch predating the start of this form of UTC does not affect its use in this era: the number of days from 1 January 1970 (the Unix epoch) to 1 January 1972 (the start of UTC) is not in question, and the number of days is all that is significant to Unix time.\n\nThe meaning of Unix time values below (i.e., prior to 1 January 1972) is not precisely defined. The basis of such Unix times is best understood to be an unspecified approximation of UTC. Computers of that era rarely had clocks set sufficiently accurately to provide meaningful sub-second timestamps in any case. Unix time is not a suitable way to represent times prior to 1972 in applications requiring sub-second precision; such applications must, at least, define which form of UT or GMT they use.\n\n, the possibility of ending the use of leap seconds in civil time is being considered. A likely means to execute this change is to define a new time scale, called \"International Time\", that initially matches UTC but thereafter has no leap seconds, thus remaining at a constant offset from TAI. If this happens, it is likely that Unix time will be prospectively defined in terms of this new time scale, instead of UTC. Uncertainty about whether this will occur makes prospective Unix time no less predictable than it already is: if UTC were simply to have no further leap seconds the result would be the same.\n\nIn Unix-like operating systems, codice_2 is the command which will print or set the current time.\n\nThe earliest versions of Unix time had a 32-bit integer incrementing at a rate of 60 Hz, which was the rate of the system clock on the hardware of the early Unix systems. The value 60 Hz still appears in some software interfaces as a result. The epoch also differed from the current value. The first edition Unix Programmer's Manual dated 3 November 1971 defines the Unix time as \"the time since 00:00:00, 1 January 1971, measured in sixtieths of a second\".\n\nThe User Manual also commented that \"the chronologically-minded user will note that 2**32 sixtieths of a second is only about 2.5 years\". Because of this limited range, the epoch was redefined more than once, before the rate was changed to 1 Hz and the epoch was set to its present value of 1 January 1970 00:00:00 UTC. This yielded a range of about 136 years, though with more than half the range in the past (see discussion of signedness above).\n\nAs indicated by the definition quoted above, the Unix time scale was originally intended to be a simple linear representation of time elapsed since an epoch. However, there was no consideration of the details of time scales, and it was implicitly assumed that there was a simple linear time scale already available and agreed upon. The first edition manual's definition does not even specify which time zone is used. Several later problems, including the complexity of the present definition, result from Unix time having been defined gradually by usage rather than fully defined from the outset.\n\nWhen POSIX.1 was written, the question arose of how to precisely define time_t in the face of leap seconds. The POSIX committee considered whether Unix time should remain, as intended, a linear count of seconds since the epoch, at the expense of complexity in conversions with civil time or a representation of civil time, at the expense of inconsistency around leap seconds. Computer clocks of the era were not sufficiently precisely set to form a precedent one way or the other.\n\nThe POSIX committee was swayed by arguments against complexity in the library functions, and firmly defined the Unix time in a simple manner in terms of the elements of UTC time. This definition was so simple that it did not even encompass the entire leap year rule of the Gregorian calendar, and would make 2100 a leap year.\n\nThe 2001 edition of POSIX.1 rectified the faulty leap year rule in the definition of Unix time, but retained the essential definition of Unix time as an encoding of UTC rather than a linear time scale. Since the mid-1990s, computer clocks have been routinely set with sufficient precision for this to matter, and they have most commonly been set using the UTC-based definition of Unix time. This has resulted in considerable complexity in Unix implementations, and in the Network Time Protocol, to execute steps in the Unix time number whenever leap seconds occur.\n\nUnix enthusiasts have a history of holding \"time_t parties\" to celebrate significant values of the Unix time number. These are directly analogous to the new year celebrations that occur at the change of year in many calendars. As the use of Unix time has spread, so has the practice of celebrating its milestones. Usually it is time values that are round numbers in decimal that are celebrated, following the Unix convention of viewing time_t values in decimal. Among some groups round binary numbers are also celebrated, such as +2 which occurred at 13:37:04 UTC on Saturday, 10 January 2004.\n\nThe events that these celebrate are typically described as \"\"N\" seconds since the Unix epoch\", but this is inaccurate. As discussed above, due to the handling of leap seconds in Unix time, the number of seconds elapsed since the Unix epoch is slightly greater than the Unix time number for times later than the epoch.\n\n\nVernor Vinge's novel \"A Deepness in the Sky\" describes a spacefaring trading civilization thousands of years in the future that still uses the Unix epoch. The \"programmer-archaeologist\" responsible for finding and maintaining usable code in mature computer systems first believes that the epoch refers to the time when man first walked on the Moon, but then realizes that it is \"the 0-second of one of Humankind's first computer operating systems\".\n\n\n"}
{"id": "40197", "url": "https://en.wikipedia.org/wiki?curid=40197", "title": "Vapor pressure", "text": "Vapor pressure\n\nVapor pressure (or vapour pressure in British spelling) or equilibrium vapor pressure is defined as the pressure exerted by a vapor in thermodynamic equilibrium with its condensed phases (solid or liquid) at a given temperature in a closed system. The equilibrium vapor pressure is an indication of a liquid's evaporation rate. It relates to the tendency of particles to escape from the liquid (or a solid). A substance with a high vapor pressure at normal temperatures is often referred to as \"volatile\". The pressure exhibited by vapor present above a liquid surface is known as vapor pressure. As the temperature of a liquid increases, the kinetic energy of its molecules also increases. As the kinetic energy of the molecules increases, the number of molecules transitioning into a vapor also increases, thereby increasing the vapor pressure.\n\nThe vapor pressure of any substance increases non-linearly with temperature according to the Clausius–Clapeyron relation. The atmospheric pressure boiling point of a liquid (also known as the normal boiling point) is the temperature at which the vapor pressure equals the ambient atmospheric pressure. With any incremental increase in that temperature, the vapor pressure becomes sufficient to overcome atmospheric pressure and lift the liquid to form vapor bubbles inside the bulk of the substance. Bubble formation deeper in the liquid requires a higher temperature due to the higher fluid pressure, because fluid pressure increases above the atmospheric pressure as the depth increases. More important at shallow depths is the higher temperature required to start bubble formation. The surface tension of the bubble wall leads to an overpressure in the very small, initial bubbles. Thus, thermometer calibration should not rely on the temperature in boiling water.\n\nThe vapor pressure that a single component in a mixture contributes to the total pressure in the system is called partial pressure. For example, air at sea level, and saturated with water vapor at 20 °C, has partial pressures of about 2.3 kPa of water, 78 kPa of nitrogen, 21 kPa of oxygen and 0.9 kPa of argon, totaling 102.2 kPa, making the basis for standard atmospheric pressure.\n\nVapor pressure is measured in the standard units of pressure. The International System of Units (SI) recognizes pressure as a derived unit with the dimension of force per area and designates the pascal (Pa) as its standard unit. One pascal is one newton per square meter (N·m or kg·m·s).\n\nExperimental measurement of vapor pressure is a simple procedure for common pressures between 1 and 200 kPa. Most accurate results are obtained near the boiling point of substances and large errors result for measurements smaller than . Procedures often consist of purifying the test substance, isolating it in a container, evacuating any foreign gas, then measuring the equilibrium pressure of the gaseous phase of the substance in the container at different temperatures. Better accuracy is achieved when care is taken to ensure that the entire substance and its vapor are at the prescribed temperature. This is often done, as with the use of an isoteniscope, by submerging the containment area in a liquid bath.\n\nVery low vapor pressures of solids can be measured using the Knudsen effusion cell method.\n\nIn a medical context, vapor pressure is sometimes expressed in other units, specifically millimeters of mercury (mmHg). This is important for volatile anesthetics, most of which are liquids at body temperature, but with a relatively high vapor pressure. Anesthetics with a higher vapor pressure at body temperature will be excreted more quickly, as they are exhaled from the lungs.\n\nThe Antoine equation is a mathematical expression of the relation between the vapor pressure and the temperature of pure liquid or solid substances. The basic form of the equation is:\n\nand it can be transformed into this temperature-explicit form:\n\nwhere: formula_3 is the absolute vapor pressure of a substance<br>\n\nA simpler form of the equation with only two coefficients is sometimes used:\n\nwhich can be transformed to:\n\nSublimations and vaporizations of the same substance have separate sets of Antoine coefficients, as do components in mixtures. Each parameter set for a specific compound is only applicable over a specified temperature range. Generally, temperature ranges are chosen to maintain the equation's accuracy of a few up to 8–10 percent. For many volatile substances, several different sets of parameters are available and used for different temperature ranges. The Antoine equation has poor accuracy with any single parameter set when used from a compound's melting point to its critical temperature. Accuracy is also usually poor when vapor pressure is under 10 Torr because of the limitations of the apparatus used to establish the Antoine parameter values.\n\nThe Wagner equation gives \"one of the best\" fits to experimental data but is quite complex. It expresses reduced vapor pressure as a function of reduced temperature.\n\nAs a general trend, vapor pressures of liquids at ambient temperatures increase with decreasing boiling points. This is illustrated in the vapor pressure chart (see right) that shows graphs of the vapor pressures versus temperatures for a variety of liquids. At the normal boiling point of a liquid, the vapor pressure is equal to the standard atmospheric pressure defined as 1 atmosphere (760 Torr or 101.325 kPa or 14.69595 PSI).\n\nFor example, at any given temperature, methyl chloride has the highest vapor pressure of any of the liquids in the chart. It also has the lowest normal boiling point (−24.2 °C), which is where the vapor pressure curve of methyl chloride (the blue line) intersects the horizontal pressure line of one atmosphere (atm) of absolute vapor pressure.\n\nAlthough the relation between vapor pressure and temperature is non-linear, the chart uses a logarithmic vertical axis to produce slightly curved lines, so one chart can graph many liquids. A nearly straight line is obtained when the logarithm of the vapor pressure is plotted against 1/(T+230) where T is the temperature in degrees Celsius. The vapor pressure of a liquid at its boiling point equals the pressure of its surrounding environment.\n\nRaoult's law gives an approximation to the vapor pressure of mixtures of liquids. It states that the activity (pressure or fugacity) of a single-phase mixture is equal to the mole-fraction-weighted sum of the components' vapor pressures:\n\nwhere formula_14 is the mixture's vapor pressure, formula_15 is the mole fraction of component formula_16 in the liquid phase and formula_17 is the mole fraction of component formula_16 in the vapor phase respectively. formula_19 is the vapor pressure of component formula_16. Raoult's law is applicable only to non-electrolytes (uncharged species); it is most appropriate for non-polar molecules with only weak intermolecular attractions (such as London forces).\n\nSystems that have vapor pressures higher than indicated by the above formula are said to have positive deviations. Such a deviation suggests weaker intermolecular attraction than in the pure components, so that the molecules can be thought of as being \"held in\" the liquid phase less strongly than in the pure liquid. An example is the azeotrope of approximately 95% ethanol and water. Because the azeotrope's vapor pressure is higher than predicted by Raoult's law, it boils at a temperature below that of either pure component.\n\nThere are also systems with negative deviations that have vapor pressures that are lower than expected. Such a deviation is evidence for stronger intermolecular attraction between the constituents of the mixture than exists in the pure components. Thus, the molecules are \"held in\" the liquid more strongly when a second molecule is present. An example is a mixture of trichloromethane (chloroform) and 2-propanone (acetone), which boils above the boiling point of either pure component.\n\nThe negative and positive deviations can be used to determine thermodynamic activity coefficients of the components of mixtures.\n\nEquilibrium vapor pressure can be defined as the pressure reached when a condensed phase is in equilibrium with its own vapor. In the case of an equilibrium solid, such as a crystal, this can be defined as the pressure when the rate of sublimation of a solid matches the rate of deposition of its vapor phase. For most solids this pressure is very low, but some notable exceptions are naphthalene, dry ice (the vapor pressure of dry ice is 5.73 MPa (831 psi, 56.5 atm) at 20 °C, which causes most sealed containers to rupture), and ice. All solid materials have a vapor pressure. However, due to their often extremely low values, measurement can be rather difficult. Typical techniques include the use of thermogravimetry and gas transpiration.\n\nThere are a number of methods for calculating the sublimation pressure (i.e., the vapor pressure) of a solid. One method is to estimate the sublimation pressure from extrapolated liquid vapor pressures (of the supercooled liquid), if the heat of fusion is known, by using this particular form of the Clausius–Clapeyron relation:\n\nwhere:\n\n\nThis method assumes that the heat of fusion is temperature-independent, ignores additional transition temperatures between different solid phases, and it gives a fair estimation for temperatures not too far from the melting point. It also shows that the sublimation pressure is lower than the extrapolated liquid vapor pressure (Δ\"H\" > 0) and the difference grows with increased distance from the melting point.\n\nLike all liquids, water boils when its vapor pressure reaches its surrounding pressure. In nature, the atmospheric pressure is lower at higher elevations and water boils at a lower temperature. The boiling temperature of water for atmospheric pressures can be approximated by the Antoine equation:\n\nor transformed into this temperature-explicit form:\n\nwhere the temperature formula_32 is the boiling point in degrees Celsius and the pressure formula_33 is in Torr.\n\nDühring's rule states that a linear relationship exists between the temperatures at which two solutions exert the same vapor pressure.\n\nThe following table is a list of a variety of substances ordered by increasing vapor pressure (in absolute units).\nSeveral empirical methods exist to estimate liquid vapor pressure from molecular structure for organic molecules. Some examples are SIMPOL.1 method, the method of Moller et al., and EVAPORATION (Estimation of VApour Pressure of ORganics, Accounting for Temperature, Intramolecular, and Non-additivity effects).\n\nIn meteorology, the term \"vapor pressure\" is used to mean the partial pressure of water vapor in the atmosphere, even if it is not in equilibrium, and the \"equilibrium vapor pressure\" is specified otherwise. Meteorologists also use the term \"saturation vapor pressure\" to refer to the equilibrium vapor pressure of water or brine above a flat surface, to distinguish it from equilibrium vapor pressure, which takes into account the shape and size of water droplets and particulates in the atmosphere.\n\n\n"}
{"id": "839196", "url": "https://en.wikipedia.org/wiki?curid=839196", "title": "Via media", "text": "Via media\n\nVia media is a Latin phrase meaning \"the middle road\" and is a philosophical maxim for life which advocates moderation in all thoughts and actions.\n\nOriginating from early Ancient Greek philosophy, where Aristotle (384–322 BCE) taught moderation, urging his students to follow the middle road between extremes, the \"via media\" was the dominant philosophical precept by which Ancient Roman civilisation and society was organised.\n\nThe term \"via media\" is frequently applied to the Anglican churches, though not without debate, as a term of apologetics. The idea of a middle way, between the Roman Catholics and the Magisterial Reformers, goes back to early in the Protestant Reformation, when theologians such as Martin Bucer, Thomas Cranmer and Heinrich Bullinger advocated a religious solution in which secular authority would hold the ring in the religious dispute, and ensure political stability. \n\nA recent scholarly study points out that, while Richard Hooker's \"Law of Ecclesiastical Polity\" has a reputation as \"the classic depiction of the English \"via media\" based upon the sound triumvirate of scripture, reason and tradition\", the actual term \"via media\" nowhere appears in the work (written in English).\n\nThree centuries later, the phrase was used by John Henry Newman in setting out his influential views on Anglicanism, as part of the argument he brought forward with the Tractarian movement.\n\"Via Media\" was the title of a series of the \"Tracts for Today\", published by Newman around 1834. These tracts in particular used the title to pay homage to the inception of the Thirty-Nine Articles and in so doing claim that the Tractarian movement was of the same vein as early Church of England scholars and theologians. They examined the Elizabethan Settlement and reinterpreted it as a compromise between Rome and Reform.\n\nThe Tractarians promoted the idea of Anglicanism as a middle way between the extremes of Protestantism and Catholicism, which became later an idea of Anglicanism as a middle way between Rome and Protestantism itself.\n\nIn justification of its idea of a \"via media\", the Oxford Movement attributed this position to the works of the Elizabethan theologian Richard Hooker and in particular his book \"Lawes of Ecclesiastical Polity\", which is accepted as a founding work on Anglican theology, a view of Hooker promoted by John Keble, who was one of the first to argue that English theology underwent such a \"decisive change\" in Hooker’s hands. However, Hooker does not use the phrase \"via media\" or \"middle way\" or the word \"Anglican\" in any of his works; the attribution of \"via media\" to him is much later. Hooker's work concerned the form of Protestant church government as an argument against the extreme advocates of Puritanism, arguing that elements of Church of England practice condemned by the Puritans, in particular the \"Book of Common Prayer\" and the institution of bishops, are proper and accord with Scripture. Later theologians analysed Hooker's approach to the particular doctrine of justification by faith as a middle way between the predestinationism of the extreme Calvinists and Lutheran and Arminian doctrines.\n\nThe Oxford Movement recast this \"via media\" as a middle way not within Protestantism but between Protestantism and Roman Catholicism. Its application to early Anglicanism has remained current in Anglican discourse.\n\n"}
{"id": "32172794", "url": "https://en.wikipedia.org/wiki?curid=32172794", "title": "Visual space", "text": "Visual space\n\nVisual space is the perceptual space housing the visual world being experienced by an aware observer; it is the subjective counterpart of the space of physical objects before an observer's eyes.\n\nIn object space the location and shape of physical targets can be accurately described with the tools of geometry. For practical purposes it is Euclidean. It is three-dimensional and various co-ordinate systems like the Cartesian x,y,z (with a defined origin in relation to an observer's head or eyes), or bipolar with angles of elevation, azimuth and binocular parallax (based on the separation of the two eyes) are interchangeable. No elaborate mathematics are needed.\n\nPercepts, the counterparts in the aware observer's conscious experience of objects in physical space, constitute an ordered ensemble or, as Ernst Cassirer explained, the perceptual world has a structure and is not an aggregate of scattered sensations. This visual space can be accessed by introspection, by interrogation, or by suitable experimental procedures which allow relative location as well as some structural properties to be assessed, even quantitatively.\n\nAn example illustrates the relationship between the concepts of object and visual space:\nTwo straight lines are presented to an observer who is asked to set them so that they appear parallel. When this has been done, the lines \"are\" parallel in visual space and now a comparison is feasible with the physical lines' setting in object space. Good precision can be achieved using psychophysical procedures in human observers or behavioral ones in trained animals. The reciprocal experiment is easier to perform but does not yield a numerical read-out as readily: show objectively parallel lines and make a determination of their inclination in the observer's perception.\n\nConsidering how it arises (see below), visual space seems, as an immediate, unmediated experience, to provide a remarkably true and unproblematic representation of a real world of objects.\n\nThe distinction is mandatory between what the eye professions call \"visual field\", the area or extent of physical space that is available to the eye or that is being imaged on the retina, and the virtual, perceptual \"visual space\" in which visual percepts are located, the subject of this entry.\nConfusion is caused by the use of \"\" in the German literature for both. There is no doubt that Ewald Hering and his followers meant visual space in their disquisitions.\n\nThe fundamental distinction was made by Rudolf Carnap between three kinds of space which he called \"formal\", \"physical\" and \"perceptual.\" Mathematicians, for example, deal with ordered structures, ensembles of elements for which rules of logico-deductive relationships hold, limited solely by being not self-contradictory. These are the \"formal\" spaces. According to Carnap, studying \"physical\" space means examining the relationship between empirically determined objects. Finally, there is the realm of what students of Kant know as \",\" immediate sensory experiences, often awkwardly translated as \"apperceptions,\" which belong to \"perceptual spaces.\"\n\nGeometry is the discipline devoted to the study of space and the rules relating the elements to each other. For example, in Euclidean space there is the Pythagorean theorem for distances. In a two-dimensional space of constant curvature, like the surface of a sphere, the rule is somewhat more complex but applies everywhere. On the two-dimensional surface of a football, the rule is more complex still and has different values depending on location. In well-behaved spaces such rules used for measurement and called \"Metrics,\" are classically handled by the mathematics invented by Riemann. Object space belongs to that class.\n\nTo the extent that it is reachable by scientifically acceptable probes, visual space as defined is also a candidate for such considerations. The first and remarkably prescient analysis was published by Ernst Mach in 1901. Under the heading \"On Physiological as Distinguished from Geometrical Space\" Mach states that \"Both spaces are threefold manifoldnesses\" but the former is \"...neither constituted everywhere and in all directions alike, nor infinite in extent, nor unbounded.\" A notable attempt at a rigorous formulation was made in 1947 by the highly talented mathematician Rudolf Luneburg, who preceded his essay by a profound analysis of the underlying principles. When features are sufficiently singular and distinct, there is no problem about a correspondence between an individual item \"A\" in object space and its correlate \"A' \" in visual space. Questions can be asked and answered such as \"If visual percepts \"A',B',C' \" are correlates of physical objects \"A,B,C,\" and if \"C\" lies between \"A\" and \"B\", does \"C' \" lie between \"A' \" and \"B' \"?\" In this manner, the possibility of visual space being metrical can be approached. If the exercise is successful, a great deal can be said about the nature of the mapping of the physical space on the visual space.\n\nOn the basis of fragmentary psychophysical data of previous generations, Luneburg concluded that visual space was hyperbolic with constant curvature, meaning that elements can be moved throughout the space without changing shape. One of Luneburg's major arguments is that, in accord with a common observation, the transformation involving hyperbolic space renders infinity into a dome (the sky). The Luneburg proposition gave rise to discussions and attempts at corroborating experiments, which on the whole did not favor it.\n\nBasic to the problem, and underestimated by Luneburg the mathematician, is the likely success of a mathematically viable formulation of the relationship between objects in physical space and percepts in visual space. Any scientific investigation of visual space is colored by the kind of access we have to it, and the precision, repeatability and generality of measurements. Insightful questions can be asked about the mapping of visual space to object space but answers are mostly limited in the range of their validity. If the physical setting that satisfies the criterion of, say, apparent parallelism varies from observer to observer, or from day to day, or from context to context, so does the geometrical nature of, and hence mathematical formulation for, visual space.\n\nAll these arguments notwithstanding, there is a major concordance between the locations of items in object space and their correlates in visual space. It is adequately veridical for us to navigate very effectively in the world, deviations from such a situation are sufficiently notable to warrant special consideration. visual space agnosia is a recognized neurological condition, and the many common distortions, called geometrical-optical illusions, are widely demonstrated but of minor consequence.\n\nIts founder, Gustav Theodor Fechner defined the mission of the discipline of psychophysics as the functional relationship between the mental and material worlds—in this particular case, the visual and object spaces—but he acknowledged an intermediate step, which has since blossomed into the major enterprise of modern neuroscience. In distinguishing between \"inner\" and \"outer\" psychophysics, Fechner recognized that a physical stimulus generates a percept by way of an effect on the organism's sensory and nervous systems. Hence, without denying that its essence is the arc between object and percept, the inquiry can concern itself with the neural substrate of visual space.\n\nTwo major concepts dating back to the middle of the 19th century set the parameters of the discussion here. Johannes Müller emphasized that what matters in a neural path is the connection it makes, and Hermann Lotze, from psychological considerations, enunciated the principle of local sign. Put together in modern neuroanatomical terms they mean that a nerve fiber from a fixed retinal location instructs its target neurons in the brain about the presence of a stimulus in the location in the eye's visual field that is imaged there. The orderly array of retinal locations is preserved in the passage from the retina to the brain, and provides what is aptly called a \"retinotopic\" mapping in the primary visual cortex. Thus in the first instance brain activity retains the relative spatial ordering of the objects and lays the foundations for a neural substrate of visual space. Unfortunately, as is so common in brain studies, simplicity and transparency ends here. Right at the outset, visual signals are analyzed not only for their position, but also, separately in parallel channels, for many other attributes such as brightness, color, orientation, depth. No single neuron or even neuronal center or circuit represents both the nature of a target feature and its accurate location. The unitary mapping of object space into the coherent visual space without internal contradictions or inconsistencies that we as observer automatically experience, demands concepts of conjoint activity in several parts of the nervous system that is at present beyond the reach of neurophysiological research.\n\nThough the details of the process by which the experience of visual space emerges remain opaque, a startling finding gives hope for future insights. Neural units have been demonstrated in the brain structure called hippocampus that show activity only when the animal is in a specific place in its environment.\n\nOnly on an astronomical scale are physical space and its contents interdependent, This major proposition of the general theory of relativity is of no concern in vision. For us, distances in object space are independent of the nature of the objects.\n\nBut this is not so simple in visual space. At a minim an observer judges the relative location of a few light points in an otherwise dark visual field, a simplistic extension from object space that enabled Luneburg to make some statements about the geometry of visual space. In a more richly textured visual world, the various visual percepts carry with them prior perceptual associations which often affect their relative spatial disposition. Identical separations in physical space can look quite different (\"are quite different\" in visual space) depending on the features that demarcate them. This is particularly so in the depth dimension because the apparatus by which values in the third visual dimension are assigned is fundamentally different from that for the height and width of objects.\n\nEven in monocular vision, which physiologically has only two dimensions, cues of size, perspective, relative motion etc. are used to assign depth differences to percepts. Looked at as a mathematical/geometrical problem, expanding a 2-dimensional object manifold into a 3-dimensional visual world is \"ill-posed,\" i.e., not capable of a rational solution, but is accomplished quite effectively by the human observer.\n\nThe problem becomes less ill-posed when binocular vision allows actual determination of relative depth by stereoscopy, but its linkage to the evaluation of distance in the other two dimensions is uncertain (see: stereoscopic depth rendition). Hence, the uncomplicated three-dimensional visual space of every-day experience is the product of many perceptual and cognitive layers superimposed on the physiological representation of the physical world of objects.\n"}
