{"id": "48624755", "url": "https://en.wikipedia.org/wiki?curid=48624755", "title": "Acceptability", "text": "Acceptability\n\nAcceptability is the characteristic of a thing being subject to acceptance for some purpose. A thing is acceptable if it is sufficient to serve the purpose for which it is provided, even if it is far less usable for this purpose than the ideal example. A thing is unacceptable (or has the characteristic of unacceptability) if it deviates so far from the ideal that it is no longer sufficient to serve the desired purpose, or if it goes against that purpose. From a logical perspective, a thing can be said to be acceptable if it has no characteristics that make it unacceptable:\n\nHungarian mathematician Imre Lakatos developed a concept of acceptability \"taken as \"a measure of the approximation to the truth\"\". This concept was criticized in its applicability to philosophy as requiring that better theories first be eliminated. Acceptability is also a key premise of negotiation, wherein opposing sides each begin from a point of seeking their ideal solution, and compromise until they reach a solution that both sides find acceptable:\n\nWhere an unacceptable proposal has been made, \"a counterproposal is generated if there are any acceptable ones that have had already been explored\". Since the acceptability of proposition to a participant in a negotiation is only known to that participant, the participant may act as though a proposal that is actually acceptable to them is not, in order to obtain a more favorable proposal. \n\nOne concept of acceptability that has been widely studied is acceptable risk in situations affecting human health. The idea of not increasing lifetime risk by more than one in a million has become commonplace in public health discourse and policy. It is a heuristic measure. It provides a numerical basis for establishing a negligible increase in risk.\n\nEnvironmental decision making allows some discretion for deeming individual risks potentially \"acceptable\" if less than one in ten thousand chance of increased lifetime risk. Low risk criteria such as these provide some protection for a case where individuals may be exposed to multiple chemicals e.g. pollutants, food additives or other chemicals. In practice, a true zero-risk is possible only with the suppression of the risk-causing activity.\n\nStringent requirements of 1 in a million may not be technologically feasible or may be so prohibitively expensive as to render the risk-causing activity unsustainable, resulting in the optimal degree of intervention being a balance between risks vs. benefit. For example, emissions from hospital incinerators result in a certain number of deaths per year. However, this risk must be balanced against the alternatives. There are public health risks, as well as economic costs, associated with all options. The risk associated with no incineration is potential spread of infectious diseases, or even no hospitals. Further investigation identifies options such as separating noninfectious from infectious wastes, or air pollution controls on a medical incinerator.\n\nAcceptable variance is the range of variance in any direction from the ideal value that remains acceptable. In project management, variance can be defined as \"the difference between what is planned and what is actually achieved\". Degrees of variance \"can be classified into negative variance, zero variance, acceptable variance, and unacceptable variance\". In software testing, for example, \"[g]enerally 0-5% is considered as acceptable variance\" from an ideal value.\n\nAcceptance testing is a practice used in chemical and engineering fields, intended to check ahead of time whether or not a thing will be acceptable.\n"}
{"id": "47736005", "url": "https://en.wikipedia.org/wiki?curid=47736005", "title": "Antinarcissism", "text": "Antinarcissism\n\nAntinarcissism is a specific form of narcissistic character that, rather than aggrandising the ego, restricts its scope without diminishing the amount of self-investment involved.\n\nChristopher Bollas introduced the concept of antinarcissism to describe a self-limiting kind of narcissist who refuse to develop themselves or use their talents, so as to maintain their exaggerated sense of self-importance \"in defeat\". \"This anti-elaborative person 'stews in his own juice' and adamantly refuses to nurture himself\". The antinarcissist may preserve a hostile, even sadistic, core behind a self-effacing facade of care and consideration for others.\n\nAndré Green similarly wrote of antinarcissism as a negative narcissism that seeks self-destructively to abolish the ego.\n\n\n"}
{"id": "13265633", "url": "https://en.wikipedia.org/wiki?curid=13265633", "title": "Beijing World Park", "text": "Beijing World Park\n\nBeijing World Park () is a theme park that attempts to give visitors the chance to see the world without having to leave Beijing. The park covers 46.7 hectares and is located in the southwestern Fengtai District of Beijing. It is about 17 km from Tiananmen, the City center, and 40 km from the Capital International Airport. The park opened in 1993 and is estimated to receive 1.5 million visitors annually.\n\nThe entrance to the park is made up of a Gothic castle, Roman corridor, and granite relief sculptures. Immediately inside the gate is an Italian-style terrace garden with grand staircases, fountains, and sculptures inspired by originals from the European Renaissance. More lawns and gardens are scattered throughout the park. On these lawns are miniature models of around 100 of the world’s most famous statues including the American Statue of Liberty, Copenhagen’s Little Mermaid, Michelangelo’s David, and the Venus de Milo. Once inside the gates, Beijing World Park consists of two main parts: the scenic portion and the shopping, dining, and entertainment area.\n\nThe scenic area of the park models itself after the naturalistic layout of the globe, representing the four major oceans, and focusing on five continents: Asia, Africa, Europe, North America, and South America. The park contains about a hundred (109) scaled-down replicas of famous landmarks from nearly 40 countries and regions around the world, including the Tower Bridge in London, the Eiffel Tower in Paris, and the Great Pyramids in Egypt. There is even a miniature Manhattan, complete with twin towers of the World Trade Center. Each landmark represents its country or region of origin and is situated in the park according to its location on the map.\n\nClose attention to detail was paid in modeling these landmarks after their originals. For example, detailed carvings and ornamentations are included. Even the materials used are modeled after their originals to create the most authentic look possible. For example, the replica of the Great Pyramids was constructed of 200,000 white marble bricks, each as large as a bar of soap. Red Square in Russia, is replicated by paving the smaller model in World Park with over 5 million red bricks smaller than mahjong tiles.\n\n\nMany of the locale-specific areas focused on the five continents also include live entertainment or some kind of an immersive cultural experience such as a dance performance or parade based on customs from their culture of origin. There is a special garden area in the scenic portion of the park that consists of China's Qingyingjing Park, Japan's Katsura Imperial Villa, and an American-style garden villa. This allows visitors to see global styles of gardening. To get around the park, visitors can travel the four “oceans” by speedboat tour. They can also take a monorail that circumvents the five continents to give tourists a glimpse of the whole “world”. Or they can take a tour of the park in battery-operated cars.\n\nThe other portion of the park includes the shopping and dining area. This area is modeled after Euro-American architecture. The various establishments allow visitors to purchase souvenirs and sample cuisine from the countries and regions represented in the scenic portion of the park. This second part of the park also includes the entertainment area where grand spectacles and cinema features take place in large theatres to showcase global cultural costume, movement, and customs. Some of these cultural activities include the daily opening ceremonies, which consist of a parade of large-scale floats. There are also culture-specific folklore dances that take place indoors or in open-air arenas. These dances may be performed by Beijing World Park dancers, or they may be performed by troupes representing their local cultures. In 2005 the Park even hosted a Thai elephant that performed four times a day from May to October.\n\nDuring the 2008 Summer Olympics, it was selected as one of the three protest zones.\n\n\"The World\" is a 2004 film directed by Jia Zhangke, which was made at Beijing World Park and focused on the lives of some fictional park employees.\n\nThe Beijing Dabaotai Western Han Dynasty Mausoleum is located southeast from the south entrance of the World Park. The mausoleum hosts the 2,000-year-old tombs of a Western Han dynasty prince and his wife.\n\n\n"}
{"id": "25884080", "url": "https://en.wikipedia.org/wiki?curid=25884080", "title": "Bhava samadhi", "text": "Bhava samadhi\n\nBhava Samadhi is a state of ecstatic consciousness that can sometimes be a seemingly spontaneous experience, but is recognized generally to be the culmination of long periods of devotional practices. It is believed by some groups to be evoked through the presence of \"higher beings.\" \"Bhava\" means \"feeling\", \"emotion\", \"mood\", \"mental attitude\" or \"devotional state of mind.\" \"Samadhi\" is a state of consciousness in which the mind becomes completely still (one-pointed or concentrated) and the consciousness of the experiencing subject becomes one with the experienced object. Thus, \"bhava samadhi\" denotes an advanced spiritual state in which the emotions of the mind are channelled into one-pointed concentration and the practitioner experiences devotional ecstasy. \"Bhava samadhi\" has been experienced by notable figures in Indian spiritual history, including Sri Ramakrishna Paramahamsa and some of his disciples, Chaitanya Mahaprabhu and his chief disciple Nityananda, Mirabai and numerous saints in the \"bhakti\" tradition.\n\n\"Bhava samadhi\", sometimes translated as 'trance', has no direct counterpart in the English language, though \"ecstasy\" is the closest translation. The various translations that have been proposed all refer to an ecstatic state of consciousness, which is attained by channelling the emotions into one-pointed concentration. For example, in \"The Gospel of Sri Ramakrishna\", the author, M., later identified as Mahendranath Gupta, recounts observing Ramakrishna Paramahamsa's introverted mood in which he became \"unconscious of the outer world.\" M. later \"learnt that this mood is called \"bhava\", ecstasy.\"\n\n\"Bhava\" denotes the mood of ecstasy and self-surrender which is induced by the maturing of devotion to one's \"'Ishta deva' \" (object of devotion). \"Bhava\" literally means feeling, emotion, mood, or devotional state of mind. This refers to the aspirant's emotional life, which in the practice of \"jnana\" or \"raja\" yoga is controlled in order to transcend the spheres of the mind and intellect. In \"bhakti\" yoga, however, \"bhava\" is neither controlled nor suppressed, but is transformed into devotion and channelled to the Lord.\" Swami Sivananda states it is an \"internal feeling\" that needs to be developed through proper practice just like any other faculty of the mind e.g. memory or will power.\n\nAccording to Ramakrishna Paramahamsa real bhava can only be said to occur when the relationship with the Divine is so established that it remains fixed in our consciousness at all times, \"whether eating, drinking, sitting or sleeping.\" \n\nOnly when the \"bhava\" has fully ripened does the \"sadhaka\" (spiritual seeker) experience \"\"bhava samadhi\".\" \"Bhava samadhi\" occurs when the emotions are perfectly channelled into one-pointed concentration on the object of one's devotion. It has also been described as \"Absorption in meditation due to emotional cause, e.g. \"kirtan\" [devotional music]\" and \"sheer ecstasy, a condition caused when the heart is seized by the Divine embrace.\"\n\nDevotional practices that can evoke \"bhava\", such as \"bhajans\" and \"kirtan\" (spiritual music), are standard practices in the \"bhakti\" tradition, and in the missions of many Indian saints including Ramakrishna Paramahamsa and Shivabalayogi Maharaj. Shri Shivabalayogi often used the words \"bhava\" and \"bhava samadhi\" interchangeably. He explained \"bhava\" as follows:\n\"Everyone is in some sort of \"bhava\" of the guru because of their attachment to the guru. The mind's attachment and devotion is the true \"bhava\".\"\n“Bhava is the beginning for samadhi and tapas. Higher souls induce it. Bhava helps in physical, mental, and spiritual progress.\"\nThe qualities required for a genuine bhava samadhi have been emphasized by Ramakrishna Paramahamsa when he said that a spiritual experience of a lower plane may be had by \"the momentary exuberance of emotions\" but the scriptures say bhava samadhi is impossible to retain unless worldly desires have been removed and proper qualities have been established like renunciation and detachment.\n\nThere have been many misuses and controversies associated with \"bhava samadhi\". Firstly, \"bhava\" itself has been mistaken to be an advanced spiritual state, whereas the great exponent of \"bhava samadhi\", Ramakrishna Paramahamsa, made it clear to his disciple, Swami Vivekananda, that \"bhava\" is a preliminary state of consciousness:\n\"Witnessing the religious ecstasy (bhava) of several devotees, Narendra (Swami Vivekananda) one day said to the Master that he too wanted to experience it. 'My child,' he was told, 'when a huge elephant enters a small pond, a great commotion is set up, but when it plunges into the Ganga, the river shows very little agitation. These devotees are like small ponds; a little experience makes their feelings flow over the brim. But you are a huge river.' \"\n\nSeveral other times Ramakrishna Paramahamsa made the same point like when he told one of his close devotees Gopalchandra Ghosh (later known as Swami Advaitananda, his most senior monastic disciple) that it was not so important to experience such a temporary ecstasy (bhava) and that on the spiritual path \"true faith and renunciation are far greater.\"\n\nThat \"bhava\" is a preliminary experience has also been emphasized by Shivabalayogi Maharaj:\n\"During this all your \"bhava\" (the mind’s feelings) will get concentrated on your favorite deity and thus your mind becomes more concentrated, more single-pointed. Then meditation itself becomes much easier and consequently one would take up meditation more willingly. \"It's like giving chocolate to a child to make it go to school. But one should not settle just for the chocolate - one must go on to school. In the same way, one must meditate.\"\nSecondly, people have falsely claimed to have spontaneously attained spiritual powers and experiences through \"bhava\", whereas \"bhava samadhi\" is the culmination of a long period of devotional practice. \"Bhava\" has even been used by people to falsely claim that they are \"possessed by sacred deities\" and to issue orders on behalf of these deities. If the \"bhava\" is genuine, however, the person will become non-violent and introverted, and will not claim or give instructions through \"bhava\". Spiritual efforts should always enable the mind to recede and become quiet, going introverted toward the Self. Swami Vivekananda warned \"sadhaks\" (spiritual aspirants) to beware of claims made of \"bhava\" experiences:\nHe pointed out that Ramakrishna had been through long years of strictest self discipline and that his ecstasy was a fruit of that discipline, not a superficial emotionalism. \"When people try to practice religion,\" said Naren \"eighty percent of them turn into cheats, and about fifteen percent go mad. Its only the remaining five percent who get some direct knowledge of the Truth and so become blessed. So Beware.\"\n\nThirdly, genuine \"bhava samadhi\", which is an internal state of consciousness, has been identified with outer movements of the body, such as dancing and singing. It has been claimed that \"the very nature of \"bhava\" itself - sometimes having such vigorous outward expression in action and movement - had always meant that those who wished attention or status in a group would sometimes simply pretend to be in \"bhava\" to obtain some personal gain.\" However, it has been made clear by Ramakrishna that emotional displays do not constitute spiritual experience:\nOne evening Subodh (later to become Swami Subodhananda) observed the devotees dancing and singing Kirtan in the Masters room at Dakshineshwar. They were overwhelmed with devotion. Shri Ramakrishna himself joined them and his ecstasy surcharged the whole place with heavenly bliss. Some were crying, some laughing, some dancing. Others were transfixed like motionless statues, and some began to roll on the floor. Subodh was very skeptical about this kind of emotional display ... [He asked] \"who had real ecstasy in the kirtan today?\" The Master thought for a while and then said, \"Today Latu (later to become Swami Adbhutananda) alone had the fullest measure of it; some others had sprinklings.\"\nThe depth of \"bhava\" experience varies across different individuals and depends on the spiritual maturity of their minds. Mature \"sadhaks\" usually do not display outward signs of \"bhava\", which are indicative of the depth of their experiences. The problem of devotees attempting to make claims about their inner state of consciousness by imitating external indicators of genuine \"bhava samadhi\" was addressed by Swami Vivekananda in the Ramakrishna Mission:\nIt was discovered that several were actually trying to induce the outer physical symptoms of \"Samadhi\" and also imitate the movements of one who is dancing in ecstasy. Naren reasoned with these devotees and persuaded them to stop starving themselves and eat wholesome food, and to try control their emotions instead of cultivating hysteria. The result was an increase in spirituality and a decrease in outer show.\n\nThe actions of people in bhava samadhi, like dancing in ecstasy, can appear very strange to some. In Shri Shivabalayogi Maharaj's mission various levels of bhava occurred to hundreds of people. Bhava was controversial throughout Shivabalayogi's public programs, and his own statements on the phenomenon appear inconsistent. Although some were acting or misusing the experience, when people complained to Sri Shivabalayogi, he was intolerant of most criticism or interference. \"It is not drama. It really happens.\"\n\nTo place bhava samadhi into the correct spiritual context Ramakrishna Paramahansa said,\n\"If the depth of spiritual experiences is to be measured, it must be done from observing one's steadfastness, renunciation, strength of character, the attenuation of desires for enjoyment etc. It is by this touchstone alone, and no other means, that the amount of dross in ecstacy can be assessed.\" \n\n"}
{"id": "54838", "url": "https://en.wikipedia.org/wiki?curid=54838", "title": "Biogas", "text": "Biogas\n\nBiogas refers to a mixture of different gases produced by the breakdown of organic matter in the absence of oxygen. Biogas can be produced from raw materials such as agricultural waste, manure, municipal waste, plant material, sewage, green waste or food waste. Biogas is a renewable energy source.\n\nBiogas is produced by anaerobic digestion with methanogen or anaerobic organisms, which digest material inside a closed system, or fermentation of biodegradable materials. This closed system is called an anaerobic digester, biodigester or a bioreactor.\n\nBiogas is primarily methane () and carbon dioxide () and may have small amounts of hydrogen sulphide (), moisture and siloxanes. The gases methane, hydrogen, and carbon monoxide () can be combusted or oxidized with oxygen. This energy release allows biogas to be used as a fuel; it can be used for any heating purpose, such as cooking. It can also be used in a gas engine to convert the energy in the gas into electricity and heat.\n\nBiogas can be compressed, the same way as natural gas is compressed to CNG, and used to power motor vehicles. In the United Kingdom, for example, biogas is estimated to have the potential to replace around 17% of vehicle fuel. It qualifies for renewable energy subsidies in some parts of the world. Biogas can be cleaned and upgraded to natural gas standards, when it becomes bio-methane. Biogas is considered to be a renewable resource because its production-and-use cycle is continuous, and it generates no net carbon dioxide. As the organic material grows, it is converted and used. It then regrows in a continually repeating cycle. From a carbon perspective, as much carbon dioxide is absorbed from the atmosphere in the growth of the primary bio-resource as is released, when the material is ultimately converted to energy.\n\nThe biogas is a renewable energy that can be used for heating, electricity, and many other operations that use a reciprocating internal combustion engine, such as GE Jenbacher or Caterpillar gas engines. To provide these internal combustion engines with biogas having ample gas pressure to optimize combustion, within the European Union ATEX centrifugal fan units built in accordance with the European directive 2014/34/EU (previously 94/9/EG) are obligatory. These centrifugal fan units, for example Combimac, Meidinger AG or Witt & Sohn AG are suitable for use in Zone 1 and 2 .\n\nOther internal combustion engines such as gas turbines are suitable for the conversion of biogas into both electricity and heat. The digestate is the remaining inorganic matter that was not transformed into biogas. It can be used as an agricultural fertiliser.\n\nBiogas is produced either; \n\nProjects such as NANOCLEAN are nowadays developing new ways to produce biogas more efficiently, using iron oxide nanoparticles in the processes of organic waste treatment. This process can triple the production of biogas.\n\nA \"biogas plant\" is the name often given to an anaerobic digester that treats farm wastes or energy crops. It can be produced using anaerobic digesters (air-tight tanks with different configurations). These plants can be fed with energy crops such as maize silage or biodegradable wastes including sewage sludge and food waste. During the process, the micro-organisms transform biomass waste into biogas (mainly methane and carbon dioxide) and digestate.\n\nThere are two key processes: mesophilic and thermophilic digestion which is dependent on temperature. In experimental work at University of Alaska Fairbanks, a 1000-litre digester using psychrophiles harvested from \"mud from a frozen lake in Alaska\" has produced 200–300 liters of methane per day, about 20%–30% of the output from digesters in warmer climates.\n\nThe air pollution produced by biogas is similar to that of natural gas. The content of toxic hydrogen sulfide presents additional risks and has been responsible for serious accidents . Leaks of unburned methane are an additional risk, because methane is a potent greenhouse gas.\n\nBiogas can be explosive when mixed in the ratio of one part biogas to 8–20 parts air. Special safety precautions have to be taken for entering an empty biogas digester for maintenance work. It is important that a biogas system never has negative pressure as this could cause an explosion. Negative gas pressure can occur if too much gas is removed or leaked; Because of this biogas should not be used at pressures below one column inch of water, measured by a pressure gauge.\n\nFrequent smell checks must be performed on a biogas system. If biogas is smelled anywhere windows and doors should be opened immediately. If there is a fire the gas should be shut off at the gate valve of the biogas system.\n\nLandfill gas is produced by wet organic waste decomposing under anaerobic conditions in a biogas.\n\nThe waste is covered and mechanically compressed by the weight of the material that is deposited above. This material prevents oxygen exposure thus allowing anaerobic microbes to thrive. Biogas builds up and is slowly released into the atmosphere if the site has not been engineered to capture the gas. Landfill gas released in an uncontrolled way can be hazardous since it can become explosive when it escapes from the landfill and mixes with oxygen. The lower explosive limit is 5% methane and the upper is 15% methane.\n\nThe methane in biogas is 28 times more potent a greenhouse gas than carbon dioxide. Therefore, uncontained landfill gas, which escapes into the atmosphere may significantly contribute to the effects of global warming. In addition, volatile organic compounds (VOCs) in landfill gas contribute to the formation of photochemical smog.\n\nBiochemical oxygen demand (BOD) is a measure of the amount of oxygen required by aerobic micro-organisms to decompose the organic matter in a sample of material being used in the biodigester as well as the BOD for the liquid discharge allows for the calculation of the daily energy output from a biodigester.\n\nAnother term related to biodigesters is effluent dirtiness, which tells how much organic material there is per unit of biogas source. Typical units for this measure are in mg BOD/litre. As an example, effluent dirtiness can range between 800–1200 mg BOD/litre in Panama.\n\nFrom 1 kg of decommissioned kitchen bio-waste, 0.45 m³ of biogas can be obtained. The price for collecting biological waste from households is approximately €70 per ton.\n\nThe composition of biogas varies depending upon the substrate composition, as well as the conditions within the anaerobic reactor (temperature, pH, and substrate concentration). Landfill gas typically has methane concentrations around 50%. Advanced waste treatment technologies can produce biogas with 55%–75% methane, which for reactors with free liquids can be increased to 80%–90% methane using in-situ gas purification techniques. As produced, biogas contains water vapor. The fractional volume of water vapor is a function of biogas temperature; correction of measured gas volume for water vapour content and thermal expansion is easily done via simple mathematics which yields the standardized volume of dry biogas.\n\nIn some cases, biogas contains siloxanes. They are formed from the anaerobic decomposition of materials commonly found in soaps and detergents. During combustion of biogas containing siloxanes, silicon is released and can combine with free oxygen or other elements in the combustion gas. Deposits are formed containing mostly silica () or silicates () and can contain calcium, sulfur, zinc, phosphorus. Such \"white mineral\" deposits accumulate to a surface thickness of several millimeters and must be removed by chemical or mechanical means.\n\nPractical and cost-effective technologies to remove siloxanes and other biogas contaminants are available.\n\nFor 1000 kg (wet weight) of input to a typical biodigester, total solids may be 30% of the wet weight while volatile suspended solids may be 90% of the total solids. Protein would be 20% of the volatile solids, carbohydrates would be 70% of the volatile solids, and finally fats would be 10% of the volatile solids.\n\nHigh levels of methane are produced when manure is stored under anaerobic conditions. During storage and when manure has been applied to the land, nitrous oxide is also produced as a byproduct of the denitrification process. Nitrous oxide () is 320 times more aggressive as a greenhouse gas than carbon dioxide and methane 25 times more than carbon dioxide.\n\nBy converting cow manure into methane biogas via anaerobic digestion, the millions of cattle in the United States would be able to produce 100 billion kiloWatt hours of electricity, enough to power millions of homes across the United States. In fact, one cow can produce enough manure in one day to generate 3 kiloWatt hours of electricity; only 2.4 kiloWatt hours of electricity are needed to power a single 100-Watt light bulb for one day. Furthermore, by converting cattle manure into methane biogas instead of letting it decompose, global warming gases could be reduced by 99 million metric tons or 4%.\n\nBiogas can be used for electricity production on sewage works, in a CHP gas engine, where the waste heat from the engine is conveniently used for heating the digester; cooking; space heating; water heating; and process heating. If compressed, it can replace compressed natural gas for use in vehicles, where it can fuel an internal combustion engine or fuel cells and is a much more effective displacer of carbon dioxide than the normal use in on-site CHP plants.\n\nRaw biogas produced from digestion is roughly 60% methane and 29% with trace elements of : inadequate for use in machinery. The corrosive nature of alone is enough to destroy the mechanisms.\n\nMethane in biogas can be concentrated via a biogas upgrader to the same standards as fossil natural gas, which itself has to go through a cleaning process, and becomes \"biomethane\". If the local gas network allows, the producer of the biogas may use their distribution networks. Gas must be very clean to reach pipeline quality and must be of the correct composition for the distribution network to accept. Carbon dioxide, water, hydrogen sulfide, and particulates must be removed if present.\n\nThere are four main methods of upgrading: water washing, pressure swing absorption, selexol absorption, and amine gas treating. In addition to these, the use of membrane separation technology for biogas upgrading is increasing, and there are already several plants operating in Europe and USA.\n\nThe most prevalent method is water washing where high pressure gas flows into a column where the carbon dioxide and other trace elements are scrubbed by cascading water running counter-flow to the gas. This arrangement could deliver 98% methane with manufacturers guaranteeing maximum 2% methane loss in the system. It takes roughly between 3% and 6% of the total energy output in gas to run a biogas upgrading system.\n\nGas-grid injection is the injection of biogas into the methane grid (natural gas grid). Until the breakthrough of micro combined heat and power two-thirds of all the energy produced by biogas power plants was lost (as heat). Using the grid to transport the gas to customers, the energy can be used for on-site generation, resulting in a reduction of losses in the transportation of energy. Typical energy losses in natural gas transmission systems range from 1% to 2%; in electricity transmission they range from 5% to 8%.\n\nBefore being injected in the gas grid, biogas passes a cleaning process, during which it is upgraded to natural gas quality. During the cleaning process trace components harmful to the gas grid and the final users are removed.\n\nIf concentrated and compressed, it can be used in vehicle transportation. Compressed biogas is becoming widely used in Sweden, Switzerland, and Germany. A biogas-powered train, named Biogaståget Amanda (The Biogas Train Amanda), has been in service in Sweden since 2005. Biogas powers automobiles. In 1974, a British documentary film titled \"Sweet as a Nut\" detailed the biogas production process from pig manure and showed how it fueled a custom-adapted combustion engine. In 2007, an estimated 12,000 vehicles were being fueled with upgraded biogas worldwide, mostly in Europe.\n\nBiogas is part of the wet gas and condensing gas (or air) category that includes mist or fog in the gas stream. The mist or fog is predominately water vapor that condenses on the sides of pipes or stacks throughout the gas flow. Biogas environments include wastewater digesters, landfills, and animal feeding operations (covered livestock lagoons).\n\nUltrasonic flow meters are one of the few devices capable of measuring in a biogas atmosphere. Most of thermal flow meters are unable to provide reliable data because the moisture causes steady high flow readings and continuous flow spiking, although there are single-point insertion thermal mass flow meters capable of accurately monitoring biogas flows with minimal pressure drop. They can handle moisture variations that occur in the flow stream because of daily and seasonal temperature fluctuations, and account for the moisture in the flow stream to produce a dry gas value.\n\nThe European Union has legislation regarding waste management and landfill sites called the Landfill Directive.\n\nCountries such as the United Kingdom and Germany now have legislation in force that provides farmers with long-term revenue and energy security.\n\nThe United States legislates against landfill gas as it contains VOCs. The United States Clean Air Act and Title 40 of the Code of Federal Regulations (CFR) requires landfill owners to estimate the quantity of non-methane organic compounds (NMOCs) emitted. If the estimated NMOC emissions exceeds 50 tonnes per year, the landfill owner is required to collect the gas and treat it to remove the entrained NMOCs. Treatment of the landfill gas is usually by combustion. Because of the remoteness of landfill sites, it is sometimes not economically feasible to produce electricity from the gas.\n\nWith the many benefits of biogas, it is starting to become a popular source of energy and is starting to be used in the United States more. In 2003, the United States consumed 147 trillion BTU of energy from \"landfill gas\", about 0.6% of the total U.S. natural gas consumption. Methane biogas derived from cow manure is being tested in the U.S. According to a 2008 study, collected by the \"Science and Children\" magazine, methane biogas from cow manure would be sufficient to produce 100 billion kilowatt hours enough to power millions of homes across America. Furthermore, methane biogas has been tested to prove that it can reduce 99 million metric tons of greenhouse gas emissions or about 4% of the greenhouse gases produced by the United States.\n\nIn Vermont, for example, biogas generated on dairy farms was included in the CVPS Cow Power program. The program was originally offered by Central Vermont Public Service Corporation as a voluntary tariff and now with a recent merger with Green Mountain Power is now the GMP Cow Power Program. Customers can elect to pay a premium on their electric bill, and that premium is passed directly to the farms in the program. In Sheldon, Vermont, Green Mountain Dairy has provided renewable energy as part of the Cow Power program. It started when the brothers who own the farm, Bill and Brian Rowell, wanted to address some of the manure management challenges faced by dairy farms, including manure odor, and nutrient availability for the crops they need to grow to feed the animals. They installed an anaerobic digester to process the cow and milking center waste from their 950 cows to produce renewable energy, a bedding to replace sawdust, and a plant-friendly fertilizer. The energy and environmental attributes are sold to the GMP Cow Power program. On average, the system run by the Rowells produces enough electricity to power 300 to 350 other homes. The generator capacity is about 300 kilowatts.\n\nIn Hereford, Texas, cow manure is being used to power an ethanol power plant. By switching to methane biogas, the ethanol power plant has saved 1000 barrels of oil a day. Over all, the power plant has reduced transportation costs and will be opening many more jobs for future power plants that will rely on biogas.\n\nIn Oakley, Kansas, an ethanol plant considered to be one of the largest biogas facilities in North America is using Integrated Manure Utilization System \"IMUS\" to produce heat for its boilers by utilizing feedlot manure, municipal organics and ethanol plant waste. At full capacity the plant is expected to replace 90% of the fossil fuel used in the manufacturing process of ethanol and methanol.\n\nThe level of development varies greatly in Europe. While countries such as Germany, Austria and Sweden are fairly advanced in their use of biogas, there is a vast potential for this renewable energy source in the rest of the continent, especially in Eastern Europe. Different legal frameworks, education schemes and the availability of technology are among the prime reasons behind this untapped potential. Another challenge for the further progression of biogas has been negative public perception.\n\nIn February 2009, the European Biogas Association (EBA) was founded in Brussels as a non-profit organisation to promote the deployment of sustainable biogas production and use in Europe. EBA's strategy defines three priorities: establish biogas as an important part of Europe’s energy mix, promote source separation of household waste to increase the gas potential, and support the production of biomethane as vehicle fuel. In July 2013, it had 60 members from 24 countries across Europe.\n\n, there are about 130 non-sewage biogas plants in the UK. Most are on-farm, and some larger facilities exist off-farm, which are taking food and consumer wastes.\n\nOn 5 October 2010, biogas was injected into the UK gas grid for the first time. Sewage from over 30,000 Oxfordshire homes is sent to Didcot sewage treatment works, where it is treated in an anaerobic digestor to produce biogas, which is then cleaned to provide gas for approximately 200 homes.\n\nIn 2015 the Green-Energy company Ecotricity announced their plans to build three grid-injecting digesters.\n\n, in Italy there are more than 200 biogas plants with a production of about 1.2GW \n\nGermany is Europe's biggest biogas producer and the market leader in biogas technology. In 2010 there were 5,905 biogas plants operating throughout the country: Lower Saxony, Bavaria, and the eastern federal states are the main regions. Most of these plants are employed as power plants. Usually the biogas plants are directly connected with a CHP which produces electric power by burning the bio methane. The electrical power is then fed into the public power grid. In 2010, the total installed electrical capacity of these power plants was 2,291 MW. The electricity supply was approximately 12.8 TWh, which is 12.6% of the total generated renewable electricity.\n\nBiogas in Germany is primarily extracted by the co-fermentation of energy crops (called 'NawaRo', an abbreviation of \"nachwachsende Rohstoffe\", German for renewable resources) mixed with manure. The main crop used is corn. Organic waste and industrial and agricultural residues such as waste from the food industry are also used for biogas generation. In this respect, biogas production in Germany differs significantly from the UK, where biogas generated from landfill sites is most common.\n\nBiogas production in Germany has developed rapidly over the last 20 years. The main reason is the legally created frameworks. Government support of renewable energy started in 1991 with the Electricity Feed-in Act (\"StrEG\"). This law guaranteed the producers of energy from renewable sources the feed into the public power grid, thus the power companies were forced to take all produced energy from independent private producers of green energy. In 2000 the Electricity Feed-in Act was replaced by the Renewable Energy Sources Act (\"EEG\"). This law even guaranteed a fixed compensation for the produced electric power over 20 years. The amount of around 8¢/kWh gave farmers the opportunity to become energy suppliers and gain a further source of income.\n\nThe German agricultural biogas production was given a further push in 2004 by implementing the so-called NawaRo-Bonus. This is a special payment given for the use of renewable resources, that is, energy crops. In 2007 the German government stressed its intention to invest further effort and support in improving the renewable energy supply to provide an answer on growing climate challenges and increasing oil prices by the ‘Integrated Climate and Energy Programme’.\n\nThis continual trend of renewable energy promotion induces a number of challenges facing the management and organisation of renewable energy supply that has also several impacts on the biogas production. The first challenge to be noticed is the high area-consuming of the biogas electric power supply. In 2011 energy crops for biogas production consumed an area of circa 800,000 ha in Germany. This high demand of agricultural areas generates new competitions with the food industries that did not exist hitherto. Moreover, new industries and markets were created in predominately rural regions entailing different new players with an economic, political and civil background. Their influence and acting has to be governed to gain all advantages this new source of energy is offering. Finally biogas will furthermore play an important role in the German renewable energy supply if good governance is focused.\n\nBiogas in India has been traditionally based on dairy manure as feed stock and these \"gobar\" gas plants have been in operation for a long period of time, especially in rural India. In the last 2–3 decades, research organisations with a focus on rural energy security have enhanced the design of the systems resulting in newer efficient low cost designs such as the Deenabandhu model.\n\nThe Deenabandhu Model is a new biogas-production model popular in India. (\"Deenabandhu\" means \"friend of the helpless.\") The unit usually has a capacity of 2 to 3 cubic metres. It is constructed using bricks or by a ferrocement mixture. In India, the brick model costs slightly more than the ferrocement model; however, India's Ministry of New and Renewable Energy offers some subsidy per model constructed.\n\nBiogas which is mainly methane/natural gas can also be used for generating protein rich cattle, poultry and fish feed in villages economically by cultivating \"Methylococcus capsulatus\" bacteria culture with tiny land and water foot print. The carbon dioxide gas produced as by product from these plants can be put to use in cheaper production of algae oil or spirulina from algaculture particularly in tropical countries like India which can displace the prime position of crude oil in near future. Union government of India is implementing many schemes to utilise productively the agro waste or biomass in rural areas to uplift rural economy and job potential. With these plants, the non edible biomass or waste of edible biomass is converted in to high value products without any water pollution or green house gas (GHG) emissions.\n\nLPG (Liquefied Petroleum Gas) is a key source of cooking fuel in urban India and its prices have been increasing along with the global fuel prices. Also the heavy subsidies provided by the successive governments in promoting LPG as a domestic cooking fuel has become a financial burden renewing the focus on biogas as a cooking fuel alternative in urban establishments. This has led to the development of prefabricated digester for modular deployments as compared to RCC and cement structures which take a longer duration to construct. Renewed focus on process technology like the Biourja process model has enhanced the stature of medium and large scale anaerobic digester in India as a potential alternative to LPG as primary cooking fuel.\n\nIn India, Nepal, Pakistan and Bangladesh biogas produced from the anaerobic digestion of manure in small-scale digestion facilities is called gobar gas; it is estimated that such facilities exist in over 2 million households in India, 50,000 in Bangladesh and thousands in Pakistan, particularly North Punjab, due to the thriving population of livestock. The digester is an airtight circular pit made of concrete with a pipe connection. The manure is directed to the pit, usually straight from the cattle shed. The pit is filled with a required quantity of wastewater. The gas pipe is connected to the kitchen fireplace through control valves. The combustion of this biogas has very little odour or smoke. Owing to simplicity in implementation and use of cheap raw materials in villages, it is one of the most environmentally sound energy sources for rural needs. One type of these system is the Sintex Digester. Some designs use vermiculture to further enhance the slurry produced by the biogas plant for use as compost.\n\nIn Pakistan, the Rural Support Programmes Network is running the Pakistan Domestic Biogas Programme which has installed 5,360 biogas plants and has trained in excess of 200 masons on the technology and aims to develop the Biogas Sector in Pakistan.\n\nIn Nepal, the government provides subsidies to build biogas plant at home.\n\nThe Chinese have experimented with the applications of biogas since 1958. Around 1970, China had installed 6,000,000 digesters in an effort to make agriculture more efficient. During the last years the technology has met high growth rates. This seems to be the earliest developments in generating biogas from agricultural waste.\n\nDomestic biogas plants convert livestock manure and night soil into biogas and slurry, the fermented manure. This technology is feasible for small-holders with livestock producing 50 kg manure per day, an equivalent of about 6 pigs or 3 cows. This manure has to be collectable to mix it with water and feed it into the plant. Toilets can be connected. Another precondition is the temperature that affects the fermentation process. With an optimum at 36 C° the technology especially applies for those living in a (sub) tropical climate. This makes the technology for small holders in developing countries often suitable.\nDepending on size and location, a typical brick made fixed dome biogas plant can be installed at the yard of a rural household with the investment between US$300 to $500 in Asian countries and up to $1400 in the African context. A high quality biogas plant needs minimum maintenance costs and can produce gas for at least 15–20 years without major problems and re-investments. For the user, biogas provides clean cooking energy, reduces indoor air pollution, and reduces the time needed for traditional biomass collection, especially for women and children. The slurry is a clean organic fertilizer that potentially increases agricultural productivity.\n\nDomestic biogas technology is a proven and established technology in many parts of the world, especially Asia. Several countries in this region have embarked on large-scale programmes on domestic biogas, such as China and India.\n\nThe Netherlands Development Organisation, SNV, supports national programmes on domestic biogas that aim to establish commercial-viable domestic biogas sectors in which local companies market, install and service biogas plants for households. In Asia, SNV is working in Nepal, Vietnam, Bangladesh, Bhutan, Cambodia, Lao PDR, Pakistan and Indonesia, and in Africa; Rwanda, Senegal, Burkina Faso, Ethiopia, Tanzania, Uganda, Kenya, Benin and Cameroon.\n\nIn South Africa a prebuilt Biogas system is manufactured and sold. One key feature is that installation requires less skill and is quicker to install as the digester tank is premade plastic.\n\nIn the 1985 Australian film \"Mad Max Beyond Thunderdome\" the post-apocalyptic settlement Barter town is powered by a central biogas system based upon a piggery. As well as providing electricity, methane is used to power Barter's vehicles.\n\n\"Cow Town\", written in the early 1940s, discuss the travails of a city vastly built on cow manure and the hardships brought upon by the resulting methane biogas. Carter McCormick, an engineer from a town outside the city, is sent in to figure out a way to utilize this gas to help power, rather than suffocate, the city.\n\nThe Biogas production is providing nowadays new opportunities for skilled employment, drawing on the development of new technologies.\n\n\n"}
{"id": "6672748", "url": "https://en.wikipedia.org/wiki?curid=6672748", "title": "Causal model", "text": "Causal model\n\nA causal model is a conceptual model that describes the causal mechanisms of a system. Causal models can improve study designs by providing clear rules for deciding which independent variables need to be included/controlled for. \n\nThey can allow some questions to be answered from existing observational data without the need for an interventional study such as a randomized controlled trial. Some interventional studies are inappropriate for ethical or practical reasons, meaning that without a causal model, some questions cannot be answered. \n\nCasual models can help with the question of external validity(whether results from one study apply to unstudied populations). Causal models can allow data from multiple studies to be merged (in certain circumstances) to answer questions that cannot be answered by any individual data set.\n\nCausal models are falsifiable, in that if they do not match data, they must be rejected as invalid.\n\nCausal models have found applications in signal processing, epidemiology and machine learning.\n\n Pearl defines a causal model as an ordered triple formula_1, where U is a set of exogenous variables whose values are determined by factors outside the model; V is a set of endogenous variables whose values are determined by factors within the model; and E is a set of structural equations that express the value of each endogenous variable as a function of the values of the other variables in U and V.\n\nAristotle defined a taxonomy of causality, including \"material\", \"formal\", \"efficient\" and \"final\" causes. Hume rejected Aristotle's taxonomy in favor of counterfactuals. At one point, he denied that objects have \"powers\" that make one a cause and another an effect. Later he adopted \"if the first object had not been, the second had never existed\" (\"but-for\" causation).\n\nIn the late 19th century, the discipline of statistics began to form. After a years-long effort to identify causal rules for domains such as biological inheritance, Galton introduced the concept of mean regression (epitomized by the sophomore slump in sports) which later led him to the non-causal concept of correlation. \n\nAs a positivist, Pearson expunged the notion of causality from much of science as an unprovable special case of association and introduced the correlation coefficient as the metric of association. He wrote, \"Force as a cause of motion is exactly the same as a tree god as a cause of growth\" and that causation was only a \"fetish among the inscrutable arcana of modern science\". Pearson founded \"Biometrika\" and the Biometrics Lab at University College London, which became the world leader in statistics.\n\nIn 1908 Hardy and Weinberg solved the problem of trait stability that had led Galton to abandon causality, by invoking Mendelian inheritance.\n\nIn 1921 Wright's path analysis became the theoretical ancestor of causal modeling and causal graphs. He developed this approach while attempting to untangle the relative impacts of heredity, development and environment on guinea pig coat patterns. He backed up his heretical claims by showing how such analyses could explain the relationship between guinea pig birth weight, in utero time and litter size. Opposition to these ideas by prominent statisticians led them to be ignored for the following 40 years (except among animal breeders). Instead scientists relied on correlations, partly at the behest of Wright's critic (and leading statistician), Fisher. One exception was Burks, a student who in 1926 was the first to apply path diagrams to represent a mediator and to assert that holding a mediator constant induces errors. She may have invented path diagrams independently. \n\nIn 1923, Neyman introduced the concept of a potential outcome, but his paper was not translated from Polish to English until 1990. \n\nIn 1958 Cox wrote warned that controlling for a variable Z is valid only if it is highly unlikely to be affected independent variables. \n\nIn the 1960s, Duncan, Blalock, Goldberger and others rediscovered path analysis. While reading Blalock's work on path diagrams, Duncan remembered a lecture by Ogburn twenty years earlier that mentioned a paper by Wright that mentioned Burk. \n\nSociologists called causal models structural equation modeling, but once it became a rote method, it lost its utility, leading some practitioners to reject any relationship to causality. Economists adopted the algebraic part of path analysis, calling it simultaneous equation modeling. However, economists still avoided attributing causal meaning to their equations.\n\nSixty years after his first paper, Wright published a piece that recapitulated it, following Karlin et al.'s critique, which objected that it handled only linear relationships and that robust, model-free presentations of data are more revealing.\n\nIn 1973 Lewis advocated replacing correlation with but-for causality (counterfactuals). He referred to humans' ability to envision alternative worlds in which a cause did or not occur and in which effect an appeared only following its cause. In 1974 Rubin introduced the notion of \"potential outcomes\" as a language for asking causal questions. \n\nIn 1983 Cartwright proposed that any factor that is \"causally relevant\" to an effect be conditioned on, moving beyond simple probability as the only guide. \n\nIn 1986 Baron and Kenny introduced principles for detecting and evaluating mediation in a system of linear equations. As of 2014 their paper was the 33rd most-cited of all time. That year Greenland and Robins introduced the \"exchangeability\" approach to handling confounding by considering a counterfactual. They proposed assessing what would have happened to the treatment group if they had not received the treatment and comparing that outcome to that of the control group. If they matched, confounding was said to be absent. \n\nPearl's causal metamodel involves a three-level abstraction he calls the ladder of causation. The lowest level, Association (seeing/observing), entails the sensing of regularities or patterns in the input data, expressed as correlations. The middle level, Intervention (doing), predicts the effects of deliberate actions, expressed as causal relationships. The highest level, Counterfactuals (imagining), involves constructing a theory of (part of) the world that explains why specific actions have specific effects and what happens in he absence of such actions.\n\nOne object is associated with another if observing one changes the probability of observing the other. Example: shoppers who buy toothpaste are more likely to also buy dental floss. Mathematically: \n\nor the probability of (purchasing) floss given (the purchase of) toothpaste. Associations can also be measured via computing the correlation of the two events. Associations have no causal implications. One event could cause the other, the reverse could be true, or both events could be caused by some third event (unhappy hygenist shames shopper into treating their mouth better ).\n\nThis level asserts specific causal relationships between events. Causality is assessed by experimentally performing some action that affects one of the events. Example: doubling the price of toothpaste (then what happens). Causality cannot be established by examining history (of price changes) because the price change may have been for some other reason that could itself affect the second event (a tariff that increases the price of both goods). Mathematically: \n\nwhere \"do\" is an operator that signals the experimental intervention (doubling the price).\n\nThe highest, counterfactual, level involves consideration of an alternate version of a past event. Example: What is the probability that If a store had doubled the price of floss, the toothpaste-purchasing shopper would still have bought it? Answering yes asserts the existence of a causal relationship. Models that can answer counterfactuals allow precise interventions whose consequences can be predicted. At the extreme, such models are accepted as physical laws (as in the laws of physics, e.g., inertia, which says that if force is not applied to a stationary object, it will not move).\n\nStatistics revolves around the analysis of relationships among multiple variables. Traditionally, these relationships are described as correlations, associations without any implied causal relationships. Causal models attempt to extend this framework by adding the notion of causal relationships, in which changes in one variable cause changes in others.\n\nTwentieth century definitions of causality relied purely on probabilities/associations. One event (X) was said to cause another if it raises the probability of the other (Y). Mathematically this is expressed as: \n\nA later definition attempted to address this ambiguity by conditioning on background factors. Mathematically: \n\nOther attempts to define causality include Granger causality, a statistical hypothesis test that causality (in economics) can be assessed by measuring the ability to predict the future values of one time series using prior values of another time series.\n\nA cause can be necessary, sufficient, contributory or some combination.\n\nFor \"x\" to be a necessary cause of \"y\", the presence of \"y\" must imply the prior occurrence of \"x\". The presence of \"x\", however, does not imply that \"y\" will occur. Necessary causes are also known as \"but-for\" causes, as in \"y\" would not have occurred but for the occurrence of \"x\". \n\nFor \"x\" to be a sufficient cause of \"y\", the presence of \"x\" must imply the subsequent occurrence of \"y\". However, another cause \"z\" may independently cause \"y\". Thus the presence of \"y\" does not require the prior occurrence of \"x\".\n\nFor \"x\" to be a contributory cause of \"y\", the presence of \"x\" must increase the likelihood of \"y\". If the likelihood is 100%, then \"x\" is instead called sufficient. A contributory cause may also be necessary.\n\nA causal diagram is a directed graph that displays causal relationships between variables in a causal model. A causal diagram includes a set of variables (or nodes). Each node is connected by an arrow to one or more other nodes upon which it has a causal influence. An arrowhead delineates the direction of causality, e.g., an arrow connecting variables A and B with the arrowhead at B indicates that a change in A causes a change in B (with an associated probability).\n\nCausal diagrams include causal loop diagrams, directed acyclic graphs, and Ishikawa diagrams.\n\nCasual diagrams are independent of the quantitative probabilities that inform them. Changes to those probabilities (e.g., due to technological improvements) do not require changes to the model.\n\nCausal models have formal structures with elements with specific properties.\n\nThe three types of connections of three nodes are linear chains, branching forks and merging colliders.\n\nChains are straight line connections with arrows pointing from cause to effect. In this model, B is a mediator in that it mediates the change that A would otherwise have on C. \n\nIn forks, one cause has multiple effects. The two effects have a common cause. Conditioning on B (for a specific value of B) reveals a positive correlation between A and C that is not causal. \n\nAn elaboration of a fork is the confounder:\n\nIn such models, B is a common cause of A and C (which also causes A), making B the confounder. \n\nIn colliders, multiple causes affect one outcome. Conditioning on B (for a specific value of B) often reveals a non-causal negative correlation between A and C. This negative correlation has been called collider bias and the \"explain-away\" effect as in, B explains away the correlation between A and C. The correlation can be positive in the case where contributions from both A and C are necessary to affect B. \n\nA mediator node modifies the effect of other causes on an outcome (as opposed to simply affecting the outcome). \n\nA confounder node affects multiple outcomes, creating a positive correlation among them.\n\nAn instrumental variable is one that: \n\n\nRegression coefficients can serve as estimates of the causal effect of an instrumental variable on an outcome as long as that effect is not confounded. In this way, instrumental variables allow causal factors to be quantified without data on confounders. \n\nFor example, given the model:\n\nZ is an independent variable, because it has a path to the outcome Y and is unconfounded, e.g., by U.\n\nDefinition: In the above example, if Z and X take binary values, then the assumption that Z = 0, X = 1 does not occur is called monotonicity. \n\nRefinements to the technique include creating an instrument by conditioning on other variable to block the paths between the instrument and the confounder and combining multiple variables to form a single instrument. \n\nDefinition: Mendelian randomization uses measured variation in genes of known function to examine the causal effect of a modifiable exposure on disease in observational studies.\n\nBecause genes vary randomly across populations, presence of a gene typically qualifies as an instrumental variable, implying that in many cases, causality can be quantified using regression on an observational study.\n\nIndependence conditions are rules for deciding whether two variables are independent of each other. Variables are independent if the values of one do not directly affect the values of the other. Multiple causal models can share independence conditions. For example, the models\n\nand \n\nhave the same independence conditions, because conditioning on B leaves A and C independent. However, the two models do not have the same meaning and can be falsified based on data. (If observations show an association between A and C after conditioning on B, then both models are incorrect). Conversely, data cannot show which of these two models are correct, because they have the same independence conditions. Conditioning on a variable is a mechanism for conducting hypothetical experiments. Conditioning on a variable involves analyzing the values of other variables for a given value of the conditioned variable. In the first example, conditioning on B implies that observations for a given value of B should show no correlation between A and C. If such a correlation exists, then the model is incorrect. Non-causal models cannot make such distinctions, because they do not make causal assertions. \n\nAn essential element of correlational study design is to identify potentially confounding influences on the variable under study, such as demographics. These variables are controlled for to eliminate those influences. However, the correct list of confounding variables cannot be determined \"a priori\". It is thus possible that a study may control for irrelevant variables or even (indirectly) the variable under study. \n\nCausal models offer a robust technique for identifying appropriate confounding variables. Formally, Z is a confounder if \"Y is associated with Z via paths not going through X\". These can often be determined using data collected for other studies. Mathematically, if \n\nthen X is a confounder for Y. \n\nEarlier, allegedly incorrect definitions include: \n\n\nThe latter is flawed in that given that in the model: \n\nZ matches the definition, but is a mediator, not a confounder, and is an example of controlling for the outcome.\n\nIn the model \n\nTraditionally, B was considered to be a confounder, because it is associated with X and with Y but is not on a causal path nor is it a descendant of anything on a causal path. Controlling for B causes it to become a confounder. This is known as M-bias. \n\nIn a causal model, the method for identifying all appropriate counfounders (deconfounding) is to block every noncausal path between X and Y without disrupting any causal paths. \n\nDefinition: a backdoor path between two variables X and Y is any path from X to Y that starts with an arrow pointing to X. \n\nX and Y are deconfounded if every backdoor path is blocked and no controlled-for variable Z is descended from X. It is not necessary to control for any variables other than the deconfounders. \n\nDefinition: the backdoor criterion is satisfied when all backdoor paths in a model are blocked.\n\nWhen the causal model is a plausible representation of reality and the backdoor criterion is satisfied, then partial regression coefficients can be used as (causal) path coefficients (for linear relationships). \n\nDefinition: a frontdoor path is a direct causal path for which data is available for all variables. \n\nThe following converts a do expression into a do-free expression by conditioning on the variables along the front-door path. \n\nPresuming data for these observable probabilities is available, the ultimate probability can be computed without an experiment, regardless of the existence of other confounding paths and without backdoor adjustment. \n\nQueries are questions asked based on a specific model. They are generally answered via performing experiments (interventions). Interventions take the form of fixing the value of one variable in a model and observing the result. Mathematically, such queries take the form (from the example): \n\nwhere the \"do\" operator indicates that the experiment explicitly modified the price of toothpaste. Graphically, this blocks any causal factors that would otherwise affect that variable. Diagramatically, this erases all causal arrows pointing at the experimental variable. \n\nMore complex queries are possible, in which the do operator is applied (the value is fixed) to multiple variables.\n\nThe do calculus is the set of manipulations that are available to transform one expression into another, with the general goal of transforming expressions that contain the do operator into expressions that do not. Expressions that do not include the do operator can be estimated from observational data alone, without the need for an experimental intervention, which might be expensive, lengthy or even unethical (e.g., asking subjects to take up smoking). The set of rules is complete (it can be used to derive every true statement in this system). An algorithm can determine whether, for a given model, a solution is computable in polynomial time. \n\nThe calculus includes three rules for the transformation of conditional probability expressions involving the do operator. \n\nRule 1 permits the addition or deletion of observations.:\n\nin the case that the variable set Z blocks all paths from W to Y and all arrows leading into X have been deleted. \n\nRule 2 permits the replacement of an intervention with an observation or vice versa.:\n\nin the case that Z satisfies the back-door criterion. \n\nRule 3 permits the deletion or addition of interventions.:\n\nin the case where no causal paths connect X and Y. \n\nThe rules do not imply that any query can have its do operators removed. In those cases, it may be possible to substitute a variable that is subject to manipulation (e.g., diet) in place of one that is not (e.g., blood cholesterol), which can then be transformed to remove the do. Example: \n\nCounterfactuals consider possibilities that are not found in data, such as whether a nonsmoker would have developed cancer had they instead been a heavy smoker. They are the highest step on Pearl's causality ladder. \n\nDefinition: A potential outcome for a variable Y is \"the value Y would have taken for individual \"u\", had X been assigned the value x\". Mathematically: \n\nThe potential outcome is defined at the level of the individual \"u.\" \n\nThe conventional approach to potential outcomes is data-, not model-driven, limiting its ability to untangle causal relationships. It treats causal questions as problems of missing data and gives incorrect answers to even standard scenarios. \n\nIn the context of causal models, potential outcomes are interpreted causally, rather than statistically.\n\nThe first law of causal inference states that the potential outcome \n\ncan be computed by modifying causal model M (by deleting arrows into X) and computing the outcome for some \"x\". Formally: \n\nExamining a counterfactual using a causal model involves three steps. The approach is valid regardless of the form of the model relationships (linear or otherwise) When the model relationships are fully specified, point values can be computed. In other cases, (e.g., when only probabilities are available) a probability-interval statement (non-smoker x would have a 10-20% chance of cancer) can be computed. \n\nGiven the model:\n\nthe equations for calculating the values of A and C derived from regression analysis or another technique can be applied, substituting known values from an observation and fixing the value of other variables (the counterfactual). \n\nApply abductive reasoning (logical inference that uses observation to find the simplest/most likely explanation) to estimate \"u\", the proxy for the unobserved variables on the specific observation that supports the counterfactual. \n\nFor a specific observation, use the do operator to establish the counterfactual (e.g., \"m\"=0), modifying the equations accordingly. \n\nCalculate the values of the output (\"y\") using the modified equations. \n\nDirect and indirect (mediated) causes can only be distinguished via conducting counterfactuals. Understanding mediation requires holding the mediator constant while intervening on the direct cause. In the model\n\nformula_28\n\nM mediates X's influence on Y, while X also has an unmediated effect on Y. Thus M is held constant, while do(X) is computed.\n\nThe Mediation Fallacy instead involves conditioning on the mediator if the mediator and the outcome are confounded, as they are in the above model.\n\nFor linear models, the indirect effect can be computed by taking the product of all the path coefficients along a mediated pathway. The total indirect effect is computed by the sum of the individual indirect effects. For linear models mediation is indicated when the coefficients of an equation fitted without including the mediator vary significantly from an equation that includes it. \n\nIn experiments on such a model, the controlled direct effect (CDE) is computed by forcing the value of the mediator M (do(M = 0)) and randomly assigning some subjects to each of the values of X (do(X=0), do(X=1), ...) and observing the resulting values of Y. \n\nEach value of the mediator has a corresponding CDE.\n\nHowever, a better experiment is to compute the natural direct effect. (NDE) This is the effect determined by leaving the relationship between X and M untouched while intervening on the relationship between X and Y. \n\nFor example, consider the direct effect of increasing dental hygenist visits (X) from every other year to every year, which encourages flossing (M). Gums (Y) get healthier, either because of the hygenist (direct) or the flossing (mediator/indirect). The experiment is to continue flossing while skipping the hygenist visit.\n\nThe indirect effect of X on Y is the \"increase we would see in Y while holding X constant and increasing M to whatever value M would attain under a unit increase in X\". \n\nIndirect effects cannot be \"controlled\" because the direct path cannot be disabled by holding another variable constant. The natural indirect effect (NIE) is the effect on gum health (Y) from flossing (M). The NIE is calculated as the sum of (floss and no-floss cases) of the difference between the probability of flossing given the hygenist and without the hygenist, or: \n\nThe above NDE calculation includes counterfactual subscripts (formula_32). For nonlinear models, the seemingly obvious equivalence \n\ndoes not apply because of anomalies such as threshold effects and binary values. However, \n\nworks for all model relationships (linear and nonlinear). It allows NDE to then be calculated directly from observational data, without interventions or use of counterfactual subscripts. \n\nCausal models provide a vehicle for integrating data across datasets, known as transport, even though the causal models (and the associated data) differ. E.g., survey data can be merged with randomized, controlled trial data. Transport offers a solution to the question of external validity, whether a study can be applied in a different context.\n\nWhere two models match on all relevant variables and data from one model is known to be unbiased, data from one population can be used to draw conclusions about the other. In other cases, where data is known to be biased, reweighting can allow the dataset to be transported. In a third case, conclusions can be drawn from an incomplete dataset. In some cases, data from studies of multiple populations can be combined (via transportation) to allow conclusions about an unmeasured population. In some cases, combining estimates (e.g., P(W|X)) from multiple studies can increase the precision of a conclusion. \n\nDo-calculus provides a general criterion for transport: A target variable can be transformed into another expression via a series of do-operations that does not involve any \"difference-producing\" variables (those that distinguish the two populations). An analogous rule applies to studies that have relevantly different participants. \n\nAny causal model can be implemented as a Bayesian network. Bayesian networks can be used to provide the inverse probability of an event (given an outcome, what are the probabilities of a specific cause). This requires preparation of a conditional probability table, showing all possible inputs and outcomes with their associated probabilities. \n\nFor example, given a two variable model of Disease and Test (for the disease) the conditional probability table takes the form: \nAccording to this table, when a patient does not have the disease, the probability of a positive test is 12%.\n\nWhile this is tractable for small problems, as the number of variables and their associated states increase, the probability table (and associated computation time) increases exponentially. \n\nBayesian networks are used commercially in applications such as wireless data error correction and DNA analysis. \n\n\n"}
{"id": "54415558", "url": "https://en.wikipedia.org/wiki?curid=54415558", "title": "Chicago Public School Boycott", "text": "Chicago Public School Boycott\n\nThe Chicago Public School Boycott, also known as Freedom Day, was a mass boycott and demonstration against the segregationist policies of the Chicago Public Schools (CPS) on October 22, 1963. More than 200,000 students stayed out of school, and tens of thousands of Chicagoans joined in a protest that culminated in a march to the office of the Chicago Board of Education. This protest preceded the larger New York City Public School boycott.\n\nAlthough \"Brown v. Board of Education\" prohibited racial segregation in schools, in 1963, Chicago's public schools continued to be segregated as a result of residential segregation. This was exacerbated by the migration of more black Americans from the Southern United States to Chicago during the Jim Crow era. School boundary lines were drawn specifically to preserve racial segregation, even as predominantly black schools grew overcrowded. Classes were held in hallways, and there were not enough books for all of the students. Some schools held double shifts, meaning students attended less than a full day of class. Rather than send black students to underpopulated white schools, CPS Superintendent Benjamin Willis instituted the use of mobile classrooms; 625 aluminium trailers parked in the parking lots and playgrounds of overcrowded schools. Rosie Simpson of Englewood, Chicago, coined the term \"Willis Wagons\" to describe the mobile classrooms. At one high school, these trailers were used to maintain segregation within the school; black students' classes were held in Willis Wagons, while white students went to class in the school.\n\nIn response to the school segregation enacted by Willis, community members began organizing resistance. Organizers included Chicago activist Albert Raby. The Coordinating Council of Community Organizations (CCCO) took the lead, organizing \"Freedom Day,\" the mass boycott and protest. According to the chairman of the CCCO, Lawrence Landry, \"The boycott is an effort to communicate dissatisfaction on how schools are being run.\" \"The Chicago Defender\" estimated that forty percent of CPS students would participate in the boycott. On October 22, 1963, nearly half of all CPS students skipped school, leaving schools on Chicago's South Side and West Side mostly empty. The \"Chicago Tribune\" reported that 224,770 students were absent from CPS, amounting to 47 percent of the student population. In addition to the boycott, nearly 10,000 protesters marched in Chicago's downtown, stopping outside the Chicago Board of Education offices. Chicago police kept protesters from entering the building.\n\nWhile Freedom Day was popular and widely covered, it did not have significant impact in changing the policies of Superintendent Willis. This \"de facto\" school segregation was supported by Mayor Richard J. Daley, who went on to nominate two school board members who did not support the CCCO's push for integration. Ultimately, CPS was not moved to integrate after Freedom Day, despite the best efforts of Black activists and the CCCO. Use of Willis Wagons prevailed, and Willis himself did not retire until 1966 (albeit four months before the end of his term). However, the size of the first Freedom Day protest inspired subsequent boycotts in Chicago and the United States.\n\nThe Freedom Day protest inspired Reverend Martin Luther King Jr.'s move to Chicago in 1966. Before Freedom Day, in August, 1963, then-college student Bernie Sanders was arrested while protesting the policies of Superintendent Willis. A documentary on Freedom Day, \"63 Boycott\", is in production. Today, CPS is the most segregated big-city school district in the United States.\n\n"}
{"id": "8015055", "url": "https://en.wikipedia.org/wiki?curid=8015055", "title": "Critical mass (sociodynamics)", "text": "Critical mass (sociodynamics)\n\nIn social dynamics, critical mass is a sufficient number of adopters of an innovation in a social system so that the rate of adoption becomes self-sustaining and creates further growth. The term is borrowed from nuclear physics and in that field it refers to the amount of a substance needed to sustain a chain reaction.\n\nSocial factors influencing critical mass may involve the size, interrelatedness and level of communication in a society or one of its subcultures. Another is social stigma, or the possibility of public advocacy due to such a factor.\n\nCritical mass may be closer to majority consensus in political circles, where the most effective position is more often that held by the majority of people in society. In this sense, small changes in public consensus can bring about swift changes in political consensus, due to the majority-dependent effectiveness of certain ideas as tools of political debate.\n\nCritical mass is a concept used in a variety of contexts, including physics, group dynamics, politics, public opinion, and technology.\n\nThe concept of critical mass was originally created by game theorist Thomas Schelling and sociologist Mark Granovetter to explain the actions and behaviors of a wide range of people and phenomenon. The concept was first established (although not explicitly named) in Schelling's essay about racial segregation in neighborhoods, published in 1971 in the Journal of Mathematical Sociology, and later refined in his book, \"Micromotives and Macrobehavior\", published in 1978. He did use the term \"critical density\" with regard to pollution in his \"On the Ecology of Micromotives\". Mark Granovetter, in his essay \"Threshold models of collective behavior\", published in the American Journal of Sociology in 1978 worked to solidify the theory. Everett Rogers later cites them both in his important work \"Diffusion of Innovations\", in which critical mass plays an important role.\n\nThe concept of critical mass had existed before it entered a sociology context. It was an established concept in medicine, specifically epidemiology, since the 1920s, as it helped to explain the spread of illnesses.\n\nIt had also been a present, if not solidified, idea in the study of consumer habits and economics, especially in General Equilibrium Theory. In his papers, Schelling quotes the well-known \"The Market for Lemons: Quality Uncertainty and the Market Mechanism\" paper written in 1970 by George Akerlof. Similarly, Granovetter cited the Nash Equilibrium game in his papers.\n\nFinally, Herbert A. Simon's essay, \"Bandwagon and underdog effects and the possibility of election predictions\", published in 1954 in Public Opinion Quarterly, has been cited as a predecessor to the concept we now know as critical mass.\n\nCritical mass and the theories behind it help us to understand aspects of humans as they act and interact in a larger social setting. Certain theories, such as Mancur Olson's Logic of Collective Action or Garrett Hardin's Tragedy of the Commons, work to help us understand why humans do or adopt certain things which are beneficial to them, or, more importantly, why they do not. Much of this reasoning has to do with individual interests trumping that which is best for the collective whole, which may not be obvious at the time.\n\nOliver, Marwell, and Teixeira tackle this subject in relation to critical theory in a 1985 article published in the American Journal of Sociology. In their essay, they define that action in service of a public good as \"collective action\". \"Collective Action\" is beneficial to all, regardless of individual contribution. By their definition, then, \"critical mass\" is the small segment of a societal system that does the work or action required to achieve the common good. The \"Production Function\" is the correlation between resources, or what individuals give in an effort to achieve public good, and the achievement of that good. Such function can be decelerating, where there is less utility per unit of resource, and in such a case, resource can taper off. On the other hand, the function can be accelerating, where the more resources that are used the bigger the payback. \"Heterogeneity\" is also important to the achievement of a common good. Variations (heterogeneity) in the value individuals put on a common good or the effort and resources people give is beneficial, because if certain people stand to gain more, they are willing to give or pay more.\n\nCritical mass theory in gender politics and collective political action is defined as the critical number of personnel needed to affect policy and make a change not as the token but as an influential body. This number has been placed at 30%, before women are able to make a substantial difference in politics. However, other research suggests lower numbers of women working together in legislature can also affect political change. Kathleen Bratton goes so far as to say that women, in legislatures where they make up less than 15% of the membership, may actually be encouraged to develop legislative agendas that are distinct from those of their male colleagues. Others argue that we should look more closely at parliamentary and electoral systems instead of critical mass.\n\nWhile critical mass can be applied to many different aspects of sociodynamics, it becomes increasingly applicable to innovations in interactive media such as the telephone, fax, or email. With other non-interactive innovations, the dependence on other users was generally sequential, meaning that the early adopters influenced the later adopters to use the innovation. However, with interactive media, the interdependence was reciprocal, meaning both users influenced each other. This is due to the fact that interactive media have high network effect, where in the value and utility of a good or service increases the more users it has. Thus, the increase of adopters and quickness to reach critical mass can therefore be faster and more intense with interactive media, as can the rate at which previous users discontinue their use. The more people that use it, the more beneficial it will be, thus creating a type of snowball effect, and conversely, if users begin to stop using the innovation, the innovation loses utility, thus pushing more users to discontinue their use.\n\nIn M. Lynne Markus' essay in \"Communication Research\" entitled \"Toward a 'Critical Mass' Theory of Interactive Media\", several propositions are made that try to predict under what circumstances interactive media is most likely to achieve critical mass and reach universal access, a \"common good\" using Oliver, \"et al\".'s terminology. One proposition states that such media's existence is all or nothing, where in if universal access is not achieved, then, eventually, use will discontinue. Another proposition suggests that a media's ease of use and inexpensiveness, as well as its utilization of an \"active notification capability\" will help it achieve universal access. The third proposition states that the heterogeneity, as discussed by Oliver, et al. is beneficial, especially if users are dispersed over a larger area, thus necessitating interactivity via media. Fourth, it is very helpful to have highly sought-after individuals to act as early adopters, as their use acts as incentive for later users. Finally, Markus posits that interventions, both monetarily and otherwise, by governments, businesses, or groups of individuals will help a media reach its critical mass and achieve universal access.\n\nAn example put forth by Rogers in \"Diffusion of Innovations\" was that of the fax machine, which had been around for almost 150 years before it became popular and widely used. It had existed in various forms and for various uses, but with more advancements in the technology of faxes, including the use of existing phone lines to transmit information, coupled with falling prices in both machines and cost per fax, the fax machine reached a critical mass in 1987, when \"Americans began to assume that 'everybody else' had a fax machine\".\n\n\n"}
{"id": "41899489", "url": "https://en.wikipedia.org/wiki?curid=41899489", "title": "Diego Mateo Zapata", "text": "Diego Mateo Zapata\n\nDiego Mateo Zapata (1664-1745) was a Spanish physician and philosopher. In 1724, he and Juan Muñoz y Peralta were both denounced to the Spanish Inquisition as judaisers. Spanish philosopher Diego Mateo Zapata was tortured during the Inquisition for following and endorsing the teachings of Judaism. .\n\n"}
{"id": "370377", "url": "https://en.wikipedia.org/wiki?curid=370377", "title": "Dinotopia", "text": "Dinotopia\n\nDinotopia is a fictional utopia created by author and illustrator James Gurney. It is the setting for the book series with which it shares its name. Dinotopia is an isolated island inhabited by shipwrecked humans and sapient dinosaurs who have learned to coexist peacefully as a single symbiotic society. The first book was published in 1992 and has \"appeared in 18 languages in more than 30 countries and sold two million copies.\" \"Dinotopia: A Land Apart from Time\" and \"Dinotopia: The World Beneath\" both won Hugo awards for best original artwork.\n\nSince its original publication, over twenty Dinotopia books have been published by various authors to expand the series. A live-action TV mini-series, a brief live-action TV series, an animated film, and several video games have also been released.\n\nGurney's assignments for \"National Geographic\" required him to work with archaeologists to envision and paint ancient cities that no one alive today has ever seen. This inspired him to imagine his own, so he painted \"Waterfall City\" and \"Dinosaur parade\". These were originally done as art prints for collectors. He later decided to create an imaginary island based on these paintings.\n\nRather than digital tools, Gurney used \"plein-air studies, thumbnail sketches, models photographed in costume and original cardboard or clay maquettes\" to create 150 oil paintings for his 2007 Dinotopia book. He called the series \"Dinotopia\": a portmanteau of \"dinosaur\" and \"utopia\".\n\nMany have claimed that some scenes in the film \"\" (particularly those in the city of Theed on Naboo) unfairly copy images from Gurney's books. Gurney acknowledges the resemblance but has remained positive about it. In 1994, director George Lucas had met with producers to discuss some of the concepts and visuals behind a \"Dinotopia\" movie that was never made.\n\nUpon the hidden island of Dinotopia, humans and dinosaurs live and work together in harmony with one another and with the Earth itself. It is a place of beauty and wonder lost to the rest of the world. The island is surrounded by a storm system and dangerous reefs that prevent safe travel to or from the island. Aside from a highly diverse ecosystem ranging from deserts to mountains to swamps, Dinotopia also has an extensive system of natural and man-made caves.\n\nThe dinosaurs, according to their own legends, have inhabited the island for millions of years, having sought shelter in the underground caverns during the climate changes that caused the extinction of dinosaurs elsewhere on the planet.\n\nThe caverns are referred to as the World Beneath, and many dinosaurs go there to die. According to legends, a half-man-half ceratopsian being named Ogthar climbed down there (where the sun is said to sleep at night) and stole pieces of it (known as \"sunstones\"), using them to create the empire of Poseidos and populating it with metal dinosaurs. However, the sea grew angry with the artificial city and rose to destroy it. It is implied that this is where the Atlantis myth originated.\nThe human population consists of shipwrecked travelers called \"Dolphinbacks\" (who are often rescued and brought to shore by \"Kentriodon\") and the descendants of such arrivals. As such, they often fall into cultural zones based on the societies from whence their ancestors came, creating a cultural landscape across the island that is both unified and incredibly diverse.\n\nDinosaurs are not the only prehistoric creatures on the island. In the higher regions of the Forbidden Mountains (a Himalaya-like mountain chain), woolly mammoths, ground sloths, chalicotheres, saber-toothed cats, and other prehistoric mammals can be found. Basal synapsids are present as well; at least one \"Lystrosaurus\" and one \"Edaphosaurus\" can be found in the city of Pooktook.\n\nPterosaurs are also common, especially the \"Quetzalcoatlus skybax\", which serve as steeds for the skybax messenger riders.\n\nThe seas surrounding the island are completely inhabited by prehistoric life such as plesiosaurs and trilobites, as well as an unknown species of dolphin.\n\nThe center of the island is made up of a dense rainforest called the \"Rainy Basin\". This place is where most of the island's large carnivores live, such as \"Giganotosaurus\", \"Tyrannosaurus\", and \"Allosaurus\". They are just as sentient as the other dinosaurs on the island but must be kept apart due to their instinctual need to hunt. They have their own language yet it is very deep and guttural and difficult even for other dinosaurs to speak or understand fluently. The Basin is cut off from the rest of the island by a series of retractable bridges. There are caravans that make passages through the basin and are made up by very large sauropod dinosaurs outfitted with heavy suits of armor studded with sharp spikes and carry offerings of meat to appease any carnivores they meet. Often, a dinosaur that senses it will die soon will make a pilgrimage to the basin and die there so that it may be eaten by the carnivores and continue to contribute to the good of the island as a whole. The carnivores are actually very respectful of this and always wait for the dinosaur to pass peacefully, never attacking them while alive. The basin is divided by packs of predators, one ruled by Giganotosaurs, the other ruled by Tyrannosaurs.\n\nThe Great Desert is a desert in the east that stretches from Chandara to Pteros. The gorges of the Amu River is where Canyon City is built.\n\nThe northern part of Dinotopia has the Backbone Mountains which runs across the eastern and western parts of Dinotopia where it forms the northern border of the Rainy Basin and ending at Crackshell Point. Many large mammalian species reside here. In addition, there is the Northern Plains at its northern coast.\n\nThe southern part of Dinotopia has Blackwood Flats, a lowland area where its visitors are targets of biting insects, poisonous snakes, a tiger-striped Allosaurus, and a red-faced Tyrannosaurus.\n\nBoth the eastern and western halves of the dinotopian society share responsibility equally and live under a common set of laws known as the Code of Dinotopia. The society is highly communal, lacking a monetary system or even a concrete concept of ownership. Individuals are educated from youth to be compassionate, co-operative, and generally conscious of others' needs. For example, food on the island is provided at no cost, but citizens take only what they need and leave the rest for others. Stealing or other crime is virtually non-existent.\n\n\n\"Dinotopia\" began as an illustrated children's book called \"Dinotopia: A Land Apart from Time\". It was a cross-over success, appealing to both children and adult readers, which led James Gurney to write and illustrate three more books called \"Dinotopia: The World Beneath,\" \"Dinotopia: First Flight\" and \"Dinotopia: Journey to Chandara\". They all deal with the adventures of Arthur and Will Denison to one degree or another. These are considered the main books of the series and establish the fictional world in which the others are set. Gurney keeps abreast with recent paleontological discoveries and has written then-newly discovered dinosaurs into his books, for example, including \"Giganotosaurus\" in \"The World Beneath\" and \"Microraptor\" in \"Journey to Chandara\"\n\nA children's flip-up version of the first book was also issued.\n\nThe \"Dinotopia Digest\" series consists of sixteen young adult novels penned by several different authors. These books feature other characters who are not specifically involved with the events of the main books, although characters from the main books (particularly the Denisons) have appeared in minor or cameo roles.\n\nTwo full-length adult fantasy novels were also issued with Gurney's authority, written by Alan Dean Foster: \"Dinotopia Lost\" and \"The Hand of Dinotopia\".\n\nSeveral video games, as well as a TV mini-series, a short-lived TV series, and an animated children's movie, were also produced. These are also set in the \"Dinotopia\" universe, but do not tie in directly with the main series. Most of them take place in the modern world, unlike the books, which are mostly set in the mid-19th century.\n\nThe plot of the main \"Dinotopia\" books concerns Arthur Denison and his son, Will, and the various people they meet in their travels in Dinotopia. In the fashion of authors such as Edgar Rice Burroughs, the first and fourth books are written as if they were Arthur's journals, with Gurney going so far as to explain in the introductions how he happened to come across the old, waterlogged volumes.\n\nIn \"Dinotopia: A Land Apart from Time\" (1992), the Denisons are shipwrecked near Dinotopia and, after making it ashore, are found by the people of the Hatchery. The Hatchery is a place where dinosaurs are born, tended by both dinosaurs and humans. The Denisons then set out to explore the island, hoping to find a means of returning to their old lives.\n\nArthur and Will undergo a broad journey, circling the island, as they endeavor to learn the customs and culture of their new neighbors. Arthur in particular develops an interest in the scientific accomplishments of the natives, which far exceed that of any human culture. Among the subjects he studies are the flora of the island, the partnership of its inhabitants, and the existence of a place known as the World Beneath. This World Beneath is an explanation for Dinotopians surviving the saurian extinction; according to the story, most of the Earth's dinosaurs were destroyed, whilst a few hid in vast underground caverns. These few became the original Dinotopians. No one has entered the World Beneath for centuries, but Arthur intends to do so.\n\nHis son Will, on the other hand, has chosen to train as a messenger of the sky; a Skybax rider, who lives in symbiosis with his mount, the great \"Quetzalcoatlus\" (nicknamed Skybax), a species of pterosaur. Training alongside Will is a girl called Sylvia, with whom Will falls in love. The natives refer to this and any other profound bond as \"Cumspiritik\", which literally means \"together-breathing.\" (Romana Denison of the later Dinotopia film series is said to be Will's daughter.)\n\nArthur, for his part, travels into the World Beneath, at the same time that Will and Sylvia are learning to fly with the Skybax. When he returns, he is fascinated by the ancient relics found there and is convinced that they may be key in enabling him to leave or explore the island.\n\nMeanwhile, Will and Sylvia learn and master Skybax flight. When at last they have been accepted as Riders, they travel to meet Arthur and his \"Protoceratops\" guide Bix, but are distracted on the way by a thunderstorm. Luckily, they survive and arrive on time to meet their kin. Will is at the time too young to marry Sylvia, but it is promised that they will. Arthur recognizes that his son has grown up, and they each accept the changes that are results of their new lives on the island.\n\n\nThe first sequel, \"Dinotopia: The World Beneath\" (1995) focuses mainly on Arthur Denison's return expedition to the World Beneath and opens with Will fly testing an invention of his father, the Dragoncopter – a steam engine ornithopter modeled on the dragonfly. The Dragoncopter fails and Will is narrowly saved by Cirrus, his Skybax mount, before the Dragoncopter plummets into a waterfall.\n\nAfter returning from his first expedition in \"A Land Apart From Time\", Arthur presents two items he discovered – a sunstone and half of a key – to the council at Waterfall City in an attempt to get a second expedition into the World Beneath.\n\nA musician named Oriana Nascava comes forward with the missing half of Arthur's key, claiming it to be a family heirloom. She is only willing to give it up if she is allowed to accompany Arthur in his expedition, a term that he reluctantly accepts. Together with Bix as a guide and the scandalous Lee Crabb, the group travels to the shady Pliosaur Canal where they board a submersible in order to take an underwater route to the World Beneath.\n\nMeanwhile, Will and Sylvia have been assigned to accompany a sauropod caravan through the Rainy Basin and keep a watch for predatory \"Tyrannosaurus\". However, Cirrus flies Will to ancient ruins in the jungle of which the \"Tyrannosaurus\" are strangely protective.\n\nArthur, Oriana, Bix, and Lee continue to explore the caverns underneath Dinotopia where they come across instantly germinating fern spores, uncut sunstones that appear to store ancestral memory, and mechanical limbs that twitch when the sunstone is brought near. Eventually, they reach an enormous man-made chamber filled with abandoned walking vehicles modelled after prehistoric animals, left behind by the ancient civilization of Poseidos, which they nickname \"Strutters\". Arthur, Oriana, and Bix commandeer a ceratopsian strutter while Crabb takes a strutter modeled after a sea scorpion and they both climb out of the World Beneath, ending up in the Rainy Basin. They join the sauropod convoy, but are attacked by a pack of \"Tyrannosaurus\" and \"Allosaurus\", during which Crabb escapes in his strutter and the head of the ceratopsian strutter is ripped off.\n\nAfter escaping the carnivores, Arthur realizes that the \"Tyrannosaurus\" at the ruins may have been guarding the mythical ruby sunstone, and takes his strutter back into the Rainy Basin with Oriana and Bix to discover it. Along the way, they come across a trapped juvenile \"Giganotosaurus\" and free it. The grateful father, named Stinktooth, protects Arthur and his companions from the tyrannosaurs and allows them passage into the ruins.\n\nInside the temple, Bix reveals that in the past, people have escaped the island and brought with them culture from Dinotopian civilizations, influencing ancient Egyptian and Greek civilizations.\n\nHowever, they are too late, as Crabb has arrived first and taken the ruby sunstone. Vowing to escape Dinotopia and bring back an army of strutters to plunder the island, he destroys Arthur's strutter with his sea scorpion and escapes. Riding on top of Stinktooth, Arthur chases Lee into the sea and pulls the sunstone out of the power socket in the strutter before Lee can escape. During this chase, Arthur's journal is lost to the ocean where it will be discovered by Philippine sailors and eventually make its way to the library where James Gurney discovers it.\n\nAt the end, the ruby sunstone is lost, a new romance is suggested between Arthur and Oriana, and Crabb is placed under guard by a pair of \"Stygimoloch\".\n\n\n\"Dinotopia: First Flight\" (1999) was a prequel published by Gurney and included a board game.\n\nThe main protagonist of the story is Gideon Altaire, a flight school student living in the capital city of Poseidos off the Dinotopian mainland, in which all organic life (save for humans) has been replaced by mechanical counterparts. After discovering an injured \"Scaphognathus\" named Razzamult, Gideon discovers that the city is planning to launch an attack on the mainland and conquer all of Dinotopia and that they have stolen the ruby sunstone from the pterosaur home of Highnest.\n\nGideon sneaks into a factory and discovers an enormous air scorpion attack strutter under construction. He locates and steals the ruby sunstone and frees a group of captive pterosaurs before escaping to the mainland in a police skimmer. He arrives only to find the island already under attack. He discovers and enlists the help of a band of indigenous creatures- Binny, a \"Necrolemur\", Bandy, a \"Plesictis\", Bongo, a \"Plesiadapis\", and Budge, an \"Estemmenosuchus\". During their trek towards Highnest, they are ambushed by a spider like attack strutter which proceeds to steal the ruby sunstone.\n\nGideon and his band reach Highnest, where they help the pterosaurs evacuate the eggs, then take to the air atop their pterosaurs to engage the air scorpion. During the battle, Gideon manages to pull the ruby sunstone out of the flying machine's power socket, causing it to crash and stopping the invasion of Dinotopia.\n\nGideon is presented as the first ever Skybax rider, although the species he rode wasn't a \"Quetzalcoatlus northropi\".\n\n\nA fourth Dinotopia book by James Gurney, \"Dinotopia: Journey to Chandara\", was published in October 2007. In it, Hugo Khan, the mysterious and reclusive emperor of Chandara, an empire long since isolated from the rest of Dinotopia, has heard of Arthur Denison and Bix's exploits and sends them a personal invitation to his court. Along the way, the duo encounter several new locals, including an old musician named Cornelius Mazurka and his companion \"Therizinosaurus\" Henriette in the ruins of an old city, a town called Bilgewater made completely out of salvaged ships that the inhabitants believe will carry them into another world, and Jorotongo, a consistently festive and completely nomadic village composed of pilgrims from the \"Sunflower\", sister ship to the \"Mayflower\".\n\nEventually, they meet Lee Crabb en route at Sauropolis, who escapes from his \"Stygimoloch\" guards and steals the invitation. Without proper passes for the border guards, Arthur and Bix are forced to sneak through the swamp of Blackwood Flats while evading packs of carnivorous \"Allosaurus\". After passing through the mountain city of Thermala, the duo encounter Neighbor Dooh, a bandit who steals all the possessions of passing travelers and compensates them with the possessions of the previous victim. Although Arthur loses all of his scientific equipment, he is given a set of desert robes which allow him and Bix to blend into a Chandaran caravan and pass the border without harassment from the guards.\n\nThey stop by the ruins of Ebulon, where Arthur finds Will and Sylvia preparing for an air-jousting tournament. Soon after that, they make their way to the capital city of Chandara. By the time they arrive, they find themselves with few possessions left to barter save for ideas, so Arthur sets up shop in the Marketplace of Ideas. During the night, the writings at Arthur's stand catches the attention of the emperor and he and Bix are invited into the court.\n\nOnce at the court, they discover that Lee Crabb has also entered the court under the guise of Arthur Denison and is attempting to gather up a stockpile of weapons, arguing that he is preparing for a \"Tyrannosaurus\" invasion. Hugo Khan finally reveals himself to be a small \"Microraptor\", and the real Denison promptly exposes Crabb. Khan punishes Crabb by assigning him to be a chef for a band of \"Acrocanthosaurus\" Shaolin-monks, who ate their last chef after he failed to satisfy them.\n\nTo commemorate Arthur and Bix's presence on the court, Hugo Khan flies out during the night to find a child in sorrow. The next day, Arthur, Bix, and a handful of the Emperor's selected entertainers arrive at the house of and greet Rita Rose and Jeffer, an orphaned \"Europasaurus\" hatchling who has lost the ability to walk. At the end of the day, Hugo Khan expresses his wish for Chandara to be reopened culturally to the rest of Dinotopia. Arthur and Bix accept the Emperor's offers to stay in Chandara for a while to fully discover the city and its culture.\n\n\nFrom 1995, James Gurney worked with a number of other authors on a series of short novels for children using the Dinotopia characters and themes, published by Random House:\n\n\n\nTwo full-length adult fantasy novels were also issued with Gurney's authority, written by Alan Dean Foster: \"Dinotopia Lost\" (1996) and \"The Hand of Dinotopia\" (1999).\n\nA 2002 four-hour TV mini-series produced by Hallmark Entertainment was also based on James Gurney's work, and was advertised as the first \"mega-series.\" The show featured new characters such as Zippo (changed to Zippeau for the TV series to avoid legal issues with the lighter maker Zippo), a troodon who is said to have worked with Sylvia; the sunstones, a technology restricted to the lost city of Poseidos in the books, are commonplace in the miniseries. The failure both of the sunstones and of Dinotopian officials to adhere to the underlying meanings of their culture's philosophy caused several discontented people – a leader-in-training, Zippeau himself, and two twentieth-century Dolphinbacks, Karl and David – to embark on a quest that led ultimately to the World Beneath. The series is presented as a sequel of sorts to the books: Will Denison's daughter followed her father into the Skybax corps (an order acknowledged to be founded by Gideon Altaire), Oriana's granddaughter is the female protagonist, the character Zippo is said to have been the dinosaur partner of Sylvia (here the Nursery overseer and not a Skybax rider), and Lee Crabb's son Cyrus features as the antagonist. The mini-series won an Emmy for its special effects.\n\nIn the world of Dinotopia, the Code of Dinotopia is an ancient stone tablet citing the rules of living in Dinotopia. As the part of the tablet cracked since it was first carved, the last portion of it had been missing, but was recovered in the TV mini-series \"Dinotopia\". The code itself is as follows:\n\nThe recovered first letters of each sentence spell out \"SOW GOOD SEED\". It was first written in the Dinotopian alphabet. In the TV mini-series, the 11th code began with \"Fin...\" and was found to be \"Find the light\".\n\nIn the original book by James Gurney, the last line of the code was a small joke amongst the historians and the librarian, who seemed to think it could have been \"Don't pee in the bath\" referencing the Saurian community's distaste with some humans' lack of cleanliness and hygiene. This was never elaborated on further or confirmed to be true though. Another theory is \"Don't put off work for tomorrow that can be done today\".\n\nA TV series of thirteen episodes was produced later in 2002 as a result of the success of the mini-series, but none of the cast of the mini-series reprised their roles. In the later TV series, a group of people known as Outsiders live outside the laws of Dinotopia and pose an additional danger aside from the featured antagonists, which include \"Pteranodon\", \"Tyrannosaurus\", and \"Postosuchus\".\n\nABC originally planned to launch the series in September 2002, but decided to wait until Thanksgiving. ABC was somewhat disappointed by the initial 5.7 million viewers and the poor ratings, but continued to air the series for a little while longer, pointing out that it had been an \"odd viewing night overall.\" The series was finally canceled in December. Only six of the thirteen episodes were aired on ABC, but all thirteen were broadcast the following year in Europe and were released onto a three-disc DVD box set.\n\nScience-fiction veteran David Winning directed two episodes of the series, and location shooting lasted for three months near Budapest, Hungary. Georgina Rylance played Marion Waldo, and Lisa Zane portrayed her old friend LeSage, the leader of the Outsiders. Michael Brandon, Jonathan Hyde, and Erik von Detten also star in the series.\n\nArtisan Entertainment released the complete series on DVD in Region 1 for the first time on 20 January 2004. This release has been discontinued and is out of print. On 15 March 2016, Mill Creek Entertainment re-released the complete series on DVD in Region 1.\n\nThere is also a 2005 traditionally animated movie called \"\". This film deviated from the original books even more than the mini-series by featuring Ogthar, a mythical ruler of the World Beneath (mentioned in the mini-series), as a human warlord rather than a benevolent, if commanding, emperor.\n\nA number of \"Dinotopia\" video games have been produced:\n\n"}
{"id": "6530978", "url": "https://en.wikipedia.org/wiki?curid=6530978", "title": "Double-aspect theory", "text": "Double-aspect theory\n\nIn the philosophy of mind, double-aspect theory is the view that the mental and the physical are two aspects of, or perspectives on, the same substance. It is also called dual-aspect monism. The theory's relationship to neutral monism is ill-defined, but one proffered distinction says that whereas neutral monism allows the context of a given group of neutral elements to determine whether the group is mental, physical, both, or neither, double-aspect theory requires the mental and the physical to be inseparable and mutually irreducible (though distinct).\nDouble-aspect theorists include, among others:-\n\n\nFrom the work of Wolfgang Pauli and Carl G. Jung results a philosophical approach, which Harald Atmanspacher titles the \"Pauli-Jung conjecture\", of dual-aspect monism which has a very specific further feature, namely that different aspects may show a complementarity in a quantum physical sense. That is, the Pauli-Jung conjecture implies that with regard to mental and physical states there may be incompatible descriptions of different parts that emerge from the whole. This stands in close analogy to quantum physics, where complementary properties cannot be determined jointly with accuracy.\n\nAtmanspacher further refers to Paul Bernays' views on complementarity in physics and in philosophy when he states that \"Two descriptions are complementary if they mutually exclude each other, yet are both necessary to describe a situation exhaustively.\"\n\n\n"}
{"id": "20227676", "url": "https://en.wikipedia.org/wiki?curid=20227676", "title": "Expertise finding", "text": "Expertise finding\n\nExpertise finding is the use of tools for finding and assessing individual expertise. In the recruitment industry, e.g., LinkedIn, expertise finding is the problem of searching for employable candidates with certain required skills set.\n\nIt can be argued that human expertise is more valuable than capital, means of production or intellectual property. Contrary to expertise, all other aspects of capitalism are now relatively generic: access to capital is global, as is access to means of production for many areas of manufacturing. Intellectual property can be similarly licensed. Furthermore, expertise finding is also a key aspect of institutional memory, as without its experts an institution is effectively decapitated. However, finding and \"licensing\" expertise, the key to the effective use of these resources, remain much harder, starting with the very first step: finding expertise that you can trust.\n\nUntil very recently, finding expertise required a mix of individual, social and collaborative practices, a haphazard process at best. Mostly, it involved contacting individuals one trusts and asking them for referrals, while hoping that one's judgment about those individuals is justified and that their answers are thoughtful.\n\nIn the last fifteen years, a class of knowledge management software has emerged to facilitate and improve the quality of expertise finding, termed \"expertise locating systems\". These software range from social networking systems to knowledge bases. Some software, like those in the social networking realm, rely on users to connect each other, thus using social filtering to act as \"recommender systems\".\n\nAt the other end of the spectrum are specialized knowledge bases that rely on experts to populate a specialized type of database with their self-determined areas of expertise and contributions, and do not rely on user recommendations. Hybrids that feature expert-populated content in conjunction with user recommendations also exist, and are arguably more valuable for doing so.\n\nStill other expertise knowledge bases rely strictly on external manifestations of expertise, herein termed \"gated objects\", e.g., citation impacts for scientific papers or data mining approaches wherein many of the work products of an expert are collated. Such systems are more likely to be free of user-introduced biases (e.g., ResearchScorecard ), though the use of computational methods can introduce other biases.\n\nThere are also hybrid approaches which use user-generated data (e.g., member profiles), community-based signals (e.g., recommendations and skill endorsements), and personalized signals (e.g., social connection between searcher and results), as at LinkedIn.\n\nExamples of the systems outlined above are listed in Table 1.\n\nTable 1: A classification of expertise location systems\n\nA number of interesting problems follow from the use of expertise finding systems:\n\n\nMeans of classifying and ranking expertise (and therefore experts) become essential if the number of experts returned by a query is greater than a handful. This raises the following social problems associated with such systems:\n\n\nMany types of data sources have been used to infer expertise. They can be broadly categorized based on whether they measure \"raw\" contributions provided by the expert, or whether some sort of filter is applied to these contributions.\n\nUnfiltered data sources that have been used to assess expertise, in no particular ranking order:\n\n\nFiltered data sources, that is, contributions that require approval by third parties (grant committees, referees, patent office, etc.) are particularly valuable for measuring expertise in a way that minimizes biases that follow from popularity or other social factors:\n\n\n\nIn no particular order...\n\n\n"}
{"id": "142867", "url": "https://en.wikipedia.org/wiki?curid=142867", "title": "Ferret", "text": "Ferret\n\nThe ferret (\"Mustela putorius furo\") is the domesticated form of the European polecat, a mammal belonging to the same genus as the weasel, \"Mustela\" of the family Mustelidae. They typically have brown, black, white, or mixed fur. They have an average length of including a tail, weigh about , and have a natural lifespan of 7 to 10 years. Ferrets are sexually dimorphic predators with males being substantially larger than females.\n\nSeveral other Mustelids also have the word \"ferret\" in their common names, including an endangered species, the black-footed ferret.\n\nThe history of the ferret's domestication is uncertain, like that of most other domestic animals, but it is likely that ferrets have been domesticated for at least 2,500 years. They are still used for hunting rabbits in some parts of the world, but increasingly they are kept only as pets.\n\nBeing so closely related to polecats, ferrets easily hybridize with them, and this has occasionally resulted in feral colonies of polecat-ferret hybrids that have caused damage to native fauna, especially in New Zealand. As a result, some parts of the world have imposed restrictions on the keeping of ferrets.\n\nThe name \"ferret\" is derived from the Latin \"furittus\", meaning \"little thief\", a likely reference to the common ferret penchant for secreting away small items. The Greek word \"ictis\" occurs in a play written by Aristophanes, \"The Acharnians\", in 425 BC. Whether this was a reference to ferrets, polecats, or the similar Egyptian mongoose is uncertain.\n\nFerrets have a typical Mustelid body-shape, being long and slender. Their average length is about 50 cm including a 13-cm tail. Their pelage has various colorations including brown, black, white or mixed. They weigh between 0.7 kg and 2.0 kg and are sexually dimorphic as the males are substantially larger than females. The average gestation period is 42 days and females may have two or three litters each year. The litter size is usually between three and seven kits which are weaned after three to six weeks and become independent at three months. They become sexually mature at approximately six months and the average life span is seven to 10 years.\n\nFerrets spend 14–18 hours a day asleep and are most active around the hours of dawn and dusk, meaning they are crepuscular. Unlike their polecat ancestors, which are solitary animals, most ferrets will live happily in social groups. A group of ferrets is commonly referred to as a \"business\". They are territorial, like to burrow, and prefer to sleep in an enclosed area.\n\nLike many other mustelids, ferrets have scent glands near their anus, the secretions from which are used in scent marking. Ferrets can recognize individuals from these anal gland secretions, as well as the sex of unfamiliar individuals. Ferrets may also use urine marking for sex and individual recognition.\n\nAs with skunks, ferrets can release their anal gland secretions when startled or scared, but the smell is much less potent and dissipates rapidly. Most pet ferrets in the US are sold de-scented (anal glands removed). In many other parts of the world, including the UK and other European countries, de-scenting is considered an unnecessary mutilation.\n\nIf excited, they may perform a behavior called the \"weasel war dance\", characterized by frenzied sideways hops, leaps and bumping into nearby objects. Despite its common name, it is not aggressive but is a joyful invitation to play. It is often accompanied by a unique soft clucking noise, commonly referred to as \"dooking\". When scared, ferrets will hiss; when upset, they squeak softly.\n\nFerrets are obligate carnivores. The natural diet of their wild ancestors consisted of whole small prey, including meat, organs, bones, skin, feathers, and fur. Ferrets have short digestive systems and quick metabolism, so they need to eat frequently. Prepared dry foods consisting almost entirely of meat (including high-grade cat food, although specialized ferret food is increasingly available and preferable) provide the most nutritional value and are the most convenient, though some ferret owners feed pre-killed or live prey (such as mice and rabbits) to their ferrets to more closely mimic their natural diet. Ferret digestive tracts lack a cecum and the animal is largely unable to digest plant matter. Before much was known about ferret physiology, many breeders and pet stores recommended food like fruit in the ferret diet, but it is now known that such foods are inappropriate, and may in fact have negative ramifications on ferret health. Ferrets imprint on their food at around six months old. This can make introducing new foods to an older ferret a challenge, and even simply changing brands of kibble may meet with resistance from a ferret that has never eaten the food as a kit. It is therefore advisable to expose young ferrets to as many different types and flavors of appropriate food as possible.\n\nFerrets have four types of teeth (the number includes maxillary (upper) and mandibular (lower) teeth) with a dental formula of 3/3 1/1 4/4 1/2:\n\nFerrets are known to suffer from several distinct health problems. Among the most common are cancers affecting the adrenal glands, pancreas, and lymphatic system. Viral diseases include canine distemper and influenza. Health problems can occur in unspayed females when not being used for breeding. Certain health problems have also been linked to ferrets being neutered before reaching sexual maturity. Certain colors of ferret may also carry a genetic defect known as Waardenburg syndrome. Similar to domestic cats, ferrets can also suffer from hairballs and dental problems.\n\nIn common with most domestic animals, the original reason for ferrets being domesticated by human beings is uncertain, but it may have involved hunting. According to phylogenetic studies, the ferret was domesticated from the European polecat (\"Mustela putorius\"), and likely descends from a North African lineage of the species. Analysis of mitochondrial DNA suggests that ferrets were domesticated around 2,500 years ago. It has been claimed that the ancient Egyptians were the first to domesticate ferrets, but as no mummified remains of a ferret have yet been found, nor any hieroglyph of a ferret, and no polecat now occurs wild in the area, that idea seems unlikely.\n\nFerrets were probably used by the Romans for hunting.\n\nColonies of feral ferrets have established themselves in areas where there is no competition from similarly sized predators, such as in the Shetland Islands and in remote regions in New Zealand. Where ferrets coexist with polecats, hybridization is common. It has been claimed that New Zealand has the world's largest feral population of ferret-polecat hybrids. In 1877, farmers in New Zealand demanded that ferrets be introduced into the country to control the rabbit population, which was also introduced by humans. Five ferrets were imported in 1879, and in 1882–1883, 32 shipments of ferrets were made from London, totaling 1,217 animals. Only 678 landed, and 198 were sent from Melbourne, Australia. On the voyage, the ferrets were mated with the European polecat, creating a number of hybrids that were capable of surviving in the wild. In 1884 and 1886, close to 4,000 ferrets and ferret hybrids, 3,099 weasels and 137 stoats were turned loose. Concern was raised that these animals would eventually prey on indigenous wildlife once rabbit populations dropped, and this is exactly what happened to New Zealand's bird species which previously had had no mammalian predators.\n\nFor millennia, the main use of ferrets was for hunting, or \"ferreting\". With their long, lean build, and inquisitive nature, ferrets are very well equipped for getting down holes and chasing rodents, rabbits and moles out of their burrows. Caesar Augustus sent ferrets or mongooses (named \"viverrae\" by Plinius) to the Balearic Islands to control the rabbit plagues in 6 BC. In England, in 1390, a law was enacted restricting the use of ferrets for hunting to the relatively wealthy:\n\nIn the United States, ferrets were relatively rare pets until the 1980s. A government study by the California State Bird and Mammal Conservation Program estimated that by 1996 about 800,000 domestic ferrets were being kept as pets in the United States.\n\nLike many household pets, ferrets require a cage. For ferrets, a wire cage at least 18 inches long and deep and 30 inches wide or longer is needed. Ferrets cannot be housed in environments such as an aquarium because of the poor ventilation. It is preferable that the cage have more than one level but this is not crucial. Usually two to three different shelves are used.\n\n\nFerrets are an important experimental animal model for human influenza, and have been used to study the 2009 H1N1 (swine flu) virus. Smith, Andrews, Laidlaw (1933) inoculated ferrets intra-nasally with human naso-pharyngeal washes, which produced a form of influenza that spread to other cage mates. The human influenza virus (Influenza type A) was transmitted from an infected ferret to a junior investigator, from whom it was subsequently re-isolated.\n\nIn the Harry Potter series by J.K.Rowling, Draco Malfoy is turned into a ferret in their 4th year by Mad-eye Moody(Barty Crouch Jr.)\n\n Male ferrets are called hobs; female ferrets are jills. A spayed female is a sprite, a neutered male is a gib, and a vasectomised male is known as a hoblet. Ferrets under one year old are known as kits. A group of ferrets is known as a \"business\", or historically as a \"busyness\". Other purported collective nouns, including \"besyness\", \"fesynes\", \"fesnyng\", and \"feamyng\", appear in some dictionaries, but are almost certainly ghost words.\n\nMost ferrets are either albinos, with white fur and pink eyes, or display the typical dark masked sable coloration of their wild polecat ancestors. In recent years fancy breeders have produced a wide variety of colors and patterns. Color refers to the color of the ferret's guard hairs, undercoat, eyes, and nose; pattern refers to the concentration and distribution of color on the body, mask, and nose, as well as white markings on the head or feet when present. Some national organizations, such as the American Ferret Association, have attempted to classify these variations in their showing standards.\n\nThere are four basic colors. The sable (including chocolate and dark brown), albino, dark eyed white (DEW), and the silver. All the other colors of a ferret are variations on one of these four categories.\n\nFerrets with a white stripe on their face or a fully white head, primarily blazes, badgers, and pandas, almost certainly carry a congenital defect which shares some similarities to Waardenburg syndrome. This causes, among other things, a cranial deformation in the womb which broadens the skull, white face markings, and also partial or total deafness. It is estimated as many as 75 percent of ferrets with these Waardenburg-like colorings are deaf.\n\nWhite ferrets were favored in the Middle Ages for the ease in seeing them in thick undergrowth. Leonardo da Vinci's painting \"Lady with an Ermine\" is likely mislabelled; the animal is probably a ferret, not a stoat, (for which \"ermine\" is an alternative name for the animal in its white winter coat). Similarly, the ermine portrait of Queen Elizabeth the First shows her with her pet ferret, which has been decorated with painted-on heraldic ermine spots.\n\n\"The Ferreter's Tapestry\" is a 15th-century tapestry from Burgundy, France, now part of the Burrell Collection housed in the Glasgow Museum and Art Galleries. It shows a group of peasants hunting rabbits with nets and white ferrets. This image was reproduced in \"Renaissance Dress in Italy 1400–1500\", by Jacqueline Herald, Bell & Hyman.\n\n\"Gaston Phoebus' Book of the Hunt\" was written in approximately 1389 to explain how to hunt different kinds of animals, including how to use ferrets to hunt rabbits. Illustrations show how multicolored ferrets that were fitted with muzzles were used to chase rabbits out of their warrens and into waiting nets.\n\n\n\n\n"}
{"id": "5250937", "url": "https://en.wikipedia.org/wiki?curid=5250937", "title": "Freedom in the World", "text": "Freedom in the World\n\nFreedom in the World is a yearly survey and report by the U.S.-based non-governmental organization Freedom House that measures the degree of civil liberties and political rights in every nation and significant related and disputed territories around the world.\n\n\"Freedom in the World\" was launched in 1973 by Raymond Gastil. It produces annual scores representing the levels of political rights and civil liberties in each state and territory, on a scale from 1 (most free) to 7 (least free). Depending on the ratings, the nations are then classified as \"Free\", \"Part(ial)ly Free\", or \"Not Free\". The report is often used by researchers in order to measure democracy and correlates highly with several other measures of democracy such as the Polity data series.\n\nThe Freedom House rankings are widely reported in the media and used as sources by political researchers. Their construction and use has been evaluated by critics and supporters.\n\nThe rankings below are from the \"Freedom in the World\" 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017 and 2018 surveys, each reflecting findings covering the previous year. Each pair of political rights and civil liberties ratings is averaged to determine an overall status of \"Free\" (1.0–2.5), \"Part(ial)ly Free\" (3.0–5.0), or \"Not Free\" (5.5–7.0).\n\nAn asterisk (*) indicates countries which are \"electoral democracies\". To qualify as an \"electoral democracy\", a state must have satisfied the following criteria:\n\n\nAn electoral democracy must have a score of 7 or more out of 12 in political rights subcategory A (Electoral Progress), an overall aggregate score of 20 in their political rights rating and an overall aggregate score of 30 in their civil liberties rating.\n\nFreedom House's term \"electoral democracy\" differs from \"liberal democracy\" in that the latter also implies the presence of a substantial array of civil liberties. In the survey, all Free countries qualify as both electoral and liberal democracies. By contrast, some Partly Free countries qualify as electoral, but not liberal, democracies.\n\nNote: The Middle East countries of Turkey, Cyprus, Georgia, Azerbaijan and Armenia can be found in the \"Europe\" and \"Eurasia\" sections of Freedom House's \"Freedom in the World\" report.\n\nAccording to Freedom House, a quarter of all declines of freedom in the world in 2016 took place in Europe.\n\nPercentage of countries in each category, from the 1973 through 2014 reports:\n\nSources: Country Status and ratings overview 1973–2016, Number and percentages of electoral democracies 1989–2016, \"Freedom in the World 2018\" report covering 2017.\n\nNotes:\n\nThere is some debate over the neutrality of Freedom House and the methodology used for the \"Freedom in the World\" report, which has been written by Raymond D. Gastil and his colleagues. The neutrality and biases of human-rights indices have been discussed in several publications by Kenneth A. Bollen. Bollen wrote that \"Considered together these criticisms suggest that some nations may have been incorrectly rated on Gastil's measures. However, none of the criticisms have demonstrated a systematic bias in all the ratings. Most of the evidence consists of anecdotal evidence of relatively few cases. Whether there is a systematic or sporadic slant in Gastil's ratings is an open question\" (Bollen, 1986, p. 586). The freedom index of \"Freedom in the World\" has a very strong and positive (at least an 80%) correlation with three other democracy-indices studied in Mainwaring (2001, p. 53).\n\nIn his 1986 study, Bollen discussed reviews of measurements of human rights, including the index reported in \"Freedom in the World\" (Bollen, 1986, p. 585). Criticisms of \"Freedom in the World\" during the 1980s were discussed by Gastil (1990), who stated that \"generally such criticism is based on opinions about Freedom House rather than detailed examination of survey ratings\", a conclusion disputed by Giannone. The definition of Freedom in Gastil (1982) and Freedom House (1990) emphasized liberties rather than the exercise of freedom, according to Adam Przeworski, who gave the following example: In the United States, citizens are free to form political parties and to vote, yet even in presidential elections only half of U.S. \"citizens\" vote; in the U.S., \"the same two parties speak in a commercially sponsored unison\", wrote .\n\nMore recent charges of ideological bias prompted Freedom House to issue this 2010 statement:\nFreedom House does not maintain a culture-bound view of freedom. The methodology of the survey is grounded in basic standards of political rights and civil liberties, derived in large measure from relevant portions of the Universal Declaration of Human Rights. These standards apply to all countries and territories, irrespective of geographical location, ethnic or religious composition, or level of economic development.\nMainwaring et alia (2001, p. 52) wrote that Freedom House's index had \"two systematic biases: scores for leftist were tainted by political considerations, and changes in scores are sometimes driven by changes in their criteria rather than changes in real conditions.\" Nonetheless, when evaluated in Latin American countries yearly, Freedom House's index was very strongly and positively correlated with the index of Adam Przeworski and with the index of the authors themselves: They evaluated Pearson's coefficient of linear correlation between their index and Freedom House's index, which was 0.82; among these indices and the two others studied, the correlations were all between 0.80 and 0.86 (Mainwaring et alia, 2001, p. 53).\n\nAs previously quoted, Bollen criticized previous studies of \"Freedom in the World\" as anecdotal and inconclusive; they raised issues needing further study by scientific methods rather than anecdotes. Bollen studied the question of ideological bias using multivariate statistics. Using their factor-analytic model for human-rights measurements, Bollen and Paxton estimate that Gastil's method produces a bias of -0.38 standard deviations (s.d.) against Marxist–Leninist countries and a larger bias, +0.5 s.d., favoring Christian countries; similar results held for the methodology of Sussman (Bollen and Paxton, 2000, p. 585). In contrast, another method by a critic of \"Freedom in the World\" produced a bias for Leftist countries during the 1980s of at least +0.8 s.d., a bias that is \"consistent with the general finding that political scientists are more favorable to leftist politics than is the general population\" (Bollen and Paxton, p. 585).\n\nCriticisms of the reception and uses of the \"Freedom in the World\" report have been noted by Diego Giannone:\n\nIn \"Political and ideological aspects in the measurement of democracy: the Freedom House case\" (2010) which reviewed changes to the methodology since 1990, Diego Giannone concluded that \"because of the changes in methodology over time and the strict interconnection between methodological and political aspects, the FH data do not offer an unbroken and politically neutral time series, such that they should not be used for cross-time analyses even for the development of first hypotheses. The internal consistency of the data series is open to question.\"\n\nOn this topic, the Freedom House website replies that they have \"made a number of modest methodological changes to adapt to evolving ideas about political rights and civil liberties. At the same time, the time series data are not revised retroactively, and any changes to the methodology are introduced incrementally in order to ensure the comparability of the ratings from year to year.\"\n\n\n\n"}
{"id": "44402565", "url": "https://en.wikipedia.org/wiki?curid=44402565", "title": "Global cascades model", "text": "Global cascades model\n\nGlobal cascades models are a class of models aiming to model large and rare cascades that are triggered by exogenous perturbations which are relatively small compared with the size of the system. The phenomenon occurs ubiquitously in various systems, like information cascades in social systems, stock market crashes in economic systems, and cascading failure in physics infrastructure networks. The models capture some essential properties of such phenomenon.\n\nTo describe and understand global cascades, a network-based threshold model has been proposed by Duncan J. Watts in 2002. The model is motivated by considering a population of individuals who must make a decision between two alternatives, and their choices depend explicitly on other people's states or choices. The model assumes that an individual will adopt a new particular opinion (product or state) if a threshold fraction of his/her neighbors have adopted the new one, else he would keep his original state. To initiate the model, a new opinion will be randomly distributed among a small fraction of individuals in the network. If the fraction satisfies a particular condition, a large cascades can be triggered.(see Global Cascades Condition) A phase transition phenomenon has been observed: when the network of interpersonal influences is sparse, the size of the cascades exhibits a power law distribution, the most highly connected nodes are critical in triggering cascades, and if the network is relatively dense, the distribution shows a bimodal form, in which nodes with average degree show more importance by serving as triggers.\n\nSeveral generalizations of the Watt's threshold model have been proposed and analyzed in the following years. For example, the original model has been combined with independent interaction models to provide a generalized model of social contagion, which classifies the behavior of the system into three universal classes. It has also been generalized on modular networks degree-correlated networks and to networks with tunable clustering. The role of the initiators has also been studied recently, shows that different initiator would influence the size of the cascades.\n\nTo derive the precise cascade condition in the original model, a generating function method could be applied. The generating function for vulnerable nodes in the network is:\n\nwhere \"p\" is the probability a node has degree \"k\", and \nand \"f\" is the distribution of the threshold fraction of individuals. The average vulnerable cluster size can be derived as:\n\nwhere \"z\" is the average degree of the network. The Global cascades occur when the average vulnerable cluster size <\"n\"> diverges\n\nThe equation could be interpreted as: When formula_5, the clusters in the network is small and global cascades will not happen since the early adopters are isolated in the system, thus no enough momentum could be generated. When formula_6, the typical size of the vulnerable cluster is infinite, which implies presence of global cascades.\n\nThe Model considers a change of state of individuals in different systems which belongs to a larger class of contagion problems. However it differs with other models in several aspects: Compared with 1) epidemic model: where contagion events between individual pairs are independent, the effect a single infected node having on an individual depends on the individual's other neighbors in the proposed model. Unlike 2) percolation or self-organized criticality models, the threshold is not expressed as the absolute number of \"infected\" neighbors around an individual, instead, a corresponding fraction of neighbors is selected. It is also different from 3) random-field ising model and majority voter model, which are frequently analyzed on regular lattices, here, however the heterogeneity of the network plays a significant role.\n\n"}
{"id": "235679", "url": "https://en.wikipedia.org/wiki?curid=235679", "title": "Gustav Metzger", "text": "Gustav Metzger\n\nGustav Metzger (10 April 1926, Nuremberg – 1 March 2017, London) was an artist and political activist who developed the concept of Auto-Destructive Art and the Art Strike. \nTogether with John Sharkey, he initiated the Destruction in Art Symposium in 1966. \nMetzger was recognised for his protests in the political and artistic realms.\n\nMetzger was born to Polish Jewish parents in Nuremberg, Germany, in 1926 and came to Britain in 1939 as a refugee under the auspices of the Refugee Children Movement. He was stateless since the late 1940s.<ref name=\"Performing Archives/Archives of Performance\"></ref> He received a grant from the UK Jewish community to study at the Royal Academy of Fine Arts in Antwerp between 1948 and 1949. It is with an experience of twentieth century society's destructive capabilities that led Metzger to a concentrated 'formulation of what destruction is and what it might be in relation to art.'\n\nHis experience of twentieth century society's destructive capabilities led Metzger to a concentrated 'formulation of what destruction is and what it might be in relation to art.' He was known as a leading exponent of the Auto-Destructive Art and the Art Strike movements. He was also active in the Committee of 100 - a 'named' member\n\nIn 1959, Metzger published the first auto-destructive manifesto \"Auto-Destructive Art\". This was given as a lecture to the Architectural Association School of Architecture (AA) in 1964, which was taken over by students as an artistic 'Happening'. The Architectural Association published, in 2015, a facsimile edition of Metzger's lecture transcript. In 1962 he participated in the Festival of Misfits organised by members of the Fluxus group, at Gallery One, London. Guitarist Pete Townshend from The Who studied with Metzger, and during the 1960s, Metzger's work was projected on screens at The Who concerts.\n\nIn 2005, he selected EASTinternational which he proclaimed to be \"The art exhibition without the art.\"\n\nThroughout the 60 years that Metzger produced politically engaged works, he incorporated materials ranging from trash to old newspapers, liquid crystals to industrial materials, and even acid.\"\n\nFrom 29 September to 8 November 2009, the Serpentine Gallery featured the most extensive exhibition in the UK of his work. Exhibits included the installation \"Flailing Trees\", 15 upturned willow trees embedded in a block of concrete, symbolising a world turned upside down by global warming. He felt that artists are especially threatened, because so many rely on nature as a big inspiration. Metzger stated that \"artists have a special part to play in opposing extinction, if only on a theoretical, intellectual basis.\"\n\nMetzger lived and worked in East London.\n\nThis was originally made in 1960 and remade as \"Recreation of First Public Demonstration of Auto-Destructive Art\" in 2004.\n\n\"Liquid Crystal Environment\" was originally made in 1965 and remade in 2005.\n\nThis ongoing series of work consists of enlarged press photographs of catastrophic events of the 20th century presented to the viewer using confrontational and experiential methods.\n\nThis was a recreation of the original demonstration made in 1960.\nAn integral piece of the installation at the Tate Britain, a bag containing rubbish, was erroneously disposed by a cleaner on 30 June 2004. Metzger declared the piece ruined and created a new bag as a replacement.\n\nOriginally conceived for Manchester Peace Garden and commissioned by Manchester International Festival in 2009, this work consists of uprooted trees inverted into a concrete block in a powerful environmental memorandum of man's destructive capabilities and violation of Nature.\n\nThe painter David Bomberg, the leading light of the Borough Group, taught Metzger and was influential in his development.\n\nMetzger died at the age of 90 at his home in London on 1 March 2017.\n\nWhen Metzger was lecturing at Ealing Art College, one of his students was rock musician Pete Townshend, who later cited Metzger's concepts as an influence for his famous guitar-smashing during performances of The Who. He has also influenced the self-eating computer virus works by the digital artist Joseph Nechvatal.\n\n\n\n"}
{"id": "23364086", "url": "https://en.wikipedia.org/wiki?curid=23364086", "title": "Hertzsprung–Russell diagram", "text": "Hertzsprung–Russell diagram\n\nThe Hertzsprung–Russell diagram, abbreviated as H–R diagram, HR diagram or HRD, is a scatter plot of stars showing the relationship between the stars' absolute magnitudes or luminosities versus their stellar classifications or effective temperatures. More simply, it plots each star on a graph plotting the star's brightness against its temperature (color).\n\nThe diagram was created circa 1910 by Ejnar Hertzsprung and Henry Norris Russell and represents a major step towards an understanding of stellar evolution.\n\nThe related colour–magnitude diagram (CMD) plots the apparent magnitudes of stars against their colour, usually for a cluster so that the stars are all at the same distance.\n\nIn the nineteenth-century large-scale photographic spectroscopic surveys of stars were performed at Harvard College Observatory, producing spectral classifications for tens of thousands of stars, culminating ultimately in the Henry Draper Catalogue. In one segment of this work Antonia Maury included divisions of the stars by the width of their spectral lines. Hertzsprung noted that stars described with narrow lines tended to have smaller proper motions than the others of the same spectral classification. He took this as an indication of greater luminosity for the narrow-line stars, and computed secular parallaxes for several groups of these, allowing him to estimate their absolute magnitude.\n\nIn 1910 Hans Rosenberg published a diagram plotting the apparent magnitude of stars in the Pleiades cluster against the strengths of the calcium K line and two hydrogen Balmer lines. These spectral lines serve as a proxy for the temperature of the star, an early form of spectral classification. The apparent magnitude of stars in the same cluster is equivalent to their absolute magnitude and so this early diagram was effectively a plot of luminosity against temperature. The same type of diagram is still used today as a means of showing the stars in clusters without having to initially know their distance and luminosity. Hertzsprung had already been working with this type of diagram, but his first publications showing it were not until 1911. This was also the form of the diagram using apparent magnitudes of a cluster of stars all at the same distance.\n\nRussell's early (1913) versions of the diagram included Maury's giant stars identified by Hertzsprung, those nearby stars with parallaxes measured at the time, stars from the Hyades (a nearby open cluster), and several moving groups, for which the moving cluster method could be used to derive distances and thereby obtain absolute magnitudes for those stars.\n\nThere are several forms of the Hertzsprung–Russell diagram, and the nomenclature is not very well defined. All forms share the same general layout: stars of greater luminosity are toward the top of the diagram, and stars with higher surface temperature are toward the left side of the diagram.\n\nThe original diagram displayed the spectral type of stars on the horizontal axis and the absolute visual magnitude on the vertical axis. The spectral type is not a numerical quantity, but the sequence of spectral types is a monotonic series that reflects the stellar surface temperature. Modern observational versions of the chart replace spectral type by a color index (in diagrams made in the middle of the 20th Century, most often the B-V color) of the stars. This type of diagram is what is often called an observational Hertzsprung–Russell diagram, or specifically a color–magnitude diagram (CMD), and it is often used by observers. In cases where the stars are known to be at identical distances such as within a star cluster, a color–magnitude diagram is often used to describe the stars of the cluster with a plot in which the vertical axis is the apparent magnitude of the stars. For cluster members, by assumption there is a single additive constant difference between their apparent and absolute magnitudes, called the distance modulus, for all of that cluster of stars. Early studies of nearby open clusters (like the Hyades and Pleiades) by Hertzsprung and Rosenberg produced the first CMDs, antedating by a few years Russell's influential synthesis of the diagram collecting data for all stars for which absolute magnitudes could be determined.\n\nAnother form of the diagram plots the effective surface temperature of the star on one axis and the luminosity of the star on the other, almost invariably in a log-log plot. Theoretical calculations of stellar structure and the evolution of stars produce plots that match those from observations. This type of diagram could be called \"temperature-luminosity diagram\", but this term is hardly ever used; when the distinction is made, this form is called the \"theoretical Hertzsprung–Russell diagram\" instead. A peculiar characteristic of this form of the H–R diagram is that the temperatures are plotted from high temperature to low temperature, which aids in comparing this form of the H–R diagram with the observational form.\n\nAlthough the two types of diagrams are similar, astronomers make a sharp distinction between the two. The reason for this distinction is that the exact transformation from one to the other is not trivial. To go between effective temperature and color requires a color–temperature relation, and constructing that is difficult; it is known to be a function of stellar composition and can be affected by other factors like stellar rotation. When converting luminosity or absolute bolometric magnitude to apparent or absolute visual magnitude, one requires a bolometric correction, which may or may not come from the same source as the color–temperature relation. One also needs to know the distance to the observed objects (\"i.e.\", the distance modulus) and the effects of interstellar obscuration, both in the color (reddening) and in the apparent magnitude (where the effect is called \"extinction\"). Color distortion (including reddening) and extinction (obscuration) are also apparent in stars having significant circumstellar dust. The ideal of direct comparison of theoretical predictions of stellar evolution to observations thus has additional uncertainties incurred in the conversions between theoretical quantities and observations.\n\nMost of the stars occupy the region in the diagram along the line called the main sequence. During the stage of their lives in which stars are found on the main sequence line, they are fusing hydrogen in their cores. The next concentration of stars is on the horizontal branch (helium fusion in the core and hydrogen burning in a shell surrounding the core). Another prominent feature is the Hertzsprung gap located in the region between A5 and G0 spectral type and between +1 and −3 absolute magnitudes (\"i.e.\" between the top of the main sequence and the giants in the horizontal branch). RR Lyrae variable stars can be found in the left of this gap on a section of the diagram called the instability strip. Cepheid variables also fall on the instability strip, at higher luminosities.\n\nThe H-R diagram can be used by scientists to roughly measure how far away a star cluster or galaxy is from Earth. This can be done by comparing the apparent magnitudes of the stars in the cluster to the absolute magnitudes of stars with known distances (or of model stars). The observed group is then shifted in the vertical direction, until the two main sequences overlap. The difference in magnitude that was bridged in order to match the two groups is called the distance modulus and is a direct measure for the distance (ignoring extinction). This technique is known as main sequence fitting and is a type of spectroscopic parallax. Not only the turn-off in the main sequence can be used, but also the tip of the red giant branch stars.\n\nContemplation of the diagram led astronomers to speculate that it might demonstrate stellar evolution, the main suggestion being that stars collapsed from red giants to dwarf stars, then moving down along the line of the main sequence in the course of their lifetimes. Stars were thought therefore to radiate energy by converting gravitational energy into radiation through the Kelvin–Helmholtz mechanism. This mechanism resulted in an age for the Sun of only tens of millions of years, creating a conflict over the age of the Solar System between astronomers, and biologists and geologists who had evidence that the Earth was far older than that. This conflict was only resolved in the 1930s when nuclear fusion was identified as the source of stellar energy.\n\nFollowing Russell's presentation of the diagram to a meeting of the Royal Astronomical Society in 1912, Arthur Eddington was inspired to use it as a basis for developing ideas on stellar physics. In 1926, in his book \"The Internal Constitution of the Stars\" he explained the physics of how stars fit on the diagram. The paper anticipated the later discovery of nuclear fusion and correctly proposed that the star's source of power was the combination of hydrogen into helium, liberating enormous energy. This was a particularly remarkable jump of insight, since at that time the source of a star's energy was still unsolved, thermonuclear energy had not been proven to exist, and even that stars are largely composed of hydrogen (see metallicity), had not yet been discovered. Eddington managed to sidestep this problem by concentrating on the thermodynamics of radiative transport of energy in stellar interiors. Eddington predicted that dwarf stars remain in an essentially static position on the main sequence for most of their lives. In the 1930s and 1940s, with an understanding of hydrogen fusion, came an evidence-backed theory of evolution to red giants following which were speculated cases of explosion and implosion of the remnants to white dwarfs. The term supernova nucleosynthesis is used to describe the creation of elements during the evolution and explosion of a pre-supernova star, a concept put forth by Fred Hoyle in 1954. The pure mathematical quantum mechanics and classical mechanical models of stellar processes enable the Hertzsprung–Russell diagram to be annotated with known conventional paths known as stellar sequences — there continue to be added rarer and more anomalous examples as more stars are analysed and mathematical models considered.\n\n\n\n"}
{"id": "320319", "url": "https://en.wikipedia.org/wiki?curid=320319", "title": "How to Solve It", "text": "How to Solve It\n\nHow to Solve It (1945) is a small volume by mathematician George Pólya describing methods of problem solving.\n\n\"How to Solve It\" suggests the following steps when solving a mathematical problem:\n\nIf this technique fails, Pólya advises: \"If you can't solve a problem, then there is an easier problem you can solve: find it.\" Or: \"If you cannot solve the proposed problem, try to solve first some related problem. Could you imagine a more accessible related problem?\"\n\n\"Understand the problem\" is often neglected as being obvious and is not even mentioned in many mathematics classes. Yet students are often stymied in their efforts to solve it, simply because they don't understand it fully, or even in part. In order to remedy this oversight, Pólya taught teachers how to prompt each student with appropriate questions, depending on the situation, such as:\n\n\nThe teacher is to select the question with the appropriate level of difficulty for each student to ascertain if each student understands at their own level, moving up or down the list to prompt each student, until each one can respond with something constructive.\n\nPólya mentions that there are many reasonable ways to solve problems. The skill at choosing an appropriate strategy is best learned by solving many problems. You will find choosing a strategy increasingly easy. A partial list of strategies is included:\n\n\nAlso suggested:\n\n\nThis step is usually easier than devising the plan. In general, all you need is care and patience, given that you have the necessary skills. Persist with the plan that you have chosen. If it continues not to work, discard it and choose another. Don't be misled; this is how mathematics is done, even by professionals.\n\nPólya mentions that much can be gained by taking the time to reflect and look back at what you have done, what worked and what didn't. Doing this will enable you to predict what strategy to use to solve future problems, if these relate to the original problem.\n\nThe book contains a dictionary-style set of heuristics, many of which have to do with generating a more accessible problem. For example:\n\n\n\n"}
{"id": "10508033", "url": "https://en.wikipedia.org/wiki?curid=10508033", "title": "Khôra", "text": "Khôra\n\nKhôra (also chora; ) was the territory of the Ancient Greek \"polis\" outside the city proper. The term has been used in philosophy by Plato to designate a receptacle (as a “third kind” [\"triton genos\"]; \"Timaeus\" 48e4), a space, a material substratum, or an interval. In Plato's account, \"khôra\" is neither being nor nonbeing but an interval between in which the \"forms\" were originally held; it \"gives space\" and has maternal overtones (a womb, matrix). Jacques Derrida has written a short text with the title \"Khôra\", using his deconstructionist approach to investigate Plato's word usage. It is the origin for the recent interest in this rather obscure Greek term.\n\nKey authors addressing \"khôra\" include Martin Heidegger, who refers to a \"clearing\" in which being happens or takes place. Julia Kristeva deploys the term as part of her analysis of the difference between the semiotic and symbolic realms, in that Plato's concept of \"khora\" is said to anticipate the emancipatory employment of semiotic activity as a way of evading the allegedly phallocentric character of symbolic activity (signification through language), which, following Jacques Lacan, is regarded as an inherently limiting and oppressive form of \"praxis\".\n\nJulia Kristeva articulates the \"khôra\" in terms of a presignifying state: \"Although the \"khôra\" can be designated and regulated, it can never be definively posited: as a result, one can situate the \"khôra\" and, if necessary, lend it a topology, but one can never give it axiomatic form.\"\n\nJacques Derrida uses \"khôra\" to name a radical otherness that \"gives place\" for being. Nader El-Bizri builds on this by more narrowly taking \"khôra\" to name the radical happening of an ontological difference between being and beings. El-Bizri's reflections on \"khôra\" are taken as a basis for tackling the meditations on \"dwelling\" and on \"being and space\" in Heidegger's thought and the critical conceptions of space and place as they evolved in architectural theory and in history of philosophy and science, with a focus on geometry and optics. Derrida argues that the subjectile is like Plato’s \"khôra\", Greek for space, receptacle or site. Plato proposes that the \"khôra\" rests between the sensible and the intelligible, through which everything passes but in which nothing is retained. For example, an image needs to be held by something, just as a mirror will hold a reflection. For Derrida, \"khôra\" defies attempts at naming or either/or logic, which he \"deconstructs\". See also Derrida's collaborative project with architect Peter Eisenmann, in \"Chora L Works: Jacques Derrida and Peter Eisenman\". The project proposed the construction of a garden in the Parc de la Villette in Paris, which included a sieve, or harp-like structure that Derrida envisaged as a physical metaphor for the receptacle-like properties of the \"khôra\". The concept of the \"khôra\", distinguished by its elusive properties, would have become a physical reality had the project been realised.\n\nFollowing Derrida, John Caputo describes \"khôra\" as:\n\nneither present nor absent, active or passive, the good nor evil, living nor nonliving - but rather atheological and nonhuman - \"khôra\" is not even a receptacle. \"Khôra\" has no meaning or essence, no identity to fall back upon. She/it receives all without becoming anything, which is why she/it can become the subject of neither a philosopheme nor mytheme. In short, the \"khôra\" is tout autre [fully other], very.\n\n\n"}
{"id": "9982139", "url": "https://en.wikipedia.org/wiki?curid=9982139", "title": "Killing of David Wilkie", "text": "Killing of David Wilkie\n\nDavid James Wilkie (9 July 1949 – 30 November 1984) was a Welsh taxi driver who was killed during the miners' strike in the United Kingdom, when two striking miners dropped a concrete block from a footbridge onto his taxi whilst he was driving a strike-breaking miner to work. The attack caused a widespread revulsion at the extent of violence in the dispute. The two miners were convicted of murder but the charge was reduced to manslaughter on appeal, becoming a leading case on the issue of the difference between the two offences.\n\nDavid Wilkie was born on 9 July 1949. He was working in Treforest, Mid Glamorgan as a taxi driver, driving a Ford Cortina for City Centre Cars, based in Bute Street, Cardiff. He was regularly engaged in driving non-striking miners to work, as the bitter industrial dispute had made them targets for physical retaliation by those miners who were on strike. The Merthyr Tydfil area was said to be the strongest in support of the strike of any mining area in Britain; it is situated in South Wales, where a large percentage of Britain's remaining coalmines were situated. There had not been much mass picketing in South Wales during the conflict, as there had been in many parts of England, because there had been so few strikebreakers.\n\nOn 30 November 1984, Wilkie's fare was David Williams, who lived in Rhymney and worked at the Merthyr Vale mine, six miles away. Wilkie was driving the same route as he had done for the previous ten days. He was accompanied by two police cars and a motorcycle outrider, and had just turned on to the A465 road north of Rhymney at the Rhymney Bridge roundabout, when two striking miners dropped a concrete block from a bridge 27 feet over the road. Wilkie died at the scene from multiple injuries; Williams escaped with minor injuries.\n\nThe incident led to a decrease in public support for the striking miners, and to an increase in the number of workers in other industries who crossed miners' picket lines (e.g. at power plants).\n\nPrime Minister Margaret Thatcher said, \"My reaction is one of anger at what this had done to a family of a person only doing his duty and taking someone to work who wanted to go to work.\" Kim Howells, speaking for the South Wales National Union of Mineworkers, blamed the attack on the attempts to persuade miners to return to work. Arthur Scargill said he had been \"deeply shocked by the tragedy\" of Wilkie's death.\n\nLabour Party leader Neil Kinnock was scheduled to appear at a Labour Party rally alongside Scargill in Stoke-on-Trent on the day of the tragedy. Kinnock's speech developed into an argument with some hecklers who saw him as having betrayed the NUM by failing to support the strike. Kinnock began by saying, \"We meet here tonight in the shadow of an outrage.\" When interrupted, Kinnock accused the hecklers of \"living like parasites off the struggle of the miners.\" As Kinnock went on to denounce the lack of the ballot, the violence against strikebreakers and the tactical approach of Scargill, he was asked by hecklers what he had done for the striking miners. Kinnock shouted back, \"Well, I was not telling them lies. That's what I was not doing during that period.\" This was a thinly-veiled attack on Scargill, whom he later admitted that he detested.\nWilkie lived with his fiancée Janice Reed, who was the mother of his two-year-old daughter and was pregnant with a baby who was born six weeks later. He also had a 12-year-old daughter and a five-year-old son by a previous partner. Funds were opened to help the family; among the donors was philanthropist Paul Getty. The Bishop of Llandaff led Wilkie's funeral service; he called for \"some sort of moratorium\" and a return to work by the miners in return for an impartial board to investigate conditions in the coal industry.\n\nThe two men who caused Wilkie's death, Dean Hancock and Russell Shankland, were found guilty of murder by a majority verdict on 16 May 1985 (by which time the strike had ended) and sentenced to life imprisonment. A third man, Anthony Williams, who had been present on the bridge but was found to have actively discouraged them from dropping the concrete block, was acquitted. The life sentences caused an outcry among the striking miners, who felt that the death of Wilkie was not a deliberate act; the strike had ended by the time the verdict was brought in, but 700 miners at Merthyr Vale walked out on hearing the news. A third miner, Dean Williams, was cleared of conspiracy to cause serious injury.\n\nRussell Shankland's solicitor was critical of Scargill's attitude. He referred to the strike as \"a war\" and said with regards to Scargill, \"In that war there were generals, and they stood outside the law and they left Russell Shankland outside the law.\"\n\nOn appeal, their convictions were reduced to manslaughter, and their life sentences were replaced with eight-year prison terms, of which they would serve just over half. The Lord Chief Justice, Lord Lane, explained that the crime would be murder if the death was a \"natural consequence\" of the miners' actions, but the legal phrase \"natural consequence\" was potentially misleading without further explanation. The appeal verdict of guilty to manslaughter was upheld in the House of Lords, in the case \"R v Hancock\". Hancock and Shankland were released on 30 November 1989, which was coincidentally the fifth anniversary of Wilkie's death.\n\nKim Howells, the South Wales NUM official who commented on the killing of David Wilkie, later became a Member of Parliament for the Labour Party and served as a minister in the Blair government and later became chair of the Intelligence and Security Committee, a committee of parliamentarians that oversees the work of Britain's intelligence and security agencies. In 2004 he said that when he heard that a taxi driver had been killed, he thought \"hang on, we've got all those records we've kept over in the NUM offices, there's all those maps on the wall, we're gonna get implicated in this\". He then destroyed \"everything\", because he feared a police raid on the union offices.\n\nThe killing is referenced in the Roger Waters songs , \"Who Needs Information\" and \"Me or Him\", on his 1984 \"Radio K.A.O.S.\" album.\n"}
{"id": "100168", "url": "https://en.wikipedia.org/wiki?curid=100168", "title": "Killology", "text": "Killology\n\nKillology is the study of the psychological and physiological effects of killing and combat on the human psyche; and the factors that enable and restrain a combatant's killing of others in these situations. \n\nThe term and field of study was invented without any scientific support by Lt. Col. Dave Grossman (US Army, Ret.) of the Killology Research Group in his 1996 book \"\".\n\nGrossman claims in his book \"On Killing\" that soldiers are faced with four options once they have entered into combat. These are:\n\nS.L.A. Marshall did a study on the firing rates of soldiers in World War II. He found that the ratio of rounds fired vs. hits was low; he also noted that the majority of soldiers were not aiming to hit their targets. This was a problem for the US military and its allies during World War II. New training implements were developed and hit rates improved. The changes were small, but effective. First, instead of shooting at bull's-eye type targets, the United States Army switched to silhouette targets that mimic an average human. Training also switched from 300 yard slow fire testing to rapid fire testing with different time and distance intervals from 20 to 300 yards. With these two changes, hitting targets became a reaction that was almost automatic.\n\nSome authors have discredited S.L.A. Marshall's book, stating that the book may be more of an idea of what was occurring and not a scientific study of what was happening. Other historians and journalists have outright accused Marshall of fabricating his study.\n\nAnother important factor that increased fire and hit rates is the development of camaraderie in training. Soldiers are taught that their actions do not only help or harm themselves, but the whole unit. This recurring theme in recollections collected from war veterans is the idea that they were not fighting for themselves at the time but more concerned for the people to their left and right. This ideology is ancient and was recorded by Sun Tzu in his book \"The Art of War\": \"If those who are sent to draw water begin by drinking themselves, the army is suffering from thirst.\"\n\nSome research has been done to say that the increase of Post-Traumatic Stress Disorder (PTSD) in the military is caused by the increase of firing rates. This brings up the classic debate of correlation vs. causation. Many believe that other factors have been influencing PTSD such as decompression time after conflicts, and the lack of moral sovereignty in engagements.\n\nVietnam is viewed by some as a less popular war than World War II. Many people who were sent to fight there thought that there was no reason for the engagement and did not feel a moral obligation to fight. In World War II many felt that they were stopping an evil empire from overtaking the globe. This helped the World War II troops' mettle to be steadfast.\n\nAnother problem with PTSD rates after World War II is the fact that there is far less decompression time for the soldiers. During World War II the main way back home was on a boat trip that took weeks. This time was spent with others who had had similar experiences and could understand the problems faced by others. During Vietnam soldiers were sent via draft to one year rotations by plane. When you arrived to your unit it was usually by yourself and you were shunned. This shunning was a result of the senior members being afraid to befriend someone who had a much higher chance of being killed than the experienced combatants. Once your time in country was over you were once again sent back home by yourself. There may have been other veterans with you but they were from a plethora of other units and you did not know them well enough to share the hardships you had seen.\n\nFinally one of the worst displays of environmental stressors was once you made it back home you were demonized by the public and discarded as a human being. Compare that to the treatment World War II veterans received when they came home from the European Theatre or the Pacific Theatre. Parades were thrown, everyone thanking the soldiers, even the invention of V for Victory was made to quickly show military members support. That symbol was changed into the Peace sign and used to show disapproval of the war in Vietnam just a few years later. These factors among many others caused Vietnam to have the highest postwar depression, suicide, and PTSD rates. To this day many are only now getting the counseling that they need to overcome the mental problems brought upon them from their service in Vietnam.\n\nIn engagements in the modern era such as the Persian Gulf War through the Iraq War and War in Afghanistan there is still a problem with a lack of decompression time. The training has improved so you train and deploy with the people you will be fighting with. But many times when you reach home you are given time off, and if one is in a reserve unit you most likely go back to work and only see your brothers in arms once a month. This lack of time to debrief and decompress can cause a feeling of isolation that only worsens the effects of PTSD. Grossman states in his book that everyone who experiences combat comes back with PTSD, the only question is to what extent their mind and psyche are damaged and how they cope with it.\n\nGrossman's theory, based on the World War II research of S.L.A. Marshall, is that most of the population deeply resists killing another human. Some veterans and historians have cast doubt on Marshall's research methodology. Professor Roger J. Spiller (Deputy Director of the Combat Studies Institute, US Army Command and General Staff College) argues in his 1988 article, \"S.L.A. Marshall and the Ratio of Fire\" (\"RUSI Journal\", Winter 1988, pages 63–71), that Marshall had not actually conducted the research upon which he based his ratio-of-fire theory. \"The 'systematic collection of data' appears to have been an invention.\"\nThis revelation has called into question the authenticity of some of Marshall's other books and has lent academic weight to doubts about his integrity that had been raised in military circles even decades earlier.\n\nAs a result of Marshall's work, modern military training was modified to attempt to override this instinct, by:\n\nBy the time of the United States involvement in the Vietnam War, says Grossman, 90% of U.S. soldiers would fire their weapons at other people.\n\nHe also says the act of killing is psychologically traumatic for the killer, even more so than constant danger or witnessing the death of others.\n\nGrossman further argues that violence in television, movies and video games contributes to real-life violence by a similar process of training and desensitization.\n\nIn \"On Combat\" (Grossman's sequel to \"On Killing\", based on ten years of additional research and interviews) he addresses the psychology and physiology of human aggression.\n\n\n"}
{"id": "28871115", "url": "https://en.wikipedia.org/wiki?curid=28871115", "title": "Libertarianism Without Inequality", "text": "Libertarianism Without Inequality\n\nLibertarianism Without Inequality is a book written in 2003 by Michael Otsuka, and published by Oxford University Press.\n\nThe book is written in three parts with part one being dedicated to self-ownership and \"world ownership.\" Part two dwells on the rights of self-defense and the right to punish those who transgress against the natural rights of others. Part three deals with the political aspects and other types of libertarianism.\n\nIan Carter has said, \"In this important contribution to rights theory, the deontology of punishment, and the problem of political obligation, Michael Otsuka argues against the belief, prevalent on both the left and the right of the political spectrum, that the fundamental principles of libertarianism conflict with the ideal of economic equality. This allows him to defend “libertarianism without inequality” – a radical and provocative normative construction that is both more egalitarian and more libertarian than mainstream (left-of-centre) liberal egalitarianism.\"\n\nRothbardian intellectual historian David Gordon has said, \"Michael Otsuka endeavors to combine two fundamental principles of political philosophy, usually considered polar opposites. In my view, his ingenious attempt does not succeed; but his failure has much to teach us.\" Timothy Hinton has said the book is a notable contribution to political philosophy.\n\n"}
{"id": "50395923", "url": "https://en.wikipedia.org/wiki?curid=50395923", "title": "Logophonetic", "text": "Logophonetic\n\nA logophonetic writing system is one that uses chiefly logographic symbols, but includes symbols or elements representing sounds.\n\n"}
{"id": "44856939", "url": "https://en.wikipedia.org/wiki?curid=44856939", "title": "Mitahara", "text": "Mitahara\n\nMitahara (Sanskrit: मिताहार, Mitāhāra) literally means the habit of moderate food. Mitahara is also a concept in Indian philosophy, particularly Yoga, that integrates awareness about food, drink, balanced diet and consumption habits and its effect on one’s body and mind. It is one of the ten yamas in ancient Indian texts.\n\n\"Mitahara\" is a Sanskrit combination word, from Mita (मित, moderate) and Ahara (आहार, taking food, diet), which together mean moderate diet. In Yoga and other ancient texts, it represents a concept linking nutrition to the health of one’s body and mind. It is considered a yamas or self-restraint virtue in some schools of Indian traditions, where one refrains from either eating too much or eating too little quantity of food, and where one refrains from either eating too much or too little of certain qualities of food. Mitahara is synonymous with Mātrāśin (मात्राशिन्).\n\nAncient and medieval era Indian literature on \"Mitahara\" are of two categories – one relates to philosophical discussion of moderate diet and proper nutrition, the other category relate to details about \"Aharatattva\" (dietetics). The former category include the Upanishads and Sutras that discuss why virtuous self-restraint is appropriate in matters of food, while the latter include Samhitas that discuss what and when certain foods are suitable. A few texts such as Hathayoga Pradipika combine both.\n\n\"Mitahara\" is discussed in Śāṇḍilya Upanishad, as well as by Svātmārāma. It is one of the yamas (virtuous self restraints) discussed in ancient Indian texts. The other nine yamas are Ahiṃsā (अहिंसा): Nonviolence, Satya (सत्य): truthfulness, Asteya (अस्तेय): not stealing, Brahmacharya (ब्रह्मचर्य): celibacy and not cheating on one’s spouse, Kṣamā (क्षमा): forgiveness, Dhṛti (धृति): fortitude, Dayā (दया): compassion, Ārjava (आर्जव): sincerity, non-hypocrisy, and Śauca (शौच): purity, cleanliness.\n\nSome of the earliest ideas behind \"Mitahara\" trace to ancient era Taittiriya Upanishad, which in various hymns discusses the importance of food to healthy living, to the cycle of life, as well as to its role in one's body and its effect on Self (Brahman, Atma, Spirit). The Upanishad, states Stiles, notes “from food life springs forth, by food it is sustained, and in food it merges when life departs”.\n\nThe Bhagavad Gita includes verses on ‘‘mitahara’’ in Chapter 6. It states in verse 6.16 that a yogi must neither eat too much nor too little, neither sleep too much nor too little. Understanding and regulating one’s established habits about eating, sleeping and recreation is suggested as essential to the practice of yoga in verse 6.17.\n\nAnother ancient text, in a South Indian language, \"Tirukkuṛaḷ\" states moderate diet as a virtuous life style. This text, written between 200 BC and 400 AD, and sometimes called the Tamil Veda, discusses eating habits and its role in a healthy life (Mitahara), dedicating Chapter 95 of Book 7 to it. \"Tirukkuṛaḷ\" states in verses 943 through 945, “eat in moderation, when you feel hungry, foods that are agreeable to your body, refraining from foods that your body finds disagreeable”. Tiruvalluvar also emphasizes overeating has ill effects on health, in verse 946, as “the pleasures of health abide in the man who eats moderately. The pains of disease dwell with him who eats excessively.”\n\nMedieval era Sanskrit texts such as Dasakumara Charita and Hatha Yoga Pradipika discuss \"Mitahara\". For example, Hatha Yoga Pradipika verse 1.57 states the importance of ‘‘mitihara’’, as\nVerses 1.57 through 1.63 of the critical edition of Hathayoga Pradipika suggests that taste cravings should not drive one’s eating habits, rather the best diet is one that is tasty, nutritious and likable as well as sufficient to meet the needs of one’s body and for one’s inner self. It recommends that one must “eat only when one feels hungry” and “neither overeat nor eat to completely fill the capacity of one’s stomach; rather leave a quarter portion empty and fill three quarters with quality food and fresh water”. Verses 1.59 to 1.61 of Hathayoga Pradipika suggests ‘‘mitahara’’ regimen of a yogi avoids foods with excessive amounts of sour, salt, bitterness, oil, spice burn, unripe vegetables, fermented foods or alcohol. The practice of \"Mitahara\", in Hathayoga Pradipika, includes avoiding stale, impure and tamasic foods, and consuming moderate amounts of fresh, vital and sattvic foods.\n\nCharaka Samhita and Sushruta Samhita are among the two largest surviving compendium on nutrition and diet that have survived from ancient and medieval periods of India. Caraka Samhita emphasizes the need to plan and understand the role of diet in health, across Chapters 5, 6, 25, 26 and 27. In verse 25.31, it states \"wholesome diet promotes health and growth, unwholesome diet is the most important cause of diseases\". In verses 25.38-39, Caraka Samhita classifies foods into groups based on its source and taste, then categorizes them into nutritive and harmful. In Chapters 26 and 27, it suggests that the same food can be nutritive in small amounts while harmful in large amounts or if cooked improperly or if eaten together with foods its lists. Food, claims Caraka Samhita, must be tailored to needs of one's body, state of one's health, climate, season, habits and personal palatability and needs. In the spirit of Mitahara, in Chapter 5, it insists even light, easily digested and nutritious food should be consumed in moderation and should not be consumed in excess of bodily requirements. In Chapter 6, Caraka Samhita recommends that food should be tailored to the season, with rich and fatty foods being beneficial in winter, while light soups, fruits and acidulated drinks more suited for summers. In verses 6.6-7, it suggests that the diet should be planned and nourishing foods consumed in rotation, tailored to one's health condition and personal needs.\n\nAs with Caraka Samhita, the other large compendium on health – Sushruta Samhita – includes many chapters on the role of diet and personal needs of an individual. In Chapter 10 of Sushruta Samhita, for example, the diet and nutrition for pregnant women, nursing mothers and young children are described. It recommends milk, butter, fluid foods, fruits, vegetables and fibrous diets for expecting mothers along with soups made from \"jangala\" (wild) meat. In most cases, vegetarian diets are preferred and recommended in the Samhitas; however, for those recovering from injuries, growing children, those who do high levels of physical exercise, and expecting mothers, Sutrasthanam's Chapter 20 and other texts recommend carefully prepared meat. Sushruta Samhita also recommends a rotation and balance in foods consumed, in moderation. For this purposes, it classifies foods by various characteristics, such as taste. In Chapter 42 of Sutrasthanam, for example, it lists six tastes – \"madhura\" (sweet), \"amla\" (acidic), \"lavana\" (saline), \"katuka\" (pungent), \"tikta\" (bitter) and \"kashaya\" (astringent). It then lists various sources of foods that deliver these tastes and recommends that all six tastes (flavors) be consumed in moderation and routinely, as a habit for good health.\n\nThe concept of \"Mitahara\" is discussed in over 30 different ancient and medieval era texts of Hinduism. However, some texts use a different word and concept for the idea of \"moderate diet and paying attention to what one eats and drinks\". For example, Shivayoga Dipika uses the term \"Niyatāshana\" (planned, regulated eating), while Dattatreya Samhita uses \"Laghrāhāra\" (eating lightly, small portions of diverse foods).\n\n"}
{"id": "41399410", "url": "https://en.wikipedia.org/wiki?curid=41399410", "title": "Mithyatva", "text": "Mithyatva\n\nMithyatva means \"false belief\", and an important concept in Jainism and Hinduism. Disappearance (\"nivrtti\") is the necessary presupposition of \"mithyatva\" because what is falsely perceived ceases to exist with the dawn of right knowledge. \"Mithyatva\", states Jayatirtha, cannot be easily defined as 'indefinable', 'non-existent', 'something other than real', 'which cannot be proved, produced by avidya or as its effect', or as 'the nature of being perceived in the same locus along with its own absolute non-existence'.\n\nMithyatva is a concept in Jainism distinguishing right knowledge from false knowledge, and parallels the concepts of \"Avidya\" in the Vedanta school of Hinduism, \"Aviveka\" in its Samkhya school, and \"Maya\" in Buddhism.\n\nThe opposite of \"Mithyatva\" (false belief) is \"Samyaktva\" (right belief).\n\nMithyatva is an important concept on false knowledge in Jainism. The Jaina scholar Hemachandra defined Mithyatva as \"belief in false divinities, false gurus and false scriptures\".\n\nJainism describes seven types of beliefs - \"mithyatva\", \"sasvadana-samyaktva\", \"mishra-mithyatva\", \"kashopashmika-samyaktva\", \"aupshamika-samyaktva\", \"vedak-samyaktva\" and \"kshayik-samyaktva\". \"Mithyatva\", meaning false or wrong belief, is the soul’s original and beginning less state of deluded world-view, at which stage the soul is in a spiritual slumber, unaware of its own bondage.\n\n\"Mithyatva\" or \"false belief, delusion\" are of five kinds in Jainism, according to one classification:\n\nSvetambara Jains classify categories of false belief under \"Mithyatva\" differently: \"Abhigrahika\" (belief limited to their own scriptures that they can defend, but refusing to study and analyse other scriptures); \"Anabhigrahika\" (belief that equal respect must be shown to all gods, teachers, scriptures); \"Abhiniviseka\" (belief of those who can discern but refuse to do so from preconceptions); \"Samsayika\" (state of hesitation or uncertainty between various conflicting, inconsistent beliefs); and \"Anabhogika\" (innate, default beliefs that a person has not thought through on one's own).\n\nDigambara Jains classify categories of false belief under \"Mithyatva\" into seven: \"Ekantika\" (absolute, one sided belief), \"Samsayika\" (uncertainty, doubt whether a course is right or wrong, unsettled belief, skepticism), \"Vainayika\" (belief that all gods, gurus and scriptures are alike), \"Grhita\" (belief derived purely from habits or default, no self analysis), \"Viparita\" (belief that true is false, false is true, everything is relative or acceptable), \"Naisargika\" (belief that living beings are devoid of consciousness and cannot discern right from wrong), \"Mudha-drsti\" (belief that violence and anger can tarnish or damage thoughts, divine, guru or dharma).\n\nMithyatva is one of three things, in Jainism, that are harmful stimuli and that distract a person from attaining right belief and correct knowledge. The other two things that distract, are \"Maya\" (deceit), and \"Nidana\" (hankering after fame and worldly pleasures).\n\nOne Jaina text lists 28 kinds of \"mohaniya\" (deluding) \"karmas\" that prevent the true perception of reality and the purity of the soul, the \"darsana mohaniya karman\" which function to prevent a soul’s insight into its own nature and therefore, deemed destructive, are \"mithyatva karman\". The term, \"mithyatva\", meaning 'perversity', is generally used to denote the idea of \"avidya\" along with \"mithyadarsana\" or \"mithyadrsti\" (wrong view), \"darsanamoha\" (delusion of vision), \"moha\" (delusion) etc.;. The state of \"mithyatva\" is manifested as a fundamental tendency to see things other than as they really are (Tattva Sutra 8:9). Passions such as Aversion (dvesa) and Attachment (raga), which are also called pursuers from the limitless past (anantanubandhi), operate in conjunction with mithyatva. \"Mithyatva\" is the one-sided or perverted world-view which generates new layers of \"karma\" and considered in Jainism as the root of human arrogance.\n\nMithyatva is a concept found in some schools of Hinduism. Other concepts in Hinduism, similar in meaning to Mithyatva, include the concepts of \"Avidya\" in the Vedanta school of Hinduism, \"Aviveka\" in its Samkhya school.\n\nIgnorance begets \"aviveka\" (lack of correct, discriminative knowledge) states Samkhya school of Hinduism. One engages in deeds, good and bad, due to \"aviveka\", earns \"punya\" or becomes a victim of sin and is reborn. Aviveka also means lack of reason or imprudence or indiscretion. Avidya is related concept in Vedanta school of Hinduism. Avidya and aviveka give \"dukkha\" i.e. suffering.\n\nMadhusudanah in his \"Advaita-siddhi\" gives five definitions of \"mithyatva\" which term is derived from \"mithya\" meaning false or indeterminable. False is something that appears and is later negated or contradicted; the unreal is never an object of experience, the concept of unreal is self-contradictory. Falsity is defined as – not being the locus of either reality or unreality, it is distinct from both reality and unreality. In practice, \"mithyatva\" has three means, – a) that which does not exist in three divisions of time, past, present and future; b) that which is removable by knowledge; and c) that which is identical with the object of sublation. Whereas \"mithya\" is other than real but not real, \"mithyatva\" is identical with sublatability. \"Mithyatva\" may also be understood as that which is negated even where it is found to exist. The followers of the Advaita School contend that the world-appearance is negated by Brahman-knowledge and hence it is illusory. To the followers of Vishishtadvaitavada, \"mithya\" is the apprehension of an object as different from its own nature.\n\nThe Advaita School considers \"Mithyatva\" to mean falsity of the world. Disappearance (\"nivrtti\") is the necessary presupposition of \"mithyatva\" because what is falsely perceived ceases to exist with the dawn of right knowledge. But, \"mithya\" or falsity, or \"mithyatva\" or falsity of the world, cannot be easily defined as indefinable or non-existent or something other than real or which cannot be proved or produced by \"avidya\" (or as its effect) or as the nature of being perceived in the same locus along with its own absolute non-existence. The opponents of the \"Advaita\" do not accept the contention that Atman is simply consciousness and cannot be the substratum of knowledge, and they insist that existence as the logical concomitant of the absence of non-existence and vice versa, with these two being mutually exclusive predicates, must be admitted. The opposite of unreality must be reality.\n\nAccording to \"Advaita\" anything which is both cognized and sublated is \"mithya\". \"Mithyatva\" is negated even where it is found. The illusoriness of the world is itself illusory. Once Brahman-knowledge arises both the cognizer and the cognized disappear.\n\nThe proof of unreality is impermanence, the permanent one is the Sole Reality. \"Mithyatva\" is apparent reality; at the level of ultimate truth, when, through the understanding of the mithyatva of all limiting adjuncts (upadhis) of name and form i.e., those that pertain to the individual body-mind (\"tvam\") and as well to the lordship of Brahman (\"tat\"), everything is seen to be not an other to pure Awareness, the distinctions of Jiva and Ishvara no longer apply, and it is the \"param Brahman\", the very essential of the Lord Itself, that is the final reality. In \"Advaita\" the method to reveal the unreality (\"mithyatva\") of things involves the idea of change and permanence i.e. what deviates and what persists.\n\n\"Mithyatva\" means 'illusoriness'. \"Advaita\" maintains that Brahman alone is real, the plurality of the universe is because the universe is illusory, the universe can be cognized; whatever that is cognized is illusory. The universe is different from the real as well as the real, the universe is indeterminable. \"Vedanta Desika\" refutes this contention because there is no such entity which is neither real nor unreal. The universe which is different from Brahman is inseparably related to Brahman. Badarayana (Brahma Sutra III.ii.28) declares that between the \"Jiva\" and Brahman there is difference as well as non-difference like the relation of light to its substratum or source on account of both being luminous, one being limited and the other all-pervading, the all-pervading is real and immortal. Rishi Damano Yamayana (Rig Veda X.xvi.4) insists that all should know about that part of the body which is immortal; the immortal part of the body is the Atman or Brahman, it is called a part because without it there cannot be life in one’s body. Vacaspati of the \"Bhamati\" school states that whereas illusion conceals, \"mithyatva\" signifies 'concealment', the real nature of the cognized object is concealed resulting in non-apprehension of difference between the real and the unreal objects. Padmapada of the \"Vivarna\" school adds to the sense of concealment the sense of inexpressibility, thus hinting at the sublatability of illusion. If the term \"anirvacaniya\" is defined by the \"Advaita\" as the nature of being different from \"sat\" and \"asat\" in essence, which is the nature of \"mithyatva\", then the element of difference must be real. Even though Jayatirtha states that there is no bar on the validity of the experience of difference, but the fact remains that difference cannot be an attribute of objects. Madhavacharya concludes that difference is not something that falls outside the content of an object or what is generally considered to constitute its essence which in perception is the sum total of its distinction from others. The perception of an object is the same as the perception of its difference from all others.\n\n\"Mithyatva\" is not a common term in Buddhism, but where mentioned implies deceit. The more common term used is Maya. Mithyatva, according to \"Abhidharmakosa\", means rebirth in the hells or as an animal or as a \"preta\". \"Ratnagotravibhagha\" terms \"mithyatva\" as the state of evil.\n"}
{"id": "3560122", "url": "https://en.wikipedia.org/wiki?curid=3560122", "title": "Moisture recycling", "text": "Moisture recycling\n\nIn hydrology, moisture recycling or precipitation recycling refer to the process by which a portion of the precipitated water that evapotranspired from a given area contributes to the precipitation over the same area. Moisture recycling is thus a component of the hydrologic cycle. The ratio of the locally derived precipitation (formula_1) to total precipitation (formula_2) is known as the recycling ratio, formula_3: formula_4.\n\nThe recycling ratio is a diagnostic measure of the potential for interactions between land surface hydrology and regional climate. Land use changes, such as deforestation or agricultural intensification, have the potential to change the amount of precipitation that falls in a region. The recycling ratio for the entire world is one, and for a single point is zero. Estimates for the recycling ratio for the Amazon basin range from 24% to 56%, and for the Mississippi basin from 21% to 24%.\n\nThe concept of moisture recycling has been integrated into the concept of the precipitationshed. A precipitationshed is the upwind ocean and land surface that contributes evaporation to a given, downwind location's precipitation. In much the same way that a watershed is defined by a topographically explicit area that provides surface runoff, the precipitationshed is a statistically defined area within which evaporation, traveling via moisture recycling, provides precipitation for a specific point.\n\n\n</ref>\n"}
{"id": "286819", "url": "https://en.wikipedia.org/wiki?curid=286819", "title": "New class", "text": "New class\n\nThe new class is used as a polemic term by critics of countries that followed the Soviet type of Communism to describe the privileged ruling class of bureaucrats and Communist Party functionaries which arose in these states. Generally, the group known in the Soviet Union as the nomenklatura conforms to the theory of the new class. The term was earlier applied to other emerging strata of the society.\n\nMilovan Đilas' \"New Class\" theory was also used extensively by anti-Communist commentators in the West in their criticism of the Communist states during the Cold War.\n\nThe term \"red bourgeoisie\" is a pejorative synonym for the term new class, crafted by leftist critics and movements (like the 1968 student demonstrations in Belgrade).\n\nNew class is also used as a term in late 1960s post-industrial sociology.\n\nA theory of the new class was developed by Milovan Đilas the Vice President of the Federal People's Republic of Yugoslavia under Josip Broz Tito, who participated with Tito in the Yugoslav People's Liberation War, but was later purged by him as Đilas began to advocate democratic and egalitarian ideals (which he believed were more in line with the way socialism and communism should look like). However, there were also personal antagonisms between the two men, and Tito felt Đilas undermined his leadership. The theory of the new class can be considered to oppose the theories of certain ruling Communists, such as Joseph Stalin, who argued that their revolutions and/or social reforms would result in the extinction of any ruling class as such. It was Đilas' observation as a member of a Communist government that Party members stepped into the role of ruling class – a problem which he believed should be corrected through revolution. Đilas' completed his primary work on his new class theory in the mid-1950s. While Đilas was in prison, it was published in 1957 in the West under the title \"\".\n\nĐilas claimed that the new class' specific relationship to the means of production was one of collective political control, and that the new class' property form was political control. Thus for Đilas the new class not only seeks expanded material reproduction to politically justify its existence to the working class, but it also seeks expanded reproduction of political control as a form of property in itself. This can be compared to the capitalist who seeks expanded value through increased sharemarket values, even though the sharemarket itself does not necessarily reflect an increase in the value of commodities produced. Đilas uses this argument about property forms to indicate why the new class sought parades, marches and spectacles despite this activity lowering the levels of material productivity.\n\nĐilas proposed that the new class only slowly came to self-consciousness of itself as a class. On arriving at a full self-consciousness the initial project undertaken would be massive industrialisation in order to cement the external security of the new class' rule against foreign or alternative ruling classes. In Đilas' schema this approximated the 1930s and 1940s in the Soviet Union. As the new class suborns all other interests to its own security during this period, it freely executes and purges its own members in order to achieve its major goal of security as a ruling class.\n\nAfter security has been achieved, the new class pursues a policy of moderation towards its own members, effectively granting material rewards and freedom of thought and action within the new classso long as this freedom is not used to undermine the rule of the new class. Đilas identified this period as the period of Khrushchev's government in the Soviet Union. Due to the emergence of conflicts of policy within the new class, the potential for palace coups, or populist revolutions is possible (as experienced in Poland and Hungary respectively).\n\nFinally Đilas predicted a period of economic decline, as the political future of the new class was consolidated around a staid programme of corruption and self-interest at the expense of other social classes. This can be interpreted as a prediction of the Leonid Brezhnev so-called Era of Stagnation by Đilas.\n\nWhile Đilas claimed that the new class was a social class with a distinct relationship to the means of production, he did not claim that this new class was associated with a self-sustaining mode of production. This claim, within Marxist theory, argues that the Soviet-style societies must eventually either collapse backwards towards capitalism, or experience a social revolution towards real socialism. This can be seen as a prediction of the downfall of the Soviet Union.\n\nRobert Kaplan's 1993 book \"Balkan Ghosts: A Journey through history\" also contains a discussion with Đilas, who used his model to anticipate many of the events that subsequently came to pass in the former Yugoslavia.\n\nMarxists, like Ernest Mandel, have criticised Djilas for ignoring the existence of a new socio-economic system, which cannot be reconciled with the old class system. \n\nOf course, the specific notions of Đilas are his own development, however the idea that bureaucrats in a typical Marxist–Leninist style state become a new class is not his original idea. Mikhail Bakunin had made this point in his International Workingmen's Association debates with Marx in the mid-to-late 19th century. This idea was repeated after the Russian revolution by anarchists like Kropotkin and Makhno, as well as some Marxists. In 1911 Robert Michels first proposed the Iron law of oligarchy, which described the development of bureaucratic hierarchies in supposedly egalitarian and democratic socialist parties. It was later repeated by a leader of the Russian Revolution, Leon Trotsky through his theory of degenerated workers state. Further on, Mao Zedong also had his own version of this idea developed during the Socialist Education Movement to criticize the Communist Party of China under Liu Shaoqi. Of course, this wide range of people over the decades had different perspectives on the matter, but there was also a degree of core agreement on this idea.\n\nFrom the other side of the fence, the work of Friedrich Hayek also anticipated many of Đilas' New Class criticisms, without placing them in a Marxist context (see esp. \"The Road to Serfdom\"). American neoconservatives adapted New Class analysis in their theory of the managerial state. Karl Popper's criticisms of utopian social pursuits in \"The Open Society and Its Enemies\" are markedly similar to Đilas' views, which were nonetheless developed independently (see note 6 to Chapter 18 of \"The Open Society and Its Enemies\" and related text).\n\nCanadian-American liberal economist John Kenneth Galbraith also wrote about a similar phenomenon under Capitalism, the emergence of a technocratic layer in \"The New Industrial State\" and \"The Affluent Society\".\n\nThe \"New Class\" model as a theory of new social groups in post-industrial societies gained ascendency during the 1970s as social and political scientists noted how \"New Class\" groups were shaped by post-material orientations in their pursuit of political and social goals (Bruce-Briggs, B. The New Class? New Brunswick, NJ: Transaction. 1979). New Class themes \"no longer have a direct relationship to the imperatives of economic security\" (Inglehart, Ronald. 'The Silent Revolution in Europe: the intergenerational change in post-industrial society'. American Political Science Review. 65. 1971:991-1017).\n\n\n\n"}
{"id": "32099529", "url": "https://en.wikipedia.org/wiki?curid=32099529", "title": "Oblique effect", "text": "Oblique effect\n\nOblique effect is the name given to the relative deficiency in perceptual performance for oblique contours as compared to the performance for horizontal or vertical contours.\n\nThe earliest known observation of this effect came about in 1861 when Ernst Mach completed an experiment in which he set a line to make it appear parallel to an adjoining one, and found observers' errors to be least for horizontal and vertical orientations and largest for an inclination of 45 degrees. The effect can be demonstrated for many visual tasks and was named \"oblique effect\" in the widely cited article by Stuart Appelle.\n\n The effect is exhibited predominantly in tasks involving discrimination of the angle of tilt of patterns or contours. People are very good at detecting whether a picture is hung vertical, but are two- to fourfold worse for a 45-degree oblique contour, even when a comparison is available. However there is no oblique deficit in some other tasks, such as judgment of lengths. Similarly, while it is harder to judge the direction of motion when it is oblique, this is not the case for speed.\n\nThe figure on the right shows the performance when an observer makes judgments about the length (top) and the orientation (bottom)of a line, in eight orientations around the clock.\n\nEven the immediate appearance of the form of a figure, often called gestalt, changes on a 45-degree rotation—the geometrical congruity of the square and the diamond does not extend to their perception as figures (see left) as was emphasized by Ernst Mach.\n\nAs with geometrical-optical illusions the oblique effect can be examined at two levels. The \"physiological\" one looks at the neural apparatus. Much pertinent information has been gathered here, yet the phenomenon was discovered in, and has ultimate relevance to, the whole organism's performance. Hence it is not contradictory to follow two separate tracks of explanation.\n\nNeural processing of contours was highlighted by the classical research by Hubel and Wiesel which revealed neural units right at the entrance of visual signals into the brain that respond preferentially to lines and edges. When the distribution of preferred orientation of these units was examined, there were fewer in the oblique meridians than in the vertical and horizontal.\n\nOrientation differences also occur in testing the visual brain with probes for cell connectivity and with imaging techniques.\n\nHowever, in contrast to the strong behavioral effect, evidence for orientation selectivity bias in primary visual cortex is weak and controversial. Actually, many human fMRI studies have failed to see this biased activity in primary visual cortex. Rather, more recent studies have suggested that, oblique effect may be due to selectivity for cardinal (i.e. horizontal and vertical) orientations in higher level visual areas and more specifically in parahippocampal place area (PPA), an area devoted to scene perception. This finding is supported by the fact that, among all visual object categories, perception of scenes (both natural and man-made environments) receives more processing benefit from the oblique effect and higher visual acuity for horizontal and vertical contours, due to their unique structure.\n\nNevertheless, there is an oblique effect for target configurations that do not directly address these \"oriented\" neural elements early in the visual path into the brain. Regardless of where in the brain of the human or animals an oblique effect is found, one would still like to know whether it is an inevitable consequence of the way neural signals are processed, or whether it is a minor error that nature hadn't been bothered to correct, or whether it fulfills a function in making us better in handling our visual environment. Proposing a \"purpose\" of the oblique effect, and developing scientific support for it is still a work in progress. A popular concept is that we live in a carpentered environment. Attempts at empirical explanations of perceptual visual phenomena have led to the examination of the orientation distribution of contours in the everyday visual world.\n\nCompeting explanations have to contend with questions, not yet finalized, of innateness of horizontal/vertical superiority, of body symmetry in anatomical organization, of methodology of measurement, and particularly, of issues associated with perceptual development in infants and children, and across cultures.\n\n\nMeridian: In vision, a plane containing the anterior-posterior axis of the eye. According to standards in the eye professions, the left side of the horizontal meridian, as seen by the subject, has 0-deg orientation, and orientations increase in a clockwise direction, again as seen by subject.\n\nCardinal directions are horizontal and vertical.\n\nThe horizontal effect is an extension of the oblique effect in which... \"When presented [with] a natural or other broad-band scene, people see oblique content the best and they actually see horizontal content the worst, with vertical usually falling in between\".\n\nVertical-horizontal illusion, the overestimation of vertical distances in vision, is not generally encompassed by the oblique effect, which mostly lumps the vertical and horizontal together in making comparisons with the obliques.\n"}
{"id": "466164", "url": "https://en.wikipedia.org/wiki?curid=466164", "title": "Onsager reciprocal relations", "text": "Onsager reciprocal relations\n\nIn thermodynamics, the Onsager reciprocal relations express the equality of certain ratios between flows and forces in thermodynamic systems out of equilibrium, but where a notion of local equilibrium exists.\n\n\"Reciprocal relations\" occur between different pairs of forces and flows in a variety of physical systems. For example, consider fluid systems described in terms of temperature, matter density, and pressure. In this class of systems, it is known that temperature differences lead to heat flows from the warmer to the colder parts of the system; similarly, pressure differences will lead to matter flow from high-pressure to low-pressure regions. What is remarkable is the observation that, when both pressure and temperature vary, temperature differences at constant pressure can cause matter flow (as in convection) and pressure differences at constant temperature can cause heat flow. Perhaps surprisingly, the heat flow per unit of pressure difference and the density (matter) flow per unit of temperature difference are equal. This equality was shown to be necessary by Lars Onsager using statistical mechanics as a consequence of the time reversibility of microscopic dynamics (microscopic reversibility). The theory developed by Onsager is much more general than this example and capable of treating more than two thermodynamic forces at once, with the limitation that \"the principle of dynamical reversibility does not apply when (external) magnetic fields or Coriolis forces are present\", in which case \"the reciprocal relations break down\".\n\nThough the fluid system is perhaps described most intuitively, the high precision of electrical measurements makes experimental realisations of Onsager's reciprocity easier in systems involving electrical phenomena. In fact, Onsager's 1931 paper refers to thermoelectricity and transport phenomena in electrolytes as well-known from the 19th century, including \"quasi-thermodynamic\" theories by Thomson and Helmholtz respectively. Onsager's reciprocity in the thermoelectric effect manifests itself in the equality of the Peltier (heat flow caused by a voltage difference) and Seebeck (electrical current caused by a temperature difference) coefficients of a thermoelectric material. Similarly, the so-called \"direct piezoelectric\" (electrical current produced by mechanical stress) and \"reverse piezoelectric\" (deformation produced by a voltage difference) coefficients are equal. For many kinetic systems, like the Boltzmann equation or chemical kinetics, the Onsager relations are closely connected to the principle of detailed balance and follow from them in the linear approximation near equilibrium.\n\nExperimental verifications of the Onsager reciprocal relations were collected and analyzed by D. G. Miller for many classes of irreversible processes, namely for thermoelectricity, electrokinetics, transference in electrolytic solutions, diffusion, conduction of heat and electricity in anisotropic solids, thermomagnetism and galvanomagnetism. In this classical review, chemical reactions are considered as \"cases with meager\" and inconclusive evidence. Further theoretical analysis and experiments support the reciprocal relations for chemical kinetics with transport.\n\nFor his discovery of these reciprocal relations, Lars Onsager was awarded the 1968 Nobel Prize in Chemistry. The presentation speech referred to the three laws of thermodynamics and then added \"It can be said that Onsager's reciprocal relations represent a further law making a thermodynamic study of irreversible processes possible.\" Some authors have even described Onsager's relations as the \"Fourth law of thermodynamics\".\n\nThe basic thermodynamic potential is internal energy. In a simple fluid system, neglecting the effects of viscosity the fundamental thermodynamic equation is written:\n\nwhere \"U\" is the internal energy, \"T\" is temperature, \"S\" is entropy, \"P\" is the hydrostatic pressure, \"V\" is the volume, formula_2 is the chemical potential, and \"M\" mass. In terms of the internal energy density, \"u\", entropy density \"s\", and mass density formula_3, the fundamental equation at fixed volume is written:\n\nFor non-fluid or more complex systems there will be a different collection of variables describing the work term, but the principle is the same. The above equation may be solved for the entropy density:\n\nThe above expression of the first law in terms of entropy change defines the entropic conjugate variables of formula_6 and formula_3, which are formula_8 and formula_9 and are intensive quantities analogous to potential energies; their gradients are called thermodynamic forces as they cause flows of the corresponding extensive variables as expressed in the following equations.\n\nThe conservation of mass is expressed locally by the fact that the flow of mass density formula_3 satisfies the continuity equation:\nwhere formula_12 is the mass flux vector. The formulation of energy conservation is generally not in the form of a continuity equation because it includes contributions both from the macroscopic mechanical energy of the fluid flow and of the microscopic internal energy. However, if we assume that the macroscopic velocity of the fluid is negligible, we obtain energy conservation in the following form:\n\nwhere formula_6 is the internal energy density and formula_15 is the internal energy flux.\n\nSince we are interested in a general imperfect fluid, entropy is locally not conserved and its local evolution can be given in the form of entropy density formula_16 as\n\nwhere formula_18 is the rate of increase in entropy density due to the irreversible processes of equilibration occurring in the fluid and formula_19 is the entropy flux.\n\nIn the absence of matter flows, Fourier's law is usually written:\n\nwhere formula_21 is the thermal conductivity. However, this law is just a linear approximation, and holds only for the case where formula_22, with the thermal conductivity possibly being a function of the thermodynamic state variables, but not their gradients or time rate of change. Assuming that this is the case, Fourier's law may just as well be written:\n\nIn the absence of heat flows, Fick's law of diffusion is usually written:\n\nwhere \"D\" is the coefficient of diffusion. Since this is also a linear approximation and since the chemical potential is monotonically increasing with density at a fixed temperature, Fick's law may just as well be written:\n\nwhere, again, formula_26 is a function of thermodynamic state parameters, but not their gradients or time rate of change. For the general case in which there are both mass and energy fluxes, the phenomenological equations may be written as:\n\nor, more concisely,\n\nwhere the entropic \"thermodynamic forces\" conjugate to the \"displacements\" formula_6 and formula_3 are formula_32 and formula_33 and formula_34 is the Onsager matrix of phenomenological coefficients.\n\nFrom the fundamental equation, it follows that:\nand\n\nUsing the continuity equations, the rate of entropy production may now be written:\n\nand, incorporating the phenomenological equations:\n\nIt can be seen that, since the entropy production must be greater than zero, the Onsager matrix of phenomenological coefficients formula_34 is a positive semi-definite matrix.\n\nOnsager's contribution was to demonstrate that not only is formula_34 positive semi-definite, it is also symmetric, except in cases where time-reversal symmetry is broken. In other words, the cross-coefficients formula_41 and formula_42 are equal. The fact that they are at least proportional follows from simple dimensional analysis (\"i.e.\", both coefficients are measured in the same units of temperature times mass density).\n\nThe rate of entropy production for the above simple example uses only two entropic forces, and a 2x2 Onsager phenomenological matrix. The expression for the linear approximation to the fluxes and the rate of entropy production can very often be expressed in an analogous way for many more general and complicated systems.\n\nLet formula_43 denote fluctuations from equilibrium values in several thermodynamic quantities, and let formula_44 be the entropy. Then, Boltzmann's entropy formula gives for the probability distribution function formula_45, \"A\"=const, since the probability of a given set of fluctuations formula_46 is proportional to the number of microstates with that fluctuation. Assuming the fluctuations are small, the probability distribution function can be expressed through the second differential of the entropy\n\nwhere we are using Einstein summation convention and formula_48 is a positive definite symmetric matrix.\n\nUsing the quasi-stationary equilibrium approximation, that is, assuming that the system is only slightly non-equilibrium, we have formula_49\n\nSuppose we define \"thermodynamic conjugate\" quantities as formula_50, which can also be expressed as linear functions (for small fluctuations): formula_51\n\nThus, we can write formula_52 where formula_53 are called \"kinetic coefficients\"\n\nThe \"principle of symmetry of kinetic coefficients\" or the \"Onsager's principle\" states that formula_54 is a symmetric matrix, that is formula_55\n\nDefine mean values formula_56 and formula_57 of fluctuating quantities formula_58 and formula_59 respectively such that they take given values formula_60 at formula_61 Note that formula_62\n\nSymmetry of fluctuations under time reversal implies that formula_63\n\nor, with formula_56, we have formula_65\n\nDifferentiating with respect to formula_66 and substituting, we get formula_67\n\nPutting formula_61 in the above equation, formula_69\n\nIt can be easily shown from the definition that formula_70, and hence, we have the required result.\n\n"}
{"id": "24935549", "url": "https://en.wikipedia.org/wiki?curid=24935549", "title": "Other World Kingdom", "text": "Other World Kingdom\n\nThe Other World Kingdom (frequently abbreviated OWK) was a large, commercial BDSM and femdom facility, resort, and micronation, which opened in 1996 using the buildings and grounds of a 16th-century chateau located in the municipality of Černá in Žďár nad Sázavou District, Czech Republic. Although not recognized by any other country, it maintained its own currency, passports, police force, courts, state flag and state hymn.\n\nThe Other World Kingdom was officially founded on June 1, 1996, and was open to visitors by the spring of 1997, after two years of construction costing £2 million.\n\nIt provided a D/s environment of a size and consistency not available at any other facility in the world.\n\nThe land and buildings were offered for sale in 2008 with an asking price of eight million euros. The sale particulars suggested the property was suitable for use as a hotel, restaurant, residence, or old people's home. As of 2016, it was still for sale.\n\nThe OWK styled itself as a matriarchy, where women rule. It also had strong BDSM and female dominance themes. The state's goal \"is to get as many male creatures under the unlimited rule of Superior Women on as much territory as possible.\" The OWK had always billed itself as a state; it was, however, a private for profit enterprise with no connection to, nor sanctioning by, any recognized sovereign state.\n\nThe OWK was supposedly ruled by Queen Patricia I, an absolute monarch whose coronation took place on May 30 and 31, 1997. She was able to amend laws and other legal issues. Her other roles include \"Sublime Supreme Administrator\" (\"supervision over all activities within the Area and the Office of the Supreme Administrator\"), \"Sublime Administrator of the Treasury\" (Financial issues) and of the Queen's court and the Queen's guard.\n\nBelow the Queen was a series of different classes. The first was the \"Sublime Ladies\" or \"Ladies citizens,\" who form the Kingdom's nobility. To become a citizen, a woman had to fulfill certain criteria. These were:\n\n\nThe next class was the Queen's subjects. These were men who followed OWK law, obey the Queen and pay her taxes, but have some rights such as \"freedom to travel, own property and deal with such property, have children, change employment, enterprise and state his opinion.\" The lowest class is the \"Slave\" class. This was a male class who had forfeited all rights, were property of the Queen or Sublime Ladies and were considered to be \"on the level of a normal farm animal\".\n\nThe site was 3 hectares (7.4 acres) in area, with several buildings, and a 250m oval track, small lake and grassed lawns. The main building was the Queen's Palace, which was the residence of the monarch, and contained a banqueting hall, library, throne room, torture chamber, schoolroom, gym, and extensive basement prison, the cells of which could be hired. Additional visitor accommodation was provided in the Long House, including the Countess Elizabeth Báthory Chambers complete with two torture chambers. This building also contained a swimming pool, pub, restaurant,\nand the Wanda Nightclub. The outdoor facilities were supplemented by a\nsawdust-covered indoor riding hall and stables.\n\n\n"}
{"id": "1884270", "url": "https://en.wikipedia.org/wiki?curid=1884270", "title": "Peace News", "text": "Peace News\n\nPeace News (PN) is a pacifist magazine first published on 6 June 1936 to serve the peace movement in the United Kingdom. From later in 1936 to April 1961 it was the official paper of the Peace Pledge Union (PPU), and from 1990 to 2004 was co-published with War Resisters' International.\n\n\"Peace News\" was begun by Humphrey Moore who was a Quaker and in 1933 had become editor of the National Peace Council's publications. Working with a peace group in Wood Green, London, Moore and his wife, Kathleen (playing the role of business manager), launched \"Peace News\" with a free trial issue in June 1936. With distribution through Moore’s contacts with the National Peace Council, the new magazine rapidly attracted attention. Within six weeks, Dick Sheppard, founder of the Peace Pledge Union, proposed to Moore that \"Peace News\" should become the PPU’s paper. Early contributors to this new organ of the PPU included Mohandas Gandhi, George Lansbury, and illustrator Arthur Wragg. \"Peace News\" also had a large number of women contributors, including Vera Brittain, Storm Jameson, Rose Macaulay, Ethel Mannin, Ruth Fry, Kathleen Lonsdale and Sybil Morrison.\n\nSome contributors were so sympathetic to the grievances of Nazi Germany that one sceptical member found it difficult to distinguish between letters to \"Peace News\" and those in the newspaper of the British Union of Fascists. The historian Mark Gilbert has argued that \"With the exception of \"Action\", the journal of the British Union of Fascists, it is hard to think of another British newspaper which was so consistent an apologist for Nazi Germany as \"Peace News\".\" However, Juliet Gardiner has noted that \"Peace News\" also urged the British government to give sanctuary to Jewish refugees from Nazism. The fact that some PN contributors were supporting appeasement and excusing Nazi actions caused PN contributor David Spreckley to express fears that \"in their scramble for peace\", they were gaining \"some questionable allies\".\n\nSales of \"Peace News\" peaked at around 40,000 during the so-called Phoney War between September 1939 and May 1940. In that month in the face of demands in parliament for the banning of the paper, the printer and distributors stopped working with \"Peace News\". However, with help from the typographer Eric Gill, Hugh Brock and many others, Moore continued to publish \"Peace News\" and arrange for distribution around the UK.\n\nHumphrey Moore’s emphasis on \"Peace News\" having a single-minded anti-war policy was increasingly being challenged. Others wanted greater emphasis on building a peaceful society once hostilities ended. In 1940 the PPU asked Moore to step aside in the post of assistant editor (which post he held until 1944), and appointed John Middleton Murry as editor. By 1946 Murry had abandoned pacifism and resigned.\n\nHugh Brock took on the role of assistant editor of Peace News in 1946 and became editor in 1955, lasting until 1964. During his period of tenure the magazine separated from the PPU as it had widened its focus into areas not directly related to absolute pacifism. \"Peace News\" in the 1940s published material from American journalist Dwight Macdonald and Maurice Cranston (later to become a noted philosopher).\n\nFrom the 1940s on, \"Peace News\" began to take a strongly critical line towards British rule in Kenya. The magazine also established links with African anti-colonial activists Kwame Nkrumah and Kenneth Kaunda, and \"\"Peace News\"′ close involvement with the anti-apartheid struggle...led to the banning of the paper in South Africa in 1959\". During the 1950s, \"Peace News\" contributors included such noted activists as André Trocmé, Martin Niemöller, Fenner Brockway, A. J. Muste, Richard B. Gregg, Alex Comfort, Donald Soper, Michael Scott, Leslie Hall, M.P., Muriel Lester, Emrys Hughes, M.P., Wilfred Wellock, and Esmé Wynne-Tyson\n\nIn 1959 a gift of £5,700 from Tom Willis enabled \"Peace News\" to buy 5 Caledonian Road, London, N1. This became its office and printing press and was also shared with Housmans Bookshop. It was at the \"Peace News\" office that the Nuclear Disarmament/Peace symbol was adopted. Describing the British pacifist tradition in the 1950s, David Widgery wrote \"at its most likeable it was the sombre decency of \"Peace News\", then a vegetarian tabloid with a Quaker emphasis on active witness\".\n\nThe magazine campaigned against nuclear weapons, often working with the Campaign for Nuclear Disarmament. During this period Brock brought to \"Peace News\" \"a staff of writer-activists committed to developing Gandhian nonviolent action in the anti-militarist cause\", including Pat Arrowsmith, Richard Boston, April Carter, Alan Lovell, Michael Randle, Adam Roberts and the American Gene Sharp. Brock's successor in 1964 was Theodore Roszak. In the same year, a Caribbean Quaker and \"PN\" writer, Marion Glean, \"contributed to a series of statements by post-colonial activists on 'race' in the run-up to the 1964 election, published by Theodore Roszak, editor of \"Peace News\".\" After the election, Glean helped bring together several activists, including David Pitt, C. L. R. James and Ranjana Ash to form the Campaign Against Racial Discrimination. Throughout the 1960s, \"Peace News\" covered issues such as opposition to the Vietnam War and the Biafran issue in the Nigerian Civil War. The magazine's coverage of the Vietnam War was notable for its support for the protests of the Vietnamese Buddhists, who it argued could become a nonviolent \"Third Force\" independent of both the Saigon and Hanoi governments. \"Peace News\" also ran lengthy analysis of left-wing thinkers, including E.P. Thompson's two-part study of C. Wright Mills and Theodore Roszak's assessment of Lewis Mumford.\n\nIn 1971 it added to its masthead the words \"for nonviolent revolution\".\n\nIn 1974, the paper moved its main office to Nottingham, where it remained until 1990.\n\nIn 1978, one worker at Housmans was injured after a bomb was sent to the \"Peace News\" offices, (allegedly by the neo-Nazi organisation Column 88) as part of a series of attacks on left-wing organisations (similar attacks were made on the Socialist Workers Party and Anti-Nazi League offices before this occurred).\n\n\"Peace News\" suspended publication at the end of 1987, intending to relaunch after a period of rethinking and planning. In May 1989 the paper resumed publication, but quickly ran into financial difficulties. In 1990 it became linked to War Resisters' International and was co-published as a monthly until 1999, then as a quarterly with a British-oriented \"Nonviolent Action\" published in the intervening months. \"Peace News\" came out strongly against the Iraq War while at the same time condemning Saddam Hussein. In 2005, \"Peace News\" resumed monthly publication, as an independent British publication and in a tabloid format.\n\nIn June 2014, \"Peace News\" ran an article calling for a \"Yes\" vote in the Scottish Independence Referendum.\n\n\"Peace News\" continues to be published in tabloid size print media and as a website by Peace News Ltd. It describes its editorial objectives as: to support and connect nonviolent and anti-militarist movements; provide a forum for such movements to develop common perspectives; take up issues suitable for campaigning; promote nonviolent, antimilitarist and pacifist analyses and strategies; stimulate thinking about the revolutionary implications of nonviolence. It is currently edited by Milan Rai and Emily Johns.\n\nThe \"Peace News\" archives are held at the Commonweal Collection in the J.B. Priestley Library, University of Bradford\n\nTony Benn has described \"Peace News\" as \" a paper that gives us hope...(it) should be widely read\".\n\n\"Peace News\" has been associated with initiating numerous campaigns, and a number of its staffmembers have been arrested for taking part in peace actions. In November 1957 Hugh Brock was one of three founders of the Direct Action Committee Against Nuclear War, which was run from the \"Peace News\" office and involved many \"Peace News\" staff. The DAC produced the first badges with the Nuclear Disarmament/Peace symbol, and organised various actions of civil disobedience against nuclear weapons and also the first of the Aldermaston Marches in Easter 1958.\n\nIn 1971 \"Peace News\", together with War Resisters' International, initiated a nonviolent direct action project, Operation Omega to Bangladesh, to challenge the Pakistani military blockade of then East Pakistan.\n\nIn the same year \"Peace News\" criticised the attempt to ban the sex education book \"The Little Red Schoolbook\", and reprinted extensive extracts from the publication in the magazine.\n\nIn 1972 \"Peace News\" co-editor Howard Clark, after meeting activists from the Canadian Greenpeace boats, initiated the group that became London Greenpeace, at first campaigning against French nuclear tests.\n\nIn 1973 \"Peace News\" played a central role in launching the British Withdrawal from Northern Ireland Campaign (BWNIC) and in supporting the \"BWNIC 14\", fourteen activists, including a member of the \"Peace News\" collective, charged with \"conspiracy to incite disaffection\" via a leaflet \"Some Information for Discontented Soldiers\". After an 11-week trial, a jury acquitted the BWNIC 14 in 1975, although two members of \"Peace News\" collective were fined for helping two AWOL soldiers go to Sweden.\n\nIn 1974, together with Nicholas Albery of BIT Information Service, \"Peace News\" began publishing the Community Levy for Alternative Projects, an invitation to supply funds for, generally, fledgling alternative projects, partly targeting shops and businesses that identified with counter-cultural ideas and aspirations.\n\nIn August 1974, \"Peace News\" published a special edition revealing and printing in full Colonel David Stirling's plans to establish\na strike-breaking \"private army\", \"Great Britain 1975\". By arrangement \"The Guardian\" led with this story on the day of publication, \"Peace News\" won the 1974 \"Scoop of the Year\" award from Granada Television.\n\nIn 1978, \"Peace News\", together with \"The Leveller\" magazine revealed the identity of Colonel B, a witness in the ABC Trial. \"Peace News\" fought its conviction for \"contempt of court\" right up to appeal in the House of Lords, where the Lord Chief Justice's \"guilty\" verdict was finally overturned.\n\nIn 1995, \"Peace News\", together with Campaign Against Arms Trade, was sued for libel by the Covert & Operational Procurement Exhibition (COPEX) for repeating allegations that the exhibition was serving as a meeting place for buyers and sellers of torture implements. The High Court struck out the case when COPEX failed to show in court and the peace groups were awarded costs.\nThe following is a partial list of \"Peace News\" publications.\n\nFenner Brockway, 1953.\n\n(Reprint: Introduced by Gene Sharp) 1963.\n\n"}
{"id": "7849222", "url": "https://en.wikipedia.org/wiki?curid=7849222", "title": "Photobacterium damselae subsp. piscicida", "text": "Photobacterium damselae subsp. piscicida\n\nPhotobacterium damselae\" subsp. \"piscicida (previously known as Pasteurella piscicida) is a gram-negative rod-shaped bacterium that causes disease in fish.\n\nPasteurellosis is also described as photobacteriosis (due to the change in the taxonomic position), is caused by the halophilic bacterium \"Photobacterium damselae\" subsp. \"piscicida\" (formerly \"Pasteurella piscicida\").\nIt was first isolated in mortalities occurring in natural populations of white perch (\"Morone americanus\") and striped bass (\"M. saxatilis\") in 1963 in Chesapeake Bay, USA (\"Snieszko \"et al.\", 1964\"). Since 1969, this disease has been one of the most important in Japan, affecting mainly yellowtail (\"Seriola quinqueradiata\") (Kusuda & Yamaoka, 1972). From 1990 it has caused economic losses in different European countries including France (Baudin-Laurencin \"et al.\", 1991), Italy (Ceschia \"et al.\", 1991), Spain (Toranzo \"et al.\", 1991), Greece (Bakopoulos \"et al.\", 1995), Turkey (Canand \"et al.\", 1996), Portugal (Baptista \"et al.\", 1996) and Malta (Bakopoulos \"et al.\", 1997). Gilthead sea bream (\"Sparus aurata\"), seabass (\"Dicentrarchus labrax\") and sole (\"Solea\" spp.) are the most affected species in Europe Mediterranean countries, as well as hybrid striped bass (\"M. saxatilis\" x \"M. chrysops\") in the USA. However, the natural hosts of the pathogen are a wide variety of marine fish (Romalde & Magariños, 1997).\n\nThis pathology is temperature dependent and occurs usually when water temperatures rise above 18-20 °C. Below this temperature, fish can harbour the pathogen as subclinical infection and become carriers for long time periods (Romalde, 2002).\n\nPastereullosis is also known as pseudotubercullosis because it is characterized by the presence, in the chronic form of the disease, of creamy-white granulomatous nodules or whitish tubercules in several internal organs, composed of masses of bacterial cells, epithelial cells, and fibroblasts. The nodules are most prominent in internal viscera, particularly kidney and spleen, and the infection is accompanied by widespread internal necrosis (Evelyn, 1996; Romalde, 2002; Barnes \"et al.\", 2005). Anorexia with darkening of the skin as well as focused necrosis of the gills are the only external clinical signs often observed. These lesions are generally missing in the acute form. The disease is difficult to eradicate with antibiotic treatments, and there is evidence that carriers under stressful conditions could suffer from reinfection (Le Breton, 1999).\n\nMorphologically, the bacteria is a rod shaped cell, with no motility.\nGram negative, with bipolar staining.\nThe presumptive identification of the pathogen is based on standard biochemical tests. In addition, although \"Ph. damselae\" subsp. \"piscicida\" is not included in the API-20E code index, this miniaturised system can also be useful for its identification, since all strains display the same profile (2005004). Slide agglutination test using specific antiserum is needed for a confirmative identification of the microorganism (Romalde, 2002).\n\nThe virulence of the pathogen implies the production of polysaccharide capsular layer, and extracellular products, and is also depending on iron availability (Lopez-Doriga \"et al.\", 2000). The bacteria spreads via infected phagocytes, mainly macrophages. This spread can be rapid, and lethal effects may occur within a few days of challenge, affecting tissues containing large numbers of the pathogens (Evelyn, 1996).\n\nNecrotizing fasciitis due to Photobacterium damsela in a man lashed by a stingray. Barber GR, Swygert JS. New England Journal of Medicine. 2000 342:824 [letter]\n\n"}
{"id": "4167915", "url": "https://en.wikipedia.org/wiki?curid=4167915", "title": "Plane of immanence", "text": "Plane of immanence\n\nPlane of immanence () is a founding concept in the metaphysics or ontology of French philosopher Gilles Deleuze. Immanence, meaning \"existing or remaining within\" generally offers a relative opposition to transcendence, that which is beyond or outside. Deleuze rejects the idea that life and creation are opposed to death and non-creation. He instead conceives of a plane of immanence that already includes life and death. \"Deleuze refuses to see deviations, redundancies, destructions, cruelties or contingency as accidents that befall or lie outside life; life and death were aspects of desire or the plane of immanence.\" This plane is a pure immanence, an unqualified immersion or embeddedness, an immanence which denies transcendence as a \"real distinction\", Cartesian or otherwise. Pure immanence is thus often referred to as a pure plane, an infinite field or smooth space without substantial or constitutive division. In his final essay entitled \"Immanence: A Life\", Deleuze writes: \"It is only when immanence is no longer immanence to anything other than itself that we can speak of a plane of immanence.\"\n\nThe plane of immanence is metaphysically consistent with Spinoza’s single substance (God or Nature) in the sense that immanence is not immanent \"to\" substance but rather that immanence \"is\" substance, that is, immanent to itself. Pure immanence therefore will have consequences not only for the validity of a philosophical reliance on transcendence, but simultaneously for dualism and idealism. Mind may no longer be conceived as a self-contained field, substantially differentiated from body (dualism), nor as the primary condition of unilateral subjective mediation of external objects or events (idealism). Thus all \"real distinctions\" (mind and body, God and matter, interiority and exteriority, etc.) are collapsed or flattened into an even consistency or plane, namely immanence itself, that is, immanence without opposition.\n\nThe plane of immanence thus is often called a plane of consistency accordingly. As a geometric plane, it is in no way bound to a mental design but rather an abstract or virtual design; which for Deleuze, is the metaphysical or ontological itself: a formless, univocal, self-organizing process which always qualitatively differentiates from itself. So in \"A Thousand Plateaus\" (with Félix Guattari), a plane of immanence will eliminate problems of preeminent forms, transcendental subjects, original genesis and real structures: \"Here, there are no longer any forms or developments of forms; nor are there subjects or the formation of subjects. There is no structure, any more than there is genesis.\" In this sense, Hegel’s Spirit (Geist) which experiences a self-alienation and eventual reconciliation with itself via its own linear dialectic through a material history becomes irreconcilable with pure immanence as it depends precisely on a pre-established form or order, namely Spirit itself. Rather on the plane of immanence there are only complex networks of forces, particles, connections, relations, affects and becomings: \"There are only relations of movement and rest, speed and slowness between unformed elements, or at least between elements that are relatively unformed, molecules, and particles of all kinds. There are only haecceities, affects, subjectless individuations that constitute collective assemblages. [...] We call this plane, which knows only longitudes and latitudes, speeds and haecceities, the plane of consistency or composition (as opposed to a plan(e) of organization or development).\"\n\nThe plane of immanence necessitates an immanent philosophy. Concepts and representations may no longer be considered vacuous forms awaiting content (concept of x, representation of y) but become active productions in themselves, constantly affecting and being affected by other concepts, representations, images, bodies etc. In their final work together, \"What is Philosophy?\", Deleuze and Guattari state that the plane of immanence constitutes \"the absolute ground of philosophy, its earth or deterritorialization, the foundation on which it creates its concepts.\"\n\nThe concept of the plane itself is significant as it implies that immanence cannot simply be conceived as the \"within\", but also as the \"upon\", as well as the \"of\". A lobster is not simply within a larger system, but folds from that very same system, functioning and operating consistently upon it, with it and through it, immanently mapping its environment, discovering its own dynamic powers and kinetic relations, as well as the relative limits of those powers and relations. Thus, without a theoretical reliance on transcendent principles, categories or real divisions producing relative breaks or screens of atomistic enclosure, the concept of the plane of immanence may replace nicely any benefits of a philosophical transcendentalism: \"Absolute immanence is in itself: it is not in something, \"to\" something; it does not depend on an object or belong to a subject. [...] When the subject or the object falling outside the plane of immanence is taken as a universal subject or as any object to which immanence is attributed, [...] immanence is distorted, for it then finds itself enclosed in the transcendent.\"\n\nFinally, Deleuze offers that pure immanence and life will suppose one another unconditionally: \"We will say of pure immanence that it is A LIFE, and nothing else. [...] A life is the immanence of immanence, absolute immanence: it is complete power, complete bliss.\" This is not some abstract, mystical notion of life but \"a\" life, a specific yet impersonal, indefinite life discovered in the real singularity of events and virtuality of moments. \"A\" life is subjectless, neutral, and preceding all individuation and stratification, is present in all things, and thus always immanent to itself. \"\"A\" life is everywhere [...]: an immanent life carrying with it the events and singularities that are merely actualized in subjects and objects.\"\n\nAn ethics of immanence will disavow its reference to judgments of good and evil, right and wrong, as according to a transcendent model, rule or law. Rather the diversity of living things and particularity of events will demand the concrete methods of immanent evaluation (ethics) and immanent experimentation (creativity). These twin concepts will become the basis of a lived Deleuzian ethic.\n\n\n\n"}
{"id": "1306405", "url": "https://en.wikipedia.org/wiki?curid=1306405", "title": "Postfeminism", "text": "Postfeminism\n\nThe term postfeminism (alternatively rendered as post-feminism) is used to describe reactions against contradictions and absences in feminism, especially second-wave feminism and third-wave feminism. The term \"postfeminism\" is sometimes confused with subsequent feminisms such as 4th wave-feminism, and \"women of color feminism\" (e.g. hooks, 1996; Spivak, 1999).\n\nThe ideology of postfeminism is recognized by its contrast with prevailing or preceding feminism. Postfeminism strives towards the next stage in gender-related progress, and as such is often conceived as in favor of a society that is no longer defined by gender binary and gender roles. A postfeminist is a person who believes in, promotes, or embodies any of various ideologies springing from the feminism of the 1970s, whether supportive of or antagonistic towards classical feminism.\n\nPostfeminism can be considered a critical way of understanding the changed relations between feminism, popular culture and femininity. Postfeminism may also present a critique of second-wave feminism or third-wave feminism by questioning their binary thinking and essentialism, their vision of sexuality, and their perception of relationships between femininity and feminism. It may also complicate or even deny entirely the notion that absolute gender equality is necessary, desirable or realistically achievable.\n\nSecond-wave feminism is often critiqued for being too \"white\", too \"straight\", and too \"liberal\", thus resulting in the needs of women from marginalized groups and cultures being ignored. However, since intersectionality is a product of third-wave feminism, the references to such as postfeminist are open to challenge and may be more properly considered feminist.\n\nWhile \"postfeminism\" was first used in the 1980s to describe a backlash against second-wave feminism, it is now used as a label for a wide range of theories that take critical approaches to previous feminist discourses and includes challenges to the second wave's ideas. It may also be used to invoke the view that feminism is no longer relevant to today's society.\n\nOver the years, the meaning of postfeminism has broadened in scope, encompassing many different meanings, as is the case with feminism. Within feminist literature, definitions tend to fall into two main categories: 1) “death of feminism”, “anti-feminism”, “feminism is irrelevant now” and 2) the next stage in feminism, or feminism that intersects with other “post-” philosophies/theories, such as postmodernism, post-structuralism and postcolonialism.\n\nIn 1919, a journal was launched in which \"female literary radicals\" stated \"'we're interested in people now—not in men and women'\", that \"moral, social, economic, and political standards 'should not have anything to do with sex'\", that it would \"be 'pro-woman without being anti-man'\", and that \"their stance [is called] 'post-feminist'\".\n\nThe term was used in the 1980s to describe a backlash against second-wave feminism. Postfeminism is now a label for a wide range of theories that take critical approaches to previous feminist discourses and includes challenges to the second wave's ideas. Other postfeminists say that feminism is no longer relevant to today's society. Amelia Jones has written that the postfeminist texts which emerged in the 1980s and 1990s portrayed second-wave feminism as a monolithic entity and were overly generalizing in their criticism.\n\nThe 1990s saw the popularization of this term, in both the academic world as well as the media world. It was seen as a term of both commendation and scorn. Toril Moi, a professor at Duke University, originally coined the term in 1985 in Sexual/Textual politics to advocate a feminism that would deconstruct the binary between equality based on \"liberal\" feminism and difference-based or \"radical\" feminism. There is confusion surrounding the intended meaning of \"post\" in the context of \"postfeminism\". This confusion has plagued the very meaning of \"postfeminism\" since the 1990s. While the term has seemed on the one hand to announce the end of feminism, on the other hand it has itself become a site of feminist politics.\n\nCurrently, feminist history is characterized by the struggle to find out the present situation—often articulated as a concern about whether there is still such a thing called \"feminism\"—by writing in the past. It is here that the meaning of \"post\" as a historical break is troubling, for \"post\" offers to situate feminism in history by proclaiming the end of this history. It then confirms feminist history as a thing of the past. However, some claim that it is impossible that feminism could be aligned with \"post\" when it is unthinkable, as it would be the same as calling the current world a post racist, post-classist, and post-sexist society.\n\nThe early part of the 1980s was when the media began labeling teenage women and women in their twenties the \"postfeminist generation\". After twenty years, the term postfeminist is still used to refer to young women, \"who are thought to benefit from the women's movement through expanded access to employment and education and new family arrangements but at the same time do not push for further political change\", Pamela Aronson, Professor of Sociology, asserts. Postfeminism is a highly debated topic since it implies that feminism is \"dead\" and \"because the equality it assumes is largely a myth\".\n\nAccording to Prof. D. Diane Davis, postfeminism is just a continuation of what first- and second-wave feminisms want.\n\nResearch conducted at Kent State University narrowed postfeminism to four main claims: support for feminism declined; women began hating feminism and feminists; society had already attained social equality, thus making feminism outdated; and the label \"feminist\" was disliked due to negative stigma.\n\nIn her 1994 book \"Who Stole Feminism? How Women Have Betrayed Women\", Christina Hoff Sommers considers much of modern academic feminist theory and the feminist movement to be gynocentric. She labels this \"gender feminism\" and proposes \"equity feminism\"—an ideology that aims for full civil and legal equality. She argues that while the feminists she designates as gender feminists advocate preferential treatment and portray women as victims, equity feminism provides a viable alternative form of feminism. These descriptions and her other work have caused Hoff Sommers to be described as an antifeminist by some other feminists.\n\nSome contemporary feminists, such as Katha Pollitt or Nadine Strossen, consider feminism to hold simply that \"women are people.\" Views that separate the sexes rather than unite them are considered by these writers to be \"sexist\" rather than \"feminist\".\n\nAmelia Jones has authored post-feminist texts which emerged in the 1980s/1990s and portrayed second-wave feminism as a monolithic entity and criticized it using generalizations.\n\nOne of the earliest modern uses of the term was in Susan Bolotin's 1982 article \"Voices of the Post-Feminist Generation\", published in \"New York Times Magazine\". This article was based on a number of interviews with women who largely agreed with the goals of feminism, but did not identify as feminists.\n\nSusan Faludi, in her 1991 book \"\", argued that a backlash against second wave feminism in the 1980s had successfully re-defined feminism through its terms. She argued that it constructed the women's liberation movement as the source of many of the problems alleged to be plaguing women in the late 1980s. She also argued that many of these problems were illusory, constructed by the media without reliable evidence. According to her, this type of backlash is a historical trend, recurring when it appeared that women had made substantial gains in their efforts to obtain equal rights.\n\nAngela McRobbie argued that adding the prefix \"post-\" to \"feminism\" undermined the strides that feminism made in achieving equality for everyone, including women. In McRobbie's opinion, postfeminism gave the impression that equality has been achieved and feminists could now focus on something else entirely. McRobbie believed that postfeminism was most clearly seen on so-called feminist media products, such as \"Bridget Jones's Diary, Sex and the City\", and \"Ally McBeal\". Female characters like Bridget Jones and Carrie Bradshaw claimed to be liberated and clearly enjoy their sexuality, but what they were constantly searching for was the one man who would make everything worthwhile.\n\nRepresentations of post feminism can be found in pop culture. Postfeminism has been seen in media as a form of feminism that accepts popular culture instead of rejecting it, as was typical with second wave feminists. Many popular shows from the 90s and early 2000s are considered to be postfeminist works because they tend to focus on women who are empowered by popular cultural representations of other women. Because of this, postfeminists claimed that such media was more accessible and inclusive than past representations of women in the media; however, some feminists believe that postfeminist works focus too much on white, middle-class women. Such shows and movies include \"The Devil Wears Prada, , The Princess Diaries,\" and \"Buffy the Vampire Slayer.\" Another example is \"Sex and the City\". Carrie Bradshaw from \"Sex and the City\" is an example of a character living a post feminist life. While her character attempts to live a sexually liberated lifestyle, Bradshaw is stuck endlessly pursuing the love and validation of a man. The balance between Bradshaw's independent life as a successful columnist and desire to find a husband exemplifies the tension of post feminism. Many of these works also involve women monitoring their appearance as a form of self-management, be it in the form of dieting, exercise, or—most popularly—makeover scenes. Postfeminist literature—also known as chicklit—has been criticized by feminists for similar themes and notions. However, the genre is also praised for being confident, witty, and complicated, bringing in feminist themes, revolving around women, and reinventing standards of fiction. Examples can also be found in \"Pretty Little Liars\". The novels explore the complexity of girlhood in a society that assumes gender equality, which is in line with postfeminism. The constant surveillance and self policing of the series' protagonists depicts the performance of heterosexuality, hyperfemininity, and critical gaze forced upon girls. The materialism and performance from the girls in \"Pretty Little Liars\" critiques the notion that society has full gender equality, and thus offers a critique of postfeminism.\n\nIn an article on print jewelry advertisements in Singapore, Michelle Lazar analyses how the construction of 'postfeminist' femininity has given rise to a neo-liberal hybrid \"pronounced sense of self or 'I-dentity'\". She states that the increasing number of female wage earners has led to advertisers updating their image of women but that \"through this hybrid postfeminist I-dentity, advertisers have found a way to reinstall a new normativity that coexists with the status quo\". Postfeminist ads and fashion have been criticized for using femininity as a commodity veiled as liberation.\n\n\n"}
{"id": "22424", "url": "https://en.wikipedia.org/wiki?curid=22424", "title": "Recapitulation theory", "text": "Recapitulation theory\n\nThe theory of recapitulation, also called the biogenetic law or embryological parallelism—often expressed using Ernst Haeckel's phrase \"ontogeny recapitulates phylogeny\"—is a historical hypothesis that the development of the embryo of an animal, from fertilization to gestation or hatching (ontogeny), goes through stages resembling or representing successive stages in the evolution of the animal's remote ancestors (phylogeny). It was formulated in the 1820s by Étienne Serres based on the work of Johann Friedrich Meckel, after whom it is also known as Meckel–Serres law.\n\nSince embryos also evolve in different ways, the shortcomings of the theory had been recognized by the early 20th century, and it had been relegated to \"biological mythology\" by the mid-20th century.\n\nAnalogies to recapitulation theory have been formulated in other fields, including cognitive development and art criticism.\n\nThe idea of recapitulation was first formulated in biology from the 1790s onwards by the German natural philosophers Johann Friedrich Meckel, Étienne Serres, and Carl Friedrich Kielmeyer, after which, Marcel Danesi states, it soon gained the status of a supposed biogenetic law.\n\nThe embryological theory was formalised by Serres in 1824–26, based on Meckel's work, in what became known as the \"Meckel-Serres Law\". This attempted to link comparative embryology with a \"pattern of unification\" in the organic world. It was supported by Étienne Geoffroy Saint-Hilaire, and became a prominent part of his ideas. It suggested that past transformations of life could have been through environmental causes working on the embryo, rather than on the adult as in Lamarckism. These naturalistic ideas led to disagreements with Georges Cuvier. The theory was widely supported in the Edinburgh and London schools of higher anatomy around 1830, notably by Robert Edmond Grant, but was opposed by Karl Ernst von Baer's ideas of divergence, and attacked by Richard Owen in the 1830s.\n\nErnst Haeckel (1834–1919) attempted to synthesize the ideas of Lamarckism and Goethe's \"Naturphilosophie\" with Charles Darwin's concepts. While often seen as rejecting Darwin's theory of branching evolution for a more linear Lamarckian view of progressive evolution, this is not accurate: Haeckel used the Lamarckian picture to describe the ontogenetic and phylogenetic history of individual species, but agreed with Darwin about the branching of all species from one, or a few, original ancestors. Since early in the twentieth century, Haeckel's \"biogenetic law\" has been refuted on many fronts.\n\nHaeckel formulated his theory as \"Ontogeny recapitulates phylogeny\". The notion later became simply known as the recapitulation theory. Ontogeny is the growth (size change) and development (structure change) of an individual organism; phylogeny is the evolutionary history of a species. Haeckel claimed that the development of advanced species passes through stages represented by adult organisms of more primitive species. Otherwise put, each successive stage in the development of an individual represents one of the adult forms that appeared in its evolutionary history.\n\nFor example, Haeckel proposed that the pharyngeal grooves between the pharyngeal arches in the neck of the human embryo not only roughly resembled gill slits of fish, but directly represented an adult \"fishlike\" developmental stage, signifying a fishlike ancestor. Embryonic pharyngeal slits, which form in many animals when the thin branchial plates separating pharyngeal pouches and pharyngeal grooves perforate, open the pharynx to the outside. Pharyngeal arches appear in all tetrapod embryos: in mammals, the first pharyngeal arch develops into the lower jaw (Meckel's cartilage), the malleus and the stapes. But these embryonic pharyngeal arches, grooves, pouches, and slits in human embryos can not at any stage carry out the same function as the gills of an adult fish.\n\nHaeckel produced several embryo drawings that often overemphasized similarities between embryos of related species. The misinformation was propagated through many biology textbooks, and popular knowledge, even today. Modern biology rejects the literal and universal form of Haeckel's theory, such as its possible application to behavioural ontogeny, i.e. the psychomotor development of young animals and human children.\n\nHaeckel's drawings were such a misrepresentation of observed human embryonic development that he attracted the opposition of several members of the scientific community, including the anatomist Wilhelm His, who had developed a rival \"causal-mechanical theory\" of human embryonic development. His specifically criticised Haeckel's methodology. His argued that the shapes of embryos were caused most immediately by mechanical pressures resulting from local differences in growth. These differences were, in turn, caused by \"heredity\". His compared the shapes of embryonic structures to those of rubber tubes that could be slit and bent, illustrating these comparisons with accurate drawings. Stephen Jay Gould's attack on Haeckel's recapitulation theory was far more fundamental than that of any empirical critic, as it effectively stated that Haeckel's \"biogenetic law\" was irrelevant.\n\nDarwin's view was that embryos resembled each other, since they shared a common ancestor, which presumably had a similar embryo, but that development did not necessarily recapitulate phylogeny: in his view, there was no reason to suppose that an embryo at any stage resembled an adult of any ancestor. Darwin supposed further that embryos were subject to less intense selection pressure than adults, and had therefore changed less.\n\nModern evolutionary developmental biology (evo-devo) follows von Baer, rather than Darwin, in pointing to active evolution of embryonic development as a significant means of changing the morphology of adult bodies. Two of the key principles of evo-devo, namely that changes in the timing (heterochrony) and positioning within the body (heterotopy) of aspects of embryonic development would change the shape of a descendant's body compared to an ancestor's, were however first formulated by Haeckel in the 1870s. These elements of his thinking about development have thus survived, whereas his theory of recapitulation has not.\n\nThe Haeckelian form of recapitulation theory is considered defunct.\nHowever, embryos do undergo a period where their morphology is strongly shaped by their phylogenetic position, rather than selective pressures. The modern view is summarised by the University of California Museum of Paleontology:\n\nThe idea that ontogeny recapitulates phylogeny has been applied to some other areas.\n\nResearch in the late 20th century confirmed that \"both biological evolution and the stages in the child's cognitive development follow much the same progression of evolutionary stages as that suggested in the archaeological record\".\n\nEnglish philosopher Herbert Spencer was one of the most energetic promoters of evolutionary ideas to explain many phenomena. He compactly expressed the basis for a cultural recapitulation theory of education in the following claim, published in 1861, five years before Haeckel first published on the subject: \n\nG. Stanley Hall used Haeckel's theories as the basis for his theories of child development. His most influential work, \"Adolescence: Its Psychology and Its Relations to Physiology, Anthropology, Sociology, Sex, Crime, Religion and Education\" in 1904 suggested that each individual's life course recapitulated humanity's evolution from \"savagery\" to \"civilization\". Though he has influenced later childhood development theories, Hall's conception is now generally considered racist. \nDevelopmental psychologist Jean Piaget favored a weaker version of the formula, according to which ontogeny \"parallels\" phylogeny because the two are subject to similar external constraints.\nThe Austrian pioneer in psychoanalysis, Sigmund Freud, also favored Haeckel's doctrine. He was trained as a biologist under the influence of recapitulation theory at the time of its domination, and retained a Lamarckian outlook with justification from the recapitulation theory. He also distinguished between physical and mental recapitulation, in which the differences would become an essential argument for his .\n\nThe musicologist Richard Taruskin in 2005 applied the term \"ontogeny becomes phylogeny\" to the process of creating and recasting art history, often to assert a perspective or argument. For example, the peculiar development of the works by modernist composer Arnold Schoenberg (here an \"ontogeny\") is generalized in many histories into a \"phylogeny\" – a historical development (\"evolution\") of Western music toward atonal styles of which Schoenberg is a representative. Such historiographies of the \"collapse of traditional tonality\" are faulted by art historians as asserting a rhetorical rather than historical point about tonality's \"collapse\".\n\nTaruskin also developed a variation of the motto into the pun \"ontogeny recapitulates ontology\" to refute the concept of \"absolute music\" advancing the socio-artistic theories of the musicologist Carl Dahlhaus. Ontology is the investigation of what exactly something is, and Taruskin asserts that an art object becomes that which society and succeeding generations made of it. For example, composer Johann Sebastian Bach's \"St. John Passion\", composed in the 1720s, was appropriated by the Nazi regime in the 1930s for propaganda. Taruskin claims the historical development of the \"Passion\" (its ontogeny) as a work with an anti-Semitic message does, in fact, inform the work's identity (its ontology), even though that was an unlikely concern of the composer. Music or even an abstract visual artwork can not be truly autonomous (\"absolute\") because it is defined by its historical and social reception.\n\n\n\n"}
{"id": "37756305", "url": "https://en.wikipedia.org/wiki?curid=37756305", "title": "Secondary causation", "text": "Secondary causation\n\nSecondary Causation is the philosophical proposition that all material and corporeal objects, having been created by God with their own intrinsic potentialities, are subsequently empowered to evolve independently in accordance with natural law. Traditional Christians would slightly modify this injunction to allow for the occasional miracle as well as the exercise of free will. Deists who deny any divine interference past the creation event would only accept free will exceptions. That the physical universe is consequentially well-ordered, consistent, and knowable subject to human observation and reason, was a primary theme of Scholasticism and further molded into the philosophy of the Western Tradition by Augustine and later by Aquinas.\n\nSecondary causation has been suggested as a necessary precursor for scientific inquiry into an established order of natural laws which are not entirely predicated on the changeable whims of a supernatural Being. Nor does this create a conflict between science and religion for, given a Creator, it is not inconsistent with the paradigm of a clockwork universe. It does however remove logical contradictions concerning the unfettered expression of man’s free will which would otherwise require not just God’s acquiescence but rather His direct intervention to implement.\n\nAccording to the Jewish Torah which brought down the original idea in Genesis, the phrase \"free will\" is a mistranslation from the Torah, rather what humans are given is \"freedom to choose\".\nFreedom to choose to do God's Will at all times even though God gave us a good inclination and a not-good inclination to use in choosing, we are told \"Therefore choose life\".\n\nOccasionalism itself was derived from the earlier school of thought of “volunteerism” emanating from Al-Ash'ari who held that every particle in the universe must be constantly recreated each instant by God’s direct intervention.\n\nAccording to the Kabbalah and brought down in Chasidic philosophy in the book \"Tania, the Book of Intermediates\" composed by Rabbi Shneur Zalman of Liadi (Russia) at the beginning of the 19th century, \nthe will and desire to create the universe is integral to the Creator's very essence & thought this being the source for all the physical and spiritual worlds.\n\nOnce the Creator has created the universe and God knows & wants the Creation as the One who created it in His very essence, God then enlivens and vivifies all parts of the universe at every moment or the physical universe & the many spiritual worlds would revert instantly to their source in the Creator from where they came.\nAt the same time as molecules move, human cells divide the Creator must know the Creation as it was a moment ago and makes allowance for the finite Creations to grow and later slowly wither, and by the evaporation of water, the wearing down of rocks and soil, the birth, growth and weakening of the flesh of fish, animals and humans all Creations as they are built from the 4 main spiritual worlds which also break down, they have a mirror on earth of air, fire, water, dust and slowly all created beings wear down by God's constant enlivening of them until each of the 4 elements return to their spiritual source which mirror the 4 elements. This is not an independent of God activity, rather it is all controlled by God's Will.\n\nTorah explains that before Creation there was only God and nothing else as is seen in the highest Name the letter \"Yud\".\nWhen it came time for Creation the want and will of God to Create a universe\nwhich meant expansion of the Holy Name the Holy Name Yud-Hey-Vav-Hey how this Creation came about by G-D's use of the 10 Sefirot would be too great a task to explain here but basically we find \"And God said let there be...\"\n\nThe Creation made no change in the Creator God was, is and always will be but the Creation is available to those Created as an order always vivified by God since God must know the Creation in order to keep it as it is, yet allow for His change according to his will. The body sees a Creation while the soul sees only The Infinite One God.\n\nHaving the gift from God, \"the Freedom to choose\" to serve God and always do God's Will here on earth makes us partners in creation. Being that God's Will was revealed to the People of Israel on Mount Sinai and spread to the world in the Torah we have Freedom to choose to do so. If a human does the opposite of God's Will it is in God's realm to alter the cosmic plan of Creation that He Himself devised, that He Himself wants but since nothing exists but God, including the universe and this \"nothing\" is not above the knowledge that \"nothing is too hard for God\" and \"Our wisdom is not His Wisdom\", it is a fundamental theme in Torah that we must do God's will yet we have Freedom to serve or not and if we go against God's will, it is still a lack on our part as partners with God here on earth but this itself is God's Will and will not upset the cosmic original plan.\n\nWe see this when Shimi cursed King David and threw stones at him that King David did not get angry since he realized that Shimi was an agent from God himself or Shimi could not possibly use his physical God given talents to speak or throw stones if God did not want. Even though Shimi was not told by God, Shimi used his Freedom to Serve in the wrong way, but if Shimi had not cursed David at God's Will, \"God has many messengers to do His will\"\n\nIn fact, with Adam and Eve there in, the Tree of the Knowledge of Good and Evil was created before Adam himself by God and this is all part of His plan, to turn away from evil.\n\nThe concept of there being two distinct truths, even concerning the same object or phenomena, was most notably developed by Averroes (1126-1198 from Spain). By thus separating the sanctity of religious revelation from the practical world of physical observation, it was an attempt to circumvent proscriptions on the discredited rationalist heresy of Muʿtazila, which had heretofore not gained traction in any venue.\n\nFollowing Augustine and many others, this concept of double truth was soundly rejected by Aquinas in his Summa Theologiae which reiterated the long established view in the West that there can be only one truth. The original quote from Augustine was\n\n\"In matters that are obscure and far beyond our vision, even in such as we may find treated in Holy Scripture, different Interpretations are sometimes possible without prejudice to the faith we have received. In such a case, we should not rush in headlong and so firmly take our stand on one side that, if further progress in the search of truth justly undermines this position, we too fall with it. That would be to battle not for the teaching of Holy Scripture but for our own, wishing its teaching to conform to ours, whereas we ought to wish ours to conform to that of Sacred Scripture.\" \nThe assignment of intrinsic qualities to objects which can mutate and evolve of their own accord without divine intervention was a crucial step in the transformation of the rational logic of the Greeks into the scientific method in the Western Tradition of the late Middle Ages. Because man could thus observe and characterize the natural flow of events without impugning the prerogatives of supernatural forces, burgeoning philosopher-scientists became free to experiment and especially to question and debate the results.\n\nIn Western Europe this rationale was further strengthened by the motivation that science was uniquely able not only to efficiently manage the world as charged to do so in Genesis but also to be able to distinguish miracles from natural occurrences.\n\nOne of the first to take advantage of this opportunity was Albertus Magnus of Cologne (1193-1206), who wrote\n\"In studying nature we have not to inquire how God the Creator may, as He freely wills, use His creatures to work miracles and thereby show forth His power; we have rather to inquire what Nature with its immanent causes can naturally bring to pass.\"\nThis sentiment was echoed in various European forums of the day notably by the secular Professor of Theology at the University of Paris, John Buridan (1300-1361) who liberally commented on the works of Aristotle.\n“It should also be noted that [when we ask whether metaphysics is the same as wisdom,] we are not comparing metaphysics to theology, which proceeds from beliefs that are not known, because although these beliefs are not known per se and most evident, we hold without doubt that theology is the more principal discipline and that it is wisdom most properly speaking. In this question, however, we are merely asking about intellectual habits based on human reason, [i.e.,] those discovered by the process of reasoning, which are deduced from what is evident to us. For it is in this sense that Aristotle calls metaphysics ‘theology’ and ‘the divine science’. Accordingly, metaphysics differs from theology in the fact that although each considers God and those things that pertain to divinity, metaphysics only considers them as regards what can be proved and implied, or inductively inferred, by demonstrative reason.”\n"}
{"id": "1639158", "url": "https://en.wikipedia.org/wiki?curid=1639158", "title": "Sentimentality", "text": "Sentimentality\n\nSentimentality originally indicated the reliance on feelings as a guide to truth, but current usage defines it as an appeal to shallow, uncomplicated emotions at the expense of reason.\n\nSentimentalism in philosophy is a view in meta-ethics according to which morality is somehow grounded in moral sentiments or emotions. Sentimentalism in literature is both a device used to induce a tender emotional response disproportionate to the situation at hand, (and thus to substitute heightened and generally uncritical feeling for normal ethical and intellectual judgments), and a heightened reader response willing to invest previously prepared emotions to respond disproportionately to a literary situation.\n\n\"A sentimentalist\", Oscar Wilde wrote, \"is one who desires to have the luxury of an emotion without paying for it.\" In James Joyce's \"Ulysses\", Stephen Dedalus sends Buck Mulligan a telegram that reads \"The sentimentalist is he who would enjoy without incurring the immense debtorship for a thing done.\" James Baldwin considered that \"Sentimentality, the ostentatious parading of excessive and spurious emotion, is the mark of dishonesty, the inability to feel...the mask of cruelty\". This Side of Paradise by F. Scott Fitzgerald contrasts sentimentalists and romantics with Amory Blaine telling Rosalind, “I'm not sentimental--I'm as romantic as you are. The idea, you know, is that the sentimental person thinks things will last--the romantic person has a desperate confidence that they won't.” \n\nIn the mid-18th century, a querulous lady had complained to Richardson: \"What, in your opinion, is the meaning of the word \"sentimental\", so much in vogue among the polite...Everything clever and agreeable is comprehended in that word...such a one is a \"sentimental\" man; we were a \"sentimental\" party\". What she was observing was the way the term was becoming a European obsession - part of the Enlightenment drive to foster the individual's capacity to recognise virtue at a visceral level. Everywhere in the sentimental novel or the sentimental comedy, \"lively and effusive emotion is celebrated as evidence of a good heart\". Moral philosophers saw sentimentality as a cure for social isolation; and Adam Smith indeed considered that \"the poets and romance writers, who best paint...domestic affections, Racine and Voltaire; Richardson, Maurivaux and Riccoboni; are, in such cases, much better instructors than Zeno\" and the Stoics.\n\nBy the close of the century, however, a reaction had occurred against what had come to be considered sentimental excess, by then (and perhaps still now) seen as false and self-indulgent - especially after Schiller's 1795 division of poets into two classes, the \"naive\" and the \"sentimental\" - and regarded respectively as natural and as artificial.\n\nIn modern times \"sentimental\" is a pejorative term that has been casually applied to works of art and literature that exceed the viewer or reader's sense of decorum—the extent of permissible emotion—and standards of taste: \"excessiveness\" is the criterion; \"Meretricious\" and \"contrived\" sham pathos are the hallmark of sentimentality, where the morality that underlies the work is both intrusive and pat.\n\n'Sentimentality often involves situations which evoke very intense feelings: love affairs, childbirth, death', but where the feelings are expressed with 'reduced intensity and duration of emotional experience...diluted to a safe strength by idealisation and simplification'.\n\nNevertheless, as a social force sentimentality is a hardy perennial, appearing for example as 'Romantic sentimentality...in the 1960s slogans \"flower power\" and \"make love not war\"'. The 1990s public outpouring of grief at the death of Diana, 'when they go on about fake sentimentality in relation to Princess Diana', also raised issues about the 'powerful streak of sentimentality in the British character' - the extent to which 'sentimentality was a grand old national tradition'.\n\nBaudrillard has cynically attacked the sentimentality of Western humanitarianism, suggesting that 'in the New Sentimental Order, the affluent become consumers of the \"ever more delightful spectacle of poverty and catastrophe, and of the moving spectacle of our own attempts to alleviate it\"'. There is also the issue of what has been called 'indecent sentimentality...[in] pornographical pseudo-classics', so that one might say for example that ' \"Fanny Hill\" is a \"very\" sentimental novel, a faked Eden'.\n\nHowever, in sociology it is possible to see the \"sentimental tradition\" as extending into the present-day - to see, for example, 'Parsons as one of the great social philosophers in the sentimental tradition of Adam Smith, Burke, McLuhan, and Goffman...concerned with the relation between the rational and sentimental bases of social order raised by the market reorientation of motivation'. Francis Fukuyama takes up the theme through the exploration of 'society's stock of shared values as \"social capital\" '.\n\nIn a 'subjective confession' of 1932, 'Ulysses: a Monologue', the analytic psychologist Carl Jung anticipates Baudrillard when he writes: 'Think of the lamentable role of popular sentiment in wartime! Think of our so-called humanitarianism! The psychiatrist knows only too well how each of us becomes the helpless but not pitiable victim of his own sentiments. Sentimentality is the superstructure erected upon brutality. Unfeelingness is the counter-position and inevitably suffers from the same defects.' [Carl Jung: The Spirit in Man, Art and Literature, London: Routledge, 2003, p. 143]\n\nComplications enter into the ordinary view of sentimentality, however, when changes in fashion and setting— the \"climate of thought\"—intrude between the work and the reader. The view that sentimentality is relative is inherent in John Ciardi's \"sympathetic contract\", in which the reader agrees to join with the writer when approaching a poem. The example of the death of Little Nell in Charles Dickens' \"The Old Curiosity Shop\" (1840–41), \"a scene that for many readers today might represent a defining instance of sentimentality\", brought tears to the eye of many highly critical readers of the day. The reader of Dickens, Richard Holt Hutton observed, \"has the painful impression of pathos feasting upon itself.\"\n\n'Recent feminist theory has clarified the use of the term as it applies to the genre' of the sentimental novel, stressing the way that 'different cultural assumptions arising from the oppression of women gave liberating significance to the works' piety and mythical power to the ideals of the heroines'.\n\nThe sentimental fallacy is an ancient rhetorical device that attributes human emotions, such as grief or anger, to the forces of nature. This is also known as the pathetic fallacy, 'a term coined by John Ruskin...for the practice of attributing human emotions to the inanimate or unintelligent world' - as in 'the sentimental poetic trope of the \"pathetic fallacy\", beloved of Theocritus, Virgil and their successors' in the pastoral tradition.\n\nThe term is also used more indiscriminately to discredit any argument as being based on a misweighting of emotion: 'sentimental fallacies...that men, that we, are better - nobler - than we know ourselves to be'; 'the \"sentimental fallacy\" of constructing novels or plays \"out of purely emotional patterns\"'.\n\n\n"}
{"id": "21073801", "url": "https://en.wikipedia.org/wiki?curid=21073801", "title": "Smart power", "text": "Smart power\n\nIn international relations, the term smart power refers to the combination of hard power and soft power strategies. It is defined by the Center for Strategic and International Studies as \"an approach that underscores the necessity of a strong military, but also invests heavily in alliances, partnerships, and institutions of all levels to expand one's influence and establish legitimacy of one's action.\"\n\nJoseph Nye, former Assistant Secretary of Defense for International Security Affairs under the Clinton administration and author of several books on smart power strategy, suggests that the most effective strategies in foreign policy today require a mix of hard and soft power resources. Employing only hard power or only soft power in a given situation will usually prove inadequate. Nye utilizes the example of terrorism, arguing that combatting terrorism demands smart power strategy. He advises that simply utilizing soft power resources to change the hearts and minds of the Taliban government would be ineffective and requires a hard power component. In developing relationships with the mainstream Muslim world, however, soft power resources are necessary and the use of hard power would have damaging effects.\n\nAccording to Chester A. Crocker, smart power \"involves the strategic use of diplomacy, persuasion, capacity building, and the projection of power and influence in ways that are cost-effective and have political and social legitimacy\"essentially the engagement of both military force and all forms of diplomacy.\n\nThe origin of the term \"smart power\" is under debate and has been attributed to both Suzanne Nossel and Joseph Nye.\n\nSuzanne Nossel, Deputy to Ambassador Holbrooke at the United Nations during the Clinton administration, is credited with coining the term in an article in Foreign Affairs entitled, \"Smart Power: Reclaiming Liberal Internationalism\", in 2004. In her article in the CNN, she has criticized the Trump administration for its \"tunnel-vision\" foreign policy that neglects both soft power and smart power. She writes: \"..Trump seems oblivious toward the brand value of what Joseph Nye has called the 'soft power' that comes from projecting appealing aspects of American society and character abroad. He is also indifferent to my own concept of 'smart power,'or the imperative to engage a broad range of tools of statecraft, from diplomacy to aid to private sector engagement to military intervention.\"\n\nJoseph Nye, however, claims that smart power is a term he introduced in 2003 \"to counter the misperception that soft power alone can produce effective foreign policy.\" He created the term to name an alternative to the hard power-driven foreign policy of the Bush administration. Nye notes that smart power strategy denotes the ability to combine hard and soft power depending on whether hard or soft power would be more effective in a given situation. He states that many situations require soft power; however, in stopping North Korea's nuclear weapons program, for instance, hard power might be more effective than soft power. In the words of the \"Financial Times\", \"to win the peace, therefore, the US will have to show as much skill in exercising soft power as it has in using hard power to win the war.\" Smart power addresses multilateralism and enhances foreign policy.\n\nA successful smart power narrative for the United States in the twenty-first century, Nye argues, will not obsess over power maximization or the preservation of hegemony. Rather, it will find \"ways to combine resources into successful strategies in the new context of power diffusion and the 'rise of the rest.'\" A successful smart power strategy will provide answers to the following questions: 1) What goals or outcomes are preferred? 2) What resources are available and in which contexts? 3) What are the positions and preferences of the targets of attempts at influence? 4) Which forms of power behavior are most likely to succeed? 5) What is the probability of success?\n\nSince the period of \"Pax Britannica\" (1815–1914) the United Kingdom has employed a combination of influence and coercion in international relations.\n\nThe term smart power emerged in the past decade, but the concept of smart power has much earlier roots in the history of the United States and is a popular notion in international relations today.\n\n1901: President Theodore Roosevelt proclaims: \"Speak softly and carry a big stick.\"\n\n1948: The United States initiates major peacetime soft power programs under the authority of the Smith-Mundt Act, including broadcasting, exchange and information world wide to combat the outreach of the Soviet Union.\n\n1991: The end of the Cold War was marked by the collapse of the Berlin Wall, which fell as a result of a combination of hard and soft power. Throughout the Cold War, hard power was used to deter Soviet aggression and soft power was used to erode faith in Communism. Joseph Nye said: \"When the Berlin Wall finally collapsed, it was destroyed not by artillery barrage but by hammers and bulldozers wielded by those who had lost faith in communism.\"\n\n2004: Joseph S. Nye introduces the term \"smart power\" in his book, \"Soft Power: The Means to Success in World Politics\". \"Smart power is neither hard nor soft. It is both,\" he writes. In an article in \"Foreign Affairs\", analyst Suzanne Nossel uses the term \"smart power\". For Nossel, \"Smart power means knowing that the United States' own hand is not always its best tool: U.S. interests are furthered by enlisting others on behalf of U.S. goals.\"\n\n2007: In light of 9/11 and the war in Iraq, the Bush administration was criticized for placing too much emphasis on a hard power strategy. To counter this hard power strategy, the Center for Strategic and International Studies released the \"Commission on Smart Power\" to introduce the concept of smart power into discussion on which principles should guide the future of U.S. foreign policy in light of 9/11 and the war in Iraq. The report identifies five critical areas of focus for the U.S.: Alliances, Global Development, Public Diplomacy, Economic Integration, and Technology and Innovation. According to the report, these five goals constitute smart foreign policy and will help the United States achieve the goal of \"American preeminence as an agent of good.\"\n\n2009: The Center for Strategic and International Studies, released a second report, \"Investing in a New Multilateralism\", to address the concept of smart power in international releases. This report addressed the United Nations as an instrument of U.S. smart power. By collaborating with the UN, the U.S. can lead the way in reinvigorating multilateralism within in the international community in the 21st century.\n\n2009: Under the Obama administration, smart power became a core principle of his foreign policy strategy. It was popularized by Hillary Clinton during her Senate confirmation hearing on January 13, 2009 for the position of Secretary of State: We must use what has been called smart power---the full range of tools at our disposal---diplomatic, economic, military, political, legal, and cultural---picking the right tool, or combination of tools, for each situation. With smart power, diplomacy will be the vanguard of foreign policy. Both Suzanne Nossel and Joseph Nye were supportive of Clinton's encouragement of smart power, since it would popularize the use of smart power in U.S. foreign policy. That popularization has been accompanied by more frequent use of the term, and David Ignatius describes it as an \"overused and vapid phrase meant to connote the kind of power between hard and soft\".\n\n2010: The \"First Quadrennial Diplomacy and Development Review (QDDR)\" entitled, \"Leading through Civilian Power\", called for the implementation of a smart power strategy through civilian leadership.\n\n2011: Obama's \"2011 May Speech on the Middle East and North Africa\" called for a smart power strategy, incorporating development, in addition to defense and diplomacy, as the third pillar of his foreign policy doctrine.\n\nThe UK government Strategic Defence and Security Review 2015 was based on a combination of hard power and soft power strategies. Following the Poisoning of Sergei and Yulia Skripal in 2018, the National Security Review described a \"fusion doctrine\", that will combine resources from British intelligence agencies, the British Armed Forces, foreign relations and economic considerations to defeat the UK's enemies.\n\nIn recent years, some scholars have sought to differentiate smart power further from soft power, while also including military posture and other tools of statecraft as part of a broad smart power philosophy. Christian Whiton, a State Department official during the George W. Bush administration, described smart power in a 2013 book, \"\", as: \"the many financial, cultural, rhetorical, economic, espionage-related, and military actions that states can take short of general war to influence political outcomes abroad,\" adding, \"It most crucially should involve a revival of political warfare: the non-violent push of ideas, people, facts, and events with which our adversaries would rather not contend.\" Whiton recalled U.S. political influence activities from the Cold War, including CIA-backed programs like the Congress for Cultural Freedom, and called for adapting these to contemporary challenges to the U.S. posed by China, Iran, and Islamists.\n\nAccording to \"Dealing with Today's Asymmetric Threat to U.S. and Global Security\", a symposium sponsored by CACI, an effective smart power strategy faces multiple challenges in transitioning from smart power as a theory to smart power in practice. Applying smart power today requires great difficulty, since it operates in an environment of asymmetric threats, ranging from cybersecurity to terrorism. These threats exist in a dynamic international environment, adding yet another challenge to the application of smart power strategy. In order to effectively address asymmetric threats arising in a dynamic international environment, the symposium suggests addressing the following factors: rule of law, organizational roadblocks, financing smart power, and strategic communications.\n\nIn order to implement smart power approaches on both a domestic and international level, the United States must develop a legal framework for the use of smart power capabilities. Developing a legal foundation for smart power, however, demands a clear concept of these asymmetric threats, which is often difficult. The cyber domain, for instance, presents an extremely nebulous concept. Hence, the challenge will be conceptualizing asymmetric threats before formulating a legal framework.\n\nThe inability to promote smart power approaches because of organizational failures within agencies presents another obstacle to successful smart power implementation. Agencies often lack either the appropriate authority or resources to employ smart power. The only way to give smart power long-term sustainability is to address these organizational failures and promote the coordination and accessibility of hard and soft power resources.\n\nWith the ongoing financial crisis, the dire need for financial resources presents a critical obstacle to the implementation of smart power. According to Secretary Gates, 'there is a need for a dramatic increase in spending on the civilian instruments of national security---diplomacy, strategic communications, foreign assistance, civic action, and economic reconstruction and development.\" In order to successfully implement smart power, the U.S. budget needs to be rebalanced so that non-military foreign affairs programs receive more funding. Sacrificing defense spending will, however, be met with stalwart resistance.\n\n\"Asymmetries of perception,\" according to the report, are a major obstacle to strategic communications. A long-term smart power strategy will mitigate negative perceptions by discussing the nature of these threats and making a case for action using smart power strategy. The report states that the central theme of our strategic communications campaign should be education of our nation in our values as a democratic nation and in the nature of the threats our nation faces today.\n\nOf all the tools at the disposal of smart power strategists in the United States, experts suggest that the U.N. is the most critical. The Center for Strategic and International Studies issued a report, \"Investing in a New Multilateralism\", in January 2009 to outline the role of the United Nations as an instrument of U.S. smart power strategy. The report suggests that in an increasingly multipolar world, the UN cannot be discarded as outdated and must be regarded as an essential tool to thinking strategically about the new multilateralism that our nation faces. An effective smart power strategy will align the interests of the U.S. and the UN, thereby effectively addressing threats to peace and security, climate change, global health, and humanitarian operations.\n\nAs announced by Secretary of State Hillary Clinton in November 2011, the United States will begin to shift its attention to the Asia-Pacific region, making the strategic relationship between the U.S. and China of supreme importance in determining the future of international affairs in the region. The Center for Strategic and International Studies, in \"Smart Power in U.S.-China Relations,\" offers recommendations for building a cooperative strategic relationship between the U.S. and China through smart power strategy. Rather than relying on unilateral action, the U.S. and China should combine their smart power resources to promote the global good and enhance the peace and security of the region. The report recommends the following policy objectives: implement an aggressive engagement agenda, launch an action agenda on energy and climate, and institute a new dialogue on finance and economics. Overall, the report suggests that U.S.-Sino relations should be pursued without the black-and-white view of China as either benign or hostile, but rather, as a partner necessary in serving the interests of the U.S. and the region while promoting the global good.\n\nThe Obama administration continually stresses the importance of smart power strategy in relations with the Middle East and especially Turkey due to its increasing leadership role as a regional soft power. As not only an Islamic democratic nation but also the only Muslim member of NATO, Turkey's leverage in the region could inspire other nations to follow in its footsteps. By establishing a cooperative relationship with Turkey and working to clarify misunderstandings through smart power, Turkey could eventually become the bridge between the East and the West. A smart power approach to U.S.-Turkish relations will expand the leadership role of Turkey in the region and increases its strategic importance to NATO.\n\nCondoleezza Rice, Bush's Secretary of State, coined the term \"Transformational Diplomacy\" to denote Bush's policy to promote democracy through a hard power driven strategy. \"Transformational diplomacy\" stands at odds with \"smart power,\" which utilizes hard and soft power resources based on the situation. The Obama administration's foreign policy was based on smart power strategy, attempting to strike a balance between defense and diplomacy.\n\nIn an interview with the Boston Globe, interviewer Anna Mundow, questioned Joseph Nye over the criticism that smart power is the friendly face of American imperialism. By the same token, the Bush doctrine has also been criticized for being \"imperialistic,\" by focusing on American power over partnerships with the rest of the world. Joseph Nye defends smart power by noting that criticism often stems from a misunderstanding of the smart power theory. Nye himself designed the theory to apply to any nation of any size, not just the United States. It was meant to be a more sophisticated method of thinking about power in the context of the information age and post-9/11 world.19 President Obama defined his vision for U.S. leadership as \"not in the spirit of a patron but the spirit of a partner.\"\n\nKen Adelman, in an article entitled \"Not-So-Smart Power,\" argues that there is no correlation between U.S. aid and the ability of America to positively influence events abroad. He points out that the nations who receive the most foreign aid, such as Egypt and Pakistan, are no more in tune with American values than those who receive less or no U.S. foreign aid. Overall, he criticizes the instruments of smart power, such as foreign aid and exchange programs, for being ineffective in achieving American national interests.\n\nIn the application of smart power in U.S. strategy, Ted Galen Carpenter, author of the work \"Smart Power\"', criticizes U.S. foreign policy for failing to question outdated alliances, such as NATO. Carpenter articulated his disapproval of interventionist foreign policy, saying, \"America does not need to be — and should not aspire to be — a combination global policeman and global social worker.\" Rather than utilizing antiquated institutions, the U.S. should rethink certain alliances in arriving at a new vision for the future of American foreign policy. Carpenter fears that America's domestic interests will be sacrificed in favor of global interests through smart power. Essentially, interventionist foreign policies advocated by U.S. smart power strategies undercut domestic liberties.\n\n\n"}
{"id": "903495", "url": "https://en.wikipedia.org/wiki?curid=903495", "title": "Spacing effect", "text": "Spacing effect\n\nThe spacing effect is the phenomenon whereby learning is greater when studying is spread out over time, as opposed to studying the same amount of content in a single session. That is, it is better to use spaced presentation rather than massed presentation. Practically, this effect suggests that \"cramming\" (intense, last-minute studying) the night before an exam is not likely to be as effective as studying at intervals in a longer time frame. It is important to note, however, that the benefit of spaced presentations does not appear at short retention intervals, in which massed presentations tend to lead to better memory performance. This effect is a desirable difficulty; it challenges the learner but leads to better learning in the long-run.\n\nThe phenomenon was first identified by Hermann Ebbinghaus, and his detailed study of it was published in the 1885 book \"Über das Gedächtnis. Untersuchungen zur experimentellen Psychologie\" (\"Memory: A Contribution to Experimental Psychology\"). This robust finding has been supported by studies of many explicit memory tasks such as free recall, recognition, cued-recall, and frequency estimation (for reviews see Crowder 1976; Greene, 1989).\n\nResearchers have offered several possible explanations of the spacing effect, and much research has been conducted that supports its impact on recall. In spite of these findings, the robustness of this phenomenon and its resistance to experimental manipulation have made empirical testing of its parameters difficult.\n\nDecades of research on memory and recall have produced many different theories and findings on the spacing effect. In a study conducted by Cepeda et al. (2006) participants who used spaced practice on memory tasks outperformed those using massed practice in 259 out of 271 cases.\n\nAs different studies support different aspects of this effect, some now believe that an appropriate account should be multifactorial, and at present, different mechanisms are invoked to account for the spacing effect in free recall and in explicit cued-memory tasks.\n\nNot much attention has been given to the study of the spacing effect in long-term retention tests. Shaughnessy (1977) found that the spacing effect is not robust for twice-presented items after a 24-hour delay in testing. The spacing effect is present, however, for items presented four or six times and tested after a 24-hour delay. The result was interesting because other studies using only twice-presented items have shown a strong spacing effect, although the lag between learning and testing was longer. Shaughnessy interprets it as evidence that no single explanatory mechanism can be used to account for the various manifestations of the spacing effect.\n\nResearch has shown reliable spacing effects in cued-memory tasks under incidental learning conditions, where semantic analysis is encouraged through orienting tasks (Challis, 1993; Russo & Mammaralla, 2002). Challis found a spacing effect for target words using a frequency estimation task after words were incidentally analyzed semantically. However, no spacing effect was found when the target words were shallowly encoded using a graphemic study task. This suggests that semantic priming underlies the spacing effect in cued-memory tasks. When items are presented in a massed fashion, the first occurrence of the target semantically primes the mental representation of that target, such that when the second occurrence appears directly after the first, there is a reduction in its semantic processing. Semantic priming wears off after a period of time (Kirsner, Smith, Lockhart, & King, 1984), which is why there is less semantic priming of the second occurrence of a spaced item. Thus on the semantic priming account, the second presentation is more strongly primed and receives less semantic processing when the repetitions are massed compared to when presentations are spaced over short lags (Challis, 1993). This semantic priming mechanism provides spaced words with more extensive processing than massed words, producing the spacing effect.\n\nFrom this explanation of the spacing effect, it follows that this effect should not occur with nonsense stimuli that do not have a semantic representation in memory. A number of studies have demonstrated that the semantically based repetition priming approach cannot explain spacing effects in recognition memory for stimuli such as unfamiliar faces, and non-words that are not amenable to semantic analysis (Russo, Parkin, Taylor, & Wilks, 1998; Russo et al., 2002; Mammarella, Russo, & Avons, 2005). Cornoldi and Longoni (1977) have even found a significant spacing effect in a forced-choice recognition memory task when nonsense shapes were used as target stimuli. Russo et al. (1998) proposed that with cued memory of unfamiliar stimuli, a short-term perceptually-based repetition priming mechanism supports the spacing effect. When unfamiliar stimuli are used as targets in a cued-memory task, memory relies on the retrieval of structural-perceptual information about the targets. When the items are presented in a massed fashion, the first occurrence primes its second occurrence, leading to reduced perceptual processing of the second presentation. Short-term repetition-priming effects for nonwords are reduced when the lag between prime and target trials is reduced from zero to six (McKone, 1995), thus it follows that more extensive perceptual processing is given to the second occurrence of spaced items relative to that given to massed items. Hence, nonsense items with massed presentation receive less extensive perceptual processing than spaced items; thus, the retrieval of those items is impaired in cued-memory tasks.\n\nCongruent with this view, Russo et al. (2002) demonstrated that changing the font in which repeated presentations of nonwords were presented reduced the short-term perceptual priming of those stimuli, especially for massed items. Upon a recognition memory test, there was no spacing effect found for the nonwords presented in different fonts during study. These results support the hypothesis that short-term perceptual priming is the mechanism that supports the spacing effects in cued-memory tasks when unfamiliar stimuli are used as targets. Furthermore, when the font was changed between repeated presentations of words in the study phase, there was no reduction of the spacing effect. This resistance to the font manipulation is expected with this two-factor account, as semantic processing of words at study determines performance on a later memory test, and the font manipulation is irrelevant to this form of processing.\n\nMammarella, Russo, & Avons (2002) also demonstrated that changing the orientation of faces between repeated presentations served to eliminate the spacing effect. Unfamiliar faces do not have stored representations in memory, thus the spacing effect for these stimuli would be a result of perceptual priming. Changing orientation served to alter the physical appearance of the stimuli, thus reducing the perceptual priming at the second occurrence of the face when presented in a massed fashion. This led to equal memory for faces presented in massed and spaced fashions, hence eliminating the spacing effect.\n\nAccording to the \"encoding variability\" view, spaced repetition typically entails some variability in presentation contexts, resulting in a greater number of retrieval cues. Contrastingly, massed repetitions have limited presentations and therefore fewer retrieval cues.\n\nTo test the Encoding Variability theory, Bird, Nicholson and Ringer (1978) presented subjects with word lists that either had massed or spaced repetitions. Subjects were asked to perform various \"orienting tasks,\" tasks which require the subject to make a simple judgment about the list item (i.e. pleasant or unpleasant, active or passive). Subjects either performed the same task for each occurrence of a word or a different task for each occurrence. If the Encoding Variability theory were true, then the case of different orienting tasks ought to provide variable encoding, even for massed repetitions, resulting in a higher rate of recall for massed repetitions than would be expected. The results showed no such effect, providing strong evidence against the importance of Encoding Variability.\n\nA theory that has gained a lot of traction recently is the \"study-phase retrieval theory\". This theory assumes that the first presentation is retrieved at the time of the second. This leads to an elaboration of the first memory trace. Massed presentations do not yield advantages because the first trace is active at the time of the second, so it is not retrieved or elaborated upon. Greene (1989) proposed a two-factor account of the spacing effect, combining deficient processing and study-phase retrieval accounts. Spacing effects in free recall are accounted for by the study-phase retrieval account. Under the assumption that free recall is sensitive to contextual associations, spaced items are at an advantage over massed items by the additional encoding of contextual information. Thus, the second occurrence of an item in a list reminds the observer of the previous occurrence of that same item and of its previous contextual features. Different contextual information is encoded with each presentation, whereas for massed items, the difference in context is relatively small. More retrieval cues, then, are encoded with spaced learning, which in turn leads to improved recall.\n\nAccording to the \"deficient processing\" view, massed repetitions lead to deficient processing of the second presentation—that we simply do not pay much attention to the later presentations (Hintzman et al., 1973). Greene (1989) proposed this to be the case in cued-memory tasks (e.g. recognition memory, frequency estimation tasks), which rely more on item information and less on contextual information. The increased voluntary rehearsal of spaced items makes this deficient processing noticeable. Findings that the spacing effect is not found when items are studied through incidental learning support this account.\n\nAccording to research conducted by Pyc and Rawson (2009) successful but effortful retrieval tasks during practice enhance memory in an account known as the \"retrieval effort hypothesis\". Spacing out the learning and relearning of items leads to a more effortful retrieval which provides for deeper processing of the item.\n\nThe spacing effect and its underlying mechanisms have important applications to the world of advertising. For instance, the spacing effect dictates that it is not an effective advertising strategy to present the same commercial back-to-back (massed repetition). If encoding variability is an important mechanism of the spacing effect, then a good advertising strategy might include a distributed presentation of different versions of the same ad.\n\nAppleton-Knapp, Bjork and Wickens (2005) examined the effects of spacing on advertising.\n\nThey found that spaced repetitions of advertisements are more affected by study-phase retrieval processes than encoding variability. They also found that at long intervals, varying the presentation of a given ad is not effective in producing higher recall rates among subjects (as predicted by variable encoding). Despite this finding, recognition is not affected by variations in an ad at long intervals.\n\nAlthough it is accepted that spacing is beneficial in learning a subject well and previous units should be revisited and practiced, textbooks are written in discrete chapters that do not support these findings. Rohrer conducted a two-part study in 2006 where students were taught how to solve math problems. In part 1, students either used mass or spaced practice, and spaced practice showed significant improvement over mass practice when tested one week later. In the second part of the experiment, practice problems were either grouped by type or mixed randomly. The desirable difficulties encountered by the randomly mixed problems were effective, and the performance by students who solved the randomly mixed problems was vastly superior to the students who solved the problems grouped by type. The reasoning behind this increased performance was that students know the formula for solving equations, but do not always know when to apply the formula. By shuffling problems around and dispersing them across multiple chapters, students also learn to identify when it is appropriate to use which formula. There is conclusive evidence that cumulative final exams promote long-term retention by forcing spaced learning to occur.\n\nThe long-term effects of spacing have also been assessed in the context of learning a foreign language. Bahrick et al. (1993) examined the retention of newly learned foreign vocabulary words as a function of relearning sessions and intersession spacing over a 9-year period.\n\nBoth the amount of relearning session and the number of days in between each session have a major impact on retention (the repetition effect and the spacing effect), yet the two variables do not interact with each other.\n\nFor all three difficulty rankings of the foreign words, recall was highest for the 56-day interval as opposed to a 28-day or a 14-day interval. Additionally, 13 sessions spaced 56 days apart yielded comparable retention to 26 sessions with a 14-day interval.\n\nThese findings have implications for educational practices. Current school and university curricula rarely provide students with opportunities for periodic retrieval of previously acquired knowledge. Without spaced repetitions, students are more likely to forget foreign language vocabulary.\n\nWhile the spacing effect refers to improved recall for spaced versus successive (mass) repetition, the term 'lag' can be interpreted as the time interval between repetitions of learning. The \"lag effect\" is simply an idea branching off the spacing effect that states recall after long lags between learning is better versus short lags. Michael Kahana's study showed strong evidence that the lag effect is present when recalling word lists. In 2008, Kornell and Bjork published a study that suggested inductive learning is more effective when spaced than massed. Inductive learning is learning through observation of exemplars, so the participants did not actively take notes or solve problems. These results were replicated and backed up by a second independent study.\n\n\n\n"}
{"id": "21102203", "url": "https://en.wikipedia.org/wiki?curid=21102203", "title": "Stable module category", "text": "Stable module category\n\nIn representation theory, the stable module category is a category in which projectives are \"factored out.\"\n\nLet \"R\" be a ring. For two modules \"M\" and \"N\", define formula_1 to be the set of \"R\"-linear maps from \"M\" to \"N\" modulo the relation that \"f\" ~ \"g\" if \"f\" − \"g\" factors through a projective module. The stable module category is defined by setting the objects to be the \"R\"-modules, and the morphisms are the equivalence classes formula_1.\n\nGiven a module \"M\", let \"P\" be a projective module with a surjection formula_3. Then set formula_4 to be the kernel of \"p\". Suppose we are given a morphism formula_5 and a surjection formula_6 where \"Q\" is projective. Then one can lift \"f\" to a map formula_7 which maps formula_4 into formula_9. This gives a well-defined functor formula_10 from the stable module category to itself.\n\nFor certain rings, such as Frobenius algebras, formula_10 is an equivalence of categories. In this case, the inverse formula_12 can be defined as follows. Given \"M\", find an injective module \"I\" with an inclusion formula_13. Then formula_14 is defined to be the cokernel of \"i\". A case of particular interest is when the ring \"R\" is a group algebra.\n\nThe functor Ω can even be defined on the module category of a general ring (without factoring out projectives), as the cokernel of the injective envelope. It need not be true in this case that the functor Ω is actually an inverse to Ω. One important property of the stable module category is it allows defining the Ω functor for general rings. When \"R\" is perfect (or \"M\" is finitely generated and \"R\" is semiperfect), then Ω(\"M\") can be defined as the kernel of the projective cover, giving a functor on the module category. However, in general projective covers need not exist, and so passing to the stable module category is necessary.\n\nNow we suppose that \"R = kG\" is a group algebra for some field \"k\" and some group \"G\". One can show that there exist isomorphisms\nfor every positive integer \"n\". The group cohomology of a representation \"M\" is given by formula_16 where \"k\" has a trivial \"G\"-action, so in this way the stable module category gives a natural setting in which group cohomology lives. \n\nFurthermore, the above isomorphism suggests defining cohomology groups for negative values of \"n\", and in this way, one recovers Tate cohomology.\n\nAn exact sequence \nin the usual module category defines an element of formula_18, and hence an element of formula_19, so that we get a sequence\nTaking formula_12 to be the translation functor and such sequences as above to be exact triangles, the stable module category becomes a triangulated category.\n\n\n"}
{"id": "6143251", "url": "https://en.wikipedia.org/wiki?curid=6143251", "title": "State dinner", "text": "State dinner\n\nA state dinner or state lunch is a dinner or banquet paid for by a government and hosted by a head of state in his or her official residence in order to renew and celebrate diplomatic ties between the host country and the country of a foreign head of state or head of government who was issued an invitation. It may form part of a state visit or diplomatic conference. In many countries around the world, there are many different rules governed by protocol. State dinners often consist of, but are not limited to, black tie or white tie dress, military honor guards, a four or five course meal, musical entertainment, dancing, and speeches made on behalf of the head of state hosting the state dinner as well as the foreign head of state.\n\nIn India, state banquets are held for foreign heads of state and government at the Rashtrapati Bhavan in New Delhi and are hosted by the President of India. Over one hundred guests usually attend state banquets, including members of the Government of India such as the Vice-President of India, the Prime minister of India, and prominent members of the ruling \nparty. Indian and foreign business leaders also attend. \n\nAt the beginning of a state banquet, a foreign head of state is greeted by the president in the North Drawing Room. A tent constructed in the Mughal Garden within the environs of the presidential palace is the outdoor setting for state banquets. During the evening, the gardens are lit up with earthen diyas, string lights, and decorated with flowers and rangoli that become a scene for entertainment. After a performance by Rajasthani singers, Indian percussion instruments like the mridangam, tabla, ghatam and khanjeera, as well as India's diverse classical dances in which Bharatnatyam, Odissi and Kathak are carefully choreographed, will all be showcased in front of the guests.\n\nInside the tent, speeches highlighting bilateral diplomatic relations are delivered by the President of India and the foreign head of state. The guests are then offered a sumptuous meal of Indian delicacies while the Indian Navy Band performs music.\n\nState banquets follow an official arrival ceremony which occurs at the Rashtrapati Bhavan earlier in the day.\n\nIn the United Kingdom, state dinners are hosted by the head of state, who is the British sovereign, currently Queen Elizabeth II. Traditionally all state dinners were held at Buckingham Palace because of its position in the heart of London. However, in recent years, banquets have also been held at Windsor Castle in Berkshire. Organisation of the state dinner usually falls to the Master of the Household with a seating plan confirmed both by the Queen and the Foreign and Commonwealth Office. State dinners are usually held for visiting heads of state and are very elaborate; Royal Protocol is generally very strict but this has been played down over recent years. All speeches that are read are again checked and confirmed by the Foreign office, and amended. Gifts are exchanged by both parties.\n\nIn the United States, a state dinner is a formal dinner, more often black tie in recent years rather than white tie, which is held in honor of a foreign head of state, such as a king, queen, president, or any head of government. A state dinner is hosted by the President of the United States and held in the State Dining Room at the White House in Washington D.C. Other formal dinners for important people of other nations, such as a prince or princess, are called official dinners, the difference being that the federal government does not pay for them. \n\nState and official dinners are dictated by strict protocol in order to ensure that no diplomatic gaffes occur. The Chief of Protocol of the United States, who is an official within the United States Department of State, the White House Chief Usher, who is head of the household staff at the White House, as well as the White House Social Secretary all oversee the planning of state and official dinners from beginning to end. The Graphics and Calligraphy Office located in the East Wing of the White House also bears numerous responsibilities. The White House Chief Calligrapher creates place cards with the names of the guests who are assigned seats around the tables in the State Dining Room. The Chief Calligrapher also designs and writes formal invitations that are mailed to the postal addresses of the guests. State dinners require close coordination between the White House Executive Chef and the White House Executive Pastry Chef who plan and prepare a four or five-course meal, as well as the White House Chief Floral Designer who arranges flowers and decorations on the candle-lit tables.\n\nAs is customary for all incoming state visits by foreign heads of state, a state dinner follows a State Arrival Ceremony which occurs on the South Lawn earlier in the day. In addition, state dinners held in recent years are also given media coverage by the public affairs TV channel, C-SPAN.\n\nIn the early 19th century, dinners honoring the president's Cabinet, Congress, or other dignitaries were called 'state dinners' even though they lacked official foreign representation. Under such conditions, large receptions and dinners were a rare occurrence as Washington, D.C., society was a collection of isolated villages widely separated and at times almost inaccessible. Times changed and so did the nation’s capital as a series of state dinners were held every winter social season to honor Congress, the Supreme Court, and members of the diplomatic corps.\nIn the late 19th century, the term state dinner became synonymous with a dinner hosted by the president honoring a foreign head of state. The first visiting head of state to attend a state dinner at the White House was King David Kalakaua of the Kingdom of Hawaii, hosted by Ulysses S. Grant on December 12, 1874. \n\nThe restoration of the White House by the architectural firm McKim, Mead, and White in 1902 created a more proper setting for official entertainment to occur. When the president's office moved to the newly constructed West Wing, the Neoclassical remodeling of the Executive Residence's state rooms gave Theodore Roosevelt a perfect venue reflecting the United States' growing power and influence around the world. While the White House underwent a complete interior reconstruction from 1948 to 1952, Harry S. Truman and Bess Truman lived at Blair House and state dinners were held in local hotels in the nation's capital. Long banquet tables were always used in the State Dining Room prior to the administration of John F. Kennedy. However, these were permanently discarded by Jacqueline Kennedy and replaced with round tables which could seat a far greater number of guests, approximately 120 to 140, in such a tight and confined space. To this day, presidents and first ladies continue to add their own personal touches and flair in entertaining foreign guests of state at the White House, having full access to the vermeil collection of gilded candelabras and flatware, the President's House crystal pattern, as well as the priceless collection of White House china which dates from the James Monroe administration to the George W. Bush administration, for use at a state dinner.\n\nDuring a state dinner, honor guards and color guards in full dress uniform from all branches of the United States Armed Forces are dispatched for ceremonial duty at the White House. At the North Portico entrance of the White House, the President of the United States and the First Lady of the United States formally greet the visiting head of state and his or her spouse, who have arrived in a motorcade from Blair House, the traditional guest quarters of foreign heads of state and dignitaries, or from a foreign ambassador's residence in the area of Embassy Row in Northwest, Washington, D.C. A brief photo opportunity for the media at the top of the staircase will occur. The president and first lady then escort the visiting head of state and his or her spouse to the Yellow Oval Room for a reception on the residence floor where the president's guests are served hors d'œuvres, cocktails, wine, or champagne. The president and first lady also introduce their guests to a wide array of people from the United States such as ambassadors, diplomats, members of Congress, members of the president's Cabinet, and other prominent people such as celebrities and Hollywood A-list movie stars invited at the discretion of the president and first lady. \n\nAfter the informal reception in the Yellow Oval Room, the president and the foreign head of state, followed by the first lady and the foreign head of state's spouse, descend the Grand Staircase to the Entrance Hall on the state floor where they are met by the United States Marine Band, \"The President's Own\". Four \"ruffles and flourishes\", immediately followed by \"Hail to the Chief\", serves as the fanfare for the president's arrival. Often, the national anthem of the foreign head of state's country as well as the \"Star Spangled Banner\" are performed. \n\nAfter a receiving line whereby the president introduces the visiting head of state to all of the invited guests, the president and the visiting head of state, his or her spouse, and the first lady, walk down the Cross Hall and proceed to the State Dining Room where a four or five-course meal, typically consisting of an appetizer/soup, fish, meat, salad and dessert, are served to the guests. The menu planned in advance for a state dinner and prepared by the White House Executive Chef and White House Executive Pastry Chef centers around the national cuisine of the visiting foreign head of state, using local ingredients, flavors, and ethnic foods. Before eating the meal, both the president and the visiting foreign head of state give a speech on a lectern, paying tribute to diplomatic relations between the United States and the foreign head of state's country. Members of the \"Strolling Strings,\" violinists from the United States Marine Band \"The President's Own,\" disperse throughout the State Dining Room and perform for the guests seated around the candle-lit tables. After the meal, the guests are seated in the East Room and are formally entertained by a musical ensemble such as a pianist, a singer, an orchestra, or band of national renown. On past occasions, dancing has also been a component at the conclusion of a state dinner.\n\nIn Switzerland, the head of the state is the Swiss Federal Council (not only its president). For this reason, the seven Federal Councillors (and their spouses) are invited to the state dinners organised in Bern during state visits.\n\n\n"}
{"id": "11885652", "url": "https://en.wikipedia.org/wiki?curid=11885652", "title": "Technical debt", "text": "Technical debt\n\nTechnical debt (also known as design debt or code debt) is a concept in software development that reflects the implied cost of additional rework caused by choosing an easy solution now instead of using a better approach that would take longer.\n\nTechnical debt can be compared to monetary debt. If technical debt is not repaid, it can accumulate 'interest', making it harder to implement changes later on. Unaddressed technical debt increases software entropy. Technical debt is not necessarily a bad thing, and sometimes (e.g., as a proof-of-concept) technical debt is required to move projects forward. On the other hand, some experts claim that the \"technical debt\" metaphor tends to minimize the impact, which results in insufficient prioritization of the necessary work to correct it.\n\nAs a change is started on a codebase, there is often the need to make other coordinated changes at the same time in other parts of the codebase or documentation. Required changes that are not completed are considered debt that must be paid at some point in the future. Just like financial debt, these uncompleted changes incur interest on top of interest, making it cumbersome to build a project. Although the term is used in software development primarily, it can also be applied to other professions.\n\nCommon causes of technical debt include:\n\nIn a discussion blog \"Technical Debt Quadrant\", Martin Fowler distinguishes four debt types based on two dichotomous categories: the first category is reckless vs prudent, the second, deliberate vs inadvertent.\n\n\"Interest payments\" are caused by both the necessary local maintenance and the absence of maintenance by other users of the project. Ongoing development in the upstream project can increase the cost of \"paying off the debt\" in the future. One pays off the debt by simply completing the uncompleted work.\n\nThe buildup of technical debt is a major cause for projects to miss deadlines. It is difficult to estimate exactly how much work is necessary to pay off the debt. For each change that is initiated, an uncertain amount of uncompleted work is committed to the project. The deadline is missed when the project realizes that there is more uncompleted work (debt) than there is time to complete it in. To have predictable release schedules, a development team should limit the amount of work in progress in order to keep the amount of uncompleted work (or debt) small at all times.\n\nIf enough work is completed on a project to not present a barrier to submission, then a project will be released which still carries a substantial amount of technical debt. If this software reaches production, then the risks of implementing any future refactors which might address the technical debt increase dramatically. Modifying production code carries the risk of outages, actual financial losses and possibly legal repercussions if contracts involve service-level agreements (SLA). For this reason we can view the carrying of technical debt to production almost as if it were an \"increase in interest rate\" and the only time this decreases is when deployments are turned down and retired.\n\nWhile Manny Lehman's Law already indicated that evolving programs continually add to their complexity and deteriorating structure unless work is done to maintain them, Ward Cunningham first drew the comparison between technical complexity and debt in a 1992 experience report:\n\nIn his 2004 text, \"Refactoring to Patterns\", Joshua Kerievsky presents a comparable argument concerning the costs associated with architectural negligence, which he describes as \"design debt\".\n\nActivities that might be postponed include documentation, writing tests, attending to TODO comments and tackling compiler and static code analysis warnings. Other instances of technical debt include knowledge that isn't shared around the organization and code that is too confusing to be modified easily.\n\nGrady Booch compares how evolving cities is similar to evolving software-intensive systems and how lack of refactoring can lead to technical debt.\n\nIn open source software, postponing sending local changes to the upstream project is a technical debt.\n\n\n"}
{"id": "20768429", "url": "https://en.wikipedia.org/wiki?curid=20768429", "title": "The Human Drift", "text": "The Human Drift\n\nThe Human Drift is a work of Utopian social planning, written by King Camp Gillette and first published in 1894. The book details Gillette's theory that replacing competitive corporations with a single giant publicly owned trust (\"the United Company\") would cure virtually all social ills. \n\nOne-third of the book is devoted to Gillette's plan for an immense three-level metropolis (called \"Metropolis\") on the site of Niagara Falls. Designed to accommodate a population of tens of millions of inhabitants, the mega-city would draw its electric power from the Falls. (A photograph of the Falls served as the book's frontispiece.) The first large electrical generating facilities at Niagara Falls, utilizing the new alternating current system of Nikola Tesla and George Westinghouse, were being constructed at the time Gillette wrote.\n\nGillette's city was to possess \"a perfect economical system of production and distribution,\" run by the United Company; it would in fact be the only city on the North American continent. Economies of scale would mean that a single one of every necessary facility — one steel mill, one shoe factory, etc. — would exist. Advances in mechanization would generate ever-greater efficiencies, and ever-greater wealth for the whole society. Social progress would be natural and inevitable; gender equality would be the rule.\n\nGillette gives a highly specific picture of his metropolis: it is shaped in a perfect rectangle, 135 miles on the long side and 45 on the short. Even with necessary farming and mining, most of the rest of North America outside Metropolis would be a natural environment. Gillette saw the city as containing the full population of the United States at that time, sixty million people; he also thought that the city could accept another thirty million in future population growth. Gillette wanted the buildings of Metropolis to be built of porcelain, for endurance and cleanliness. (His thinking on this point may have been influenced by the famous \"White City\" of the World's Columbian Exposition of 1893.) Gillette favored circular buildings, even for residences (25-floor apartment complexes), and a hexagonal street plan.\n\nThe text of \"The Human Drift\" was accompanied with abundant illustrations and plans, a graph of the \"Educational and Industrial Pyramid,\" and other features of Gillette's scheme.\n\n\"His book is important for the attention it received in its time,\" though today it is \"a curiosity.\"\n\nGillette continued his Utopian argument in two subsequent books, \"World Corporation\" (1910) and \"The People's Corporation\" (1924). Other works also propounded his views.\n\nNothing approaching Gillette's Metropolis has ever been attempted; but the Niagara Falls area was the site of one planned community, a model workers' town named Echota (which means \"town of refuge\" in the Cherokee language). It was designed by the architectural firm of McKim, Mead, and White in 1891 and built by the Niagara Falls Power Company.\n\n"}
{"id": "58325", "url": "https://en.wikipedia.org/wiki?curid=58325", "title": "Vigilante", "text": "Vigilante\n\nA vigilante (, ; ; , ) is a civilian or organization acting in a law enforcement capacity (or in the pursuit of self-perceived justice) without legal authority.\n\n\"Vigilante justice\" is often rationalized by the concept that proper legal forms of criminal punishment are either nonexistent, insufficient, or inefficient. Vigilantes normally see the government as ineffective in enforcing the law; such individuals often claim to justify their actions as a fulfillment of the wishes of the community.\n\nPersons alleged to be escaping the law or above the law are sometimes the victims of vigilantism.\n\nVigilante conduct involves varied degrees of violence. Vigilantes could assault targets verbally and/or physically, damage and/or vandalize property, or even murder individuals.\n\nIn a number of cases, vigilantism has involved targets with mistaken identities.\n\nVigilantism and the vigilante ethos existed long before the word \"vigilante\" was introduced into the English language. There are conceptual and psychological parallels between the Dark Age and medieval aristocratic custom of private war or vendetta and the modern vigilante philosophy.\n\nElements of the concept of vigilantism can be found in the Biblical account in Genesis 34 of the abduction and rape (or, by some interpretations, seduction) of Dinah, the daughter of Jacob, in the Canaanite city of Shechem by the eponymous son of the ruler, and the violent reaction of her brothers Simeon and Levi, who slew all of the males of the city in revenge, rescued their sister and plundered Shechem. When Jacob protested that their actions might bring trouble upon him and his family, the brothers replied \"Should he [i.e., Shechem] treat our sister as a harlot?\"\n\nSimilarly, in , Absalom kills Amnon after King David, their father, fails to punish Amnon for raping Tamar, their sister.\n\nRecourse to personal vengeance and dueling was considered a class privilege of the sword-bearing aristocracy before the formation of the modern centralized liberal-bureaucratic nation-state (see Marc Bloch, trans. L. A. Manyon, \"Feudal Society\", Vol. I, 1965, p. 127). In addition, sociologists have posited a complex legal and ethical interrelationship between vigilante acts and rebellion and tyrannicide.\n\nIn the Western literary and cultural tradition, characteristics of \"vigilantism\" have often been vested in folkloric heroes and legendary outlaws (e.g., Robin Hood). Vigilantism in literature, folklore and legend is connected to the fundamental issues of dissatisfied morality, injustice, the failures of authority and the ethical adequacy of legitimate governance.\n\nDuring medieval times, punishment of felons was sometimes exercised by such secret societies as the courts of the Vehm (cf. the medieval Sardinian \"Gamurra\" later become \"Barracelli\", the Sicilian Vendicatori and the Beati Paoli), a type of early vigilante organization, which became extremely powerful in Westphalian Germany during the 15th century.\n\nFormally-defined vigilantism arose in the early American colonies.\n\n\nIn India, vigilante refers to when a group metes out extralegal punishment to alleged lawbreakers. Vigilantism is also referred to as \"mob justice\". It is usually caused by perception of corruption and delays in the judicial system.\n\nAs boom-towns, or mining towns in California because of the Gold Rush, started appearing towards the 1850s, vigilantes started taking justice into their own hands because these towns did not have any established forms of government. These people would assault accused thieves, rapists and murderers. When they assaulted these thieves, they would steal their gold and give it to the accuser. Other than reports and newspapers, there are not many records of vigilantes. Few names or groups are known.\n\nLater in the United States, vigilante groups arose in poorly governed frontier areas where criminals preyed upon the citizenry with impunity.\n\n\n\n"}
{"id": "4860340", "url": "https://en.wikipedia.org/wiki?curid=4860340", "title": "Wind profiler", "text": "Wind profiler\n\nA wind profiler is a type of weather observing equipment that uses radar or sound waves (SODAR) to detect the wind speed and direction at various elevations above the ground. Readings are made at each kilometer above sea level, up to the extent of the troposphere (i.e., between 8 and 17 km above mean sea level). Above this level there is inadequate water vapor present to produce a radar \"bounce.\" The data synthesized from wind direction and speed is very useful to meteorological forecasting and timely reporting for flight planning. A twelve-hour history of data is available through NOAA websites.\n\nIn a typical implementation, the radar or sodar can sample along each of five beams: one is aimed vertically to measure vertical velocity, and four are tilted off vertical and oriented orthogonal to one another to measure the horizontal components of the air's motion. A profiler's ability to measure winds is based on the assumption that the turbulent eddies that induce scattering are carried along by the mean wind. The energy scattered by these eddies and received by the profiler is orders of magnitude smaller than the energy transmitted. However, if sufficient samples can be obtained, then the amplitude of the energy scattered by these eddies can be clearly identified above the background noise level, then the mean wind speed and direction within the volume being sampled can be determined. The radial components measured by the tilted beams are the vector sum of the horizontal motion of the air toward or away from the radar and any vertical motion present in the beam. Using appropriate trigonometry, the three-dimensional meteorological velocity components (u,v,w) and wind speed and wind direction are calculated from the radial velocities with corrections for vertical motions.\n\nPulse-Doppler radar wind profilers operate using electromagnetic (EM) signals to remotely sense winds aloft. The radar transmits an electromagnetic pulse along each of the antenna's pointing directions. A UHF profiler includes subsystems to control the radar's transmitter, receiver, signal processing, and Radio Acoustic Sounding System (RASS), if provided, as well as data telemetry and remote control.\n\nThe duration of the transmission determines the length of the pulse emitted by the antenna, which in turn corresponds to the volume of air illuminated (in electrical terms) by the radar beam. Small amounts of the transmitted energy are scattered back (referred to as backscattering) toward and received by the radar. Delays of fixed intervals are built into the data processing system so that the radar receives scattered energy from discrete altitudes, referred to as range gates. The Doppler frequency shift of the backscattered energy is determined, and then used to calculate the velocity\nof the air toward or away from the radar along each beam as a function of altitude. The source of the backscattered energy (radar “targets”) is small-scale turbulent fluctuations that induce irregularities in the radio refractive index of the atmosphere. The radar is most sensitive to scattering by turbulent eddies whose spatial scale is ½ the wavelength of the radar, or approximately 16 centimeters (cm) for a UHF profiler.\n\nA boundary-layer radar wind profiler can be configured to compute averaged wind profiles for periods ranging from a few minutes to an hour. Boundary-layer radar wind profilers are often configured to sample in more than one mode. For example, in a “low mode,” the pulse of energy transmitted by the profiler may be 60 m in length. The pulse length determines the depth of the column of air being sampled and thus the vertical resolution of the data. In a “high mode,” the pulse length is increased, usually to 100 m or greater. The longer pulse length means that more energy is being transmitted for each sample, which improves the signal-to-noise ratio (SNR) of the data. Using a longer pulse length increases the depth of the sample volume and thus decreases the vertical resolution in the data. The greater energy output of the high mode increases the maximum altitude to which the radar wind profiler can sample, but at the expense of coarser vertical resolution and an increase in the\naltitude at which the first winds are measured. When radar wind profilers are operated in multiple modes, the data are often combined into a single overlapping data set to simplify postprocessing and data validation procedures.\n\nAlternatively, a wind profiler may use sound waves to measure wind speed at various heights above the ground, and the thermodynamic structure of the lower layer of the atmosphere. These sodars can be divided in mono-static system using the same antenna for transmitting and receiving, and bi-static system using separate antennas. The difference between the two antenna systems determines whether atmospheric scattering is by temperature fluctuations (in mono-static systems), or by both temperature and wind velocity fluctuations (in bi-static systems).\n\nMono-static antenna systems can be divided further into two categories: those using multiple axis, individual antennas and those using a single phased array antenna. The multiple-axis systems generally use three individual antennas aimed in specific directions to steer the acoustic beam. One antenna is generally aimed vertically, and the other two are tilted slightly from the vertical at an orthogonal angle. Each of the individual antennas may use a single transducer focused into a parabolic reflector to form a parabolic loudspeaker, or an array of speaker drivers and horns (transducers) all transmitting in-phase to form a single beam. Both the tilt angle from the vertical and the azimuth angle of each antenna are fixed when the system is set up.\n\nThe vertical range of sodars is approximately 0.2 to 2 kilometers (km) and is a function of frequency, power output, atmospheric stability, turbulence, and, most importantly, the noise environment in which a sodar is operated. Operating frequencies range from less than 1000 Hz to over 4000 Hz, with power levels up to several hundred watts. Due to the attenuation characteristics of the atmosphere, high power, lower frequency sodars will generally produce greater height coverage. Some sodars can be operated in different modes to better match vertical resolution and range to the application. This is accomplished through a relaxation between pulse length and maximum altitude.\n\n"}
{"id": "29162673", "url": "https://en.wikipedia.org/wiki?curid=29162673", "title": "Workplace incivility", "text": "Workplace incivility\n\nWorkplace incivility has been defined as low-intensity deviant behavior with ambiguous intent to harm the target. Uncivil behaviors are characteristically rude and discourteous, displaying a lack of regard for others. The authors hypothesize there is an \"incivility spiral\" in the workplace made worse by \"asymmetric global interaction\".\n\nIncivility is distinct from violence. The reduction of workplace incivility is a fertile area for applied psychology research.\n\nA summary of research conducted in Europe suggests that workplace incivility is common there. In research on more than 1000 U.S. civil service workers, Cortina, Magley, Williams, and Langhout (2001) found that more than 70% of the sample experienced workplace incivility in the past five years. Similarly, Laschinger, Leiter, Day, and Gilin found that among 612 staff nurses, 67.5% had experienced incivility from their supervisors and 77.6% had experienced incivility from their coworkers. In addition, they found that low levels of incivility along with low levels of burnout and an empowering work environment were significant predictors of nurses' experiences of job satisfaction and organizational commitment. Incivility was associated with occupational stress and reduced job satisfaction. Other research shows that workplace incivility relates to job stress, depression, and life satisfaction as well.\n\nAfter conducting more than six hundred interviews with \"employees, managers, and professionals in varying industries across the United States\" and collecting \"survey data from an additional sample of more than 1,200 employees, managers, and professionals representing all industrial categories in the United States and Canada\", researchers Christine M. Pearson and Christine L. Porath wrote in 2004 that \"The grand conclusion: incivility does matter. Whether its costs are borne by targets, their colleagues, their organizations, their families, their friends outside work, their customers, witnesses to the interactions, or even the instigators themselves, there is a price to be paid for uncivil encounters among coworkers.\" Citing previous research (2000) Pearson writes that \"more than half the targets waste work time worrying about the incident or planning how to deal with or avert future interactions with the instigator. Nearly 40 percent reduced their commitment to the organization; 20 percent told us that they reduced their work effort intentionally as a result of the incivility, and 10 percent of targets said that they deliberately cut back the amount of time they spent at work.\"\n\nStudies suggest that social support can buffer the negative effects of workplace incivility. Individuals who felt emotionally and organizationally socially supported reported fewer negative consequences (less depression and job stress, and higher in job and life satisfaction) of workplace incivility compared to those who felt less supported. Research also suggests that the negative effects of incivility can be offset by feelings of organizational trust and high regard for one’s workgroup.\n\nExamples at the more subtle end of the spectrum include:\n\nSomewhere between the extremes are numerous everyday examples of workplace rudeness and impropriety such as:\n\n\nOther overt forms of incivility might include emotional tirades and losing one's temper.\n\n\nA number of studies have shown that women are more likely than men to experience workplace incivility and its associated negative outcomes. Research also shows that employees who witness incivility directed toward female coworkers have lower psychological wellbeing, physical health, and job satisfaction, which in turn relates lowered commitment toward the organization and higher job burnout and turnover intentions. Miner-Rubino and Cortina (2004) found that observing incivility toward women related to increased work withdrawal for both male and female employees, especially in work contexts where there were more men.\n\nOther research shows that incivility directed toward same-gender coworkers tends to lead to more negative emotionality for observers. While both men and women felt anger, fear, and anxiety arising from same-gender incivility, women additionally reported higher levels of demoralization after witnessing such mistreatment. Furthermore, the negative effects of same-gender incivility were more pronounced for men observing men mistreating other men than for women observing women mistreating other women. Miner and Eischeid (2012) suggest this disparity reflects men perceiving uncivil behavior as a “clear affront to the power and status they have learned to expect for their group in interpersonal interactions.”\n\nMotherhood status has also been examined as a possible predictor of being targeted for incivility in the workplace. This research shows that mothers with three or more children report more incivility than women with two, one, or zero children. Fathers, on the other hand, report more incivility than men without children, but still less than mothers. While motherhood appears to predict increases in workplace incivility, results also showed that the negative outcomes associated with incivility were mitigated by motherhood status. Fatherhood status, on the other hand, did not mitigate the relationship between incivility and outcomes. Childless women reported more workplace incivility than childless men, and showed a stronger relationship between incivility and negative outcomes than childless men, mothers, and fathers.\n\nWorkplace bullying overlaps to some degree with workplace incivility but tends to encompass more intense and typically repeated acts of disregard and rudeness. Negative spirals of increasing incivility between organizational members can result in bullying, but isolated acts of incivility are not conceptually bullying despite the apparent similarity in their form and content. In case of bullying, the intent of harm is less ambiguous, an unequal balance of power (both formal and informal) is more salient, and the target of bullying feels threatened, vulnerable and unable to defend himself or herself against negative recurring actions.\n\nAnother related notion is petty tyranny, which also involves a lack of consideration towards others, although petty tyranny is more narrowly defined as a profile of leaders and can also involve more severe forms of abuse of power and of authority.\n\n\n"}
{"id": "35483827", "url": "https://en.wikipedia.org/wiki?curid=35483827", "title": "Yuri Leiderman", "text": "Yuri Leiderman\n\nYuri Leiderman (born in 1963, Odessa, Ukraine) is an artist and writer, one of the Moscow Conceptualists. He participated in apartment exhibitions in Moscow and Odessa since 1982. He graduated from the Moscow Institute of Chemical Technology named after D. Mendeleyev in 1987. He was one of the founding members of the \"Medical Hermeneutics\" group in 1987, leaving the group in 1990. He was awarded the Andrei Belyi literature prize in 2005. He was a member of the groups \"Kapiton\" and \"Corbusier\", 2008-2010. He was a participant in the 68th Venice International Film Festival. He resides and works in Berlin.\n\n\n"}
{"id": "44130641", "url": "https://en.wikipedia.org/wiki?curid=44130641", "title": "Zenker's turaco", "text": "Zenker's turaco\n\nZenker's turaco (\"Tauraco persa zenkeri\"), is a subspecies of the Guinea turaco. It is a green turaco, in the family Musophagidae, subfamily Tauracinae, a group of near-passerines birds. Zenker's turaco is found in forests of Central Africa in the Congo Basin in Gabon, DR Congo and Congo Brazaville and south to northern Angola. It forms part of a superspecies complex that extends from West Africa to East Africa and as far south as the Cape in Southern Africa and include the black-billed turaco, Emin's, Schalow's turaco, Livingstone's turaco, the Transvaal turaco and the Knysna turaco, as subspecies within the group.\n"}
