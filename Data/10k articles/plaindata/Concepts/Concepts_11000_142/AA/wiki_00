{"id": "1411852", "url": "https://en.wikipedia.org/wiki?curid=1411852", "title": "Active structure", "text": "Active structure\n\nAn active structure (also known as a smart or adaptive structure) is a mechanical structure with the ability to alter its configuration, form or properties in response to changes in the environment.\n\nThe term active structure also refers to structures that, unlike traditional engineering structures (e.g., bridges, buildings), require constant motion and hence power input to remain stable. The advantage of active structures is that they can be far more massive than a traditional static structure: an example would be a space fountain, a building that reaches into orbit.\n\nThe result of the activity is a structure more suited for the type and magnitude of the load it is carrying. For example, an orientation change of a beam could reduce the maximum stress or strain level, while a shape change could render a structure less susceptible to dynamic vibrations. A good example of an adaptive structure is the human body where the skeleton carries a wide range of loads and the muscles change its configuration to do so. Consider carrying a backpack. If the upper body did not adjust the centre of mass of the whole system slightly by leaning forward, the person would fall on his or her back.\n\nAn active structure consists of three integral components besides the load carrying part. They are the \"sensors\", the \"processor\" and the \"actuators\". In the case of a human body, the sensory nerves are the sensors which gather information of the environment. The brain acts as the processor to evaluate the information and decide to act accordingly and therefore instructs the muscles, which act as actuators to respond. In heavy engineering, there is already an emerging trend to incorporate activation into bridges and domes to minimize vibrations under wind and earthquake loads.\n\nAviation engineering and aerospace engineering have been the main driving force in developing modern active structures. Aircraft (and spacecraft) require adaptation because they are exposed to many different environments, and therefore loadings, during their lifetime. Prior to launching they are subjected to gravity or dead loads, during\ntakeoff they are subjected to extreme dynamic and inertial loads and in-flight they need to be in a configuration which minimizes drag but promotes lift. A lot of effort has been committed into adaptive aircraft wings to produce one that can control the separation of boundary layers and turbulence. Many space structures utilize adaptivity to survive extreme environmental challenges in space or to achieve precise accuracies. For example, space antennas and mirrors can be activated to precise orientation. As space technology advances, some sensitive equipment (namely interferometric optical and infrared astronomical instruments) are required to be accurate in position as delicate as a few nanometres, while the supporting active structure is tens of metres in dimensions.\n\nHuman-made actuators existing in the market, even the most sophisticated ones, are nearly all one-dimensional. This means they are only capable of extending and contracting along, or rotating about 1 axis. Actuators capable of movement in both forward and reverse directions are known as two-way actuators, as opposed to one-way actuators which can only move in one direction. The limiting capability of actuators has restricted active structures to two main types: \"active truss structures\", based on linear actuators, and \"manipulator arms\", based on rotary\nactuators.\n\nA good active structure has a number of requirements. First, it needs to be easily actuated. The actuation should be energy-saving. A structure which is very stiff and strongly resists morphing is therefore not desirable. Second, the resulting structure must have structural integrity to carry the design loads. Therefore the process of actuation should not jeopardize the structure's strength. More precisely, we can say: We seek an active structure where actuation of some members will lead to a geometry change without substantially altering its stress state. In other words, a structure that has both statical determinacy and kinematic determinacy is optimal for actuation.\n\nActive-control technology is applied in civil engineering, mechanical engineering and aerospace engineering. Although most civil engineering structures are static, active control is utilized in some civil structures for deployment against seismic loading, wind loading and environmental vibration. Also, active control is proposed to be used for damage tolerance purposes where human intervention is restricted. Korkmaz et al. demonstrated configuration of active control system for a damage tolerance and deployment of a bridge.\n\n"}
{"id": "42069780", "url": "https://en.wikipedia.org/wiki?curid=42069780", "title": "Affine monoid", "text": "Affine monoid\n\nIn abstract algebra, a branch of mathematics, an affine monoid is a commutative monoid that is finitely generated, and is isomorphic to a submonoid of a free abelian group ℤ, \"d\" ≥ 0. Affine monoids are closely connected to convex polyhedra, and their associated algebras are of much use in the algebraic study of these geometric objects.\n\n\n\n\n\nformula_30 defines the addition.\n\n\n\nis the integral closure of formula_8 in formula_14. If formula_51, then formula_8 is integrally closed\n\n\n\n"}
{"id": "283867", "url": "https://en.wikipedia.org/wiki?curid=283867", "title": "Algebraic data type", "text": "Algebraic data type\n\nIn computer programming, especially functional programming and type theory, an algebraic data type is a kind of composite type, i.e., a type formed by combining other types.\n\nTwo common classes of algebraic types are product types (i.e., tuples and records) and sum types (i.e., tagged or disjoint unions or \"variant types\").\n\nThe values of a product type typically contain several values, called \"fields\". All values of that type have the same combination of field types. The set of all possible values of a product type is the set-theoretic product, i.e., the Cartesian product, of the sets of all possible values of its field types.\n\nThe values of a sum type are typically grouped into several classes, called \"variants\". A value of a variant type is usually created with a quasi-functional entity called a \"constructor\". Each variant has its own constructor, which takes a specified number of arguments with specified types. The set of all possible values of a sum type is the set-theoretic sum, i.e., the disjoint union, of the sets of all possible values of its variants. Enumerated types are a special case of sum types in which the constructors take no arguments, as exactly one value is defined for each constructor.\n\nValues of algebraic types are analyzed with pattern matching, which identifies a value by its constructor or field names and extracts the data it contains.\n\nAlgebraic data types were introduced in Hope, a small functional programming language developed in the 1970s at the University of Edinburgh.\n\nOne of the most common examples of an algebraic data type is the singly linked list. A list type is a sum type with two variants, codice_1 for an empty list and codice_2 for the combination of a new element \"x\" with a list \"xs\" to create a new list. Here is an example of how a singly linked list would be declared in Haskell:\n\ncodice_3 is an abbreviation of \"cons\"truct. Many languages have special syntax for lists defined in this way. For example, Haskell and ML use codice_4 for codice_1, codice_6 or codice_7 for codice_3, respectively, and square brackets for entire lists. So codice_9 would normally be written as codice_10 or codice_11 in Haskell, or as codice_12 or codice_13 in ML.\n\nFor a slightly more complex example, binary trees may be implemented in Haskell as follows:\nHere, codice_14 represents an empty tree, codice_15 contains a piece of data, and codice_16 organizes the data into branches.\n\nIn most languages that support algebraic data types, it is possible to define parametric types. Examples are given later in this article.\n\nSomewhat similar to a function, a data constructor is applied to arguments of an appropriate type, yielding an instance of the data type to which the type constructor belongs. For example, the data constructor codice_15 is logically a function codice_18, meaning that giving an integer as an argument to codice_15 produces a value of the type codice_20. As codice_16 takes two arguments of the type codice_20 itself, the datatype is recursive.\n\nOperations on algebraic data types can be defined by using pattern matching to retrieve the arguments. For example, consider a function to find the depth of a codice_20, given here in Haskell:\n\nThus, a codice_20 given to codice_25 can be constructed using any of codice_14, codice_15, or codice_16 and must be matched for any of them respectively to deal with all cases. In case of codice_16, the pattern extracts the subtrees codice_30 and codice_31 for further processing.\n\nAlgebraic data types are highly suited to implementing abstract syntax. For example, the following algebraic data type describes a simple language representing numerical expressions:\n\nAn element of such a data type would have a form such as codice_32.\n\nWriting an evaluation function for this language is a simple exercise; however, more complex transformations also become feasible. For example, an optimization pass in a compiler might be written as a function taking an abstract expression as input and returning an optimized form.\n\nWhat is happening is that there is a datatype which can be \"one of several types of things\". Each \"type of thing\" is associated with an identifier called a \"constructor\", which can be viewed as a kind of tag for that kind of data. Each constructor can carry with it a different type of data. A constructor could carry no data (e.g., \"Empty\" in the example above), or one piece of data (e.g., “Leaf” has one Int value), or multiple pieces of data (e.g., “Node” has two Tree values).\n\nTo do something with a value of this Tree algebraic data type, it is \"deconstructed\" using a process termed \"pattern matching\". It involves \"matching\" the data with a series of \"patterns\". The example function \"depth\" above pattern-matches its argument with three patterns. When the function is called, it finds the first pattern that matches its argument, performs any variable bindings that are found in the pattern, and evaluates the expression corresponding to the pattern.\n\nEach pattern above has a form that resembles the structure of some possible value of this datatype. The first pattern simply matches values of the constructor \"Empty\". The second pattern matches values of the constructor \"Leaf\". Patterns are recursive, so then the data that is associated with that constructor is matched with the pattern \"n\". In this case, a lowercase identifier represents a pattern that matches any value, which then is bound to a variable of that name — in this case, a variable “codice_33” is bound to the integer value stored in the data type — to be used in the expression to evaluate.\n\nThe recursion in patterns in this example are trivial, but a possible more complex recursive pattern would be something like codice_34. Recursive patterns several layers deep are used for example in balancing red-black trees, which involve cases that require looking at colors several layers deep.\n\nThe example above is operationally equivalent to the following pseudocode:\n\nThe comparison of this with pattern matching will point out some of the advantages of algebraic data types and pattern matching. The first advantage is type safety. The pseudocode above relies on the diligence of the programmer to not access field2 when the constructor is a Leaf, for example. Also, the type of field1 is different for Leaf and Node (for Leaf it is Int; for Node it is Tree), so the type system would have difficulties assigning a static type to it in a safe way in a traditional record data structure. However, in pattern matching, the type of each extracted value is checked based on the types declared by the relevant constructor, and how many values can be extracted is known based on the constructor, so it does not face these problems.\n\nSecond, in pattern matching, the compiler statically checks that all cases are handled. If one of the cases of the \"depth\" function above were missing, the compiler would issue a warning, indicating that a case is not handled. This task may seem easy for the simple patterns above, but with many complex recursive patterns, the task becomes difficult for the average human (or compiler, if it must check arbitrary nested if-else constructs) to handle. Similarly, there may be patterns which never match (i.e., are already covered by prior patterns), and the compiler can also check and issue warnings for these, as they may indicate an error in reasoning.\n\nDo not confuse these patterns with regular expression patterns used in string pattern matching. The purpose is similar: to check whether a piece of data matches certain constraints, and if so, extract relevant parts of it for processing. However, the mechanism is very different. This kind of pattern matching on algebraic data types matches on the structural properties of an object rather than on the character sequence of strings.\n\nA general algebraic data type is a possibly recursive sum type of product types. Each constructor tags a product type to separate it from others, or if there is only one constructor, the data type is a product type. Further, the parameter types of a constructor are the factors of the product type. A parameterless constructor corresponds to the empty product. If a datatype is recursive, the entire sum of products is wrapped in a recursive type, and each constructor also rolls the datatype into the recursive type.\n\nFor example, the Haskell datatype:\n\nis represented in type theory as\nformula_1\nwith constructors formula_2 and formula_3.\n\nThe Haskell List datatype can also be represented in type theory in a slightly different form, thus:\nformula_4.\n(Note how the formula_5 and formula_6 constructs are reversed relative to the original.) The original formation specified a type function which body was a recursive type. The revised version specifies a recursive function on types. (The type variable formula_7 is used to suggest a function rather than a \"base type\" like formula_8, since formula_7 is like a Greek \"f\".) The function must also now be applied formula_7 to its argument type formula_11 in the body of the type.\n\nFor the purposes of the List example, these two formulations are not significantly different; but the second form allows expressing so-called nested data types, i.e., those where the recursive type differs parametrically from the original. (For more information on nested data types, see the works of Richard Bird, Lambert Meertens, and Ross Paterson.)\n\nIn set theory the equivalent of a sum type is a disjoint union, a set which elements are pairs consisting of a tag (equivalent to a constructor) and an object of a type corresponding to the tag (equivalent to the constructor arguments).\n\nMany programming languages incorporate algebraic data types as a first class notion, including:\n"}
{"id": "6239377", "url": "https://en.wikipedia.org/wiki?curid=6239377", "title": "Anal expulsiveness", "text": "Anal expulsiveness\n\nAnal expulsiveness is the state of a person who exhibits cruelty, emotional outbursts, disorganization, self-confidence, artistic ability, generosity, rebelliousness and general carelessness. \n\nSigmund Freud's psychoanalysis theory claims the anal stage follows the oral stage of infant/early-childhood development. This is a time when an infant's attention moves from oral stimulation to anal stimulation (usually the bowels but occasionally the bladder), usually synchronous with learning to control their excretory functions, a time of toilet training. For a child in this stage of development, control of bowel movements is the stage at which the child can express autonomy by withholding, refusing to comply, or soiling himself or herself. Conflicts with bullying parents regarding toilet training can produce a fixation in this stage, which can manifest itself in adulthood by a continuation of erotic pleasure in defecation.\n\nAnal-expulsive refers to a personality trait present in people fixated in the anal stage of psychosexual development. The anal stage is the second of five stages of psychosexual development.\n\nIn modern times, psychosexual stages are considered to have limited value in understanding the more severe psychopathology.\n\n"}
{"id": "13564463", "url": "https://en.wikipedia.org/wiki?curid=13564463", "title": "Bopomofo", "text": "Bopomofo\n\nZhuyin Fuhao (), Zhuyin (), Bopomofo (ㄅㄆㄇㄈ) or Mandarin Phonetic Symbols is the major Chinese transliteration system for Taiwanese Mandarin. It is also used to transcribe other varieties of Chinese, particularly other varieties of Standard Chinese and related Mandarin dialects, as well as Taiwanese Hokkien.\n\nZhuyin Fuhao and Zhuyin are traditional terms, whereas Bopomofo is the colloquial term, also used by the ISO and Unicode. Consisting of 37 characters and four tone marks, it transcribes all possible sounds in Mandarin. Zhuyin was introduced in China by the Republican Government in the 1910s and used alongside the Wade-Giles system, which used a modified Latin alphabet. The Wade system was replaced by Hanyu Pinyin in 1958 by the Government of the People's Republic of China, and at the International Organization for Standardization (ISO) in 1982. Although Taiwan adopted Hanyu Pinyin as its official romanization system in 2009, Bopomofo is still an official transliteration system there and remains widely used as an educational tool and for electronic input methods.\n\nThe informal name \"Bopomofo\" is derived from the first four syllables in the conventional ordering of available syllables in Mandarin Chinese. The four Bopomofo characters () that correspond to these syllables are usually placed first in a list of these characters. The same sequence is sometimes used by other speakers of Chinese to refer to other phonetic systems.\n\nThe original formal name of the system was \"Guóyīn Zìmǔ\" ( ,  , \"Phonetic Alphabet of the National Language\") and \"Zhùyīn Zìmǔ\" ( ,  ,  \"Phonetic Alphabet\" or \"Annotated Phonetic Letters\"). It was later renamed \"Zhùyīn Fúhào\" ( ,  ), meaning \"phonetic symbols\".\n\nIn official documents, Zhuyin is occasionally called \"Mandarin Phonetic Symbols I\" (), abbreviated as \"MPS I\" \n\nIn English translations, the system is often also called either \"Chu-yin\" or \"the Mandarin Phonetic Symbols\". A romanized phonetic system was released in 1984 as Mandarin Phonetic Symbols II (MPS II).\n\nThe Commission on the Unification of Pronunciation, led by Wu Zhihui from 1912 to 1913, created a system called \"Zhuyin Zimu\", which was based on Zhang Binglin's shorthand. A draft was released on July 11, 1913, by the Republic of China National Ministry of Education, but it was not officially proclaimed until November 23, 1928. It was later renamed first \"Guoyin Zimu\" and then, in April 1930, \"Zhuyin Fuhao\". The last renaming addressed fears that the alphabetic system might independently replace Chinese characters.\n\nZhuyin remains the predominant phonetic system in teaching reading and writing in elementary school in Taiwan. It is also one of the most popular ways to enter Chinese characters into computers and smartphones and to look up characters in a dictionary.\n\nIn elementary school, particularly in the lower years, Chinese characters in textbooks are often annotated with Zhuyin as ruby characters as an aid to learning. Additionally, one children's newspaper in Taiwan, the \"Mandarin Daily News\", annotates all articles with Zhuyin ruby characters.\n\nIn teaching Mandarin, Taiwan institutions and some overseas communities use Zhuyin as a learning tool.\n\nThe Zhuyin characters were created by Zhang Binglin, and taken mainly from \"regularised\" forms of ancient Chinese characters, the modern readings of which contain the sound that each letter represents. It is to be noted that the first consonants are articulated from the front of the mouth to the back, /b/, /p/, /m/, /f/, /d/, /t/, /n/, /l/ etc.\n\nZhuyin is written in the same stroke order rule as Chinese characters. Note that ㄖ is written with three strokes, unlike the character from which it is derived (日, Hanyu Pinyin: rì), which has four strokes. \n\nAs shown in the following table, tone marks for the second, third, and fourth tones are shared between bopomofo and pinyin. In bopomofo, the lack of a marker is used to indicate the first tone while a dot above indicates the fifth tone (also known as the neutral tone). In pinyin, a macron indicates the first tone and the lack of a marker indicates the fifth tone.\n\nUnlike Hanyu Pinyin, Zhuyin aligns well with the hanzi characters in books whose texts are printed vertically, making Zhuyin better suited for annotating the pronunciation of vertically oriented Chinese text.\n\nZhuyin, when used in conjunction with Chinese characters, are typically placed to the right of the Chinese character vertically or to the top of the Chinese character in a horizontal print (see Ruby character).\n\nBelow is an example for the word \"bottle\" ():\n\nÉrhuà-ed words merge as a single syllable, which means is attached to the precedent syllable (like gēr). In case the syllable uses other tones than 1st tone, the tone is attached to the penultimate syllable, but not to (e.g. nǎr; yīdiǎnr; hǎowánr). \n\nZhuyin and pinyin are based on the same Mandarin pronunciations, hence there is a one-to-one correspondence between the two systems:\n\nThree letters formerly used in non-standard dialects of Mandarin are now also used to write other Chinese varieties. Some Zhuyin fonts do not contain these letters; see External links for PDF pictures.\n\nIn Taiwan, Bopomofo is used to teach Taiwanese Hokkien, and is also used to transcribe it phonetically in contexts such as on storefront signs, karaoke lyrics, and film subtitles.\n\nZhuyin can be used as an input method for Chinese characters. It is one of the few input methods that can be found on most modern personal computers without the user having to download or install any additional software. It is also one of the few input methods that can be used for inputting Chinese characters on certain cell phones.\n\nZhuyin was added to the Unicode Standard in October 1991 with the release of version 1.0.\n\nThe Unicode block for Zhuyin, called Bopomofo, is U+3100–U+312F:\nAdditional characters were added in September 1999 with the release of version 3.0.\n\nThe Unicode block for these additional characters, called Bopomofo Extended, is U+31A0–U+31BF:\n\nUnicode 3.0 also added the characters U+02EA and U+02EB, in the Spacing Modifier Letters block. These two characters are now (since Unicode 6.0) classified as Bopomofo characters.\n\n\n"}
{"id": "38345606", "url": "https://en.wikipedia.org/wiki?curid=38345606", "title": "Cardboard bicycle", "text": "Cardboard bicycle\n\nA cardboard bicycle is a bicycle composed mostly or entirely of cardboard. Only prototypes have been made . Reported benefits include low cost, and construction from recyclable and renewable materials. The low cost is also expected to act as a theft deterrent.\n\nIn 2008, Phil Bridge created a cardboard bicycle as part of a three-year degree course in Product Design at Sheffield Hallam University. It was intended to discourage theft, supports a rider up to , and is constructed from a structural cardboard called Hexacomb. It is waterproof, but is only expected to survive six months of constant use. The drivetrain and brakes are metal, as on a conventional bike, and it rolls on standard pneumatic tires.\n\nIn 2012, Izhar Gafni, an Israeli mechanical engineer and cycling enthusiast, unveiled a prototype bicycle made almost entirely out of cardboard in his workshop in Moshav Ahituv. The components, including bike’s frame, wheels, handlebars and saddle, consist of sheets of cardboard folded and glued together. The complete bike weighs , and is treated to be fireproof and waterproof. Gafni reports that it can support riders up to . It has solid rubber tires made from recycled car tires. Power is transferred from the pedals to the rear wheel with a belt, also made from recycled rubber. Gafni and a business partner plan to mass-produce a bike based on the prototype and retail it for 20 USD, with a unit cost of 9 to 12 USD. The target market is low-income countries. The prototype was featured at the November 2012 Microsoft ThinkNext event in Tel Aviv.\nGafni has been trying to raise $2 million on Indiegogo to fund the project.\nAs of 25 June 2013, he had raised $10 thousand.\nThe campaign has ended with a total of $40,107 raised.\n\n"}
{"id": "26953508", "url": "https://en.wikipedia.org/wiki?curid=26953508", "title": "Chang Chun-ha", "text": "Chang Chun-ha\n\nChang Chun-ha (장준하, 張俊河, August 27, 1918 in Uiju County – August 17, 1975 in Uijeongbu, Gyeonggi Province) was a Korean independence and democracy activist who later became a journalist in South Korea.\n\nWhen Korea was under Japanese rule, he participated in education activities and voluntarily joined the Japanese army called Sugada but he escaped the army in 1944 when he was in Suzhou, Jiangsu. His joining of Sugada army was only nominally voluntarily as it was forced by the Japanese army for Korean males to join the army. He then was trained at China Central Officer School and became a warrant officer in the Chinese Central Army. In 1945, he visited Korean Liberation Army located in Suzhou and joined the army from the February as a commissioned officer. While serving the Korean liberation army, he participated activities with the Office of Strategic Services (OSS; the predecessor of the CIA). In November 1945, he came back to Korea via the Provisional Government of the Republic of Korea.\n\nAfter returning to Korea, Chang worked as a secretary of Kim Gu and participated in Lee Beom-seok's Chosun Ethnic Youth League. After the establishment of Republic of Korea, during the First Republic of South Korea, he worked for the government as a secretary. In 1950, he was in charge of citizen spirit reformation in the Ministry of Education and Culture (문교부). In 1952, he was the directing manager of national ideology research institution. He also worked in two more positions in the Ministry of Education and Culture and founded a periodical called \"Sasangge\". He established the Dong-in Literary Award in 1956. \"Sasangge\" acrimoniously denounced the 자유당 administration and became the starting fire of the 4.19 revolution. After the 4.19 revolution took place, he took positions in the Ministry of Munkyo during the second republic.\n\nAfter the May 16 coup, he opposed the South Korea–Japan conference and the sending of troops to Vietnam War. During the South Korean presidential election, 1967, he made an issue out of the career of Yun Bo-seon on Park Chung-hee's pro-Japanese and Workers' Party of South Korea activities. He was then sent into prison for insulting the head of the state. After he came out of prison, he worked with Yun Bo-seon and New Democratic Party (South Korea) in the Korean National Party.\n\nFrom 1975, when he was preparing to fight against the Park Chung-hee administration, he died mysteriously in Pocheon, Gyeonggi-do. The South Korean government announced that Chang's death had been caused by loss of footing while climbing down a mountain. However, after Chang's death, continuous doubts were raised whether it was a homicide by Park's government. To such doubts, the Park administration declared the state of national emergency and arrested anyone who mentioned the death of Chang Chun-ha. The death of Chang Chun-ha has been re-investigated, but there have not been any clear conclusions yet.\n\nChang was awarded the 1962 Ramon Magsaysay Award for Journalism, Literature, and the Creative Communication Arts.\n\n\n"}
{"id": "37855437", "url": "https://en.wikipedia.org/wiki?curid=37855437", "title": "Collective self-esteem", "text": "Collective self-esteem\n\nCollective self-esteem is a concept originating in the field of psychology that describes the aspect of an individual’s self-image that stems from how the individual interacts with others and the groups that the individual is a part of. The idea originated during the research of Jennifer Crocker, during which she was trying to learn about the connection between a person’s self-esteem and their attitude towards or about the group that the person is part of.\n\nCollective self-esteem is talked about subjectively as a concept as well as measured objectively with various scales and assessments. The data from such research is used practically to give importance and weight to the idea that most individuals benefit from being in a group setting for at least sometime as well as being able to identify with being a part of a group.\n\nJennifer Crocker and Riia Luhtanen were the first to study collective self-esteem. They believed there was a relationship between people’s self-esteem and how they felt about groups they were a part of. Crocker hypothesized that people who were high in the trait of collective self-esteem would be more likely to “react to threats to collective self-esteem by derogating out-groups and enhancing the in-group.”\n\n1) Private collective self-esteem. Positive evaluation of one’s group\n\n2) Membership esteem. How one sees him/herself in a group. Is he/she a good member? \n\n3) Public collective self-esteem. How the group one belongs to is evaluated by others. \n\n4) Importance to identity. How important membership in a group is to self-concept. \n\nThe Collective Self-Esteem Scale (CSES) was developed by Luhtanen and Crocker as a measure to assess individuals’ social identity based on their membership in ascribed groups such as race, ethnicity, gender, and the like. The CSES has been one of the most widely used instruments in the field of psychology to assess collective group identity among racial and ethnic populations. In the initial CSES development study, exploratory factor analysis (EFA) and confirmatory factor analysis (CFA) techniques were used to examine the underlying factor structure of the proposed four-factor CSES. The preliminary prototype of the CSES consisted of 43 items. To shorten the instrument, Crocker and Luhtanen selected the 4 items with the highest factor loadings from each subscale to represent the final 16-item CSES. The actual scale is a list of statements that pertain to the person’s membership in a group or category and each is rated on a 7-point scale. The scoring is done through four subscales that are categorized as follows: \n1) Items 1, 5, 9, and 13 = Membership self-esteem. \n2) Items 2, 6, 10, and 14 = Private collective self-esteem. \n3) Items 3, 7, 11, and 15 = Public collective self-esteem. \n4) Items 4, 8, 12, and 16 = Importance to identity. \nFirst, reverse-score answers to items 2,4, 5, 7, 10, 12, 13, and 15, such that (1=7), (2=6), (3=5), (4=4), (5=3), (6=2), (7=1). Then sum the answers to the four items for each respective subscale score, and divide each by 4. \nBecause the subscales measure distinct construct, it is strongly recommended against creating an overall or composite score for collective self-esteem.\n\nIn the context of cross-cultural research using the CSES, Crocker Luhtanen, Blaine, and Broadnax explored the nature of collective self-esteem in Asian, Black, and White college students. Crocker et al. found that collective self-esteem was a significant predictor of psychological adjustment and that among the Black students in their sample, the correlation between public and private collective self-esteem was essentially zero. Based on this later finding, Crocker et al. surmised that Black college students might separate how they privately feel about their group from how they believe others may evaluate them. This separation between public and private evaluations may represent an important survival strategy for Black Americans because of the prejudice and discrimination they face in the United States. The literature has suggested that Black Americans may generally feel good about their own racial group but still believe that external perceptions of their racial group may be negative or derogatory. This phenomenon might indicate a CSES factor structure for Black Americans that is different from that of other racial or ethnic groups in the United States.\n\nCremer et al. began a study expecting that people high in CSE engaged in indirect enhancement of the in-group. This finding suggests that predictions made by Social Identity Theory are more applicable to individuals with a high level of CSE. In this study, Cremer et al. found that participants identified more with the in-group than with the out-group. Participants high in CSE evaluated in-group members as fairer and more competent than participants low in CSE. Cremer et al. also found that women expressed a higher level of CSE than men. These findings provide additional evidence that individuals high in CSE are more likely to engage in in-group distorting evaluations when there is a possible threat to their CSE. Applying the CSE scale in situations of a potential threat to the in-group (i.e., success or failure feedback), Crocker and Luhtanen found that, in contrast to people low in CSE, people high in CSE showed in-group favoritism, thereby indirectly enhancing the in-group. Individuals high in CSE will evaluate in-group members more positively than those low in CSE. The results concerning the in-group and outgroup evaluations seem to suggest that people with high CSE can be considered more confident about their esteemed social identity, making them search for more opportunities to enhance the collective self. As a result, those people will feel a greater need to evaluate their in-group members more positively (i.e., in-group favoritism) than people low in CSE. On the other hand, people low in CSE do not feel very confident about their social identity and, in order to avoid failure, they will consider out-group derogation as a more useful strategy to protect their social identity.\n\nCollective Self-Esteem can be seen in real-world applications through the use of BIRGing and CORFing. Both different concepts but both effect and incorporate collective self-esteem into our everyday lives.\n\nBIRGing (Basking In Reflected Glory) is when a person uses the association of another person’s success to boost his or her self-esteem or self-glory. The act of BIRGing is often used to offset any threats made to a persons or a group’s self-esteem. Sometimes, this act is actually made unknowingly and unintentional. An example of BIRGing can be seen when your favorite sports team, say the University of Kansas men’s basketball team, has just won the national title. You will notice when walking around campus or reading the local newspaper, you will be more likely to hear or read “We won!” or “We are the champs!” instead of \"KU won!\"\n\nCORFing (Cutting Off Reflected Failure) can be seen in this same example, except when the team suffers a loss, the fans will change “we” to “they”. For example, “They did not show enough heart” or “They really got outplayed today”. \n\nWhen someone uses BIRGing or CORFing, his or her collective self-esteem will be altered in either a positive or negative way and one must not only be careful on the frequency they are using these techniques, but also that these are often used unintentionally and can be taken out of context in many situations. In an article titled “BIRGing and CORFing: From the Hardcourt to the Boardroom” by Kevin Meyer, the author provides an example of BIRGing and CORFing being used in the workplace for job security. This can be seen when an employee aligns them self only with good projects or products and distances them self from poor outcomes and products. Employees must be careful when using these techniques often, because their coworkers may start to believe that that individual is only doing certain things to benefit him or herself and as Meyer states “throwing us under the bus” when hard times arise. Meyer also states “An effective leader must be willing to weather the storm, sharing in the collective successes but also standing up for their team when things don’t go to plan. For most, BIRG and CORF can be more difficult to accomplish in the workplace as our affiliation with a particular team or project is often more obvious”.\n\nOne related topic to collective self-esteem is social identity theory. This theory is when an individual’s self-concept is formed by a particular social group. When individuals categorize themselves in these social groups they create this self-concept and self-esteem. Prototypes and stereotypes are involved in the social identity theory. This theory can predict inter-group relations and behaviors. Henri Tajfel and John Turner created this theory to explain inter-group behavior. Tajfel and Turner felt that people innately categorize themselves into in-groups(8). Social identity theory correlates with collective self-esteem because an individual bases their self-esteem or self-concept off of group membership. Social identity theory is also related to self-categorization theory. Both collective self-esteem and social identity theory use groups to identify themselves and acquire self-esteem.\n\nAnother related topic is in-group bias, also known as in-group favoritism. This is simply when an individual has a preference for their own in-group. In-group is superiority is stressed. There is believed superiority over the out-group. A bias is formed for the in-group and the out-group is excluded. In-group bias is also related to group conflict. Tajfel and Turner in, An Integrative Theory of Inter-group Conflict discuss this. Individuals who have in-group bias tend to reject negative information about the in-group and only accept and stress positive information. People who have in-group bias are more likely to have positive collective self-esteem. This could also result in increased collective self-esteem as well. Social identity theory is a major theoretical approach for the phenomenon of in-group bias. To go along with in-group bias is out-group bias, also known as out-group homogeneity. This is when people in the in-group view themselves as more diverse than the out-group, saying that they are more alike than them. There has been research done regarding this phenomenon with race. Dickter and Bartholow did a study on, in-group and out-group attention biases revealed by event related brain potentials. When looking at faces of other races (out-group) the brain interpreted them as similar and could not identify them, whereas facial recognition with same race (in-group) was easier for the brain and these faces appeared to be more diverse. Other examples of this are with sorority and fraternities, as well as gender. Both in-group and out-group bias are used to protect the collective self-esteem. Jane Elliot at a school in Iowa did a famous experiment done with in-group out-group bias. Dividing children into groups based on their eye color observed the prejudice that can occur with in-group, out-group bias. The in-group in this situation was blue-eyed children who proceeded to criticize children who were in the out-group, or did not have blue eyes. The roles were reversed but had the same results.\n"}
{"id": "37445789", "url": "https://en.wikipedia.org/wiki?curid=37445789", "title": "Disability and poverty", "text": "Disability and poverty\n\nThe world's poor are significantly more likely to have or incur a disability within their lifetime compared to more financially privileged populations. The rate of disability within impoverished nations is notably higher than that found in more developed countries. Though no one explanation entirely accounts for this connection, recently there has been a substantial amount of research illustrating the cycle by which poverty and disability are mutually reinforcing. Physical, cognitive, mental, emotional, sensory, or developmental impairments independently or in tandem with one another may increase one's likelihood of becoming impoverished, while living in poverty may increase one's potential of having or acquiring special needs in some capacity.\n\nA multitude of studies have been shown to demonstrate a significant rate of disability among individuals living in poverty. People with disabilities were shown by the World Bank to comprise 15 to 20 percent of the poorest individuals in developing countries. Former World Bank President James Wolfensohn has stated that this connection reveals a link that should be broken. He stated, “People with disabilities in developing countries are over-represented among the poorest people. They have been largely overlooked in the development agenda so far, but the recent focus on poverty reduction strategies is a unique change to rethink and rewrite that agenda.” The link between disability and development has been further stressed by Judith Heumann, the World Bank's first advisor for international disability rights, who indicated that of the 650 million people living with disabilities today eighty percent live in developing countries. Additionally, some research investigations with proved social impact are opening venues that lead to establish enabling factors to break the cycle of deprivation faced by poor people with disabilities. According to the United Kingdom Department for International Development, 10,000 individuals with disabilities die each day as a result of extreme poverty, showing that the connection between these two constructs is especially problematic and deep-seated. This connection is also present in developed countries, with the Disability Funders Network reporting that in the United States alone those with disabilities are twice as likely to live below the poverty line than those without special needs.\n\nAccording to the World Bank, “Persons with disabilities on average as a group experience worse socioeconomic outcomes than persons without disabilities, such as less education, worse health outcomes, less employment, and higher poverty rates.” Researchers have demonstrated that these reduced outcomes may be attributed to a myriad of institutional barriers and other factors. Furthermore, the prevalence of disabilities in impoverished populations has been predicted to follow a cyclical pattern by which those who live in poverty are more likely to acquire a disability and those who have a disability are more likely to become impoverished.\n\nExperts from the United Kingdom Disabled Persons Council attribute the connection between disability and poverty to many systemic factors that promote a “vicious circle.” Statistics affirm the mutually reinforcing nature of special needs and low socioeconomic status, showing that people with disabilities are significantly more likely to become impoverished and people who are impoverished are significantly more likely to become disabled. Barriers presented for those with disabilities can lead individuals to be deprived of access to essential resources, such as opportunities for education and employment, thus causing them to fall into poverty. Likewise, poverty places individuals at a much greater risk of acquiring a disability due to the general lack of health care, nutrition, sanitation, and safe working conditions that the poor are subject to.\n\nExperts assert that this cycle is perpetuated mainly by the lack of agency afforded to those living in poverty. The few options available to the poor often necessitate that these individuals put themselves in harms way, consequently resulting in an increase in the acquisition of preventable impairments. Living in poverty is also shown to decrease an individual's access to preventative health services, which results in an increase in the acquisition of potentially preventable disabilities. In a study by Oxfam, the organization found that well over half of the instances of childhood blindness and hearing impairment in Africa and Asia were considered preventable or treatable. Another estimate released by Oxfam provides further evidence of this vicious circle, finding that 100 million people living in poverty suffer from impairments acquired due to malnutrition and lack of proper sanitation.\n\nDiscrimination\nPrejudice held against individuals with disabilities, otherwise termed ableism, is shown to be a significant detriment to the successful outcomes of persons in this population. According to one study following the lives of children with disabilities in South Africa, the children in the sample described \"discrimination from other metal children and adults in the community as their most significant daily problem.\"\n\nAdditional forms of discrimination may lead disability to be more salient in already marginalized populations. Women and individuals belonging to certain ethnic groups who have disabilities have been found to more greatly suffer from discrimination and endure negative outcomes. Some researchers attribute this to what they believe is a “double rejection” of girls and women who are disabled on the basis of their sex in tandem with their special needs. The stereotypes that accompany both of these attributes lead females with disabilities to be seen as particularly dependent upon others and serve to amplify the misconception of this population as burdensome. In a study done by Oxfam, the societal consequences of having a disability while belonging to an already marginalized population were highlighted, stating, “A disabled women suffers a multiple handicap. Her chances of marriage are very slight, and she is most likely to be condemned to a twilight existence as a non-productive adjunct to the household of her birth… it is small wonder that many disabled female babies do not survive.” Additionally, women with disabilities are particularly susceptible to abuse. A 2004 UN survey in Orissa, India, found that every women with disabilities in their sample had experienced some form of physical abuse. This double discrimination is also shown to be prevalent in more industrialized nations. In the United States, for example, 72 percent of women with disabilities live below the poverty line. The intensified discrimination individuals with disabilities may face due to their sex is especially important to consider when taking into account that, according to the Organisation for Economic Co-operation and Development, women report higher incidences of disability than men. Furthermore, the connection between disability and poverty holds particular significance for the world's women, with females accounting for roughly 70 percent of all individuals living in poverty.\n\nInstitutional discrimination also exists as there are policies existing in organizations that result in inequality between a disabled person and non-disabled person. Some of these organizations systematically ignore the needs of disabled people and some interfere in their lives as a means of social control.\n\nAnother reason individuals living with disabilities are often impoverished is the high medical costs associated with their needs. One study, conducted in villages in South India, demonstrated that the annual cost of treatment and equipment needed for individuals with disabilities in the area ranged from three days of income to upwards of two years' worth, with the average amount spent on essential services totaling three months worth of income. This figure does not take into account the unpaid work of caregivers who must provide assistance after these procedures and the opportunity costs leading to a loss of income during injury, surgery, and rehabilitation.\nStudies reported by medical anthropologists Benedicte Ingstad and Susan Reynolds Whyte have also shown that access to medical care is significantly impaired when one lacks mobility. They report that in addition to the direct medical costs associated with special needs, the burden of transportation falls most heavily on those with disabilities. This is especially true for the rural poor whose distance from urban environments necessitates extensive movement in order to obtain health services. Due to these barriers, both economic and physical, it is estimated that only 2 percent of individuals with disabilities have access to adequate rehabilitation services.\n\nThe inaccessibility of health care for those living in poverty has a substantial impact on the rate of disability within this population. Individuals living in poverty face higher health risks and are often unable to obtain proper treatment, leading them to be significantly more likely to acquire a disability within their lifetime. Financial barriers are not the only obstacles those living in poverty are confronted with. Research shows that matters of geographic inaccessibility, availability, and cultural limitations all provide substantial impediments to the acquisition of proper care for the populations of developing countries. Sex-specific ailments are particularly harmful for women living in poverty. The World Health Organization estimates that each year 20 million women acquire disabilities due to complications during pregnancy and childbirth that could be significantly mitigated with proper pre-natal, childbirth, and post-natal medical care.\nOther barriers to care are present in the lack of treatments developed to target diseases of poverty. Experts assert that the diseases most commonly affecting those in poverty attract the least research funding. This discrepancy, known as the 10/90 gap, reveals that only 10 percent of global health research focuses on conditions that account for 90 percent of the global disease burden. Without a redistribution in research capital, it is likely that many of the diseases known to cause death and disability in impoverished populations will persist.\n\nResearchers assert that institutional barriers play a substantial role in the incidence of poverty in those with disabilities.\n\nPhysical environment may be a large determinant in one's ability to access ladders of success or even basic sustenance. Professor of urban planning Rob Imrie concluded that most spaces contain surmountable physical barriers that unintentionally create an “apartheid by design,” whereby individuals with disabilities are excluded from areas because of the inaccessible layout of these spaces. This \"apartheid\" has been seen by some, such as the United Kingdom Disabled Persons Council, as especially concerning with regard to public transportation, education and health facilities, and perhaps most relevantly places of employment. Physical barriers are also commonly found in the home, with those in poverty more likely to occupy tighter spaces inaccessible to wheelchairs. Beyond physical accessibility, other potential excluding agents include a lack of Braille, sign language and shortage of audio tape availability for those who are blind and deaf.\n\nThe roots of unemployment are speculated to begin with discrimination at an early age. UNESCO reports that 98 percent of children with disabilities in developing countries are denied access to formal education. According to the World Bank, at least 40 million children with disabilities do not receive an education thus barring them from obtaining knowledge essential to gainful employment and forcing them to grow up to be financially dependent upon others. This is also reflected in a finding obtained by the World Development Report that 77 percent of persons with disabilities are illiterate. This statistic is even more jarring for women with disabilities, with the United Nations Development Program reporting that the global literacy rate for this population is a mere 1 percent. This may be attributed to the fact that, according to the World Health Organization, boys with disabilities are significantly more likely to receive an education than similarly abled girls. Beyond simply the skills obtained, experts such as former World Bank advisor Judith Heumann speculate that the societal value of education and the inability of schools to accommodate special needs children substantially contributes to the discrimination of these individuals. It is important to note that the deprivation of education to individuals with special needs may not be solely an issue of discrimination, but an issue of resources. Children with disabilities often require specialized educational resources and teaching practices largely unavailable in developing countries.\n\nSome sociologists have found a number of barriers to employment for individuals with disabilities. These may be seen in employer discrimination, architectural barriers within the workplace, pervasive negative attitudes regarding skill, and the adverse reactions of customers. According to sociologist Edward Hall, \"More disabled people are unemployed, in lower status occupations, on low earnings, or out of the labour market altogether, than non-disabled people.\" The International Labour Organization estimates that roughly 386 million of the world's working age population have some form of disability, however, up to eighty percent of these employable individuals with disabilities are unable to find work. Statistics show that individuals with disabilities in both industrialized and developing countries are generally unable to obtain formal work. In India, only 100,000 of the country's 70 million individuals with disabilities are employed. In the United States, 14.3 of a projected 48.9 million people with disabilities were employed, with two-thirds of those unemployed reporting that they were unable to find work. Similarly in Belgium, only 30 percent of persons with disabilities were able to find gainful employment. In the United Kingdom, 45 percent of adults with disabilities were found to live below the poverty line. Reliable data on the rate of unemployment for persons with disabilities has yet to be determined in most developing countries.\n\nSociologists Colin Barnes and Geof Mercer demonstrated that this exclusion of persons with disabilities from the paid labor market is a primary reason why the majority of this population experiences far greater levels of poverty and are more reliant on the financial support of others. In addition to the economic gains associated with employment, researchers have shown that participation in the formal economic sector reduces discrimination of persons with disabilities. One anthropologist who chronicled the lives of persons with disabilities in Botswana noted that individuals who were able to find formal employment “will usually obtain a position in society equal to that of non-disabled citizens.” Because the formal workplace is such a social space, the exclusion of individuals with disabilities from this realm is seen by some sociologists to be a significant impediment to social inclusion and equality.\n\nEquity in employment has been strategized by some, such as sociologists Esther Wilder and William Walters, to depend on heightened awareness of current barriers, wider use of assistive technologies that can make workplaces and tasks more accessible, more accommodating job development, and most importantly deconstructing discrimination.\n\nCreating inclusive employment that better facilitates the participation of individuals with disabilities is demonstrated to have a significantly positive impact on not only the lives of these individuals, but also the economies of nations who implement such measures. The International Labour Organization estimates that the current exclusion of employable individuals with special needs is costing countries possible gains of 1 to 7 percent of their GDP.\n\nThe relationship between disability and poverty is seen by many to be especially problematic given that it places those with the greatest needs in a position where they have access to the fewest resources. Researchers from the United Nations and the Yale School of Public Health refer to the link between disability and poverty as a manifestation of a self-fulfilling prophecy where the assumption that this population is a drain of resources leads society to deny them access to avenues of success. Such exclusion of individuals on the basis of their special needs in turn denies them the opportunity to make meaningful contributions that disprove these stereotypes. Oxfam asserts that this negative cycle is largely due to a gross underestimation of the potential held by individuals with disabilities and a lack of awareness of the possibilities that each person may hold if the proper resources were present.\n\nThe early onset of preventable deaths has been demonstrated as a significant consequence of disability for those living in poverty. Researchers show that families who lack adequate economic agency are unable to care for children with special medical needs, resulting in preventable deaths. In times of economic hardship studies show families may divert resources from children with disabilities because investing in their livelihood is often perceived as an investment caretakers cannot afford to make. Benedicte Ingstad, an anthropologist who studied families with a member with disabilities, asserted that what some may consider neglect of individuals with disabilities “was mainly a reflection of the general hardship that the household was living under.\" A study conducted by Oxfam found that the rejection of a child with disabilities was not uncommon in areas of extreme poverty. The report went on to show that neglect of children with disabilities was far from a deliberate choice, but rather a consequence of a lack of essential resources. The study also demonstrated that services necessary to the well being of these children “are seized upon” when they are made available. The organization thus concludes that if families had the capacity to care for children with special needs they would do so willingly, but often the inability to access crucial resources bars them from administering proper care.\n\nInitiatives on the local, national, and transnational levels addressing the connection between poverty and disability are exceedingly rare. According to the UN, only 45 countries throughout the world have anti-discrimination and other disability-specific laws. Additionally, experts point to the Western world as a demonstration that the association between poverty and disability is not naturally dissolved through the development process. Instead, a conscious effort toward inclusive development is seen by theorists, such as Disability Policy expert Mark Priestley, as essential in the remediation process.\n\nDisability rights advocate James Charlton asserts that it is crucial to better incorporate the voices of individuals with disabilities into the decision making process. His literature on disability rights made popular the slogan, “Nothing about us without us,” evidencing the need to ensure those most affected by policy have an equitable hand in its creation. This need for agency is an issue particularly salient for those with special needs who are often negatively stereotyped as dependent upon others. Furthermore, many who are part of the disability rights movement argue that there is too little emphasis on aid designed to eliminate the physical and social barriers those with disabilities face. The movement asserts that unless these obstacles are rectified, the connection between disability and poverty will persist.\n\nEmployment is seen as a critical agent in reducing stigma and increasing capacity in the lives of individuals with disabilities. The lack of opportunities currently available is shown to perpetuate the vicious cycle, causing individuals with disabilities to fall into poverty. To address these concerns many recent initiatives have begun to develop more inclusive employment structures. One example of this is the Ntiro Project for Supported and Inclusive Employment. Located in South Africa, the project aims to eliminate the segragationist models prevalent in the country through coordinated efforts between districts, NGOs, and community organizations. The model stresses education and pairs individuals with intellectual disabilities with mentors until they have developed the skills necessary to perform their roles independently. The program then matches individuals with local employers. This gradualist model ensures that people who may have been deprived of the resources necessary to acquire essential skills are able to build their expertise and enter the workforce.\n\nThe United Nations has been at the forefront of initiating legislation that aims to deter the current toll disabilities take on individuals in society, especially those in poverty. In 1982 the UN published the World Programme of Action Concerning Disabled Persons, which explicitly states \"Particular efforts should be made to integrate the disabled in the development process and that effective measures for prevention, rehabilitation and equalization of opportunities are therefore essential.\" This doctrine set stage for the UN Decade of the Disabled Person from 1983 to 1992, where, at its close, the General Assembly adopted the Standard Rules of the Equalization of Opportunities for Persons with Disabilities. The Standard Rules encourages states to remove social, cultural, economic, educational, and political barriers that bar individuals with disabilities from participating equally in society. Proponents claim that these movements on behalf of the UN helped facilitate more inclusive development policy and brought disability rights to the forefront.\n\nCritics assert that the relationship between disability and poverty may be overstated. Cultural differences in the definition of disability, bias leading to more generous estimates on behalf of researchers, and the variability in incidences that are not accounted for between countries are all speculated to be part of this mischaracterization. These factors lead some organizations to conclude that the projection asserting 10 percent of the global population belongs to the disabled community is entirely too broad. Speculation over the projection of a 10 percent disability rate has led other independent studies to collect varying results. The World Health Organization updated their estimate to 4 percent for developing countries and 7 percent for industrialized countries. USAID maintains the initial 10 percent figure, while the United Nations works off half of that rate with a projection of 5 percent. The percentage of the world's population with disabilities remains a highly contested matter.\n\nThe argument that development should be channeled to better the agency of individuals with disabilities has been contested on several grounds. First, critics argue that development is enacted to harness potential that most individuals in this population do not possess. Second, the case that health care costs for many persons with special needs are simply too great to be shouldered by the government or NGO's has been made, especially with regard to emerging economies. Furthermore, there is no guarantee that investing in an individual's rehabilitation will result in substantial change in their agency. Lastly is the proposition of priorities. It is argued that most countries in need of extensive development must focus on health ails such as infant mortality, diarrhea, and malaria that are widespread killers not limited to a specific population.\n\nCritique with respect to potential solutions has also been made. In regards to implementing change through policy, critics have noted that the weak legal standing of United Nations' documents and the lack of resources available to aid in their implementation have resulted in a struggle to achieve the goals set forth by the General Assembly. Other studies have shown that policy on a national level has not necessarily equated to marked improvements within these countries. One such example is the United States where sociologists Esther Wilder and William Walters purport that “the employment of disabled individuals has increased only marginally since the Americans with Disabilities Act was passed.” The smaller than anticipated impact of the ADA and other policy-based initiatives is seen as a critical flaw in legislation. This is because many issues surrounding disability, namely employment discrimination, are generally reconciled through the legal system necessitating that individuals engage in the often expensive process of litigation.\n\n"}
{"id": "8864225", "url": "https://en.wikipedia.org/wiki?curid=8864225", "title": "Discovery doctrine", "text": "Discovery doctrine\n\nThe Discovery doctrine is a concept of public international law expounded by the United States Supreme Court in a series of decisions, most notably \"Johnson v. M'Intosh\" in 1823. Chief Justice John Marshall explained and applied the way that colonial powers laid claim to lands belonging to foreign sovereign nations during the Age of Discovery. Under it, title to lands lay with the government whose subjects travelled to and occupied a territory whose inhabitants were not subjects of a European Christian monarch. The doctrine has been primarily used to support decisions invalidating or ignoring aboriginal possession of land in favor of colonial or post-colonial governments.\n\nThe 1823 case was the result of collusive lawsuits where land speculators worked together to make claims to achieve a desired result. John Marshall explained the Court's reasoning. The decision has been the subject of a number of law review articles and has come under increased scrutiny by modern legal theorists.\n\nThe Doctrine of Discovery was promulgated by European monarchies in order to legitimize the colonization of lands outside of Europe. Between the mid-fifteenth century and the mid-twentieth century, this idea allowed European entities to seize lands inhabited by indigenous peoples under the guise of discovery. In 1494, the Treaty of Tordesillas declared that only non-Christian lands could be colonized under the Discovery Doctrine.\n\nIn 1792, U.S. Secretary of State Thomas Jefferson declared that the Doctrine of the Discovery would extend from Europe to the infant U.S. government. The Doctrine and its legacy continue to influence American imperialism and treatment of indigenous peoples.\n\nThe plaintiff Johnson had inherited land, originally purchased from the Piankeshaw tribes. Defendant McIntosh claimed the same land, having purchased it under a grant from the United States. It appears that in 1775 members of the Piankeshaw tribe sold certain land in the Indiana Territory to Lord Dunmore, royal governor of Virginia and others. In 1805 the Piankeshaw conveyed much of the same land to William Henry Harrison, governor of the Indiana Territory, thus giving rise to conflicting claims of title. In reviewing whether the courts of the United States should recognize land titles obtained from Native Americans prior to American independence, the court decided that they should not. Chief Justice John Marshall had large real estate holdings that would have been affected if the case were decided in favor of Johnson. Rather than remove himself from the case, however, the chief justice wrote the decision for a unanimous US Supreme Court.\n\nMarshall found that ownership of land comes into existence by virtue of discovery of that land, a rule that had been observed by all European countries with settlements in the New World. Legally, the United States was the true owner of the land because it inherited that ownership from Britain, the original discoverer.\n\nMarshall noted:\nOn the discovery of this immense continent, the great nations of Europe ... as they were all in pursuit of nearly the same object, it was necessary, in order to avoid conflicting settlements, and consequent war with each other, to establish a principle which all should acknowledge as the law by which the right of acquisition, which they all asserted, should be regulated as between themselves. This principle was that discovery gave title to the government by whose subjects, or by whose authority, it was made, against all other European governments, which title might be consummated by possession. ... The history of America, from its discovery to the present day, proves, we think, the universal recognition of these principles. \n\nChief Justice Marshall noted the 1455 papal bull Romanus Pontifex approved Portugal's claims to lands discovered along the coast of West Africa, and the 1493 Inter Caetera had ratified Spain's right to conquer newly found lands, after Christopher Columbus had already begun doing so, but stated:\n\"Spain did not rest her title solely on the grant of the Pope. Her discussions respecting boundary, with France, with Great Britain, and with the United States, all show that she placed it on the rights given by discovery. Portugal sustained her claim to the by the same title.\"\n\nMarshall pointed to the exploration charters given to John Cabot as proof that the British had operated under the doctrine. The tribes which occupied the land were, at the moment of discovery, no longer completely sovereign and had no property rights but rather merely held a right of occupancy. Further, only the discovering nation or its successor could take possession of the land from the natives by conquest or purchase.\n\nThe doctrine was cited in other cases as well. With \"Cherokee Nation v. Georgia\", it supported the concept that tribes were not independent states but \"domestic dependent nations\". The decisions in \"Oliphant v. Suquamish Indian Tribe\" and \"Duro v. Reina\" used the doctrine to prohibit tribes from criminally prosecuting first non-Indians, then Indians who were not a member of the prosecuting tribe.\n\nThe doctrine has been cited by the US Supreme Court as recently as 2005, in City of Sherrill, NY v. Oneida Nation: \"Under the 'doctrine of discovery...' fee title (ownership) to the lands occupied by Indians when the colonists arrived became vested in the sovereign-first the discovering European nation and later the original states and the United States.\"\n\nThe Doctrine of Discovery was promulgated by European monarchies in order to legitimize the colonization of lands outside of Europe. Between the mid-fifteenth century and the mid-twentieth century, this idea allowed European entities to seize lands inhabited by indigenous peoples under the guise of discovery. In 1494, the Treaty of Tordesillas declared that only non-Christian lands could be colonized under the Discovery Doctrine.\n\nIn 1792, U.S. Secretary of State Thomas Jefferson declared that the Doctrine of the Discovery would extend from Europe to the infant U.S. government. The Doctrine and its legacy continue to influence American imperialism and treatment of indigenous peoples.\n\nAs the Piankeshaw were not party to the litigation, \"no Indian voices were heard in a case which had, and continues to have, profound effects on Indian property rights.\"\n\nProfessor Blake A. Watson of the University of Dayton School of Law finds Marshall's claim of \"universal recognition\" of the \"doctrine of discovery\" historically inaccurate.\n\nIn reviewing the history of European exploration Marshall did not take note of Spanish Dominican philosopher Francisco de Vitoria's 1532 \"De Indis\" nor \"De Jure belli Hispanorum in barbaros\". Vitoria adopted from Thomas Aquinas the Roman law concept of \"ius gentium\", and concluded that the Indians were rightful owners of their property and that their chiefs validly exercised jurisdiction over their tribes, a position held previously by Palacios Rubios. His defense of American Indians was based on a scholastic understanding of the intrinsic dignity of man, a dignity he found being violated by Spain's policies in the New World.\n\nMarshall also overlooked more recent American experience, specifically Roger Williams's purchase of the Providence Plantation. In order to forestall Massachusetts and Plymouth designs on the land, Williams subsequently traveled to England to obtain a patent which referenced the purchase from the natives. The Royal Charter of Rhode Island issued by Charles II acknowledged the rights of the Indians to the land.\n\nNor does Justice Marshall seem to have taken note of the policy of the Dutch West India Company which only conferred ownership rights in New Netherland after the grantee had acquired title by purchase from the Indian owners, a practice also followed by the Quakers in Pennsylvania.\n\nWatson and others, such as Robert Williams Jr. suggest that Marshall misinterpreted the \"discovery doctrine\" as giving exclusive right to lands discovered, rather than the exclusive right to treaty with the inhabitants thereof.\n\nDiscovery doctrine has been severely condemned as socially unjust, racist, and in violation of basic and fundamental human rights. The United Nations Permanent Forum on Indigenous Issues noted Discovery doctrine \"as the foundation of the violation of their (Indigenous people) human rights\".\n\nIn 2012, the United Nations Economic and Social Council Permanent Forum on Indigenous Issues called for a mechanism to investigate historical land claims.\n\nDuring the General Convention of the Episcopal Church conducted on 8–17 August 2009, the bishops of the church adopted a resolution officially repudiating the discovery doctrine.\n\nAt the 2012 Unitarian Universalist Association General Assembly in Phoenix, AZ, delegates of the Unitarian Universalist Association passed a resolution repudiating the Doctrine of Discovery and calling on Unitarian Universalists to study the Doctrine and eliminate its presence from the current-day policies, programs, theologies, and structures of Unitarian Universalism. In 2013, at its 29th General Synod, the United Church of Christ followed suit in repudiating the doctrine in a near-unanimous vote.\n\nAt the 2016 Synod, 10-17 June in Grand Rapids, MI, delegates to the annual general assembly of the Christian Reformed Church rejected the Doctrine of Discovery as heresy in response to a study report on the topic.\n\nAlso in 2016, on November 3, a group of 500 clergy members publicly denounced the Doctrine of Discovery at the Standing Rock pipeline protests.\n\n\n"}
{"id": "41079", "url": "https://en.wikipedia.org/wiki?curid=41079", "title": "Dynamic range", "text": "Dynamic range\n\nDynamic range, abbreviated DR, DNR, or DYR is the ratio between the largest and smallest values that a certain quantity can assume. It is often used in the context of signals, like sound and light. It is measured either as a ratio or as a base-10 (decibel) or base-2 (doublings, bits or stops) logarithmic value of the difference between the smallest and largest signal values.\n\nElectronically reproduced audio and video is often processed to fit the original material with a wide dynamic range into a narrower recorded dynamic range that can more easily be stored and reproduced; This processing is called dynamic range compression. \n\nThe human senses of sight and hearing have a very high dynamic range. A human cannot perform these feats of perception at both extremes of the scale at the same time. The eyes take time to adjust to different light levels, and the dynamic range of the human eye in a given scene is actually quite limited due to optical glare. The instantaneous dynamic range of human audio perception is similarly subject to masking so that, for example, a whisper cannot be heard in loud surroundings.\n\nA human is capable of hearing (and usefully discerning) anything from a quiet murmur in a soundproofed room to the loudest heavy metal concert. Such a difference can exceed 100 dB which represents a factor of 100,000 in amplitude and a factor 10,000,000,000 in power. The dynamic range of human hearing is roughly 140 dB, varying with frequency, from the threshold of hearing (around −9 dB SPL at 3 kHz) to the threshold of pain (from 120–140 dB SPL). This wide dynamic range cannot be perceived all at once, however; the tensor tympani, stapedius muscle, and outer hair cells all act as mechanical dynamic range compressors to adjust the sensitivity of the ear to different ambient levels.\n\nA human can see objects in starlight or in bright sunlight, even though on a moonless night objects receive 1/1,000,000,000 of the illumination they would on a bright sunny day; a dynamic range of 90 dB.\n\nIn practice, it is difficult for humans to achieve the full dynamic experience using electronic equipment. For example, a good quality LCD has a dynamic range limited to around 1000:1, and some of the latest CMOS image sensors now have measured dynamic ranges of about 23,000:1 Paper reflectance can produce a dynamic range of about 100:1. A professional video camera such as the Sony Digital Betacam achieves a dynamic range of greater than 90 dB in audio recording.\n\nAudio engineers use \"dynamic range\" to describe the ratio of the amplitude of the loudest possible undistorted signal to the noise floor, say of a microphone or loudspeaker. Dynamic range is therefore the signal-to-noise ratio (SNR) for the case where the signal is the loudest possible for the system. For example, if the ceiling of a device is 5 V (rms) and the noise floor is 10 µV (rms) then the dynamic range is 500000:1, or 114 dB:\n\nIn digital audio theory the dynamic range is limited by quantization error. The maximum achievable dynamic range for a digital audio system with \"Q\"-bit uniform quantization is calculated as the ratio of the largest sine-wave rms to rms noise is:\n\nHowever, the usable dynamic range may be greater, as a properly dithered recording device can record signals well below the noise floor.\n\nThe 16-bit compact disc has a theoretical undithered dynamic range of about 96 dB; however, the \"perceived\" dynamic range of 16-bit audio can be 120 dB or more with noise-shaped dither, taking advantage of the frequency response of the human ear.\n\nDigital audio with undithered 20-bit quantization is theoretically capable of 120 dB dynamic range. 24-bit digital audio affords 144 dB dynamic range. Most Digital audio workstations process audio with 32-bit floating-point representation which affords even higher dynamic range and so loss of dynamic range is no longer a concern in terms of digital audio processing. Dynamic range limitations typically result from improper gain staging, recording technique including ambient noise and intentional application of dynamic range compression.\n\nDynamic range in analog audio is the difference between low-level thermal noise in the electronic circuitry and high-level signal saturation resulting in increased distortion and, if pushed higher, clipping. Multiple noise processes determine the noise floor of a system. Noise can be picked up from microphone self-noise, preamp noise, wiring and interconnection noise, media noise, etc.\n\nEarly 78 rpm phonograph discs had a dynamic range of up to 40 dB, soon reduced to 30 dB and worse due to wear from repeated play. Vinyl microgroove phonograph records typically yield 55-65 dB, though the first play of the higher-fidelity outer rings can achieve a dynamic range of 70 dB.\n\nGerman magnetic tape in 1941 was reported to have had a dynamic range of 60 dB, though modern day restoration experts of such tapes note 45-50 dB as the observed dynamic range. Ampex tape recorders in the 1950s achieved 60 dB in practical usage, In the 1960s, improvements in tape formulation processes resulted in 7 dB greater range, and Ray Dolby developed the Dolby A-Type noise reduction system that increased low- and mid-frequency dynamic range on magnetic tape by 10 dB, and high-frequency by 15 dB, using companding (compression and expansion) of four frequency bands. The peak of professional analog magnetic recording tape technology reached 90 dB dynamic range in the midband frequencies at 3% distortion, or about 80 dB in practical broadband applications. The Dolby SR noise reduction system gave a 20 dB further increased range resulting in 110 dB in the midband frequencies at 3% distortion. \n\nCompact Cassette tape performance ranges from 50 to 56 dB depending on tape formulation, with type IV tape tapes giving the greatest dynamic range, and systems such as XDR, dbx and Dolby noise reduction system increasing it further. Specialized bias and record head improvements by Nakamichi and Tandberg combined with Dolby C noise reduction yielded 72 dB dynamic range for the cassette.\n\nThe rugged elements of moving-coil microphones can have a dynamic range of up to 140 dB (at increased distortion), while condenser microphones are limited by the overloading of their associated electronic circuitry. Practical considerations of acceptable distortion levels in microphones combined with typical practices in a recording studio result in a useful operating range of 125 dB.\n\nIn 1981, researchers at Ampex determined that a dynamic range of 118 dB on a dithered digital audio stream was necessary for subjective noise-free playback of music in quiet listening environments.\n\nSince the early 1990s, it has been recommended by several authorities, including the Audio Engineering Society, that measurements of dynamic range be made with an audio signal present, which is then filtered out to get the noise floor. This avoids questionable measurements based on the use of blank media, or muting circuits.\n\nWhen showing a movie or a game, a display is able to show both shadowy nighttime scenes and bright outdoor sunlit scenes, but in fact the level of light coming from the display is much the same for both types of scene (perhaps different by a factor of 10). Knowing that the display does not have a huge dynamic range, the producers do not attempt to make the nighttime scenes millions of times less bright than the daytime scenes, but instead use other cues to suggest night or day. A nighttime scene will usually contain duller colours and will often be lit with blue lighting, which reflects the way that the human eye sees colours at low light levels.\n\nElectronics engineers apply the term to: \n\nIn metrology, such as when performed in support of science, engineering or manufacturing objectives, dynamic range refers to the range of values that can be measured by a sensor or metrology instrument. Often this dynamic range of measurement is limited at one end of the range by saturation of a sensing signal sensor or by physical limits that exist on the motion or other response capability of a mechanical indicator. The other end of the dynamic range of measurement is often limited by one or more sources of random noise or uncertainty in signal levels that may be described as defining the sensitivity of the sensor or metrology device. When digital sensors or sensor signal converters are a component of the sensor or metrology device, the dynamic range of measurement will be also related to the number of binary digits (bits) used in a digital numeric representation in which the measured value is linearly related to the digital number. For example, a 12-bit digital sensor or converter can provide a dynamic range in which the ratio of the maximum measured value to the minimum measured value is up to 2 = 4096. With gamma correction, this limitation can be relaxed somewhat; for example, the 8-bit encoding used in sRGB image encoding represents a maximum to minimum ratio of about 3000.\n\nMetrology systems and devices may use several basic methods to increase their basic dynamic range. These methods include averaging and other forms of filtering, correction of receivers characteristics, repetition of measurements, nonlinear transformations to avoid saturation, etc. In more advance forms of metrology, such as multiwavelength digital holography, interferometry measurements made at different scales (different wavelengths) can be combined to retain the same low-end resolution while extending the upper end of the dynamic range of measurement by orders of magnitude.\n\nIn music, \"dynamic range\" is the difference between the quietest and loudest volume of an instrument, part or piece of music. In modern recording, this range is often limited through dynamic range compression, which allows for louder volume, but can make the recording sound less exciting or live.\n\nThe term \"dynamic range\" may be confusing in music because it has two conflicting definitions, particularly in the understanding of the loudness war phenomenon. \"Dynamic range\" may refer to micro-dynamics, related to crest factor, whereas the European Broadcasting Union, in EBU3342 Loudness Range, defines \"dynamic range\" as the difference between the quietest and loudest volume, a matter of macro-dynamics.\n\nThe dynamic range of music as normally perceived in a concert hall does not exceed 80 dB, and human speech is normally perceived over a range of about 40 dB.\n\nPhotographers use \"dynamic range\" for the luminance range of a scene being photographed, or the limits of luminance range that a given digital camera or film can capture, or the \"opacity range\" of developed film images, or the \"reflectance range\" of images on photographic papers.\n\nThe dynamic range of digital photography is comparable to the capabilities of photographic film and both are comparable to the capabilities of the human eye.\n\nThere are photographic techniques that support higher dynamic range. \n\nConsumer-grade image file formats sometimes restrict dynamic range. The most severe dynamic-range limitation in photography may not involve encoding, but rather reproduction to, say, a paper print or computer screen. In that case, not only local tone mapping, but also \"dynamic range adjustment\" can be effective in revealing detail throughout light and dark areas: The principle is the same as that of dodging and burning (using different lengths of exposures in different areas when making a photographic print) in the chemical darkroom. The principle is also similar to gain riding or automatic level control in audio work, which serves to keep a signal audible in a noisy listening environment and to avoid peak levels which overload the reproducing equipment, or which are unnaturally or uncomfortably loud.\n\nIf a camera sensor is incapable of recording the full dynamic range of a scene, high-dynamic-range (HDR) techniques may be used in postprocessing, which generally involve combining multiple exposures using software.\n\n\n"}
{"id": "258980", "url": "https://en.wikipedia.org/wiki?curid=258980", "title": "Ergodic hypothesis", "text": "Ergodic hypothesis\n\nIn physics and thermodynamics, the ergodic hypothesis says that, over long periods of time, the time spent by a system in some region of the phase space of microstates with the same energy is proportional to the volume of this region, i.e., that all accessible microstates are equiprobable over a long period of time.\n\nLiouville's Theorem states that, for Hamiltonian systems, the local density of microstates following a particle path through phase space is constant as viewed by an observer moving with the ensemble (i.e., the convective time derivative is zero). Thus, if the microstates are uniformly distributed in phase space initially, they will remain so at all times. But Liouville's theorem does \"not\" imply that the ergodic hypothesis holds for all Hamiltonian systems.\n\nThe ergodic hypothesis is often assumed in the statistical analysis of computational physics. The analyst would assume that the average of a process parameter over time and the average over the statistical ensemble are the same. This assumption that it is as good to simulate a system over a long time as it is to make many independent realizations of the same system is not always correct. (See, for example, the Fermi–Pasta–Ulam–Tsingou experiment of 1953.)\n\nAssumption of the ergodic hypothesis allows proof that certain types of perpetual motion machines of the second kind are impossible. The figure above displays situations where the ergodic hypothesis does and does not hold for a simplistic model of an ideal gas. If the walls are perfectly smooth and circular, the ergodic hypothesis does \"not\" hold. If it was possible to construct a sort of \"tunnel\" whereby specular reflections cause atoms to move from a less populated container to an identical one with greater density, this would allow the direct conversion of random thermal energy into useful work in a way that does not require a heat bath. But, by Liouville's theorem, if all regions of phase space are equally populated at time, t=0, then they are equally probable for all time. No reflective 'trap' or Maxwell's demon (such as depicted in the figure) will 'unmix' a gas that has randomly filled both containers with equal density and pressure.\n\nIn macroscopic systems, the timescales over which a system can truly explore the entirety of its own phase space can be sufficiently large that the thermodynamic equilibrium state exhibits some form of ergodicity breaking. A common example is that of spontaneous magnetisation in ferromagnetic systems, whereby below the Curie temperature the system preferentially adopts a non-zero magnetisation even though the ergodic hypothesis would imply that no net magnetisation should exist by virtue of the system exploring all states whose time-averaged magnetisation should be zero. The fact that macroscopic systems often violate the literal form of the ergodic hypothesis is an example of spontaneous symmetry breaking.\n\nHowever, complex disordered systems such as a spin glass show an even more complicated form of ergodicity breaking where the properties of the thermodynamic equilibrium state seen in practice are much more difficult to predict purely by symmetry arguments. Also conventional glasses (e.g. window glasses) violate ergodicity in a complicated manner. In practice this means that on sufficiently short time scales (e.g. those of parts of seconds, minutes, or a few hours) the systems may behave as \"solids\", i.e. with a positive shear modulus, but on extremely long scales, e.g. over millennia or eons, as \"liquids\", or with two or more time scales and \"plateaux\" in between.\n\nErgodic theory is a branch of mathematics which deals with dynamical systems that satisfy a version of this hypothesis, phrased in the language of measure theory.\n\n"}
{"id": "1924227", "url": "https://en.wikipedia.org/wiki?curid=1924227", "title": "Ethical arguments regarding torture", "text": "Ethical arguments regarding torture\n\nEthical arguments have arisen regarding torture, and its debated value to society. Despite worldwide condemnation and the existence of treaty provisions that forbid it, some countries still use it. The ethical assertion that torture is a tool is at question.\n\nThe basic ethical debate is often presented as a matter of deontological versus utilitarian viewpoint. A utilitarian thinker may believe, when the overall outcome of lives saved due to torture are positive, torture can be justified; the intended outcome of an action is held as the primary factor in determining its merit or morality. The opposite view is the deontological, from Greek \"deon\" (duty), which proposes general rules and values that are to be respected regardless of outcome. However, if the outcome of policies allowing torture are uncertain (or if the outcome can not be definitely traced back to the use of torture) then there can be a utilitarian view that torture is wrong (see issues related to the ends justifying the means in analysis of the ticking time bomb scenario).\n\nIt has been suggested that one of the reasons torture endures is that torture indeed works in some instances to extract information/confession if those who are being tortured are indeed guilty. Richard Posner, a highly influential judge on the United States Court of Appeals for the Seventh Circuit, argued that \"If torture is the only means of obtaining the information necessary to prevent the detonation of a nuclear bomb in Times Square, torture should be used—and will be used—to obtain the information... No one who doubts that this is the case should be in a position of responsibility.\" However, some experienced intelligence officers have more recently come forward claiming that not only does torture not work, it can result in false information since people undergoing torture will say anything just to make the torture stop. Some people also point to neuroscience to demonstrate that torture may further impair a person's ability to tell the truth. And the debate continues.\n\nA utilitarian argument against torture is that the majority of tortures are employed not as a method of extracting information, but as a method of terrorizing and subjugating the population, enabling state forces to dispense with ordinary means of establishing innocence or guilt and with the whole legal apparatus altogether. Therefore, it is better that a few individuals be killed by bombers than a much greater number—possibly thousands of innocent people—be tortured and murdered and legal and constitutional provisions destroyed. During the investigation of Italian Prime Minister Aldo Moro's kidnapping, General Carlo Alberto Dalla Chiesa reportedly responded to a member of the security services who suggested torture against a suspect, \"Italy can survive the loss of Aldo Moro. It would not survive the introduction of torture.\"\n\nHistorically, torture has been reviled as an idea, yet employed as a tool and defended by its wielders, often in direct contradiction to their own averred beliefs. Judicial torture was a common feature of the legal systems of many countries including all Civil Law countries in Europe until around the French Revolution. This was part of ancient Greek and Roman Law theory that remained valid in Europe. Roman Law assumed, for example, that slaves would not tell the truth in a legal court as they were always vulnerable to threats from their owners. Their testimony could only be of value if it were extracted by a greater fear of torture. Legal scholars were well aware of the problems of false testimony produced by the threat of torture. In theory torture was not meant to produce a confession as such, but rather details of the crime or crime scene which only the guilty party would know.\n\nThe Spanish Inquisition is probably the most infamous example in which torture was used to extract information regarding allegations of heresy. In early modern times under certain conditions, torture was used in England. For example, the confession of Marc Smeaton at the trial of Anne Boleyn was presented in written form only, either to hide from the court that Smeaton had been tortured on the rack for four hours, or because Thomas Cromwell was worried that he would recant his confession if cross examined. When Guy Fawkes was arrested for his role in the Gunpowder Plot of 1605 he was tortured until he revealed all he knew about the plot. This was not so much to extract a confession, which was not needed to prove his guilt, but to extract from him the names of his fellow conspirators. By this time torture was not routine in England and a special warrant from King James I was needed before he could be tortured. The wording of the warrant shows some concerns for humanitarian considerations, the severity of the methods of interrogation were to be increased gradually until the interrogators were sure that Fawkes had told all he knew. In the end this did not help Fawkes much as he was broken on the only rack in England, which was in the Tower of London. Torture was abolished in England around 1640 (except \"peine forte et dure\" which was abolished in 1772).\n\nThe use of torture in Europe came under attack during the Enlightenment. Cesare Beccaria's \"On Crimes and Punishments\" (1764) denounced the use of torture as cruel and contrary to reason. The French Revolution abolished the use of torture in France and the French Armies carried abolition to most of the rest of Europe. The last European jurisdictions to abolish legal torture were Portugal (1828) and the canton of Glarus in Switzerland (1851).\n\nUnder codified legal systems such as France, torture was superseded with a legal system that is highly dependent on investigating magistrates and the confession remains \"The Queen of Proofs\". Such magistrates are often under pressure to produce results. It is alleged that in many cases police violence towards suspects has been ignored by the magistrates. In the adversarial system of Common Law used throughout the English-speaking world, the experience is a different one. As the two parties have to convince a jury whether the defendant in a case is guilty or innocent of a crime, if the defence can persuade a jury that reasonable doubt exists over the credibility of a confession then the jury is likely to disregard the confession. If the defence can show that the confession was made under such duress that most people would make such a confession, then the jury is likely to question the confession's credibility. Usually the more duress that can be shown to have been used by law enforcement by the defence, the less weight most juries will place on confessions. In Britain partly to protect the individual against police brutality and partly to make confessions credible to a jury, all interviews with a suspect are audio taped on a machine which make two simultaneous copies one for the police and one for the defendant. In Northern Ireland, where society is more polarised than in the rest of the United Kingdom, which means that allegations of police brutality are perceived by sections of the community to carry more credence, interviews are video taped.\n\nIt has been alleged that in certain circumstances torture, even though it is illegal, may have been used by some European countries. In \"anti-terrorist\" campaigns where information is needed for intelligence purposes, and not to obtain a confession for use in court, there is a temptation by the security forces, whether authorised by governments or not, to extract intelligence from alleged terrorists using any means available including the use of torture. Where there is a time component to a crime, for example in a kidnapping case, there is also a temptation for the police to try to extract information by methods which would nullify the use of such information in court.\n\nSome scholars have argued that the need for information outweighs the moral and ethical arguments against torture.\n\nYasmin Alibhai-Brown in an opinion article published in The Independent on 23 May 2005 wrote:\nTwo academics at Deakin University in Victoria, Australia, Professor Mirko Bagaric, a Croatian born Australian based author and lawyer, who is the head of Deakin University's Law School, and a fellow Deakin law lecturer, Julie Clarke, published a paper in the \"University of San Francisco Law Review\" arguing that when many lives are in imminent danger, \"all forms of harm\" may be inflicted on a suspect, even if this might result in \"annihilation\".The reasoning behind the proposal to legalise torture is that:\nIt was observed that Bagaric \"was not the author of what he wrote, all he did was reintroducing Alan M. Dershowitz’ thesis, Sharon’s [Israeli] government legal adviser and the theorist of the legal torture\".\n\nIt was also observed that Bagaric has written many pieces calling for men to be sentenced more than women for the same crime. This calls into question his ability to deal impartially with regard to crime and torture, which is mainly committed against men.\n\nWhen reviewing Alan Dershowitz's book, \"Why Terrorism Works: Understanding the Threat, Responding to the Challenge\" Richard Posner, a judge of the United States Court of Appeals for the Seventh Circuit, wrote\nOn December 20, 2005, Albert Mohler, president of the Southern Baptist Theological Seminary addressed the problem of whether torture should be used by American military forces in order to gain important information from terrorist suspects. Although he spoke out against any form of legal codification, he did state the following:\n\nMany experts argue that torture is an unreliable means of obtaining useful information.\nHowever, many states have used torture not to extract information, but as a means of terrorising their populations or specific communities. Frantz Fanon, in \"The Wretched of the Earth\" reports the French in Algeria using \"preventative torture\" on entirely innocent people to stop them doing anything in future.\n\nIn most countries torture is illegal, and this being so, outside the normal framework for establishing guilt or innocence. Therefore, an abnormally large proportion of torture victims are either innocent (apart from membership of target communities) or of mistaken identity. For example, Khalid el-Masri, an innocent German citizen was kidnapped and tortured, having been mistaken for Al-Qaida chief Khalid al-Masri. The Red Cross in Iraq estimated that 80% of detainees at Abu Ghraib were the \"wrong people\".\n\nIn response to the article by Professor Bagaric and Mrs Clarke, Amnesty International spokeswoman Nicole Bieske, who is also a lawyer, was stunned by the idea of regulating torture.\nProfessor Bagaric and Mrs Clarke submitted the paper to an American law journal because of:\nJoe Navarro, one of the F.B.I.’s top experts in questioning techniques, told The New Yorker,\n\nToleration of torture and arbitrary detention has been likened to a \"cancer of democracy\" in a book of the same title by Pierre Vidal-Naquet, which begins to undermine all other aspects of a state's legitimacy. On the 20th anniversary of the coming into force of the United Nations Convention against Torture, Philip Hensher writes \"Civilization is at once compromised if, in defense of other freedoms, it decides to regress, to accept the possibility of torture as it is seen in the movies.\"\n\nIn law enforcement, one perceived argument is the necessity of force to extract information from a suspect when regular interrogation yields no results and time is of the essence, as can be seen in the most frequently cited theoretical example is the \"ticking time bomb\" scenario, where a known terrorist has planted a nuclear bomb. In such circumstances, it has been argued, that not to use torture would be wrong, and by others that using torture would change society in a manner which would be worse.\n\nThe obvious rebuttal to this stance is that no such scenario has ever existed. In addition, those situations resembling such a case were resolved without the need to torture any suspect. Furthermore, it is asked whether torture would be limited to suspects, or whether one could torture the family and friends of this detainee to make him compliant.\n\n\n\nBeyond that, another reason is that torture fails to elicit the expected information because the subject is saying anything interrogators want to hear to stop the ordeal (or deliberately lies to waste the interrogators' time and make it more likely the bomb will go off), or worse: the detainee is innocent. By adopting a \"the ends justifies the means\" approach this would allow nine innocent people to be tortured as long as the tenth offered a full confession.\n\nIt has been estimated that as few as two dozen of the 600 detainees at Guantanamo had any potential intelligence value even if it could be obtained from them.\n\n\n\n"}
{"id": "43823562", "url": "https://en.wikipedia.org/wiki?curid=43823562", "title": "Framework law", "text": "Framework law\n\nFramework laws are laws that are more specific than constitutional provisions. They lay down general obligations and principles but leave to governing authorities the task of enacting the further legislation and other specific measures, as may be required.\n"}
{"id": "48424171", "url": "https://en.wikipedia.org/wiki?curid=48424171", "title": "Free Will (book)", "text": "Free Will (book)\n\nFree Will is a 2012 book by American neuroscientist and author Sam Harris. Harris argues that the truth about the human mind (that free will is an illusion) does not undermine morality or diminish the importance of political and social freedom, and can as well as should change the way we think about some of the most important questions in life.\n\nHarris says the idea of free will \"cannot be mapped on to any conceivable reality\" and is incoherent. According to Harris, science \"reveals you to be a biochemical puppet.\" People's thoughts and intentions, Harris says, \"emerge from background causes of which we are unaware and over which we exert no conscious control.\" Every choice we make is made as a result of preceding causes. These choices we make are determined by those causes, and are therefore not really choices at all. Harris also draws a distinction between conscious and unconscious reactions to the world. Even without free will, consciousness has an important role to play in the choices we make. Harris argues that this realization about the human mind does not undermine morality or diminish the importance of social and political freedom, but it can and should change the way we think about some of the most important questions in life.\n\nThe book has undergone some critique.\n\nThe author uses the Libet argument, which has been challenged by Adrian G. Guggisberg and Annaïs Mottaz. A study by Aaron Schurger and colleagues published in PNAS challenged assumptions about the causal nature of the readiness potential itself (and the \"pre-movement buildup\" of neural activity in general), thus denying the conclusions drawn from studies such as Libet's and Fried's.\n\n"}
{"id": "5392914", "url": "https://en.wikipedia.org/wiki?curid=5392914", "title": "History of measurement systems in India", "text": "History of measurement systems in India\n\nThe history of measurement systems in India begins in early Indus Valley Civilisation with the earliest surviving samples dated to the 5th millennium BCE. Since early times the adoption of standard weights and measures has reflected in the country's architectural, folk, and metallurgical artifacts. A complex system of weights and measures was adopted by the Maurya empire (322–185 BCE), which also formulated regulations for the usage of this system. Later, the Mughal empire (1526–1857) used standard measures to determine land holdings and collect land tax as a part of Mughal land reforms. The formal metrication in India is dated to 1st october 1958 when the Indian Government adopted the International System of Units (SI).\n\nStandard weights and measures were developed by the Indus Valley Civilisation. The centralised weight and measure system served the commercial interest of Indus merchants as smaller weight measures were used to measure luxury goods while larger weights were employed for buying bulkier items, such as food grains etc. Weights existed in multiples of a standard weight and in categories. Technical standardisation enabled gauging devices to be effectively used in angular measurement and measurement for construction. Uniform units of length were used in the planning of towns such as Lothal, Surkotada, Kalibangan, Dolavira, Harappa, and Mohenjo-daro. The weights and measures of the Indus civilisation also reached Persia and Central Asia, where they were further modified. Shigeo Iwata describes the excavated weights unearthed from the Indus civilisation:\n\nRulers made from Ivory were in use by the Indus Valley Civilisation prior to 1500 BCE. Excavations at Lothal (2400 BCE) have yielded one such ruler calibrated to about . Ian Whitelaw (2007)—on the subject of a ruler excavated from the Mohenjo-daro site—writes that: 'the Mohenjo-Daro ruler is divided into units corresponding to 1.32 inches (33.5 mm) and these are marked out in decimal subdivisions with amazing accuracy—to within 0.005 of an inch. Ancient bricks found throughout the region have dimensions that correspond to these units.' The Indus civilisation constructed pan balances made of copper, bronze, and ceramics. One excavated pan balance from Mohenjo-daro (2600–1900 BCE) was constructed using a cord-pivot type fulcrum, a bronze beam, and two pans. A number of excavated surveying instruments and measuring rods have yielded evidence of early cartographic activity.\n\nWeights and measures are mentioned throughout the religious and secular works of the Vedic period in India. Some sources that mention various units of measurement are \"Satapatha Brahmana\", \"Apastamba Sutra\", and the \"Eight Chapters\" of the grammarian Pāṇini. Indian astronomers kept a pañcānga for calculations of \"tithi\" (lunar day), vāra (weekday), naksatra (asterism), and \"karan\" (half lunar day) for social and religious events. Klostermaier (2003) states that: \"Indian astronomers calculated the duration of one \"kalpa\" (a cycle of the universe during which all the heavenly bodies return to their original positions) to be 432,00,00,000 years.\"\n\nSteelyard balances—found in India since the 4th century BCE—have been excavated from the archaeological sites of Gandhara and Amravati. Evidence of a complex system of weights and measures existing in use for multiple purposes under the central control of the Maurya administration (322–185 BCE) is found in the \"Arthashastra\". Archaeologist Frank Raymond Allchin outlines the details of the measurement systems of the Maurya state:\n\nDepiction of equal arm balances is found in the art of Ajanta cave (No. 17) in the Maharashtra state. Beams of steelyard balances have been unearthed from the 8th century CE archaeological sites at Sirpur and Arang. The research conducted by Abū Rayḥān Muḥammad ibn Aḥmad al-Bīrūn, an Islamic scholar who undertook one of the first studies of India's traditions in his \"Tahriq-e-Hind\", also reflect on the regular usage of the steelyard in India.\n\nThe Chinese merchant Ma Huan (1413–51) outlines the standardised weight and currency system in place at the port city of Cochin. Ma Huan noted that gold coins, known as \"fanam\", or locally known as \"panam\", were issued in Cochin and weighed a total of one \"fen\" and one \"li\" according to the Chinese standards. They were of fine quality and could be exchanged in China for 15 silver coins of four-\"li\" weight each.\n\nThe Mughal empire (1526–1857) undertook central agrarian reforms, under which statistical data was compiled by the local \"quanungo\" officials on instructions from then revenue minister Todar Mal. As a part of these reforms, Akbar the Great (1556–1605) enforced practical standardisation in the empire's weight and measure system. The Mughal measurement system measured land in terms of \"gaz\" and \"bigha\". The measure of agricultural output was the \"man\". Todar Mal's reforms were resisted by large land holders in India, following which the land of these \"zamindars\" was placed under the control of the Mughal treasury. Mughal surveying parties used standardised bamboo rods with iron joints to clearly record land according to the standard imperial land measures. These records were later used to collect land revenue corresponding to the land holdings.\n\nBritish units of measurement were adopted in India as first the East India Company and later colonial rule gained foothold. The Republic of India adopted the metric system on 1st october 1958. However, the traditional units still prevail in some areas. Chakrabarti (2007) holds that: 'Yet a few areas have still remained untouched by the metric system. In the land-measuring system in India, possibly one of the most complex and archaic systems, we follow different sets of measuring units and systems in different parts of the country. Different State governments have tried to standardise this by introducing a suitable metric system through which official transactions take place and official records are kept. But the land dealings are still done in a number of archaic units. It appears that people are satisfied and comfortable with them.'\n\n\n"}
{"id": "13110480", "url": "https://en.wikipedia.org/wiki?curid=13110480", "title": "Human spirit", "text": "Human spirit\n\nThe human spirit is a component of human philosophy, psychology, art, and knowledge - the spiritual or mental part of humanity. While the term can be used with the same meaning as \"human soul\", human spirit is sometimes used to refer to the impersonal, universal or higher component of human nature in contrast to soul or psyche which can refer to the ego or lower element. The human spirit includes our intellect, emotions, fears, passions, and creativity.\n\nIn the models of Daniel A. Helminiak and Bernard Lonergan, human spirit is considered to be the mental functions of awareness, insight, understanding, judgement and other reasoning powers. It is distinguished from the separate component of psyche which comprises the entities of emotion, images, memory and personality.\n\nJohn Teske views human spirit as a social construct representing the qualities of purpose and meaning which transcend the individual human.\n\nAccording to historian Oswald Spengler, a distinction between Spirit and Soul has been made by the West and earlier civilizations which influenced its development.\nThe human spirit can be seen as the heavenly component of human's non material makeup - the part that is impersonal or universal. Whereas souls are the personal element unique to each individual. As Spengler writes in \"The Decline of the West\":\n\nIn Christianity, the Bible identifies humanity's three basic elements: spirit, soul and body. Christians emphasise that the human spirit is the 'real person', the very core of a person's being, the essential seat of their existence. When a person accepts Jesus Christ as their Saviour, it is their human spirit that is transformed as they become 'new creatures' in Jesus Christ. The soul which is the seat of the will, mind and emotions does not get converted but needs to be renewed on a daily basis through the recommended Christian disciplines such as prayer and reading the Bible. In Islam, Muslims are viewed as having their own spirits, but one that in a sense is one with God's spirit. For Spengler, the perception of unity this idea led to was important for the emergence of the \"consensus\" that maintained harmony in Islamic culture, especially during the Golden Age of Islam.\n"}
{"id": "6834311", "url": "https://en.wikipedia.org/wiki?curid=6834311", "title": "Joseph Raya", "text": "Joseph Raya\n\nJoseph Raya (August 15, 1916 – June 10, 2005), born in Zahlé, Lebanon, was a prominent Melkite Greek Catholic archbishop, theologian, civil rights advocate and author. He served as metropolitan of Akko, Haifa, Nazareth and All Galilee from 1968 until 1974 and was particularly known for his commitment to seeking reconciliation between Christians, Jews and Muslims. He was also a leading advocate of celebrating the Divine Liturgy in vernacular languages.\n\nJoseph-Marie Raya was born to Almez and Mikhail Raya of Zahle and was the seventh of eight children. After finishing his elementary education at the Oriental College he studied in Paris before entering St. Anne's seminary in Jerusalem in 1937. He was ordained a priest of the Melkite Catholic Church on July 20, 1941. He later taught at the Patriarchal College on Queen Nazli Street in Cairo. Raya was expelled from Egypt in 1948 by King Farouk for defending the rights of women. He emigrated to the United States in 1949.\n\nAfter serving as assistant pastor of St. Ann's Melkite Catholic Church in Woodland Park, New Jersey, he was appointed pastor of St. George Melkite Greek Catholic Church in Birmingham, Alabama, in 1952. His championing civil rights brought him into close friendship with Rev. Martin Luther King, Jr. Raya marched several times at King's side and suffered three times at the hands of the Ku Klux Klan, including one occasion when he was kidnapped and severely beaten by three Klansmen. Defying the threat of excommunication issued by Roman Catholic Archbishop Thomas Toolen, Raya helped King and other civil rights demonstrators organize protests and marches throughout Alabama during the 1960s. Raya went on to found Saint Moses the Black Mission, the first Eastern Catholic mission for African Americans, located in downtown Birmingham.\n\nHe was also very close to social justice activist Catherine Doherty, and he became the first Associate Priest of her Madonna House Apostolate in Combermere, Ontario, Canada, on July 1, 1959. When he became Archbishop of Nazareth he ordained her husband Eddie Doherty to the priesthood.\n\nAs a priest in Alabama, Raya advocated for younger generations to have church services in their own languages, and translated the Gospels, Missal, and Byzantine Divine Liturgy into English. Raya created a controversy when he invited Bishop Fulton J. Sheen, the famous television Catholic personality, to celebrate the Pontifical Byzantine Divine Liturgy in English in 1958 at the Melkite National Convention. Bishop Sheen celebrated the Liturgy in English on television, inspiring some Catholics to renew calls for widespread use of the vernacular but raising the ire of traditionalists.\n\nThe Roman Catholic archbishop of Mobile, Alabama, Thomas Toolen, banned Raya from celebrating the Divine Liturgy in English in December 1959. However, Pope John XXIII intervened in March 1960 at the request of Melkite Patriarch Maximos IV Sayegh to decide the question in favor of the Byzantine custom of celebrating the Divine Mysteries in the vernacular. In 1963 Raya's liturgical translation was declared the official English translation for the Catholic Byzantine rites.\n\nPatriarch Maximos IV recognized Raya's successes by elevating him to the dignity of Grand Archimandrite of Jerusalem and appointing him a member of the Melkite patriarchal delegation to the Second Vatican Council. In a significant break with tradition, the church fathers of Vatican II decided to allow widespread use of vernacular in the Catholic Church. After completing his work at Vatican II Raya continued to translate Melkite works into English. In 1968, with Baron Jose de Vinck of Alleluia Press in New Jersey, he authored \"Byzantine Daily Worship\", a compendium in English of the Divine Liturgy, Office of the Hours, and the sacraments.\n\nFollowing his appointment as archbishop of Akko, Haifa, Nazareth and All Galilee on October 20, 1968, Archbishop Joseph led a peaceful demonstration of thousands of Arabs and Jews in Israel seeking justice for the villages of Kafr Bir'im and Iqrit in Upper Galilee that had been depopulated in 1948, and then destroyed. Iqrit was the hometown of his second successor, Archbishop Elias Chacour. He sought justice through non-violent means and called upon Palestinians to be good citizens of Israel.\n\nDescribing Raya's actions, Father John Catoir wrote in 1969:\n\nIn August, 1972 he ordered all churches in his eparchy closed one Sunday to mourn for \"the death of justice in Israel\" as the two villages remained dispossessed. Explaining his position, Raya said:\n\nAs archbishop, Raya was a controversial figure. While many admired his charismatic style and ecumenical leadership, some Arabs and members of the church hierarchy resented his overtures to Israel. Raya was opposed to the Melkite Holy Synod's proposal to internationalize Jerusalem. He also upset the Vatican with his aggressive campaign for the return of the Bir'im and Ikrit refugees and the sale of church land to impoverished Moslem farmers. Raya's letter of resignation declared that the Church hierarchy forced his decision to leave his post. The government of Israel considered him dangerous, but when he resigned Prime Minister Golda Meir begged him to reconsider.\n\nRaya's resignation came as a shock to many. The local Christian Youth Club collected several thousand signatures asking him to reconsider, and prominent Muslim, Jewish and Christian leaders in Israel and abroad voiced disappointment. Yoram Kaniuk described their thoughts on the archbishop, describing Raya as \"incomprehensible. The great majority of people viewed him with suspicion. Because he failed to conform to generally accepted notions ... Unusual people, deeply religious men, men of morals we won't understand. Bishop Raya was out of step. He bore Israel no animosity. He cared for and looked after his spiritual flock.\" Departing his post, Raya used his final pastoral letter to underscore his ecumenical approach:\n\nAfter resigning his archbishopric on July 13, 1974, Raya moved to Madonna House in Combermere, Ontario, Canada. At some time between his resignation and 1975 he suffered a massive heart attack and had a quadruple bypass operation in Lexington, Kentucky. From his home in Combermere he lectured and wrote on Byzantine spirituality at various places, among them Fordham University's John XXIII Ecumenical Center in The Bronx and the Patriarchal Major Seminary at Raboue in Antelias, Lebanon. He returned to Lebanon in 1985 to assist the Diocese of Beirut with Archbishop Habib Bacha. In 1987 he assumed interim leadership of the Archdiocese of Paneas in Marjayoun, Lebanon, which had been destroyed by the 1974 − 1991 Lebanese civil war. He moved back to Canada after the completion of this assignment and retired at Madonna House in 1990. Raya died on June 10, 2005, at St. Francis Memorial Hospital in Barry's Bay, Ontario, Canada.\n\nRaya authored several books, including hymnals, theological works, and monographs on church history. Most of his publications were written in English. They include:\n\n\n\n"}
{"id": "7569364", "url": "https://en.wikipedia.org/wiki?curid=7569364", "title": "Journaliste en danger", "text": "Journaliste en danger\n\nJournaliste en danger (JED) is an independent, non partisan non-profit organization () founded on November 20, 1998 in Kinshasa, Democratic Republic of Congo, on the initiative of a group of Congolese journalists for the defence and promotion of the press freedom in DR Congo. \n\nJED was founded out of the concern that press freedom was being violated and that journalists had become victims of unfair justice. JED is not an association reserved solely for journalists, but rather a wholly independent and open structure to all those who feel like having a vocation to defend and promote their right to inform and to be informed freely without any restriction. \n\nSince May 2003, JED has been active in eight other Central African countries: Burundi, Cameroon, Congo Brazzaville, Gabon, Equatorial Guinea, The Central African Republic, Rwanda and Chad.\n\nJED is a member of the International Freedom of Expression Exchange, a global network of non-governmental organisations that monitors freedom of expression worldwide and defends journalists, writers, internet users and others who are persecuted for exercising their right to freedom of expression.\n\n\n"}
{"id": "20627460", "url": "https://en.wikipedia.org/wiki?curid=20627460", "title": "Kind (type theory)", "text": "Kind (type theory)\n\nWhile a `type` is a description of a specific set of values, a `kind` is a descripton of a specific quantity of `types` as in 1, 2, 3, etc. Some `type` constructors, such as a `Monad`, require only one `type` to be able to fully construct a new `type` while others may require two or three `types` to completely construct/describe their `type`. Still, another `type` constuctor may return a `type` constructor that requires even more `types`. Note, that the function `type` constructor, `->`, has a `kind` of 2 and is shown as, `(* -> *)`, where `*` represents a `kind`. The idea of kinds generalizes the notion of generics.\n\nIn the area of mathematical logic and computer science known as type theory, a kind is the type of a type constructor or, less commonly, the type of a higher-order type operator. A kind system is essentially a simply typed lambda calculus \"one level up\", endowed with a primitive type, denoted formula_1 and called \"type\", which is the kind of any data type which does not need any type parameters.\n\nA kind is sometimes confusingly described as the \"type of a (data) type\", but it is actually more of an arity specifier. Syntactically, it is natural to consider polymorphic types to be type constructors, thus non-polymorphic types to be nullary type constructors. But all nullary constructors, thus all monomorphic types, have the same, simplest kind; namely formula_1.\n\nSince higher-order type operators are uncommon in programming languages, in most programming practice, kinds are used to distinguish between data types and the types of constructors which are used to implement parametric polymorphism. Kinds appear, either explicitly or implicitly, in languages whose type systems account for parametric polymorphism in a programatically accessible way, such as Haskell and Scala.\n\n\nHaskell's kind system has just two rules:\n\n\nAn inhabited type (as proper types are called in Haskell) is a type which has values. For instance, ignoring type classes which complicate the picture, codice_1 is a value of type codice_2, while codice_3 is a value of type codice_4 (list of Ints). Therefore, codice_2 and codice_4 have kind formula_1, but so does any function type, for instance codice_7 or even codice_8.\n\nA type constructor takes one or more type arguments, and produces a data type when enough arguments are supplied, i.e. it supports partial application thanks to currying. This is how Haskell achieves parametric types. For instance, the type codice_9 (list) is a type constructor - it takes a single argument to specify the type of the elements of the list. Hence, codice_4 (list of Ints), codice_11 (list of Floats) and even codice_12 (list of lists of Ints) are valid applications of the codice_9 type constructor. Therefore, codice_9 is a type of kind formula_4. Because codice_2 has kind formula_1, applying it to codice_9 results in codice_4, of kind formula_1. The 2-tuple constructor codice_18 has kind formula_5, the 3-tuple constructor codice_19 has kind formula_17 and so on.\n\nStandard Haskell does not allow polymorphic kinds. This is in contrast to parametric polymorphism on types, which is supported in Haskell. For instance, in the following example:\n\nthe kind of codice_20 could be anything, including formula_1, but also formula_4 etc. Haskell by default will always infer kinds to be formula_1, unless the type explicitly indicates otherwise (see below). Therefore the type checker will reject the following use of codice_21:\nbecause the kind of codice_9, formula_4 does not match the expected kind for codice_20, which is always formula_1.\n\nHigher-order type operators are allowed however. For instance:\n\nhas kind formula_23, i.e. codice_24 is expected to be a unary data constructor, which gets applied to its argument, which must be a type, and returns another type.\n\nGHC has the extension codice_25, which, together with codice_26, allows polymorphic kinds. For example:\nIn GHC 8.0.1, types and kinds can merged using the experimental compiler option codice_27.\n\n\n"}
{"id": "1312317", "url": "https://en.wikipedia.org/wiki?curid=1312317", "title": "Language policy in France", "text": "Language policy in France\n\nFrance has one official language, the French language. The French government does not regulate the choice of language in publications by individuals, but the use of French is required by law in commercial and workplace communications. In addition to mandating the use of French in the territory of the Republic, the French government tries to promote French in the European Union and globally through institutions such as La Francophonie. The perceived threat from anglicisation has prompted efforts to safeguard the position of the French language in France.\n\nBesides French, there exist many other vernacular minority languages of France, both in European France, in Overseas France, and in French overseas territories. These languages are recognized by article 75-1 of the French constitution. In France proper, Corsican, Breton, Gallo, Basque, Franco-Provençal, Occitan, and Catalan have an official status in the regions where they are spoken. The 1999 report written for the French government by Bernard Cerquiglini identified 75 languages (including just eight in continental France proper) that would qualify for recognition were the government to ratify the European Charter for Regional or Minority Languages (currently only signed but not ratified).\n\nThe Ordinance of Villers-Cotterêts of 1539 made French the administrative language of the kingdom of France for legal documents and laws. Previously, official documents were written in medieval Latin, which was the language used by the Roman Catholic Church.\n\nThe Académie française was established in 1635 to act as the official authority on the usages, vocabulary, and grammar of the French language, and to publish an official dictionary of the French language. Its recommendations however carry no legal power and are sometimes disregarded even by governmental authorities. In recent years the Académie has tried to prevent the Anglicisation of the French language.\n\nPrior to the French Revolution of 1789, French kings did not take a strong position on the language spoken by their subjects. However, in sweeping away the old provinces, parlements and laws, the Revolution strengthened the unified system of administration across the state. At first, the revolutionaries declared liberty of language for all citizens of the Republic; this policy was subsequently abandoned in favour of the imposition of a common language which was to do away with the other languages of France. Other languages were seen as keeping the peasant masses in obscurantism.\n\nThe new idea was expounded in the \"Report on the necessity and means to annihilate the patois and to universalise the use of the French language\". Its author, Henri Grégoire, deplored that France, the most advanced country in the world with regard to politics, had not progressed beyond the Tower of Babel as far as languages were concerned, and that only three million of the 25 million inhabitants of France spoke a pure Parisian French as their native tongue. The lack of ability of the population to understand the language in which were the political debates and the administrative documents was then seen as antidemocratic.\n\nThe report resulted the same year in two laws which stated that the only language tolerated in France in public life and in schools would be French. Within two years, the French language had become the symbol of the national unity of the French State. However, the Revolutionaries lacked both time and money to implement a language policy.\n\nIn the 1880s, the Third Republic sought to modernize France, and in particular to increase literacy and general knowledge in the population, especially the rural population, and established free compulsory primary education. The choice of French for education seemed natural, given that it was the only language widely spoken in France in which a sizable number of newspapers and historical, scientific etc. books were available. \n\nThe only language allowed in primary school was French. All other languages were forbidden, even in the schoolyard, and transgressions were severely punished. After 1918, the use of German in Alsace-Lorraine was outlawed. In 1925, Anatole de Monzie, Minister of public education, stated that \"for the linguistic unity of France, the Breton language must disappear.\" As a result, the speakers of minority languages began to be ashamed when using their own language – especially in the educational system – and over time, many families stopped teaching their language to their children and tried to speak only French with them. In neighbouring Belgium, a parallel policy to expand the use of standard French also took place.\n\nThe 1950s were also the first time the French state recognised the right of the regional languages to exist. A law allowed for the teaching of regional languages in secondary schools, and the policy of repression in the primary schools came to an end. The Breton language began to appear in the media during this time.\n\nThe French government allowed in 1964 for the first time one and a half minutes of Breton on regional television. But even in 1972, president Georges Pompidou declared that \"there is no place for minority languages in a France destined to make its mark on Europe.\"\n\nIn 1992 the constitution was amended to state explicitly that \"the language of the Republic is French.\"\n\nIn 2006 a French subsidiary of a US company was fined €500,000 plus an ongoing fine of €20,000 per day for providing software and related technical documentation to its employees in English only. See the Toubon Law.\n\nIn 2008, a revision of the French constitution creating official recognition of regional languages was implemented by the Parliament in Congress at Versailles.\n\nIn 1999 the Socialist government of Lionel Jospin signed the Council of Europe’s European Charter for Regional or Minority Languages, but it was not ratified. The Constitutional Council of France declared that the Charter contains unconstitutional provisions since the Constitution states that the language of the Republic is French.\n\nThe European Charter for Regional or Minority Languages is a European convention (ETS 148) adopted in 1992 under the auspices of the Council of Europe to protect and promote historical regional and minority languages in Europe, ratified and implemented by 25 States, but not by France, . The charter contains 98 articles of which signatories must adopt a minimum of 35 (France signed 39). The signing, and the failure to have it ratified, provoked a public debate in French society over the charter.\n\nMore recently, in a letter to several deputies dated 4 June 2015, François Hollande announced the upcoming filing of a constitutional bill for the ratification of the European Charter for Regional and Minority Languages. On the 30 July 2015, the Council of State gave an unfavorable opinion on the charter. On the 27 October 2015, the Senate rejected the draft law on ratification of the European Charter for Regional or Minority Languages driving away the assumption of Congress for the adoption of the constitutional reform which would have given the value and legitimacy to regional languages.\n\nExcluding the languages spoken in the overseas regions and other overseas territories, and the languages of recent immigrants, the following languages are spoken by sizeable minorities in France:\n\n\nThe non-French Oïl languages and Franco-Provençal are highly endangered; because of their similarity to standard French, their speakers conformed much more readily. The other languages are still spoken but are all considered endangered.\n\nIn the 1940s, more than one million people spoke Breton as their main language. The countryside in western Brittany was still overwhelmingly Breton-speaking. Today, about 170,000 people are able to speak Breton (around 8% of the population in the traditionally Breton speaking area), most of whom are elderly. Other regional languages have generally followed the same pattern; Alsatian and Corsican have resisted better, while Occitan has followed an even worse trend.\n\nAccurate information on the state of language use is complicated by the inability (due to constitutional provisions) of the state to ask language use questions in the census.\n\nSince the rejection of ratification of the European Charter, French governments have offered token support to regional languages within the limits of the law. The \"Délégation générale à la langue française\" (General delegation of the French tongue) has acquired the additional function of observing and studying the languages of France and has had \"et aux langues de France\" (...and languages of France) added to its title.\n\nThe French government hosted the first \"Assises nationales des langues de France\" in 2003, but this national round table on the languages of France served to highlight the contrast between cultural organisations and language activists on the one hand and the state on the other.\n\nThe decentralization has not extended to giving power in language policy to the regions.\n\nFrance presents itself as a big country struggling for cultural diversity against the predominance of English in international affairs. According to French republican ideology (see also Laïcité), all \"citizens\" are equal and therefore no \"groups\" may exercise extra rights; this is an idea stemming from the French Revolution, contrasting with the previous situation in which many groups had special rights and privileges.\n\nThis policy of cultural homogeneity has been challenged from both the right wing and the left wing. In the 1970s, nationalist or regionalist movements emerged in regions such as Brittany, Corsica and Occitania. Even though they remain a minority, networks of schools teaching France's regional languages have arisen, such as the Diwan in Brittany, the Ikastola in the Basque Country, the Calandreta in Occitania, and the La Bressola schools in Northern Catalonia.\n\nDespite popular demand for official recognition, regional language teaching is not supported by the state. However, in certain areas, such as Brittany, regional councils maintain bilingual public schools as far as it is within the law. Other Breton education is provided by Catholic schools and private schools, Dihun and Diwan, respectively. In 2011, only 14,000 pupils were enrolled in French-Breton bilingual schools, although this number reflected an increase of around 30% from the year 2006, when the number of pupils was just over 11,000. The Ofis Publik ar Brezhoneg also reported in 2011 that a further 16,000 students from early childhood to adulthood were learning Breton as a second language (at primary schools, collèges, lycées, university or evening courses), bringing the total number of Breton learners to at least 30,000.\n\nA long campaign of defacing road signs led to the first bilingual road signs in the 1980s. These are now increasingly common in Brittany, because of the help given by the Ofis ar Brezhoneg in bilingualizing many road, town hall and other official signs.\n\nAs far as the media are concerned, there is still little Breton to be found on the airwaves, although since 1982 a few Breton-speaking radio stations have been created on an associative basis. The launching of the Breton TV Breizh in 2000 was intended to offer wider coverage of Breton. However, Breton-language programme schedules gradually decreased in favour of French-language broadcasting, until in 2010 they totally disappeared.\n\nIn Corsica, the 1991 \"Joxe Statute\", in setting up the Collectivité Territoriale de Corse, also provided for the Corsican Assembly, and charged it with developing a plan for the optional teaching of Corsu. At the primary school level Corsu is taught up to a fixed number of hours per week (three in the year 2000) and is a voluntary subject at the secondary school level, but is required at the University of Corsica.\n\nThere is some opposition to the Loi Toubon mandating the use of French (or at least a translation into French) in commercial advertising and packaging, as well as in some other contexts.\n\n\n\n"}
{"id": "7570928", "url": "https://en.wikipedia.org/wiki?curid=7570928", "title": "Michael S. Harper", "text": "Michael S. Harper\n\nMichael Steven Harper (March 18, 1938 – May 7, 2016) was an American poet and English professor at Brown University, who was the Poet Laureate of Rhode Island from 1988 to 1993. His poetry was influenced by jazz and history.\n\nAmong the influences which shaped his writing, he said that the most important lesson he learned from musicians was phrasing, the authenticity of phrasing, and the transcendence and spiritual mastery. He published ten books of poetry, two of which—\"Dear John, Dear Coltrane\" (1970) and \"Images of Kin\" (1977)—were nominated for the National Book Award. Many of his poems have been included as examples of African-American literature and jazz poetry in various anthologies.\n\nHarper was born in Brooklyn as the first of three children into a lower-middle-class black family. His maternal grandfather, Roland Johnson, was a well-respected Canadian physician and was the delivery doctor for Harper at their home. He was also the primary influence in his early decision to pursue pre-med at Los Angeles City College.\n\nHis father Walter (who went by his middle name, Warren) was the originator of \"overnight\" mail and worked as a post office supervisor. His mother Katherine Louise, née Johnson was a medical secretary. Of his parents Harper once remarked, \"My parents did not have much money, but they had a great record collection.\" This would of course later influence his work, blending poetry with jazz.\n\nHis younger brother Jonathan Paul was born in 1941 and died in a motorcycle accident in 1977. His younger sister, Katherine Winifred, was born in 1943. They grew up in Bedford-Stuyvesant, a north-central portion of Brooklyn, until his family moved in 1951 to their homestead in Los Angeles, where he attended Dorsey High School. As a teenager, Harper was eager to get out of his father's house and into his own, marking his desire to work, create and learn on his own terms.\n\nIn 1955, he attended Los Angeles City College, initially enrolling in pre-med courses and later literature, graduating in 1959 with an associate of arts degree. At the Los Angeles State College of Applied Arts and Sciences (now California State University, Los Angeles, he earned a B.A. and an MA in English studies in 1961. While there he worked part-time in the post office and called this experience his \"real education\".\n\nHe joined the Iowa Writers' Workshop at the University of Iowa, where he largely resisted the movement of writing in syllabics, which he called \"incredibly mechanical\". During his time in Iowa City Harper said, \"Most of the things I learned…had nothing to do with the Workshop\" given the uprising of the Civil Rights Movement at that time. He earned an MFA in 1963.\n\nHe taught English at Contra Costa College in San Pablo, California and his poems appeared in small magazines. In 1968, he became poet in residence at Lewis & Clark College and taught at Reed College both in Portland, Oregon. Later that year, he entered a manuscript in the U.S. Poetry Prize Competition, submitted by Gwendolyn Brooks. Although he did not win, Brooks calls his work her “clear winner.” Harper said Brooks gave him his career. With Brooks behind him, his book was published by the University of Pittsburgh and reviewed in \"Time\" in 1970 with a cover essay by Ralph Ellison. In 1996 Brooks presented Harper with the George Kent Poetry Award for Honorable Amendments.\n\nIn 1970, he taught at California State College in Hayward, California. He joined the English faculty at Brown University, where he taught literature courses and poetry workshops to undergraduates. Students referred to him as \"MSH\", \"the Chief\", and \"the Big Man\". He remained there until retiring in December 2013 and was the longest serving professor of English and Literary Arts at that institution.\n\nIn 1993, Nathan A. Scott wrote in the \"Princeton Encyclopedia of Poetry and Poetics\" that \"Harper has created a body of work which, though it has won much respect and admiration, deserves to be far more widely known than it is.\"\n\nShortly after his retirement from Brown, former student George Makari remarked that Harper \"deeply entered our personal lives, challenged us to rethink who we thought we were, [and] asked us to leave behind childhood and enter a kind of creative crisis.\"\n\nHarper was married and had a daughter, Rachel, who is also a writer, and two sons, Roland and Patrice. He later divorced. He lived in Providence, Rhode Island until his death on May 7, 2016.\nHarper had two children who died at birth, which inspired several of his early poems, including the famous \"Nightmare Begins Responsibility\". .\n\nHarper wrote about important and historically influential African-Americans, including Jackie Robinson, Richard Wright and John Brown. He said in a 2000 interview with Terry Gross, that the most important thing he learned from musicians was phrasing, the authenticity of phrasing, and the transcendence and spiritual mastery.\n\nHarper often wrote about his wife, Shirley (commonly referred to as \"Shirl\"), their children, and their ancestors, as well as friends and various black historical and cultural figures. On poetry, Harper stated: \"A good poem is a true poem. Often it cannot be distilled into a slogan, or an easy thesis.\" This relates to much of his work, most notably \"Blue Ruth: America\" (1971) in which the nation is portrayed in a hospital bed. In a 2009 interview, he commented on the need for public rhetoric, noting that people had trouble with President Barack Obama because he spoke about consequential things when most were used to \"sound bites\".\nPoetry accompanied by jazz music:\n\n\n\n"}
{"id": "36395362", "url": "https://en.wikipedia.org/wiki?curid=36395362", "title": "Micronized rubber powder", "text": "Micronized rubber powder\n\nMicronized rubber powder (MRP) is classified as fine, dry, powdered elastomeric crumb rubber in which a significant proportion of particles are less than 100 µm and free of foreign particulates (metal, fiber, etc.). MRP particle size distributions typically range from 180 µm to 10 µm. Narrower distributions can be achieved depending on the classification technology.\n\nMRP is typically made from vulcanized elastomeric material, most often from end-of-life tire material, but can also be produced from post-industrial nitrile rubber, ethylene propylene diene monomer (EPDM), butyl and natural rubber compounds.\n\nMRP is a free flowing, black rubber powder that disperses into a multitude of systems and applications. Due to its micron size, MRP can be incorporated into multiple polymers, and provides a smooth surface appearance on finished products. In some cases, in order to improve compatibility with host materials, the MRP is given a chemical treatment to activate, or “make functional” the surface of the powder particles. This is referred to as functionalized MRP or FMRP.\n\nMRP represents an evolution over previous post-manufactured rubber technologies. The most basic rubber processing technology converts end-of-life tire and post-industrial rubber material into rubber chips that are typically one inch or larger in size. These chips are then used in tire-derived fuel and civil engineering projects. A second-generation processing technology converts end-of-life tire and rubber material into crumb rubber, also known as ground tire rubber (GTR). GTR typically comprises chips between one inch and 30 mesh in size, with the associated fiber and steel mostly removed. This material is used in asphalt, as garden mulch and in playgrounds. \n\nMRP is a micron-size material that is produced in various sizes, including 80 mesh and down to 300 mesh. MRP is virtually metal and fiber-free, enabling its use in a wide range of advanced products. \n\nMRP is used as a compound extender to offset the use of natural rubber and synthetic polymers as well as act as a process aid in material production. In some cases, MRP can reduce formulation costs, because it replaces commodity-priced rubber- and oil-based feedstocks, According to some estimates, MRP offers up to 50 percent cost savings over virgin raw materials. \n\nMRP also can improve the sustainability, and in some cases the performance, of the compounds in which it is used. For example, the smaller particle sizes of MRP are known to increase the impact strength of certain plastic compositions. However, in all applications the particle size and loading levels depend on the target application.\n\nDue to its size and composition, MRP can be incorporated into more advanced and higher-value applications than crumb rubber. Industries incorporating MRP into their products include tire, automotive, construction, industrial components and consumer products. It is also used as an additive in tires, plastics, asphalt, coatings, and sealants. MRP can also be incorporated into prime or recycled grade polypropylene (PP), high-density polyethylene (HDPE) and nylons. Additionally, the incorporation of MRPs in thermoplastic elastomers (TPE) and thermoplastic vulcanizates (TPV) makes it a feasible ingredient for automotive and building and construction applications.\n\nCurrently, the leading producer of MRP is Lehigh Technologies, which utilizes a cryogenic turbo mill process with more than 100 million pounds of annual production capacity. MRP produced by Lehigh has set high benchmarks for performance in a range of applications with customers and third-party research institutions, including several studies on increased asphalt performance. Lehigh claims more than 250 million tires on the road today have been made using its MRP.\n\nThere is an applicable American Society for Testing and Materials (ASTM) specification [ASTM D5603-01 (2008)] for the classification of rubber powder, including MRP.\n\nNumerous U.S. and European studies have found crumb rubber and MRP meet standards for human health and safety. Recently, an EPA study found that crumb rubber in field turfs and playgrounds contained concentrations of materials below harmful levels. \n"}
{"id": "2509041", "url": "https://en.wikipedia.org/wiki?curid=2509041", "title": "Mimizuka", "text": "Mimizuka\n\nThe , an alteration of the original is a monument in Kyoto, Japan, dedicated to the sliced noses of killed Korean soldiers and civilians as well as Ming Chinese troops taken as war trophies during the Japanese invasions of Korea from 1592 to 1598. The monument enshrines the severed noses of at least 38,000 Koreans killed during Toyotomi Hideyoshi's invasions.\nThe shrine is located just to the west of Toyokuni Shrine, the Shinto shrine honoring Hideyoshi in Kyoto.\n\nTraditionally, Japanese warriors brought back the heads of enemies slain on the battlefield as proof of their deeds. Nose collection in lieu of heads became a feature of the second Korean invasion. Remuneration was paid to soldiers by their \"daimyō\" commanders based on the severed heads upon submission to collection stations, where inspectors meticulously counted, recorded, salted and packed the noses bound for Japan. However, because of the number of civilians killed along with soldiers, and crowded conditions on the ships that transported troops, it was far easier to just bring back noses instead of whole heads. Hideyoshi was especially insistent upon receiving noses of people his samurai had killed as proof that his men really were killing people in Korea.\n\nJapanese chroniclers on the second invading campaign mention that the ears hacked off the faces of the massacred were also of ordinary civilians mostly in the provinces Gyeongsang, Jeolla, and Chungcheong. In the second invasion Hideyoshi's orders were thus:\n\nOne hundred and sixty-thousand Japanese troops had gone to Korea where they had taken 185,738 Korean heads and 29,014 Chinese ones, a grand total of 214,752. As some might have been discarded, it is improbable to enumerate how many were killed in total during the war. \nThe Mimizuka was dedicated September 28, 1597. Though the exact reasons as to its construction are not entirely known, scholars contend that during the second Japanese invasion of Korea in 1597, Toyotomi Hideyoshi demanded his commanders show receipts of their martial valor in the destruction, dispatching congratulatory letters to his high-ranking warriors in the field as evidence of their service. Hideyoshi then ordered the relics entombed in a shrine on the grounds of Hokoji Temple, and set Buddhist priests to work praying for the repose of the souls of the hundreds of thousands of Koreans from whose bodies they had come; an act that chief priest Saishō Jōtai (1548–1608) in a fit of toadyism would hail as a sign of Hideyoshi's \"great mercy and compassion.\" The shrine initially was known as , Mound of Noses, but several decades later this would come to be regarded as too cruel-sounding a name, and would be changed to the more euphonious but inaccurate , Mound of Ears, the misnomer by which it is known to this day. Other nose tombs dating from the same period are found elsewhere in Japan, such as at Okayama.\n\nThe Mimizuka is almost unknown to the Japanese public unlike to the Koreans. The British historian Stephen Turnbull called the Mimizuka \"...Kyoto's least mentioned and most often avoided tourist attraction\". A plaque, which was later removed, stood in front of the Ear Mound in the 1960s with the passage, \"One cannot say that cutting off noses was so atrocious by the standard of the time.\" Most guidebooks do not mention the Ear Mound, and only a few Japanese or foreign tourists visit the site. The majority of visiting tourists are Korean – Korean tour buses are often seen parked near the Ear Mound.\n\nIn 1982, not a single Japanese school textbook mentioned the Ear Mound. As of 1997, the mound is referred to in about half of all high-school history textbooks according to Shigeo Shimoyama, an official of Jikkyo, a publishing company. The publisher released the first Japanese text book mentioning the Ear Mound in the mid-1980s. The Education Ministry of Japan at that time opposed the description as \"too vivid\" and pressured the publisher to reduce the tone and also to praise Hideyoshi for religiously dedicating the Ear Mound to store the spirits of the killed people.\n\nIn the 1970s under the Park Chung-hee administration, some of the officials of the South Korean government asked Japan to level the monument. However, most Koreans said that the mound should stay in Japan as a reminder of past savagery.\nActivity since the 1990s is aptly conveyed thus:\nOn September 28, 1997, the 400th anniversary of the Mimizuka, a ceremony was held in respect for those killed, which people of all nationalities and faiths attended. The current caretaker of Mimizuka as of August 2009 is Shimizu Shirou (清水四郎).\n\n\n"}
{"id": "89344", "url": "https://en.wikipedia.org/wiki?curid=89344", "title": "Nemesis (hypothetical star)", "text": "Nemesis (hypothetical star)\n\nNemesis is a hypothetical red dwarf or brown dwarf, originally postulated in 1984 to be orbiting the Sun at a distance of about 95,000 AU (1.5 light-years), somewhat beyond the Oort cloud, to explain a perceived cycle of mass extinctions in the geological record, which seem to occur more often at intervals of 26 million years. , more than 1800 brown dwarfs have been identified. There are actually fewer brown dwarfs in our cosmic neighborhood than previously thought. Rather than one star for every brown dwarf, there may be as many as six stars for every brown dwarf. The majority of solar-type stars are single. The previous idea stated half or perhaps most stellar systems were binary, trinary, or multiple-star systems associated with clusters of stars, rather than the single-star systems that tend to be seen most often. In a 2017 paper, Sarah Sadavoy and Steven Stahler argued that the Sun was likely part of a binary system at the time of its formation, leading them to suggest \"there probably was a Nemesis, a long time ago.” Such a star would have separated from this binary system over four billion years ago, meaning it could not be responsible for the more recent perceived cycle of mass extinctions, Douglas Vakoch told \"Business Insider\", adding that \"If the sun really was part of a binary star system in its early days, its early twin deserves a benign name like Companion, rather than the threatening Nemesis.\" \n\nMore recent theories suggest that other forces, like close passage of other stars, or the angular effect of the galactic gravity plane working against the outer solar orbital plane, may be the cause of orbital perturbations of some outer Solar System objects. In 2011, Coryn Bailer-Jones analyzed craters on the surface of the Earth and reached the conclusion that the earlier findings of simple periodic patterns (implying periodic comet showers dislodged by a hypothetical Nemesis star) were statistical artifacts, and found that the crater record shows no evidence for Nemesis. However, in 2010, A.L. Melott and R.K. Bambach found evidence in the fossil record confirming the extinction event periodicity originally claimed by Raup & Sepkoski in 1984, but at a higher confidence level and over a time period nearly twice as long. The Infrared Astronomical Satellite (IRAS) failed to discover Nemesis in the 1980s. The 2MASS astronomical survey, which ran from 1997 to 2001, failed to detect an additional star or brown dwarf in the Solar System.\n\nUsing newer and more powerful infrared telescope technology which is able to detect brown dwarfs as cool as 150 kelvins out to a distance of 10 light-years from the Sun, the Wide-field Infrared Survey Explorer (WISE survey) has not detected Nemesis. In 2011, David Morrison, a senior scientist at NASA known for his work in risk assessment of near Earth objects, has written that there is no confidence in the existence of an object like Nemesis, since it should have been detected in infrared sky surveys.\n\nIn 1984, paleontologists David Raup and Jack Sepkoski published a paper claiming that they had identified a statistical periodicity in extinction rates over the last 250 million years using various forms of time series analysis. They focused on the extinction intensity of fossil families of marine vertebrates, invertebrates, and protozoans, identifying 12 extinction events over the time period in question. The average time interval between extinction events was determined as 26 million years. At the time, two of the identified extinction events (Cretaceous–Paleogene and Eocene–Oligocene) could be shown to coincide with large impact events. Although Raup and Sepkoski could not identify the cause of their supposed periodicity, they suggested a possible non-terrestrial connection. The challenge to propose a mechanism was quickly addressed by several teams of astronomers.\n\nIn 2010, Melott & Bambach re-examined the fossil data, including the now-improved dating, and using a second independent database in addition to that Raup & Sepkoski had used. They found evidence for a signal showing an excess extinction rate with a 27-million-year periodicity, now going back 500 million years, and at a much higher statistical significance than in the older work. They also determined that this periodicity is inconsistent with the Nemesis hypothesis. The change from 26 to 27 million years is expected based on a 3% \"stretch\" in the geological timescale since the 1980s.\n\nTwo teams of astronomers, Daniel P. Whitmire and Albert A. Jackson IV, and Marc Davis, Piet Hut, and Richard A. Muller, independently published similar hypotheses to explain Raup and Sepkoski's extinction periodicity in the same issue of the journal \"Nature\". This hypothesis proposes that the Sun may have an undetected companion star in a highly elliptical orbit that periodically disturbs comets in the Oort cloud, causing a large increase of the number of comets visiting the inner Solar System with a consequential increase of impact events on Earth. This became known as the \"Nemesis\" or \"Death Star\" hypothesis.\n\nIf it does exist, the exact nature of Nemesis is uncertain. Muller suggests that the most likely object is a red dwarf with an apparent magnitude between 7 and 12, while Daniel P. Whitmire and Albert A. Jackson argue for a brown dwarf. If a red dwarf, it would exist in star catalogs, but it would only be confirmed by measuring its parallax; due to orbiting the Sun it would have a low proper motion and would escape detection by older proper motion surveys that have found stars like the 9th-magnitude Barnard's star. (The proper motion of Barnard's star was detected in 1916.) Muller expects Nemesis to be discovered by the time parallax surveys reach the 10th magnitude.\n\nMuller, referring to the date of a recent extinction at 11 million years before the present day, posits that Nemesis has a semi-major axis of about and suggests it is located (supported by Yarris, 1987) near Hydra, based on a hypothetical orbit derived from original aphelions of a number of atypical long-period comets that describe an orbital arc meeting the specifications of Muller's hypothesis. Richard Muller's most recent paper relevant to the Nemesis theory was published in 2002. In 2002, Muller speculated that Nemesis was perturbed 400 million years ago by a passing star from a circular orbit into an orbit with an eccentricity of 0.7.\n\nThe trans-Neptunian object Sedna has an extra-long and unusual elliptical orbit around the Sun, ranging between 76 and 937 AU. Sedna's orbit is estimated to last between 10.5 and 12 thousand years. Its discoverer, Michael Brown of Caltech, noted in a \"Discover\" magazine article that Sedna's location seemed to defy reasoning: \"Sedna shouldn't be there\", Brown said. \"There's no way to put Sedna where it is. It never comes close enough to be affected by the Sun, but it never goes far enough away from the Sun to be affected by other stars.\" Brown therefore postulated that a massive unseen object may be responsible for Sedna's anomalous orbit. This line of inquiry eventually led to the hypothesis of Planet Nine.\n\nBrown has stated that it is more likely that one or more non-companion stars, passing near the Sun billions of years ago, could have pulled Sedna out into its current orbit. In 2004, Kenyon forwarded this explanation after analysis of Sedna's orbital data and computer modeling of possible ancient non-companion star passes.\n\nSearches for Nemesis in the infrared are important because cooler stars comparatively shine more brightly in infrared light. The University of California's Leuschner Observatory failed to discover Nemesis by 1986. The Infrared Astronomical Satellite (IRAS) failed to discover Nemesis in the 1980s. The 2MASS astronomical survey, which ran from 1997 to 2001, failed to detect a star, or brown dwarf, in the Solar System. If Nemesis exists, it may be detected by Pan-STARRS or the planned LSST astronomical surveys.\n\nIn particular, if Nemesis is a red dwarf or a brown dwarf, the WISE mission (an infrared sky survey that covered most of our solar neighborhood in movement-verifying parallax measurements) was expected to be able to find it. WISE can detect 150-kelvin brown dwarfs out to 10 light-years. But the closer a brown dwarf is, the easier it is to detect. Preliminary results of the WISE survey were released on April 14, 2011. On March 14, 2012, the entire catalog of the WISE mission was released. In 2014 WISE data ruled out a Saturn or larger-sized body in the Oort cloud out to ten thousand AU.\n\nCalculations in the 1980s suggested that a Nemesis object would have an irregular orbit due to perturbations from the galaxy and passing stars. The Melott & Bambach work shows an extremely regular signal, inconsistent with the expected irregularities in such an orbit. Thus, while supporting the extinction periodicity, it appears to be inconsistent with the Nemesis hypothesis, though of course not inconsistent with other kinds of substellar objects. According to a 2011 NASA news release, \"recent scientific analysis no longer supports the idea that extinctions on Earth happen at regular, repeating intervals, and thus, the Nemesis hypothesis is no longer needed.\"\n\n\n\n"}
{"id": "21690", "url": "https://en.wikipedia.org/wiki?curid=21690", "title": "Number", "text": "Number\n\nA number is a mathematical object used to count, measure and also label. The original examples are the natural numbers 1, 2, 3, 4 and so forth. A notational symbol that represents a number is called a numeral. In addition to their use in counting and measuring, numerals are often used for labels (as with telephone numbers), for ordering (as with serial numbers), and for codes (as with ISBNs). In common usage, \"number\" may refer to a symbol, a word, or a mathematical abstraction.\n\nIn mathematics, the notion of number has been extended over the centuries to include 0, negative numbers, rational numbers such as and , real numbers such as and , and complex numbers, which extend the real numbers by adding a square root of. Calculations with numbers are done with arithmetical operations, the most familiar being addition, subtraction, multiplication, division, and exponentiation. Their study or usage is called arithmetic. The same term may also refer to number theory, the study of the properties of numbers.\n\nBesides their practical uses, numbers have cultural significance throughout the world. For example, in Western society, the number 13 is regarded as unlucky, and \"a million\" may signify \"a lot.\" Though it is now regarded as pseudoscience, numerology, the belief in a mystical significance of numbers, permeated ancient and medieval thought. Numerology heavily influenced the development of Greek mathematics, stimulating the investigation of many problems in number theory which are still of interest today.\n\nDuring the 19th century, mathematicians began to develop many different abstractions which share certain properties of numbers and may be seen as extending the concept. Among the first were the hypercomplex numbers, which consist of various extensions or modifications of the complex number system. Today, number systems are considered important special examples of much more general categories such as rings and fields, and the application of the term \"number\" is a matter of convention, without fundamental significance.\n\nNumbers should be distinguished from numerals, the symbols used to represent numbers. The Egyptians invented the first ciphered numeral system, and the Greeks followed by mapping their counting numbers onto Ionian and Doric alphabets. Roman numerals, a system that used combinations of letters from the Roman alphabet, remained dominant in Europe until the spread of the superior Arabic numeral system around the late 14th century, and the Arabic numeral system remains the most common system for representing numbers in the world today. The key to the effectiveness of the system was the symbol for zero, which was developed by ancient Indian mathematicians around 500 AD.\n\nNumbers can be classified into sets, called number systems, such as the natural numbers and the real numbers. The major categories of numbers are as follows:\n\nThere is generally no problem in identifying each number system with a proper subset of the next one (by abuse of notation), because each of these number systems is canonically isomorphic to a proper subset of the next one. The resulting hierarchy allows, for example, to talk, formally correctly, about real numbers that are rational numbers, and is expressed symbolically by writing\n\nThe most familiar numbers are the natural numbers (sometimes called whole numbers or counting numbers): 1, 2, 3, and so on. Traditionally, the sequence of natural numbers started with 1 (0 was not even considered a number for the Ancient Greeks.) However, in the 19th century, set theorists and other mathematicians started including 0 (cardinality of the empty set, i.e. 0 elements, where 0 is thus the smallest cardinal number) in the set of natural numbers. Today, different mathematicians use the term to describe both sets, including 0 or not. The mathematical symbol for the set of all natural numbers is N, also written formula_2, and sometimes formula_3 or formula_4 when it is necessary to indicate whether the set should start with 0 or 1, respectively.\n\nIn the base 10 numeral system, in almost universal use today for mathematical operations, the symbols for natural numbers are written using ten digits: 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9. The radix or base is the number of unique numerical digits, including zero, that a numeral system uses to represent numbers (for the decimal system, the radix is 10). In this base 10 system, the rightmost digit of a natural number has a place value of 1, and every other digit has a place value ten times that of the place value of the digit to its right.\n\nIn set theory, which is capable of acting as an axiomatic foundation for modern mathematics, natural numbers can be represented by classes of equivalent sets. For instance, the number 3 can be represented as the class of all sets that have exactly three elements. Alternatively, in Peano Arithmetic, the number 3 is represented as sss0, where s is the \"successor\" function (i.e., 3 is the third successor of 0). Many different representations are possible; all that is needed to formally represent 3 is to inscribe a certain symbol or pattern of symbols three times.\n\nThe negative of a positive integer is defined as a number that produces 0 when it is added to the corresponding positive integer. Negative numbers are usually written with a negative sign (a minus sign). As an example, the negative of 7 is written −7, and . When the set of negative numbers is combined with the set of natural numbers (including 0), the result is defined as the set of integers, Z also written formula_5. Here the letter Z comes . The set of integers forms a ring with the operations addition and multiplication.\n\nThe natural numbers form a subset of the integers. As there is no common standard for the inclusion or not of zero in the natural numbers, the natural numbers without zero are commonly referred to as positive integers, and the natural numbers with zero are referred to as non-negative integers.\n\nA rational number is a number that can be expressed as a fraction with an integer numerator and a positive integer denominator. Negative denominators are allowed, but are commonly avoided, as every rational number is equal to a fraction with positive denominator. Fractions are written as two integers, the numerator and the denominator, with a dividing bar between them. The fraction represents \"m\" parts of a whole divided into \"n\" equal parts. Two different fractions may correspond to the same rational number; for example and are equal, that is:\n\nIn general,\n\nIf the absolute value of \"m\" is greater than \"n\" (supposed to be positive), then the absolute value of the fraction is greater than 1. Fractions can be greater than, less than, or equal to 1 and can also be positive, negative, or 0. The set of all rational numbers includes the integers, since every integer can be written as a fraction with denominator 1. For example −7 can be written . The symbol for the rational numbers is Q (for \"quotient\"), also written formula_9.\n\nThe symbol for the real numbers is R, also written as formula_10 They include all the measuring numbers. Every real number corresponds to a point on the number line. The following paragraph will focus primarily on positive real numbers. The treatment of negative real numbers is according to the general rules of arithmetic and their denotation is simply prefixing the corresponding positive numeral by a minus sign, e.g. -123.456.\n\nMost real numbers can only be \"approximated\" by decimal numerals, in which a decimal point is placed to the right of the digit with place value 1. Each digit to the right of the decimal point has a place value one-tenth of the place value of the digit to its left. For example, 123.456 represents , or, in words, one hundred, two tens, three ones, four tenths, five hundredths, and six thousandths. A real number can be expressed by a finite number of decimal digits only if it is rational and its fractional part has a denominator whose prime factors are 2 or 5 or both, because these are the prime factors of 10, the base of the decimal system. Thus, for example, one half is 0.5, one fifth is 0.2, one tenth is 0.1, and one fiftieth is 0.02. Representing other real numbers as decimals would require an infinite sequence of digits to the right of the decimal point. If this infinite sequence of digits follows a pattern, it can be written with an ellipsis, or another notation that indicates the repeating pattern. Such a decimal is called a repeating decimal. Thus can be written as 0.333..., with an ellipsis to indicate that the pattern continues. Forever repeating 3s are also written as 0..\n\nIt turns out that these repeating decimals (including the repetition of zeroes) denote exactly the rational numbers, i.e., all rational numbers are also real numbers, but it is not the case that every real number is rational. A real number that is not rational is called irrational. A famous irrational real number is the number , the ratio of the circumference of any circle to its diameter. When pi is written as\nas it sometimes is, the ellipsis does not mean that the decimals repeat (they do not), but rather that there is no end to them. It has been proved that is irrational. Another well known number, proven to be an irrational real number, is\nthe square root of 2, that is, the unique positive real number whose square is 2. Both these numbers have been approximated (by computer) to trillions of digits.\n\nNot only these prominent examples, but almost all real numbers are irrational and therefore have no repeating patterns and hence no corresponding decimal numeral. They can only be approximated by decimal numerals, denoting rounded or truncated real numbers. Any rounded or truncated number is necessarily a rational number, of which there are only countably many. All measurements are, by their nature, approximations, and always have a margin of error. Thus 123.456 is considered an approximation of any real number greater or equal to and strictly less than (rounding to 3 decimals), or of any real number greater or equal to and strictly less than (truncation after the 3. decimal). Digits that suggest a greater accuracy than the measurement itself does, should be removed. The remaining digits are then called significant digits. For example, measurements with a ruler can seldom be made without a margin of error of at least 0.001 meters. If the sides of a rectangle are measured as 1.23 meters and 4.56 meters, then multiplication gives an area for the rectangle between and . Since not even the second digit after the decimal place is preserved, the following digits are not \"significant\". Therefore the result is usually rounded to 5.61.\n\nJust as the same fraction can be written in more than one way, the same real number may have more than one decimal representation. For example, 0.999..., 1.0, 1.00, 1.000, ..., all represent the natural number 1. A given real number has only the following decimal representations: an approximation to some finite number of decimal places, an approximation in which a pattern is established that continues for an unlimited number of decimal places, or an exact value with only finitely many decimal places. In this last case, the last non-zero digit may be replaced by the digit one smaller followed by an unlimited number of 9's, or the last non-zero digit may be followed by an unlimited number of zeros. Thus the exact real number 3.74 can also be written 3.7399999999... and 3.74000000000... . Similarly, a decimal numeral with an unlimited number of 0's can be rewritten by dropping the 0's to the right of the decimal place, and a decimal numeral with an unlimited number of 9's can be rewritten by increasing the rightmost non-9 digit by one, changing all the 9's to the right of that digit to 0's. Finally, an unlimited sequence of 0's to the right of the decimal place can be dropped. For example, 6.849999999999... = 6.85 and 6.850000000000... = 6.85. Finally, if all of the digits in a numeral are 0, the number is 0, and if all of the digits in a numeral are an unending string of 9's, you can drop the nines to the right of the decimal place, and add one to the string of 9s to the left of the decimal place. For example 99.999... = 100. \n\nThe real numbers also have an important but highly technical property called the least upper bound property.\n\nIt can be shown that any ordered field, which is also complete, is isomorphic to the real numbers. The real numbers are not, however, an algebraically closed field, because they do not include a solution (often called a square root of minus one) to the algebraic equation formula_13.\n\nMoving to a greater level of abstraction, the real numbers can be extended to the complex numbers. This set of numbers arose historically from trying to find closed formulas for the roots of cubic and quadratic polynomials. This led to expressions involving the square roots of negative numbers, and eventually to the definition of a new number: a square root of −1, denoted by \"i\", a symbol assigned by Leonhard Euler, and called the imaginary unit. The complex numbers consist of all numbers of the form\nwhere \"a\" and \"b\" are real numbers. Because of this, complex numbers correspond to points on the complex plane, a vector space of two real dimensions. In the expression , the real number \"a\" is called the real part and \"b\" is called the imaginary part. If the real part of a complex number is 0, then the number is called an imaginary number or is referred to as \"purely imaginary\"; if the imaginary part is 0, then the number is a real number. Thus the real numbers are a subset of the complex numbers. If the real and imaginary parts of a complex number are both integers, then the number is called a Gaussian integer. The symbol for the complex numbers is C or formula_15.\n\nThe fundamental theorem of algebra asserts that the complex numbers form an algebraically closed field, meaning that every polynomial with complex coefficients has a root in the complex numbers. Like the reals, the complex numbers form a field, which is complete, but unlike the real numbers, it is not ordered. That is, there is no consistent meaning assignable to saying that \"i\" is greater than 1, nor is there any meaning in saying that \"i\" is less than 1. In technical terms, the complex numbers lack of a total order that is compatible with field operations.\n\nAn even number is an integer that is \"evenly divisible\" by two, that is divisible by two without remainder; an odd number is an integer that is not even. (The old-fashioned term \"evenly divisible\" is now almost always shortened to \"divisible\".) Any odd number \"n\" may be constructed by the formula for a suitable integer \"k\". Starting with the first non-negative odd numbers are {1, 3, 5, 7, ...}. Any even number \"m\" has the form where \"k\" is again an integer. Similarly, the first non-negative even numbers are {0, 2, 4, 6, ...}.\n\nA prime number is an integer greater than 1 that is not the product of two smaller positive integers. The first few prime numbers are 2, 3, 5, 7, and 11. There is no such simple formula as for odd and even numbers to generate the prime numbers. The primes have been widely studied for more than 2000 years and have led to many questions, only some of which have been answered. The study of these questions belong to number theory. An example of a still unanswered question is, whether every even number is the sum of two primes. This is called Goldbach's conjecture.\n\nThe question, whether every integer greater than one is a product of primes in only one way, except for a rearrangement of the primes, has been answered to the positive: this proven claim is called fundamental theorem of arithmetic. A proof appears in Euclid's Elements.\n\nMany subsets of the natural numbers have been the subject of specific studies and have been named, often after the first mathematician that has studied them. Example of such sets of integers are Fibonacci numbers and perfect numbers. For more examples, see Integer sequence.\n\nAlgebraic numbers are those that are a solution to a polynomial equation with integer coefficients. Real numbers that are not rational numbers are called irrational numbers. Complex numbers which are not algebraic are called transcendental numbers. The algebraic numbers that are solutions of a monic polynomial equation with integer coefficients are called algebraic integers.\n\nA computable number, also known as \"recursive number\", is a real number such that there exists an algorithm which, given a positive number \"n\" as input, produces the first \"n\" digits of the computable number's decimal representation. Equivalent definitions can be given using μ-recursive functions, Turing machines or λ-calculus. The computable numbers are stable for all usual arithmetic operations, including the computation of the roots of a polynomial, and thus form a real closed field that contains the real algebraic numbers.\n\nThe computable numbers may be viewed as the real numbers that may be exactly represented in a computer: a computable number is exactly represented by its first digits and a program for computing further digits. However, the computable numbers are rarely used in practice. One reason is that there is no algorithm for testing the equality of two computable numbers. More precisely, there cannot exist any algorithm which takes any computable number as an input, and decides in every case if this number is equal to zero or not.\n\nThe set of computable numbers has the same cardinality as the natural numbers. Therefore, almost all real numbers are non-computable. However, it is very difficult to produce explicitly a real number that is not computable.\n\nThe \"p\"-adic numbers may have infinitely long expansions to the left of the decimal point, in the same way that real numbers may have infinitely long expansions to the right. The number system that results depends on what base is used for the digits: any base is possible, but a prime number base provides the best mathematical properties. The set of the \"p\"-adic numbers contains the rational numbers, but is not contained in the complex numbers.\n\nThe elements of an algebraic function field over a finite field and algebraic numbers have many similar properties (see Function field analogy). Therefore, they are often regarded as numbers by number theorists. The \"p\"-adic numbers play an important role in this analogy.\n\nSome number systems that are not included in the complex numbers may be constructed from the real numbers in a way that generalize the construction of the complex numbers. They are sometimes called hypercomplex numbers. They include the quaternions H, introduced by Sir William Rowan Hamilton, in which multiplication is not commutative, the octonions, in which multiplication is not associative in addition to not being commutative, and the sedenions, in which multiplication is not alternative, neither associative nor commutative.\n\nFor dealing with infinite sets, the natural numbers have been generalized to the ordinal numbers and to the cardinal numbers. The former gives the ordering of the set, while the latter gives its size. For finite sets, both ordinal and cardinal numbers are identified with the natural numbers. In the infinite case, many ordinal numbers correspond to the same cardinal number.\n\nHyperreal numbers are used in non-standard analysis. The hyperreals, or nonstandard reals (usually denoted as *R), denote an ordered field that is a proper extension of the ordered field of real numbers R and satisfies the transfer principle. This principle allows true first-order statements about R to be reinterpreted as true first-order statements about *R.\n\nSuperreal and surreal numbers extend the real numbers by adding infinitesimally small numbers and infinitely large numbers, but still form fields.\n\nA relation number is defined as the class of relations consisting of all those relations that are similar to one member of the class.\n\nBones and other artifacts have been discovered with marks cut into them that many believe are tally marks. These tally marks may have been used for counting elapsed time, such as numbers of days, lunar cycles or keeping records of quantities, such as of animals.\n\nA tallying system has no concept of place value (as in modern decimal notation), which limits its representation of large numbers. Nonetheless tallying systems are considered the first kind of abstract numeral system.\n\nThe first known system with place value was the Mesopotamian base 60 system (ca. 3400 BC) and the earliest known base 10 system dates to 3100 BC in Egypt.\n\nThe first known documented use of zero dates to AD 628, and appeared in the \"Brāhmasphuṭasiddhānta\", the main work of the Indian mathematician Brahmagupta. He treated 0 as a number and discussed operations involving it, including division. By this time (the 7th century) the concept had clearly reached Cambodia as Khmer numerals, and documentation shows the idea later spreading to China and the Islamic world.\n\nBrahmagupta's Brahmasphuṭasiddhanta is the first book that mentions zero as a number, hence Brahmagupta is usually considered the first to formulate the concept of zero. He gave rules of using zero with negative and positive numbers, such as 'Zero plus a positive number is the positive number, and a negative number plus zero is the negative number'. The Brahmasphutasiddhanta is the earliest known text to treat zero as a number in its own right, rather than as simply a placeholder digit in representing another number as was done by the Babylonians or as a symbol for a lack of quantity as was done by Ptolemy and the Romans.\n\nThe use of 0 as a number should be distinguished from its use as a placeholder numeral in place-value systems. Many ancient texts used 0. Babylonian and Egyptian texts used it. Egyptians used the word \"nfr\" to denote zero balance in double entry accounting. Indian texts used a Sanskrit word or to refer to the concept of \"void\". In mathematics texts this word often refers to the number zero. In a similar vein, Pāṇini (5th century BC) used the null (zero) operator in the Ashtadhyayi, an early example of an algebraic grammar for the Sanskrit language (also see Pingala).\n\nThere are other uses of zero before Brahmagupta, though the documentation is not as complete as it is in the Brahmasphutasiddhanta.\n\nRecords show that the Ancient Greeks seemed unsure about the status of 0 as a number: they asked themselves \"how can 'nothing' be something?\" leading to interesting philosophical and, by the Medieval period, religious arguments about the nature and existence of 0 and the vacuum. The paradoxes of Zeno of Elea depend in part on the uncertain interpretation of 0. (The ancient Greeks even questioned whether  was a number.)\n\nThe late Olmec people of south-central Mexico began to use a symbol for zero, a shell glyph, in the New World, possibly by the but certainly by 40 BC, which became an integral part of Maya numerals and the Maya calendar. Mayan arithmetic used base 4 and base 5 written as base 20. Sanchez in 1961 reported a base 4, base 5 \"finger\" abacus.\n\nBy 130 AD, Ptolemy, influenced by Hipparchus and the Babylonians, was using a symbol for 0 (a small circle with a long overbar) within a sexagesimal numeral system otherwise using alphabetic Greek numerals. Because it was used alone, not as just a placeholder, this Hellenistic zero was the first \"documented\" use of a true zero in the Old World. In later Byzantine manuscripts of his \"Syntaxis Mathematica\" (\"Almagest\"), the Hellenistic zero had morphed into the Greek letter omicron (otherwise meaning 70).\n\nAnother true zero was used in tables alongside Roman numerals by 525 (first known use by Dionysius Exiguus), but as a word, meaning \"nothing\", not as a symbol. When division produced 0 as a remainder, , also meaning \"nothing\", was used. These medieval zeros were used by all future medieval computists (calculators of Easter). An isolated use of their initial, N, was used in a table of Roman numerals by Bede or a colleague about 725, a true zero symbol.\n\nThe abstract concept of negative numbers was recognized as early as 100–50 BC in China. \"The Nine Chapters on the Mathematical Art\" contains methods for finding the areas of figures; red rods were used to denote positive coefficients, black for negative. The first reference in a Western work was in the 3rd century AD in Greece. Diophantus referred to the equation equivalent to (the solution is negative) in \"Arithmetica\", saying that the equation gave an absurd result.\n\nDuring the 600s, negative numbers were in use in India to represent debts. Diophantus' previous reference was discussed more explicitly by Indian mathematician Brahmagupta, in \"Brāhmasphuṭasiddhānta\" 628, who used negative numbers to produce the general form quadratic formula that remains in use today. However, in the 12th century in India, Bhaskara gives negative roots for quadratic equations but says the negative value \"is in this case not to be taken, for it is inadequate; people do not approve of negative roots.\"\n\nEuropean mathematicians, for the most part, resisted the concept of negative numbers until the 17th century, although Fibonacci allowed negative solutions in financial problems where they could be interpreted as debts (chapter 13 of \"Liber Abaci\", 1202) and later as losses (in ). At the same time, the Chinese were indicating negative numbers by drawing a diagonal stroke through the right-most non-zero digit of the corresponding positive number's numeral. The first use of negative numbers in a European work was by Nicolas Chuquet during the 15th century. He used them as exponents, but referred to them as \"absurd numbers\".\n\nAs recently as the 18th century, it was common practice to ignore any negative results returned by equations on the assumption that they were meaningless, just as René Descartes did with negative solutions in a Cartesian coordinate system.\n\nIt is likely that the concept of fractional numbers dates to prehistoric times. The Ancient Egyptians used their Egyptian fraction notation for rational numbers in mathematical texts such as the Rhind Mathematical Papyrus and the Kahun Papyrus. Classical Greek and Indian mathematicians made studies of the theory of rational numbers, as part of the general study of number theory. The best known of these is Euclid's \"Elements\", dating to roughly 300 BC. Of the Indian texts, the most relevant is the Sthananga Sutra, which also covers number theory as part of a general study of mathematics.\n\nThe concept of decimal fractions is closely linked with decimal place-value notation; the two seem to have developed in tandem. For example, it is common for the Jain math sutra to include calculations of decimal-fraction approximations to pi or the square root of 2. Similarly, Babylonian math texts had always used sexagesimal (base 60) fractions with great frequency.\n\nThe earliest known use of irrational numbers was in the Indian Sulba Sutras composed between 800 and 500 BC. The first existence proofs of irrational numbers is usually attributed to Pythagoras, more specifically to the Pythagorean Hippasus of Metapontum, who produced a (most likely geometrical) proof of the irrationality of the square root of 2. The story goes that Hippasus discovered irrational numbers when trying to represent the square root of 2 as a fraction. However Pythagoras believed in the absoluteness of numbers, and could not accept the existence of irrational numbers. He could not disprove their existence through logic, but he could not accept irrational numbers, and so, allegedly and frequently reported, he sentenced Hippasus to death by drowning, to impede spreading of this disconcerting news.\n\nThe 16th century brought final European acceptance of negative integral and fractional numbers. By the 17th century, mathematicians generally used decimal fractions with modern notation. It was not, however, until the 19th century that mathematicians separated irrationals into algebraic and transcendental parts, and once more undertook scientific study of irrationals. It had remained almost dormant since Euclid. In 1872, the publication of the theories of Karl Weierstrass (by his pupil E. Kossak), Eduard Heine (\"Crelle,\" 74), Georg Cantor (Annalen, 5), and Richard Dedekind was brought about. In 1869, Charles Méray had taken the same point of departure as Heine, but the theory is generally referred to the year 1872. Weierstrass's method was completely set forth by Salvatore Pincherle (1880), and Dedekind's has received additional prominence through the author's later work (1888) and endorsement by Paul Tannery (1894). Weierstrass, Cantor, and Heine base their theories on infinite series, while Dedekind founds his on the idea of a cut (Schnitt) in the system of real numbers, separating all rational numbers into two groups having certain characteristic properties. The subject has received later contributions at the hands of Weierstrass, Kronecker (\"Crelle\", 101), and Méray.\n\nThe search for roots of quintic and higher degree equations was an important development, the Abel–Ruffini theorem (Ruffini 1799, Abel 1824) showed that they could not be solved by radicals (formulas involving only arithmetical operations and roots). Hence it was necessary to consider the wider set of algebraic numbers (all solutions to polynomial equations). Galois (1832) linked polynomial equations to group theory giving rise to the field of Galois theory.\n\nContinued fractions, closely related to irrational numbers (and due to Cataldi, 1613), received attention at the hands of Euler, and at the opening of the 19th century were brought into prominence through the writings of Joseph Louis Lagrange. Other noteworthy contributions have been made by Druckenmüller (1837), Kunze (1857), Lemke (1870), and Günther (1872). Ramus (1855) first connected the subject with determinants, resulting, with the subsequent contributions of Heine, Möbius, and Günther, in the theory of Kettenbruchdeterminanten.\n\nThe existence of transcendental numbers was first established by Liouville (1844, 1851). Hermite proved in 1873 that \"e\" is transcendental and Lindemann proved in 1882 that π is transcendental. Finally, Cantor showed that the set of all real numbers is uncountably infinite but the set of all algebraic numbers is countably infinite, so there is an uncountably infinite number of transcendental numbers.\n\nThe earliest known conception of mathematical infinity appears in the Yajur Veda, an ancient Indian script, which at one point states, \"If you remove a part from infinity or add a part to infinity, still what remains is infinity.\" Infinity was a popular topic of philosophical study among the Jain mathematicians c. 400 BC. They distinguished between five types of infinity: infinite in one and two directions, infinite in area, infinite everywhere, and infinite perpetually.\n\nAristotle defined the traditional Western notion of mathematical infinity. He distinguished between actual infinity and potential infinity—the general consensus being that only the latter had true value. Galileo Galilei's \"Two New Sciences\" discussed the idea of one-to-one correspondences between infinite sets. But the next major advance in the theory was made by Georg Cantor; in 1895 he published a book about his new set theory, introducing, among other things, transfinite numbers and formulating the continuum hypothesis.\n\nIn the 1960s, Abraham Robinson showed how infinitely large and infinitesimal numbers can be rigorously defined and used to develop the field of nonstandard analysis. The system of hyperreal numbers represents a rigorous method of treating the ideas about infinite and infinitesimal numbers that had been used casually by mathematicians, scientists, and engineers ever since the invention of infinitesimal calculus by Newton and Leibniz.\n\nA modern geometrical version of infinity is given by projective geometry, which introduces \"ideal points at infinity\", one for each spatial direction. Each family of parallel lines in a given direction is postulated to converge to the corresponding ideal point. This is closely related to the idea of vanishing points in perspective drawing.\n\nThe earliest fleeting reference to square roots of negative numbers occurred in the work of the mathematician and inventor Heron of Alexandria in the , when he considered the volume of an impossible frustum of a pyramid. They became more prominent when in the 16th century closed formulas for the roots of third and fourth degree polynomials were discovered by Italian mathematicians such as Niccolò Fontana Tartaglia and Gerolamo Cardano. It was soon realized that these formulas, even if one was only interested in real solutions, sometimes required the manipulation of square roots of negative numbers.\n\nThis was doubly unsettling since they did not even consider negative numbers to be on firm ground at the time. When René Descartes coined the term \"imaginary\" for these quantities in 1637, he intended it as derogatory. (See imaginary number for a discussion of the \"reality\" of complex numbers.) A further source of confusion was that the equation\nseemed capriciously inconsistent with the algebraic identity\nwhich is valid for positive real numbers \"a\" and \"b\", and was also used in complex number calculations with one of \"a\", \"b\" positive and the other negative. The incorrect use of this identity, and the related identity\nin the case when both \"a\" and \"b\" are negative even bedeviled Euler. This difficulty eventually led him to the convention of using the special symbol \"i\" in place of formula_19 to guard against this mistake.\n\nThe 18th century saw the work of Abraham de Moivre and Leonhard Euler. De Moivre's formula (1730) states:\nwhile Euler's formula of complex analysis (1748) gave us:\n\nThe existence of complex numbers was not completely accepted until Caspar Wessel described the geometrical interpretation in 1799. Carl Friedrich Gauss rediscovered and popularized it several years later, and as a result the theory of complex numbers received a notable expansion. The idea of the graphic representation of complex numbers had appeared, however, as early as 1685, in Wallis's \"De Algebra tractatus\".\n\nAlso in 1799, Gauss provided the first generally accepted proof of the fundamental theorem of algebra, showing that every polynomial over the complex numbers has a full set of solutions in that realm. The general acceptance of the theory of complex numbers is due to the labors of Augustin Louis Cauchy and Niels Henrik Abel, and especially the latter, who was the first to boldly use complex numbers with a success that is well known.\n\nGauss studied complex numbers of the form , where \"a\" and \"b\" are integral, or rational (and \"i\" is one of the two roots of ). His student, Gotthold Eisenstein, studied the type , where \"ω\" is a complex root of Other such classes (called cyclotomic fields) of complex numbers derive from the roots of unity for higher values of \"k\". This generalization is largely due to Ernst Kummer, who also invented ideal numbers, which were expressed as geometrical entities by Felix Klein in 1893.\n\nIn 1850 Victor Alexandre Puiseux took the key step of distinguishing between poles and branch points, and introduced the concept of essential singular points. This eventually led to the concept of the extended complex plane.\n\nPrime numbers have been studied throughout recorded history. Euclid devoted one book of the \"Elements\" to the theory of primes; in it he proved the infinitude of the primes and the fundamental theorem of arithmetic, and presented the Euclidean algorithm for finding the greatest common divisor of two numbers.\n\nIn 240 BC, Eratosthenes used the Sieve of Eratosthenes to quickly isolate prime numbers. But most further development of the theory of primes in Europe dates to the Renaissance and later eras.\n\nIn 1796, Adrien-Marie Legendre conjectured the prime number theorem, describing the asymptotic distribution of primes. Other results concerning the distribution of the primes include Euler's proof that the sum of the reciprocals of the primes diverges, and the Goldbach conjecture, which claims that any sufficiently large even number is the sum of two primes. Yet another conjecture related to the distribution of prime numbers is the Riemann hypothesis, formulated by Bernhard Riemann in 1859. The prime number theorem was finally proved by Jacques Hadamard and Charles de la Vallée-Poussin in 1896. Goldbach and Riemann's conjectures remain unproven and unrefuted.\n\n\n\n"}
{"id": "42361", "url": "https://en.wikipedia.org/wiki?curid=42361", "title": "On the Waterfront", "text": "On the Waterfront\n\nOn the Waterfront is a 1954 American crime drama film directed by Elia Kazan, and written by Budd Schulberg. It stars Marlon Brando, and features Karl Malden, Lee J. Cobb, Rod Steiger, Pat Henning and Eva Marie Saint in her film debut. The soundtrack score was composed by Leonard Bernstein. The film was suggested by \"Crime on the Waterfront\" by Malcolm Johnson, a series of articles published in November–December 1948 in the \"New York Sun\" which won the 1949 Pulitzer Prize for Local Reporting, but the screenplay by Budd Schulberg is directly based on his own original story. The film focuses on union violence and corruption amongst longshoremen, while detailing widespread corruption, extortion, and racketeering on the waterfronts of Hoboken, New Jersey.\n\n\"On the Waterfront\" was a critical and commercial success, and received twelve Academy Award nominations, winning eight, including Best Picture, Best Actor for Brando, Best Supporting Actress for Saint, and Best Director for Kazan. In 1997, it was ranked by the American Film Institute as the eighth-greatest American movie of all time, and in AFI's 2007 list, it was ranked 19th. It is Bernstein's only original film score not adapted from a stage production with songs.\n\nIn 1989, \"On the Waterfront\" was deemed \"culturally, historically, or aesthetically significant\" by the Library of Congress, and selected for preservation in the United States National Film Registry.\n\nMob-connected union boss Johnny Friendly (Lee J. Cobb) gloats about his iron-fisted control of the waterfront. The police and the Waterfront Crime Commission know that Friendly is behind a number of murders, but witnesses play \"D and D\" (\"deaf and dumb\"), accepting their subservient position, rather than risking the danger and shame of informing.\n\nTerry Malloy (Marlon Brando) is a dockworker whose brother Charley \"the Gent\" (Rod Steiger) is Friendly's right-hand man. Some years earlier, Terry had been a promising boxer, until Friendly had Charley instruct him to deliberately lose a fight that he could have won, so that Friendly could win money betting against him. Terry is used to coax Joey Doyle (Ben Wagner), a popular dockworker, into an ambush, preventing Joey from testifying against Friendly before the Crime Commission. Terry assumed that Friendly's enforcers were only going to \"lean\" on Joey to pressure him into silence, and is surprised when Joey is killed.\n\nJoey's sister Edie (Eva Marie Saint), angry about her brother's death, shames \"waterfront priest\" Father Barry (Karl Malden) into fomenting action against the mob-controlled union. Friendly sends Terry to attend and inform on a dockworkers' meeting Father Barry holds in the church, which is broken up by Friendly's men. Terry helps Edie escape the violence, and is smitten with her. Another dockworker, Timothy J. \"Kayo\" Dugan (Pat Henning), who agrees to testify after Father Barry promises unwavering support, ends up dead after Friendly arranges for him to be crushed by a load of whiskey in a staged accident.\n\nAlthough Terry resents being used as a tool in Joey's death, and despite Father Barry's impassioned \"sermon on the docks\" reminding the longshoremen that Christ walks among them and that every murder is a Calvary, Terry is at first willing to remain \"D and D\", even when subpoenaed to testify. However, when Edie, unaware of Terry's role in her brother's death, begins to return Terry's feelings, Terry is tormented by his awakening conscience and confesses the circumstances of Joey's death to Father Barry and Edie. Horrified, Edie breaks up with him.\n\nAs Terry increasingly leans toward testifying, Friendly decides that Terry must be killed unless Charley can coerce him into keeping quiet. Charley tries bribing Terry with a good job and finally threatens Terry by holding a gun against him, but recognizes that he has failed to sway Terry, who blames his own downward spiral on his well-off brother. In what has become an iconic scene, Terry reminds Charley that had it not been for the fixed fight, Terry's prizefighting career would have bloomed. \"I coulda' been somebody. I coulda' been a contender\", laments Terry to his brother, \"Instead of a bum, which is what I am – let's face it.\" Charley gives Terry the gun, and advises him to run. Terry flees to Edie's apartment, where she first refuses to let him in, but finally admits her love for him. Friendly, having had Charley watched, has Charley murdered and his body hung in an alley as bait to lure Terry out to his death, but Terry and Edie both escape the attempt on Terry's life.\n\nAfter finding Charley's body, Terry sets out to shoot Friendly, but Father Barry prevents it by blocking Terry's line of fire and convincing Terry to fight Friendly by testifying instead. Terry proceeds to give damaging testimony implicating Friendly in Joey's murder and other illegal activities, causing Friendly's mob boss to cut him off and Friendly to face indictment.\n\nAfter the testimony, Friendly announces that Terry will not find employment anywhere on the waterfront. Terry is shunned by his former friends and by a neighborhood boy who had previously looked up to him. Refusing Edie's suggestion that they move away from the waterfront together, Terry shows up during recruitment at the docks. When he is the only man not hired, Terry openly confronts Friendly, calling him out and proclaiming that he is proud of what he did. The confrontation develops into a vicious brawl, with Terry getting the upper hand until Friendly's thugs gang up on Terry and nearly beat him to death. The dockworkers, who witness the confrontation, show their support for Terry by refusing to work, unless Terry is working, too, and pushing Friendly into the river. Encouraged by Father Barry and Edie, the badly injured Terry forces himself to his feet and enters the dock, followed by the other workers. A soaking wet and face-scarred Friendly, now left with nothing, swears revenge on them all, but his threats fall on deaf ears as they enter the garage, and the door closes behind them.\n\n\nThe film is widely considered to be Elia Kazan's answer to those who criticized him for identifying eight (former) Communists in the film industry before the House Committee on Un-American Activities (HUAC) in 1952. One of Kazan's critics was his friend and collaborator, the noted playwright Arthur Miller, who had earlier written the first version of the script, originally entitled \"The Hook\". Kazan had agreed to direct it, and in 1951 they met with Harry Cohn at Columbia Pictures about making the picture. Cohn agreed in principle to make \"The Hook\", but there were concerns about the portrayal of corrupt union officials. When Cohn asked the antagonists be changed to Communists, Miller refused. Cohn sent a letter telling Miller it was interesting he had resisted Columbia's desire to make the movie \"pro-American\". Kazan asked Miller to rewrite the script; Miller declined due to his disenchantment with Kazan's friendly testimony before the HUAC. Kazan then replaced Miller with Budd Schulberg.\n\nAfter rewriting the script, Schulberg and Kazan approached Darryl F. Zanuck, who eventually told them he did not like a single thing about the script, asking, \"Who's going to care about a bunch of sweaty longshoremen?\" This led Kazan and Schulberg to meet with independent producer Sam Spiegel, who set up a deal with Columbia. Spiegel was insistent on Schulberg delivering a perfect screenplay, and barraged the writer with changes and suggestions, to the frustration of Schulberg.\n\nSchulberg's script nonetheless went through a number of changes before reaching the screen. In an early draft, the Terry Malloy character was not an ex-pug dockworker, but a cynical investigative reporter, as well as an older, divorced man.\n\nTerry Malloy's fight against corruption was in part modeled after whistle-blowing longshoreman Anthony DeVincenzo, who testified before a real-life Waterfront Commission about activities on the Hoboken Docks and suffered a degree of ostracism for his deed. DeVincenzo sued and settled, many years after, with Columbia Pictures over the appropriation of what he considered his story. DeVincenzo claimed to have recounted his story to screenwriter Budd Schulberg during a month-long session of waterfront barroom meetings. Schulberg attended DeVincenzo's waterfront commission testimony every day during the hearing.\n\nThe character of Father Barry was based on the real-life \"waterfront priest\" Father John M. Corridan, a Jesuit priest and graduate of Regis High School who operated a Roman Catholic labor school on the west side of Manhattan. Father Corridan was interviewed extensively by Schulberg, who also wrote the foreword to a biography of Father Corridan, \"Waterfront Priest\" by Allen Raymond.\n\nThe character of Johnny Friendly was partially based on International Longshoremen's Association boss Michael Clemente. Friendly also had aspects of former Murder, Inc. head Albert Anastasia, who was a top enforcer for the crime family that ran the Hoboken docks, the Lucianolater Genovesefamily. In 1979, Clemente and other members of the Genovese family were indicted for corruption and racketeering on the New York waterfront.\n\nAccording to Richard Schickel in his biography of Kazan, Marlon Brando initially refused the role of Terry Malloy, and Frank Sinatra (a native of Hoboken, where the film was being made) then had \"a handshake deal\"but no formally signed contractto play the part, even attending an initial costume fitting. But Kazan still favored Brando for the role, partly because casting Brando would assure a larger budget for the picture. While Brando's agent, Jay Kanter, attempted to persuade Brando to change his mind, Kazan enlisted actor Karl Malden, whom Kazan considered more suited to a career as a director than as an actor, to direct and film a screen test of a \"more Brando-like\" actor as Terry Malloy, in an effort to persuade Spiegel that \"an actor like Marlon Brando\" could perform the role more forcefully than Sinatra. To that end, Malden filmed a screen test of Actors Studio members Paul Newman and Joanne Woodward performing the love scene between Terry and Edie. Persuaded by the Newman/Woodward screen test, Spiegel agreed to reconsider Brando for the role, and shortly afterwards, Kanter convinced Brando to reconsider his refusal. Within a week, Brando signed a contract to perform in the film. At that point, a furious Sinatra demanded to be cast in the role of Father Barry, the waterfront priest. It was left to Spiegel to break the news to Sinatra that Malden had already been signed for that role.\n\nThe part of Edie Doyle was offered to Grace Kelly, who turned it down, preferring to make \"Rear Window\" instead. Kazan said in his autobiography \"A Life\" that the choice of an actress to play Edie Doyle was narrowed down to Elizabeth Montgomery and Eva Marie Saint. There was something well-bred about Montgomery that Kazan thought would not be becoming for Edie, who was raised on the waterfront in Hoboken, New Jersey. He gave the part to Saint.\n\nThe role of Terry's older brother Charley was originally offered to Lawrence Tierney, who asked for too much money, so the role went to Rod Steiger. Despite playing Terry's older brother, Steiger was one year younger than Brando.\n\nSeveral of the labor boss' men in the film, including Abe Simon as Barney, Tony Galento as Truck, and Tami Mauriello as Tillio, were former professional heavyweight boxers in real life.\n\n\"On the Waterfront\" was filmed over 36 days on location in various places in Hoboken, New Jersey, including the docks, workers' slum dwellings, bars, littered alleys, and rooftops. The church used for exterior scenes in the film was the historic Our Lady of Grace, built in 1874, while the interiors were shot at the Church of St. Peter and St. Paul at 400 Hudson Street.\n\nUpon its release, the film received positive reviews from critics, and was a commercial success, earning an estimated $4.2 million at the North American box office in 1954. In his July 29, 1954, review, \"New York Times\" critic A. H. Weiler called the film \"an uncommonly powerful, exciting, and imaginative use of the screen by gifted professionals\".\n\nOn Rotten Tomatoes, the film has a critical score of 98% with an average rating of 9.2/10 and a critical consensus of \"With his electrifying performance in Elia Kazan's thought-provoking, expertly constructed melodrama, Marlon Brando redefined the possibilities of acting for film and helped permanently alter the cinematic landscape\".\n\nAcademy Awards\n\n\"On the Waterfront\" received twelve Academy Awards nominations in ten categories, and won in eight of the categories.\nAmerican Film Institute recognition\n\nLegacy\n\nIn 1989, the film was deemed \"culturally, historically, or aesthetically significant\" by the Library of Congress, and selected for preservation in the United States National Film Registry. It is also on the Vatican's list of 45 greatest films, compiled in 1995.\n\n The first home video release of the film was by Columbia Pictures Home Entertainment in 1982, on VHS and Beta. RCA/Columbia Pictures Home Video later re-released it in 1984, 1986, and 1990, respectively, the latter being a part of the Columbia Classics line-up. Columbia TriStar later reissued the film on VHS in 1995 as part of the line-up's \"Studio Heritage Collection\", and the first DVD version was released in 2001. Among the special features is the featurette \"Contender: Mastering the Method\", a video photo gallery, an interview with Elia Kazan, an audio commentary, filmographies, production notes, and theatrical trailers. The film has been added to the Criterion Collection. \n\nThe 2013 Criterion Collection release presents the film in three aspect ratios: 1.66:1, 1.85:1, and 1.33:1. The accompanying booklet explains the reasoning behind this choice: \"In 1953, Columbia Pictures was transitioning to the new widescreen format and declared that all its upcoming films, including \"On The Waterfront\", would be suitable for projection in any aspect ratio from the full frame of 1.33:1 to the then widest standard of 1.85:1. The customary frame of European cinematographer Boris Kaufman (\"Twelve Angry Men\", \"Baby Doll\") split the difference at 1.66:1, so that all that was required was for him to leave extra room at the top and bottom of the frame and make sure that nothing essential would be lost in the widescreen presentation. At its premiere in 1954, \"On The Waterfront\" was projected at 1.85:1. Over subsequent decades, millions of television viewers became accustomed to seeing the film with the open-matte 1.33:1 framing, a presentation that has carried over into the home video era. Here, for the first time, Criterion is presenting the film in all three aspect ratios so that viewers can compare and choose the version they prefer.\"\n\n\"Kabzaa\" (1988) and \"Ghulam\" (1998) – Indian movies inspired by \"On the Waterfront\"\n\nNotes\nBibliography\n\nFurther reading\n\n"}
{"id": "886000", "url": "https://en.wikipedia.org/wiki?curid=886000", "title": "Open-question argument", "text": "Open-question argument\n\nThe open-question argument is a philosophical argument put forward by British philosopher G. E. Moore in §13 of \"Principia Ethica\" (1903), to refute the equating of the property of goodness with some non-moral property, X, whether naturalistic (e.g. pleasure) or supernatural (e.g. God's command). That is, Moore's argument attempts to show that no moral property is identical to a natural property. The argument takes the form of syllogistic modus tollens:\n\nThe type of question Moore refers to in this argument is an identity question, \"Is it true that X is \"Y\"?\" Such a question is an \"open question\" if a conceptually competent speaker can question this; otherwise the question is \"closed.\" For example, \"I know he is a vegan, but does he eat meat?\" would be a closed question. However, \"I know that it is pleasurable, but is it good?\" is an open question; the question cannot be deduced from the conceptual terms alone.\n\nThe open-question argument claims that any attempt to identify morality with some set of observable, natural properties will always be an open question (unlike, say, a horse, which can be defined in terms of observable properties). Moore further argued that if this is true, then moral facts cannot be reduced to natural properties and that therefore ethical naturalism is false. Put another way, what Moore is saying is that any attempt to define good in terms of a naturalistic property fails because all definitions can be transformed into closed questions (the subject and predicate being conceptually identical; it is given in language itself that the two terms mean the same thing); however, all purported naturalistic definitions of good are transformable into open questions. It is still controversial whether good is the same thing as pleasure, etc. Shortly before (in section §11), Moore said if good is defined as pleasure (or any other naturalistic property) \"good\" may be substituted for \"pleasure\" anywhere it occurs. However, \"pleasure is good\" is a meaningful, informative statement; but \"good is good\" (after making the substitution) is an empty, non-informative tautology.\n\nThe idea that Moore begs the question (i.e. assumes the conclusion in a premise) was first raised by W. Frankena. Since analytic equivalency, for two objects X and Y, logically results in the question \"Is it true that X is Y?\" being meaningless (by Moore's own argument), to say that the question is meaningless is to concede analytic equivalency. Thus Moore begs the question in the second premise. He assumes that the question is a meaningful one (i.e. that it is an open question). This begs the question and the open-question argument thus fails.\n\nIn response to this, the open-question argument can be reformulated. The Darwall-Gibbard-Railton reformulation argues for the impossibility of equating a moral property with a non-moral one using the internalist theory of motivation.\nThis evidently presupposes the internalist theory of motivation (i.e. a belief can itself motivate), in contrast to the externalist theory of motivation, also known as the Humean theory of motivation (i.e. both a belief and a desire are required to motivate). If internalism is true, then the OQA avoids begging the question against the naturalist, and succeeds in showing that the good cannot be equated to some other property.\n\nThe argument is also contested on the grounds of the supremacy of internalism. Internalism is supported by the belief–desire–intention model of motivation, whereby desire (i.e. that some proposition ought to be made or kept true) and belief (i.e. that some proposition is true) combine to form intention, and thereby, action. To argue for the special motivational effects of moral beliefs is to commit the fallacy of special pleading.\n\nThe main assumption within the open-question argument can be found within premise 1. It is assumed that analytic equivalency will result in meaningless analysis. Thus, if we understand Concept C, and Concept C* can be analysed in terms of Concept C, then we should grasp concept C* by virtue of our understanding of Concept C. Yet it is obvious that such understanding of Concept C* only comes about through the analysis proper. Mathematics would be the prime example: mathematics is tautological and its claims are true by definition, yet we can develop new mathematical conceptions and theorems. Thus, X (i.e. some non-moral property) might well be analytically equivalent to the good, and still the question of \"Is \"X\" good?\" can be meaningful. Ergo premise 1 does not hold and the argument falls.\n\nSense and reference are two different aspects of some terms' meanings. A term's \"reference\" is the \"object\" to which the term refers, while the term's \"sense\" is the \"way\" that the term refers to that object. \n\nThere is a difference between the sense of a term and its reference (i.e. the object itself). Thus, we can understand a claim like \"goodness is identical with pleasure\" as an \"a posteriori\" identity claim similar to \"Water is HO\". The question \"This is HO but is it water?\" is intelligible and so, in that limited sense, whether or not water is HO is an open question; note that this does not address the issue of significance. But that does not lead us to conclude that water is not HO. \"Water is HO\" is an identity claim that is known to be true \"a posteriori\" (i.e., it was discovered via empirical investigation). Another example is \"redness\" being identical to certain phenomena of electromagnetism. This is discovered by empirical investigation. Similarly, many moral naturalists argue that \"rightness\" can be discovered as an \"a posteriori\" truth, by investigating the different claims, like that of pleasure being the good, or of duty being the good.\n\nThis is done by invoking rightness and wrongness to explain certain empirical phenomena, and then discovering \"a posteriori\" whether maximizing utility occupies the relevant explanatory role. For example, they argue that since right actions contingently have certain effects e.g. being causally responsible for a tendency towards social stability—so it follows we can fix the term \"right\" refer to the empirical description \"the property of acts, whatever it is, that is causally responsible for their tendency towards social stability.\" With this description for \"right,\" we can then investigate which acts accomplish this: e.g. those actions that maximize utility. We can then conclude that we have learned that \"right\" refers to \"maximizing utility\" through \"a posteriori\" means.\n\nThe Frege sense–reference distinction can be understood in layman's terms by using the analogy of the Masked Man. A citizen living on the frontiers of the Wild West is told by the sheriff that his brother is the Masked Man who has recently been robbing banks. The citizen protests that he understands who his brother is, and who the Masked Man is supposed to be, and can meaningfully ask, \"Is my brother the Masked Man?\" Obviously, analytic equivalency is of no relevance here. The matter is an empirical one, which the citizen must investigate a posteriori. The absurdity of dismissing the claim as such is apparent.\n\nHowever, the above account of a sort of \"a posteriori\" moral search is unsatisfactory in that normal value, and not moral value, can be used to explain the relevant events. Normal value arises from the relationship between desire and a state of affairs. People tend also to objectify such value, into categorical moral value, though this is fallacious. So, a situation that can be explained by the existence of real moral value (e.g. the fulfillment of preferences, the tendency towards social stability) can also be explained by non-moral value. This explanation is far simpler, given the ontological difficulties surrounding moral value. As J. L. Mackie argued with his argument from queerness, moral values (i.e. oughts) that exist in the natural world (of facts), is highly queer, and we ought to favour a completely naturalistic explanation instead.\n\nAnother problem with the \"a posteriori\" moral search is that we lack an epistemic account of how we access moral facts. This is the epistemic aspect of Mackie's argument from queerness. Failing such an account, the postulation of moral value will be egregious.\n"}
{"id": "33606303", "url": "https://en.wikipedia.org/wiki?curid=33606303", "title": "Otium", "text": "Otium\n\nOtium, a Latin abstract term, has a variety of meanings, including leisure time in which a person can enjoy eating, playing, resting, contemplation and academic endeavors. It sometimes, but not always, relates to a time in a person's retirement after previous service to the public or private sector, opposing \"active public life\". \"Otium\" can be a temporary time of leisure, that is sporadic. It can have intellectual, virtuous or immoral implications. It originally had the idea of withdrawing from one's daily business (\"negotium\") or affairs to engage in activities that were considered to be artistically valuable or enlightening (\"i.e.\" speaking, writing, philosophy). It had particular meaning to businessmen, diplomats, philosophers and poets.\n\nIn ancient Roman culture \"otium\" was a military concept as its first Latin usage. This was in Ennius' \"Iphigenia.\"\n\n\"Iphigenia\" (241–248)\nAccording to historian Carl Deroux in his work \"Studies in Latin literature and Roman history\", the word \"otium\" appears for the first time in a chorus of Ennius' Iphigenia. Ennius' first use of the term \"otium\" around 190 BC showed the restlessness and boredom during a reprieve from war and was termed \"otium negotiosum\" (free time to do what one wanted) and \"otium otiosum\" (idle wasteless free time). Aulus Gellius, while discussing the word \"praeterpropter\" (\"more or less\") quotes a fragment of Ennius's \"Iphigenia\", which contrasts \"otium\" with \"negotium\" repeatedly. Ennius imagined the emotions of Agamemnon's soldiers at Aulus, that while in the field and not at war and not allowed to go home, as \"more or less\" \"living\".\n\nThe earliest extant appearance of the word in Latin literature occurs in a fragment from the soldiers' chorus in the \"Iphigenia\" of Ennius, where it is contrasted to \"negotium\". Researches have determined the etymological and semantic use of \"otium\" was never a direct translation of the Greek word \"schole\", but derived from specifically Roman contexts. \"Otium\" is an example of the usage of the term \"praeterpropter\", meaning more or less of leisure. It was first used in military terms related to inactivity during war. In ancient Roman times soldiers were many times unoccupied, resting and bored to death when not at war (\"i.e.\", winter months, weather not permitting war). This was associated with \"otium otiosum\" (unoccupied and pointless leisure—idle leisure). The opposite of this was \"otium negotiosum\" (busy leisure) - leisure with a satisfying hobby or being able to take care of one's personal affairs or one's own estate. This was \"otium privatum\" (private leisure), equal to \"negotium\" (a type of business).\n\nThe oldest citation for \"otium\" is this chorus of soldiers, singing about idleness on campaign, in an otherwise lost Latin tragedy by Ennius. Andre shows in these lines that Ennius is showing the soldiers in the field would rather go home tending to their own affairs (\"otium\") than to be idle doing nothing. Its military origin meant to stop fighting in battle and laying down of the weapons - a time for peace. Even though originally \"otium\" was a military concept in early Roman culture of laying down one's weapons, it later became an elite prestigious time for caring for oneself. The ancient Romans had a sense of obligatory work ethics in their culture and considered the idle-leisure definition of \"otium\" as a waste of time. Historians of ancient Roman considered \"otium\" a time of laborious leisure of much personal duties instead of public duties. Author Almasi shows that historians Jean-Marie Andre and Brian Vickers point out the only legitimate form of \"otium\" was transpired with intellectual activity. \"Otium\" was thought of by the wise elite as being free from work and other obligations (\"negotium\") and leisure time spent on productive activities, however a time that should not be wasted as was thought the non-elite did with their leisure time.\n\nThe favorable sense of \"otium\" in Ciceronian Latin reflects the Greek term (\"skholē\", \"leisure\", a meaning retained in Modern Greek as σχόλη, \"schólē\"); \"leisure\" having a complex history in Greek philosophy before being used in Latin (through Latin the word became the root of many education-related English terms, such as school, scholar and scholastic). In Athens, leisure was one of the marks of the Athenian gentleman: the time to do things right, unhurried time, time to discuss in. From there it became \"discussion\", and from there, philosophical and educational schools, which were both conducted by discussion. Four major Greek philosophical schools influenced the Roman gentlemen of Cicero's time. Plato (and his contemporaries, if the \"Greater Hippias\" be not authentic) brought \"schole\" into philosophy; as often, Plato can be quoted on both sides of the question whether leisure is better than the business of a citizen. In the \"Greater Hippias\", it is one weakness of the title character that, although he has the education and manners of a gentleman, he has no leisure; but Socrates, in the \"Apology\", has no leisure either; he is too busy as a gad-fly, keeping his fellow Athenians awake to virtue. However, by the time the Romans encountered Plato's school, the Academy, they had largely ceased to discuss anything so practical as the good life; the New Academy of Carneades practiced verbal agility and boundless skepticism.\n\nTheophrastus and Dicaearchus, students of Aristotle, debated much on the contemplative life and the active life.\n\nRoman Epicureans used \"otium\" for the quiet bliss promised by Epicurus. An Epicurean proverb \nThe phrase \"to be at ease\" can have the meaning \"to be of good cheer\" or \"to be without fear\" these being interdependent. The Epicurean idea of \"otium\" favors contemplation, compassion, gratitude and friendship. The Epicurean view is that wisdom has as much to contribute to the benefit of the public as does that of contributions of politicians and laborers (\"i.e.\" sailors). The rustic \"otium\" concept incorporates country living into Epicureanism. The active city public life of \"negotium\" and an \"otium\" of reserved country life of reflection have been much written about by Cicero and Seneca the Younger.\n\nEpicurus's philosophy was contrary to Hellenistic Stoicism. Epicurus promised enjoyment in retirement as a concept of \"otium\". The concept of the Epicurean \"otium\" (private world of leisure) and the contemplative life were represented in Epicurus' school of philosophy and his garden. The portraits of the Garden of Epicurus near Athens represented political and cultural heroes of the time. Twenty-first–century historians Gregory Warden and David Romano have argued that the layout of the sculptures in \"The Garden\" were designed to give the viewer contrasting viewpoints of the Epicurean \"otium\" and the Hellenistic Stoic viewpoint of \"otium\" (\"i.e.\" private or public; contemplation or \"employment\"; \"otium\" or \"negotium\").\n\nIn early and colloquial Latin, despite the etymological contrast, \"otium\" is often used pejoratively, in contrast rather to \"officium\", \"office, duty\" than to \"negotium\" ('business\"). There was a difference established in ancient Roman times (second century BCE and forward) developing the idea elite social status was when one fulfilled one's duties in business and then \"otium\" meant \"leisure\" while \"negotium\" meant \"non-leisure\" (work duties still needed to be done). This new time of \"otium\" was filled by Greek scholarly pursuits and Greek pleasures. The time environment within which a person existed had sides to it that were filled with Greek customs such as pastimes, hobbies, interchanges of thoughts and ideas, and private bathing. \"Otium \" and \"negotium\" was then a new social concept which has perpetuated to our own time.\n\nHistorian J.M. Andre concludes that the original sense of \"otium\" was related to military service and the idleness that happened in the winter, as opposed to the business (\"negotium\") of the rest of the year. The most ancient Roman calendar divided the year into ten months devoted to war and farming, leaving the winter months of January and February vacant for individual \"otium.\" Andre shows that the beauty of the individual \"otium\" poses rest. Titus Maccius Plautus in his play \"Mercator\" says that while you are young is the time to save up for your retirement otium so you can enjoy it later, in his claim \"tum in otium te conloces, dum potes, ames\" (then you may set yourself at your ease, drink and be amorous).\n\nCicero speaks of himself, with the expression \"otium cum dignitate\", that time spent in activities one prefers is suitable for a Roman citizen who has retired from public life. When he was ousted from each office, this forced an inactive period, which he used for \"worthy leisure\". During this time he composed \"Tusculanae Disputationes\", a series of books on Stoic philosophy. Cicero saw free time as a time to devote to writing. Cicero defines \"otium\" as leisure, avoiding active participation in politics. He further defines it as a state of security and peace (pax) - a type of \"public health\". It is often associated with tranquility. Cicero advises in his third book \"On Duties\" that when the city life becomes too much, one should retreat to the country for leisure. The term \"otium cum dignitate\" in Cicero's \"Pro Sestio\" was to mean peace (pax) for all and distinction for some. Cicero says in \"Pro Sestio\", XLV., 98\nCicero explains that, while not necessarily his cup of tea, he recognized many different ways to spend leisure time and \"otium\". In one passage of \"De Oratore\" he explains that Philistus spent his retirement writing history as his \"otium\". He goes on to say in \"De Oratore\" Book iii that other men passed their \"otium\" of leisure due to bad weather that prevented them from doing their daily chores to playing ball, knucklebones, dice games or just games they made up. Others that were \"retired\" from public life for whatever reason devoted their \"otium cum seritio\" (leisure with service) to poetry, mathematics, music and teaching children.\n\nGerman historian Klaus Bringmann shows in Cicero's works that one can not characterize him as a hypocrite while in \"otium\" because of his sense of duty to serve the state. Cicero's concept of \"otium\" does not mean selfish pursuit of pleasure. It means the well-earned leisure which is a culmination of a long career of action and achievement. It's a reward. Idleness (\"desidia\") had derogatory implications and unqualified \"otium\" was a problem for Cicero's elite group of followers. Its break away from civic affairs contrasted with \"negotia publica\", participation in civic affairs of the republican aristocracy. To distinguish between plain \"idleness\" and aristocratic \"otium homestum\", \"otium liberale\" or \"otium cum dignitate\", writers of the day said that literary and philosophical pursuits were worthwhile activities and that they had benefit to \"res publica\" (the general public). These pursuits were a type of 'employment' and therefore not mere laziness.\n\nCicero praises Cato the Elder for his respectful use of \"otium\" in his expression \"non minus otii quam negotii\" (no less for doing nothing than business). Cicero was associating \"otium\" with writing and thinking when he admires Cato for pointing out that Scipio Africanus claimed he was \"never less idle than when he was at leisure, and never less lonely than when he was alone.\" Cicero in his \"De Officiis\" (book III 1-4) further says of Scipio Africanus \"Leisure and solitude, which serve to make others idle, in Scipio's case acted as a goad.\" Cicero's idea of \"otium cum dignitate\" (leisure with dignity) is considerably different from today's version of the concept. In his time, this kind of \"free time\" was only for the few privileged elite and was mostly made possible by the toil of slaves. It was associated with an egotistic and arrogant lifestyle, compared to those who had to earn their own living with no slaves. Today technology and educational systems enter into the equation on making leisure time (\"otium\") available to almost everyone, not just the privileged elite, which enables the pursuit of hobbies. Cicero has a number of different concept versions for \"otium\". In one concept he feels that a lifetime of loyalty attending one's duty (\"maximos labores\") should be rewarded with some form of retirement. This then promotes great sacrifices which promotes civic peace with honor within the state. He points out that the tranquillity one enjoys is due to the efforts of the majority. This concept of retirement through a lifetime of work was enjoyed only by the ruling class and the elite. The common people could only hope to enjoy a leisurely retirement with dignity as an inheritance.\n\nCatullus, a late Roman Republic poet, in his poems shows that the significance of \"otium\" of the middle Republican time of autonomy into the concept of how, why, and when a member of the patronal class might exchange political activity for literary leisure. He tended to mark \"otium\" with erotic influence.\n\nThe imperial dictatorship by Augustus during the fall of the Roman Republic put the Roman ruling classes into a state of change as their traditional role as always being in town was no longer needed. They were given much leisure time (\"otium\") because of their wealth. The wives of wealthy men were known to write poetry in special rooms devoted to education of the entire family (except the master of the house as it would have been below his dignity).\n\nThe home of choice (\"domus\") then became the countryside villa as the rise of the Roman Empire made them even wealthier. They could afford almost anything they could dream up in the way of a residence, especially among the large and middle class Roman landowners. Greek-style architecture became their new villa otium outside of town. In ancient Roman times the \"otium villa \" was a Dionysian idealistic rural home setting that evoked peace, leisure, simplicity and serenity. Often in ancient writings is found the mention of restorative powers due its natural setting (\"otium\") in the rural country home, contrasted to the busy city life with all the businesses (\"negotium\"). The \"villa with a garden\" and \"villa by the sea\" was associated with \"otium\".\nThe life at the Roman villa was associated with Greek culture in rooms which had Greek themes indicating a \"superior world\" of living.\n\nThe Imperial Roman poet Statius writes of a \"otium villa\" that he planned to retire to in Naples in his work \"Silvae\": \"It has secure peace, an idle life of leisure, non-troubled rest and sleep. There is no madness in the market-place, no strict laws in dispute...\" Pliny the Younger exemplified the philosophy of the Roman elite in \"otium\" of the time by the life he lived from his \"otium villas\". He would dictate letters to his secretary, read Greek and Latin speeches, go on walks on the villa's grounds, dine and socialize with friends, meditate, exercise, bath, take naps and occasionally hunt.\n\nTibullus was an Augustan elegiac poet who offered an alternative lifestyle to the Roman ideal of the military man or the man of action. He preferred the country lifestyle. In his existing first two books of poetry he compares the lifestyle of his chief friend and patron Marcus Valerius Messalla Corvinus as a commander and soldier to that of a farmer. Tibullus in his poem 1.3 rejects the work style of the rich man, \"adsiduus labor\", and military service (\"militia\"). He shows in his poetry that originally \"otium\" was a military concept, the disuse of one's weapons. Tibullus prefers the rustic agricultural landscape and a simple life. He indicates that while he would do agricultural work, he would only be interested in doing it sometimes (\"interdum\") and therefore inserts \"otium\" (peace and leisure time) into agricultural life. He expresses in his attitude of his poetry that the qualities of the Epicurean resolve to quietism (occultism—religious mysticism) and pacifism (abstention from violence) as the pursuits of \"ignobile otium\" (mean leisure) - peace of mind (peace with one's self) and detachment from worldly ambitions.\n\nSeneca compares the difference in the Epicurean and Stoic choice of \"otium\". He confesses that classic Stoicism urges active public life while Epicurus has a tendency not to advance public life unless forced to. Seneca views Stoicism and Epicureanism as legitimate to inaction in the proper situations. He defends the Stoic philosophy as leaning toward \"otium\". The main responsibility for the Stoic is to benefit the public in some manner. This could be done by the cultivation of virtue or the research of nature in retirement. This would mean a life of meditation and contemplation rather than an active political life. Seneca shows that \"otium\" is not really \"free time\", but a study of other matters (\"i.e.\", reading, writing) other than political and career gains.\n\nThe increase of retirees retreating to rural villas became more attractive as writers of the day wrote that Stoic ways included pursuits of reading, writing and philosophy. This meant that the work of public duties was replaced by \"otium liberale\" (liberal leisure) and was sanctified if the retiree did pursuits of reading, writing and philosophy. The benefits of the simplicity of rustic country life was reinforced in the intellectual legitimacy of \"otium ruris\" (rural leisure) because it drew out the spiritual implications of Horatian and Vergilian images of this type of life. Seneca's doctrine of \"De Otio\" describes retirement from public life. The contemplative life that Seneca revised was a Roman debate on \"otium\" (a productive peaceful time) and at some point in the evolution of the term was later contrasted to \"negotium\".\n\nThese are some of the elements in Seneca's doctrine of \"De Otio\":\n\nWhile Seneca's doctrine appears to be close to the doctrine of Athenodorus's \"De Tranquillitate\" it is basically different. In \"De Otio 3.5\" Seneca points out the benefits towards man in general, while in \"De Tranquillitate\" the theme is peace of mind.\n\nSaint Augustine of Hippo reminded Romans of \"otium philosophandi\", a positive element, that life was happiest when one had time to philosophize. Augustine points out that \"otium\" was the prerequisite for contemplation. It was because of \"otium\" that Alypius of Thagaste steered Augustine away from marriage. He said that they could not live a life together in the love of wisdom if he married. Augustine described \"Christianae vita otium\" as the Christian life of leisure. Many Christian writers of the time interpreted the Roman idea of \"otium\" as the deadly sin of acedia (sloth). Some Christian writers formulated \"otium\" as meaning to serve God through deep thought. Christian writers encouraged biblical studies to justify \"otium\". These same Christian writers also showed \"otium ruris\" (secluded rural leisure) as a needed step to monastic \"propositum\". Augustine describes the monastic life as \"otium sanctum\" (sanctified leisure or approved leisure). In Augustine's time the idea of philosophy had two poles of ambitions—one to be a worthy Christian (vacation - \"negotium\") and the other to be a worthy friend of God (devotion - \"otium\").\n\nPetrarch, 14th-century poet and Renaissance humanist, discusses \"otium\" in his \"De vita solitaria\" as it relates to a human life of simple habits and self-restraint. Like his favorite Roman authors Cicero, Horace, Seneca, Ovid and Livy, he sees \"otium\" not as leisure time devoted to idleness, passion, entertainment or mischievous wrongdoing; but time ideally spent on nature appreciation, serious research, meditation, contemplation, writing and friendship.\n\nPetrarch considered solitude (\"i.e.\", rural setting, \"villa otium\", his Vaucluse home) and its relationship to \"otium\" as a great possession for a chance at intellectual activity, the same philosophy as Cicero and Seneca. He would share such a precious commodity with his best friends in the spirit of Seneca when he said \"no good thing is pleasant to possess without friends to share it.\"\n\nHistorian Julia Bondanella translates Petrarch's Latin words of his own personal definition of \"otium\":\nPetrarch stressed the idea of an active mind even in \"otium\" (leisure). He refers back to Augustine's \"Vetus Itala\" in \"De otio religioso\" where in Christianity it was associated with contemplation and \"vacatio\" (vacate—be still). He points out that this is associated with \"videre\" (to see), which in Christianity is physical and mental activity aimed at moral perfection. He relates this concept of \"otium\" as \"vacate et videte\" (be still and see—a form of meditation, contemplation). Petrarch points out that one should not take leisure as so relaxed as to weaken the mind, but to be active in leisure to build up strength in the view of a unique character and religion.\n\nAndrew Marvell’s seventeenth-century poem The Garden is a lyric of nine stanzas in which he rejects all forms of activity in the world in favor of solitude of the retiree in a garden. It is a type of retirement poem expressing the love of retirement—an ancient Roman concept related to \"otium\". The poem shows the high degree of pleasure of rural retirement. Some critics see that he shows \"otium\" to mean peace, quiet and leisure—a goal for retirement from politics and business. Others see a Christianized \"otium\" with Marvell showing a representation of the progress of a soul from the pursuit for the pagan promised land of the peaceful countryside and contemplation to the search for the lost heaven on earth.\n\nPart of Marvell's poem The Garden below:\n\nBrian Vickers, a 20th-century British literary scholar, points out an expression made by Friedrich Nietzsche, a 19th-century German philosopher, on scholars' opinion of \"otium\" in his 1878 publication \"Menschliches, Allzumenschliches\":\n\nPrivate life meaning of \"otium\" meant personal retirement—the opposite of business. It meant leisure only for one's own pleasure with no benefit to the state or public. Examples here is where one attends only his own farm or estate. Another is hunting. It was the opposite of \"active public life\". One would not be a historian in this case.\n\nIn public life \"otium\" meant public peace and relief after war. It meant freedom from the enemy with no hostilities. It was not only freedom from external assault (the enemy), it was also freedom from internal disorder (civil war). This then had the meaning of leisure, peace and safety at the homeland. This eventually became the \"status quo\": acceptance of existing political and social conditions of the local laws, the custom of the ancestors, the powers of magistrates, authority of the senate, religions, the military, the treasury, and the praise of the empire.\n\n\n\"Otium\" carried with it many different meanings (including but not limited to time, chance, opportunity), depending on the time period or the philosophers involved in determining the concept.\n\nSynonyms of positive connotations are:\n\nSynonyms of negative connotations are:\n\nFor a comparatively recent and exhaustive authoritative study on \"otium\"' in the history of Roman culture see \n\nGregory M. Sadlek gives even a more recent and exhaustive study on \"otium\"' in his 2004 book \" Idleness working: the discourse of love's labor from Ovid through Chaucer and Gower\", .\n\n\n\n\n"}
{"id": "43621346", "url": "https://en.wikipedia.org/wiki?curid=43621346", "title": "Pascal's mugging", "text": "Pascal's mugging\n\nIn philosophy, Pascal's mugging is a thought-experiment demonstrating a problem in expected utility maximization. A rational agent should choose actions whose outcomes, when weighed by their probability, have higher utility. But some very unlikely outcomes may have very great utilities, and these utilities can grow faster than the probability diminishes. Hence the agent should focus more on vastly improbable cases with implausibly high rewards; this leads first to counter-intuitive choices, and then to incoherence as the utility of every choice becomes unbounded.\n\nThe name refers to Pascal's Wager, but unlike the wager does not require infinite rewards. This sidesteps many objections to the Pascal's Wager dilemma that are based on the nature of infinity.\n\nIn one description, Blaise Pascal is accosted by a mugger who has forgotten his weapon. However, the mugger proposes a deal: the philosopher gives him his wallet, and in exchange the mugger will return twice the amount of money tomorrow. Pascal declines, pointing out that it is unlikely the deal will be honoured. The mugger then continues naming higher rewards, pointing out that even if it is just one chance in 1000 that he will be honourable, it would make sense for Pascal to make a deal for a 2000 times return. Pascal responds that the probability for that high return is even lower than one in 1000. The mugger argues back that for any low probability of being able to pay back a large amount of money (or pure utility) there exists a finite amount that makes it rational to take the bet – and given human fallibility and philosophical scepticism a rational person must admit there is at least \"some\" non-zero chance that such a deal would be possible. In one example, the mugger succeeds by promising Pascal 1,000 quadrillion happy days of life. Convinced by the argument, Pascal gives the mugger the wallet.\n\nThe term \"Pascal's mugging\" to refer to this problem was originally coined by Eliezer Yudkowsky in the Less Wrong forum. In one of Yudkowsky's examples, the mugger succeeds by saying \"give me five dollars, or I'll use my magic powers from outside the Matrix to run a Turing machine that simulates and kills formula_1 people\". Here, the number formula_1 uses Knuth's up-arrow notation; writing the number out in base 10 would require enormously more writing material than there are atoms in the known universe.\n\nMoreover, in many reasonable-seeming decision systems, \"Pascal's mugging\" causes the expected utility of any action to fail to converge, as an unlimited chain of successively dire scenarios similar to Pascal's mugging would need to be factored in.\n\nPhilosopher Nick Bostrom argues that Pascal's mugging, like Pascal's wager, suggests that giving a superintelligent artificial intelligence a flawed decision theory could be disastrous. Pascal's mugging may also be relevant when considering low-probability, high-stakes events such as existential risk or charitable interventions with a low probability of success but extremely high rewards. Common sense seems to suggest that spending effort on too unlikely scenarios is irrational.\n\nOne advocated remedy might be to only use bounded utility functions: rewards cannot be arbitrarily large. Another approach is to use Bayesian reasoning to (qualitatively) judge the quality of evidence and probability estimates rather than naively calculate expectations. Other approaches are to penalize the prior probability of hypotheses that argue that we are in a surprisingly unique position to affect large numbers of other people who cannot symmetrically affect us, reject the providing the probability of a payout first, or abandon quantitative decision procedures in the presence of extremely large risks.\n\n"}
{"id": "548265", "url": "https://en.wikipedia.org/wiki?curid=548265", "title": "Peak–end rule", "text": "Peak–end rule\n\nThe peak–end rule is a psychological heuristic in which people judge an experience largely based on how they felt at its peak (i.e., its most intense point) and at its end, rather than based on the total sum or average of every moment of the experience. The effect occurs regardless of whether the experience is pleasant or unpleasant. According to the heuristic, other information aside from that of the peak and end of the experience is not lost, but it is not used. This includes net pleasantness or unpleasantness and how long the experience lasted. The peak–end rule is thereby a specific form of the more general extension neglect and duration neglect.\n\nThe peak–end rule is an elaboration on the snapshot model of remembered utility proposed by Barbara Fredrickson and Daniel Kahneman. This model dictates that an event is not judged by the entirety of an experience, but by prototypical moments (or \"snapshots\") as a result of the representativeness heuristic. The remembered value of snapshots dominates the actual value of an experience. Fredrickson and Kahneman theorized that these snapshots are actually the average of the most affectively intense moment of an experience and the feeling experienced at the end. The effects of the duration of an experience upon retrospective evaluation are extremely slight. Fredrickson and Kahneman labeled this phenomenon \"duration neglect\". The peak–end rule is applicable only when an experience has definite beginning and end periods.\n\nA 1993 study titled \"When More Pain Is Preferred to Less: Adding a Better End\" by Kahneman, Fredrickson, Charles Schreiber, and Donald Redelmeier provided groundbreaking evidence for the peak–end rule. Participants were subjected to two different versions of a single unpleasant experience. The first trial had subjects submerge a hand in 14 °C water for 60 seconds. The second trial had subjects submerge the other hand in 14 °C water for 60 seconds, but then keep their hand submerged for an additional 30 seconds, during which the temperature was raised to 15 °C. Subjects were then offered the option of which trial to repeat. Against the law of temporal monotonicity, subjects were more willing to repeat the second trial, despite a prolonged exposure to uncomfortable temperatures. Kahneman et al. concluded that \"subjects chose the long trial simply because they liked the memory of it better than the alternative (or disliked it less).\"\n\nSimilarly, a 1996 study by Kahneman and Redelmeier assessed patients' appraisals of uncomfortable colonoscopy or lithotripsy procedures and correlated the remembered experience with real-time findings. They found that patients consistently evaluated the discomfort of the experience based on the intensity of pain at the worst (peak) and final (end) moments. This occurred regardless of length or variation in intensity of pain within the procedure.\n\nAnother study by Kahneman and Ziv Carmon identified a boundary condition for the peak–end rule. Participants interacted with a computer program that had them wait to be served, while assessing their satisfaction as they were waiting. Kahneman and Carmon found that how participants felt at the final moment of the experience was a good predictor of their responses when they were asked to retrospectively evaluate their experiences. For example, participants who felt very dissatisfied during much of the experience but were satisfied in the final few seconds (because the waiting line moved faster than expected toward the end) summarized the experience as satisfying. Kahneman and Carmon concluded that real time experiences that are based on expectations are discounted after the fact if those expectations are unfulfilled.\n\nA third study by Kahneman, Redelmeier, and Joel Katz corroborated and expanded upon the discoveries made in the 1996 study. Colonoscopy patients were randomly divided into two groups. One underwent a colonoscopy procedure wherein the scope was left in for three extra minutes, but not moved, creating a sensation that was uncomfortable, but not painful. The other group underwent a typical colonoscopy procedure. Kahneman et al. found that, when asked to retrospectively evaluate their experiences, patients who underwent the longer procedure rated their experience as less unpleasant than patients who underwent the typical procedure. Moreover, the patients in the prolonged discomfort group were far more likely to return for subsequent procedures because a less painful end led them to evaluate the procedure more positively than those who faced a shorter procedure.\n\nPeople exhibit better memory for more intensely emotional events than less intensely emotional events. The precise cause of this is unclear, but it has been demonstrated, for decades, across a wide variety of surveys and experiments. In addition, people do not always recognize that the events that they remember are more emotionally intense than the \"average\" event of its kind. This failure to correct for the atypicality of extreme memories can lead people to believe those extreme moments are representative of the \"set\" being judged. Boston Red Sox fans asked to recall any one game they saw when the Red Sox won, for example, tended to recall the best game they could remember. They only realized this game was unrepresentative of past winning games by the Red Sox if they were explicitly asked to recall the best game they could remember, as evidenced by their subsequent \"affective forecasts\". This bias for more intense emotional experiences is evident in \"nostalgic preferences\". People asked to recall a television show or movie from the past tend to recall the most enjoyable show or movie that they can remember, and use this extreme example to rate all shows from its era unless they are also able to spontaneously recall shows or movies that are worse than the first show or movie they remember.\n\nPeople exhibit serial position effects such that they have better memory for \"both\" the beginning and end of sequences, phenomena known as primacy bias and recency bias, respectively. A paper by Garbinsky, Morewedge, and Shiv (2014) found evidence that for extended hedonic experiences, better memory for the end of the experience than the beginning (recency > primacy) can be attributed to memory interference effects. As a person eats potato chips, for example, the formation of a new memory of the most recently eaten chip makes it harder for them to recall how the previously eaten chips tasted. Garbinsky and colleagues found that (1) recency effects better predicted recalled enjoyment of a small meal (e.g., eating 5 or 15 chips) than did primacy effects, (2) that people had a worse memory for the first bite of the meal than the last bite of the meal, but (3) providing people with their ratings of the first bite lead them to use their enjoyment of that first bite as much as their enjoyment of the last bite when rating their overall enjoyment of the meal.\n\nSince most consumer interactions have set beginnings and ends, they fit the peak–end model. As a consequence, negative occurrences in any consumer interaction can be counteracted by establishing a firmly positive peak and end. This can be accomplished through playing music customers enjoy, giving out free samples, or paying a clerk to hold the door for patrons as they leave. As Scott Stratten has suggested, \"A really great salesperson who helps with an exchange can erase negative experiences along the way. The long wait in line and the bad music in the changing room are forgotten\". However, as research by Talya Miron-Shatz suggests, retrospective evaluations of day-long experiences do not appear to follow the peak–end rule, which brings into question the applicability of this rule to approximately day-length consumer–business interactions, such as hotel stays.\n\nIn 2006, a study was carried out at the University of Canterbury in Christchurch, New Zealand, analyzing the implications of the peak–end rule on the perceived happiness experienced on vacations. The study found that participants' remembered overall happiness was approximately predicted by the peak–end rule, although it was actually better predicted by their happiness during the \"most memorable or most unusual 24-h period\". Still, the duration of a vacation appeared to have negligible effects on remembered happiness. The results of the study could be applied to choosing more economical durations for vacations.\n\nThe peak–end rule is particularly salient in regard to medical procedures, since it suggests that it is preferable to have longer procedures that include a period of decreased discomfort than to have shorter procedures. In particular, the rule \"suggests that the memory of a painful medical treatment is likely to be less aversive if relief from the pain is gradual than if relief is abrupt\". Furthermore, the quality of a remembered procedure can drastically influence medical futures. If people recall necessary but onerous procedures more positively, then they are more likely to return for repeat procedures later in life.\nHowever, factoring the effect of the peak–end rule upon evaluations of medical procedures is problematic, since adding a period of decreasing pain to a procedure is still added pain. Even though this certainly yields a better memory of the process, the patient still endures more pain than is strictly necessary. Doctors and patients are forced to confront the choice between objectively less painful forms of treatment and forms of treatment that will be remembered more favorably. Kahneman claims that \"it is safe to assume that few patients will agree to expose themselves to pain for the sole purpose of improving a future memory\".\n\nCritiques of the peak–end rule typically derive from its conflation of a complex mental evaluation into a simplistic framework. A 2008 study found some support for the peak–end rule, but also found that it was \"not an outstandingly good predictor\" of remembered experiential value, and that the happiness of the most memorable part of an experience predicted remembered happiness better than did the happiness of the peak or of the end. Additionally, the extreme effect of peaks fades more rapidly over time, causing peaks to be recalled less positively and troughs recalled less negatively over time. Episodic memory endures for only a few weeks; at some point, mental accounting shifts over to semantic memory, leading to potential over-valuation of the \"end\" and diminished weighting of the peak. Additionally, memories that are available for evaluation may change due to the fading affect associated with memory or differing goals in recall. Goal orientation or initial expectations can also affect the weighting of a peak or an end, causing an end to be over-weighted as the culmination of a goal. Finally, Ariely and Carmon have theorized that evaluations of past events are affected by feelings at the time of evaluation.\n\n\n"}
{"id": "161758", "url": "https://en.wikipedia.org/wiki?curid=161758", "title": "Poppy", "text": "Poppy\n\nA poppy is a flowering plant in the subfamily Papaveroideae of the family Papaveraceae. Poppies are herbaceous plants, often grown for their colourful flowers. One species of poppy, \"Papaver somniferum\", is the source of the crude drug opium which contains powerful medicinal alkaloids such as morphine and has been used since ancient times as an analgesic and narcotic medicinal and recreational drug. It also produces edible seeds. Following the trench warfare in the poppy fields of Flanders during World War I, poppies have become a symbol of remembrance of soldiers who have died during wartime.\n\nPoppies are herbaceous annual, biennial or short-lived perennial plants. Some species are monocarpic, dying after flowering. Poppies can be over a metre tall with flowers up to 15 centimetres across. Flowers of species (not cultivars) have 4 to 6 petals, many stamens forming a conspicuous whorl in the center of the flower and an ovary of from 2 to many fused carpels. The petals are showy, may be of almost any color and some have markings. The petals are crumpled in the bud and as blooming finishes, the petals often lie flat before falling away. In the temperate zones, poppies bloom from spring into early summer. Most species secrete latex when injured. Bees use poppies as a pollen source. The pollen of the oriental poppy, \"Papaver orientale\", is dark blue, that of the field or corn poppy (\"Papaver rhoeas\") is grey to dark green. The opium poppy, \"Papaver somniferum\", grows wild in eastern and southern Asia, and South Eastern Europe. It is believed that it originated in the Mediterranean region.\n\nPoppies belong to the subfamily Papaveroideae of the family Papaveraceae, which includes the following genera: \n\nThe flowers of most poppy species are attractive and are widely cultivated as annual or perennial ornamental plants. This has resulted in a number of commercially important cultivars, such as the Shirley poppy, a cultivar of \"Papaver rhoeas\" and semi-double or double (flore plena) forms of the opium poppy \"Papaver somniferum\" and oriental poppy (\"Papaver orientale\"). Poppies of several other genera are also cultivated in gardens. A few species have other uses, principally as sources of drugs and foods. The opium poppy is widely cultivated and its worldwide production is monitored by international agencies. It is used for production of dried latex and opium, the principal precursor of narcotic and analgesic opiates such as morphine, heroin and codeine. Poppy seeds are rich in oil, carbohydrates, calcium and protein. Poppy oil is often used as cooking oil, salad dressing oil, or in products such as margarine. Poppy oil can also be added to spices for cakes, or breads. Poppy products are also used in different paints, varnishes, and some cosmetics.\n\nAncient Egyptian doctors would have their patients eat seeds from a poppy to relieve pain. Poppy seeds contain small quantities of both morphine and codeine, which are pain-relieving drugs that are still used today. Poppy seeds and fixed oils can also be nonnarcotic because when they are harvested about twenty days after the flower has opened, the morphine is no longer present.\n\nIn Mexico, Grupo Modelo, the makers of Corona beer, used red poppy flowers in most of its advertising images until the 1960s.\n\nArtificial poppies (called \"Buddy Poppies\") are used in the veterans' aid campaign by the Veterans of Foreign Wars, which provides money to the veterans who assemble the poppies and various aid programs to veterans and their families.\n\nA poppy flower is depicted on the reverse of the Macedonian 500-denar banknote, issued in 1996 and 2003. The poppy is also part of the coat of arms of the Republic of Macedonia.\n\nCanada issued special quarters (25-cent coins) with a red poppy on the reverse in 2004, 2008 and 2010. The 2004 Canadian \"poppy\" quarter was the world's first colored circulation coin.\n\nThe girl's given name \"Poppy\" is taken from the name of the flower.\n\nPoppies have long been used as a symbol of sleep, peace, and death: Sleep because the opium extracted from them is a sedative, and death because of the common blood-red color of the red poppy in particular. In Greek and Roman myths, poppies were used as offerings to the dead. Poppies used as emblems on tombstones symbolize eternal sleep. This symbolism was evoked in the children's novel \"The Wonderful Wizard of Oz\", in which a magical poppy field threatened to make the protagonists sleep forever.\n\nA second interpretation of poppies in Classical mythology is that the bright scarlet color signifies a promise of resurrection after death.\n\nThe poppy of wartime remembrance is \"Papaver rhoeas\", the red-flowered corn poppy. This poppy is a common weed in Europe and is found in many locations, including Flanders, which is the setting of the famous poem \"In Flanders Fields\" by the Canadian surgeon and soldier John McCrae. In Canada, the United Kingdom, the United States, Australia, South Africa and New Zealand, artificial poppies (plastic in Canada, paper in the UK, Australia, South Africa, Malta and New Zealand) are worn to commemorate those who died in war. This form of commemoration is associated with Remembrance Day, which falls on November 11. In Canada, Australia and the UK, poppies are often worn from the beginning of November through to the 11th, or Remembrance Sunday if that falls on a later date. In New Zealand and Australia, soldiers are also commemorated on ANZAC day (April 25), although the poppy is still commonly worn around Remembrance Day. Wearing of poppies has been a custom since 1924 in the United States. Miss Moina Michael of Georgia is credited as the founder of the Memorial Poppy in the United States.\n\nRed-flowered poppy is unofficially considered the national flower of the Albanians in Albania, Kosovo and elsewhere. This is due to its red and black colors, the same as the colors of the flag of Albania.\n\nThe California poppy, \"Eschscholzia californica\", is the state flower of California.\n\nThe powerful symbolism of \"Papaver rhoeas\" has been borrowed by various advocacy campaigns, such as the White Poppy and Simon Topping's black poppy.\n\n\"Papaver somniferum\" was domesticated by the indigenous people of Western and Central Europe between 6000 and 3500 BC. However, it is believed that its origins may come from the Sumerian people, where the first use of opium was recognized. Poppies and opium made made their way around the world along the silk road. Juglets resembling poppy seed pods have been discovered with trace amounts of opium and the flower appeared in jewelry and on art pieces in Egypt, dated 1550-1292 BC.\n\nThe eradication of poppy cultivation came about in the early 1900's through international conferences due to safety concerns associated with the production of opium. In the 1970's the American war on drugs targeted Turkish production of the plant, leading to an more negative popular opinion of the U.S.\n\n"}
{"id": "31329046", "url": "https://en.wikipedia.org/wiki?curid=31329046", "title": "Pre-attentive processing", "text": "Pre-attentive processing\n\nPre-attentive processing is the subconscious accumulation of information from the environment. All available information is pre-attentively processed. Then, the brain filters and processes what is important. Information that has the highest salience (a stimulus that stands out the most) or relevance to what a person is thinking about is selected for further and more complete analysis by conscious (attentive) processing. Understanding how pre-attentive processing works is useful in advertising, in education, and for prediction of cognitive ability.\n\nThe reasons are unclear as to why certain information proceeds from pre-attentive to attentive processing while other information does not. It is generally accepted that the selection involves an interaction between the salience of a stimulus and a person’s current intentions and/or goals. Two models of pre-attentive processing are pure-capture and contingent-capture. \n\nThe \"pure-capture\" model focuses on stimulus salience. If certain properties of a stimulus stand out from its background, the stimulus has a higher chance of being selected for attentive processing. This is sometimes referred to as \"bottom-up\" processing, as it is the properties of the stimuli which affect selection. Since things that affect pre-attentive processing do not necessarily correlate with things that affect attention, stimulus salience may be more important than conscious goals. For example, pre-attentive processing is slowed by sleep deprivation while attention, although less focused, is not slowed. Furthermore, when searching for a particular visual stimulus among a variety of visual distractions, people often have more trouble finding what they are looking for if one or more of the distractions is particularly salient. For example, it is easier to locate a bright, green circle (which is salient) among distractor circles if they are all grey (a bland color) than it is to locate a green circle among distractor circles if some are red (also salient colour). This is thought to occur because the salient red circles attract our attention away from the target green circle. However, this is difficult to prove because when given a target (like the green circle) to search for in a laboratory experiment, participants may generalize the task to searching for anything that stands out, rather than solely searching for the target. If this happens, the conscious goal becomes finding anything that stands out, which would direct the person’s attention towards red distractor circles as well as the green target. This means that a person’s goal, rather than the salience of the stimuli, could be causing the delayed ability to find the target.\n\nThe \"contingent-capture\" model emphasizes the idea that a person’s current intentions and/or goals affect the speed and efficiency of pre-attentive processing. The brain directs an individual’s attention towards stimuli with features that fit in with their goals. Consequently, these stimuli will be processed faster at the pre-attentive stage and will be more likely to be selected for attentive processing. Since this model focuses on the importance of conscious processes (rather than properties of the stimulus itself) in selecting information for attentive processing, it is sometimes called \"top-down\" selection. In support of this model, it has been shown that a target stimulus can be located faster if it is preceded by the presentation of a similar, priming stimulus. For example, if an individual is shown the color green and then required to find a green circle among distractors, the initial exposure to the color will make it easier to find the green circle. This is because they are already thinking about and envisioning the color green, so when it shows up again as the green circle, their brain readily directs its attention towards it. This suggests that processing an initial stimulus speeds up a person’s ability to select a similar target from pre-attentive processing. However, it could be that the speed of pre-attentive processing itself is not affected by the first stimulus, but rather that people are simply able to quickly abandon dissimilar stimuli, enabling them to re-engage to the correct target more quickly. This would mean that the difference in reaction time occurs at the attentive level, after pre-attentive processing and stimulus selection has already taken place.\n\nInformation for pre-attentive processing is detected through the five senses. In the visual system, the receptive fields at the back of the eye (retina) transfer the image via axons to the thalamus, specifically the lateral geniculate nuclei. The image then travels to the primary visual cortex and continues on to be processed by the visual association cortex. At each stage, the image is processed with increasing complexity. Pre-attentive processing starts with the retinal image; this image is magnified as it moves from retina to the cortex of the brain. Shades of light and dark are processed in the lateral geniculate nuclei of the thalamus. Simple and complex cells in the brain process boundary and surface information by deciphering the image's contrast, orientation, and edges. When the image hits the fovea, it is highly magnified, facilitating object recognition. The images in the periphery are less clear but help to create a complete image used for scene perception. \n\nVisual scene segmentation is a pre-attentive process where stimuli are grouped together into specific objects against a background. Figure and background regions of an image activate different processing centres: figures use the lateral occipital areas (which involve object processing) and background engages dorso-medial areas. \n\nVisual pre-attentive processing uses a distinct memory mechanism. When a stimulus is presented consecutively, the stimulus is perceived at a faster rate than if different stimuli are presented consecutively. The theory behind this is called the dimension-weighting account (DWA) where each time a specific stimulus (i.e. color) is presented it contributes to the weight of the stimuli. More presentations increase the weight of the stimuli, and therefore, subsequently decrease the reaction time to the stimulus. The dimensional-weighting system, which calculates pre-attentive processing for our visual system, codes the stimulus and thus directs attention to the stimulus with the most weight. \n\nVisual pre-attentive processing is also involved in the perception of emotion. Human beings are social creatures and are very adept at critiquing facial expressions. We have the ability to unconsciously process emotional stimuli and equate the stimuli, such as a face, with meaning.\n\nThe auditory system is also very important in accumulating information for pre-attentive processing. When a person’s eardrum is struck by incoming sound waves, it vibrates. This sends messages, via the auditory nerve, to the brain for pre-attentive processing. The ability to adequately filter information from pre-attentive processing to attentive processing is necessary for the normal development of social skills. For acoustic pre-attentive processing, the temporal cortex was believed to be the main site of activation, however, recent evidence has indicated involvement of the frontal cortex as well. The frontal cortex is predominantly associated with attentional processing, but it may also be involved in pre-attentive processing of complex and/or salient acoustic stimuli. For example, detecting slight variations in complex musical patterns has been shown to activate the right ventromedial prefrontal cortex.\n\nIt has been shown that in acoustic pre-attentive processing there is some degree of lateralization. The left hemisphere responds more to temporal acoustic information whereas the right hemisphere responds to the frequency of auditory information. Also, there is lateralization in the perception of speech which is left hemisphere dominant for pre-attentive processing.\n\n Vision, sound, smell, touch, and taste are processed together pre-attentively when more than one sensory stimuli are present. This multisensory integration increases activity in the superior temporal sulcus (STS), thalamus, and superior colliculus. Specifically, the pre-attentive process of multisensory integration works jointly with attention to activate brain regions such as the STS. Multisensory integration seems to give a person the advantage of greater comprehension if both auditory and visual stimuli are being processed together. But it is important to note that multisensory integration is affected by what a person pays attention to and their current goals.\n\n Training can lead to changes in activity and brain structures involved in pre-attentive processing. Professional musicians, in particular, show larger ERP (Event-related potential) responses to deviations in auditory stimuli and have possibly related structural differences in their brains (Heschl’s gyrus, corpus callosum, and pyramidal tracts). This plasticity of pre-attentive processing has also been shown in perception. Using EEG (electroencephalography) methods in pre-attentive colour perception, a study observed how easy it was for bilinguals to adapt to the linguistic constructs of a different culture. This means that pre-attentive processes are not hard-wired but malleable.\n\nDeficits in the transition from pre-attentive processing to attentive processing are associated with disorders such as schizophrenia, Alzheimer's disease, and autism. Abnormal prefrontal cortex function in schizophrenics results in the inability to use pre-attentive processing to recognize familiar auditory stimuli as non-threatening. Schizophrenics with positive symptoms have a greater capability of pre-attentively processing emotionally negative odors. This heightened ability to distinguish odors seems to be involved in their hypersensitivity to threatening situations. Alzheimer's disease is typically thought to affect high-level brain functioning (like memory) but can also have negative impacts on visual pre-attentive processing. Some of the difficulties with social interaction seen in autistics may be due to an impairment in filtration of pre-attentive auditory information. For example, they often have difficulty following a conversation as they cannot distinguish which parts are important and are easily distracted by other sounds.\n\n"}
{"id": "13442065", "url": "https://en.wikipedia.org/wiki?curid=13442065", "title": "Privacy-invasive software", "text": "Privacy-invasive software\n\nPrivacy-invasive software is computer software that ignores users’ privacy and that is distributed with a specific intent, often of a commercial nature. Three typical examples of privacy-invasive software are adware, spyware and content hijacking programs.\n\nIn a digital setting, such as the Internet, there are a wide variety of privacy threats. These vary from the tracking of user activity (sites visited, items purchased etc.), to mass marketing based on the retrieval of personal information (spam offers and telemarketing calls are more common than ever), to the distribution of information on lethal technologies used for, e.g., acts of terror.\n\nToday, software-based privacy-invasions occur in numerous aspects of Internet usage. Spyware programs set to collect and distribute user information secretly download and execute on users’ workstations. Adware displays advertisements and other commercial content often based upon personal information retrieved by spyware programs. System monitors record various actions on computer systems. Keyloggers record users’ keystrokes in order to monitor user behavior. Self-replicating malware downloads and spreads disorder in systems and networks. Data-harvesting software programmed to gather e-mail addresses have become conventional features of the Internet, which among other things results in that spam e-mail messages fill networks and computers with unsolicited commercial content. With those threats in mind, we hereby define privacy-invasive software as:\n\nIn this context, \"ignoring users’ right to be left alone\" means that the software is unsolicited and that it does not permit users to determine for themselves when, how and to what extent personally identifiable data is gathered, stored or processed by the software. \"Distributed\" means that it has entered the computer systems of users from (often unknown) servers placed on the Internet infrastructure. \"Often of a commercial nature\" means that the software (regardless of type or quality) is used as a tool in some sort of a commercial plan to gain revenues.\n\nIn early 2000, Steve Gibson formulated the first description of spyware after realizing software that stole his personal information had been installed on his computer. His definition reads as follows:\n\nThis definition was valid in the beginning of the spyware evolution. However, as the spyware concept evolved over the years it attracted new kinds of behaviours. As these behaviours grew both in number and in diversity, the term spyware became hollowed out. This evolution resulted in that a great number of synonyms sprang up, e.g. thiefware, scumware, trackware, and badware. It is believed that the lack of a single standard definition of spyware depends on the diversity in all these different views on what really should be included, or as Aaron Weiss put it:\n\nDespite this vague comprehension of the essence in spyware, all descriptions include two central aspects. The degree of associated user consent, and the level of negative impact they impair on the user and their computer system (further discussed in Section 2.3 and Section 2.5 in ). Because of the diffuse understanding in the spyware concept, recent attempts to define it have been forced into compromises. The Anti-Spyware Coalition (ASC) which is constituted by public interest groups, trade associations, and anti-spyware companies, have come to the conclusion that the term spyware should be used at two different abstraction levels. At the low level they use the following definition, which is similar to Steve Gibson’s original one:\n\nHowever, since this definition does not capture all the different types of spyware available they also provide a wider definition, which is more abstract in its appearance:\n\nDifficulties in defining spyware, forced the ASC to define what they call \"Spyware (and Other Potentially Unwanted Technologies)\" instead. This includes any software that does not have the users’ appropriate consent for running on their computers. Another group that has tried to define spyware is a group called StopBadware.org, which consists of actors such as Harvard Law School, Oxford University, Google, Lenovo, and Sun Microsystems. Their result is that they do not use the term spyware at all, but instead introduce the term \"badware\". Their definition thereof span over seven pages, but the essence looks as follows:\n\nBoth definitions from ASC and StopBadware.org show the difficulty with defining spyware. We therefore regard the term spyware at two different abstraction levels. On the lower level it can be defined according to Steve Gibsons original definition. However, in its broader and in a more abstract sense the term spyware is hard to properly define, as concluded above.\n\nA joint conclusion is that it is important, for both software vendors and users, that a clear separation between acceptable and unacceptable software behaviour is established. The reason for this is the subjective nature of many spyware programs included, which result in inconsistencies between different users beliefs, i.e. what one user regards as legitimate software could be regarded as a spyware by others. As the spyware concept came to include increasingly more types of programs, the term got hollowed out, resulting in several synonyms, such as trackware, evilware and badware, all negatively emotive. We therefore choose to introduce the term \"privacy-invasive software\" to encapsulate all such software. We believe this term to be more descriptive than other synonyms without having as negative connotation. Even if we use the word \"invasive\" to describe such software, we believe that an invasion of privacy can be both desired and beneficial for the user as long as it is fully transparent, e.g. when implementing specially user-tailored services or when including personalization features in software. \n\nThe work by Warkentins et al. (described in Section 7.3.1 in ) can be used as a starting point when developing a classification of privacy-invasive software, where we classify privacy-invasive software as a combination between \"user consent\" and \"direct negative consequences\". User consent is specified as either \"low\", \"medium\" or \"high\", while the degree of direct negative consequences span between \"tolerable\", \"moderate\", and \"severe\". This classification allows us to first make a distinction between legitimate software and spyware, and secondly between spyware and malicious software. All software that has a low user consent, \"or\" which impairs severe direct negative consequences should be regarded as malware. While, on the other hand, any software that has high user consent, \"and\" which results in tolerable direct negative consequences should be regarded as legitimate software. By this follows that spyware constitutes the remaining group of software, i.e. those that have medium user consent or which impair moderate direct negative consequences. This classification is described in further detail in Chapter 7 in .\n\nIn addition to the direct negative consequences, we also introduce \"indirect negative consequences\". By doing so our classification distinguishes between any negative behaviour a program has been designed to carry out (direct negative consequences) and security threats introduced by just having that software executing on the system (indirect negative consequences). One example of an indirect negative consequence is the exploitation risk of software vulnerabilities in programs that execute on users’ systems without their knowledge.\n\nThe term privacy-invasive software is motivated in that software types such as adware and spyware are essentially often defined according to their actions instead of their distribution mechanisms (as with most malware definitions, which also rarely correspond to motives of, e.g., business and commerce). The overall intention with the concept of privacy-invasive software is consequently to convey the commercial aspect of unwanted software contamination. The threats of privacy-invasive software consequently do not find their roots in totalitarianism, malice or political ideas, but rather in the free market, advanced technology and the unbridled exchange of electronic information. By the inclusion of purpose in its definition, the term privacy-invasive software is a contribution to the research community of privacy and security.\n\nIn the mid-1990s, the development of the Internet increased rapidly due to the interest from the general public. One important factor behind this accelerating increase was the 1993 release of the first browser, called Mosaic. This marked the birth of the graphically visible part of the Internet known as the World Wide Web (WWW) that was introduced in 1990. Commercial interests became well aware of the potential offered by the WWW in terms of electronic commerce especially because the restrictions on the commercial use of the Internet were removed which opened the space for companies to use the web as a platform to advertise and sell their goods. Thus, shortly after, companies selling goods over the Internet emerged, i.e. pioneers such as book dealer Amazon.com and CD retailer CDNOW.com, which both were founded in 1994. \n\nDuring the following years, personal computers and broadband connections to the Internet became more commonplace. Also, the increased use of the Internet resulted in that e-commerce transactions involved considerable amounts of money. As competition over customers intensified, some e-commerce companies turned to questionable methods in their battle to entice customers into completing transactions with them. This opened ways for illegitimate actors to gain revenues by stretching the limits used with methods for collecting personal information and for propagating commercial advertisements. Buying such services allowed for some e-commerce companies to get an advantage over their competitors, e.g. by using advertisements based on unsolicited commercial messages (also known as spam) .\n\nThe use of questionable techniques, such as Spam, were not as destructive as the more traditional malicious techniques, e.g. computer viruses or trojan horses. Compared to such malicious techniques the new ones differed in two fundamental ways. First, they were not necessarily illegal, and secondly, their main goal was gaining money instead of creating publicity for the creator by reaping digital havoc. Therefore, these techniques grouped as a “grey”area next to the already existing “dark” side of the Internet.\n\nBehind this development stood advertisers that understood that Internet was a “merchant’s utopia”, offering huge potential in global advertising coverage at a relatively low cost. By using the Internet as a global notice board, e-commerce companies could market their products through advertising agencies that delivered online ads to the masses. In 2004, online advertisement yearly represented between $500 million and $2 billion markets, which in 2005 increased to well over $6 billion-a-year. The larger online advertising companies report annual revenues in excess of $50 million each. In the beginning of this development such companies distributed their ads in a broadcast-like manner, i.e. they were not streamlined towards individual users’ interests. Some of these ads were served directly on Web sites as banner ads, but dedicated programs, called adware, soon emerged. Adware were used to display ads through pop-up windows without depending on any Internet access or Web pages.\n\nIn the search for more effective advertising strategies, these companies soon discovered the potential in ads that were targeted towards user interests. Once targeted online ads started to appear, the development took an unfortunate turn. Now, some advertisers developed software that became known as spyware, collecting users’ personal interests, e.g. through their browsing habits. Over the coming years spyware would evolve into a significant new threat to Internet-connected computers, bringing along reduced system performance and security. The information gathered by spyware were used for constructing user profiles, including personal interests, detailing what users could be persuaded to buy. The introduction of online advertisements also opened a new way to fund software development by having the software display advertisements to its users. By doing so the software developer could offer their software “free of charge”, since they were paid by the advertising agency. Unfortunately, many users did not understand the difference between “free of charge” and a “free gift”, where difference is that a free gift is given without any expectations of future compensation, while something provided free of charge expects something in return. A dental examination that is provided free of charge at a dentist school is not a free gift. The school expects gained training value and as a consequence the customer suffers increased risks. As adware were combined with spyware, this became a problem for computer users. When downloading software described as “free of charge” the users had no reason to suspect that it would report on for instance their Internet usage, so that presented advertisements could be targeted towards their interests.\nSome users probably would have accepted to communicate their browsing habits because of the positive feedback, e.g. “offers” relevant to their interests. However, the fundamental problem was that users were not properly informed about neither the occurrence nor the extent of such monitoring, and hence were not given a chance to decide on whether to participate or not. As advertisements became targeted, the borders between adware and spyware started to dissolve, combining both these programs into a single one, that both monitored users and delivered targeted ads. The fierce competition soon drove advertisers to further “enhance” the ways used for serving their ads, e.g. replacing user-requested content with sponsored messages instead, before showing it to the users.\n\nAs the chase for faster financial gains intensified, several competing advertisers turned to use even more illegitimate methods in an attempt to stay ahead of their competitors. This targeted advertising accelerated the whole situation and created a “gray” between conventional adds that people chose to see, such as subscribing to an Internet site & adds pushed on users through \"pop-up adds\" or downloaded adds displayed in a program itself.\n\nThis practice pushed Internet advertising closer to the “dark” side of Spam & other types of invasive, privacy compromising advertising. During this development, users experienced infections from unsolicited software that crashed their computers by accident, change application settings, harvested personal information, and deteriorated their computer experience. Over time these problems led to the introduction of countermeasures in the form of anti-spyware tools.\n\nThese tools purported to clean computers from spyware, adware, and any other type of shady software located in that same “gray” area. This type of software can lead to false positives as some types of legitimate software came to be branded by some users as \"Spyware\" (i.e. Spybot: Search & Destroy identifies the ScanSpyware program as a Spybot.) These tools were designed similarly to anti-malware tools, such as antivirus software. Anti-spyware tools identify programs using signatures (semantics, program code, or other identifying attributes). The process only works on known programs, which can lead to the false positives mentioned earlier & leave previously unknown spyware undetected. To further aggravate the situation, a few especially illegitimate companies distributed fake anti-spyware tools in their search for a larger piece of the online advertising market. These fake tools claimed to remove spyware, but instead installed their own share of adware and spyware on unwitting users’ computers. Sometimes even accompanied by the functionality to remove adware and spyware from competing vendors. Anti-Spyware has become a new area of online vending with fierce competition.\n\nNew spyware programs are being added to the setting in what seems to be a never-ending stream, although the increase has levelled out somewhat over the last years. However, there still does not exist any consensus on a common spyware definition or classification, which negatively affects the accuracy of anti-spyware tools. As mentioned above, some spyware programs remain undetected on users' computers. Developers of anti-spyware programs officially state that the fight against spyware is more complicated than the fight against viruses, trojan horses, and worms.\n\nThere are several trends integrating computers and software into people’s daily lives. One example is traditional media-oriented products which are being integrated into a single device, called media centres. These media centres include the same functionality as conventional television, DVD-players, and stereo equipment, but combined with an Internet connected computer. In a foreseeable future these media centres are anticipated to reach vast consumer impact. In this setting, spyware could monitor and surveillance for instance what television channels are being watched, when/why users change channel or what DVD movies users have purchased and watch. This is information that is highly attractive for any advertising or media-oriented corporation to obtain. This presents us with a probable scenario where spyware is tailored towards these new platforms; the technology needed is to a large extent the same as is used in spyware today.\n\nAnother interesting area for spyware vendors is the increasing amount of mobile devices being shipped. Distributors of advertisements have already turned their eyes to these devices. So far this development have not utilized the geographic position data stored in these devices. However, during the time of this writing companies are working on GPS-guided ads and coupons destined for mobile phones and hand-held devices. In other words, development of location-based marketing that allow advertising companies to get access to personal geographical data so that they can serve geographically dependent ads and coupons to their customers. Once such geographic data is being harvested and correlated with already accumulated personal information, another privacy barrier has been crossed.\n\n"}
{"id": "30855043", "url": "https://en.wikipedia.org/wiki?curid=30855043", "title": "Revised NEO Personality Inventory", "text": "Revised NEO Personality Inventory\n\nThe Revised NEO Personality Inventory (NEO PI-R) is a personality inventory that examines a person's Big Five personality traits (openness to experience, conscientiousness, extraversion, agreeableness, and neuroticism). In addition, the NEO PI-R also reports on six subcategories of each Big Five personality trait (called facets).\n\nHistorically, development of the Revised NEO PI-R began in 1978 with the publication of a personality inventory by Costa and McCrae. These researchers published three updated versions of their personality inventory in 1985, 1990, and 2010 which are called the NEO PI, NEO PI-R (or Revised NEO PI), and NEO PI-3, respectively. The revised inventories feature updated norms.\n\nThe inventories have both longer and shorter versions with the full NEO PI-R consisting of 240 items and providing detailed facet scores, whereas the shorter NEO-FFI (NEO Five-Factor Inventory) has only 60 items (12 per domain). The test was originally developed for use with adult men and women without overt psychopathology. It has also been found to be valid for use with children.\n\nA table of the personality dimensions measured by the NEO PI-R, including facets, is as follows:\n\nIn the 1970s, Paul Costa and Robert McCrae were researching age-related changes in personality. Costa and McCrae reported that they began by looking for the broad and agreed-upon traits of \"Neuroticism\" (N) and \"Extraversion\" (E), but cluster analyses led them to a third broad trait, \"Openness to Experience\" (O). The original version of the inventory, which was published in 1978, included only those three factors. The inventory was then called the Neuroticism-Extraversion-Openness Inventory (NEO-I). This version would be included in the Augmented Baltimore Longitudinal Study of Aging.\n\nBased on data from the Baltimore study, Costa and McCrae recognized two additional factors: \"Agreeableness\" (A) and \"Conscientiousness\" (C). Accordingly, in 1985 they published the first manual for the NEO that included all five factors, which are now known as the Big Five personality traits. Costa and McCrae renamed their instrument the NEO Personality Inventory (NEO PI). In this version, \"NEO\" was now considered part of the name of the test and was no longer an acronym. The assessment at this time included six facet sub-scales for the three original factors (N, E, & O). This naming convention continued with the third version, with the Revised NEO Personality Inventory, published in 1990, being referred to as NEO PI-R.\n\nResearch began to accumulate that indicated that the five factors were sufficiently broad and useful. There were also calls for a more detailed view of personality. In 1992 Costa and McCrae published a Revised NEO manual which included six facets for each factor (30 in total).\n\nIn the mid- to late-1990s, Costa and McCrae came to understand that some items on the NEO PI-R were outdated or too difficult for many test-takers to understand. Research also began to show that the NEO PI-R had the potential to be used with adolescents and children as young as 10. The possibility of using the NEO with young people led Costa and McCrae in 2002 to administer the NEO PI-R to over 1,900 high school students. The research identified 48 \"problem\" items that reflected either participant difficulties with item wording and/or low corrected item total correlations (CITCs). Alternative items were developed to replace the \"problem\" items; the revised instrument was administered to new samples The NEO PI-R and a revised version of the instrument were administered to 500 adolescents, 635 adults, and 449 middle school age children (12-13 year old). For both the adolescent and adult sample, the item/facet correlations were higher for the revised version of the NEO than for NEO PI-R. In addition, the internal consistency, factor structure, and tests of convergent and discriminant validity suggested that the revised version could be used with middle school children. Of the original 48 \"problem\" items, 37 were improved in terms of clarity and/or CITC. In 2005, Costa and McCrae published the latest version of the NEO Inventories, NEO PI-3. The new version included revisions of 37 items. With the creation of the NEO PI-3, Costa and McCrae intended to make the inventory accessible to a wider portion of the population. The improved readability of the NEO PI-3 compared to the NEO PI-R allowed the newer measure to be used with younger populations and adults with lower educational levels. Additionally, with the replacement of the 37 items, the psychometric properties of the NEO PI-3 were slightly improved over the NEO PI-R. In addition to increasing the readability of the NEO PI, the NEO PI-3 added a glossary of less familiar terms to aid in administration.\n\nIn the most recent publication, there are two forms for the NEO, self-report (form S) and observer-report (form R) versions. Both forms consist of 240 items (descriptions of behavior) answered on a five-point Likert scale. Finally, there is a 60-item inventory, the NEO FFI. There are paper and computer versions of both forms.\n\nThe manual reports that administration of the full version should take between 30 and 40 minutes. Costa and McCrae reported that an individual should not be evaluated if more than 40 items are missing. They also state that despite the fact that the assessment is \"balanced\" to control for the effects of acquiescence and nay-saying, that if more than 150 responses, or fewer than 50 responses, are \"agree\" or \"strongly agree,\" the results should be interpreted with caution.\n\nScores can be reported to most test-takers on \"Your NEO Summary,\" which provides a brief explanation of the assessment, and gives the individuals domain levels and a strengths-based description of three levels (high, medium, and low) in each domain. For example, low N reads \"Secure, hardy, and generally relaxed even under stressful conditions,\" whereas high N reads \"Sensitive, emotional, and prone to experience feelings that are upsetting.\" For profile interpretation, facet and domain scores are reported in T scores and are recorded visually as compared to the appropriate norming group.\n\nThe internal consistency of the NEO scales was assessed on 1,539 individuals. The internal consistency of the NEO PI-R was high, at: N = .92, E = .89, O = .87, A = .86, C = .90. The internal consistency of the facet scales ranged from .56 to .81. The internal consistency of the NEO PI-3 was consistent with that of the NEO PI-R, with \"α\" ranging from .89 to .93 for the five domains. Internal consistency coefficient from the facets, with each facet scale comprising fewer items than each of the Big Five scales, were necessarily smaller, ranging from .54 to .83.\n\nFor the NEO FFI (the 60 item domain only version) the internal consistencies reported in the manual were: N = .79, E = .79, O = .80, A = .75, C = .83. In the literature, the NEO FFI is used more often, with investigators using the NEO PI-R usually using the items from just the domains they are interested in. Sherry et al. (2007) found internal consistencies for the FFI to be as follows: N = .85, E = .80, O = .68, A = .75, C = .83.\n\nThe NEO has been translated into many languages. The internal consistency coefficients of the domain scores of a translation of the NEO that has been used in the Philippines are satisfactory. The alphas for the domain scores range from .78 to .90, with facet alphas having a median of .61. Observer-ratings NEO PI-R data from 49 different cultures was used as criterion in a recent study which tested whether individuals' perceptions of the \"national character\" of a culture accurately reflected the personality of the members of that culture (it did not).\n\nThe test-retest reliability of the NEO PI-R has also been found to be satisfactory. The test-retest reliability of an early version of the NEO after 3 months was: N = .87, E = .91, O = .86. The test-retest reliability for over 6 years, as reported in the NEO PI-R manual, was the following: N = .83, E = .82, O = .83, A = .63, C = .79. Costa and McCrae pointed out that these findings not only demonstrate good reliability of the domain scores, but also their stability (among individuals over the age of 30). Scores measured six years apart varied only marginally more than scores measured a few months apart.\n\nThe psychometric properties of NEO PI-R scales have been found to generalize across ages, cultures, and methods of measurement.\n\nAlthough individual differences (rank-order) tend to be relatively stable in adulthood, there are maturational changes in personality that are common to most people (mean-level changes). Most cross-sectional and longitudinal studies suggest that neuroticism, extraversion, and openness tend to decline, whereas agreeableness and conscientiousness tend to increase during adulthood. A meta-analysis of 92 personality studies that used several different inventories (among them NEO PI-R) found that social dominance, conscientiousness, and emotional stability increased with age, especially in the age span of 20 to 40.\n\nCosta and McCrae reported in the NEO manual research findings regarding the convergent and discriminant validity of the inventory. Examples of these findings include the following:\n\nA number of studies evaluated the criterion validity of the NEO. For example, Conard (2005) found that Conscientiousness significantly predicted the GPA of college students, over and above using SAT scores alone. In a study conducted in Seville, Spain, Cano-Garcia and his colleagues (2005) found that, using a Spanish version of the inventory, dimensions of the NEO correlated with teacher burnout. Neuroticism was related to the \"emotional exhaustion\" dimension of burnout, and Agreeableness, with the \"personal accomplishment\" burnout dimension. Finally, Korukonda (2007) found that Neuroticism was positively related to computer anxiety; Openness and Agreeableness were negatively related to computer anxiety.\n\nCritical reviews of the NEO PI-R were published in the 12th edition of the Mental Measurements Yearbook (MMY). The NEO-Pi-R (which only measures 57% of the known trait variance in the normal personality sphere alone) has been severely criticized both in terms of its factor analytic/construct validity and its psychometric properties. Widiger criticized the NEO for not controlling for social desirability bias. He argued that test developers cannot assume participants will be honest, especially in settings where it benefits people to present themselves in a better light (e.g., forensic or personnel settings). Ben-Porath and Waller pointed out that the NEO Inventories could be improved with the addition of controls for dishonesty and social desirability.\n\nJuni, in another review of the NEO PI-R for the MMY, praised the NEO PI-R for including both self- and other-report scales, making it easier for psychologists to corroborate information provided by a client or research participant. However, Juni criticized the NEO PI-R for its conceptualization using the Five Factor Model (FFM) of personality. Juni argued that the existence of the FFM was phenomenological and atheoretical, the model gaining popularity as a result of the influence of the authors (McCrae and Costa) in the psychological community. The NEO PI-R has also been criticized because of its market-oriented, proprietary nature. In response to the expense involved in using proprietary personality inventories such as the NEO, other researchers have contributed to the development of the International Personality Item Pool (IPIP); IPIP items and scales are available free of charge.\n\nA shortened version of NEO PI-R has been published. The shortened version is the NEO Five-Factor Inventory (NEO-FFI). It comprises 60 items and is designed to take 10 to 15 minutes to complete; by contrast, the NEO PI-R takes 45 to 60 minutes to complete. The NEO-FFI was revised in 2004. With the publication of the NEO PI-3 in 2005, a revised version of the NEO-FFI was also published. The revision of the NEO-FFI involved the replacement of 15 of the 60 items. The revised edition is thought to be more suitable for younger individuals. The new version had a stronger factor structure and increased reliability.\n\nAdditionally, an even shorter measure of personality has been developed. It is called the Ten Item Personality Inventory (TIPI). It too assesses the Big Five personality traits. The TIPI can be used in situations in which time is limited. Gosling et al. collected data comparing the TIPI to longer, multiple-item scales, and determined the TIPI to be psychometrically inferior. However, they also identified a number of the TIPI's strengths. First, Gosling et al. found acceptable correlations with more widely used Big Five measures. Second, their data suggested the TIPI has adequate test-retest reliability. Third, the data's correlations between the TIPI and criterion variables resemble the correlations found in research with longer Big-Five measures. They suggested the TIPI may, thus, be useful when very brief measures of personality are needed.\n\nEvidence of the NEO scales' stability in different countries and cultures can be considered evidence of its validity. A great deal of cross-cultural research has been carried out on the Five-Factor Model of Personality. Much of the research has relied on the NEO PI-R and the shorter NEO-FFI. McCrae and Allik (2002) edited a book consisting of papers bearing on cross-cultural research on the FFM. Research from China, Estonia, Finland, the Philippines, France, German-speaking countries, India, Portugal, Russia, South Korean, Turkey, Vietnam, and Zimbabwe have shown the FFM to be robust across cultures.\n\nRolland, on the basis of the data from a number of countries, asserted that the neuroticism, openness, and conscientiousness dimensions are cross-culturally valid. Rolland further advanced the view that the extraversion and agreeableness dimensions are more sensitive to cultural context. Age differences in the five-factors of personality across the adult life span are parallel in samples from Germany, Italy, Portugal, Croatia, and South Korea. Data examined from many different countries have shown that the age and gender differences in those countries resembled differences found in U.S. samples. An intercultural factor analysis yielded a close approximation to the five-factor model.\n\nMcCrae, Terracciano et al. (2005) further reported data from 51 cultures. Their study found a cross-cultural equivalency between NEO PI-R five factors and facets.\n\nWith the recent development of the NEO PI-3, cross-cultural research will likely begin to compare the newer version with the NEO PI-R. Piedmont and Braganza (2015) compared the NEO PI-R to the NEO PI-3 using an adult sample from India. They used an English version of the NEO PI-3 in order to measure its utility in individuals who speak English as a second language. Piedmont and Braganza found that the NEO PI-3 had slightly higher item/total correlations and better test-retest reliability than the NEO PI-R. They suggested that the NEO PI-3 has the potential to be utilized with those who do not speak English as their first language.\n\nThe NEO PI-R has been used in research pertaining to both (a) genotype and personality and (b) brain and personality. Such studies, however, have not always been conclusive. For example, one study found some evidence for an association between NEO PI-R facets and polymorphism in the tyrosine hydroxylase gene, while another study could not confirm the finding.\n\nIn a study published in Science, Lesch et al. (1996) found a relationship between the serotonin transporter gene regulatory region (5-HTTLPR) and the neuroticism subscale. Individuals with a shorter allele had higher neuroticism scores than individuals with the longer allele. The effect was significant for heterozygotes and even stronger for people homozygous for the shorter allele. Although the finding is important, this specific gene contributes to only 4% of the phenotypic variation in neuroticism. The authors concluded that \"if other genes were hypothesized to contribute similar gene dosage effects to anxiety, approximately 10 to 15 genes might be predicted to be involved.\"\n\n\n"}
{"id": "46466101", "url": "https://en.wikipedia.org/wiki?curid=46466101", "title": "Right to personal identity", "text": "Right to personal identity\n\nThe right to personal identity is recognised in international law through a range of declarations and conventions. From as early as birth, an individual’s identity is formed and preserved by registration or being bestowed with a name. However, personal identity becomes more complex as an individual develops a conscience. But human rights exist to defend and protect individuality, as quoted by Professor Jill Marshall: \"Human rights law exist to ensure that individual lifestyle choices are protected from majoritarian or populist infringement.\" Despite the complexity of personal identity, it is preserved and encouraged through privacy, personality rights and the right to self-expression.\n\nThe right to personal identity begins with the right to life. It is only through existing that individuals can cultivate their identity. Nevertheless, since ancient Greek philosophy, humans have been recognised with a \"soul\", making them more than physical bone and flesh. The Universal Declaration of Human Rights was created to preserve the biological and philosophical elements of human beings since its establishment in 1948. Therefore, the notion of individual identity and personality has been encouraged and preserved from the birth of human rights. However, throughout the years there have been developments towards the protection of personal identity through avenues that manifest identity such as private life, expression rights, personality rights and the right to know your biological origin.\n\nArticle 8 of the European Court of Human Rights has been interpreted to include \"personal identity\" within the meaning of \"private life.\" Article 8 protects against unwanted intrusion and provides for the respect of an individual's private space. Professor Marshall explains that this is important to be uninterrupted and to \"think reflectively without interference is to be in control of one’s own faculties,\" as Macklem puts it: \"independence of mind.\" This protection of inner privacy allows individuals to develop and cultivate their personal identity. \"Private life\" has also be interpreted to protect the development of relationships.The ECHR highlighted in the case of \"Bruggemann and Scheuten v Germany Yearbook\" the significance of relationships concerning the \"emotional field\" and \"the development of one’s own personality.\"\n\nWith respect of privacy comes respect for personal autonomy, which Article 8 has also been interpreted to protect. The ECHR Online states that the scope of Article 8 is to \"embrace personal autonomy\" and the freedom to make choices without the interference of the state to develop one’s own personal life. As illustrated by the Stanford Encyclopaedia of Philosophy, through protecting a person’s autonomy, a person’s identity is also protected, as both are integral to each other.\n\nThe Declaration of Human Rights Article 19 and Article 10 of the ECHR give everyone the right to freedom of opinion and expression. Macklem explains that \"freedom of expression is not merely the freedom to communicate one’s voice to others. It is more importantly the freedom to develop a distinctive voice of one’s own.\" Therefore, Articles 19 and 10 encourage the manifestation of personal identity. In the case of \"Handyside v UK\" the court stated \"Freedom of expression constitutes one of the essential foundations of such a society, one of the basic conditions for its progress and for the development of every man.\" Freedom of expression not only endorses individuals to participate and contribute to public life but it also gives them the opportunity to discover who they personally are.\n\nArticle 9 of the ECHR also provides the right to freedom (and the manifestation) of thought, conscience and religion. According to Locke, thought and consciousness establish personal identity, for these are the foundations of who a person is. In addition, a person’s beliefs also contribute to internal and external identity. For example, some believe women who have freely chosen to wear the Islamic headscarf or full-face veil are expressing their religious beliefs and personal identity. This has led to much debate and controversy within states which have banned the wearing of full-face veils in public.\n\nAs of 2011, both Belgium and France have banned the full-face Islamic veil in public places. The ban occurred under the administration of President Sarkozy, who stated that veils oppressed women and were \"not welcome\" in France. But Marshall highlights that the ban is disproportionate and it is not government's place to determine what women should wear especially when it misrecognises her and disrespects her identity and personality. While France explains that the intentions of the ban were to promote public order and secularism, \"Arslan v Turkey\" held that Article 9 had been violated and that France has failed to recognise the intrinsic worth of women who freely choose to wear such veils. As illustrated under Article 1 of the Declaration of Human Rights, all beings are born equal and therefore have equal worth. Finally, Amnesty International has repeatedly urged France not to impose the ban, saying it violates European human rights law.\n\nThe issue of the full-face veil ban in France and Belgium illustrates the extent of legal protection an individual has on their personal identity. Being empowered to make self-determined choices, such as freely choosing to wear a full-face veil to illustrate beliefs, Marshall believes, is an interpreted concept of human dignity and human freedom, allowing each woman's identity to be legally recognised. The enjoyment of these rights and freedoms in the ECHR are protected under Article 14, and \"shall be secured without discrimination regardless of sex, race, colour, language, religion, political or other opinion, national or social origin, association with a national minority, property, birth or other status.\"\n\nHowever, these are the opinions of only some scholars. This is a contested issue and others believe that the banning of full-face veils is about liberating females to express their sexuality and providing them the opportunity to show the world who they truly are. These aspects also promote and encourage personal identity.\n\nThe right to have and develop a personality is addressed in Article 22 of the UDHR: \"Everyone is entitled to the realisation of the rights needed for one’s dignity and the free development of their personality.\" Article 29 also protects the right to develop one’s personality: \"[e]veryone has duties to the community in which alone the free and full development of his personality is possible.\" Manuc explains that personality rights can be defined as those expressing the quintessence of the human person, and are intrinsic to being human. These rights recognise the \"spirit\" within an individual and have developed from the issues of privacy. Personality rights emerged from the German legal system in the late twentieth century to seek distance from the horrors of Naziism. It was also a mechanism to improve tort law surrounding privacy, as illustrated in the \"Criminal Diary\" case.\n\nThe case concerned the issue of personality structure and having the right to determine oneself. Ederle explained this as a right individuals have to choose how to be related in the world. Through the help of the German Constitutional Court, an individual can actively seek and create an intimate sphere so his personality can develop and be protected. Some states see no need for a specific law to personality, as their system of law possesses a different foundation for personality protection. For example, France, South Africa and England have an all-embracing law that protects an individual’s interest concerning physical integrity, feelings, dignity and privacy and identity. However, in addition to substantial protection to personality through privacy, the Netherlands and Austria also recognise a general right to personality.\n\nThe UN Convention on the Rights of the Child stresses the value and importance of a child’s identity. Giroux and De Lorenzi separate the understanding of identity into two parts: static and dynamic. The static aspects of identity concern attributes that make one visible to the outside world, for example, physical features, sex, name, genetics, and nationality. Dynamic aspects include morals and religious and cultural characteristics. Under Article 7, a child has a right to have a \"legal\" identity by being registered, and has a right to a name and a nationality. These protect mainly the static aspects of identity. However, Article 8 protects and encourages the child’s dynamic aspects of identity through preserving his or her identity in relation to nationality, name and family relations. Article 8 illustrates the state’s duty to protect this right, both passively and actively.\n\nArticles 7 and 8 developed to confront the issues of children in political struggles and disappearances. For example, \"Gelman v Urugnay\" concerned the kidnapping of Maria Gelman which prevented her from developing relationships with her parents and concealing her true identity from her. 193 states have ratified the convention, making it the most-ratified convention in history, including all United Nation members except the United States, Somalia and South Sudan.\n\nIdentity is also within people's genes as evidenced by debates concerning anonymity for gamete donation. Since 2005, in the UK, donor-received people can contact their donor once over 18 to find out where they have come from and prevent genealogical bewilderment. However, there are global differences towards the debate; for example, in Canada and the United States there are no regulations, whereas in Switzerland the donor must be willing to be identified, and in France, anonymity is forced.\n\nThere are some scholars who believe the right to identity must be treated with caution. Rosemary Coombe expressed her concerns of personal identity becoming property as there is the belief that through marginalising identity, it could be accepted as private and exclusive property. Lionel Bently is also concerned with this idea and highlights his worry through a quote from the \"Du Boulay\" case: \"Property rights in 'identity'… have the potential to curtail the liberties of those who wish to build their own identities, in whatever way, and for whatever reason.\" Other scholars believe that enshrining personal identity into the law is restricting people's choices and flexibility to transform and change who they are. However, human rights can also perform the contrary and protect individuals' choices on personal identity. While developing personal identity comes down to the individual to manifest character and work out 'who they are,' Marshall highlights that jurisprudence has evolved to create a positive obligation on states to provide social conditions such as private and personality rights to be respected, demonstrating that the international audience acknowledges that legal recognition is necessary to allow individuals to choose how they want to live and who they are.\n\n"}
{"id": "2371889", "url": "https://en.wikipedia.org/wiki?curid=2371889", "title": "Romantic friendship", "text": "Romantic friendship\n\nA romantic friendship, passionate friendship, or affectionate friendship is a very close but typically non-sexual relationship between friends, often involving a degree of physical closeness beyond that which is common in the contemporary Western societies. It may include for example holding hands, cuddling, hugging, kissing, giving massages, and sharing a bed, or co-sleeping, without sexual intercourse or other physical sexual expression.\n\nIn historical scholarship, the term may be used to describe a very close relationship between people of the same sex during a period of history when homosexuality did not exist as a social category. In this regard, the term was coined in the later 20th century in order to retrospectively describe a type of relationship which until the mid 19th century had been considered unremarkable but since the second half of the 19th century had become more rare as physical intimacy between non-sexual partners came to be regarded with anxiety. Romantic friendship between women in Europe and North America became especially prevalent in the late 18th and early 19th centuries, with the simultaneous emergence of female education and a new rhetoric of sexual difference.\nThe study of historical romantic friendship is difficult because the primary source material consists of writing about love relationships, which typically took the form of love letters, poems, or philosophical essays rather than objective studies. Most of these do not explicitly state the sexual or nonsexual nature of relationships; the fact that homosexuality was taboo in Western European cultures at the time means that some sexual relationships may be hidden, but at the same time the rareness of romantic friendship in modern times means that references to nonsexual relationships may be misinterpreted, as alleged by Faderman, Coontz, Anthony Rotundo, Douglas Bush, and others.\n\nThe content of Shakespeare's works has raised the question of whether he may have been bisexual.\nAlthough twenty-six of Shakespeare's sonnets are love poems addressed to a married woman (the \"Dark Lady\"), one hundred and twenty-six are addressed to an adolescent boy (known as the \"Fair Youth\"). The amorous tone of the latter group, which focus on the boy's beauty, has been interpreted as evidence for Shakespeare's bisexuality, although others interpret them as referring to intense friendship or fatherly affection, not sexual love.\n\nAmong those of the latter interpretation, in the preface to his 1961 Pelican edition, Douglas Bush writes:\nBush cites Montaigne, who distinguished male friendships from \"that other, licentious Greek love\", as evidence of a platonic interpretation.\n\nThe French philosopher Montaigne described the concept of romantic friendship (without using this English term) in his essay \"On Friendship.\" In addition to distinguishing this type of love from homosexuality (\"this other Greek licence\"), another way in which Montaigne differed from the modern view was that he felt that friendship and platonic emotion were a primarily masculine capacity (apparently unaware of the custom of female romantic friendship which also existed):\n\nLesbian-feminist historian Lillian Faderman cites Montaigne, using \"On Friendship\" as evidence that romantic friendship was distinct from homosexuality, since the former could be extolled by famous and respected writers, who simultaneously disparaged homosexuality. (The quotation also furthers Faderman's beliefs that gender and sexuality are socially constructed, since they indicate that each sex has been thought of as \"better\" at intense friendship in one or another period of history.)\n\nSome historians have used the relationship between Abraham Lincoln and Joshua Speed as another example of a relationship that modern people see as ambiguous or possibly gay, but which was most likely to have been a romantic friendship. Lincoln and Speed lived together, shared a bed in their youth and maintained a lifelong friendship. David Herbert Donald pointed out that men at that time often shared beds for financial reasons; men were accustomed to same-sex nonsexual intimacy, since most parents could not afford separate beds or rooms for male siblings. Anthony Rotundo notes that the custom of romantic friendship for men in America in the early 19th century was different from that of Renaissance France, and it was expected that men would distance themselves emotionally and physically somewhat after marriage; he claims that letters between Lincoln and Speed show this distancing after Lincoln married Mary Todd. Such distancing is still practiced today.\n\nProponents of the romantic friendship hypothesis also make reference to the Bible. Historians like Faderman and Robert Brain believe that the descriptions of relationships such as David and Jonathan or Ruth and Naomi in this religious text establish that the customs of romantic friendship existed and were thought of as virtuous in the ancient Near East, despite the simultaneous taboo on homosexuality.\n\nThe relationship between King David and Jonathan, son of King Saul, is often cited as an example of male romantic friendship; for example, Faderman uses 2 Samuel 1:26 on the title page of her book: \"Your love was wonderful to me, passing the love of women.\"\n\nRuth and her mother-in-law Naomi are the female Biblical pair most often cited as a possible romantic friendship, as in the following verse commonly used in same-sex wedding ceremonies:\n\nFaderman writes that women in Renaissance and Victorian times made reference to both Ruth and Naomi and \"Davidean\" friendship as the basis for their romantic friendships.\n\nWhile some authors, notably John Boswell, have claimed that ecclesiastical practice in earlier ages blessed \"same sex unions\", the accurate interpretation of these relationships rests on a proper understanding of the mores and values of the participants, including both the parties receiving the rite in question and the clergy officiating at it. Boswell himself concedes that past relationships are ambiguous; when describing Greek and Roman attitudes, Boswell states that \"[A] consensual physical aspect would have been utterly irrelevant to placing the relationship in a meaningful taxonomy.\" Boswell's own interpretation has been thoroughly critiqued, notably by Brent D. Shaw, himself a homosexual, in a review written for the New Republic:\n\nHistorian Robert Brain has also traced these ceremonies from Pagan \"blood brotherhood\" ceremonies through medieval Catholic ceremonies called \"gossipry\" or \"siblings before God,\" on to modern ceremonies in some Latin American countries referred to as \"compadrazgo\"; Brain considers the ceremonies to refer to romantic friendship.\n\nAs the American suffrage movement succeeded in gaining rights for white middle- and upper-class women, heterosexual marriage became less of a necessity, and many more women went to college and continued to live in female-centric communities after graduation. The all-women peer culture formed at women's colleges allowed students to create their own social rules and hierarchies, to become each other's leaders and heroes, and to idolize each other. These idolizations often took the forms of romantic friendships, which contemporaries called \"smashes\", \"crushes\" and \"spoons.\"\n\nThe practice of \"smashing\" involved one student showering another with gifts: notes, chocolates, sometimes even locks of hair. When the object of the student's affections was wooed and the two of them began spending all their time together, the \"aggressor\" was perceived by her friends as \"smashed\". In the early twentieth century, \"crush\" gradually replaced the term \"smash\", and generally signified a younger girl's infatuation with an older peer. Historian Susan Van Dyne has documented an \"intimate friendship\" between Mary Mathers and Frona Brooks, two members of the Smith College class of 1883. Mathers and Brooks exchanged tokens of affection, relished time spent alone together, and celebrated an anniversary.\n\nRomantic friendships kindled in women's colleges sometimes continued after graduation, with women living together in \"Boston marriages\" or cooperative houses. Women who openly committed themselves to other women often found acceptance of their commitment and lifestyle in academic fields, and felt comfortable expressing their feelings for their same-sex companions.\n\nAt the turn of the century, smashes and crushes were considered an essential part of the women's college experience, and students who wrote home spoke openly about their involvement in romantic friendships. By the 1920s, however, public opinion had turned against crushes as the American public began to realize that female sexuality might not be dependent on men.\n\nSeveral small groups of advocates and researchers have advocated for the renewed use of the term, or the related term Boston marriage, today. Several lesbian, gay, and feminist authors (such as Lillian Faderman, Stephanie Coontz, Jaclyn Geller and Esther Rothblum) have done academic research on the topic; these authors typically favor the social constructionist view that sexual orientation is a modern, culturally constructed concept.\n\n"}
{"id": "14907107", "url": "https://en.wikipedia.org/wiki?curid=14907107", "title": "Single-subject design", "text": "Single-subject design\n\nIn design of experiments, single-subject design or single-case research design is a research design most often used in applied fields of psychology, education, and human behavior in which the subject serves as his/her own control, rather than using another individual/group. Researchers use single-subject design because these designs are sensitive to individual organism differences vs group designs which are sensitive to averages of groups. Often there will be large numbers of subjects in a research study using single-subject design, however—because the subject serves as their own control, this is still a single-subject design. These designs are used primarily to evaluate the effect of a variety of interventions in applied research.\n\nThe following are requirements of single-subject designs:\n\n\nIt is important that the data are stable (steady trend and low variability) before the researcher moves to the next phase. Single-subject designs produce or approximate three levels of knowledge: (1) descriptive, (2) correlational, and (3) causal.\n\nSingle-subject designs are preferred because they are highly flexible and highlight individual differences in response to intervention effects. In general, single-subject designs have been shown to reduce interpretation bias for counselors when doing therapy.\n\nIn order to determine the effect of the independent variable on the dependent variable, the researcher will graph the data collected and visually inspect the differences between phases. If there is a clear distinction between baseline and intervention, and then the data returns to the same trends/level during reversal, a functional relation between the variables is inferred. Sometimes, visual inspection of the data demonstrates results that statistical tests fail to find.\n\nResearchers utilizing single-subject design begin with \"graphic analysis\". During the baseline, data are repeatedly collected and then graphed on the behavior of interest. This provides a visual representation of the subject's behavior before application of the intervention. It is critical that several (three to five is often recommended) data points are collected during baseline to allow the researcher to describe the effects on the target behavior during intervention.\n\nIn interpreting, the general strategy of all single-subject research is to use the subject as their own control. Experimental logic argues that the subject's baseline behavior would match its behavior in the intervention phase unless the intervention does something to change it. This logic then holds to rule out confound, one needs to replicate. It is the within-subject replication and allows for the determination of functional relationships. Thus the goal is:\n\nResearch designs are traditionally preplanned so that most of the details about to whom and when the intervention will be introduced are decided prior to the beginning of the study. However, in single-subject designs, these decisions are often made as the data are collected. In addition, there are no widely agreed-upon rules for altering phases, so—this could lead to conflicting ideas as to how a research experiment should be conducted in single-subject design.\n\nThe major criticism of single-subject designs are:\n\nHistorically, single-subject designs have been closely tied to the experimental analysis of behavior and applied behavior analysis.\n\n"}
{"id": "33604595", "url": "https://en.wikipedia.org/wiki?curid=33604595", "title": "Situational analysis", "text": "Situational analysis\n\nSituational analysis (or Situational logic) is a concept advanced by Popper in his The Poverty of Historicism. \"Situational analysis\" is a process by which a social scientist tries to reconstruct the problem situation confronting an agent in order to understand that agent's choice.\n\nKoertge (1975) provides a helpful clarificatory summary.\n"}
{"id": "1358959", "url": "https://en.wikipedia.org/wiki?curid=1358959", "title": "Sobol sequence", "text": "Sobol sequence\n\nSobol sequences (also called LP sequences or (\"t\", \"s\") sequences in base 2) are an example of quasi-random low-discrepancy sequences. They were first introduced by the Russian mathematician Ilya M. Sobol (Илья Меерович Соболь) in 1967.\n\nThese sequences use a base of two to form successively finer uniform partitions of the unit interval and then reorder the coordinates in each dimension.\n\nLet \"I\" = [0,1] be the \"s\"-dimensional unit hypercube, and \"f\" a real integrable function over \"I\". The original motivation of Sobol was to construct a sequence \"x\" in \"I\" so that\nand the convergence be as fast as possible.\n\nIt is more or less clear that for the sum to converge towards the integral, the points \"x\" should fill \"I\" minimizing the holes. Another good property would be that the projections of \"x\" on a lower-dimensional face of \"I\" leave very few holes as well. Hence the homogeneous filling of \"I\" does not qualify because in lower dimensions many points will be at the same place, therefore useless for the integral estimation.\n\nThese good distributions are called (\"t\",\"m\",\"s\")-nets and (\"t\",\"s\")-sequences in base \"b\". To introduce them, define first an elementary \"s\"-interval in base \"b\" a subset of \"I\" of the form\nwhere \"a\" and \"d\" are non-negative integers, and formula_3 for all \"j\" in {1, ...,s}.\n\nGiven 2 integers formula_4, a (\"t\",\"m\",\"s\")-net in base \"b\" is a sequence \"x\" of \"b\" points of \"I\" such that formula_5 for all elementary interval \"P\" in base \"b\" of hypervolume \"λ\"(\"P\") = \"b\".\n\nGiven a non-negative integer \"t\", a (\"t\",\"s\")-sequence in base \"b\" is an infinite sequence of points \"x\" such that for all integers formula_6, the sequence formula_7 is a (\"t\",\"m\",\"s\")-net in base \"b\".\n\nIn his article, Sobol described \"Π-meshes\" and \"LP sequences\", which are (\"t\",\"m\",\"s\")-nets and (\"t\",\"s\")-sequences in base 2 respectively. The terms (\"t\",\"m\",\"s\")-nets and (\"t\",\"s\")-sequences in base \"b\" (also called Niederreiter sequences) were coined in 1988 by Harald Niederreiter. The term \"Sobol sequences\" was introduced in late English-speaking papers in comparison with Halton, Faure and other low-discrepancy sequences.\n\nA more efficient Gray code implementation was proposed by Antonov and Saleev.\n\nAs for the generation of Sobol numbers, they are clearly aided by the use of Gray code formula_8 instead of \"n\" for constructing the \"n\"-th point draw.\n\nSuppose we have already generated all the Sobol sequence draws up to \"n\" − 1 and kept in memory the values \"x\" for all the required dimensions. Since the Gray code \"G\"(\"n\") differs from that of the preceding one \"G\"(\"n\" − 1) by just a single, say the \"k\"-th, bit (which is a rightmost bit of \"n\" − 1), all that needs to be done is a single XOR operation for each dimension in order to propagate all of the \"x\" to \"x\", i.e.\n\nSobol introduced additional uniformity conditions known as property A and A’.\n\n\n\nThere are mathematical conditions that guarantee properties A and A'.\n\n\n\nTests for properties A and A’ are independent. Thus it is possible to construct the Sobol sequence that satisfies both properties A and A’ or only one of them.\n\nTo construct a Sobol sequence, a set of direction numbers \"v\" needs to be selected. There is some freedom in the selection of initial direction numbers. Therefore, it is possible to receive different realisations of the Sobol sequence for selected dimensions. A bad selection of initial numbers can considerably reduce the efficiency of Sobol sequences when used for computation.\n\nArguably the easiest choice for the initialisation numbers is just to have the \"l\"-th leftmost bit set, and all other bits to be zero, i.e. \"m\" = 1 for all \"k\" and \"j\". This initialisation is usually called \"unit initialisation\". However, such a sequence fails the test for Property A and A’ even for low dimensions and hence this initialisation is bad.\n\nGood initialisation numbers for different numbers of dimensions are provided by several authors. For example, Sobol provides initialisation numbers for dimensions up to 51. The same set of initialisation numbers is used by Bratley and Fox.\n\nInitialisation numbers for high dimensions are available on Joe and Kuo. Peter Jäckel provides initialisation numbers up to dimension 32 in his book \"Monte Carlo methods in finance\".\n\nOther implementations are available as C, Fortran 77, or Fortran 90 routines in the Numerical Recipes collection of software. A free/open-source implementation in up to 1111 dimensions, based on the Joe and Kuo initialisation numbers, is available in C and Julia. A different free/open-source implementation is available for C++, Fortran 90, Matlab, and Python.\n\nFinally, commercial Sobol sequence generators are available within, for example, the NAG Library. A version is available from the British-Russian Offshore Development Agency (BRODA). MATLAB also contains an implementation as part of its Statistics Toolbox.\n\n\n"}
{"id": "553920", "url": "https://en.wikipedia.org/wiki?curid=553920", "title": "Subsequential limit", "text": "Subsequential limit\n\nIn mathematics, a subsequential limit of a sequence is the limit of some subsequence. Every subsequential limit is a cluster point, but not conversely. In first-countable spaces, the two concepts coincide.\n\nThe supremum of the set of all subsequential limits of some sequence is called the limit superior, or limsup. Similarly, the infimum of such a set is called the limit inferior, or liminf. See limit superior and limit inferior.\n\nIf (X,d) is a metric space and there is a Cauchy sequence such that there is a subsequence converging to some x, then the sequence also converges to x.\n"}
{"id": "3977880", "url": "https://en.wikipedia.org/wiki?curid=3977880", "title": "Terminological inexactitude", "text": "Terminological inexactitude\n\nTerminological inexactitude is a phrase introduced in 1906 by British politician Winston Churchill. Today, it is used as a euphemism or circumlocution meaning a lie or untruth.\n\nChurchill first used the phrase following the 1906 election. Speaking in the House of Commons on 22 February 1906 as Under-Secretary of the Colonial Office, he had occasion to repeat what he had said during the campaign. When asked that day whether the Government was condoning slavery of Chinese labourers in the Transvaal, Churchill replied:\n\nIt has been used as a euphemism for a lie in the House of Commons, as to accuse another member of lying is considered unparliamentary.\n\nIn more recent times, the term was used by Conservative MP Jacob Rees-Mogg to the Leader of the Oppositon, Jeremy Corbyn over an accusation that Rees-Mogg's company had moved a hedge fund into the Eurozone despite him being in favour of Brexit.\n\n\n"}
{"id": "39789268", "url": "https://en.wikipedia.org/wiki?curid=39789268", "title": "The Defector: Escape from North Korea", "text": "The Defector: Escape from North Korea\n\nThe Defector: Escape from North Korea is a 2012 documentary film about North Korean defectors, directed by Korean-Canadian filmmaker Ann Shin. The film premiered at the Hot Docs Canadian International Documentary Festival and had its broadcast premiere on TVOntario on June 26. The film's release was accompanied by \"The Defector Interactive\", an interactive documentary that uses a videogame-like approach to let the user find out about life in and escape from North Korea.\n\n"}
{"id": "2113007", "url": "https://en.wikipedia.org/wiki?curid=2113007", "title": "The Story of My Experiments with Truth", "text": "The Story of My Experiments with Truth\n\nThe Story of My Experiments with Truth is the autobiography of Mohandas K. Gandhi, covering his life from early childhood through to 1921. It was written in weekly instalments and published in his journal \"Navjivan\" from 1925 to 1929. Its English translation also appeared in installments in his other journal \"Young India\". It was initiated at the insistence of Swami Anand and other close co-workers of Gandhi, who encouraged him to explain the background of his public campaigns. In 1999, the book was designated as one of the \"100 Best Spiritual Books of the 20th Century\" by a committee of global spiritual and religious authorities.\n\nThis section is written by Mahadev Desai who translated the book from Gujarati to English in 1940. In this preface Desai notes that the book was originally published in two volumes, the first in 1927 and second in 1929. He also mentions that the original was priced at and had a run of five editions by the time of the writing of his preface. 50,000 copies had been sold in Gujarati but since the English edition was expensive it prevented Indians from purchasing it. Desai notes the need to bring out a cheaper English version. He also mentions that the translation has been revised by an English scholar who did not want his name to be published. Chapters XXIX-XLIII of Part V were translated by Desai's friend and colleague Pyarelal.\n\nThe introduction is written by Gandhi himself mentioning how he has resumed writing his autobiography at the insistence of Jeramdas, a fellow prisoner in Yerwada Central Jail with him. He mulls over the question a friend asked him about writing an autobiography, deeming it a Western practice, something \"nobody does in the east\". Gandhi himself agrees that his thoughts might change later in life but the purpose of his story is just to narrate his experiments with truth in life. He also says that through this book he wishes to narrate his spiritual and moral experiments rather than political.\n\nThe first part narrates incidents of Gandhi's childhood, his experiments with eating meat, smoking, drinking, stealing and subsequent atonement. There are two texts that had a lasting influence on Gandhi, both of which he read in childhood. He records the profound impact of the play \"Harishchandra\" and says,\"I read it with intense interest...It haunted me and I must have acted Harishchandra to myself times without number.\" Another text he mentions reading that deeply affected him was \"Shravana Pitrabhakti Nataka\", a play about Shravana's devotion to his parents. Gandhi got married at the age of 13. In his words, \"It is my painful duty to have to record here my marriage at the age of thirteen...I can see no moral argument in support of such a preposterously early marriage.\" Another important event documented in this part is the demise of Gandhi's father Karamchand Gandhi. Gandhi wrote the book to deal with his experiment for truth. His disdain for physical training at school, particularly gymnastics has also been written about in this part.\n\nAfter a long history of antagonism, the British and the Dutch shared power in South Africa, with Britain ruling the regions of Natal and Cape Colony, while the Dutch settlers known as the Boers taking charge in the Orange Free State and the Transvaal, two independent republics. The white settler and the independent Boer states continued to engage in volatile interactions with the British, so a threat of violent eruptions always loomed large. In order to placate both the Boer and other white settlers, the British adopted a number of racist policies, and while the Indians, most of them working on sugar and coffee plantations, did not suffer as much as the black population, they clearly experienced a treatment as second-class citizens. The initial story of Gandhi’s travails in South Africa and of his systematic struggle against oppression is well known. Gandhi repeatedly experienced the sting of humiliation during his long African sojourn. The incident at Maritzburg, where Gandhi was thrown off the train has become justly famous. When Gandhi, as a matter of principle, refused to leave the first class compartment, he was thrown off the train. Later, Gandhi also had difficulty being admitted to hotels, and saw that his fellow-Indians, who were mostly manual laborers, experienced even more unjust treatment.\n\nVery soon after his arrival, Gandhi's initial bafflement and indignation at racist policies turned into a growing sense of outrage and propelled him into assuming a position as a public figure at the assembly of Transvaal Indians, where he delivered his first speech urging Indians not to accept inequality but instead to unite, work hard, learn English and observe clean living habits. Although Gandhi's legal work soon start to keep him busy, he found time to read some of Tolstoy's work, which greatly influenced his understanding of peace and justice and eventually inspired him to write to Tolstoy, setting the beginning of a prolific correspondence. Both Tolstoy and Gandhi shared a philosophy of non-violence and Tolstoy's harsh critique of human society resonated with Gandhi's outrage at racism in South Africa.\n\nBoth Tolstoy and Gandhi considered themselves followers of the Sermon on the Mount from the New Testament, in which Jesus Christ expressed the idea of complete self-denial for the sake of his fellow men. Gandhi also continued to seek moral guidance in the Bhagavad-Gita, which inspired him to view his work not as self-denial at all, but as a higher form of self-fulfillment. Adopting a philosophy of selflessness even as a public man, Gandhi refused to accept any payment for his work on behalf of the Indian population, preferring to support himself with his law practice alone.\n\nBut Gandhi's personal quest to define his own philosophy with respect to religion did not rely solely on sacred texts. At the time, he also engaged in active correspondence with a highly educated and spiritual Jain from Bombay, his friend Raychandra, who was deeply religious, yet well versed in a number of topics, from Hinduism to Christianity. The more Gandhi communicated with Raychandra, the more deeply he began to appreciate Hinduism as a non violent faith and its related scriptures. Yet, such deep appreciation also gave birth to a desire to seek inner purity and illumination, without solely relying on external sources, or on the dogma within every faith. Thus, although Gandhi sought God within his own tradition, he espoused the idea that other faiths remained worthy of study and contained their own truths.\n\nNot surprisingly, even after his work assignment concluded, Gandhi soon found a reason to remain in South Africa. This pivotal reason involved the \"Indian Franchise Bill\", with which the Natal legislature intended to deprive Indians of the right to vote. No opposition existed against this bill, except among some of Gandhi's friends who asked him to stay in South Africa and work with them against this new injustice against Indians, who white South Africans disparagingly called \"coolies.\" He found that racist attitudes had become deeply entrenched, especially in the Dutch-ruled regions, where they lived in the worst urban slums and could not own property or manage agricultural land. Even in Natal, where Indians had more influence, they were not allowed to go out after 9 p.m. without a pass, while in Cape Colony, another British territory, they were not allowed to walk on the sidewalk. The new bill which prohibited Indians from voting in Natal only codified existing injustice in writing.\n\nAlthough a last-minute petition drive failed to the Indian Franchise Bill from passing, Gandhi remained active and organized a much larger petition, which he sent to the Secretary of State for the Colonies in London, and distributed to the press in South Africa, Britain and India. The petition raised awareness of the plight of Indians and generating discussions in all three continents to the point where both the Times of London and the Times of India published editorials in support of the Indian right to the vote. Gandhi also formed a new political organization called the Natal Indian Congress (a clear reference to the Indian National Congress), which held regular meetings and soon, after some struggles with financing, started its own library and debating society. They also issued two major pamphlets, An Appeal to Every Briton in South Africa, and The Indian Franchise–An Appeal, which offered a logical argument against racial discrimination. He was also thrown of the Train when he didn't agree to move from his first class seat which he paid for.\n\nThough, at first, Gandhi intended to remain in South Africa for a month, or a year at most, he ended up working in South Africa for about twenty years. After his initial assignment was over, he succeeded in growing his own practice to about twenty Indian merchants who contracted manage their affairs. This work allowed him to both earn a living while also finding time to devote to his mission as a public figure. During his struggle against inequality and racial discrimination in South Africa, Gandhi became known among Indians all around the world as \"Mahatma,\" or \"Great Soul.\"\n\nIn South Africa with the Family, the Boer War, Bombay and South Africa Again.\n\nIn 1896, Gandhi made a brief return to India and returned with his wife and children. In India, he published another pamphlet, known as the Green Pamphlet, on the plight of Indians in South Africa. For the first time, Gandhi realized that Indians had come to admire his work greatly and experienced a taste of his own popularity among the people, when he visited Madras, an Indian province, where most manual laborers had originated. Although his fellow-Indians greeted him in large crowds with applause and adulation, he sailed back to South Africa with his family in December 1896.\n\nGandhi had become very well known in South Africa as well, to the point where a crowd of rioters awaited him at Port Natal, determined that he should not be allowed to enter. Many of them also mistakenly believed that all the dark-skinned passenger on the ship that took Gandhi to Natal were poor Indian immigrants he had decided to bring along with him, when, in reality, these passengers were mostly returning Indian residents of Natal. Fortunately, Gandhi was able to establish a friendly relationship with the British in South Africa so the Natal port's police superintendent and his wife escorted him to safety. After this incident, local white residents began to actually regard him with greater respect.\n\nAs Gandhi resumed his work at the Natal Indian Congress, his loyalty to the British guided him to assist them in the Boer War, which started three years later. Because Gandhi remained a passionate pacifist, he wanted to participate in the Boer War without actually engaging in violence so he organized and led an Indian Medical Corps which served the British in a number of battles, including the important battle of Spion Kop in January 1900.\n\nAt the time, Gandhi believed that the British Empire shared the values of liberty and equality that he himself embraced and that, by virtue of defending those principles, the British constitution deserved the loyalty of all British subjects, including Indians. He viewed racist policy in South Africa as a temporary characteristic aberration, rather than a permanent tendency. With respect to the British in India, at this point in his life, Gandhi considered their rule beneficial and benevolent.\n\nThe armed conflict between the British and Dutch raged on for over three years of often brutal fighting with the British conquering the Transvaal and Orange Free state territories. Gandhi expected that the British victory would establish justice in South Africa and present him with an opportunity to return to India. He wanted to attend the 1901 meeting of the Indian National Congress, whose mission was to provide a social and political forum for the Indian upper class. Founded in 1885 by the British, the Congress had no real political power and expressed pro-British positions. Gandhi wanted to attend its meeting nevertheless, as he was hoping to pass a resolution in support of the Indian population in South Africa. Before he left for Bombay, Gandhi promised the Natal Indian Congress that he would return to support their efforts, should they need his help.\n\nAs Gandhi attended the 1901 Indian National Congress, his hopes came true. G.K. Gokhale, one of the most prominent Indian politicians of the time, supported the resolution for the rights of Indians in South Africa and the resolution passed. Through Gokhale, in whose house Gandhi stayed for a month, Gandhi met many political connections that would serve him later in life.\n\nHowever, his promise to always aid his friends in Natal soon prompted him to return to South Africa, when he received an urgent telegram informing him that the British and Boers had now formed a peaceful relationship and often acted together to the detriment of the Indian population, as Britain was planning to live local white individuals in power in South Africa, much like it had done in Canada and Australia.\n\nGandhi travelled back to South Africa immediately and met with Joseph Chamberlain, Secretary of State for the Colonies, and presented him with a paper on the injustice against the Indian population but Chamberlain indicated that the Indians would have to obey the new rulers of South Africa, now called the \"Afrikaners,\" which included both Dutch and British local settlers.\n\nGandhi began to organize a fast response to this new South African political configuration. Instead of working in Natal, he now established a camp in the newly conquered Transvaal region and began helping Indians who had escaped from the war in that region, and now had to purchase overly expensive re-entry passes. He also represented poor Indians whose dwellings in a shantytown the authorities had dispossessed. Gandhi also started a new magazine, Indian Opinion, that advocated for political liberty and equal rights in South Africa. The magazine, which initially included several young women from Europe, expanded its staff around the country, increasing both Gandhi's popularity and the public support for his ideas.\n\nAt round same time, Gandhi read John Ruskin's book \"Unto This Last\", which maintained that the life of manual labor was superior to all other ways of living. As he adopted this belief, Gandhi chose to abandon Western dress and habits, and he moved his family and staff to a Transvaal farm called the Phoenix, where he even gave renounced the use of an oil-powered engine and printed \"Indian Opinion\" by hand-wheel, and performed agriculture labor using old, manual farming equipment. He began to conceive of his public work as a mission to restore old Indian virtue and civilization, rather than fall prey to modern Western influence, which included electricity and technology.\n\nBetween 1901 and 1906, he also changed another aspect of his personal life by achieving Brahmacharya, or the voluntary abstention from sexual relations. He made this choice as part of his philosophy of selflessness and self-restraint. Finally, he also formulated his own philosophy of political protest, called Satyagraha, which literally meant \"truth-force\" in Sanskrit. In practice, this practice meant protesting injustice steadfastly, but in a non-violent manner.\n\nHe put this theory into practice on September 8, 1906, when, at a large gathering of the Indian community in Transvaal, he asked the whole community to take a vow of disobedience to the law, as the Transvaal government had started an effort to register every Indian child over the age of eight, which would make them an official part of the South African population.\n\nSetting a personal example, Gandhi became the first Indian to appear before a magistrate for his refusal to register, and he was sentenced to two months in prison. He actually asked for a heavier sentence, a request, consistent with his philosophy of self-denial. After his release, Gandhi continued his campaign and thousands of Indians burned their registration cards, crossing the Transvaal-Natal border without passes. Many went to jail, including Gandhi, who went to jail again in 1908.\n\nGandhi did not waiver when a South African General by the name of Jan Christiaan Smuts promised to eliminate the registration law, but broke his word. Gandhi went all the way to London in 1909 and gathered enough support among the British to convince Smuts to eliminate the law in 1913. Yet, the Transvaal Prime Minister continued to regard Indians as second-class citizens while the Cape Colony government passed another discriminatory law making all non-Christian marriages illegal, which meant that all Indian children would be considered born out of wedlock. In addition, the government in Natal continued to impose crippling poll tax for entering Natal only upon Indians.\n\nIn response to these strikingly unjust rules, Gandhi organized a large-scale satyagraha, which involved women crossing the Natal-Transvaal border illegally. When they were arrested, five thousand Indian coal miners also went on strike and Gandhi himself led them across the Natalese border, where they expected arrest.\n\nAlthough Smuts and Gandhi did not agree on many points, they had respect for each other. In 1913, Smuts relented due to the sheer number of Indians involved in protest and negotiated a settlement which provided for the legality of Indian marriages and abolished the poll tax. Further, the import of indentured laborers from India was to be phased out by 1920. In July 1914, Gandhi sailed for Britain, now admired as \"Mahatma,\" and known throughout the world for the success of satyagraha.\n\nPart IV. Mahatma in the Midst of World Turmoil\n\nGandhi was in England when World War I started and he immediately began organizing a medical corps similar to the force he had led in the Boer War, but he also faced health problems that caused him to return to India, where he met the applauding crowds with enthusiasm once again. Indians continued to refer to him as \"Mahatma\" or \"Great Soul,\" an appellation reserved only for the holiest men of Hinduism. While Gandhi accepted the love and admiration of the crowds, he also insisted that all souls were equal and did not accept the implication of religious sacredness that his new name carried.\n\nIn order to retreat into a life of humility and restraint, as his personal principles mandated, he decided to withdraw from public life for a while spending his first year in India focusing on his personal quest for purity and healing. He also lived in a communal space with untouchables, a choice which many of his financial supporters resented, because they believed that the very presence of untouchables defiled higher-caste Indians. Gandhi even considered moving to a district in Ahmedabad inhabited entirely by the untouchables when a generous Muslim merchant donated enough money to keep up his current living space for another year. By that time, Gandhi's communal life with the untouchables had become more acceptable.\n\nAlthough Gandhi had withdrawn from public life, he briefly met with the British Governor of Bombay (and future Viceroy of India), Lord Willington, whom Gandhi promised to consult before he launched any political campaigns. Gandhi also felt the impact of another event, the passing of G.K. Gokhale, who had become his supporter and political mentor. He stayed away from the political trend of Indian nationalism, which many of the members of the Indian National Congress embraced. Instead, he stayed busy resettling his family and the inhabitants of the Phoenix Settlement in South Africa, as well as the Tolstoy Settlement he had founded near Johannesburg. For this purpose, on May 25, 1915, he created a new settlement, which came to be known as the Satyagraha ashram ( derive from Sanskrit word \"Satya\" means \"truth\" ) near the town of Ahmedabad and close to his place of birth in the western Indian province of Gujarati. All the inhabitants of the ashram, which included one family of untouchables, swore to poverty and chastity.\n\nAfter a while, Gandhi became influenced by the idea of Indian independence from the British, but he dreaded the possibility that a westernized Indian elite would replace the British government. He developed a strong conviction that Indian independence should take place as a large-scale sociopolitical reform, which would remove the old plagues of extreme poverty and caste restrictions. In fact, he believed that Indians could not become worthy of self-government unless they all shared a concern for the poor.\n\nAs Gandhi resumed his public life in India in 1916, he delivered a speech at the opening of the new Hindu University in the city of Benares, where he discussed his understanding of independence and reform. He also provided specific examples of the abhorrent living conditions of the lower classes that he had observed during his travels around India and focused specifically on sanitation.\n\nAlthough the Indians of the higher-castes did not readily embrace the ideas in the speech, Gandhi had now returned to public life and he felt ready to convert these ideas to actions. Facing the possibility of arrest, just like he always did in South Africa, Gandhi first spoke for the rights of impoverished indigo-cultivators in the Champaran district. His efforts eventually led to the appointment of a government commission to investigate abuses by the indigo planters.\n\nHe also interefered whenever he saw violence. When a group of Ahmedabad mill workers went on strike and became violent, he resolved to fast until they returned to peace. Though some political commentators condemned Gandhi's behavior as a form of blackmail, the fast only lasted three days before the workers and their employers negotiated an agreement. Through this situation, Gandhi discovered the fast as one of his most effective weapons in late years and set a precedent for later action as part of satyagraha.\n\nAs the First World War continued, Gandhi also became involved in recruiting men for the British Army, an involvement which his followers had a difficult time accepting, after listening to his passionate speeches about resisting injustice in a non-violent manner. Not surprisingly, at this point, although Gandhi still remained loyal to Britain and enamored with the ideals of the British constitution, his desire to support and independent home rule became stronger. As time passed, Gandhi became exhausted from his long journey around the country and fell ill with dysentery. He refused conventional treatment and chose to practice his own healing methods, relying on diet and spending a long time bedridden, while in recovery in his ashram.\n\nIn the meantime, India’s unrest was overwhelming at the prospect of the British destroying the world's only Muslim power, the Ottoman Empire. While the British alleged that they fought to protect the rights of small states and independent peoples from tyranny, in India, an increasing number of people found this alleged commitment less than genuine.\n\nAfter the end of the war, the British government decided to follow the recommendations of the Rowlatt Committee, which advocated the retention of various wartime restrictions in India, including curfews and measures to suppress free speech. Gandhi was still sick when these events took place and, although he could not protest actively, he felt his loyalty to the British Empire weaken significantly.\n\nLater, when the Rowlatt Act actually became law, Gandhi proposed that the entire country observe a day of prayer, fasting, and abstention from physical labor as a peaceful protest against the injustice of the oppressive law. Gandhi's plea generated an overwhelming response as millions of Indians did not go to work on April 6, 1919.\n\nAs the entire country stood still, the British arrested Gandhi, which provoked angry crowds to fill the streets of India's cities and, much to Gandhi's dislike, violence erupted everywhere. Gandhi could not tolerate violence so he called off his campaign and asked that everyone return to their homes. He acted in accordance with his firm belief that if satyagraha could not be carried out without violence, it should not take place at all.\n\nUnfortunately, not all protesters shared Gandhi's conviction as ardently. In Amritsar, capital of the region known as the Punjab, where the alarmed British authorities had deported the local Hindu and Muslim members of the Congress, the street mobs became very violent and the British summoned Brigadier-General Reginald E.H. Dyer to restore order. Dyer prohibited all public meetings and instituted public whippings for Indians who approached British policemen. Despite these new regulations, a crowd of over ten thousand protesters gathered in the center of Armitsar, and Dyer responded with bringing his troops there and opening fire without warning. Tightly packed together, the protesters had nowhere to run from the fire, even when they threw themselves down on the ground the fire was then directed on the ground, ceasing only when the British troops no longer had ammunition. Hundreds died and many more were wounded.\n\nThis unfortunate occurrence became known as the Amritsar Massacre, it outraged the British public almost as much as Indian society. The authorities in London eventually condemned Dyer's conduct, forcing him to resign in disgrace. The effect the massacre had on Indian society became even more profound as more moderate politicians, like Gandhi, now began to wholeheartedly support the idea of Indian independence, creating an intense climate of mutual hostility. After the massacre, Gandhi eventually obtained permission to travel to Amritsar and conduct his own investigation. He produced a report months later and his work on the report motivated him to contact a number of Indian politicians, who advocated for the idea of independence from British rule.\n\nAfter Amritsar, Gandhi attended the Muslim Conference being held in Delhi, where Indian Muslims discussed their fears that the British would suppress Caliphs of Turkey. Muslims considered the Caliphs as heirs of Mohammed and spiritual heads of Islam. While the British considered such suppression a necessary effort to restore order after World War I, the Muslim populations viewed it as slap in the face. Gandhi urged them not to accept the actions of the British. He proposed a boycott of British goods, and stated that if the British continued to insist on the elimination of the Caliphate, Indian Muslims should take even more drastic measures of non-cooperation, involving areas such as government employment and taxes.\n\nDuring the months that followed, Gandhi continued to advocate for peace and caution, however, since Britain and Turkey were still negotiating their peace terms. Unlike more nationalistic politicians, he also supported the Montagu-Chelmsford Reforms for India, as they laid the foundation for constitutional self-government. Eventually, other politicians who thought the reforms did not go far enough had to agree with Gandhi simply because his popularity and influence had become so great that the Congress could accomplish little without him.\n\nAs the British remained determined to put an end to the Muslim Caliphate, they enforced the Rowlatt Act resolutely. Even Gandhi became less tolerant towards British practices and in April 1920, he urged all Indians, Muslim and Hindu, to begin a \"non-cooperation\" protest against the British rule by giving up their Western clothing and British jobs. As a personal example, on August 1, he returned the kasar-i-hind medal that he had received for providing medical service to the Boer War's wounded British army in South Africa. He also became the first president of the Home Rule League, a largely symbolic position which confirmed his position as an advocate for Indian Independence.\n\nIn September 1920, Gandhi also passed an official constitution for the Congress, which created a system of two national committees and numerous local units, all working to mobilize a spirit of non-cooperation across India. Gandhi and other volunteers traveled around India further establishing this new grass roots organization, which achieved great success. The new British Viceory in India, Lord Reading, did not dare to interfere because of Gandhi's immense popularity.\n\nBy 1922, Gandhi decided that the initiative of non-cooperation had to transform into open civil disobedience, but in March 1922, Lord Reading finally ordered Gandhi's arrest after a crowd in the city of Chauri Chaura attacked and killed the local representatives of British authority. Gandhi, who had never encouraged or sanctioned this type of conduct, condemned the actions of the violent crowds and retreated into a period of fasting and prayer as a response to this violent outburst. However, the British saw the event as a trigger point and a reason for his arrest.\n\nThe British authorities placed Gandhi on trial for sedition and sentenced him to six years in prison, marking the first time that he faced prosecution in India. Because of Gandhi's fame, the judge, C.N. Broomfield, hesitated to impose a harsher punishment. He considered Gandhi clearly guilty as charged, despite the fact that Gandhi admitted his guilt and even went as far as requesting the heaviest possible sentence. Such willingness to accept imprisonment conformed to his philosophy of satyagraha, so Gandhi felt that his time in prison only furthered his commitment and goals. The authorities allowed him to use a spinning wheel and receive reading materials while in prison, so he felt content. He also wrote most of his autobiography while serving his sentence.\n\nHowever, in Gandhi's absence, Indians returned to their British jobs and their every day routines. Even worse, the unity between Muslims and Hindu, which Gandhi advocated so passionately, had already begun to fall apart to the point where the threat of violence loomed large over many communities with mixed population. The fight for Indian independence could not continue while Indians themselves suffered disunity and conflict, all the more difficult to overcome in a huge country like India, which had always suffered religious divisions, as well as divisions by language, and even caste.\n\nGandhi realized that Independence and that the British had lost the will and power to sustain their empire, but he always acknowledged that Indians could not rely simply on the weakening of Britain in order to achieve independence. He believed that Indians had to become morally ready for Independence. He planned to contribute to such readiness through his speeches and writing, advocating humility, restraint, good sanitation, as well as an end to child marriages. He acknowledged that he had changed his position on many issues, like child marriages, and that he had not always managed to discern the most moral course of action in his life.\n\nAfter his imprisonment ended, he resumed his personal quest for purification and truth. He ends his autobiography by admitting that he continues to experience and fight with \"the dormant passion\" that lie within his own soul. He felt ready to continue the long and difficult path of taming those passions and putting himself last among his fellow human beings, the only way to achieve salvation, according to him.\n\n\"That is why the worlds' praise fails to move me; indeed it very often stings me. To conquer the subtle passions is far harder than the physical conquest of the world by the force of arms,\"\n\nGandhi writes in his \"Farewell\" to the readers, a suitable conclusion for an autobiography that he never intended to be an autobiography, but a tale of experiments with life, and with truth.\n\nAfter its initiation, \"The Story of My Experiments with Truth\" remained in the making for 4–5 years (including the time while Gandhi was imprisoned at Yerwada Central Jail near Pune, Maharashtra), and then it first appeared as a series in the weekly Gujarati magazine \"Navjivan\" during 1925–1928, which was published from Ahmedabad, India.\n\n\"The Story of My Experiments with Truth\" was first published in the United States in 1948 by Public Affairs Press of Washington, DC.\n\nGandhi wrote in his autobiography that the three most important modern influences in his life were Leo Tolstoy's \"The Kingdom of God Is Within You\", John Ruskin's \"Unto This Last\", and the poet Shrimad Rajchandra (Raychandbhai).\n\n\n"}
{"id": "38589404", "url": "https://en.wikipedia.org/wiki?curid=38589404", "title": "Throffer", "text": "Throffer\n\nIn political philosophy, a throffer is a proposal (also called an intervention) that mixes an offer with a threat which will be carried out if the offer is not accepted. The term was first used in print by political philosopher Hillel Steiner; while other writers followed, it has not been universally adopted and it is sometimes considered synonymous with \"carrot and stick\". Though the threatening aspect of a throffer need not be obvious, or even articulated at all, an overt example is\n\nKill this man and receive £100; fail to kill him and I'll kill you.\n\nSteiner differentiated offers, threats and throffers based on the preferability of compliance and noncompliance for the subject when compared to the normal course of events that would have come about were no intervention made. Steiner's account was criticised by philosopher Robert Stevens, who instead suggested that what was important in differentiating the kinds of intervention was whether performing or not performing the requested action was more or less preferable than it would have been were no intervention made. Throffers form part of the wider moral and political considerations of coercion, and form part of the question of the possibility of coercive offers. Contrary to received wisdom that only threats can be coercive, throffers lacking explicit threats have been cited as an example of coercive offers, while some writers argue that offers, threats and throffers may all be coercive if certain conditions are met. For others, by contrast, if a throffer is coercive, it is explicitly the threat aspect that makes it so, and not all throffers can be considered coercive.\n\nThe theoretical concerns surrounding throffers have been practically applied concerning workfare programmes. In such systems, individuals receiving social welfare have their aid decreased if they refuse the offer of work or education. Robert Goodin criticised workfare programmes which presented throffers to individuals receiving welfare, and was responded to by Daniel Shapiro, who found his objections unconvincing. Several writers have also observed that throffers presented to people convicted of crimes, particularly sex offenders, can result in more lenient sentences if they accept medical treatment. Other examples are offered by psychiatrist Julio Arboleda-Flórez, who presents concerns about throffers in community psychiatry, and management expert John J. Clancey, who talks about throffers in employment.\n\nThe term \"throffer\" is a portmanteau of \"threat\" and \"offer\". It was first used by Canadian philosopher Hillel Steiner in a 1974–75 \"Proceedings of the Aristotelian Society\" article. Steiner had considered a quote from the 1972 film \"The Godfather\": \"I'm gonna make him an offer he can't refuse\". While the line seemed to be amusingly ironic (because a threat is being made, not an offer), Steiner was unsatisfied that the difference between an offer and a threat was merely that one promises to confer a benefit and the other a penalty. He thus coined \"throffer\" to describe the \"offer\" in \"The Godfather\". One prominent thinker who adopted the term was political scientist Michael Taylor, and his work on throffers has been frequently cited.\n\n\"Throffer\" has not, however, been universally adopted; Michael R. Rhodes notes that there has been some controversy in the literature on whether to use \"throffer\", citing a number of writers, including Lawrence A. Alexander, David Zimmerman and Daniel Lyons, who do not use the term. Some, including political scientists Deiniol Jones and Andrew Rigby, consider \"throffer\" to be synonymous with \"carrot and stick\", an idiom which refers to the way a donkey is offered a carrot to encourage compliance, while noncompliance is punished with a stick. Other writers, while electing to use the word, consider it a poor one. For instance, literary scholar Daniel Shore calls it \"a somewhat unfortunate term\", while using it in his analysis of John Milton's \"Paradise Regained\".\n\nIn addition to Steiner's original account of throffers, other authors have suggested definitions and ideas on how to differentiate throffers from threats and offers.\n\nIn the article that introduces the term \"throffer\", Steiner considers the difference between interventions in the form of a threat and those in the form of an offer. He concludes that the distinction is based on how the consequences of compliance or noncompliance differ for the subject of the intervention when compared with \"the norm\". Steiner observes that a concept of \"normalcy\" is presupposed in literature on coercion, as changes in well-being for the subject of an intervention are not merely relative, but absolute; any possibility of an absolute change requires a standard, and this standard is \"the description of the normal and preducable course of events, that is, the course of events which would confront the recipient of the intervention were the intervention not to occur\".\n\nFor an offer, such as \"you may use my car whenever you like\", the consequence of compliance \"represents a situation which is preferred to the norm\". Noncompliance, that is, not taking up the offer of the use of the car, is identical to the norm, and so neither more nor less preferable. Threats, on the other hand, are characterised by compliance that leads to an outcome less preferable to the norm, with noncompliance leading to an outcome less desirable still. For instance, if someone is threatened with \"your money or your life\", compliance would lead to them losing their money, while noncompliance would lead to them losing their life. Both are less desirable than the norm (that is, not being threatened at all), but, for the subject of the threat, losing money is more desirable than being killed. A throffer is a third kind of intervention. It differs from both a threat and an offer, as compliance is preferable to the norm, while noncompliance is less preferable than the norm.\n\nFor Steiner, all of offers, threats and throffers affect the practical deliberations of their recipient in the same way. What is significant for the subject of the intervention is not the extent to which the consequences of compliance or noncompliance differ in desirability from the norm, but the extent to which they differ in desirability from each other. Thus, an offer does not necessarily exert less influence on its recipient than a threat. The strength of the force exerted by an intervention depends upon the difference in desirability between compliance and noncompliance alone, regardless of the manner of the intervention.\n\nResponding to Steiner, Robert Stevens provides examples of what he categorises variously as offers, threats and throffers that fail to meet Steiner's definitions. He gives an example of an intervention he considers a throffer, as opposed to a threat, but in which both compliance and noncompliance are less preferable to the norm. The example is that of someone who makes the demand \"either you accept my offer of a handful of beans for your cow, or I kill you\". For the subject, keeping the cow is preferred to both compliance and noncompliance with the throffer. Using this and other examples, Stevens argued that Steiner's account of differentiating the three kinds of interventions is incorrect.\n\nIn its place, Stevens suggests that determining whether an intervention is a throffer depends not on the desirability of compliance and noncompliance when compared to the norm, but on the desirability of the actions entailed in compliance or noncompliance when compared with what their desirability would have been were no intervention made. He proposes that a throffer is made if P attempts to encourage Q to do A by increasing \"the desirability to Q of Q doing A relative to what it would have been if P made no proposal and decrease the desirability to Q of Q doing not-A relative to what it would have been if P made no proposal\". An offer, by contrast, increases the desirability to Q of Q doing A compared to how it would have been without P's intervention, leaving the desirability to Q of Q doing not-A as it would have been. A threat decreases the desirability to Q of Q doing not-A compared to what it would have been without P's intervention, while leaving the desirability to Q of Q doing A as it would have been.\n\nPolitical philosopher Kristján Kristjánsson differentiates threats and offers by explaining that the former is a proposal that creates an obstacle, while the latter is one kind of proposal (another example being a request) that does not. He also draws a distinction between \"tentative proposals\" and \"final proposals\", which he feels earlier authors ignored. A tentative proposal does not logically create any kind of obstacle for its subject, and, as such, is an offer. For instance, \"if you fetch the paper for me, you'll get candy\" is a tentative proposal, as it does not logically entail that a failure to fetch the paper will result in no candy; it is possible that candy can be acquired by another route. In other words, if the subject fetches the paper, then they get candy. By contrast, if the proposal was a final proposal, it would take the form of \"if and only if you fetch the paper for me, you'll get candy\". This entails that candy can only be acquired if the subject fetches the paper, and no other way. For Kristjánsson, this kind of final proposal constitutes a throffer. There is an offer to fetch the paper (\"if\"), and a threat that candy can only be acquired through this route (\"only if\"). As such, an obstacle has been placed on the route of acquiring candy.\n\nPrevious authors (Kristjánsson cites Joel Feinberg, Alan Wertheimer and Robert Nozick) provided moral and statistical analyses of various thought experiments to determine whether the proposals they involve are threats or offers. On Kristjánsson's account, by contrast, all of the thought experiments considered are throffers. Instead, he argues, the previous thinkers' analyses attempted to differentiate offers that limit freedom from those that do not. They conflate two tasks, that of differentiating threats and offers and that of differentiating freedom-restricting threats from non-freedom-restricting threats. He concludes that the thinkers' methods are also inadequate for determining the difference between freedom-restricting and non-freedom-restricting threats, for which a test of moral responsibility would be required.\n\nPolitical philosopher and legal theorist Michael R. Rhodes offers an account of threats, offers and throffers based upon the perception of the subject of the proposal (and, in the case of proposals from agents as opposed to nature, the perception of the agent making the proposal.) Rhodes presents seven different motivational-want-structures, that is, seven reasons why \"P\" may want to do what leads to \"B\":\n\n\nProposals that motivate \"P\" to act because of \"W\", \"W\" or \"W\" represent offers. Those that do so because of \"W\" represent threats. Rhodes notes that offers and threats are asymmetrical: while an offer requires only a slight approbation, a high degree of disapprobation is required before a proposal can be called a threat. The disapprobation must be high enough to provoke the \"perception of a threat and correlative sense of fear\". Rhodes labels as \"throffers\" those proposals that motivate \"P\" to act because of \"W\", \"W\" or \"W\", but notes that the name is not universally used.\n\nFor Rhodes, throffers can not merely be biconditional proposals. If \"Q\" proposes that \"P\" pay $10,000 so that \"Q\" withholds information that would lead to \"P\"<nowiki>'</nowiki>s arrest, then despite the fact that the proposal is biconditional (that is, \"P\" may choose to pay or not pay, which would lead to different outcomes) it is not a throffer. This is because choosing to pay cannot be considered attractive for \"P\" independent of \"Q\"<nowiki>'</nowiki>s proposal. \"P\"<nowiki>'</nowiki>s paying of \"Q\" does not lead to the satisfaction of an attainment-want, which is a necessary condition for a proposal's being an offer under Rhodes's account. The exception to this is when an agent offers to help another overcome a background threat (a threat that was not introduced by the proposal). Biconditionals, in addition to either threats or offers, may contain \"neutral proposals\", and so not be throffers. The possibility of another agent's not acting is necessarily neutral. Throffers are those biconditional proposals that contain both a threat and an offer, as opposed to biconditional proposals containing a threat and neutral proposal, or an offer and a neutral proposal. In the case of throffers, it is always going to be difficult or even impossible to determine whether an agent acts on the threatening aspect of the proposal or the offer.\n\nConsideration of throffers forms part of the wider question of coercion and, specifically, the possibility of a coercive offer. Determining whether throffers are coercive, and, if so, to what extent, is difficult. The traditional assumption is that offers cannot be coercive, only threats can, but throffers can challenge this. The threatening aspect of a throffer need not be explicit, as it was in Steiner's examples. Instead, a throffer may take the form of an offer, but carry an implied threat. Philosopher John Kleinig sees a throffer as an example of an occasion when an offer alone may be considered coercive. Another example of a coercive offer may be when the situation in which the offer is made is already unacceptable; for instance, if a factory owner takes advantage of a poor economic environment to offer workers an unfair wage. For Jonathan Riley, a liberal society has a duty to protect its citizens from coercion, whether that coercion comes from a threat, offer, throffer or some other source. \"If other persons ... attempt to frustrate the right-holder's wants, then a liberal society must take steps to prevent this, by law if necessary. All exercises of power by others to frustrate the relevant individual or group preferences constitute unwarranted 'interference' with liberty in purely private matters.\"\n\nIan Hunt concurs that offers may be considered coercive, and claims that, whatever form the interventions take, they may be considered coercive \"when they are socially corrigible influences over action that diminish an agent's freedom overall\". He accepts that a possible objection to his claim is that at least some coercive offers do seemingly increase the freedom of their recipients. For instance, in the thought experiment of the lecherous millionaire, a millionaire offers a mother money for treatment for her son's life-threatening illness in exchange for her becoming the millionaire's mistress. Joel Feinberg considers the offer coercive, but in offering a possibility of treatment, the millionaire has increased the options available to the mother, and thus her freedom. For Hunt, Feinberg \"overlooks the fact that the millionaire's offer opens the option of [the mother] saving her child on condition that the option of not being [the millionaire's] mistress is closed\". Hunt does not see the mother as more free; \"while it is clear that she has a greater capacity to pursue her interests as a parent once the offer has been made, and to that extent can be regarded as freer, it is clear also that her capacity to pursue her sexual interests may have been diminished.\" Every coercive proposal, whether threat, offer or throffer, according to Hunt, contains a simultaneous loss and gain of freedom. Kristjánsson, by contrast, argues that Feinberg's account of \"coercive offers\" is flawed because these are not offers at all, but throffers.\n\nPeter Westen and H. L. A. Hart argue that throffers are not always coercive, and, when they are, it is specifically the threat that makes them so. For a throffer to be coercive, they claim, the threat must meet three further conditions; firstly, the person making the throffer \"must be intentionally bringing the threat to bear on X in order that X do something, Z\", secondly, the person making the throffer must know that \"X would not otherwise do or wish to be constrained to do\" Z, and, thirdly, the threat part of the throffer must render \"X's option of doing Z more eligible in X's eyes than it would otherwise be\". As such, for the authors, there is the possibility of non-coercive throffers. The pair present three possible examples. Firstly, when the threat aspect of the throffer is a joke; secondly, when the offer aspect is already so desirable to the subject that the threat does not affect their decision-making; or, thirdly, when the subject mistakenly believes the threat immaterial because of the attractiveness of the offer. Rhodes similarly concludes that if a throffer is coercive, it is because of the threatening aspect. For him, the question is \"whether one regards the threat component of a throffer as both a necessary and sufficient condition of the performance of a behaviour\". He argues that if the offer without the threat would have been enough for the agent subject to the proposal to act, then the proposal is not coercive. However, if both offer and threat aspects of the throffer are motivating factors, then it is tricky to determine whether the agent subject to the proposal was coerced. He suggests that differentiating between \"pure coercion\" and \"partial coercion\" may help solve this problem, and that the question of coercion in these cases is one of degree.\n\nThe conceptual issues around throffers are practically applied in studies in a number of areas, but the term is also used outside of academia. For instance, it has seen use in British policing and in British courts.\n\nConceptual thinking about throffers is practically applied in considerations of conditional aid, such as is used in workfare systems. For philosopher and political theorist Gertrude Ezorsky, the denial of welfare when subjects refuse work is the epitome of a throffer. Conditional welfare is also labelled a throffer by political philosopher Robert Goodin. In the words of Daniel Shapiro, also a political philosopher, the offer aspect of workfare is seen in the \"benefits one receives if one learns new skills, gets a job, alters destructive behaviors and the like\", while the threat aspect is executed with \"the elimination or reduction of aid, if the person does not, after a certain period of time, accept the offer\". For Goodin, the moral questionability of the threat aspect of a throffer is generally mitigated by the attractiveness of the offer aspect. In this way, workfare can represent a \"genuine\" throffer, but only when a person receiving welfare payments does not need the payments to survive, and so possesses a genuine choice as to whether to accept the throffer. When, however, an individual would be unable to survive if he or she stopped receiving welfare payments, there is no genuine choice; the individual is, for Goodin, unable to refuse the throffer. This cancels out the morally mitigating factor usually possessed by a throffer. This is presented as an argument against workfare, and Goodin anticipates that advocates would respond paternalistically by claiming that, regardless of issues of freedom, the individual in question would benefit from taking part in the work or education offered.\n\nShapiro responds to Goodin's argument by challenging his factual assumption that individuals would starve if they refused the workfare throffer. In state-sponsored (see welfare state) workfare systems, he claims, only monetary assistance is eliminated by a refusal to accept the throffer, while in private systems (that is, non-state charities or organisations offering conditional aid), other groups than the one operating a workfare system exist. In either system, recipients of welfare may also turn to family and friends for help. For these reasons, he does not consider the throffer to be unrefusable in the cases in which Goodin believes it is. A second (and, Shapiro claims, more important) objection is also presented. State welfare without sanctions fails to mirror the way that working individuals who do not rely on welfare payments take responsibility for their lives. If a person who works stops working, Shapiro observes, then they will typically find their economic situation worsened. Unconditional state welfare does not reflect this, and instead reflects the unusual position of the person who would be no worse off if they refused to work. As unconditional welfare does not mirror the situation of ordinary workers, it is unable to determine whether or not people are willing to take responsibility for their lives.\n\nFor Ivar Lødemel and Heather Trickey, editors of \"<nowiki>'</nowiki>An Offer You Can't Refuse': Workfare in International Perspective\", workfare programmes' reliance on compulsion makes them throffers. Citing the Danish model as a particular example, the pair argue that workfare involves the use of compulsory offers; while the work or education is presented as an offer, because recipients of welfare are dependent upon the help they would lose if they refuse the offer, they effectively have no choice. The compulsive aspect reveals that at least some recipients of welfare, in the eyes of policy makers, require coercion before they will accept offers of work. Neither the chance of paid work nor participation in labour schemes are, alone, enough to encourage some to freely accept the offers they receive. Such compulsion serves to reintegrate people into the labour market, and serves as a kind of \"new paternalism\". The authors are concerned about this compulsion, and present several arguments against it which are possible or have been utilised in the literature: Firstly, it impacts the rights of those against whom it is used. This may make it objectionable in and of itself, or it may result in undesirable outcomes. Secondly, it can be argued that benefits must be unconditional in order to act as a genuine safety net. Thirdly, compulsion undermines consumer feedback, and so no differentiation can be made between good and poor programmes presented to those receiving welfare. Fourthly, such coercion may contribute to a culture of resistance among those receiving welfare.\n\nForensic psychologist Eric Cullen and prison governor Tim Newell claim that prisoners face a throffer once they are told that they must acknowledge their guilt before they are offered parole or moved to an open prison. Cullen and Newell cite the example of a prisoner who falsely admitted guilt to move to an open prison; once there, however, he felt he could no longer lie about his guilt, and confessed to the prison's governor. He was subsequently transferred back to a maximum security prison. In the case of sex offenders, a throffer is presented when they are offered release if they take up treatment, but are threatened with extended sentences if they do not. Cullen and Newell are concerned about the predicament that these throffers present to prisoners, including those found innocent on appeal. Concerns surrounding throffers proposed to convicted sex offenders have also been discussed in print by Alex Alexandrowicz, himself wrongly imprisoned, and criminologist David Wilson. The latter observed the difficulties for those innocent people wrongly imprisoned who are faced with the throffer of having their sentence shortened if they \"acknowledge their guilt\", but noted that, as perspectives of prisoners were rarely considered, the problem is usually not visible.\n\nLikewise, therapeutic treatment of non-criminals with mental health problems can be considered in terms of throffers. In community psychiatry, patients with mental health problems will sometimes be presented with the provision of social services, such as financial or housing aid, in exchange for changing their lifestyle and reporting for the administration of medicines. Psychiatrist Julio Arboleda-Flórez considers these throffers a form of social engineering, and worries that they\nhave multiple implications in regard to coercive mechanisms from implicit curtailments of freedom to ascription of vulnerability. The former would include threats to personal autonomy, instilling fear in regard to a potential loss of freedom, an increase of dependency with mistrust of one's own capabilities to manage the business of living and, hence, an increase of feelings and attitudes of helplessness. The ascription of vulnerability overrides the principle of equality between the partners, constitutes and invasion of privacy and impacts on the positive rights of individuals.\nAccording to management researcher John J. Clancey, scientific management can involve the use of throffers. While piecework had been utilised since the Middle Ages, Frederick Winslow Taylor blended rationalised management with piecework, to create a new system. Productivity processes were standardised, after which point managers were able to present a throffer to workers: higher pay was offered if they were able to exceed the standard, while lower pay was threatened for any who did not meet expectations.\n\n\n\n"}
{"id": "1269245", "url": "https://en.wikipedia.org/wiki?curid=1269245", "title": "Traditional engineering", "text": "Traditional engineering\n\nTraditional engineering, also known as sequential engineering, is the process of marketing, engineering design, manufacturing, testing and production where each stage of the development process is carried out separately, and the next stage cannot start until the previous stage is finished. Therefore, the information flow is only in one direction, and it is not until the end of the chain that errors, changes and corrections can be relayed to the start of the sequence, causing estimated costs to be under predicted.\n\nThis can cause many problems; such as time consumption due to many modifications being made as each stage does not take into account the next. This method is hardly used today, as the concept of concurrent engineering is more efficient.\n\nTraditional engineering is also known as over the wall engineering as each stage blindly throws the development to the next stage over the wall.\n\nTraditional manufacturing has been driven by sales forecasts that companies need to produce and stockpile inventory to support. Lean manufacturing is based on the concept that production should be driven by the actual customer demands and requirements. Instead of pushing product to the marketplace, it is pulled through by the customers' actual needs.\n\n\n\n"}
{"id": "3461254", "url": "https://en.wikipedia.org/wiki?curid=3461254", "title": "Transcendentals", "text": "Transcendentals\n\nThe transcendentals () are the properties of being that correspond to three aspects of the human field of interest and are their ideals; science (\"truth\"), the arts (\"beauty\") and religion (\"goodness\"). Philosophical disciplines that study them are logic, aesthetics and ethics.\n\nParmenides first inquired of the properties co-extensive with being. Socrates, spoken through Plato, then followed (see \"Form of the Good\").\n\nAristotle's substance theory (being a substance belongs to being \"qua\" being) has been interpreted as a theory of transcendentals. Aristotle discusses only unity (\"One\") explicitly because it is the only transcendental intrinsically related to being, whereas truth and goodness relate to rational creatures.\n\nIn the Middle Ages, Catholic philosophers elaborated the thought that there exist transcendentals (\"transcendentalia\") and that they transcended each of the ten Aristotelian categories. A doctrine of the transcendentality of the good was formulated by Albert the Great. His pupil, Saint Thomas Aquinas, posited five transcendentals: \"res, unum, aliquid, bonum, verum\"; or \"thing\", \"one\", \"something\", \"good\", and \"true\". Saint Thomas derives the five explicitly as transcendentals, though in some cases he follows the typical list of the transcendentals consisting of the One, the Good, and the True. The transcendentals are ontologically one and thus they are convertible: e.g., where there is truth, there is beauty and goodness also.\n\nIn Christian theology the transcendentals are treated in relation to theology proper, the doctrine of God. The transcendentals, according to Christian doctrine, can be described as the ultimate desires of man. Man ultimately strives for perfection, which takes form through the desire for perfect attainment of the transcendentals. The Catholic Church teaches that God is Himself truth, goodness, and beauty, as indicated in the \"Catechism of the Catholic Church\". Each transcends the limitations of place and time, and is rooted in being. The transcendentals are not contingent upon cultural diversity, religious doctrine, or personal ideologies, but are the objective properties of all that exists.\n\n\n\n"}
{"id": "41210058", "url": "https://en.wikipedia.org/wiki?curid=41210058", "title": "Ujukatā", "text": "Ujukatā\n\nUjukatā (Pali) is a Buddhist term translated as \"rectitude\", and it is the basis for the following pair of mental factors within the Theravada Abhidharma teachings:\n\nThese two mental factors have the characteristic of uprightness of the mental body and consciousness, respectively.\n\nBhikkhu Bodhi states:\n\nNina van Gorkom explains:\n\n\n"}
{"id": "45563661", "url": "https://en.wikipedia.org/wiki?curid=45563661", "title": "Unbolted", "text": "Unbolted\n\nUnbolted is a UK based online personal asset-based peer-to-peer lending platform, where individuals can borrow from other individuals by using high value personal assets such as luxury watches, cars, fine arts, antiques, jewellery and commodities such as gold. The company started trading in November 2014 and has its offices in London.\n\nUnbolted was set up by Ashwin Parmeswaran and Rito Haldar with an aim to make short-term borrowing transparent, affordable and convenient for consumers and small business owners, by using their high-value personal assets to access low-cost loans. It raised seed funding in July 2014 prior to its public launch in November 2014.\n"}
{"id": "44687858", "url": "https://en.wikipedia.org/wiki?curid=44687858", "title": "Xiao Hui Wang Art Center", "text": "Xiao Hui Wang Art Center\n\nThe Xiao Hui Wang Art Center is an art institute at Tongji University, Shanghai. Founded in 2003, it is administered by Professor Xiao Hui Wang, and is known for its numerous commissioned urban design projects and for being the first art institute in China to focus on new media art.\n\nIts design for the 2010 Shanghai Expo Urban Footprint Pavilion was chosen after international competition, and it worked with the Shanghai Museum, the sponsor and official organizer, to carry it out.\n\nOther projects have included urban art projects for the cities of Langfang, Datong, Nantong, Suzhou and Zhuhai, artistic design work for the Nanjing World Trade Center, the Tianjin Innovation-China e•Group building, the Zendai Radisson Hotel in Shanghai and numerous designs and multimedia installations for Christofle, BMW MINI, J. P. Morgan, Audi, Van Cleef & Arpels and Sergio Rossi among others.\n"}
