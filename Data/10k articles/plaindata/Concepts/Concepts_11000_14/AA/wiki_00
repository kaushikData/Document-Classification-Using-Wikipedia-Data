{"id": "22759797", "url": "https://en.wikipedia.org/wiki?curid=22759797", "title": "Adi (metaphysical plane)", "text": "Adi (metaphysical plane)\n\nAdi (Skt., 'first') is a Hindu, Buddhist, and Theosophical term meaning the first part of reality. It has to do with the first cause or even Adi-Ananta—at least as unity of infinity (as Plotinus and others say) emanating the first finite one. This is called Brahm (in Hinduism) and Adi-Buddha, and Theosophy also says it has to do with the Divine Monad, e.g. Logos.\n\n"}
{"id": "1949268", "url": "https://en.wikipedia.org/wiki?curid=1949268", "title": "Aether (classical element)", "text": "Aether (classical element)\n\nAccording to ancient and medieval science, aether (, \"aither\"), also spelled æther or ether and also called quintessence, is the material that fills the region of the universe above the terrestrial sphere. The concept of aether was used in several theories to explain several natural phenomena, such as the traveling of light and gravity. In the late 19th century, physicists postulated that aether permeated all throughout space, providing a medium through which light could travel in a vacuum, but evidence for the presence of such a medium was not found in the Michelson–Morley experiment, and this result has been interpreted as meaning that no such luminiferous aether exists.\n\nThe word (\"aithēr\") in Homeric Greek means \"pure, fresh air\" or \"clear sky\". In Greek mythology, it was thought to be the pure essence that the gods breathed, filling the space where they lived, analogous to the \"air\" breathed by mortals. It is also personified as a deity, Aether, the son of Erebus and Nyx in traditional Greek mythology. Aether is related to \"to incinerate\", and intransitive \"to burn, to shine\" (related is the name \"Aithiopes\" (Ethiopians; see Aethiopia), meaning \"people with a burnt (black) visage\").\n\nIn Plato's \"Timaeus\" (58d) speaking about air, Plato mentions that \"there is the most translucent kind which is called by the name of aether (αίθηρ)\". but otherwise he adopted the classical system of four elements. Aristotle, who had been Plato's student at the Akademia, agreed on this point with his former mentor, emphasizing additionally that fire sometimes has been mistaken for aether. However, in his Book \"On the Heavens\" he introduced a new \"first\" element to the system of the classical elements of Ionian philosophy. He noted that the four terrestrial classical elements were subject to change and naturally moved linearly. The first element however, located in the celestial regions and heavenly bodies, moved circularly and had none of the qualities the terrestrial classical elements had. It was neither hot nor cold, neither wet nor dry. With this addition the system of elements was extended to five and later commentators started referring to the new first one as the fifth and also called it \"aether\", a word that Aristotle had not used.\n\nAether did not follow Aristotelian physics either. Aether was also incapable of motion of quality or motion of quantity. Aether was only capable of local motion. Aether naturally moved in circles, and had no contrary, or unnatural, motion. Aristotle also noted that crystalline spheres made of aether held the celestial bodies. The idea of crystalline spheres and natural circular motion of aether led to Aristotle's explanation of the observed orbits of stars and planets in perfectly circular motion in crystalline aether.\n\nMedieval scholastic philosophers granted \"aether\" changes of density, in which the bodies of the planets were considered to be more dense than the medium which filled the rest of the universe. Robert Fludd stated that the aether was of the character that it was \"subtler than light\". Fludd cites the 3rd-century view of Plotinus, concerning the aether as penetrative and non-material. See also Arche.\n\nQuintessence is the Latinate name of the fifth element used by medieval alchemists for a medium similar or identical to that thought to make up the heavenly bodies. It was noted that there was very little presence of quintessence within the terrestrial sphere. Due to the low presence of quintessence, earth could be affected by what takes place within the heavenly bodies. This theory was developed in the 14th century text \"The testament of Lullius\", attributed to Ramon Llull. The use of quintessence became popular within medieval alchemy. Quintessence stemmed from the medieval elemental system, which consisted of the four classical elements, and aether, or quintessence, in addition to two chemical elements representing metals: sulphur, \"the stone which burns\", which characterized the principle of combustibility, and mercury, which contained the idealized principle of metallic properties.\n\nThis elemental system spread rapidly throughout all of Europe and became popular with alchemists, especially in medicinal alchemy. Medicinal alchemy then sought to isolate quintessence and incorporate it within medicine and elixirs. Due to quintessence's pure and heavenly quality, it was thought that through consumption one may rid oneself of any impurities or illnesses. In \"The book of Quintessence\", a 15th-century English translation of a continental text, quintessence was used as a medicine for many of man's illnesses. A process given for the creation of quintessence is distillation of alcohol seven times. Over the years, the term quintessence has become synonymous with elixirs, medicinal alchemy, and the philosopher's stone itself.\n\nWith the 18th century physics developments, physical models known as \"aether theories\" made use of a similar concept for the explanation of the propagation of electromagnetic and gravitational forces. As early as the 1670s, Newton used the idea of aether to help match observations to strict mechanical rules of his physics. However, the early modern aether had little in common with the aether of classical elements from which the name was borrowed. These aether theories are considered to be scientifically obsolete, as the development of special relativity showed that Maxwell's equations do not require the aether for the transmission of these forces. However, Einstein himself noted that his own model which replaced these theories could itself be thought of as an aether, as it implied that the empty space between objects had its own physical properties.\n\nDespite the early modern aether models being superseded by general relativity, occasionally some physicists have attempted to reintroduce the concept of aether in an attempt to address perceived deficiencies in current physical models. One proposed model of dark energy has been named \"quintessence\" by its proponents, in honor of the classical element. This idea relates to the hypothecial form of dark energy postulated as an explanation of observations of an accelerating universe. It has also been called a fifth fundamental force.\n\nThe motion of light was a long-standing investigation in physics for hundreds of years before the 20th century. The use of aether to describe this motion was popular during the 17th and 18th centuries, including a theory proposed by Johann II Bernoulli, who was recognized in 1736 with the prize of the French Academy. In his theory, all space is permeated by aether containing \"excessively small whirlpools\". These whirlpools allow for aether to have a certain elasticity, transmitting vibrations from the corpuscular packets of light as they travel through.\n\nThis theory of luminiferous aether would influence the wave theory of light proposed by Christiaan Huygens, in which light traveled in the form of longitudinal waves via an \"omnipresent, perfectly elastic medium having zero density, called aether\". At the time, it was thought that in order for light to travel through a vacuum, there must have been a medium filling the void through which it could propagate, as sound through air or ripples in a pool. Later, when it was proved that the nature of light wave is transverse instead of longitudinal, Huygens' theory was replaced by subsequent theories proposed by Maxwell, Einstein and de Broglie, which rejected the existence and necessity of aether to explain the various optical phenomena. These theories were supported by the results of the Michelson–Morley experiment in which evidence for the motion of aether was conclusively absent. The results of the experiment influenced many physicists of the time and contributed to the eventual development of Einstein's theory of special relativity.\n\nAether has been used in various gravitational theories as a medium to help explain gravitation and what causes it. It was used in one of Sir Isaac Newton's first published theories of gravitation, \"Philosophiæ Naturalis Principia Mathematica\" (the \"Principia\"). He based the whole description of planetary motions on a theoretical law of dynamic interactions. He renounced standing attempts at accounting for this particular form of interaction between distant bodies by introducing a mechanism of propagation through an intervening medium. He calls this intervening medium aether. In his aether model, Newton describes aether as a medium that \"flows\" continually downward toward the Earth's surface and is partially absorbed and partially diffused. This \"circulation\" of aether is what he associated the force of gravity with to help explain the action of gravity in a non-mechanical fashion. This theory described different aether densities, creating an aether density gradient. His theory also explains that aether was dense within objects and rare without them. As particles of denser aether interacted with the rare aether they were attracted back to the dense aether much like cooling vapors of water are attracted back to each other to form water. In the \"Principia\" he attempts to explain the elasticity and movement of aether by relating aether to his static model of fluids. This elastic interaction is what caused the pull of gravity to take place, according to this early theory, and allowed an explanation for action at a distance instead of action through direct contact. Newton also explained this changing rarity and density of aether in his letter to Robert Boyle in 1679. He illustrated aether and its field around objects in this letter as well and used this as a way to inform Robert Boyle about his theory. Although Newton eventually changed his theory of gravitation to one involving force and the laws of motion, his starting point for the modern understanding and explanation of gravity came from his original aether model on gravitation.\n\n"}
{"id": "3409", "url": "https://en.wikipedia.org/wiki?curid=3409", "title": "Being", "text": "Being\n\nIn philosophy, being means the existence of a thing. Anything that exists has being. Ontology is the branch of philosophy that studies being. Being is a concept encompassing objective and subjective features of reality and existence. Anything that partakes in being is also called a \"being\", though often this usage is limited to entities that have subjectivity (as in the expression \"human being\"). The notion of \"being\" has, inevitably, been elusive and controversial in the history of philosophy, beginning in Western philosophy with attempts among the pre-Socratics to deploy it intelligibly. The first effort to recognize and define the concept came from Parmenides, who famously said of it that \"what is-is\". Common words such as \"is\", \"are\", and \"am\" refer directly or indirectly to being.\n\nAs an example of efforts in recent times, Martin Heidegger (who himself drew on ancient Greek sources) adopted after German terms like \"Dasein\" to articulate the topic. Several modern approaches build on such continental European exemplars as Heidegger, and apply metaphysical results to the understanding of human psychology and the human condition generally (notably in the Existentialist tradition). By contrast, in mainstream Analytical philosophy the topic is more confined to abstract investigation, in the work of such influential theorists as W. V. O. Quine, to name one of many. One of the most fundamental questions that continues to exercise philosophers is posed by William James: \"How comes the world to be here at all instead of the nonentity which might be imagined in its place? ... from nothing to being there is no logical bridge.\"\n\nThe deficit of such a bridge was first encountered in history by the Pre-Socratic philosophers during the process of evolving a classification of all beings (noun). Aristotle, who wrote after the Pre-Socratics, applies the term category (perhaps not originally) to ten highest-level classes. They comprise one category of substance (ousiae) existing independently (man, tree) and nine categories of accidents, which can only exist in something else (time, place). In Aristotle, substances are to be clarified by stating their definition: a note expressing a larger class (the genus) followed by further notes expressing specific differences (differentiae) within the class. The substance so defined was a species. For example, the species, man, may be defined as an animal (genus) that is rational (difference). As the difference is potential within the genus; that is, an animal may or may not be rational, the difference is not identical to, and may be distinct from, the genus.\n\nApplied to being, the system fails to arrive at a definition for the simple reason that no difference can be found. The species, the genus, and the difference are all equally being: a being is a being that is being. The genus cannot be nothing because nothing is not a class of everything. The trivial solution that being is being added to nothing is only a tautology: being is being. There is no simpler intermediary between being and non-being that explains and classifies being.\nPre-Socratic reaction to this deficit was varied. As substance theorists they accepted a priori the hypothesis that appearances are deceiving, that reality is to be reached through reasoning. Parmenides reasoned that if everything is identical to being and being is a category of the same thing then there can be neither differences between things nor any change. To be different, or to change, would amount to becoming or being non-being; that is, not existing. Therefore, being is a homogeneous and non-differentiated sphere and the appearance of beings is illusory. Heraclitus, on the other hand, foreshadowed modern thought by denying existence. Reality does not exist, it flows, and beings are an illusion upon the flow.\n\nAristotle knew of this tradition when he began his \"Metaphysics\", and had already drawn his own conclusion, which he presented under the guise of asking what being is:\"And indeed the question which was raised of old is raised now and always, and is always the subject of doubt, viz., what being is, is just the question, what is substance? For it is this that some assert to be one, others more than one, and that some assert to be limited in number, others unlimited. And so we also must consider chiefly and primarily and almost exclusively what that is which is in this sense.\"\n\nand reiterates in no uncertain terms: \"Nothing, then, which is not a species of a genus will have an essence – only species will have it ...\". Being, however, for Aristotle, is not a genus.\n\nOne might expect a solution to follow from such certain language but none does. Instead Aristotle launches into a rephrasing of the problem, the Theory of Act and Potency. In the definition of man as a two-legged animal Aristotle presumes that \"two-legged\" and \"animal\" are parts of other beings, but as far as man is concerned, are only potentially man. At the point where they are united into a single being, man, the being, becomes actual, or real. Unity is the basis of actuality: \"... 'being' is being combined and one, and 'not being' is being not combined but more than one.\" Actuality has taken the place of existence, but Aristotle is no longer seeking to know what the actual is; he accepts it without question as something generated from the potential. He has found a \"half-being\" or a \"pre-being\", the potency, which is fully being as part of some other substance. Substances, in Aristotle, unite what they actually are now with everything they might become.\n\nSome of Thomas Aquinas' propositions were reputedly condemned by Étienne Tempier, the local Bishop of Paris (not the Papal Magisterium itself) in 1270 and 1277, but his dedication to the use of philosophy to elucidate theology was so thorough that he was proclaimed a Doctor of the Church in 1568. Those who adopt it are called Thomists.\n\nIn a single sentence, parallel to Aristotle's statement asserting that being is substance, St. Thomas pushes away from the Aristotelian doctrine: \"Being is not a genus, since it is not predicated univocally but only analogically.\" His term for analogy is Latin \"analogia\". In the categorical classification of all beings, all substances are partly the same: man and chimpanzee are both animals and the animal part in man is \"the same\" as the animal part in chimpanzee. Most fundamentally all substances are matter, a theme taken up by science, which postulated one or more matters, such as earth, air, fire or water (Empedocles). In today's chemistry the carbon, hydrogen, oxygen and nitrogen in a chimpanzee are identical to the same elements in a man.\n\nThe original text reads, \"Although equivocal predications must be reduced to univocal, still in actions, the non-univocal agent must precede the univocal agent. For the non-univocal agent is the universal cause of the whole species, as for instance the sun is the cause of the generation of all men; whereas the univocal agent is not the universal efficient cause of the whole species (otherwise it would be the cause of itself, since it is contained in the species), but is a particular cause of this individual which it places under the species by way of participation. Therefore the universal cause of the whole species is not an univocal agent; and the universal cause comes before the particular cause. But this universal agent, whilst it is not univocal, nevertheless is not altogether equivocal, otherwise it could not produce its own likeness, but rather it is to be called an analogical agent, as all univocal predications are reduced to one first non-univocal analogical predication, which is being.\"\n\nIf substance is the highest category and there is no substance, being, then the unity perceived in all beings by virtue of their existing must be viewed in another way. St. Thomas chose the analogy: all beings are like, or analogous to, each other in existing. This comparison is the basis of his Analogy of Being. The analogy is said of being in many different ways, but the key to it is the real distinction between existence and essence. Existence is the principle that gives reality to an essence not the same in any way as the existence: \"If things having essences are real, and it is not of their essence to be, then the reality of these things must be found in some principle other than (really distinct from) their essence.\" Substance can be real or not. What makes an individual substance – a man, a tree, a planet – real is a distinct act, a \"to be\", which actuates its unity. An analogy of proportion is therefore possible: \"essence is related to existence as potency is related to act.\"\n\nExistences are not things; they do not themselves exist, they lend themselves to essences, which do not intrinsically have them. They have no nature; an existence receives its nature from the essence it actuates. Existence is not being; it gives being – here a customary phrase is used, existence is a principle (a source) of being, not a previous source, but one which is continually in effect. The stage is set for the concept of God as the cause of all existence, who, as the Almighty, holds everything actual without reason or explanation as an act purely of will.\n\nAristotle's classificatory scheme had included the five predicables, or characteristics that might be predicated of a substance. One of these was the property, an essential universal true of the species, but not in the definition (in modern terms, some examples would be grammatical language, a property of man, or a spectral pattern characteristic of an element, both of which are defined in other ways). Pointing out that predicables are predicated univocally of substances; that is, they refer to \"the same thing\" found in each instance, St. Thomas argued that whatever can be said about being is not univocal, because all beings are unique, each actuated by a unique existence. It is the analogous possession of an existence that allows them to be identified as being; therefore, being is an analogous predication.\n\nWhatever can be predicated of all things is universal-like but not universal, category-like but not a category. St. Thomas called them (perhaps not originally) the \"transcendentia\", \"transcendentals\", because they \"climb above\" the categories, just as being climbs above substance. Later academics also referred to them as \"the properties of being.\" The number is generally three or four.\n\nThe nature of \"being\" has also been debated and explored in Islamic philosophy, notably by Ibn Sina (Avicenna), Suhrawardi, and Mulla Sadra. A modern linguistic approach which notices that Persian language has exceptionally developed two kinds of \"is\"es, i.e. \"ast\" (\"is\", as a copula) and \"hast\" (as an existential \"is\") examines the linguistic properties of the two lexemes in the first place, then evaluates how the statements made by other languages with regard to \"being\" can stand the test of Persian frame of reference.\n\nIn this modern linguistic approach, it is noticed that the original language of the source, e.g. Greek (like German or French or English), has only one word for two concepts, \"ast\" and \"hast\", or, like Arabic, has no word at all for either word. It therefore exploits the Persian \"hast\" (existential \"is\") versus \"ast\" (predicative \"is\" or copula) to address both Western and Islamic ontological arguments on \"being\" and \"existence\".\n\nThis linguistic method shows the scope of confusion created by languages which cannot differentiate between existential be and copula. It manifests, for instance, that the main theme of Heidegger's \"Being and Time\" is \"astī\" (is-ness) rather than \"hastī\" (existence). When, in the beginning of his book, Heidegger claims that people always talk about existence in their everyday language, without knowing what it means, the example he resorts to is: \"the sky \"is\" blue\" which in Persian can be ONLY translated with the use of the copula \"ast\", and says nothing about \"being\" or \"existence\".\n\nIn the same manner, the linguistic method addresses the ontological works written in Arabic. Since Arabic, like Latin in Europe, had become the official language of philosophical and scientific works in the so-called Islamic World, the early Persian or Arab philosophers had difficulty discussing \"being\" or \"existence\", since the Arabic language, like other Semitic languages, had no verb for either predicative \"be\" (copula) or existential \"be\". So if you try to translate the aforementioned Heidegger's example into Arabic it appears as السماء زرقاء (viz. \"The Sky-- blue\") with no linking \"is\" to be a sign of existential statement. To overcome the problem, when translating the ancient Greek philosophy, certain words were coined like ایس \"aysa\" (from Arabic لیس \"laysa\" 'not') for 'is'. Eventually the Arabic verb وجد \"wajada\" (to find) prevailed, since it was thought that whatever is existent, is to be \"found\" in the world. Hence \"existence\" or Being was called وجود \"wujud\" (Cf. Swedish \"finns\" [found]> there exist; also the Medieval Latin coinage of \"exsistere\" 'standing out (there in the world)' > appear> exist). Now, with regard to the fact that Persian, as the mother tongue of both Avicenna and Sadrā, was in conflict with either Greek or Arabic in this regard, these philosophers should have been warned implicitly by their mother tongue not to confuse two kinds of linguistic beings (viz. copula vs. existential). In fact when analyzed thoroughly, copula, or Persian \"ast\" ('is') indicates an ever-moving chain of relations with no fixed \"entity\" to hold onto (every \"entity\", say A, will be dissolved into \"A is B\" and so on, as soon as one tries to define it). Therefore, the whole reality or what we see as existence (\"found\" in our world) resembles an ever-changing world of \"astī\" (is-ness) flowing in time and space. On the other hand, while Persian \"ast\" can be considered as the 3rd person singular of the verb 'to be', there is no verb but an arbitrary one supporting \"hast\" ('is' as an existential be= exists) has neither future nor past tense and nor a negative form of its own: \"hast\" is just a single untouchable lexeme. It needs no other linguistic element to be complete (\"Hast.\" is a complete sentence meaning \"s/he it exists\"). In fact, any manipulation of the arbitrary verb, e.g. its conjugation, turns \"hast\" back into a copula.\n\nEventually from such linguistic analyses, it appears that while \"astī\" (is-ness) would resemble the world of Heraclitus, \"hastī\" (existence) would rather approaches a metaphysical concept resembling the Parmenidas's interpretation of \"existence\".\n\nIn this regard, Avicenna, who was a firm follower of Aristotle, could not accept either Heraclitian \"is-ness\" (where only constant was \"change\"), nor Parmenidean \"monist immoveable existence\" (the \"hastī\" itself being constant). To solve the contradiction, it so appeared to Philosophers of Islamic world that Aristotle considered the core of existence (i.e. its \"substance\"/\"essence\") as a fixed constant, while its facade (accident) was prone to change. To translate such a philosophical image into Persian it is like having \"hastī\" (existence) as a unique constant core covered by \"astī\" (is-ness) as a cloud of ever-changing relationships. It is clear that the Persian language, deconstructs such a composite as a sheer mirage, since it is not clear how to link the interior core (existence) with the exterior shell (is-ness). Furthermore, \"hast\" cannot be linked to anything but itself (as it is self-referent).\n\nThe argument has a theological echos as well: assuming that God is the \"Existence\", beyond time and space, a question is raised by philosophers of the Islamic world as how he, as a transcendental existence, may ever create or contact a world of \"is-ness\" in space-time.\n\nHowever, Avicenna who was more philosopher than theologian, followed the same line of argumentation as that of his ancient master, Aristotle, and tried to reconcile between \"ast\" and \"hast\", by considering the latter as higher order of existence than the former. It is like a hierarchical order of existence. It was a philosophical Tower of Babel that the restriction of his own mother tongue (Persian) would not allow to be built, but he could maneuver in Arabic by giving the two concepts the same name \"wujud\", although with different attributes. So, implicitly, \"astī\" (is-ness) appears as ممکن الوجود \"momken-al-wujud\" (contingent being), and \"hastī\" (existence) as واجب الوجود \"wājeb-al-wujud\" (necessary being).\n\nOn the other hand, centuries later, Sadrā, chose a more radical route, by inclining towards the reality of \"astī\" (is-ness), as the true mode of existence, and tried to get rid of the concept of \"hastī\" (existence as fixed or immovable). Thus, in his philosophy, the universal movement penetrates deep into the Aristotelian \"substance\"/\"essence\", in unison with changing accident. He called this deep existential change حرکت جوهری \"harekat-e jowhari\" (Substantial Movement). In such a changing existence, the whole world has to go through instantaneous annihilation and recreation incessantly, while as Avicenna had predicted in his remarks on Nature, such a universal change or substantial movement would eventually entail the shortening and lengthening of time as well which has never been observed. This logical objection, which was made on Aristotle's argumentation, could not be answered in the ancient times or medieval age, but now it does not sound contradictory to the real nature of Time (as addressed in relativity theory), so by a reverse argument, a philosopher may indeed deduce that everything is changing (moving) even in the deepest core of Being.\n\nAlthough innovated in the late medieval period, Thomism was dogmatized in the Renaissance. From roughly 1277 to 1567, it dominated the philosophic landscape. The rationalist philosophers, however, with a new emphasis on Reason as a tool of the intellect, brought the classical and medieval traditions under new scrutiny, exercising a new concept of doubt, with varying outcomes. Foremost among the new doubters were the empiricists, the advocates of scientific method, with its emphasis on experimentation and reliance on evidence gathered from sensory experience. In parallel with the revolutions against rising political absolutism based on established religion and the replacement of faith by reasonable faith, new systems of metaphysics were promulgated in the lecture halls by charismatic professors, such as Immanuel Kant, and Hegel. The late 19th and 20th centuries featured an emotional return to the concept of existence under the name of existentialism. These philosophers were concerned mainly with ethics and religion. The metaphysical side became the domain of the phenomenalists. In parallel with these philosophies Thomism continued under the protection of the Catholic Church; in particular, the Jesuit order.\n\nRationalism and empiricism have had many definitions, most concerned with specific schools of philosophy or groups of philosophers in particular countries, such as Germany. In general rationalism is the predominant school of thought in the multi-national, cross-cultural Age of reason, which began in the century straddling 1600 as a conventional date, empiricism is the reliance on sensory data gathered in experimentation by scientists of any country, who, in the Age of Reason were rationalists. An early professed empiricist, Thomas Hobbes, known as an eccentric denizen of the court of Charles II of England (an \"old bear\"), published in 1651 \"Leviathan\", a political treatise written during the English civil war, containing an early manifesto in English of rationalism.\nHobbes said:\"The Latines called Accounts of mony Rationes ... and thence it seems to proceed that they extended the word Ratio, to the faculty of Reckoning in all other things...When a man reasoneth hee does nothing else but conceive a summe totall ... For Reason ... is nothing but Reckoning ... of the consequences of generall names agreed upon, for the marking and signifying of our thoughts ...\"\nIn Hobbes reasoning is the right process of drawing conclusions from definitions (the \"names agreed upon\"). He goes on to define error as self-contradiction of definition (\"an absurdity, or senselesse Speech\") or conclusions that do not follow the definitions on which they are supposed to be based. Science, on the other hand, is the outcome of \"right reasoning,\" which is based on \"natural sense and imagination\", a kind of sensitivity to nature, as \"nature it selfe cannot erre.\"\n\nHaving chosen his ground carefully Hobbes launches an epistemological attack on metaphysics. The academic philosophers had arrived at the Theory of Matter and Form from consideration of certain natural paradoxes subsumed under the general heading of the Unity Problem. For example, a body appears to be one thing and yet it is distributed into many parts. Which is it, one or many? Aristotle had arrived at the real distinction between matter and form, metaphysical components whose interpenetration produces the paradox. The whole unity comes from the substantial form and the distribution into parts from the matter. Inhering in the parts giving them really distinct unities are the accidental forms. The unity of the whole being is actuated by another really distinct principle, the existence.\n\nIf nature cannot err, then there are no paradoxes in it; to Hobbes, the paradox is a form of the absurd, which is inconsistency: \"Natural sense and imagination, are not subject to absurdity\" and \"For error is but a deception ... But when we make a generall assertion, unlesse it be a true one, the possibility of it is inconceivable. And words whereby we conceive nothing but the sound, are those we call Absurd ...\" Among Hobbes examples are \"round quadrangle\", \"immaterial substance\", \"free subject.\" Of the scholastics he says:\"Yet they will have us beleeve, that by the Almighty power of God, one body may be at one and the same time in many places [the problem of the universals]; and many bodies at one and the same time in one place [the whole and the parts]; ... And these are but a small part of the Incongruencies they are forced to, from their disputing philosophically, instead of admiring, and adoring of the Divine and Incomprehensible Nature ...\"\n\nThe real distinction between essence and existence, and that between form and matter, which served for so long as the basis of metaphysics, Hobbes identifies as \"the Error of Separated Essences.\" The words \"Is, or Bee, or Are, and the like\" add no meaning to an argument nor do derived words such as \"Entity, Essence, Essentially, Essentiality\", which \"are the names of nothing\" but are mere \"Signes\" connecting \"one name or attribute to another: as when we say, \"a man is a living body\", we mean not that the \"man\" is one thing, the \"living body\" another, and the \"is\", or \"being\" a third: but that the \"man\", and the \"living body\", is the same thing; ...\" Metaphysiques, Hobbes says, is \"far from the possibility of being understood\" and is \"repugnant to natural reason.\"\n\nBeing to Hobbes (and the other empiricists) is the physical universe:The world, (I mean ... the Universe, that is, the whole masse of all things that are) is corporeall, that is to say, Body; and hath the dimension of magnitude, namely, Length, Bredth and Depth: also every part of Body, is likewise Body ... and consequently every part of the Universe is Body, and that which is not Body, is no part of the Universe: and because the Universe is all, that which is no part of it is nothing; and consequently no where.\"\n\nHobbes' view is representative of his tradition. As Aristotle offered the categories and the act of existence, and Aquinas the analogy of being, the rationalists also had their own system, the great chain of being, an interlocking hierarchy of beings from God to dust.\n\nIn addition to the materialism of the empiricists, under the same aegis of Reason, rationalism produced systems that were diametrically opposed now called idealism, which denied the reality of matter in favor of the reality of mind. By a 20th-century classification, the idealists (Kant, Hegel and others), are considered the beginning of continental philosophy, while the empiricists are the beginning, or the immediate predecessors, of analytical philosophy. \n\nSome philosophers deny that the concept of \"being\" has any meaning at all, since we only define an object's existence by its relation to other objects, and actions it undertakes. The term \"I am\" has no meaning by itself; it must have an action or relation appended to it. This in turn has led to the thought that \"being\" and nothingness are closely related, developed in existential philosophy.\n\nExistentialist philosophers such as Sartre, as well as continental philosophers such as Hegel and Heidegger have also written extensively on the concept of being. Hegel distinguishes between the being of objects (being in itself) and the being of people (\"Geist)\". Hegel, however, did not think there was much hope for delineating a \"meaning\" of being, because being stripped of all predicates is simply nothing.\n\nHeidegger, in his quest to re-pose the original pre-Socratic question of Being, wondered at how to meaningfully ask the question of the meaning of being, since it is both the greatest, as it includes everything that is, and the least, since no particular thing can be said of it. He distinguishes between different modes of beings: a privative mode is present-at-hand, whereas beings in a fuller sense are described as ready-to-hand. The one who asks the question of Being is described as Da-sein (\"there/here-being\") or being-in-the-world. Sartre, popularly understood as misreading Heidegger (an understanding supported by Heidegger's essay \"Letter on Humanism\" which responds to Sartre's famous address, \"Existentialism is a Humanism\"), employs modes of being in an attempt to ground his concept of freedom ontologically by distinguishing between being-in-itself and being-for-itself.\n\nBeing is also understood as one's \"state of being,\" and hence its common meaning is in the context of human (personal) experience, with aspects that involve expressions and manifestations coming from an innate \"being\", or personal character. Heidegger coined the term \"dasein\" for this property of being in his influential work \"Being and Time\" (\"this entity which each of us is himself…we shall denote by the term 'dasein.'\"), in which he argued that being or \"dasein\" links one's sense of one's body to one's perception of world. Heidegger, amongst others, referred to an innate language as the foundation of being, which gives signal to all aspects of being.\n\n\nPhilosophers\n\n"}
{"id": "35475501", "url": "https://en.wikipedia.org/wiki?curid=35475501", "title": "Birks' law", "text": "Birks' law\n\nBirks' law (named after British physicist John B. Birks) is an empirical formula for the light yield per path length as a function of the energy loss per path length for a particle traversing a scintillator, and gives a relation that is not linear at high loss rates.\n\nThe relation is:\nwhere \"L\" is the light yield, \"S\" is the scintillation efficiency, \"dE/dx\" is the energy loss of the particle per path length, and \"k\" is Birks' constant, which depends on the material. \"k\" is 0.126 mm/MeV for polystyrene-based scintillators and 1.26–2.07 × 10 g/(MeV cm) for polyvinyltoluene-based scintillators.\n\nBirks speculated that the loss of linearity is due to recombination and quenching effects between the excited molecules and the surrounding substrate. Birks' law has mostly been tested for organic scintillators. Its applicability to inorganic scintillators is debated. A good discussion can be found in \"Particle Detectors at Accelerators: Organic scintillators\". A compilation of Birks' constant for various materials can be found in \"Semi-empirical calculation of quenching factors for ions in scintillators\". A more complete theory of scintillation saturation, that gives Birks' law when only unimolecular de-excitation is included, can be found in a paper by Blanc, Cambou, and De Laford.\n"}
{"id": "31211209", "url": "https://en.wikipedia.org/wiki?curid=31211209", "title": "Breath gas analysis", "text": "Breath gas analysis\n\nBreath gas analysis is a method for gaining non-invasive information on the clinical state of an individual by monitoring volatile organic compounds present in the exhaled breath. Breath gas concentration can then be related to blood concentrations via mathematical modeling as for example in blood alcohol testing.\n\nThe area of modern breath testing commenced in 1971, when Nobel Prize winner Linus Pauling demonstrated that human breath is a complex gas, containing more than 200 different volatile organic compounds. However, physicians have used breath analysis since the days of Hippocrates.\n\nEndogenous volatile organic compounds (VOCs) are released within the human organism as a result of normal metabolic activity or due to pathological disorders. They enter the blood stream and are eventually metabolized or excreted via exhalation, skin emission, urine, etc.\n\nBreath sampling is non-invasive and breath samples can be extracted as often as desired.\n\nIdentification and quantification of potential disease biomarkers can be seen as the driving force for the analysis of exhaled breath. Moreover, future applications for medical diagnosis and therapy control with dynamic assessments of normal physiological function or pharmacodynamics are intended.\n\nExogenous VOCs penetrating the body as a result of environmental exposure can be used to quantify body burden. Also breath tests are often based on the ingestion of isotopically labeled precursors, producing isotopically labeled carbon dioxide and potentially many other metabolites.\n\nHowever, breath sampling is far from being a standardized procedure due to the numerous confounding factors biasing the concentrations of volatiles in breath. These factors are related to both the breath sampling protocols as well as the complex physiological mechanisms underlying pulmonary gas exchange. Even under resting conditions exhaled breath concentrations of VOCs can strongly be influenced by specific physiological parameters such as cardiac output and breathing patterns, depending on the physico-chemical properties of the compound under study.\n\nUnderstanding the influence of all this factors and their control is necessary for achieving an accurate standardization of breath sample collection and for the correct deduction of the corresponding blood concentration levels.\n\nThe simplest model relating breath gas concentration to blood concentrations was developed by Farhi\nwhere formula_2 denotes the alveolar concentration which is assumed to be equal to the measured concentration.\nIt expresses the fact that the concentration of an inert gas in the alveolar air depends on the mixed venous concentration formula_3, the substance-specific blood:air partition coefficient formula_4, and the ventilation-perfusion ratio formula_5.\nBut this model fails when two prototypical substances like acetone (partition coefficient formula_6) or isoprene (partition coefficient formula_7 ) are measured.\n\nE.g., multiplying the proposed population mean of approximately formula_8 acetone in end-tidal breath by the partition coefficient formula_6 at body temperature grossly underestimates observed (arterial) blood levels spreading around formula_10. Furthermore, breath profiles of acetone (and other highly soluble volatile compounds such as 2-pentanone or methyl acetate) associated with moderate workload ergometer challenges of normal healthy volunteers drastically depart from the trend suggested by the equation above.\n\nHence some more refined models are necessary. Such models have been developed recently.\n\nBreath gas analysis is used in a number of breath tests.\n\n\nBreath can be collected using a variety of home-made and commercially available devices. The three basic types of breath collector for VOC analysis are:\n\n\nEach of these can be used as a vehicle for direct introduction of a gas sample into an appropriate analytical instrument, or serve as a reservoir of breath gas into which an absorption device such as an SPME fiber is placed to collect specific compounds.\n\nBreath analysis can be done with various forms of mass spectrometry, but there are also simpler methods for specific purposes, such as the Halimeter and the breathalyzer.\n\n\n"}
{"id": "225193", "url": "https://en.wikipedia.org/wiki?curid=225193", "title": "Carl Adam Petri", "text": "Carl Adam Petri\n\nCarl Adam Petri (12 July 1926 – 2 July 2010) was a German mathematician and computer scientist. He was born in Leipzig.\n\nPetri created his major scientific contribution, the concept of the Petri net, in 1939 at the age of 13, for the purpose of describing chemical processes. In 1941 his father told him about Konrad Zuse's work on computing machines and Carl Adam started building his own analog computer.\n\nAfter earning his Abitur at the Thomasschule he was in 1944 drafted into the Wehrmacht and eventually went into British captivity.\n\nPetri started studying mathematics at the Technischen Hochschule Hannover (today, the University of Hanover) in 1950. He documented the Petri net in 1962 as part of his dissertation, \"Kommunikation mit Automaten\" (communication with automata). He worked from 1959 until 1962 at the University of Bonn and received his PhD degree in 1962 from the Darmstadt University of Technology. From 1963 to 1968 he directed the computing centre of Bonn University. In 1968 he became head of \"Forschungsinstitut für Informationssysteme\" of the newly founded Gesellschaft für Mathematik und Datenverarbeitung (GMD). He retired in 1991.\n\nIn 1988 Petri became honorary professor of the University of Hamburg. He was a member of the Academia Europaea.\n\nPetri's work significantly advanced the fields of parallel computing and distributed computing, and it helped define the modern studies of complex systems and workflow management systems. His contributions have been in the broader area of network theory which includes coordination models and theories of interaction, and eventually led to the formal study of software connectors.\n\nPetri was honored with the following awards:\n\n\n\n"}
{"id": "4484964", "url": "https://en.wikipedia.org/wiki?curid=4484964", "title": "Copwatch", "text": "Copwatch\n\nCopwatch (also Cop Watch) is a network of activist organizations, typically autonomous and focused in local areas, in the United States and Canada (and to a lesser extent Europe) that observe and document police activity while looking for signs of police misconduct and police brutality. They believe that monitoring police activity on the streets is a way to prevent police brutality.\n\nThe stated goal of at least one Copwatch group is to engage in monitoring and videotaping police activity in the interest of holding the police accountable in the events involving assaults or police misconduct.\n\nCopwatch was first started in Berkeley, California in 1990.\n\nThe main function of most Copwatch groups is monitoring police activity. \"Copwatchers\" go out on foot or driving patrols in their communities and record interactions between the police and civilians. Copwatchers hope that monitoring police activity will provide a deterrent against police misconduct. Some groups also patrol at protests and demonstrations to ensure that police do not violate the rights of protesters. One Copwatch organization states that it has a policy of non-interference with the police, although this may not be true for all groups. In Phoenix, Arizona, copwatchers have increased efforts of \"reverse surveillance\" on the police in an effort to document racial profiling. They believe that Arizona Senate Bill 1070, a controversial law that allows police to question people they believe are illegal immigrants, will increase racial profiling by police.\n\nCopwatch groups also hold \"Know Your Rights\" forums to educate the public about their legal and human rights when interacting with the police, and some groups organize events to highlight problems of police abuse in their communities.\n\nIn 2003, Kendra James was fatally shot by Portland, Oregon Officer Scott McCollister as she attempted to drive away from a traffic stop with Officer McCollister attempting to pull her out of the vehicle. After the shooting Copwatch offered a reward for a photograph of McCollister. It then produced and distributed posters bearing McCollister's photo and the phrase \"Getting away with murder\".\n\nThe editorial staff of Willamette Week opined that the poster was \"inflamed rhetoric\" which would harm \"the relationship between the Portland police and the community it serves\", and claimed that protest posters put up by the Rose City chapter of Copwatch were aimed at \"inciting generalized anti-cop hysteria at the expense of informed criticism\".\n\nA member of the Rose City Copwatch group, which seeks to \"disrupt the polices' ability to enforce race and class lines\", says that the shooting \"demonstrate[s] a culture of racism and brutality that's really sort of at the core of policing\". A grand jury later found no criminal wrongdoing on McCollister's part.\n\nOn November 3, 2006, CopWatch LA posted a video showing the arrest of William Cardenas, whom police described as \"a known gang member who had been wanted on a felony warrant for receiving stolen property\". According to the arrest report, when officers tried to arrest Cardenas as he was drinking beer on the sidewalk with two others, he fled, but was caught and tripped by the officers, who then began to attempt to handcuff Cardenas as he fought with the officers to avoid being arrested.\n\nThe video, in which Cardenas struggles to prevent the police from handcuffing him, shows an officer repeatedly punching him in the face while trying to force his hands together. The officers indicated that they were unable to subdue Cardenas with pepper spray, which seemed to have \"little effect\", and that some of the punches were delivered in response to Cardenas putting his hand on one officer's gun holster during the struggle. According to the arrest report, several witnesses confirmed that Cardenas threw punches at the officers, who were only able to handcuff him after two of his friends arrived and told him to stop fighting.\n\nThe circulation of this video led to nationwide media coverage of Copwatch, and, although the LAPD had begun a use-of-force investigation the same day as the arrest, prompted an additional investigation into police conduct by the Federal Bureau of Investigation. A Superior Court commissioner had previously concluded that the use of force was reasonable because Cardenas was resisting arrest.\n\nIn 2013 Berkeley Copwatch was awarded the James Madison Freedom of Information Award by the Society of Professional Journalists, Northern California chapter, for \"effective use of public records to block a Homeland Security grant for putting an armored military vehicle on the streets of Albany and Berkeley.\"\n\nJoe Arpaio, the controversial Sheriff of Maricopa County, Arizona, has said that his opponents' videotaping of police during traffic stops \"create safety concerns for his deputies.\"\n\nTim Dees, former police officer and editor-in-chief of Officer.com, alleges that Copwatch selectively distributes video and photographic media to \"spin\" incidents against law enforcement.\n\nThe following is an inexhaustive list of local Copwatch organizations\n\nOn 2 August 2016, the BBC documentary \"NYPD: Biggest Gang in New York?\" aired on the British television channel BBC One, focusing on the activities of cop watchers in New York, including Ramsey Orta who filmed the death of Eric Garner.\n\nThe documentary film \"Copwatch\" premiered at the 2017 Tribeca Film Festival, which depicted the organization WeCopwatch, including segments on Ramsey Orta, Kevin Moore, who filmed the police abuse of Freddie Gray, and David Whitt who lived in the apartment complex where Michael Brown was killed, as well as Jacob Crawford, who seeded and co-founded Copwatch groups inspired by the Berkeley Copwatch group.\n\n\n\n\n"}
{"id": "25471054", "url": "https://en.wikipedia.org/wiki?curid=25471054", "title": "Correlation diagram", "text": "Correlation diagram\n\nTerms such as correlation diagram(s), diagram(s) of correlation, and the like may refer to:\n\nIn chemistry, there are several types of correlation diagrams:\n\n"}
{"id": "19159508", "url": "https://en.wikipedia.org/wiki?curid=19159508", "title": "Culture", "text": "Culture\n\nCulture () is the social behavior and norms found in human societies. Culture is considered a central concept in anthropology, encompassing the range of phenomena that are transmitted through social learning in human societies. Cultural universals are found in all human societies; these include expressive forms like art, music, dance, ritual, religion, and technologies like tool usage, cooking, shelter, and clothing. The concept of material culture covers the physical expressions of culture, such as technology, architecture and art, whereas the immaterial aspects of culture such as principles of social organization (including practices of political organization and social institutions), mythology, philosophy, literature (both written and oral), and science comprise the intangible cultural heritage of a society.\n\nIn the humanities, one sense of culture as an attribute of the individual has been the degree to which they have cultivated a particular level of sophistication in the arts, sciences, education, or manners. The level of cultural sophistication has also sometimes been seen to distinguish civilizations from less complex societies. Such hierarchical perspectives on culture are also found in class-based distinctions between a high culture of the social elite and a low culture, popular culture, or folk culture of the lower classes, distinguished by the stratified access to cultural capital. In common parlance, culture is often used to refer specifically to the symbolic markers used by ethnic groups to distinguish themselves visibly from each other such as body modification, clothing or jewelry. Mass culture refers to the mass-produced and mass mediated forms of consumer culture that emerged in the 20th century. Some schools of philosophy, such as Marxism and critical theory, have argued that culture is often used politically as a tool of the elites to manipulate the lower classes and create a false consciousness, and such perspectives are common in the discipline of cultural studies. In the wider social sciences, the theoretical perspective of cultural materialism holds that human symbolic culture arises from the material conditions of human life, as humans create the conditions for physical survival, and that the basis of culture is found in evolved biological dispositions.\n\nWhen used as a count noun, a \"culture\" is the set of customs, traditions, and values of a society or community, such as an ethnic group or nation. Culture is the set of knowledge acquired over time. In this sense, multiculturalism values the peaceful coexistence and mutual respect between different cultures inhabiting the same planet. Sometimes \"culture\" is also used to describe specific practices within a subgroup of a society, a subculture (e.g. \"bro culture\"), or a counterculture. Within cultural anthropology, the ideology and analytical stance of cultural relativism holds that cultures cannot easily be objectively ranked or evaluated because any evaluation is necessarily situated within the value system of a given culture.\n\nThe modern term \"culture\" is based on a term used by the Ancient Roman orator Cicero in his \"Tusculanae Disputationes\", where he wrote of a cultivation of the soul or \"cultura animi,\" using an agricultural metaphor for the development of a philosophical soul, understood teleologically as the highest possible ideal for human development. Samuel Pufendorf took over this metaphor in a modern context, meaning something similar, but no longer assuming that philosophy was man's natural perfection. His use, and that of many writers after him, \"refers to all the ways in which human beings overcome their original barbarism, and through artifice, become fully human.\"\n\nIn 1986, philosopher Edward S. Casey wrote, \"The very word \"culture\" meant 'place tilled' in Middle English, and the same word goes back to Latin \"colere\", 'to inhabit, care for, till, worship' and \"cultus\", 'A cult, especially a religious one.' To be cultural, to have a culture, is to inhabit a place sufficiently intensive to cultivate it—to be responsible for it, to respond to it, to attend to it caringly.\"\n\nCulture described by Richard Velkley: ... originally meant the cultivation of the soul or mind, acquires most of its later modern meaning in the writings of the 18th-century German thinkers, who were on various levels developing Rousseau's criticism of \"modern liberalism and Enlightenment\". Thus a contrast between \"culture\" and \"civilization\" is usually implied in these authors, even when not expressed as such.\n\nIn the words of anthropologist E.B. Tylor, it is \"that complex whole which includes knowledge, belief, art, morals, law, custom and any other capabilities and habits acquired by man as a member of society.\" Alternatively, in a contemporary variant, \"Culture is defined as a social domain that emphasizes the practices, discourses and material expressions, which, over time, express the continuities and discontinuities of social meaning of a life held in common.\n\nThe \"Cambridge English Dictionary\" states that culture is \"the way of life, especially the general customs and beliefs, of a particular group of people at a particular time.\" Terror management theory posits that culture is a series of activities and worldviews that provide humans with the basis for perceiving themselves as \"person[s] of worth within the world of meaning\"—raising themselves above the merely physical aspects of existence, in order to deny the animal insignificance and death that \"Homo sapiens\" became aware of when they acquired a larger brain.\n\nThe word is used in a general sense as the evolved ability to categorize and represent experiences with symbols and to act imaginatively and creatively. This ability arose with the evolution of behavioral modernity in humans around 50,000 years ago, and is often thought to be unique to humans, although some other species have demonstrated similar, though much less complex, abilities for social learning. It is also used to denote the complex networks of practices and accumulated knowledge and ideas that is transmitted through social interaction and exist in specific human groups, or cultures, using the plural form.\n\nIt has been estimated from archaeological data that the human capacity for cumulative culture emerged somewhere between 500,000–170,000 years ago.\n\nRaimon Panikkar identified 29 ways in which cultural change can be brought about, including growth, development, evolution, involution, renovation, reconception, reform, innovation, revivalism, revolution, mutation, progress, diffusion, osmosis, borrowing, eclecticism, syncretism, modernization, indigenization, and transformation. In this context, modernization could be viewed as adoption of Enlightenment era beliefs and practices, such as science, rationalism, industry, commerce, democracy, and the notion of progress. Rein Raud, building on the work of Umberto Eco, Pierre Bourdieu and Jeffrey C. Alexander, has proposed a model of cultural change based on claims and bids, which are judged by their cognitive adequacy and endorsed or not endorsed by the symbolic authority of the cultural community in question.\n\nCultural invention has come to mean any innovation that is new and found to be useful to a group of people and expressed in their behavior but which does not exist as a physical object. Humanity is in a global \"accelerating culture change period,\" driven by the expansion of international commerce, the mass media, and above all, the human population explosion, among other factors. Culture repositioning means the reconstruction of the cultural concept of a society.\nCultures are internally affected by both forces encouraging change and forces resisting change. These forces are related to both social structures and natural events, and are involved in the perpetuation of cultural ideas and practices within current structures, which themselves are subject to change. (See structuration.)\n\nSocial conflict and the development of technologies can produce changes within a society by altering social dynamics and promoting new cultural models, and spurring or enabling generative action. These social shifts may accompany ideological shifts and other types of cultural change. For example, the U.S. feminist movement involved new practices that produced a shift in gender relations, altering both gender and economic structures. Environmental conditions may also enter as factors. For example, after tropical forests returned at the end of the last ice age, plants suitable for domestication were available, leading to the invention of agriculture, which in turn brought about many cultural innovations and shifts in social dynamics.\n\nCultures are externally affected via contact between societies, which may also produce—or inhibit—social shifts and changes in cultural practices. War or competition over resources may impact technological development or social dynamics. Additionally, cultural ideas may transfer from one society to another, through diffusion or acculturation. In diffusion, the form of something (though not necessarily its meaning) moves from one culture to another. For example, hamburgers, fast food in the United States, seemed exotic when introduced into China. \"Stimulus diffusion\" (the sharing of ideas) refers to an element of one culture leading to an invention or propagation in another. \"Direct borrowing,\" on the other hand, tends to refer to technological or tangible diffusion from one culture to another. Diffusion of innovations theory presents a research-based model of why and when individuals and cultures adopt new ideas, practices, and products.\n\nAcculturation has different meanings, but in this context it refers to replacement of the traits of one culture with those of another, such as what happened to certain Native American tribes and to many indigenous peoples across the globe during the process of colonization. Related processes on an individual level include assimilation (adoption of a different culture by an individual) and transculturation. The transnational flow of culture has played a major role in merging different culture and sharing thoughts, ideas, and beliefs.\n\nImmanuel Kant (1724–1804) formulated an individualist definition of \"enlightenment\" similar to the concept of \"bildung\": \"Enlightenment is man's emergence from his self-incurred immaturity.\" He argued that this immaturity comes not from a lack of understanding, but from a lack of courage to think independently. Against this intellectual cowardice, Kant urged: \"Sapere aude\", \"Dare to be wise!\" In reaction to Kant, German scholars such as Johann Gottfried Herder (1744–1803) argued that human creativity, which necessarily takes unpredictable and highly diverse forms, is as important as human rationality. Moreover, Herder proposed a collective form of \"bildung\": \"For Herder, Bildung was the totality of experiences that provide a coherent identity, and sense of common destiny, to a people.\"\nIn 1795, the Prussian linguist and philosopher Wilhelm von Humboldt (1767–1835) called for an anthropology that would synthesize Kant's and Herder's interests. During the Romantic era, scholars in Germany, especially those concerned with nationalist movements—such as the nationalist struggle to create a \"Germany\" out of diverse principalities, and the nationalist struggles by ethnic minorities against the Austro-Hungarian Empire—developed a more inclusive notion of culture as \"worldview\" (\"Weltanschauung\"). According to this school of thought, each ethnic group has a distinct worldview that is incommensurable with the worldviews of other groups. Although more inclusive than earlier views, this approach to culture still allowed for distinctions between \"civilized\" and \"primitive\" or \"tribal\" cultures.\n\nIn 1860, Adolf Bastian (1826–1905) argued for \"the psychic unity of mankind.\" He proposed that a scientific comparison of all human societies would reveal that distinct worldviews consisted of the same basic elements. According to Bastian, all human societies share a set of \"elementary ideas\" (\"Elementargedanken\"); different cultures, or different \"folk ideas\" (\"Völkergedanken\"), are local modifications of the elementary ideas. This view paved the way for the modern understanding of culture. Franz Boas (1858–1942) was trained in this tradition, and he brought it with him when he left Germany for the United States.\n\nIn the 19th century, humanists such as English poet and essayist Matthew Arnold (1822–1888) used the word \"culture\" to refer to an ideal of individual human refinement, of \"the best that has been thought and said in the world.\" This concept of culture is also comparable to the German concept of \"bildung\": \"...culture being a pursuit of our total perfection by means of getting to know, on all the matters which most concern us, the best which has been thought and said in the world.\"\n\nIn practice, \"culture\" referred to an elite ideal and was associated with such activities as art, classical music, and haute cuisine. As these forms were associated with urban life, \"culture\" was identified with \"civilization\" (from lat. \"civitas\", city). Another facet of the Romantic movement was an interest in folklore, which led to identifying a \"culture\" among non-elites. This distinction is often characterized as that between high culture, namely that of the ruling social group, and low culture. In other words, the idea of \"culture\" that developed in Europe during the 18th and early 19th centuries reflected inequalities within European societies.\n\nMatthew Arnold contrasted \"culture\" with anarchy; other Europeans, following philosophers Thomas Hobbes and Jean-Jacques Rousseau, contrasted \"culture\" with \"the state of nature.\" According to Hobbes and Rousseau, the Native Americans who were being conquered by Europeans from the 16th centuries on were living in a state of nature; this opposition was expressed through the contrast between \"civilized\" and \"uncivilized.\" According to this way of thinking, one could classify some countries and nations as more civilized than others and some people as more cultured than others. This contrast led to Herbert Spencer's theory of Social Darwinism and Lewis Henry Morgan's theory of cultural evolution. Just as some critics have argued that the distinction between high and low cultures is really an expression of the conflict between European elites and non-elites, other critics have argued that the distinction between civilized and uncivilized people is really an expression of the conflict between European colonial powers and their colonial subjects.\n\nOther 19th-century critics, following Rousseau, have accepted this differentiation between higher and lower culture, but have seen the refinement and sophistication of high culture as corrupting and unnatural developments that obscure and distort people's essential nature. These critics considered folk music (as produced by \"the folk,\" i.e., rural, illiterate, peasants) to honestly express a natural way of life, while classical music seemed superficial and decadent. Equally, this view often portrayed indigenous peoples as \"noble savages\" living authentic and unblemished lives, uncomplicated and uncorrupted by the highly stratified capitalist systems of the West.\n\nIn 1870 the anthropologist Edward Tylor (1832–1917) applied these ideas of higher versus lower culture to propose a theory of the evolution of religion. According to this theory, religion evolves from more polytheistic to more monotheistic forms. In the process, he redefined culture as a diverse set of activities characteristic of all human societies. This view paved the way for the modern understanding of culture.\n\nAlthough anthropologists worldwide refer to Tylor's definition of culture, in the 20th century \"culture\" emerged as the central and unifying concept of American anthropology, where it most commonly refers to the universal human capacity to classify and encode human experiences symbolically, and to communicate symbolically encoded experiences socially. American anthropology is organized into four fields, each of which plays an important role in research on culture: biological anthropology, linguistic anthropology, cultural anthropology, and in the United States, archaeology. The term \"Kulturbrille\", or \"culture glasses,\" coined by German American anthropologist Franz Boas, refers to the \"lenses\" through which we see our own countries. Martin Lindstrom asserts that \"Kulturbrille\", which allow us to make sense of the culture we inhabit, also \"can blind us to things outsiders pick up immediately.\"\n\nThe sociology of culture concerns culture as manifested in society. For sociologist Georg Simmel (1858–1918), culture referred to \"the cultivation of individuals through the agency of external forms which have been objectified in the course of history.\" As such, culture in the sociological field can be defined as the ways of thinking, the ways of acting, and the material objects that together shape a people's way of life. Culture can be any of two types, non-material culture or material culture. Non-material culture refers to the non-physical ideas that individuals have about their culture, including values, belief systems, rules, norms, morals, language, organizations, and institutions, while material culture is the physical evidence of a culture in the objects and architecture they make or have made. The term tends to be relevant only in archeological and anthropological studies, but it specifically means all material evidence which can be attributed to culture, past or present.\n\nCultural sociology first emerged in Weimar Germany (1918–1933), where sociologists such as Alfred Weber used the term \"Kultursoziologie\" (cultural sociology). Cultural sociology was then \"reinvented\" in the English-speaking world as a product of the \"cultural turn\" of the 1960s, which ushered in structuralist and postmodern approaches to social science. This type of cultural sociology may be loosely regarded as an approach incorporating cultural analysis and critical theory. Cultural sociologists tend to reject scientific methods, instead hermeneutically focusing on words, artifacts and symbols. \"Culture\" has since become an important concept across many branches of sociology, including resolutely scientific fields like social stratification and social network analysis. As a result, there has been a recent influx of quantitative sociologists to the field. Thus, there is now a growing group of sociologists of culture who are, confusingly, not cultural sociologists. These scholars reject the abstracted postmodern aspects of cultural sociology, and instead look for a theoretical backing in the more scientific vein of social psychology and cognitive science. \n\nThe sociology of culture grew from the intersection between sociology (as shaped by early theorists like Marx, Durkheim, and Weber) with the growing discipline of anthropology, wherein researchers pioneered ethnographic strategies for describing and analyzing a variety of cultures around the world. Part of the legacy of the early development of the field lingers in the methods (much of cultural sociological research is qualitative), in the theories (a variety of critical approaches to sociology are central to current research communities), and in the substantive focus of the field. For instance, relationships between popular culture, political control, and social class were early and lasting concerns in the field.\n\nIn the United Kingdom, sociologists and other scholars influenced by Marxism such as Stuart Hall (1932–2014) and Raymond Williams (1921–1988) developed cultural studies. Following nineteenth-century Romantics, they identified \"culture\" with consumption goods and leisure activities (such as art, music, film, food, sports, and clothing). They saw patterns of consumption and leisure as determined by relations of production, which led them to focus on class relations and the organization of production.\n\nIn the United States, cultural studies focuses largely on the study of popular culture; that is, on the social meanings of mass-produced consumer and leisure goods. Richard Hoggart coined the term in 1964 when he founded the Birmingham Centre for Contemporary Cultural Studies or CCCS. It has since become strongly associated with Stuart Hall, who succeeded Hoggart as Director. Cultural studies in this sense, then, can be viewed as a limited concentration scoped on the intricacies of consumerism, which belongs to a wider culture sometimes referred to as \"Western civilization\" or \"globalism.\"\n\nFrom the 1970s onward, Stuart Hall's pioneering work, along with that of his colleagues Paul Willis, Dick Hebdige, Tony Jefferson, and Angela McRobbie, created an international intellectual movement. As the field developed, it began to combine political economy, communication, sociology, social theory, literary theory, media theory, film/video studies, cultural anthropology, philosophy, museum studies, and art history to study cultural phenomena or cultural texts. In this field researchers often concentrate on how particular phenomena relate to matters of ideology, nationality, ethnicity, social class, and/or gender. Cultural studies is concerned with the meaning and practices of everyday life. These practices comprise the ways people do particular things (such as watching television, or eating out) in a given culture. It also studies the meanings and uses people attribute to various objects and practices. Specifically, culture involves those meanings and practices held independently of reason. Watching television in order to view a public perspective on a historical event should not be thought of as culture, unless referring to the medium of television itself, which may have been selected culturally; however, schoolchildren watching television after school with their friends in order to \"fit in\" certainly qualifies, since there is no grounded reason for one's participation in this practice.\n\nIn the context of cultural studies, the idea of a \"text\" includes not only written language, but also films, photographs, fashion or hairstyles: the texts of cultural studies comprise all the meaningful artifacts of culture. Similarly, the discipline widens the concept of \"culture.\" \"Culture\" for a cultural-studies researcher not only includes traditional high culture (the culture of ruling social groups) and popular culture, but also everyday meanings and practices. The last two, in fact, have become the main focus of cultural studies. A further and recent approach is comparative cultural studies, based on the disciplines of comparative literature and cultural studies.\n\nScholars in the United Kingdom and the United States developed somewhat different versions of cultural studies after the late 1970s. The British version of cultural studies had originated in the 1950s and 1960s, mainly under the influence of Richard Hoggart, E.P. Thompson, and Raymond Williams, and later that of Stuart Hall and others at the Centre for Contemporary Cultural Studies at the University of Birmingham. This included overtly political, left-wing views, and criticisms of popular culture as \"capitalist\" mass culture; it absorbed some of the ideas of the Frankfurt School critique of the \"culture industry\" (i.e. mass culture). This emerges in the writings of early British cultural-studies scholars and their influences: see the work of (for example) Raymond Williams, Stuart Hall, Paul Willis, and Paul Gilroy.\n\nIn the United States, Lindlof and Taylor write, \"Cultural studies [were] grounded in a pragmatic, liberal-pluralist tradition.\" The American version of cultural studies initially concerned itself more with understanding the subjective and appropriative side of audience reactions to, and uses of, mass culture; for example, American cultural-studies advocates wrote about the liberatory aspects of fandom. The distinction between American and British strands, however, has faded. Some researchers, especially in early British cultural studies, apply a Marxist model to the field. This strain of thinking has some influence from the Frankfurt School, but especially from the structuralist Marxism of Louis Althusser and others. The main focus of an orthodox Marxist approach concentrates on the \"production\" of meaning. This model assumes a mass production of culture and identifies power as residing with those producing cultural artifacts. In a Marxist view, those who control the means of production (the economic \"base\") essentially control a culture. Other approaches to cultural studies, such as feminist cultural studies and later American developments of the field, distance themselves from this view. They criticize the Marxist assumption of a single, dominant meaning, shared by all, for any cultural product. The non-Marxist approaches suggest that different ways of consuming cultural artifacts affect the meaning of the product. This view comes through in the book \"Doing Cultural Studies: The Story of the Sony Walkman\" (by Paul du Gay \"et al.\"), which seeks to challenge the notion that those who produce commodities control the meanings that people attribute to them. Feminist cultural analyst, theorist, and art historian Griselda Pollock contributed to cultural studies from viewpoints of art history and psychoanalysis. The writer Julia Kristeva is among influential voices at the turn of the century, contributing to cultural studies from the field of art and psychoanalytical French feminism.\n\nPetrakis and Kostis (2013) divide cultural background variables into two main groups:\n\nIn 2016, a new approach to culture was suggested by Rein Raud, who defines culture as the sum of resources available to human beings for making sense of their world and proposes a two-tiered approach, combining the study of texts (all reified meanings in circulation) and cultural practices (all repeatable actions that involve the production, dissemination or transmission of meanings), thus making it possible to re-link anthropological and sociological study of culture with the tradition of textual theory.\n\n\n\n\n"}
{"id": "34514568", "url": "https://en.wikipedia.org/wiki?curid=34514568", "title": "De analysi per aequationes numero terminorum infinitas", "text": "De analysi per aequationes numero terminorum infinitas\n\nDe analysi per aequationes numero terminorum infinitas (or \"On analysis by infinite series\", \"On Analysis by Equations with an infinite number of terms\",\n\"On the Analysis by means of equations of an infinite number of terms\",\"About completely loosening infinity by way of number equalisations limits\")\ncf. (\"\", \"\" = ἀναλύω and \"\") \nis a mathematical work of Isaac Newton.\n\nComposed in 1669, during the mid-part of that year probably, from ideas Newton had acquired during the period 1665–1666. Newton wrote\nThe explication was written to remedy apparent weaknesses in the \"logarithmic series\" [infinite series for formula_1] , that had become republished due to Nicolaus Mercator, or through the encouragement of Isaac Barrow in 1669, to ascertain the knowing of the prior authorship of a general method of \"infinite series\". The writing was circulated amongst scholars as a manuscript in 1669, including John Collins a mathematics \"\" for a group of British and continental mathematicians. His relationship with Newton in the capacity of informant proved instrumental in securing Newton recognition and contact with John Wallis at the Royal Society.\nBoth Cambridge University Press and Royal Society rejected the treatise from publication, being instead published in London in 1711 by William Jones, and again in 1744, as \"Methodus fluxionum et serierum infinitarum cum eisudem applicatione ad curvarum geometriam\" in \"Opuscula mathematica, philosophica et philologica\" by Marcum-Michaelem Bousquet at that time edited by Johann Castillioneus.\n\nThe exponential series, i.e. tending toward infinity, was discovered by Newton and is contained within the \"Analysis\". The treatise contains also the sine series and cosine series and arc series, the logarithmic series and the binomial series.\n\n\n"}
{"id": "3919573", "url": "https://en.wikipedia.org/wiki?curid=3919573", "title": "Deutsch limit", "text": "Deutsch limit\n\nThe Deutsch limit is an aphorism about the information density of visual programming languages originated by L. Peter Deutsch that states:\n\nThe term was made up by Fred Lakin, after Deutsch made the following comment at a talk on visual programming by Scott Kim and Warren Robinett: \"Well, this is all fine and well, but the problem with visual programming languages is that you can’t have more than 50 visual primitives on the screen at the same time. How are you going to write an operating system?\"\n\nThe primitives in a visual language are the separate graphical elements used to build a program, and having more of them available at the same time allows the programmer to read more information. This \"limit\" is sometimes cited as an example of the advantage of textual over visual languages, pointing out the greater information density of text, and posing a difficulty in scaling the language.\n\nHowever, criticisms of the limit include that it is not clear whether a similar limit also exists in textual programming languages; and that the limit could be overcome by applying modularity to visual programming as is commonly done in textual programming.\n\n\n"}
{"id": "440018", "url": "https://en.wikipedia.org/wiki?curid=440018", "title": "Duck typing", "text": "Duck typing\n\nDuck typing in computer programming is an application of the duck test—\"If it walks like a duck and it quacks like a duck, then it must be a duck\"—to determine if an object can be used for a particular purpose. With normal typing, suitability is determined by an object's type. In duck typing, an object's suitability is determined by the presence of certain methods and properties, rather than the type of the object itself. \n\nThis is a simple example in Python 3 that demonstrates how any object may be used in any context, up until it is used in a way that it does not support.\nclass Duck:\n\nclass Airplane:\n\nclass Whale:\n\ndef lift_off(entity):\n\nduck = Duck()\nairplane = Airplane()\nwhale = Whale()\n\nlift_off(duck) # prints `Duck flying`\nlift_off(airplane) # prints `Airplane flying`\nlift_off(whale) # Throws the error `'Whale' object has no attribute 'fly'`\nCertain usually statically typed languages such as Boo and the version 4 release of C# have extra type annotations\n\nTemplates in C++ allow the language to use compile-time duck typing. \n\nDuck typing is similar to, but distinct from structural typing. Structural typing is a static typing system that determines type compatibility and equivalence by a type's structure, whereas duck typing is dynamic and determines type compatibility by only that part of a type's structure that is accessed during run time.\n\nThe OCaml, Scala, Go, Elm, Gosu and PureScript languages support structural typing to varying degrees.\n\nProtocols and interfaces can provide some of the benefits of duck typing, but duck typing is distinct in that no explicit interface is defined. For example, if a third party library implements a class that cannot be modified, a client cannot use an instance of it with an interface unknown to that library even if the class does in fact satisfy the interface requirements. (A common solution to this problem is the Adapter pattern.) Duck typing would allow this. Again, all of an interface must be satisfied for compatibility.\n\nTemplate, or generic functions or methods apply the duck test in a static typing context; this brings all the advantages and disadvantages of static versus dynamic type checking in general. Duck typing can also be more flexible in that only the methods \"actually called at run time\" need to be implemented, while templates require implementation of all methods that \"cannot be proven unreachable at compile time\".\n\nExamples include the languages C++ and D with templates, which developed from Ada generics.\n\nA possible advantage of duck typing over inheritance is used in the API design for scikit-learn. There the interface dictates how the estimator functions are defined and for adding new functions you do not have to inherit any classes. There duck typing helps in decoupling the code from the API design hence making it easier for others to contribute to and extend the existing library. \n\nUse of the term \"duck typing\" has been considered superfluous in light of the fact that other terms, such as dynamic binding, express the concept more clearly. To proponents of static type checking, duck typing suggests the \"absence\" of typing, making its incorporation of the term \"typing\" appear incoherent.\n\n"}
{"id": "1115806", "url": "https://en.wikipedia.org/wiki?curid=1115806", "title": "ERG theory", "text": "ERG theory\n\nERG theory is a theory in psychology proposed by Clayton Alderfer.\nAlderfer further developed Maslow's hierarchy of needs by categorizing the hierarchy into his ERG theory (Existence, Relatedness and Growth). The existence group is concerned with providing the basic material existence requirements of humans. They include the items that Maslow considered to be physiological and safety needs. The second group of needs is those of relatedness – the desire people have for maintaining important interpersonal relationships. These social and status desires require interaction with others if they are to be satisfied, and they align with Maslow's social need and the external component of Maslow's esteem classification. Finally, Alderfer isolates growth needs: an intrinsic desire for personal development. These include the intrinsic component from Maslow's esteem category and the characteristics included under self-actualization.\nAlderfer categorized the lower order needs (Physiological and Safety) into the Existence category. He fit Maslow's interpersonal love and esteem needs into the Relatedness category. The Growth category contained the self-actualization and self-esteem needs. Alderfer also proposed a regression theory to go along with the ERG theory. He said that when needs in a higher category are not met then individuals redouble the efforts invested in a lower category need. For example if self-actualization or self-esteem is not met then individuals will invest more effort in the relatedness category in the hopes of achieving the higher need.\n\nThis theory was published originally in \"Organizational Behavior and Human Performance\".\n\n\n"}
{"id": "11494409", "url": "https://en.wikipedia.org/wiki?curid=11494409", "title": "Element (category theory)", "text": "Element (category theory)\n\nIn category theory, the concept of an element, or a point, generalizes the more usual set theoretic concept of an element of a set to an object of any category. This idea often allows restating of definitions or properties of morphisms (such as monomorphism or product) given by a universal property in more familiar terms, by stating their relation to elements. Some very general theorems, such as Yoneda's lemma and the Mitchell embedding theorem, are of great utility for this, by allowing one to work in a context where these translations are valid. This approach to category theory, in particular the use of the Yoneda lemma in this way, is due to Grothendieck, and is often called the method of the functor of points.\n\nSuppose C is any category and \"A\", \"T\" are two objects of C. A \"T\"-valued point of \"A\" is simply an arrow formula_1. The set of all \"T\"-valued points of \"A\" varies functorially with \"T\", giving rise to the \"functor of points\" of \"A\"; according to the Yoneda lemma, this completely determines \"A\" as an object of C.\n\nMany properties of morphisms can be restated in terms of points. For example, a map formula_2 is said to be a monomorphism if\nSuppose formula_7 and formula_8 in \"C\". Then \"g\" and \"h\" are \"A\"-valued points of \"B\", and therefore monomorphism is equivalent to the more familiar statement\nSome care is necessary. \"f\" is an epimorphism if the dual condition holds:\nIn set theory, the term \"epimorphism\" is synonymous with \"surjection\", i.e.\nThis is clearly not the translation of the first statement into the language of points, and in fact these statements are \"not\" equivalent in general. However, in some contexts, such as abelian categories, \"monomorphism\" and \"epimorphism\" are backed by sufficiently strong conditions that in fact they do allow such a reinterpretation on points.\n\nSimilarly, categorical constructions such as the product have pointed analogues. Recall that if \"A\", \"B\" are two objects of C, their product \"A\"×\"B\" is an object such that\nIn this definition, \"f\" and \"g\" are \"T\"-valued points of \"A\" and \"B\", respectively, while \"h\" is a \"T\"-valued point of \"A\"×\"B\". An alternative definition of the product is therefore:\nThis is the more familiar definition of the product of two sets.\n\nThe terminology is geometric in origin; in algebraic geometry, Grothendieck introduced the notion of a scheme in order to unify the subject with arithmetic geometry, which dealt with the same idea of studying solutions to polynomial equations (i.e. algebraic varieties) but where the solutions are not complex numbers but rational numbers, integers, or even elements of some finite field. A scheme is then just that: a scheme for collecting together all the manifestations of a variety defined by the same equations but with solutions taken in different number sets. One scheme gives a complex variety, whose points are its formula_19-valued points, as well as the set of formula_20-valued points (rational solutions to the equations), and even formula_21-valued points (solutions modulo \"p\").\n\nOne feature of the language of points is evident from this example: it is, in general, not enough to consider just points with values in a single object. For example, the equation formula_22 (which defines a scheme) has no real solutions, but it has complex solutions, namely formula_23. It also has one solution modulo 2 and two modulo 5, 13, 29, etc. (all primes that are 1 modulo 4). Just taking the real solutions would give no information whatsoever.\n\nThe situation is analogous to the case where C is the category Set, of sets of actual elements. In this case, we have the \"one-pointed\" set {1}, and the elements of any set \"S\" are the same as the {1}-valued points of \"S\". In addition, though, there are the {1,2}-valued points, which are pairs of elements of \"S\", or elements of \"S\"×\"S\". In the context of sets, these higher points are extraneous: \"S\" is determined completely by its {1}-points. However, as shown above, this is special (in this case, it is because all sets are iterated coproducts of {1}).\n\n"}
{"id": "20402646", "url": "https://en.wikipedia.org/wiki?curid=20402646", "title": "FTC v. Dean Foods Co.", "text": "FTC v. Dean Foods Co.\n\nFTC v. Dean Foods Co., 384 U.S. 597 (1966), is a 1966 decision of the United States Supreme Court holding that the Federal Trade Commission (FTC) may sue in federal court to obtain a preliminary injunction to maintain the status quo against the consummation of a merger that the agency persuasively contends violates the antitrust laws.\n\nMore broadly, the \"Dean Foods\" case stands for the proposition that a federal agency may, by invoking the \"All Writs Act,\" seek equitable relief in federal court against a person's threatened action that will substantially interfere with the agency's performance of its statutory duty and thus adversely affect the relevant court's ability to review the agency's ultimate order with respect to the threatened action.\n\nDean Foods and Bowman Dairy, two substantial competitors in the sale of milk in the Chicago area, agreed to a merger. Dean was the second largest firm and Bowman the third or fourth, and together they accounted for 23% of milk sales in the area. The FTC filed an administrative complaint to prevent the merger and sought to maintain the status quo pending completion of administrative hearings by filing a petition with the United States Court of Appeals for the Seventh Circuit for a temporary restraining order and preliminary injunction under the All Writs Act.\n\nThe FTC argued that preliminary injunctive relief was needed pending completion of the administrative process in the FTC. According to the agency, otherwise, Dean would eliminate Bowman as a competitive entity by selling off its milk routes, its plants, and its equipment. This would prevent restoration of Dean as an effective competitor if the agency later found the merger unlawful. The FTC maintained that such preemptive action by Dean would effectively deprive the court of appeals of its appellate jurisdiction to review the FTC’s final order, because any order would be meaningless as a practical matter. On that basis, the FTC maintained, the All Writs Act supplied the court of appeals for the area in which the companies operated with jurisdiction to issue preliminary injunctive relief so that a meaningful appeal could later occur.\n\nThe Seventh Circuit dismissed the petition on the ground that the FTC lacked authority to seek such relief (that is, it had no standing to sue), since Congress had not passed any statute giving the FTC specific authority to seek a preliminary injunction. At that point Dean immediately began closing Bowman down and eliminating it as a business.\n\nThe Supreme Court (5-4), in an opinion by Justice Tom C. Clark. recognized that the FTC had several times, without success, asked Congress to pass a law authorizing it to obtain preliminary injunctions in merger cases. The Court said that this did not matter: \"Congress neither enacted nor rejected these proposals; it simply did not act on them.\" In any case, the Court added, nothing has limited the courts' historic powers under the All Writs Act. \"We thus hold that the Commission has standing to seek preliminary relief from the Court of Appeals under the circumstances alleged.\"\n\nJustice Abe Fortas wrote a strong dissent, saying that such awesome power to interfere with mergers should not be entrusted to the FTC without a specific authorization from Congress. Fortas had been legal advisor to Federated Department Stores, Inc, at the time of its four-month fight with the chairman and co-founder of Bullock's, Inc., which had involved an intra-family proxy fight in which such an injunction had been threatened. Federated eventually acquired Bullock's, but it was forced to compromise with the FTC in a consent decree in which Federated agreed to make no more acquisitions until 1970. \n\nThe reasoning of the Court is not limited to the FTC and by its terms would apply with equal force to any other federal agency in similar circumstances. \n\nHowever, the principle may be limited to situations of great urgency. In \"FTC v. PepsiCo, Inc.\", the FTC sought a preliminary injunction against a merger and the Second Circuit denied it. The court said that, under \"Dean Foods\", an injunction can issue only if the Commission can show that \"an effective remedial order, once the merger was implemented, would otherwise be virtually impossible, thus rendering the enforcement of any final decree of divestiture futile.\" The Second Circuit thought that the merger probably violated the antitrust laws but did not believe that effective relief would be “virtually impossible.”\n\nIn \"Sampson v. Murray\", the Supreme Court held \"Dean Foods\" inapplicable to a stay of a GSA dismissal, stating: \"In direct contrast to the claim of the FTC in \"Dean Foods\" that its jurisdiction would be effectively defeated by denial of relief, the Commission here has argued that judicial action interferes with the normal agency processes. And we see nothing in the record to suggest that any judicial review available under the doctrine of \"Service v. Dulles\" would be defeated in the same manner as review in \"Dean Foods\".\"\n\nIn the wake of similar decisions, in 1973 Congress passed specific legislation granting the FTC authority to bring suits in district courts to obtain preliminary injunctions to prevent consummation of mergers pending action before the FTC, under a legal standard requiring likelihoods of success on the merits rather than a defeat of the agency proceeding. In addition, Congress passed the Hart-Scott-Rodino Act, which also created administrative clearance procedures having the effect of delaying consummation of mergers while agency evaluation of the competitive impact took place.\n\nIn 1984, the D.C. Circuit relied on \"Dean Foods\" as authority for issuance of an order to compel the FCC to act on a petition that it had allegedly delayed unreasonably in acting upon.\n\n"}
{"id": "8227721", "url": "https://en.wikipedia.org/wiki?curid=8227721", "title": "Geothermobarometry", "text": "Geothermobarometry\n\nGeothermobarometry is the science of measuring the previous pressure and temperature history of a metamorphic or intrusive igneous rocks. Geothermobarometry is a combination of \"geobarometry\", where a pressure of mineral formation is resolved, and \"geothermometry\" where a temperature of formation is resolved.\n\nGeothermobarometry relies upon understanding the temperature and pressure of the formation of minerals within metamorphic and igneous rocks, and is particularly useful in metamorphic rocks. There are several methods of measuring the temperature or pressure of mineral formation relying on chemical equilibrium between metamorphic minerals or by measuring the chemical composition of individual minerals.\n\nThermobarometry relies upon the fact that mineral pairs/assemblages vary their compositions as a function of temperature and pressure. There are numerous extra factors to consider such as oxygen fugacity and water activity (roughly, the same as concentration). The distribution of component elements between the mineral assemblages is then analysed using an electron microprobe or scanning electron microscope (SEM).\n\nData on the geothermometers and geobarometers is derived from both laboratory studies on artificial mineral assemblages, where minerals are grown at known temperatures and pressures and the chemical equilibrium measured directly, and from calibration using natural systems.\n\nFor example, one of the best known and most widely applicable geothermometers is the garnet-biotite relationship where the relative proportions of Fe and Mg in garnet and biotite change with increasing temperature, so measurement of the compositions of these minerals to give the Fe-Mg distribution between them allows the temperature of crystallization to be calculated, given some assumptions.\n\nIn natural systems, the chemical reactions occur in open systems with unknown geological and chemical histories, and application of geothermobarometers relies on several assumptions that must hold in order for the laboratory data and natural compositions to relate in a valid fashion:\n\n\nSome techniques include:\n\nNote that the Fe-Mg exchange thermometers are empirical (laboratory tested and calibrated) as well as calculated based on a theoretical thermodynamic understanding of the components and phases involved. The Ti-in-biotite thermometer is solely empirical and not well understood thermodynamically.\n\n\nVarious mineral assemblages rely more upon pressure than temperature; for example reactions which involve a large volume change. At high pressure, specific minerals assume lower volumes (therefore density increases, as the mass does not change) - it is these minerals which are good indicators of paleo-pressure.\n\n"}
{"id": "20901697", "url": "https://en.wikipedia.org/wiki?curid=20901697", "title": "Greenhouse gas removal", "text": "Greenhouse gas removal\n\nGreenhouse gas removal projects are a type of climate engineering that seek to remove greenhouse gases from the atmosphere, and thus they tackle the root cause of global warming. These techniques either directly remove greenhouse gases, or alternatively seek to influence natural processes to remove greenhouse gases indirectly. The discipline overlaps with carbon capture and storage and carbon sequestration, and some projects listed may not be considered to be climate engineering by all commentators, instead being described as mitigation.\n\nA wide range of techniques for carbon sequestration exist. These range from ideas to remove from the atmosphere (carbon dioxide air capture), flue gases (carbon capture and storage) and by preventing carbon in biomass from re-entering the atmosphere, such as with Bio-energy with carbon capture and storage (BECCS).\n\nAtmospheric chlorofluorocarbon (CFC) removal is an idea which suggests using lasers to break up CFCs, an important family of greenhouse gases, in the atmosphere.\n\nMethane potentially poses major challenges for remediation. It is around 20 times as powerful a greenhouse gas as . Large quantities may be outgassed from permafrost and clathrates as a result of global warming, notably in the Arctic.\n\nThere are existing climate engineering proposals. Methane is removed by several natural processes, which can be enhanced.\n\n\n"}
{"id": "1437184", "url": "https://en.wikipedia.org/wiki?curid=1437184", "title": "Guns versus butter model", "text": "Guns versus butter model\n\nIn macroeconomics, the guns versus butter model is an example of a simple production–possibility frontier. It demonstrates the relationship between a nation's investment in defense and civilian goods. In this example, a nation has to choose between two options when spending its finite resources. It may buy either guns (invest in defense/military) or butter (invest in production of goods), or a combination of both. This may be seen as an analogy for choices between defense and civilian spending in more complex economies.\n\nThe \"guns or butter\" model is used generally as a simplification of national spending as a part of GDP. The nation will have to decide which balance of guns versus butter best fulfills its needs, with its choice being partly influenced by the military spending and military stance of potential opponents. Researchers in political economy have viewed the trade-off between military and consumer spending as a useful predictor of election success.\n\nOne theory on the origin of the concept comes from William Jennings Bryan's resignation as United States Secretary of State in the Wilson Administration. At the outbreak of World War I, the leading global exporter of nitrates for gunpowder was Chile. Chile had maintained neutrality during the war and provided nearly all of the US's nitrate requirements. It was also the principal ingredient of chemical fertilizer in farming. The export product was sodium nitrate, a salt mined in the northern part of Chile which is often called \"Chilean saltpeter.\"\n\nWith substantial popular opinion running against US entry into the war, the Bryan resignation and peace campaign (joined prominently with Henry Ford's efforts) became a banner for local versus national interests. However, Bryan was no more pro-German than the infamously nativistic Wilson; his motivation, instead, was to expose and publicize what he considered to be an unconscionable public policy.\n\nThe National Defense Act of 1916 directed the president to select a site for the artificial production of nitrates within the United States. It was not until September 1917, several months after the United States entered World War I, that Wilson selected Muscle Shoals, Alabama, after more than a year of competition among political rivals. A deadlock in the Congress was broken when South Carolina Senator Ellison D. Smith sponsored the National Defense Act of 1916 that directed \"the Secretary of Agriculture to manufacture nitrates for fertilizers in peace and munitions in war at water power sites designated by the President.\" This was presented by the news media as \"guns and butter.\"\n\nPerhaps the best known use of the phrase (in translation) was in Nazi Germany. In a speech on January 17, 1936, Minister of Propaganda Joseph Goebbels stated: \"We can do without butter, but, despite all our love of peace, not without arms. One cannot shoot with butter, but with guns.\" Referencing the same concept, sometime in the summer of the same year another Nazi official, Hermann Göring, announced in a speech: \"Guns will make us powerful; butter will only make us fat.\"\n\nUS President Lyndon B. Johnson used the phrase to catch the attention of the national media while reporting on the state of national defense and the economy.\n\nAnother use of the phrase was British Prime Minister Margaret Thatcher's statement, in a 1976 speech she gave at a Kensington Town Hall, in which she said, \"The Soviets put guns over butter, but we put almost everything over guns.\"\n\nThe song \"Guns Before Butter\" by Gang of Four from their 1979 album \"Entertainment!\" is about this concept.\n\nThe Prodigy's 1997 album \"The Fat of the Land\" has the following text on the fold-out booklet: \"We have no butter, but I ask you /Would you rather have butter or guns? /Shall we import lard or steel? Let me tell you /Preparedness makes us powerful. /Butter merely makes us fat.\"\n\nThis phrase as the title for an episode (\"Guns Not Butter\") in season four of the television show \"The West Wing\" (1999–2006) that focused on the portion of the federal budget devoted to foreign aid.\n\nThe concept was also discussed in the 2001 movie Baby Boy.\n\nThe concept is referenced in the title of the song \"Gunz n Butter\" on \"A$AP Rocky\"'s 2018 album \"TESTING\".\n\nLyndon B. Johnson's Great Society programs in the 1960s, when he was President of the United States, are examples of the guns versus butter model. While Johnson wanted to continue New Deal programs and expand welfare with his own Great Society programs, he was also involved in both the arms race of the Cold War and in the Vietnam War. These wars put strains on the economy and hampered his Great Society programs.\n\nThis is in stark contrast to President Dwight D Eisenhower's own objections to the expansion and endless warfare of the military-industrial complex. In his \"Chance For Peace\" speech in 1953, he referred to this very trade-off, giving specific examples:Every gun that is made, every warship launched, every rocket fired signifies, in the final sense, a theft from those who hunger and are not fed, those who are cold and are not clothed. This world in arms is not spending money alone. It is spending the sweat of its laborers, the genius of its scientists, the hopes of its children.The cost of one modern heavy bomber is this: a modern brick school in more than 30 cities. It is two electric power plants, each serving a town of 60,000 population. It is two fine, fully equipped hospitals. It is some fifty miles of concrete pavement. We pay for a single fighter plane with a half million bushels of wheat. We pay for a single destroyer with new homes that could have housed more than 8,000 people.This is, I repeat, the best way of life to be found on the road the world has been taking. This is not a way of life at all, in any true sense. Under the cloud of threatening war, it is humanity hanging from a cross of iron. ... Is there no other way the world may live?\n\n"}
{"id": "3211137", "url": "https://en.wikipedia.org/wiki?curid=3211137", "title": "Günther Storck", "text": "Günther Storck\n\nGünther Storck (2 October 1938 – 23 April 1993) was a conservative Catholic bishop from Germany. He was ordained to the priesthood on 21 September 1973 by sedevacantist , Roman Catholic apostolic prefect of Yungchow, China, and consecrated – without permission of Pope John Paul II – a bishop on 30 April 1984, in Etiolles, France, by sedeprivationist bishop Guerard des Lauriers.\n\nGünther Storck was born on 2 October 1938 in Borken, North Rhine-Westphalia, as the youngest child of the Storck family, who ran a craft business. The father died early, so that the mother not only had to take charge of the family, but also had to run the business. The young Günther Storck was regarded as mentally very sensitive and highly gifted.\n\nAfter his Abitur (1958) he studied classical philology and German studies at universities in Münster, Berlin and Munich. For his vocation to the priesthood he returned to Münster, his bishop's resident city, where he started a degree in theology at the seminary (1962). In the meanwhile he passed his state examination in philology, philosophy and theology. He escaped from the beginning influence of Vatican II on Münster – Karl Rahner, the former council adviser to Cardinal Julius Döpfner, took over the chair in dogmatic theology and history of dogma at the University of Münster (1967-1971), that Joseph Ratzinger had recently vacated (1963-1966) – to Munich (1967), where he continued his studies at the theological and philosophical faculties of the University of Munich. Few years later he became Research Associate of Leo Scheffczyk, which meant that Cardinal Döpfner's allowance to become ordained to the priesthood in the Roman Catholic Archdiocese of Munich and Freising became a remote likelihood, which is why he switched to Egg in Switzerland for ordination (21 September 1973). The day after he celebrated his First Mass in Damenstiftskirche St. Anna. He did his doctorate in theology with a tripartite graduate thesis on Johann Gottlieb Fichte's \"Wissenschaftslehre of 1794/95\" (first part) and on his \"Wissenschaftslehre of 1804\" (second part); its concluding third part achieves the theological application to the doctrine of the Trinity.\n\nAlthough many were concerned about the poor health of Bishop Günther Storck - he had been lying in a Munich hospital after a collapse for about a month - so the news of his death on 23 April came as a surprise to outsiders. There was a prospect of improvement and arrangements had already been made for a subsequent spa stay. But it turned out differently. Bishop Storck had internal bleeding that could no longer be stopped. Informed about the possibility of premature death, his priests had gathered at his deathbed and a small circle of people who had been especially close to him lately. On Friday, April 30, the solemn requiem was held for the deceased in St. Mary's Church, Munich – exactly nine years after his consecration as bishop. On the following Monday, 3 May, the funeral took place at the Munich West Cemetery.\n\nThe first review of Storck's thesis appeared twenty years after its publication (see § Secondary literature). Seven additional years later, his teacher in philosophy resumed Storck's application of transcendental philosophy to the Trinity Doctrine, claiming that \"the absolute difference between the Godmanhood of Jesus and the pure essence of God should have been worked out\": \"Godmanhood is not simply the same as the Godhead.\" Which eventually is confirmed by a revisionist reading of the Islamic view in Quran : \"One notices the reference to Matthew : »Yet no one knows the day or hour when this will be, not the angels in heaven, nor the Son. Only the Father knows.«\"\n\n\n"}
{"id": "41131139", "url": "https://en.wikipedia.org/wiki?curid=41131139", "title": "Identity fusion", "text": "Identity fusion\n\nIdentity fusion, a psychological construct rooted in social psychology and cognitive anthropology, is a form of alignment with groups in which members experience a visceral sense of oneness with the group. The construct relies on a distinction between the personal self and the social self. The personal self refers to the characteristics that make someone a unique person (e.g., tall, old, intelligent), while the social self pertains to the characteristics that align the person with groups (e.g., American, fraternity brother, student council member, etc.). As the name suggests, identity fusion involves the union of the personal and social selves. When fusion occurs, both the personal and social selves remain salient and influential but the boundaries between them become highly permeable. In addition, the theory proposes that fused persons come to regard other group members as “family” and develop strong relational ties to them as well as ties to the collective. Therefore, fused persons are not just bound to the collective; they are tied to the individual members of the collective.\n\nThe potency of the personal self and relational ties distinguish identity fusion from other forms of alignment with groups, such as “group identification”. In group identification, allegiance to the collective eclipses the personal self and relational ties to other group members. Because of this, the personal self and relational ties are not as involved in theories of group identification. Identity fusion theorizes that fusion measures should be more predictive of extreme pro-group behavior than previously proposed measures of identification. In fact, there is growing evidence of this. Measures of identity fusion are particularly powerful predictors of personally costly pro-group behaviors, including endorsement of extreme behaviors, such as fighting and dying for the group.\n\nThe identity fusion construct builds upon earlier work by emphasizing aspects of the relationship of people to groups that were de-emphasized within the social identity perspective (i.e., social identity theory and self-categorization theory). Like social identity theory, identity fusion theory rests on the distinction between the personal and social identities. However, the social identity approach assumes that there is a hydraulic relationship between personal and social identities. That is, the increases in the salience and influence of one identity diminishes the salience and influence of the other. One important implication of this assumption is that as the group identity becomes salient and apt to guide behavior, the personal identity becomes less salient and less likely to guide behavior. In contrast, the theory of identity fusion theory proposes that both the personal and social identities of a person can be salient and influential simultaneously.\nSocial identity theory also suggests that group members are only linked to one another through their allegiance to the collective; theoretically, personal relationships between group members do not foster identification with the group (with the exception of one study). In contrast, fused individuals feel deeply connected to other group members as individuals, as well as to the larger group as a whole. This is reflected in measures of identify fusion. For example, the verbal measure of identity fusion taps feelings of reciprocal strength between the individual and the group (e.g., \"I am strong because of my group\"; \"I would do more for my group than any other group members would do\") as well as feelings of oneness with the group (e.g., \"I am one with my group\"; \"My group is me\").\n\nThe characteristics of identity fusion theory have been summarized in the form of four principles:\n\nSince the experimental study of actual extreme pro-group acts raises large ethical red flags, researchers have largely focused on endorsements of extreme pro-group acts. Several studies have shown that fusion is a robust predictor of willingness to fight and die on behalf of one’s group. Other research has examined responses to variations of the trolley dilemma adapted for groups. In scenarios that pitted the desire for self-preservation against self-sacrifice for others, strongly fused persons were especially willing to endorse sacrificing their lives for fellow in-group members (but not for out-group members). Using a different approach, researchers examined group members’ reactions to significant group losses and found that highly identified individuals tend to detach themselves from the group following a group failure, whereas strongly fused persons predicted that they would “go down with the ship”. For example, in parallel studies of the 2008 presidential elections in Spain and the United States, people who were strongly fused with their political party internalized both victory and defeat, but highly identified persons internalized only victory. Additional field research with terrorist groups like ISIS and rebel groups is also beginning to shed light on the role identity fusion plays in extreme pro-group behaviors.\n\nIn addition to predicting endorsement of extreme pro-group behaviors, research suggests that fusion is a predictor of a variety of personally costly pro-group behaviors in the real world. In a study of transsexuals considering sex reassignment surgery, individuals strongly fused with their desired sex underwent surgical procedures designed to permanently change their primary sex characteristics. Weakly fused participants were far less likely to undergo these procedures.\n\nAdditional research has shown that fusion could also be a strong predictor of group-directed helping behaviors. In some studies, individuals donated money to the group. In others, they provided social and emotional support to fellow group members. Other research has also suggested that strongly fused individuals are especially willing to go out of their way to protect the group and maintain its integrity. For instance, strongly fused employees were more likely to report having “blown the whistle” sometime during their employment. Presumably, such whistle-blowing activity was motivated by a conviction that their actions would ultimately benefit the group. Another study found that students who were strongly fused with their university were willing to whistle-blow against a cheating fellow student despite the cost of time, energy, and the possibility of retaliation from the cheater.\n\nSince Charles Darwin, the willingness of some humans to sacrifice themselves for genetically unrelated members of the same large, diffuse group (such as a religion or a nation) has raised a theoretical challenge. Social psychological perspectives have contended that such sacrifices are motivated by commitment to the larger collective whereas anthropological perspectives have contended that such sacrifices are triggered by commitment to other members of the group. The distinction between \"local\" and \"extended\" fusion provides an explanation for these apparently competing explanations. \"Local fusion\" is proposed to occur in relatively small, homogeneous groups whose members attach to each other through direct personal contact (e.g., families or work teams). In contrast, \"extended fusion\" occurs in relatively large groups whose members do not all have personal relationships (e.g., political parties or nation states). In extended fusion, even though fused individuals may not actually know all of their fellow group members, they still feel like they know them and even think of them as like family.\n\nIn short, identity fusion theory posits that fused people project feelings of relational ties they have with known group members onto unknown group members. The projection of relational ties explains why fused individuals are sometimes willing to make sacrifices for members of large heterogeneous groups that most people would make only for small, tight-knit groups. Through the process of projection, they psychologically transform genetically unrelated individuals into kin.\n\nAlthough most fusion research to date has focused on the nature and consequences of fusion, recent research has revealed some starting points for understanding the causes of fusion. Perceptions of shared essence, the belief that one shares essential core qualities with the group, appears to be a key building block of identity fusion. Perceptions of shared essence arise in different ways in local and extended fusion. In local fusion, individuals have direct experiences with other group members that foster the conclusion that they share essential qualities with those individuals. In extended fusion, the perception of psychological kinship is fostered by the presence of certain characteristics that are perceived as fundamental to who the person is. For example, people are more likely to fuse with large extended groups when they become convinced that members of the group share with them genes or core values, especially if they hold those values sacred.\n\nThe \"relational ties\" principle of fusion suggests that highly fused individuals will feel that they and other group members synergistically strengthen each other. This perception of reciprocal strength should foster the perception that together, members of the group are uniquely invulnerable. These feelings of invulnerability may serve to insulate strongly fused individuals from fully recognizing the risks associated with extreme acts. Perceptions of invulnerability have been shown to mediate the effects of fusion on endorsement of pro-group behavior.\n\nThe identity synergy principle of fusion assumes that the borders between the personal and social selves are highly permeable for strongly fused individuals. These porous borders encourage people to channel their personal agency into group behavior, raising the possibility that strongly fused individuals will channel their feelings of personal agency into pro-group behavior. Perceptions of agency have been shown to mediate the effect of fusion on pro-group behavior.\n\nA study found that groups that share painful or strong negative experiences can cause visceral bonding, and pro-group behavior.\n"}
{"id": "22851039", "url": "https://en.wikipedia.org/wiki?curid=22851039", "title": "Isbell conjugacy", "text": "Isbell conjugacy\n\nIsbell conjugacy (named after John R. Isbell) is a fundamental construction of enriched category theory formally introduced by William Lawvere in 1986.\n\nLet formula_1 be a symmetric monoidal closed category, and let formula_2 be a small category enriched in formula_1.\n\nThe Isbell conjugacy is an adjunction between the categories formula_4 and formula_5 arising from the Yoneda embedding formula_6 and the dual Yoneda embedding formula_7.\n\n"}
{"id": "264416", "url": "https://en.wikipedia.org/wiki?curid=264416", "title": "Joseph Gelfer", "text": "Joseph Gelfer\n\nJoseph Gelfer (born 1974 in Southampton, England) is a British author and academic.He is noted for his academic analysis of spiritual and religious topics and masculinity. His book \"2012: Decoding the Countercultural Apocalypse\" which brought together scholarly analyses of the end of the world phenomenon from anthropology, Mayan studies, religious studies and cultural studies attracted considerable media attention. He continues to examine spiritual and gender issues using rigorous academic methods and cross disciplinary studies.\n\nGelfer is the founding and current editor of Journal of Men, Masculinities and Spirituality, creator of the Future Masculinity online course and Director of Masculinity Research\n\nGelfer has a BA Hons from University of Bristol and a doctorate in religious studies from Victoria University of Wellington. His thesis was titled \"Numen, old men : contemporary masculine spiritualities and the problem of patriarchy.\"\n\nJoseph Gelfer is a lecturer and tutor at Université Catholique de l'Ouest. He has had concurrent careers in research in religion and masculinities and in academic editing and coaching. He has held positions as Adjunct Research Associate at the School of Political and Social Inquiry at Monash University, Honorary Research Associate at University of Divinity, Melbourne, as Editorial Specialist at the Royal Melbourne Institute of Technology (RMIT) and an Assistant Editor at the University of Londo n.\n\nThe book derived from his doctoral thesis, \"Numen, Old Men: Contemporary Masculine Spiritualities and the Problem of Patriarchy\" (Equinox Publishers, 2009) proposed that masculine spirituality tends to perpetuate a patriarchal spirituality, and that gay spirituality and queer theory can be a useful way to think about masculinities for all men, gay or straight. He has published extensively on how masculinity functions in contemporary society and, more specifically, in contemporary religion. He proposes that by questioning the social construction of masculinity in the everyday it is possible to create a more equitable and sustainable society.\n\nLeading up to and during 2012 Gelfer received media attention surrounding the publishing of his book \"2012: Decoding the Countercultural Apocalypse\" which brought together scholarly analyses of the end of the world phenomenon from anthropology, Mayan studies, religious studies and cultural studies. The book examined the \"merits and demerits of cultural appropriation\" and \"the lack of consensus between different scholars and the inconsistent goals of different disciplines.\" Gelfer's aim in writing the book was \"to strike some balance between visionary and critical thinking\" and he was criticised by members from the skeptical, catastrophist, conspiracy and spiritually inclined communities for his non-partisan views. His proposition from the book's analysis was that no physical event would occur but \"that people [would] realise the changes they dearly wish to see in the world will not come from some cosmic source, but rather instead political agency and social activism. And that, ironically, may result in 2012 being a catalyst for a shift in human consciousness, exactly as the prophets predicted.\"\n\nIn addition to his academic work, Gelfer has also been active in social commentary publishing articles about such wide-ranging topics as psychedelic substances within a spiritual context, the commercialisation of spiritualities, child discipline, open access publishing and teetotalism. He has also published a number of travel articles and a book of Latrinalia called \"The Little Book of Toilet Graffiti\" (which according to Gelfer was simply a fund raising exercise and was followed by \"The Little Book of Student Bollocks\" and \"The Little Book of Office Bollocks).\"\n\n\nIn 2012 Gelfer undertook a number of interviews on a variety of topics published through InformitTV. \n\n\n\n\n\n\n\n"}
{"id": "918466", "url": "https://en.wikipedia.org/wiki?curid=918466", "title": "Laguerre form", "text": "Laguerre form\n\nIn mathematics, the Laguerre form is generally given as a third degree tensor-valued form, that can be written as,\n"}
{"id": "1361082", "url": "https://en.wikipedia.org/wiki?curid=1361082", "title": "Language death", "text": "Language death\n\nIn linguistics, language death occurs when a language loses its last native speaker. By extension, language extinction is when the language is no longer known, including by second-language speakers. Other similar terms include linguicide, the death of a language from natural or political causes, and rarely glottophagy, the absorption or replacement of a minor language by a major language.\n\nLanguage death is a process in which the level of a speech community's linguistic competence in their language variety decreases, eventually resulting in no native or fluent speakers of the variety. Language death can affect any language form, including dialects. Language death should not be confused with language attrition (also called language loss), which describes the loss of proficiency in a first language of an individual.\n\nIn the modern period (c. 1500 CE–present; following the rise of colonialism), language death has typically resulted from the process of cultural assimilation leading to language shift and the gradual abandonment of a native language in favour of a foreign \"lingua franca\", largely those of European countries.\n\nAs of the 2000s, a total of roughly 7,000 natively spoken languages existed worldwide. Most of these are minor languages in danger of extinction; one estimate published in 2004 expected that some 90% of the currently spoken languages will have become extinct by 2050.\n\nLanguage death is typically the final outcome of language shift and may manifest itself in one of the following ways:\n\nThe most common process leading to language death is one in which a community of speakers of one language becomes bilingual with another language, and gradually shifts allegiance to the second language until they cease to use their original, heritage language. This is a process of assimilation which may be voluntary or may be forced upon a population. Speakers of some languages, particularly regional or minority languages, may decide to abandon them based on economic or utilitarian grounds, in favor of languages regarded as having greater utility or prestige.\n\nLanguages with a small, geographically isolated population of speakers can die when their speakers are wiped out by genocide, disease, or natural disaster.\n\nA language is often declared to be dead even before the last native speaker of the language has died. If there are only a few elderly speakers of a language remaining, and they no longer use that language for communication, then the language is effectively dead. A language that has reached such a reduced stage of use is generally considered moribund. Half of the spoken languages of the world are not being taught to new generations of children. Once a language is no longer a native language—that is, if no children are being socialized into it as their primary language—the process of transmission is ended and the language itself will not survive past the current generations.\n\nLanguage death is rarely a sudden event, but a slow process of each generation learning less and less of the language, until its use is relegated to the domain of traditional use, such as in poetry and song. Typically the transmission of the language from adults to children becomes more and more restricted, to the final setting that adults speaking the language will raise children who never acquire fluency. One example of this process reaching its conclusion is that of the Dalmatian language.\n\nDuring language loss—sometimes referred to as \"obsolescence\" in the linguistic literature—the language that is being lost generally undergoes changes as speakers make their language more similar to the language that they are shifting to. This process of change has been described by Appel (1983) in two categories, though they are not mutually exclusive. Often speakers replace elements of their own language with something from the language they are shifting toward. Also, if their heritage language has an element that the new language does not, speakers may drop it.\n\nLanguage revitalization is an attempt to slow or reverse language death. Revitalization programs are ongoing in many languages, and have had varying degrees of success.\n\nThe revival of the Hebrew language in Israel is the only example of a language's acquiring new first language speakers after it became extinct in everyday use for an extended period, being used only as a liturgical language. Even in the case of Hebrew, there is a theory that argues that \"the Hebrew revivalists who wished to speak pure Hebrew failed. The result is a fascinating and multifaceted Israeli language, which is not only multi-layered but also multi-sourced. The revival of a clinically dead language is unlikely without cross-fertilization from the revivalists' mother tongue(s).\"\n\nOther cases of language revitalization which have seen some degree of success are Irish, Welsh, Hawaiian, Cherokee and Navajo. \n\nAs a response to English linguistic dominance, de-anglicisation became a matter of national pride in some places and especially in regions that were once under colonial rule, where vestiges of colonial domination are a sensitive subject. Following centuries of English rule in Ireland and English imposition of the English language, an argument for de-anglicization was delivered before the Irish National Literary Society in Dublin, 25 November 1892; \"When we speak of 'The Necessity for De-Anglicising the Irish Nation', we mean it, not as a protest against imitating what is best in the English people, for that would be absurd, but rather to show the folly of neglecting what is Irish, and hastening to adopt, pell-mell, and indiscriminately, everything that is English, simply because it is English.\" Language was one of the features of Anglicisation in Ireland: although it never died out and became an official language after independence, Irish had lost its status as the island's principal vernacular to become a minority language during the period of English rule; similarly, in North America indigenous languages have been replaced by those of the colonists.\n\nAccording to Ghil'ad Zuckermann, \"language reclamation will become increasingly relevant as people seek to recover their cultural autonomy, empower their spiritual and intellectual sovereignty, and improve wellbeing. There are various ethical, aesthetic and utilitarian benefits of language revival—for example, historical justice, diversity and employability, respectively.\"\n\nGoogle launched the Endangered Languages Project aimed at helping preserve languages that are at risk of extinction. Its goal is to compile up-to-date information about endangered languages and share the latest research about them.\n\nAnthropologist Akira Yamamoto has identified nine factors that he believes will help prevent language death:\n\nLinguists distinguish between language \"death\" and the process where a language becomes a \"dead language\" through normal language change, a linguistic phenomenon analogous to pseudoextinction. This happens when a language in the course of its normal development gradually morphs into something that is then recognized as a separate, different language, leaving the old form with no native speakers. Thus, for example, Old English may be regarded as a \"dead language\" although it changed and developed into Middle English, Early Modern English and Modern English. Dialects of a language can also die, contributing to the overall language death. For example, the Ainu language is slowly dying - \"The UNESCO Atlas of the World's Languages in Danger lists Hokkaido Ainu as critically endangered with 15 speakers ... and both Sakhalin and Kuril Ainu as extinct.\"\n\nThe process of language change may also involve the splitting up of a language into a family of several daughter languages, leaving the common parent language \"dead\". This has happened to Latin, which (through Vulgar Latin) eventually developed into the Romance languages, and to Prakrit, which developed into the New Indo-Aryan languages. Such a process is normally not described as \"language death\", because it involves an unbroken chain of normal transmission of the language from one generation to the next, with only minute changes at every single point in the chain. Thus with regard to Latin, for example, there is no point at which Latin \"died\"; it evolved in different ways in different geographic areas, and its modern forms are now identified by a plethora of different names such as French, Portuguese, Spanish, Italian, Romanian, Catalan, Galician, Venetian, etc.\n\nExcept in case of linguicide, languages do not suddenly become extinct; they become moribund as the community of speakers gradually shifts to using other languages. As speakers shift, there are discernible, if subtle, changes in language behavior. These changes in behavior lead to a change of linguistic vitality in the community. There are a variety of systems that have been proposed for measuring the vitality of a language in a community. One of the earliest is GIDS (Graded Intergenerational Disruption Scale) proposed by Joshua Fishman in 1991. A noteworthy publishing milestone in measuring language vitality is an entire issue of \"Journal of Multilingual and Multicultural Development\" devoted to the study of ethnolinguistic vitality, Vol. 32.2, 2011, with several authors presenting their own tools for measuring language vitality. A number of other published works on measuring language vitality have been published, prepared by authors with varying situations and applications in mind. These include works by Arienne Dwyer, Martin Ehala, M. Lynne Landweer, Mark Karan, András Kornai, and Paul Lewis and Gary Simons.\n\n"}
{"id": "11071229", "url": "https://en.wikipedia.org/wiki?curid=11071229", "title": "Laurence Powell", "text": "Laurence Powell\n\nLaurence Michael Powell (born August 26, 1962 in Los Angeles, California) is a former Los Angeles Police Department officer. He was one of the five officers involved in the beating of Rodney King on March 3, 1991.\n\nPowell graduated from Crescenta Valley High School. He later enrolled in the police academy. \n\nOn March 3, 1991, Powell and three other officers, Sgt. Stacey Koon, Officer Theodore Briseno, and Officer Timothy Wind were videotaped repeatedly striking Rodney King with their police batons in Lake View Terrace. Officer Powell was partnered with Officer Wind at the time. \n\nThe Los Angeles District Attorney charged the four officers with assault with a deadly weapon and use of excessive force. A year later, after a change of venue from Los Angeles to Ventura County, a jury of ten whites, one Asian and one Hispanic, acquitted the four officers of the assault charge, but deadlocked on the excessive force charge for Powell. The verdict led to the 1992 Los Angeles riots.\n\nThe four officers were later indicted on federal charges for violating Rodney King's civil rights. Powell and Koon were convicted in 1993 and were sentenced to 30 months in prison.\n\nThe 1992 song \"Guerillas in the Mist\" by Da Lench Mob uses a sample of the phrase \"gorillas in the mist\" uttered by Powell. The LAPD officer had used the phrase to describe a black family in a domestic dispute that he responded to just before stopping King, named after the 1988 film \"Gorillas in the Mist\".\n"}
{"id": "4023213", "url": "https://en.wikipedia.org/wiki?curid=4023213", "title": "List of Jewish political milestones in the United States", "text": "List of Jewish political milestones in the United States\n\nThe following is a list of Jewish political milestones in the United States.\n\n\n"}
{"id": "16888135", "url": "https://en.wikipedia.org/wiki?curid=16888135", "title": "Lumpenbourgeoisie", "text": "Lumpenbourgeoisie\n\nLumpenbourgeoisie is a term used primarily in the context of colonial and neocolonial elites in Latin America, which became heavily dependent on and supportive of the neocolonial powers. It is a hybrid compound of the German word \"Lumpen\" (\"rags\") and the French word \"bourgeoisie\".\n\nLumpenbourgeoisie is a term often attributed to Andre Gunder Frank in 1972 (although the term is already present in Paul Baran's \"The Political Economy of Growth\" from 1957) to describe a type of a middle class and upper class (merchants, lawyers, industrialists, etc.); one who has little collective self-awareness or economic base and who supports the colonial masters. The term is most often used in the context of Latin America.\n\nFrank writing on the origins of the term noted that he created this neologism \"lumpenbourgeoisie\" from lumpenproletariat and bourgeoisie because, although the colonial and neocolonial elites in Latin America were similar to European bourgeoisie on many levels, they had one major difference. This difference was their mentality of the Marxist lumpenproletariat, the \"refuse of all classes\" (as described in Marx's \"The Eighteenth Brumaire of Louis Napoleon\") easy to manipulate to support the capitalist system, often turning to crime. Similarly, the colonial elites would—although not involved in crime activities—hurt the local economy by aiding the foreign exploiters. Foreign colonial powers want to acquire resources and goods found in the colonies, and they find this facilitated with incorporation of the local elites into the system, as they become intermediaries between the rich colonial buyers and the poor local producers. The local elites become increasingly reliant on the system in which they supervise gathering of the surplus production from the colonies, taking their cut and before the remaining goods are sold abroad. Frank termed this economic system \"lumpendevelopment\" and the countries affected by it, \"lumpenstates\".\n\nThe term Lumpenbourgeoisie was already used in Austria by about 1926. The author was an Austrian social democratic journalist and he used the term in at least one article in a Viennese periodical. Another example of the use of the term was given by Czech philosopher Karel Kosík in 1997. In his article, \"Lumpenburžoazie a vyšší duchovní pravda\" (\"Lumpenbourgeoisie and the higher spiritual truth\") he defines Lumpenbourgeoisie as \"a militant, openly anti-democratic enclave within a functioning, however half-hearted and thus helpless democracy\".\n\n\"Lumpen-bourgeoisie\" also occurs in E. Franklin Frazier's \"The Black Bourgoisie\" (1957), which was translated from the original French text that was published in 1955. He uses it to describe African American businessmen who cling to what he terms the \"myth of Negro business\" to affect meaningful change in racial politics (173). He was especially focused on the development of black-owned business that developed and expanded in both the U.S. South and North during the first decades of the 20th century.\n\n\n a Joseph L. Love wrote that the term is misattributed to Frank and was in fact coined by C. Wright Mills in \"White Collar\" (1956). Nonetheless, the term was popularized by Frank's book \"Lumpenbourgeoisie and Lumpendevelopment: Dependency, Class and Politics in Latin America\" (1972) which used it in its title.\n\n"}
{"id": "28590294", "url": "https://en.wikipedia.org/wiki?curid=28590294", "title": "Michael Krausz", "text": "Michael Krausz\n\nMichael Krausz (born 1942) is a Swiss-born American philosopher as well as an artist and orchestral conductor. His philosophical works focus on the theory of interpretation, theory of knowledge, philosophy of science, philosophy of history, and philosophy of art and music. Krausz is Milton C. Nahm Professor of Philosophy at Bryn Mawr College, and he teaches Aesthetics at the Curtis Institute of Music. He has taught at University of Toronto and has been visiting professor at American University, Georgetown University, Oxford University, Hebrew University of Jerusalem, American University in Cairo, University of Nairobi, Indian Institute of Advanced Study, and University of Ulm, among others. Krausz is the co-founder (with Joseph Margolis) and former Chair of the fourteen-institution Greater Philadelphia Philosophy Consortium.\n\nKrausz is a son of musician and artist Laszlo Krausz (1903–1979) and pianist and composer Susan Krausz (1914–2008), and he is husband of artist Constance Costigan.\n\nKrausz earned a PhD from University of Toronto, including post-graduate work at Linacre College, Oxford University. He was a Special Student at the London School of Economics, and holds a BA from Rutgers University and an MA from Indiana University. His notable teachers include Sir Isaiah Berlin, William Dray, Patrick Gardiner, Rom Harré, Sir Karl Popper, and John Oulton Wisdom. He has also been influenced by R.G. Collingwood, Joseph Margolis and Bimal Krishna Matilal.\n\nKrausz currently serves as series editor for a number of publications, including Brill Publishers Series in Philosophy of History and Culture, Rowman and Littlefield Publishers Series on Philosophy and the Global Context, Rodopi Publishers Series on Interpretation and Translation, and Penn State University Press Series of the Greater Philadelphia Philosophy Consortium.\n\nI. \"Ideals and Aims of Interpretation\". Michael Krausz’s interests in the theory of interpretation address the following cluster of questions. For such cultural phenomena as works of art, music, literature, and the self, can a single admissible interpretation exist? Or, for such phenomena, can a multiplicity of admissible interpretations exist? For such phenomena, can opposing interpretations be jointly defended? Does interpretive activity affect the nature and number of that which is interpreted? Does interpretation aim for the elucidation of its objects, the edification of its interpreters, or both? How does the question of singularity or multiplicity of admissible interpretations bear on the singularity or multiplicity of life paths and projects?\n\nSingularism asserts that objects of interpretation always answer to one and only one ideal interpretation. In contrast, multiplism asserts that objects of interpretation may answer to more than one opposed interpretation. Both singularism and multiplism require that competing interpretations address one and the same object of interpretation. Where different interpretations address different objects of interpretation, an innocuous pluralism occurs. Where objects of interpretation cannot be delineated as to number, neither singularism nor multiplism can apply.\n\nKrausz probes the relation between these ideals of interpretation and their ontologies. Singularism and multiplism are each compatible with either realism or constructivism. Singularism does not uniquely entail realism (and vice versa) and multiplism does not uniquely entail constructivism (and vice versa). Orthodox combinations include singularist-realism and multiplist-constructivism.Heterodox combinations include singularist-constructivism and multiplist-realism.\n\nKrausz affirms that the contest between singularism and multiplism is logically detachable from the contest between realism and constructivism. He further shows that the contest between singularism and multiplism is detachable from a range of other ontologies that fall under the reconciliatory heading of “constructive realism.” None of the ontologies in Krausz’s inventory of constructive realisms uniquely entails either singularism or multiplism (and vice versa). Yet Krausz denies that his “detachability thesis” demonstrates that ontology as such is unnecessary for the theory of interpretation. For the question of the countability of objects of interpretation as well as interpretations themselves is ontological. Krausz extends the notion of ideals of interpretation to ideals of life paths or projects, such as self-realization. That is, directional singularism is the view that for a given person there is one admissible life path, and directional multiplism is the view that for a given person there may be more than one admissible life path. He develops the idea of directional multiplism from a non-essentialist or non-foundational view of human nature.\n\nII. \"Relativism\". In addition, Krausz’s work on relativism canvasses the range and significance of relativistic doctrines and rehearses their virtues and vices. He considers relativism as the claim that truth, goodness, or beauty (among other values) is relative to some reference frame, and no absolute standards to adjudicate between reference frames exist. He defines and differentiates various strands of absolutism: realism, universalism and foundationalism. Krausz argues that when these strands of absolutism are unwoven, and when relativism is understood as the negation of these strands, classical self-refutation arguments against relativism do not apply. In turn, Krausz considers whether the idea of “undifferentiated unity” survives the relativist challenge. He suggests that the assertion of undifferentiated unity, instanced for example in some Asian soteriologies, is compatible with relativism as here he defines it.\n\nMichael Krausz is the founding Artistic Director and Conductor of the Great Hall Chamber Orchestra at Bryn Mawr. The GHCO is composed of 42 young professional and conservatory musicians, and has collaborated with principal players of the New York Philharmonic and Philadelphia Orchestra as soloists. Krausz studied violin with Cleveland Orchestra concertmaster Josef Gingold. He received his early conducting coaching from his father, Laszlo Krausz, noted violist with l'Orchestre de la Suisse Romande and the Cleveland Orchestra, and conductor of the Akron Symphony Orchestra. Frederik Prausnitz of the Peabody Conservatory, and Luis Biava, Resident Conductor Laureate of the Philadelphia Orchestra also coached Michael Krausz. Krausz has been guest conductor of professional orchestras in Bulgaria, including the Pleven, Vratsa and Plovdiv Philharmonic Orchestras.\n\nMichael Krausz has had thirty-three solo and duo shows in galleries in the U.S., U.K., and India, and he has participated in many group exhibitions. He was elected Fellow of the Royal Society of Arts (F.R.S.A.) in 1973. Krausz studied at the Philadelphia College of Art and Haystack Mountain School of Crafts. He was Resident Fellow at the Ossabaw Foundation. Krausz's artworks have been reproduced in numerous publications including the British journal, \"Leonardo\". He is a member of Artist's Exchange, DE, and Delaware by Hand, which, in 2009, awarded him the status of “Master.”\n\nKrausz's paintings depict various spatial planes at once, embodying scripted messages of no literal significance. They are concerned with the emergence and dissolution of ciphers in infinite spaces. The works embody a kind of automatic writing arising from conductorial musical gestures in meditative spaces. The paintings are done with dry pigment on museum board and other mixed media.\n\n\n"}
{"id": "46476968", "url": "https://en.wikipedia.org/wiki?curid=46476968", "title": "Model worker", "text": "Model worker\n\nModel worker (, abbreviated as 劳模 or láomó) is a Communist Chinese political term referring to an exemplary worker who exhibits some or all of the traits appropriate to the ideal of the socialist worker. The idea is similar to the Soviet Stakhanovite icon. Model workers are selected in China by central and provincial-level departments. Some cities and large companies also have processes for selecting and praising model workers.\n\nThe basic criteria for model workers are patriotism, \"worship of science,\" activities in environmental protection, and the pursuit of excellence.\n\nModel workers are often afforded privileges not available to other citizens or Communist Party members. \"The possibility to become a model worker offered peasants and workers one of the few opportunities for upward mobility other than joining the army,\" writes scholar Yu Miin-lin. Model workers have an easier time joining the Communist Party, and also to become a higher-level cadre, manager, or other leader.\n\nOne of the earliest model workers was the teenage textile worker Hao Jianxiu (awarded 1951), who invented the \"Hao Jianxiu Work Method\". She was sent to study at East China Textile Engineering Institute and was elevated to the upper echelon of Chinese politics, serving as Minister of Textile Industry, secretary of the CPC Central Secretariat, and vice chair of the State Planning Commission.\nAnother prominent model worker was Ni Zhifu (awarded 1959), a fitter who invented the \"Ni Zhifu drill\". He was elevated to leadership positions in the municipal governments of Beijing, Shanghai, and Tianjin, and became a member of the Politburo of the Communist Party of China. He also served as Chairman of the All-China Federation of Trade Unions.\n"}
{"id": "45215713", "url": "https://en.wikipedia.org/wiki?curid=45215713", "title": "Molecubes", "text": "Molecubes\n\nMolecubes are a collection of modular robots created by Hod Lipson and Victor Zykov from Cornell University. A molecube is made of two rotatable halves, one with the microprocessor which represents the intelligence behind the unit, and the other with a motor for rotating the joint. A group of the cubes can be connected into a variety of shapes.\n\nA robot constructed entirely of molecubes would be able to repair itself using extra cubes, and to create a copy of itself using the same number of cubes.\n\n"}
{"id": "391106", "url": "https://en.wikipedia.org/wiki?curid=391106", "title": "Moshé Feldenkrais", "text": "Moshé Feldenkrais\n\nMoshé Pinchas Feldenkrais (Hebrew: משה פנחס פלדנקרייז, May 6, 1904 – July 1, 1984) was an Israeli engineer and physicist and the founder of the Feldenkrais Method,which is claimed to improve human functioning by increasing self-awareness through movement; it is not supported by medical evidence.\n\nFeldenkrais' theory is that \"thought, feeling, perception and movement are closely interrelated and influence each other.\"\n\nHe was born in the Russian Empire city of Slavuta (now in Ukraine) and grew up in Baranovichi, Belarus. In 1918, he immigrated to Palestine. He worked as a laborer and obtained his high school diploma from Gymnasia Herzliya in 1925. After graduation, he worked as a cartographer for the British survey office and began to study self-defense, including Ju-Jitsu. A soccer injury in 1929 promoted the development of his method in later years.\n\nDuring the 1930s, he lived in France, where he earned his engineering degree from the École Spéciale des Travaux Publics and, later, his Doctor of Science in Physics at the University of Paris, where Marie Curie was one of his teachers. \n\nHe worked as a research assistant to nuclear chemist and Nobel Prize laureate Frédéric Joliot-Curie at the Radium Institute. In September 1933, he met Jigoro Kano, the founder of judo in Paris. Kano encouraged him to study Judo under Mikinosuke Kawaishi. Feldenkrais became a close friend of Kano and corresponded with him regularly. In 1936, he earned a black belt in judo, and later gained his 2nd degree black belt in 1938. He was a co-founding member of the Ju-Jitsu Club de France, one of the oldest Judo clubs in Europe, which still exists today. Frédéric and Irène Joliot-Curie and Bertrand Goldschmidt took Judo lessons from him during their time together at the institute.\n\nOn the eve of the Nazi invasion of France in 1940, Feldenkrais fled to Britain with a jar of heavy water and a sheaf of research material, with instructions to deliver them to the British Admiralty War Office. Until 1946, he was a science officer in the Admiralty working on anti-submarine weaponry in Fairlie, Scotland. His work on improving sonar led to several patents. He also taught self-defense techniques to his fellow servicemen. On slippery submarine decks, he re-aggravated an old soccer knee injury. Refusing an operation, he was prompted to intently explore and develop self-rehabilitation and awareness techniques by self-observation that later evolved into the method. His discoveries led him to begin sharing with others (including colleague J. D. Bernal) through lectures, experimental classes, and one-on-one work with a few.\n\nAfter leaving the Admiralty, he lived and worked in private industry in London. His self-rehabilitation enabled him to continue his judo practice. From his position on the International Judo Committee, he began to study judo scientifically, incorporating the knowledge that he had gained by self-rehabilitation. In 1949, he published the first book on his method, \"Body and Mature Behavior: A Study of Anxiety, Sex, Gravitation and Learning\". He studied the work of Gurdjieff, F. Matthias Alexander, Elsa Gindler and William Bates. He also traveled to Switzerland to study with Heinrich Jacoby.\nIn 1951, he returned to Israel. In 1954, after directing the IDF Department of Electronics for several years, he settled in Tel Aviv and began to teach his method full-time. In 1957, he met Mia Segal, who became his assistant and worked with him for thirty years. He also became the personal trainer of David Ben-Gurion, the Prime Minister of Israel, whom he taught to stand on his head in a yoga pose.\n\nThroughout the 1960s, 1970s, and the 1980, he presented his method in Europe and in North America (including an Awareness Through Movement program for human potential trainers at Esalen Institute in 1972). He trained the first group of 13 teachers in the method from 1969 to 1971 in Tel Aviv. Over the course of four summers, from 1975 to 1978, he trained 65 teachers in San Francisco at Lone Mountain College under the auspices of the Humanistic Psychology Institute. In 1980, 235 students began his summer teacher-training course at Hampshire College in Amherst, Massachusetts. After becoming ill in the fall of 1981, after teaching two of the planned four summers, he stopped teaching publicly.\n\nThe Feldenkrais is intended to teach better posture and improve quality of life, by means of instruction and gentle manipulation of the body.\n\nIn 2015 the Australian Government's Department of Health published the results of a review of alternative therapies that sought to determine if any were suitable for being covered by health insurance; the Feldenkrais Method was one of 17 therapies evaluated for which no clear evidence of effectiveness was found. The report notes that there is \"a paucity of evidence regarding the effectiveness of Feldenkrais for the improvement of health outcomes for any clinical condition\".\n\n\n\n\n"}
{"id": "21675", "url": "https://en.wikipedia.org/wiki?curid=21675", "title": "Natural resource", "text": "Natural resource\n\nNatural resources are resources that exist without actions of humankind. This includes all valued characteristics such as magnetic, gravitational, electrical properties and forces etc. On earth it includes: sunlight, atmosphere, water, land (includes all minerals) along with all vegetation, crops and animal life that naturally subsists upon or within the heretofore identified characteristics and substances.\n\nParticular areas such as the rainforest in Fatu-Hiva are often characterized by the biodiversity and geodiversity existent in their ecosystems. Natural resources may be further classified in different ways. Natural resources are materials and components (something that can be used) that can be found within the environment. Every man-made product is composed of natural resources (at its fundamental level). A natural resource may exist as a separate entity such as fresh water, air, and as well as a living organism such as a fish, or it may exist in an alternate form that must be processed to obtain the resource such as metal ores, rare earth metals, petroleum, and most forms of energy.\n\nThere is much debate worldwide over natural resource allocations, this is particularly true during periods of increasing scarcity and shortages (depletion and overconsumption of resources) but also because the exportation of natural resources is the basis .\n\nThere are various methods of categorizing natural resources, these include source of origin, stage of development, and by their renewability.\n\nOn the basis of origin, natural resources may be divided into two types:\n\n\nConsidering their stage of development, natural resources may be referred to in the following ways:\n\n\nMany natural resources can be categorized as either renewable or non-renewable:\n\n\nResource extraction involves any activity that withdraws resources from nature. This can range in scale from the traditional use of preindustrial societies, to global industry. Extractive industries are, along with agriculture, the basis of the primary sector of the economy. Extraction produces raw material, which is then processed to add value. Examples of extractive industries are hunting, trapping, mining, oil and gas drilling, and forestry. Natural resources can add substantial amounts to a country's wealth, however a sudden inflow of money caused by a resource boom can create social problems including inflation harming other industries (\"Dutch disease\") and corruption, leading to inequality and underdevelopment, this is known as the \"resource curse\".\n\nExtractive industries represent a large growing activity in many less-developed countries but the wealth generated does not always lead to sustainable and inclusive growth. People often accuse extractive industry businesses as acting only to maximize short-term value, implying that less-developed countries are vulnerable to powerful corporations. Alternatively, host governments are often assumed to be only maximizing immediate revenue. Researchers argue there are areas of common interest where development goals and business cross. These present opportunities for international governmental agencies to engage with the private sector and host governments through revenue management and expenditure accountability, infrastructure development, employment creation, skills and enterprise development and impacts on children, especially girls and women. A strong civil society can play an important role in ensuring effective management of natural resources. Norway can serve as a role model in this regard as it has good institutions and open and dynamic public debate with strong civil society actors that provide an effective checks and balances system for government's management of extractive industries.\n\nIn recent years, the depletion of natural resources has become a major focus of governments and organizations such as the United Nations (UN). This is evident in the UN's Agenda 21 Section Two, which outlines the necessary steps for countries to take to sustain their natural resources. The depletion of natural resources is considered a sustainable development issue. The term sustainable development has many interpretations, most notably the Brundtland Commission's 'to ensure that it meets the needs of the present without compromising the ability of future generations to meet their own needs', however in broad terms it is balancing the needs of the planet's people and species now and in the future. In regards to natural resources, depletion is of concern for sustainable development as it has the ability to degrade current environments and potential to impact the needs of future generations.\n\nDepletion of natural resources is associated with social inequity. Considering most biodiversity are located in developing countries, depletion of this resource could result in losses of ecosystem services for these countries. Some view this depletion as a major source of social unrest and conflicts in developing nations.\n\nAt present, with it being the year of the forest, there is particular concern for rainforest regions that hold most of the Earth's biodiversity. According to Nelson deforestation and degradation affect 8.5% of the world's forests with 30% of the Earth's surface already cropped. If we consider that 80% of people rely on medicines obtained from plants and ¾ of the world's prescription medicines have ingredients taken from plants, loss of the world's rainforests could result in a loss of finding more potential life saving medicines.\n\nThe depletion of natural resources is caused by 'direct drivers of change' such as Mining, petroleum extraction, fishing and forestry as well as 'indirect drivers of change' such as demography, economy, society, politics and technology. The current practice of Agriculture is another factor causing depletion of natural resources. For example, the depletion of nutrients in the soil due to excessive use of nitrogen and desertification.\nThe depletion of natural resources is a continuing concern for society. This is seen in the cited quote given by Theodore Roosevelt, a well-known conservationist and former United States president, who was opposed to unregulated natural resource extraction.\n\nIn 1982, the UN developed the World Charter for Nature, which recognized the need to protect nature from further depletion due to human activity. It states that measures must be taken at all societal levels, from international to individual, to protect nature. It outlines the need for sustainable use of natural resources and suggests that the protection of resources should be incorporated into national and international systems of law. To look at the importance of protecting natural resources further, the World Ethic of Sustainability, developed by the IUCN, WWF and the UNEP in 1990, set out eight values for sustainability, including the need to protect natural resources from depletion. Since the development of these documents, many measures have been taken to protect natural resources including establishment of the scientific field and practice of conservation biology and habitat conservation, respectively.\n\nConservation biology is the scientific study of the nature and status of Earth's biodiversity with the aim of protecting species, their habitats, and ecosystems from excessive rates of extinction. It is an interdisciplinary subject drawing on science, economics and the practice of natural resource management. The term \"conservation biology\" was introduced as the title of a conference held at the University of California, San Diego, in La Jolla, California, in 1978, organized by biologists Bruce A. Wilcox and Michael E. Soulé.\n\nHabitat conservation is a land management practice that seeks to conserve, protect and restore, habitat areas for wild plants and animals, especially conservation reliant species, and prevent their extinction, fragmentation or reduction in range.\n\nNatural resource management is a discipline in the management of natural resources such as land, water, soil, plants, and animals—with a particular focus on how management affects quality of life for present and future generations. Hence, sustainable development is followed according to judicial use of resources to supply both the present generation and future generations.\n\nManagement of natural resources involves identifying who has the right to use the resources and who does not for defining the boundaries of the resource. The resources are managed by the users according to the rules governing of when and how the resource is used depending on local condition.\n\nA \"...successful management of natural resources depends on freedom of speech, a dynamic and wide-ranging public debate through multiple independent media channels and an active civil society engaged in natural resource issues...\", because of the nature of the shared resources the individuals who are affected by the rules can participate in setting or changing them. The users have rights to devise their own management institutions and plans under the recognition by the government. The right to resources includes land, water, fisheries and pastoral rights. The users or parties accountable to the users have to actively monitor and ensure the utilisation of the resource compliance with the rules and to impose penalty on those peoples who violates the rules. These conflicts are resolved in a quick and low cost manner by the local institution according to the seriousness and context of the offence. The global science-based platform to discuss natural resources management is the World Resources Forum, based in Switzerland.\n\n"}
{"id": "3159897", "url": "https://en.wikipedia.org/wiki?curid=3159897", "title": "Occupational Personality Questionnaires", "text": "Occupational Personality Questionnaires\n\nThe Occupational Personality Questionnaires, OPQ or OPQ32, are widely used occupational personality questionnaires. The authors were Saville et al., including Roger Holdsworth, Gill Nyfield, Lisa Cramp and Bill Mabey, and they were launched by Saville and Holdsworth Ltd. in 1984. The series included the first commercially available Big Five instrument.\n\nOPQ32 provides an indication of an individual's preferred behavioural style at work; to help employers gauge how a candidate will fit into certain work environments, how they will work with other people and how they will cope with different job requirements. It is now available in more than 30 languages and uses item response theory to shorten the questionnaire down to under 30 minutes.\n\nThe OPQ32 is used in selection, development, team building, succession planning and organisational change. Independent reviews are available online.\n\nDistribution is done today by SHL Group Limited.\n"}
{"id": "46612927", "url": "https://en.wikipedia.org/wiki?curid=46612927", "title": "Parrish Dennison", "text": "Parrish Dennison\n\nOn March 5, 2013, Parrish Dennison was shot and killed by Albuquerque police officers Perdue, Dedler and Aragon. Dennison was suspected of being one of three people attempting to sell a stolen banjo from the robbery of a local music store. Police fired on Dennison after they alleged that he pointed out a revolver on them. Dennison's mother, Charlotte Ingraham asserts that footage from a helicopter camera shows otherwise. Police video from after the shooting shows Dennison on the ground with a gun in his hand. Since the incident Dennison’s family has filed a lawsuit for a wrongful death. The shooting was ruled as justified by the Bernalillo County District Attorney's Office.\n\nA witness to the incident says that Dennison did not point a gun at officers before the shooting, and that Dennison tried to break through a window but bounced off before police fired upon him. The witness said, \"He never once pointed the gun at them, nothing… He just hit that window and just bounced back and then boom, boom, boom.\"\n\n"}
{"id": "1239866", "url": "https://en.wikipedia.org/wiki?curid=1239866", "title": "Population history of indigenous peoples of the Americas", "text": "Population history of indigenous peoples of the Americas\n\nThe population figure of indigenous peoples of the Americas before the 1492 voyage of Christopher Columbus have proven difficult to establish. Scholars rely on archaeological data and written records from settlers from Europe. Most scholars writing at the end of the 19th century estimated that the pre-Columbian population was as low as 10 million; by the end of the 20th century most scholars gravitated to a middle estimate of around 50 million, with some historians arguing for an estimate of 100 million or more. Contact with the Europeans led to the European colonization of the Americas, in which millions of immigrants from Europe eventually settled in the Americas.\n\nThe population of African and Eurasian peoples in the Americas grew steadily, while the indigenous population plummeted. Eurasian diseases such as influenza, bubonic plague and pneumonic plagues, yellow fever, smallpox, and malaria devastated the Native Americans, who did not have immunity to them. Conflict and outright warfare with Western European newcomers and other American tribes further reduced populations and disrupted traditional societies. The extent and causes of the decline have long been a subject of academic debate, along with its characterization as a genocide.\n\nGiven the fragmentary nature of the evidence, even semi-accurate pre-Columbian population figures are impossible to obtain. Scholars have varied widely on the estimated size of the indigenous populations prior to colonization and on the effects of European contact. Estimates are made by extrapolations from small bits of data. In 1976, geographer William Denevan used the existing estimates to derive a \"consensus count\" of about 54 million people. Nonetheless, more recent estimates still range widely.\n\nUsing an estimate of approximately 37 million people in Mexico, Central and South America in 1492 (including 6 million in the Aztec Empire, 5-10 million in the Mayan States, 11 million in what is now Brazil, and 12 million in the Inca Empire), the lowest estimates give a death toll due from disease of 90% by the end of the 17th century (nine million people in 1650). Latin America would match its 15th-century population early in the 19th century; it numbered 17 million in 1800, 30 million in 1850, 61 million in 1900, 105 million in 1930, 218 million in 1960, 361 million in 1980, and 563 million in 2005. In the last three decades of the 16th century, the population of present-day Mexico dropped to about one million people. The Maya population is today estimated at six million, which is about the same as at the end of the 15th century, according to some estimates. In what is now Brazil, the indigenous population declined from a pre-Columbian high of an estimated four million to some 300,000.\n\nWhile it is difficult to determine exactly how many Natives lived in North America before Columbus, estimates range from a low of 2.1 million to 7 million people to a high of 18 million.\n\nThe aboriginal population of Canada during the late 15th century is estimated to have been between 200,000 and two million, with a figure of 500,000 currently accepted by Canada's Royal Commission on Aboriginal Health. Repeated outbreaks of Old World infectious diseases such as influenza, measles and smallpox (to which they had no natural immunity), were the main cause of depopulation. This combined with other factors such as dispossession from European/Canadian settlements and numerous violent conflicts resulted in a forty- to eighty-percent aboriginal population decrease after contact. For example, during the late 1630s, smallpox killed over half of the Wyandot (Huron), who controlled most of the early North American fur trade in what became Canada. They were reduced to fewer than 10,000 people.\n\nHistorian David Henige has argued that many population figures are the result of arbitrary formulas selectively applied to numbers from unreliable historical sources. He believes this is a weakness unrecognized by several contributors to the field, and insists there is not sufficient evidence to produce population numbers that have any real meaning. He characterizes the modern trend of high estimates as \"pseudo-scientific number-crunching.\" Henige does not advocate a low population estimate, but argues that the scanty and unreliable nature of the evidence renders broad estimates inevitably suspect, saying \"high counters\" (as he calls them) have been particularly flagrant in their misuse of sources. Many population studies acknowledge the inherent difficulties in producing reliable statistics, given the scarcity of hard data.\n\nThe population debate has often had ideological underpinnings. Low estimates were sometimes reflective of European notions of cultural and racial superiority. Historian Francis Jennings argued, \"Scholarly wisdom long held that Indians were so inferior in mind and works that they could not possibly have created or sustained large populations.\"\n\nThe indigenous population of the Americas in 1492 was not necessarily at a high point and may actually have been in decline in some areas. Indigenous populations in most areas of the Americas reached a low point by the early 20th century. In most cases, populations have since begun to climb.\n\nGenetic diversity and population structure in the American land mass using DNA micro-satellite markers (genotype) sampled from North, Central, and South America have been analyzed against similar data available from other indigenous populations worldwide. The Amerindian populations show a lower genetic diversity than populations from other continental regions. Observed is both a decreasing genetic diversity as geographic distance from the Bering Strait occurs and a decreasing genetic similarity to Siberian populations from Alaska (genetic entry point). Also observed is evidence of a higher level of diversity and lower level of population structure in western South America compared to eastern South America. A relative lack of differentiation between Mesoamerican and Andean populations is a scenario that implies coastal routes were easier than inland routes for migrating peoples (Paleo-Indians) to traverse. The overall pattern that is emerging suggests that the Americas were recently colonized by a small number of individuals (effective size of about 70-250), and then they grew by a factor of 10 over 800 – 1000 years. The data also show that there have been genetic exchanges between Asia, the Arctic and Greenland since the initial peopling of the Americas. A new study in early 2018 suggests that the effective population size of the original founding population of Native Americans was about 250 people.\n\nAccording to Noble David Cook, a community of scholars have recently, albeit slowly, \"been quietly accumulating piece by piece data on early epidemics in the Americas and their relation to subjugation of native peoples.\" They now believe that widespread epidemic disease, to which the natives had no prior exposure or resistance, was the primary cause of the massive population decline of the Native Americans. Earlier explanations for the population decline of the American natives include the European immigrants' accounts of the brutal practices of the Spanish conquistadores, as recorded by the Spaniards themselves. This was applied through the encomienda, which was a system ostensibly set up to protect people from warring tribes as well as to teach them the Spanish language and the Catholic religion, but in practice was tantamount to serfdom and slavery. The most notable account was that of the Dominican friar Bartolomé de las Casas, whose writings vividly depict Spanish atrocities committed in particular against the Taínos. It took five years for the Taíno rebellion to be quelled by both the \"Real Audiencia\"—through diplomatic sabotage, and through the Indian auxiliaries fighting with the Spanish. After Emperor Charles V personally eradicated the notion of the encomienda system as a use for slave labour, there were not enough Spanish to have caused such a large population decline. The second European explanation was a perceived divine approval, in which God removed the natives as part of His \"divine plan\" to make way for a new Christian civilization. Many Native Americans viewed their troubles in terms of religious or supernatural causes within their own belief systems.\n\nSoon after Europeans and enslaved Africans arrived in the New World, bringing with them the infectious diseases of Europe and Africa, observers noted immense numbers of indigenous Americans began to die from these diseases. One reason this death toll was overlooked is that once introduced, the diseases raced ahead of European immigration in many areas. Disease killed a sizable portion of the populations before European written records were made. After the epidemics had already killed massive numbers of natives, many newer European immigrants assumed that there had always been relatively few indigenous peoples. The scope of the epidemics over the years was tremendous, killing millions of people—possibly in excess of 90% of the population in the hardest hit areas—and creating one of \"the greatest human catastrophe in history, far exceeding even the disaster of the Black Death of medieval Europe\", which had killed up to one-third of the people in Europe and Asia between 1347 and 1351.\n\nOne of the most devastating diseases was smallpox, but other deadly diseases included typhus, measles, influenza, bubonic plague, cholera, malaria, tuberculosis, mumps, yellow fever and pertussis, which were chronic in Eurasia.\n\nThis transfer of disease between the Old and New Worlds was later studied as part of what has been labeled the \"Columbian Exchange\".\n\nThe epidemics had very different effects in different regions of the Americas. The most vulnerable groups were those with a relatively small population and few built-up immunities. Many island-based groups were annihilated. The Caribs and Arawaks of the Caribbean nearly ceased to exist, as did the Beothuks of Newfoundland. While disease raged swiftly through the densely populated empires of Mesoamerica, the more scattered populations of North America saw a slower spread.\n\nViral and bacterial diseases that kill victims before the illnesses spread to others tend to flare up and then die out. A more resilient disease would establish an equilibrium; if its victims lived beyond infection, the disease would spread further. The evolutionary process selects against quick lethality, with the most immediately fatal diseases being the most short-lived. A similar evolutionary pressure acts upon victim populations, as those lacking genetic resistance to common diseases die and do not leave descendants, whereas those who are resistant procreate and pass resistant genes to their offspring. For example, in the first fifty years of the sixteenth century, an unusually strong strain of syphilis killed a high proportion of infected Europeans within a few months; over time, however, the disease has become much less virulent.\n\nThus both infectious diseases and populations tend to evolve towards an equilibrium in which the common diseases are non-symptomatic, mild or manageably chronic. When a population that has been relatively isolated is exposed to new diseases, it has no resistance to the new diseases (the population is \"biologically naive\"). These people die at a much higher rate, resulting in what is known as a \"virgin soil\" epidemic. Before the European arrival, the Americas had been isolated from the Eurasian-African landmass. The peoples of the Old World had had thousands of years for their populations to accommodate to their common diseases.\n\nThe fact that all members of an immunologically naive population are exposed to a new disease simultaneously increases the fatalities. In populations where the disease is endemic, generations of individuals acquired immunity; most adults had exposure to the disease at a young age. Because they were resistant to reinfection, they are able to care for individuals who caught the disease for the first time, including the next generation of children. With proper care, many of these \"childhood diseases\" are often survivable. In a naive population, all age groups are affected at once, leaving few or no healthy caregivers to nurse the sick. With no resistant individuals healthy enough to tend to the ill, a disease may have higher fatalities.\n\nThe natives of the Americas were faced with several new diseases at once creating a situation where some who successfully resisted one disease might die from another. Multiple simultaneous infections (e.g., smallpox and typhus at the same time) or in close succession (e.g., smallpox in an individual who was still weak from a recent bout of typhus) are more deadly than just the sum of the individual diseases. In this scenario, death rates can also be elevated by combinations of new and familiar diseases: smallpox in combination with American strains of yaws, for example.\n\nOther contributing factors:\n\n\nWhen Old World diseases were first carried to the Americas at the end of the fifteenth century, they spread throughout the southern and northern hemispheres, leaving the indigenous populations in near ruins. No evidence has been discovered that the earliest Spanish colonists and missionaries deliberately attempted to infect the American natives, and some effort was actually made to limit the devastating effects of disease before it killed off what remained of their forced slave labor under their encomienda system. The cattle introduced by the Spanish contaminated various water reserves which Native Americans dug in the fields to accumulate rain water. In response, the Franciscans and Dominicans created public fountains and aqueducts to guarantee access to drinking water. But when the Franciscans lost their privileges in 1572, many of these fountains were not guarded any more and deliberate well poisoning may have happened. Although no proof of such poisoning has been found, some historians believe the decrease of the population correlates with the end of religious orders' control of the water.\n\nIn the centuries that followed, accusations and discussions of biological warfare were common. Well-documented accounts of incidents involving both threats and acts of deliberate infection are very rare, but may have occurred more frequently than scholars have previously acknowledged. Many of the instances likely went unreported, and it is possible that documents relating to such acts were deliberately destroyed, or sanitized. By the middle of the 18th century, colonists had the knowledge and technology to attempt biological warfare with the smallpox virus. They well understood the concept of quarantine, and that contact with the sick could infect the healthy with smallpox, and those who survived the illness would not be infected again. Whether the threats were carried out, or how effective individual attempts were, is uncertain.\n\nOne such threat was delivered by fur trader James McDougall, who is quoted as saying to a gathering of local chiefs, \"You know the smallpox. Listen: I am the smallpox chief. In this bottle I have it confined. All I have to do is to pull the cork, send it forth among you, and you are dead men. But this is for my enemies and not my friends.\" Likewise, another fur trader threatened Pawnee Indians that if they didn't agree to certain conditions, \"he would let the smallpox out of a bottle and destroy them.\" The Reverend Isaac McCoy was quoted in his \"History of Baptist Indian Missions\" as saying that the white men had deliberately spread smallpox among the Indians of the southwest, including the Pawnee tribe, and the havoc it made was reported to General Clark and the Secretary of War. Artist and writer George Catlin observed that Native Americans were also suspicious of vaccination, \"They see white men urging the operation so earnestly they decide that it must be some new mode or trick of the pale face by which they hope to gain some new advantage over them.\" So great was the distrust of the settlers that the Mandan chief Four Bears denounced the white man, whom he had previously treated as brothers, for deliberately bringing the disease to his people.\n\nDuring the Seven Years' War, British militia took blankets from their smallpox hospital and gave them as gifts to two neutral Lenape Indian dignitaries during a peace settlement negotiation, according to the entry in the Captain's ledger, \"To convey the Smallpox to the Indians\". In the following weeks, the high commander of the British forces in North America conspired with his Colonel to \"Extirpate this Execreble Race\" of Native Americans, writing, \"Could it not be contrived to send the small pox among the disaffected tribes of Indians? We must on this occasion use every stratagem in our power to reduce them.\" His Colonel agreed to try. Most scholars have asserted that the 1837 Great Plains smallpox epidemic was \"started among the tribes of the upper Missouri River by failure to quarantine steam boats on the river\", and Captain Pratt of the \"St. Peter\" \"was guilty of contributing to the deaths of thousands of innocent people. The law calls his offense criminal negligence. Yet in light of all the deaths, the almost complete annihilation of the Mandans, and the terrible suffering the region endured, the label criminal negligence is benign, hardly befitting an action that had such horrendous consequences.\" However, some sources attribute the 1836–40 epidemic to the deliberate communication of smallpox to Native Americans, with historian Ann F. Ramenofsky writing, \"\"Variola Major\" can be transmitted through contaminated articles such as clothing or blankets. In the nineteenth century, the U. S. Army sent contaminated blankets to Native Americans, especially Plains groups, to control the Indian problem.\" Well into the 20th century, deliberate infection attacks continued as Brazilian settlers and miners transported infections intentionally to the native groups whose lands they coveted.\"\n\nAfter Edward Jenner's 1796 demonstration that the smallpox vaccination worked, the technique became better known and smallpox became less deadly in the United States and elsewhere. Many colonists and natives were vaccinated, although, in some cases, officials tried to vaccinate natives only to discover that the disease was too widespread to stop. At other times, trade demands led to broken quarantines. In other cases, natives refused vaccination because of suspicion of whites. The first international healthcare expedition in history was the Balmis expedition which had the aim of vaccinating indigenous peoples against smallpox all along the Spanish Empire in 1803. In 1831, government officials vaccinated the Yankton Sioux at Sioux Agency. The Santee Sioux refused vaccination and many died.\n\nWhile epidemic disease was a leading factor of the population decline of the American indigenous peoples after 1492, there were other contributing factors, all of them related to European contact and colonization. One of these factors was warfare. According to demographer Russell Thornton, although many lives were lost in wars over the centuries, and war sometimes contributed to the near extinction of certain tribes, warfare and death by other violent means was a comparatively minor cause of overall native population decline.\n\nFrom the U.S. Bureau of the Census in 1894: \"The Indian wars under the government of the United States have been more than 40 in number. They have cost the lives of about 19,000 white men, women and children, including those killed in individual combats, and the lives of about 30,000 Indians. The actual number of killed and wounded Indians must be very much higher than the given... Fifty percent additional would be a safe estimate...\"\n\nThere is some disagreement among scholars about how widespread warfare was in pre-Columbian America, but there is general agreement that war became deadlier after the arrival of the Europeans and their firearms. The South or Central American infrastructure allowed for thousands of European conquistadors and tens of thousands of their Indian auxiliaries to attack the dominant indigenous civilization. Empires such as the Incas depended on a highly centralized administration for the distribution of resources. Disruption caused by the war and the colonization hampered the traditional economy, and possibly led to shortages of food and materials. The Arauco War, Chichimeca War, Red Cloud's War, Seminole Wars, War of 1812, Pontiac's Rebellion, Beaver Wars, French-Indian War, American Civil War, American Revolution, Modoc War, Oka Crisis, Battle of Cut Knife, all represented either pyrrhic victories by colonial forces, outright defeat, military stalemates, or further alliance-politics. Across the western hemisphere, war with various Native American civilizations constituted alliances based out of both necessity or economic prosperity and, resulted in mass-scale intertribal warfare. European colonization in the North American continent also contributed to a number of wars between Native Americans, who fought over which of them should have first access to new technology and weaponry—like in the Beaver Wars.\n\nSome Spaniards objected to the encomienda system, notably Bartolomé de las Casas, who insisted that the Indians were humans with souls and rights. Due to many revolts and military encounters, Emperor Charles V helped relieve the strain on both the Indian laborers and the Spanish vanguards probing the Caribana for military and diplomatic purposes. Later on New Laws were promulgated in Spain in 1542 to protect isolated natives, but the abuses in the Americas were never entirely or permanently abolished. The Spanish also employed the pre-Columbian draft system called the \"mita\", and treated their subjects as something between slaves and serfs. Serfs stayed to work the land; slaves were exported to the mines, where large numbers of them died. In other areas the Spaniards replaced the ruling Aztecs and Incas and divided the conquered lands among themselves ruling as the new feudal lords with often, but unsuccessful lobbying to the viceroys of the Spanish crown to pay Tlaxcalan war demnities. The infamous Bandeirantes from São Paulo, adventurers mostly of mixed Portuguese and native ancestry, penetrated steadily westward in their search for Indian slaves. Serfdom existed as such in parts of Latin America well into the 19th century, past independence.\n\nFriar Bartolomé de las Casas and Antonius Flávio Chesta (Tony Chesta) and other dissenting Spaniards from the colonial period described the manner in which the natives were treated by colonials. This has helped to create an image of the Spanish conquistadores as cruel in the extreme.\n\nGreat revenues were drawn from Hispaniola so the advent of losing manpower didn't benefit the Spanish crown. At best, the reinforcement of vanguards sent by the Council of the Indies to explore the Caribana country and gather information on alliances or hostilities was the main goal of the local viceroys and their adelantados. Although mass killings and atrocities were not a significant factor in native depopulation, no mainstream scholar dismisses the sometimes humiliating circumstances now believed to be precipitated by civil disorder as well as Spanish cruelty.\n\n\nThe populations of many Native American peoples were reduced by the common practice of intermarrying with Europeans. Although many Indian cultures that once thrived are extinct today, their descendants exist today in some of the bloodlines of the current inhabitants of the Americas.\n\nOn 8 September 2000, the head of the United States Bureau of Indian Affairs (BIA) formally apologized for the agency's participation in the \"ethnic cleansing\" of Western tribes.\n\n\n\n\n\n"}
{"id": "28770788", "url": "https://en.wikipedia.org/wiki?curid=28770788", "title": "Responsible Parenthood and Reproductive Health Act of 2012", "text": "Responsible Parenthood and Reproductive Health Act of 2012\n\nThe Responsible Parenthood and Reproductive Health Act of 2012 (Republic Act No. 10354), informally known as the Reproductive Health Law or RH Law, is a law in the Philippines, which guarantees universal access to methods on contraception, fertility control, sexual education, and maternal care.\n\nWhile there is general agreement about its provisions on maternal and child health, there is great debate on its mandate that the Philippine government and the private sector will fund and undertake widespread distribution of family planning devices such as condoms, birth control pills, and IUDs, as the government continues to disseminate information on their use through all health care centers.\n\nPassage of the legislation was controversial and highly divisive, with tambays, academics, religious institutions, and major political figures declaring their support or opposition while it was pending in the legislature. Heated debates and rallies both supporting and opposing the RH Bill took place nationwide.\n\nThe Supreme Court delayed implementation of the law in March 2013 in response to challenges. On April 3,2014, the Court ruled that the law was \"not unconstitutional\" but struck down eight provisions partially or in full.\n\nThe Senate Policy Brief titled \"Promoting Reproductive Health\", the history of reproductive health in the Philippines dates back to 1967 when leaders of 12 countries including the Philippines' Ferdinand Marcos signed the Declaration on Population. The Philippines agreed that the population problem should be considered as the principal element for long-term economic development. Thus, the Population Commission was created to push for a lower family size norm and provide information and services to lower fertility rates.\n\nStarting 1967, the USAID began shouldering 80% of the total family planning commodities (contraceptives) of the country, which amounted to $3 million annually. In 1975, the United States adopted as its policy the National Security Study Memorandum 200: Implications of Worldwide Population Growth for U.S. Security and Overseas Interests (NSSM200). The policy gives \"paramount importance\" to population control measures and the promotion of contraception among 13 populous countries, including the Philippines to control rapid population growth which they deem to be inimical to the sociopolitical national interests of the United States, since the \"U.S. economy will require large and increasing amounts of minerals from abroad\", and these countries can produce destabilizing opposition forces against the United States. It recommends the U.S. leadership to \"influence national leaders\" and that \"improved world-wide support for population-related efforts should be sought through increased emphasis on mass media and other population education and motivation programs by the UN, USIA, and USAID.\n\nDifferent presidents had different points of emphasis. President Ferdinand Marcos pushed for a systematic distribution of contraceptives all over the country, a policy that was called \"coercive\", by its leading administrator. The Corazon Aquino administration focused on giving couples the right to have the number of children they prefer, while Fidel V. Ramos shifted from population control to population management. Joseph Estrada used mixed methods of reducing fertility rates, while Rvee Jude A. Olandsca focused on mainstreaming natural family planning, while stating that contraceptives are openly sold in the country.\n\nIn 1989, the Philippine Legislators’ Committee on Population and Development (PLCPD) was established, \"dedicated to the formulation of viable public policies requiring legislation on population management and socio-economic development\". In 2000, the Philippines signed the Millennium Declaration and committed to attain the MDGs by 2015, including promoting gender equality and health. In 2003 USAID started its phase out of a 33-year-old program by which free contraceptives were given to the country. Aid recipients such as the Philippines faced the challenge to fund its own contraception program. In 2004 the Department of Health introduced the Philippines Contraceptive Self-Reliance Strategy, arranging for the replacement of these donations with domestically provided contraceptives.\n\nIn August 2010, the government announced a collaborative work with the USAID in implementing a comprehensive marketing and communications strategy in favor of family planning called \"May Plano Sila\".\n\nThe basic content of the Consolidated Reproductive Health Bill is divided into the following sections.\nSEC. 1. Pamagat\nSEC. 2. Declaration of Policy\nSEC. 3. Guiding Principles\nSEC. 4. Definition of Terms\nSEC. 5. Midwives for Skilled Attendance\nSEC. 6. Emergency Obstetric Care\nSEC. 7. Access to Family Planning\nSEC. 8. Maternal and Newborn Health Care in Crisis Situations\nSEC. 9. Maternal Death Review\nSEC. 10. Role of the Food and Drug Administration\nSEC. 11. Procurement and Distribution of Family Planning Supplies\nSEC. 12. Integration of Family Planning and Responsible Parenthood Component in Anti-Poverty Programs\nSEC. 13. Roles of Local Government in Family Planning Programs\nSEC. 14. Benefits for Serious and Life-Threatening Reproductive Health Conditions\nSEC. 15. Mobile Health Care Service\nSEC. 16. Mandatory Age-Appropriate Reproductive Health and Sexuality Education\nSEC. 17. Pro Bono Services for Indigent Women.\nSEC. 18. Certificate of Compliance\nSEC. 19. Capability Building of Barangay Health Workers\nSEC. 20. Pro Bono Services for Indigent Women\nSEC. 21. Sexual And Reproductive Health Programs For Persons With Disabilities (PWDs)\nSEC. 22. Right to Reproductive Health Care Information\nSEC. 23. Implementing Mechanisms\nSEC. 24. Reporting Requirements\nSEC. 25. Congressional Committee\nSEC. 26. Prohibited Acts\nSEC. 27. Penalties\nSEC. 28. Appropriations\nSEC. 29. Implementing Rules and Regulations\nSEC. 30-32. Separability Clause, Repealing Clause, Effectivity\n\nThe bill mandates the government to \"promote, without biases, all effective natural and modern methods of family planning that are medically safe and legal.\"\n\nAlthough abortion is recognized as illegal and punishable by law, the bill states that “the government shall ensure that all women needing care for post-abortion complications shall be treated and counseled in a humane, non-judgmental and compassionate manner”.\n\nThe bill calls for a \"multi-dimensional approach\" integrates a component of family planning and responsible parenthood into all government anti-poverty programs. Age-appropriate reproductive health and sexuality education is required from grade five to fourth year high school using \"life-skills and other approaches.\"\n\nThe bill also mandates the Department of Labor and Employment to guarantee the reproductive health rights of its female employees. Companies with fewer than 200 workers are required to enter into partnership with health care providers in their area for the delivery of reproductive health services.\n\nEmployers with more than 200 employees shall provide reproductive health services to all employees in their own respective health facilities. Those with fewer than 200 workers shall enter into partnerships with health professionals for the delivery of reproductive health services. Employers shall inform employees of the availability of family planning. They are also obliged to monitor pregnant working employees among their workforce and ensure they are provided paid half-day prenatal medical leaves for each month of the pregnancy period that they are employed.\n\nThe national government and local governments will ensure the availability of reproductive health care services like family planning and prenatal care.\n\nAny person or public official who prohibits or restricts the delivery of legal and medically safe reproductive health care services will be meted penalty by imprisonment or a fine.\n\nFree choice regarding reproductive health enables people, especially the poor, to have the number of children they want and can feasibly care and provide for. There are several studies cited by those who support the bill:\n\nOpponents of the bill argue that:\n\n\nPresident Aquino stated he was not an author of the bill. He also stated that he gives full support to a firm population policy, educating parents to be responsible, providing contraceptives to those who ask for them, but he refuses to promote contraceptive use. He said that his position \"is more aptly called responsible parenthood rather than reproductive health\".\n\nThe Philippines is the 39th most densely populated country, with a density over 335 per square kilometer, and the population growth rate is 1.9% (2010 Census), 1.957% (2010 est. by CIA World Factbook), or 1.85% (2005–2010 high variant estimate by the UN Population Division, \"World Population Prospects: The 2008 Revision\") coming from 3.1 in 1960.\n\nThe 2013 total fertility rate (TFR) is 3.20 births per woman, from a TFR of 7 in 1960. In addition, the total fertility rate for the richest quintile of the population is 2.0, which is about one third the TFR of the poorest quintile (5.9 children per woman). The TFR for women with college education is 2.3, about half that of women with only an elementary education (4.5 children per woman).\n\nCongressman Lagman states that the bill \"recognizes the verifiable link between a huge population and poverty. Unbridled population growth stunts socioeconomic development and aggravates poverty\".\n\nThe University of the Philippines School of Economics presented two papers in support of the bill: \"Population and Poverty: the Real Score\" (2004), and \"Population, Poverty, Politics and the Reproductive Health Bill\" (2008). According to these economists, which include Solita Monsod, Gerardo Sicat, Cayetano Paderanga, Ernesto M. Pernia, and Stella Alabastro-Quimbo, \"rapid population growth and high fertility rates, especially among the poor, do exacerbate poverty and make it harder for the government to address it\", while at the same time clarifying that it would be \"extreme\" to view \"population growth as the principal cause of poverty that would justify the government resorting to draconian and coercive measures to deal with the problem (e.g., denial of basic services and subsidies to families with more than two children)\". They illustrate the connection between rapid population growth and poverty by comparing the economic growth and population growth rates of Thailand, Indonesia, and the Philippines, wherein the first two grew more rapidly than the Philippines due to lower population growth rates. They stressed that \"the experience from across Asia indicates that a population policy cum government-funded [family planning] program has been a critical complement to sound economic policy and poverty reduction\".\n\nIn \"Population and Poverty\", Aniceto Orbeta, Jr., showed that poverty incidence is higher among big families: 57.3% of Filipino families with seven children are in poverty while only 23.8% of families who have two children live below the poverty threshold.\n\nProponents argue that smaller families and wider birth intervals resulting from the use of contraceptives allow families to invest more in each child’s education, health, nutrition and eventually reduce poverty and hunger at the household level. At the national level, fertility reduction cuts the cost of social services with fewer people attending school or seeking medical care and as demand eases for housing, transportation, jobs, water, food, and other natural resources. The Asian Development Bank in 2004 also listed a large population as one of the major causes of poverty in the country, together with weak macroeconomic management, employment issues, an underperforming agricultural sector and an unfinished land reform agenda, governance issues including corruption.\n\nOpponents refer to a 2003 study of Rand Corporation, which concluded that \"there is little cross-country evidence that population growth impedes or promotes economic growth...population neutralism has in fact been the predominant school in thinking among academics about population growth for the last half-century\". For example, the 1992 study of Ross Levine and David Renelt, which covered 119 countries over 30 years (versus a University of the Philippines study of 3 countries over a few years). The RAND study also said that a large population can promote growth given the right fundamentals. Thus, they refer to the HSBC 2012 projection for 2050 that the Philippines will be 16th largest economy due to its large growing population, and those whose populations are decreasing will suffer decline.\n\nIn a recent development, two authors of the Reproductive Health Bill changed their stand on the provisions of the bill regarding population and development. Reps. Emerciana de Jesus and Luzviminda Ilagan wanted to delete three provisions which state that \"gender equality and women empowerment are central elements of reproductive health and population and development\", which integrate responsible parenthood and family planning programs into anti-poverty initiatives, and which name the Population Commission as a coordinating body. The two party-list representatives strongly state that poverty is not due to over-population but because of inequality and corruption.\n\nThe \"Wall Street Journal\" in July 2012 said that Aquino's \"promotion of a 'reproductive health' bill is jarring\" since it could lead to \"a demographic trap of too few workers. The Philippines doesn't have too many people, it has too few pro-growth policies\".\n\nOpposing the bill, Former Finance Secretary Roberto de Ocampo wrote that it is \"truly disingenuous for anyone to proceed on the premise that the poor are to blame for the nation’s poverty:. He emphasized that the government should apply the principle of first things first and focus on the root causes of the poverty (e.g., poor governance, corruption) and apply many other alternatives to solve the problem (e.g., giving up pork barrel, raising tax collection efficiency).\n\nMaternal deaths in the Philippines, according to the World Health Organization, is at 5.7 per day, not 10–11 deaths a day, as per the proponents who repeated these numbers \"to drive home the point\".\n\nThe proponents state that the passage of the RH Bill would mean:\n\nThe Department of Health states that family planning can reduce maternal mortality by about 32%. The bill is \"meant to prevent maternal deaths related to pregnancy and childbirth\", said Clara Padilla of Engender Rights. She reported that every day, \"there are 11 women dying while giving birth in the Philippines. These preventable deaths could have been avoided if more Filipino women have access to reproductive health information and healthcare\".\n\nThe key to solving maternal deaths, according to the \"Senate Policy Brief\" on reproductive health, is the establishment of birthing centers.\n\nThe Philippine Medical Association (PMA) stated in their Position Paper that the goal of reducing the rise of maternal and child deaths \"could be attained by improving maternal and child health care without the necessity of distributing contraceptives. The millions of funds intended for the contraceptive devices may just well be applied in improving the skills of our health workers in reducing maternal and child mortality in the Philippines\".\n\nSenator Majority Floor Leader Tito Sotto said that the RH Bill is redundant to a 2009 law referred to as the Magna Carta for Women, which contains reproductive health provisions, asking the Senate to drop the bill.\n\nUsing data from the 2008 National Demographic and Health Survey, Lagman stated that \"Twenty-two percent of married Filipino women have an unmet need for family planning services, an increase by more than one-third since the 2003 National Demographic and Housing Survey\". \"Our women are having more children than they desire, as seen in the gap between desired fertility (2.5 children) and actual fertility (3.5 children), implying a significant unmet need for reproductive health services\", state some Ateneo de Manila University professors. The Bill provides that \"the State shall assist couples, parents and individuals to achieve their desired family size within the context of responsible parenthood for sustainable development and encourage them to have two children as the ideal family size.\"\n\nWriting against the bill, Bernardo Villegas wrote about the \"Myth of Unmet Family Planning Needs\", citing development economist Lant Pritchett who said that the term \"unmet need\" is an elitist construct, an imposition of a need on the poor, disrespectful of their real preferences. Pritchett said that it is \"based on a discrepancy...identified by the analyst through the comparison of responses to items in separate blocks of the questionnaire\" and is \"an inference on the part of the researcher, not a condition reported by the respondents themselves\". Pritchett argued this term is applied to women who are not sexually active, are infecund, whose husband is absent, etc., thus bloating the numbers to favor the pharmaceutical companies and those with a population control agenda. Villegas stressed: \"Because [the poor] have been deprived of the infrastructures they need, such as farm-to-market roads, irrigation systems, post-harvest facilities and other support services that the State neglected to provide them, the only economic resources they have are their children\". He also challenged that he is willing to bet that if the government will provide cash money to the poor to buy condoms, the poor will use the cash for food and basic needs, thus exploding the myth.\n\nOne of the main concerns of the proponents is the perceived lack of access to family planning devices such as contraceptives and sterilization. The bill intends to provide universal access through government funding, complementing thus private sector initiatives for family planning services, such as those offered by the International Planned Parenthood Federation (IPPF) which supports the Family Planning Organizations of the Philippines and the 97 organizations of the Philippine NGO Council.\nThe UP School of Economics argues, in contrast, that there is lack of access especially for poor people, because contraceptive use is extremely low among them and \"among the poorest families, 22% of married women of reproductive age express a desire to avoid pregnancies but are still not using any family planning method\". They say that lack of access leads to a number of serious problems which demand attention: (1) \"too many and too spaced children raises the risk of illness and premature deaths (for mother and child alike),\" (2) \"the health risks associated with mistimed and unwanted pregnancies are higher for adolescent mothers, as they are more likely to have complications during labor,\" (3) women who have mistimed pregnancies are \"constrained to rely more on public education and health services and other publicly provided goods and services\", further complicating limited public resources, (4) families are not able to achieve their desired family size. Thus the UP economists \"strongly and unequivocally support\" the thrust of the bill to enable \"couples and individuals to decide freely and responsibly the number and spacing of their children and to have the information and means to carry out their decisions”. Proponents argue that government-funded access is the key to breaking the inter-generational poverty that many people are trapped in.\n\nProponents of the bill contend that \"natural family planning methods have not proven to be as reliable as artificial means of birth control\".\n\nAccording to the RH bill, one of its components is \"prevention of abortion and management of post-abortion complications\". It provides that \"the government shall ensure that all women needing care for post-abortion complications shall be treated and counseled in a humane, non-judgmental and compassionate manner\". It also states that \"abortion remains a crime and is punishable\", as the Constitution declares that “the State shall equally protect the life of the mother and the life of the unborn from conception”.\n\nThe position of the Philippine Medical Association (PMA) \"is founded strongly on the principle that 'life or conception begins at fertilization' at that moment where there is fusion or union of the sperm and the egg and thus a human person or human being already does exist at the moment of fertilization\". The PMA condemns abortifacients that \"destroys the fertilized egg or the embryo\" and \"abhors any procedure...or medication that will interrupt any stage of fertilization and prevents its normal, physiological, uninterrupted growth to adulthood\".\n\nJo Imbong, founder of the Abay Pamilya Foundation, reported that \"Lagman said in a House hearing that the bill would protect human life 'from implantation' \", and not from fertilization, noting at the same time that the Records of the Constitutional Commission state that “Human life begins at fertilization”.\n\nProponents argue that research by the Guttmacher Institute, involved in advancing international reproductive health, reveals that the use of contraceptives can reduce abortion rates by 85%. Proponents such as 14 Ateneo de Manila University professors, argued thus: \"Studies show that the majority of women who go for an abortion are married or in a consensual union (91%), the mother of three or more children (57%), and poor (68%) (Juarez, Cabigon, and Singh 2005). For these women, terminating a pregnancy is an anguished choice they make in the face of severe constraints. When women who had attempted an abortion were asked their reasons for doing so, their top three responses were: they could not afford the economic cost of raising another child (72%); their pregnancy occurred too soon after the last one (57%); and they already have enough children (54%). One in ten women (13%) who had attempted an abortion revealed that this was because her pregnancy resulted from forced sex (ibid.). Thus, for these women, abortion has become a family planning method, in the absence of information on and access to any reliable means to prevent an unplanned and unwanted pregnancy\".\n\nThe bill, said Clara Padilla of EnGender Rights Inc, will \"help reduce the number of abortions by providing increased access to information and services on modern contraceptive methods, that in turn will reduce the number of unwanted—and often aborted—pregnancies\".\n\nBoth sides of the debate accuse the other side of deception and misleading the public. The pro-RH people accuse the anti-RH group of misleading the public by calling the bill an abortion bill, when the bill states that abortion remains a crime and is punishable. The anti-RH advocates accuse the RH supporters of hiding from the public the international population control agenda which includes abortion and they refer to U.S. Secretary Hillary Clinton who said that RH includes abortion.\n\nFourteen professors from Ateneo de Manila University, a prominent Catholic University, considering the empirical evidence of the dire socioeconomic conditions of the Filipino poor, urged that the bill be passed to help them. They argued: \"As Catholics and Filipinos, we share the hope and mission of building a Church of the Poor. We are thus deeply disturbed and saddened by calls made by some members of the Catholic Church to reject a proposed legislation that promises to improve the wellbeing of Filipino families, especially the lives of women, children, adolescents, and the poor\". They announced that \"Catholic social teachings recognize the primacy of the well-formed conscience over wooden compliance to directives from political and religious authorities\", urging Catholic authorities to withdraw their opposition the bill. Citing Catholic documents and scientific studies, they reasoned that \"the RH Bill is pro-life, pro-women, pro-poor, pro-youth, and pro-informed choice\". They emphasized that the bill \"promotes quality of life, by enabling couples, especially the poor, to bring into the world only the number of children they believe they can care for and nurture to become healthy and productive members of our society\". Thus, they called their paper \"Catholics Can Support the RH Bill in Good Conscience\".\n\nIn response, the Ateneo administration announced its unity with Catholic teaching and that it had \"serious objections to the present bill\".\n\nProponents such as Lagman also stressed that official Catholic teaching itself, expressed in the Encyclical \"Humanae Vitae\" issued only forty years ago in 1964, is not infallible. He said that the Papal Commission on Birth Control, which included ranking prelates and theologians, recommended that the Church change its teaching on contraception as it concluded that “the regulation of conception appears necessary for many couples who wish to achieve a responsible, open and reasonable parenthood in today’s circumstances”. The editorial of the \"Philippine Daily Inquirer\", moreover, stated that Catholic teaching is \"only\" a religious teaching and should not be imposed with intolerance on a secular state.\n\nOpponents argue that misery is not the result of the church which they say is the largest charitable organization in the world, but of a breakdown in moral sense that gives order to society, nor does misery come from parents who bring up children in faithfulness, discipline, love and respect for life, but from those who strip human beings of moral dignity and responsibility, by treating them as mere machines, which they believe contraception does.\n\nStating that contraception is a lie and \"against the beginning of new life\", the Philippine Medical Association also stressed that the \"health risks of contraception to women are considerable; the list of side effects is long, and includes high blood pressure, strokes, increased incidence of some forms of cancer\".\n\nProponents such as E. Ansioco of Democratic Socialist Women of the Philippines argued that \"The World Health Organization (WHO) includes contraceptives in its Model Lists of Essential Drugs\" and thus are safe medicines. \"Medical and scientific evidence,\" says the main proponent, \"shows that all the possible medical risks connected with contraceptives are infinitely lower than the risks of an actual pregnancy and everyday activities...The risk of dying within a year of using pills is 1 in 200,000. The risk of dying from a vasectomy is 1 in 1 million and the risk of dying from using an IUD is 1 in 10 million...But the risk of dying from a pregnancy is 1 in 10,000.\"\n\nThe RH bill provides for \"prevention and treatment of HIV/AIDS and other, STIs/STDs\", especially since the number of HIV cases among the young nearly tripled from 41 in 2007 to 110 in 2008. Proponents emphasized that RH will help in stemming the AIDS epidemic that is worsening in the Philippines. Lagman explained that \"Globally, the new number of reported cases of HIV infections and deaths has dropped by nearly 20 percent. It is therefore both ironic and tragic that the Philippines’ trajectory is towards the other direction. Our country’s HIV/AIDS statistics have increased by 30 percent!\" Primary among the means is distribution of condoms. The proponents applauded government efforts last February 2010 when it distributed condoms in some areas of Manila.\n\nProponents refer to many surveys conducted by two prominent locally based organizations (SWS and Pulse Asia) which show majority support for the bill. A survey conducted in 2008 by the Social Weather Stations, commissioned by the Forum for Family Planning and Development (FFPD), a non-government advocacy group, showed that 68 percent of Filipinos agree that there should be a law requiring government to distribute legal contraceptives. SWS President and RH Bill proponent, \"Mahar Mangahas\" reported that the \"survey found 71 percent in favor [of the RH Bill], 21 percent undecided, and a mere 8 percent opposed. Among those who originally knew of the bill, the score is 84 percent in favor, and 6 percent opposed. Among those who learned of the bill for the first time because of the survey, the score is 59 percent in favor, versus 11 percent opposed. Pulse Asia reported that in an October 2008 survey \"most Filipinos are aware of the reproductive health bill pending at the House of Representatives (68%) and are in favor of the bill (63%)\". In December 2010, Pulse Asia announced based on the results of an October 2010 survey, 69% of the Filipinos are in favor of the bill.\n\nPresident of Prolife Philippines, Lito Atienza, said that the surveys conducted by SWS and Pulse Asia were misleading, because the participants were not fully informed of the bill, were merely aware of it, and informed that it was about health and \"modern methods\". Instead he referred to the Filipino Family survey of December 2009 conducted by the HB&A International (an affiliate of Louis Harris & Associates) together with the personnel of Asia Research Organization (the Philippine affiliate of Gallup International). The survey concluded that 92% of people in metropolitan Manila rejected the bill, \"85 percent are not aware that once passed the RH bill would allow teenagers to secure 'abortifacient devices and substances' without their parents’ knowledge and consent...90 percent do not agree that Congress should appropriate P2 billion to the detriment of other essential medicines for free children’s vaccinations, treatment of dreaded diseases and other more important health and medical concerns.\" Mangahas acknowledged that the SWS surveys did not include the penalties.\n\nA TV Debate was also hosted by ABS-CBN last May 2011. Leaders of both sides, including Rep. Lagman and Rep. Golez were present. According to the ABS-CBN news which reported on the results: \"In the SMS poll, 69.58% of votes cast reject the RH bill while 30.42% support it\". In the separate online poll held on the Harapan microsite that livestreamed the debate, majority voted against the bill at the very end of the debate.\n\nOn TV5's Debate \"Hamon sa Pagbabago\" on August 21, 2011, the studio audience voted 100% against the bill, while 58.7% of the viewers voted against the RH Bill via text messaging, versus 41.3% in favor.\n\nThe online poll conducted by the \"Philippine Star\" published on May 18, 2011, showed that 56% were against the RH Bill, while 44% were in favor.\n\nBeginning in late 2010, there were rallies for and against the bill.\n\nThere is mandatory sexuality education starting grade 5, and \"malicious disinformation\" is penalized. All health care service providers which provide reproductive health services, including faith-based hospital administrators, may be imprisoned or fined if they refuse to provide family planning services such as tubal ligation and vasectomy. The same may happen to employers who do not provide free services to employees. Imprisonment ranges from one to six months or a fine ranging from ten thousand pesos (P10,000.00) to fifty thousand pesos (P50,000.00). Former Finance Secretary Roberto de Ocampo stated that these punitive provisions \"are tantamount to an affront to civil liberties and smack of religious persecution\".\n\nDefending the bill, Felipe Medalla, former dean of the University of the Philippines School of Economics, said that \"although the poor’s access to family planning services can be improved even without the law, the absence of the law makes it easier to block the program\".\n\nThe head of the Roman Catholic Church in the Philippines, Archbishop Luis Antonio Tagle opposes the Reproductive Health Bill, along with abortion and contraception. Because 81% of Filipinos are Catholics, the Catholic Church exerts a strong influence in public and moral life. Its staunch opposition to the bill has drawn the controversy among non-Catholics and Catholics alike who support the bill whereby many invoke the principle of separation of church and state.\n\nFr. Joaquin Bernas, S.J, one of the drafters of the Philippine Constitution and a prominent lawyer and writer, explained that the concept of separation of church and state is directed towards the state, rather than the church, as it is a political concept. Technically it means \"non-establishment of religion,\" as the Constitution states, \"No law shall be passed respecting an establishment of religion.\" It means that the state should be guided by the principle that it should support no specific religion and so government funding should not be allocated for building churches or mosques and not favor any particular religion. It does not prevent the church, parents, supervisors, teachers and other moral educators from expressing their views and educating their wards on the morality of their personal and social actions. The Catholic Church also states that their stand is based on secular reasons and natural law that are both acceptable to non-Catholics as well. Proponents, on the other hand, state that the church should not meddle in matters of the state and should focus on religious matters, not political matters.\n\nThe national debate is seen as part of a wider culture war. Passing it or not passing it of the bill has negative implications depending on the views. Proponents state that the not passing the bill will make the Philippines no longer be a backward state and unable to achieve the Millennium Development Goals, especially the points on poverty alleviation and maternal health. It will mean reneging on international commitments and will slow down modernization. Also, the poor will not have free access to family planning support that many have want, and thus will have more children than they can care for and will not have the money to invest in education to break the intergenerational poverty they are trapped in. Proponents also accuse the Catholic Church of holding the Philippines \"hostage\" and violating the separation of church and state. They argue that a decreased population growth will lead to improved quality of life and economic development.\n\nDepartment of Health is proposing 13.7 billion pesos to be fund the RH Bill if it is passed in 2012, according to Senator Pia Cayetano.\n\nFilipinos for Life, an anti-abortion organization, claimed that the bill was funded by foreign population control groups, a claim that Rep. Edcel Lagman denied as \"an old yarn which is destitute of factual basis\", saying that the lobby opposing the bill was the one which was backed by the \"wealthy Catholic hierarchy with the aid of dozens of lay organizations”.\n\nYoung Nine Legislators (Y9L)—including Aliah Dimaporo, Lucy Torres-Gomez, Karlo Alexei Nograles—said that “The proposed P3 billion appropriation for the RH bill, if put towards education, can help secure the future of young Filipinos. That amount can build 4,644 new classrooms…or it can subsidize the college education of 300,000 scholars—a chance for underprivileged student achievers to earn their diploma”.\n\nLagman on the other hand said that both these priorities are important but with a burgeoning population the budget will become even tighter, thus population growth is a major issue.\n\nEuropean Union Ambassador to the Philippines Alistair MacDonald said \"We have all seen the figures on illegal abortion a year in the Philippines and I very much hope that both Houses of Congress will take these issues into account in producing a reproductive health legislation which will really help people make their own choices and to provide for their families\".\n\nMacDonald said that lack of effective access to reproductive health services in the Philippines was \"antithetical\" to the country’s struggle against poverty and \"It seems to me extremely unlikely that the Philippines will be able to meet its commitment under the MDGs under the present policy\". MacDonald noted that the total fertility rate for the richest quintile of the population is 2.0, while the total fertility rate of the poorest quintile is 5.9. The total fertility rate for women with a college education is 2.3, about half that of women with only elementary education (4.5). He mentioned that the lack of access to RH services is anti-women, citing the slow decline in the maternal mortality ratio in the Philippines. He also said surveys suggest that the total wanted fertility rate for the Philippines is 2.4 children, or below the actual TFR of 3.3 children.\n\nOn January 31, 2011, six different bills were consolidated into a single RH Bill which was then unanimously approved for plenary debate by the House Committee on Population and Family Relations. On February 7, 2011, the bill was scheduled to go before the House Appropriations Committee. On February 16, 2011, the bill was endorsed by the House Appropriations Committee with amendment and referred back to the Population Committee for finalizing the language.\n\nPresident Noynoy Aquino during the presidential campaign said that it confounds him why he is always associated with the RH Bill and reiterated that he is neither an author nor a co-author and did not sign the committee report regarding the bill. He said that \"he will fully support the crafting of a firm policy that will address the serious problem on population\" At the same time, Aquino said that \"artificial contraception was a matter of choice and conscience and that health professionals who fool people into using artificial contraceptives should be penalized. As a Catholic, Aquino said he himself was not promoting artificial contraception but believes that the government should be able to provide it to Filipinos who ask for it\". Aquino stressed, \"I’m a Catholic, I'm not promoting it. My position is more aptly called responsible parenthood rather than reproductive health.\"\n\nAccording to Rina Jimenez David who is pro-RH, during the “Women Deliver Philippines” Conference held September 2010, Dinky Soliman, Aquino's Secretary of Social Welfare and Development, said that \"choice and access” constituted the keystone of the Aquino government’s policy, reiterating the administration’s support for the pending reproductive health bills.\n\nOn December 2010, the Cabinet and the CBCP agreed to have a joint campaign providing full information on the advantages and risks of contraceptives, natural and artificial family planning and responsible parenthood. They have established a technical working group for this purpose. They also agreed that government will not be an \"instrument to enforce or violate the conscience of the people about these issues.\"\n\nHowever, by April 2011 Aquino has given his full support to the entire RH Bill in a speech at the University of the Philippines Diliman and promised to push for its passage even at the \"risk of excommunication.\"\n\nSenate President Juan Ponce Enrile, Congressman Roilo Golez and Buhay party-list separately filed bills that seek to restrict abortion and birth control use. These bills have been seen either as a nullification of the RH Bill, its alternative, or as a way of achieving unity among the populace, since the RH Bill proponents have stated their concern in preventing abortion.\n\nPresidential candidate Gilbert Teodoro or Gibo suggested a cash transfer from the government to individuals wanting access to family planning methods, whether natural or artificial. The individuals can then make use of the cash they receive to purchase birth control devices they may choose, thus guaranteeing freedom of choice.\n\nThe Loyola School of Theology and the John J. Carroll Institute on State and Church Issues issued nine talking points on the RH Bill. Among other points, they proposed a study on the meaning of conception in the Constitution, and if it means fertilization, abortifacients \"are to be banned even now and regardless of whether the RH Bill is passed\". They also proposed \"parallel programs for providing information and training, one for Natural Family Planning (NFP) and another for artificial methods of family planning\". Columnist Jose Sison of the \"Philippine Star\" criticized that \"a Catholic School of theology has actually proposed in public, the use of tax payers’ money to train Filipinos to employ methods that are objectively and intrinsically evil\" and cites \"empirical evidence and scientific proofs confirming the harmful and evil effects of contraceptives to individuals and to society.\"\n\nIn September 2010, Aquino, during this visit to the United States, reiterated his stand that he is in favor of responsible parenthood and respects the decision of each couple as to the number of children they want, and if they need the government support for contraception, the government will provide it. This statement has created a furor as Catholic church leaders say that Aquino has sold out the Filipino soul in exchange for some \"measly\" aid from the United States. The President of the Catholic Bishops Conference said that there could be an excommunication of the President if he continues his stance. Pro-RH Bill snators encouraged the President to be steadfast to do his duties towards the state. The President's spokesperson Edwin Lacierda explained that the President \"has not changed his stand\" and is reaching out to the prelates and said that he has not made any decision in support of the Reproductive Health Bill as he was still studying the document. Lacierda said that the Executive Branch \"is not involved in the passage of the RH bill, saying the measure's fate rests solely on the legislative branch.\"\n\nFilipino Freethinkers, an association of agnostics, atheists, progressives, etc., very active in the fight in favor of the RH bill, stepped up the pressure, creating more controversy that fired up renewed interest in the bill on both sides. On September 30, 2010, one of the freethinkers, Carlos Celdran staged a protest action against the Catholic Church, holding a sign which read \"DAMASO,\" a reference to the villainous, corrupt clergyman Father Dámaso of the novel \"Noli Me Tangere\" by Filipino revolutionary writer Jose Rizal, and shouting \"stop getting involved in politics!\" A fan page, Free Carlos Celdran was created in Facebook, which generated 23,808 fans in 24 hours. Francisco Montalvan of the \"Inquirer\" said that in the end the Damasos are the scheming, corrupt and deceptive people, implying that the \"pro-death advocates\" are these, while the Cardinal Rosales who started a nationwide fund for the poor is very far from Damaso. Meanwhile, the Imam Council of the Philippines, the top leaders of the Moslem population which at 4.5 million constitutes 5% of the Philippine population, declared that they are against contraceptives since using them \"underestimates God\" and \"makes one lose morality in the process.\"\n\nDuring the first public hearing on 24 Nov, the chair of the Committee on Population handling the bill said that there is no instruction from the Speaker of the House to expedite the bill. Upon the call of anti-RH congressmen, the Committee Chair decided to refer the bill also to the Committee on Health since the bill is about Reproductive Health. Leader of the pro-RH group, Elizabeth Ansioco, said that the bill is doomed if it is referred to the Committee on Health. Anti-RH Deputy Speaker Congressman Pablo Garcia said the members of the Committee on Health knew of the WHO announcement on the carcinogenicity of combined estrogen-progestogen oral contraceptives.\n\nHouse Speaker Belmonte said that Congress is not likely to rush the legislation of the bill and will tackle it in plenary early next year. Belmonte said it is better that highly contentious bills be given more attention.\n\nOn 3 December, the Senate cut the proposed budget of P 880M for contraceptives down to P 8M for condoms since other contraceptives violated the Constitution's ban on abortifacients, and Senator Tito Sotto III said that his constituents never asked for contraceptives.\n\nOn 27 July 2012, the Speaker of the House decided to put to a vote, by August 7, 2012, on whether the debates have to be terminated. Meanwhile, six co-authors of the bill withdrew support, with the head of the minority group of the house declaring that eight of their group are withdrawing their previous support for the bill.\n\nAt 3 in the morning on December 13, 2012, the House of Representatives voted on second reading in favor of the bill with 113–109 while five representatives abstained. In the upper house, the Senate voted, on December 18, 2012, to pass the bill on second reading with 13–8, while Senators Sergio Osmeña III and Lito Lapid were absent.\n\nOn the same day, both houses passed the bill on the third and final reading. Members of the House of Representatives voted 133–79, while seven representatives abstained. The Senate registered 13–8, the same result as the second reading.\n\nOn December 19, 2012, both versions of the bill were passed to the Bicameral Committee to produce a final version to be signed by the President Aquino. The committee quickly passed the bill in just one session. It was transmitted back to the House of Representatives and the Senate, which both ratified the bill, with the Senate voting 11–5 in favor of ratification, and the House of Representatives voting via voice vote\n\nOn December 21, 2012, President Aquino signed the bill into law, codifying the bill as Republic Act No. 10354, otherwise known as the \"Responsible Parenthood and Reproductive Health Act of 2012\". News of the signing was announced by House Majority Leader Neptali Gonzales II on December 28, 2012.\n\nIn response to petitions challenging the law's constitutionality, the Supreme Court voted 10-5 on March 19, 2013 to issue a status quo ante order halting the implementation of the law for four months.\nOral arguments were set for June 18 but postponed until July 9 after the Supreme Court received additional petitions and interventions.\n\nDuring oral arguments, several justices indicated that the court \"does not seem to be the right forum–at least for now.\" Ir could not settle medical issues, such as whether any contraceptives to be made available were actually abortifacients. Chief Justice Maria Lourdes Sereno said the court might have no choice but to exercise \"judicial restraint\" on the 15 petitions opposing the law.\n\nOn July 16, the justices voted 8-7 to extend the status quo ante order, which would have expired the next day \"until further orders effective immediately.\"\nOral arguments concluded on August 27, with the petitioners against and for the law being instructed to submit memorandums within 60 days.\n\nOn April 8, 2014, the Supreme Court upheld the constitutionality of the law. The justices, however, struck down eight provisions of the law partially or in full.\n\nIn 2015, the Supreme Court's issued a temporary restraining order on certain provisions of the law, forbidding the distribution of contraceptive implants.\n\nNotwithstanding the restraining order on certain methods of contraception, in September 2018, President Duterte decided to ensure, within 2018, free contraception for 6 million women with unmet needs for modern family planning - in 2018, for 2 million women identified as poor, and later for further 4 million women.\n\n\n"}
{"id": "4644634", "url": "https://en.wikipedia.org/wiki?curid=4644634", "title": "Rhetorical modes", "text": "Rhetorical modes\n\nRhetorical modes (also known as modes of discourse) describe the variety, conventions, and purposes of the major kinds of language-based communication, particularly writing and speaking. Four of the most common rhetorical modes and their purpose are narration, description, exposition, and argumentation. The first codification of these rhetorical modes was by Samuel P. Newman in \"A Practical System of Rhetoric\" in 1827. \n\nThe purpose of narration is to tell a story or narrate an event or series of events. This writing mode frequently uses the tools of descriptive writing. Narration is an especially useful tool for sequencing or putting details and information into some kind of logical order, usually chronological. Working with narration helps us see clear sequences separate from all other mental functions. Examples include:\n\nThe purpose of description is to re-create, invent, or visually present a person, place, event, or action so that the reader can picture that which is being described. Descriptive writing can be found in the other rhetorical modes. Examples include:\n\nExpository writing is a type of writing where the purpose is to explain, inform, or even describe. It is considered to be one of the four most common rhetorical modes.\n\nThe purpose of expository writing is to explain and analyze information by presenting an idea, relevant evidence, and appropriate discussion. In narrative contexts (such as history and fiction), exposition provides background information to teach or entertain. In other nonfiction contexts (such as technical communication), the purpose is to teach and inform. Examples include:\n\nThe purpose of argumentation (also called \"persuasive writing\") is to prove the validity of an idea, or point of view, by presenting sound reasoning, discussion, and argument to thoroughly convince the reader. Persuasive writing/Persuasion is a type of argumentation with the additional aim to urge the reader to take some form of action. Examples include:\n\nAnother form of persuasive rhetoric is satirical rhetoric, or using humor in order to make a point about some aspect of life or society. Perhaps the most famous example is Jonathan Swift's \"A Modest Proposal\".\n\nEach fiction-writing mode has its own purposes and conventions. Agent and author Evan Marshall identifies five fiction-writing modes: action, summary, dialogue, feelings/thoughts, and background. Author and writing-instructor Jessica Page Morrell lists six delivery modes for fiction-writing: action, exposition, description, dialogue, summary, and transition. Author Peter Selgin refers to \"methods\", including action, dialogue, thoughts, summary, scene, and description.\n\n\n\n"}
{"id": "7338160", "url": "https://en.wikipedia.org/wiki?curid=7338160", "title": "Self-confidence", "text": "Self-confidence\n\nThe concept of self-confidence self-assurance in one's personal judgment, ability, power, etc. One increases self-confidence from experiences of having mastered particular activities. It is a positive belief that in the future one can generally accomplish what one wishes to do. Self-confidence is not the same as self-esteem, which is an evaluation of one’s own worth, whereas self-confidence is more specifically trust in one’s ability to achieve some goal, which one meta-analysis suggested is similar to generalization of self-efficacy. Abraham Maslow and many others after him have emphasized the need to distinguish between self-confidence as a generalized personality characteristic, and self-confidence with respect to a specific task, ability or challenge (i.e. self-efficacy). Self-confidence typically refers to general self-confidence. This is different from self-efficacy, which psychologist Albert Bandura has defined as a “belief in one’s ability to succeed in specific situations or accomplish a task” and therefore is the term that more accurately refers to specific self-confidence. Psychologists have long noted that a person can possess self-confidence that he or she can complete a specific task (self-efficacy) (e.g. cook a good meal or write a good novel) even though they may lack general self-confidence, or conversely be self-confident though they lack the self-efficacy to achieve a particular task (e.g. write a novel). These two types of self-confidence are, however, correlated with each other, and for this reason can be easily conflated.\n\nIdeas about the causes and effects of self-confidence have appeared in English language publications describing characteristics of a sacrilegious attitude toward God, the character of the British empire, and the culture of colonial-era American society (where it seemed to connote arrogance and be a negative attribute.) \n\nIn 1890, the philosopher William James in his \"Principles of Psychology\" wrote, “Believe what is in the line of your needs, for only by such belief is the need fulled ... Have faith that you can successfully make it, and your feet are nerved to its accomplishment,” expressing how self-confidence could be a virtue. That same year, Dr. Frederick Needham in his presidential address to the opening of the British Medical Journal’s Section of Psychology praised a progressive new architecture of an asylum accommodation for insane patients as increasing their self-confidence by offering them greater “liberty of action, extended exercise, and occupation, thus generating self-confidence and becoming, not only excellent tests of the sanity of the patient, but operating powerfully in promoting recovery.” In doing so, he seemed to early on suggest that self-confidence may bear a scientific relation to mental health.\n\nWith the arrival of World War I, psychologists praised self-confidence as greatly decreasing nervous tension, allaying fear, and ridding the battlefield of terror; they argued that soldiers who cultivated a strong and healthy body would also acquire greater self-confidence while fighting. At the height of the Temperance social reform movement of the 1920s, psychologists associated self-confidence in men with remaining at home and taking care of the family when they were not working. During the Great Depression, Philip Eisenberg and Paul Lazerfeld noted how a sudden negative change in one’s circumstances, especially a loss of a job, could lead to decreased self-confidence, but more commonly if the jobless person believes the fault of his unemployment is his. They also noted how if individuals do not have a job long enough, they became apathetic and lost all self-confidence.\n\nIn 1943, Abraham Maslow in his paper “A Theory of Human Motivation” argued that an individual only was motivated to acquire self-confidence (one component of “esteem”) after he or she had achieved what they needed for physiological survival, safety, and love and belonging. He claimed that satisfaction of self-esteem led to feelings of self-confidence that, once attained, led to a desire for “self-actualization.\" As material standards of most people rapidly rose in developed countries after World War II and fulfilled their material needs, a plethora of widely cited academic research about-confidence and many related concepts like self-esteem and self-efficacy emerged.\n\nSocial psychologists have found self-confidence to be correlated with other psychological variables within individuals, including saving money, how individuals exercise influence over others, and being a responsible student. Marketing researchers have found that general self-confidence of a person is negatively correlated with their level of anxiety.\n\nSome studies suggest various factors within and beyond an individual's control that affect their self-confidence. Hippel and Trivers propose that people will deceive themselves about their own positive qualities and negative qualities of others so that they can display greater self-confidence than they might otherwise feel, thereby enabling them to advance socially and materially. Others have found that new information about an individual’s performance interacts with an individual’s prior self-confidence about their ability to perform. If that particular information is negative feedback, this may interact with a negative affective state (low self-confidence) causing the individual to become demoralized, which in turn induces a self-defeating attitude that increases the likelihood of failure in the future more than if they did not lack self-confidence. On the other hand, some also find that self-confidence increases a person's general well-being and one's motivation and therefore often performance. It also increases one's ability to deal with stress and mental health.\n\nA meta-analysis of 12 articles found that generally when individuals attribute their success to a stable cause (a matter under their control) they are less likely to be confident about being successful in the future. If an individual attributes their failure to an unstable cause (a factor beyond their control, like a sudden and unexpected storm) they are less likely to be confident about succeeding in the future. Therefore, if an individual believes he/she and/or others failed to achieve a goal (e.g. give up smoking) because of a factor that was beyond their control, he or she is more likely to be more self-confident that he or she can achieve the goal in the future. Whether a person in making a decision seeks out additional sources of information depends on their level of self-confidence specific to that area. As the complexity of a decision increases, a person is more likely to be influenced by another person and seek out additional information. However, people can also be relatively self-confident about what they believe if they consult sources of information that agree with their world views (e.g. New York Times for liberals, Fox News for conservatives), even if they do not know what will happen tomorrow. Several psychologists suggest that people who are self-confident are more willing to examine evidence that both supports and contradicts their attitudes. Meanwhile, people who are less self-confident about their perspective and are more defensive about them may prefer proattitudinal information over materials that challenge their perspectives. (see also Byrne, 1961; Olson & Zanna, 1982b; for related views in other domains, see Tesser, 2001).\n\nAn individual’s self-confidence can vary in different environments, such as at home or in school, and with respect to different types of relationships and situations. In relation to general society, some have found that the more self-confident an individual is, the less likely they are to conform to the judgments of others. Leon Festinger found that self-confidence in an individual’s ability may only rise or fall where that individual is able to compare themselves to others who are roughly similar in a competitive environment. Furthermore, when individuals with low self-confidence receive feedback from others, they are averse to receiving information about their relative ability and negative informative feedback, and not averse to receiving positive feedback.\n\nPeople with high self-confidence can easily impress others, as others perceive them as more knowledgeable and more likely to make correct judgments, despite the fact that often a negative correlation is sometimes found between the level of their self-confidence and accuracy of their claims. When people are uncertain and unknowledgeable about a topic, they are more likely to believe the testimony, and follow the advice of those that seem self-confident. However, expert psychological testimony on the factors that influence eyewitness memory appears to reduce juror reliance on self-confidence.\n\nPeople are more likely to choose leaders with greater self-confidence than those with less self-confidence. Heterosexual men who exhibit greater self-confidence than other men are more likely to attract single and partnered women. Salespeople who are high in self-confidence are more likely to set higher goals for themselves and therefore more likely to stay employed. yield higher revenues and customer service satisfaction In relation to leadership, leaders with high self-confidence are more likely to influence others through persuasion rather than coercive means. Individuals low in power and thus in self-confidence are more likely to use coercive methods of influence and to become personally involved while those low in self-confidence are more likely to refer problem to someone else or resort to bureaucratic procedures to influence others (e.g. appeal to organizational policies or regulations). Others suggest that self-confidence does not affect style of leadership but is only correlated with years of supervisory experience and self-perceptions of power.\n\nSocial scientists have found ways in which self-confidence seems to operate differently within various groups in society.\n\nIn children, self-confidence emerges differently than adults. For example, Fenton suggested that only children as a group are more self-confident than other children. Zimmerman claimed that if children are self-confident they can learn they are more likely to sacrifice immediate recreational time for possible rewards in the future. enhancing their self-regulative capability. By adolescence, youth that have little contact with friends tend to have low self-confidence. Successful performance of children in music also increases feelings of self-confidence, increasing motivation for study.\n\nMany studies focus on students in school. In general, students who perform well have increased confidence which likely in turn encourages students to take greater responsibility to successfully complete tasks. Students who perform better receive more positive evaluations report and greater self-confidence. Low achieving students report less confidence and high performing students report higher self-confidence. Teachers can greatly affect the self-confidence of their students depending on how they treat them. In particular, Steele and Aronson established that black students perform more poorly on exams (relative to white students) if they must reveal their racial identities before the exam, a phenomenon known as “stereotype threat.” Keller and Dauenheimer find a similar phenomena in relation to female student’s performance (relative to male student's) on math tests Sociologists of education Zhou and Lee have observed the reverse phenomena occurring amongst Asian-Americans, whose confidence becomes tied up in expectations that they will succeed by both parents and teachers and who claim others perceive them as excelling academically more than they in fact are.\n\nIn one study of UCLA students, males (compared to females) and adolescents with more siblings (compared to those with less) were more self-confident. Individuals who were self-confident specifically in the academic domain were more likely to be happy but higher general self-confidence was not correlated with happiness. With greater anxiety, shyness and depression, emotionally vulnerable students feel more lonely due to a lack of general self-confidence. Another study of first year college students found men to be much more self-confident than women in athletic and academic activities. In regards to inter-ethnic interaction and language learning, studies show that those who engage more with people of a different ethnicity and language become more self-confident in interacting with them.\n\nIn the aftermath of the first wave of feminism and women’s role in the labor force during the World War, Maslow argued that some women who possessed a more “dominant” personality were more self-confident and therefore would aspire to and achieve more intellectually than those that had a less “dominant” personality—even if they had the same level of intelligence as the “less dominant” women. However, Phillip Eisenberg later found the same dynamic among men.\n\nAnother common finding is that males who have low generalized self-confidence are more easily persuaded than males of high generalized self-confidence. Some have found that women who are either high or low in general self-confidence are more likely to be persuaded to change their opinion than women with medium self-confidence. However, when specific high confidence (self-efficacy) is high, generalized confidence plays less of a role in affecting their ability to carry out the task. Research finds that females report self-confidence levels in supervising subordinates proportionate to their experience level, while males report being able to supervise subordinates well regardless of experience. Women tend to respond less to negative feedback and be more averse to negative feedback than men. Barber and Odean find that male common stock investors trade 45% more than their female counterparts, which they attribute greater self-confidence (though also recklessness) of men, reducing men's net returns by 2.65 percentage points per year versus women's 1.72 percentage points. Niederle and Westerlund found that men are much more competitive and obtain higher compensation than women and that this difference is due to differences in self-confidence, while risk and feedback-aversion play a negligible role. Some scholars partly attribute the fact to women being less likely to persist in engineering college than men to women's diminished sense of self-confidence.\n\nEvidence also has suggested that women who are more self-confident may received high performance evaluations but not be as well liked as men that engage in the same behavior. However confident women were considered a better job candidates than both men and women who behaved modestly This may be related to gender roles, as a study found that after women who viewed commercials with women in traditional gender roles, they appeared less self-confident in giving a speech than after viewing commercials with women taking on more masculine roles. Such self-confidence may also be related to body image, as one study found a sample of overweight people in Australia and the US are less self-confident about their body’s performance than people of average weight, and the difference is even greater for women than for men. Others have found that if a baby child is separated from their mother at birth the mother is less self-confident in their ability to raise that child than those mothers who are not separated from their children, even if the two mothers did not differ much in their care-taking skills. Furthermore, women who initially had low self-confidence are likely to experience a larger drop of self-confidence after separation from their children than women with relatively higher self-confidence.\n\nSome have suggested that self-confidence is more adaptive in cultures where people are not very concerned about maintaining harmonious relationships. But in cultures that value positive feelings and self-confidence less, maintenance of smooth interpersonal relationships are more important, and therefore self-criticism and a concern to save face is more adaptive. For example, Suh et al. (1998) argue that East Asians are not as concerned as maintaining self-confidence as Americans and many even find Asians perform better when they \"lack\" confidence.\n\nMany sports psychologists have noted the importance of self-confidence in winning athletic competitions. Amongst athletes, gymnasts who tend to talk to themselves in an instructional format tended to be more self-confident than gymnasts that did not. Researchers have found that self-confidence is also one of the most influential factors in how well an athlete performs in a competition. In particular, \"robust self-confidence beliefs\" are correlated with aspects of \"mental toughness,\" or the ability to cope better than your opponents with many demands and remain determined, focused and in control under pressure. In particular, Bull et al. (2005) make the distinction between \"robust confidence\" which leads to tough thinking, and \"resilient confidence\" which involves over-coming self doubts and maintaining self-focus and generates \"tough thinking.\" These traits enable athletes to \"bounce back from adversity.\" When athletes confront stress while playing sports, their self-confidence decreases. However feedback from their team members in the form of emotional and informational support reduces the extent to which stresses in sports reduces their self-confidence. At high levels of support, performance related stress does not affect self-confidence.\n\nOne of the earliest measures of self-confidence used a 12-point scale centered on zero, ranging from a minimum score characterizing someone who is “timid and self-distrustful, Shy, never makes decisions, self effacing” to an upper extreme score representing someone who is “able to make decisions, absolutely confident and sure of his own decisions and opinions.”\n\nSome have measured self-confidence as a simple construct divided into affective and cognitive components: anxiety as an affective aspect and self-evaluations of proficiency as a cognitive component.\n\nThe more context-based Personal Evaluation Inventory (PEI), developed by Shrauger (1995), measures specific self-esteem and self-confidence in different aspects (speaking in public spaces, academic performance, physical appearance, romantic relationships, social interactions, athletic ability, and general self-confidence score. Other surveys have also measured self-confidence in a similar way by evoking examples of more concrete activities (e.g. making new friends, keeping up with course demands, managing time wisely, etc.). The Competitive State Anxiety Inventory-2 (CSAI-2) measures on a scale of 1 to 4 how confident athletes feel about winning an upcoming match. Likewise, the Trait Robustness of Sports-Confidence Inventory (TROSCI) requires respondents to provide numerical answers on a nine-point scale answering such questions about how much one's self-confidence goes up and down, and how sensitive one's self-confidence is to performance and negative feedback.\n\nOthers, skeptical about the reliability of such self-report indices, have measured self-confidence by having examiners assess non-verbal cues of subjects, measuring on a scale of 1 to 5 whether the individual\n\nVarious systemic theories exist that are related to self-confidence.\n\nThe Wheel of Wellness was the first theoretical model of Wellness based in counseling theory. It is a model based on Adler's individual psychology and cross-disciplinary research on characteristics of healthy people who live longer and with a higher quality of life. The Wheel of Wellness includes five life tasks that relate to each other: spirituality, self-direction, work and leisure, friendship, and love. There are 15 subtasks of self-direction areas: sense of worth, sense of control, realistic beliefs, emotional awareness and coping, problem solving and creativity, sense of humor, nutrition, exercise, self-care, stress management, gender identity, and cultural identity. There are also five second-order factors, the Creative Self, Coping Self, Social Self, Essential Self, and Physical Self, which allow exploration of the meaning of wellness within the total self. In order to achieve a high self-esteem, it is essential to focus on identifying strengths, positive assets, and resources related to each component of the Wellness model and using these strengths to cope with life challenges.\n\nImplicitly measured self-esteem has been found to be weakly correlated with explicitly measured self-esteem. This leads some critics to assume that explicit and implicit self-confidence are two completely different types of self-esteem. Therefore, this has drawn the conclusion that one will either have a distinct, unconscious self-esteem OR they will consciously misrepresent how they feel about themselves. Recent studies have shown that implicit self-esteem doesn't particularly tap into the unconscious, rather that people consciously overreport their levels of self-esteem. Another possibility is that implicit measurement may be assessing a different aspect of conscious self-esteem altogether. Inaccurate self-evaluation is commonly observed in healthy populations. In the extreme, large differences between oneʼs self-perception and oneʼs actual behavior is a hallmark of a number of disorders that have important implications for understanding treatment seeking and compliance.\n\n"}
{"id": "3111057", "url": "https://en.wikipedia.org/wiki?curid=3111057", "title": "Sensory threshold", "text": "Sensory threshold\n\nIn psychophysics, sensory threshold is the weakest stimulus that an organism can detect. Unless otherwise indicated, it is usually defined as the weakest stimulus that can be detected half the time, for example, as indicated by a point on a probability curve. Methods have been developed to measure thresholds in any of the senses.\n\nThe first systematic studies to determine sensory thresholds were conducted by Ernst Heinrich Weber, a physiologist and pioneer of experimental psychology at the Leipzig University. His experiments were intended to determine the absolute and difference, or differential, thresholds. Weber was able to define absolute and difference threshold statistically which lead to the establishment of Weber’s Law and the concept of just noticeable difference to describe threshold perception of stimuli.\n\nFollowing Weber’s work, Gustav Fechner, a pioneer of psychophysics, studied the relationship between the physical intensity of a stimulus and the psychologically perceived intensity of the stimulus. Comparing the measured intensity of sound waves with the perceived loudness, Fechner concluded that the intensity of a stimulus changes in proportion to the logarithm of the stimulus intensity. His findings would lead to the creation of the decibel scale.\n\nSeveral different sensory thresholds have been defined;\nMeasuring and Testing Sensory Thresholds\n\nDefining and measuring sensory thresholds requires setting the sensitivity limit such that the perception observations lead to the absolute threshold. The level of sensitivity is usually assumed to be constant in determining the threshold limit. There are three common methods used to determine sensory thresholds:\nIn measuring sensory threshold, noise must be accounted for. Signal noise is defined as the presence of extra, unwanted energy in the observational system which obscures the information of interest. As the measurements come closer to the absolute threshold, the variability of the noise increases causing the threshold to be obscured. Different types of internal and external noise include excess stimuli, nervous system over- or under-stimulation, and conditions that falsely stimulate nerves in the absence of external stimuli.\n\nA universal absolute threshold is difficult to define a standard because of the variability of the measurements. While sensation occurs at the physical nerves, there can be reasons why it is not consistent. Age or nerve damage can affect sensation. Similarly, psychological factors can affect perception of physical sensation. Mental state, memory, mental illness, fatigue, and other factors can alter perception.\n\nAviation use. When related to motion in any of the possible six degrees of freedom (6-DoF), the fact that sensory\nthresholds exist is why it is essential that aircraft have blind-flying instruments. Sustained flight in cloud is not possible by `seat-of-the-pants' cues alone since errors build up due to aircraft movements below the pilot's sensory threshold,\nultimately leading to loss of control. \n\n"}
{"id": "12310990", "url": "https://en.wikipedia.org/wiki?curid=12310990", "title": "Separation of protection and security", "text": "Separation of protection and security\n\nIn computer sciences the separation of protection and security is a design choice. Wulf et al. identified protection as a mechanism and security as a policy, therefore making the protection-security distinction a particular case of the separation of mechanism and policy principle.\n\nThe adoption of this distinction in a computer architecture, usually means that protection is provided as a fault tolerance mechanism by hardware/firmware and kernel, whereas the operating system and applications implement their security policies. In this design, security policies rely therefore on the protection mechanisms and on additional cryptography techniques.\n\nThe major hardware approach for security or protection is the use of hierarchical protection domains. Prominent example of this approach is ring architecture with \"supervisor mode\" and \"user mode\"). Such approach adopts a policy already at the lower levels (hardware/firmware/kernel), restricting the rest of the system to rely on it. Therefore, the choice to distinguish between protection and security in the overall architecture design implies rejection of the hierarchical approach in favour of another one, the capability-based addressing.\n\nExamples of models with protection and security separation include: access matrix, UCLA Data Secure Unix, take-grant and filter. Such separation is not found in models like: high-water mark, Bell–LaPadula (original and revisited), information flow, strong dependency and constraints.\n\n"}
{"id": "15954084", "url": "https://en.wikipedia.org/wiki?curid=15954084", "title": "Shear and moment diagram", "text": "Shear and moment diagram\n\nShear and bending moment diagrams are analytical tools used in conjunction with structural analysis to help perform structural design by determining the value of shear force and bending moment at a given point of a structural element such as a beam. These diagrams can be used to easily determine the type, size, and material of a member in a structure so that a given set of loads can be supported without structural failure. Another application of shear and moment diagrams is that the deflection of a beam can be easily determined using either the moment area method or the conjugate beam method.\n\nAlthough these conventions are relative and any convention can be used if stated explicitly, practicing engineers have adopted a standard convention used in design practices.\n\nThe normal convention used in most engineering applications is to label a positive shear force one that spins an element clockwise (up on the left, and down on the right). Likewise the normal convention for a positive bending moment is to warp the element in a \"u\" shape manner (Clockwise on the left, and counterclockwise on the right). Another way to remember this is if the moment is bending the beam into a \"smile\" then the moment is positive, with compression at the top of the beam and tension on the bottom. \n\nIn structural engineering and in particular concrete design the positive moment is drawn on the tension side of the member. This convention puts the positive moment below the beam described above. A convention of placing moment diagram on the tension side allows for frames to be dealt with more easily and clearly. Additionally placing the moment on the tension side of the member shows the general shape of the deformation and indicates on which side of a concrete member rebar should be placed, as concrete is weak in tension.\n\nWith the loading diagram drawn the next step is to find the value of the shear force and moment at any given point along the element. For a horizontal beam one way to perform this is at any point to \"chop off\" the right end of the beam.\n\nThe example below includes a point load, a distributed load, and an applied moment. The supports include both hinged supports and a fixed end support. The first drawing shows the beam with the applied forces and displacement constraints. The second drawing is the loading diagram with the reaction values given without the calculations shown or what most people call a free body diagram. The third drawing is the shear force diagram and the fourth drawing is the bending moment diagram. For the bending moment diagram the normal sign convention was used. Below the moment diagram are the stepwise functions for the shear force and bending moment with the functions expanded to show the effects of each load on the shear and bending functions.\n\nThe example is illustrated using United States customary units. Point loads are expressed in kips (1 kip = 1000 lbf = 4.45 kN), distributed loads are expressed in k/ft (1 k/ft = 1 kip/ft = 14.6 kN/m), moments are expressed in ft-k (1 ft-k = 1 ft-kip = 1.356 kNm), and lengths are in ft (1 ft = 0.3048 m).\n\nThe first step obtaining the bending moment and shear force equations is to determine the reaction forces. This is done using a free body diagram of the entire beam.\n\nThe beam has three reaction forces, \"R\", \"R\" at the two supports and \"R\" at the clamped end. The clamped end also has a reaction couple \"M\". These four quantities have to be determined using two equations, the balance of forces in the beam and the balance of moments in the beam. Four unknowns cannot be found given two independent equations in these unknown variables and hence the beam is statically indeterminate. One way of solving this problem is to use the principle of linear superposition and break the problem up into the superposition of a number of statically determinate problems. The extra boundary conditions at the supports have to be incorporated into the superposed solution so that the deformation of the entire beam is compatible.\n\nFrom the free-body diagram of the entire beam we have the two balance equations\nSumming the forces, we have\nand summing the moments around the free end (A) we have\nWe can solve these equations for \"R\" and \"R\" in terms of \"R\" and \"M\" :\nand\nIf we sum moments about the first support from the left of the beam we have\nIf we plug in the expressions for \"R\" and \"R\" we get the trivial identity \"0 = 0\" which indicates that this equation is not independent of the previous two. Similarly, if we take moments around the second support, we have\nOnce again we find that this equation is not independent of the first two equations. We could also try to compute moments around the clamped end of the beam to get\nThis equation also turns out not to be linearly independent from the other two equations. Therefore, the beam is statically indeterminate and we will have to find the bending moments in segments of the beam as functions of \"R\" and \"M\".\n\nAfter the reaction forces are found, you then break the beam into pieces. The location and number of external forces on the member determine the number and location of these pieces. The first piece always starts from one end and ends anywhere before the first external force.\n\nLet \"V\" and \"M\" be the shear force and bending moment respectively in a cross-section of the first beam segment. As the section of the beam moves towards the point of application of the external force the magnitudes of the shear force and moment may change. This makes the shear force and bending moment a function of the position of cross-section (in this example \"x\").\n\nBy summing the forces along this segment and summing the moments, the equations for the shear force and bending moment are obtained. These equations are:\nand\nTherefore,\n\nTaking the second segment, ending anywhere before the second internal force, we have\nand\nTherefore,\nNotice that because the shear force is in terms of x, the moment equation is squared. This is due to the fact that the moment is the integral of the shear force. The tricky part of this moment is the distributed force. Since the force changes with the length of the segment, the force will be multiplied by the distance after 10 ft. i.e. (x-10) the moment location is defined in the middle of the distributed force, which is also changing. This is where (x+10)/2 is derived from.\n\nAlternatively, we can take moments about the cross-section to get\nAgain, in this case,\n\nTaking the third segment, and summing forces, we have\nand summing moments about the cross-section, we get\nTherefore, \nand\nNotice that the distributed force can now be considered one force of 15 kips acting in the middle of where it is positioned.\n\nTaking the fourth and final segment, a balance of forces gives\nand a balance of moments around the cross-section leads to\nSolving for \"V\" and \"M\", we have\nand\nBy plotting each of these equations on their intended intervals, you get the bending moment and shear force diagrams for this beam. In particular, at the clamped end of the beam, \"x\" = 50 and we have\n\nWe now use the Euler-Bernoulli beam theory to compute the deflections of the four segments. The differential equation that relates the beam deflection (\"w\") to the bending moment (\"M\") is\nwhere \"E\" is the Young's modulus and \"I\" is the area moment of inertia of the beam cross-section.\n\nSubstituting the expressions for \"M\", \"M\", \"M\", \"M\" into the beam equation and solving for the deflection gives us\n\nNow we will apply displacement boundary conditions for the four segments to determine the integration constants.\n\nFor the fourth segment of the beam, we consider the boundary conditions at the clamped end where \"w\" = d\"w\"/d\"x\" = 0 at \"x\" = 50. Solving for \"C\" and \"C\" gives\nTherefore, we can express \"w\" as\nNow, \"w\" = \"w\" at \"x\" = 37.5 (the point of application of the external couple). Also, the slopes of the deflection curves at this point are the same, i.e., \"dw\"/\"dx\" = \"dw\"/\"dx\". Using these boundary conditions and solving for \"C\" and \"C\", we get\nSubstitution of these constants into the expression for \"w\" gives us\nSimilarly, at the support between segments 2 and 3 where \"x\" = 25, \"w\" = \"w\" and \"dw\"/\"dx\" = \"dw\"/\"dx\". Using these and solving for \"C\" and \"C\" gives\nTherefore,\nAt the support between segments 1 and 2, \"x\" = 10 and \"w\" = \"w\" and \"dw\"/\"dx\" = \"dw\"/\"dx\". These boundary conditions give us\nTherefore,\n\nBecause \"w\" = 0 at \"x\" = 25, we can solve for \"M\" in terms of \"R\" to get\nAlso, since \"w\" = 0 at \"x\" = 10, expressing the deflection in terms of \"R\" (after eliminating \"M\") and solving for \"R\", gives\n\nWe can now calculate the reactions \"R\" and \"R\", the bending moments \"M\", \"M\", \"M\", \"M\", and the shear forces \"V\", \"V\", \"V\", \"V\". These expressions can then be plotted as a function of length for each segment.\n\nIt is important to note the relationship between the two diagrams. The moment diagram is a visual representation of the area under the shear force diagram. That is, the moment is the integral of the shear force. If the shear force is constant over an interval, the moment equation will be in terms of x (linear). If the shear force is linear over an interval, the moment equation will be quadratic (parabolic).\n\nAnother note on the shear force diagrams is that they show where external force and moments are applied. With no external forces, the piecewise functions should attach and show no discontinuity. The discontinuities on the graphs are the exact magnitude of either the external force or external moments that are applied. For example, at x = 10 on the shear force diagram, there is a gap between the two equations. This gap goes from -10 to 15.3. The length of this gap is 25.3, the exact magnitude of the external force at that point. At section 3 on the moment diagram, there is a discontinuity of 50. This is from the applied moment of 50 on the structure. The maximum and minimum values on the graphs represent the max forces and moments that this beam will have under these circumstances.\n\nSince this method can easily become unnecessarily complicated with relatively simple problems, it can be quite helpful to understand different relations between the loading, shear, and moment diagram. The first of these is the relationship between a distributed load on the loading diagram and the shear diagram. Since a distributed load varies the shear load according to its magnitude it can be derived that the slope of the shear diagram is equal to the magnitude of the distributed load. The relationship between distributed load and shear force magnitude is:\n\nSome direct results of this is that a shear diagram will have a point change in magnitude if a point load is applied to a member, and a linearly varying shear magnitude as a result of a constant distributed load.\nSimilarly it can be shown that the slope of the moment diagram at a given point is equal to the magnitude of the shear diagram at that distance. The relationship between distributed shear force and bending moment is:\n\nA direct result of this is that at every point the shear diagram crosses zero the moment diagram will have a local maximum or minimum. Also if the shear diagram is zero over a length of the member, the moment diagram will have a constant value over that length. By calculus it can be shown that a point load will lead to a linearly varying moment diagram, and a constant distributed load will lead to a quadratic moment diagram.\n\nIn practical applications the entire stepwise function is rarely written out. The only parts of the stepwise function that would be written out are the moment equations in a nonlinear portion of the moment diagram; this occurs whenever a distributed load is applied to the member. For constant portions the value of the shear and/or moment diagram is written right on the diagram, and for linearly varying portions of a member the beginning value, end value, and slope or the portion of the member are all that are required.\n\n\n\n"}
{"id": "33767742", "url": "https://en.wikipedia.org/wiki?curid=33767742", "title": "Silent protest", "text": "Silent protest\n\nSilent protest is an organized effort where the participants stay quiet to demonstrate disapproval. It is used as a form of civil disobedience and nonviolent resistance.\n\n"}
{"id": "1008074", "url": "https://en.wikipedia.org/wiki?curid=1008074", "title": "Social dilemma", "text": "Social dilemma\n\nA social dilemma is a situation in which an individual profits from selfishness unless everyone chooses the selfish alternative, in which case the whole group loses. Problems arise when too many group members choose to pursue individual profit and immediate satisfaction rather than behave in the group’s best long-term interests. Social dilemmas can take many forms and are studied across disciplines such as psychology, economics, and political science. Examples of phenomena that can be explained using social dilemmas include resource depletion, low voter turnout, and overpopulation.\n\nThe prisoner's dilemma is a simple game that serves as the basis for research on social dilemmas. The premise of the game is that two partners in crime are imprisoned separately and each are offered leniency if they provide evidence against the other. As seen in the table below, the optimal individual outcome is to testify against the other without being testified against. However, the optimal group outcome is for the two prisoners to cooperate with each other.\n\nA public goods dilemma is a situation in which the whole group can benefit if some of the members give something for the common good but individuals benefit from “free riding” if enough others contribute. Public goods are defined by two characteristics: non-excludability and non-rivalry—meaning that anyone can benefit from them and one person’s use of them does not hinder another person’s use of them. An example is public broadcasting that relies on contributions from viewers. Since no single viewer is essential for providing the service, viewers can reap the benefits of the service without paying anything for it. If not enough people contribute, the service cannot be provided. In economics, the literature around public goods dilemmas refers to the phenomenon as the free rider problem. The economic approach is broadly applicable and can refer to the free-riding that accompanies any sort of public good. In social psychology, the literature refers to this phenomenon as social loafing. Whereas as free-riding is generally used to describe public goods, social loafing refers specifically to the tendency for people to exert less effort when in a group than when working alone.\n\nA replenishing resource management dilemma is a situation in which group members share a renewable resource that will continue to produce benefits if group members do not over harvest it but in which any single individual profits from harvesting as much as possible.\n\nThe tragedy of the commons is a type of replenishing resource management dilemma. The dilemma arises when members of a group share a common good. A common good is rivalrous and non-excludable, meaning that anyone can use the resource but there is a finite amount of the resource available and it is therefore prone to overexploitation.\n\nThe paradigm of the tragedy of the commons first appeared in an 1833 pamphlet by English economist William Forster Lloyd. According to Lloyd, \"If a person puts more cattle into his own field, the amount of the subsistence which they consume is all deducted from that which was at the command, of his original stock; and if, before, there was no more than a sufficiency of pasture, he reaps no benefit from the additional cattle, what is gained in one way being lost in another. But if he puts more cattle on a common, the food which they consume forms a deduction which is shared between all the cattle, as well that of others as his own, in proportion to their number, and only a small part of it is taken from his own cattle\".\n\nThe template of the tragedy of the commons can be used to understand myriad problems, including various forms of resource depletion. For example, overfishing in the 1960s and 1970s led to depletion of the previously abundant supply of Atlantic Cod. By 1992, the population of cod had completely collapsed because fishers had not left enough fish to repopulate the species.\n\nA social trap occurs when individuals or groups pursue immediate rewards that later prove to have negative or even lethal consequences. This type of dilemma arises when a behavior produces rewards initially but continuing the same behavior produces diminishing returns. Stimuli that cause social traps are called sliding reinforcers, since they reinforce the behavior in small doses and punish it in large doses.\n\nAn example of a social trap is the use of vehicles and the resulting air pollution. Viewed individually, vehicles are an adaptive technology that have revolutionized transportation and greatly improved quality of life. But their current widespread use causes negative externalities. In many places air pollution continues unabated because the convenience of driving a car is immediate and the environmental costs are distant and often do not become obvious until much later.\n\nA perceptual dilemma arises during conflict and is a product of outgroup bias. In this dilemma, the parties to the conflict prefer cooperation while simultaneously believing that the other side would take advantage of conciliatory gestures.\n\nThe prevalence of perceptual dilemmas in conflict has led to the development of two distinct schools of thought on the subject. According to deterrence theory, the best strategy to take in conflict is to show signs of strength and willingness to use force if necessary. This approach is intended to dissuade attacks before they happen. Conversely, the conflict spiral view holds that deterrence strategies increase hostilities and defensiveness and that a clear demonstration of peaceful intentions is the most effective way to avoid escalation.\n\nAn example of the deterrence theory in practice is the Cold War strategy (employed by both the United States and the Soviet Union) of mutually assured destruction (MAD). Because both countries had second strike capability, each side knew that the use of nuclear weapons would result in their own destruction. While controversial, MAD succeeded in its primary purpose of preventing nuclear war and kept the Cold War cold.\n\nConciliatory gestures have also been used to great effect, in keeping with conflict spiral theory. For example, Egyptian President Anwar El Sadat's 1977 visit to Israel during a prolonged period of hostilities between the two countries was well-received and ultimately contributed in the Egypt–Israel Peace Treaty.\n\nSocial dilemmas have attracted a great deal of interest in the social and behavioral sciences. Economists, biologists, psychologists, sociologists, and political scientists alike study behavior in social dilemmas. The most influential theoretical approach is economic game theory (i.e., rational choice theory, expected utility theory). Game theory assumes that individuals are rational actors motivated to maximize their utilities. Utility is often narrowly defined in terms of people's economic self-interest. Game theory thus predicts a non-cooperative outcome in a social dilemma. Although this is a useful starting premise there are many circumstances in which people may deviate from individual rationality, demonstrating the limitations of economic game theory.\n\nBiological and evolutionary approaches provide useful complementary insights into decision-making in social dilemmas. According to selfish gene theory, individuals may pursue a seemingly irrational strategy to cooperate if it benefits the survival of their genes. The concept of inclusive fitness delineates that cooperating with family members might pay because of shared genetic interests. It might be profitable for a parent to help their off-spring because doing so facilitates the survival of their genes. Reciprocity theories provide a different account of the evolution of cooperation. In repeated social dilemma games between the same individuals, cooperation might emerge because participants can punish a partner for failing to cooperate. This encourages reciprocal cooperation. Reciprocity serves as an explanation for why participants cooperate in dyads, but fails to account for larger groups. Evolutionary theories of indirect reciprocity and costly signaling may be useful to explain large-scale cooperation. When people can selectively choose partners to play games with, it pays to develop a cooperative reputation. Cooperation communicates kindness and generosity, which combine to make someone an attractive group member.\n\nPsychological models offer additional insights into social dilemmas by questioning the game theory assumption that individuals are confined to their narrow self-interest. Interdependence Theory suggests that people transform a given pay-off matrix into an effective matrix that is more consistent with their social dilemma preferences. A prisoner’s dilemma with close kin, for example, changes the pay-off matrix into one in which it is rational to be cooperative. Attribution models offer further support for these transformations. Whether individuals approach a social dilemma selfishly or cooperatively might depend upon whether they believe people are naturally greedy or cooperative. Similarly, goal-expectation theory assumes that people might cooperate under two conditions: They must (1) have a cooperative goal, and (2) expect others to cooperate. Another psychological model, the appropriateness model, questions the game theory assumption that individuals rationally calculate their pay-offs. Instead many people base their decisions on what people around them do and use simple heuristics, like an equality rule, to decide whether or not to cooperate. The logic of appropriateness suggests that people ask themselves the question: \"what does a person like me (identity) do (rules/heuristics) in a situation like this (recognition) given this culture (group)?\" (Weber \"et al.\", 2004) \n(Kopelman 2009) and that these factors influence cooperation.\n\nStudying the conditions under which people cooperate can shed light on how to resolve social dilemmas. The literature distinguishes between three broad classes of solutions—motivational, strategic, and structural—which vary in whether they see actors as motivated purely by self-interest and in whether they change the rules of the social dilemma game.\n\nMotivational solutions assume that people have other-regarding preferences. There is a considerable literature on social value orientations which shows that people have stable preferences for how much they value outcomes for self versus others. Research has concentrated on three social motives: (1) individualism—maximizing own outcomes regardless of others; (2) competition—maximizing own outcomes relative to others; and (3) cooperation—maximizing joint outcomes. The first two orientations are referred to as proself orientations and the third as a prosocial orientation. There is much support for the idea that prosocial and proself individuals behave differently when confronted with a social dilemma in the laboratory as well as the field. People with prosocial orientations weigh the moral implications of their decisions more and see cooperation as the most preferable choice in a social dilemma. When there are conditions of scarcity, like a water shortage, prosocials harvest less from a common resource. Similarly prosocials are more concerned about the environmental consequences of, for example, taking the car or public transport.\n\nResearch on the development of social value orientations suggest an influence of factors like family history (prosocials have more sibling sisters), age (older people are more prosocial), culture (more individualists in Western cultures), gender (more women are prosocial), even university course (economics students are less prosocial). However, until we know more about the psychological mechanisms underlying these social value orientations we lack a good basis for interventions.\n\nAnother factor that might affect the weight individuals assign to group outcomes is the possibility of communication. A robust finding in the social dilemma literature is that cooperation increases when people are given a chance to talk to each other. It has been quite a challenge to explain this effect. One motivational reason is that communication reinforces a sense of group identity.\n\nHowever, there may be strategic considerations as well. First, communication gives group members a chance to make promises and explicit commitments about what they will do. It is not clear if many people stick to their promises to cooperate. Similarly, through communication people are able to gather information about what others do. On the other hand, this information might produce ambiguous results; an awareness of other people's willingness to cooperate may cause a temptation to take advantage of them.\n\nA second category of solutions are primarily strategic. In repeated interactions cooperation might emerge when people adopt a Tit for tat strategy (TFT). TFT is characterized by first making a cooperative move while the next move mimics the decision of the partner. Thus, if a partner does not cooperate, you copy this move until your partner starts to cooperate. Computer tournaments in which different strategies were pitted against each other showed TFT to be the most successful strategy in social dilemmas. TFT is a common strategy in real-world social dilemmas because it is nice but firm. Consider, for instance, about marriage contracts, rental agreements, and international trade policies that all use TFT-tactics.\n\nHowever, TFT is quite an unforgiving strategy and in noisy real-world dilemmas a more forgiving strategy has its own advantages. Such a strategy is known as Generous-tit-for-tat (GTFT). This strategy always reciprocates cooperation with cooperation, and usually replies to defection with defection. However, with some probability GTFT with forgive a defection by the other player and cooperate. In a world of errors in action and perception, such a strategy can be a Nash equilibrium and evolutionarily stable. The more beneficial cooperation is, the more forgiving GTFT can be while still resisting invasion by defectors.\n\nEven when partners might not meet again it could be strategically wise to cooperate. When people can selectively choose who to interact with it might pay to be seen as a cooperator. Research shows that cooperators create better opportunities for themselves than non-cooperators: They are selectively preferred as collaborative partners, romantic partners, and group leaders. This only occurs however when people’s social dilemma choices are monitored by others. Public acts of altruism and cooperation like charity giving, philanthropy, and bystander intervention are probably manifestations of reputation-based cooperation.\n\nStructural solutions change the rules of the game either through modifying the social dilemma or removing the dilemma altogether. Field research on conservation behaviour has shown that selective incentives in the form of monetary rewards are effective in decreasing domestic water and electricity use. Furthermore, numerous experimental and case studies show that cooperation is more likely based on a number of factors, including whether or not individuals have the ability to monitor the situation, to punish or \"sanction\" defectors, if they are legitimized by external political structures to cooperate and self-organize, can communicate with one another and share information, know one another, have effective arenas for conflict resolution, and are managing social and ecological systems that have well-defined boundaries or are easily monitorable. Yet implementation of reward and punishment systems can be problematic for various reasons. First, there are significant costs associated with creating and administering sanction systems. Providing selective rewards and punishments requires support institutions to monitor the activities of both cooperators and non-cooperators, which can be quite expensive to maintain. Second, these systems are themselves public goods because one can enjoy the benefits of a sanctioning system without contribution to its existence. The police, army, and judicial system will fail to operate unless people are willing to pay taxes to support them. This raises the question if many people want to contribute to these institutions. Experimental research suggests that particularly low trust individuals are willing to invest money in punishment systems. A considerable portion of people are quite willing to punish non-cooperators even if they personally do not profit. Some researchers even suggest that altruistic punishment is an evolved mechanism for human cooperation. A third limitation is that punishment and reward systems might undermine people’s voluntary cooperative intention. Some people get a \"warm glow\" from cooperation and the provision of selective incentives might crowd out their cooperative intention. Similarly the presence of a negative sanctioning system might undermine voluntary cooperation. Some research has found that punishment systems decrease the trust that people have in others. Other research has found that \"graduated\" sanctions, where initial punishments have low severity, make allowances for unusual hardships, and allow the violator to reenter the trust of the collective, have been found to support collective resource management and increase trust in the system.,\n\nBoundary structural solutions modify the social dilemma structure and such strategies are often very effective. Experimental studies on commons dilemmas show that overharvesting groups are more willing to appoint a leader to look after the common resource. There is a preference for a democratically elected prototypical leader with limited power especially when people’s group ties are strong. When ties are weak, groups prefer a stronger leader with a coercive power base. The question remains whether authorities can be trusted in governing social dilemmas and field research shows that legitimacy and fair procedures are extremely important in citizen’s willingness to accept authorities. Other research emphasizes a greater motivation for groups to successfully self-organize, without the need for an external authority base, when they do place a high value on the resources in question but, again, before the resources are severely overharvested. An external \"authority\" is not presumed to be the solution in these cases, however effective self-organization and collective governance and care for the resource base is.\n\nAnother structural solution is reducing group size. Cooperation generally declines when group size increases. In larger groups people often feel less responsible for the common good and believe, rightly or wrongly, that their contribution does not matter. Reducing the scale—for example through dividing a large scale dilemma into smaller more manageable parts—might be an effective tool in raising cooperation. Additional research on governance shows that group size has a curvilinear effect, since at low numbers, governance groups may also not have the person-power to effectively research, manage, and administer the resource system or the governance process.\n\nAnother proposed boundary solution is to remove the social from the dilemma, by means of privatization. This restructuring of incentives would remove the temptation to place individual needs above group needs. However, it is not easy to privatize moveable resources such as fish, water, and clean air. Privatization also raises concerns about social justice as not everyone may be able to get an equal share. Privatization might also erode people’s intrinsic motivation to cooperate, by externalizing the locus of control.\n\nIn society, social units which face a social dilemma within are typically embedded in interaction with other groups, often competition for resources of different kinds. Once this is modeled the social dilemma is strongly attenuated.\n\nThere are many additional structural solutions which modify the social dilemma, both from the inside and from the outside. The likelihood of successfully co-managing a shared resource, successfully organizing to self-govern, or successfully cooperating in a social dilemma depends on many variables, from the nature of the resource system, to the nature of the social system the actors are a part of, to the political position of external authorities, to the ability to communicate effectively, to the rules-in-place regarding the management of the commons. However, \"sub-optimal\" or \"failed\" results in a social dilemma (and perhaps the need for privatization or an external authority) tend to occur \"when resource users do \"not\" know who all is involved, do not have a foundation of trust and reciprocity, cannot communicate, have no established rules, and lack effective monitoring and sanctioning mechanisms.\" \n\nClose examination reveals that social dilemmas underlie many of the most pressing global issues, from climate change to conflict escalation. Their widespread importance warrants widespread understanding of the main types of dilemmas and accompanying paradigms. Fortunately, the literature on the subject is expanding to accommodate the pressing need to understand social dilemmas as the basis for real-world problems.\n\nResearch in this area is applied to areas such as organizational welfare, public health, local and global environmental change. The emphasis is shifting from pure laboratory research towards research testing combinations of motivational, strategic, and structural solutions. It is encouraging that researchers from various behavioral sciences are developing unifying theoretical frameworks to study social dilemmas (like evolutionary theory; or the Social-Ecological Systems framework developed by Elinor Ostrom and her colleagues). For instance, there is a burgeoning neuroeconomics literature studying brain correlates of decision-making in social dilemmas with neuroscience methods. The interdisciplinary nature of the study of social dilemmas does not fit into the conventional distinctions between fields, and demands a multidisciplinary approach that transcends divisions between economics, political science, and psychology.\n\n"}
{"id": "6500531", "url": "https://en.wikipedia.org/wiki?curid=6500531", "title": "Surrogate model", "text": "Surrogate model\n\nA surrogate model is an engineering method used when an outcome of interest cannot be easily directly measured, so a model of the outcome is used instead. Most engineering design problems require experiments and/or simulations to evaluate design objective and constraint functions as function of design variables. For example, in order to find the optimal airfoil shape for an aircraft wing, an engineer simulates the air flow around the wing for different shape variables (length, curvature, material, ..). For many real world problems, however, a single simulation can take many minutes, hours, or even days to complete. As a result, routine tasks such as design optimization, design space exploration, sensitivity analysis and \"what-if\" analysis become impossible since they require thousands or even millions of simulation evaluations.\n\nOne way of alleviating this burden is by constructing approximation models, known as surrogate models, response surface models, \"metamodels\" or \"emulators\", that mimic the behavior of the simulation model as closely as possible while being computationally cheap(er) to evaluate. Surrogate models are constructed using a data-driven, bottom-up approach. The exact, inner working of the simulation code is not assumed to be known (or even understood), solely the input-output behavior is important. A model is constructed based on modeling the response of the simulator to a limited number of intelligently chosen data points. This approach is also known as behavioral modeling or black-box modeling, though the terminology is not always consistent. When only a single design variable is involved, the process is known as curve fitting.\n\nThough using surrogate models in lieu of experiments and simulations in engineering design is more common, surrogate modelling may be used in many other areas of science where there are expensive experiments and/or function evaluations.\n\nThe scientific challenge of surrogate modeling is the generation of a surrogate that is as accurate as possible, using as few simulation evaluations as possible. The process comprises three major steps which may be interleaved iteratively:\n\n\nThe accuracy of the surrogate depends on the number and location of samples (expensive experiments or simulations) in the design space. Various design of experiments (DOE) techniques cater to different sources of errors, in particular, errors due to noise in the data or errors due to an improper surrogate model.\n\nThe most popular surrogate models are polynomial response surfaces, kriging, Gradient-Enhanced Kriging (GEK), radial basis function, support vector machines, space mapping, and artificial neural networks. For some problems, the nature of true function is not known a priori so it is not clear which surrogate model will be most accurate. In addition, there is no consensus on how to obtain the most reliable estimates of the accuracy of a given surrogate.\nMany other problems have known physics properties. In these cases, physics-based surrogates such as space-mapping based models are the most efficient.\n\nA recent survey of surrogate-assisted evolutionary optimization techniques can be found in.\n\nSpanning two decades of development and engineering applications, Rayas-Sanchez reviews aggressive space mapping exploiting surrogate models. Recently, Razavi et al. has published a state-of-the-art review of surrogate models used in water resources management field. \n\nRecently proposed comparison-based surrogate models (e.g. ranking support vector machine) for evolutionary algorithms, such as CMA-ES, allow to preserve some invariance properties of surrogate-assisted optimizers:\n\n\nAn important distinction can be made between two different applications of surrogate models: design optimization and design space approximation (also known as emulation).\n\nIn surrogate model based optimization an initial surrogate is constructed using some of the available budget of expensive experiments and/or simulations. The remaining experiments/simulations are run for designs which the surrogate model predicts may have promising performance. The process usually takes the form of the following search/update procedure.\n\n\nDepending on the type of surrogate used and the complexity of the problem, the process may converge on a local or global optimum, or perhaps none at all.\n\nIn design space approximation, one is not interested in finding the optimal parameter vector but rather in the global behavior of the system. Here the surrogate is tuned to mimic the underlying model as closely as needed over the complete design space. Such surrogates are a useful, cheap way to gain insight into the global behavior of the system. Optimization can still occur as a post processing step, although with no update procedure (see above) the optimum found cannot be validated.\n\n\n\n"}
{"id": "2802848", "url": "https://en.wikipedia.org/wiki?curid=2802848", "title": "Temperley–Lieb algebra", "text": "Temperley–Lieb algebra\n\nIn statistical mechanics, the Temperley–Lieb algebra is an algebra from which are built certain transfer matrices, invented by Neville Temperley and Elliott Lieb. It is also related to integrable models, knot theory and the braid group, quantum groups and subfactors of von Neumann algebras.\n\nLet formula_1 be a commutative ring and fix formula_2. The Temperley–Lieb algebra formula_3 is the formula_1-algebra generated by the elements formula_5, subject to the Jones relations: \n\nformula_3 may be represented diagrammatically as the vector space over noncrossing pairings on a rectangle with \"n\" points on two opposite sides. The five basis elements of formula_16 are the following:\n\nMultiplication on basis elements can be performed by placing two rectangles side by side, and replacing any closed loops by a factor of formula_17, for example:\n\nThe identity element is the diagram in which each point is connected to the one directly across the rectangle from it, and the generator formula_19 is the diagram in which the \"i\"th point is connected to the \"i+1\"th point, the \"2n − i + 1\"th point is connected to the \"2n − i\"th point, and all other points are connected to the point directly across the rectangle. The generators of formula_20 are:\n\nFrom left to right, the unit 1 and the generators U, U, U, U.\n\nThe Jones relations can be seen graphically:\n\nConsider an interaction-round-a-face model e.g. a square lattice model and let formula_22 be the number of sites on the lattice. Following Temperley and Lieb we define the Temperley-Lieb hamiltonian (the TL hamiltonian) as\n\nformula_23\n\nwhere formula_24, for some spectral parameter formula_25.\n\nWe will firstly consider the case formula_26. The TL hamiltonian is formula_27, namely \n\nformula_28 = 2 - - .\n\nWe have two possible states,\n\nIn acting by formula_28 on these states, we find\n\nformula_28 = 2 - - = - ,\n\nand\n\nformula_28 = 2 - - = - + .\n\nWriting formula_28 as a matrix in the basis of possible states we have,\n\nformula_33\n\nThe eigenvector of formula_28 with the \"lowest\" eigenvalue is known as the ground state. In this case, the lowest eigenvalue formula_35 for formula_28 is formula_37. The corresponding eigenvector is formula_38. As we vary the number of sites formula_22 we find the following table\n\nwhere we have used the notation formula_40 formula_41-times e.g. formula_42.\n\nAn interesting observation is that the largest components of the ground state of formula_28 have a combinatorial enumeration as we vary the number of sites, as was first observed by Murray Batchelor, Jan de Gier and Bernard Nienhuis. Using the resources of the on-line encyclopedia of integer sequences, Batchelor \"et al.\" found, for an even numbers of sites \n\nformula_44\n\nand for an odd numbers of sites \n\nformula_45\n\nSurprisingly, these sequences corresponded to well known combinatorial objects. For formula_22 even, this corresponds to cyclically symmetric transpose complement plane partitions and for formula_22 odd, , these correspond to formula_48 alternating sign matrices symmetric about the vertical axis.\n\n"}
{"id": "316929", "url": "https://en.wikipedia.org/wiki?curid=316929", "title": "Triple bottom line", "text": "Triple bottom line\n\nThe triple bottom line (or otherwise noted as TBL or 3BL) is an accounting framework with three parts: social, environmental (or ecological) and financial. Some organizations have adopted the TBL framework to evaluate their performance in a broader perspective to create greater business value. Business writer John Elkington claims to have coined the phrase in 1994.\n\nIn traditional business accounting and common usage, the \"bottom line\" refers to either the \"profit\" or \"loss\", which is usually recorded at the very bottom line on a statement of revenue and expenses. Over the last 50 years, environmentalists and social justice advocates have struggled to bring a broader definition of bottom line into public consciousness by introducing full cost accounting. For example, if a corporation shows a monetary profit, but their asbestos mine causes thousands of deaths from asbestosis, and their copper mine pollutes a river, and the government ends up spending taxpayer money on health care and river clean-up, how do we perform a full societal cost benefit analysis? The triple bottom line adds two more \"bottom lines\": social and environmental (ecological) concerns. With the ratification of the United Nations and ICLEI TBL standard for urban and community accounting in early 2007, this became the dominant approach to public sector full cost accounting. Similar UN standards apply to natural capital and human capital measurement to assist in measurements required by TBL, e.g. the EcoBudget standard for reporting ecological footprint. Use of the TBL is fairly widespread in South African media, as found in a 1990–2008 study of worldwide national newspapers.\n\nAn example of an organization seeking a triple bottom line would be a social enterprise run as a non-profit, but earning income by offering opportunities for handicapped people who have been labelled \"unemployable\", to earn a living by recycling. The organization earns a profit, which is controlled by a volunteer Board, and ploughed back into the community. The social benefit is the meaningful employment of disadvantaged citizens, and the reduction in the society's welfare or disability costs. The environmental benefit comes from the recycling accomplished. In the private sector, a commitment to corporate social responsibility (CSR) implies a commitment to transparent reporting about the business' material impact for good on the environment and people. Triple bottom line is one framework for reporting this material impact. This is distinct from the more limited changes required to deal only with ecological issues. The triple bottom line has also been extended to encompass four pillars, known as the quadruple bottom line (QBL). The fourth pillar denotes a future-oriented approach (future generations, intergenerational equity, etc.). It is a long-term outlook that sets sustainable development and sustainability concerns apart from previous social, environmental, and economic considerations.\n\nThe challenges of putting the TBL into practice relate to the measurement of social and ecological categories. Despite this, the TBL framework enables organizations to take a longer-term perspective and thus evaluate the future consequences of decisions.\n\nSustainable development was defined by the Brundtland Commission of the United Nations in 1987. Triple bottom line (TBL) accounting expands the traditional reporting framework to take into account social and environmental performance in addition to financial performance.\n\nIn 1981, Freer Spreckley first articulated the triple bottom line in a publication called 'Social Audit - A Management Tool for Co-operative Working'. In this work, he argued that enterprises should measure and report on financial performance, social wealth creation, and environmental responsibility. The phrase \"triple bottom line\" was articulated more fully by John Elkington in his 1997 book \"Cannibals with Forks: the Triple Bottom Line of 21st Century Business'.' \n\nIn articulating the concept of triple bottom line, Elkington has also made reference to Jed Emerson's concept of blended value, which refers to the idea that all value consists of social, environmental, and financial components. A \"Triple Bottom Line Investing\" group advocating and publicizing these principles was founded in 1998 by Robert J. Rubinstein.\n\nFor reporting their efforts companies may demonstrate their commitment to corporate social responsibility (CSR) through the following:\n\n\nThe concept of TBL demands that a company's responsibility lies with stakeholders rather than shareholders. In this case, \"stakeholders\" refers to anyone who is influenced, either directly or indirectly, by the actions of the firm. Examples of stakeholders include employees, customers, suppliers, local residents, government agencies, and creditors. According to the stakeholder theory, the business entity should be used as a vehicle for coordinating stakeholder interests, instead of maximizing shareholder (owner) profit. A growing number of financial institutions incorporate a triple bottom line approach in their work. It is at the core of the business of banks in the Global Alliance for Banking on Values, for example.\n\nThe Detroit-based Avalon International Breads interprets the triple bottom line as consisting of \"Earth\", \"Community\", and \"Employees\".\n\nThe triple bottom line consists of social equity, economic, and environmental factors. The phrase, \"people, planet, and profit\" to describe the triple bottom line and the goal of sustainability, was coined by John Elkington in 1994 while at Sustain Ability, and was later used as the title of the Anglo-Dutch oil company Shell's first sustainability report in 1997. As a result, one country in which the 3P concept took deep root was The Netherlands.\n\nThe people, social equity, or human capital bottom line pertains to fair and beneficial business practices toward labour and the community and region in which a corporation conducts its business. A TBL company conceives a reciprocal social structure in which the well-being of corporate, labour and other stakeholder interests are interdependent.\n\nAn enterprise dedicated to the triple bottom line seeks to provide benefit to many constituencies and not to exploit or endanger any group of them. The \"upstreaming\" of a portion of profit from the marketing of finished goods back to the original producer of raw materials, for example, a farmer in fair trade agricultural practice, is a common feature. In concrete terms, a TBL business would not use child labour and monitor all contracted companies for child labour exploitation, would pay fair salaries to its workers, would maintain a safe work environment and tolerable working hours, and would not otherwise exploit a community or its labour force. A TBL business also typically seeks to \"give back\" by contributing to the strength and growth of its community with such things as health care and education. Quantifying this bottom line is relatively new, problematic and often subjective. The Global Reporting Initiative (GRI) has developed guidelines to enable corporations and NGOs alike to comparably report on the social impact of a business.\n\nThe planet, environmental bottom line, or natural capital bottom line refers to sustainable environmental practices. A TBL company endeavors to benefit the natural order as much as possible or at the least do no harm and minimize environmental impact. A TBL endeavour reduces its ecological footprint by, among other things, carefully managing its consumption of energy and non-renewables and reducing manufacturing waste as well as rendering waste less toxic before disposing of it in a safe and legal manner. \"Cradle to grave\" is uppermost in the thoughts of TBL manufacturing businesses, which typically conduct a life cycle assessment of products to determine what the true environmental cost is from the growth and harvesting of raw materials to manufacture to distribution to eventual disposal by the end user.\n\nCurrently, the cost of disposing of non-degradable or toxic products is borne financially by governments and environmentally by the residents near the disposal site and elsewhere. In TBL thinking, an enterprise which produces and markets a product which will create a waste problem should not be given a free ride by society. It would be more equitable for the business which manufactures and sells a problematic product to bear part of the cost of its ultimate disposal.\n\nEcologically destructive practices, such as overfishing or other endangering depletions of resources are avoided by TBL companies. Often environmental sustainability is the more profitable course for a business in the long run. Arguments that it costs more to be environmentally sound are often specious when the course of the business is analyzed over a period of time. Generally, sustainability reporting metrics are better quantified and standardized for environmental issues than for social ones. A number of respected reporting institutes and registries exist including the Global Reporting Initiative, CERES, Institute 4 Sustainability and others.\n\nThe ecological bottom line is akin to the concept of eco-capitalism.\n\nThe profit or economic bottom line deals with the economic value created by the organization after deducting the cost of all inputs, including the cost of the capital tied up. It therefore differs from traditional accounting definitions of profit. In the original concept, within a sustainability framework, the \"profit\" aspect needs to be seen as the real economic benefit enjoyed by the host society. It is the real economic impact the organization has on its economic environment. This is often confused to be limited to the internal profit made by a company or organization (which nevertheless remains an essential starting point for the computation). Therefore, an original TBL approach cannot be interpreted as simply traditional corporate accounting profit \"plus\" social and environmental impacts unless the \"profits\" of other entities are included as a social benefit.\n\nFollowing the initial publication of the triple bottom line concept, students and practitioners have sought greater detail in how the pillars can be evaluated.\n\nThe \"people\" concept for example can be viewed in three dimensions – organisational needs, individual needs, and community issues.\n\nEqually, \"profit\" is a function of both a healthy sales stream, which needs a high focus on customer service, coupled with the adoption of a strategy to develop new customers to replace those that die away.\n\nAnd \"planet\" can be divided into a multitude of subdivisions, although reduce, reuse and recycle is a succinct way of steering through this division.\n\nWhile Triple Bottom Line inspires companies to take into consideration people, profit and planet simultaneously, Integrated Bottom Line (IBL) takes the concept a step further and encourages companies to integrate their financial, economic and social performance reporting into one integrated balance sheet. Integrating these performance measurements into one report will provide a more holistic look into a company's performance and assist decision makers in understanding opportunities for long term value creation that can be leveraged during long term organizational planning. \n\nThere is evidence in support an Integrated Management approach that prioritizes IBL reporting and asserts that enterprises with strong integration of environmental, social and governance (ESG) data into reporting and subsequent decision making often out perform firms with weak ESG integration. This can be largely attributed to the new opportunities for value creation that are discovered through the process of collecting, analyzing and understanding the ESG data collected and what it means for a company's long term success.\n\nPrioritizing an IBL approach begins with changing the way we think about traditional financial measurements as these do not take into consideration the full extent of the short and long term impacts of a decision or action. Instead, Return on Investment can be expanded to Return on Integration, Internal Rate of Return can evolve into Integrated Rate of Return and instead of focusing on Net Present Value, companies can plan for Integrated Future Value. \n\nThe following business-based arguments support the concept of TBL:\n\n\n\n\nFiscal policy of governments usually claims to be concerned with identifying social and natural deficits on a less formal basis. However, such choices may be guided more by ideology than by economics. The primary benefit of embedding one approach to measurement of these deficits would be first to direct monetary policy to reduce them, and eventually achieve a global monetary reform by which they could be systematically and globally reduced in some uniform way.\n\nThe argument is that the Earth's carrying capacity is at risk, and that in order to avoid catastrophic breakdown of climate or ecosystems, there is need for comprehensive reform of global financial institutions similar in scale to what was undertaken at Bretton Woods in 1944.\n\nWith the emergence of an externally consistent green economics and agreement on definitions of potentially contentious terms such as full-cost accounting, natural capital and social capital, the prospect of formal metrics for ecological and social loss or risk has grown less remote since the 1990s.\n\nIn the United Kingdom in particular, the London Health Observatory has undertaken a formal programme to address social deficits via a fuller understanding of what \"social capital\" is, how it functions in a real community (that being the City of London), and how losses of it tend to require both financial capital and significant political and social attention from volunteers and professionals to help resolve. The data they rely on is extensive, building on decades of statistics of the Greater London Council since World War II. Similar studies have been undertaken in North America.\n\nStudies of the value of Earth have tried to determine what might constitute an ecological or natural life deficit. The Kyoto Protocol relies on some measures of this sort, and actually relies on some value of life calculations that, among other things, are explicit about the ratio of the price of a human life between developed and developing nations (about 15 to 1). While the motive of this number was to simply assign responsibility for a cleanup, such stark honesty opens not just an economic but political door to some kind of negotiation — presumably to reduce that ratio in time to something seen as more equitable. As it is, people in developed nations can be said to benefit 15 times more from ecological devastation than in developing nations, in pure financial terms. According to the IPCC, they are thus obliged to pay 15 times more per life to avoid a loss of each such life to climate change — the Kyoto Protocol seeks to implement exactly this formula, and is therefore sometimes cited as a first step towards getting nations to accept formal liability for damage inflicted on ecosystems shared globally.\n\nAdvocacy for triple bottom line reforms is common in Green Parties. Some of the measures undertaken in the European Union towards the Euro currency integration standardize the reporting of ecological and social losses in such a way as to seem to endorse in principle the notion of unified accounts, or unit of account, for these deficits.\n\nTo address financial bottom line profitability concerns, some argue that focusing on the TBL will indeed increase profit for the shareholders in the long run. In practice, John Mackey, CEO of Whole Foods, uses Whole Foods's Community Giving Days as an example. On days when Whole Foods donates 5% of their sales to charity, this action benefits the community, creates goodwill with customers, and energizes employees—which may lead to increased, sustainable profitability in the long-run.\n\nWhile many people agree with the importance of good social conditions and preservation of the environment, there are also many who disagree with the triple bottom line as the way to enhance these conditions. The following are the reasons why:\n\n\nA focus on people, planet and profit has led to legislation changes around the world, often through social enterprise or social investment or through the introduction of a new legal form, the Community Interest Company. In the United States, the BCorp movement has been part of a call for legislation change to allow and encourage a focus on social and environmental impact, with BCorp a legal form for a company focused on \"stakeholders, not just shareholders\".\n\nIn Western Australia, the triple bottom line was adopted as a part of the State Sustainability Strategy, and accepted by the Government of Western Australia but its status was increasingly marginalised by subsequent premiers Alan Carpenter and Colin Barnett.\n\n\n"}
{"id": "30874303", "url": "https://en.wikipedia.org/wiki?curid=30874303", "title": "Volunteering", "text": "Volunteering\n\nVolunteering is generally considered an altruistic activity where an individual or group provides services for no financial or social gain \"to benefit another person, group or organization\". Volunteering is also renowned for skill development and is often intended to promote goodness or to improve human quality of life. Volunteering may have positive benefits for the volunteer as well as for the person or community served. It is also intended to make contacts for possible employment. Many volunteers are specifically trained in the areas they work, such as medicine, education, or emergency rescue. Others serve on an as-needed basis, such as in response to a natural disaster.\n\nThe verb was first recorded in 1755. It was derived from the noun \"volunteer\", in C.1600, \"one who offers himself for military service,\" from the Middle French \"voluntaire\". In the non-military sense, the word was first recorded during the 1630s. The word \"volunteering\" has more recent usage—still predominantly military—coinciding with the phrase \"community service\".\nIn a military context, a volunteer army is a military body whose soldiers chose to enter service, as opposed to having been conscripted. Such volunteers do not work \"for free\" and are given regular pay.\n\nDuring this time, America experienced the Great Awakening. People became aware of the disadvantaged and realized the cause for movement against slavery. Younger people started helping the needy in their communities . In 1851, the first YMCA in the United States was started, followed seven years later by the first YWCA. During the American Civil War, women volunteered their time to sew supplies for the soldiers and the \"Angel of the Battlefield\" Clara Barton and a team of volunteers began providing aid to servicemen. Barton founded the American Red Cross in 1881 and began mobilizing volunteers for disaster relief operations, including relief for victims of the Johnstown Flood in 1889.\n\nThe Salvation Army is one of the oldest and largest organizations working for disadvantaged people. Though it is a charity organization, it has organized a number of volunteering programs since its inception.\nPrior to the 19th century, few formal charitable organizations existed to assist people in need.\n\nIn the first few decades of the 20th century, several volunteer organizations were founded, including the Rotary International, Kiwanis International, Association of Junior Leagues International, and Lions Clubs International.\n\nThe Great Depression saw one of the first large-scale, nationwide efforts to coordinate volunteering for a specific need. During World War II, thousands of volunteer offices supervised the volunteers who helped with the many needs of the military and the home front, including collecting supplies, entertaining soldiers on leave, and caring for the injured.\n\nAfter World War II, people shifted the focus of their altruistic passions to other areas, including helping the poor and volunteering overseas. A major development was the Peace Corps in the United States in 1960. When President Lyndon B. Johnson declared a \"War on Poverty\" in 1964, volunteer opportunities started to expand and continued into the next few decades. The process for finding volunteer work became more formalized, with more volunteer centers forming and new ways to find work appearing on the World Wide Web.\n\nAccording to the Corporation for National and Community Service (in 2012), about 64.5 million Americans, or 26.5 percent of the adult population, gave 7.9 billion hours of volunteer service worth $175 billion. This calculates at about 125–150 hours per year or 3 hours per week at a rate of $22 per hour. Volunteer hours in the UK are similar; the data for other countries is unavailable.\n\nIn 1960, after the so called revolutionary war in Cuba ended, Ernesto Che Guevara created the concept of volunteering work. It was created with the intention that workers across the country volunteer a few hours of work on their work centers.\n\nMany schools on all education levels offer service-learning programs, which allow students to serve the community through volunteering while earning educational credit. According to Alexander Astin in the foreword to \"Where's the Learning in Service-Learning?\" by Janet Eyler and Dwight E. Giles, Jr.,\"...we promote more wide-spread adoption of service-learning in higher education because we see it as a powerful means of preparing students to become more caring and responsible parents and citizens and of helping colleges and universities to make good on their pledge to 'serve society.'\" When describing service learning, the Medical Education at Harvard says, \"Service learning unites academic study and volunteer community service in mutually reinforcing ways. ...service learning is characterized by a relationship of partnership: the student learns from the service agency and from the community and, in return, gives energy, intelligence, commitment, time and skills to address human and community needs.\" Volunteering in service learning seems to have the result of engaging both mind and heart, thus providing a more powerful learning experience; according to Janet Eyler and Dwight E. Giles, it succeeds by the fact that it \"...fosters student development by capturing student interest...\" While not recognized by everyone as a legitimate approach, research on the efficacy of service learning has grown. Janet Eyler and Dwight E. Giles conducted a national study of American college students to ascertain the significance of service learning programs, According to Eyler and Giles,\"These surveys, conducted before and after a semester of community service, examine the impact of service-learning on students.\" They describe their experience with students involved in service-learning in this way: \"Students like service-learning. When we sit down with a group of students to discuss service-learning experiences, their enthusiasm is unmistakable. ...it is clear that [the students]believe that what they gain from service-learning differs qualitatively from what they often derive from more traditional instruction.\"\n\n\"Skills-based volunteering\" is leveraging the specialized skills and the talents of individuals to strengthen the infrastructure of nonprofits, helping them build and sustain their capacity to successfully achieve their missions. This is in contrast to traditional volunteering, where specific training is not required. The average hour of traditional volunteering is valued by the Independent Sector at between $18–20 an hour. Skills-based volunteering is valued at $40–500 an hour, depending on the market value of the time.\n\nAn increasingly popular form of volunteering among young people, particularly gap year students and graduates, is to travel to communities in the developing world to work on projects with local organisations. Activities include teaching English, working in orphanages, conservation, assisting non-governmental organizations and medical work. International volunteering often aims to give participants valuable skills and knowledge in addition to benefits to the host community and organization.\n\nAlso called \"e-volunteering\" or \"online volunteering\", virtual volunteering is a volunteer who completes tasks, in whole or in part, offsite from the organization being assisted. They use the Internet and a home, school, telecenter or work computer, or other Internet-connected device, such as a PDA or smartphone. Virtual volunteering is also known as cyber service, telementoring, and teletutoring, as well as various other names. Virtual volunteering is similar to telecommuting, except that instead of online employees who are paid, these are online volunteers who are not paid.\n\nMicro-volunteering is a task performed via an internet-connected device. An individual typically does this task in small, un-paid increments of time. Micro-volunteering is distinct from \"virtual volunteering\" in that it typically does not require the individual volunteer to go through an application process, screening process, or training period.\n\nEnvironmental volunteering refers to the volunteers who contribute towards environmental management or conservation. Volunteers conduct a range of activities including environmental monitoring, ecological restoration such as re-vegetation and weed removal, protecting endangered animals, and educating others about the natural environment.\n\nVolunteering often plays a pivotal role in the recovery effort following natural disasters, such as tsunamis, floods, droughts, hurricanes, and earthquakes. For example, the 1995 Great Hanshin-Awaji earthquake in Japan was a watershed moment, bringing in many first-time . The 2004 Indian Ocean earthquake and tsunami attracted a large number of volunteers worldwide, deployed by non-governmental organizations, government agencies, and the United Nations.\n\nDuring the 2012 hurricane Sandy emergency, Occupy Sandy volunteers, formed a \"laterally organized rapid-response team\" that provided much needed help during and after the storm, from food to shelter to reconstruction. It is an example of mutualism at work, pooling resources and assistance and leveraging social media.\n\nResource poor schools around the world rely on government support or on efforts from volunteers and private donations, in order to run effectively. In some countries, whenever the economy is down, the need for volunteers and resources increases greatly. There are many opportunities available in school systems for volunteers. Yet, there are not many requirements in order to volunteer in a school system. Whether one is a high school or TEFL (Teaching English as a Foreign Language) graduate or college student, most schools require just voluntary and selfless effort.\n\nMuch like the benefits of any type of volunteering there are great rewards for the volunteer, student, and school. In addition to intangible rewards, volunteers can add relevant experience to their resumes. Volunteers who travel to assist may learn foreign culture and language.\n\nVolunteering in schools can be an additional teaching guide for the students and help to fill the gap of local teachers. Cultural and language exchange during teaching and other school activities can be the most essential learning experience for both students and volunteers.\n\nBenefacto, a volunteering brokerage, describe corporate volunteering as \"Companies giving their employees an allowance of paid time off annually, which they use to volunteer at a charity of their choice.\"\n\nA majority of the companies at the Fortune 500 allow their employees to volunteer during work hours. These formalized Employee Volunteering Programs (EVPs), also called Employer Supported Volunteering (ESV), are regarded as a part of the companies' sustainability efforts and their social responsibility activities. About 40% of Fortune 500 companies provide monetary donations, also known as volunteer grants, to nonprofits as a way to recognize employees who dedicate significant amounts of time to volunteering in the community.\n\nAccording to the information from VolunteerMatch, a service that provides Employee Volunteering Program solutions, the key drivers for companies that produce and manage EVPs are building brand awareness and affinity, strengthening trust and loyalty among consumers, enhancing corporate image and reputation, improving employee retention, increasing employee productivity and loyalty, and providing an effective vehicle to reach strategic goals.\n\nIn April 2015, David Cameron pledged to give all UK workers employed by companies with more 250 staff mandatory three days’ paid volunteering leave, which if implemented will generate an extra 360 million volunteering hours a year.\n\nCommunity volunteering refers globally to those who work to improve their local community. This activity commonly occurs through not for profit organizations, local governments and churches; but also encompasses ad-hoc or informal groups such as recreational sports teams.\n\nThere are many proven personal benefits of community volunteerism. Working together with a group of people who have different ethnicity, backgrounds, and views reduces stereotypes. Community volunteerism has also been proven to improve student's academic success.\n\nAccording to \"Where's the Learning in Service Learning?\" by Janet Eyler and Dwight E. Giles, immersing oneself into service learning and serving others has many positive effects both academic and personal. Not only does surrounding oneself with new people and learning how to work together as a group help one improve teamwork and relational skills, it reduces stereotypes, increases appreciation of other cultures, and works to allow young people to find others that they relate to.\n\nEyler and Giles noted that at the beginning and end of a college semester that included three hours of community service a week, students reported a much higher regard for cultural differences. At the end of the semester those who had participated in service-learning were noted as saying that the most important things that they had learned were not to judge others, and to appreciate every type of person because everyone shares some similar key characteristics.\n\nCommunity volunteer work has proven to be a powerful predictor in students' academic lives and college experience as a whole. Studies have shown that students who participate in community service as a part of their college course of study have a much higher correlation of completing their degree (Astin, 1992; Pascarella and Terenzini, 1991). In addition, college students who participate in community volunteer projects as a part of their college experience report finding a much greater relevance in their academic studies after completing community volunteer projects. According to University Health Services, studies have found that volunteering can positively impact a student’s overall mental and emotional health.\n\nIn some European countries government organisations and non-government organisations provide auxiliary positions for a certain period in institutions like hospitals, schools, memorial sites and welfare institutions. The difference to other types of volunteering is that there are strict legal regulations, what organisation is allowed to engage volunteers and about the period a volunteer is allowed to work in a voluntary position. Due to that fact, the volunteer is getting a limited amount as a pocket money from the government. An organization having one of the biggest manpower in Europe is the German Federal volunteers service (Bundesfreiwilligendienst), that was founded in 2011, by having more than 35.000 federal volunteers in 2012. A much older institution is the Voluntary social year (Freiwilliges Soziales Jahr) in Austria and Germany.\n\nSochi Olympics 25,000 volunteers worked at the 2014 Sochi Winter Olympics. They supported the organisers in more than 20 functional areas: meeting guests, assisting navigation, organising the opening and closing ceremonies, organising food outlets, etc. Volunteer applications were open to any nationals of Russia and other countries. The Sochi 2014 Organising Committee received about 200,000 applications, 8 applicants per place. Volunteers received training over the course of more than a year at 26 volunteer centres in 17 cities across Russia. The majority of participants were between 17 and 22 years old. At the same time, 3000 applications were submitted from people over 55 years old. Some of them worked as volunteers during the 1980 Olympics in Moscow. It was the first experience with such a large-scale volunteer program in the contemporary Russia.\n\nFor the first time in its history, Russia will host the FIFA World Cup from June 14 till July 15, 2018. Moreover, it will be the first time the World Cup games will be played both in Europe and Asia. The games will be hosted by 12 stadiums in 11 Russian cities.\n\nThe volunteer program of the 2018 FIFA World Cup has engaged thousands of people from Russia and other countries around the world.\n\nThe program included several stages: recruitment, selection and training of volunteers, organisation of their work during the championship. The recruitment of volunteers for the FIFA Confederations Cup and the FIFA World Cup via FIFA.com started on June 1, 2016 and closed on December 30, 2016. Some of the volunteers worked at the 2017 FIFA Confederations Cup: 1733 people assisted the organisers in Saint Petersburg, 1590 worked in Moscow, 1261 in Sochi, 1260 in Kazan, a total of 5844 participants.\n\nThe FIFA World Cup will be supported by 17,040 volunteers of the Russia 2018 Local Organising Committee.\n\nCandidates living in Russia were selected by 15 volunteer centres in the host cities based in some of Russia's leading higher educational institutions: Synergy University, Moscow State Institute of International Relations, Plekhanov Russian University of Economics, Russian State Social University, Moscow Automobile and Road Construction University, Saint Petersburg State University of Economics, Samara State University, Volga Region State Academy of Physical Culture, Sport and Tourism, Don State Technical University, Ogarev Mordovia State University, Volgograd State University, State University of Nizhny Novgorod, Samara State Aerospace University, Immanuel Kant Baltic Federal University, and Ural Federal University.\n\nCandidates from other countries were selected remotely.\n\nCandidates had to be at least 18 years old, have a good knowledge of English, have a higher or vocational secondary education, and possess teamwork skills.\n\nVolunteers were trained remotely, in volunteer centres and at World Cup venues.\n\nVolunteers will be providing assistance in a variety of areas:\n\n\nTheir work started ahead of the events: on May 10, 2017 for the 2017 FIFA Confederations Cup, and on May 10, 2018 for the 2018 FIFA World Cup.\n\nOn October 20, 2017, the Russian National Competition of Important Social Projects “Legacy of 2018 FIFA World Cup Volunteer Program” was launched. The competition has engaged about 1500 people: applicants to the 2018 FIFA World Cup volunteer program and future city volunteers.\n\nThe idea of the competition was that anyone could submit a project that would draw the attention of Russian cities residents to the FIFA World Cup in Russia and leave a legacy after the championship was over.\n\nThe project was expected to produce tangible (work of art, place of attraction for guests and residents in the city, open playground, graffiti, developed areas in city parks, films, etc.) or intangible (events, conferences, festivals, exhibitions) legacy.\n\n26 projects qualified to the final and were supported by the Russia 2018 Local Organising Committee and the host cities of the 2018 FIFA World Cup. The jury included the General Director of the Russia 2018 Local Organising Committee Alexey Sorokin, Ambassador of the 2018 FIFA World Cup in Russia Alexey Smertin and Advisor to the Head of the Federal Tourism Agency Svetlana Sergeeva.\n\nSome of the projects were combined or further developed by the Local Organising Committee.\n\nAmong the projects were: Football Championship for Moms, Ramp Production out of Recycled Plastic, Your Championship Sticker Packs, etc.\n\nDesignated days, weeks and years observed by a country or as designated by the United Nations to encourage volunteering / community service \n\nModern societies share a common value of people helping each other; not only do volunteer acts assist others, but they also benefit the volunteering individual on a personal level. Despite having similar objectives, tension can arise between volunteers and state-provided services. In order to curtail this tension, most countries develop policies and enact legislation to clarify the roles and relationships among governmental stakeholders and their voluntary counterparts; this regulation identifies and allocates the necessary legal, social, administrative, and financial support of each party. This is particularly necessary when some voluntary activities are seen as a challenge to the authority of the state(e.g., on 29 January 2001, President Bush cautioned that volunteer groups should supplement—not replace—government agencies’ work).\n\nVolunteering that benefits the state but challenges paid counterparts angers labor unions that represent those who are paid for their volunteer work; this is particularly seen in combination departments, such as volunteer fire departments.\n\nDifficulties in the cross-national aid model of volunteering can arise when it is applied across national borders. The presence of volunteers who are sent from one state to another can be viewed as a breach of sovereignty and showing a lack of respect towards the national government of the proposed recipients. Thus, motivations are important when states negotiate offers to send aid and when these proposals are accepted, particularly if donors may postpone assistance or stop it altogether. Three types of conditionality have evolved:\n\nSome international volunteer organizations define their primary mission as being altruistic: to fight poverty and improve the living standards of people in the developing world, (e.g. Voluntary Services Overseas has almost 2,000 skilled professionals working as volunteers to pass on their expertise to local people so that the volunteers' skills remain long after they return home). When these organizations work in partnership with governments, the results can be impressive. However, when other organizations or individual First World governments support the work of volunteer groups, there can be questions as to whether the organizations' or governments' real motives are poverty alleviation. Instead, a focus on creating wealth for some of the poor or developing policies intended to benefit the donor states is sometimes reported. Many low-income countries’ economies suffer from industrialization without prosperity and investment without growth. One reason for this is that development assistance guides many Third World governments to pursue development policies that have been wasteful, ill-conceived, or unproductive; some of these policies have been so destructive that the economies could not have been sustained without outside support.\n\nIndeed, some offers of aid have distorted the general spirit of volunteering, treating local voluntary action as contributions in kind, i.e., existing conditions requiring the modification of local people’s behavior in order for them to earn the right to donors’ charity. This can be seen as patronizing and offensive to the recipients because the aid expressly serves the policy aims of the donors rather than the needs of the recipients.\n\nBased on a case study in China, Xu and Ngai (2011) revealed that the developing grassroots volunteerism can be an enclave among various organizations and may be able to work toward the development of civil society in the developing countries. The researchers developed a \"Moral Resources and Political Capital\" approach to examine the contributions of volunteerism in promoting the civil society. Moral resource means the available morals could be chosen by NGOs. Political capital means the capital that will improve or enhance the NGOs’ status, possession or access in the existing political system.\n\nMoreover, Xu and Ngai (2011) distinguished two types of Moral Resources: Moral Resource-I and Moral Resource-II (ibid).\n\nThanks to the intellectual heritage of Blau and Duncan (1967), two types of political capital were identified:\nObviously, \"Moral resource-I itself contains the self-determination that gives participants confidence in the ethical beliefs they have chosen\", almost any organizations may have Moral Resource-I, while not all of them have the societal recognized Moral Resource-II. However, the voluntary service organizations predominantly occupy Moral Resource-II because a sense of moral superiority makes it possible that for parties with different values, goals and cultures to work together in promoting the promotion of volunteering. Thus the voluntary service organizations are likely to win the trust and support of the masses as well as the government more easily than will the organizations whose morals are not accepted by mainstream society. In other words, Moral Resource II helps the grassroots organizations with little Political Capital I to win Political Capital-II, which is a crucial factor for their survival and growth in developing countries such as China. Therefore, the voluntary service realm could be an enclave of the development of civil society in the developing nations.\n\nVolunteering has the ability to improve the quality of life and health including longevity of those who donate their time and research has found that older adults will benefit the most from volunteering. Physical and mental ailments plaguing older adults can be healed through the simple act of helping others; however, one must be performing the good deed from a selfless nature. There are barriers that can prevent older adults from participating in volunteer work, such as socio-economic status, opinions held by others, and even current health issues. However, these barriers can be overcome so that if one would like to be involved in volunteer work they can do so. Volunteering improves not only the communities in which one serves, but also the life of the individual who is providing help to the community.\n\nVolunteering is known not only to be related to happiness but also to increase happiness . Also, giving help was a more important benefit of better reported mental health than receiving help . Studies have also shown that volunteering can cause a decrease in loneliness for those volunteering as well as those for whom people volunteer.\n\nIn the United States, statistics on volunteering have historically been limited. In 2013, the U.S. Current Population Survey (US) included a volunteering supplement which produced statistics on volunteering.\n\nIn the 1960s, Ivan Illich offered an analysis of the role of American volunteers in Mexico in his speech entitled \"To Hell With Good Intentions\". His concerns, along with those of critics such as Paulo Freire and Edward Said, revolve around the notion of altruism as an extension of Christian missionary ideology. In addition, he mentions the sense of responsibility/obligation as a factor, which drives the concept of noblesse oblige—first developed by the French aristocracy as a moral duty derived from their wealth. Simply stated, these apprehensions propose the extension of power and authority over indigenous cultures around the world. Recent critiques of volunteering come from Westmier and Kahn (1996) and bell hooks (née Gloria Watkins) (2004). Also, Georgeou (2012) has critiqued the impact of neoliberalism on international aid volunteering.\n\nThe field of the medical tourism (referring to volunteers who travel overseas to deliver medical care) has recently attracted negative criticism when compared to the alternative notion of sustainable capacities, i.e., work done in the context of long-term, locally-run, and foreign-supported infrastructures. A preponderance of this criticism appears largely in scientific and peer-reviewed literature. Recently, media outlets with more general readerships have published such criticisms as well.\n\n\n\n\n"}
{"id": "18403522", "url": "https://en.wikipedia.org/wiki?curid=18403522", "title": "West African Youth League", "text": "West African Youth League\n\nThe West African Youth League (WAYL) was a political organisation founded by I. T. A. Wallace-Johnson in June 1935. The group was a major political force against the colonial government in West Africa, especially in the Gold Coast and Sierra Leone. The League was the first political movement in the region \"to recruit women into the main membership and the decision-making bodies of the organisation\".\n\nIn 1938 the popularity of the League increased in Sierra Leone as Wallace-Johnson returned. The league contested and won the Freetown City Council elections in the same year. At the time Wallace-Johnson claimed that the organisation had a membership of 40 000. Following the Freetown election victory, the British authorities arrested Wallace-Johnson. The league went into disarray after Wallace-Johnson was sent to prison on Sherbro Island in 1939. After attempting to revive the organisation in 1944, Wallace-Johnson took it into the Pan-African Federation set up in Manchester, United Kingdom. He decided to merge it into the National Council of Sierra Leone in 1950.\n\nMary Lokko served as Wallace-Johnson's assistant for a time beginning in 1936, becoming likely the first woman in West Africa to hold a position in a political organization.\n\n"}
{"id": "19996292", "url": "https://en.wikipedia.org/wiki?curid=19996292", "title": "Workplace aggression", "text": "Workplace aggression\n\nWorkplace aggression is a specific type of aggression which occurs in the workplace. Workplace aggression can include a wide range of behaviors, ranging from verbal acts (e.g., insulting someone or spreading rumors) to physical attacks (e.g., punching or slapping).\n\nAggression, in general, is any behavior an individual carries out with the intent to harm another person or group of people. The aggressor must believe that their behavior is harmful to their target, and that the target is motivated to avoid this behavior. A defining feature of aggression is the intent or motivation to harm. For a behavior to be considered an aggressive act, the individual committing the behavior must intend harm. In other words, if they inflict harm on another without that specific intent, it is not considered aggression.\n\nAggression can occur in a variety of situations. One important domain to understand aggression is in the workplace. Workplace aggression is considered a specific type of counterproductive work behavior (CWB) and is defined as \"any act of aggression, physical assault, threatening or coercive behavior that causes physical or emotional harm in a work setting.\"\n\nSome researchers specify that workplace aggression only includes efforts to harm coworkers, former coworkers, current employers, or past employers. Others include in workplace aggression any behaviors intended to harm another person that are enacted in a workplace.\n\nTo delineate the range of behaviors that can be considered aggressive workplace behaviors, researchers have developed schemes of classification for workplace aggression. Neuman and Baron (1998) offer these three dimensions that encompass the range of workplace aggression:\n\nIn an attempt to further break down the wide range of aggressive workplace behaviors, Baron and Neuman (1996) also classify workplace aggression based on these three dichotomies:\n\nAggressive acts can take any possible combination of these three dichotomies. For example, failing to deny false rumors about a coworker would be classified as verbal–passive–indirect. Purposely avoiding the presence of a coworker you know is searching for your assistance could be considered physical–passive–direct.\n\nOther researchers offer a classification system based on the aggressor's relationship to the victim.\n\nIn the workplace much of the aggressive behavior enacted on targets are considered covert in nature. According to Bjorkqvist, Osterman, and Hjelt-Back, covert behaviors are those behaviors that are designed to disguise the aggressive behavior or aggressive intentions from the target. Overt aggression, on the other hand, includes behaviors that do not hide the aggressive intent and are open in their intentions. Typically, covert aggression is verbal, indirect, and passive in nature, while overt aggression reflects the physical, direct, and active side of the dichotomies.\n\nWorkplace aggression often takes the form of covert behaviors. This can be attributed to what Bjorkqvist, Osterman, and Lagerspetz call the effect/danger ratio. This term refers to the aggressors' subjective evaluation of the relative effects and danger of committing an aggressive act. For an aggressor, it is ideal to have a larger effect/danger ratio. In other words, aggressors want an act to have a large effect with relatively low risk of danger to themselves.\n\nIndividuals in the workplace are subjected to prolonged exposure to each other. This prolonged exposure means the victims of the aggressors' actions likely have more time to retaliate, thus increasing the danger aspect of the ratio. Also, workplaces are often communal in nature. That is, people often work in groups and are surrounded by others. The presence of others acts as a built in audience that could \"punish\" the aggressor for harming a victim. It is for these reasons that individuals often choose covert forms of aggression.\n\nPredictors of workplace aggression can occur at both the organizational level and the individual level. Organizational factors examined here include organizational justice, supervision and surveillance, changes in the work environment, and specific job characteristics. At the individual level, gender, age, and alcohol consumption are examined here. While this is not a comprehensive listing of predictors, it does cover the majority of workplace aggression predictors addressed in the empirical literature.\n\nPerceived interpersonal justice, the degree to which people feel they are treated with fairness and respect, is negatively related to both psychological and physical aggression against supervisors. Inness, Barling, and Turner found similar results; perceived interpersonal injustice was related to workplace aggression in participants' primary and secondary jobs.\n\nMoreover, perceived procedural justice, the extent to which formal organizational procedures are assumed fair, is related to workplace aggression against supervisors. Greenberg and Barling found that the greater the perceptions of procedural justice, the less workplace aggression was reported.\n\nThe most extreme forms of workplace aggression may result from personnel decisions, such as individual termination and mass layoffs. In 2009 a man killed one and wounded five others at his former place of employment two years after he was let go from the company due to poor performance. A similar event occurred in 2012 when a man shot and killed four employees and then himself after losing his job earlier that day.\n\nDownsizing is a tactic used by organizations where there is a slow-down in business in order to remain profitable or minimize losses. This tactic is most commonly observed during widespread economic hardships, such as the Great Recession of 2008.\n\nPerceived job insecurity, or feelings of impending termination, has been found to be a predictor of workplace aggression.\n\nWorkplace surveillance (employee monitoring) is positively related to workplace aggression against supervisors, such that the greater the number of employee surveillance methods used, the greater the amount of workplace aggression. Furthermore, supervisory control over work performance has also been shown positively related to workplace aggression against supervisors. This type of behavior has been observed both adults and teenagers.\n\nBaron and Neuman found that certain changes in the work environment can lead to increased aggression that they attribute to heightened anxiety and stress. Specifically, pay cuts or freezes, changes in management, increased monitoring systems (e.g., increased computer monitoring), increased diversity, and the increased use of part-time employees all were related to higher levels of workplace aggression.\n\nOther antecedents of workplace aggression found in the literature are specific job characteristics. LeBlanc and Kelloway found that certain job features, such as handling guns or collecting valuable items, were significantly more related to workplace aggression.\n\nHarvey and Keashly found that length of time at work was able to predict workplace aggression such that the longer hours a person worked, the more likely they were to report aggression. The authors attributed this finding to two possible reasons. First, the more hours worked, the greater statistical probability of being victimized. Second, longer hours worked could contribute to fatigue and frustration. This in turn may increase the likelihood of aggressive actions towards coworkers.\n\nIn some studies, gender has been shown a significant predictor of workplace aggression. For example, being male is significantly related to reports of aggression against supervisors. Furthermore, males are more likely to commit aggressive acts in the presence of other men. This can be attributed to societal cultures that dictate \"codes of honor.\" Females, on the other hand, are no more likely to act aggressively in either the presence of females or males.\n\nAge is significantly related to aggression. In their study of age and job performance, Ng and Feldman found that older workers (age 40 or older) engaged in less workplace aggression than younger workers.\n\nThe frequency and amount of alcohol typically consumed by a person predicts aggressive behavior. Those who consume more alcohol more frequently are more likely to aggress against a coworker. The Hebei tractor rampage began as workplace aggression following alcohol consumption.\n\nCyber-aggression or cyber-harassment:\n\nWhat many do not realize that the phenomenon of cyber-bullying often associated with teenage culture has spread to the workplace in a variety of ways. While this trend is seemingly silent and slow growing, its effects are considered equally hurtful as any form of harassment.\n\nOften, cyber-aggression is the result of individuals in a workplace being offended/upset/or feeling threatened by organizational problems. They then resort to virtual communication as a form of retaliation. These actions are referred to as \"flaming\" by Whitty & Carr, or essentially when an individual online writes with hostility towards a particular person or group of people.\n\nInstant messaging has become both a help and a hindrance in organizations. The ease of use with instant messaging, is partially to blame, \"Employees can see who else is available, and if it’s someone they want to talk to, they’re able to connect in real time\". While this has become an extremely useful tool in workplace communication, instant messengers such at AIM or MSN Messenger are not easily regulated from a managerial aspect, which leads to employees being able to have private conversations on a public platform. These conversations can foster aggressive talk and lead to potentially hurtful information being spread among an organization. Some argue that instant messages are beneficial to the work process because it can easily resolve problems without having to distract the person via phone and you don’t have to wait for an email response.\n\nThis is one of the most prevalent tools used in cyber-aggression because of its prevalence in workplace communications. Often upset workers send loaded messages and attach the email to a large group of co-workers that are not involved in the issue to bring attention to it. An article from the Travel Trade Gazette give some advice to avoid being aggressive in emails.\n\nThis is one of the fastest growing ways that workers can lash out against each other. The opportunity is high for individuals to be aggressive in a highly public and open forum. Many choose to speak out at their co-workers or superiors because it is a way for them to vent their feelings while not having to say these things face to face.\n\nLike the array of behaviors considered workplace aggression, the consequences of workplace aggression are also extensive. For example, Ng and Feldman suggest that \"acts of workplace aggression can cause bodily harm to employees, pose physical danger for customers, create public relations crises, and harm the business reputation of the firm as a whole.\" The outcomes of workplace aggression addressed here include the health and well-being of targeted employees and job performance. Gender differences in outcomes are also addressed.\n\nSeverity of the repercussions may be influenced by the position of the aggressor. Hershcovis and Barling found that \"...supervisor aggression has the strongest adverse effects across attitudinal and behavioral outcomes\", followed by co-worker aggression and outsider aggression.\n\nWorkplace aggression can have devastating effects on an organization's employees. For example, it has been found that targets of workplace aggression report lower levels of well-being. Other studies have shown that aggression in the workplace can cause the victims of such behaviors to suffer from health problems and displaced aggression - including perpetuating aggression towards random strangers in the street. Bjorkqvist, Osterman, and Hjelt-Back even found that targets exhibited symptoms similar to those of post-traumatic stress disorder (PTSD), such as anxiety and depression. . Sorensen et al. found possible associations between harassment at work and well-being measures of lower back pain and sleep deficiency among a sample of hospital workers.\n\nResearch has looked at the negative impacts of workplace aggression on team performance and particularly team effectiveness as was evidenced in a recent study by Aube and Rousseau.\n\nVictims of workplace aggression may suffer from reduced job satisfaction. Lapierre, Spector, and Leck found that those who perceived being targets of workplace aggression reported significantly lower overall job satisfaction. Similarly, those who perceive abuse from their supervisors report lower levels of job satisfaction.\n\nResearch has shown that males and females react to workplace aggression differently. While both males and females have reported lower well-being after experiencing aggression in the workplace, studies indicate that the relationship between experienced workplace aggression and decreased well-being was stronger for men. In one study, results showed that men who experienced work aggression were more likely to report physical, psychosocial, affective, and cognitive problems. This study also showed that the type of aggression, whether it is overt or covert, did not matter for these outcomes. The study attributes these findings to the idea of modern-day masculinity, which stresses achievement and success in the workplace.\n\nFor females, nonsexual aggression has been found to have a stronger impact on job satisfaction than sexual aggression. Also, nonsexual aggression has a stronger relationship with job satisfaction in females than in males.\n\nPrevention programs focus on reducing instances of workplace aggression. Programs that incorporate personnel selection, organizational sanctions, and training are recommended.\n\nBased on a workplace prevention program developed by the United States Postal Service (USPS), Neuman and Baron encourage organizations to use personnel screening and testing to identify potential employees who are likely to behave aggressively before they are even hired. This proactive strategy prevents individuals who are predisposed to aggress from even entering the workplace.\n\nExplicit policies regarding workplace aggression may help organizations to reduce aggression. Employees who perceived that their organization would punish workplace aggressors reported less workplace aggression even when their perceptions of interpersonal justice were high. Neuman and Baron also suggest using organizational policies to curb workplace aggression and to shape strong anti-aggressive organizational norms.\n\nTraining is also an important part of a prevention program. Neuman and Baron suggest that training for both supervisors and subordinates should focus on teaching employees methods for dealing with aggression. Similarly, Rai advises that appropriate training should inform employees that management takes threats seriously, encourage employees to report incidents, and demonstrate management's commitment to deal with reported incidents.\n\nOrganizational support can influence the effects of workplace aggression. Schat and Kelloway isolated two forms of organizational support: instrumental and informational. Instrumental support refers to providing some type of assistance directly to an afflicted individuals, whereas information support refers to providing employees with self-help informational resources.\n\n"}
{"id": "419093", "url": "https://en.wikipedia.org/wiki?curid=419093", "title": "Zeitgeist", "text": "Zeitgeist\n\nThe Zeitgeist (, German pronunciation ) is a concept from 18th- to 19th-century German philosophy, translated as \"spirit of the age\" or \"spirit of the times\". It refers to an invisible agent or force dominating the characteristics of a given epoch in world history.\n\nThe term is now mostly associated with Hegel, contrasting with Hegel's use of \"Volksgeist\" \"national spirit\" and \"Weltgeist\" \"world-spirit\", \nbut its coinage and popularization precedes Hegel, and is mostly due to Herder and Goethe. Other philosophers who were associated with such ideas include Spencer and Voltaire. \n\nContemporary use of the term may, more pragmatically, refer to a schema of fashions or fads which prescribes what is considered to be acceptable or tasteful for an era, e.g. in the field of architecture.\n\nHegel in \"Phenomenology of the Spirit\" (1807) uses both \"Weltgeist\" and \"Volksgeist\" but prefers the phrase \"Geist der Zeiten\" \"spirit of the times\" over the compound \"Zeitgeist\".\n\nThe Hegelian concept does not necessarily contrast with the Great Man theory as by Thomas Carlyle, which sees history as the result of the actions of heroes and geniuses, as Hegel perceived such \"great men\", specifically Napoleon, as the \"embodiment of the world-spirit\" (\"Die Weltseele zu Pferde\" \"the world-soul on horseback\")\n\nHegel believed that art reflected, by its very nature, the culture of the time in which it is created. Culture and art are inextricable because an individual artist is a product of his or her time and therefore brings that culture to any given work of art. Furthermore, he believed that in the modern world it was impossible to produce classical art, which he believed represented a \"free and ethical culture\", which depended more on the philosophy of art and theory of art, rather than a reflection of the social construct, or \"Zeitgeist\" in which a given artist lives.\n\nA \"zeitgeist theory of leadership\" has been contrasted with Thomas Carlyle’s great man theory by Forsyth (2009). In his theory, Carlyle stresses that leaders do not become leaders by fate or accident. Instead, these individuals possess characteristics of great leaders and these characteristics allow them to obtain positions of power.\n\nAccording to Forsyth, Leo Tolstoy disagreed with Carlyle’s perspective, \ninstead believing that leadership, like other things, was a product of the \"zeitgeist\", the social circumstances at the time. \n\nGreat man theory and zeitgeist theory can be included in two main areas of thought in psychology. For instance, great man theory is very similar to the trait approach. Trait researchers are interested in identifying the various personality traits that underline human behaviors such as conformity, leadership or other social behaviors. Thus, they agree that leadership is primarily a quality of an individual and that some people are pre-dispositioned to be a leader whereas others are born to follow these leaders. In contrast, situationist researchers believe that social behavior is a product of society. That is, social influence is what determines human behaviors. Therefore, situationism is of the same opinion as zeitgeist theory—leaders are created from the social environment and are molded from the situation. The concept of zeitgeist also relates to the sociological tradition that stems from Émile Durkheim and recently developed into social capital theory as exemplified by the work of Patrick Hunout.\n\nThese two perspectives have been combined to create what is known as the \"interactional\" approach to leadership. This approach asserts that leadership is developed through the mixing of personality traits and the situation. Further, this approach was expressed by social psychologist, Kurt Lewin, by the equation B = f(P, E) where behavior (B) is a function (f) of the person (P) and the environment (E).\n\nExecutives, venture capitalists, journalists and authors have argued that the idea of a zeitgeist is useful in understanding the emergence of industries, simultaneous invention and evaluating the relative value of innovations. Malcolm Gladwell argued in his book \"Outliers\" that entrepreneurs who succeeded often share similar characteristics—early personal or significant exposure to knowledge and skills in the early stages of a nascent industry. He proposed that the timing of involvement in an industry and often in sports as well affected the probability of success. In Silicon Valley, a number of people (Peter Thiel, Alistair Davidson, Mac Levchin, Nicholas G. Carr, Vinod Khosla) have argued that much innovation has been shaped by easy access to the Internet, open source software, component technologies for both hardware and software (e.g., software libraries, software as a service), and the ability to reach narrow markets across a global market. Peter Thiel has commented: \"There is so much incrementalism now.\"\n\nIn a zeitgeist market, the number of new entrants is high, differentiation in high value products (the strongest predictor of new product success) is more difficult to achieve, and business models emphasizing service and solution over product and process will enhance success. Examples include innovation in product experience, legal rights and bundling, privacy rights, and agency (where businesses act on behalf of customers).\n\n\"Zeitgeist\" in the sense intellectual or aesthetic fashion or fad:\n\n\n\n\n"}
