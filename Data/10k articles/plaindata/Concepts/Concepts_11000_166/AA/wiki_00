{"id": "52363072", "url": "https://en.wikipedia.org/wiki?curid=52363072", "title": "Agent causation", "text": "Agent causation\n\nAgent causation, or Agent causality, is an idea in philosophy which states that an agent can start new causal chains not determined by prior events. This is in contrast to causal determinism.\n\nDefenders of this theory include Thomas Reid and Roderick Chisholm.\n\n"}
{"id": "17974424", "url": "https://en.wikipedia.org/wiki?curid=17974424", "title": "Akbulut cork", "text": "Akbulut cork\n\nIn topology, an Akbulut cork is a structure that is frequently used to show that in four dimensions, the smooth h-cobordism theorem fails. It was named after Turkish mathematician Selman Akbulut.\n\nA compact contractible Stein 4-manifold C with involution F on its boundary is called an Akbulut cork, if F extends to a self-homeomorphism but cannot extend to a self-diffeomorphism inside (hence a cork is an exotic copy of itself relative to its boundary). A cork (C,F) is called a cork of a smooth 4-manifold X, if removing C from X and re-gluing it via F changes the smooth structure of X (this operation is called \"cork twisting\"). Any exotic copy X' of a closed simply connected 4-manifold X differs from X by a single cork twist.\n\nThe basic idea of the Akbulut cork is that when attempting to use the h-corbodism theorem in four dimensions, the cork is the sub-cobordism that contains all the exotic properties of the spaces connected with the cobordism, and when removed the two spaces become trivially h-cobordant and smooth. This shows that in four dimensions, although the theorem does not tell us that two manifolds are diffeomorphic (only homeomorphic), they are \"not far\" from being diffeomorphic.\n\nTo illustrate this (without proof), consider a smooth h-cobordism \"W\" between two 4-manifolds \"M\" and \"N\". Then within \"W\" there is a sub-cobordism \"K\" between \"A\" ⊂ \"M\" and \"B\" ⊂ \"N\" and there is a diffeomorphism\n\nwhich is the content of the h-cobordism theorem for \"n\" ≥ 5 (here int \"X\" refers to the interior of a manifold \"X\"). In addition, \"A\" and \"B\" are diffeomorphic with a diffeomorphism that is an involution on the boundary ∂\"A\" = ∂\"B\". Therefore, it can be seen that the h-corbordism \"K\" connects \"A\" with its \"inverted\" image \"B\". This submanifold \"A\" is the Akbulut cork.\n\n"}
{"id": "10591072", "url": "https://en.wikipedia.org/wiki?curid=10591072", "title": "Axiom of reducibility", "text": "Axiom of reducibility\n\nThe axiom of reducibility was introduced by Bertrand Russell in the early 20th century as part of his ramified theory of types. Russell devised and introduced the axiom in an attempt to manage the contradictions he had discovered in his analysis of set theory.\n\nWith Russell's discovery (1901, 1902) of a paradox in Gottlob Frege's 1879 \"Begriffsschrift\" and Frege's acknowledgment of the same (1902), Russell tentatively introduced his solution as \"Appendix B: Doctrine of Types\" in his 1903 \"The Principles of Mathematics\". This contradiction can be stated as \"the class of all classes that do not contain themselves as elements\". At the end of this appendix Russell asserts that his \"doctrine\" would solve the immediate problem posed by Frege, but \"there is at least one closely analogous contradiction which is probably not soluble by this doctrine. The totality of all logical objects, or of all propositions, involves, it would seem a fundamental logical difficulty. What the complete solution of the difficulty may be, I have not succeeded in discovering; but as it affects the very foundations of reasoning...\"\n\nBy the time of his 1908 \"Mathematical logic as based on the theory of types\" Russell had studied \"the contradictions\" (among them the Epimenides paradox, the Burali-Forti paradox, and Richard's paradox) and concluded that \"In all the contradictions there is a common characteristic, which we may describe as self-reference or reflexiveness\". \n\nIn 1903, Russell defined \"predicative\" functions as those whose order is one more than the highest-order function occurring in the expression of the function. While these were fine for the situation, \"impredicative\" functions had to be disallowed:\n\nHe repeats this definition in a slightly different way later in the paper (together with a subtle prohibition that they would express more clearly in 1913):\n\nThis usage carries over to Alfred North Whitehead and Russell's 1913 \"Principia Mathematica\" wherein the authors devote an entire subsection of their Chapter II: \"The Theory of Logical Types\" to subchapter I. \"The Vicious-Circle Principle\": \"We will define a function of one variable as \"predicative\" when it is of the next order above that of its argument, i.e. of the lowest order compatible with its having that argument. . . A function of several arguments is predicative if there is one of its arguments such that, when the other arguments have values assigned to them, we obtain a predicative function of the one undetermined argument.\"\n\nThey again propose the definition of a \"predicative function\" as one that does not violate The Theory of Logical Types. Indeed the authors assert such violations are \"incapable [to achieve]\" and \"impossible\":\n\nThe authors stress the word \"impossible\":\n\nThe axiom of reducibility states that any truth function (i.e. propositional function) can be expressed by a formally equivalent \"predicative\" truth function. It made its first appearance in Bertrand Russell's (1908) \"Mathematical logic as based on the theory of types\", but only after some five years of trial and error. In his words: \nFor relations (functions of two variables such as \"For all x and for all y, those values for which f(x,y) is true\" i.e. ∀x∀y: f(x,y)), Russell assumed an \"axiom of relations\", or [the same] axiom of reducibility.\n\nIn 1903, he proposed a possible process of evaluating such a 2-place function by comparing the process to double integration: One after another, plug into \"x\" definite values \"a\" (i.e. the particular \"a\" is \"a constant\" or a parameter held constant), then evaluate f(\"a\",\"y\") across all the \"n\" instances of possible \"y\". For all \"y\" evaluate f(a, \"y\"), then for all \"y\" evaluate f(\"a\", \"y\"), etc until all the \"x\" = \"a\" are exhausted). This would create an \"m\" by \"n\" matrix of values: TRUE or UNKNOWN. (In this exposition, the use of indices is a modern convenience.) \n\nIn 1908, Russell made no mention of this matrix of \"x\", \"y\" values that render a two-place function (e.g. relation) TRUE, but by 1913 he has introduced a matrix-like concept into \"function\". In *12 of Principia Mathematica (1913) he defines \"a matrix\" as \"any function, of however many variables, which does not involve any apparent variables. Then any possible function other than a matrix is derived from a matrix by means of generalisation, i.e. by considering the proposition which asserts that the function in question is true with all possible values or with some values of one of the arguments, the other argument or arguments remaining undetermined\". For example, if one asserts that \"∀y: f(x, y) is true\", then \"x\" is the apparent variable because it is unspecified. \n\nRussell now defines a matrix of \"individuals\" as a \"first-order\" matrix, and he follows a similar process to define a \"second-order matrix\", etc. Finally, he introduces the definition of a \"predicative function\":\n\nFrom this reasoning, he then uses the same wording to propose the same \"axioms of reducibility\" as he did in his 1908.\n\nAs an aside, Russell in his 1903 considered, and then rejected, \"a temptation to regard a relation as definable in extension as a class of couples\", i.e. the modern set-theoretic notion of ordered pair. An intuitive version of this notion appeared in Frege's (1879) \"Begriffsschrift\" (translated in van Heijenoort 1967:23); Russell's 1903 followed closely the work of Frege (cf Russell 1903:505ff). Russell worried that \"it is necessary to give sense to the couple, to distinguish the referent from the relatum: thus a couple becomes essentially distinct from a class of two terms, and must itself be introduced as a primitive idea. It would seem, viewing the idea philosophically, that sense can only be derived from some relational proposition . . . it seems therefore more correct to take an intensional view of relations, and to identify them rather with class-concepts than with classes\". As shown below, Norbert Wiener (1914) reduced the notion of relation to class by his definition of an ordered pair.\n\nThe outright prohibition implied by Russell's \"axiom of reducibility\" was roundly criticised by Ernst Zermelo in his 1908 \"Investigations in the foundations of set theory I\", stung as he was by a demand similar to that of Russell that came from Poincaré:\n\nZermelo countered:\n\nIn his 1914 \"A simplification of the logic of relations\", Norbert Wiener removed the need for the axiom of reducibility as applied to relations between two variables \"x\", and \"y\" e.g. φ(\"x\",\"y\"). He did this by introducing a way to express a relation as a set of ordered pairs: \"It will be seen that what we have done is practically to revert to Schröder's treatment of a relation as a class [set] of ordered couples\". Van Heijenoort observes that \"[b]y giving a definition of the ordered pair of two-elements in terms of class operations, the note reduced the theory of relations to that of classes.\" But Wiener opined that while he had dispatched Russell and Whitehead's two-variable version of the axiom *12.11, the single-variable version of the axiom of reducibility for (axiom *12.1 in \"Principia Mathematica\") was still necessary.\n\nLudwig Wittgenstein, while imprisoned in a prison camp, finished his \"Tractatus Logico-Philosophicus\". His introduction credits \"the great works of Frege and the writings of my friend Bertrand Russell\". Not a self-effacing intellectual, he pronounced that \"the \"truth\" of the thoughts communicated here seems to me unassailable and definitive. I am, therefore, of the opinion that the problems have in essentials been finally solved.\" So given such an attitude, it is no surprise that Russell's theory of types comes under criticism:\nThis appears to support the same argument Russell uses to erase his \"paradox\". This \"using the signs\" to \"speak of the signs\" Russell criticises in his introduction that preceded the original English translation:\n\nThis problem appears later when Wittgenstein arrives at this gentle disavowal of the axiom of reducibility—one interpretation of the following is that Wittgenstein is saying that Russell has made (what is known today as) a category error; Russell has asserted (inserted into the theory) a \"further law of logic\" when \"all\" the laws (e.g. the unbounded Sheffer stroke adopted by Wittgenstein) have \"already\" been asserted:\n\nRussell in his 1919 \"Introduction to Mathematical Philosophy\", a non-mathematical companion to his first edition of \"PM\", discusses his Axiom of Reducibility in Chapter 17 \"Classes\" (pp. 146ff). He concludes that \"we cannot accept \"class\" as a primitive idea; the symbols for classes are \"mere conveniences\" and classes are \"logical fictions, or (as we say) 'incomplete symbols' ... classes cannot be regarded as part of the ultimate furniture of the world\" (p. 146). The reason for this is because of the problem of impredicativity: \"classes cannot be regarded as a species of individuals, on account of the contradiction about classes which are not members of themselves ... and because we can prove that the number of classes is greater than the number of individuals, [etc]\". What he then does is propose 5 obligations that must be satisfied with respect to a theory of classes, and the result is his axiom of reducibility. He states that this axiom is \"a generalised form of Leibniz's identity of indiscernibles\" (p. 155). But he concludes Leibniz's assumption is not necessarily true for all possible predicates in all possible worlds, so he concludes that:\n\nThe goal that he sets for himself then is \"adjustments to his theory\" of avoiding classes:\nThoralf Skolem in his 1922 \"Some remarks on axiomatised set theory\" took a less than positive attitude toward \"Russell and Whitehead\" (i.e. their work \"Principia Mathematica\"):\n\nSkolem then observes the problems of what he called \"nonpredicative definition\" in the set theory of Zermelo:\n\nWhile Skolem is mainly addressing a problem with Zermelo's set theory, he does make this observation about the \"axiom of reducibility\":\n\nIn his 1927 \"Introduction\" to the second edition of \"Principia Mathematica\" Russell criticises his own axiom:\n\nWittgenstein's 5.54ff is more centred on the notion of function:\n\nA possible interpretation of Wittgenstein's stance is that the thinker A i.e. \"p\" \"is identically\" the thought \"p\", in this way the \"soul\" remains a unit and not a composite. So to utter \"the thought thinks the thought\" is nonsense, because per 5.542 the utterance does not specify anything.\n\nJohn von Neumann in his 1925 \"An axiomatisation of set theory\" wrestled with the same issues as did Russell, Zermelo, Skolem, and Fraenkel. He summarily rejected the effort of Russell:\n\nHe then notes the work of the set theorists Zermelo, Fraenkel and Schoenflies, in which \"one understands by \"set\" nothing but an object of which one knows no more and wants to know no more than what follows about it from the postulates. The postulates [of set theory] are to be formulated in such a way that all the desired theorems of Cantor's set theory follow from them, but not the antinomies.\n\nWhile he mentions the efforts of David Hilbert to prove the consistency of his axiomatisation of mathematics von Neumann placed him in the same group as Russell. Rather, von Neumann considered his proposal to be \"in the spirit of the second group ... We must, however, avoid forming sets by collecting or separating elements [durch Zusammenfassung oder Aussonderung von Elementen], and so on, as well as eschew the unclear principle of 'definiteness' that can still be found in Zermelo. [...] We prefer, however, to axiomatise not 'set' but 'function'.\"\n\nVan Heijenoort observes that ultimately this axiomatic system of von Neumann's, \"was simplified, revised, and expanded ... and it come to be known as the von Neumann-Bernays-Gödel set theory.\"\n\nDavid Hilbert's axiomatic system that he presents in his 1925 \"The Foundations of Mathematics\" is the mature expression of a task he set about in the early 1900s but let lapse for a while (cf his 1904 \"On the foundations of logic and arithmetic\"). His system is neither set theoretic nor derived directly from Russell and Whitehead. Rather, it invokes 13 axioms of logic—four axioms of Implication, six axioms of logical AND and logical OR, 2 axioms of logical negation, and 1 ε-axiom (\"existence\" axiom)-- plus a version of the Peano axioms in 4 axioms including mathematical induction, some definitions that \"have the character of axioms, and certain \"recursion axioms\" that result from a general recursion schema\" plus some formation rules that \"govern the use of the axioms\". \n\nHilbert states that, with regard to this system, i.e. \"Russell and Whitehead's theory of foundations[,] ... the foundation that it provides for mathematics rests, first, upon the axiom of infinity and, then upon what is called the axiom of reducibility, and both of these axioms are genuine contentual assumptions that are not supported by a consistency proof; they are assumptions whose validity in fact remains dubious and that, in any case, my theory does not require ... reducibility is not presupposed in my theory ... the execution of the reduction would be required only in case a proof of a contradiction were given, and then, according to my proof theory, this reduction would always be bound to succeed.\"\n\nIt is upon this foundation that modern recursion theory rests.\n\nIn 1925, Frank Plumpton Ramsey argued that it is not needed. However in the second edition of Principia Mathematica (1927, page xiv) and in Ramsey's 1926 paper it is stated that certain theorems about real numbers could not be proved using Ramsey's approach. Most later mathematical formalisms (Hilbert's Formalism or Brower's Intuitionism for example) do not use it.\n\nRamsey showed that it is possible to reformulate the definition of \"predicative\" by using the definitions in Wittgenstein's Tractatus Logico-Philosophicus. As a result, all functions of a given order are \"predicative\", irrespective of how they are expressed. He goes on to show that his formulation still avoids the paradoxes. However, the \"Tractatus\" theory did not appear strong enough to prove some mathematical results.\n\nKurt Gödel in his 1944 \"Russell's mathematical logic\" offers in the words of his commentator Charles Parsons, \"[what] might be seen as a defense of these [realist] attitudes of Russell against the reductionism prominent in his philosophy and implicit in much of his actual logical work. It was perhaps the most robust defense of realism about mathematics and its objects since the paradoxes and come to the consciousness of the mathematical world after 1900\". \n\nIn general, Gödel is sympathetic to the notion that a propositional function can be reduced to (identified with) the \"real objects\" that satisfy it, but this causes problems with respect to the theory of real numbers, and even integers (p. 134). He observes that the first edition of \"PM\" \"abandoned\" the realist (constructivistic) \"attitude\" with his proposal of the axiom of reducibility (p. 133). However, within the introduction to the second edition of \"PM\" (1927) Gödel asserts \"the constructivistic attitude is resumed again\" (p. 133) when Russell \"dropped\" of the axiom of reducibility in favour of the matrix (truth-functional) theory; Russell \"stated explicitly that all primitive predicates belong to the lowest type and that the only purpose of variables (and evidently also of constants) is to make it possible to assert more complicated truth-functions of atomic propositions ... [i.e.] the higher types and orders are solely a \"façon de parler\"\" (p. 134). But this only works when the number of individuals and primitive predicates is finite, for one can construct finite strings of symbols such as:\nAnd from such strings one can form strings of strings to obtain the equivalent of classes of classes, with a mixture of types possible. However, from such finite strings the whole of mathematics cannot be constructed because they cannot be \"analyzed\", i.e. reducible to the law of identity or disprovable by a negations of the law:\nBut he observes that \"this procedure seems to presuppose arithmetic in some form or other\" (p. 134), and he states in the next paragraph that \"the question of whether (or to what extent) the theory of integers can be obtained on the basis of the ramified hierarchy must be considered as unsolved.\" (p. 135)\n\nGödel proposed that one should take a \"more conservative approach\":\n\nIn a critique that also discusses the pros and cons of Ramsey (1931) Quine calls Russell's formulation of \"types\" to be \"troublesome ... the confusion persists as he attempts to define \"n\"th order propositions'... the method is indeed oddly devious ... the axiom of reducibility is self-effacing\", etc.\n\nLike Kleene Quine observes that Ramsey (1926), (1931) divided the various paradoxes into two varieties (i) \"those of pure set theory\" and (ii) those derived from \"semantic concepts such as falsity and specifiability\", and Ramsey believed that the second variety should have been left out of Russell's solution. Quine ends with the opinion that \"because of the confusion of propositions with sentences, and of attributes with their expressions, Russell's purported solution of the semantic paradoxes was enigmatic anyway.\"\n\nIn his section §12. First inferences from the paradoxes, subchapter \"LOGICISM\" Kleene (1952) traces the development of Russell's theory of types:\nKleene observes that \"to exclude impredicative definitions within a type, the types above type 0 [primary objects or individuals \"not subjected to logical analysis\"] are further separated into orders. Thus for type 1 [properties of individuals, i.e. logical results of the propositional calculus ], properties defined without mentioning any totality belong to \"order\" 0, and properties defined using the totality of properties of a given order below to the next higher order)\".\n\nKleene, however, parenthetically observes that \"the logicistic definition of natural number now becomes predicative when the [property] P in it is specified to range only over properties of a given order; in [this] case the property of being a natural number is of the next higher order\". But this separation into orders makes it impossible to construct the familiar analysis, which [see Kleene's example at Impredicativity] contains impredicative definitions. To escape this outcome, Russell postulated his \"axiom of reducibility\". But, Kleene wonders, \"on what grounds should we believe in the axiom of reducibility?\" He observes that, whereas \"Principia Mathematica\" is presented as derived from \"intuitively\"-derived axioms that \"were intended to be believed about the world, or at least to be accepted as plausible hypotheses concerning the world[,] ... if properties are to be constructed, the matter should be settled on the basis of constructions, not by an axiom.\" Indeed, he quotes Whitehead and Russell (1927) questioning their own axiom: \"clearly it is not the sort of axiom with which we can rest content\".\n\nKleene references work of Ramsey 1926 but notes that \"neither Whitehead and Russell nor Ramsey succeeded in attaining the logicistic goal constructively\" and \"an interesting proposal ... by Langford 1927 and Carnap 1931-2, is also not free of difficulties.\" Kleene ends this discussion with quotes from Weyl (1946) that \"the system of \"Principia Mathematica\" ... [is founded on] a sort of logician's paradise\" and anyone \"who is ready to believe in this 'transcendental world' could also accept the system of axiomatic set theory (Zermelo, Fraenkel, etc), which, for the deduction of mathematics, has the advantage of being simpler in structure.\"\n\n"}
{"id": "31199101", "url": "https://en.wikipedia.org/wiki?curid=31199101", "title": "Bioliquids", "text": "Bioliquids\n\nBioliquids are liquid fuels made from biomass for energy purposes other than transport (i.e. heating and electricity).\n\nBioliquids are usually made from virgin or used vegetable and seed oils, like palm or soya oil. These oils are burned in a power station to create heat, which can then be used to warm homes or boil water to make steam. This steam can then be used to drive a turbine to generate electricity.\n\nRudolf Diesel's first public exhibition of the internal combustion engine, that was to later bear his name, ran on peanut oil.\n\nBioliquids have been used for many years to provide heat for homes on a small scale but now big energy providers are looking at their use on a larger scale.\n\nA controversial plant in Bristol (UK) was recently given the go ahead despite receiving several hundred complaints. The plant will be built and operated by W4B and provide enough power for 25,000 homes.\n\nBioliquids have several key advantages over other sources of renewable energy:\n\nMany of the same problems that affect biofuels also affect bioliquids and there are various social, economic, environmental and technical issues, which have been discussed in the popular media and scientific journals. These include: the effect of moderating oil prices, the \"food vs fuel\" debate, poverty reduction potential, carbon emissions levels, sustainable biofuel production, deforestation and soil erosion, loss of biodiversity, impact on water resources, as well as energy balance and efficiency.\n\nBioliquids also have several key problems compared to other sources of renewable energy:\n"}
{"id": "972564", "url": "https://en.wikipedia.org/wiki?curid=972564", "title": "British National Vegetation Classification", "text": "British National Vegetation Classification\n\nThe British National Vegetation Classification or NVC is a system of classifying natural habitat types in Great Britain according to the vegetation they contain.\n\nA large scientific meeting of ecologists, botanists, and other related professionals in the United Kingdom resulted in the publication of a compendium of five books: \"British Plant Communities\", edited by John S. Rodwell, which detail the incidence of plant species in twelve major habitat types in the British natural environment. They are the first systematic and comprehensive account of the vegetation types of the country. They cover all natural, semi-natural and major artificial habitats in Great Britain (not Northern Ireland) and represent fifteen years of research by leading plant ecologists.\n\nFrom the data collated from the books, commercial software products have been developed to help to classify vegetation identified into one of the many habitat types found in Great Britain – these include \"MATCH\", \"TABLEFIT\" and \"MAVIS\".\n\nThe following is a list of terms used in connection with the British National Vegetation Classification, together with their meanings:\n\n\n\nFor a list of the constant species, and the NVC communities in which they are present, see List of constant species in the British National Vegetation Classification.\n\n\nFor a list of these rare species, and the NVC communities in which they are present, see List of rare species in the British National Vegetation Classification.\n\nIn total there are 286 communities in the British National Vegetation Classification. They are grouped into the following major categories:\n\n\nA full list of these communities, grouped into the above categories, can be found at List of plant communities in the British National Vegetation Classification.\n"}
{"id": "42495464", "url": "https://en.wikipedia.org/wiki?curid=42495464", "title": "Catherine Richards", "text": "Catherine Richards\n\nCatherine Richards (born 1952) is a Canadian new media artist. Richards is known for her work with virtual reality technologies. She was the first artist to use VR technology in a work of art in Canada, which was incorporated in her 1991 artwork \"Spectral Bodies\". \n\nRichards is Professor of Media Arts at the University of Ottawa.\n\nCatherine Richards' work explores the spectator and information technology relationship as the \"jam in the electro-magnetic sandwich.\" She deals with both old and new types of technology, exploreing the volatile sense of ourselves as we are shifting our boundaries - a process in which new technologies play a starring role.\n\nRichards has exhibited within and without North America at major venues including the 2004 Sydney Biennale. Her work has been discussed in publications by major theorists in the field including Katherine Hayles and Frances Dyson, and has been included in key surveys such as Art & Science Now, edited by Stephen Wilson. Richards is well known for collaborating with scientists and won the Artist in Residence for Research Fellowship (AIRes) at the National Research Council of Canada, 2002-2005, and is the recipient of the highest media prize in Canada from Canada Council for the Arts, Media Arts. Her work on virtuality and new media is considered groundbreaking in setting the aesthetic terrain, realm of artistic intervention and substantive issues.\n\nRichards was the first and sole artist to be awarded University Research Chair at the University of Ottawa. As a model for other universities, it is part of an on-going movement across North America to accept art as research within universities, a goal in which Richards has played a significant pioneering role.\n\n2004 Biennale of Sydney\n\n\"Spectral Bodies\" (1991) video artwork utilizes the juxtaposition of short narratives to reveal how the self can be lost if the body is lost.\n\n\"Body in Ruins\" (1986) published in \"Body invaders: panic sex in America\" is a photo piece, composed of video stills and text, which explores the uncertainty of the body in virtual reality.\n\n"}
{"id": "55931", "url": "https://en.wikipedia.org/wiki?curid=55931", "title": "Chronology", "text": "Chronology\n\nChronology (from Latin \"chronologia\", from Ancient Greek , \"chrónos\", \"time\"; and , \"-logia\") is the science of arranging events in their order of occurrence in time. Consider, for example, the use of a timeline or sequence of events. It is also \"the determination of the actual temporal sequence of past events\".\n\nChronology is part of periodization. It is also part of the discipline of history, including earth history, the earth sciences, and study of the geologic time scale.\n\nChronology is the science of locating historical events in time. It relies upon chronometry, which is also known as timekeeping, and historiography, which examines the writing of history and the use of historical methods. Radiocarbon dating estimates the age of formerly living things by measuring the proportion of carbon-14 isotope in their carbon content. Dendrochronology estimates the age of trees by correlation of the various growth rings in their wood to known year-by-year reference sequences in the region to reflect year-to-year climatic variation. Dendrochronology is used in turn as a calibration reference for radiocarbon dating curves.\n\nThe familiar terms \"calendar\" and \"era\" (within the meaning of a coherent system of numbered calendar years) concern two complementary fundamental concepts of chronology. For example, during eight centuries the calendar belonging to the Christian era, which era was taken in use in the 8th century by Bede, was the Julian calendar, but after the year 1582 it was the Gregorian calendar. Dionysius Exiguus (about the year 500) was the founder of that era, which is nowadays the most widespread dating system on earth. An epoch is the date (year usually) when an era begins.\n\n\"Ab Urbe condita\" is Latin for \"from the founding of the City (Rome)\", traditionally set in 753 BC. It was used to identify the Roman year by a few Roman historians. Modern historians use it much more frequently than the Romans themselves did; the dominant method of identifying Roman years was to name the two consuls who held office that year. Before the advent of the modern critical edition of historical Roman works, AUC was indiscriminately added to them by earlier editors, making it appear more widely used than it actually was.\n\nIt was used systematically for the first time only about the year 400, by the Iberian historian Orosius. Pope Boniface IV, in about the year 600, seems to have been the first who made a connection between these this era and Anno Domini. (AD 1 = AUC 754.)\n\nDionysius Exiguus’ Anno Domini era (which contains only calendar years \"AD\") was extended by Bede to the complete Christian era (which contains, in addition all calendar years \"BC\", but no \"year zero\"). Ten centuries after Bede, the French astronomers Philippe de la Hire (in the year 1702) and Jacques Cassini (in the year 1740), purely to simplify certain calculations, put the Julian Dating System (proposed in the year 1583 by Joseph Scaliger) and with it an astronomical era into use, which contains a leap year zero, which precedes the year 1 (AD).\n\nWhile of critical importance to the historian, methods of determining chronology are used in most disciplines of science, especially astronomy, geology, paleontology and archaeology.\n\nIn the absence of written history, with its chronicles and , late 19th century archaeologists found that they could develop relative chronologies based on pottery techniques and styles. In the field of Egyptology, William Flinders Petrie pioneered sequence dating to penetrate pre-dynastic Neolithic times, using groups of contemporary artefacts deposited together at a single time in graves and working backwards methodically from the earliest historical phases of Egypt. This method of dating is known as seriation.\n\nKnown wares discovered at strata in sometimes quite distant sites, the product of trade, helped extend the network of chronologies. Some cultures have retained the name applied to them in reference to characteristic forms, for lack of an idea of what they called themselves: \"The Beaker People\" in northern Europe during the 3rd millennium BCE, for example. The study of the means of placing pottery and other cultural artifacts into some kind of order proceeds in two phases, classification and typology: Classification creates categories for the purposes of description, and typology seeks to identify and analyse changes that allow artifacts to be placed into sequences.\n\nLaboratory techniques developed particularly after mid-20th century helped constantly revise and refine the chronologies developed for specific cultural areas. Unrelated dating methods help reinforce a chronology, an axiom of corroborative evidence. Ideally, archaeological materials used for dating a site should complement each other and provide a means of cross-checking. Conclusions drawn from just one unsupported technique are usually regarded as unreliable.\n\nThe fundamental problem of chronology is to synchronize events. By synchronizing an event it becomes possible to relate it to the current time and to compare the event to other events. Among historians, a typical need to is to synchronize the reigns of kings and leaders in order to relate the history of one country or region to that of another. For example, the Chronicon of Eusebius (325 A.D.) is one of the major works of historical synchronism. This work has two sections. The first contains narrative chronicles of nine different kingdoms: Chaldean, Assyrian, Median, Lydian, Persian, Hebrew, Greek, Peloponnesian, Asian, and Roman. The second part is a long table synchronizing the events from each of the nine kingdoms in parallel columns. The adjacent image shows two pages from the second section.\n\nBy comparing the parallel columns, the reader can determine which events were contemporaneous, or how many years separated two different events. To place all the events on the same time scale, Eusebius used an Anno Mundi (A.M.) era, meaning that events were dated from the supposed beginning of the world as computed from the Book of Genesis in the Hebrew Pentateuch. According to the computation Eusebius used, this occurred in 5199 B.C. The Chronicon of Eusebius was widely used in the medieval world to establish the dates and times of historical events. Subsequent chronographers, such as George Syncellus (died circa 811), analyzed and elaborated on the Chronicon by comparing with other chronologies. The last great chronographer was Joseph Justus Scaliger (1540-1609) who reconstructed the lost Chronicon and synchronized all of ancient history in his two major works, \"De emendatione temporum\" (1583) and \"Thesaurus temporum\" (1606). Much of modern historical datings and chronology of the ancient world ultimately derives from these two works. Scaliger invented the concept of the Julian Day which is still used as the standard unified scale of time for both historians and astronomers.\n\nIn addition to the literary methods of synchronism used by traditional chronographers such as Eusebius, Syncellus and Scaliger, it is possible to synchronize events by archaeological or astronomical means. For example, the Eclipse of Thales, described in the first book of Herodotus can potentially be used to date the Lydian War because the eclipse took place during the middle of an important battle in that war. Likewise, various eclipses and other astronomical events described in ancient records can be used to astronomically synchronize historical events. Another method to synchronize events is the use of archaeological findings, such as pottery, to do sequence dating.\n\n\n\nAspects and examples of non-chronological story-telling:\n\n\n\n\n"}
{"id": "16372307", "url": "https://en.wikipedia.org/wiki?curid=16372307", "title": "Combinatory categorial grammar", "text": "Combinatory categorial grammar\n\nCombinatory categorial grammar (CCG) is an efficiently parsable, yet linguistically expressive grammar formalism. It has a transparent interface between surface syntax and underlying semantic representation, including predicate-argument structure, quantification and information structure. The formalism generates constituency-based structures (as opposed to dependency-based ones) and is therefore a type of phrase structure grammar (as opposed to a dependency grammar).\n\nCCG relies on combinatory logic, which has the same expressive power as the lambda calculus, but builds its expressions differently. The first linguistic and psycholinguistic arguments for basing the grammar on combinators were put forth by Steedman and Szabolcsi. More recent prominent proponents of the approach are Jacobson and Baldridge.\n\nFor example, the combinator B (the compositor) is useful in creating long-distance dependencies, as in \"Who do you think Mary is talking about?\" and the combinator W (the duplicator) is useful as the lexical interpretation of reflexive pronouns, as in \"Mary talks about herself\". Together with I (the identity mapping) and C (the permutator) these form a set of primitive, non-interdefinable combinators. Jacobson interprets personal pronouns as the combinator I, and their binding is aided by a complex combinator Z, as in \"Mary lost her way\". Z is definable using W and B.\n\nThe CCG formalism defines a number of combinators (application, composition, and type-raising being the most common). These operate on syntactically-typed lexical items, by means of Natural deduction style proofs. The goal of the proof is to find some way of applying the combinators to a sequence of lexical items until no lexical item is unused in the proof. The resulting type after the proof is complete is the type of the whole expression. Thus, proving that some sequence of words is a sentence of some language amounts to proving that the words reduce to the type \"S\".\n\nThe syntactic type of a lexical item can be either a primitive type, such as \"S\", \"N\", or \"NP\", or complex, such as \"S\\NP\", or \"NP/N\".\n\nThe complex types, schematizable as \"X/Y\" and \"X\\Y\", denote functor types that take an argument of type \"Y\" and return an object of type \"X\". A forward slash denotes that the argument should appear to the right, while a backslash denotes that the argument should appear on the left. Any type can stand in for the \"X\" and \"Y\" here, making syntactic types in CCG a recursive type system.\n\nThe application combinators, often denoted by \">\" for forward application and \"<\" for backward application, apply a lexical item with a functor type to an argument with an appropriate type. The definition of application is given as:\n\nformula_1\n\nformula_2\n\nThe composition combinators, often denoted by formula_3 for forward composition and formula_4 for backward composition, are similar to function composition from mathematics, and can be defined as follows:\n\nformula_5\n\nformula_6\n\nThe type-raising combinators, often denoted as formula_7 for forward type-raising and formula_8 for backward type-raising, take argument types (usually primitive types) to functor types, which take as their argument the functors that, before type-raising, would have taken them as arguments.\n\nformula_9\n\nformula_10\n\nThe sentence \"the dog bit John\" has a number of different possible proofs. Below are a few of them. The variety of proofs demonstrates the fact that in CCG, sentences don't have a single structure, as in other models of grammar.\n\nLet the types of these lexical items be\n\nformula_11\n\nWe can perform the simplest proof (changing notation slightly for brevity) as:\n\nformula_12\n\nOpting to type-raise and compose some, we could get a fully incremental, left-to-right proof. The ability to construct such a proof is an argument for the psycholinguistic plausibility of CCG, because listeners do in fact construct partial interpretations (syntactic and semantic) of utterances before they have been completed.\n\nformula_13\n\nCCGs are known to be able to generate the language formula_14 (which is a non-context-free indexed language). A grammar for this language can be found in Vijay-Shanker and Weir (1994).\n\nVijay-Shanker and Weir (1994) demonstrates that Linear Indexed Grammars, Combinatory Categorial Grammars, Tree-adjoining Grammars, and Head Grammars are weakly equivalent formalisms, in that they all define the same string languages. Kuhlmann et al. (2015) show that this equivalence, and the ability of CCG to describe formula_15, rely crucially on the ability to restrict the use of the combinatory rules to certain categories, in ways not explained above.\n\n\n\n\n"}
{"id": "1052172", "url": "https://en.wikipedia.org/wiki?curid=1052172", "title": "Confederation (Poland)", "text": "Confederation (Poland)\n\nA konfederacja (, \"confederation\") was an \"ad hoc\" association formed by Polish-Lithuanian \"szlachta\" (nobility), clergy, cities, or military forces in the Polish-Lithuanian Commonwealth for the attainment of stated aims. A konfederacja often took the form of an armed rebellion aimed at redressing perceived abuses or trespasses of some (e.g. royal) authority. Such \"confederations\" acted in lieu of state authority or to force their demands upon that authority. They could be seen as a primary expression of direct democracy and right of revolution in the Commonwealth, and as a way for the nobles to act on their grievances and against the state's central authority.\n\nIn the late 13th century, confederations of cities, aiming to support public safety and provide security from rampant banditry, appeared, with the first confederation being that of several towns (Poznań, Pyzdry, Gniezno and Kalisz in Greater Poland) in 1298. In the mid-14th century, confederations of nobility, directed against the central authorities, emerged, with the first such confederation being that of 1352. During interregnums, confederations (essentially vigilance committees) formed to replace the inactive royal court, protect internal order, and defend the country from external dangers. The confederations, as a right of revolution, were recognized in Polish law through the Henrician articles (1573), part of the pacta conventa sworn by every Polish king since 1576. They stated (in the \"articulus de non praestanda oboedientia\", a rule dating to 1501 from Privilege of Mielnik) that if the monarch did not recognize or abused the rights and privileges of the nobility (szlachta), the nobles would no longer be bound to obey him and would have the legal right to disobey him.\n\nWith the beginning of the 17th century, confederations became an increasingly significant element of the Commonwealth's political scene. In the 17th and 18th centuries, confederations were organized by magnates, and were either pro- or anti-royal. A confederation not recognized by the king was considered a \"rokosz\" (\"rebellion\"), although some of the rokosz would be eventually recognized by the king, who could even join them himself. Most pro-royal confederations were usually formed as a response to an anti-royal one, and some would take a form of an extraordinary session of the parliament (sejm), as happened in 1710, 1717 and 1735.\n\nConfederations where usually formed in one part of the country, and could expand into \"general confederations\" taking in most or all of the voivodeships of the Polish-Lithuanian Commonwealth. However, even such general confederations would be formed separately for the Crown of the Kingdom of Poland and for the Grand Duchy of Lithuania.\n\nEach confederation had a key document explaining its goals, known as the act of the confederation, which was deposited with the court (usually the local court for the region the confederation was formed). Additional resolutions of the confederates, known as \"sanctia\", would also be deposited with the court. Membership of the confederation was voluntary, and required an oath. The executive branch of a confederation was headed by a marshal, and a group of advisers, each known as \"konsyliarz konfederacji\". A marshal and associated konsyliarze were known as a generality (\"generalność\"). A confederation would also have a larger council, similar to a parliament (\"walna rada\"), which made decisions by majority vote. Until around the mid-18th century, resolutions of the council had to be unanimous, but afterwards, majority voting became more common. The chief military commanders of confederations were known as regimentarze.\n\nAlso in the 18th century an institution known as a \"confederated sejm\" evolved. It was a parliament session (sejm) that operated under the rules of a confederation. Its primary purpose was to avoid being subject to disruption by the \"liberum veto\", unlike the national Sejm, which was paralyzed by the veto during this period. On some occasions, a confederated sejm was formed from the whole membership of the national Sejm, so that the \"liberum veto\" would not operate there.\n\nConfederations were proscribed by law in 1717, but continued to operate, indicating a weakness of the Commonwealth's central authority. They were also abolished by the Constitution of May 3, 1791 (adopted by the Four-Year Sejm of 1788–1792, itself a confederated sejm). But in practice this prohibition was not observed. The May 3rd Constitution was overthrown in mid-1792, by the Targowica Confederation of Polish magnates backed by Russian Empire and eventually joined, under extreme duress, by King Stanisław II August. The ensuing Russian military intervention led (to the Confederates' surprise) to the Second Partition of Poland in 1793. One of the last instances of confederation came in 1812, when the General Confederation of the Kingdom of Poland was formed in Warsaw to Napoleon I's campaign against the Russian Empire.\n\nSome confederations from Polish history included:\n\n"}
{"id": "1911151", "url": "https://en.wikipedia.org/wiki?curid=1911151", "title": "Conflict between good and evil", "text": "Conflict between good and evil\n\nThe conflict between good and evil is one of the most common conventional themes in literature, and is sometimes considered to be a universal part of the human condition.. There are several variations on this conflict, one being the battle between individuals or ideologies, with one side held up as \"Good\", while the other is portrayed as \"Evil\". Another variation is the inner struggle in characters (and by extension, humans in reality) between good and evil. \n\nThe form of tragedy described as best by Aristotle and exemplified by \"Oedipus Rex\" is, properly, concerned more with the tragic operations of fate than with a thematized conflict between good and evil. Nevertheless, the conflict between the good and the flawed aspects of the tragic hero form an important part of tragic catharsis in Aristotle's theory. There is, moreover, the form of tragedy with a happy ending that, although denigrated from Aristotle, was quite common in antiquity. This form, perhaps best exemplified by the Alcestis of Euripides, ends with a hero or god decisively beating an evil character. Northrop Frye has suggested that this form of \"tragedy\" is, in fact, the basic template for melodrama.\nWriters from the earliest times have thematized the conflict between good and evil, understood, of course, in religious terms. In the Old Testament, Yahweh asks the prophet Jeremiah: \"The heart is devious above all else; it is perverse—who can understand it?\" (, NRSV). Compare also the Book of Job.\nIn addition to explicating classical myth and stories to reveal a hidden conflict between good and evil in them, they wrote into their own texts different versions of the conflict. The basic forms may be described as the apocalyptic, in which the writer describes real, social events (whether historical or imagined) as manifestations of the eternal conflict between God and Satan, good and evil — a struggle that, if controlled in the end by God's omnipotence, was nevertheless of deep importance for humans. In a different way, Christian writers could focus on the internal struggle to find or maintain belief. This literature is exemplified by the \"Psychomachia\" of Prudentius, whose title continues to signify great psychological turmoil, and supremely by Augustine of Hippo's \"Confessions\", the model for countless later psychological biographies. Special mention might also be made of the \"Consolation of Philosophy\" of Boethius, a work that combines Platonic, Christian, and Stoic thought on the nature of suffering.\n\nJoseph Conrad defined all humans as having an \"inner evil\" or \"Heart of Darkness\" in his novella of the same name.\n\nWhile certainly not as widely seen as the direct good vs. evil conflict, the concept of \"individual vs. self\" is often much more compelling to a reader/watcher, especially if it is the protagonist. In both literature and film, it requires well-written character development in order to truly succeed.\n\nJoseph Conrad's version of the inner evil conflict, known as the Heart of Darkness, is a human's struggle with their own morals, and their own battle with their hidden evil. Although first chiefly used in the novel, this improved device was commonly used, as opposed to the old devices used in literature before the turn of the century. It is a conflict that exists outside of literature as well, making it a universal truth of the human condition.\n\n"}
{"id": "36211405", "url": "https://en.wikipedia.org/wiki?curid=36211405", "title": "Conspiracies against the laity", "text": "Conspiracies against the laity\n\nConspiracies against the laity is a term coined by George Bernard Shaw in his 1906 play The Doctor's Dilemma. The conspiracies refer to the methods used by professions to acquire prestige, power and wealth.\n\nOn the subject of such conspiracies, Tim Harford argued the following in his 2006 book The Undercover Economist: \"... doctors, actuaries, accountants and lawyers manage to maintain high wages through... erecting virtual 'green belts' to make it hard for competitors to set up shop. Typical virtual green belts will include very long qualification periods and professional bodies that give their approval only to a certain number of candidates per year. Many of the organisations that are put forth to protect us from 'unqualified' professionals in fact serve to maintain the high rates of the 'qualified' to whom we are directed.\"\n\nShaw's sentiments echo Adam Smith's earlier writing: \"People of the same trade seldom meet together, even for merriment and diversion, but the conversation ends in a conspiracy against the public, or in some contrivance to raise prices.\"\n"}
{"id": "1844155", "url": "https://en.wikipedia.org/wiki?curid=1844155", "title": "Creative class", "text": "Creative class\n\nThe creative class is a posited socioeconomic class identified by American economist and social scientist Richard Florida, a professor and head of the Martin Prosperity Institute at the Rotman School of Management at the University of Toronto. According to Florida, the creative class are a key driving force for economic development of post-industrial cities in the United States.\n\nFlorida describes the creative class as comprising 40 million workers (about 30 percent of the U.S. workforce). He breaks the class into two broad sections, derived from Standard Occupational Classification System codes:\n\nIn addition to these two main groups of creative people, the usually much smaller group of Bohemians is also included in the creative class.\n\nIn his 2002 study, Florida concluded that the creative class would be the leading force of growth in the economy expected to grow by over 10 million jobs in the next decade, which would in 2012 equal almost 40% of the population.\n\nThe social theories advanced by Florida have sparked much debate and discussion. Florida's work proposes that a new or emergent class—or demographic segment made up of knowledge workers, intellectuals and various types of artists—is an ascendant economic force, representing either a major shift away from traditional agriculture- or industry-based economies or a general restructuring into more complex economic hierarchies.\n\nThe theses developed by Florida in various publications were drawn from, among other sources, U.S. Census Bureau demographic data, focusing first on economic trends and shifts apparent in major U.S. cities, with later work expanding the focus internationally.\n\nA number of specific cities and regions (including California's Silicon Valley, Boston's Route 128, The Triangle in North Carolina, Austin, Seattle, Bangalore, Dublin and Sweden) have come to be identified with these economic trends. In Florida's publications, the same places are also associated with large Creative Class populations.\n\nFlorida argues that the creative class is socially relevant because of its members' ability to spur regional economic growth through innovation (2002).\n\nWalter Grünzweig, professor for American Studies at Technical University of Dortmund, has shown that the origin of the term “creative class” does not lie with Florida, but instead goes back to a passage in Ralph Waldo Emerson's essay \"Power\" in his collection \"The Conduct of Life\" (1860).\n\nFlorida says that the creative class is a class of workers whose job is to create meaningful new forms (2002). It is composed of scientists and engineers, university professors, poets and architects, and also includes \"people in design, education, arts, music and entertainment, whose economic function is to create new ideas, new technology and/or creative content\" (Florida, 2002, p. 8). The designs of this group are seen as broadly transferable and useful. Another sector of the Creative Class includes positions that are knowledge intensive; these usually require a high degree of formal education (Florida, 2002). Examples of workers in this sector are health professionals and business managers, who are considered part of the sub-group called Creative Professionals. Their primary job is to think and create new approaches to problems. Creativity is becoming more valued in today's global society. Employers see creativity as a channel for self-expression and job satisfaction in their employees. About 38.3 million Americans and 30 percent of the American workforce identify themselves with the creative class. This number has increased by more than 10 percent in the past 20 years.\n\nThe creative class is also known for its departure from traditional workplace attire and behavior. Members of the creative class may set their own hours and dress codes in the workplace, often reverting to more relaxed, casual attire instead of business suits and ties. Creative class members may work for themselves and set their own hours, no longer sticking to the 9–5 standard. Independence is also highly regarded among the creative class and expected in the workplace (Florida, 2002).\n\nThe Creative Class is not a class of workers among many, but a group believed to bring economic growth to countries that can attract its members. The economic benefits conferred by the Creative Class include outcomes in new ideas, high-tech industry and regional growth. Even though the Creative Class has been around for centuries, the U.S. was the first large country to have a Creative Class dealing with information technology, in the 1960s and 1970s. In the 1960s less than five percent of the U.S. population was part of the Creative Class, a number that has risen to 26 percent. Seeing that having a strong Creative Class is vital in today's global economy, Europe is now almost equal with America's numbers for this group. Inter-city competition to attract members of the Creative Class has developed.\n\nFollowing an empirical study across 90 nations, Rindermann et al. (2009) argued that high-ability classes (or smart classes) are responsible for economic growth, stable democratic development, and positively valued political aspects (government effectiveness, rule of law, and liberty).\n\nFlorida's use of census and economic data, presented in works such as \"The Rise of the Creative Class\" (2002), \"Cities and the Creative Class\" (2004), and \"The Flight of the Creative Class\" (2007), as well as \"Bobos in Paradise\" by David Brooks (whose \"bobos\" roughly correspond to Florida's creative class), and NEO Power by Ross Honeywill (whose NEOs deliver a more sophisticated level of evidence), has shown that cities which attract and retain creative residents prosper, while those that do not stagnate. This research has gained traction in the business community, as well as among politicians and urban planners. Florida and other Creative Class theorists have been invited to meetings of the National Conference of Mayors and numerous economic development committees, such the Denver mayor's Task Force on Creative Spaces and Michigan governor Jennifer Granholm's Cool Cities Initiative.\n\nIn \"Cities and the Creative Class\", Florida devotes several chapters to discussion of the three main prerequisites of creative cities (though there are many additional qualities which distinguish creative magnets). For a city to attract the Creative Class, he argues, it must possess \"the three 'T's\": Talent (a highly talented/educated/skilled population), Tolerance (a diverse community, which has a 'live and let live' ethos), and Technology (the technological infrastructure necessary to fuel an entrepreneurial culture). In \"Rise of the Creative Class\", Florida argues that members of the Creative Class value meritocracy, diversity and individuality, and look for these characteristics when they relocate (2002).\n\nAs Florida demonstrates in his books, Buffalo, New Orleans and Louisville are examples of cities which have tried to attract the Creative Class but, in comparison to cities which better exemplify the \"three 'T's\", have failed. Creative Class workers have sought out cities that better accommodate their cultural, creative, and technological needs, such as Chapel Hill, San Francisco, Washington, D.C., Austin, Seattle, Toronto, Ontario and Portland, Oregon. Florida also notes that Lexington and Milwaukee, Wisconsin have the ingredients to be a \"leading city in a new economy\".\n\nThe \"Creativity Index\" is another tool that Florida uses to describe how members of the Creative Class are attracted to a city. The Creativity Index includes four elements: \"the Creative Class share of the workforce; innovation, measured as patents per capita; high tech industry, using the Milken Institute's widely accepted Tech Pole Index…; and diversity, measured by the Gay Index, a reasonable proxy for an area's openness\" (2002, pp. 244–5). Using this index, Florida rates and ranks cities in terms of innovative high-tech centers, with San Francisco being the highest ranked (2002).\n\nFlorida and others have found a strong correlation between those cities and states that provide a more tolerant atmosphere toward culturally unconventional people, such as gays, artists, and musicians (exemplified by Florida's \"Gay Index\" and \"Bohemian Index\" developed in \"The Rise of the Creative Class\"), and the numbers of Creative Class workers that live and move there (2002).\n\nResearch involving the preferences and values of this new socioeconomic class has shown that where people choose to live can no longer be predicted according to conventional industrial theories (such as \"people will go to where the jobs/factories are\"). Creative workers are no longer bound by physical products, rather working with intellectual products. Their migration to metropolitan urban areas where creative work is available is more due to the attraction of leisure life and community rather than actual work. Although the Creative Class works towards the globalization of progressive and innovative ideas and products, they can also be considered to value local community and local autonomy. Sociologists and urban theorists have noted a gradual and broad shift of values over the past decade. Creative workers are looking for cultural, social, and technological climates in which they feel they can best \"be themselves\".\n\n\"The main assumption underlying this approach is that creative workers seek creative outlets in all aspects of their lives and therefore migrate to cities that actively support their preferred lifestyles\" (Donegan et al., 2008, p. 181).\n\nEach year Florida and the Martin Prosperity Institute release the Global Creativity Index, an international study of nations, ranking countries on the 3Ts of economic development - talent, technology, and tolerance. \"The GCI is a broad-based measure for advanced economic growth and sustainable prosperity based on the 3Ts of economic development - talent, technology, and tolerance. It rates and ranks 139 nations worldwide on each of these dimensions and on our overall measure of creativity and prosperity\" (Florida et al., 2015). The GCI takes into account the diversity of geographical locations noting their openness as the means for progressive ideas to prosper. \"Tolerance and openness to diversity is part and parcel of the broad cultural shift toward post-materialist values... Tolerance—or, broadly speaking openness to diversity—provides an additional source of economic advantage that works alongside technology and talent\" (Florida, 2012, p. 233). Diversity allows these locations to attract creative individuals and therefore stimulate economic growth. The findings from the 2015 GCI measured 139 countries on their creativity and prosperity. Ranked number one on the 2015 GCI is Australia.\n\nShare of Gross National Product spent on Research and Development is constantly raising at world level. Creative activities are growing at a fast pace in most advanced countries. 60% of the products that will be sold in 2030 do not yet exist. Repetitive works are being robotized. The whole world is becoming a \"Creational Society\".\n\nThe diverse and individualistic lifestyles enjoyed by the Creative Class involve active participation in a variety of experiential activities. Florida (2002) uses the term \"Street Level Culture\" to define this kind of stimulation. Street Level Culture may include a \"teeming blend of cafes, sidewalk musicians, and small galleries and bistros, where it is hard to draw the line between participant and observer, or between creativity and its creators\" (p. 166). Members of the Creative Class enjoy a wide variety of activities (e.g., traveling, antique shopping, bike riding, and running) that highlight the collective interest in being participants and not spectators (Florida, 2002).\n\nNumerous studies have found fault with the logic or empirical claims of Florida's Creative Class theory. This body of critical empirical research demonstrates how the Creative Class thesis, and the associated creative city policy prescriptions, in fact exacerbate social and economic inequalities in cities in North America, Europe, Australia, and Asia. Jamie Peck argues that the Creative Class theory offers no causal mechanism and suffers from circular logic. John Montgomery writes that \"what Florida has devised is a set of indices which simply mirror more fundamental truths about creative milieux or dynamic cities.\" Montgomery also disagrees with the cities that Florida designates as most creative, writing that London, not Manchester and Leicester, should be one of the top in the U.K. A critique of Florida's research and theoretical framework has been developed by Matteo Pasquinelli (2006) in the context of Italian Operaismo.\n\nScholars in the disciplines of economics, geography, sociology, and related social sciences have challenged Florida's conception of the \"creative class\", particularly for the perceived fuzziness of the concept and the lack of analytical precision. A number of studies have found problems with Florida's statistical indices. Hoyman and Faricy, using Florida's own indices, find no statistical evidence that cities with higher proportions of Creative Class workers correlated with any type of economic growth from 1990–2004. By using metropolitan areas as the unit of analysis, the high degree of socio-spatial variation across the metropolitan region is ignored. Studies and popular accounts have questioned whether the creative class is more likely to live in the homogenous, low-density suburban periphery.\n\nSocial scientists have also identified problems with the occupational composition of the creative class. Economic geographer Stefan Kratke challenges the inclusion of financial and real estate professionals within the creative class on two accounts: 1) these individuals played a decisive role as the \"dealer class\" in the ongoing financial crises, and therefore cannot be considered a basis for sustainable urban and regional economic growth; and 2) the financial and real estate industries (especially in headquarter cities) are economically significant regional/urban players only because they are largely \"reliant on inflows of wealth created by productive activities in other regions.\" Moreover, Kratke argues that the \"political class\" is also ill-suited to be included within creative class, as they are, in many cases, implicated in neoliberal financial deregulation and the rise in highly unstable urban and regional growth regimes evident through real estate bubbles across the United States and in other countries. In \"Urban Development and the Politics of the Creative Class\", Ann Markusen argues that workers qualified as being in the Creative Class have no concept of group identity, nor are they in occupations that are inherently creative. Markusen also notes that the definition of the Creative Class is based largely on educational attainment, suggesting that Florida's indices become insignificant after controlling for education. Markusen argues that Florida \"does not seem to understand the nature of the occupational statistics he uses\" and calls for the major occupational groups to be disaggregated. She questions the inclusion of particular occupations within these broad categories such as claim adjusters, funeral directors, tax collectors, yet argues that \"[t]hese occupations may indeed be creative, but so too are airplane pilots, ship engineers, millwrights, and tailors – all of whom are uncreative in Florida's tally.\" Moreover, it is questioned whether human creativity can be conflated with education since \"[p]eople at all levels of education exercise considerable inventiveness.\"\n\nResearch shows that economic growth is experienced when the significance of scientifically/technologically and artistically creative workers is taken into account, but this macro-level conclusion can be drawn without Florida's creative class theory, which provides more of an \"affirmation of contemporary class relations.\" Other scholars have criticized the very basis for Florida's definition of \"creativity\" which many argue is conceived of narrowly and is only valued for the potential for financial and economic growth. Studies have too questioned Florida's argument that jobs and economic growth follow the creative class, and the migration patterns of the creative class have been challenged. Rather than validating Florida's causal logic that attracting the creative class will lead to economic growth, empirical research shows that successful regions pull and maintain human capital.\n\nThe creative class thesis—and Richard Florida himself—have been criticized for what appears to be a change in Florida's prognosis for America's ailing Rust Belt cities. Florida's message was so quickly and enthusiastically adopted by cities because he argued that any city had the potential to become a vibrant, creative city with the right infrastructure investments, policies, and consulting advice. A 2009 article, \"The Ruse of the Creative Class\", questions Florida's costly speaking engagements in struggling industrial cities in which he offered optimistic prognoses—and his more recent pronouncements that many American cities may never be saved in the wake of the Great Recession. The creative class thesis has also drawn criticisms for relying on inner city property development, gentrification, and urban labor markets reliant on low-wage service workers, particularly in the hospitality industry. Florida has called for service workers' wages to rise.\n\nCreative Class Struggle, a Toronto-based collective, has brought these criticisms outside academic circles, challenging Florida's Creative Class theories as well as their widespread adoption into urban policy. The group manages an online clearinghouse for information about creative city strategies and policies, publishes a newsletter and other materials, and works to engage the media and public in critical discussion. In June 2009, Creative Class Struggle and art magazine \"Fuse\" organized a public forum in Toronto to debate these issues.\n\n\n\n\n\n"}
{"id": "28028997", "url": "https://en.wikipedia.org/wiki?curid=28028997", "title": "Domar serfdom model", "text": "Domar serfdom model\n\nThe Domar Serfdom Model is a mid-to-late 20th century model that develops a hypothesis concerning the causes of agricultural slavery or serfdom in historical societies. Evsey Domar first presented this model in his 1970 paper, “ The Causes of Slavery or Serfdom: A Hypothesis” published in the Economic History Review. The Domar Serfdom Model revives a hypothesis originally suggested by Russian Historian Vasily Klyuchevsky, who looks at the causes of slavery through the lens of the Russian experience in the 16th and 17th centuries. In his revisiting of the hypothesis, Domar aims to give it wider applicability while focusing more on an analysis that yields an economic model as an explanation of the causes of slavery.\n\nIn his model of serfdom, Domar assumes the following:\n\n\nIn addition, Domar asserts that there are 3 elements of agricultural structure relevant in his model: \n\nDomar claims that any 2 of these 3 elements can exist cohesively, but never can all 3 elements exist simultaneously in any particular situation. Dependent on which elements exist cohesively, one will see differing types of societies. (Domar, 5)\nAdditionally, production units will require bond labor. It is this attribution of bond labor resulting from human choice that separates Domar’s model from models that attribute slavery to a product of nature, such as Staple’s Thesis. (Domar, 5)\n\nDomar concentrates the testing of his model on historical analysis of serfdom and slavery in Russia, Poland-Lithuania, Western Europe, and the United States. \n\nThe Domar Serfdom Model has been met with criticism since its publishing in \"The Journal of Economic History\" in 1970. In 2009, Professor Jean Jacques Rosa authored a paper titled, “The Causes of Serfdom: A Hypothesis Revisited” in which Rosa argues that serfdom and slavery are mutually exclusive events. Where Domar argues that labor scarcity is a necessary but exogenous political factor needed for serfdom, Rosa challenges his view by showing that “in an agrarian subsistence economy the complementary conditions of serfdom are (a) oligopsony power in labor demand, sustained by (b) an oligopolistic supply of violence by large land owners.” In addition, Professor Erik Green of Lund University argues that Domar’s model fails in explaining the origins of slavery in Cape Colony, South Africa, where the consensus is that slavery evolved as an urban phenomenon, which would call into question the land/labor ratio Domar uses to explain the causes of slavery and serfdom. Moreover, Professor Jonathon Cunning of Hunter University authored a paper titled “On the Causes of Slavery or Serfdom and the Roads to Agrarian Capitalism: Domar’s Hypothesis Revisited” In his paper, Cunning offers a “simple general equilibrium formalization of Domar’s famous hypothesis on the causes of slavery or serfdom that emphasizes the interactions between factor endowments, the nature of the production technologies, and the initial distribution of property rights over land.” Cunning’s paper is viewed more so as an expansion upon Domar’s model rather than a direct challenge to his hypothesis. \n"}
{"id": "5477326", "url": "https://en.wikipedia.org/wiki?curid=5477326", "title": "Egosurfing", "text": "Egosurfing\n\nEgosurfing (also Googling yourself, vanity searching, egosearching, egogoogling, autogoogling, self-googling) is the practice of searching for one's own name, or pseudonym on a popular search engine in order to review the results. Similarly, an egosurfer is one who surfs the Internet for his or her own name to see what information appears. It has become increasingly popular with the rise of internet search engines, as well as free blogging and web-hosting services. Though Google is the search engine most commonly mentioned when referring to egosurfing, other widely known search engines include Yahoo and Bing.\n\nThe term was coined by Sean Carton in 1995 and first appeared in print as an entry in Gareth Branwyn's March 1995 Jargon Watch column in \"Wired\".\n\nEgosurfing is employed by many people for a variety of reasons. According to a study by the Pew Internet & American life project, 47% of American adult internet users have undertaken a vanity search in Google or another search engine. Some egosurf purely for entertainment, such as finding celebrities with the same name. However, many people egosurf as a means of online reputation management. Egosurfing can be used to find data spills, released information that is undesirable to have in the public eye. By searching one's own name in an online search engine, one can take on the perspective of a stranger attempting to find out personal information. Some egosurf in order to conceal personal images or information from potential employers, clients, identity thieves, and the like. Similarly, some use egosurfing to maintain a positive public image and to achieve self-promotion.\n\nMany social networking sites, such as Facebook, allow users to make their profiles \"searchable,\" meaning that their profile will appear in the appropriate search results. As a result, those seeking to maintain their privacy often egosurf in order to ensure that their profile does not appear in search engine results. As more people create online personas, many feel the need to more cautiously monitor their digital footprint, including information that they have not chosen to share online, such as telephone numbers and public records.\n\nAlthough personal information available online can be difficult to remove, in 2009 Google introduced a feature allowing users to create a small box listing personal information such as name, occupation, and location that appears on the first page of results when their name is searched. The box links to a full profile page, similar to one seen on Facebook. This Google profile can be linked to other social networking sites, such as one's blog, company website or Twitter feed. The more information that one includes on their Google profile, the higher one's informational box will rank in the results, thus essentially encouraging one to post personal information online and continue egosurfing.\n\n\n"}
{"id": "32419813", "url": "https://en.wikipedia.org/wiki?curid=32419813", "title": "Eunomics", "text": "Eunomics\n\nEunomics is a term, first proposed by legal scholar Lon Fuller in 1954, to describe \"the science, theory or study of good order and workable arrangements\". Stemming from Behavioral Systems Theory, it was an attempt to fuse what Fuller saw as the inherent morality of law with the empirical data and methods of the objective sciences. Its main practical application appears to be as a form of industrial dispute resolution.\n\nNobel prize winner Oliver E. Williamson, a leading mind of the New Institutional Economics (NIE) movement, refers to Fuller's definition of eunomics as a simulacrum of his understanding of the term \"governance\". Same as with the NIE, eunomics are foremost concerned with the question how to achieve a desired outcome.\n\n"}
{"id": "42574375", "url": "https://en.wikipedia.org/wiki?curid=42574375", "title": "Fan effect", "text": "Fan effect\n\nThe fan effect is a psychological phenomenon under the branch of cognitive psychology where recognition times or error rate for a particular concept increases as more information about the concept is acquired. The word \"fan\" refers to the number of associations correlated with the concept.\n\nThe fan effect first appeared in a series of experiments conducted by John R. Anderson, a cognitive psychologist, in 1974. The three experiments he conducted involved participants learning 26 sentences that paired a person with a location. Additionally, they were asked to determine whether or not a particular sentence that was given to them belonged to the 26 they were asked to study. An example of a sentence Anderson used in his experiment was: \"A hippie is in the park.\" Some sentences seemed similar in the sense that a person was paired with another location. For instance, \"A hippie is in the church.\" Results revealed that participants produced a longer retrieval time when a person was paired with more than one location. Overall, these experiments demonstrated that multiple associations, such as including a large number of nouns in a sentence, interfered with the recognition time by producing a much slower effect.\n\nIn the brain, memory stores information in a network of nodes that are linked together. When a memory is retrieved, activation spreads on the links until it intersects or enough time has passed. If there is one association, the activation only has to spread to a single link whereas multiple associations would divide the activation to many links. Because there are so many connections, it causes the brain to take a longer time to identify the concepts and retrieve the memory.\n\nThe fan effect is due to multiple mental models and is included as part of the ACT-R theory. The key factors that the fan effect is dependent on are the strength and degree to which one of the variables can predict the other and the importance of the concept to a person during the retrieval process. Concepts can be better recognized by similar ideas instead of a random order of ideas. When stored in a random order, the concepts are placed in independent places in the brain instead of putting the concepts together as one unit. The fan effect can be reduced if random sentences are exposed frequently and unified into one concept.\n"}
{"id": "54010544", "url": "https://en.wikipedia.org/wiki?curid=54010544", "title": "Flowers in a Glass Vase", "text": "Flowers in a Glass Vase\n\nFlowers in a glass vase on a partly draped stone ledge is a circa 1667 floral painting by Nicolaes van Verendael in the collection of the Suermondt-Ludwig-Museum.\n\nNicolaes van Verendael was a respected flower painter in Antwerp who worked with Jan Davidsz. de Heem, among others. The early provenance of this painting is unknown but it can be dated based on other works by Van Verendael, such as his garland painting in the Prado which was long attributed to Jan Brueghel the Elder, who began floral painting in Antwerp a half-century beforehand.\nThis painting dates to the period in Antwerp when hothouses became popular among the nobility and patrons ordered paintings to record their personal hothouse triumphs. Not all of the blooms would have bloomed at the same time, and the painting was meant more for decoration than for botanical accuracy. The work shows the following flower species: Rosa alba, Tropaeolum majus, Hepatica nobilis, rosemary, Tulipa, Delphinium, Aquilegia, Punica granatum, Rosa × centifolia.\n\nThis painting was stored for safe-keeping during WWII in the Albrechtsburg in Meissen, from whence it was found and taken by marauding Soviet troops in 1945. Somehow it ended up in the possession of a German citizen who later migrated to Canada and it appeared on the art market in the 1970s, whereupon it was returned to the museum after 70 years.\n\n"}
{"id": "2894539", "url": "https://en.wikipedia.org/wiki?curid=2894539", "title": "Frugality", "text": "Frugality\n\nFrugality is the quality of being frugal, sparing, thrifty, prudent or economical in the consumption of consumable resources such as food, time or money, and avoiding waste, lavishness or extravagance.\n\nIn behavioral science, frugality has been defined as the tendency to acquire goods and services in a restrained manner, and resourceful use of already owned economic goods and services, to achieve a longer term goal.\n\nCommon strategies of frugality include the reduction of waste, curbing costly habits, suppressing instant gratification by means of fiscal self-restraint, seeking efficiency, avoiding traps, defying expensive social norms, detecting and avoiding manipulative advertising, embracing cost-free options, using barter, and staying well-informed about local circumstances and both market and product/service realities. Frugality may contribute to health by leading people to avoid products that are both expensive and unhealthy when used to excess. Frugal living is mainly practiced by those who aim to cut expenses, have more money, and get the most they possibly can from their money.\n\nIn the context of some belief systems, frugality is a \"philosophy\" in which one does not trust (or is deeply wary of) \"expert\" knowledge from commercial markets or corporate cultures, claiming to know what is in the best economic, material, or spiritual interests of the individual.\n\nDifferent spiritual communities consider frugality to be a virtue or a spiritual discipline. The Religious Society of Friends and the Puritans are examples of such groups. The basic philosophy behind this is the idea that people ought to save money in order to allocate it to more charitable purposes, such as helping others in need.\n\nThere are also environmentalists who consider frugality to be a virtue through which humans can make use of their ancestral skills as hunter-gatherers, carrying little and needing little, and finding meaning in nature instead of man-made conventions or religion. Henry David Thoreau expressed a similar philosophy in \"Walden\", with his zest for self-reliance and minimal possessions while living simply in the woods.\n\nFrugality has been adopted as a strategic imperative by large enterprises as a means of cost reduction through engenderment of a philosophy of careful spending amongst the workforce. Cost reduction is often perceived negatively, be it within a corporate organisation or in society, so inviting each employee to embrace frugality transfers the burden of cost reduction from management to the employee. In doing so, corporations introduce a moral obligation to cost cutting, proposing the notion that careful management of costs is in the company, shareholder and employee's best interests.\n"}
{"id": "43103489", "url": "https://en.wikipedia.org/wiki?curid=43103489", "title": "Galor-Zeira model", "text": "Galor-Zeira model\n\nThe Galor-Zeira model is a modern macroeconomic theory about income distribution and its impact on economic growth. It challenges the prevalent neoclassical doctrine that equality inhibits the development of human capital.\n\nThe Galor-Zeira model describes the inequality-education-growth relationship, explained by the mechanism of unequal access to education due to imperfect capital markets. It claims that this mechanism might lead to a negative effect of inequality on growth. The initial distribution of income determines whether an economy will converge to a low- or high-income regime, as there are multiple steady states in the model.\n\nThe model was developed by Oded Galor and Joseph Zeira in 1988, and it was published in the paper “Income Distribution and Macroeconomics”, 1993.\n\nThe Neoclassical viewpoint has been challenged in the past two decades, as both theories and subsequent empirical evidence have demonstrated that income distribution has a significant impact on the growth process. In contrast to the representative agent approach which dominated the field of macroeconomics for several decades, the modern perspective, originated by Galor and Zeira (1988, 1993), has underlined the role of heterogeneity in the determination of macroeconomic activity. It has advanced a novel viewpoint that heterogeneity, and thus income distribution, plays an important role in the determination of aggregate economic activity and economic growth in the short-run as well as in the long-run.\n\nGalor and Zeira have demonstrated that in the presence of credit market imperfections, income distribution has a long-lasting effect on investment in human capital, aggregate income, and economic development. Moreover, in contrast to the classical hypothesis, which underscored the virtues of inequality for economic growth, their research advanced the hypothesis that inequality may be detrimental for human capital formation and economic development.\n\nThe model consists of four assumptions:\n\nThere are three possible individual scenarios in the model:\n\nThe economic situation of each dynasty depends on its initial wealth, due to credit markets’ imperfections and an indivisibility in investment in human capital. In rich dynasties, all generations invest in human capital, work as skilled and leave a large inheritance to their children. In poor dynasties people inherit less, work as unskilled, and leave less to the next generation. They can get stuck in a poverty trap. Thus, the initial distribution of wealth determines the size of these two groups: the rich and the poor dynasties. As a result, it also determines GDP, as skilled are more productive. Therefore, inequality may negatively affect macroeconomic activity and economic development due to “intergenerational transfers and their effect on the persistence of inequality”.\n\nA government policy can change the long-run equilibrium. The government can subsidize education, which reduces the individual cost of education and finance it by taxing those who study and become skilled. This increases human capital investment in the short- and long-run, increases GDP and is a Pareto improving policy. The model gives an explanation to the rise of public education.\n\nAccording to the Galor-Zeira model, the wealth-equality relationship works three ways:\nThat means, wealth and equality are highly correlated and affect one another. On the one hand, countries with a greater income per capita have more equal distribution of income and smaller wage differentials. On the other hand, countries with more equal initial distribution of wealth and have higher income level in the long run.\n\nThe Oxford University Press named the Galor-Zeira paper(\"Income Distribution and Macroeconomics\") among the 11 most path-breaking papers published in The Review of Economic Studies in the past 60 years.\n\n"}
{"id": "16832087", "url": "https://en.wikipedia.org/wiki?curid=16832087", "title": "Gender schema theory", "text": "Gender schema theory\n\nGender schema theory was formally introduced by Sandra Bem in 1981 as a cognitive theory to explain how individuals become gendered in society, and how sex-linked characteristics are maintained and transmitted to other members of a culture.\nGender-associated information is predominantly transmuted through society by way of schemata, or networks of information that allow for some information to be more easily assimilated than others. Bem argues that there are individual differences in the degree to which people hold these gender schemata. These differences are manifested via the degree to which individuals are sex-typed.\n\nCore gender identity is tied up in the sex typing that an individual undergoes. This typing can be heavily influenced by child rearing, media, school, and other forms of cultural transmission. Bem refers to four categories in which an individual may fall: sex-typed, cross-sex-typed, androgynous, and undifferentiated. Sex-typed individuals process and integrate information that is in line with their gender. Cross-sex-typed individuals process and integrate information that is in line with the opposite gender. Androgynous individuals process and integrate traits and information from both genders. Finally, undifferentiated individuals do not show efficient processing of sex-typed information.\n\nBeing that gender schema theory is a theory of process and not content, this theory can help explain some of the processes by which gender stereotypes become so psychologically ingrained in our society. Specifically, having strong gender schemata provides a filter through which we process incoming stimuli in the environment. This leads to an easier ability to assimilate information that is stereotype congruent, hence further solidifying the existence of gender stereotypes. Within adolescent development, Bem hypothesizes that children must choose among a plethora of dimensions, but that gender schemas lead to the regulation of behaviors that conform to the cultural definition of what it means to be male or female. Additionally, Bem asserts that there is also a heterosexuality subschema, which likely encouraged the development of gender schemas. Most societies treat exclusive heterosexuality as the benchmark for proper masculinity and femininity—that is, heterosexuality is the norm. Furthermore, the heterosexuality subschema asserts that men and women are supposed to be different from one another. It is hypothesized that this is why cross-sexed interactions are likely to be sexually coded. Sex-typed individuals have a general readiness to invoke the heterosexuality subschema in social interactions, behaving differently towards individuals of the opposite sex that they find attractive v. unattractive.\n\nSome of the early tests of gender schema theory came in the form of memory and other cognitive tasks designed to assess facilitated processing of sex-typed information. Much of this early research found that participants who were sex-typed remembered more traits associated with their sex, as well as processed sex-type congruent information more efficiently, suggesting that the gender schemata possessed by sex-typed individuals help to assimilate sex-associated information into one’s self-concept (see Bem, 1981).\nBem showed that when given the option of clustering words by either semantic meaning or gender, sex-typed individuals are more likely to use the gender clustering system, followed by undifferentiated individuals. Cross-typed individuals had the lowest percentage of words clustered by gender.\n\nA strong source of sex-typing comes from the rearing practices of parents. Bem offers strong suggestions for preventing the sex-typing of children, including the prevention of access to media that promotes sex-typing, altering media and stories to eliminate sex-typing information, and modeling equal roles for mothers and fathers in the household.\nFor example, Bem edited the books that her children read to create a more androgynous view. This included, for example, drawing long hair and feminine body characteristics on male figures. Ultimately, however, this is somewhat limited because children will become exposed to some of this sex-typing information, particularly when they begin attending school. Therefore, Bem suggests teaching alternative schemata to children so that they are less likely to build and maintain a gender schema. Some examples include an individual differences schema, where children learn to process information on a person-by-person basis rather than make wide assumptions about groups based on information from individuals. Also, providing children with a sexism schema, where children learn to process sex-typed information through a filter that promotes moral outrage when sexist information is being promoted, can assist in providing children with the resources to not only keep from becoming sex-typed but also promote positive social change.\n\nBem wished to raise consciousness that the male/female dichotomy is used as an organizing framework, often unnecessarily, especially in the school curriculum. She stressed that the omnirelevance of gender has a negative impact on society, and that the gender schema should be more limited in scope. Within the feminist lens, androgyny is not radical enough, because androgyny means that “masculine” and “feminine” still exist. Rather, society should decrease the use of the gender dichotomy as a functional unit, and be aschematic.\n\nThe legacy of gender schema theory has not been one of obvious lasting impact in the psychology of gender. Bem's theory was undoubtedly informed by the cognitive revolution of the 1970s and 1980s and was coming at a time when the psychology of gender was drastically picking up interest as more and more women were entering academic fields. While gender schema theory does provide a cognitive backbone for how gender stereotypes may continue to be maintained in current society, it lost wind as more broad sociological theories became the dominant force in the psychology of gender. A major limitation of gender schema theory has been that once research supported the nature of the process, there was little work that followed.\n\nThe longest-lasting contribution to the field has been the Bem Sex-Role Inventory. Originally developed as a tool to identify sex-typed individuals, many researchers use the measure to look at other components of gender, including endorsement of gender stereotypes and as a measure of masculinity/femininity. Caution should be employed when examining research that uses the Bem Sex-Role Inventory for measuring constructs that it was not created to measure.\n\nBem herself admitted that she was ill-prepared to develop the Bem Sex-Role Inventory and never anticipated it being as widely used as it still is today.\n"}
{"id": "743068", "url": "https://en.wikipedia.org/wiki?curid=743068", "title": "Gift card", "text": "Gift card\n\nA gift card (also known as gift certificate in North America, or gift voucher or gift token in the UK) is a prepaid stored-value money card usually issued by a retailer or bank to be used as an alternative to cash for purchases within a particular store or related businesses. Gift cards are also given out by retailers and marketers as part of a promotion strategy, to entice the recipient to come in or return to the store, and at times such cards are called cash cards. Gift cards are generally redeemable only for purchases at the relevant retail premises and cannot be cashed out, and in some situations may be subject to an expiry date or fees. Visa and MasterCard credit cards produce generic gift cards which need not be redeemed at particular stores, and which are widely used for cashback marketing strategies. A feature of these cards is that they are generally anonymous and are disposed of when the stored value on a card is exhausted.\n\nFrom the purchaser's point of view, a gift card is a gift, given in place of an object which the recipient may not need, when the giving of cash as a present may be regarded as socially inappropriate. In the United States, gift cards are highly popular, ranking in 2006 as the second-most given gift by consumers and the most-wanted gift by women, and the third-most wanted by males. Gift cards have become increasingly popular as they relieve the donor of selecting a specific gift. In 2012, nearly 50% of all US consumers claimed to have purchased a gift card as a present during the holiday season. In Canada, $1.8 billion was spent on gift cards, and in the UK it is estimated to have reached £3 billion in 2009, whereas in the United States about US$80 billion was paid for gift cards in 2006. The recipient of a gift card can use it at their discretion within the restrictions set by the issue, for example as to validity period and businesses that accept a particular card.\n\nThe first giftcard using a payments infrastructure was introduced by Neiman Marcus in late 1994, though Blockbuster Entertainment was the first company to do so at a wide scale, test-marketing them in 1995 and launching them around the country the next year. In the beginning, the Blockbuster giftcard replaced gift certificates that were being counterfeited with recently introduced color copiers and color printers. Blockbuster's first giftcard transactions were processed by what was then Nabanco of Sunrise, Florida. Nabanco was the developer of the first third-party platform for the processing of giftcards using existing payment infrastructure.\n\nNeiman Marcus and Blockbuster were later followed by the Mobil gas card which initially offered prepaid phone value provided by MCI. Kmart was next with the introduction of the Kmart Cash Card which in the early generations provided prepaid phone time with AT&T. Later this feature was dropped as it was not profitable for both Kmart and Mobil. The Kmart Mags Pangilinan Cash Card was the first replacement for cash returns when a shopper did not have a receipt for a gift. This practice of giving a cash card in place of cash for non-receipted returns is common place today with most merchants. From these early introductions, other retailers began to adapt a giftcard program to replace their gift certificate programs.\n\nA gift card may resemble a credit card or display a specific theme on a plastic card the size of a credit card. The card is identified by a specific number or code, not usually with an individual name, and thus could be used by anybody. They are backed by an on-line electronic system for authorization. Some gift cards can be reloaded by payment and can be used thus multiple times.\n\nCards may have a barcode or magnetic strip, which is read by an electronic credit card machine. Many cards have no value until they are sold, at which time the cashier enters the amount which the customer wishes to put on the card. This amount is rarely stored on the card but is instead noted in the store's database, which is crosslinked to the card ID. Gift cards thus are generally not stored-value cards as used in many public transport systems or library photocopiers, where a simplified system (with no network) stores the value only on the card itself. To thwart counterfeiting, the data is encrypted. The magnetic strip is also often placed differently than on credit cards, so they cannot be read or written with standard equipment. Other gift cards may have a set value and need to be activated by calling a specific number.\n\nGift cards can also be custom tailored to meet specific needs. By adding a custom message or name on the front of the card, it can make for an individualized gift or incentive to an employee to show how greatly they are appreciated.\n\nGift cards are divided into \"open loop\" or \"network\" cards and \"closed loop\" cards. The former are issued by banks or credit card companies and can be redeemed by different establishments, the latter by a specific store or restaurant and can be only redeemed by the issuing provider. The latter, however, tend to have fewer problems with card value decay and fees. In either case the giver would buy the gift card (and may have to pay an additional purchase or activation fee), and the recipient of the card would use the value of the card at a later transaction. A third form is the \"hybrid closed loop\" card where the issuer has bundled a number of closed loop cards; an example is free gift cards for a specific mall.\n\nGift cards differ from gift certificates, in that the latter are usually sold as a paper document with an authorized signature by a restaurant, store, or other individual establishment as a voucher for a future service; there is no electronic authorization. A gift certificate may or may not have an expiration date and generally has no administrative fees.\n\nBank-issued gift cards may be used in lieu of checks as a way to disburse rebate funds. Some retailers use the gift card system for refunds in lieu of cash thereby assuring that the customer will spend the funds at their store.\n\nA Charity Gift Card allows the gift giver to make a charitable donation, and the gift recipient to choose a charity that will receive the donation.\n\nMobile gift cards are delivered to mobile phones via email or SMS and phone apps allow users to carry only their cell phone. Benefits include tying them to a particular phone number and ease of distribution.\n\nVirtual gift cards are delivered via e-mail to the recipient, the benefits being that they cannot be lost and that the consumer does not have to drive to the bricks and mortar location to purchase a gift card.\n\nOther companies have introduced virtual gift cards that users redeem on their smartphones. As the merchant is not involved in the loop, it is considered a cash transfer rather than a traditional gift card.\n\nIt has been argued that holiday giving destroys value due to mismatching gifts. The most efficient way to keep value in gifting would be to give cash; however, this is socially acceptable only within limits. Gift cards, to a degree, may overcome this problem but have certain pitfalls. Some feel that the absence of the thought of selecting a specific gift makes a gift card a worse choice than a poorly executed but individual gift. New products in the gift card industry are evolving to tackle this \"impersonal\" pitfall of gift cards. New services launched by some service providers allows for customization and personalization of gift cards.\n\nGift cards have been criticized for the ability of the issuer to set rules that are detrimental to the purchaser or card recipient. For example, gift cards may be subject to an expiry date, administrative fees, restrictions on use, and absence of adequate protection in case of fraud or loss. Over time fees may render the value of a gift card zero. However, these issues have been addressed in recent years in some jurisdictions. In the United States, many jurisdictions limit or prohibit all fees or expiration dates for gift cards. Further, because of the negative impact on sales that such policies can have, most merchants have adopted and even advertise a \"no fee, no expiration\" policy for their gift cards, whether or not state laws require it. In 2011, an estimated 2.5% of gift cards were subject to an expiration date and 2.7% to post-sale fees.\n\nA quarter of gift card recipients still have not spent gift cards a year after receiving them, according to a Consumer Reports survey. And a majority of people say they end up spending more than the value of the card once they get to the store.\n\nIn the event of the bankruptcy of the issuing retailer, the outstanding value on gift cards is considered unsecured debt, and as such gift cards may become valueless. If the company intends to continue trading, gift cards may be honoured even in bankruptcy.\n\nAnother issue regarding gift cards are the growing concerns from retailers and other businesses about what can be done to prevent gift cards from being exploited by fraudsters. Gift card information can either be stolen from their rightful owners by fraudsters or they can be purchased with stolen credit card information. In recent years, cybercriminals have increased their efforts to take advantage of fraudulent gift cards as they are simple to exploit with automated brute-force bot attacks. The most common form of gift card fraud is committed through the act of stealing card information for activated cards with an existing balance by attacking a retailer’s systems which store gift card data. Once a gift card has been compromised, the fraudster will then check the balance through online customer portals before using the funds or reselling on the secondary gift card market.\n\nNot all gift cards are redeemed. The card may be lost; there may be time decay (expiration and fees) or complex rules of redemption; the recipient may not be interested in the store that accepts the card or be under the false assumption that not using it will save money for the giver. It has been estimated that perhaps 10% of cards are not redeemed, amounting to a gain for retailers of about $8 billion in the United States in 2006.\n\nIn 2012, over $100 billion in gift cards will be purchased in the United States, where over 20% of those gift cards will go unredeemed or unused. This has amassed a large opportunity in the secondary market, similar to the secondary ticket market in the early 2000s. Some companies have created a business in the secondary gift card market that allow consumers to sell their unused gift cards or buy discounted gift cards to their favorite brands. This has helped their users recoup their share of some $55 million per day that goes unredeemed in the United States every year, by turning their unused gift cards into cash.\n\nAll Canadian provinces have legislations passed to ban expiry dates and fees collected on gift cards. However, provincial gift card legislations do not apply to sectors that are regulated under federal laws. For example, gift cards that resemble credit cards (i.e. with Visa, MasterCard or American Express branding) and phone cards are regulated by the federal government. Under the federal \"Prepaid Payment Products Regulations\", effective May 1, 2014, federally regulated gift cards may only charge maintenance fees under certain conditions, and may not set an expiry date for funds on those cards.\n\nIn the past, uniform standards concerning gift cards did not exist. This was set to change as an addendum to the Credit CARD Act of 2009 directs the federal government to create consumer-friendly standards pertaining to gift cards. Most notably, the new regulations prohibit retailers from setting expiration dates unless they are at least 5 years after the card’s date of issue or the date on which funds were last added to the card. In addition, retailers are no longer able to assess dormancy, inactivity, or service fees unless the card has been inactive for at least 12 months, and if fees are added after that period, the details of such fees must be clearly disclosed on the card. Additionally, retailers are unable to levy more than one fee per month. The new provisions took effect on August 22, 2010.\n\nOpen loop cards are governed by rules of the Comptroller of the Currency, however oversight has been criticized. Closed loop gift cards are subject to rules set by different state regulations, and issuing authorities vary widely in the rules they set for the consumer. Rules can be changed by the issuer without notifying the consumer.\n\n"}
{"id": "7862242", "url": "https://en.wikipedia.org/wiki?curid=7862242", "title": "Gun violence", "text": "Gun violence\n\nGun-related violence is violence committed with the use of a gun (firearm or small arm). Gun-related violence may or may not be considered criminal. Criminal violence includes homicide (except when and where ruled justifiable), assault with a deadly weapon, and suicide, or attempted suicide, depending on jurisdiction. Non-criminal violence includes accidental or unintentional injury and death (except perhaps in cases of criminal negligence). Also generally included in gun violence statistics are military or para-military activities.\n\nAccording to GunPolicy.org, 75 percent of the world's 875 million guns are civilian controlled. Roughly half of these guns (48 percent) are in the United States, which has the highest rate of gun ownership in the world. Globally, millions are wounded and killed by the use of guns. Assault by firearm resulted in 180,000 deaths in 2013 up from 128,000 deaths in 1990. There were additionally 47,000 unintentional firearm-related deaths in 2013.\n\nLevels of gun-related violence vary greatly among geographical regions, countries, and even subnationally. Rates of violent deaths by firearm range from as low as 0.03 and 0.04 per 100,000 population in Singapore and Japan, to 59 and 67 per 100,000 in Honduras and Venezuela. The highest rates of violent deaths by firearm in the world occur in low-income South and Central American countries such as Honduras, Venezuela, Colombia, El Salvador, Guatemala and Jamaica. The United States has the 11th highest rate of gun violence in the world, and by far the largest of any large or highly developed nation, having a gun homicide rate which is 25 times higher, an unintentional gun death rate which is 6 times higher, a firearm suicide rate which is 8 times higher, and an overall firearm death rate which is 10 times higher than the average respective rates of other high income nations. Compared to similarly wealthy nations with strict gun control laws, such as Japan, the United Kingdom, or South Korea, the United States has an overall rate of firearms death per capita, which is 50–100 times greater than many of its peers. The high rates of gun violence in the United States, which has the highest rate of gun-related deaths per capita among developed countries, despite having the highest number of police officers, is sometimes thought to be attributable to its extreme rate of gun ownership, as it is the only nation in which guns exceed people. Nearly all studies have found a positive association between gun ownership and gun-related homicide and suicide rates.\n\nAccording to the United Nations, deaths from small firearms exceed that of all other weapons combined, and more die each year from gun-related violence than did in the atomic bombings of Hiroshima and Nagasaki combined. The global death toll from use of guns may number as high as 1,000 dead each day.\n\nA number of ideas have been proposed on how to lessen the incidence of gun-related violence.\n\nSome propose keeping a gun at home to keep one safer. Studies show that guns in the home is associated with an increased risk of violent death in the home. According to the FBI, gun-related violence is linked to gun ownership and is not a function or byproduct of crime. Their study indicates that more than 90% of gun-related deaths were not part of a commission of a crime, rather they were directly related to gun ownership. \"Mother Jones\" reports that \"[a] Philadelphia study found that the odds of an assault victim being shot were 4.5 times greater if he carried a gun\" and that \"[h]is odds of being killed were 4.2 times greater\" when armed.Others propose arming civilians to counter mass shootings. FBI research shows that between 2000 and 2013, \"In 5 incidents (3.1%), the shooting ended after armed individuals who were not law enforcement personnel exchanged gunfire with the shooters.\" Another proposal is to expand self defense laws for cases where a person is being aggressed upon, although \"those policies have been linked to a 7 to 10% increase in homicides\" (that is, shootings where self-defense cannot be claimed).\n\nThere is a strong relationship between guns in the home, as well as access to guns more generally, and suicide risk, the evidence for which is strongest in the United States. A 1992 case-control study conducted in Tennessee and Washington found that individuals in a firearm owning home are close to five times more likely to commit suicide than those individuals who do not own firearms. A 2002 study found that access to guns in the home was associated with an increased risk of suicide among middle-aged and older adults, even after controlling for psychiatric illness. As of 2008, there were 12 case-control studies that had been conducted in the U.S., all of which had found that guns in the home were associated with an increased risk of suicide. However, a 1996 New Zealand study found no significant relationship between household guns and suicide. Assessing data from 14 developed countries where gun ownership levels were known, the Harvard Injury Control Research Center found statistically significant correlations between those levels and suicide rates. However, the parallels were lost when data from additional nations was included. A 2006 study found a significant effect of changes in gun ownership rates on gun suicide rates in multiple Western countries. During the 1980s and 1990s, the rate of adolescent suicides with guns caught up with adult rates, and the 75-and-older rate rose above all others. The use of firearms in suicides ranges from less than 10 percent in Australia to 50 percent in the United States, where it is the most common method and where suicides outnumber homicides 2-to-1. Those whoe purchased a firearm where found to be high risk for suicide within a week of the purchase The United States has both the highest number of Suicides and Gun ownerships for a developed country and firearms are the most popular method to commit suicide. In the United States when Gun ownerships rise so too does suicide by firearm. Suicide can be an impulsive act, 40% of those who survived a suicide attempt said that they only considered suicide up to five minutes before attempting the act. This impulsivity can lead to the use of a firearm as it is seen as a quick and lethal method.\n\nAccording to U.S. criminologist Gary Kleck, studies that try to link gun ownership to victimology often fail to account for the presence of guns owned by other people. Research by economists John Lott of the U.S. and John Whitley of Australia indicates that safe-storage laws do not appear to affect juvenile accidental gun-related deaths or suicides. In contrast, a 2004 study led by Daniel Webster found that such laws were associated with slight reductions in suicide rates among children. The same study criticized Lott and Whitley's study on the subject for inappropriately using a Tobit model. A committee of the U.S. National Research Council said ecological studies on violence and firearms ownership provide contradictory evidence. The committee wrote: \"[Existing] research studies and data include a wealth of descriptive information on homicide, suicide, and firearms, but, because of the limitations of existing data and methods, do not credibly demonstrate a causal relationship between the ownership of firearms and the causes or prevention of criminal violence or suicide.\"\n\nThe United Nations Office on Drugs and Crime (UNODC) defines intentional homicide as \"acts in which the perpetrator intended to cause death or serious injury by his or her actions.\" This excludes deaths: related to conflicts (war); caused by recklessness or negligence; or justifiable, such as in self-defense or by law enforcement in the line of duty. A 2009 report by the Geneva Declaration using UNODC data showed that worldwide firearms were used in an average of 60 percent of all homicides. In the U.S. in 2011, 67 percent of homicide victims were killed by a firearm: 66 percent of single-victim homicides and 79 percent of multiple-victim homicides. In 2009, the United States' homicide rate was reported to be 5.0 per 100,000. A 2016 Harvard study claims that in 2010 the homicide rate was about 7 times higher than that of other high-income countries, and that the US gun homicide rate was 25.2 times higher. Another Harvard study found that higher gun availability was strongly correlated with higher homicide rates across 26 high-income countries. Access to guns is associated with an increased risk of being the victim of homicide. Access to firearms is not the sole contributor to increased homicide rates, however, as one study by the Southern Criminal Justice Association in 2011 found. Equally important seems to be the particular societal conditions in a given area, socio-culturally. These conditions include, but are not limited to societal age structure, economic inequality, cultural symbolism associated with firearms and the cultural value of individual life. \n\nSome gun control advocates say that the strongest evidence linking availability of guns to death and injury is found in domestic violence studies, often referring to those by public health policy analyst Arthur Kellermann. In response to suggestions by some that homeowners would be wise to acquire firearms for protection from home invasions, Kellermann investigated in-home homicides in three cities over five years. He found that the risk of a homicide was in fact slightly higher in homes where a handgun was present. The data showed that the risk of a crime of passion or other domestic dispute ending in a fatal injury was higher when a gun was readily available (essentially loaded and unlocked) compared to when no gun was readily available. Kellerman said this increase in mortality overshadowed any protection a gun might have deterring or defending against burglaries or invasions. He also concluded that further research of domestic violence causes and prevention are needed.\n\nCritics of Kellermann's study say that it is more directly a study of domestic violence than of gun ownership. Gary Kleck and others dispute the work. Kleck says that few of the homicides that Kellermann studied were committed with guns belonging to the victim or members of his or her household, and that it was implausible that victim household gun ownership contributed to their homicide. Instead, according to Kleck, the association that Kellermann found between gun ownership and victimization reflected that people who live in more dangerous circumstances are more likely to be murdered, but also were more likely to have acquired guns for self-protection.\n\nIn studies of nonfatal gun use, it was found that guns can contribute to coercive control, which can then escalate into chronic and more severe violence. Guns can have a negative impact on victims even without being discharged. Threats of gun use or showing a weapon can create damaging and long-lasting fear and emotional stress in victims because they are aware of the danger of having an abuser who has access to a gun.\n\nThe United Nations Office on Drugs and Crime defines robbery as the theft of property by force or threat of force. Assault is defined as a physical attack against the body of another person resulting in serious bodily injury. In the case of gun-related violence, the definitions become more specific and include only robbery and assault committed with the use of a firearm. Firearms are used in this threatening capacity four to six times more than firearms used as a means of protection in fighting crime. Hemenway's figures are disputed by other academics, who assert there are many more defensive uses of firearms than criminal uses. See John Lott's \"More Guns, Less Crime\".\n\nIn terms of occurrence, developed countries have similar rates of assaults and robberies with firearms, whereas the rates of homicides by firearms vary greatly by country.\n\nFrom 1979 to 1997, almost 30,000 people in the United States alone died from accidental firearm injuries. A disproportionately high number of these deaths occurred in parts of the United States where firearms are more prevalent.\n\nViolence committed with guns leads to significant public health, psychological, and economic costs.\n\nThe economic cost of gun-related violence in the United States is $229 billion a year, meaning a single murder has average direct costs of almost $450,000, from the police and ambulance at the scene, to the hospital, courts, and prison for the murderer. A 2014 study found that from 2006 to 2010, gun-related injuries in the United States cost $88 billion.\n\nAssault by firearm resulted in 180,000 deaths worldwide in 2013, up from 128,000 deaths worldwide in 1990. There were 47,000 unintentional firearm deaths worldwide in 2013.\n\nEmergency medical care is a major contributor to the monetary costs of such violence. It was determined in a study that for every firearm death in the United States for the year beginning 1 June 1992, an average of three firearm-related injuries were treated in hospital emergency departments.\n\nChildren exposed to gun-related violence, whether they are victims, perpetrators, or witnesses, can experience negative psychological effects over the short and long terms. Psychological trauma also is common among children who are exposed to high levels of violence in their communities or through the media. Psychologist James Garbarino, who studies children in the U.S. and internationally, found that individuals who experience violence are prone to mental and other health problems, such as post-traumatic stress disorder and sleep deprivation. These problems increase for those who experience violence as children.\n\nGun violence in the United States results in tens of thousands of deaths and injuries annually. In 2013, there were 73,505 nonfatal firearm injuries (23.2 injuries per 100,000 U.S. citizens), and 33,636 deaths due to \"injury by firearms\" (10.6 deaths per 100,000 U.S. citizens). These deaths consisted of 11,208 homicides, 21,175 suicides, 505 deaths due to accidental or negligent discharge of a firearm, and 281 deaths due to firearms use with \"undetermined intent\". Of the 2,596,993 total deaths in the US in 2013, 1.3% were related to firearms. The ownership and control of guns are among the most widely debated issues in the country.\n\nIn 2010, 67% of all homicides in the U.S. were committed using a firearm. In 2012, there were 8,855 total firearm-related homicides in the US, with 6,371 of those attributed to handguns. In 2012, 64% of all gun-related deaths in the U.S. were suicides. In 2010, there were 19,392 firearm-related suicides, and 11,078 firearm-related homicides in the U.S. In 2010, 358 murders were reported involving a rifle while 6,009 were reported involving a handgun; another 1,939 were reported with an unspecified type of firearm.\n\nFirearms were used to kill 13,286 people in the U.S. in 2015, excluding suicide. Approximately 1.4 million people have been killed using firearms in the U.S. between 1968 and 2011, equivalent to a top 10th largest U.S. city in 2016, falling between the populations of San Antonio and Dallas, Texas.\n\nCompared to 22 other high-income nations, the U.S. gun-related murder rate is 25 times higher. Although it has half the population of the other 22 nations combined, the U.S. had 82 percent of all gun deaths, 90 percent of all women killed with guns, 91 percent of children under 14 and 92 percent of young people between ages 15 and 24 killed with guns. In 2010, gun violence cost U.S. taxpayers approximately $516 million in direct hospital costs.\n\nGun violence is most common in poor urban areas and frequently associated with gang violence, often involving male juveniles or young adult males. Although mass shootings have been covered extensively in the media, mass shootings in the US account for a small fraction of gun-related deaths and the frequency of these events steadily declined between 1994 and 2007, rising between 2007 and 2013.\n\nLegislation at the federal, state, and local levels has attempted to address gun violence through a variety of methods, including restricting firearms purchases by youths and other \"at-risk\" populations, setting waiting periods for firearm purchases, establishing gun buyback programs, law enforcement and policing strategies, stiff sentencing of gun law violators, education programs for parents and children, and community-outreach programs. Despite widespread concern about the impacts of gun violence on public health, Congress has prohibited the Centers for Disease Control (CDC) from conducting research that advocates in favor of gun control. The CDC has interpreted this ban to extend to all research on gun violence prevention, and so has not funded any research on this subject since 1996.\n\nThe Port Arthur massacre of 1996 horrified the Australian public. The gunman opened fire on shop owners and tourists, killing 35 people and wounding 23. This massacre, kick started Australia’s laws against guns. The Prime Minister at that time, John Howard, proposed a gun law that prevented the public from having all semi-automatic rifles, all semi-automatic and pump-action shotguns, in addition to a tightly restrictive system of licensing and ownership controls.\n\nThe government also bought back guns from people. In 1996–2003 it was estimated they bought back and destroyed nearly 1 million firearms. By the end of 1996, whilst Australia was still reeling from the Port Arthur massacre, the gun law was fully in place. Since then, the number of deaths related to gun-related violence dwindled almost every year. In 1979 six hundred and eighty-five people died due to gun violence, and in 1996 it was five hundred and sixteen. The numbers continue to drop, however they were declining also before the gun law was in place.\n\nOn the Australia's most mediated gun violence-related incident since Port Arthur, was the 2014 Sydney Hostage Crisis. On 15–16 December 2014, a lone gunman, Man Haron Monis, held hostage 17 customers and employees of a Lindt chocolate café. The perpetrator was on bail at the time, and had previously been convicted of a range of offences.\n\nThe following year in August, the New South Wales Government tightened the laws of bail and illegal firearms, creating a new offence for the possession of a stolen firearm, with a maximum of 14 years imprisonment.\n\nSweden witnessed a steep increase in gun violence in males aged 15 to 29 in the two decades prior to 2018, in addition to a rising trend in gun violence there was also a high rate of gun violence in Sweden compared to other countries in Western Europe. According to a report published by academic researchers in 2017, shooting incidents with fatal outcomes are about 4 to 5 times as common in Sweden compared to neighbouring countries such as Germany and Norway when taking population size into account. The city with the highest prevalence of shootings was Malmö. The grave violence in the studied period also changed character, from criminal motorcycle gangs to city suburbs.\n\n\n\n"}
{"id": "37473881", "url": "https://en.wikipedia.org/wiki?curid=37473881", "title": "Hypocognition", "text": "Hypocognition\n\nHypocognition, in cognitive linguistics, means missing and being unable to communicate cognitive and linguistic representations because there are no words for particular concepts.\n\nThe word hypocognition (and its opposite, hypercognition) was coined by American psychiatrist and anthropologist Robert Levy in his 1973 book \"Tahitians: Mind and Experience in the Society Islands\". After 26 months of studying them, Levy described Tahitians as having no words to describe sorrow or guilt, resulting in people who had suffered personal losses describing themselves as feeling sick or strange instead of sad. Levy believed the Tahitians' lack of frames for thinking about and expressing grief contributed to their high suicide rate. He believed that a balance between hypercognition and hypocognition was culturally most desirable.\n\nHypocognition is a phrase commonly used in linguistics. In 2004 George Lakoff used it to describe political progressives in the United States, saying that relative to conservatives they suffer from \"massive hypocognition,\" which he described as the lack of having a progressive philosophy framed around the progressive core values of empathy and responsibility such as \"effective government\" versus \"less government\" or \"broader prosperity\" versus \"free markets.\"\n\nHypocognition has been blamed for preventing the practical application of evidence-based medicine in areas where frames (contextual and presentational influences on perceptions of reality) obscure facts. More generally, experts often overuse their own expertise: e.g. cardiologist diagnose a heart problem when the actual problem is something else.\n"}
{"id": "16781685", "url": "https://en.wikipedia.org/wiki?curid=16781685", "title": "Informal social control", "text": "Informal social control\n\nInformal social control, or the reactions of individuals and groups that bring about conformity to norms and laws, includes peer and community pressure, bystander intervention in a crime, and collective responses such as citizen patrol groups. The agents of the criminal justice system exercise more control when informal social control is weaker (Black, 1976). It is people who know each other informally controlling each other in subtle ways subconsciously.\n\n"}
{"id": "573827", "url": "https://en.wikipedia.org/wiki?curid=573827", "title": "Junior League", "text": "Junior League\n\nThe Association of Junior Leagues International, Inc. (AJLI) is a non-profit organization of 291 Junior Leagues in Canada, Mexico, United Kingdom and the United States. Junior Leagues are educational and charitable women's organizations aimed at improving their communities through volunteerism and building their members' civic leadership skills through training. According to its mission, \"The Association of Junior Leagues International Inc. (AJLI) is an organization of women committed to promoting voluntarism, developing the potential of women and improving communities through the effective action and leadership of trained volunteers. Its purpose is exclusively educational and charitable.\"\n\nThe first Junior League, the Junior League for the Promotion of the Settlement Movement (now the Junior League of the City of New York, Inc. also called the New York Junior League) was founded in 1901 in New York City by Mary Harriman Rumsey, daughter of railroad executive Edward H. Harriman. Inspired by a lecture on settlement movements that chronicled the works of social reformers such as Lillian Wald and Jane Addams, Mary organized others to become involved in settlement work. The organization's first project was working at the College Settlement on Rivington Street in New York City’s Lower East Side. The League was soon emulated, and by 1921, 30 Leagues joined to form the national association.\n\nIn 1921, after serving as New York City's Junior League president from 1907 to 1910 Dorothy Payne Whitney became the first president of the Association of Junior Leagues International Inc., operating as the umbrella organization for all Junior Leagues worldwide. There are currently 291 Leagues in Canada, the United States, Mexico and the UK. The first League outside the United States was in Montreal, Quebec, Canada.\n\nIn 1996, a man called Clark Clementsen tried to join but was denied membership.\n\nCalifornia \n\nMembers listed are not solely a representation of women who have dedicated their time to the Junior League, but rather are or have been associated with the organization. Several have achieved eminence on their own merit apart from this organization.\n\nJunior League membership is divided into levels: Provisional, Active, Sustainer, and Sustainer Emeritus. Provisional status is reserved for members in their first year of membership, which is dedicated to training. Provisional status is followed by Active membership, then Sustainer status, followed by an option of Sustainer Emeritus status for members who are 80 years or older. Requirements for Active and Sustainer status vary by League.\n\n"}
{"id": "456715", "url": "https://en.wikipedia.org/wiki?curid=456715", "title": "Kerr metric", "text": "Kerr metric\n\nThe Kerr metric or Kerr geometry describes the geometry of empty spacetime around a rotating uncharged axially-symmetric black hole with a quasispherical event horizon. The Kerr metric is an exact solution of the Einstein field equations of general relativity; these equations are highly non-linear, which makes exact solutions very difficult to find.\n\nThe Kerr metric is a generalization of the Schwarzschild metric, discovered by Karl Schwarzschild in 1915, which described the geometry of spacetime around an uncharged, spherically-symmetric, and non-rotating body. The corresponding solution for a \"charged\", spherical, non-rotating body, the Reissner–Nordström metric, was discovered soon afterwards (1916–1918). However, the exact solution for an uncharged, \"rotating\" black-hole, the Kerr metric, remained unsolved until 1963, when it was discovered by Roy Kerr. The natural extension to a charged, rotating black-hole, the Kerr–Newman metric, was discovered shortly thereafter in 1965. These four related solutions may be summarized by the following table:\n\nwhere \"Q\" represents the body's electric charge and \"J\" represents its spin angular momentum.\n\nAccording to the Kerr metric, such rotating black-holes should exhibit frame-dragging (also known as Lense–Thirring precession), a distinctive prediction of general relativity. Measurement of this frame dragging effect was a major goal of the Gravity Probe B experiment. Roughly speaking, this effect predicts that objects coming close to a rotating mass will be entrained to participate in its rotation, not because of any applied force or torque that can be felt, but rather because of the swirling curvature of spacetime itself associated with rotating bodies. At close enough distances, all objects – even light – \"must\" rotate with the black-hole; the region where this holds is called the ergosphere.\n\nRotating black holes have surfaces where the metric appears to have a singularity; the size and shape of these surfaces depends on the black hole's mass and angular momentum. The outer surface encloses the ergosphere and has a shape similar to a flattened sphere. The inner surface marks the \"radius of no return\" also called the \"event horizon\"; objects passing through this radius can never again communicate with the world outside that radius. However, neither surface is a true singularity, since their apparent singularity can be eliminated in a different coordinate system. Objects between these two horizons must co-rotate with the rotating body, as noted above; this feature can be used to extract energy from a rotating black hole, up to its invariant mass energy, \"Mc\".\n\nThe LIGO experiment which detected gravitational waves also provided the first direct observation of a pair of Kerr black holes.\n\nThe Kerr metric describes the geometry of spacetime in the vicinity of a mass \"M\" rotating with angular momentum \"J\". The line element in Boyer–Lindquist coordinates is\n\nwhere the coordinates formula_1 are standard spherical coordinate system, which are equivalent to the cartesian coordinates\n\nand \"r\" is the Schwarzschild radius \nand where the length-scales a, Σ and Δ have been introduced for brevity\n\nA key feature to note in the above metric is the cross product formula_2 term. There is coupling between time and motion in the plane of rotation that disappears when the black hole's angular momentum goes to zero.\n\nIn the non-relativistic limit where \"M\" (or, equivalently, \"r\") goes to zero, the Kerr metric becomes the orthogonal metric for the oblate spheroidal coordinates\n\nThe total mass equivalent \"M\" (the gravitating mass) of the body (including its rotational energy) and its irreducible mass \"M\" are related by\n\nIf the complete rotational energy \"E\"=\"c\"²(\"M\"-\"M\") of a black hole is extracted, for example with the Penrose process, the remaining mass cannot shrink below the irreducible mass. Therefore, if a black hole rotates with the spin \"a=M\", its total mass-equivalent \"M\" is higher by a factor of in comparison with a corresponding Schwarzschild black hole where \"M\" is equal to \"M\". The reason for this is that in order to get a static body to spin, energy needs to be applied to the system. Because of the mass–energy equivalence this energy also has a mass-equivalent, which adds to the total mass-energy of the system, \"M\".\n\nThe Kerr metric can be expressed in \"Kerr–Schild\" form, using a particular set of Cartesian coordinates as follows. These solutions were proposed by Kerr and Schild in 1965.\n\nNotice that k is a unit vector. Here \"M\" is the constant mass of the spinning object, \"η\" is the Minkowski tensor, and \"a\" is a constant rotational parameter of the spinning object. It is understood that the vector formula_4 is directed along the positive z-axis. The quantity \"r\" is not the radius, but rather is implicitly defined like this:\n\nNotice that the quantity \"r\" becomes the usual radius \"R\"\n\nwhen the rotational parameter \"a\" approaches zero. In this form of solution, units are selected so that the speed of light is unity (\"c\" = 1). At large distances from the source (R » a), these equations reduce to the Eddington–Finkelstein form of the Schwarzschild metric.\n\nIn the Kerr–Schild form of the Kerr metric, the determinant of the metric tensor is everywhere equal to negative one, even near the source.\n\nSince even a direct check on the Kerr metric involves cumbersome calculations, the contravariant components formula_6 of the metric tensor in Boyer–Lindquist coordinates are shown below in the expression for the square of the four-gradient operator: \n& \\frac{1}{c^{2}\\Delta}\\left(r^{2} + a^{2} + \\frac{r_{s}ra^{2}}{\\Sigma }\\sin^{2}\\theta\\right)\\left(\\frac{\\partial}{\\partial{t}}\\right)^{2} + \\frac{2r_{s}ra}{c\\Sigma \\Delta}\\frac{\\partial}{\\partial{\\phi}}\\frac{\\partial}{\\partial{t}} \\\\\n\nWe may rewrite the Kerr metric () in the following form:\n\nThis metric is equivalent to a co-rotating reference frame that is rotating with angular speed Ω that depends on both the radius \"r\" and the colatitude θ, where Ω is called the Killing horizon.\n\nThus, an inertial reference frame is entrained by the rotating central mass to participate in the latter's rotation; this is called frame-dragging, and has been tested experimentally.\nQualitatively, frame-dragging can be viewed as the gravitational analog of electromagnetic induction. An \"ice skater\", in orbit over the equator and rotationally at rest with respect to the stars, extends her arms. The arm extended toward the black hole will be torqued spinward. The arm extended away from the black hole will be torqued anti-spinward. She will therefore be rotationally sped up, in a counter-rotating sense to the black hole. This is the opposite of what happens in everyday experience. If she is already rotating at a certain speed when she extends her arms, inertial effects and frame-dragging effects will balance and her spin will not change. Due to the Principle of Equivalence gravitational effects are locally indistinguishable from inertial effects, so this rotation rate, at which when she extends her arms nothing happens, is her local reference for non-rotation. This frame is rotating with respect to the fixed stars and counter-rotating with respect to the black hole. A useful metaphor is a planetary gear system with the black hole being the sun gear, the ice skater being a planetary gear and the outside universe being the ring gear. This can be also be interpreted through Mach's principle.\n\nThe Kerr metric has two physical relevant surfaces on which it appears to be singular. The inner surface corresponds to an event horizon similar to that observed in the Schwarzschild metric; this occurs where the purely radial component \"g\" of the metric goes to infinity. Solving the quadratic equation 1/\"g\" = 0 yields the solution:\n\nAnother singularity occurs where the purely temporal component \"g\" of the metric changes sign from positive to negative. Again solving a quadratic equation \"g\"=0 yields the solution:\n\nDue to the cosθ term in the square root, this outer surface resembles a flattened sphere that touches the inner surface at the poles of the rotation axis, where the colatitude θ equals 0 or π; the space between these two surfaces is called the ergosphere. Within this volume, the purely temporal component \"g\" is negative, i.e., acts like a purely spatial metric component. Consequently, particles within this ergosphere must co-rotate with the inner mass, if they are to retain their time-like character. A moving particle experiences a positive proper time along its worldline, its path through spacetime. However, this is impossible within the ergosphere, where \"g\" is negative, unless the particle is co-rotating with the interior mass \"M\" with an angular speed at least of Ω. Thus, no particle can rotate opposite to the central mass within the ergosphere.\n\nAs with the event horizon in the Schwarzschild metric the apparent singularities at r and r are an illusion created by the choice of coordinates (i.e., they are coordinate singularities). In fact, the space-time can be smoothly continued through them by an appropriate choice of coordinates.\n\nA black hole in general is surrounded by a surface, called the event horizon and situated at the Schwarzschild radius for a nonrotating black hole, where the escape velocity is equal to the velocity of light. Within this surface, no observer/particle can maintain itself at a constant radius. It is forced to fall inwards, and so this is sometimes called the \"static limit\".\nA rotating black hole has the same static limit at its event horizon but there is an additional surface outside the event horizon named the \"ergosurface\" given by formula_11 in Boyer–Lindquist coordinates, which can be intuitively characterized as the sphere where \"the rotational velocity of the surrounding space\" is dragged along with the velocity of light. Within this sphere the dragging is greater than the speed of light, and any observer/particle is forced to co-rotate.\n\nThe region outside the event horizon but inside the surface where the rotational velocity is the speed of light, is called the \"ergosphere\" (from Greek \"ergon\" meaning \"work\"). Particles falling within the ergosphere are forced to rotate faster and thereby gain energy. Because they are still outside the event horizon, they may escape the black hole. The net process is that the rotating black hole emits energetic particles at the cost of its own total energy. The possibility of extracting spin energy from a rotating black hole was first proposed by the mathematician Roger Penrose in 1969 and is thus called the Penrose process. Rotating black holes in astrophysics are a potential source of large amounts of energy and are used to explain energetic phenomena, such as gamma-ray bursts.\n\nThe Kerr geometry exhibits many noteworthy features: the maximal analytic extension includes a sequence of asymptotically flat exterior regions, each associated with an ergosphere, stationary limit surfaces, event horizons, Cauchy horizons, closed timelike curves, and a ring-shaped curvature singularity. The geodesic equation can be solved exactly in closed form. In addition to two Killing vector fields (corresponding to \"time translation\" and \"axisymmetry\"), the Kerr geometry admits a remarkable Killing tensor. There is a pair of principal null congruences (one \"ingoing\" and one \"outgoing\"). The Weyl tensor is algebraically special, in fact it has Petrov type D. The global structure is known. Topologically, the homotopy type of the Kerr spacetime can be simply characterized as a line with circles attached at each integer point.\n\nNote that the inner Kerr geometry is unstable with regard to perturbations in the interior region. This instability means that although the Kerr metric is axis-symmetric, a black hole created through gravitational collapse may not be so. This instability also implies that many of the features of the Kerr geometry described above may not be present inside such a black hole.\n\nA surface on which light can orbit a black hole is called a photon sphere. The Kerr solution has infinitely many photon spheres, lying between an inner one and an outer one. In the nonrotating, Schwarzschild solution, with \"a\"=0, the inner and outer photon spheres degenerate, so that there is only one photon sphere at a single radius. The greater the spin of a black hole, the farther from each other the inner and outer photon spheres move. A beam of light traveling in a direction opposite to the spin of the black hole will circularly orbit the hole at the outer photon sphere. A beam of light traveling in the same direction as the black hole's spin will circularly orbit at the inner photon sphere. Orbiting geodesics with some angular momentum perpendicular to the axis of rotation of the black hole will orbit on photon spheres between these two extremes. Because the space-time is rotating, such orbits exhibit a precession, since there is a shift in the formula_12 variable after completing one period in the formula_13 variable.\n\nThe equations of motion for test particles in the Kerr spacetime are governed by four constants of motion. The first is the invariant mass formula_14 of the test particle, defined by the relation\nwhere formula_16 is the four-momentum of the particle. Furthermore, there are two constants of motion given by the time translation and rotation symmetries of Kerr spacetime, the energy formula_17, and the component of the orbital angular momentum parallel to spin of the black hole formula_18.\nUsing Hamilton–Jacobi theory, Brandon Carter showed that there exists a fourth constant of motion, formula_21, now referred to as the Carter constant. It is related to the total angular momentum of the particle and is given by\nSince there are four (independent) constants of motion for degrees of freedom the equations of motion for a test particle in Kerr spacetime are integrable.\n\nUsing these constants of motion, the trajectory equations for a test particle can be written (using natural units of G=M=c=1),\n\nwith\n\nWhere, formula_30 is an affine parameter such that formula_31. In particular, when formula_32 the affine parameter formula_30, is related to the proper time formula_34 through formula_35.\n\nBecause of the Frame-dragging-effect a zero angular momentum observer (ZAMO) is corotating with the angular velocity formula_36 which is defined with respect to the bookkeeper's coordinate time formula_37. The local velocity formula_38 of the test-particle is measured relative to a probe corotating with formula_36. The gravitational time-dilation between a ZAMO at fixed formula_40 and a stationary observer far away from the mass is\n\nThe group of isometries of the Kerr metric is the subgroup of the ten-dimensional Poincaré group which takes the two-dimensional locus of the singularity to itself. It retains the time translations (one dimension) and rotations around its axis of rotation (one dimension). Thus it has two dimensions. Like the Poincaré group, it has four connected components: the component of the identity; the component which reverses time and longitude; the component which reflects through the equatorial plane; and the component that does both.\n\nIn physics, symmetries are typically associated with conserved constants of motion, in accordance with Noether's theorem. As shown above, the geodesic equations have four conserved quantities: one of which comes from the definition of a geodesic, and two of which arise from the time translation and rotation symmetry of the Kerr geometry. The fourth conserved quantity does not arise from a symmetry in the standard sense and is commonly referred to as a hidden symmetry.\n\nThe location of the event horizon is determined by the larger root of formula_42. When formula_43 (i.e. formula_44), there are no (real valued) solutions to this equation, and there is no event horizon. With no event horizons to hide it from the rest of the universe, the black hole ceases to be a black hole and will instead be a naked singularity.\n\nAlthough the Kerr solution appears to be singular at the roots of Δ = 0, these are actually coordinate singularities, and, with an appropriate choice of new coordinates, the Kerr solution can be smoothly extended through the values of formula_40 corresponding to these roots. The larger of these roots determines the location of the event horizon, and the smaller determines the location of a Cauchy horizon. A (future-directed, time-like) curve can start in the exterior and pass through the event horizon. Once having passed through the event horizon, the formula_40 coordinate now behaves like a time coordinate, so it must decrease until the curve passes through the Cauchy horizon.\n\nThe region beyond the Cauchy horizon has several surprising features. The formula_40 coordinate again behaves like a spatial coordinate and can vary freely. The interior region has a reflection symmetry, so that a (future-directed time-like) curve may continue along a symmetric path, which continues through a second Cauchy horizon, through a second event horizon, and out into a new exterior region which is isometric to the original exterior region of the Kerr solution. The curve could then escape to infinity in the new region or enter the future event horizon of the new exterior region and repeat the process. This second exterior is sometimes thought of as another universe. On the other hand, in the Kerr solution, the singularity is a ring, and the curve may pass through the center of this ring. The region beyond permits closed time-like curves. Since the trajectory of observers and particles in general relativity are described by time-like curves, it is possible for observers in this region to return to their past. This interior solution is not likely to be physical and considered as a purely mathematical artefact.\n\nWhile it is expected that the exterior region of the Kerr solution is stable, and that all rotating black holes will eventually approach a Kerr metric, the interior region of the solution appears to be unstable, much like a pencil balanced on its point. This is related to the idea of cosmic censorship.\n\nThe Kerr geometry is a particular example of a stationary axially symmetric vacuum solution to the Einstein field equation. The family of all stationary axially symmetric vacuum solutions to the Einstein field equation are the Ernst vacuums.\n\nThe Kerr solution is also related to various non-vacuum solutions which model black holes. For example, the Kerr–Newman electrovacuum models a (rotating) black hole endowed with an electric charge, while the Kerr–Vaidya null dust models a (rotating) hole with infalling electromagnetic radiation.\n\nThe special case formula_48 of the Kerr metric yields the Schwarzschild metric, which models a \"nonrotating\" black hole which is static and spherically symmetric, in the Schwarzschild coordinates. (In this case, every Geroch moment but the mass vanishes.)\n\nThe \"interior\" of the Kerr geometry, or rather a portion of it, is locally isometric to the Chandrasekhar–Ferrari CPW vacuum, an example of a colliding plane wave model. This is particularly interesting, because the global structure of this CPW solution is quite different from that of the Kerr geometry, and in principle, an experimenter could hope to study the geometry of (the outer portion of) the Kerr interior by arranging the collision of two suitable gravitational plane waves.\n\nEach asymptotically flat Ernst vacuum can be characterized by giving the infinite sequence of relativistic multipole moments, the first two of which can be interpreted as the mass and angular momentum of the source of the field. There are alternative formulations of relativistic multipole moments due to Hansen, Thorne, and Geroch, which turn out to agree with each other. The relativistic multipole moments of the Kerr geometry were computed by Hansen; they turn out to be\nThus, the special case of the Schwarzschild vacuum (\"a\"=0) gives the \"monopole point source\" of general relativity.\n\n\"Warning:\" do not confuse these relativistic multipole moments with the \"Weyl multipole moments\", which arise from treating a certain metric function (formally corresponding to Newtonian gravitational potential) which appears the Weyl-Papapetrou chart for the Ernst family of all stationary axisymmetric vacuum solutions using the standard euclidean scalar multipole moments. In a sense, the Weyl moments only (indirectly) characterize the \"mass distribution\" of an isolated source, and they turn out to depend only on the \"even order\" relativistic moments. In the case of solutions symmetric across the equatorial plane the \"odd order\" Weyl moments vanish. For the Kerr vacuum solutions, the first few Weyl moments are given by\nIn particular, we see that the Schwarzschild vacuum has nonzero second order Weyl moment, corresponding to the fact that the \"Weyl monopole\" is the Chazy–Curzon vacuum solution, not the Schwarzschild vacuum solution, which arises from the Newtonian potential of a certain finite length uniform density thin \"rod\".\n\nIn weak field general relativity, it is convenient to treat isolated sources using another type of multipole, which generalize the Weyl moments to \"mass multipole moments\" and \"momentum multipole moments\", characterizing respectively the distribution of mass and of momentum of the source. These are multi-indexed quantities whose suitably symmetrized (anti-symmetrized) parts can be related to the real and imaginary parts of the relativistic moments for the full nonlinear theory in a rather complicated manner.\n\nPerez and Moreschi have given an alternative notion of \"monopole solutions\" by expanding the standard NP tetrad of the Ernst vacuums in powers of \"r\" (the radial coordinate in the Weyl-Papapetrou chart). According to this formulation:\nIn this sense, the Kerr vacuums are the simplest stationary axisymmetric asymptotically flat vacuum solutions in general relativity.\n\nThe Kerr geometry is often used as a model of a rotating black hole. But if we hold the solution to be valid only outside some compact region (subject to certain restrictions), in principle we should be able to use it as an exterior solution to model the gravitational field around a rotating massive object other than a black hole, such as a neutron star, or the Earth. This works out very nicely for the non-rotating case, where we can match the Schwarzschild vacuum exterior to a Schwarzschild fluid interior, and indeed to more general static spherically symmetric perfect fluid solutions. However, the problem of finding a rotating perfect-fluid interior which can be matched to a Kerr exterior, or indeed to any asymptotically flat vacuum exterior solution, has proven very difficult. In particular, the Wahlquist fluid, which was once thought to be a candidate for matching to a Kerr exterior, is now known not to admit any such matching. At present it seems that only approximate solutions modeling slowly rotating fluid balls are known. (Slowly rotating fluid balls are the relativistic analog of oblate spheroidal balls with nonzero mass and angular momentum but vanishing higher multipole moments.) However, the exterior of the Neugebauer–Meinel disk, an exact dust solution which models a rotating thin disk, approaches in a limiting case the formula_51 Kerr geometry. Physical thin-disk solutions obtained by identifying parts of the Kerr space-time are also known.\n\n\n"}
{"id": "6257886", "url": "https://en.wikipedia.org/wiki?curid=6257886", "title": "Landfill tax", "text": "Landfill tax\n\nA landfill tax or levy is a form of tax that is applied in some countries to increase the cost of landfill. The tax is typically levied in units of currency per unit of weight or volume (£/t, €/t, $/yard³). The tax is in addition to the overall cost of landfill and forms a proportion of the gate fee.\n\nA tax or fee may be imposed on landfills or other disposal facilities as a means of raising general revenues, to generate funds for inspection programs or long-term mitigation of environmental impacts related to disposal, or as a means of inhibiting disposal by raising the cost in comparison to preferable alternatives, in the same manner as an excise or \"sin tax\".\n\nLandfilling is discouraged due to a number of key reasons:\n\n\nIn New Zealand the Waste Minimisation Act 2008 allowed for the charging of a landfill levy. Half of the landfill levy goes to local authorities with the other half going into a Waste Minimisation Fund. As of 2010 it is set at $10 a tonne but it can be reviewed by the Minister for the Environment.\n\nLandfill Tax was introduced in 1996 by Conservative Secretary of State for the Environment, John Gummer, and was the UK's first environmental tax. The tax is seen as a key mechanism in enabling the UK to meet its targets set out in the Landfill Directive for the landfilling of biodegradable waste. Through increasing the cost of landfill, other advanced waste treatment technologies with higher gate fees are made to become more financially attractive.\n\nThe amount of tax levied is calculated according to the weight of the material disposed of and whether it is active or inactive waste.\n\n\nInevitably there will be some mixing of waste such as inert bricks and concrete mixed with small amounts of wood or mineral dust packaged in polythene bags. It is for the producer of the waste to decide what is reasonable and acceptable in terms of active waste contamination of inert waste.\n\nThe landfill site operator is responsible for paying the tax and will pass the cost on to businesses and local councils on top of normal landfill fees. VAT is charged on the landfill fees and the landfill tax.\n\nOperators can reduce their tax liability by making payments to the Landfill Communities Fund under the Landfill Tax Credit Scheme. The fund aids community or environmental projects in the vicinity of a landfill site and is regulated by ENTRUST, a company limited by guarantee.\n\nLandfill tax rates 2013–2014\n\n\nSections from the coalition government's Emergency Budget 2010:\n\nLandfill tax rates 2015–16\n\nFrom 1 April 2015 to April 2016 the landfill tax rates were:\n\nWith the Scotland Act 2012, the Scottish Government gained the devolved power to levy its own landfill tax. The Scottish Landfill Tax was introduced by the Landfill Tax (Scotland) Act 2014 and began to be payable on 1 April 2015.\n\nIn 2015 and 2016, there were concerns that some companies disposing waste to landfill were wrongly putting waste with a biodegradable and non-inert content into landfills at the inactive waste taxation level. A new testing regime was introduced, where a small amount of the load is tested to ascertain its biodegradable content.\n\nLandfill tax rates 2018-19\n\nFrom 1 April 2018 to April 2019 the landfill tax rates are:\n\nThe Welsh Government will take responsibility for the tax in Wales from April 2018.\n\nThe United States has numerous federal laws and regulations regarding the operation of landfills, but there is no national landfill tax or fee. Many states and local governments collect fees and taxes on the collection or disposal of solid waste.\n\nLandfills in California are subject to fees and taxes levied by cities and counties, as well as by the state. The Integrated Waste Management Act of 1989 authorized a state fee (set at $1.40 per ton effective 2002-07-01) to fund the activities of the California Integrated Waste Management Board (CIWMB). Many cities and counties collect fees from landfills within their jurisdiction to recover the costs of local solid waste planning and inspection programs, to operate programs for the collection and disposal of household hazardous wastes, and to fund some costs of recycling and reuse programs.\n\nLandfills in San Jose are subject to the highest disposal tax in California, with the Disposal Facility Tax set at $13.00 per ton in 1992.\n\n\n"}
{"id": "6814223", "url": "https://en.wikipedia.org/wiki?curid=6814223", "title": "Lynn Hershman Leeson", "text": "Lynn Hershman Leeson\n\nLynn Hershman Leeson (born 1941) is an American artist and filmmaker. Her work combines art with social commentary, particularly on the relationship between people and technology. Leeson's work in media-based technology helped legitimize digital art forms.\n\nLeeson's work has as its themes: identity in a time of consumerism, privacy in an era of surveillance, interfacing of humans and machines, and the relationship between real and virtual worlds. Her work grew out of an installation art and performance tradition, with an emphasis on interactivity. With a practice spanning more than 40 years, Leeson has worked in performance, moving image, drawing, collage, text-based work, site-specific interventions, and later new media / digital technologies, and interactive net-based works.\n\nHer projects explore technology in digital media and science. Leeson was the first artist to launch an interactive piece using Videodisc, a precursor to DVD (Lorna, 1983–84), as well the first artist to incorporate a touch screen interface into her artwork (Deep Contact, 1984-1989). Her networked robotic art installation (The Difference Engine #3, 1995-1998) is an example of her tendency to expand her artwork beyond the traditional realms of art.\n\nWork by Lynn Hershman Leeson is featured in the public collections of the Museum of Modern Art, the William Lehmbruck Museum, the ZKM (Zentrum fur Kunst und Medientechnologie), the Los Angeles County Museum of Art, the National Gallery of Canada, di Rosa, the Walker Art Center and the University Art Museum, Berkeley, in addition to the private collections of Donald Hess and Arturo Schwarz, among many others. Commissions include projects for the Tate Modern, San Francisco Museum of Modern Art, de Young Museum, Daniel Langlois and Stanford University, and Charles Schwab.\n\nFrom 1974 until 1978, Leeson ‘developed’ a fictional persona and alter ego of \"Roberta Breitmore.\" It consisted not only of a physical self-transformation through make-up, clothing, and wigs, but a fully-fledged personality existing over an extended period of time and whose existence could be proven in the world through physical evidence: from a driver’s license and credit card to letters from her psychiatrist.\n\nThis was later taken to further lengths when Leeson introduced another three ‘Robertas’, by hiring other performers to enact her character. These ‘clones’ of Roberta adopted the same look and attire, engaged in some of Roberta’s correspondence and also went on some of Roberta (Leeson’s) dates. Towards the end, the ‘original’ Roberta, withdrew from her character leaving the three ‘clones’ to continue her work, until they were retired in a performance at the Palazzo dei Diamanti in Ferrara, Italy in 1978, during an exorcism at the grave of Lucrezia Borgia. What remains are the physical artefacts of any life: documentation and personal effects such as legal and medical documents and a diary).\n\nBetween 1995–2000 Roberta transformed into the CybeRoberta, an interactive artificial intelligent sculpture on the web. In 2006 Roberta Breitmore developed into a character in Second Life. After Stanford University acquired her archive, Leeson worked with Henry Lowood (Stanford Humanities Lab) to convert parts of the archive into something for a broader public. They worked to recreate and re-enact both Roberta Breitmore and The Dante Hotel in a virtual space.\n\nLORNA was an early project of Leeson's. The first interactive laser artdisk, LORNA tells the story of an Agoraphobic woman. Viewers have the option of directing her life into several possible plots and endings. LORNA never left her one-room apartment. As LORNA watched the news and ads, she became fearful, afraid to leave her tiny room. Viewers were invited to liberate LORNA from her fears, using remote control units.\n\nThe plot has multiple variations that can be seen backwards, forwards, at increased or decreased speeds, and from several points of view. There is no hierarchy in the ordering of decisions. And the icons were often made of cut-off and dislocated body parts such as a mouth, or an eye.\n\nIn 2001, Leeson created the \"Agent Ruby\" for the SFMOMA . Since that time Agent Ruby has conversed with online users, which has shaped her memory, knowledge, and moods. In 2013 the SFMOMA presented \"Lynn Hershman Leeson: The Agent Ruby Files\". This digital and analog presentation reinterprets dialogues drawn from the decade-long archive of text files of Agent Ruby’s conversations with online users and reflects on technologies, recurrent themes, and patterns of audience engagement.\n\nA 1990 documentary, \"Desire Inc.\" features a series of seductive television ads in which a sexy woman asked for viewers to call her.\n\nLeeson's three feature films - \"Strange Culture\", \"Teknolust\", \"Conceiving Ada\" - have been part of the Sundance Film Festival, the Toronto International Film Festival and The Berlin International Film Festival, among others, and have won numerous awards.\n\nIn 2011 Hershman released the ground-breaking !Women Art Revolution, a feature-length documentary about the Feminist art movement in the United States. According to Leeson:\nThe films are all about loss and technology. Ada Lovelace invented computer language, but was never credited and was basically erased from history. Teknolust is about artificial intelligence clones: the bots that escape into reality and interact with human life, in effect a symbiosis between technological life and human life, and how the two can marry. Strange Culture again was about misidentity, where the media created a fictional character that they blame this crime on, rather than the actual person. All of these works are about erasure of identity and how technology adds to it and creates it. And how you can defeat that.\n\nAs part of her 2014 exhibition \"How To Disappear,\" she premiered her video \"The Ballad of JT LeRoy\", examining Laura Albert's use of the literary persona JT LeRoy. Reflecting on the parallels between JT LeRoy and Roberta Breitmore, Hershman Leeson has commented:\nThe concept of an alter ego is not new at all. Writers have been protecting themselves in that way for centuries. Mary Shelley did it. Of course Laura took this practice further and I think that was very smart and I do not think she deserves the kind of condemnation that she got. If I had done the Roberta thing ten years later, I would have faced the same problems.\n\nIn 2007 a retrospective at the Whitworth Art Gallery in Manchester, \"Autonomous Agents\", featured a comprehensive range of the artist's work - from the \"Roberta Breitmore\" series (1974–78) to videos from the 1980s and interactive installations that use the Internet and artificial intelligence software. Her influential early ventures into performance and photography are also featured in the current touring exhibition \"WACK! Art and the Feminist Revolution\", organized by the Los Angeles Museum of Contemporary Art. \"The Art and Films of Lynn Hershman Leeson: Secret Agents, Private I\", was published by The University of California Press in 2005 on the occasion of another retrospective at the Henry Gallery in Seattle.\n\nIn 2014 a retrospective at The ZKM Museum of Contemporary Art in Karlsruhe, Germany, \"Civic Radar\", has realized the first retrospective which not only ensures an overview of all creative phases in Lynn Hershman Leeson’s oeuvre but also the most recent productions of this innovative artist.\n\nLynn Hershman Leeson has been honored with grants from Creative Capital, The National Endowment for the Arts, Nathan Cummings Foundation, Siemens International Media Arts Award, Prix Ars Electronica, and Alfred P Sloan Foundation Prize for Writing and Directing. In 2009 she was the recipient of a John Simon Guggenheim Memorial Foundation Fellowship. Also in 2009, she received the SIGGRAPH Distinguished Artist Award. The Digital Art Museum in Berlin recognized her work with the d.develop digital award (ddaa) for Lifetime Achievement in the field of New Media in 2010. Her work was recently included in Arthur and Marilouise Kroker's Top Ten for the January 2013 issue of Artforum.\n\nIn 2014, IFP Pixel Market Prize went to \"The Infinity Engine\" starring Tilda Swinton, directed by Lynn Hershman Leeson in collaboration with producer Lisa Cortes, whose credits include the Academy Award and Sundance Film Festival winning film \"Precious\". \"The Infinity Engine\" is an installation, film and online interactive website. The prize comprises a six-month fellowship at the Media Center and an invitation to participate in next year’s No Borders programme. \nLeeson was also featured in the Women's eNews \"21 Leaders for the 21st Century\" special in 2014 for her role in empowering young female artists to strengthen their artistic voices. Her documentary !W.A.R. raises awareness for the fact that the art world is a male-dominated realm and explores the many influential works of female artists over the decades.\n\nHershman Leeson served as Chair of the Film Department at the San Francisco Art Institute, as Professor Emeritus at the University of California, Davis, and as an A. D. White Professor at Large at Cornell University. She is the 2013-2014 Dorothy H. Hirshon \"Director in Residence\" at The New School.\n\nIn 2004, Stanford University Libraries acquired Hershman Leeson's working archive. Stanford also acquired a collection of the interviews compiled for Hershman Leeson's 2010 documentary !Women Art Revolution.\n\nIn 2018, The Women's Caucus for Art awarded Lynn Hershman Leeson with the Lifetime Achievement Award, in Los Angeles CA\n\n"}
{"id": "52763276", "url": "https://en.wikipedia.org/wiki?curid=52763276", "title": "Maggot farming", "text": "Maggot farming\n\nMaggot farming is the act of growing maggots for the industry. It is distinct from vermicomposting as there is no separate composting process going on, and maggots are used which are distinct from earthworms (they also consume only flesh, not plant-based material).\n\nA variety of species can be used, including the black soldier fly, as well as various other fly species. Due to convenience, fly species are often used which are indigenous to the area of cultivation.\n\nWhen using indigenous fly species, one tactic (employed by the Songhai Center in Benin) is to simply dump offal or meat that has exceeded the sell-by date in concrete bins. The bins are then covered with chicken wire to prevent any large animals of being able to feed off it. Then, flies deposit eggs on the offal and meat, and maggots hatch and consume it. After that, the bins are filled with water, so the maggots start to float (separating themselves from any leftovers). The maggots are then harvested and the leftover is discarded or further processed (e.g., bones can be ground to bone meal, ...).\n\nThe maggots are often sold and used as animal feed. Especially fish, chickens, pig, ducks appreciate them. It can also be sold as fishing bait.\n\n\n"}
{"id": "1087195", "url": "https://en.wikipedia.org/wiki?curid=1087195", "title": "Might makes right", "text": "Might makes right\n\nMight makes right is an aphorism with several potential meanings (in order of increasing complexity):\nThe idea of \"woe to the conquered\" can be found in Homer and the hawk parable in Hesiod's \"Works and Days\" and in Livy, in which \"vae victis\", Latin for \"woe to the conquered\", is first recorded.\n\nThe first commonly quoted use of \"might makes right\" in the English language was in 1846 by the American pacifist and abolitionist Adin Ballou (1803–1890), who wrote, \"But now, instead of discussion and argument, brute force rises up to the rescue of discomfited error, and crushes truth and right into the dust. 'Might makes right,' and hoary folly totters on in her mad career escorted by armies and navies.\" (Christian Non-Resistance: In All Its Important Bearings, Illustrated and Defended, 1846.)\n\nThe phrase in reverse is echoed in Abraham Lincoln's words in his February 26, 1860, Cooper Union Address (\"Let us have faith that right makes might, and in that faith, let us, to the end, dare to do our duty as we understand it\") in his attempt to defend a policy of neutral engagement with those who practised slavery, perhaps to appear more nationally oriented and religiously convicted in hopes of winning the presidential election later that year (which he did).\n\nThe idea, though not the wording, has been attributed to the \"History of the Peloponnesian War\" by the ancient Greek historian Thucydides, who stated that \"right, as the world goes, is only in question between equals in power, while the strong do what they can and the weak suffer what they must.\" Montague coined the term \"Kratocracy\", from the Greek κρατερός \"krateros\", meaning \"strong\", for government by those who are strong enough to seize power through force or cunning.\n\nIn a letter to Albert Einstein from 1932, Sigmund Freud clearly explores this idea of \"might versus right\" as well. He discusses the relationship between the two and how this concept has in fact existed throughout time.\n\nIn the first chapter of Plato's \"The Republic\", Thrasymachus claims that \"justice is nothing else than the interest of the stronger\", which Socrates then disputes.\n\n\"Might makes right\" has been described as the credo of totalitarian regimes. Realist scholars of international politics think of it as a game in a kind of \"state of nature\" in which might makes right.\n\nThe author T.H. White covered this topic extensively in the Arthurian novel \"The Once and Future King\". Merlin teaches young Arthur to challenge this concept; Arthur, after assuming the throne, attempts to reduce violence through various means and with varying degrees of success.\n\n"}
{"id": "30091567", "url": "https://en.wikipedia.org/wiki?curid=30091567", "title": "Online Consciousness Conference", "text": "Online Consciousness Conference\n\nThe Online Consciousness Conference held at Consciousness Online was founded in 2008 by Richard Brown, then a graduate student at the CUNY Graduate Center. The conference was inspired by and partially modeled on the Online Philosophy Conference started by Thomas Nadellhoffer and Eddie Nahmias, with one major difference being the heavy use of homemade video and audio presentations, and another difference being the restricted focus of the conference on consciousness studies as opposed to all of philosophy.\n\nThe first conference was held February 20-March 4, 2009 with the theme \"Philosophers Facing Phenomenal Consciousness\" and a keynote given by David M. Rosenthal. Papers from this conference were subsequently published in the Journal of Consciousness Studies. The second conference was held February 19-March 5, 2010 with the theme \"Lessons and Limits from the Empirical Study of Consciousness\" with keynote talks by Hakwan Lau (psychology Columbia) with commentary by Ned Block, David Rosenthal and David Chalmers, and Collin Clifford (psychology University of Sydney). Papers from it are due to be published in Consciousness and Cognition. The third conference was held February 18-March 4, 2011 with the theme \"Neurophilosophy and the Philosophy of Neuroscience\" and featured invited talks from Paul Churchland, Kathleen Akins, Stevan Harnad and Jesse Prinz. \n\nPast presentations and the discussion at the conference is preserved online and can be viewed at Consciousness Online.\n"}
{"id": "5608629", "url": "https://en.wikipedia.org/wiki?curid=5608629", "title": "Open book decomposition", "text": "Open book decomposition\n\nIn mathematics, an open book decomposition (or simply an open book) is a decomposition of a closed oriented 3-manifold \"M\" into a union of surfaces (necessarily with boundary) and solid tori. Open books have relevance to contact geometry, with a famous theorem of Emmanuel Giroux (given below) that shows that contact geometry can be studied from an entirely topological viewpoint.\n\nDefinition. An \"open book decomposition\" of a 3-dimensional manifold \"M\" is a pair (\"B\", π) where\nThis is the special case \"m\" = 3 of an open book decomposition of an \"m\"-dimensional manifold, for any \"m\".\n\nWhen Σ is an oriented compact surface with \"n\" boundary components and φ: Σ → Σ is a homeomorphism which is the identity near the boundary, we can construct an open book by first forming the mapping torus Σ. Since φ is the identity on ∂Σ, ∂Σ is the trivial circle bundle over a union of circles, that is, a union of tori; one torus for each boundary component. To complete the construction, solid tori are glued to fill in the boundary tori so that each circle \"S\" × {\"p\"} ⊂ \"S\"×∂\"D\" is identified with the boundary of a page. In this case, the binding is the collection of \"n\" cores \"S\"×{q} of the \"n\" solid tori glued into the mapping torus, for arbitrarily chosen \"q\" ∈ \"D\". It is known that any open book can be constructed this way. As the only information used in the construction is the surface and the homeomorphism, an alternate definition of open book is simply the pair (Σ, φ) with the construction understood. In short, an open book is a mapping torus with solid tori glued in so that the core circle of each torus runs parallel to the boundary of the fiber.\n\nEach torus in ∂Σ is fibered by circles parallel to the binding, each circle a boundary component of a page. One envisions a rolodex-looking structure for a neighborhood of the binding (that is, the solid torus glued to ∂Σ)—the pages of the rolodex connect to pages of the open book and the center of the rolodex is the binding. Thus the term \"open book\".\n\nIt is a 1972 theorem of Elmar Winkelnkemper that for \"m\" > 6, a simply-connected \"m\"-dimensional manifold has an open book decomposition if and only if it has signature 0. In 1977 Terry Lawson proved that for odd \"m\" > 6, every \"m\"-dimensional manifold has an open book decomposition. For even \"m\" > 6, an \"m\"-dimensional manifold has an open book decomposition if and only if an asymmetric Witt group obstruction is 0, by a 1979 theorem of Frank Quinn.\n\nIn 2002, Emmanuel Giroux published the following result:\n\nTheorem. Let \"M\" be a compact oriented 3-manifold. Then there is a bijection between the set of oriented contact structures on \"M\" up to isotopy and the set of open book decompositions of \"M\" up to positive stabilization.\n\n\"Positive stabilization\" consists of modifying the page by adding a 2-dimensional 1-handle and modifying the monodromy by adding a positive Dehn twist along a curve that runs over that handle exactly once. Implicit in this theorem is that the new open book defines the same contact 3-manifold. Giroux's result has led to some breakthroughs in what is becoming more commonly called contact topology, such as the classification of contact structures on certain classes of 3-manifolds. Roughly speaking, a contact structure corresponds to an open book if, away from the binding, the contact distribution is isotopic to the tangent spaces of the pages through confoliations. One imagines smoothing the contact planes (preserving the contact condition almost everywhere) to lie tangent to the pages.\n\n"}
{"id": "36794221", "url": "https://en.wikipedia.org/wiki?curid=36794221", "title": "Pregnancy from rape", "text": "Pregnancy from rape\n\nPregnancy is a potential result of rape. It has been studied in the context of war, particularly as a tool for genocide, as well as other unrelated contexts, such as rape by a stranger, statutory rape, incest, and underage pregnancy. The current scientific consensus is that rape is at least as likely to lead to pregnancy as consensual sexual intercourse, with some studies suggesting rape may actually result in higher rates of pregnancy than consensual intercourse.\n\nRape can cause difficulties during and after pregnancy, with potential negative consequences for both the victim and a resulting child. Medical treatment following a rape includes testing for, preventing, and managing pregnancy. A woman who becomes pregnant after a rape may face a decision about whether to raise the child, give the child up for adoption or parenting by others, or to have an abortion. In some countries where abortion is illegal after rape and incest, over 90% of pregnancies in girls age 15 and under are due to rape by family members.\n\nThe false belief that pregnancy can almost never result from rape was widespread for centuries. In Europe, from medieval times well into the 18th century a man could use a woman's pregnancy as a legal defense to \"prove\" that he could not have raped her, since her pregnancy was thought to mean that she had enjoyed the sex and, therefore, consented to it. In recent decades, some pro-life organizations and politicians (such as Todd Akin) who oppose legal abortion in cases of rape have advanced claims that pregnancy very rarely arises from rape, and that the practical relevance of such exceptions to abortion law is, therefore, limited or non-existent.\n\nAny female capable of ovulation may become pregnant after rape by a virile male.\n\nEstimates of the numbers of pregnancies from rape vary widely. Recent estimates suggest that rape conception happens between 25,000 and 32,000 times each year in the U.S.\n\nIn a 1996 three-year longitudinal study of 4,000 American women, physician Melisa Holmes estimated from data from her study that forced sexual intercourse causes over 32,000 pregnancies in the United States each year. Physician Felicia H. Stewart and economist James Trussell estimated that the 333,000 assaults and rapes reported in the US in 1998 caused about 25,000 pregnancies, and up to 22,000 of those pregnancies could have been prevented by prompt medical treatment, such as emergency contraception.\n\nA 1996 study of 44 cases of rape-related pregnancy estimated that in the United States, the pregnancy rate is 5.0% per rape among victims of reproductive age (aged 12 to 45). A 1987 study also found a 5% pregnancy rate from rape among 18- to 24-year-old college students in the US. A 2005 study placed the rape-related pregnancy rate at around 3–5%.\nA study of Ethiopian adolescents who reported being raped found that 17% subsequently became pregnant, and rape crisis centres in Mexico reported the figure the rate of pregnancy from rape at 15–18%. Estimates of rape-related pregnancy rates may be inaccurate since the crime is under-reported, resulting in some pregnancies from rape not being recorded as such, or alternately, social pressure may mean some rapes are not reported if no pregnancy results.\n\nAlthough most studies suggest that conception rates are independent of whether insemination is due to rape or consensual sex, some analysts have suggested that the rate of conception may be higher, or lower, from insemination due to rape.\nPsychologist Robert L. Smith states that some studies have reported \"unusually high rates of conception following rape\". He cites a paper by C.A. Fox and Beatrice Fox, reporting that biologist Alan Sterling Parkes had speculated in personal correspondence that \"there is a high conception rate in rape, where hormonal release, due to fear or anger, could produce reflex ovulation\". Smith also cites veterinary scientist Wolfgang Jöchle, who \"proposed that rape may induce ovulation in human females\". Literary scholar Jonathan Gottschall and economist Tiffani Gottschall argued in a 2003 \"Human Nature\" article that previous studies of rape-pregnancy statistics were not directly comparable to pregnancy rates from consensual intercourse, because the comparisons were largely uncorrected for such factors as the use of contraception. Adjusting for these factors, they estimated that rapes are about twice as likely to result in pregnancies (7.98%) as \"consensual, unprotected penile-vaginal intercourse\" (2–4%). They discuss a variety of possible explanations and advance the hypothesis that rapists tend to target victims with biological \"cues of high fecundity\" or subtle indications of ovulation.\nIn contrast, psychologists Tara Chavanne and Gordon Gallup Jr., citing unpublished dissertations by Rogel and Morgan, argued that female adaptations reduce the likelihood of rape during fertile periods. A 1995 study of women who became pregnant after rape found that 60% had been impregnated during consensual intercourse. Anthropologist Daniel Fessler disputed these findings, saying, \"analysis of conception rates reveals that the probability of conception following rape does not differ from that following consensual coitus\".\n\nSociobiologists and evolutionary psychologists have hypothesized that causing pregnancy by rape may be a mating strategy in humans, as a way for males to ensure the survival of their genes by passing them on to future generations. Randy Thornhill and Craig T. Palmer are key popularizers of this hypothesis. They assert that most rape victims are women of childbearing age and that many cultures treat rape as a crime against the victim's husband. They state that rape victims suffer less emotional distress when they are subjected to more violence, and that married women and women of childbearing age experience greater psychological distress after a rape than do girls, single women or post-menopausal women. Rape-pregnancy rates are crucial in evaluating these theories, because a high or low pregnancy rate from rape would determine whether such adaptations are favored or disfavored by natural selection.\n\nIn 1995–96, the journal \"Family Planning Perspectives\" published a study by the Guttmacher Institute, a sexual health research and policy organization, on statutory rape (sexual intercourse with a minor) and resulting pregnancies. It drew on other research to conclude that \"at least half of all babies born to minor women are fathered by adult men\", and that \"although relatively small proportions of 13–14-year-olds have had intercourse, those who become sexually active at an early age are especially likely to have experienced coercive sex: Seventy-four percent of women who had intercourse before age 14 and 60% of those who had sex before age 15 report having had a forced sexual experience\". Because of difficulties in bringing such cases to trial, however, \"data from the period 1975–1978 ... indicate that, on average, only 413 men were arrested annually for statutory rape in California, even though 50,000 pregnancies occurred among underage women in 1976 alone\". In that state, it was found that two thirds of babies born to school-age mothers were fathered by adult men.\nSexual abuse early in life can lead young women to feel a lack of control over their sexual lives, decrease their future likelihood of using contraceptives such as condoms, and increase their chances of becoming pregnant or acquiring sexually transmitted infections. A 2007 paper by Child Trends examined studies from 2000 to 2006 to identify links between sexual abuse and teenage pregnancy, starting with Blinn-Pike et al.'s 2002 metastudy of 15 studies since 1989. It found that childhood sexual abuse has a \"significant association\" with adolescent pregnancy. Direct connections have been demonstrated both by retrospective studies examining antecedents to reported pregnancies and prospective studies, which track the lives of sex abuse victims and \"can be helpful for determining causality\". The more severe forms of abuse, such as rape and incest, entail a greater risk of adolescent pregnancy. Although some researchers suggest that pregnancy could be a choice made to escape a \"bad situation\", it may also be \"a direct result of unwanted intercourse\", which one study found to be the case for about 13% of participants in a Texas parenting program.\n\nIn Nicaragua, between 2000 and 2010, around 172,500 births were recorded for girls under 14, representing around 13% of the 10.3 million births during that period. These were attributed to poverty, laws forbidding abortion for rape and incest, lack of access to justice, and beliefs held in the culture and legal system. A 1992 study in Peru found that 90% of babies delivered to mothers aged 12–16 were conceived through rape, typically by a father, stepfather, or other close relative. In 1991 in Costa Rica, the figure was similar, with 95% of adolescent mothers under 15 having become pregnant through rape.\n\nMany of the youngest documented birth mothers in history experienced precocious puberty and were impregnated as a result of rape, including incest. The youngest, Peruvian Lina Medina, was impregnated when she was four and had a live birth in 1939, at age five.\n\nRape has been used as a weapon of psychological warfare for centuries, to terrorize, humiliate, and undermine the morale of the enemy. Rape was also used as an act of ethnic cleansing to produce babies that share the perpetrators' ethnicity. Forced pregnancy has been noted in places including Bangladesh, Darfur, and Bosnia. More broadly, pregnancy commonly results from wartime rape that was perpetrated without the intention of impregnating the enemy, as has been found in conflicts in East Timor, Liberia, Kosovo, and Rwanda. Gita Sahgal of Amnesty International commented that, rather than being primarily about \"spoils of war\" or sexual gratification, rape is often used in ethnic conflicts as a way for attackers to perpetuate social control and redraw ethnic boundaries. Children may be born to women and girls forced to \"marry\" abductors and occupiers; this happened in the Indonesian occupation of East Timor and in the Lord's Resistance Army's conflict in Uganda.\n\nRape during war is recognized under United Nations Security Council Resolution 1820 as a war crime and a crime against humanity. \"Forced pregnancy\" is specifically enumerated as a war crime and crime against humanity in the Rome Statute, which was the \"first international criminal tribunal ever officially to criminalize forced pregnancy\".\nChildren born as the result of wartime rape may be identified with the enemy and grow up stigmatized and excluded by their communities; they may be denied basic rights or even killed before reaching adulthood. Children are particularly at risk for such abuse when they are visibly identifiable as sharing half their ethnicity with the occupying forces, as in the case of half-Arab children of Darfuri women raped by janjaweed soldiers as part of the war in Darfur. Children of war rape are also at risk due to neglect by traumatized mothers unable to provide sufficient care.\n\nIn 1937 the Japanese army took over Nanking, which at the time was the capital of China. In the resulting seven-week occupation known as the Rape of Nanking, as many as 80,000 people were raped. Chinese women and girls of all ages were raped, mutilated, tortured, sexually enslaved, and killed; unknown numbers of them were left pregnant. Many pregnant Nanking women killed themselves in 1938, and others committed infanticide when their babies were born. During the rest of the 20th century there was no record of any Chinese woman acknowledging her child as having been born as a result of the Rape of Nanking.\n\nDuring the 1992–95 Bosnian War, pregnancy from rape was used to perpetrate genocide. There were reports of deliberately created \"rape camps\" intended to impregnate captive Muslim and Croatian women. Women were reported to have been kept in confinement until their pregnancies had advanced beyond a stage at which abortion would be safe. In the context of a patrilineal society, in which children inherit their father's ethnicity, such camps were intended to create a new generation of Serbian children. The women's group \"Tresnjevka\" claimed that more than 35,000 women and children were held in such Serb-run camps. Estimates range from 20,000 to 50,000 victims. Feryal Gharahi of Equality Now reported:\n\nAfter the Bosnian War, the International Criminal Court updated its statute to prohibit \"confin[ing] one or more women forcibly made pregnant, with the intent of affecting the ethnic composition of any population\".\n\nImmediate post-rape protocols call \nfor medical professionals to assess the likelihood that a victim will become pregnant in their assessment of the physical damage done to the woman. Protocol for gaining a history of the use of contraceptives, as a woman's use of birth control pills or other contraceptives before a rape affect her chance of becoming pregnant. Treatment protocols also call for clinicians to provide access to emergency contraception and counseling on abortion in countries where it is legal. High-dose estrogen pills were tried as an experimental treatment after rape in the 1960s, and in 1972 Canadian physician A. Albert Yuzpe and his colleagues began systematic studies on the use of ethinylestradiol and norgestrel to provide emergency contraception after an assault. These treatments reduced the rate of pregnancy after rape by 84%. This method is now called the Yuzpe regimen. Before being treated with pregnancy prevention measures, a rape victim is given a HCG pregnancy test to determine whether she was already pregnant before the rape.\n\nWhen being discharged from emergency care, clinicians provide information about pregnancy as well as other complications such as infection and emotional trauma. While a woman who has become pregnant during the past 48 hours will test negative for pregnancy in an HCG pregnancy test (unless she was already pregnant before the rape), pregnancy resulting from the rape can be detected at the two-week follow-up visit.\n\nDecisions of whether to end a rape-related pregnancy or carry it to term, and whether to keep the child or give it up for adoption can be severely traumatizing for a woman.\n\nAbortion rates for pregnancies due to rape vary significantly by culture and demographics; women who live in countries where abortion is illegal must often give birth to the child or secretly undergo a dangerous, back-alley abortion. Some women do not wish to get abortions for religious or cultural reasons. In a third of cases, rape-related pregnancies are not discovered until the second trimester of pregnancy, which may reduce a woman's options, particularly if she doesn't have easy access to legal abortion or is still recovering from the trauma of the rape itself.\n\nIn the United States, 1 percent of 1,900 women questioned in 1987 listed rape or incest as the reason for having an abortion; of these, 95 percent named other reasons as well. \n\nA 1996 study of thousands of US women showed that, of pregnancies resulting from rape, 50% were aborted, 12% resulted in miscarriage, and 38% were brought to term and either given up for adoption or raised. Peer-reviewed studies have reported from 38% of American women to 90% of Peruvian adolescents carrying the pregnancy to term. In Lima, Peru, where abortion is illegal, 90% of girls aged 12 to 16 who became pregnant through rape carried the child to term.\n\nOf all children born, 1% are put up for adoption; the number of children conceived from rape who are given up for adoption was found to be about 6% in one study and 26% in another.\n\nWhen a mother commits neonaticide, killing an infant younger than 24 hours old, the child's birth being the result of rape is a main cause, although other psychological and situational factors are generally present. Some people turn to drugs or alcohol to cope with emotional trauma after a rape; use of these during pregnancy can harm the fetus.\n\nWhen a mother chooses to raise her child conceived in rape, the traumatic effect of the rape and the child's blood relationship to the rapist has the potential to create some psychological challenges, but the circumstance of conception is no guarantee to cause psychological problems.\nIf a woman decides to keep and raise the child, she may have difficulty accepting it, and both mother and child face ostracism in some societies.\nMothers may also face legal difficulties. In most US states, the rapist maintains parental rights. Research by legal scholar Shauna Prewitt indicates that the resulting continued contact with the rapist is damaging for women who keep the child. She wrote in 2012 that in the US, 31 states allow rapists to assert custody and visitation rights over children conceived through rape.\n\nChildren whose births result from rape have been killed by their mothers at various times in history. During ancient and medieval times, such infanticide was not prohibited (however, penance was expected of these mothers in medieval Europe).\n\nIn contrast to the modern scientific consensus that rape-induced pregnancies are no less likely than any other, beliefs that rape could not lead to pregnancy were widespread in both legal and medical opinion for centuries. Galen, an ancient Greek physician, believed that a woman must experience pleasure to release \"seed\" and become pregnant, and could not derive such pleasure from nonconsensual sex. Galen's thinking influenced understanding from medieval England to Colonial America.\nThe Ancient Greek philosopher Aristotle also believed that female pleasure played a central role in conception. Female reproduction was, in many ways, viewed through the lens of male reproductive processes, imagining that female organs functioned as inverted versions of male organs, and hence orgasm was required for conception.\n\nCenturies later, in medieval Europe, the belief that pregnancy could not occur without consent was still standard; in fact, conception by a woman was considered a legitimate defense against charges of rape. The belief was codified in the medieval British law texts \"Fleta\" and \"Britton\". \"Britton\" states:\n\n\"If the defendant confesses the fact, but says that the woman at the same time conceived by him, and can prove it, then our will is that it be adjudged no felony, because no woman can conceive if she does not consent.\"\n\nMedieval literary scholar Corinne Saunders acknowledged a difficulty in determining how widely held was the belief that pregnancy implies consent, but concluded that it influenced \"at least some justices\", citing a 1313 case in Kent.\n\nBy the late 1700s, scientists no longer universally accepted the view that pregnancy was impossible without pleasure, although this view was still common. A 1795 British legal text, \"Treatise of Pleas of the Crown\", disparaged the belief's legal utility and its biological veracity:\n\n\"Also it hath been said by some to be no rape to force a woman who conceives at the time; for it is said, that if she had not consented, she could not have conceived, but this opinion seems very questionable, not only because the previous violence is no way extenuated by such a subsequent consent, but also because, if it were necessary to shew that the woman did not conceive, the offender could not be tried till such time as it might appear whether she did or did not, and likewise because the philosophy of this notion may very well be doubted of.\"\n\nThe 1814 British legal text \"Elements of Medical Jurisprudence\" by Samuel Farr claimed that conception \"probably\" could not occur without a woman's \"enjoyment\", so that an \"absolute rape\" was unlikely to lead to pregnancy. On the other hand, in the US in an 1820 court case in the Arkansas Territory a man pleaded not guilty to rape charges because the victim became pregnant, but the court rejected the argument:\n\n\"The old notion that if the woman conceive, it could not be a rape, because she must have in such case have consented, is quite exploded. Impregnation, it is well known, does not depend on the consciousness or volition of the female. If the uterine organs be in a condition favorable to impregnation, this may take place as readily as if the intercourse was voluntary.\"\n\nHistorian Ian Talbot has written about how countries with Quran-based Islamic codes on rape and pregnancy use Sura An-Nur, verse 2, as a legal basis: \"The law of evidence in all sexual crimes required either self-confession or the testimony of four upright (\"salah\") Muslim males. In the case of a man, self-confession involved a verbal confession. For women however medical examinations and pregnancy arising from rape were admissible as proof of self-guilt.\"\n\nPregnancy from rape is an ethical and moral issue in the context of opposition to legal abortion. In recent decades, claims of the improbability of rape-induced pregnancy, reminiscent of historical beliefs, have again begun to play a role in political discourse surrounding abortion regulation in cases of rape, particularly in the United States.\n\nAmong female victims of partner violence who filed a protective order, 68% reported they were raped by their intimate partner and 20% reported a rape-related pregnancy.\n\nClaims that rape reduces the chance of pregnancy have at times been made by referring to the fact that chronic stress can reduce a woman's fertility over a long term. However the current scientific consensus is that this is specific to long-term stress—the acute stress reaction that occurs during rape cannot \"shut down ovulation that has already begun\". In a 1972 article, physician and anti-abortion activist Fred Mecklenburg argued that pregnancy from rape is \"extremely rare\", adding that a woman exposed to the trauma of rape \"will not ovulate even if she is 'scheduled' to\". Blythe Bernhard wrote in \"The Washington Post\", \"That article has influenced two generations of anti-abortion activists with the hope to build a medical case to ban all abortions without any exception.\" John C. Willke, a former president of the National Right to Life Committee and a general practitioner with training in obstetrics, has published similar statements since 1985. In a 2012 interview he said, \"This is a traumatic thing – she's, shall we say, she's uptight. She is frightened, tight, and so on. And sperm, if deposited in her vagina, are less likely to be able to fertilize. The tubes are spastic.\" These assertions were disputed by a number of gynecology professors. A 1997 book published by the group Human Life International (which opposes legal abortion in all cases, including rape) claims that several studies performed in the 1970s show that only 0.08% of rapes result in pregnancy, and alternatively offers an estimate of 0.8% from other data. The same book dismisses contrary statistics, claiming that \"the women who obtain abortions for 'rape' are almost always lying\".\n\nRelated views have also been expressed by pro-life groups outside the United States. The United Kingdom pro-life group Society for the Protection of Unborn Children similarly claims that rape-pregnancy is \"extremely rare\", in part because the \"trauma of being raped makes it difficult for fertilisation or implantation to occur\". The Irish pro-life group Youth Defence published claims on its web site that \"trauma from the rape may bring into play some natural defence mechanisms that reduce the likelihood of pregnancy\", but removed these statements in 2012 following the Akin controversy, explaining that the group now deemed them \"unreliable\". The Irish group Precious Life published claims that \"the trauma of sexual assault is likely to inhibit ovulation\" and \"the rate of pregnancy arising from sexual assault is 0.1%\". Other groups that say rape prevents ovulation include the Australian group \"Pro-Life Victoria\", and the group \"Pro-Life Philippines\". The Austrian group \"Jugend für das Leben\" (Youth for Life) writes that \"pregnancies after rape are extremely rare\" because \"protective mechanisms\" from the stress of rape will \"almost always prevent conception\".\n\nIn the US, several politicians in the Republican Party have advanced claims that pregnancy from rape is rare. Pennsylvania state representative Stephen Freind claimed in 1988 that the odds of a pregnancy resulting from rape were \"one in millions and millions and millions\". James Leon Holmes published a letter in 1980 stating, \"concern for rape victims is a red herring because conceptions from rape occur with approximately the same frequency as snowfall in Miami\". Holmes apologized for this remark in 2003 after he was nominated as a United States federal judge. (He was confirmed in 2004). In 1995, North Carolina House of Representatives member Henry Aldridge remarked during a debate to eliminate a state abortion fund for poor women: \"The facts show that people who are raped – who are truly raped – the juices don't flow, the body functions don't work and they don't get pregnant. Medical authorities agree that this is a rarity, if ever.\" In 1998, Arkansas state senator Fay Boozman lost a campaign for a US Senate seat after remarking that fear-induced hormonal changes made rape victims unlikely to become pregnant. He later apologized and eventually called the claim a mistake, but the controversy was renewed in 1999 when he was appointed director of the Arkansas Department of Health by then-governor Mike Huckabee. During his campaign in the United States Senate election in Missouri, 2012, US Representative Todd Akin commented on abortion exceptions for rape victims: \"I understand from doctors, that's really rare. If it's a legitimate rape, the female body has ways to try to shut that whole thing down.\" The comment was widely criticized, and Akin apologized, saying he \"misspoke\". Akin's suggestion that rape is unlikely to result in pregnancy was defended by some prominent individuals and groups which oppose legal abortion. A SurveyUSA poll one day after Akin's comments reported that 76% of Missouri adults disagreed with Akin, 13% agreed with the statement and 11% were unsure (±3.8%).\n\n\n"}
{"id": "24097", "url": "https://en.wikipedia.org/wiki?curid=24097", "title": "Principle of bivalence", "text": "Principle of bivalence\n\nIn logic, the semantic principle (or law) of bivalence states that every declarative sentence expressing a proposition (of a theory under inspection) has exactly one truth value, either true or false. A logic satisfying this principle is called a two-valued logic or bivalent logic.\n\nIn formal logic, the principle of bivalence becomes a property that a semantics may or may not possess. It is not the same as the law of excluded middle, however, and a semantics may satisfy that law without being bivalent.\n\nThe principle of bivalence is studied in philosophical logic to address the question of which natural-language statements have a well-defined truth value. Sentences which predict events in the future, and sentences which seem open to interpretation, are particularly difficult for philosophers who hold that the principle of bivalence applies to all declarative natural-language statements. Many-valued logics formalize ideas that a realistic characterization of the notion of consequence requires the admissibility of premises which, owing to vagueness, temporal or quantum indeterminacy, or reference-failure, cannot be considered classically bivalent. Reference failures can also be addressed by free logics.\n\nThe principle of bivalence is related to the law of excluded middle though the latter is a syntactic expression of the language of a logic of the form \"P ∨ ¬P\". The difference between the principle and the law is important because there are logics which validate the law but which do not validate the principle. For example, the three-valued Logic of Paradox (LP) validates the law of excluded middle, but not the law of non-contradiction, ¬(P ∧ ¬P), and its intended semantics is not bivalent. In classical two-valued logic both the law of excluded middle and the law of non-contradiction hold.\n\nMany modern logic programming systems replace the law of the excluded middle with the concept of negation as failure. The programmer may wish to add the law of the excluded middle by explicitly asserting it as true; however, it is not assumed \"a priori\".\n\nThe intended semantics of classical logic is bivalent, but this is not true of every semantics for classical logic. In Boolean-valued semantics (for classical propositional logic), the truth values are the elements of an arbitrary Boolean algebra, \"true\" corresponds to the maximal element of the algebra, and \"false\" corresponds to the minimal element. Intermediate elements of the algebra correspond to truth values other than \"true\" and \"false\". The principle of bivalence holds only when the Boolean algebra is taken to be the two-element algebra, which has no intermediate elements.\n\nAssigning Boolean semantics to classical predicate calculus requires that the model be a complete Boolean algebra because the universal quantifier maps to the infimum operation, and the existential quantifier maps to the supremum; this is called a Boolean-valued model. All finite Boolean algebras are complete.\n\nIn order to justify his claim that true and false are the only logical values, Suszko (1977) observes that every structural Tarskian many-valued propositional logic can be provided with a bivalent semantics.\n\nA famous example is the \"contingent sea battle\" case found in Aristotle's work, \"De Interpretatione\", chapter 9:\n\nThe principle of bivalence here asserts:\n\nAristotle to embrace bivalence for such future contingents; Chrysippus, the Stoic logician, did embrace bivalence for this and all other propositions. The controversy continues to be of central importance in both the philosophy of time and the philosophy of logic.\n\nOne of the early motivations for the study of many-valued logics has been precisely this issue. In the early 20th century, the Polish formal logician Jan Łukasiewicz proposed three truth-values: the true, the false and the \"as-yet-undetermined\". This approach was later developed by Arend Heyting and L. E. J. Brouwer; see Łukasiewicz logic.\n\nIssues such as this have also been addressed in various temporal logics, where one can assert that \"\"Eventually\", either there will be a sea battle tomorrow, or there won't be.\" (Which is true if \"tomorrow\" eventually occurs.)\n\nSuch puzzles as the Sorites paradox and the related continuum fallacy have raised doubt as to the applicability of classical logic and the principle of bivalence to concepts that may be vague in their application. Fuzzy logic and some other multi-valued logics have been proposed as alternatives that handle vague concepts better. Truth (and falsity) in fuzzy logic, for example, comes in varying degrees. Consider the following statement in the circumstance of sorting apples on a moving belt:\n\nUpon observation, the apple is an undetermined color between yellow and red, or it is motled both colors. Thus the color falls into neither category \" red \" nor \" yellow \", but these are the only categories available to us as we sort the apples. We might say it is \"50% red\". This could be rephrased: it is 50% true that the apple is red. Therefore, P is 50% true, and 50% false. Now consider:\n\nIn other words, P and not-P. This violates the law of noncontradiction and, by extension, bivalence. However, this is only a partial rejection of these laws because P is only partially true. If P were 100% true, not-P would be 100% false, and there is no contradiction because P and not-P no longer holds.\n\nHowever, the law of the excluded middle is retained, because P and not-P implies P or not-P, since \"or\" is inclusive. The only two cases where P and not-P is false (when P is 100% true or false) are the same cases considered by two-valued logic, and the same rules apply.\n\nExample of a 3-valued logic applied to vague (undetermined) cases: Kleene 1952 (§64, pp. 332–340) offers a 3-valued logic for the cases when algorithms involving partial recursive functions may not return values, but rather end up with circumstances \"u\" = undecided. He lets \"t\" = \"true\", \"f\" = \"false\", \"u\" = \"undecided\" and redesigns all the propositional connectives. He observes that:\n\nThe following are his \"strong tables\":\nFor example, if a determination cannot be made as to whether an apple is red or not-red, then the truth value of the assertion Q: \" This apple is red \" is \" u \". Likewise, the truth value of the assertion R \" This apple is not-red \" is \" u \". Thus the AND of these into the assertion Q AND R, i.e. \" This apple is red AND this apple is not-red \" will, per the tables, yield \" u \". And, the assertion Q OR R, i.e. \" This apple is red OR this apple is not-red \" will likewise yield \" u \".\n\n"}
{"id": "57093743", "url": "https://en.wikipedia.org/wiki?curid=57093743", "title": "Pro-Truth Pledge", "text": "Pro-Truth Pledge\n\nThe Pro-Truth Pledge is an initiative promoting truth seeking and rational thinking, particularly in politics.\n\nFirst published in December 2016, the pledge is a movement and initiative of the Rational Politics (RAP) project of Intentional Insights (InIn), a nonprofit organization dedicated to promoting rational thinking and good decision making in various areas of life. The Pro-Truth Pledge is partially a reaction (and a would-be answer) to recent political trends in the US and UK, for example to alternative facts, growth of fake news and post-truth politics; all of which are seen as acute problems.\n\nThe founders of Pro-Truth Pledge come from its mother organization, Intentional Insights. The behavior and social science methodologies behind the Pro-Truth Pledge were applied to the topic by Dr. Gleb Tsipursky, one of the founders of Intentional Insights.\n\nAccording to the project's home page, as of August 26, 2018, there are 8,374 signatories to the pledge, including 85 organizations, 625 government officials, and 850 public figures (including Jonathan Haidt, Michael Shermer, Steven Pinker and Pierre Whalon). The Pro-Truth Pledge has received media coverage.\n\nAt least two peer-reviewed studies have been conducted to determine the effectiveness of taking the Pro-Truth Pledge. \n\nA study published in the journal Behavior and Social Issues examined the sharing of news-related content on Facebook before and after taking the pledge. The findings \"suggest that taking the PTP had a statistically significant effect on behavior change in favor of more truthful sharing on Facebook.\"\n\nAnother study, published in the Journal of Social and Political Psychology, used a different methodology and reached a similar conclusion: \"taking the\npledge results in a statistically significant increase in alignment with the behaviors of the pledge.\"\n\nThe pledge has been translated into Spanish, Hungarian, Russian, Ukrainian, Portuguese and German, but the map of the pledge takers shows that most (above 90%) of the pledge takers live in North America, mainly in the US.\n"}
{"id": "25308", "url": "https://en.wikipedia.org/wiki?curid=25308", "title": "Quasispecies model", "text": "Quasispecies model\n\nThe quasispecies model is a description of the process of the Darwinian evolution of certain self-replicating entities within the framework of physical chemistry. A quasispecies is a large group or \"cloud\" of related genotypes that exist in an environment of high mutation rate (at stationary state), where a large fraction of offspring are expected to contain one or more mutations relative to the parent. This is in contrast to a species, which from an evolutionary perspective is a more-or-less stable single genotype, most of the offspring of which will be genetically accurate copies.\n\nIt is useful mainly in providing a qualitative understanding of the evolutionary processes of self-replicating macromolecules such as RNA or DNA or simple asexual organisms such as bacteria or viruses (see also viral quasispecies), and is helpful in explaining something of the early stages of the origin of life. Quantitative predictions based on this model are difficult because the parameters that serve as its input are impossible to obtain from actual biological systems. The quasispecies model was put forward by Manfred Eigen and Peter Schuster based on initial work done by Eigen.\n\nWhen evolutionary biologists describe competition between species, they generally assume that each species is a single genotype whose descendants are mostly accurate copies. (Such genotypes are said to have a high reproductive \"fidelity\".) Evolutionarily, we are interested in the behavior and fitness of that one species or genotype over time.\n\nSome organisms or genotypes, however, may exist in circumstances of low fidelity, where most descendants contain one or more mutations. A group of such genotypes is constantly changing, so discussions of which single genotype is the most fit become meaningless. Importantly, if many closely related genotypes are only one mutation away from each other, then genotypes in the group can mutate back and forth into each other. For example, with one mutation per generation, a child of the sequence AGGT could be AGTT, and a grandchild could be AGGT again. Thus we can envision a \"cloud\" of related genotypes that is rapidly mutating, with sequences going back and forth among different points in the cloud. Though the proper definition is mathematical, that cloud, roughly speaking, is a quasispecies.\n\nQuasispecies behavior exists for large numbers of individuals existing at a certain (high) range of mutation rates.\n\nIn a species, though reproduction may be mostly accurate, periodic mutations will give rise to one or more competing genotypes. If a mutation results in greater replication and survival, the mutant genotype may out-compete the parent genotype and come to dominate the species. Thus, the individual genotypes (or species) may be seen as the units on which selection acts and biologists will often speak of a single genotype's fitness.\n\nIn a quasispecies, however, mutations are ubiquitous and so the fitness of an individual genotype becomes meaningless: if one particular mutation generates a boost in reproductive success, it can't amount to much because that genotype's offspring are unlikely to be accurate copies with the same properties. Instead, what matters is the \"connectedness\" of the cloud. For example, the sequence AGGT has 12 (3+3+3+3) possible single point mutants AGGA, AGGG, and so on. If 10 of those mutants are viable genotypes that may reproduce (and some of whose offspring or grandchildren may mutate back into AGGT again), we would consider that sequence a well-connected node in the cloud. If instead only two of those mutants are viable, the rest being lethal mutations, then that sequence is poorly connected and most of its descendants will not reproduce. The analog of fitness for a quasispecies is the tendency of nearby relatives within the cloud to be well-connected, meaning that more of the mutant descendants will be viable and give rise to further descendants within the cloud.\n\nWhen the fitness of a single genotype becomes meaningless because of the high rate of mutations, the cloud as a whole or quasispecies becomes the natural unit of selection.\n\nQuasispecies represents the evolution of high-mutation-rate viruses such as HIV and sometimes single genes or molecules within the genomes of other organisms. Quasispecies models have also been proposed by Jose Fontanari and Emmanuel David Tannenbaum to model the evolution of sexual reproduction. Quasispecies was also shown in compositional replicators (based on the Gard model for abiogenesis) and was also suggested to be applicable to describe cell's replication, which amongst other things requires the maintenance and evolution of the internal composition of the parent and bud.\n\nThe model rests on four assumptions:\n\nIn the quasispecies model, mutations occur through errors made in the process of copying already existing sequences. Further, selection arises because different types of sequences tend to replicate at different rates, which leads to the suppression of sequences that replicate more slowly in favor of sequences that replicate faster. However, the quasispecies model does not predict the ultimate extinction of all but the fastest replicating sequence. Although the sequences that replicate more slowly cannot sustain their abundance level by themselves, they are constantly replenished as sequences that replicate faster mutate into them. At equilibrium, removal of slowly replicating sequences due to decay or outflow is balanced by replenishing, so that even relatively slowly replicating sequences can remain present in finite abundance.\n\nDue to the ongoing production of mutant sequences, selection does not act on single sequences, but on mutational \"clouds\" of closely related sequences, referred to as \"quasispecies\". In other words, the evolutionary success of a particular sequence depends not only on its own replication rate, but also on the replication rates of the mutant sequences it produces, and on the replication rates of the sequences of which it is a mutant. As a consequence, the sequence that replicates fastest may even disappear completely in selection-mutation equilibrium, in favor of more slowly replicating sequences that are part of a quasispecies with a higher average growth rate. Mutational clouds as predicted by the quasispecies model have been observed in RNA viruses and in \"in vitro\" RNA replication.\n\nThe mutation rate and the general fitness of the molecular sequences and their neighbors is crucial to the formation of a quasispecies. If the mutation rate is zero, there is no exchange by mutation, and each sequence is its own species. If the mutation rate is too high, exceeding what is known as the error threshold, the quasispecies will break down and be dispersed over the entire range of available sequences.\n\nA simple mathematical model for a quasispecies is as follows: let there be formula_1 possible sequences and let there be formula_2 organisms with sequence \"i\". Let's say that each of these organisms asexually gives rise to formula_3 offspring. Some are duplicates of their parent, having sequence \"i\", but some are mutant and have some other sequence. Let the mutation rate formula_4 correspond to the probability that a \"j\" type parent will produce an \"i\" type organism. Then the expected fraction of offspring generated by \"j\" type organisms that would be \"i\" type organisms is formula_5,\n\nwhere formula_6.\n\nThen the total number of \"i\"-type organisms after the first round of reproduction, given as formula_7, is\n\nSometimes a death rate term formula_9 is included so that:\n\nwhere formula_11 is equal to 1 when i=j and is zero otherwise. Note that the \"n-th\" generation can be found by just taking the \"n-th\" power of W substituting it in place of W in the above formula.\n\nThis is just a system of linear equations. The usual way to solve such a system is to first diagonalize the W matrix. Its diagonal entries will be eigenvalues corresponding to certain linear combinations of certain subsets of sequences which will be eigenvectors of the W matrix. These subsets of sequences are the quasispecies. Assuming that the matrix W is a primitive matrix (irreducible and aperiodic), then after very many generations only the eigenvector with the largest eigenvalue will prevail, and it is this quasispecies that will eventually dominate. The components of this eigenvector give the relative abundance of each sequence at equilibrium.\n\nW being primitive means that for some integer formula_12, that the formula_13 power of W is > 0, i.e. all the entries are positive. If W is primitive then each type can, through a sequence of mutations (i.e. powers of W) mutate into all the other types after some number of generations. W is not primitive if it is periodic, where the population can perpetually cycle through different disjoint sets of compositions, or if it is reducible, where the dominant species (or quasispecies) that develops can depend on the initial population, as is the case in the simple example given below.\n\nThe quasispecies formulae may be expressed as a set of linear differential equations. If we consider the difference between the new state formula_7 and the old state formula_2 to be the state change over one moment of time, then we can state that the time derivative of formula_2 is given by this difference, formula_17 we can write:\n\nThe quasispecies equations are usually expressed in terms of concentrations formula_19 where\n\nThe above equations for the quasispecies then become for the discrete version:\n\nor, for the continuum version:\n\nThe quasispecies concept can be illustrated by a simple system consisting of 4 sequences. Sequences [0,0], [0,1], [1,0], and [1,1] are numbered 1, 2, 3, and 4, respectively. Let's say the [0,0] sequence never mutates and always produces a single offspring. Let's say the other 3 sequences all produce, on average, formula_24 replicas of themselves, and formula_25 of each of the other two types, where formula_26. The W matrix is then:\n\nThe diagonalized matrix is:\n\nAnd the eigenvectors corresponding to these eigenvalues are:\n\nOnly the eigenvalue formula_29 is more than unity. For the n-th generation, the corresponding eigenvalue will be formula_30 and so will increase without bound as time goes by. This eigenvalue corresponds to the eigenvector [0,1,1,1], which represents the quasispecies consisting of sequences 2, 3, and 4, which will be present in equal numbers after a very long time. Since all population numbers must be positive, the first two quasispecies are not legitimate. The third quasispecies consists of only the non-mutating sequence 1. It's seen that even though sequence 1 is the most fit in the sense that it reproduces more of itself than any other sequence, the quasispecies consisting of the other three sequences will eventually dominate (assuming that the initial population was not homogeneous of the sequence 1 type).\n\n"}
{"id": "63753", "url": "https://en.wikipedia.org/wiki?curid=63753", "title": "Rationalism", "text": "Rationalism\n\nIn philosophy, rationalism is the epistemological view that \"regards reason as the chief source and test of knowledge\" or \"any view appealing to reason as a source of knowledge or justification\". More formally, rationalism is defined as a methodology or a theory \"in which the criterion of the truth is not sensory but intellectual and deductive\".\n\nIn an old controversy, rationalism was opposed to empiricism, where the rationalists believed that reality has an intrinsically logical structure. Because of this, the rationalists argued that certain truths exist and that the intellect can directly grasp these truths. That is to say, rationalists asserted that certain rational principles exist in logic, mathematics, ethics, and metaphysics that are so fundamentally true that denying them causes one to fall into contradiction. The rationalists had such a high confidence in reason that empirical proof and physical evidence were regarded as unnecessary to ascertain certain truths – in other words, \"there are significant ways in which our concepts and knowledge are gained independently of sense experience\".\n\nDifferent degrees of emphasis on this method or theory lead to a range of rationalist standpoints, from the moderate position \"that reason has precedence over other ways of acquiring knowledge\" to the more extreme position that reason is \"the unique path to knowledge\". Given a pre-modern understanding of reason, rationalism is identical to philosophy, the Socratic life of inquiry, or the zetetic (skeptical) clear interpretation of authority (open to the underlying or essential cause of things as they appear to our sense of certainty). In recent decades, Leo Strauss sought to revive \"Classical Political Rationalism\" as a discipline that understands the task of reasoning, not as foundational, but as maieutic.\n\nIn politics, rationalism, since the Enlightenment, historically emphasized a \"politics of reason\" centered upon rational choice, utilitarianism, secularism, and irreligion – the latter aspect's antitheism was later softened by the adoption of pluralistic methods practicable regardless of religious or irreligious ideology.\n\nIn this regard, the philosopher John Cottingham noted how rationalism, a methodology, became socially conflated with atheism, a worldview:\nRationalism is often contrasted with empiricism. Taken very broadly these views are not mutually exclusive, since a philosopher can be both rationalist and empiricist. Taken to extremes, the empiricist view holds that all ideas come to us \"a posteriori\", that is to say, through experience; either through the external senses or through such inner sensations as pain and gratification. The empiricist essentially believes that knowledge is based on or derived directly from experience. The rationalist believes we come to knowledge \"a priori\" – through the use of logic – and is thus independent of sensory experience. In other words, as Galen Strawson once wrote, \"you can see that it is true just lying on your couch. You don't have to get up off your couch and go outside and examine the way things are in the physical world. You don't have to do any science.\" Between both philosophies, the issue at hand is the fundamental source of human knowledge and the proper techniques for verifying what we think we know. Whereas both philosophies are under the umbrella of epistemology, their argument lies in the understanding of the warrant, which is under the wider epistemic umbrella of the theory of justification.\n\nThe theory of justification is the part of epistemology that attempts to understand the justification of propositions and beliefs. Epistemologists are concerned with various epistemic features of belief, which include the ideas of justification, warrant, rationality, and probability. Of these four terms, the term that has been most widely used and discussed by the early 21st century is \"warrant\". Loosely speaking, justification is the reason that someone (probably) holds a belief.\n\nIf \"A\" makes a claim, and \"B\" then casts doubt on it, \"A\"'s next move would normally be to provide justification. The precise method one uses to provide justification is where the lines are drawn between rationalism and empiricism (among other philosophical views). Much of the debate in these fields are focused on analyzing the nature of knowledge and how it relates to connected notions such as truth, belief, and justification.\n\nAt its core, rationalism consists of three basic claims. For one to consider themselves a rationalist, they must adopt at least one of these three claims: The Intuition/Deduction Thesis, The Innate Knowledge Thesis, or The Innate Concept Thesis. In addition, rationalists can choose to adopt the claims of Indispensability of Reason and or the Superiority of Reason – although one can be a rationalist without adopting either thesis.\n\nRationale: \"Some propositions in a particular subject area, S, are knowable by us by intuition alone; still others are knowable by being deduced from intuited propositions.\"<ref name=\"The Intuition/Deduction Thesis\">Stanford Encyclopedia of Philosophy, The Intuition/Deduction Thesis First published August 19, 2004; substantive revision March 31, 2013 cited on May 20, 2013.</ref>\n\nGenerally speaking, intuition is \"a priori\" knowledge or experiential belief characterized by its immediacy; a form of rational insight. We simply \"see\" something in such a way as to give us a warranted belief. Beyond that, the nature of intuition is hotly debated.\n\nIn the same way, generally speaking, deduction is the process of reasoning from one or more general premises to reach a logically certain conclusion. Using valid arguments, we can deduce from intuited premises.\n\nFor example, when we combine both concepts, we can intuit that the number three is prime and that it is greater than two. We then deduce from this knowledge that there is a prime number greater than two. Thus, it can be said that intuition and deduction combined to provide us with \"a priori\" knowledge – we gained this knowledge independently of sense experience.\n\nEmpiricists such as David Hume have been willing to accept this thesis for describing the relationships among our own concepts. In this sense, empiricists argue that we are allowed to intuit and deduce truths from knowledge that has been obtained \"a posteriori\".\n\nBy injecting different subjects into the Intuition/Deduction thesis, we are able to generate different arguments. Most rationalists agree mathematics is knowable by applying the intuition and deduction. Some go further to include ethical truths into the category of things knowable by intuition and deduction. Furthermore, some rationalists also claim metaphysics is knowable in this thesis.\n\nIn addition to different subjects, rationalists sometimes vary the strength of their claims by adjusting their understanding of the warrant. Some rationalists understand warranted beliefs to be beyond even the slightest doubt; others are more conservative and understand the warrant to be belief beyond a reasonable doubt.\n\nRationalists also have different understanding and claims involving the connection between intuition and truth. Some rationalists claim that intuition is infallible and that anything we intuit to be true is as such. More contemporary rationalists accept that intuition is not always a source of certain knowledge – thus allowing for the possibility of a deceiver who might cause the rationalist to intuit a false proposition in the same way a third party could cause the rationalist to have perceptions of nonexistent objects.\n\nNaturally, the more subjects the rationalists claim to be knowable by the Intuition/Deduction thesis, the more certain they are of their warranted beliefs, and the more strictly they adhere to the infallibility of intuition, the more controversial their truths or claims and the more radical their rationalism.\n\nTo argue in favor of this thesis, Gottfried Wilhelm Leibniz, a prominent German philosopher, says, \"The senses, although they are necessary for all our actual knowledge, are not sufficient to give us the whole of it, since the senses never give anything but instances, that is to say particular or individual truths. Now all the instances which confirm a general truth, however numerous they may be, are not sufficient to establish the universal necessity of this same truth, for it does not follow that what happened before will happen in the same way again. … From which it appears that necessary truths, such as we find in pure mathematics, and particularly in arithmetic and geometry, must have principles whose proof does not depend on instances, nor consequently on the testimony of the senses, although without the senses it would never have occurred to us to think of them…\"\n\nRationale: \"We have knowledge of some truths in a particular subject area, S, as part of our rational nature.\"\n\nThe Innate Knowledge thesis is similar to the Intuition/Deduction thesis in the regard that both theses claim knowledge is gained \"a priori\". The two theses go their separate ways when describing how that knowledge is gained. As the name, and the rationale, suggests, the Innate Knowledge thesis claims knowledge is simply part of our rational nature. Experiences can trigger a process that allows this knowledge to come into our consciousness, but the experiences don't provide us with the knowledge itself. The knowledge has been with us since the beginning and the experience simply brought into focus, in the same way a photographer can bring the background of a picture into focus by changing the aperture of the lens. The background was always there, just not in focus.\n\nThis thesis targets a problem with the nature of inquiry originally postulated by Plato in \"Meno\". Here, Plato asks about inquiry; how do we gain knowledge of a theorem in geometry? We inquire into the matter. Yet, knowledge by inquiry seems impossible. In other words, \"If we already have the knowledge, there is no place for inquiry. If we lack the knowledge, we don't know what we are seeking and cannot recognize it when we find it. Either way we cannot gain knowledge of the theorem by inquiry. Yet, we do know some theorems.\" The Innate Knowledge thesis offers a solution to this paradox. By claiming that knowledge is already with us, either consciously or unconsciously, a rationalist claims we don't really \"learn\" things in the traditional usage of the word, but rather that we simply bring to light what we already know.\n\nRationale: \"We have some of the concepts we employ in a particular subject area, S, as part of our rational nature.\"\n\nSimilar to the Innate Knowledge thesis, the Innate Concept thesis suggests that some concepts are simply part of our rational nature. These concepts are \"a priori\" in nature and sense experience is irrelevant to determining the nature of these concepts (though, sense experience can help bring the concepts to our conscious mind).\n\nSome philosophers, such as John Locke (who is considered one of the most influential thinkers of the Enlightenment and an empiricist) argue that the Innate Knowledge thesis and the Innate Concept thesis are the same. Other philosophers, such as Peter Carruthers, argue that the two theses are distinct from one another. As with the other theses covered under the umbrella of rationalism, the more types and greater number of concepts a philosopher claims to be innate, the more controversial and radical their position; \"the more a concept seems removed from experience and the mental operations we can perform on experience the more plausibly it may be claimed to be innate. Since we do not experience perfect triangles but do experience pains, our concept of the former is a more promising candidate for being innate than our concept of the latter.\n\nIn his book, \"Meditations on First Philosophy\", René Descartes postulates three classifications for our ideas when he says, \"Among my ideas, some appear to be innate, some to be adventitious, and others to have been invented by me. My understanding of what a thing is, what truth is, and what thought is, seems to derive simply from my own nature. But my hearing a noise, as I do now, or seeing the sun, or feeling the fire, comes from things which are located outside me, or so I have hitherto judged. Lastly, sirens, hippogriffs and the like are my own invention.\"\n\nAdventitious ideas are those concepts that we gain through sense experiences, ideas such as the sensation of heat, because they originate from outside sources; transmitting their own likeness rather than something else and something you simply cannot will away. Ideas invented by us, such as those found in mythology, legends, and fairy tales are created by us from other ideas we possess. Lastly, innate ideas, such as our ideas of perfection, are those ideas we have as a result of mental processes that are beyond what experience can directly or indirectly provide.\n\nGottfried Wilhelm Leibniz defends the idea of innate concepts by suggesting the mind plays a role in determining the nature of concepts, to explain this, he likens the mind to a block of marble in the \"New Essays on Human Understanding\", \"This is why I have taken as an illustration a block of veined marble, rather than a wholly uniform block or blank tablets, that is to say what is called tabula rasa in the language of the philosophers. For if the soul were like those blank tablets, truths would be in us in the same way as the figure of Hercules is in a block of marble, when the marble is completely indifferent whether it receives this or some other figure. But if there were veins in the stone which marked out the figure of Hercules rather than other figures, this stone would be more determined thereto, and Hercules would be as it were in some manner innate in it, although labour would be needed to uncover the veins, and to clear them by polishing, and by cutting away what prevents them from appearing. It is in this way that ideas and truths are innate in us, like natural inclinations and dispositions, natural habits or potentialities, and not like activities, although these potentialities are always accompanied by some activities which correspond to them, though they are often imperceptible.\"\n\nThe three aforementioned theses of Intuition/Deduction, Innate Knowledge, and Innate Concept are the cornerstones of rationalism. To be considered a rationalist, one must adopt at least one of those three claims. The following two theses are traditionally adopted by rationalists, but they aren't essential to the rationalist's position.\n\nThe Indispensability of Reason Thesis has the following rationale, \"The knowledge we gain in subject area, \"S\", by intuition and deduction, as well as the ideas and instances of knowledge in \"S\" that are innate to us, could not have been gained by us through sense experience.\" In short, this thesis claims that experience cannot provide what we gain from reason.\n\nThe Superiority of Reason Thesis has the following rationale, '\"The knowledge we gain in subject area \"S\" by intuition and deduction or have innately is superior to any knowledge gained by sense experience\". In other words, this thesis claims reason is superior to experience as a source for knowledge.\n\nIn addition to the following claims, rationalists often adopt similar stances on other aspects of philosophy. Most rationalists reject skepticism for the areas of knowledge they claim are knowable \"a priori\". Naturally, when you claim some truths are innately known to us, one must reject skepticism in relation to those truths. Especially for rationalists who adopt the Intuition/Deduction thesis, the idea of epistemic foundationalism tends to crop up. This is the view that we know some truths without basing our belief in them on any others and that we then use this foundational knowledge to know more truths.\n\nRationalism - as an appeal to human reason as a way of obtaining knowledge - has a philosophical history dating from antiquity. The analytical nature of much of philosophical enquiry, the awareness of apparently a priori domains of knowledge such as mathematics, combined with the emphasis of obtaining knowledge through the use of rational faculties (commonly rejecting, for example, direct revelation) have made rationalist themes very prevalent in the history of philosophy.\n\nSince the Enlightenment, rationalism is usually associated with the introduction of mathematical methods into philosophy as seen in the works of Descartes, Leibniz, and Spinoza. This is commonly called continental rationalism, because it was predominant in the continental schools of Europe, whereas in Britain empiricism dominated.\n\nEven then, the distinction between rationalists and empiricists was drawn at a later period and would not have been recognized by the philosophers involved. Also, the distinction between the two philosophies is not as clear-cut as is sometimes suggested; for example, Descartes and Locke have similar views about the nature of human ideas.\n\nProponents of some varieties of rationalism argue that, starting with foundational basic principles, like the axioms of geometry, one could deductively derive the rest of all possible knowledge. The philosophers who held this view most clearly were Baruch Spinoza and Gottfried Leibniz, whose attempts to grapple with the epistemological and metaphysical problems raised by Descartes led to a development of the fundamental approach of rationalism. Both Spinoza and Leibniz asserted that, \"in principle\", all knowledge, including scientific knowledge, could be gained through the use of reason alone, though they both observed that this was not possible \"in practice\" for human beings except in specific areas such as mathematics. On the other hand, Leibniz admitted in his book \"Monadology\" that \"we are all mere Empirics in three fourths of our actions.\"\n\nAlthough rationalism in its modern form post-dates antiquity, philosophers from this time laid down the foundations of rationalism. In particular, the understanding that we may be aware of knowledge available only through the use of rational thought. \n\nAjita Kesakambali was an ancient Indian philosopher in the 6th century BCE. He is considered to be the first known proponent of Indian materialism, and forerunner to the Charvaka school of Indian thought, which holds direct perception, empiricism, and conditional inference as proper sources of knowledge, embraces philosophical skepticism and rejects Vedas, Vedic ritualism, and supernaturalism.\n\nPythagoras was one of the first Western philosophers to stress rationalist insight. He is often revered as a great mathematician, mystic and scientist, but he is best known for the Pythagorean theorem, which bears his name, and for discovering the mathematical relationship between the length of strings on lute and the pitches of the notes. Pythagoras \"believed these harmonies reflected the ultimate nature of reality. He summed up the implied metaphysical rationalism in the words \"All is number\". It is probable that he had caught the rationalist's vision, later seen by Galileo (1564–1642), of a world governed throughout by mathematically formulable laws\". It has been said that he was the first man to call himself a philosopher, or lover of wisdom.\n\nPlato held rational insight to a very high standard, as is seen in his works such as Meno and The Republic. He taught on the Theory of Forms (or the Theory of Ideas) which asserts that the highest and most fundamental kind of reality is not the material world of change known to us through sensation, but rather the abstract, non-material (but substantial) world of forms (or ideas). For Plato, these forms were accessible only to reason and not to sense. In fact, it is said that Plato admired reason, especially in geometry, so highly that he had the phrase \"Let no one ignorant of geometry enter\" inscribed over the door to his academy.\n\nAristotle's main contribution to rationalist thinking was the use of syllogistic logic and its use in argument. Aristotle defines syllogism as \"a discourse in which certain (specific) things having been supposed, something different from the things supposed results of necessity because these things are so.\" Despite this very general definition, Aristotle limits himself to categorical syllogisms which consist of three categorical propositions in his work \"Prior Analytics\". These included categorical modal syllogisms.\n\nAlthough the three great Greek philosophers disagreed with one another on specific points, they all agreed that rational thought could bring to light knowledge that was self-evident – information that humans otherwise couldn't know without the use of reason. After Aristotle's death, Western rationalistic thought was generally characterized by its application to theology, such as in the works of Augustine, the Islamic philosopher Avicenna and Jewish philosopher and theologian Maimonides. One notable event in the Western timeline was the philosophy of Thomas Aquinas who attempted to merge Greek rationalism and Christian revelation in the thirteenth-century.\n\nEarly modern rationalism has its roots in the 17th-century Dutch Republic, with some notable intellectual representatives like Hugo Grotius, René Descartes, and Baruch Spinoza.\n\nDescartes was the first of the modern rationalists and has been dubbed the 'Father of Modern Philosophy.' Much subsequent Western philosophy is a response to his writings, which are studied closely to this day.\n\nDescartes thought that only knowledge of eternal truths including the truths of mathematics, and the epistemological and metaphysical foundations of the sciences could be attained by reason alone; other knowledge, the knowledge of physics, required experience of the world, aided by the scientific method. He also argued that although dreams appear as real as sense experience, these dreams cannot provide persons with knowledge. Also, since conscious sense experience can be the cause of illusions, then sense experience itself can be doubtable. As a result, Descartes deduced that a rational pursuit of truth should doubt every belief about sensory reality. He elaborated these beliefs in such works as \"Discourse on Method\", \"Meditations on First Philosophy\", and \"Principles of Philosophy\". Descartes developed a method to attain truths according to which nothing that cannot be recognised by the intellect (or reason) can be classified as knowledge. These truths are gained \"without any sensory experience,\" according to Descartes. Truths that are attained by reason are broken down into elements that intuition can grasp, which, through a purely deductive process, will result in clear truths about reality.\n\nDescartes therefore argued, as a result of his method, that reason alone determined knowledge, and that this could be done independently of the senses. For instance, his famous dictum, \"cogito ergo sum\" or \"I think, therefore I am\", is a conclusion reached \"a priori\" i.e., prior to any kind of experience on the matter. The simple meaning is that doubting one's existence, in and of itself, proves that an \"I\" exists to do the thinking. In other words, doubting one's own doubting is absurd. This was, for Descartes, an irrefutable principle upon which to ground all forms of other knowledge. Descartes posited a metaphysical dualism, distinguishing between the substances of the human body (\"\"res extensa\") and the mind or soul (\"res cogitans\"\"). This crucial distinction would be left unresolved and lead to what is known as the mind-body problem, since the two substances in the Cartesian system are independent of each other and irreducible.\n\nThe philosophy of Baruch Spinoza is a systematic, logical, rational philosophy developed in seventeenth-century Europe. Spinoza's philosophy is a system of ideas constructed upon basic building blocks with an internal consistency with which he tried to answer life's major questions and in which he proposed that \"God exists only philosophically.\" He was heavily influenced by Descartes, Euclid and Thomas Hobbes, as well as theologians in the Jewish philosophical tradition such as Maimonides. But his work was in many respects a departure from the Judeo-Christian tradition. Many of Spinoza's ideas continue to vex thinkers today and many of his principles, particularly regarding the emotions, have implications for modern approaches to psychology. To this day, many important thinkers have found Spinoza's \"geometrical method\" difficult to comprehend: Goethe admitted that he found this concept confusing. His \"magnum opus\", \"Ethics\", contains unresolved obscurities and has a forbidding mathematical structure modeled on Euclid's geometry. Spinoza's philosophy attracted believers such as Albert Einstein and much intellectual attention.\n\nLeibniz was the last of the great Rationalists who contributed heavily to other fields such as metaphysics, epistemology, logic, mathematics, physics, jurisprudence, and the philosophy of religion; he is also considered to be one of the last \"universal geniuses\". He did not develop his system, however, independently of these advances. Leibniz rejected Cartesian dualism and denied the existence of a material world. In Leibniz's view there are infinitely many simple substances, which he called \"monads\" (possibly taking the term from the work of Anne Conway).\n\nLeibniz developed his theory of monads in response to both Descartes and Spinoza, because the rejection of their visions forced him to arrive at his own solution. Monads are the fundamental unit of reality, according to Leibniz, constituting both inanimate and animate objects. These units of reality represent the universe, though they are not subject to the laws of causality or space (which he called \"well-founded phenomena\"). Leibniz, therefore, introduced his principle of pre-established harmony to account for apparent causality in the world.\n\nKant is one of the central figures of modern philosophy, and set the terms by which all subsequent thinkers have had to grapple. He argued that human perception structures natural laws, and that reason is the source of morality. His thought continues to hold a major influence in contemporary thought, especially in fields such as metaphysics, epistemology, ethics, political philosophy, and aesthetics.\n\nKant named his brand of epistemology \"Transcendental Idealism\", and he first laid out these views in his famous work \"The Critique of Pure Reason\". In it he argued that there were fundamental problems with both rationalist and empiricist dogma. To the rationalists he argued, broadly, that pure reason is flawed when it goes beyond its limits and claims to know those things that are necessarily beyond the realm of all possible experience: the existence of God, free will, and the immortality of the human soul. Kant referred to these objects as \"The Thing in Itself\" and goes on to argue that their status as objects beyond all possible experience by definition means we cannot know them. To the empiricist he argued that while it is correct that experience is fundamentally necessary for human knowledge, reason is necessary for processing that experience into coherent thought. He therefore concludes that both reason and experience are necessary for human knowledge. In the same way, Kant also argued that it was wrong to regard thought as mere analysis. \"In Kant's views, a priori concepts do exist, but if they are to lead to the amplification of knowledge, they must be brought into relation with empirical data\".\n\nRationalism has become a rarer label \"tout court\" of philosophers today; rather many different kinds of specialised rationalisms are identified. For example, Robert Brandom has appropriated the terms rationalist expressivism and rationalist pragmatism as labels for aspects of his programme in \"Articulating Reasons\", and identified linguistic rationalism, the claim that the content of propositions \"are essentially what can serve as both premises and conclusions of inferences\", as a key thesis of Wilfred Sellars.\n\nRationalism was criticized by William James for being out of touch with reality. James also criticized rationalism for representing the universe as a closed system, which contrasts to his view that the universe is an open system.\n\n\n\n"}
{"id": "34970247", "url": "https://en.wikipedia.org/wiki?curid=34970247", "title": "Respect for persons", "text": "Respect for persons\n\nRespect for persons is the concept that all people deserve the right to fully exercise their autonomy. Showing respect for persons is a system for interaction in which one entity ensures that another has agency to be able to make a choice.\n\nThis concept is usually discussed in the context of research ethics. It is one of the three basic principles of research ethics stated in the Belmont Report issued by the Office of Human Subject Research; it comprises two essential moral requirements: to recognize the right for autonomy and to protect individuals who are disadvantaged to the extent that they cannot practice this right.\n\nAn autonomous person is defined as an individual who is capable of self-legislation and is able to make judgments and actions based on his/her particular set of values, preferences, and beliefs. Respecting a person’s autonomy thus involves considering his/her choices and decisions without deliberate obstruction. It also requires that subjects be treated in a non-degrading manner out of respect for their dignity. In practice, respect for persons is operationalized by obtaining Informed Consent from all individuals who are going to be research subjects.\n\nThe standard case for applying respect for persons is when the person receiving the health intervention is of sound mind, fit to make personal decisions, and empowered to choose from various options. Other cases involve showing respect to people who for whatever reason are not free to choose among the typical range of options when making a decision.\n\nIn medical research ethics, the term Vulnerable Populations generally refers to individuals whose situations do not allow them to protect their own interests. The categories of individuals that constitute Vulnerable Populations are outlined under The Common Rule (45 CFR 46, Subparts A-D). These include individuals who are minors, prisoners, pregnant, physically handicapped, mentally disabled, old, economically disadvantaged, educationally disadvantaged, or subordinates in hierarchical groups (e.g. a soldier).\n\nThese individuals are entitled to protection, and additional ethical justification is needed to involve such populations in human subject studies. In such cases, a balance should be established between protecting subjects from exploitation and depriving these subjects of access to the potential benefits of research.\n\nReasons justifying the participation of these subjects would include that some studies could not be carried out without a vulnerable population. Another justification would be that the aim of the study is to gain knowledge to improve diagnosis, prevention or treatment of diseases associated specifically with that population.\n"}
{"id": "3255193", "url": "https://en.wikipedia.org/wiki?curid=3255193", "title": "Round-robin story", "text": "Round-robin story\n\nA round-robin story, or simply \"round robin,\" is a type of collaborative fiction or storytelling in which a number of authors write chapters of a novel or pieces of a story, in rounds. Round-robin novels were invented in the 19th century, and later became a tradition particularly in science fiction. In modern usage, the term often applies to collaborative fan fiction, particularly on the Internet, though it can also refer to friends or family telling stories at a sleepover, around a campfire, etc.\n\n\n"}
{"id": "46324244", "url": "https://en.wikipedia.org/wiki?curid=46324244", "title": "Skeletal changes of organisms transitioning from water to land", "text": "Skeletal changes of organisms transitioning from water to land\n\nInnovations conventionally associated with terrestrially first appeared in aquatic elpistostegalians such as \"Panderichthys rhombolepis\", \"Elpistostege watsoni\", and \"Tiktaalik roseae\". Phylogenetic analyses distribute the features that developed along the tetrapod stem and display a stepwise process of character acquisition, rather than abrupt. The complete transition occurred over a period of 25 million years beginning with the tetrapodomorph diversification in the Middle Devonian (380 myr).\n\nBy the Upper Devonian period, the fin-limb transition as well as other skeletal changes such as gill arch reduction, opercular series loss, mid-line fin loss, and scale reduction were already completed in many aquatic organisms. As aquatic tetrapods began their transition to land, several skeletal changes are thought to have occurred to allow for movement and respiration on land. Some adaptations required to adjust to non-aquatic life include the movement and use of alternating limbs, the use of pelvic appendages as sturdy propulsors, and the use of a solid surface at the organism’s base to generate propulsive force required for walking.\n\nThe Osteolepiformes and Elpistostegalia are two crown groups of rhipidistians with respect to the tetrapods. The development of skull roof and cheekbone patterns in these organisms match those found in the first tetrapods. Palatal and nasal skeletal features like choanae are present in these groups and are also observed in modern amphibians. This indicates that incipient air breathing was developed, as well as modification of the hyoid arch towards stapes development. These characteristics account for why osteichthyans are accepted as the sister group of tetrapods.\n\nThe elpistostegalid fish are considered the most apomorphic of fish in comparison to tetrapods. From well-preserved fossils, it is observed that they share a paltybasic skull with eye ridges, and external nares situated on the margin of the mouth. Development of eye ridges and flatting of the skull are also observed in primitive fossil amphibians and reptiles. The most likely reason for the traits to be adaptive was for their use in aerial vision above the waterline. The traits enabled animals to check area on land for safe spots if being chased by a predator in water, as well as being useful for searching for prey items above the water. The water-based lateral line system was used substantially by these aquatic tetrapods to detect danger from predators. Within the Osteichthyan diversification, there were no changes related to respiration in the transition as can be seen by the nasal region and palatal morphology in elpistostegalid fishes. The primary change from basic ostelepiform ancestors to the first elpistostegalid in the middle Devonian was to the pre-existing roof skulls.\n\nIn \"Elginerpeton pancheni\", a prototetrapod from the late Frasnian, basic tetrapod characteristics in the lower jaw and the cranium are observed. The taxon is believed to fill the gap between elpistostegalid fishes and well-preserved Devonian tetrapods. The \"Elginerpeton\" is considered more derived than the elpistostegalid fishes due to presence of paired fangs on the parasymphysial toothplate, a slender shaped anterior coronoid, and in the loss of the intracranial joint and coronoid fossa. The loss of the intercranial joint was a direct functional necessity to strengthen the broad and long platybasic skull when the animal was out of the water. The tubular lower jaw of the \"Elginerpeton\", compared to the flat-lamina jaw shape of fishes gave it superior cross-sectional force, required when not supported in an aquatic setting – allowing for opening of the mouth outside of water. The adaptation may also be interpreted as a specialization for buccopharyngeal breathing. It is speculated to be the first step towards aerial respiration in the transition from fish to tetrapod.\n\nIn the tetrapod and higher clades from the lower-middle Famennian there are several defining changes on the basis of anatomy of \"Ichthyostega\", \"Tulerpeton\", and \"Acanthostega\". In the cranium, there is a stapes derived from the hyomandibular of fishes; a single bilateral pair of nasal bones, and a fenestra ovalis in the otic capsule of the braincase. The opening of the otic wall of the braincase can be considered a paedomorphic feature for tetrapods and is linked to the stapes functionally. The stapes was thought to be just a structural support between the palate and the stapedial plate of the braincase. In the \"Acanthostega\", it is likely that due to the otic capsule of the brain case being mesial to the stapedial plate, sound was picked up from the palate or the otic notch to allow for rudimentary hearing. It was able to perceive vibrations by opening its mouth by way of the palate. Other factors that caused aquatic tetrapods to spend more time on land caused the development of terrestrial hearing with the development of a tympanum within an otic notch and developed by convergent evolution at least three times.\nThere was also a change in the dermal bones of the skull in the aquatic tetrapods. It involved the enlargement of the jugal, ceasing the contact of the maxilla with the squamosal and the single bilateral pair of nasal bones. The feature allows for a stronger bite as well as increasing the strength of the skull.\n\nFeeding on land is a completely different task than feeding in water. Water is much more dense and viscous compared to air, causing hunting techniques adapted in water to be less successful when applied on land. The main technique used in water is suction feeding and is used by most aquatic vertebrates. This technique does not function in air so animals use methods of overtaking prey with jaws followed by biting down. Transitional forms prior to fully developed terrestrial tetrapods such as \"Acanthostega\", are thought to have captured prey in the water. Large coronoid fangs are present in the fishes \"Eusthenopteron\", \"Panderichthys\", and \"Tiktaalik\", and the early tetrapod, \"Ventasega\". In \"Acanthostega\", which is more derived, the large teeth are absent. In \"Eusthenopetron\" and \"Panderichthys\", an ossified operculum is exhibited unlike in the \"Tiktaalik\", \"Ventastega\", and \"Acanthostega\". These differences as well as reductions of the gill chamber and changes in the nature of the lower jaw are hypothesized to indicate a reduced reliance on suction feeding in early tetrapods in comparison to osteolepiform fish. This morphological data is not enough however to prove that suction feeding was less used as the morphological changes have been found in fish that use the suction feeding mechanism.\n\nCranial sutures are indicators of skull function and morphologies can be linked to specific feeding modes. Transitional feeding changes can be observed by examining cross sectional morphology of a suture in taxa of the fish-tetrapod transition. Comparing positionally comparable sutures in extant fish allows for the creation of a sutural morphospace. The main cause of sutural deformation is caused by strain during feeding activity, most prominent with feeding mechanisms involving sucking a prey into the mouth. There is a tension anteriorly, and compression posteriorly strain patterns are observed in \"Polypterus\", a prey-sucking predator. In terrestrial tetrapod \"Phonerpeton\", there is compression between the frontals and parietals and a complex loading between the post parietals. There is no evidence of tensile strain in any sutures. \"Acanthostega\" fossil records demonstrate that no strain pattern was exhibited that relate to prey capture by means of suction. The load compression is similar to extant tetrapods. It is most likely that the organism captured prey by biting in the water or near the edge of the water. This finding indicates that the terrestrial mode of feeding first emerged in an aquatic environment.\n\nThe cranial endoskeleton of \"T. roseae\" shares derived features with tetrapods. There was a loss of opercular and extrascapular elements, enhancing head mobility in \"T. roseae\" compared to other tetrapodomorph fish. The formation of the neck allowed for locomotion in shallow waters. This environment allows for less motility compared to the three-dimensional space that fish are able to orient themselves in. The body of the organism in these environments would be fixed in the shallow pools with appendages planted on a substrate.\n\nIn the \"Acanthostega\" and \"Ichthyostega\", which are considered to be more derived than other basal aquatic tetrapods, the pectoral girdle is decoupled from the skull. There is also a loss of the dorsal pectoral girdle bones, which permits a large degree of movement for the shoulder. This allowed for a greater degree of movement, and is a necessity for improving aquatic maneuveurs and terrestrial locomotion. This could have been driven by the need to lift the head to aid aerial respiration by using nostrils and choanae.\n\nLimbs in vertebrates are occasionally organized into stylopod (relating to the humerus and femur), zeugopod (relating to the radius and tibia, along with associated structures) and autopod (relating to digits) categories, although anatomically, the evolutionary differences between these groups in early tetrapods tends to be vague.\nThe transition from fins to limbs occurred once an endoskeleton entered the base of the fin, as seen in today's lungfish. This is thought to have originated in the group Sarcopterygians, including osteolipiforms like \"Eusthenopteron\", due to the homology of the tetrapod forelimb and the osteolepiform fin endoskeleton.\n\n\"Acanthostega\" is a partially aquatic tetrapod with developed limbs that shares features common with the earlier tetrapods, \"Panderichthys\" and \"Eusthenopteron\". Like \"Panderichthys\", the humerus of \"Acanthostega\" is flattened dorso-ventrally, the intermedium terminates level with the radius, and the endoskeleton can be divided into stylopodium, zeugopodium and autopodium segments. Similar to \"Eusthenopteron\", the radials do not articulate with the radius on the distal end. \"Acanthostega\" also has a 1:2 ratio of humerus to radius and ulna, a feature seen in all tetrapods higher than \"Acanthostega\" on the phylogeny.\n\nUnlike \"Panderichthys\", \"Acanthostega\" hind limbs are at least the size of its fore limbs, if not larger. This development of larger limbs is required to physically support the organism during emergence from an aquatic setting to land. The humerus and femur of \"Acanthostega\" also contain evidence of greater development of the appendicular muscles compared to more aquatic tetrapods, hinting at the presence of digits.\n\nSimilarly, \"Ossinodus\" has two hindlimbs located bilaterally and proximodistally aymmetrical. Due to the presence of a small femur during juvenile development, this Carboniferous- period tetrapod is thought to be aquatic during juvenile development; only emerging onto land once it reaches adulthood. \"Ossinodus\" also has a broad, flat tibia, akin to \"Acanthostega\", and is thought to be only partially terrestrial.\n\nThe development of the pelvic region was crucial for the adaptation from water to land, yet some features of tetrapod locomotion are thought to have arose before the origin of digited limbs or the transition from water to land. The fossil record of early tetrapods shows evidence of distinct pelvic development occurring in osteolepiforms, further supporting osteolepiform ancestry of terrestrial tetrapods.\n\n\"Acanthostega\" has a large pelvis, with the iliac region articulating with the axial skeleton and a broad ischial plate. It has a sacrum; a fundamental skeletal feature that allows the organism to transfer force produced in its hindlimbs to its axial skeleton, and move in a terrestrial environment. A pubo-ischiadic symphysis is also observed, uniting the two pelvic halves.\n\nIn contrast, \"Protopterus annectens\" (a member of lungfish, thought to be a sister group to tetrapods) has a small, anatomically simpler pelvis, a derived limb endoskeleton and a lack of digits. Yet, it shares the ability to lift itself using a solid surface as a base with its pelvic region with \"Acanthostega\" and is also observed to move with tetrapod-like locomotion in an aquatic environment. This illustrates that a fundamental innovation in tetrapods is also found in a lower, sister taxon, in which members lack a sacrum.\n\n\"Acanthostega\" is the earliest example of a digitized tetrapod. The humerus and femur of \"Acanthostega\" contain evidence of greater development of the appendicular muscles compared to more aquatic tetrapods. \"Acanthostega\" has a total lack of dermal fin rays and displays the presence of two or more spool-shaped bones or cartilages articulating individually in antero-posterial sets on the distal end of its limbs. This feature can now be distinguished as digits instead of the endoskeletal radials seen in earlier tetrapods.\n\n\"Pederpes\", a tetrapod from the Early Carboniferous period, also has hindlimbs containing 5 digits that are rotated to face anteriorly. Unlike previous tetrapods, who have been only partially adapted to land, \"Pederpes\" has the novel ability to bend its limbs and propel itself forwards in a terrestrial setting. This is attributed to the symmetry of the digits and limbs in \"Pederpes\", allowing it to rotate its hindlimbs to an anteriorly facing position and propel itself from the edge of the foot when moving forward. This morphological development of bendable wrists and ankles can distinguish \"Pederpes\" the first true terrestrial tetrapod.\n"}
{"id": "655183", "url": "https://en.wikipedia.org/wiki?curid=655183", "title": "Sphere-world", "text": "Sphere-world\n\nThe idea of a sphere-world was constructed by Henri Poincaré who, while pursuing his argument for conventionalism (see philosophy of space and time), offered a thought experiment about a sphere with strange properties.\n\nPoincaré asks us to imagine a sphere of radius \"R\". The temperature of the sphere decreases from its maximum at the center to absolute zero at its extremity such that a body’s temperature at a distance \"r\" from the center is proportional to formula_1.\n\nIn addition, all bodies have the same coefficient of dilatation so every body shrinks and expands in similar proportion as they move about the sphere. To finish the story, Poincaré states that the index of refraction will also vary with the distance \"r\", in inverse proportion to formula_1.\n\nHow will this world look to inhabitants of this sphere? \n\nIn many ways it will look \"normal\". Bodies will remain intact upon transfer from place to place, as well as seeming to remain the same size (the Spherians would shrink along with them). The geometry, on the other hand, would seem quite different. Supposing the inhabitants were to view rods believed to be rigid, or measure distance with light rays. They would find that a geodesic is not a straight line, and that the ratio of a circle’s circumference to its radius is greater than formula_3.\n\nThese inhabitants would in fact determine that their universe is not ruled by Euclidean geometry, but instead by hyperbolic geometry.\n\nThis thought experiment is discussed in Roberto Torretti's book \"Philosophy of Geometry from Riemann to Poincaré\" and in Jeremy Gray's article \"Epistemology of Geometry\" in the Stanford Encyclopedia of Philosophy. This sphere-world is also described in Ian Stewart's book \"Flatterland\" (chapter 10, Platterland).\n\n"}
{"id": "12249109", "url": "https://en.wikipedia.org/wiki?curid=12249109", "title": "Sustainable automotive air conditioning", "text": "Sustainable automotive air conditioning\n\nSustainable automotive air conditioning is the subject of a debate – nicknamed the \"Cool War\" – about the next-generation refrigerant in car air conditioning. The Alliance for CO Solutions supports the uptake of carbon dioxide (CO) as a refrigerant in passenger cars, and the chemical industry is developing new chemical blends.\n\nThe Alliance and its supporters – scientists, NGOs and business leaders – urge the car industry to replace high global warming chemical substances with the natural refrigerant carbon dioxide (CO, R744/ R-744) in car cooling and heating. This, they argue, would lead to 10% less car emissions, and knock out 1% of total greenhouse gas emissions worldwide. If CO Technology is applied in other sectors, such as commercial and industrial refrigeration, heat pumps for water heating etc., it may even save up to 3% of the world’s greenhouse gases.\n\nOpponents of the Alliance claim that CO Technology is not cost-efficient and safe, hence seeking to postpone the global industry decision to be taken to develop new chemical blends instead.\n\nThe Cool War has emanated from the decision of the European Union to phase out the current high global warming refrigerant HFC-134a in car air conditioning from January 2011 onwards. To comply with the legislation, carmakers have to decide today on a new refrigerant, as they typically need 3–4 years to develop and introduce a new car platform including the air conditioning system. (It would take much less time to design merely a new air conditioning system to install in a new version of an existing model.) The current total value of the car air conditioning market is estimated to be $14.5 billion in 2007.\n\nThe Alliance for CO Solutions and its supporters agree that the refrigerant CO is:\n\n\nCO Technology requires the design of completely new high-pressure systems whereas so-called \"drop-in solutions\" (the adaptation of current systems to new substances) are potentially more cost-efficient. \n\nThe Alliance for CO Solutions claims, however that the initial costs of CO systems will be around €5 higher than drop-in solutions and that over a car’s life cycle, CO air conditioning systems will be more cost-efficient than any currently used or proposed new chemical blends. (see Arguments for CO).\n\n\nButane and propane are very flammable petroleum products; they are used as fuels for gas barbecue grills, disposable lighters, etc. Like gasoline, to which it chemically is closely related, propane has a tendency to explode if mixed with oxygen and ignited in an enclosed container.\n\nThe use of highly flammable hydrocarbon gases such as butane and propane as automotive refrigerants raises serious safety concerns. The EPA, in evaluating motor vehicle air conditioning substitutes for CFC-12 (Freon, or R-12) under its SNAP program, has classified as \"Unacceptable Substitutes\" other \"Flammable blend[s] of hydrocarbons\" by reason of \"insufficient data to demonstrate safety.\" The EPA defines \"Unacceptable\" in this context as \"illegal for use as a CFC-12 substitute in motor vehicle air conditioners\". All of the refrigerants which EPA approved for motor vehicle use in place of CFC-12 (as of 28 September 2006) contain no more than 4% total flammable hydrocarbons (butane, isobutane, and/or isopentane). Therefore, it appears unlikely, for safety reasons, that EPA will approve 'Greenfreeze' or similar hydrocarbon-based refrigerants for automotive use.\n\nIn September 2007, the German Association of the Automotive Industry (VDA) officially announced its decision to use CO as the refrigerant in next-generation air conditioning. Other carmakers from Europe and the rest of the world may follow the German lead.\n\nA working group at ACEA, the European carmakers’ association, was to be drafting a common position on the issue to be adopted across the whole industry by end-2007.\n\nHowever, on 9 April 2009, German public television channel ARD aired a report claiming that VDA members would be using loopholes in the law to avoid complying with the EU directive.\n\n\n\n"}
{"id": "20882689", "url": "https://en.wikipedia.org/wiki?curid=20882689", "title": "Sylvia Tamale", "text": "Sylvia Tamale\n\nSylvia Rosila Tamale is a Ugandan academic, and human rights activist in Uganda. She was the first female dean in the Law Faculty at Makerere University, Uganda.\n\nTamale received her Bachelor of Laws with honors from Makerere University, her Master of Laws from Harvard Law School, and her Doctor of Philosophy in sociology and feminist studies from the University of Minnesota in 1997. Tamale received her Diploma in Legal Practice from the Law Development Center, Kampala, in 1990, graduating at the top of her class.\n\nTamale has been a visiting professor at the African Gender Institute of the University of Cape Town and a visiting scholar at the University of Wisconsin. In 2003 she was condemned by Ugandan conservatives for proposing that gay men and lesbians be included in the definition of \"minority\". Tamale was the dean of the Faculty of Law and Jurisprudence at Makerere University in Kampala, Uganda, from 2004 to 2008.\n\nFrom 1993 until 1997, she received a Fulbright-MacArthur Scholarship to pursue her studies at Harvard. In 2003, she won the University of Minnesota Award for International Distinguished Leadership for her work at the university. In 2004, she was awarded the Akina Mama wa Afrika Award by Akina Mama wa Afrika, an international, Pan-African, non-governmental development organisation for African women based in the United Kingdom with its African headquarters in Kampala, Uganda. In 2004, she was recognized by several women's organisations in Uganda, for her for human rights activism.\n\nOn 28 October 2016, she became the first female lecturer to give a professorial inaugural lecture at Makerere University. Her lecture was entitled \"Nudity, Protests and the Law,\" inspired, in part, by the earlier-in-the-year nude protest of Stella Nyanzi at the university. In her speech, Tamale called for a revision of the Ugandan laws that discriminate against women.\n\nIn March 2018, Makerere University selected Dr Tamale to chair a select committee to investigate \"sexual harassment\" at the public institution of higher learning. The report of the committee is expected in May 2018.\n\nShe has spoken out in support of the traditional practice of labia stretching, arguing that any comparison to female genital mutilation is invalid.\n\n\n\n"}
{"id": "28545000", "url": "https://en.wikipedia.org/wiki?curid=28545000", "title": "Talk Reason", "text": "Talk Reason\n\nTalk Reason is a website dedicated to opposing creationism and promoting evolution. Talk Reason collects articles for this purpose and provides a forum to present them.\n\n\n"}
{"id": "390009", "url": "https://en.wikipedia.org/wiki?curid=390009", "title": "Thomas Luckmann", "text": "Thomas Luckmann\n\nThomas Luckmann (; October 14, 1927 – May 10, 2016) was an American-Austrian sociologist of German and Slovene origin who taught mainly in Germany. His contributions were central to studies in sociology of communication, sociology of knowledge, sociology of religion, and the philosophy of science.\n\nHe was born in Jesenice, then part of the Kingdom of Yugoslavia. His father was an Austrian industrialist, while his mother was from a Slovene family from Ljubljana. On his mother's side, he was the cousin of the Slovene poet Božo Vodušek. He grew up in a bilingual environment. In the family, they spoke both Slovene and German, and he attended Slovene-language schools in Jesenice until 1941, and then German ones.\n\nDuring World War II, in 1943, he and his mother moved to Vienna. In 1944 he was drafted for the army, joining the Luftwaffe where he served as a Luftwaffenhelfer. In 1945 he became a prisoner of war, and escaping after three months. He then settled in Vienna.\n\nLuckmann studied philosophy and linguistics at the University of Vienna and Innsbruck. In 1950 he married Benita Petkevic, with whom he moved to the United States, where he studied at The New School in New York City. The couple had three daughters.\n\nHe worked as a professor of Sociology at the University of Konstanz in Germany from 1970 to his retirement, and later professor emeritus.\n\nHe died at the age of 88 on May 10, 2016, at his home in Austria.\n\nLuckmann was a follower of the phenomenologically oriented school of sociology, established by the Austrian-American scholar Alfred Schütz. He contributed to the foundation of phenomenological sociology, the sociology of religion in modern societies, and the sociology of knowledge and communication.\n\nIn his works, he developed the theory of social constructionism, which argues that all knowledge, including the most basic common sense knowledge of everyday reality, is derived from and maintained by social interactions. Together with Peter L. Berger, he wrote the book \"The Social Construction of Reality\" in 1966. The book was an important part of the move in sociology, and partiuclarly the sociology of religion, away from the view of religion and religious values as central to the social order, arguing that social order is socially constructed by individuals and/or groups of individuals.\n\nIn 1982 he continued the work of Alfred Schütz, drawing on Schütz's notes and unfinished manuscripts to complete \"Structures of the Life-World\", published (posthumously for Schütz) in 1982.\n\nTogether with Richard Grathoff and Walter M. Sprondel, Luckmann founded the Social Science Archive Konstanz (also known as the Alfred Schütz Memorial Archives).\n\nLuckmann was a member of the Slovenian Academy of Sciences and Arts and held honorary doctorates from the Universities of Linköping, Ljubljana, Trier and Buenos Aires.\n\nIn 1998 he was awarded an honorary doctorate from the Norwegian University of Science and Technology (NTNU).\n\nIn 2004 Luckmann became an honorary member of the Slovenian Sociological Association. The German Sociological Association awarded him a prize for his outstanding lifetime contribution to sociology at its 2002 Congress, and Luckmann became an honorary member in 2016.\n\n\n\n\n"}
{"id": "1080292", "url": "https://en.wikipedia.org/wiki?curid=1080292", "title": "Two Concepts of Liberty", "text": "Two Concepts of Liberty\n\n\"Two Concepts of Liberty\" was the inaugural lecture delivered by the liberal philosopher Isaiah Berlin before the University of Oxford on 31 October 1958. It was subsequently published as a 57-page pamphlet by Oxford at the Clarendon Press. It also appears in the collection of Berlin's papers entitled \"Four Essays on Liberty\" (1969) and was more recently reissued in a collection entitled simply \"Liberty\" (2002).\n\nThe essay, with its analytical approach to the definition of political concepts, re-introduced the study of political philosophy to the methods of analytic philosophy. It is also one of Berlin's first expressions of his ethical ontology of value-pluralism. Berlin defined negative liberty (as the term \"liberty\" was used by Thomas Hobbes ) as the absence of coercion or interference with agents' possible private actions, by an exterior social-body. He also defined it as a comparatively recent political ideal, which re-emerged in the late 17th century, after its slow and inarticulate birth in the Ancient doctrines of Antiphon the Sophist, the Cyrenaic discipleship, and of Otanes after the death of pseudo-Smerdis. In an introduction to the essay, Berlin writes: \n\"As for Otanes, he wished neither to rule nor to be ruled—the exact opposite of Aristotle's notion of true civic liberty. ... [This ideal] remains isolated and, until Epicurus, undeveloped ... the notion had not explicitly emerged\".\n\nPositive liberty may be understood as self-mastery, and includes one's having a role in choosing who governs the society of which one is a part. Berlin traced positive liberty from Aristotle's definition of citizenship, which is historically derived from the social role of the freemen of classical Athens: it was, Berlin argued, the liberty in choosing their government granted to citizens, and extolled, most famously, by Pericles. Berlin granted that both concepts of liberty represent valid human ideals, and that both forms of liberty are necessary in any free and civilised society.\n\nFor Berlin, negative liberty represents a different, and sometimes contradictory, understanding of the concept of liberty, which needs to be carefully examined. Its later proponents (such as Tocqueville, Constant, Montesquieu, John Locke, David Hume and John Stuart Mill, who accepted Chrysippus' understanding of self-determination) insisted that constraint and discipline were the antithesis of liberty and so were (and are) less prone to confusing liberty and constraint in the manner of rationalists and the philosophical harbingers of totalitarianism. This concept of negative liberty, Berlin argued, constitutes an alternative, and sometimes even opposed, concept to positive liberty, and one often closer to the intuitive modern usage of the word. Berlin considered negative liberty one of the distinguishing concepts of modern liberalism and observed\n\nIsaiah Berlin notes that historically positive liberty has proven particularly susceptible to rhetorical abuse; especially from the 18th century onwards, it has either been paternalistically re-drawn from the third-person, or conflated with the concept of negative liberty and thus disguised underlying value-conflicts.\n\nBerlin contended that under the influence of Plato, Aristotle, Jean-Jacques Rousseau, Immanuel Kant, and G. W. F. Hegel, modern political thinkers often conflated positive liberty with rational action, based upon a rational knowledge to which, it is argued, only a certain elite or social group has access. This rationalist conflation was open to political abuses, which encroached on negative liberty, when such interpretations of positive liberty were, in the nineteenth century, used to defend nationalism, paternalism, social engineering, historicism, and collective rational control over human destiny. Berlin argued that, following this line of thought, demands for freedom paradoxically could become demands for forms of collective control and discipline—those deemed necessary for the \"self-mastery\" or \"self-determination\" of nations, classes, democratic communities, and even humanity as a whole. There is thus an elective affinity, for Berlin, between positive liberty, when it is rhetorically conflated with goals imposed from the third-person that the individual is told they \"should\" rationally desire, and the justifications for political totalitarianism, which contrary to value-pluralism, presupposed that values exist in Pythagorean harmony. \n\nBerlin did not argue that the concept of positive liberty should be rejected—on the contrary, he recognised it as one human value among many, and one necessary to any free society. He argued that positive liberty was a genuine and valuable version of liberty, so long as it was identified with the autonomy of individuals, and not with the achievement of goals that individuals 'ought to' 'rationally' desire. Berlin argued, rather, that these differing concepts showed the plurality, and incompatibility of human values, and the need to analytically distinguish and trade-off between, rather than conflate, them.\n\nThus, Berlin offers in his \"Two Concepts of Liberty\" essay, \"Where it is to be drawn is a matter of argument, indeed of haggling. Men are largely interdependent, and no man's activity is so completely private as never to obstruct the lives of others in any way. 'Freedom for the pike is death for the minnows'; the liberty of some must depend on the restraint of others. Freedom for an Oxford don, others have been known to add, is a very different thing from freedom for an Egyptian peasant.\"\n\n\n"}
{"id": "37198049", "url": "https://en.wikipedia.org/wiki?curid=37198049", "title": "World War II looting of Poland", "text": "World War II looting of Poland\n\nThe looting of Polish cultural artifacts during World War II was carried out by Nazi Germany and the Soviet Union side by side after the invasion of Poland of 1939. A significant portion of Poland's cultural heritage, estimated at about half a million art objects, was plundered by the occupying powers. Cataloged pieces are still occasionally recovered elsewhere and returned to Poland.\n\nPriceless pieces of art still considered missing or found in Russian museums include works by Canaletto, Anna Bilińska-Bohdanowiczowa, Józef Brandt, Lucas Cranach the Elder, Lucas Cranach the Younger, Albrecht Dürer, Anthony van Dyck, Hans Holbein the Younger, Jacob Jordaens, Frans Luycx, Jacek Malczewski, Raphael, Rembrandt van Rijn, Peter Paul Rubens, Henryk Siemiradzki, Veit Stoss, Alfred Wierusz-Kowalski, Leon Wyczółkowski, Jan Matejko, Henri Gervex, Ludwig Buchhorn, Józef Simmler, Henri-Pierre Danloux, Jan Miense Molenaer and many others.\n\nAs part of the efforts to locate and retrieve the missing pieces of art, the Ministry of Culture and National Heritage founded the Database of War Losses, as of 2013 containing over 63,000 entries. The list is periodically sent to over 100 auction houses around the world, published by the Ministry and also submitted to the National Institute of Museology and Collections Protection, Polish embassies, and the Central Registry of Information on Looted Cultural Property 1933-1945 (lootedart.com). In addition, the Ministry also founded the Lost Museum website, a virtual museum containing historic photographs of many pieces of art still missing.\n\nAt the beginning of the 1939 invasion of Poland, the Polish interwar government attempted to conceal the nation's most valued cultural heritage such as the royal treasures of the Wawel Castle in Kraków. The royal accessories including the Jagiellonian tapestries were secretly shipped to Western Europe and then to Canada among other places. At the end of the war, two parallel Polish governments, the Western-supported Polish government-in-exile and the Soviet-backed government in Communist Poland laid claims to these national treasures. The cultural artifacts were released by Canada to the People's Republic of Poland in February 1961.\n\nFollowing the German invasion of Poland in September 1939 and the occupation of Poland by German forces, the Nazi regime attempted to suppress Polish culture. As part of that process, the Nazis confiscated Polish national heritage assets and much private property. Acting on the legal decrees of October 19 and December 16 (\"Verordnung über die Beschlagnahme Kunstgegeständen im Generalgouvernement\"), several German agencies began the process of looting Polish museums and other collections, ostensibly considered necessary for the \"securing\" of German national interests.\n\nThousands of art objects were plundered, as the Nazis carried out a plan put in place before the start of hostilities. The looting was supervised by experts of the \"SS\"-\"Ahnenerbe\", \"Einsatzgruppen\" units, who were responsible for art; and, by experts of \"Haupttreuhandstelle Ost\", who were responsible for confiscating businesses and more mundane objects. Nazi officials responsible for carrying-out the plan included Hans Posse, Josef Mühlmann and his half-brother Kajetan (a.k.a. Kai, both from the SS), overseen by Dagobert Frey, an SS historian originally also from Austria, selected by Berlin to validate Poland as a \"Teutonic land\" without Jews. In addition to the official looting by Nazi authorities, some looting was also carried out by individuals acting on their own initiative; in fact Mühlmann complained as early as on October 6, 1939, that many items he was tasked to secure had already been moved or simply stolen. While the Nazis kept extensive documentation of newly acquired looted art pieces, the system was not foolproof, and they lost track of much of the looted goods during the increasingly haphazard evacuation of Polish territories in 1944.\n\nMost of the important art pieces had been \"secured\" by the Nazis within six months of September 1939; by the end of 1942, German officials estimated that \"over 90%\" of the art previously in Poland was in their possession. Some art was shipped to German museums, such as the planned \"Führermuseum\" in Linz, while other art became the private property of Nazi officials. In 1940, Hitler received a \"gift\" from Hans Frank, governor of occupied Poland - a collection, prepared by Mühlmann, of 521 of the most valuable art pieces. Frank aided by \"Oberführer\" Mühlmann, an art connoisseur, amassed a large collection of Polish art pieces. The total cost of the Nazi theft and destruction of Polish art is estimated at 11.14 billion dollars (value in 2001 dollars).\n\nOver 516,000 individual art pieces were taken. The exact number is uncertain as not all art was registered, and much of that documentation was lost as well. Assessment of losses began during World War II under the auspices of the Polish government in exile and the Polish Underground State; in 1944, Karol Estreicher published in London the first work on this subject, \"Cultural Losses of Poland\". A 2010 estimate gave a figure of 75% for the percentage of cultural heritage lost by Poland during the war (though that estimate covers both destroyed and lost cultural heritage). The looted art included 11,000 paintings by Polish painters; 2,800 paintings by other European painters; 1,400 sculptures; 75,000 manuscripts; 25,000 maps, 22,000 books printed before 1800 (\"starodruki\"); 300,000 graphics, and hundreds of thousands of other items of artistic and historical value. The number of looted or destroyed books is estimated at 1.5 million to as high as 15 or 22 million. Even exotic animals were taken from the zoos.\n\nDuring the genocidal campaign against the Polish Jews culminating in the \"Aktion Reinhard\" of 1942, extortion and mass looting became part of German economics, not only Nazi policy towards Poland's heritage. \n\nAfter the Soviet Union invaded Poland on September 17, 1939, it similarly engaged in the looting and destruction of the Polish cultural heritage. It is estimated that soon after the invasion, about half of Polish museums and similar public institutions were dismantled in the territories occupied by the Soviets. Many items were shipped to Soviet museums such as the Moscow Museum of History and the Central Anti-Religious Museum (also in Moscow). Other collections were simply done away with. For instance, during the liquidation of the Poland's \"Lwów Historical Museum\" in the early 1940, its holdings were transported to the basement of the Black (\"Czarna\") Kamienica \"(pictured)\", away from public eye, and deliberately destroyed there.\n\nFollowing the Soviet advance across the German-occupied Polish lands, the looting and plunder of anything of value continued up to 1947 even though the looted territories were theoretically assigned to its own ally, the communist Poland already. The Soviet forces engaged in particularly extensive plunder in the former eastern territories of Germany that were to be transferred to Poland, stripping them of any piece of equipment left behind. Even the Polish Communists felt uneasy about the scope of their crimes. In 1945, the future Chairman of the Polish Council of State, Gen. Aleksander Zawadzki, worried that \"raping and looting of the Soviet army would provoke a civil war\"\n\nThe operations of these \"war trophy brigades\" were regulated by detailed orders issued by Soviet vice-minister of defence Nikolai Bulganin in early 1946. Until 1948 these brigades sent at least 239'000 freight cars to USSR transporting natural resources, complete factories and individual machines. Town of Bydgoszcz lost 30 complete factories and 250 ships, from Grudziądz the army confiscated all machinery from factories, regardless of their size. From Toruń all gristmills were taken, causing temporary deficit of bread. Blachownia Śląska lost a large, German-built synthetic fuel producing installation, transported to USSR on 10,000 train cars. A similar production line in Police was transported using 14,000 cars. Gliwice lost a pipe factory, Bobrek and Łabędy - iron furnaces. Complete power stations were taken from Miechowice, Zabrze, Zdzieszowice, Mikulczyce, Blachownia Śląska i Chełmsk Śląski. Smaller industries were also confiscated in Sosnowiec, Dąbrowa Górnicza, Częstochowa, Zgoda, Chorzów, Siemianowice, Poznań, Bydgoszcz, Grudziądz, Toruń, Inowrocław, Włocławek, Chojnice, Łódź, Dziedzice and Oświęcim.\n\nFarming animals were also significant target of looting: until 1 September 1945 the Red Army confiscated 506,000 cows, 114,000 sheep and 206,000 horses. In February 1945 alone over 72,000 tons of sugar was taken. In Toruń region alone 14,000 tons of grain, 20,000 of potatoes and 21,000 beetroots were taken during that period. These number represent looting alone, as the Polish government also supplied food to the Red Army officially at that time (150,000 tons of grains, 250,000 tons of potatoes, 25,000 tons of meat and 100,000 tons of straws).\n\nIndividual Red Army soldiers were also allowed to send home \"war trophies\", with the weight depending on their rank, which resulted in widespread looting of private houses of anything valuable, including food, clothes, shoes, radios, jewelry, utensils, clothes, bicycles, and even ceramic toilet bowls. Scale of individual looting can be estimated by the example of Russian town of Kursk, which received only 300 personal parcels from soldiers in January 1945 but till May their number reached 87,000.\n\nAfter these transports were finished, the Red Army also started looting the train infrastructure itself—repair yards, signalling installations and the rails themselves: around 5,500 km of rails were looted.\n\nIn 1946 the scale of looting was estimated by Polish authorities at 2.375 billions of 1938 dollars (equivalent of $54 billion in 2015 dollars).\n\nAfter the war, the Polish Ministry of Culture and Art took over efforts to compile a list of lost art, locate it and recover it. The Bureau of Revindication and Damages (\"Biuro Rewindykacji i Odszkodowań\") operated from 1945 to 1951. The realities of the Cold War made retrieval of looted cultural heritage difficult, and it was only in the 1980s and 1990s that the situation changed. In 1991, a new body was formed for that purpose, the Bureau of the Government Representative for Polish Cultural Heritage Abroad (\"Biuro Pełnomocnika Rządu ds. Polskiego Dziedzictwa Kulturalnego Za Granicą\"), operating at the Ministry of Culture and Art. In 1999, the initiative received support from the Polish Ministry of Foreign Affairs. Once a looted piece of art is located, the Polish government issues a request for its restoration, and as noted on the Ministry website, all requests to date have been successful. , the Ministry listed 30 objects which had been retrieved in the years 2001-2012. Among the recovered art is the Aleksander Gierymski's \"Jewish Woman\"—found by surprise at the \"Eva Aldag\" auction house in Buxtehude in November 2010, returned to National Museum, Warsaw at the end of July 2011 and restored.\nOn 1 August 2012, the Polish Ministry of Foreign Affairs announced that one of the most famous pieces of Polish art, Raphael's painting \"Portrait of a Young Man\", had been found \"in a bank vault in an undisclosed location\"; a ministry spokesman was confident that the painting would eventually be returned to Poland. In April 2014, Francesco Guardi's \"Palace Stairs\" was recovered.\n\nCurrently, Poland is planning to build a virtual museum, the Lost Museum (), which would feature the lost art.\n\nBoth Germany and the countries of the former Soviet Union still have much Polish material looted during World War II. In particular, recovering looted art from the former states of the Soviet Union, such as Russia, is proving difficult.\n\n\n"}
