{"id": "15881367", "url": "https://en.wikipedia.org/wiki?curid=15881367", "title": "Abdul Rahman al-Lahim", "text": "Abdul Rahman al-Lahim\n\nAbdul Rahman Al-Lahim (alternatively al-Lahem) (born 1971) is a Saudi human rights lawyer active in defending the civil rights of Saudi citizens.\n\nAl-Lahim was born in the deeply conservative Qassim region. Until the late 1990s, Al-Lahim, who holds a degree in sharia, was an Arabic teacher as well as an Islamist, active in As-Sahwa al-Islamiyya. He is married and the father of two.\nHe became committed to civil rights while attending \"Sharia school\" in Imam Muhammad ibn Saud Islamic University in Riyadh. According to analysts of his work, Al-Lahim is effective because \"He not only crafts effective legal arguments, but he also understands the conservative forces that hold the gavel.\"\n\nOn 6 November 2004, Al-Lahim was arrested by Saudi authorities for defending three reform activists on trial in Saudi Arabia charged with \"issuing statements and collecting as many signatures as possible on petitions\" calling for reforms in the Kingdom as well as of calling for the adoption of a constitutional monarchy and \"using Western terminology\" in demanding political reforms. All three activists and Al-Lahem were released in early August 2005 at the intervention of King Abdullah.\n\nIn 2005, he defended a high school teacher, Mohammad Al-Harbi, who had been sentenced to a punishment of 750 public lashes for mocking religion by speaking out against terrorism. Al-Harbi later received a royal pardon and all charges were dropped.\n\nIn 2007, Al-Lahim also defended the rape victim known as the \"Qatif girl\" from a sentence of 200 public lashes and faced disbarment for taking her case. He was suspended from the case as a result of the appeal against the punishment and his licence, granted to Saudi lawyers by the ministry of justice, was revoked. He was charged with criticizing the judiciary and conducting activist campaigns in the media. Later his license was returned to him. Human rights organization Amnesty International criticized the persecution of al-Lahem.\n\nA travel ban has been imposed against Al-Lahim since 2004 and protested by human right organizations.\n\nIn 2008, Al-Lahim was awarded The American Bar Association’s 2008 International Human Rights Lawyer Award in Vienna, although he was unable to attend due to his travel ban.\n\nHe started writing a column for Okaz Newspaper in April 2016.\n\n"}
{"id": "35553201", "url": "https://en.wikipedia.org/wiki?curid=35553201", "title": "American gentry", "text": "American gentry\n\nThe American gentry were members of the American upper classes, particularly early in the settlement of the United States.\n\nThe Colonial American use of \"gentry\" followed the British usage (i.e., landed gentry) before the independence of the United States. The Southern plantation was commonly evidenced in land holdings by estate owners in Virginia, Maryland and the Carolinas. North of Maryland, there were few large compatible rural estates, except in the Dutch domains in the Hudson Valley of New York.\n\nThe families of Virginia (see First Families of Virginia) who formed the Virginia gentry class, such as General Robert E. Lee's ancestors, were among the earliest settlers in Virginia. Lee's family of Stratford Hall was considered among the oldest of the Virginia gentry class. Lee's family is one of Virginia's first families, originally arriving in Virginia from England in the early 1600s with the arrival of Richard Lee I, Esq., \"the Immigrant\" (1618–64), from the county of Shropshire. His mother grew up at Shirley Plantation, one of the most elegant homes in Virginia. His maternal great-great grandfather, Robert \"King\" Carter of Corotoman, was the wealthiest man in the colonies when he died in 1732. \n\nThomas Jefferson, the patron of American agrarianism, wrote in his \"Notes on Virginia\" (1785), \"Those who labor in the earth are the chosen people of God, if He ever had a chosen people, whose breasts He has made His peculiar deposit for substantial and genuine virtue.\" Jefferson who spent much of his childhood at Tuckahoe Plantation was a great-grandson of William Randolph a colonist and land owner who arrived in Virginia from England in the mid 1600s and played an important role in the history and government of the English colony of Virginia.\n\nGeorge Washington, the first president of the United States was also the wealthiest man to ever hold the office until the election of Donald Trump in 2016 according to valuations of his putative assets. He was a commercial farmer much interested in innovations, and happily quit his public duties in 1783 and again in 1797 to manage his plantation at Mount Vernon. Washington lived an upper-class lifestyle—fox hunting was a favorite leisure activity enjoyed by gentry, worldwide. Like most planters in Virginia, Washington imported luxury items and other fine wares from England. He paid for them by exporting his tobacco crop. \n\nExtravagant spending and the unpredictability of the tobacco market meant that many Virginia planters financial resources were unstable. Thomas Jefferson was deeply in debt when he died and his heirs were forced to sell Monticello to cover his debts. In 1809, Henry Lee III went bankrupt and served one year in debtors' prison in Montross, Virginia; his son, Robert E. Lee was two years old at the time.Despondent and nearly broke, William Byrd III of Westover Plantation committed suicide in 1777. \n\nWood notes that \"Few members of the American gentry were able to live idly off the rents of tenants as the English landed aristocracy did.\" Some landowners, especially in the Dutch areas of upstate New York, leased out their lands to tenants, but generally —\"Plain Folk of the Old South\"— ordinary farmers owned their cultivated holdings.\n\nFirst Families of Virginia originated with colonists from England who primarily settled at Jamestown and along the James River and other navigable waters in the Colony of Virginia during the 17th century. As there was a propensity to marry within their narrow social scope for many generations, many descendants bear surnames which became common in the growing colony.\n\nMany of the original English colonists considered members of the First Families of Virginia migrated to the Colony of Virginia during the English Civil War and English Interregnum period (1642–1660). Royalists left England on the accession to power of Oliver Cromwell and his Parliament. Because most of Virginia's leading families recognized Charles II as King following the execution of Charles I in 1649, Charles II is reputed to have called Virginia his \"Old Dominion\", a nickname that endures today. The affinity of many early aristocratic Virginia settlers for the Crown led to the term \"distressed Cavaliers\", often applied to the Virginia oligarchy. Many Cavaliers who served under King Charles I fled to Virginia. Thus, it came to be that FFVs often refer to Virginia as \"Cavalier Country\". These men were offered rewards of land, etc., by King Charles II, but they had settled Virginia and so remained in Virginia.\n\nMost of such early settlers in Virginia were so-called \"Second Sons\". Primogeniture favored the first sons' inheriting lands and titles in England. Virginia evolved in a society of second or third sons of English aristocracy who inherited land grants or land in Virginia. They formed part of the southern elite in America.\n\nMany of the great Virginia dynasties traced their roots to families like the Lees and the Fitzhughs, who traced lineage to England's county families and baronial legacies. But not all: even the most humble Virginia immigrants aspired to the English manorial trappings of their \"betters.\" Virginia history is not the sole province of English aristocrats. Such families as the Shackelfords, who gave their name to a Virginia hamlet, rose from modest beginnings in Hampshire to a place in the Virginia firmament based on hard work and \"smart\" marriages. Some families like the Gilliams arrived in Virginia in the 17th century as indentured servants. By the late 18th century through hard work and \"smart\" marriages the Gilliam family had risen from servitude to landed gentry having amassed several large plantations including Weston Manor. Families such as the Mathews from the later Scotch-Irish immigration also formed political dynasties in Old Virginia. At the same time, other once-great families were decimated not only by the English Civil War, but also by the enormous power of the London merchants to whom they were in debt and who could move markets \"with the stroke of a pen.\"\n\nThe Colonial families of Maryland were the leading families in the Province of Maryland. Several also had interests in the Colony of Virginia, and the two are sometimes referred to as the Chesapeake Colonies. Many of the early settlers came from the West Midlands in England, although the Maryland families were composed of a variety of European nationalities, e.g., French, Irish, Welsh, Scottish, and Swedish, in addition to English.\n\nThe Carroll family is an example of a prominent political family from Maryland, of Irish descent and origin in the ancient kingdom of Éile, commonly anglicized Ely, as a branch of the ruling O'Carroll family. Another is the Mason family of Virginia, who descended from the progenitor of the Mason family, George Mason I, a Cavalier member of the Parliament of England born in Worcestershire, England. The Riggin family of Maryland who also had holdings on the Eastern Shore of Virginia is of Irish descent and origin in the ancient kingdom of Meath. The surname is an anglicization of the Gaelic Ó Riagáin. The Ó Riagáins were one of the families that made up the Four Tribes of Tara, an alliance of powerful clans. The Four Tribes were major defenders of Ireland against Viking invasions during the 8th and 9th centuries. The Ó Riagáins were chiefs of Hy-Riagain, now the barony of Tinnehinch in the Queen's County.\n\nCharles I of England granted the province palatinate status under Cecilius Calvert, 2nd Baron Baltimore. The foundational charter created an aristocracy of lords of the manor for Maryland. Maryland was uniquely created as a colony for Catholic aristocracy and landed gentry, but Anglicanism eventually came to dominate, partly through influence from neighboring Virginia.\n\n"}
{"id": "68753", "url": "https://en.wikipedia.org/wiki?curid=68753", "title": "Attention", "text": "Attention\n\nAttention is the behavioral and cognitive process of selectively concentrating on a discrete aspect of information, whether deemed subjective or objective, while ignoring other perceivable information. It is a state of arousal. It is the taking possession by the mind in clear and vivid form of one out of what seem several simultaneous objects or trains of thought. Focalization, the concentration of consciousness, is of its essence. Attention has also been described as the allocation of limited cognitive processing resources.\n\nAttention remains a major area of investigation within education, psychology, neuroscience, cognitive neuroscience, and neuropsychology. Areas of active investigation involve determining the source of the sensory cues and signals that generate attention, the effects of these sensory cues and signals on the tuning properties of sensory neurons, and the relationship between attention and other behavioral and cognitive processes like working memory and psychological vigilance. A relatively new body of research, which expands upon earlier research within psychopathology, is investigating the diagnostic symptoms associated with traumatic brain injury and its effects on attention. Attention also varies across cultures.\n\nThe relationships between attention and consciousness are complex enough that they have warranted perennial philosophical exploration. Such exploration is both ancient and continually relevant, as it can have effects in fields ranging from mental health and the study of disorders of consciousness to artificial intelligence and its domains of research and development.\n\nPrior to the founding of psychology as a scientific discipline, attention was studied in the field of philosophy. Thus, many of the discoveries in the field of attention were made by philosophers. Psychologist John B. Watson calls Juan Luis Vives the father of modern psychology because, in his book \"De Anima et Vita\" (\"The Soul and Life\"), he was the first to recognize the importance of empirical investigation. In his work on memory, Vives found that the more closely one attends to stimuli, the better they will be retained.\n\nBy the 1990s, psychologists began using positron emission tomography (PET) and later functional magnetic resonance imaging (fMRI) to image the brain while monitoring tasks involving attention. Because this expensive equipment was generally only available in hospitals, psychologists sought cooperation with neurologists. Psychologist Michael Posner (then already renowned for his seminal work on visual selective attention) and neurologist Marcus Raichle pioneered brain imaging studies of selective attention. Their results soon sparked interest from the neuroscience community, which had until then focused on monkey brains. With the development of these technological innovations, neuroscientists became interested in this type of research that combines sophisticated experimental paradigms from cognitive psychology with these new brain imaging techniques. Although the older technique of electroencephalography (EEG) had long been used to study the brain activity underlying selective attention by cognitive psychophysiologists, the ability of the newer techniques to actually measure precisely localized activity inside the brain generated renewed interest by a wider community of researchers.\n\nIn cognitive psychology there are at least two models which describe how visual attention operates. These models may be considered loosely as metaphors which are used to describe internal processes and to generate hypotheses that are falsifiable. Generally speaking, visual attention is thought to operate as a two-stage process. In the first stage, attention is distributed uniformly over the external visual scene and processing of information is performed in parallel. In the second stage, attention is concentrated to a specific area of the visual scene (i.e., it is focused), and processing is performed in a serial fashion.\n\nThe first of these models to appear in the literature is the spotlight model. The term \"spotlight\" was inspired by the work of William James, who described attention as having a focus, a margin, and a fringe. The focus is an area that extracts information from the visual scene with a high-resolution, the geometric center of which being where visual attention is directed. Surrounding the focus is the fringe of attention, which extracts information in a much more crude fashion (i.e., low-resolution). This fringe extends out to a specified area, and the cut-off is called the margin.\n\nThe second model is called the zoom-lens model and was first introduced in 1986. This model inherits all properties of the spotlight model (i.e., the focus, the fringe, and the margin), but it has the added property of changing in size. This size-change mechanism was inspired by the zoom lens one might find on a camera, and any change in size can be described by a trade-off in the efficiency of processing. The zoom-lens of attention can be described in terms of an inverse trade-off between the size of focus and the efficiency of processing: because attentional resources are assumed to be fixed, then it follows that the larger the focus is, the slower processing will be of that region of the visual scene, since this fixed resource will be distributed over a larger area. It is thought that the focus of attention can subtend a minimum of 1° of visual angle, however the maximum size has not yet been determined.\n\nA significant debate emerged in the last decade of the 20th century in which Treisman's 1993 Feature Integration Theory (FIT) was compared to Duncan and Humphrey's 1989 attentional engagement theory (AET). FIT posits that \"objects are retrieved from scenes by means of selective spatial attention that picks out objects' features, forms feature maps, and integrates those features that are found at the same location into forming objects.\" Duncan and Humphrey's AET understanding of attention maintained that \"there is an initial pre-attentive parallel phase of perceptual segmentation and analysis that encompasses all of the visual items present in a scene. At this phase, descriptions of the objects in a visual scene are generated into structural units; the outcome of this parallel phase is a multiple-spatial-scale structured representation. Selective attention intervenes after this stage to select information that will be entered into visual short-term memory.\" The contrast of the two theories placed a new emphasis on the separation of visual attention tasks alone and those mediated by supplementary cognitive processes. As Rastophopoulos summarizes the debate: \"Against Treisman's FIT, which posits spatial attention as a necessary condition for detection of objects, Humphreys argues that visual elements are encoded and bound together in an initial parallel phase without focal attention, and that attention serves to select among the objects that result from this initial grouping.\"\n\nIn the twentieth century, the pioneering research of Lev Vygotsky and Alexander Luria led to the three-part model of neuropsychology defining the working brain as being represented by three co-active processes listed as Attention, Memory, and Activation. Attention is identified as one of the three major co-active processes of the working brain. A.R. Luria published his well-known book \"The Working Brain\" in 1973 as a concise adjunct volume to his previous 1962 book \"Higher Cortical Functions in Man\". In this volume, Luria summarized his three-part global theory of the working brain as being composed of three constantly co-active processes which he described as the; (1) Attention system, (2) Mnestic (memory) system, and (3) Cortical activation system. The two books together are considered by Homskaya's account as \"among Luria's major works in neuropsychology, most fully reflecting all the aspects (theoretical, clinical, experimental) of this new discipline.\" The product of the combined research of Vygotsky and Luria have determined a large part of the contemporary understanding and definition of attention as it is understood at the start of the 21st-century.\n\nMultitasking can be defined as the attempt to perform two or more tasks simultaneously; however, research shows that when multitasking, people make more mistakes or perform their tasks more slowly. Attention must be divided among all of the component tasks to perform them. In divided attention, individuals attend or give attention to multiple sources of information at once at the same time or perform more than one task.\n\nOlder research involved looking at the limits of people performing simultaneous tasks like reading stories, while listening and writing something else, or listening to two separate messages through different ears (i.e., dichotic listening). Generally, classical research into attention investigated the ability of people to learn new information when there were multiple tasks to be performed, or to probe the limits of our perception (c.f. Donald Broadbent). There is also older literature on people's performance on multiple tasks performed simultaneously, such as driving a car while tuning a radio or driving while telephoning.\n\nThe vast majority of current research on human multitasking is based on performance of doing two tasks simultaneously, usually that involves driving while performing another task, such as texting, eating, or even speaking to passengers in the vehicle, or with a friend over a cellphone. This research reveals that the human attentional system has limits for what it can process: driving performance is worse while engaged in other tasks; drivers make more mistakes, brake harder and later, get into more accidents, veer into other lanes, and/or are less aware of their surroundings when engaged in the previously discussed tasks.\n\nThere has been little difference found between speaking on a hands-free cell phone or a hand-held cell phone, which suggests that it is the strain of attentional system that causes problems, rather than what the driver is doing with his or her hands. While speaking with a passenger is as cognitively demanding as speaking with a friend over the phone, passengers are able to change the conversation based upon the needs of the driver. For example, if traffic intensifies, a passenger may stop talking to allow the driver to navigate the increasingly difficult roadway; a conversation partner over a phone would not be aware of the change in environment.\n\nThere have been multiple theories regarding divided attention. One, conceived by Kahneman, explains that there is a single pool of attentional resources that can be freely divided among multiple tasks. This model seems to be too oversimplified, however, due to the different modalities (e.g., visual, auditory, verbal) that are perceived. When the two simultaneous tasks use the same modality, such as listening to a radio station and writing a paper, it is much more difficult to concentrate on both because the tasks are likely to interfere with each other. The specific modality model was theorized by Navon and Gopher in 1979. However, more recent research using well controlled dual-task paradigms points at the importance of tasks. Specifically, in spatial visual-auditory as well as in spatial visual-tactile tasks interference of the two tasks is observed. In contrast, when one of the tasks involves object detection, no interference is observed. Thus, the multi-modal advantage in attentional resources is task dependent.\n\nAs an alternative, resource theory has been proposed as a more accurate metaphor for explaining divided attention on complex tasks. Resource theory states that as each complex task is automatized, performing that task requires less of the individual's limited-capacity attentional resources. Other variables play a part in our ability to pay attention to and concentrate on many tasks at once. These include, but are not limited to, anxiety, arousal, task difficulty, and skills.\n\nSimultaneous attention is a type of attention, classified by attending to multiple events at the same time. Simultaneous attention is demonstrated by children in Indigenous communities, who learn through this type of attention to their surroundings. Simultaneous attention is present in the ways in which children of indigenous backgrounds interact both with their surroundings and with other individuals. Simultaneous attention requires focus on multiple simultaneous activities or occurrences. This differs from multitasking, which is characterized by alternating attention and focus between multiple activities, or halting one activity before switching to the next.\n\nSimultaneous attention involves uninterrupted attention to several activities occurring at the same time. Another cultural practice that may relate to simultaneous attention strategies is coordination within a group. Indigenous heritage toddlers and caregivers in San Pedro were observed to frequently coordinate their activities with other members of a group in ways parallel to a model of simultaneous attention, whereas middle-class European-descent families in the U.S. would move back and forth between events. Research concludes that children with close ties to Indigenous American roots have a high tendency to be especially wide, keen observers. This points to a strong cultural difference in attention management.\n\nAttention may be differentiated into \"overt\" versus \"covert\" orienting.\n\n\"Overt orienting\" is the act of selectively attending to an item or location over others by moving the eyes to point in that direction. Overt orienting can be directly observed in the form of eye movements. Although overt eye movements are quite common, there is a distinction that can be made between two types of eye movements; reflexive and controlled. Reflexive movements are commanded by the superior colliculus of the midbrain. These movements are fast and are activated by the sudden appearance of stimuli. In contrast, controlled eye movements are commanded by areas in the frontal lobe. These movements are slow and voluntary.\n\n\"Covert orienting\" is the act to mentally shifting one's focus without moving one's eyes. Simply, it is changes in attention that are not attributable to overt eye movements. Covert orienting has the potential to affect the output of perceptual processes by governing attention to particular items or locations (for example, the activity of a V4 neuron whose receptive field lies on an attended stimuli will be enhanced by covert attention) but does not influence the information that is processed by the senses. Researchers often use \"filtering\" tasks to study the role of covert attention of selecting information. These tasks often require participants to observe a number of stimuli, but attend to only one. The current view is that visual covert attention is a mechanism for quickly scanning the field of view for interesting locations. This shift in covert attention is linked to eye movement circuitry that sets up a slower saccade to that location.\n\nThere are studies that suggest the mechanisms of overt and covert orienting may not be controlled separately and independently as previously believed. Central mechanisms that may control covert orienting, such as the parietal lobe, also receive input from subcortical centres involved in overt orienting. In support of this, general theories of attention actively assume bottom-up (reflexive) processes and top-down (voluntary) processes converge on a common neural architecture, in that they control both covert and overt attentional systems. For example, if individuals attend to the right hand corner field of view, movement of the eyes in that direction may have to be actively suppressed.\n\nOrienting attention is vital and can be controlled through external (exogenous) or internal (endogenous) processes. However, comparing these two processes is challenging because external signals do not operate completely exogenously, but will only summon attention and eye movements if they are important to the subject.\n\n\"Exogenous\" (from Greek \"exo\", meaning \"outside\", and \"genein\", meaning \"to produce\") orienting is frequently described as being under control of a stimulus. Exogenous orienting is considered to be reflexive and automatic and is caused by a sudden change in the periphery. This often results in a reflexive saccade. Since exogenous cues are typically presented in the periphery, they are referred to as \"peripheral cues\". Exogenous orienting can even be observed when individuals are aware that the cue will not relay reliable, accurate information about where a target is going to occur. This means that the mere presence of an exogenous cue will affect the response to other stimuli that are subsequently presented in the cue's previous location.\n\nSeveral studies have investigated the influence of valid and invalid cues. They concluded that valid peripheral cues benefit performance, for instance when the peripheral cues are brief flashes at the relevant location before to the onset of a visual stimulus. Posner and Cohen (1984) noted a reversal of this benefit takes place when the interval between the onset of the cue and the onset of the target is longer than about 300 ms. The phenomenon of valid cues producing longer reaction times than invalid cues is called inhibition of return.\n\n\"Endogenous\" (from Greek \"endo\", meaning \"within\" or \"internally\") orienting is the intentional allocation of attentional resources to a predetermined location or space. Simply stated, endogenous orienting occurs when attention is oriented according to an observer's goals or desires, allowing the focus of attention to be manipulated by the demands of a task. In order to have an effect, endogenous cues must be processed by the observer and acted upon purposefully. These cues are frequently referred to as \"central cues\". This is because they are typically presented at the center of a display, where an observer's eyes are likely to be fixated. Central cues, such as an arrow or digit presented at fixation, tell observers to attend to a specific location.\n\nWhen examining differences between exogenous and endogenous orienting, some researchers suggest that there are four differences between the two kinds of cues: \n\nThere exist both overlaps and differences in the areas of the brain that are responsible for endogenous and exogenous orientating. Another approach to this discussion has been covered under the topic heading of \"bottom-up\" versus \"top-down\" orientations to attention. Researchers of this school have described two different aspects of how the mind focuses attention to items present in the environment. The first aspect is called bottom-up processing, also known as stimulus-driven attention or exogenous attention. These describe attentional processing which is driven by the properties of the objects themselves. Some processes, such as motion or a sudden loud noise, can attract our attention in a pre-conscious, or non-volitional way. We attend to them whether we want to or not. These aspects of attention are thought to involve parietal and temporal cortices, as well as the brainstem.\n\nThe second aspect is called top-down processing, also known as goal-driven, endogenous attention, attentional control or executive attention. This aspect of our attentional orienting is under the control of the person who is attending. It is mediated primarily by the frontal cortex and basal ganglia as one of the executive functions. Research has shown that it is related to other aspects of the executive functions, such as working memory, and conflict resolution and inhibition.\n\nA ‘hugely influential’ theory regarding selective attention is the Perceptual load theory, which states that there are two mechanisms that affect attention: cognitive and perceptual. The perceptual considers the subject’s ability to perceive or ignore stimuli, both task-related and non task-related. Studies show that if there are many stimuli present (especially if they are task-related), it is much easier to ignore the non-task related stimuli, but if there are few stimuli the mind will perceive the irrelevant stimuli as well as the relevant. The cognitive refers to the actual processing of the stimuli. Studies regarding this showed that the ability to process stimuli decreased with age, meaning that younger people were able to perceive more stimuli and fully process them, but were likely to process both relevant and irrelevant information, while older people could process fewer stimuli, but usually processed only relevant information.\n\nSome people can process multiple stimuli, e.g. trained morse code operators have been able to copy 100% of a message while carrying on a meaningful conversation. This relies on the reflexive response due to \"overlearning\" the skill of morse code reception/detection/transcription so that it is an autonomous function requiring no specific attention to perform.\n\nAttention is best described as the sustained focus of cognitive resources on information while filtering or ignoring extraneous information. Attention is a very basic function that often is a precursor to all other neurological/cognitive functions. As is frequently the case, clinical models of attention differ from investigation models. One of the most used models for the evaluation of attention in patients with very different neurologic pathologies is the model of Sohlberg and Mateer. This hierarchic model is based in the recovering of attention processes of brain damage patients after coma. Five different kinds of activities of growing difficulty are described in the model; connecting with the activities those patients could do as their recovering process advanced.\n\nThis model has been shown to be very useful in evaluating attention in very different pathologies, correlates strongly with daily difficulties and is especially helpful in designing stimulation programs such as attention process training, a rehabilitation program for neurological patients of the same authors.\n\nMost experiments show that one neural correlate of attention is enhanced firing. If a neuron has a certain response to a stimulus when the animal is not attending to the stimulus, then when the animal does attend to the stimulus, the neuron's response will be enhanced even if the physical characteristics of the stimulus remain the same.\n\nIn a 2007 review, Knudsen describes a more general model which identifies four core processes of attention, with working memory at the center:\n\nNeurally, at different hierarchical levels spatial maps can enhance or inhibit activity in sensory areas, and induce orienting behaviors like eye movement.\n\nIn many cases attention produces changes in the EEG. Many animals, including humans, produce gamma waves (40–60 Hz) when focusing attention on a particular object or activity.\n\nAnother commonly used model for the attention system has been put forth by researchers such as Michael Posner. He divides attention into three functional components: alerting, orienting, and executive attention that can also interact and influence each other.\n\nChildren appear to develop patterns of attention related to the cultural practices of their families, communities, and the institutions in which they participate.\n\nIn 1955, Jules Henry suggested that there are societal differences in sensitivity to signals from many ongoing sources that call for the awareness of several levels of attention simultaneously. He tied his speculation to ethnographic observations of communities in which children are involved in a complex social community with multiple relationships.\n\nMany Indigenous children in the Americas predominantly learn by observing and pitching in. There are several studies to support that the use of keen attention towards learning is much more common in Indigenous Communities of North and Central America than in a middle-class European-American setting. This is a direct result of the Learning by Observing and Pitching In model.\n\nKeen attention is both a requirement and result of learning by observing and pitching-in. Incorporating the children in the community gives them the opportunity to keenly observe and contribute to activities that were not directed towards them. It can be seen from different Indigenous communities and cultures, such as the Mayans of San Pedro, that children can simultaneously attend to multiple events. Most Maya children have learned to pay attention to several events at once in order to make useful observations.\n\nOne example is simultaneous attention which involves uninterrupted attention to several activities occurring at the same time. Another cultural practice that may relate to simultaneous attention strategies is coordination within a group. San Pedro toddlers and caregivers frequently coordinated their activities with other members of a group in multiway engagements rather than in a dyadic fashion. Research concludes that children with close ties to Indigenous American roots have a high tendency to be especially keen observers.\n\nThis learning by observing and pitching-in model requires active levels of attention management. The child is present while caretakers engage in daily activities and responsibilities such as: weaving, farming, and other skills necessary for survival. Being present allows the child to focus their attention on the actions being performed by their parents, elders, and/or older siblings. In order to learn in this way, keen attention and focus is required. Eventually the child is expected to be able to perform these skills themselves.\n\nIn the domain of computer vision, efforts have been made to model the mechanism of human attention, especially the bottom-up intentional mechanism and its semantic significance in classification of video contents. Both spatial attention and temporal attention have been incorporated in such classification efforts.\n\nGenerally speaking, there are two kinds of models to mimic the bottom-up salience mechanism in static images. One way is based on the spatial contrast analysis. For example, a center–surround mechanism has been used to define salience across scales, inspired by the putative neural mechanism. It has also been hypothesized that some visual inputs are intrinsically salient in certain background contexts and that these are actually task-independent. This model has established itself as the exemplar for salience detection and consistently used for comparison in the literature; the other way is based on the frequency domain analysis. This method was first proposed by Hou et al., this method was called SR, and then PQFT method was also introduced. Both SR and PQFT only use the phase information. In 2012, the HFT method was introduced, and both the amplitude and the phase information are made use of.\n\nHemispatial neglect, also called \"unilateral neglect\", often occurs when people have damage to their right hemisphere. This damage often leads to a tendency to ignore the left side of one's body or even the left side of an object that can be seen. Damage to the left side of the brain (the left hemisphere) rarely yields significant neglect of the right side of the body or object in the person's local environments.\n\nThe effects of spatial neglect, however, may vary and differ depending on what area of the brain was damaged. Damage to different neural substrates can result in different types of neglect. Attention disorders (lateralized and nonlaterized) may also contribute to the symptoms and effects. Much research has asserted that damage to gray matter within the brain results in spatial neglect.\n\nNew technology has yielded more information, such that there is a large, distributed network of frontal, parietal, temporal, and subcortical brain areas that have been tied to neglect. This network can be related to other research as well; the dorsal attention network is tied to spatial orienting. The effect of damage to this network may result in patients neglecting their left side when distracted about their right side or an object on their right side.\n\nSocial attention is one special form of attention that involves the allocation of limited processing resources in a social context. Previous studies on social attention often regard how attention is directed toward socially relevant stimuli such as faces and gaze directions of other individuals. In contrast to attending-to-others, a different line of researches has shown that self-related information such as own face and name automatically captures attention and is preferentially processed comparing to other-related information. These contrasting effects between attending-to-others and attending-to-self prompt a synthetic view in a recent Opinion article proposing that social attention operates at two polarizing states: In one extreme, individual tends to attend to the self and prioritize self-related information over others', and, in the other extreme, attention is allocated to other individuals to infer their intentions and desires. Attending-to-self and attending-to-others mark the two ends of an otherwise continuum spectrum of social attention. For a given behavioral context, the mechanisms underlying these two polarities might interact and compete with each other in order to determine a saliency map of social attention that guides our behaviors. An imbalanced competition between these two behavioral and cognitive processes will cause cognitive disorders and neurological symptoms such as autism spectrum disorders and Williams syndrome.\n\nPsychologist Daniel E. Berlyne credits the first extended treatment of attention to philosopher Nicolas Malebranche in his work \"The Search After Truth\". \"Malebranche held that we have access to ideas, or mental representations of the external world, but not direct access to the world itself.\" Thus in order to keep these ideas organized, attention is necessary. Otherwise we will confuse these ideas. Malebranche writes in \"The Search After Truth\", \"because it often happens that the understanding has only confused and imperfect perceptions of things, it is truly a cause of our errors... It is therefore necessary to look for means to keep our perceptions from being confused and imperfect. And, because, as everyone knows, there is nothing that makes them clearer and more distinct than attentiveness, we must try to find the means to become more attentive than we are\". According to Malebranche, attention is crucial to understanding and keeping thoughts organized.\n\nPhilosopher Gottfried Wilhelm Leibniz introduced the concept of apperception to this philosophical approach to attention. Apperception refers to \"the process by which new experience is assimilated to and transformed by the residuum of past experience of an individual to form a new whole.\" Apperception is required for a perceived event to become a conscious event. Leibniz emphasized a reflexive involuntary view of attention known as exogenous orienting. However, there is also endogenous orienting which is voluntary and directed attention. Philosopher Johann Friedrich Herbart agreed with Leibniz's view of apperception; however, he expounded on it in by saying that new experiences had to be tied to ones already existing in the mind. Herbart was also the first person to stress the importance of applying mathematical modeling to the study of psychology.\n\nIn the beginning of the 19th century, it was thought that people were not able to attend to more than one stimulus at a time. However, with research contributions by Sir William Hamilton, 9th Baronet this view was changed. Hamilton proposed a view of attention that likened its capacity to holding marbles. You can only hold a certain amount of marbles at a time before it starts to spill over. His view states that we can attend to more than one stimulus at once. William Stanley Jevons later expanded this view and stated that we can attend to up to four items at a time.\n\nDuring this period of attention, various philosophers made significant contributions to the field. They began the research on the extent of attention and how attention is directed.\n\nThis period of attention research took the focus from conceptual findings to experimental testing. It also involved psychophysical methods that allowed measurement of the relation between physical stimulus properties and the psychological perceptions of them. This period covers the development of attentional research from the founding of psychology to 1909.\n\nWilhelm Wundt introduced the study of attention to the field of psychology. Wundt measured mental processing speed by likening it to differences in stargazing measurements. Astronomers in this time would measure the time it took for stars to travel. Among these measurements when astronomers recorded the times, there were personal differences in calculation. These different readings resulted in different reports from each astronomer. To correct for this, a personal equation was developed. Wundt applied this to mental processing speed. Wundt realized that the time it takes to see the stimulus of the star and write down the time was being called an \"observation error\" but actually was the time it takes to switch voluntarily one's attention from one stimulus to another. Wundt called his school of psychology voluntarism. It was his belief that psychological processes can only be understood in terms of goals and consequences.\n\nFranciscus Donders used mental chronometry to study attention and it was considered a major field of intellectual inquiry by authors such as Sigmund Freud. Donders and his students conducted the first detailed investigations of the speed of mental processes. Donders measured the time required to identify a stimulus and to select a motor response. This was the time difference between stimulus discrimination and response initiation. Donders also formalized the subtractive method which states that the time for a particular process can be estimated by adding that process to a task and taking the difference in reaction time between the two tasks. He also differentiated between three types of reactions: simple reaction, choice reaction, and go/no-go reaction.\n\nHermann von Helmholtz also contributed to the field of attention relating to the extent of attention. Von Helmholtz stated that it is possible to focus on one stimulus and still perceive or ignore others. An example of this is being able to focus on the letter u in the word house and still perceiving the letters h, o, s, and e.\n\nOne major debate in this period was whether it was possible to attend to two things at once (split attention). Walter Benjamin described this experience as \"reception in a state of distraction.\" This disagreement could only be resolved through experimentation.\n\nIn 1890, William James, in his textbook \"The Principles of Psychology\", remarked:\n\nJames differentiated between censorial attention and intellectual attention. Censorial attention is when attention is directed to objects of sense, stimuli that are physically present. Intellectual attention is attention directed to ideal or represented objects; stimuli that are not physically present. James also distinguished between immediate or derived attention: attention to the present versus to something not physically present. According to James, attention has five major effects. Attention works to make us perceive, conceive, distinguish, remember, and shorten reactions time.\n\nDuring this period, research in attention waned and interest in behaviorism flourished, leading some to believe, like Ulric Neisser, that in this period, \"There was no research on attention\". However, Jersild published very important work on \"Mental Set and Shift\" in 1927. He stated, \"The fact of mental set is primary in all conscious activity. The same stimulus may evoke any one of a large number of responses depending upon the contextual setting in which it is placed\". This research found that the time to complete a list was longer for mixed lists than for pure lists. For example, if a list was names of animals versus a list with names of animals, books, makes and models of cars, and types of fruits, it takes longer to process. This is task switching.\n\nIn 1931, Telford discovered the psychological refractory period. The stimulation of neurons is followed by a refractory phase during which neurons are less sensitive to stimulation. In 1935 John Ridley Stroop developed the Stroop Task which elicited the Stroop Effect. Stroop's task showed that irrelevant stimulus information can have a major impact on performance. In this task, subjects were to look at a list of colors. This list of colors had each color typed in a color different from the actual text. For example, the word Blue would be typed in Orange, Pink in Black, and so on.\n\nExample: Blue Purple Red Green Purple Green\n\nSubjects were then instructed to say the name of the ink color and ignore the text. It took 110 seconds to complete a list of this type compared to 63 seconds to name the colors when presented in the form of solid squares. The naming time nearly doubled in the presence of conflicting color words, an effect known as the Stroop Effect.\n\nIn the 1950s, research psychologists renewed their interest in attention when the dominant epistemology shifted from positivism (i.e., behaviorism) to realism during what has come to be known as the \"cognitive revolution\". The cognitive revolution admitted unobservable cognitive processes like attention as legitimate objects of scientific study.\n\nModern research on attention began with the analysis of the \"cocktail party problem\" by Colin Cherry in 1953. At a cocktail party how do people select the conversation that they are listening to and ignore the rest? This problem is at times called \"focused attention\", as opposed to \"divided attention\". Cherry performed a number of experiments which became known as dichotic listening and were extended by Donald Broadbent and others. In a typical experiment, subjects would use a set of headphones to listen to two streams of words in different ears and selectively attend to one stream. After the task, the experimenter would question the subjects about the content of the unattended stream.\n\nBroadbent's Filter Model of Attention states that information is held in a pre-attentive temporary store, and only sensory events that have some physical feature in common are selected to pass into the limited capacity processing system. This implies that the meaning of unattended messages is not identified. Also, a significant amount of time is required to shift the filter from one channel to another. Experiments by Gray and Wedderburn and later Anne Treisman pointed out various problems in Broadbent's early model and eventually led to the Deutsch–Norman model in 1968. In this model, no signal is filtered out, but all are processed to the point of activating their stored representations in memory. The point at which attention becomes \"selective\" is when one of the memory representations is selected for further processing. At any time, only one can be selected, resulting in the \"attentional bottleneck\".\n\nThis debate became known as the early-selection vs. late-selection models. In the early selection models (first proposed by Donald Broadbent), attention shuts down (in Broadbent's model) or attenuates (in Triesman's refinement) processing in the unattended ear before the mind can analyze its semantic content. In the late selection models (first proposed by J. Anthony Deutsch and Diana Deutsch), the content in both ears is analyzed semantically, but the words in the unattended ear cannot access consciousness. Lavie's perceptual load theory, however, ‘provided elegant solution to’ what had once been a ‘heated debate’.\n\n"}
{"id": "326572", "url": "https://en.wikipedia.org/wiki?curid=326572", "title": "Attorney–client privilege", "text": "Attorney–client privilege\n\nIn the law of the United States, attorney–client privilege or lawyer–client privilege is a \"client's right privilege to refuse to disclose and to prevent any other person from disclosing confidential communications between the client and the attorney.\"\n\nThe attorney–client privilege is one of the oldest recognized privileges for confidential communications. The United States Supreme Court has stated that by assuring confidentiality, the privilege encourages clients to make \"full and frank\" disclosures to their attorneys, who are then better able to provide candid advice and effective representation.\n\nAlthough there are minor variations, the elements necessary to establish the attorney client privilege generally are:\n\n\nThere are a number of exceptions to the privilege in most jurisdictions, chief among them:\n\nA corollary to the attorney–client privilege is the joint defense privilege, which is also called the common interest rule. The common interest rule \"serves to protect the confidentiality of communications passing from one party to another party where a joint defense or strategy has been decided upon and undertaken by the parties and their respective counsel.\"\n\nAn attorney speaking publicly in regard to a client's personal business and private affairs can be reprimanded by the bar and/or disbarred, regardless of the fact that he or she may be no longer representing the client. Discussing a client's or past client's criminal history, or otherwise, is viewed as a breach of confidentiality.\n\nThe attorney–client privilege is separate from and should not be confused with the work-product doctrine.\n\nWhen an attorney is not acting primarily as an attorney but, for instance, as a business advisor, member of the Board of Directors, or in another non-legal role, then the privilege generally does not apply.\n\nThe privilege protects the confidential communication, and not the underlying information. For instance, if a client has previously disclosed confidential information to a third party who is not an attorney, and then gives the same information to an attorney, the attorney–client privilege will still protect the communication to the attorney, but will not protect the communication with the third party.\n\nThe privilege may be waived if the confidential communications are disclosed to third parties.\n\nOther limits to the privilege may apply depending on the situation being adjudicated.\n\nThe crime-fraud exception can render the privilege moot when communications between an attorney and client are themselves used to further a crime, tort, or fraud. In \"Clark v. United States\", the US Supreme Court stated that \"A client who consults an attorney for advice that will serve him in the commission of a fraud will have no help from the law. He must let the truth be told.\" The crime-fraud exception also \"does\" require that the crime or fraud discussed between client and attorney be carried out to be triggered. US Courts have not yet conclusively ruled how little knowledge an attorney can have of the underlying crime or fraud before the privilege detaches and the attorney's communications or requisite testimony become admissible.\n\nLawyers may disclose confidential information relating to the retainer where they are reasonably seeking to collect payment for services rendered. This is justified on policy grounds. If lawyers were unable to disclose such information, many would undertake legal work only where payment is made in advance. This would arguably adversely affect the public's access to justice.\n\nLawyers may also breach the duty where they are defending themselves against disciplinary or legal proceedings. A client who initiates proceedings against a lawyer effectively waives rights to confidentiality. This is justified on grounds of procedural fairness—a lawyer unable to reveal information relating to the retainer would be unable to defend themselves against such action.\n\nAnother case is for the probate of a last will and testament. Previously confidential communications between the lawyer and testator may be disclosed in order to prove that a will represented the intent of the now deceased decedent. In many instances, the will, codicil, or other parts of the estate plan require explanation or interpretation through other proof (extrinsic evidence), such as the attorney's file notes or correspondence from the client.\n\nIn certain cases, the client may desire or consent to revelation of personal or family secrets only after his or her death; for example, the will may leave a legacy to a paramour or a natural child.\n\nCourts have occasionally revoked the privilege after the death of the client if it is deemed that doing so serves the client's intent, such as in the case of resolving testamentary disputes among heirs.\n\nIn the United States, communications between accountants and their clients are usually not privileged. A person who is worried about accusations of questionable accounting, such as tax evasion, may decide to work only with an attorney or only with an accountant who is also an attorney; some or all of the resulting communications may be privileged provided that all the requirements for the attorney–client privilege are met. The mere fact that the practitioner is an attorney will not create a valid attorney–client privilege with respect to a communication, for example, that involves business or accounting advice rather than legal advice.\n\nUnder Federal tax law in the United States, for communications on or after July 22, 1998, there is a limited Federally authorized accountant–client privilege that may apply to certain communications with non–attorneys.\n\nIf a case arises in the federal court system, the federal court will apply Rule 501 of the Federal Rules of Evidence to determine whether to apply the privilege law of the relevant state or federal common law. If the case is brought to the federal court under diversity jurisdiction, the law of the relevant state will be used to apply the privilege. If the case involves a federal question, the federal court will apply the federal common law of attorney–client privilege; however, Rule 501 grants flexibility to the federal courts, allowing them to construe the privilege \"in light of experience and reason\".\n\n"}
{"id": "12247219", "url": "https://en.wikipedia.org/wiki?curid=12247219", "title": "Aṇḍa", "text": "Aṇḍa\n\nIn Kaśmir Śaivism the world is described as being composed of four spheres () that contain a series of phenomenal elements (tattva). The four \"\" are described to appear by the means of the internal abundance of Śiva's divine powers. Outside the four is Śiva tattva which is the substrate and essential nature of all the other tattvas.\nProjected by the absolute, is the first step of creation. Also called \"the pure creation\" because at this level the divine nature of Śiva is not obscured, it manifests a state of diversity in unity. The divine powers (Śakti) gradually descend from \"Ānanda Śakti\" (bliss) to \"Icchā Śakti\" (the power of will), \"Jñāna Śakti\" (the power of knowledge) and \"Kriyā Śakti\" (the power of action), at the same time creating the basis for the dual creation. At this stage, though, the duality is only \"in concept\"; there is no actual division or limitation yet. This \"\" contains Śakti tattva, Sadāśiva tattva, Iśvara tattva and Śuddha-vidyā tattva (all the pure tattvas except the first one, Śiva tattva).\nThe \"sphere of Māyā\" causes the divine nature and purity that exists in to be forgotten. The divine creation is covered with five limitations (\"kañcuka\") that make the infinite, eternal, perfect in itself, all knowing and all powerful nature of God, as manifested first in the \"\", appear limited in space (Niyati tattva) and time (Kāla tattva), incomplete (Rāga tattva), with limited knowledge (Aśuddha-vidyā tattva) and power of action (Kalā tattva). This contains seven tattvas, from Māyā tattva to .\n\n"}
{"id": "57172319", "url": "https://en.wikipedia.org/wiki?curid=57172319", "title": "Busemann G-space", "text": "Busemann G-space\n\nIn mathematics, a Busemann \"G\"-space is a type of metric space first described by Herbert Busemann in 1942.\n\nIf formula_1 is a metric space such that\n\n\nthen \"X\" is said to be a \"Busemann\" \"G\"-\"space\". Every Busemann \"G\"-space is a homogenous space.\n\nThe Busemann conjecture states that every Busemann \"G\"-space is a topological manifold. It is a special case of the Bing–Borsuk conjecture. The Busemann conjecture is known to be true for dimensions 1 to 4. \n"}
{"id": "706311", "url": "https://en.wikipedia.org/wiki?curid=706311", "title": "Canonical coordinates", "text": "Canonical coordinates\n\nIn mathematics and classical mechanics, canonical coordinates are sets of coordinates on phase space which can be used to describe a physical system at any given point in time. Canonical coordinates are used in the Hamiltonian formulation of classical mechanics. A closely related concept also appears in quantum mechanics; see the Stone–von Neumann theorem and canonical commutation relations for details.\n\nAs Hamiltonian mechanics is generalized by symplectic geometry and canonical transformations are generalized by contact transformations, so the 19th century definition of canonical coordinates in classical mechanics may be generalized to a more abstract 20th century definition of coordinates on the cotangent bundle of a manifold (the mathematical notion of phase space).\n\nIn classical mechanics, canonical coordinates are coordinates formula_1 and formula_2 in phase space that are used in the Hamiltonian formalism. The canonical coordinates satisfy the fundamental Poisson bracket relations:\n\nA typical example of canonical coordinates is for formula_1 to be the usual Cartesian coordinates, and formula_2 to be the components of momentum. Hence in general, the formula_2 coordinates are referred to as \"conjugate momenta.\"\n\nCanonical coordinates can be obtained from the generalized coordinates of the Lagrangian formalism by a Legendre transformation, or from another set of canonical coordinates by a canonical transformation.\n\nCanonical coordinates are defined as a special set of coordinates on the cotangent bundle of a manifold. They are usually written as a set of formula_7 or formula_8 with the \"x\" 's or \"q\" 's denoting the coordinates on the underlying manifold and the \"p\" 's denoting the conjugate momentum, which are 1-forms in the cotangent bundle at point \"q\" in the manifold.\n\nA common definition of canonical coordinates is any set of coordinates on the cotangent bundle that allow the canonical one-form to be written in the form\n\nup to a total differential. A change of coordinates that preserves this form is a canonical transformation; these are a special case of a symplectomorphism, which are essentially a change of coordinates on a symplectic manifold.\n\nIn the following exposition, we assume that the manifolds are real manifolds, so that cotangent vectors acting on tangent vectors produce real numbers.\n\nGiven a manifold , a vector field on (a section of the tangent bundle ) can be thought of as a function acting on the cotangent bundle, by the duality between the tangent and cotangent spaces. That is, define a function\n\nsuch that\n\nholds for all cotangent vectors in formula_12. Here, formula_13 is a vector in formula_14, the tangent space to the manifold at point . The function formula_15 is called the \"momentum function\" corresponding to .\n\nIn local coordinates, the vector field at point may be written as\n\nwhere the formula_17 are the coordinate frame on . The conjugate momentum then has the expression \n\nwhere the formula_2 are defined as the momentum functions corresponding to the vectors formula_17:\n\nThe formula_22 together with the formula_23 together form a coordinate system on the cotangent bundle formula_24; these coordinates are called the \"canonical coordinates\".\n\nIn Lagrangian mechanics, a different set of coordinates are used, called the generalized coordinates. These are commonly denoted as formula_25 with formula_1 called the generalized position and formula_27 the generalized velocity. When a Hamiltonian is defined on the cotangent bundle, then the generalized coordinates are related to the canonical coordinates by means of the Hamilton–Jacobi equations.\n\n"}
{"id": "5370", "url": "https://en.wikipedia.org/wiki?curid=5370", "title": "Category of being", "text": "Category of being\n\nIn ontology, the different kinds or ways of being are called categories of being; or simply categories. To investigate the categories of being is to determine the most fundamental and the broadest classes of entities. A distinction between such categories, in making the categories or applying them, is called an ontological distinction.\n\nThe process of abstraction required to discover the number and names of the categories has been undertaken by many philosophers since Aristotle and involves the careful inspection of each concept to ensure that there is no higher category or categories under which that concept could be subsumed. The scholars of the twelfth and thirteenth centuries developed Aristotle's ideas, firstly, for example by Gilbert of Poitiers, dividing Aristotle's ten categories into two sets, primary and secondary, according to whether they inhere in the subject or not:\nSecondly, following Porphyry’s likening of the classificatory hierarchy to a tree, they concluded that the major classes could be subdivided to form subclasses, for example, Substance could be divided into Genus and Species, and Quality could be subdivided into Property and Accident, depending on whether the property was necessary or contingent. An alternative line of development was taken by Plotinus in the second century who by a process of abstraction reduced Aristotle’s list of ten categories to five: Substance, Relation, Quantity, Motion and Quality. Plotinus further suggested that the latter three categories of his list, namely Quantity, Motion and Quality correspond to three different kinds of relation and that these three categories could therefore be subsumed under the category of Relation. This was to lead to the supposition that there were only two categories at the top of the hierarchical tree, namely Substance and Relation, and if relations only exist in the mind as many supposed, to the two highest categories, Mind and Matter, reflected most clearly in the dualism of René Descartes.\n\nAn alternative conclusion however began to be formulated in the eighteenth century by Immanuel Kant who realised that we can say nothing about Substance except through the relation of the subject to other things. In the sentence \"This is a house\" the substantive subject \"house\" only gains meaning in relation to human use patterns or to other similar houses. The category of Substance disappears from Kant's tables, and under the heading of Relation, Kant lists \"inter alia\" the three relationship types of Disjunction, Causality and Inherence. The three older concepts of Quantity, Motion and Quality, as Peirce discovered, could be subsumed under these three broader headings in that Quantity relates to the subject through the relation of Disjunction; Motion relates to the subject through the relation of Causality; and Quality relates to the subject through the relation of Inherence. Sets of three continued to play an important part in the nineteenth century development of the categories, most notably in G.W.F. Hegel's extensive tabulation of categories, and in C.S. Peirce's categories set out in his work on the logic of relations. One of Peirce's contributions was to call the three primary categories Firstness, Secondness and Thirdness which both emphasises their general nature, and avoids the confusion of having the same name for both the category itself and for a concept within that category.\n\nIn a separate development, and building on the notion of primary and secondary categories introduced by the Scholastics, Kant introduced the idea that secondary or \"derivative\" categories could be derived from the primary categories through the combination of one primary category with another. This would result in the formation of three secondary categories: the first, \"Community\" was an example that Kant gave of such a derivative category; the second, \"Modality\", introduced by Kant, was a term which Hegel, in developing Kant's dialectical method, showed could also be seen as a derivative category; and the third, \"Spirit\" or \"Will\" were terms that Hegel and Schopenhauer were developing separately for use in their own systems. Karl Jaspers in the twentieth century, in his development of existential categories, brought the three together, allowing for differences in terminology, as Substantiality, Communication and Will. This pattern of three primary and three secondary categories was used most notably in the nineteenth century by Peter Mark Roget to form the six headings of his Thesaurus of English Words and Phrases. The headings used were the three objective categories of Abstract Relation, Space (including Motion) and Matter and the three subjective categories of Intellect, Feeling and Volition, and he found that under these six headings all the words of the English language, and hence any possible predicate, could be assembled.\n\nIn the twentieth century the primacy of the division between the subjective and the objective, or between mind and matter, was disputed by, among others, Bertrand Russell and Gilbert Ryle. Philosophy began to move away from the metaphysics of categorisation towards the linguistic problem of trying to differentiate between, and define, the words being used. Ludwig Wittgenstein’s conclusion was that there were no clear definitions which we can give to words and categories but only a \"halo\" or \"corona\" of related meanings radiating around each term. Gilbert Ryle thought the problem could be seen in terms of dealing with \"a galaxy of ideas\" rather than a single idea, and suggested that category mistakes are made when a concept (e.g. \"university\"), understood as falling under one category (e.g. abstract idea), is used as though it falls under another (e.g. physical object). With regard to the visual analogies being used, Peirce and Lewis, just like Plotinus earlier, likened the terms of propositions to points, and the relations between the terms to lines. Peirce, taking this further, talked of univalent, bivalent and trivalent relations linking predicates to their subject and it is just the number and types of relation linking subject and predicate that determine the category into which a predicate might fall. Primary categories contain concepts where there is one dominant kind of relation to the subject. Secondary categories contain concepts where there are two dominant kinds of relation. Examples of the latter were given by Heidegger in his two propositions \"the house is on the creek\" where the two dominant relations are spatial location (Disjunction) and cultural association (Inherence), and \"the house is eighteenth century\" where the two relations are temporal location (Causality) and cultural quality (Inherence). A third example may be inferred from Kant in the proposition \"the house is impressive or sublime\" where the two relations are spatial or mathematical disposition (Disjunction) and dynamic or motive power (Causality). Both Peirce and Wittgenstein introduced the analogy of colour theory in order to illustrate the shades of meanings of words. Primary categories, like primary colours, are analytical representing the furthest we can go in terms of analysis and abstraction and include Quantity, Motion and Quality. Secondary categories, like secondary colours, are synthetic and include concepts such as Substance, Community and Spirit.\n\nOne of Aristotle’s early interests lay in the classification of the natural world, how for example the genus \"animal\" could be first divided into \"two-footed animal\" and then into \"wingless, two-footed animal\". He realised that the distinctions were being made according to the qualities the animal possesses, the quantity of its parts and the kind of motion that it exhibits. To fully complete the proposition \"this animal is…\" Aristotle stated in his work on the Categories that there were ten kinds of predicate where...\n\n\"…each signifies either substance or quantity or quality or relation or where or when or being-in-a-position or having or acting or being acted upon\".\n\nHe realised that predicates could be simple or complex. The simple kinds consist of a subject and a predicate linked together by the \"categorical\" or inherent type of relation. For Aristotle the more complex kinds were limited to propositions where the predicate is compounded of two of the above categories for example \"this is a horse running\". More complex kinds of proposition were only discovered after Aristotle by the Stoic, Chrysippus, who developed the \"hypothetical\" and \"disjunctive\" types of syllogism and these were terms which were to be developed through the Middle Ages and were to reappear in Kant's system of categories.\n\n\"Category\" came into use with Aristotle's essay \"Categories\", in which he discussed univocal and equivocal terms, predication, and ten categories:\n\nPlotinus in writing his \"Enneads\" around AD 250 recorded that \"philosophy at a very early age investigated the number and character of the existents… some found ten, others less…. to some the genera were the first principles, to others only a generic classification of existents\". He realised that some categories were reducible to others saying \"why are not Beauty, Goodness and the virtues, Knowledge and Intelligence included among the primary genera?\" He concluded that such transcendental categories and even the categories of Aristotle were in some way posterior to the three Eleatic categories first recorded in Plato's dialogue \"Parmenides\" and which comprised the following three coupled terms: \n\nPlotinus called these \"the hearth of reality\" deriving from them not only the three categories of Quantity, Motion and Quality but also what came to be known as \"the three moments of the Neoplatonic world process\":\n\nPlotinus likened the three to the centre, the radii and the circumference of a circle, and clearly thought that the principles underlying the categories were the first principles of creation. \"From a single root all being multiplies\". Similar ideas were to be introduced into Early Christian thought by, for example, Gregory of Nazianzus who summed it up saying \"Therefore Unity, having from all eternity arrived by motion at duality, came to rest in trinity\".\n\nIn the \"Critique of Pure Reason\" (1781), Immanuel Kant argued that the categories are part of our own mental structure and consist of a set of \"a priori\" concepts through which we interpret the world around us. These concepts correspond to twelve logical functions of the understanding which we use to make judgements and there are therefore two tables given in the \"Critique\", one of the Judgements and a corresponding one for the Categories. To give an example, the logical function behind our reasoning from ground to consequence (based on the Hypothetical relation) underlies our understanding of the world in terms of cause and effect (the Causal relation). In each table the number twelve arises from, firstly, an initial division into two: the Mathematical and the Dynamical; a second division of each of these headings into a further two: Quantity and Quality, and Relation and Modality respectively; and, thirdly, each of these then divides into a further three subheadings as follows.\nTable of Judgements\n\nMathematical\nDynamical\n\nTable of Categories\n\nMathematical\nDynamical\n\nCriticism of Kant's system followed, firstly, by Arthur Schopenhauer, who amongst other things was unhappy with the term \"Community\", and declared that the tables \"do open violence to truth, treating it as nature was treated by old-fashioned gardeners\", and secondly, by W.T.Stace who in his book \"The Philosophy of Hegel\" suggested that in order to make Kant's structure completely symmetrical a third category would need to be added to the Mathematical and the Dynamical. This, he said, Hegel was to do with his category of Notion.\n\nG.W.F. Hegel in his \"Science of Logic\" (1812) attempted to provide a more comprehensive system of categories than Kant and developed a structure that was almost entirely triadic. So important were the categories to Hegel that he claimed \"the first principle of the world, the Absolute, is a system of categories… the categories must be the reason of which the world is a consequent\".\n\nUsing his own logical method of combination, later to be called the Hegelian dialectic, of arguing from thesis through antithesis to synthesis, he arrived, as shown in W.T.Stace's work cited, at a hierarchy of some 270 categories. The three very highest categories were Logic, Nature and Spirit. The three highest categories of Logic, however, he called Being, Essence and Notion which he explained as follows:\nSchopenhauer's category that corresponded with Notion was that of Idea, which in his \"Four-Fold Root of Sufficient Reason\" he complemented with the category of the Will. The title of his major work was \"The World as Will and Idea\". The two other complementary categories, reflecting one of Hegel's initial divisions, were those of Being and Becoming. At around the same time, Goethe was developing his colour theories in the \"Farbenlehre\" of 1810, and introduced similar principles of combination and complementation, symbolising, for Goethe, \"the primordial relations which belong both to nature and vision\". Hegel in his \"Science of Logic\" accordingly asks us to see his system not as a tree but as a circle.\n\nCharles Sanders Peirce, who had read Kant and Hegel closely, and who also had some knowledge of Aristotle, proposed a system of merely three phenomenological categories: Firstness, Secondness, and Thirdness, which he repeatedly invoked in his subsequent writings. Like Hegel, C.S.Peirce attempted to develop a system of categories from a single indisputable principle, in Peirce's case the notion that in the first instance he could only be aware of his own ideas. \n\nAlthough Peirce's three categories correspond to the three concepts of relation given in Kant's tables, the sequence is now reversed and follows that given by Hegel, and indeed before Hegel of the three moments of the world-process given by Plotinus. Later, Peirce gave a mathematical reason for there being three categories in that although monadic, dyadic and triadic nodes are irreducible, every node of a higher valency is reducible to a \"compound of triadic relations\". Ferdinand de Saussure, who was developing \"semiology\" in France just as Peirce was developing \"semiotics\" in the US, likened each term of a proposition to \"the centre of a constellation, the point where other coordinate terms, the sum of which is indefinite, converge\".\n\nEdmund Husserl (1962, 2000) wrote extensively about categorial systems as part of his phenomenology.\n\nFor Gilbert Ryle (1949), a category (in particular a \"category mistake\") is an important semantic concept, but one having only loose affinities to an ontological category.\n\nContemporary systems of categories have been proposed by John G. Bennett (The Dramatic Universe, 4 vols., 1956–65), Wilfrid Sellars (1974), Reinhardt Grossmann (1983, 1992), Johansson (1989), Hoffman and Rosenkrantz (1994), Roderick Chisholm (1996), Barry Smith (ontologist) (2003), and Jonathan Lowe (2006).\n\n\n\n"}
{"id": "2723186", "url": "https://en.wikipedia.org/wiki?curid=2723186", "title": "Chastisement", "text": "Chastisement\n\nChastisement is the infliction of corporal punishment as defined by law.\n\nEnglish common law allowed parents and others who have \"lawful control or charge\" of a child to use \"moderate and reasonable\" chastisement or correction. In the 1860 Eastbourne manslaughter case, Alexander Cockburn as Chief Justice ruled: \"By the law of England, a parent ... may for the purpose of correcting what is evil in the child, inflict moderate and reasonable corporal punishment, always, however, with this condition, that it is moderate and reasonable.\" It was left to the courts to decide what is meant by \"moderate and reasonable\" in any particular case.\n\nThe rights of parents, guardians and teachers, in regard to the chastisement of children, were expressly recognized in English law by the Prevention of Cruelty to Children Act 1904 (§ 28). A master had a right to inflict moderate chastisement upon his apprentice for neglect or other misbehaviour, provided that he did so himself, and that the apprentice was under age (Archbold, Cr. Pl., 23rd ed., 795).\n\nIn England and Wales, section 58 of the Children Act 2004 enables parents to justify common assault or battery (crime) of their children as \"reasonable punishment\", but prevents the defence being used in relation to Assault occasioning actual bodily harm (i.e. when causing anything beyond \"transient and trifling\" such as bruising) and any more serious harm.\n\nIn law in the Republic of Ireland, the rule of law allowing \"physical chastisement\" by teachers was abolished in 1997, and the common-law defence of \"reasonable chastisement\" by parents and guardians was abolished in 2015.\n\nWilliam Blackstone wrote in the 18th century in the Commentaries on the Laws of England:\nIn the UK the traditional right of a husband to inflict moderate corporal punishment on his wife in order to keep her \"within the bounds of duty\" was similarly removed in 1891.\n\nIn the 1870s, courts in the United States overruled the common-law principle that a husband had the right to \"physically chastise an errant wife\".\n\n"}
{"id": "31660921", "url": "https://en.wikipedia.org/wiki?curid=31660921", "title": "Comfort", "text": "Comfort\n\nComfort (or being \"comfortable\") is a sense of physical or psychological ease, often characterized as a lack of hardship. Persons who are lacking in comfort are uncomfortable, or experiencing discomfort. A degree of psychological comfort can be achieved by recreating experiences that are associated with pleasant memories, such as engaging in familiar activities, maintaining the presence of familiar objects, and consumption of comfort foods. Comfort is a particular concern in health care, as providing comfort to the sick and injured is one goal of healthcare, and can facilitate recovery. Persons who are surrounded with things that provide psychological comfort may be described as being \"in their comfort zone\". Because of the personal nature of positive associations, psychological comfort is highly subjective.\n\nThe use of \"comfort\" as a verb generally implies that the subject is in a state of pain, suffering or affliction, and requires alleviation from that state. Where the term is used to describe the support given to someone who has experienced a tragedy, the word is synonymous with consolation or solace. However, comfort is used much more broadly, as one can provide physical comfort to someone who is not in a position to be uncomfortable. For example, a person might sit in a chair without discomfort, but still find the addition of a pillow to the chair to increase their feeling of comfort. Something that provides this type of comfort, which does not seek to relieve hardship, can also be referred to as being \"comfy\". Like certain other terms describing positive feelings or abstractions (hope, charity, chastity), comfort may also be used as a personal name.\n\nThere are various psychological studies about the feeling of comfort, and they have resulted in a few conclusions. The idea of comfort varies among each person; however, there are a few universal themes of comfort that apply to everyone. Most of these universal themes falls under the physical comfort such as contact comfort, comfort food, and thermal comfort.\n\nContact comfort is the satisfaction with someone's touch, like a mother's embrace. This is essential to a child's development.\n\nOne of the most famous developmental psychological studies is Harry Harlow's development experiment with monkeys. He separated baby monkeys at birth and raised them with surrogate mothers. There were two types of surrogate mothers, a metal wire one and a one covered with cloth, and each had a nozzle that represented breast feeding. The surrogate mother covered in cloth represented comfort. At the end of the experiment, the psychologist saw that the monkeys would choose the cloth surrogate over the wire surrogate. They concluded that having basics needs is essential, but there is the need of closeness and affection.\n\nThis experiment justified that importance of comfort and warmth for child development. All the monkeys that grew up from the experiment expressed a behavior of aggression and atypical sexual behaviors.\n\nComfort foods are foods intentionally consumed to move the eater into a pleasurable state. This could be credited to food preferences and childhood experiences (like a mother's cooking).\n\nComfort food are usually chosen because of previous experiences of happiness linked with it. For example, chocolate is held as a popular comfort food as it is follows the pleasurable sweetness and the positive association with gifts/rewards.\n\nThe time of day also play a role in consuming comfort foods. Most people tend to eat simply because \"it's lunch time\" and only 20% of the time is due to actual hunger \nFood preferences splits into two categories, snack-related and meal-related. If a child was exposed to many snacks growing up, he/she would focus on more snack-related comfort foods later on in life.\n\nFood preference ranges through male/female, and younger/older\n\nThermal comfort is a satisfaction of the ambient air temperature and humidity. Psychologists devised a study to determine the most comfortable temperature. The study had people answering a survey as the temperature changed around them. From the surveys, psychologist found many people had no opinion of a range of temperature.This was labeled temperature neutrality, which is the rate as the person metabolism is shifting the same rate as the surrounding temperature. The average comfortable temperature is 30 degrees Celsius. Temperatures too hot (35 °C and above) and temperatures too low (12 °C and below) are considered uncomfortable to many people.\n\nThermal Neutrality (Thermal Neutral Zone) is the temperature range where it is neither comfortable or uncomfortable. The human body's metabolism is burning calories at the same rate as the temperature around. This would be around 24 °C (room temperature), and people have no opinion about the temperature. Thermal Neutrality is often also used in animal raising.For example, Farmers maintain the neutral temperature for cattle to prevent Cold Stress.\n\n\n"}
{"id": "25102198", "url": "https://en.wikipedia.org/wiki?curid=25102198", "title": "Conservation photography", "text": "Conservation photography\n\nConservation photography is the active use of the photographic process and its products, within the parameters of photojournalism, to advocate for conservation outcomes.\n\nConservation photography combines nature photography with the proactive, issue-oriented approach of documentary photography as an agent for protecting nature and improving the biosphere and natural environment. Conservation Photography furthers environmental conservation, wildlife conservation, habitat conservation or cultural conservation by expanding public awareness of issues and stimulating remedial action.\n\nPhotography has developed as a powerful medium to empower conservation. Photography has served this role since the 1860s, although not widely acknowledged as such. A notable example are the powerful images of Carleton Watkins which were successfully used to stimulate the establishment of Yosemite National Park in 1864 and William Henry Jackson and Ansel Adams who advocated for expansion and continued funding of the park.\n\nRenewed emphasis on photography-for-conservation arose at the beginning of the 21st century, primarily in response to the human-caused environmental crisis, recognizing that the global pattern of ecosystem degradation was not sustainable.\n\nThe modern field of conservation photography was formalized in October 2005 with the founding of the International League of Conservation Photographers by photographer Cristina Mittermeier, during the 8th World Wilderness Congress in Anchorage, Alaska. Prior to 2005 \"conservation photography\" was not widely recognized as a discipline.\n\nConservation and photography appear as two distinct fields, but their combined impact can be profound. Simply put, conservation photography is photography that empowers or enables conservation.\n\nAccording to the acclaimed photographer, Joel Sartore, “the typical nature photograph shows a butterfly on a pretty flower. The conservation photograph shows the same thing, but with a bulldozer coming at it in the background. This doesn’t mean there’s no room for beautiful pictures, in fact we need beautiful images just as much as the issues. It does mean that the images exist for a reason; to save the Earth while we still can.”\n\nThe serious conservation photographer brings to their work a deep empathy for the natural world. Proper use of the resulting images has the power to bring about positive change.\n\nConservation photographs fall into two broad categories, both of which are equally valuable:\n\n\nSuch photographs have a stronger impact on the audience. One may also proactively seek opportunities to take crafted conservation pictures. Determined effort can result in excellent photo stories that can move people’s hearts and minds.\n\nIn order to create an impact, conservation pictures should be put to work for specific causes. Though not every picture may find an immediate use, a carefully catalogued archive of conservation pictures can help increase impact of conservation related news stories, provide material for public awareness campaigns, including internet activism and sometimes serve as Investigative journalism evidence in court proceedings. Images of habitat destruction, especially in protected areas, can be important as legal evidence against the activity.\nSeveral specialty fields benefit from their use of conservation photography, these include:\n\nSome subjects of conservation photography include:\n\n\n\nThere are many environmental organizations that effectively use conservation photography to help advocate their goals. Just a few are:\n"}
{"id": "55383004", "url": "https://en.wikipedia.org/wiki?curid=55383004", "title": "Constitutionalism in the United States", "text": "Constitutionalism in the United States\n\nConstitutionalism in the United States is a basic value espoused by political parties, activist groups and individuals across a wide range of the political spectrum, that the powers of federal, state and local governments are limited by the Constitution of the United States and that the civil and political rights of citizens should not be violated.\n\nAs a political movement, many constitutionalists have expressed concern over provisions of the 2001 USA Patriot Act, civil asset forfeiture laws, mass surveillance, police checkpoints and militarization of police, while differing over other issues, such as restrictions on firearms, states' rights to determine drug and restroom laws, and federal management of public lands.\n\n"}
{"id": "58107763", "url": "https://en.wikipedia.org/wiki?curid=58107763", "title": "Dahyan air strike", "text": "Dahyan air strike\n\nOn 9 August 2018, Saudi Arabian expeditionary aircraft bombed a civilian school bus passing through a crowded market with U.S made bombs in Dahyan, Saada Governorate, Yemen, near the border with Saudi Arabia. At least 40 children were killed, all under 15 years old and most under age 10. Sources disagree on the exact number of deaths, but they estimate that the air strike killed about 51 people.\n\nAccording to Save the Children, at the time of the attack the children were on a bus heading back to school from a picnic when the driver stopped to get refreshment at the market in Dahyan. Most of the children were under age 10, according to the International Committee of the Red Cross. A Red Cross – supported hospital in Saada received the bodies of 29 children under 15 years of age and 48 wounded individuals, 30 of whom were children. A total of 40 children were killed in the strike.\n\nAccording to a resident of Dahyan, the warplanes had been hovering over the area for more than an hour before they attacked. Another witness said, \"Our shops were open and shoppers were walking around as usual. All those who died were residents, children and shop owners.\" According to Yahya Hussein, a teacher who was traveling separately from the bus, \"The scene can't be described—there was body parts and blood everywhere.\"\n\nThe bomb that killed the children was a MK 82 bomb made by Lockheed Martin. It had been supplied by the United States to Saudi Arabia.\n\nThe attack came to light after videos were posted on Twitter depicting the remains of the bus and the children. Images of the victims were aired on the Al Masirah TV network, highlighting dramatic images of blood and debris-covered children lying on hospital stretchers. The Saudi Arabian coalition later issued a statement saying that they conducted an airstrike in Saada but were targeting Houthi missile launchers. The mass funeral of the children was aired on the Al Mariah TV network, with thousands of Yemenis participating.\n\nThe official Saudi Arabian press agency called the strike a \"legitimate military action\" which targeted those who were responsible for a rebel missile attack on the Saudi Arabian city of Jizan on Wednesday. They also claimed that the airstrikes \"conformed to international and humanitarian laws\" and that Houthis were using children as human shields. Yemeni journalist Nasser Arrabyee reported that there were no Houthis in the vicinity of the strike. A Houthi spokesman said that the coalition showed \"clear disregard for civilian life\", as the attack had targeted a crowded public place in the city. During the mass funeral of the children, many signs were visible protesting against the United States, Saudi Arabia, and Israel.\n\nOn 1 September 2018, the Saudi Arabian-led coalition admitted mistakes, expressing regrets and pledged to hold those responsible for the strikes accountable.\n\nUnited Nations Secretary-General António Guterres condemned the attack and called for an independent and prompt investigation, and UNICEF strongly condemned the attack.\n\nThe United States Department of State called for Saudi Arabia to conduct an investigation into the strike. The United Kingdom's Foreign and Commonwealth Office expressed \"deep concern\", called for a transparent investigation, and called upon all parties to prevent civilian casualties and to co-operate with the UN to reach a lasting political solution in Yemen. UK Foreign Secretary Jeremy Hunt defended the Saudi–British alliance as important in fighting Islamists.\n\nThe head of the Yemeni delegation of the International Committee of the Red Cross tweeted, \"@ICRC_Yemen-supported hospital has received dozens of dead and wounded. Under international humanitarian law, civilians must be protected during conflict.\"\n\n"}
{"id": "1240011", "url": "https://en.wikipedia.org/wiki?curid=1240011", "title": "Dirichlet's principle", "text": "Dirichlet's principle\n\nIn mathematics, and particularly in potential theory, Dirichlet's principle is the assumption that the minimizer of a certain energy functional is a solution to Poisson's equation.\n\nDirichlet's principle states that, if the function formula_1 is the solution to Poisson's equation \n\non a domain formula_3 of formula_4 with boundary condition \n\nthen \"u\" can be obtained as the minimizer of the Dirichlet's energy \n\namongst all twice differentiable functions formula_7 such that formula_8 on formula_9 (provided that there exists at least one function making the Dirichlet's integral finite). This concept is named after the German mathematician Peter Gustav Lejeune Dirichlet.\n\nSince the Dirichlet's integral is bounded from below, the existence of an infimum is guaranteed. That this infimum is attained was taken for granted by Riemann (who coined the term \"Dirichlet's principle\") and others until Weierstrass gave an example of a functional that does not attain its minimum. Hilbert later justified Riemann's use of Dirichlet's principle.\n\n\n"}
{"id": "54390", "url": "https://en.wikipedia.org/wiki?curid=54390", "title": "Disassembler", "text": "Disassembler\n\nA disassembler is a computer program that translates machine language into assembly language—the inverse operation to that of an assembler. A disassembler differs from a decompiler, which targets a high-level language rather than an assembly language. Disassembly, the output of a disassembler, is often formatted for human-readability rather than suitability for input to an assembler, making it principally a reverse-engineering tool.\n\nAssembly language source code generally permits the use of constants and programmer comments. These are usually removed from the assembled machine code by the assembler. If so, a disassembler operating on the machine code would produce disassembly lacking these constants and comments; the disassembled output becomes more difficult for a human to interpret than the original annotated source code. Some disassemblers provide a built-in code commenting feature where the generated output gets enriched with comments regarding called API functions or parameters of called functions. Some disassemblers make use of the symbolic debugging information present in object files such as ELF. For example, IDA allows the human user to make up mnemonic symbols for values or regions of code in an interactive session: human insight applied to the disassembly process often parallels human creativity in the code writing process.\n\nOn CISC platforms with variable-width instructions, more than one disassembly may be correct. Disassemblers do not handle code that varies during execution.\n\nWriting a disassembler which produces code which, when assembled, produces exactly the original binary is possible; however, there are often differences. This poses demands on the expressivity of the assembler. For example, an x86 assembler takes an arbitrary choice between two binary codes for something as simple as codice_1. If the original code uses the other choice, the original code simply cannot be reproduced at any given point in time. However, even when a fully correct disassembly is produced, problems remain if the program requires modification. For example, the same machine language jump instruction can be generated by assembly code to jump to a specified location (for example, to execute specific code), or to jump to a specified number of bytes (for example, to skip over an unwanted branch). A disassembler cannot know what is intended, and may use either syntax to generate a disassembly which reproduces the original binary. However, if a programmer wants to add instructions between the jump instruction and its destination, it is necessary to understand the program's operation to determine whether the jump should be absolute or relative, i.e., whether its destination should remain at a fixed location, or be moved so as to skip both the original and added instructions.\n\nA disassembler may be stand-alone or interactive. A stand-alone disassembler, when executed, generates an assembly language file which can be examined; an interactive one shows the effect of any change the user makes immediately. For example, the disassembler may initially not know that a section of the program is actually code, and treat it as data; if the user specifies that it is code, the resulting disassembled code is shown immediately, allowing the user to examine it and take further action during the same run.\n\nAny interactive debugger will include some way of viewing the disassembly of the program being debugged. Often, the same disassembly tool will be packaged as a standalone disassembler distributed along with the debugger. For example, objdump, part of GNU Binutils, is related to the interactive debugger gdb.\n\nA dynamic disassembler can be incorporated into the output of an emulator or hypervisor to 'trace out', line-by-line, the real time execution of any executed machine instructions. In this case, as well as lines containing the disassembled machine code, the register(s) and/or data change(s) (or any other changes of \"state\", such as condition codes) that each individual instruction causes can be shown alongside or beneath the disassembled instruction. This provides extremely powerful debugging information for ultimate problem resolution, although the size of the resultant output can sometimes be quite large, especially if active for an entire program's execution. OLIVER provided these features from the early 1970s as part of its CICS debugging product offering and is now to be found incorporated into the XPEDITER product from Compuware.\n\n\n\n"}
{"id": "38802668", "url": "https://en.wikipedia.org/wiki?curid=38802668", "title": "Economic democracy", "text": "Economic democracy\n\nEconomic democracy is a socioeconomic philosophy that proposes to shift decision-making power from corporate managers and corporate shareholders to a larger group of public stakeholders that includes workers, customers, suppliers, neighbors and the broader public. No single definition or approach encompasses economic democracy, but most proponents claim that modern property relations externalize costs, subordinate the general well-being to private profit and deny the polity a democratic voice in economic policy decisions. In addition to these moral concerns, economic democracy makes practical claims, such as that it can compensate for capitalism's inherent effective demand gap.\n\nProponents of economic democracy generally argue that modern capitalism periodically results in economic crises characterized by deficiency of effective demand as society is unable to earn enough income to purchase its output production. Corporate monopoly of common resources typically creates artificial scarcity, resulting in socio-economic imbalances that restrict workers from access to economic opportunity and diminish consumer purchasing power. Economic democracy has been proposed as a component of larger socioeconomic ideologies, as a stand-alone theory and as a variety of reform agendas. For example, as a means to securing full economic rights, it opens a path to full political rights, defined as including the former. Both market and non-market theories of economic democracy have been proposed. As a reform agenda, supporting theories and real-world examples range from decentralization and economic liberalization to democratic cooperatives, public banking, fair trade and the regionalization of food production and currency.\nAccording to many analysts, deficiency of effective demand is the most fundamental economic problem. That is, modern society does not earn enough income to purchase its output. For example, geographer David Harvey claims, \"Workers spending their wages is one source of effective demand, but the total wage bill is always less than the total capital in circulation (otherwise there would be no profit), so the purchase of wage goods that sustain daily life (even with a suburban lifestyle) is never sufficient for the profitable sale of the total output\". While balanced mixed economies have existed briefly throughout history, veteran Project Manager for the U.S. Treasury Department, Richard C. Cook and other critics claim that command economies are predominate, citing state capitalism and imperialism as related. As common resources are monopolized by imperial centers of wealth and power, conditions of scarcity are imposed artificially upon the majority, resulting in large-scale socio-economic imbalance.\n\nIn the Georgist view of any economic system, \"wealth\" includes all material things produced by labor for the satisfaction of human desires and having exchange value. Land, labor and capital are generally considered the essential factors in producing wealth. Land includes all natural opportunities and forces. Labor includes all human exertion. Capital includes the portion of wealth devoted to producing more wealth. While the income of any individual might include proceeds from any combination of these three sources—land, labor and capital are generally considered mutually exclusive factors in economic models of the production and distribution of wealth. According to Henry George: \"People seek to satisfy their desires with the least exertion\". Human beings interact with nature to produce goods and services that other human beings need or desire. The laws and customs that govern the relationships among these entities constitute the economic structure of a given society.\n\nAlternately, David Schweickart asserts in his book, \"After Capitalism\": \"The structure of a capitalist society consists of three basic components:\n\n\nSupply and demand are generally accepted as market functions for establishing prices. Organisations typically endeavor to 1) minimize the cost of production; 2) increase sales; in order to 3) maximize profits. But, according to David Schweickart, if \"those who produce the goods and services of society are paid less than their productive contribution\", then as consumers they cannot buy all the goods produced, and investor confidence tends to decline, triggering declines in production and employment. Such economic instability stems from a central contradiction: Wages are both a cost of production and an essential source of effective demand (needs or desires backed with purchasing power), resulting in deficiency of effective demand along with a growing interest in economic democracy.\n\nIn chapter 3 of his book, \"Community Organizing: Theory and Practice\", Douglas P. Biklen discusses a variety of perspectives on \"The Making of Social Problems\". One of those views suggests that \"writers and organizers who define social problems in terms of social and economic democracy see problems not as the experiences of poor people, but as the relationship of poverty to wealth and exploitation\". Biklen states that according to this viewpoint:\n\nIn his 1879 book \"Progress and Poverty\", Henry George argued that a majority of wealth created in a \"free market\" economy was appropriated by land owners and monopolists through economic rents, and that concentration of such unearned wealth was the root cause of poverty. \"Behind the abstraction known as 'the market' lurks a set of institutions designed to maximize the wealth and power of the most privileged group of people in the world—the creditor-rentier class of the first world and their junior partners in the third\". Schweickart claimed that private savings are not only unnecessary for economic growth, they are often harmful to the overall economy.\n\nIn an advanced industrial society, business credit is necessary for a healthy economy. A business that wants to expand production needs to command the labor of others, and money is the default mechanism for exercising this authority. It is often cheaper for a business to borrow capital from a bank than to stockpile cash.\n\nIf private savings are loaned out to entrepreneurs who use them to buy raw materials and hire workers, then aggregate demand is not reduced. However, when private savings are not reinvested, the whole economy suffers recession, unemployment, and disappearance of savings which characterize deficiency of effective demand.\n\nIn this view, unemployment is not an aberration, indicating any sort of systemic malfunction. Rather, unemployment is a necessary structural feature of capitalism, intended to discipline the workforce. If unemployment is too low, workers make wage demands that either cut into profits to an extent that jeopardizes future investment, or are passed on to consumers, thus generating inflationary instability. Schweickart suggested, \"Capitalism cannot be a full-employment economy, except in the very short term. For unemployment is the \"invisible hand\"—carrying a stick—that keeps the workforce in line.\" In this view, Adam Smith's \"invisible hand\" does not seem reliable to guide economic forces on a large scale.\n\nAssuming business credit could come from public sources rather than from private savers, Schweickart and other analysts consider interest payments to private savers both undeserved and unnecessary for economic growth. Moreover, the personal decision to save rather than consume decreases aggregate demand, increases the likelihood of unemployment, and exacerbates the tendency toward economic stagnation. Since wealthy people tend to save more than poor people, the propensity of an economy to slump because of excess saving becomes ever more acute as a society becomes more affluent. Richard Wilkinson and Kate Pickett suggested that health and social problems are significantly worse in more unequal wealthy nations. They argue that there are \"pernicious effects that inequality has on societies: eroding trust, increasing anxiety and illness, (and) encouraging excessive consumption\"\n\nRegarding a social and economic democracy perspective on social problems, Douglas P. Biklen states:\n\nThe discipline of economics is largely a study of scarcity management; \"the science which studies human behavior as a relationship between ends and scarce means which have alternative uses\". Absent scarcity and alternative uses of available resources, many analysts claim there is no economic problem\". For example, Richard C. Cook asserts that conditions of scarcity are artificially maintained by corporate structures that confine abundance to an exclusively entitled minority. In this view, socio-economic imbalance stems not from a failure to manage limited resources in a world of scarcity, but from mismanagement of virtually unlimited abundance and prosperity. American businessman Edward Kellogg (1790–1858) supports this perspective in his 1849 book, \"Labor and Other Capital\", where he states:\n\nWhile he considers these functions a public wrong, Kellogg also asserted the responsibility of the public to find and implement a remedy. Generally considered monopoly power, some view this \"public wrong\" as the most influential factor in artificial scarcity. For example, Henry George further suggested:\n\nFor example, many analysts consider invention a \"more or less costless store of knowledge, captured by monopoly capital and protected in order to make it secret and a 'rare and scarce commodity', for sale at monopoly prices. So far as invention is concerned, a price is put on them not because they are scarce but in order to make them scarce to those who want to use them.\" Patent monopolies raise share prices above tangible labor value. The difference between labor-value and monopoly-value raises goods prices, and is collected as \"profit\" by intermediaries who have contributed nothing to earn it.\n\nAnalysts generally agree that such conditions typically result in a deficiency of effective demand. Labor does not earn enough to buy what enterprises produce. For example, Richard C. Cook maintains that the difference between earnings and prices is typically appropriated by industrial and banking centers of capital through monopoly control of finance and other market resources. Such exclusive entitlement tends to artificially impose conditions of economic scarcity upon the majority of the population. While the accelerating advance of technology, developed and maintained by labor, tends to generate abundance, this process depresses wages as workers are replaced by machines, ironically minimizing the purchasing power of workers in the market. According to Jack Rasmus, author of \"The Trillion Dollar Income Shift\", in June 2006, investment bank Goldman Sachs reported: \"The most important contribution to the higher profit margins over the past five years has been a decline in Labor's share of national income.\" \n\nArtificially restricted access of labor to common resources is generally considered monopoly or enclosure of the commons. Due to the economic imbalance inherently imposed, such monopoly structures tend to be centrally dictated by law, and must be maintained by military force, trade agreements, or both.\n\nIn 1911, American journalist Ambrose Bierce defined \"land\" as:\n\nIn \"The Servile State\" (1912), Hilaire Belloc referred to the Enclosures Movement when he said, \"England was already captured by a wealthy oligarchy before the series of great industrial discoveries began\". If you sought the accumulated wealth preliminary to launching new industry, \"you had to turn to the class which had already monopolized the bulk of the means of production in England. The rich men alone could furnish you with those supplies\".\n\nAccording to Peter Barnes, author of \"Capitalism 3.0\", when Adam Smith wrote \"The Wealth of Nations\" in 1776, the dominant form of business was partnership, in which regional groups of co-workers ran co-owned businesses. From this perspective, many considered the corporate model—stock sold to strangers—inherently prone to fraud. While numerous scandals historically support this dim view of corporate policy, small partnerships could not possibly compete with the aggregate capital generated by corporate economies of scale. The greatest advantage of corporations over any other business model is their ability to raise capital from strangers. The corporate model benefits from laws that limit stockholders' liability to the amounts they have invested.\n\nIn \"A Preface To Economic Democracy\", Robert A. Dahl suggests that agrarian economy and society in the early United States \"underwent a revolutionary transformation into a new system of commercial and industrial capitalism that automatically generated vast inequalities of wealth, income, status, and power.\" Dahl claims that such inequalities result from the \"liberty to accumulate unlimited economic resources and to organize economic activity into hierarchically governed enterprises.\"\n\nAccording to author Greg MacLeod, the concept of the corporation originated in Roman times. However, \"the modern business corporation evolved radically from its ancient roots into a form with little relation to the purpose as understood by historians of law.\" John Davis, a legal historian, noted that the precursor of the business corporation was the first monastery, established in the sixth century, the purpose of which was to serve society. Most business corporations before 1900 developed in Great Britain, where they were established by royal charter, with the expectation of contributions to society. Incorporation was a privilege granted in return for service to the crown or the nation. MacLeod goes on to say:\n\nBy the mid-nineteenth century, corporations could live forever, engage in any legal activity, and merge with or acquire other corporations. In 1886, the U.S. Supreme Court legally recognized corporations as “persons”, entitled under the Fourteenth Amendment to the same protections as living citizens. Unlike average citizens, large corporations had large flows of money at their disposal. With this money they can hire lobbyists, donate copiously to politicians, and sway public opinion.\n\nBut, despite Supreme Court rulings, the modern corporation is not a real person. Rather, the publicly traded stock corporation is what Barnes terms an \"automaton\", explicitly designed to maximize return to its owners. A corporation never sleeps or slows down. It externalizes as many costs as possible, and never reaches an upper limit of profitability, because no such limit has yet been established. As a result, corporations keep getting larger. In 1955, sales of the Fortune 500 accounted for one-third of U.S. gross domestic product. By 2004 they commanded two-thirds. In other words, these few hundred corporations replaced smaller firms organized as partnerships or proprietorships. Corporations have established a homogeneous global playing field around which they can freely move raw materials, labor, capital, finished products, tax-paying obligations, and profits. Thus, corporate franchise has become a perpetual grant of sovereignty, including immortality, self-government, and limited liability. By the end of the twentieth century, corporate power—both economic and political—stretched worldwide. International agreements not only lowered tariffs but extended corporate property rights and reduced the ability of sovereign nations to regulate corporations.\n\nDavid Schweickart submits that such \"hypermobility of capital\" generates economic and political insecurity. \"If the search for lower wages comes to dominate the movement of capital, the result will be not only a lowering of worldwide wage disparities (the good to which some economists point) but also a lowering of total global income (a straight-out utilitarian bad).\" Jack Rasmus, author of \"The War At Home\" and \"The Trillion Dollar Income Shift\", argues that the increasing concentration of corporate power is a cause of the large-scale debt, unemployment, and poverty characteristic of economic recession and depression. According to Rasmus, income inequality in contemporary America increased as the relative share of income for corporations and the wealthiest one percent of households rose while income shares declined for 80-percent of the United States workforce. After rising steadily for three decades after World War II, the standard of living for most American workers has sharply declined between the mid-1970s to the present. Rasmus likens the widening income gap in contemporary American society to the decade leading up to the Great Depression, estimating \"well over $1 trillion in income is transferred annually from the roughly 90 million working class families in America to corporations and the wealthiest non-working-class households. While a hundred new billionaires were created since 2001, real weekly earnings for 100 million workers are less in 2007 than in 1980 when Ronald Reagan took office\".\n\nAccording to economist Richard D. Wolff, the 1970s brought an end to the labor shortage which had facilitated more than a century of rising average real wages in the United States. Wolff says Americans responded to the resulting deficiency of effective demand by working more hours and excessive borrowing; the latter paving the way for the financial crisis of 2007–08.\n\nAccording to David Harvey, \"the export of capital and the cultivation of new markets around the world\" is a solution \"as old as capitalism itself\" for the deficiency of effective demand. Imperialism, as defined by \"Dictionary of Human Geography\", is \"the creation and/or maintenance of an unequal economic, cultural, and territorial relationship, usually between states and often in the form of an empire, based on domination and subordination.\" \"These geographic shifts\", according to David Harvey, \"are the heart of uneven geographic development\".\n\nVladimir Lenin viewed imperialism as the highest stage of capitalism. He asserted that the merging of banks and industrial cartels gave rise to finance capital, which was then exported (rather than goods) in pursuit of greater profits than the home market could offer. Political and financial power became divided among international monopolist firms and European states, colonizing large parts of the world in support of their businesses. According to analyst Michael Parenti, imperialism is \"the process whereby the dominant politico-economic interests of one nation expropriate for their own enrichment the land, labor, raw materials, and markets of another people.\" Parenti says imperialism is older than capitalism. Given its expansionist nature, capitalism has little inclination to stay home. While he conceded imperialism is not typically recognized as a legitimate allegation about the United States, Parenti argued:\n\nIn his book, \"The Political Struggle for the 21st century\", J.W. Smith examines the economic basis for the history of imperial civilization. On a global scale, he says developed nations tended to impede or prohibit the economic and technological advancement of weaker developing countries through the military force, martial law, and inequitable practices of trade that typically characterize colonialism. Rhetorically termed as \"survival of the fittest\", or \"might makes right\", such economic crises stem from the imbalances imposed by corporate imperialism. Just as cities in the Middle Ages monopolized the means of production by conquering and controlling the sources of raw materials and countryside markets, Smith claims that contemporary centers of capital now control our present world through private monopoly of public resources sometimes known as \"the commons\". Through inequalities of trade, developing countries are overcharged for import of manufactured goods and underpaid for raw material exports, as wealth is siphoned from the periphery of empire and hoarded at the imperial-centers-of-capital:\n\nSmith goes on to say that, like other financial empires in history, the contemporary model forms alliances necessary to develop and control wealth, keeping peripheral nations impoverished providers of cheap resources for the imperial capital centers. Belloc estimated that, during the British Enclosures, \"perhaps half of the whole population was proletarian\", while roughly the other \"half\" owned and controlled the means of production. Under modern Capitalism, J.W. Smith claimed that fewer than 500 individuals possess more wealth than half of the earth's population. The wealth of 1/2 of 1-percent of the United States population roughly equals that of the lower 90-percent. \n\nRichard Cook claimed that the United States maintained stability by economically dominating the world as a means of filling the gap between production and consumption. Beginning with loans to European combatants during World War I, and continuing through the lend-lease program of World War II, U.S. domination of trade peaked through economic recovery measures following those wars. Though forming the basis for U.S. prosperity during the 1950s and 1960s, U.S trade domination was exhausted by the mid-1970s, when the United States implemented a policy known as dollar hegemony, intended to stabilize the economy.\n\nAdvocating for an \"alternative economic system free of capitalism's structural flaws\", economist Richard D. Wolff says reform agendas are fundamentally inadequate, given that capitalist corporations, the dominant institutions of the existing system, retain the incentives and the resources to undo any sort of reform policy. For example, Wolff goes on to say:\n\nAccording to David Schweickart, a serious critique of any problem cannot be content to merely note the negative features of the existing model. Instead, we must specify precisely the structural features of an alternative: \"But if we want to do more than simply denounce the evils of capitalism, we must confront the claim that 'there is no alternative'—by proposing one.\" Schweickart argued that both full employment and guaranteed basic income are impossible under the restrictions of the U.S. economic system for two primary reasons: a) unemployment is an essential feature of capitalism, not an indication of systemic failure; and b) while capitalism thrives under polyarchy, it is not compatible with genuine democracy. Assuming these \"democratic deficits\" significantly impact the management of both the workplace and new investment, many proponents of economic democracy tend to favor the creation and implementation of a new economic model over reform of the existing one.\n\nFor example, Dr. Martin Luther King Jr. claimed \"Communism forgets that life is individual. Capitalism forgets that life is social, and the Kingdom of Brotherhood is found neither in the thesis of Communism nor the antithesis of Capitalism but in a higher synthesis. It is found in a higher synthesis that combines the truths of both\". Regarding the gap between productivity and purchasing power, Dr. King maintained:\n\nAccording to historian and political economist, Gar Alperovitz: \"King’s final judgment stands as instructive evidence of his understanding of the nature of systemic challenge — and also as a reminder that given the failures of both traditional socialism and corporate capitalism, it is time to get serious about clarifying not only the question of strategy, but what, in fact, the meaning of changing the system in a truly democratic direction might one day entail.\"\n\nTrade unionist and social activist Allan Engler argued further that economic democracy was the working-class alternative to capitalism. In his book, \"Economic Democracy\", Engler stated:\n\nAssuming that \"democracy is not just a political value, but one with profound economic implications, the problem is not to choose between plan and market, but to integrate these institutions into a democratic framework\". Like capitalism, economic democracy can be defined in terms of three basic features:\n\n\nIn real-world practice, Schweickart concedes economic democracy will be more complicated and less \"pure\" than his model. However, to grasp the nature of the system and to understand its essential dynamic, it is important to have a clear picture of the basic structure. Capitalism is characterized by private ownership of productive resources, the market, and wage labor. The Soviet economic model subordinated private ownership of productive resources to public ownership by collectivizing farms and factories. It further subordinated the market to central planning—but retained the institution of wage labor.\n\nMost proposed models for economic democracy generally begin with democratizing the workplace and the ownership of capital. Other proposals advocate replacing the market with some form of planning, as well.\n\nIn worker self-management, each productive enterprise is controlled by those who work there. Workers are responsible for the operation of the facility, including organization, discipline, production techniques, and the nature, price, and distribution of products. Decisions concerning distribution are made democratically. Problems of authority delegation are solved by democratic representation. Management is chosen by the worker, not appointed by the State, not elected by the community at large and not selected by a board of directors elected by stockholders. Ultimate authority rests with the enterprise's workers, following the one-person, one-vote principle.\n\nAccording to veteran World Bank economic adviser David P. Ellerman it's the employment contract that needs to be abolished, not private property. In other words, \"a firm can be socialized and yet remain 'private' in the sense of not being government-owned.\" In his book, \"The Democratic Firm\", Ellerman stated:\n\nAlternately, in Schweickart's model, workers control the workplace, but they do not \"own\" the means of production. Productive resources are regarded as the collective property of the society. Workers run the enterprise, use its capital assets as they see fit, and distribute the profits among themselves. Here, societal \"ownership\" of the enterprise manifests itself in two ways: 1) All firms pay tax on their capital assets, which goes into society's investment fund. In effect, workers rent capital assets from society. 2) Firms are required to preserve the value of the capital stock entrusted to them. This means that a depreciation fund must be maintained to repair or replace existing capital stock. This money may be spent on capital replacements or improvements, but not to supplement workers' incomes. \n\nItaly's Legacoop and Spain's Mondragon multi-sectoral worker-cooperatives have both been able to reach significant scale and demonstrate long-term sustainability. According to a study conducted by Massachusetts Institute of Technology, the greatest lesson to be learned from these European experiences is the importance of developing an economically integrated network of cooperatives rather than a single cooperative. The report goes on to say:\n\nWhile there is no single approach or 'blueprint' for social control of investment, many strategies have been proposed. For example, Gar Alperovitz claims many real-world strategies have already emerged to democratize and decentralize the ownership of wealth and capital. In addition to worker cooperatives, Alperovitz highlights ESOPs, credit unions and other cooperative forms, social enterprises, municipally-owned utilities and public banks as starting points for what he has termed a \"Pluralist Commonwealth\".\n\nAlternately, David Schweickart proposes a flat-rate tax on capital assets to replace all other business taxes. This \"capital assets tax\" is collected and invested by the central government. Funds are dispersed throughout society, first to regions and communities on a per capita basis, then to public banks in accordance with past performance, then to those firms with profitable project proposals. Profitable projects that promise increased employment are favored over those that do not. At each level, national, regional and local, legislatures decide what portion of their funds is to be used for public capital expenditures, then send the remainder to the next lower level. Associated with most banks are entrepreneurial divisions, which promote firm expansion and new firm creation. For large (regional or national) enterprises, local investment banks are complemented by regional and national investment banks. These too would be public institutions that receive their funds from the national investment fund.\n\nBanks are public, not private, institutions that make grants, not loans, to business enterprises. According to Schweickart, these grants do not represent \"free money\", since an investment grant counts as an addition to the capital assets of the enterprise, upon which the capital-asset tax must be paid. Thus the capital assets tax functions as an interest rate. A bank grant is essentially a loan requiring interest payments but no repayment of principal.\n\nWhile an economy of worker-self-managed enterprises might tend toward lower unemployment than under capitalism - because banks are mandated to consistently prioritize investment projects that would increase employment - Schweickart notes that it does not guarantee full employment. Social control of investment serves to increase employment. If the market provides insufficient employment, the public sector becomes the employer of last resort. The original formulation of the U.S. Humphrey-Hawkins Act of 1978 assumed that only in this way could full employment be assured in a market economy. Economic Democracy adopts this approach. Social control of investment then blocks the cyclical unemployment typical of capitalism.\n\nHungarian historian Karl Polanyi suggested that market economies should subordinate themselves to larger societal needs. He states that human-beings, the source of labor, do not reproduce for the sole purpose of providing the market with workers. In The Great Transformation Polanyi says that, while modern states and market economies tend to grow under capitalism, both are mutually interdependent for functional development. In order for market economies to be truly prosperous, he claims social constructs must play an essential role. Polanyi claimed that land, labor, and money are all commodified under capitalism, though the inherent purpose of these items was never intended \"for sale\"—what he labels \"fictitious commodities.\" He says natural resources are \"God-given\", money is a bookkeeping entry validated by law, and labor is a human prerogative, not a personal obligation to market economies.\n\nSchweickart's economic democracy is a form of market economy, at least insofar as the allocation of consumer and capital goods is concerned. Firms buy raw materials and machinery from other firms and sell their products to other enterprises or consumers. \"Prices are largely unregulated except by supply and demand, although in some cases price controls or price supports might be in order – as they are deemed in order in most real-world forms of capitalism.\"\n\nWithout a price mechanism sensitive to supply and demand, it is extremely difficult for a producer or planner to know what and how much to produce, and which production and marketing methods are the most efficient. Otherwise, it is difficult to motivate producers to be both efficient and innovative. Market competition resolves these problems, to a significant if incomplete degree, in a non-authoritarian, non-bureaucratic fashion.\n\nEnterprises still strive to make a profit. However, \"profit\" in a worker-run firm is calculated differently than under capitalism. For a capitalist firm, labor is counted as a cost. For a worker-run enterprise it is not. Labor is not another \"factor of production\" on par with land and capital. Labor is the residual claimant. Workers get all that remains, once other costs, including depreciation set asides and the capital assets tax, have been paid.\n\nBecause of the way workplaces and the investment mechanism are structured, Schweickart's model aims to facilitate fair trade, not free trade, between nations. Under Economic Democracy, there would be virtually no cross-border capital flows. Enterprises themselves would not relocate abroad, since they are democratically controlled by their own workers. Finance capital stays mostly at home, since funds for investment are publicly generated and are mandated by law to be reinvested domestically. \"Capital doesn't flow into the country, either, since there are no stocks nor corporate bonds nor businesses to buy. The capital assets of the country are collectively owned – and hence not for sale.\"\n\nAccording to Michael Howard, \"in preserving commodity exchange, a market socialism has greater continuity with the society it displaces than does nonmarket socialism, and thus it is more likely to emerge from capitalism as a result of tendencies generated within it.\" But Howard also suggested, \"one argument against the market in socialist society has been that it blocks progress toward full communism or even leads back to capitalism\". From this perspective, nonmarket models of economic democracy have also been proposed.\n\nEconomic democracy is described as an integral component of an inclusive democracy in Takis Fotopoulos' \"Towards An Inclusive Democracy\" as a stateless, moneyless and marketless economy that precludes private accumulation of wealth and the institutionalization of privileges for some sections of society, without relying on a mythical post-scarcity state of abundance, or sacrificing freedom of choice.\n\nThe proposed system aims to meet the basic needs of all citizens (macroeconomic decisions), and secure freedom of choice (microeconomic decisions). Therefore, the system consists of two basic elements: (1) democratic planning, which involves a feedback process between workplace assemblies, demotic assemblies and a confederal assembly, and (2) an artificial market using personal vouchers, which ensures freedom of choice but avoids the adverse effects of real markets. Although David Pepper called this system \"a form of money based on the labour theory of value\", it is not a money model since vouchers cannot be used as a general medium of exchange and store of wealth.\n\nAnother distinguishing feature of inclusive democracy is its distinction between basic and non-basic needs. Remuneration is determined separately according to the cost of basic needs, and according to degree of effort for non-basic needs. Inclusive democracy is based on the principle that meeting basic needs is a fundamental human right which is guaranteed to all who are in a physical condition to offer a minimal amount of work. By contrast, participatory economics guarantees that basic needs are satisfied only for public goods or are covered by compassion and by a guaranteed basic income for the unemployed and those who cannot work. Many advocates of participatory economics and Participism have contested this.\n\nAs part of inclusive democracy, economic democracy is the authority of demos (community) in the economic sphere—which requires equal distribution of economic power. Therefore, all macroeconomic decisions (overall level of production, consumption and investment, amounts of work and leisure implied, technologies to be used and so on) are made collectively and without representation. However, microeconomic decisions are made by the individual production or consumption unit through a proposed system of vouchers.\n\nAs with the case of direct democracy, economic democracy is only feasible if the participants can easily cooperate.\n\nWhile reform agendas tend to critique the existing system and recommend corrective measures, they do not necessarily suggest alternative models to replace the fundamental structures of capitalism; private ownership of productive resources, the market and wage labor.\n\nRather than an economic shortfall, many analysts consider the gap between production and purchasing power a social dividend. In this view, credit is a public utility rather than debt to financial centers. Once reinvested in human productive potential, the surplus of societal output could actually increase Gross Domestic Product rather than throttling it, resulting in a more efficient economy, overall. Social Credit is an economic reform movement that originates from theories developed by Scottish engineer Major C. H. Douglas. His aim to make societal improvement the goal of economic systems is reflected in the term \"Social Credit\", and published in his book, entitled \"Economic Democracy\". In this view, the term \"economic democracy\" does not mean worker control of industry. While technological advancement tends to increase unemployment along with productivity, Douglas suggests that our perspective will determine whether this problem is a \"catastrophe\" or a \"magnificent achievement\":\n\nA national dividend and a compensated price mechanism are the two most essential components of the Social Credit program. While these measures have never been implemented in their purest form, they have provided a foundation for Social Credit political parties in many countries and for reform agendas that retain the title, \"economic democracy\".\n\nFollowing Douglas and a reform program based on direct government spending set forth by groups such as the American Monetary Institute, veteran Project Manager for the U.S. Treasury Department, Richard C. Cook proposes two general measures, which together he terms, \"economic democracy\":\n\n\n\nWhile Smith and others suggest an economic crisis might be necessary to drive a movement toward large-scale economic democracy, Cook argues that \"most economic reform programs address symptoms, not causes\":\n\nCook avoided collectivist solutions. Rather, he affirmed the value of \"democratic capitalism,\" combined with a shift to more public control of credit, and suggested a new approach to achieving worldwide prosperity, starting with economic recovery in the United States. Cook's argument stemmed from prior success in the United States treating credit as a public utility, including colonial paper currencies which allowed an emerging American society to monetize the value of its goods and services, the Greenbacks issued by President Abraham Lincoln during the American civil war, and the Reconstruction Finance Corporation (RFC) which moved to recapitalize failing state banks in rural areas and small towns during the Great Depression. While President Herbert Hoover's efforts failed, Cook credits RFC programs with providing low interest loans to the railroad industry, farmers, exporters, state and local governments, and wartime industries over a period of at least 20 years.\n\nCook also proposed a national dividend, sometimes known as a Basic Income Guarantee or \"BIG\", was advocated in the United States by economists, politicians and reformers, including Thomas Paine, Milton Friedman, Dr. Martin Luther King Jr., and John Kenneth Galbraith. Friedman originally proposed a negative income tax to support this system, but then opposed the bill because its revised implementation would have merely supplemented rather than replacing existing tax-structures. Cook suggested that racism might have been at the root of BIG's demise in the late 1960s, as \"many beneficiaries of the program would have been African-American\". In 2006, Representative Bob Filner (D-CA) proposed a modest basic income guarantee via an inflation-adjusted refundable tax credit. According to the U.S. Basic Income Guarantee Network:\n\nCook suggests existing U.S. (GDP) could support such a system. GDP of $12.98-trillion minus $9.21-trillion in purchasing power (\"wages\") equals a difference of $3.77-trillion. Distributed equally amongst United States citizens, Cook estimates a \"National Dividend\" of approximately $12,600 could be provided annually to every U.S. citizen. A primary function of monetary reform is to \"provide sufficient individual income\"—not merely \"create jobs\"—for American workers displaced by technological advancement, outsourcing, and other economic influences beyond their control. Funding of the National Dividend would be drawn from a national credit account, which would include all factors that generate production costs and create new capital assets. The national credit account could also be used for price subsidies to discourage manufacturers from cutting costs by shipping jobs overseas. \nRather than Federal Reserve Notes, circulated only through debt payable to a bank with interest, the National Dividend would be \"real money\", based on the productive capacity of the economy expressed as GDP. Cook says, \"it's important to realize that Social Credit is not a socialist system. Rather it is 'democratic capitalism,' in contrast to the 'finance capitalism' that has become so damaging\". Rooted in the ideals of Social Credit, proposed by Douglas in the 1920s, Cook explains:\n\nIn his book, \"Capitalism 3.0\", Peter Barnes likens a \"National Dividend\" to the game of Monopoly, where all players start with a fair distribution of financial opportunity to succeed, and try to privatize as much as they can as they move around \"the commons\". Distinguishing the board game from real-world business, Barnes claims that \"the top 5 percent of the population owns more property than the remaining 95 percent\", providing the smaller minority with an unfair advantage of approximately \"$5-trillion\" annually, at the beginning of the game. Contrasting \"redistribution\" of income (or property) with \"predistribution\", Barnes argues for \"propertizing\" (without corporately privatizing) \"the commons\" to spread ownership universally, without taking wealth from some and giving it to others. His suggested mechanism to this end is the establishment of a \"Commons Sector\", ensuring payment from the Corporate Sector for \"the commons\" they utilize, and equitably distributing the proceeds for the benefit of contemporary and future generations of society.\n\nOne real-world example of such reform is in the U.S. State of Alaska, where each citizen receives an annual share of the part of the state's oil revenues via the \"Alaska Permanent Fund Dividend\". Barnes suggests this model could extend to other states and nations because \"we jointly own many valuable assets\". As corporate pollution of common assets increased, the permits for such pollution would become more scarce, driving prices for those permits up. \"Less pollution would equal more revenue\", and over time, \"trillions of dollars could flow into an American Permanent Fund\".\n\nHowever, none of these proposals aspire to the mandates recommended by Dr. Martin Luther King Jr.:\n\nBarnes deemed any such reform unlikely. Thomas Paine originally recommended a National Dividend to compensate for the brutality of British Enclosures, but his idea was never adopted.\n\nRather than superficially compensating for legalized inequities, Smith recommends abolishing or redefining property rights laws with particular respect for \"the commons\". According to Smith exclusive title to natural resources and technologies should be converted to inclusive conditional titles—the condition being that society should collect rental values on all natural resources. Smith suggests the basic principles of monopolization under feudalism were never abandoned, and residues of exclusive feudal property rights restrict the potential efficiency of capitalism in Western cultures. He estimated that roughly 60 percent of American capital is little more than capitalized values of unearned wealth. He proposed that elimination of these monopoly values would double economic efficiency, maintain quality of life, and reduce working hours by half. Wasteful monetary flows could be stopped only by eliminating all methods of monopolization typical in Western economies.\n\nSmith divided \"primary (feudal) monopoly\" into four general categories: banking; land; technology and communications. He listed three general categories of \"secondary (modern) monopoly\"; insurance, law, health care. Smith further claimed that converting these exclusive entitlements to inclusive human rights would minimize battles for market share, thereby eliminating most offices and staff needed to maintain monopoly structures, and stop the wars generated to protect them. Dissolving roughly half the economic activity of a monopoly system would reduce the costs of common resources by roughly half, and significantly minimize the most influential factors of poverty.\n\nIn Smith's view, most taxes should be eliminated, and productive enterprise should be privately owned and managed. Inventors should be paid well and all technology placed in the public domain. Crucial services currently monopolized through licensing should be legislated as human rights.\n\nSmith envisioned a balanced economy under a socially owned banking commons within an inclusive society with full and equal rights for all. Federated regions collect resource rents on land and technology to a social fund to operate governments and care for social needs. Socially owned banks provide finance capital by creating debt-free money for social infrastructure and industry. Rental values return to society through expenditure on public infrastructures. Local labor is trained and employed to build and maintain water systems, sewers, roads, communication systems, railroads, ports, airports, post offices, and education systems. Purchasing power circulates regionally, as labor spends wages in consumption and governments spend resource rent and banking profits to maintain essential services.\n\nAccording to Smith, all monetary systems, including money markets, should function within fractional-reserve banking. Financial capital should be the total savings of all citizens, balanced by primary-created money to fill any shortfall, or its destruction through increased reserve requirements to eliminate any surplus. Adjustments of required reserves should facilitate the balance between building with socially created money or savings. Any shortage of savings within a socially owned banking system should be alleviated by simply printing it.\n\nA cooperative is an autonomous association of persons united voluntarily to meet their common economic, social, and cultural needs and aspirations through a jointly-owned and democratically-controlled enterprise. By various names, cooperatives play an essential role in all forms of Economic Democracy. Classified as either consumer cooperatives or worker cooperatives, the cooperative business model is fundamental to the interests of economic democracy.\n\nAccording to the International Cooperative Alliance's Statement on the Cooperative Identity, \"cooperatives are democratic organizations controlled by their members, who actively participate in setting policies and making decisions. Men and women serving as elected representatives are accountable to the membership. In primary cooperatives members have equal voting rights (one member, one vote) and cooperatives at other levels are also organized in a democratic manner.\"\n\nAccording to the United States Federation of Worker Cooperatives: \"Worker cooperatives are business entities that are owned and controlled by their members, the people who work in them. The two central characteristics of worker cooperatives are: 1) workers invest in and own the business and (2) decision-making is democratic, generally adhering to the principle of one worker-one vote.\" Worker cooperatives occupy multiple sectors and industries in the United States, mostly in the Northeast, the West Coast and the Upper Midwest, totaling 300 democratic workplaces in the United States, employing over 3,500 people and generating over $400 million in annual revenues. While a few are larger enterprises, most are small. Growing steadily between 1990 and 2010, technology and home health care experienced most of the recent increase.\n\nWorker cooperatives generally employ an industrial model called workplace democracy, which rejects the \"master-servant relationship\" implicit in the traditional employment contract. According to Wilkinson and Pickett, neither ownership or participation alone are sufficient to establish democracy in the workplace. \"[M]any share-ownership schemes amount to little more than incentive schemes, intended to make employees more compliant with management and sometimes to provide a nest-egg for retirement... To make a reliable difference to company performance, share-ownership has to be combined with more participative management methods.\" Dahl further argued that self-governing enterprises should not be confused with other systems they might resemble:\n\nIn worker cooperatives, net income is called surplus instead of profit and is distributed among the members based on hours worked, seniority, or other criteria. In a worker cooperative, workers own their jobs, and therefore have a direct stake in the local environment and the power to conduct business in ways that benefit the community rather than destroying it. Some worker cooperatives maintain what is known as a “multiple bottom line”, evaluating success not merely in terms of net income, but also by factors like their sustainability as a business, their contribution to the community, and the happiness and longevity of their workers.\n\nWorker-control can take many forms depending on the size and type of the business. Approaches to decision-making include: an elected board of directors, elected managers, management job roles, no management at all, consensus, majority vote, or combinations of the above. Participation in decision-making becomes the responsibility and privilege of each member. In one variation, workers usually invest money when they begin working. Each member owns one share, which provides its owner with one vote in company decision-making. While membership is not a requirement of employment, only employees can become members.\n\nAccording to Kenneth W. Stikkers, the Mondragon cooperatives in the Basque region of Spain have achieved a previously unknown level of economic democracy. Established in 1956, Mondragon has since become an economic model that transcends the capitalist-socialist dichotomy and thereby helps us to imagine creative solutions to current economic problems. Economist Richard D. Wolff argues that Mondragon is an example of \"a stunningly successful alternative to the capitalist organization of production.\"\n\nA consumers' cooperative is owned by its customers for their mutual benefit. Oriented towards service rather than profit, consumers often provide capital to launch or purchase the enterprise. In practice, consumer cooperatives price goods and services at competitive market rates. The co-op returns profits to the consumer/owner according to a formula instead of paying a separate investor group.\n\nIn his book, \"From Mondragon To America\", Greg MacLeod argues that \"in consumer cooperatives where the customer-members own the capital and the employees are subject to capital, the normal dynamic is the adversarial relationship of labor to capital. Sometimes the result is strikes of labor against management.\" In some cooperatives, however, consumer/owners are workers as well. For example, Mondragon has developed a large \"hybrid\" cooperative which sells groceries and furniture in Spain.\n\nConsumer cooperatives vary in organization and operations, but typically follow the Rochdale Principles. Consumer cooperatives may also form Co-operative Federations. These may take the form of co-operative wholesale societies, through which they collectively purchase goods at wholesale prices and, in some cases, cooperatively own factories. Alternatively, they may be members of Co-operative unions.\n\nConsumer cooperatives are very different from \"discount clubs,\" which charge annual fees in exchange for a discount on purchases. The club is not owned or governed by the members and profits go to investors, not to members.\n\nMost food co-ops are consumer cooperatives that specialize in grocery products. Members patronize the store and vote in elections. The members elect a board of directors to make high-level decisions and recruit managers. Food cooperatives were originally established to provide fresh, organic produce as a viable alternative to packaged imports. The ideas of local and slow food production can help local farmers prosper, in addition to providing consumers with fresher products. But the growing ubiquity of organic food products in corporate stores testifies to broadening consumer awareness, and to the dynamics of global marketing.\n\nFor example, associated with national and international cooperative communities, Portland Oregon cooperatives manage to survive market competition with corporate franchise. As Lee Lancaster, financial manager for Food Front, states, \"cooperatives are potentially one democratic economic model that could help guide business decisions toward meeting human needs while honoring the needs of society and nature\". He admits, however, it is difficult to maintain collaboration among cooperatives while also avoiding integration that typically results in centralized authority.\n\nAccording to Thomas H. Greco, Jr., author of New Money for Healthy Communities \"The pinnacle of power in today's world is the power to issue money. If that power can be democratized and focused in a direction which gives social and ecological concerns top priority, then there may yet be hope for saving the world\". In this regard, he recommended the regionalization of currencies. Cook suggested that \"under the Bretton Woods system, the Federal Reserve acted as the world's central bank. This gave America enormous leverage over economic policies of its principal trading partners\". Cook claimed that developing nations were susceptible to exploitation mainly because they have no independent monetary system, using the U.S. dollar instead. This fed the fractional reserve banking system, operated by the U.S., Canada, Europe, and Japan. Developing nations paid heavily for this service through market interest rates and because banking profits and property ownership emigrate to financial centers elsewhere.\n\nAccording to Smith, \"Currency is only the representation of wealth produced by combining land (resources), labor, and industrial capital\". He claimed that no country was free when another country has such leverage over its entire economy. But by combining their resources, Smith claimed that developing nations have all three of these foundations of wealth:\n\nSmith further explained that developed countries need resources from the developing world as much as developing countries need finance capital and technology from the developed world. Aside from the superior military power of the imperial centers, the undeveloped world actually has superior bargaining leverage. With independent trading currencies, developing countries could barter their resources to the developed world for the latest industrial technologies. Barter avoids \"hard money monopolization\" and the unequal trade between weak and strong nations that result. Smith suggested that barter was how Germany resolved many financial difficulties \"put in place to strangle her\", and that \"World Wars I and II settled that trade dispute\". He claimed that their intentions of exclusive entitlement were clearly exposed when the imperial centers resorted to military force to prevent such barter and maintain monopoly control of others' resources.\n\nWorkplace democracy has been cited as a possible solution to the problems that arise from excluding employees from decision-making such as low-employee morale, employee alienation, and low employee engagement.\n\nPolitical theorist Isabelle Ferreras argues that there exists “a great contradiction between the democratic nature of our times and the reality of the work experience.” She argues that the modern corporation's two basic inputs, capital and labor, are treated in radically different ways. Capital owners of a firm wield power within a system of shareholder democracy that allocates voice democratically according to how much capital investment they place in the firm. Labor, on the other hand, rarely benefits from a system to voice their concerns within the firm. She argues that firms are more than just economic organizations especially given the power that they wield over people's livelihoods, environment, and rights. Rather, Ferreras holds that firms are best understood as political entities. And as political entities “it is crucial that firms be made compatible with the democratic commitments of our nations.”\n\nGermany and to a lesser extent the broader European Union have experimented with a way of workplace democracy known as Co-determination, a system that allows workers to elect representatives that sit on the board of directors of a company. Common criticisms of workplace democracy include that democratic workplaces are less efficient than hierarchical workplace, that managers are best equipped to make company decisions since they are better educated and aware of the broader business context.\n\nOne of the biggest criticisms against capitalism is that it concentrates economic and, as a result, political power in few hands. Theorists of economic democracy have argued that one solution to this unequal concentration of power is to create mechanism that distribute ownership of productive assets across the entire population. In \"Justice as Fairness: A Restatement\", John Rawls argues that only two systems could embody the main features of his principles of justice: liberal socialism or a property-owning democracy. Within property-owning democracy Rawls envisions widespread use of worker-owned cooperatives, partial-employee ownership of firms, systems to redistribute one's asset after death to prevent the accumulation of wealth, as well as a strong system of asset-based redistribution that encourages workers to own productive assets.\n\nOperating under the idea that making ownership more widespread leads to more equitable outcomes various proposals of asset-based welfare and asset-redistribution have been conceived. Individualistic and liberal asset-based welfare strategies such as the United Kingdom's Child Trust Fund of the United States Individual Development Account aimed to help people save money so that it could be invested on education, home-ownership, or entrepreneurship. More expirmental and left-leaning proposals include worker owned cooperatives, ESOPS, or Roemers coupon socialism.\n\nLudwig von Mises argued that ownership and control over the means of production belongs to private firms and can only be sustained by means of consumer choice, exercised daily in the marketplace. \"The capitalistic social order\", he claimed, therefore \"is an economic democracy in the strictest sense of the word\". Critics of this claim point out that consumers only vote on the value of the product when they make a purchase—they are not participating in the management of firms, or voting on how the profits are to be used.\n\n\n\n\n\n"}
{"id": "3626123", "url": "https://en.wikipedia.org/wiki?curid=3626123", "title": "Ekaggata", "text": "Ekaggata\n\nEkaggatā (Pali; Sanskrit \"ekāgratā\", एकाग्रता) is a Buddhist term translated as \"unification of awareness\" or \"one-pointedness.\" It is a mental factor that has the function to focus on an object.\n\n\"Ekaggatā\" is identified within the Buddhist teachings as:\n\nBhikkhu Bodhi states:\n\nBhikkhu Bodhi also explains that at the level of profound concentration (i.e. in the jhanas), it manifests as peace, and its proximate cause is happiness. \n\nNina van Gorkom explains:\n\nThe Atthasālinī (1, Part IV, Chapter 1. 118, 119) states about ekaggatā (in the context of sammā-samādhi):\n\nAjahn Sucitto explains:\n\n\n\n"}
{"id": "22944626", "url": "https://en.wikipedia.org/wiki?curid=22944626", "title": "Everything which is not forbidden is allowed", "text": "Everything which is not forbidden is allowed\n\n\"Everything which is not forbidden is allowed\" is a constitutional principle of English law—an essential freedom of the ordinary citizen or subject. The converse principle—\"everything which is not allowed is forbidden\"—used to apply to public authorities, whose actions were limited to the powers explicitly granted to them by law. The restrictions on local authorities were lifted by the Localism Act 2011 which granted a \"general power of competence\" to local authorities.\n\nIn \"The Once and Future King\", author T. H. White proposed the opposite as the rule of totalitarianism: \"Everything which is not forbidden is compulsory.\" This quote has been suggested as a principle of physics.\n\nThe claim that “everything which is not fobidden is allowed” is a principle only of English law does not stand the test of reality. In fact, it is a common principle of liberal democracies. In Germany, it has constitutional rank under Art. 2(1) of the GG which protects the general freedom to act (Allgemeine Handlungsfreiheit), as demonstrated e.g. by the judgment of the Bundesverfassungsgericht known as “Reiten im Walde” (BVerfGE 80, 137).\n\n"}
{"id": "9670", "url": "https://en.wikipedia.org/wiki?curid=9670", "title": "Evolutionism", "text": "Evolutionism\n\nEvolutionism describes the belief in the evolution of organisms. Its exact meaning has changed over time as the study of evolution has progressed. In the 19th-century, it was used to describe the belief that organisms deliberately improved themselves through progressive inherited change (orthogenesis). The teleological belief went on to include cultural evolution and social evolution. In the 1970s the term Neo-Evolutionism was used to describe the idea \"that human beings sought to preserve a familiar style of life unless change was forced on them by factors that were beyond their control\".\n\nThe term is also sometimes used by the creationist movement to describe adherence to the scientific consensus on evolution as equivalent to a secular religion. The term is very seldom used within the scientific community, since the scientific position on evolution is accepted by the overwhelming majority of scientists. Because evolutionary biology is the default scientific position, it is assumed that \"scientists\" or \"biologists\" are \"evolutionists\" unless specifically noted otherwise. In the creationevolution controversy, creationists often call those who accept the validity of the modern evolutionary synthesis \"evolutionists\" and the theory itself \"evolutionism\".\n\nBefore its use to describe biological evolution, the term \"evolution\" was originally used to refer to any orderly sequence of events with the outcome somehow contained at the start. The first five editions of Darwin's in \"Origin of Species\" used the word \"evolved\", but the word \"evolution\" was only used in its sixth edition in 1872. By then, Herbert Spencer had developed the concept theory that organisms strive to evolve due to an internal \"driving force\" (orthogenesis) in 1862. Edward B. Tylor and Lewis H Morgan brought the term \"evolution\" to anthropology though they tended toward the older pre-Spencerian definition helping to form the concept of unilineal (social) evolution used during the later part of what Trigger calls the Antiquarianism-Imperial Synthesis period (c1770-c1900). The term evolutionism subsequently came to be used for the now discredited theory that evolution contained a deliberate component, rather than the selection of beneficial traits from random variation by differential survival.\n\nIn modern times, the term \"evolution\" is widely used, but the terms \"evolutionism\" and \"evolutionist\" are seldom used in the scientific community to refer to evolutionary biology, since the term is considered both redundant and anachronistic. \n\nHowever, the term has been used by creationists in discussing the creation-evolution controversy. For example, the Institute for Creation Research, in order to imply placement of evolution in the category of 'religions', including atheism, fascism, humanism and occultism, commonly uses the words \"evolutionism\" and \"evolutionist\" to describe the consensus of mainstream science and the scientists subscribing to it, thus implying through language that the issue is a matter of religious belief. The BioLogos Foundation, an organization that promotes the idea of theistic evolution, uses the term \"evolutionism\" to describe \"the atheistic worldview that so often accompanies the acceptance of biological evolution in public discourse.\" It views this as a subset of scientism.\n\n\n"}
{"id": "17687", "url": "https://en.wikipedia.org/wiki?curid=17687", "title": "False dilemma", "text": "False dilemma\n\nA false dilemma is a type of informal fallacy in which something is falsely claimed to be an \"either/or\" situation, when in fact there is at least one additional option.\n\nA false dilemma can arise intentionally, when a fallacy is used in an attempt to force a choice or outcome. The opposite of this fallacy is false compromise. For example, what is described as the \"TINA factor\" in elections is often in reality a false dilemma, as there are about 3 to 25 electoral candidates for most electoral seats.\n\nThe false dilemma fallacy can also arise simply by accidental omission of additional options rather than by deliberate deception. For example, \"Stacey spoke out against capitalism, therefore she must be a communist\" (she may be neither capitalist nor communist). \"Roger opposed an atheist argument against Christianity, so he must be a Christian\" (When it's assumed the opposition by itself means he's a Christian). Roger might be an atheist who disagrees with the logic of some particular argument against Christianity. Additionally, it can be the result of habitual tendency, whatever the cause, to view the world with limited sets of options.\n\nSome philosophers and scholars believe that \"unless a distinction can be made rigorous and precise it isn't really a distinction\". An exception is analytic philosopher John Searle, who called it an incorrect assumption that produces false dichotomies. Searle insists that \"it is a condition of the adequacy of a precise theory of an indeterminate phenomenon that it should precisely characterize that phenomenon as indeterminate; and a distinction is no less a distinction for allowing for a family of related, marginal, diverging cases.\" Similarly, when two options are presented, they often are, although not always, two extreme points on some spectrum of possibilities; this may lend credence to the larger argument by giving the impression that the options are mutually exclusive of each other, even though they need not be. Furthermore, the options in false dichotomies typically are presented as being collectively exhaustive, in which case the fallacy may be overcome, or at least weakened, by considering other possibilities, or perhaps by considering a whole spectrum of possibilities, as in fuzzy logic.\n\nCommon phrases expressing similar or synonymous concepts include:\n\n\nThe presentation of a \"false choice\" often reflects a deliberate attempt to eliminate several options that may occupy the middle ground on an issue. A common argument against noise pollution laws involves a false choice. It might be argued that in New York City noise should not be regulated, because if it were, the city would drastically change in a negative way. This argument assumes that, for example, a bar must be shut down to prevent disturbing levels of noise emanating from it after midnight. This ignores the fact that the bar could simply lower its noise levels, or install soundproofing structural elements to keep the noise from excessively transmitting onto others' properties.\n\nIn psychology, a phenomenon related to the false dilemma is black-and-white thinking. There are people who routinely engage in black-and-white thinking, an example of which is someone who categorizes other people as all good or all bad.\n\n"}
{"id": "49693383", "url": "https://en.wikipedia.org/wiki?curid=49693383", "title": "Fusion adaptive resonance theory", "text": "Fusion adaptive resonance theory\n\nFusion adaptive resonance theory (fusion ART) is a generalization of self-organizing neural networks known as Adaptive Resonance Theory for learning recognition categories (or cognitive codes) across multiple pattern channels. It unifies a number of neural network models, supports several learning paradigms, notably unsupervised learning, supervised learning, and reinforcement learning, and can be applied for domain knowledge integration, memory representation, and modelling of high level cognition.\n\nFusion adaptive resonance theory models is a natural extension of the original adaptive resonance theory (ART) models developed by Stephen Grossberg and Gail A. Carpenter from a single pattern field to multiple pattern channels. Whereas the original ART models perform unsupervised learning of recognition nodes in response to incoming input patterns, fusion ART learns multi-channel mappings simultaneously across multi-modal pattern channels in an online and incremental manner.\n\nFusion ART employs a multi-channel architecture (as shown below), comprising a category field formula_1 connected to a fixed number of (\"K\") pattern channels or input fields formula_2 through bidirectional conditionable pathways. The model unifies a number of network designs, most notably Adaptive Resonance Theory (ART), Adaptive Resonance Associative Map (ARAM) and Fusion Architecture for Learning and COgNition (FALCON), developed over the past decades for a wide range of functions and applications.\n\nGiven a set of multimodal patterns, each presented at a pattern channel, the fusion ART pattern encoding cycle comprises five key stages, namely code activation, code competition, activity readout, template matching, and template learning, as described below.\n\nThe network dynamics described above can be used to support numerous learning operations. We show how fusion ART can be used for a variety of traditionally distinct learning tasks in the subsequent sections.\n\nWith a single pattern channel, the fusion ART architecture reduces to the original ART model. Using a selected vigilance value $\\rho$, an ART model learns a set of recognition nodes in response to an incoming stream of input patterns in a continuous manner. Each recognition node in the formula_1 field learns to encode a template pattern representing the key characteristics of a set of patterns. ART has been widely used in the context of unsupervised learning for discovering pattern groupings. Please refer to the selected ART literatures for a review of ART's functionalities, interpretations, and applications.\n\nBy synchronizing pattern coding across multiple pattern channels, fusion ART learns to encode associative mappings across distinct pattern spaces. A specific instance of fusion ART with two pattern channels is known as adaptive resonance associative map (ARAM), that learns multi-dimensional supervised mappings from one pattern space to another pattern space. An ARAM system consists of an input field formula_18, an output field formula_19, and a category field formula_1. Given a set of feature vectors presented at formula_18 with their corresponding class vectors presented at formula_19, ARAM learns a predictive model (encoded by the\nrecognition nodes in formula_1) that associates combinations of key features to their respective classes.\n\nFuzzy ARAM, based on fuzzy ART operations, has been successfully applied to numerous machine learning tasks, including personal profiling, document classification, personalized content management, and DNA gene expression analysis. In many benchmark experiments, ARAM has demonstrated predictive performance superior to those of many state-of-the-art machine learning systems, including C4.5, Backpropagation Neural Network, K Nearest Neighbour, and Support Vector Machines.\n\nDuring learning, fusion ART formulates recognition categories of input patterns across multiple channels. The knowledge that fusion ART discovers during learning, is compatible with symbolic rule-based representation. Specifically, the recognition categories learned by the formula_1 category nodes are compatible with a class of IF-THEN rules that maps a set of input attributes (antecedents) in one pattern channel to a disjoint set of output attributes (consequents) in another channel. Due to this\ncompatibility, at any point of the incremental learning process, instructions in the form of IF-THEN rules can be readily translated into the recognition categories of a fusion ART system. The rules are conjunctive in the sense that the attributes in the\nIF clause and in the THEN clause have an \"AND\" relationship. Augmenting a fusion ART network with domain knowledge through\nexplicit instructions serves to improve learning efficiency and predictive accuracy.\n\nThe fusion ART rule insertion strategy is similar to that used in Cascade ARTMAP, a generalization of ARTMAP that performs domain knowledge insertion, refinement, and extraction. For direct knowledge insertion, the IF and THEN clauses of each instruction (rule) is translated into a pair of vectors A and B respectively. The vector pairs derived are then used\nas training patterns for inserting into a fusion ART network. During rule insertion, the vigilance parameters are set to 1s to ensure that each distinct rule is encoded by one category node.\n\nFor details on integrating domain knowledge into fusion ART, please refer to a recent paper.\n\nReinforcement learning is a paradigm wherein an autonomous system learns to adjust its behaviour based on\nreinforcement signals received from the environment. An instance of fusion ART, known as FALCON (fusion architecture for learning and cognition), learns mappings simultaneously across multi-modal input patterns, involving states, actions, and rewards, in an online and incremental manner. Compared with other ART-based reinforcement learning systems, FALCON presents a truly\nintegrated solution in the sense that there is no implementation of a separate reinforcement learning module or Q-value table. Using competitive coding as the underlying principle of computation, the network dynamics encompasses several learning\nparadigms, including unsupervised learning, supervised learning, as well as reinforcement learning.\n\nFALCON employs a three-channel architecture, comprising a category field formula_1 and three pattern fields, namely a sensory field formula_26 for representing current states, a motor field formula_27 for representing actions, and a feedback field formula_28 for representing reward values. A class of FALCON networks, known as TD-FALCON, incorporates Temporal Difference (TD) methods to estimate and learn value function \"Q(s,a)\", that indicates the goodness to take a certain action \"a\" in a given state \"s\".\n\nThe general sense-act-learn algorithm for TD-FALCON is summarized. Given the current state \"s\", the FALCON network is used to predict the value of performing each available action \"a\" in the action set A based on the corresponding state vector formula_29 and action vector formula_30. The value functions are then processed by an action selection strategy (also known as policy) to select an action. Upon receiving a feedback (if any) from the environment after performing the\naction, a TD formula is used to compute a new estimate of the Q-value for performing the chosen action in the current state. The new Q-value is then used as the teaching signal (represented as reward vector R) for FALCON to learn the association of the current state and the chosen action to the estimated value.\n\n"}
{"id": "15298616", "url": "https://en.wikipedia.org/wiki?curid=15298616", "title": "Galaxy color–magnitude diagram", "text": "Galaxy color–magnitude diagram\n\nThe galaxy color–magnitude diagram shows the relationship between absolute magnitude (a measure of luminosity) and mass of galaxies. A preliminary description of the three areas of this diagram was made in 2003 by Eric F. Bell et al. from the COMBO-17 survey that clarified the bimodal distribution of red and blue galaxies as seen in analysis of Sloan Digital Sky Survey data and even in de Vaucouleurs' 1961 analyses of galaxy morphology. \n\nNoticed in this diagram are three main features: the red sequence, the green valley, and the blue cloud. The red sequence includes most red galaxies which are generally elliptical galaxies. The blue cloud includes most blue galaxies which are generally spirals. In between the two distributions is an underpopulated space known as the green valley which includes a number of red spirals. \n\nUnlike the comparable Hertzsprung–Russell diagram for stars, galaxy properties are not necessarily completely determined by their location on the color–magnitude diagram. The diagram also shows considerable evolution through time. The red sequence earlier in evolution of the universe was more constant in color across magnitudes and the blue cloud was not as uniformly distributed but showed sequence progression.\n\nNew research suggests the green valley is actually composed of two different populations of galaxies: one of late-type galaxies, where star formation has been quenched due to their gas supplies being shut off followed by exhaustion of their gas reservoirs for several billion years, and another of early-type galaxies where both the gas supplies and gas reservoirs have been destroyed very quickly, likely because of mergers with other galaxies and/or the presence of an active galactic nucleus.\n\nThe Milky Way and the Andromeda Galaxy are assumed to lie in the green valley because their star formation is slowing down due to running out of gas.\n"}
{"id": "8880793", "url": "https://en.wikipedia.org/wiki?curid=8880793", "title": "Gray ceiling", "text": "Gray ceiling\n\nThe gray ceiling is a business/societal phenomenon where the existing workforce of those born during the baby boom era prevents the slightly younger Generation Xers from advancing or being promoted at their jobs.\n\nThe gray ceiling phenomenon is most likely unintentional. Although it is true that baby boomers are staying on the job longer than previous generations, by sheer number they are also competing within their own generation and their children who are joining the workforce at the turn of the 21st century.\n\nAs the children of the baby boomers advance from below, the Gen-Xers, usually with middle management jobs, feel threatened and trapped in a job that is going nowhere and might be given away to the next younger candidate.\n\n"}
{"id": "4104986", "url": "https://en.wikipedia.org/wiki?curid=4104986", "title": "How to Solve it by Computer", "text": "How to Solve it by Computer\n\nHow to Solve it by Computer is a computer science book by R. G. Dromey, first published by Prentice-Hall in 1982.\nIt is occasionally used as a textbook, especially in India.\n\nIt is an introduction to the \"why\"s of algorithms and data structures.\nFeatures of the book:\n\nThe very fundamental algorithms portrayed by this book are mostly presented in Pseudocode and/or Pascal notation.\n\n"}
{"id": "39948152", "url": "https://en.wikipedia.org/wiki?curid=39948152", "title": "Impairment rating", "text": "Impairment rating\n\nAn impairment rating is a percentage intended to represent the degree of an individual's impairment, which is a deviation away from one's normal health status and functionality. Impairment is distinct from disability. An individual's impairment rating is based on the restrictive impact of an impairment, whereas disability is broadly the consequences one's impairment. Impairment ratings given to an individual by different medical examiners are sometimes problematically inconsistent with each other.\n"}
{"id": "30138254", "url": "https://en.wikipedia.org/wiki?curid=30138254", "title": "Intervasion of the UK", "text": "Intervasion of the UK\n\nThe Intervasion of the UK was a 1994 electronic civil disobedience and collective action against John Major's Criminal Justice Bill which sought to outlaw outdoor dance festivals and \"music with a repetitive beat\". Launched by a group called The Zippies from San Francisco's 181 Club on Guy Fawkes Day, November 5, 1994, it resulted in government websites going down for at least a week. It utilised a form of Distributed Denial of Service (DDoS) known as the Email bomb in order to overload servers as a form of online protest and Internet activism. It was the first such use of the Internet and technology as a weapon of struggle and/or civil disobedience, and preceded the .\n\nUnder the Criminal Justice and Public Order Act of 1994, the definition of music played at a rave was given as: \"music includes sounds wholly or predominantly characterised by the emission of a succession of repetitive beats\".\n\nSections 63, 64 & 65 of the Act targeted electronic dance music played at raves. The Criminal Justice and Public Order Act empowered police to stop a rave in the open air when \"ten or more people are attending, or where two or more are making preparations for a rave\". Section 65 allowed any uniformed constable who believes a person is on their way to a rave within a radius to stop them and direct them away from the area; \"non-compliant citizens may be subject to a maximum fine not exceeding level 3 on the standard scale (£1000)\".\n\nThe Zippies sought to jam the mailboxes of UK politicians associated with the Bill in order to bring their attention to the issue of natural justice involving basic rights and freedoms. In effect, the collective action was saying: \"If you take away our freedom, we have the power to take away something you take for granted, and to do this in a way which deploys the Internet as a weapon\".\n\nSeveral hackers not directly associated with the group launched all-out penetration testing and load testing operations against several UK government websites, resulting in a tit-for-tat battle, as the Zippies mailbox on morph.com went down, along with the entire server of Morph, a well-known Bay Area BBS.\n\nThe event was broadcast on Radio Free Berkeley.\n\nThe protest action occurred during the manhunt for Kevin Mitnick and was thus partly an underground event. The media also refused to entertain the implications of electronic civil disobedience, with public attention focused on the problem of illegal raves and black-hat hackers, prompting scare stories about \"evil hackers\" and \"young hoodlums\" penetrating the British defences while the zippies were written off as nothing more than electrohippies.\n\nSome criticism of the Intervasion was expressed by the Electronic Frontier Foundation who complained about the \"lack of a cutoff date\". Other criticism on the Whole Earth 'Lectronic Link (WELL) BBS centered on the use of militant language. The online protest action also suffered from conflict between its stated aims: \"To clog the servers of the UK government\", and its call to send email messages to UK politicians. Thus jamming mailboxes with email and large file attachments defeated the purpose of the exercise which was essentially an early form of hacktivism. The nature of the collective action was also not articulated well enough. For instance, the digital be-in which occurred during the launch of Timothy Leary's Chaos and Cyberculture, in which Leary was \"kidnapped\" without the consent of his publisher, and \"forced\" to DDoS the UK government, was simply a protest message, sent repeatedly to mail-boxes around the world.\n\n\n"}
{"id": "50959785", "url": "https://en.wikipedia.org/wiki?curid=50959785", "title": "KHOPCA clustering algorithm", "text": "KHOPCA clustering algorithm\n\nKHOPCA is an adaptive clustering algorithm originally developed for dynamic networks. KHOPCA (formula_1-hop clustering algorithm) provides a fully distributed and localized approach to group elements such as nodes in a network according to their distance from each other. KHOPCA operates proactively through a simple set of rules that defines clusters, which are optimal with respect to the applied distance function.\n\nKHOPCA's clustering process explicitly supports joining and leaving of nodes, which makes KHOPCA suitable for highly dynamic networks. However, it has been demonstrated that KHOPCA also performs in static networks.\n\nBesides applications in ad hoc and wireless sensor networks, KHOPCA can be used in localization and navigation problems, networked swarming, and real-time data clustering and analysis.\n\nKHOPCA (formula_1-hop clustering algorithm) operates proactively through a simple set of rules that defines clusters with variable formula_1-hops. A set of local rules describes the state transition between nodes. A node's weight is determined only depending on the current state of its neighbors in communication range. Each node of the network is continuously involved in this process. As result, formula_1-hop clusters are formed and maintained in static as well as dynamic networks.\n\nKHOPCA does not require any predetermined initial configuration. Therefore, a node can potentially choose any weight (between formula_5 and formula_6). However, the choice of the initial configuration does influence the convergence time.\n\nThe prerequisites in the start configuration for the application of the rules are the following.\nThe following rules describe the state transition for a node formula_9 with weight formula_25. These rules have to be executed on each node in the order described here.\n\nThe first rule has the function of constructing an order within the cluster. This happens through a node formula_9 detects the direct neighbor with the highest weight formula_27, which is higher than the node's own weight formula_25. If such a direct neighbor is detected, the node formula_9 changes its own weight to be the weight of the highest weight within the neighborhood subtracted by 1. Applied iteratively, this process creates a top-to-down hierarchical cluster structure.\nif max(W(N(n))) > w_n\n\nThe second rule deals with the situation where nodes in a neighborhood are on the minimum weight level. This situation can happen if, for instance, the initial configuration assigns the minimum weight to all nodes. If there is a neighborhood with all nodes having the minimum weight level, the node formula_9 declares itself as cluster center. Even if coincidently all nodes declare themselves as cluster centers, the conflict situation will be resolved by one of the other rules.\nif max(W(N(n)) == MIN & w_n == MIN\n\nThe third rule describes situations where nodes with leveraged weight values, which are not cluster centers, attract surrounding nodes with lower weights. This behavior can lead to fragmented clusters without a cluster center. In order to avoid fragmented clusters, the node with higher weight value is supposed to successively decrease its own weight with the objective to correct the fragmentation by allowing the other nodes to reconfigure according to the rules. \nif max(W(N(n))) <= w_n && w_n != MAX\n\nThe fourth rule resolves the situation where two cluster centers connect in 1-hop neighborhood and need to decide which cluster center should continue its role as cluster center. Given any specific criterion (e.g., device ID, battery power), one cluster center remains while the other cluster center is hierarchized in 1-hop neighborhood to that new cluster center. The choice of the specific criterion to resolve the decision-making depends on the used application scenario and on the available information. \nif max(W(N(n)) == MAX && w_n == MAX\n\nAn exemplary sequence of state transitions applying the described four rules is illustrated below.\n\nKHOPCA acting in a dynamic 2-D simulation. The geometry is based on a geometric random graph; all existing links are drawn in this network.\n\nKHOPCA also works in a dynamic 3-D environment. The cluster connections are illustrated with bold lines.\n\nIt has been demonstrated that KHOPCA terminates after a finite number of state transitions in static networks.\n"}
{"id": "17038145", "url": "https://en.wikipedia.org/wiki?curid=17038145", "title": "Krvna osveta", "text": "Krvna osveta\n\nKrvna Osveta (, Blood feud) is a law of vendetta in Montenegro and Herzegovina, practised by Montenegrins, Serbs, Bosniaks and Albanian families throughout history since medieval times. It is an oath of revenge for vendetta, meaning that the person must take revenge on whomever killed his relative by killing the murderer or one of the murderer's close relatives.\n\nThe practice started in the Balkans in the 15th century, under Turkish rule and the law decreased in the 19th century, when the Balkan countries slowly got their independence from the Ottoman Empire. In pre-Ottoman Serbian principalities, blood money (\"Vražda\") was paid, one half went to the Serbian Orthodox Church, while the other to the victim's family. Stefan Uroš (1240–1272) talks about the Vražda in his works. After Ottoman conquest of Serbia, self-governing clans often feuded with each other. Families in Serbia abandoned the tradition as the bigger threats to family integrity was ethnic Albanians and Turks rather than of their own ethnic group.\n\nWhen a family member has been killed, the perpetrator's family has a \"Blood debt\" (\"krvni dug\") which can only be removed when the victim's family (an appointed member, \"osvetnik\") has had their revenge by killing the aggressor or any member of the murderer's family (Often a close male kinsman, preferably the brother, killing of children was not encouraged). Only then has the family of the victim received peace (However, the blood feud continues if a relative decides to revenge, disregardless of who started). However, killing in your own house is the worst action, representing unmorality, which is a great shame in Montenegrin and Albanian cultures. If a criminal was murdered, it often did not result in a feud as criminality was negative in the eyes of society, but in some cases the criminal's family went on to kill serdars and other high ranked people.\n\n\nThe Osveta is not limited to males, females that have their husbands or relatives killed could take on the \"blood debt\", an instance is recorded from the Bjelopavlići clan, where a widow took out revenge for the murder of her husband.\n\nIf a bratstva finds and captures a thief or murderer (in connection to the bratstva) they could go to the person's house or relatives and tell them that their relative is a murderer or thief and end with something like \"If we kill him, we are not to be held for\" If the relatives answers \"Do what you like with him\" -the bratstva, if they kill the captive, they don't have a blood debt to his relatives because they settled his fate.\n\nThe blood feuds resulted in major instability in Montenegro, Kosovo and diaspora of Montenegrins in later centuries. In Kosovo, most cases of blood feuds were reconciled in the early 1990s in the course of a large-scale reconciliation movement to end blood feuds led by Anton Çetta. The largest reconciliation gathering took place at Verrat e Llukës on 1 May 1990, which had between 100,000 and 500,000 participants. By 1992 the reconciliation campaign ended at least 1,200 deadly blood feuds, and in 1993 not a single homicide occurred in Kosovo.\n\n"}
{"id": "35240745", "url": "https://en.wikipedia.org/wiki?curid=35240745", "title": "Lexical hypothesis", "text": "Lexical hypothesis\n\nThe lexical hypothesis (also known as the fundamental lexical hypothesis, lexical approach, or sedimentation hypothesis) is a thesis, current primarily in early personality psychology, and subsequently subsumed by many later efforts in that subfield. Despite some variation in its definition and application, the hypothesis is generally defined by two postulates. The first states that those personality characteristics that are important to a group of people will eventually become a part of that group's language. The second follows from the first, stating that more important personality characteristics are more likely to be encoded into language as a single word. With origins in the late 19th century, use of the lexical hypothesis began to flourish in English and German psychology in the early 20th century. The lexical hypothesis is a major foundation of the Big Five personality traits, the HEXACO model of personality structure and the 16PF Questionnaire and has been used to study the structure of personality traits in a number of cultural and linguistic settings.\n\nSir Francis Galton was one of the first scientists to apply the lexical hypothesis to the study of personality, stating:\n\nDespite Galton's early ventures into the lexical study of personality, over two decades passed before English-language scholars continued his work. A 1910 study by G. E. Partridge listed approximately 750 English adjectives used to describe mental states, while a 1926 study of Webster's New International Dictionary by M. L. Perkins provided an estimate of 3,000 such terms. These early explorations and estimates were not limited to the English-speaking world, with philosopher and psychologist Ludwig Klages stating in 1929 that the German language contains approximately 4,000 words to describe inner states.\n\nNearly half a century after Galton first investigated the lexical hypothesis, Franziska Baumgarten published the first psycholexical classification of personality-descriptive terms. Using dictionaries and characterology publications, Baumgarten identified 1,093 separate terms in the German language used in the description of personality and mental states. Although this figure is similar in size to the German and English estimates offered by earlier researchers, Gordon Allport and Henry S. Odbert revealed this to be a severe underestimate in a 1936 study. Similar to the earlier work of M. L. Perkins, they used Webster's New International Dictionary as their source. From this list of approximately 400,000 words, Allport and Odbert identified 17,953 unique terms used to describe personality or behavior.\n\nThis is one of the most influential psycholexical studies in the history of trait psychology. Not only was it the longest, most exhaustive list of personality-descriptive words at the time, it was also one of the earliest attempts at classifying English-language terms with the use of psychological principles. Using their list of nearly 18,000 terms, Allport and Odbert separated these into four categories or \"columns\":\n\nAllport and Odbert did not present these four columns as representing orthogonal concepts. Many of their nearly 18,000 terms could have been differently classified or placed into multiple categories, particularly those in Columns I and II. Although the authors attempted to remedy this with the aid of three outside editors, the average level of agreement between these independent reviewers was approximately 47%. Noting that each outside judge seemed to have a preferred column, the authors decided to present the classifications performed by Odbert. Rather than try to rationalize this decision, Allport and Odbert presented the results of their study as somewhat arbitrary and unfinished.\n\nThroughout the 1940s, researchers such as Raymond Cattell and Donald Fiske used factor analysis to explore the overarching structure of the trait terms in Allport and Odbert's Column I. Rather than rely on the factors obtained by these researchers, Warren Norman conducted an independent analysis of Allport and Odbert's terms in 1963. Despite finding a five-factor structure similar to Fiske's, Norman decided to return to Allport and Odbert's original list to create a more precise and better-structured taxonomy of terms. Using the 1961 edition of Webster's International Dictionary, Norman added relevant terms and removed those from Allport and Odbert's list that were no longer in use. This resulted in a source list of approximately 40,000 potential trait-descriptive terms. Using this list, Norman then removed terms that were deemed archaic or obsolete, solely evaluative, overly obscure, dialect-specific, loosely related to personality, and purely physical. By doing so, Norman reduced his original list to 2,797 unique trait-descriptive terms. Norman's work would eventually serve as the basis for Dean Peabody and Lewis Goldberg's explorations of the Big Five personality traits.\n\nIn the 1970s, , a thought leader of the Moscow Semantic School, developed the systemic, or systematic, approach to lexicography which utilizes the concept of \"the language picture of the world\". Also the concept is called \"the naive picture of the world\" in order to stress the non-scientific view of the world which is imprinted in natural language. In his book \"Systematic Lexicography\", which was published in English in 2000, J.D.Apresjan puts forward the idea of building dictionaries on the basis of \"reconstructing the so-called naive picture of the world, or the \"world-view\", underlying the partly universal and partly language specific pattern of conceptualizations inherent in any natural language\". In his opinion, the general world-view can be fragmented into different more local pictures of reality, such as naive geometry, naive physics, naive psychology, and so forth. In particular, one chapter of the book Apresjan allots to the description of lexicographic reconstruction of \"the language picture of the human being\" in the Russian language. Later on, Apresjan's work laid basis for Sergey Golubkov's further attempts to build \"the language personality theory\" which would be different from other lexically-based personality theories (e.g. by Allport, Cattell, Eysenck, etc.) due to its meronomic (partonomic) nature versus the taxonomic nature of the previously mentioned personality theories.\n\nConcepts similar to the lexical hypothesis are at the root of ordinary language philosophy. Similar to the use of the lexical hypothesis to understand personality, ordinary language philosophers propose that philosophical problems can be solved or better understood through an exploration of everyday language. In his essay \"A Plea for Excuses,\" J. L. Austin cited three main justifications for this approach: words are tools, words are not only facts or things, and commonly used words \"embod[y] all the distinctions men have found worth drawing...we are using a sharpened awareness of words to sharpen our perception of, though not as the final arbiter of, the phenomena.\"\n\nDespite its widespread use in the study of personality, the lexical hypothesis has been challenged for a number of reasons. The following list describes some of the major critiques levelled against the lexical hypothesis and personality models founded on psycholexical studies.\n\n\n"}
{"id": "3647249", "url": "https://en.wikipedia.org/wiki?curid=3647249", "title": "Logical graph", "text": "Logical graph\n\nA logical graph is a special type of diagramatic structure in any one of several systems of graphical syntax that Charles Sanders Peirce developed for logic.\n\nIn his papers on \"qualitative logic\", \"entitative graphs\", and \"existential graphs\", Peirce developed several versions of a graphical formalism, or a graph-theoretic formal language, designed to be interpreted for logic.\n\nIn the century since Peirce initiated this line of development, a variety of formal systems have branched out from what is abstractly the same formal base of graph-theoretic structures.\n\n\n"}
{"id": "52806824", "url": "https://en.wikipedia.org/wiki?curid=52806824", "title": "Mathematicism", "text": "Mathematicism\n\nMathematicism is any opinion, viewpoint, school of thought, or philosophy that states that everything can be described/defined/modelled ultimately by mathematics, or that the universe and reality (both material and mental/spiritual) are fundamentally/fully/only mathematical, i.e. that 'everything is mathematics' necessitating the ideas of logic, reason, mind, and spirit.\n\nMathematicism is a form of rationalist idealist or mentalist/spiritualist monism). The idea started in the West with ancient Greece's Pythagoreanism, and continued in other rationalist idealist schools of thought such as Platonism. The term 'mathematicism' has additional meanings among Cartesian idealist philosophers and mathematicians, such as describing the ability and process to study reality mathematically.\n\nMathematicism includes (but is not limited to) the following (chronological order):\n\n\n"}
{"id": "320735", "url": "https://en.wikipedia.org/wiki?curid=320735", "title": "Meiklejohn Civil Liberties Institute", "text": "Meiklejohn Civil Liberties Institute\n\nThe Meiklejohn Civil Liberties Institute (MCLI) is a Berkeley, California-based non-profit corporation. MCLI was founded in 1965 .\n\nThe Meiklejohn Civil Liberties Institute carries on a wide range of activities, including research, publication, advocacy, and education. It publishes a quarterly newsletter, \"Human Rights Now!\"\n\nThe organization is named after Alexander Meiklejohn, a philosopher, university administrator, and free-speech advocate.\n\nThe executive director is Ann Fagan Ginger .\n\n"}
{"id": "41409857", "url": "https://en.wikipedia.org/wiki?curid=41409857", "title": "Morality throughout the Life Span", "text": "Morality throughout the Life Span\n\nMorality is “the ability to distinguish right from wrong, to act on this distinction and to experience pride when we do the right things and guilt or shame when we do not.” Both Piaget and Kohlberg made significant contributions to this area of study. Developmental psychologists have divided the subject of morality into three main topics: affective element, cognitive element, and behavioral element. The affective element consists of the emotional response to actions that may be considered right or wrong. This is the emotional part of morality that covers the feeling of guilt as well as empathy. The cognitive element focuses on how people use social cognitive processes to determine what actions are right or wrong. For example, if an eight-year-old child was informed by an authoritative adult not to eat the cookies in the jar and then was left in the room alone with the cookies, what is going on in the child's brain? The child may think “I really want that cookie, but it would be wrong to eat it and I will get into trouble.” Lastly, the behavioral element targets how people behave when they are being enticed to deceive or when they are assisting someone who needs help.\n\nMoral affect is “emotion related to matters of right and wrong”. Such emotion includes shame, guilt, embarrassment, and pride. Shame is correlated with the disapproval by one's peers. Guilt is correlated with the disapproval of oneself. Embarrassment is feeling disgraced while in the public eye. Pride is a feeling generally brought about by positive opinion of oneself when admired by one's peers \n\nEmpathy is also tied in with moral affect and is an emotional unfolding that allows you to be able to understand how another person feels. If we see someone is crying, then we also feel sad. If someone has just accomplished a lifelong goal, we bask in his happiness. Empathy falls under the affective component of morality and is the main reasoning behind selflessness. According to theorist Martin Hoffman, empathy plays a key role in the progression of morality. Empathy causes people to be more prominent in prosocial behavior as discussed earlier. Without empathy, there would be no humanity.\n\nMoral reasoning is the thinking process involved in deciding whether an act is right or wrong. Feeds off the development of social cognition that helps us experience other people's distress. These skills also allow us to go beyond our egocentrism to construct a concept of reciprocity and fairness. According to piaget and kholberg, Moral reasoning progresses through a constant sequence, a very fixed and universal order of stages, each of which contains a consistent way of thinking about moral issues that are all distinct from one another.\n\nJean Piaget's view: Jean Piaget was the first psychologist to suggest a theory of moral development. According to Piaget, development only emerges with action, and a person constructs and reconstructs his knowledge of the world as the result of new interactions with his environment. Piaget said that people pass through three different stages of moral reasoning: Premoral period, heteronomous morality, and Autonomous morality. The first stage, premoral, occurs during the preschool years: children show very little awareness or understanding of rules and cannot be considered moral beings. The next stage, heteronomous which is defined as under the rule of another, and it involves children ages 6 to 10 years old. The child will take rules more seriously, believing that they are handed down by authority figures and are sacred and never to be altered. No matter if the intentions were good or bad, any violator to these passed-down rules will be judged as wrongdoing. The last stage, Autonomous, appears at 10 to 11 years of age. Children begin to appreciate that rules are agreements between individuals.\n\nLawrence Kohlberg's view: Influenced by Piaget's work, Kohlberg created an influential cognitive development theory of moral development. Like Piaget, Kohlberg formulated that moral growth occurs in a very universal and consistent sequence of three moral levels, but for him the stages in the sequence are connected with one another, and grows out of the preceding stage. Kholberg's view represents a more complex way of thinking about moral issues.\n\nThe first level of Kohlberg's moral reasoning is called preconventional morality. In this stage, children obey rules more externally. This means the children will conform to rules authority sets for them, to avoid punishment or to receive personal rewards. There are two stages within preconventional morality. The first is punishment and obedience orientation. In this stage, the child determines how wrong his action was according to the punishment he receives. If he does not get scolded for his bad act, he will believe he did nothing wrong. The second stage of preconventional morality is called instrumental hedonism. The main principle for this stage is “You scratch my back and I’ll scratch yours.” A person in the second stage conforms to rules for personal gain. There is a hint of understanding the ruler's perspective, although, the main objective is to gain the benefit in return.\n\nThe next level is conventional morality. At this level, many moral values have been internalized by the individual. He works to obey rules set by authority and to seek approval. The points of views of others are now clearly recognized and taken into serious consideration. The first stage to this level is labeled “Good boy” or “Good girl” morality. The following stage captures the golden rule “treat others the way you want to be treated.” The emphasis in this stage consists of being nice and meaning well. What is seen as “good” is now what pleases and helps others. The last stage of this level is “Authority and social order- maintaining morality.” This is conforming to set rules created by legitimate authorities that benefits society as a whole. The basis of reciprocation is growing more complex. The purpose of conforming is no longer based on the fear of punishment, but on the value people place on respecting law and doing one's duty for society.\n\nThe final level of moral reasoning is postconventional morality. The individual determines what the moral ideal for the society is. The individual will begin to distinguish between what is morally acceptable and what is legal. He will recognize some laws violate basic moral principles. He will go beyond perspectives of social groups or authorities and will start to accept the perspectives of all points of views around the world. In the first stage of postconventional, called “morality of contract, individual rights and democratic accepted law”, all people willingly work towards benefitting everyone. They understand all social groups within a society have different values, but believe all intellectual individuals would agree on two points. First point being freedom of liberty and life. Second, they would agree to having democratic vote for changing unfair laws and improving their society. The final stage of the final level of Kohlberg's moral reasoning is “morality of individual principles of conscience”, At this highest stage of moral reasoning, Kohlberg defines his stage 6 subjects as having a clear and broad concept of universal principles. The stage 6 thinker does not create the basis for morality. Instead, he explores, through self-evaluation, complex principles of respect for all individuals and for their rights that all religions or moral authorities would view as moral. Kohlberg stopped using stage 6 in his scoring manual, saying it was a “theoretical stage”. He began to score postconventional responses only up to stage 5.\n\nThe Social Learning Theory is based on the observing of others and modeling behaviors, emotions and attitude of others. This theory is derived from the concept or perspective of behaviorism but has elements of cognitive learning as well. The theory says that people and especially children learn from observing others and their environment around them. It also says that imitation or modeling has a major role in learning and development of the person and their beliefs or morals. Albert Bandura was a major contributor to the theory of social learning and made many contributions to the field with social experiments and research. The social learning theory says that children learn and develop morals from observing what is around them and having role models that they imitate their behavior and learn from. Role models guide children indirectly with developing morals and morality. By watching the response of others and society around them, children learn what is acceptable and what is not acceptable and try to act similar to what is deemed acceptable by the society around them.\n\nFor example, a child's older brother or sister tends to serve as one of their first role models, especially because they share the same surroundings and authority figures. When the older child misbehaves or does something unacceptable the younger child takes the older one as an example and acts like him or her. However, if the parent punishes the older child or there is a consequence to their behavior, the younger child often does not act like the older one because they were able to observe that that behavior was “unacceptable” by the “society” surrounding them, meaning their family. This example coincides with the fact that children know that stealing, killing, and lying is bad, and honesty, kindness, and being polite is good.\n\nThe Evolutionary Theory of Morality tries to explain morality and its development in terms of evolution and how it may at first seem contradictory for humans to have morals and morality in the evolutionary opinion. Evolution has many beliefs and parts to it but as most commonly seen, it is the survival of the fittest. This behavior is driven from the desire to pass on your genes. Whether that means being selfish or caring for your young just to make sure that your genes survive. That may seem to conflict with morality and having morals, however some argue that that may be related and having morality and morals might actually be a factor that contributes to the theory of evolution. Additionally, humans have developed communities and a social lifestyle which makes it necessary to develop morals. In an evolutionary view, a human being that acts immorally will suffer consequences, such as paying a fine, going to prison, or being an outcast. Whatever it is, there is a loss to that human. Therefore, that individual eventually learns that to be accepted in society, it is necessary to develop morals and act on them.\n\nAn infant is not held accountable for wrongdoings because an infant is considered to be amoral. Infants do not have the ability to understand morality at this stage of their life. Infancy ranges from birth to the age of two. It is during this time that infants learn how to be prosocial and empathetic by observing those around them.\n\nAn example of early moral training is given by Roger Burton. Roger Burton observed his 1-year-old daughter Ursula take her sister's Halloween candy. The older sister quickly scolded the infant for doing so. After a week had passed the infant went and stole candy again but was confronted by her mother this time. Yet again the infant stole her sister's candy, so Burton approached his daughter himself. As Burton was about to say something to Ursula, she spoke and said, “No this is Maria’s, not Ursula’s.” This specific example given by Burton shows how the infant grows and imputes morality slowly but surely. Over time infants start to understand that their behavior can cause repercussions. The infants learn by observing their surrounding environment. A 1 year old may cease to commit a certain action because of feelings of apprehension due to past experiences of criticism. Both disapproval and reward are key factors in furthering the infants’ development. If a baby is especially close to his mom, her disapproval and reward are that much more impactful in this process than it would be for a baby who was unattached to his mom. Infants are excellent at reading the emotions of others which in turn directs them in knowing what is good and what is bad. A strong attachment between parent and infant is the key to having success with the infants’ socialization. If the relationship between the two is not stable by 15 months, at 4 the child will show animosity and troublesome behavior. At 5 the child is likely to show signs of destructive behavior. In order to ensure this does not happen, a mutually responsive orientation among the parent and offspring is necessary. Meaning, “A close emotionally positive and cooperative relationship in which child and care giver cares about each other and are sensitive to each other’s needs.” With this bond, parents can help their child's conscience grow.\n\nEmpathy and Prosocial Behavior: Unlike what Freud, Kohlberg, and Piaget said about infants being focused solely on themselves, Krebs's has a different take on the infant. Krebs's outlook on infants is that they can and do express signs of empathy and prosocial behavior. After being born, newborns show empathy in a primitive way by showing signs of agony when hearing other babies cry. Although it is not certain whether these babies can tell the difference between their own cry and another infants, Martin Hoffman explains that between the age of 1 and 2 infants are more able to perform real moral acts. Hoffman observed a 2-year olds reaction to another child in distress. The 2 year old offered his own teddy bear to his peer. This simple act shows that the 2 year old was putting himself in the shoes of his peer Another study was done by Carolyn Zahn-Waxler and her team on this topic. Waxler found that over half of the infants involved took part in at least one act of prosocial behavior Hoffman feels that as children mature empathy in turn becomes less about oneself and more about cognitive development.\n\nKohlberg contributed to our understanding of moral development in children and said that moral reasoning is what makes ethical behavior. He said that there are six developmental stages that are constructive that eventually lead to the development of morality. Kohlberg also said that moral development does not just stop or is complete at a certain time or stage but instead that it is continuous and happens throughout a person's lifetime. Kohlberg used stories that are known as Heinz dilemmas which are controversial stories to see what people felt was morally acceptable and what was not. From that he developed his theory with his six stages of moral development which are: obedience, self-interest, conformity, law and order, human rights, and universal human ethics.\n\nAs mentioned before, Kohlberg believed that moral development was an ongoing thing and that through experiences it develops and adapts. Moreover, in children's moral thinking there comes a time where morality and moral decisions are made based on consequences, that is to say that they start weighing intentions in order to decide and learn whether something is morally okay or not.\n\nAn example of this can be seen in identifying the intentions of specific behaviors. We as a society also operate in that way because the same outcome or behavior can be considered acceptable had the intentions for it been good versus another time where the intentions were bad and in that case it would be unacceptable. In a situation where a child is trying to help their parent clean and misplaces something accidentally, it is easily forgiven and is considered morally acceptable because his/her intentions were good. However, if the child purposely hid the object so their parent would not find it, it is morally unacceptable because the intentions were bad. Ultimately, it is the same situation and same outcome but the intentions are different. At the age of 10–11 years children realize this concept and start justifying their actions and behaviors by weighing their intentions.\n\nIn Kohlberg's theory of moral development there are six definite stages. The first stage is called obedience and punishment orientation. And this stage is governed by the concept of having rules, laws, and things that one must follow. In children there is a set of rules set down by their authority figures, often being their parents. At this stage of moral development, morality is defined by these rules and laws that they think are set in stone and can never change. To a child, these rules are ones that can never be wrong and that define good and bad and show the difference between them. Later on however, these rules become more like guidelines and can change based on the situation they are presented in. Another aspect of these rules is the concept of punishment and reinforcement and that's how children realize what is considered morally okay or not.\n\nAn example of this is in the dilemmas used by Kolhberg, when he asked children to justify or judge the situation and their answer was always justified by something similar to “this is not fair” or “this is wrong because lying is bad” etc. This shows that at this particular stage children are not yet contributing members in terms of moral development.\n\nThe theory of mind tells us that children need to develop a sense of self in order to develop morality and moral concepts. As the child develops, s/he needs to experience and observe in order to realize how they fit into society and eventually become part of it thereby establishing a sense of self-identity. This sense of self-identity depends on many things and it is important to develop and learn from those stages in order to fully develop understand good from bad.\n\nMoral socialization talks about how morality is passed on. This perspective says that adults or parents pass down and teach their children the acceptable behavior through techniques and teaching as well as punishments and reinforcements. This then shows that children who learn and develop morality have good listening and are more compliant and it is because of that that they are morally developed. Therefore, if a parent fails to teach that to their child, the child then would not develop morality. Moral socialization also has studies that show that parents who use techniques that are not violent or aggressive and unconditional raise children with more conscience. Therefore, these children have more of a sense of morality and are more morally developed.\n\nAs adolescents gain the ability to independently think about complex and hypothetical ideas, and as they map out their identities, many of them begin to see their values and moral standards and some centralize on their morality. On the other side of the spectrum there are the adolescents who engage in serious antisocial behavior.\n\nThe teen years are a significant period for moral growth. The moral reasoning percentage range displays that the preconventional reasoning (stage one and two) decrease rapidly as they reach teen years. During adolescence, conventional reasoning (stage three and four) become the centralized mode of moral thinking. Early year teens reflect stage 2 (instrumental hedonism) - “treat others how you would like to be treated”- or stage 3 (good boy or good girl) which involves earning approval and being polite. Almost half the percent of 16- to 18-year-olds display stage 3 reasoning and only a fifth were scored as stage 4 (authority and social order-maintaining morality) arguments. As adolescents start to age more they begin to take a broad societal perspective and act in ways to benefit the social system. The main developmental trend in moral reasoning occurs during the shift from preconventional to conventional reasoning. During the shift, individuals carry out moral standards that have been passed down by authority. Many teens characterize a moral person as caring fair, and honest. Adolescents who display these aspects tend to advance their moral reasoning.\n\nThe adolescents who do not internalize society's moral standards, are more likely to be involved in antisocial conduct such as, muggings, rapes, armed robberies, etc. Most antisocial adults start their bad behavior in childhood and continue on into adolescence. Misbehavior seen in childhood increases, resulting in juvenile delinquency as adolescents. Actions such as leaving school early, having difficulty maintaining a job, and later participating in law-breaking as adults. Children who are involved in these risky activities are diagnosed as having conduct disorder and will later develop antisocial personality disorder. There are at least two outcomes of antisocial youths. First, a noticeable group of children who are involved in animal cruelty, and violence amongst their peers which then consistently progresses throughout their entire lifespan. The second group consists of the larger population of adolescents who misbehave primarily due to peer pressure but outgrow this behavior when they reach adulthood. Juveniles are most likely to depend on preconventional moral reasoning. Some offenders lack a well-developed sense of right and wrong. A majority of delinquents are able to reason conventionally but commit illegal acts anyway.\n\nKenneth Dodge progressed on the understanding of aggressive/aggression behavior with creating his social information processing model. He believed that people's retaliation to frustration, anger or provocation do not depend so much on social cues in the situation as on the ways which they process and interpret this information. An individual who is provoked goes through a six step progress of processing information. According to dodge the individual will encode and interpret cues, then clarify goals. Following this, he will respond search and decision, thinking of possible actions to achieve goal to then weigh pros and cons of alternative options. Finally, he will perform behavioral enactment, or take action. People do not go through these steps in the exact order in all cases, they may take two and work on them simultaneously or repeat the steps. Also, an individual may come up with information from not only the current situation, but also from a previous similar social experience and work off of that. The six steps in social information processing advance as one ages.\n\nGerald Patterson observed that highly antisocial children and adolescents tend to be raised in coercive family environments in which family members try to control the others through negative, coercive tactics. The parental guidance in these households realize they are able to control their children temporarily with threatening them and providing negative reinforcement. The child also learns to use negative reinforcement to get what they want by ignoring, whining and throwing tantrums. Eventually, both sides (parents and children) lose all power of the situation and nothing is resolved. It becomes evident of how a child is raised on how they will become very hostile or present aggressiveness to resolve all their disputes. Patterson discusses how growing up in a coercive family environment creates an antisocial adolescent due to the fact they are already unpleasant to be around they begin to perform poorly in school and are rejected to all their peers. With no alternative option, they turn to the low-achieving, antisocial youths who encourage bad behavior. Dodge's theory is well supported by many cases of ineffective parenting contributing to the child's behavior problems, association with antisocial groups, resulting in antisocial behavior in adolescence.\n\nThe next theory that may contribute to antisocial behavior is between genetic predisposition and social learning experiences. Aggression is seen more in the evolutionary aspect. An example being, males are more aggressive than females and are involved in triple the amount of crime. Aggression has been present in males for centuries due to male dependence on dominance to compete for mates and pass genes to further generations. Most individuals are born with temperaments, impulsive tendencies, and other characteristics that relate to a delinquent. Although predisposed aspects have a great effect on the adolescent's behavior, if the child grows up in a dysfunctional family and receive poor parenting or is involved in an abusive environment that will increase the chances immensely for antisocial behaviors to appear.\n\nLawrence Kohlberg has led most of the studies based on moral development in adults. In Kohlberg's studies, the subjects are ranked accordingly at one of the six stages. Stage 1 is called the Obedience and Punishment Orientation. Stage 1 is preconventional morality because the subject sees morality in an egocentric way. Stage 2 is also considered to be preconventional morality and is labeled as Individualism and Exchange. At this stage, the individual is still concentrated on the self and his surrounding environment. Stages 3 and 4 are at level 2, Conventional Morality. Stage 3 is called Interpersonal Relationships and is the point where the individual realizes that people should behave in good ways not only for the benefit of themselves but the family and community as well. Stage 4, Maintaining the Social Order, the individual is more concentrated on society as a whole. Stage 5 and 6 are both at level 3, post-conventional Morality. Stage 5 is the social contract and Individual Rights. At this stage people are contemplating what makes a good society rather than just what makes a society run smoothly. Stage 6, Universal Principles is the last stage which interprets the foundation of justice Kohlberg completed a 20-year study and found that most 30-year-old adults were at stage 3 and 4, the conventional level. According to this study there is still room for moral development in adulthood.\n\nKohlberg's theory of moral development greatly influenced the scientific body of knowledge. That being said, it is now being evaluated and researchers are looking for a more thorough explanation of moral development. Kohlberg stated that there is no variation to his stages of development. Despite this, modern research explains that kids are more morally mature than Kohlberg had noted. On top of that, there is no strong evidence that suggests that people go from conventional to post-conventional. Lastly, it is now known that Kohlberg's theory is biased against numerous numbers of people.\n\nThe main focus of developmentalists now is how important emotion is in terms of morality. Developmental researchers are examining the emotions of child and adult when exhibiting good and bad behavior. The importance of innate feelings and hunches in regards to morality are also being contemplated by doctors. Researchers have also come up with dual-process models of morality. This process examines Kohlberg's rational deliberation as well as the more recent study of innate feelings and emotions. Joshua Green is one scholar who backs the dual-process models of morality and feels that there are different situations that call for each view. All in all, there are many factors that come into play in deciding whether a person will behave in a certain fashion when dealing with a moral decision.\n\nSocial and Clinical Psychology.\nSite.\n"}
{"id": "8299349", "url": "https://en.wikipedia.org/wiki?curid=8299349", "title": "New realism (philosophy)", "text": "New realism (philosophy)\n\nNew realism was a philosophy expounded in the early 20th century by a group of six US based scholars, namely Edwin Bissell Holt (Harvard University), Walter Taylor Marvin (Rutgers College), William Pepperell Montague (Columbia University), Ralph Barton Perry (Harvard), Walter Boughton Pitkin (Columbia) and Edward Gleason Spaulding (Princeton University).\n\nThe central feature of the new realism was a rejection of the epistemological dualism of John Locke and of older forms of realism. The group maintained that, when one is conscious of, or knows, an object, it is an error to say that the object in itself and our knowledge of the object are two distinct facts. If we know a particular cow is black, is the blackness on that cow or in the observer's mind? Holt wrote: \"That color out there is the thing in consciousness selected for such inclusion by the nervous system's specific response.\" Consciousness is not physically identical with the nervous system: it is \"out there\" with the cow, all throughout the field of sight (and smell, and hearing) and identical with the set of facts it knows at any moment. The nervous system is merely a system of selection.\n\nThis position, which belongs to a broader category of views sometimes called neutral monism or, following William James, radical empiricism, has not worn well over the subsequent century, partly because of the problem of the nature of abstract ideas such as blackness. It seems very natural to locate blackness as an abstract idea in the mind that's useful in dealing with the world. The new realists did not want to acknowledge representationalism at all but later embraced something akin to Aristotle's form of realism: blackness is a general quality that many objects have in common, and the nervous system selects not just the object but the commonality as a fact. But Arthur Lovejoy showed in his book \"The Revolt Against Dualism\" that the perception of black varies so much, depending on context in the visual field, the perceiver's personal history and cultural usage, that it cannot be reduced to commonalities within objects. Better, Lovejoy thought, to bring representational ideas back into the account after all.\n\nIn the framework of continental hermeneutics, as a reaction against its constructivist or nihilistic outcomes, Maurizio Ferraris has proposed the so-called new realism (\"Manifesto del nuovo realismo\", 2012), a philosophical orientation shared by both analytic philosophers (such as Mario De Caro, see Bentornata Realtà, ed. by De Caro and Ferraris, 2012), and continental philosophers, such as Mauricio Beuchot (\"Manifesto del realismo analogico\", 2013), and Markus Gabriel (\"Fields of Sense: A New Realist Ontology\", 2014). In South America (\"Cenários da Filosofia contemporânea: fim da pós-modernidade e new realism?\", São Paulo, 2015; \"Cosa resta della Filosofia Contemporanea?\", Salerno-Roma, 2013) propose a political philosophy based on the Italian New Realism. (New realism intersects with other realistic continental movements that arose independently but responding to similar needs, such as the \"speculative realism\" defended by the French philosopher Quentin Meillassoux and the American philosopher Graham Harman.)\n\nFor new realism, the assumption that science is not systematically the ultimate measure of truth and reality does not mean that we should abandon the notions of reality, truth or objectivity, as was posited by much of twentieth century philosophy. Rather, it means that philosophy, as well as jurisprudence, linguistics or history, has something important and true to say about the world. In this context, new realism presents itself primarily as a negative realism: the resistance that the outside world poses to our conceptual schemes should not be seen as a failure, but as a resource – a proof of the existence of an independent world. If this is the case, however, this negative realism turns into a positive realism: in resisting us reality does not merely set a limit we cannot trespass, but it also offers opportunities and resources. This explains how, in the natural world, different life-forms can interact in the same environment without sharing any conceptual scheme and how, in the social world, human intentions and behaviors are made possible by a reality that is first given, and that only at a later time may be interpreted and, if necessary, transformed.\n\n"}
{"id": "15718985", "url": "https://en.wikipedia.org/wiki?curid=15718985", "title": "Non-Fatal Offences Against the Person Act 1997", "text": "Non-Fatal Offences Against the Person Act 1997\n\nThe Non-Fatal Offences against the Person Act 1997 is an Act of the Oireachtas which virtually codified the criminal law on offences against the person in the Republic of Ireland. The Act replaced the greater part of the Offences against the Person Act 1861, scrapping such concepts as actual bodily harm and grievous bodily harm, and recognised the use of modern technology as a weapon:\n\nThe Act also made it an offence to use a syringe as a weapon, particularly where it is used to make the victim \"...believe that he or she may become infected with disease\".\n\nOffences Against the Person Act\n\n"}
{"id": "2437436", "url": "https://en.wikipedia.org/wiki?curid=2437436", "title": "Orbit insertion", "text": "Orbit insertion\n\nOrbit insertion is the spaceflight operation of adjusting a spacecraft’s momentum, in particular to allow for entry into a stable orbit around a planet, moon, or other celestial body. This maneuver involves either deceleration from a speed in excess of the respective body’s escape velocity, or acceleration to it from a lower speed.\n\nThe result may also be a transfer orbit. There is e.g., the term \"descent orbit insertion\". Often this is called orbit injection.\n\nThe first kind of orbit insertion is used when capturing into orbit around a celestial body other than Earth, owing to the excess speed of interplanetary transfer orbits relative to their destination orbits. This shedding of excess velocity is typically achieved via a rocket firing known as an orbit insertion burn. For such a maneuver, the spacecraft’s engine thrusts in its direction of travel for a specified duration to slow its velocity relative to the target body enough to enter into orbit. Another technique, used when the destination body has a tangible atmosphere, is called aerocapture, which can use the friction of the atmospheric drag to slow down a spacecraft enough to get into orbit. This is very risky, however, and it has never been tested for an orbit insertion. Generally the orbit insertion deceleration is performed with the main engine so that the spacecraft gets into a highly elliptical “capture orbit” and only later the apocenter can be lowered with further decelerations, or even using the atmospheric drag in a controlled way, called aerobraking, to lower the apocenter and circularize the orbit while minimizing the use of onboard fuel. To date, only a handful of NASA and ESA missions have performed aerobraking (Magellan, Mars Reconnaissance Orbiter, Trace Gas Orbiter, Venus Express, ...).\n\nThe second type of orbit insertion is used for newly launched satellites and other spacecraft. The majority of space launch vehicles used today can only launch a payload into a very narrow range of orbits. The angle relative to the equator and maximum altitude of these orbits are constrained by the rocket and launch site used. Given this limitation, most payloads are first launched into a transfer orbit, where an additional thrust maneuver is required to circularize the elliptical orbit which results from the initial space launch. The key difference between this kind of maneuver and powered trans-planetary orbit insertion is the significantly lesser change in velocity required to raise or circularize an existing planetary orbit, versus canceling out the considerable velocity of interplanetary cruise.\n\nAlthough current orbit insertion maneuvers require precisely timed burns of conventional chemical rockets, some headway has been made towards the use of alternative means of stabilizing orbits, such as ion thrusters or plasma propulsion engines to achieve the same result using less fuel over a longer period of time. In addition, research into the use of electrically conducting space tethers to magnetically repel the Earth’s magnetic field has shown some promise, which would virtually eliminate the need for fuel altogether.\n"}
{"id": "188418", "url": "https://en.wikipedia.org/wiki?curid=188418", "title": "Overconsumption", "text": "Overconsumption\n\nOverconsumption is a situation where resource use has outpaced the sustainable capacity of the ecosystem. A prolonged pattern of overconsumption leads to environmental degradation and the eventual loss of resource bases.\n\nGenerally, the discussion of overconsumption parallels that of human overpopulation; that is the more people, the more consumption of raw materials takes place to sustain their lives. But, humanity's overall impact on the planet is affected by many factors besides the raw number of people. Their lifestyle (including overall affluence and resource utilization) and the pollution they generate (including carbon footprint) are equally important. Currently, the inhabitants of the developed nations of the world consume resources at a rate almost 32 times greater than those of the developing world, who make up the majority of the human population (7.4 billion people).\n\nHowever, the developing world is a growing market of consumption. These nations are quickly gaining more purchasing power and it is expected that the Global South, which includes cities in Asia, Latin America and Africa, will account for 56% of consumption growth by 2030. This means that consumption rates will plateau for the developed nations and shift more into these developing countries. \n\nThe theory of overpopulation reflects issues of carrying capacity without taking into account per capita consumption, by which developing nations are evaluated to consume more than their land can support. It is expected that world population growth will increase by 41% from 2000 to 2050, reaching a height of 8.9 billion people. On top of the rapid growth expectancy, it will be highly concentrated in the developing nations. This poses issues with inequality of consumption as well. The nations that will come into consumer dominance must abstain from abusing certain forms of consumption, especially energy consumption of CO2. Green parties and the ecology movement often argue that consumption per person, or ecological footprint, is typically lower in poor than in rich nations.\n\nIn understanding the effects of over-consumption, it is pertinent to understand what causes the phenomenon. There is a spectrum of goods and services that the world population constantly consume. These range from food and beverage, clothing and footwear, housing, energy, technology, transportation, education, health and personal care, financial services and other utilities. Each of these require a different resource and once that resource is exploited to a certain point, that qualifies as over consumption. Since the developing nations are rising quickly into the consumer class, it is important to note the trends happening in these nations. According to the World Bank, the highest shares of consumption lie in food and beverage and clothing and footwear. This applies regardless of sector of income. \n\nTwo main factors of why we buy so much and so often is due to planned and perceived obsolescence. This factor of production was introduced first in the United States and it revolves around the design of products and with these methods, the products are intentionally designed to get rid of after a short amount of time. As of 2012, only 1% of goods purchased were still in use after 6 months. This is due to planned and perceived obsolescence. When it is planned, designers create products that will not be able to work after a certain amount of time but they work for enough time to ensure the customers will come back to buy again. Perceived obsolescence comes in a lot with fashion and trends and fueled by advertising and media consumption. Through this technique, consumers are convinced that certain products do not have value anymore because it is out of style, and in order to have value, consumers must buy more up to date styles. Here is where fast fashion was born. \nAs of 2015, the top five consumer markets in the world included the United States, Japan, Germany, China and France. \n\nA fundamental effect of overconsumption is a reduction in the planet's carrying capacity. Excessive unsustainable consumption will exceed the long term carrying capacity of its environment (ecological overshoot) and subsequent resource depletion, environmental degradation and reduced ecosystem health.\n\nLooking at the two largest sectors of over consumption, the fashion and food industries, we can see most of the harmful effects on the Earth starting here. The fashion industry has created a new venue, fast fashion, which in 2013 produced 15.1 million tons of textile waste and of that, 12.8 million tons were thrown out. The United States, being the largest consumer market, deals with excess clothing by exporting it to poorer, developing nations but this solution is not sustainable because the demand will go down as cheap clothing becomes more readily available. Another way of disposal is to throw out into landfills or burn up in incinerators which is the least sustainable disposal solution. \n\nThe food industry is the other largest sector of consumption and studies show that people waste a fifth of food products just through disposal or overconsumption. The UN Food and Agriculture Organization collected data and found that by the time food reaches the consumer, 9% (160 million tons) goes uneaten and 10% is lost to overconsumption - meaning consumers ate more than the calorie intake requirement. Other aspects of losses surrounding dry matter came at each stage in the food system, the highest amount being from livestock production at 43.9%, transportation accounted for 18% and consumer waste accounting for 12.2% loss. When the consumer takes in too much, this not only explains losses in the beginning of the stage at production (and over production) but also lends itself to overconsumption of energy and protein, having harmful effects on the body. \nThe scale of modern life's overconsumption has enabled an overclass to exist, displaying affluenza and obesity . However once again both of these claims are controversial with the latter being correlated to other factors more so than over-consumption. Within the topic of overconsumption there are many other ideas that should be considered in order to find the true cause of it. Some important events that coincide are poverty, population and the development of an area. Overconsumption can also lead to a decline in the economy and financial instability. \n\nIn the long term, these effects can lead to increased conflict over dwindling resources and in the worst case a Malthusian catastrophe. Lester Brown of the Earth Policy Institute, has said: \"It would take 1.5 Earths to sustain our present level of consumption. Environmentally, the world is in an overshoot mode.\"\n\nAs of 2012, the United States alone was using 30% of the world’s resources and if everyone were to consume at that rate, we would need 3-5 planets to sustain this type of living. Resources are quickly becoming depleted, with about ⅓ already gone. With new consumer markets rising in the developing countries which account for a much higher percent of the world’s population, this number can only rise. \n\nThe Worldwatch Institute said China and India, with their booming economies, along with the United States, are the three planetary forces that are shaping the global biosphere. The State of the World 2005 report said the two countries' high economic growth exposed the reality of severe pollution. The report states that\nThe world's ecological capacity is simply insufficient to satisfy the ambitions of China, India, Japan, Europe and the United States as well as the aspirations of the rest of the world in a sustainable way.\n\nThe idea of overconsumption is also strongly tied to the idea of an ecological footprint. The term “ecological footprint” refers to the “resource accounting framework for measuring human demand on the biosphere.” Currently, China is roughly 11 times lower in per capita footprint, yet has a population that is more than four times the size of the USA. It is estimated that if China developed to the level of the United States that world consumption rates would roughly double. According to Scientific American, one person from China uses 53 times fewer resources than the average American.\n\nA 2018 study published in \"Science\" postulates that meat consumption is set to increase as the result of human population growth and rising affluence, which will increase greenhouse gas emissions and further reduce biodiversity.\n\nThe most obvious solution to the issue of overconsumption is to simply slow the rate at which materials are becoming depleted. Less consumption naturally has negative effects on economies - so instead, countries must look to curb consumption rates while allowing for new industries, such as renewable energy and recycling technologies, to flourish and deflect some of the economic burden. A fundamental shift in the global economy may be necessary in order to account for the current change that is taking place or that will need to take place. Movements and lifestyle choices related to stopping overconsumption include: anti-consumerism, freeganism, green economics, ecological economics, degrowth, frugality, downshifting, simple living, minimalism, and thrifting.\n\nRecent grassroots movements have been coming up with creative ways to decrease the amount of goods we consume. The Freecycle Network is a network of people in one's community that are willing to trade goods for other goods or services. It is a new take on thrifting while still being beneficial to both parties.\nOther researchers and movements such as the Zeitgeist Movement suggest a new socioeconomic model which, through a structural increase of efficiency, collaboration and locality in production as well as effective sharing, increased modularity, sustainability and optimal design of products, are expected to reduce resource-consumption.\nAdded information about overconsumption and excess garbage and its effect on urban communities and the environment. Solutions offered include consumers using market forces to influence businesses towards more sustainable manufacturing and products. \n\n"}
{"id": "12775176", "url": "https://en.wikipedia.org/wiki?curid=12775176", "title": "Parent–child interaction therapy", "text": "Parent–child interaction therapy\n\nParent-Child Interaction Therapy (PCIT) is an intervention developed by Sheila Eyberg (1988) to treat children between ages 2 and 7 with disruptive behavior problems. PCIT is an evidence-based treatment (EBT) for young children with behavioral and emotional disorders that places emphasis on improving the quality of the parent-child relationship and changing parent-child interaction patterns.\n\nDisruptive behavior is the most common reason for referral of young children for mental health services and can vary from relatively minor infractions such as talking back to significant acts of aggression. The most commonly treated Disruptive Behavior Disorders may be classified as Oppositional Defiant Disorder (ODD) or Conduct Disorder (CD), depending on the severity of the behavior and the nature of the presenting problems. The disorders often co-occur with Attention-Deficit Hyperactivity Disorder (ADHD). It uses a unique combination of behavioral therapy, play therapy, and parent training to teach more effective discipline techniques and improve the parent–child relationship.\n\nPCIT is typically administered once a week, with 1-hour sessions, for 10-14 sessions total and consists of two treatment phases: Child-Directed Interaction (CDI) and Parent-Directed Interaction (PDI). The CDI component focuses on improving the quality of the parent-child relationship, which will help promote changes in behavior. This sets the foundation for the PDI stage, which continues to encourage appropriate play while also focusing on a structured and consistent approach to discipline.\n\nPCIT was derived from several theories, including attachment theory, social learning theory, and parenting styles theory. \n\nAccording to attachment theory by Ainsworth, “sensitive and responsive parenting” during infancy and toddlerhood leads the child to develop an expectation that their needs can be met by the parent. Thus, parents who show their young children greater warmth and are more responsive and sensitive to their needs promote a sense of security that they can later apply to relationships with others. This can also help with more effective emotion regulation.Children who are referred to clinics for externalizing behaviors are more likely than non-referred children to display distress when separated from the parent and to display indicators of an insecure attachment to their parent.\n\nThe Child Directed Interaction (CDI) component of the PCIT applies attachment theory through its goal to “restructure the parent-child relationship and provide a secure attachment for the child”. The CDI component makes use of the idea that parents can have a dramatic effect on their child's behavior, especially during the early preschool years. This is a critical period where children are more responsive to their parent's and less so to other influences such as teachers or peers.\n\nSocial learning theory suggests that new behaviors can be learned by watching and imitating the behaviors of others. Patterson (1975) further expands on this and proposes that child behavior problems are “inadvertently established or maintained by dysfunctional parent-child interactions”. There can be a “coercive interaction cycle” between parent and child where both try to control the behavior of the other. Behaviors such as arguing and aggression in children are reinforced by parent behaviors (e.g., withdrawal of demands), but negative parent behaviors can subsequently be reinforced by negative child behaviors. In sum, children can learn many behaviors from their parents’ feedback, but this can result in negative externalizing behaviors, as well. The PDI component targets this cycle specifically by establishing consistent parenting behaviors that encourage the desired behavior in children. \n\nAccording to Diana Baumrind’s parenting style theory (year link citation) found that the authoritative parenting style leads to the healthiest outcomes for children transitioning into adolescence. This style combines responsive and nurturing interactions with clear communication and firm discipline. The influence of this theory can be seen particularly in the PDI treatment phase where parents are taught to use direct commands to increase desired behavior, along with other positive and nurturing behaviors.\n\nEyberg’s original paper (1988) thoroughly describes each assessment and treatment phase of the PCIT and includes suggestions for applying the therapy.\n\nFirst, parents attend a training session during which the therapist explains each rule and its rationale. Each parent is also taught through one-on-one role play interactions with the therapist. Parents are also given a handout at the end of the session that summarizes the basic directions so they can review it at home.\n\nAfter this training session, the sessions that follow will include the child. The sessions are held in a playroom, with the child playing with one parent at a time. Meanwhile, the therapist and the other parent will be observing the play through a one-way mirror or video system. The therapist can provide immediate feedback and suggestions through a “bug-in-ear” device or sit in the room to do the coaching. At the end of the session, the therapist discusses the child's progress, using summary sheets that parents can use to guide their interactions during practice sessions at home. These practice sessions serve as a “homework assignment” for parents, during which they practice the interaction with their child for five minutes a day, using homework sheets to track progress. The treatment begins with the Child-Directed Interaction phase, then is followed by the Parent-Directed Interaction (PDI) phase.\n\nAccording to Eyberg (1988), the parent's goal during this stage is to follow the child's lead during play while being sure to follow the “Don’t Rules” and “Do Rules of CDI”. The child should be free to lead the activity and make their own decisions about what and how to play. By letting their children take control of the play, the parents help their child develop autonomy and independence.\n\nAccording to Eyberg (1988), the Don’t rules help parents step back and encourage child-led play by avoiding commands, questions, and criticisms. Commands, or instructions, would take the lead away from the child could also introduce potential disagreements into the play. Parents are also encouraged to not ask questions. This can include questions such as “How about putting the toys away?” which are actually implied commands. The concern about asking questions is that they may come off as accusatory (“Why did you choose that toy?”) or take the conversation to an “adult” level instead of letting the child play freely and naturally. The general idea is that questions provide little information, so they have limited usefulness in therapy. The third “don’t” rule is to avoid criticizing. Though criticisms can range from mild to blatant attacks on the child, criticisms in general can lead to damaging effects on the child's self-esteem. As children learn which behaviors are good or bad, they rely on what their parents say about them and believe it. Criticisms may also frustrate or anger the child and can lead to a counterattack. Taken together, criticisms are not only unproductive in therapy, but also are threats to the positive relationship that the PCIT emphasizes.\n\nAccording to Eyberg (1988), the Do rules of CDI that promote positive behavior throughout play. The first Do is to describe what the child is doing during the activity. Doing this may seem unnatural at first, but describing serves a few purposes: it allows the child to (1) lead play, (2) improve attention towards independent activities, (3) clarify the activity and encourage the child to further elaborate the play, and (4) help teach the child different concepts in a positive way. For example, the child learns through positive feedback (“you found the red one”) instead of coercion (“find the red one”).\n\nThe second Do is imitation. Eyberg recommends that the parents “sit close and do the same thing as the child”. The parent can add to the child's play, or do something similar, but the focus should still remain on the child's style of play. The attention that imitation can demonstrate can show the child that the parent is interested and believes what they are doing is important. Imitation may even lead to the child imitating the parent. The aim is that through the parent-child play, the child can learn cooperative play skills that they can one day use with other children.\n\nParents are encouraged to reflect what the child says during play, the third Do of CDI. This helps parents practice listening to their child. For example, when the child says “The car is fast,” the parent might say “Yes, the car is fast”. These reflections show that the parent understands and accepts what the child is saying. Additionally, using reflective statements can improve the child's vocabulary and grammar by providing clarity to the child's thoughts. It also gives the child an opportunity to agree or disagree with the parent's understanding and elaborate if needed.\n\nPraise is the fourth Do, and is very important because it can make children feel good and increase warmth, an important goal of the CDI. Praise statements such as “Good job!” show the child that their creations and actions are important. This is important because children tend to believe the things parents say to them, whether they be positive or negative. The manual specifies two types of praise. “Labeled praise” statements specify exactly what the parent likes about their behavior. For example, saying “You did a beautiful job of drawing that picture” not only teaches children that they did something the parent liked, but also teaches them what they did to earn that praise.Because PCIT can be used from ages 2 through 7, coaching takes into account the developmental differences at each age and teaches parents to be mindful of those differences. Parents are encouraged to praise and reflect all attempts of their child to verbally communicate, as speech skills are concurrently developing.\n\nAccording to Eyberg (1988), during the PDI component, parents continue the skills learned in the CDI, but this time they are taught new skills to lead the play. These skills include giving verbal directions and applying the appropriate consequences to the child in a fair manner that the child can understand clearly. These steps are practiced at the clinic, and parents are not encouraged to practice at home until they feel confident.\n\nEyberg (1988) states that he first step is to give clear, direct commands for the desired behavior from the child and to avoid indirect commands, which can be too vague and confusing to the child. For example, “Put this red table in the house” is a direct command. However, an indirect command such as “Will you color the leaves green?” can be interpreted by the child as a genuine question. Another example of an indirect command is “Let’s clean up the toys”, which does not indicate clearly if both the parent and child will be doing the task or how much of the task the child will do themselves. Additionally, phrases that are too general, such as “Be good”, should be avoided, as it does not provide enough information about what exactly is expected of the child. In sum, clear statements should be used towards the child so they can understand easily without getting confused.\n\nEyberg (1988) provides some guidelines for parents to teach parents when giving direct commands. First, the commands should be stated positively and should tell the child what to do, rather than what not to do. For example, “Put your hands in your lap” should be used instead of “Stop grabbing the toys”. Secondly, the command should be one that is age-appropriate for the child. For example, telling a 2-year-old “Tie your shoe” would be considered not age-appropriate. Lastly, the command should require only one behavior at a time. This way, children need not remember long strings of orders in a single command.\n\nThe second step of PDI involves labeled praise when the child displays the desired behavior. For example, “I like it when you do what I tell you to do so quickly!” tells the child what specific action pleased the parent and this praise will help increase that desired behavior.\n\nThe third step is to initiate time-out whenever the child is noncompliance. Eyberg states that noncompliance will be reinforced by both parental attention and when the child is able to get out of something they do not want to do. An example may be a warning followed by a three-minute time-out.\n\nEventually, as these skills are mastered by the parent, the commands can begin to address relevant behavioral problems the child may be displaying. The approach depends on the treatment goal. For example, if the goal is to increase a certain desired behavior, the parent must break the skill down into simpler parts that can be built on through practice and labeled praise until the child masters it.\n\nDPICS is an observational system originally created for conduct problem families. It uses direct observations of behaviors to assess parent-child interactions. DPICS has undergone two revisions since its first edition published in 1981. The DPICS categories serve as indicators of relationship quality, measured by verbal and physical behaviors during social interactions. Examples of parent behavior categories are direct and indirect commands, behavior descriptions, reflective statements, praise, information descriptions, questions, and negative talk. Child behavior categories include compliance and noncompliance, physical positive and negative, yell, whine, smart talk, laugh, and destructive behavior.\n\nThe ECBI is a 36-item behavior scale that is used to track disruptive behaviors in children. It was constructed from data indicating the most typical problem behaviors reported by parents of conduct problem children. The measure includes two scales: Intensity and Problem. Parents report Intensity by rating how frequent each item occurs. The Problem scale asks parents “Is this behavioral problem for you?” to which parents respond “yes” or “no”. This measure can be used for children aged 2 to 16.\n\nDisruptive behavior problems are the leading reason for children's referrals to mental health professionals. and PCIT was first created to target these behaviors. Results from a randomized controlled trial examining the efficacy of PCIT on clinic referred children with diagnoses of Oppositional Defiant Disorder indicated that compared to the waitlist control group, parents interacted more positively with their children and were more successful at gaining compliance. Additionally, parents in the treatment group reported decreased parenting stress and more control. Parents also reported significant improvements in their child's behavior following treatment.\n\nSimilar results have been shown in a quasi-experimental study by Boggs and colleagues (2004) that evaluated families who completed the treatment program compared to families who dropped out of the study before completion. For those who completed treatment, parents reported positive changes 10-30 months following treatment in their child's behavior and their parenting stress. Those who dropped out of treatment early did not show significant changes.\n\nIn a meta-analysis that conducted a comprehensive review of PCIT's efficacy with children diagnosed with ADHD, ODD, or CD, PCIT was found to be an “efficacious intervention for improving externalizing behavior in children with disruptive behavior disorders”. Another meta-analysis that focused on parenting stress in addition to child behaviors as outcomes found PCIT to have a “beneficial impact on parents’ and primary caregivers’ perceptions of all outcomes examined, including child externalizing behaviors, child's temperament and self-regulatory abilities, frequency of behavior problems, the difficulty of parent-child interactions, and parent overall distress”.\n\nThe treatment effects of PCIT can also be demonstrated in school settings, despite the treatment program being lab- or home-based. In a study by Funderburk and colleagues (2009), school assessments were administered at 12 months and 18 months following PCIT. At 12 months, results indicated that children in the treatment group maintained their post-treatment improvements, improving within “normal range of conduct problems” compared to the control group. However, though maintaining improvements with compliance, the 18-month followup indicated some declines into the range of levels before treatment.\n\nStudies have examined the effectiveness of PCIT with families at risk or engaged in child maltreatment. Evidence suggests that factors such as coercive patterns of parent-child interactions, less sensitivity towards the child, and insecure child attachment can be risks for child maltreatmen.t In a randomized controlled trial composing of 12-session PCIT, mothers reported less internalizing and externalizing behaviors in children in the PCIT group. Additionally, mothers reported less stress, more positive verbalizations and maternal sensitivity. Other studies have found similar results, including a reduction of abuse risk post-treatment compared to the waitlist control.\n\nPCIT may also be an effective intervention for maltreated children in foster care settings. Because children with behavioral problems in foster care are more likely to have multiple foster care placements and mental health problems, the interventions that improve foster parents’ skills in managing children's difficult behaviors are needed. Findings from a study comparing foster parents and their foster children to non-abusive biological parents and their children demonstrated PCIT's effectiveness in reducing child behavior problems and caregiver distress following treatment for both groups.\n\nThe PCIT has been adapted to treat major depressive disorder in preschool-aged children, called the PCIT-ED. The Emotional Development module (ED) was added to target emotion development impairments in very young children, specifically. Its goal is to help children regulate and understand their own emotions more effectively. The two phases of PCIT, CDI and PDI, are retained, but are shortened to six session per phase. Parents are taught skills that help their child in identifying and managing their emotions. For example, this may involve recognizing the child's “triggers” and using relaxation techniques to calm them. Often, parents may try to stop the child's expression of negative emotion, but during ED, parents are taught to tolerate these negative emotions so their child can learn to regulate them.\n\nThe pilot study of the PCIT-ED was an open trial study that examined a group of preschool children with depression, assessing symptoms before and after treatment. This study showed decreased depressive symptoms in children, and most children no longer met major depressive disorder criteria upon completion of treatment. Additionally, children improved their coping skills, prosocial behaviors, and thought processes. The first randomized controlled trial that compared PCIT-ED to psychoeducation in depressed preschoolers and their caregivers also showed significant improvement two weeks posttreatment for the PCIT-ED group in emotion development, child executive functioning, and parenting stress.\n\nSeparation anxiety disorder (SAD) is the most common anxiety disorder in children which is characterized by an “excessive fear response to real or imagined separation from a caregiver”. PCIT involves many parenting skills that are important in reducing children's anxiety, such as command training, selective attention, reinforcement, and shaping the child's behavior.\n\nPilot study results by Pincus and colleagues (2008) evaluating the efficacy of PCIT in 10 young children with SAD showed that did not improve to nonclinical levels posttreatment, however there was improvement in the severity of SAD. Pincus and colleagues (2008) also proposed an adaptation to the PCIT that would include the Bravery-Directed Interaction (BDI) phase. The BDI phase includes a psychoeducational component for the parents about anxiety. It also includes a gradual exposure to the separation situations the child fears. This exposure is key for all anxiety disorders. The BDI focuses on establishing a sense of control in the child by giving them the freedom to choose one exposure activity a week from the “Bravery Ladder” homework assignment, rather than having their parent choose. An initial randomized controlled trial has been conducted to evaluate the modified PCIT, comparing its efficacy to a waitlist control group. It seeks to assess the maintenance of change at 3, 6, and 12 months posttreatment. Preliminary results of study show decreased severity of SAD post-treatment.\n\nChildren are at an especially high risk for externalizing and internalizing problems following interparental violence-exposure or domestic violence. Borrego and colleagues (2008) have provided rationale for the use of PCIT with domestic violence-exposed women and their children, proposing that the parent training component may be very beneficial for mothers who may have “low levels of confidence in their own parenting capabilities and may also have low self-esteem”. Additionally, Borrego and colleagues (2008) emphasized that because PCIT is relationship-based, it may improve the quality of the mother-child relationship, developing a secure attachment between mother and child, and may lead to a decrease in the severity of trauma symptoms experienced by both.\n\nOne study by Timmer and colleagues (2010) compared the effectiveness of PCIT in reducing behavior problems in maltreated children exposed to interparental violence (IPV) and similar children with no history of IPV-exposure. Results indicated that there were decreases in behavior problems and caregivers’ distress from pre- to posttreatment in both the IPV-exposed and non-exposed groups. However, there was no significant difference between variations of IPV exposure.\n\nThe implementation of PCIT in the home has been examined in order to increase accessibility. Protocol was followed as closely as possible, with the exception that treatment was conducted within the home. Some modifications may be necessary in this setting. For example, the bug-in-ear (a small, wireless earpiece) was used for coaching parents could not be used. Instead, therapists were present in the same room for coaching, typically behind the caregiver, giving discrete feedback. Therapists were able to conduct DPICS observations, however these observations were coded live.\n\nThe in-home administration of PCIT in a single-subject study by Ware and colleagues (2012) has yielded promising results, such as decrease in caregiver use of negative behavior and increase in use of positive behavior and praise posttreatment. PCIT has also shown to improve child outcomes as well. PCIT completers were found to have significantly lower risk of child abuse compared to noncompleters, decrease in child behavior problems, and increased child compliance posttreatment.\n\nThere are certain advantages that come with in-home PCIT. For example, therapists are able to take advantage of more authentic, “real life” behaviors that may not be accurately captured within a laboratory or clinic setting. Additionally, in-home PCIT can combat attrition, a problem commonly faced by therapists.\n\nThis approach has potential drawbacks, as well. For example, because homes vary greatly across families, it is much more difficult for therapists control, unlike a laboratory or clinic setting. It may also be more difficult to keep children within the room and within the therapist's sight, as the child has more freedom to “escape” if needed. These problems can be avoided by deciding beforehand which room the therapy will take place and by minimizing potential distractions. Availability of resources can be an issue as well, particularly when the treatment requires use of age-appropriate toys that are typically controlled by the therapist in clinical settings. In homes, there may be limited options of activities. However, talking to the parent beforehand about what they might prefer to play with may be helpful, and the therapist can plan to bring the toys needed.\n\nPCIT implemented in the community involves administration in community settings such as in the home, mental health services agencies, or family services agencies. Few studies have examined the effectiveness of PCIT in community settings, however one implementation through community agencies has shown decreases in behavior problems, improved parent-child interactions, reduced parental stress in a four-family clinical case study posttreatment. Additionally, a study by Lanier and colleagues (2014) found PCIT to be effective for maltreatment prevention in a group of families receiving PCIT at posttreatment followup.\n\nIn an effort to increase accessibility and address obstacles of receiving treatment, especially in underserved communities, an internet-based delivery of PCIT has been proposed and tested. This method uses video conferencing, webcams, and wireless earpieces, allowing for therapists to continue to provide real-time feedback to caregivers, right from the comfort of their home. Advantages of this method include the ability to generalize findings better because families were treated in natural settings, which are the settings in which child disruptive behaviors are most likely to manifest.\n\nAvailability of resources can pose as a problem when implementing this method of PCIT. Success is dependent on families owning, or being provided, microphones, ear pieces, webcams, computers, and Wi-Fi hotspots. In homes that lack Wi-Fi or have suboptimal internet connections, real-time feedback from therapists may be affected. Treatment providers may be able to provide the necessary equipment for families to borrow, however this depends heavily on the availability of grant funds.\n\nA randomized trial has been conducted with the Internet-Delivered Parent-Child Interaction Therapy (I-PCIT) and has shown support for its effectiveness in treating children with disruptive behavior disorders. Parents perceived less barriers to treatment when compared to those receiving clinic-based PCIT. This study demonstrated decreases in children's symptoms and burden to parents in a randomized clinical trial compared to a waitlist control group, and to traditional in-office PCIT administration. Additionally, roughly half of the children in the study no longer met the diagnostic criteria for disruptive behavior disorder.\n\nIn addition to the time-out component, Eyberg (1988) also recommended swatting child's bottom and other physical punishment as a form of discipline, however in a study by Timmer and colleagues (2005), physical punishment was not found to be necessary and has since been removed from the PCIT protocol. Timmer (2005) further suggested that it did not add anything and suggested a more hands-off approach to parenting.\n\nAttrition rates among families receiving PCIT are an ongoing concern. In a meta-analysis by Thomas and Zimmer-Gembeck (2012), attrition rates ranged from 18 to 35% among studies that reported attrition.\n\n"}
{"id": "23015", "url": "https://en.wikipedia.org/wiki?curid=23015", "title": "Programming language", "text": "Programming language\n\nA programming language is a formal language, which comprises a set of instructions used to produce various kinds of output. Programming languages are used in computer programming to create programs that implement specific algorithms.\n\nMost programming languages consist of instructions for computers, although there are programmable machines that use a limited set of specific instructions, rather than the general programming languages of modern computers. Early ones preceded the invention of the digital computer, the first probably being the automatic flute player described in the 9th century by the brothers Musa in Baghdad, during the Islamic Golden Age. From the early 1800s, programs were used to direct the behavior of machines such as Jacquard looms, music boxes and player pianos. However, their programs (such as a player piano's scrolls) could not produce different behavior in response to some input or condition.\n\nThousands of different programming languages have been created, mainly in the computer field, and many more still are being created every year. Many programming languages require computation to be specified in an imperative form (i.e., as a sequence of operations to perform) while other languages use other forms of program specification such as the declarative form (i.e. the desired result is specified, not how to achieve it).\n\nThe description of a programming language is usually split into the two components of syntax (form) and semantics (meaning). Some languages are defined by a specification document (for example, the C programming language is specified by an ISO Standard) while other languages (such as Perl) have a dominant implementation that is treated as a reference. Some languages have both, with the basic language defined by a standard and extensions taken from the dominant implementation being common.\n\nA programming language is a notation for writing programs, which are specifications of a computation or algorithm. Some, but not all, authors restrict the term \"programming language\" to those languages that can express \"all\" possible algorithms. Traits often considered important for what constitutes a programming language include:\n\n\nMarkup languages like XML, HTML, or troff, which define structured data, are not usually considered programming languages. Programming languages may, however, share the syntax with markup languages if a computational semantics is defined. XSLT, for example, is a Turing complete language entirely using XML syntax. Moreover, LaTeX, which is mostly used for structuring documents, also contains a Turing complete subset.\n\nThe term \"computer language\" is sometimes used interchangeably with programming language. However, the usage of both terms varies among authors, including the exact scope of each. One usage describes programming languages as a subset of computer languages. In this vein, languages used in computing that have a different goal than expressing computer programs are generically designated computer languages. For instance, markup languages are sometimes referred to as computer languages to emphasize that they are not meant to be used for programming.\n\nAnother usage regards programming languages as theoretical constructs for programming abstract machines, and computer languages as the subset thereof that runs on physical computers, which have finite hardware resources. John C. Reynolds emphasizes that formal specification languages are just as much programming languages as are the languages intended for execution. He also argues that textual and even graphical input formats that affect the behavior of a computer are programming languages, despite the fact they are commonly not Turing-complete, and remarks that ignorance of programming language concepts is the reason for many flaws in input formats.\n\nVery early computers, such as Colossus, were programmed without the help of a stored program, by modifying their circuitry or setting banks of physical controls.\n\nSlightly later, programs could be written in machine language, where the programmer writes each instruction in a numeric form the hardware can execute directly. For example, the instruction to add the value in two memory location might consist of 3 numbers: an \"opcode\" that selects the \"add\" operation, and two memory locations. The programs, in decimal or binary form, were read in from punched cards or paper tape or magnetic tape or toggled in on switches on the front panel of the computer. Machine languages were later termed \"first-generation programming languages\" (1GL).\n\nThe next step was development of so-called \"second-generation programming languages\" (2GL) or assembly languages, which were still closely tied to the instruction set architecture of the specific computer. These served to make the program much more human-readable and relieved the programmer of tedious and error-prone address calculations.\n\nThe first \"high-level programming languages\", or \"third-generation programming languages\" (3GL), were written in the 1950s. An early high-level programming language to be designed for a computer was Plankalkül, developed for the German Z3 by Konrad Zuse between 1943 and 1945. However, it was not implemented until 1998 and 2000.\n\nJohn Mauchly's Short Code, proposed in 1949, was one of the first high-level languages ever developed for an electronic computer. Unlike machine code, Short Code statements represented mathematical expressions in understandable form. However, the program had to be translated into machine code every time it ran, making the process much slower than running the equivalent machine code.\nAt the University of Manchester, Alick Glennie developed Autocode in the early 1950s. A programming language, it used a compiler to automatically convert the language into machine code. The first code and compiler was developed in 1952 for the Mark 1 computer at the University of Manchester and is considered to be the first compiled high-level programming language.\n\nThe second autocode was developed for the Mark 1 by R. A. Brooker in 1954 and was called the \"Mark 1 Autocode\". Brooker also developed an autocode for the Ferranti Mercury in the 1950s in conjunction with the University of Manchester. The version for the EDSAC 2 was devised by D. F. Hartley of University of Cambridge Mathematical Laboratory in 1961. Known as EDSAC 2 Autocode, it was a straight development from Mercury Autocode adapted for local circumstances and was noted for its object code optimisation and source-language diagnostics which were advanced for the time. A contemporary but separate thread of development, Atlas Autocode was developed for the University of Manchester Atlas 1 machine.\n\nIn 1954, FORTRAN was invented at IBM by John Backus. It was the first widely used high-level general purpose programming language to have a functional implementation, as opposed to just a design on paper. It is still a popular language for high-performance computing and is used for programs that benchmark and rank the world's fastest supercomputers.\n\nAnother early programming language was devised by Grace Hopper in the US, called FLOW-MATIC. It was developed for the UNIVAC I at Remington Rand during the period from 1955 until 1959. Hopper found that business data processing customers were uncomfortable with mathematical notation, and in early 1955, she and her team wrote a specification for an English programming language and implemented a prototype. The FLOW-MATIC compiler became publicly available in early 1958 and was substantially complete in 1959. Flow-Matic was a major influence in the design of COBOL, since only it and its direct descendant AIMACO were in actual use at the time.\n\nThe increased use of high-level languages introduced a requirement for \"low-level programming languages\" or \"system programming languages\". These languages, to varying degrees, provide facilities between assembly languages and high-level languages and can be used to perform tasks which require direct access to hardware facilities but still provide higher-level control structures and error-checking.\nThe period from the 1960s to the late 1970s brought the development of the major language paradigms now in use:\nEach of these languages spawned descendants, and most modern programming languages count at least one of them in their ancestry.\n\nThe 1960s and 1970s also saw considerable debate over the merits of \"structured programming\", and whether programming languages should be designed to support it. Edsger Dijkstra, in a famous 1968 letter published in the Communications of the ACM, argued that GOTO statements should be eliminated from all \"higher level\" programming languages.\n\nThe 1980s were years of relative consolidation. C++ combined object-oriented and systems programming. The United States government standardized Ada, a systems programming language derived from Pascal and intended for use by defense contractors. In Japan and elsewhere, vast sums were spent investigating so-called \"fifth generation\" languages that incorporated logic programming constructs. The functional languages community moved to standardize ML and Lisp. Rather than inventing new paradigms, all of these movements elaborated upon the ideas invented in the previous decades.\n\nOne important trend in language design for programming large-scale systems during the 1980s was an increased focus on the use of \"modules\" or large-scale organizational units of code. Modula-2, Ada, and ML all developed notable module systems in the 1980s, which were often wedded to generic programming constructs.\n\nThe rapid growth of the Internet in the mid-1990s created opportunities for new languages. Perl, originally a Unix scripting tool first released in 1987, became common in dynamic websites. Java came to be used for server-side programming, and bytecode virtual machines became popular again in commercial settings with their promise of \"Write once, run anywhere\" (UCSD Pascal had been popular for a time in the early 1980s). These developments were not fundamentally novel, rather they were refinements of many existing languages and paradigms (although their syntax was often based on the C family of programming languages).\n\nProgramming language evolution continues, in both industry and research. Current directions include security and reliability verification, new kinds of modularity (mixins, delegates, aspects), and database integration such as Microsoft's LINQ.\n\n\"Fourth-generation programming languages\" (4GL) are computer programming languages which aim to provide a higher level of abstraction of the internal computer hardware details than 3GLs. \"Fifth generation programming languages\" (5GL) are programming languages based on solving problems using constraints given to the program, rather than using an algorithm written by a programmer.\n\nAll programming languages have some primitive building blocks for the description of data and the processes or transformations applied to them (like the addition of two numbers or the selection of an item from a collection). These primitives are defined by syntactic and semantic rules which describe their structure and meaning respectively.\n\nA programming language's surface form is known as its syntax. Most programming languages are purely textual; they use sequences of text including words, numbers, and punctuation, much like written natural languages. On the other hand, there are some programming languages which are more graphical in nature, using visual relationships between symbols to specify a program.\n\nThe syntax of a language describes the possible combinations of symbols that form a syntactically correct program. The meaning given to a combination of symbols is handled by semantics (either formal or hard-coded in a reference implementation). Since most languages are textual, this article discusses textual syntax.\n\nProgramming language syntax is usually defined using a combination of regular expressions (for lexical structure) and Backus–Naur form (for grammatical structure). Below is a simple grammar, based on Lisp:\nThis grammar specifies the following:\n\nThe following are examples of well-formed token sequences in this grammar: codice_1, codice_2 and codice_3.\n\nNot all syntactically correct programs are semantically correct. Many syntactically correct programs are nonetheless ill-formed, per the language's rules; and may (depending on the language specification and the soundness of the implementation) result in an error on translation or execution. In some cases, such programs may exhibit undefined behavior. Even when a program is well-defined within a language, it may still have a meaning that is not intended by the person who wrote it.\n\nUsing natural language as an example, it may not be possible to assign a meaning to a grammatically correct sentence or the sentence may be false:\n\nThe following C language fragment is syntactically correct, but performs operations that are not semantically defined (the operation codice_4 has no meaning for a value having a complex type and codice_5 is not defined because the value of codice_6 is the null pointer):\n\nIf the type declaration on the first line were omitted, the program would trigger an error on undefined variable \"p\" during compilation. However, the program would still be syntactically correct since type declarations provide only semantic information.\n\nThe grammar needed to specify a programming language can be classified by its position in the Chomsky hierarchy. The syntax of most programming languages can be specified using a Type-2 grammar, i.e., they are context-free grammars. Some languages, including Perl and Lisp, contain constructs that allow execution during the parsing phase. Languages that have constructs that allow the programmer to alter the behavior of the parser make syntax analysis an undecidable problem, and generally blur the distinction between parsing and execution. In contrast to Lisp's macro system and Perl's codice_7 blocks, which may contain general computations, C macros are merely string replacements and do not require code execution.\n\nThe term \"semantics\" refers to the meaning of languages, as opposed to their form (syntax).\n\nThe static semantics defines restrictions on the structure of valid texts that are hard or impossible to express in standard syntactic formalisms. For compiled languages, static semantics essentially include those semantic rules that can be checked at compile time. Examples include checking that every identifier is declared before it is used (in languages that require such declarations) or that the labels on the arms of a case statement are distinct. Many important restrictions of this type, like checking that identifiers are used in the appropriate context (e.g. not adding an integer to a function name), or that subroutine calls have the appropriate number and type of arguments, can be enforced by defining them as rules in a logic called a type system. Other forms of static analyses like data flow analysis may also be part of static semantics. Newer programming languages like Java and C# have definite assignment analysis, a form of data flow analysis, as part of their static semantics.\n\nOnce data has been specified, the machine must be instructed to perform operations on the data. For example, the semantics may define the strategy by which expressions are evaluated to values, or the manner in which control structures conditionally execute statements. The \"dynamic semantics\" (also known as \"execution semantics\") of a language defines how and when the various constructs of a language should produce a program behavior. There are many ways of defining execution semantics. Natural language is often used to specify the execution semantics of languages commonly used in practice. A significant amount of academic research went into formal semantics of programming languages, which allow execution semantics to be specified in a formal manner. Results from this field of research have seen limited application to programming language design and implementation outside academia.\n\nA type system defines how a programming language classifies values and expressions into \"types\", how it can manipulate those types and how they interact. The goal of a type system is to verify and usually enforce a certain level of correctness in programs written in that language by detecting certain incorrect operations. Any decidable type system involves a trade-off: while it rejects many incorrect programs, it can also prohibit some correct, albeit unusual programs. In order to bypass this downside, a number of languages have \"type loopholes\", usually unchecked casts that may be used by the programmer to explicitly allow a normally disallowed operation between different types. In most typed languages, the type system is used only to type check programs, but a number of languages, usually functional ones, infer types, relieving the programmer from the need to write type annotations. The formal design and study of type systems is known as \"type theory\".\n\nA language is \"typed\" if the specification of every operation defines types of data to which the operation is applicable, with the implication that it is not applicable to other types. For example, the data represented by codice_8 is a string, and in many programming languages dividing a number by a string has no meaning and will be rejected by the compilers. The invalid operation may be detected when the program is compiled (\"static\" type checking) and will be rejected by the compiler with a compilation error message, or it may be detected when the program is run (\"dynamic\" type checking), resulting in a run-time exception. Many languages allow a function called an exception handler to be written to handle this exception and, for example, always return \"-1\" as the result.\n\nA special case of typed languages are the \"single-type\" languages. These are often scripting or markup languages, such as REXX or SGML, and have only one data type-—most commonly character strings which are used for both symbolic and numeric data.\n\nIn contrast, an \"untyped language\", such as most assembly languages, allows any operation to be performed on any data, which are generally considered to be sequences of bits of various lengths. High-level languages which are untyped include BCPL, Tcl, and some varieties of Forth.\n\nIn practice, while few languages are considered typed from the point of view of type theory (verifying or rejecting \"all\" operations), most modern languages offer a degree of typing. Many production languages provide means to bypass or subvert the type system, trading type-safety for finer control over the program's execution (see casting).\n\nIn \"static typing\", all expressions have their types determined prior to when the program is executed, typically at compile-time. For example, 1 and (2+2) are integer expressions; they cannot be passed to a function that expects a string, or stored in a variable that is defined to hold dates.\n\nStatically typed languages can be either \"manifestly typed\" or \"type-inferred\". In the first case, the programmer must explicitly write types at certain textual positions (for example, at variable declarations). In the second case, the compiler \"infers\" the types of expressions and declarations based on context. Most mainstream statically typed languages, such as C++, C# and Java, are manifestly typed. Complete type inference has traditionally been associated with less mainstream languages, such as Haskell and ML. However, many manifestly typed languages support partial type inference; for example, C++, Java and C# all infer types in certain limited cases. Additionally, some programming languages allow for some types to be automatically converted to other types; for example, an int can be used where the program expects a float.\n\n\"Dynamic typing\", also called \"latent typing\", determines the type-safety of operations at run time; in other words, types are associated with \"run-time values\" rather than \"textual expressions\". As with type-inferred languages, dynamically typed languages do not require the programmer to write explicit type annotations on expressions. Among other things, this may permit a single variable to refer to values of different types at different points in the program execution. However, type errors cannot be automatically detected until a piece of code is actually executed, potentially making debugging more difficult. Lisp, Smalltalk, Perl, Python, JavaScript, and Ruby are all examples of dynamically typed languages.\n\n\"Weak typing\" allows a value of one type to be treated as another, for example treating a string as a number. This can occasionally be useful, but it can also allow some kinds of program faults to go undetected at compile time and even at run time.\n\n\"Strong typing\" prevents the above. An attempt to perform an operation on the wrong type of value raises an error. Strongly typed languages are often termed \"type-safe\" or \"safe\".\n\nAn alternative definition for \"weakly typed\" refers to languages, such as Perl and JavaScript, which permit a large number of implicit type conversions. In JavaScript, for example, the expression codice_9 implicitly converts codice_10 to a number, and this conversion succeeds even if codice_10 is codice_12, codice_13, an codice_14, or a string of letters. Such implicit conversions are often useful, but they can mask programming errors.\n\"Strong\" and \"static\" are now generally considered orthogonal concepts, but usage in the literature differs. Some use the term \"strongly typed\" to mean \"strongly, statically typed\", or, even more confusingly, to mean simply \"statically typed\". Thus C has been called both strongly typed and weakly, statically typed.\n\nIt may seem odd to some professional programmers that C could be \"weakly, statically typed\". However, notice that the use of the generic pointer, the void* pointer, does allow for casting of pointers to other pointers without needing to do an explicit cast. This is extremely similar to somehow casting an array of bytes to any kind of datatype in C without using an explicit cast, such as codice_15 or codice_16.\n\nMost programming languages have an associated core library (sometimes known as the 'standard library', especially if it is included as part of the published language standard), which is conventionally made available by all implementations of the language. Core libraries typically include definitions for commonly used algorithms, data structures, and mechanisms for input and output.\n\nThe line between a language and its core library differs from language to language. In some cases, the language designers may treat the library as a separate entity from the language. However, a language's core library is often treated as part of the language by its users, and some language specifications even require that this library be made available in all implementations. Indeed, some languages are designed so that the meanings of certain syntactic constructs cannot even be described without referring to the core library. For example, in Java, a string literal is defined as an instance of the codice_17 class; similarly, in Smalltalk, an anonymous function expression (a \"block\") constructs an instance of the library's codice_18 class. Conversely, Scheme contains multiple coherent subsets that suffice to construct the rest of the language as library macros, and so the language designers do not even bother to say which portions of the language must be implemented as language constructs, and which must be implemented as parts of a library.\n\nProgramming languages share properties with natural languages related to their purpose as vehicles for communication, having a syntactic form separate from its semantics, and showing \"language families\" of related languages branching one from another. But as artificial constructs, they also differ in fundamental ways from languages that have evolved through usage. A significant difference is that a programming language can be fully described and studied in its entirety, since it has a precise and finite definition. By contrast, natural languages have changing meanings given by their users in different communities. While constructed languages are also artificial languages designed from the ground up with a specific purpose, they lack the precise and complete semantic definition that a programming language has.\n\nMany programming languages have been designed from scratch, altered to meet new needs, and combined with other languages. Many have eventually fallen into disuse. Although there have been attempts to design one \"universal\" programming language that serves all purposes, all of them have failed to be generally accepted as filling this role. The need for diverse programming languages arises from the diversity of contexts in which languages are used:\n\nOne common trend in the development of programming languages has been to add more ability to solve problems using a higher level of abstraction. The earliest programming languages were tied very closely to the underlying hardware of the computer. As new programming languages have developed, features have been added that let programmers express ideas that are more remote from simple translation into underlying hardware instructions. Because programmers are less tied to the complexity of the computer, their programs can do more computing with less effort from the programmer. This lets them write more functionality per time unit.\nNatural language programming has been proposed as a way to eliminate the need for a specialized language for programming. However, this goal remains distant and its benefits are open to debate. Edsger W. Dijkstra took the position that the use of a formal language is essential to prevent the introduction of meaningless constructs, and dismissed natural language programming as \"foolish\". Alan Perlis was similarly dismissive of the idea. Hybrid approaches have been taken in Structured English and SQL.\n\nA language's designers and users must construct a number of artifacts that govern and enable the practice of programming. The most important of these artifacts are the language \"specification\" and \"implementation\".\n\nThe specification of a programming language is an artifact that the language users and the implementors can use to agree upon whether a piece of source code is a valid program in that language, and if so what its behavior shall be.\n\nA programming language specification can take several forms, including the following:\n\nAn \"implementation\" of a programming language provides a way to write programs in that language and execute them on one or more configurations of hardware and software. There are, broadly, two approaches to programming language implementation: \"compilation\" and \"interpretation\". It is generally possible to implement a language using either technique.\n\nThe output of a compiler may be executed by hardware or a program called an interpreter. In some implementations that make use of the interpreter approach there is no distinct boundary between compiling and interpreting. For instance, some implementations of BASIC compile and then execute the source a line at a time.\n\nPrograms that are executed directly on the hardware usually run several orders of magnitude faster than those that are interpreted in software.\n\nOne technique for improving the performance of interpreted programs is just-in-time compilation. Here the virtual machine, just before execution, translates the blocks of bytecode which are going to be used to machine code, for direct execution on the hardware.\n\nAlthough most of the most commonly used programming languages have fully open specifications and implementations, many programming languages exist only as proprietary programming languages with the implementation available only from a single vendor, which may claim that such a proprietary language is their intellectual property. Proprietary programming languages are commonly domain specific languages or internal scripting languages for a single product; some proprietary languages are used only internally within a vendor, while others are available to external users.\n\nSome programming languages exist on the border between proprietary and open; for example, Oracle Corporation asserts proprietary rights to some aspects of the Java programming language, and Microsoft's C# programming language, which has open implementations of most parts of the system, also has Common Language Runtime (CLR) as a closed environment.\n\nMany proprietary languages are widely used, in spite of their proprietary nature; examples include MATLAB, VBScript, and Wolfram Language. Some languages may make the transition from closed to open; for example, Erlang was originally an Ericsson's internal programming language.\n\nThousands of different programming languages have been created, mainly in the computing field.\nSoftware is commonly built with 5 programming languages or more.\n\nProgramming languages differ from most other forms of human expression in that they require a greater degree of precision and completeness. When using a natural language to communicate with other people, human authors and speakers can be ambiguous and make small errors, and still expect their intent to be understood. However, figuratively speaking, computers \"do exactly what they are told to do\", and cannot \"understand\" what code the programmer intended to write. The combination of the language definition, a program, and the program's inputs must fully specify the external behavior that occurs when the program is executed, within the domain of control of that program. On the other hand, ideas about an algorithm can be communicated to humans without the precision required for execution by using pseudocode, which interleaves natural language with code written in a programming language.\n\nA programming language provides a structured mechanism for defining pieces of data, and the operations or transformations that may be carried out automatically on that data. A programmer uses the abstractions present in the language to represent the concepts involved in a computation. These concepts are represented as a collection of the simplest elements available (called primitives). \"Programming\" is the process by which programmers combine these primitives to compose new programs, or adapt existing ones to new uses or a changing environment.\n\nPrograms for a computer might be executed in a batch process without human interaction, or a user might type commands in an interactive session of an interpreter. In this case the \"commands\" are simply programs, whose execution is chained together. When a language can run its commands through an interpreter (such as a Unix shell or other command-line interface), without compiling, it is called a scripting language.\n\nDetermining which is the most widely used programming language is difficult since the definition of usage varies by context. One language may occupy the greater number of programmer hours, a different one has more lines of code, and a third may consume the most CPU time. Some languages are very popular for particular kinds of applications. For example, COBOL is still strong in the corporate data center, often on large mainframes; Fortran in scientific and engineering applications; Ada in aerospace, transportation, military, real-time and embedded applications; and C in embedded applications and operating systems. Other languages are regularly used to write many different kinds of applications.\n\nVarious methods of measuring language popularity, each subject to a different bias over what is measured, have been proposed:\n\nCombining and averaging information from various internet sites, stackify.com reported the ten most popular programming languages as (in descending order by overall popularity): Java, C, C++, Python, C#, JavaScript, VB .NET, R, PHP, and MATLAB.\n\nA dialect of a programming language or a data exchange language is a (relatively small) variation or extension of the language that does not change its intrinsic nature. With languages such as Scheme and Forth, standards may be considered insufficient, inadequate or illegitimate by implementors, so often they will deviate from the standard, making a new dialect. In other cases, a dialect is created for use in a domain-specific language, often a subset. In the Lisp world, most languages that use basic S-expression syntax and Lisp-like semantics are considered Lisp dialects, although they vary wildly, as do, say, Racket and Clojure. As it is common for one language to have several dialects, it can become quite difficult for an inexperienced programmer to find the right documentation. The BASIC programming language has many dialects.\n\nThe explosion of Forth dialects led to the saying \"If you've seen one Forth... you've seen \"one\" Forth.\"\n\nThere is no overarching classification scheme for programming languages. A given programming language does not usually have a single ancestor language. Languages commonly arise by combining the elements of several predecessor languages with new ideas in circulation at the time. Ideas that originate in one language will diffuse throughout a family of related languages, and then leap suddenly across familial gaps to appear in an entirely different family.\n\nThe task is further complicated by the fact that languages can be classified along multiple axes. For example, Java is both an object-oriented language (because it encourages object-oriented organization) and a concurrent language (because it contains built-in constructs for running multiple threads in parallel). Python is an object-oriented scripting language.\n\nIn broad strokes, programming languages divide into \"programming paradigms\" and a classification by \"intended domain of use,\" with general-purpose programming languages distinguished from domain-specific programming languages. Traditionally, programming languages have been regarded as describing computation in terms of imperative sentences, i.e. issuing commands. These are generally called imperative programming languages. A great deal of research in programming languages has been aimed at blurring the distinction between a program as a set of instructions and a program as an assertion about the desired answer, which is the main feature of declarative programming. More refined paradigms include procedural programming, object-oriented programming, functional programming, and logic programming; some languages are hybrids of paradigms or multi-paradigmatic. An assembly language is not so much a paradigm as a direct model of an underlying machine architecture. By purpose, programming languages might be considered general purpose, system programming languages, scripting languages, domain-specific languages, or concurrent/distributed languages (or a combination of these). Some general purpose languages were designed largely with educational goals.\n\nA programming language may also be classified by factors unrelated to programming paradigm. For instance, most programming languages use English language keywords, while a minority do not. Other languages may be classified as being deliberately esoteric or not.\n"}
{"id": "25090526", "url": "https://en.wikipedia.org/wiki?curid=25090526", "title": "Relational transgression", "text": "Relational transgression\n\nRelational transgressions occur when people violate implicit or explicit relational rules. These transgressions include a wide variety of behaviors. \n\nScholars tend to delineate relational transgressions into three categories or approaches. \n\nThe first approach focuses on the aspect of certain behaviors as a violation of relational norms and rules. \n\nThe second approach focuses on the interpretive consequences of certain behaviors, particularly the degree to which they hurt the victim, imply disregard for the victim, and imply disregard for the relationship.\n\nThe third and final approach focuses more specifically on behaviors that constitute infidelity (a common form of relational transgression).\n\nThe boundaries of relational transgressions are permeable. Betrayal for example, is often used as a synonym for a relational transgression. In some instances, betrayal can be defined as a rule violation that is traumatic to a relationship, and in other instances as destructive conflict or reference to infidelity. Common forms of relational transgressions include the following: having sex with someone else, wanting to or actually dating others, deceiving one's partner about something significant, flirting with or kissing someone else, keeping secrets from your partner, becoming emotionally involved with someone else, and betraying the partner's confidence.\n\nRule violations are events, actions, and behaviors that violate an implicit or explicit relationship norm or rule. Explicit rules tend to be relationship specific, such as those prompted by the bad habits of a partner (e.g., excessive drinking or drug abuse), or those that emerge from attempts to manage conflict (e.g., rules that prohibit spending time with a former spouse or talking about a former girlfriend or boyfriend). Implicit rules tend to be those that are accepted as cultural standards for proper relationship conduct (e.g., monogamy and secrets kept private). The focus on relational transgressions as rule violations presents an opportunity to examine a wide range of behaviors across a variety of relationship types. This method facilitates analysis of transgressions from a rules perspective. In a study of college students' relational transgressions, the following nine categories emerged consistently.\n\nCameron, Ross, and Holmes (2002) identified 10 categories of common relational negative behavior that constitute relational transgressions as rule violations:\n\nInfidelity is widely recognized as one of the most hurtful relational transgressions. Around 30% to 40% of dating relationships are marked by at least one incident of sexual infidelity. It is typically among the most difficult transgressions to forgive. There are typically four methods of discovery: \nPartners who found out through a third party or by witnessing the infidelity firsthand were the least likely to forgive. Partners who confessed on their own were the most likely to be forgiven.\n\nSexual infidelity refers to sexual activity with someone other than a person's partner. Sexual infidelity can span a wide range of behavior and thoughts, including: sexual intercourse, heavy petting, passionate kissing, sexual fantasies, and sexual attraction. It can involve a sustained relationship, a one-night stand, or a prostitute. Most people in the United States openly disapprove of sexual infidelity, but research indicates that infidelity is common. Men are typically more likely than women to engage in a sexual affair, regardless if they are married or in a dating relationship.\n\nEmotional infidelity refers to emotional involvement with another person, which leads one's partner to channel emotional resources to someone else. Emotional infidelity can involve strong feelings of love and intimacy, nonsexual fantasies of falling in love, romantic attraction, or the desire to spend time with another individual. Emotional infidelity may involve a coworker, Internet partner, face-to-face communication, or a long distance phone call. Emotional infidelity is likely related to dissatisfaction with the communication and social support an individual is receiving in his or her current relationship.\n\nEach type of infidelity evokes different responses. Sexual infidelity is more likely to result in hostile, shocked, repulsed, humiliated, homicidal, or suicidal feelings. Emotional infidelity is more likely to evoke feelings of being undesirable, insecure, depressed, or abandoned. When both types of infidelity are present in a relationship, couples are more likely to break up than when only one type of infidelity is involved.\n\nWhile gender is not a reliable predictor of how any individual will react to sexual and emotional infidelity, there are nonetheless differences in how men and women on average react to sexual and emotional infidelity. Culturally Western men, relative to culturally Western women, find it more difficult to forgive a partner's sexual infidelity than a partner's emotional infidelity. Western men are also more likely to break up in response to a partner's sexual infidelity than in response to a partner's emotional infidelity. Conversely, Western women on average find it more difficult to forgive a partner's emotional infidelity than a partner's sexual infidelity, and are more likely to end a relationship in response to a partner's emotional infidelity. A possible explanation for these differences has been proposed by evolutionary psychologists: over human evolution, a partner's sexual infidelity placed men, but not women, at risk of investing resources in a rival's offspring. Therefore, a partner's sexual infidelity represents a potentially more costly adaptive problem for men than women. As such, modern men have psychological mechanisms that are acutely sensitive to a partner's sexual infidelity. \n\nWhereas on average Western men are more acutely sensitive to sexual infidelity (supposedly driven by evolutionary requirements noted above), Western women are commonly believed to have greater sensitivity to emotional infidelity. This response in women is, by the arguments of the theory above, driven by the perception that emotional infidelity suggests a long-term diversion of a partner's commitment, and a potential loss of resources. Evolutionary psychology explains this difference by arguing that a woman's loss of male support would result in a diminished chance of survival for both the woman and her offspring. Consequently, relationship factors that are more associated with commitment and partner investment play a more critical role in the psyche of women in contrast to men.\n\nWhen infidelity involves a former romantic partner, as opposed to a new partner, it is perceived to be more distressing – especially for women. Both men and women overall view situations of sexual infidelity as more distressing than situations of emotional involvement. The typical man, however, viewed only the former partner scenario as more distressing with regard to sexual infidelity; men made no distinction for emotional infidelity. Women, however, view a former partner scenario as the most distressing option for both sexual and emotional infidelity. Men and women both judge infidels of the opposite gender as acting more intentionally than their own gender.\n\nRecent research provides support for conceptualizing infidelity on a continuum ranging in severity from superficial/informal behavior to involving or goal-directed behavior. This perspective accounts for the varying degrees of behavior (e.g., sexual, emotional) on the Internet. A number of acts not involving direct, one-to-one communication with another person (e.g. posting a personal ad or looking at pornography) can be perceived as forms of infidelity. Thus, communication with another live person is not necessary for infidelity to occur. Accordingly, Internet infidelity is defined by Docan-Morgan and Docan (2007) as follows: \"An act or actions engaged via the internet by one person with a committed relationship, where such an act occurs outside the primary relationship, and constitutes a breach of trust and/or violation of agreed-upon norms (overt or covert) by one or both individuals in that relationship with regard to relational exclusivity, and is perceived as having a particular degree of severity by one or both partners.\"\n\nJealousy is the result of a relational transgression, such as a partner having a sexual or emotional affair. Jealousy can also be seen as a transgression in its own right, when a partner's suspicions are unfounded. Thus, jealousy is an important component of relational transgressions. There are several types of jealousy. Romantic jealousy occurs when a partner is concerned that a potential rival might interfere with his or her existing romantic relationship. Sexual jealousy is a specific form of romantic jealousy where an individual worries that a rival is having or wants to have sex with his or her partner. \n\nOther forms of jealousy include:\n\nJealousy is different from envy and rivalry. Envy occurs when people want something valuable that someone else has. Rivalry occurs when two people are competing for something that neither person has.\n\nIndividuals who are experiencing jealous thoughts typically make primary and secondary cognitive appraisals about their particular situation. Primary appraisals involve general evaluations about the existence and quality of a rival relationship. Secondary appraisals involve more specific evaluations about the jealous situation, including possible causes of the jealousy and potential outcomes to the situation. There are four common types of secondary appraisals: \n\nJealous individuals make appraisals to develop coping strategies and assess potential outcomes.\n\nJealous individuals normally experience combinations of emotions, in addition to the aforementioned cognitive appraisals. The most common emotions associated with jealousy are fear and anger; people are fearful of losing their relationship and they are often angry at their partner or rival. Other common negative emotions associated with jealousy are sadness, guilt, hurt, and envy. Sometimes, however, jealousy leads to positive emotions, including increased passion, love, and appreciation. \n\nRelational partners sometimes intentionally induce jealousy in their relationship. There are typically two types of goals for jealousy induction. Relational rewards reflect the desire to improve the relationship, increase self-esteem, and increase relational rewards. The second type of goal, relational revenge, reflects the desire to punish one's partner, the need for revenge, and the desire to control one's partner. The tactic of inducing jealousy may produce unintended consequences, as jealousy often leads to other relational transgressions including violence.\n\nJealousy can involve a wide range of communicative responses. These responses are based upon the individuals' goals and emotions. The most common of these responses are negative affect expression, integrative communication, and distributive communication. When people want to maintain their relationship, they use integrative communication and compensatory restoration. People who are fearful of losing their relationships typically use compensatory restoration. \n\nConversely, people who are concerned with maintaining their self-esteem allege that they deny jealous feelings. When individuals are motivated to reduce uncertainty about their partner, they use integrative communication, surveillance, and rival contacts to seek additional information. Communicative responses to jealousy may help reduce uncertainty and restore self-esteem, but they may actually increase uncertainty and negatively impact relationships and self-esteem in some instances. The type of communicative response used is critical. \n\nFor example, avoidance/denial may be used to protect one's self-esteem, but it may also result in increased uncertainty and relational dissatisfaction, if the jealous partner is left with lingering suspicions. Similarly, compensatory restoration may improve the relationship in some instances, but it may also communicate low self-esteem and desperation by the jealous individual. Distributive communication, which includes behaviors such as yelling and confrontation, may serve to vent negative emotion and retaliate by making the partner feel bad. This may exacerbate an already negative situation and make reconciliation less likely.\n\nJealousy is generally considered to be a relationship dysfunction, though it may have some positive relational properties. These positive properties can be attained through development of one's ability to manage jealousy in a productive way, so that the jealous individual shows care and concern without seeming overly fearful, aggressive, or possessive. Negative affect expression can be effective if used in conjunction with integrative communication. Compensatory restoration can be effective, but when used in excess, too much can make an individual seem desperate and too eager to please, which can have detrimental effects on the relationships.\n\nFrom the aspect of jealousy, rumination reflects uncomfortable mulling about the security of a relationship. Rumination refers to thoughts that are conscious, recurring, and not demanded by the individual's current environment. Ruminative thoughts occur repetitively and are difficult to eliminate. In the context of relational threats, rumination can be described as obsessive worry about the security of the current relationship. Individuals who ruminate are very likely to respond to jealousy differently from individuals who do not ruminate. Rumination is positively associated with several communicative responses to jealousy (e.g. compensatory restoration, negative affect expression, showing signs of possession, and derogation of competitors) that attempt to strengthen a relationship. Rumination is also associated with responses that are counterproductive. Despite efforts to restore relational intimacy, rumination sustains uncertainty, which thereby forms a cycle where rumination is sustained. Rumination intensifies over time and serves as a constant reminder to the threat to the relationship, resulting in increased negative affect. This negative affect is associated with destructive responses to jealousy including violent communication and violence towards objects. Finally, jealous rumination is associated with relational distress and counterproductive responses to jealousy.\n\nWomen generally experience more hurt, sadness, anxiety, and confusion than men, perhaps because they often blame themselves for the jealous situation. Conversely, men have been found to deny jealous feelings and focus on increasing their self-esteem. Generally speaking, women tend to be more focused on the relationship, while men tend to be more focused on individual concerns. In communicative responses, women tend to use integrative communication, express negative affect, enhance their appearance, and use counterjealousy induction more often than jealous men. Jealous men more often contact the rival, restrict the partner's access to potential rivals, and give gifts and spend money on the partner. Jealous men also engage in dangerous behaviors, such as getting drunk and engaging in promiscuous sex with others. Analysis from an evolutionary perspective would suggest that men focus on competing for mates and displaying resources (e.g., material goods to suggest financial security), while women focus on creating and enhancing social bonds and showcasing their beauty.\n\nDeception is a major relational transgression that often leads to feelings of betrayal and distrust between relational partners. Deception violates relational rules and is considered to be a negative violation of expectations. Most people expect friends, relational partners, and even strangers to be truthful most of the time. If people expected most conversations to be untruthful, talking and communicating with others would simply be unproductive and too difficult. On a given day, it is likely that most human beings will either deceive or be deceived by another person. A significant amount of deception occurs between romantic and relational partners.\n\nDeception includes several types of communications or omissions that serve to distort or omit the complete truth. Deception itself is intentionally managing verbal and/or nonverbal messages so that the message receiver will believe in a way that the message sender knows is false. Intent is critical with regard to deception. Intent differentiates between deception and an honest mistake. The Interpersonal Deception Theory explores the interrelation between \ncommunicative context and sender and receiver cognitions and behaviors in deceptive exchanges. \n\nFive primary forms of deception consist of the following:\n\nThere are three primary motivations for deceptions in close relationships.\n\nDeception detection between relational partners is extremely difficult, unless a partner tells a blatant or obvious lie or contradicts something the other partner knows to be true. While it is difficult to deceive a partner over a long period of time, deception often occurs in day-to-day conversations between relational partners. Detecting deception is difficult because there are no known completely reliable indicators of deception. Deception, however, places a significant cognitive load on the deceiver. He or she must recall previous statements so that his or her story remains consistent and believable. As a result, deceivers often leak important information both verbally and nonverbally. \n\nDeception and its detection is a complex, fluid, and cognitive process that is based on the context of the message exchange. The Interpersonal Deception Theory posits that interpersonal deception is a dynamic, iterative process of mutual influence between a sender, who manipulates information to depart from the truth, and a receiver, who attempts to establish the validity of the message. A deceiver's actions are interrelated to the message receiver's actions. It is during this exchange that the deceiver will reveal verbal and nonverbal information about deceit. Some research has found that there are some cues that may be correlated with deceptive communication, but scholars frequently disagree about the effectiveness of many of these cues to serve as reliable indicators. Noted deception scholar Aldert Vrij even states that there is no nonverbal behavior that is uniquely associated with deception. As previously stated, a specific behavioral indicator of deception does not exist. There are, however, some nonverbal behaviors that have been found to be correlated with deception. Vrij found that examining a \"cluster\" of these cues was a significantly more reliable indicator of deception than examining a single cue. \n\nIn terms of perceptions about the significance of deceiving a partner, women and men typically differ in their beliefs about deception. Women view deception as a much more profound relational transgression than men. Additionally, women rate lying in general as a less acceptable behavior than men. Finally, women are much more likely to view any act of lying as significant (regardless of the subject matter) and more likely to report negative emotional reactions to lying.\n\nThe truth bias significantly impairs the ability of relational partners to detect deception. In terms of deception, a truth bias reflects a tendency to judge more messages as truths than lies, independent of their actual veracity. When judging message veracity, the truth bias contributes to an overestimate of the actual number of truths relative to the base rate of actual truths. The truth bias is especially strong within close relationships. People are highly inclined to trust the communications of others and are unlikely to question the relational partner unless faced with a major deviation of behavior that forces a reevaluation. When attempting to detect deceit from a familiar person or relational partner, a large amount of information about the partner is brought to mind. This information essentially overwhelms the receiver's cognitive ability to detect and process any cues to deception. It is somewhat easier to detect deception in strangers, when less information about that person is brought to mind.\n\nMessages that convey negative feelings or rejection lead to emotions such as hurt and anger. Hurtful messages are associated with less satisfying relationships. Intentionally hurtful messages are among the most serious, as perceived by a partner. Unlike physical pain that usually subsides over time, hurtful messages and hurt feelings often persist for a long period of time and be recalled even years after the event. The interpersonal damage caused by hurtful messages is sometimes permanent. People are more likely to be upset if they believe their relational partner said something to deliberately hurt them. Some of the most common forms of hurtful messages include evaluations, accusations, and informative statements. \n\nFeeling devalued is a central component of hurtful messages. Similar to verbally aggressive messages, hurtful messages that are stated intensely may be viewed as particularly detrimental. The cliché \"It's not what you say, but how you say it\" is very applicable with regard to recipients' appraisals of hurtful messages. Females tend to experience more hurt than males in response to hurtful messages.\n\nIndividuals tend to experience a wide array of complex emotions following a relational transgression. These emotions are shown to have utility as an initial coping mechanism. For example, fear can result in a protective orientation following a serious transgression; sadness results in contemplation and reflection while disgust causes us to repel from its source. However, beyond the initial situation these emotions can be detrimental to one’s mental\nand physical state. Consequently, forgiveness is viewed as a more productive means of dealing with the transgression along with engaging the one who committed the transgression.\n\nForgiving is not the act of excusing or condoning. Rather, it is the process whereby negative emotions are transformed into positive emotions for the purpose of bringing emotional normalcy to a relationship. In order to achieve this transformation the offended must forgo retribution and claims for retribution. McCullough, Worthington, and Rachal (1997) defined forgiveness as a, “set of motivational changes whereby one becomes (a) decreasingly motivated to retaliate against an offending relationship partner, (b) decreasingly motivated to maintain estrangement from the offender, and (c) increasingly motivated by conciliation and goodwill for the offender, despite the offender’s hurtful actions”. In essence, relational partners choose constructive behaviors that show an emotional commitment and willingness to sacrifice in order to achieve a state of forgiveness.\n\nThe link between reconciliation and forgiveness involves exploring two dimensions of forgiveness: intrapsychic and interpersonal. The intrapsychic dimension relates to the cognitive processes and interpretations associated with a transgression (i.e. internal state), whereas interpersonal forgiveness is the interaction between relational partners. \"Total forgiveness\" is defined as including both the intrapsychic and interpersonal components which brings about a return to the conditions prior to the transgression. To only change one’s internal state is \"silent forgiveness\", and only having interpersonal interaction is considered \"hollow forgiveness\".\n\nHowever, some scholars contend that these two dimensions (intrapsychic and interpersonal) are independent as the complexities associated with forgiveness involve gradations of both dimensions. For example, a partner may not relinquish negative emotions yet choose to remain in the relationship because of other factors (e.g., children, financial concerns, etc.). Conversely, one may grant forgiveness and release all negative emotions directed toward their partner, and still exit the relationship because trust cannot be restored. Given this complexity, research has explored whether the transformation of negative emotions to positive emotions eliminates negative affect associated with a given offense. The conclusions drawn from this research suggest that no correlation exists between forgiveness and unforgiveness. Put simply, while forgiveness may be granted for a given transgression, the negative affect may not be reduced a corresponding amount.\n\nMcCullough et al. (1998) outlined predictors of forgiveness into four broad categories \n\nWhile personality variables and characteristics of the relationship are preexisting to the occurrence of forgiveness, nature of the offense and social-cognitive determinants become apparent at the time of the transgression.\n\nForgivingness is defined as one’s general tendency to forgive transgressions. However, this tendency differs from forgiveness which is a response associated with a specific transgression. Listed below are characteristics of the forgiving personality as described by Emmons (2000).\n\nIn terms of personality traits, agreeableness and neuroticism (i.e., instability, anxiousness, aggression) show consistency in predicting forgivingness and forgiveness. Since forgiveness requires one to discard any desire for revenge, a vengeful personality tends to not offer forgiveness and may continue to harbor feelings of vengeance long after the transgression occurred.\n\nResearch has shown that agreeableness is inversely correlated with motivations for revenge and avoidance, as well as positively correlated with benevolence. As such, one who demonstrates the personality trait of agreeableness is prone to forgiveness as well as has a general disposition of forgivingness. Conversely, neuroticism was positively correlated with avoidance and vengefulness, but negatively correlated with benevolence. Consequently, a neurotic personality is less apt to forgive or to have a disposition of forgivingness.\n\nThough the personality traits of the offended have a predictive value of forgiveness, the personality of the offender also has an effect on whether forgiveness is offered. Offenders who show sincerity when seeking forgiveness and are persuasive in downplaying the impact of the transgression will have a positive effect on whether the offended will offer forgiveness. \n\nNarcissistic personalities, for example, may be categorized as persuasive transgressors. This is driven by the narcissist to downplay their transgressions, seeing themselves as perfect and seeking to save face at all costs. Such a dynamic suggests that personality determinants of forgiveness may involve not only the personality of the offended, but also that of the offender.\n\nThe quality of a relationship between offended and offending partners can affect whether forgiveness is both sought and given. In essence, the more invested one is in a relationship, the more prone they are to minimize the hurt associated with transgressions and seek reconciliation. \n\nMcCullough et al. (1998) provides seven reasons behind why those in relationships will seek to forgive:\n\nRelationship maintenance activities are a critical component to maintaining high quality relationships. While being heavily invested tends to lead to forgiveness, one may be in a skewed relationship where the partner who is heavily invested is actually under benefitted. This leads to an over benefitted partner who is likely to take the relationship for granted and will not be as prone to exhibit relationship repair behaviors. As such, being mindful of the quality of a relationship will best position partners to address transgressions through a stronger willingness to forgive and seek to normalize the relationship.\n\nAnother relationship factor that affects forgiveness is history of past conflict. If past conflicts ended badly (i.e., reconciliation/forgiveness was either not achieved or achieved after much conflict), partners will be less prone to seek out or offer forgiveness. As noted earlier, maintaining a balanced relationship (i.e. no partner over/under benefitted) has a positive effect on relationship quality and tendency to forgive. In that same vein, partners are more likely to offer forgiveness if their partners had recently forgiven them for a transgression. However, if a transgression is repeated resentment begins to build which has an adverse effect on the offended partner’s desire to offer forgiveness.\n\nThe most notable feature of a transgression to have an effect on forgiveness is the seriousness of the offense. Some transgressions are perceived as being so serious that they are considered unforgivable. To counter the negative affect associated with a severe transgression, the offender may engage in repair strategies to lessen the perceived hurt of the transgression. The offender’s communication immediately following a transgression has the greatest predictive value on whether forgiveness will be granted. \n\nConsequently, offenders who immediately apologize, take responsibility and show remorse have the greatest chance of obtaining forgiveness from their partner. Further, self-disclosure of a transgression yields much greater results than if a partner is informed of the transgression through a third party. By taking responsibility for one’s actions and being forthright through self-disclosure of an offense, partners may actually form closer bonds from the reconciliation associated with a serious transgression. As noted in the section on personality, repeated transgressions cause these relationship repair strategies to have a more muted effect as resentment begins to build and trust erodes.\n\nAttributions of responsibility for a given transgression may have an adverse effect on forgiveness. Specifically, if a transgression is viewed as intentional or malicious, the offended partner is less likely to feel empathy and forgive. Based on the notion that forgiveness is driven primarily by empathy, the offender must accept responsibility and seek forgiveness immediately following the transgression, as apologies have shown to elicit empathy from the offended partner. The resulting feelings of empathy elicited in the offended partner may cause them to better relate to the guilt and loneliness their partner may feel as a result of the transgression. In this state of mind, the offended partner is more likely to seek to normalize the relationship through granting forgiveness and restoring closeness with their partner.\n\nPrior sections offered definitions of forgiveness along with determinants of forgiveness from the perspective of the partner who has experienced the hurtful transgression. As noted earlier, swift apologies and utilization of repair strategies by the offender have the greatest likelihood of eliciting empathy from the offended and ultimately receiving forgiveness for the transgression. The sections below address remedial strategies offenders may use to facilitate a state in which the offended more likely to offer forgiveness and seek to normalize the relationship.\n\nMost common of the remedial strategies, an apology is the most straightforward means by which to admit responsibility, express regret, and seek forgiveness. Noted earlier, apologies are most effective if provided in a timely manner and involve a self-disclosure. Apologies occurring after discovery of a transgression by a third party are much less effective. Though apologies can range from a simple, “I’m sorry” to more elaborate forms, offenders are most successful when offering more complex apologies to match the seriousness of the transgression.\n\nRather than accepting responsibility for a transgression through the form of an apology, a transgressor who explains why they engaged in a behavior is engaging in excuses or justifications. While excuses and justifications aim to minimize blame on the transgressor, the two address blame minimization from completely opposite perspectives. Excuses attempt to minimize blame by focusing on a transgressor’s inability to control their actions (e.g., “How would I have known my exgirlfriend was going to be at the party.”) or displace blame on a third party (e.g., “I went to lunch with my exgirlfriend because I did not want to hurt her feelings.”) Conversely, a justification minimizes blame by suggesting that actions surrounding the transgression were justified or that the transgression was not severe. For example, a transgressor may justify having lunch with a past romantic interest, suggesting to their current partner that the lunch meeting was of no major consequence (e.g., “We are just friends.”)\n\nRefusals are where a transgressor claims no blame for the perceived transgression. This is a departure from apologies and excuses/justifications which involve varying degrees of blame acceptance. In the case of a refusal, the transgressor believes that they have not done anything wrong. Such a situation points out the complexity of relational transgressions. Perception of both partners must be taken into account when recognizing and addressing transgressions. For example, Bob and Sally have just started to date, but have not addressed whether they are mutually exclusive. When Bob finds out that Sally has been on a date with someone else, he confronts Sally. Sally may engage in refusal of blame because Bob and Sally had not explicitly noted whether they were mutually exclusive. The problem with these situations is that the transgressor shows no sensitivity to the offended. As such, the offended is less apt to exhibit empathy which is key towards forgiveness. As such, research has shown that refusals tend to aggravate situations, rather than serve as a meaningful repair strategy.\n\nAppeasement is used to offset hurtful behavior through the transgressor ingratiating themselves in ways such as promising never to commit the hurtful act or being overly kind to their partner. Appeasement may elicit greater empathy from the offended, through soothing strategies exhibited by the transgressor (e.g., complimenting, being more attentive, spending greater time together). However, the danger of appeasement is the risk that the actions of transgressor will be viewed as being artificial. For example, sending your partner flowers every day resulting from an infidelity you have committed, may be viewed as downplaying the severity of the transgression if the sending of flowers is not coupled with other soothing strategies that cause greater immediacy.\n\nAvoidance involves the transgressor making conscious efforts to ignore the transgression (also referred to as “silence”). Avoidance can be effective after an apology is sought and forgiveness is granted (i.e., minimizing discussion around unpleasant subjects once closure has been obtained). However, total avoidance of a transgression where the hurt of the offended is not recognized and forgiveness is not granted can result in further problems in the future. As relational transgressions tend to develop the nature of the relationship through drawing of new rules/boundaries, avoidance of a transgression does not allow for this development. Not surprisingly, avoidance is ineffective as a repair strategy, particularly for instances in which infidelity has occurred.\n\nRelationship talk is a remediation strategy that focuses on discussing the transgression in the context of the relationship. Aune et al. (1998) identified two types of relationship talk, relationship invocation and metatalk. Relationship invocation involves using the relationship as a backdrop for a discussion of the transgression. For example, “We are too committed to this relationship to let it fail.”, or “Our relationship is so much better than any of my previous relationships.” Metatalk involves discussing the effect of the transgression on the relationship. For example, infidelity may cause partners to redefine rules of the relationship and reexamine the expectations of commitment each partner expects from the other.\n\nRelational transgressions are a part of any relationship. In each instance, partners must weigh the severity of the transgression against how much they value the relationship. In some cases, trust can be so severely damaged that repair strategies are fruitless. With each transgression both transgressor and victim assume risks. The transgressor’s efforts at reconciliation may be rejected by the victim, which results in loss of face and potentially an avenue of attack by the victim. If the victim offers forgiveness, there is risk that the transgressor may view the forgiveness as a personality trait that may prompt future transgressions (e.g., “I’ll be forgiven by my partner just like every other time”).\n\nThese risks aside, promptly engaging in repair strategies helps to ensure the relationship recovers from transgressions. Addressing relational transgressions can be a very painful process. Utilizing repair strategies can have a transformative effect on the relationship through redefining rules and boundaries. An added benefit can be gained through the closeness that can be realized as partners address transgressions. Engaging in relationship talk such as metatalk prompts broader discussions about what each partner desires from the relationship and aligns expectations. Such efforts can mitigate the effects of future transgressions, or even minimize the frequency and severity of transgressions.\n\n\n"}
{"id": "51498214", "url": "https://en.wikipedia.org/wiki?curid=51498214", "title": "Sandra Cabrera", "text": "Sandra Cabrera\n\nSandra Cabrera (27 October 1970 – 27 January 2004) was an Argentine street-based sex worker, trade unionist, and campaigner for sex worker rights. She was killed in 2004 when her anti-corruption efforts started being successful and threatened police corruption networks and her defense of street workers' rights threatened brothel owners.\n\nCabrera was born in San Juan, Argentina\nin 1970. She moved to Rosario in 1994, leaving two children with her mother. People who met her remember her eyes, dark enough to seem black, a wealth of black hair, and her direct gaze, sometimes interpreted as inquisitive, \nsometimes as defiant.\nAt the time of her murder, she had been planning to travel to another city with a friend to attend a rock festival.\nHer first contact with AMMAR (Association of Women Prostitutes of Argentina) occurred in 2000, but she didn't begin working with the union until 2001.\n\nDuring Argentina's economic crisis in 2001, Cabrera discovered the power of union association. She complained publicly that the lack of cash had left the women on the street desperately poor, to the point of not knowing if they would have food for their families on Christmas. The Ministry of Social Advancement gave the union sacks of food that Cabrera distributed, working from 8 in the morning to 10 at night.\n\nCabrera seems to have been a devoted mother to the daughter she called \"Maca\". She encouraged her education and ensured that she learned computer skills. She took her to the theater. At the time of the murder, Macarena was away at a scouting camp in Mendoza.\n\nCabrera's work to stop the spread of AIDS and other STDs began in October, 2001.\n\nLacking resources for its own place, AMMAR's Rosario chapter maintained an office with the Government Workers Association, where the AMMAR women were known affectionately as \"the Sanjua gang\", after Cabrera's place of origin. Cabrera reportedly had a love/hate relationship with AMMAR's national leadership. Her interactions with Elena Reynaga, Secretary General of the union at the time, have been described as \"explosive\". Reynaga acknowledged that no one she worked with fought her as Cabrera did, \nbut she also acknowledged Cabrera's effectiveness:\n\nOrganizing gave her a position in a social network that she couldn't get any other way.\n\nAnd in Rosario, she found that recognition.\n\nShe was the consummate union organizer. For two years before her murder, she used her motorcycle to take her every place in Rosario where her colleagues were working the street, listening to their complaints and planning how to use the union to protect them. After she died this was how many people remembered her; on her bike, distributing condoms, asking about people's problems. She learned how to make statements to the news media that the media would pick up and publish, spreading her message.\n\nPerhaps Cabrera's hardest task was just persuading her colleagues that a union was possible. Claudia Lucero, her close friend, sometime dancing partner, godmother of her daughter, and eventual successor as secretary general of the Rosario chapter of AMMAR, had to be won over.\n\nOther street workers had other reasons for not joining. Stella Maris Longoni found it easier just to pay her police extortionist fifty pesos a week and be left in peace, until the day she paid and the police arrested her anyway. Released from jail, with police warnings not to join the union still ringing in her ears, she went straight to Cabrera and AMMAR.\n\nDuring Sandra Cabrera's murder investigation, a source inside the investigation told a news reporter that Cabrera had been defending herself on three fronts: the violence and risk of working on the street, the assaults arranged by the brothel businesses she and her independent street colleagues were competing with, and the harassment and threats from the police whose graft networks she was disrupting. The corrupt connection between the brothel owners and the police meant that Cabrera's struggle to protect street workers was necessarily a struggle against corruption.\n\nBy way of background, the journalist Carlos del Frade has described a meeting in 1991 between Rodolfo Enrique Riegé, Secretary of Public Security for the province of Santa Fe, and Atilio Bléfari, head of police in Rosario. Riegé informed Bléfari that if he didn't increase the graft revenues that he was paying to Riegé, Riegé would relieve him of command and force him to retire. The resulting struggle for control of the city of Rosario was not a struggle between justice and corruption, but a struggle between a corrupt police officer and his even more corrupt superior. The document that del Frade cites as a source goes on to list the sources of extralegal income the police were collecting, which included theft of goods during transport, bank robbery, car theft, gambling, exploitation of juveniles in nightclubs and discos, and medical quackery, among other things. Category number two was income from the Public Morality units of the police, and included\n\nSince the nightclubs and discos were fronts for brothels, this can be summarized as drugs and prostitution.\n\nAs part of her campaign for sex worker rights, Cabrera fought for the elimination of the articles of the provincial Misdemeanor Code that criminalized prostitution. Different officers in Rosario's Public Morality unit were able to use the Code to play different sides. Some officers were paid by the brothel owners to enforce the Code against the street workers who were competing with the brothels. Other officers collected bribes from street workers for not enforcing the Code.\n\nAnother complication was her romantic involvement with an officer of the Federal police, Diego Parvlucyk. She acted as an informant and he gave her drugs that had been seized during police operations, which she sold. Initially her connection with the Federal police seemed to give her some protection from the provincial police, or at least a feeling of protection, but the relationship with Parvlucyk became more difficult and he was later arrested for her murder, although released and never tried. For many of Argentina's sex workers, this relationship didn't seem unusual.\n\nFrom 1999 to 2002, there is record of ten formal complaints made by Cabrera against the police. Most of them involve harassment or threats against her or a colleague.\n\nOn March 4, 2003, Marcela Patricia Morelli, a street sex worker and member of AMMAR, filed a complaint against officers of the Ludueña neighborhood police station. She and two trans sex workers had been detained by the officers in what she felt was an excessive manner and with \"a humiliating treatment\". One of the trans sex workers was released after two hours, but Morelli and the other remained for more than six hours. The police removed money and clothing from the other trans sex worker, humiliated the person, and left the person lying on the floor, naked. In the morning the station chief arrived and told them that he didn't want \"male or female prostitutes\" in his jurisdiction, and threatened more violent measures. He threatened to drag Morelli by her hair if he saw her again.\n\nAs leader of AMMAR's Rosario chapter, Cabrera made the following statements.\n\nOn September 10, 2003, Cabrera organized a formal complaint against the chief and deputy chief of the Public Morality unit of the police, accusing them of harassing sex workers at the stops near the Rosario Bus Terminal in order to protect brothels in the area from competition from independent street workers. An investigation turned up evidence that they were providing protection for a brothel whose women included underage girls and women trafficked from the Dominican Republic. The complaint also accused police officers of forcing street-based sex workers to pay bribes in order to work. The chief and deputy chief were removed, and the new head of the Public Morality section was a woman, nominally chosen to ensure that the corrupt practices ended, but perhaps in reality chosen to take the pressure off without actually changing anything. Not quite five months later, when Cabrera was murdered, the disgraced ex-chief of Public Morality had been reassigned as head of the Zavalla police station.\nFollowing the complaint, some of the bars the two police chiefs had been protecting closed, and a few others stopped making payments to the police. One of the sex workers who joined Cabrera in the complaint testified during the murder investigation that Cabrera had expressed fear that the police would retaliate. \n\nOn October 9, someone called the headquarters of the Government Workers Association, where AMMAR kept an office, and said \"Tell Sandra that the girl will die before tomorrow.\" The girl was Cabrera's eight year old daughter, Macarena. From then until shortly before Cabrera's murder, the police Personal Security unit kept a nightly guard on her home.\n\nA few days later, an anonymous complaint filed in the juvenile courts claimed that \"Sandra sent the girl to beg and did not go to school.\" A social worker investigated and confirmed that Macarena had, in fact, been going to school.\n\nOn October 17 a sex worker was assaulted by a cyclist who struck her strongly in the head with a chain. The women went to the police and together they searched the area, but her assailant was never found.\n\nAt some point, while the police guard was temporarily away from Cabrera's home, someone got in and beat her. At another point, two men got into her home, put a gun to the head of her dog, and told her to stop fucking with them.\n\nThe last complaint Cabrera helped file, four days before her murder, involved an officer who charged a woman fifty pesos a week for the privilege of working without trouble from the police. In spite of punctual payments, the woman was arrested by the Public Morality police, who informed her that the officer she was paying was no longer working in that division. When she announced that she was going to join AMMAR, the Morality police threatened her with daily arrests. Cabrera went with her to file an extortion complaint and was with her when she spoke to the news media. Cabrera told a reporter that she was afraid that someone from the Public Morality unit would come back for her.\n\nDuring her murder investigation, the investigating judge found evidence of fifteen death threats against her.\n\nTestimony indicates that Sandra Cabrera was standing at her usual corner with a colleague at around 4:30 on the morning of 27 January 2004. She was approached by a client and walked toward him, away from her colleague. The colleague never saw the man's face and didn't recognize him. From behind, he appeared to be tall and thin, in light shorts. Cabrera and the client left together to go to her home.\n\nShe was found dead from a shot in the back of the neck. The investigation determined that the muzzle of the gun had been pushed against her neck and the gun was partially supported by her skull when it was fired.\nHer body was two blocks from the Omnibus Terminal, one block from the stop where she worked. \nThere were no scuff marks on her sandals or clothes, indicating that she had not been dragged, but had been killed where she was found. The bullet that killed her has been sometimes reported to be a 9 mm, but news reports at the time of the investigation said the investigators found a .32 caliber slug. (Various police departments in Argentina have issued 9 mm handguns to their officers (Bersa Thunder 9), but the .32 caliber bullet is considered too small for police work.)\n\nHer pants were pulled down below her hips but her underpants were in their normal position. A condom wrapper was found next to her body.\nThe police lab found semen in her vaginal and anal cavities. Testimony indicated that she had had sex with her lover a couple of hours before she was last seen alive.\n\nAnalysis by forensic experts suggests a planned assassination, not a spur-of-the-moment crime of passion. Her killer was probably either familiar with the area, or had cased it ahead of time, and led her to the entry of a private house where they were less likely to be observed. The murder weapon was a small caliber handgun that was easier to hide than the larger handguns issued by police departments. The killer apparently knew how to use a smaller weapon to ensure that Cabrera was killed by one shot, without the need for a second. The forensic analysts believe that he stood behind her, held her head with his left hand, forced the muzzle of the gun against her neck, and fired while standing with his knees slightly flexed. She died instantly.\n\nThe forensic report concluded:\n\nHer murder was a shock to the national political system. Almost immediately the national Minister of Justice got in touch with the Santa Fe governor, and the Secretary for National Human Rights came to Rosario to get involved in the case. Within two days the investigating judge had convened a multi-disciplinary meeting at the Institute of Forensic Medicine with ten forensic experts; an extraordinary step in any murder case, much less a case involving a street-based sex worker.\nThe investigating judge laid out three lines of investigation: she was killed by a client, she was killed by the brothel owners, or she was killed by the police. At the time, there were no reports of clients committing crimes against prostitutes. Sources in the police department reported that the police were seriously looking into the possibility that she had been killed by someone hired by the brothel owners. Her first encounter with AMMAR occurred when she complained to the media about being assaulted by pimps and bouncers, i.e. by men associated with a bar that was a front for a brothel. The two chiefs of the Public Morality unit who were removed after her complaint were being paid by brothel owners to drive off street sex workers. Her struggle against the brothel owners went back many years.\n\nEveryone else assumed it was the police. The police in Argentina have a history of killing reformers and killing sex workers. A few years earlier, investigators in Mar del Plata had uncovered a police conspiracy to use a fictional serial killer to cover up the murder of thirty two sex workers,\n\nkilled because they wanted to leave their pimps, who were paying the police. \nThe Santa Fe police killed the social activist Claudio Lepratti during the disturbances of 2001 when he tried to warn them off from firing at a school where children were eating lunch. At Cabrera's funeral procession, the AMMAR members shouted \"We know, we know, that our colleague was killed by the police.\"\n\nAmong political insiders and unionists there were two theories about the police. The first was that they killed Cabrera to silence her. The second was that they killed her to warn the new provincial government in Santa Fe to stop interfering with the graft. Changes of administrations in Santa Fe had typically been accompanied by some sort of organized criminal action by the police, just to remind everybody of the rules of the game. Carlos del Frade relates that during Obeid's first administration as governor of Santa Fe, he removed a half dozen police officers who had been involved in state sponsored torture and terrorism. The police responded with a message that if he removed any more police officers, it would break the institutional peace in the province.\n\nThe previous administration had responded to Cabrera's complaints by removing high ranking officers for corruption, and the new administration was arresting street level officers for extortion. This second theory implies that Cabrera's killers just wanted to warn off the government from the dangerous experiment of punishing police officers for corruption.\n\nTo show its resolve in moving the case forward, the provincial government quickly appointed a committee of five police officers to investigate. The committee was under the direct orders of José Manuel Maldonado, head of Regional Unit II of the provincial police; in effect, chief of police for the city of Rosario. Previously, as head of the Judicial Police, he had been accused of shielding officers involved with the killing of Claudio Lepratti and protecting 1st Precinct policemen accused of raping a sixteen year old girl. According to the assistant secretary of the CTA (the union federation AMMAR belongs to), Maldonado ensured that proceeds from the payments extorted from sex workers and the money that pimps paid for protection were distributed to officers and politicians appropriately. The unit he currently headed had been linked three years previously to extra-judicial executions and deaths in custody. So he was head of a police unit accused of exactly the type of assassination people were accusing the police of in the Cabrera case, he was believed to be responsible for administering the graft that she had been campaigning against, and he appeared to be experienced in covering up police crimes. The CTA and the Rosario association of prosecutors demanded his removal.\n\nAfter several months of investigation, the investigating judge ordered the detention of Parvlucyk, the Federal police officer Cabrera had been involved with, and charged him with murder. In addition to their troubled relationship, Parvlucyk had stated in his testimony that his immediate superior had asked him to silence Cabrera, and his superior has stated that Parvlucyk always complied with orders.\nOn appeal, the judge's decision was overturned, and neither Parvlucyk nor anyone else has been tried for Cabrera's murder. The panel of judges that overturned the decision of the investigating judge referred to some of the witnesses as \"\"people with street activities who pass their mornings with a wandering itinerary\". One of the appeals judges discredited the testimony of over twenty street-based sex workers because they were women who \"have an activity that can not be described as good morals,\" according to the journalist Carlos Del Frade. He claims that the possibility that Parvlucyk killed Cabrera because he was ordered to silence her was never properly investigated. Del Frade also claims that the link between Cabrera's murder and the structural corruption in the provincial and Federal police organizations was never investigated.\n\nAmong the members of AMMAR's Rosario chapter, the reaction to Cabrera's murder was a mixture of fear and bravado. \"If AMMAR dies, we fatten the pigs,\" they told each other, and \"She paid for everything, and that's why the fight continues.\" But when Cabrera's friend Claudia Lucero took over as general secretary of the chapter, she found it sometimes difficult to reach out to her colleagues. They warned her, \"Don't get involved, you'll end up like Sandra.\" A lawyer who worked for CTA, the trade union federation that AMMAR is part of, and who represented AMMAR during the murder investigation, said that some of the women who might have testified refused to get involved in the case, and others left Santa Fe province out of fear. Four years later, Lucero reported similar fear among the street workers.\n\nAs for Macarena Cabrera, Sandra's daughter, one can only imagine her reaction to it all. Growing up she had access to the Internet and knew about the failure of the murder investigation and the various theories people proposed. Nine years after the murder she went to the Rosario branch of the Government Workers Association, the union that had provided AMMAR Rosario with office space and where her mother had been well known, and told them she really wanted to know what happened to her mother. They were probably able to give her a lot of information, but the source for this says that actual explanations were in short supply.\n\nIf the police were, in fact, trying to send a message to the new provincial administration, it backfired. Cabrera's murder gave the government the political cover it needed to eliminate the Public Morality section of the provincial police. High on the list of her legacies is the fact that two days after her murder, the Santa Fe Minister of Government announced that the Public Morality section would be dissolved and the officers that served in it would be distributed among different units. He said that the move had been under consideration since the beginning of the new administration:\n\nBut he acknowledged that he was making the announcement in the context of Cabrera's murder.\nOn the other hand, the government wanted to amend, rather than repeal, the articles of the Misdemeanor Code that criminalized sex work and transgenderism. The political opposition favored straight repeal, but the government was concerned about voter reaction and opposition from the Church. As the Minister of Government put it with Victorian sensibility:\n\nAs the new secretary general of AMMAR's Rosario chapter, Claudia Lucero continued the campaign to repeal the articles.\n\nFor AMMAR Rosario, the campaign was an important rallying point. The constant police harassment intimidated the women in the street, but it also gave them strong motivation to take steps, however risky, to join the campaign against laws that legitimized the harassment. And since the campaign had been Cabrera's initiative, it gave the union frequent opportunity to invoke the name of Sandra Cabrera, who had become an important symbol for sex workers' rights.\n\nFinally, in 2010, after years of lobbying and activism by AMMAR Rosario, a different administration repealed the articles of the law that Cabrera had campaigned against.\nWhile she didn't live to see the victory, she was part of the fight, and it was a more fundamental restructuring of society's relationship with sex work than eliminating a police unit.\n\nWalter Miranda, who had been removed as chief of the Public Morality unit following Cabrera's complaint because he had been providing protection to brothels that trafficked underage girls and foreign women, as well as extorting bribes from street-based sex workers, continued to advance through the ranks. By 2012 he had become head of Regional Unit II, which gave him control of all provincial police forces in the city of Rosario. The brothels in the area around the bus terminal that he had been protecting were still there. During an investigation of trafficking in humans, a phone tap of one of the brothel owners recorded the owner asking how to go about selling cocaine. The answer was that he should bribe Hugo Tognoli, the highest ranking police officer in the province of Santa Fe. Laura Cosido, a judge in the Federal Chamber of Rosario, stated that \"There was never such an open relationship between narcos and police.\" AMMAR, Cabrera's union, complained that the drug and human trafficking networks were the same.\nAround the same time, Claudia Lucero was complaining that in spite of the revocation of the articles of the Misdemeanor Code that criminalized street prostitution, the police were still finding ways to extort money from street workers.\n\nShortly thereafter, sex work in Rosario went through the same change in law and enforcement that the rest of Argentina experienced. The anti-trafficking law, passed a few years earlier, began to be enforced consistently, ending the legal regulation of brothels by municipalities. One of the last cabarets in Rosario was closed when city inspectors found an entertainer performing oral sex on a customer. Announcing that they were closing the place, the inspectors were attacked by the madame and the female employees, who threw glasses and punched and kicked the inspectors. Driven into the street, they took refuge in a local bar where the women found them and again attacked, throwing and breaking tables and stealing the tape used to officially seal a closed business. Not everyone in Argentina embraced the new anti-trafficking regime.\n\nSome of the women who had worked in the cabarets and \"whiskerías\" that fronted for brothels moved to private apartments operated by the same type of people who had operated the brothels.\nThe motels that once rented to independent sex workers now turn them away in order to avoid problems with the city. In response, some of the women have organized \"casitas\", where a group of women rent a house and set aside a part of the earnings from each client session to cover the rent. Basically, women who were previously working in brothels are still working in brothels, and women who were previously independent are still independent. Georgina Orellano, secretary general of AMMAR as of 2017, says of the new legal regime that \"It took all the activity to a much darker and hidden place.\"\n\nFor the street based workers, Orellano says that since Mauricio Macri became Argentina's president, institutional violence has increased in public spaces.\n\nOrellano says that the new prohibitionist regime recognizes only victims and pimps, and criminalizes women who enter sex work voluntarily. The current municipal law on sex work in Rosario, passed in 2013, mentions assistance and support; Orellano says that the city department responsible for implementing the assistance has helped so few women that they can be counted on the fingers of the two hands. AMMAR's last survey found one hundred women working the streets in Rosario, and it is estimated that another six hundred are working indoors.\n\nThe legal and police system that Sandra Cabrera struggled with has changed. Where it once tolerated brothels it has become prohibitionist. The laws that once referred to street based sex work as \"scandalous\" and an \"offense to modesty\" now speak of assisting sex workers. The owners of regulated brothels who once assaulted independent sex workers in the streets, or paid the police to do the same, now operate illegally in private apartments, driven underground by the prohibitionist movement that currently opposes AMMAR in the media and the legislatures. The political projects of Cabrera and AMMAR's founders have been challenged by new social forces with different concepts of sex worker rights and agency. And yet street based sex workers face the same problems, including police violence and extortion. Perhaps Cabrera's most important legacy is illustrated by a little story told by Claudia Lucero a year after Cabrera's death:\n\nNine days after Cabrera's murder, marches were held in Rosario, Buenos Aires, and Salta province, demanding justice and sex workers rights. The counts of participants weren't given for the marches, but the Rosario march was described as \"multitudinous\" and included a cross section of Rosario civic society, including leaders of political parties, unions, and human rights groups.\nAnother march in Rosario a month later was described in the same terms and included the same civic groups.\nThese marches have been repeated over the years on the anniversary of her murder, up to the present.\n\nIn 2012 the director Lucrecia Mastrangelo released a film titled, \"Sex, dignity and death: Sandra Cabrera, unpunished crime\".\n\nIn 2017, a small public square in the area around the bus terminal where she had worked was named in her honor.\n"}
{"id": "15325852", "url": "https://en.wikipedia.org/wiki?curid=15325852", "title": "School bullying", "text": "School bullying\n\nSchool bullying is a type of bullying that occurs in any educational setting.\n\nFor an act to be considered bullying it must meet certain criteria. This includes hostile intent, imbalance of power, repetition, distress, and provocation. Bullying can have a wide spectrum of effects on a student including anger, depression, stress and suicide. Additionally, the bully can develop different social disorders or have a higher chance of engaging in criminal activity.\n\nIf there is suspicion that a child is being bullied or is a bully, there are warning signs in their behavior. There are many programs and organizations worldwide which provide bullying prevention services or information on how children can cope if they have been bullied.\n\nThere is no universal definition of school bullying; however, it is widely agreed that bullying is a subcategory of aggressive behavior characterized by the following three minimum criteria:\n\nThe following two additional criteria have been proposed to complement the above-mentioned criteria:\n\nSome of these characteristics have been disputed (e.g., for power imbalance: bullies and victims often report that conflicts occur between two equals); nevertheless, they remain widely established in the scientific literature.\n\nThe underlying causes of school violence and bullying include gender and social norms and wider contextual and structural factors.\n\nDiscriminatory gender norms that shape the dominance of men and the subservience of women and the perpetuation of these norms through violence are found in some form in many cultures. Gender inequality and the prevalence of violence against women in society exacerbate the problem. Similarly, social norms that support the authority of teachers over children may legitimise the use of violence to maintain discipline and control.\n\nThe pressure to conform to dominant gender norms is also high. Young people who cannot or who choose not to conform to these norms are often punished for this through violence and bullying at school.\n\nSchools themselves can \"teach\" children to be violent through discriminatory practices, curricula and textbooks. If unchecked, gender discrimination and power imbalances in schools can encourage attitudes and practices that subjugate children, uphold unequal gender norms and tolerate violence, including corporal punishment.\n\nSome attribute part of the cause of bullying to the atmosphere in which it occurs. Thornberg and Knutsen state in their study, \"School attributing refers to attributing the cause of bullying to the school setting.\" They say that school attributing has two subcategories which are \"boredom in school\" and \"poor antibullying practices\". Boredom in school involves a student who does not have anything else to do other than bully. Poor antibullying practices may include teachers and staff not caring enough to intervene, or a school not having enough teachers for students. This may lead to the students feeling unwanted or unimportant due to the lack of care from the school's staff.\n\nSchools and the education system also operate within the context of wider social and structural factors and may reflect and reproduce environments that do not protect children and adolescents from violence and bullying. For example, physical and sexual violence may be more prevalent in schools in contexts where it is also more prevalent in wider society. Studies suggest that sexual violence and harassment of girls is worse in schools where other forms of violence are prevalent, and in conflict and emergency contexts, and that gang violence is more common in schools where gangs, weapons and drugs are part of the local culture.\n\nIn their paper \"Predicting Bullying: Exploring the Contributions of Childhood Negative Life Experiences in Predicting Adolescent Bullying Behavior,\" Connell, Morris and Piquero identify three primary aspects of a child's life- family, school and peers- as major indicators to whether or not that child exhibits behavior akin to bullying.\n\nBullying can threaten students' physical and emotional safety at school and can negatively impact their ability to learn. The best way to address bullying is to stop it before it starts. There are many different groups that can intervene to address bullying (and cyberbullying) in schools: parents, teachers, and school leadership. The most commonly used strategies by teachers to prevent it are to communicate, mediate and seek help. Training school staff and students to prevent and address bullying can help sustain bullying prevention efforts over time. There are no federal mandates for bullying curricula or staff training. In addition to addressing bullying before it occurs, a great prevention strategy is to educate the students on bullying.\n\nExamples of activities to teach about bullying include:\n\n\nA victim, in the short term, may feel depressed, anxious, angry, have excessive stress, learned helplessness, feel as though their life has fallen apart, have a significant drop in school performance, or may commit suicide (bullycide). In the long term, they may feel insecure, lack trust, exhibit extreme sensitivity (hypervigilant), or develop a mental illness such as psychopathy, avoidant personality disorder or PTSD. They may also desire vengeance, sometimes leading them to torment others in return.\n\nAnxiety, depression and psychosomatic symptoms are common among both bullies and their victims. Among these participants alcohol and substance abuse are commonly seen later in life. It is known that people suffering from depression feel much better when they talk to others about it, but victims of bullying fear may not talk to others about their feelings in fear of being bullied, which can worsen their depression.\n\nIn the short term, being a bystander \"can produce feelings of anger, fear, guilt, and sadness... Bystanders who witness repeated victimizations of peers can experience negative effects similar to the victimized children themselves.\"\n\nWhile most bullies, in the long term, grow up to be emotionally functional adults, many have an increased risk of developing antisocial personality disorder, which is linked to an increased risk of committing criminal acts (including domestic violence).\n\nThe educational effects on victims of school violence and bullying are significant. Violence and bullying at the hands of teachers or other students may make children and adolescents afraid to go to school and interfere with their ability to concentrate in class or participate in school activities. It can also have similar effects on bystanders.\n\nThe consequences include missing classes, avoiding school activities, playing truant or dropping out of school altogether. This in turn has an adverse impact on academic achievement and attainment and on future education and employment prospects. Children and adolescents who are victims of violence may achieve lower grades and may be less likely to anticipate going on to higher education. Analyses of international learning assessments highlight the impact of bullying on learning outcomes. These analyses clearly show that bullying reduces students' achievement in key subjects, such as mathematics, and other studies have documented the negative impact of school violence and bullying on educational performance.\n\nBystanders and the school climate as a whole are also affected by school violence and bullying. Unsafe learning environments create a climate of fear and insecurity and a perception that teachers do not have control or do not care about the students, and this reduces the quality of education for all.\n\nThe 2006 UN World Report on Violence against Children shows that victims of corporal punishment, both at school and at home, may develop into adults who are passive and over-cautious or aggressive. Involvement in school bullying can be a predictor of future antisocial and criminal behaviour. Being bullied is also linked to a heightened risk of eating disorders and social and relationship difficulties.\n\nOther studies have shown the longer-term effects of bullying at school. One study of all children born in England, Scotland and Wales during one week in 1958 analyzes data on 7,771 children who had been bullied at ages 7 and 11. At age 50, those who had been bullied as children were less likely to have obtained school qualifications and less likely to live with a spouse or partner or to have adequate social support. They also had lower scores on word memory tests designed to measure cognitive IQ even when their childhood intelligence levels were taken into account and, more often reported, that they had poor health. The effects of bullying were visible nearly four decades later, with health, social and economic consequences lasting well into adulthood. For children, \"peers are a much more important influence than has been realised. It is a terrible thing to be excluded by your peers\".\n\nThe economic impact of violence against children and adolescents is substantial. Youth violence in Brazil alone is estimated to cost nearly US$19 billion every year, of which US$943 million can be linked to violence in schools. The estimated cost to the economy in the USA of violence associated with schools is US$7.9 billion a year.\n\nAnalytic work supported by the United States Agency for International Development (USAID) shows that school-related gender-based violence alone can be associated with the loss of one primary grade of schooling, which translates to an annual cost of around US$17 billion to low- and middle-income countries.\n\nIn the East Asia and Pacific region, it is estimated that the economic costs of just some of the health consequences of child maltreatment were equivalent to between 1.4% and 2.5% of the region's annual GDP.\n\nIn Argentina, the forgone benefit to society from overall early school dropout is 11.4% of GDP, and in Egypt, nearly 7% of potential earnings is lost as a result of the number of children dropping out of school.\n\nA study has shown that each year Cameroon, Democratic Republic of Congo and Nigeria lose US$974 million, US$301 million and US$1,662 million respectively for failing to educate girls to the same standard as boys, and violence in school is one of the key factors contributing to the under-representation of girls in education.\n\nAccording to the American Psychological Association, \"40% to 80% of school-age children experience bullying at some point during their school careers.\" Various studies show that students from lower socioeconomic backgrounds experience bullying more often than other students. The following statistics help illustrate the severity of bullying within classrooms:\n\n\nStatistics referencing the prevalence of bullying in schools may be inaccurate and tend to fluctuate. In a U.S. study of 5,621 students ages 12–18, 64% of the students had experienced bullying and did not report it.\n\nProactive aggression is a behavior that expects a reward. With bullying each individual has a role to defend. Some children act proactively but will show aggression to defend themselves if provoked. These children will react aggressively but tend to never be the ones to attack first.\n\nThere have been two subtypes created in bully classification; popular aggressive and unpopular aggressive. Popular aggressive bullies are social and do not encounter a great deal of social stigma from their aggression. Unpopular aggressive bullies, however, are most often rejected by other students and use aggression to seek attention.\n\nIn a survey by the Eunice Kennedy Shriver National Institute of Child Health and Human Development (NICHD), students were asked to complete a questionnaire.\n\nA total of 10.6% of the children replied that they had sometimes bullied other children, a response category defined as moderate bullying. An additional 8.8% said they had bullied others once a week or more, defined as frequent bullying. Similarly, 8.5% said they had been targets of moderate bullying, and 8.4% said they were bullied frequently. Out of all the students, 13% said they had engaged in moderate or frequent bullying of others, while 10.6% said they had been bullied either moderately or frequently. Some students — 6.3% — had both bullied others and been bullied themselves. In all, 29% of the students who responded to the survey had been involved in some aspect of bullying, either as a bully, as the target of bullying or both.\n\nAccording to Tara Kuther, an associate professor of psychology at Western Connecticut State University, \"...bullying gets so much more sophisticated and subtle in high school. It's more relational. It becomes more difficult for teens to know when to intervene; whereas with younger kids, bullying is more physical and, therefore, more clear-cut.\"\n\nThere are four basic types of bullying: verbal, physical, psychological, and cyber. Cyberbullying is becoming one of the most common types. While victims can experience bullying at any age, it is witnessed most often in school-aged children.\n\nDirect bullying is a relatively open attack on a victim that is physical and/or verbal in nature. Indirect bullying is more subtle and harder to detect, but involves one or more forms of relational aggression, including social isolation via intentional exclusion, spreading rumors to defame one's character or reputation, making faces or obscene gestures behind someone's back, and manipulating friendships or other relationships.\n\nPack bullying is bullying undertaken by a group. The 2009 Wesley Report on bullying found that pack bullying was more prominent in high schools and lasted longer than bullying undertaken by individuals.\n\nPhysical bullying is any unwanted physical contact between the bully and the victim. This is one of the most easily identifiable forms of bullying. Examples include:\n\nEmotional bullying is any form of bullying that causes damage to a victim's psyche and/or emotional well-being. Examples include:\n\n\nVerbal bullying is any slanderous statements or accusations that cause the victim undue emotional distress. Examples include:\n\n\nCyberbullying is the quickest growing form of harassment of school campuses in the U.S. and 40% of adolescent report being a victim. Most definitions of cyberbullying come from definitions of school bullying. Thus, this conduct is often described as an intentional aggressive behavior that takes place via new technologies, during which groups or individuals hurt classmates who cannot easily defend themselves. Cyberbullying events can occur via cellphones or computers, by means of text messages, e-mails, online social networks, chatrooms or blogs. This form of bullying can easily go undetected because of the lack of parental or authoritative supervision. Because bullies can pose as someone else, it is the most anonymous form of bullying. Like the bullying that occurs in school, the following four profiles have been identified: cyberneutral, cyberbully, cybervictim and cyberbully-victim. Many who are bullied in school are likely to be bullied over the Internet and vice versa. Since students have become more reliant on internet, the advancement in social media and technology has altered the fear of in-person bullying away from schoolyards but has rather increase cyberbullying. Studies have shown almost half of cyberbullies are repeat offenders and harass others as few at three times. Males are more likely to be active cyberbullies than females. Cyberbullying can happen 24 hours a day and seven days a week and reach a child even when they are alone.\nDeleting inappropriate or harassing messages, texts or pictures is extremely difficult after being posted or sent.\n\nAccording to the website Stop Cyberbullying, \"When schools try and get involved by disciplining the student for cyberbullying actions that took place off campus and outside of school hours, they are often sued for exceeding their authority and violating the student's free speech right.\" They suggest for schools to make revisions to their policies that would allow for disciplinary actions to take place even if off campus or after hours. They say if the act is likely to affect a student mentally or physically while in school then the revision of the policy would allow for the staff to intervene without violating the student's constitutional rights. Many principals are hesitant to act because school discipline codes and states laws do not define cyberbullying. According to professor Bernard James, \"educators are empowered to maintain safe schools, the timidity of educators in this context of emerging technology is working in advantage of the bullies.\"\n\nCyberbullying has become extremely prevalent; 95% of teens who use social media reported having witnessed malicious behavior on social media from 2009 to 2013. As sites like Facebook or Twitter offer no routine monitoring, children from a young age must learn proper internet behavior, say Abraham Foxman and Cyndi Silverman. \"This is a call for parents and educators to teach these modern skills... through awareness and advocacy.\" Per Scott Eidler, \"Parents and educators need to make children aware at a young age of the life-changing effects cyberbullying can have on the victim. The next step for prevention is advocacy. For example, three high school students from Melville, New York organized a Bullying Awareness Walk, where several hundred people turned out to show their support.\"\n\nClara Wajngurt writes, \"Other than organizing events, calling for social media sites to take charge could make the difference between life and death. Cyberbullying is making it increasingly difficult to enforce any form of prevention.\" Joanna Wojcik concludes, \"The rapid growth of social media is aiding the spread of cyberbullying, and prevention policies are struggling to keep up. In order for prevention policies to be put in place, the definition of cyberbullying must be stated, others must be educated on how to recognize and prevent bullying, and policies that have already attempted to be enacted need to be reviewed and learned from.\"\n\nResearcher Charisse Nixon found that students do not reach out for help with cyberbullying for four main reasons: they do not feel connected to the adults around them; the students do not see the cyberbullying as an issue that is worth bringing forward; they do not feel the surrounding adults have the ability to properly deal with the cyberbullying; and the teenagers have increased feelings of shame and humiliation regarding the cyberbullying. Nixon also found that when bystanders took action in helping end the cyberbullying in adolescents, the results were more positive than when the adolescents attempted to resolve the situation without outside help.\n\nSexual bullying is \"any bullying behavior, whether physical or non-physical, that is based on a person's sexuality or gender. It is when sexuality or gender is used as a weapon by boys or girls towards other boys or girls—although it is more commonly directed at girls. It can be carried out to a person's face, behind their back or through the use of technology.\"\n\nAs part of its research into sexual bullying in schools, the BBC TV series \"Panorama\" commissioned a questionnaire aimed at people aged 11 to 19 in schools and youth clubs across five regions of England. The survey revealed that of the 273 respondents, 28 had been forced to do something sexual, and 31 had seen it happen to someone else. Of the 273 respondents, 40 had experienced unwanted touching. U.K. government figures show that in the 2007–2008 school year, there were 3,450 fixed-period exclusions and 120 expulsions from schools in England due to sexual misconduct. This included incidents such as groping and using sexually insulting language. From April 2008 to March 2009, ChildLine counselled a total of 156,729 children, 26,134 of whom spoke about bullying as a main concern and 300 of whom spoke specifically about sexual bullying.\n\nThe U.K. charity Beatbullying has claimed that as gang culture enters, children are being bullied into providing sexual favours in exchange for protection. However, other anti-bullying groups and teachers' unions, including the National Union of Teachers, challenged the charity to provide evidence of this.\n\nSexting cases are also on the rise and have become a major source of bullying. The circulation of explicit photos of those involved either around school or the internet put the originators in a position to be scorned and bullied. There have been reports of some cases in which the bullying has been so extensive that the victim has taken their life.\n\nAccording to HealthDay News, 15 percent of college students claim to have been victims of bullying while at college. In the article, \"Bullying not a thing of the past for college students,\" Kaitlyn Krasselt writes, \"Bullying comes in all forms but is usually thought of as a K-12 issue that ceases to exist once students head off to college.\" The misconception that bullying does not occur in higher education began to receive attention after the death of college student Tyler Clementi. According to an experiment conducted by Dr. Gary R. Walz, \"21.47% of participants reported rarely being victims of cyberbullying; 93.29% reported rarely cyberbullying others. Overall, there was a low prevalence rate for cyberbullying.\"\n\nBullying is usually associated with an imbalance of power. A bully has a perceived authority over another due to factors such as size, gender, or age. Boys tend to bully peers based on the peer's physical weakness, short temper, friend group, and clothing. Bullying among girls, on the other hand, results from factors such as facial appearance, emotional factors, being overweight, and academic status. Both sexes tend to target people with speech impediments of some sort (such as stutter).\n\nBullies often come from families that use physical forms of discipline.\n\nBullying locations vary by context. Most bullying in elementary school happens in the playground. In middle school and high school, it occurs most in the hallways, which have little supervision. According to the U.S Department of Education's National Center for Education Statistics, more than 47% of kids reported getting bullied in hallways and stairway. Bus stops and bus rides to and from school tend to be hostile environments as well; children tend to view the driver as someone with no disciplinary authority.\n\nBullying may also follows people into adult life and university. Bullying can take over the lives of both lecturers and students, and can lead to supervisors putting pressure on students. Bullying can happen in any place at any time.\n\nVictims of bullying typically are physically smaller, more sensitive, unhappy, cautious, anxious, quiet, and withdrawn. They are often described as passive or submissive. Possessing these qualities make these individuals vulnerable, as they are seen as being less likely to retaliate.\n\nSigns that a child is being bullied include:\n\nSigns that a child is bullying others include:\n\nSigns that a child has witnessed bullying include:\n\nMcNamee and Mercurio state that there is a \"bullying triangle\", consisting of the person doing the bullying, the person getting bullied, and the bystander.\n\nThe US Department of Health and Human Services divides the people involved in bullying into several roles:\n\nIn her book, \"The Bully, the Bullied, and the Bystander\", Barbara Coloroso divides bullies into several types:\n\nParsons identifies school bullying cultures as typically having a web of dynamics which are much more complex than just considering bullying amongst students. These dynamics include:\n\nResearchers have identified many misconceptions regarding bullying:\n\n\nStudies have shown that bullying programs set up in schools with the help and engagements of staff and faculty have been shown to reduce peer victimization and bullying. Incidences of bullying are noticeably reduced when the students themselves disapprove of bullying.\n\nMeasures such as increasing awareness, instituting zero tolerance for fighting, or placing troubled students in the same group or classroom are actually ineffective in reducing bullying; methods that \"are\" effective include increasing empathy for victims; adopting a program that includes teachers, students, and parents; and having students lead anti-bullying efforts. Success is most associated with beginning interventions at an early age, constantly evaluating programs for effectiveness, and having some students simply take online classes to avoid bullies at school.\n\nSection 89 of the Education and Inspections Act 2006 provides for an anti-bullying policy for all state schools to be made available to parents.\n\nThe victims of some school shootings have sued both the shooters' families and the schools. At one point only 23 states had Anti-Bullying laws. In 2015 Montana became the last state to have an anti-bullying law and at that point all 50 states had an anti-bullying law. These laws are not going to abolish bullying but it does bring attention to the behavior and it lets the aggressors know it will not be tolerated.\n\nIn 2016, a legal precedent was set by a mother and her son, after the son was bullied at his public school. The mother and son won a court case against the Ottawa-Carleton District School Board, making this the first case in North America where a school board has been found negligent in a bullying case for failing to meet the standard of care (the \"duty of care\" that the school board owes to its students). A similar bullying case was won in Australia in 2013 (\"Oyston v. St. Patricks College\").\n\nThe Ministry of Education launched a serial of project. In 2006, they started the 'anti-bully plan'. In 2008, they launched the 'prevent bully video from public project', and also building multiple informants route, monitoring the school, in hope that it could improve the education quality.\n\nSchool bullying is associated with school shootings; the vast majority of students (87%) believe that shootings occur in direct retaliation to bullying. School shooters who left behind evidence that they were bullied include Eric Harris and Dylan Klebold (perpetrators of the Columbine High School massacre), Nathan Ferris, Edmar Aparecido Freitas, Brian Head, Seung-Hui Cho, Wellington Menezes Oliveira, Kimveer Gill, Karl Pierson, and Jeff Weise.\n\nEvents and organizations which address bullying in schools include:\n\n"}
{"id": "276949", "url": "https://en.wikipedia.org/wiki?curid=276949", "title": "Sexual identity", "text": "Sexual identity\n\nSexual identity is how one thinks of oneself in terms of to whom one is romantically or sexually attracted. \"Sexual identity\" may also refer to sexual orientation identity, which is when people identify or dis-identify with a sexual orientation or choose not to identify with a sexual orientation. Sexual identity and sexual behavior are closely related to sexual orientation, but they are distinguished, with \"identity\" referring to an individual's conception of themselves, \"behavior\" referring to actual sexual acts performed by the individual, and \"sexual orientation\" referring to romantic or sexual attractions toward persons of the opposite sex or gender, the same sex or gender, to both sexes or more than one gender, or to no one.\n\nHistorical models of sexual identity have tended to view its formation as a process undergone only by sexual minorities, while more contemporary models view the process as far more universal and attempt to present sexual identity within the larger scope of other major identity theories and processes.\n\nSexual identity has been described as a component of an individual's identity that reflects their sexual self-concept. The integration of the respective identity components (e.g. moral, religious, ethnic, occupational) into a greater overall identity is essential to the process of developing the multi-dimensional construct of identity.\n\nSexual identity can change throughout an individual's life, and may or may not align with biological sex, sexual behavior or actual sexual orientation.<ref name=\"Concordance/discordance in SO\"></ref> For example, gay, lesbian, and bisexual people may not openly identify as such in a homophobic/heterosexist setting or in areas whose record on LGBT rights is poor. In a 1990 study by the Social Organization of Sexuality, only 16% of women and 36% of men who reported some level of same-sex attraction had a homosexual or bisexual identity.\n\nSexual identity is more closely related to sexual behavior than sexual orientation is. The same survey found that 96% of women and 87% of men with a homosexual or bisexual identity had engaged in sexual activity with someone of the same sex, contrasted to 32% of women and 43% of men who had same-sex attractions. Upon reviewing the results, the organization commented: \"Development of self-identification as homosexual or gay is a psychological and socially complex state, something which, in this society, is achieved only over time, often with considerable personal struggle and self-doubt, not to mention social discomfort.\"\n\nUnlabeled sexuality is when an individual chooses to not label their sexual identity. This identification could stem from one's uncertainty about their sexuality or their unwillingness to conform to a sexuality because they don't necessarily like labels, or they wish to feel free in their attractions instead of feeling forced into same, other, both, or pan attractions because of their sexual identity. Identifying as unlabeled could also be because of one's \"unwillingness to accept their sexual minority status.\" Because being unlabeled is the purposeful decision of no sexual identity, it is different from bisexuality or any other sexual identity. Those who are unlabeled are more likely to view sexuality as less stable and more fluid and tend to focus more on the “person, not the gender.”\nIt is reported that some women who identify as unlabeled did so because they are unable or uncertain about the types of relationships they will have in the future. As such, this divergence from sexual labels could provide for a person to be able to more fully realize their \"true\" sexuality because it frees them from the pressure of liking and being attracted to who their sexual identification dictates they should like.\n\nMost of the research on sexual orientation identity development focuses on the development of people who are attracted to the same sex. Many people who feel attracted to members of their own sex come out at some point in their lives. \"Coming out\" is described in three phases. The first phase is the phase of \"knowing oneself,\" and the realization emerges that one is sexually and emotionally attracted to members of one's own sex. This is often described as an internal coming out and can occur in childhood or at puberty, but sometimes as late as age 40 or older. The second phase involves a decision to come out to others, e.g. family, friends, and/or colleagues, while the third phase involves living openly as an LGBT person. In the United States today, people often come out during high school or college age. At this age, they may not trust or ask for help from others, especially when their orientation is not accepted in society. Sometimes they do not inform their own families.\n\nAccording to Rosario, Schrimshaw, Hunter, Braun (2006), \"the development of a lesbian, gay, or bisexual (LGB) sexual identity is a complex and often difficult process. Unlike members of other minority groups (e.g., ethnic and racial minorities), most LGB individuals are not raised in a community of similar others from whom they learn about their identity and who reinforce and support that identity\" and \"[r]ather, LGB individuals are often raised in communities that are either ignorant of or openly hostile toward homosexuality.\"\n\nSome individuals with unwanted sexual attractions may choose to actively dis-identify with a sexual minority identity, which creates a different sexual orientation identity from their actual sexual orientation. Sexual orientation identity, but not sexual orientation, can change through psychotherapy, support groups, and life events. A person who has homosexual feelings can self-identify in various ways. An individual may come to accept an LGB identity, to develop a heterosexual identity, to reject an LGB identity while choosing to identify as ex-gay, or to refrain from specifying a sexual identity. In a \"The Wall Street Journal\" article on reconciling faith and homosexuality, researcher Judith Glassgold, who chaired the task force, stated, \"We're not trying to encourage people to become \"ex-gay\"’\" and \"there has been little research on the long-term effects of rejecting a gay identity, but there is 'no clear evidence of harm' and 'some people seem to be content with that path'\".\n\nSeveral models have been created to describe coming out as a process for gay and lesbian identity development (e.g. Dank, 1971; Cass, 1984; Coleman, 1989; Troiden, 1989). These historical models have taken a view of sexual identity formation as a sexual-minority process only. However, not every LGBT person follows such a model. For example, some LGBT youth become aware of and accept their same-sex desires or gender identity at puberty in a way similar to which heterosexual teens become aware of their sexuality, i.e. free of any notion of difference, stigma or shame in terms of the gender of the people to whom they are attracted. More contemporary models take the stance that it is a more universal process. Current models for the development of sexual identity attempt to incorporate other models of identity development, such as Marcia’s ego-identity statuses.\n\nThe Cass identity model, established by Vivienne Cass, outlines six discrete stages transited by individuals who successfully come out: (1) identity confusion, (2) identity comparison, (3) identity tolerance, (4) identity acceptance, (5) identity pride, and (6) identity synthesis. Fassinger's model of gay and lesbian identity development contains four stages at the individual and group level: (1) awareness, (2) exploration, (3) deepening/commitment, and (4) internalization/synthesis.\n\nSome models of sexual identity development do not use discrete, ordered stages, but instead conceptualize identity development as consisting of independent identity processes. For example, D'Augelli's model describes six unordered independent identity processes: (1) exiting heterosexual identity, (2) Developing personal LGB identity status, (3) Developing a LGB social identity, (4) Becoming a LGB offspring, (5) Developing a LGB intimacy status, and (6) Entering a LGB community.\n\nThe Unifying Model of Sexual Identity Development is currently the only model that incorporates heterosexual identity development within its statuses to include compulsory heterosexuality, active exploration, diffusion, deepening and commitment to status, and synthesis.\n\nContemporary models view sexual identity formation as a universal process, rather than a sexual minority one, in that it is not only sexual minorities that undergo sexual identity development, but heterosexual populations as well. More recent research has supported these theories, having demonstrated that heterosexual populations display all of Marcia’s statuses within the domain of sexual identity.\n\n\n"}
{"id": "53313647", "url": "https://en.wikipedia.org/wiki?curid=53313647", "title": "Shram suvidha", "text": "Shram suvidha\n\nShram Suvidha is a Web Portal to provide a single platform for all labour compliances.\n\nIndia has more than 40 labour laws regulated by theMinistry of Labour and Employment, the Government of India and State Government. Compliance with these laws is tedious and time-consuming. As a result, employers often do not comply. The scale of noncompliance exceeds the ability of enforcement agencies to regulate. Amidst the confusion, employees do not always receive what they are entitled to under these laws. \n\nThe primary objectives of Shram Suvidha are:\n\n\nShram Suvidha allows an organization to know what labour laws apply to it.\n\nCompliances are reportable in a single form that makes it simple for those filing such forms. Performance is monitored using key indicators thus making the evaluation process objective. It promotes the use of a common Labour Identification Number (LIN) by all Implementing agencies.\n"}
{"id": "18486930", "url": "https://en.wikipedia.org/wiki?curid=18486930", "title": "Shūgi-bukuro", "text": "Shūgi-bukuro\n\nA is a special envelope in which money is given as a gift at weddings in Japan.\n\nIt is very common in Japan to give a gift of money at weddings. The giver inserts the money into a \"shūgi-bukuro\" on which they have written their name. The \"shūgi-bukuro\" is handed to the receptionist of the reception party. \"Shūgi-bukuro\" are sold at supermarkets and stationery stores.\n\nThe amount given in \"shūgi-bukuro\" differs according to the givers relationship to the couple, their social status and the style of venue. In the case of friends or company colleagues, it is usually between ¥30,000 and ¥50,000. In the case of close friends or those in a senior position at the bride or bridegroom’s company, ¥30,000 to ¥50,000 is common, and in the case of relatives, ¥50,000 to ¥100,000 is not unusual.\n\nFor married couples that attend the wedding ¥50,000 would be common, as opposed to unmarried couples where each person would give a separate amount say ¥30,000 each.\n\nIt is common to give amounts in which the leading digit(s) form an odd number, such as ¥30,000 or ¥50,000, in order to symbolize the fact that the newly married couple cannot be divided. When the leading digit forms an even number, as in ¥20,000, the amount is usually given in an odd number of bills (e.g. 1 x ¥10,000 and 2 x ¥5,000). Amounts in which the leading digit forms a multiple of 4, such as ¥40,000, are not recommended since the number 4 in Japanese can be pronounced as \"shi\" which is the same as the pronunciation of the Japanese word for death. Likewise, multiples of 9 are avoided because the pronunciation of this number can mean suffering.\n\n\n"}
{"id": "22509875", "url": "https://en.wikipedia.org/wiki?curid=22509875", "title": "Simulation algorithms for atomic DEVS", "text": "Simulation algorithms for atomic DEVS\n\nGiven an atomic DEVS model, simulation algorithms are methods to generate the model's legal behaviors which are trajectories not to reach to illegal states. (see Behavior of DEVS). [Zeigler84] originally introduced the algorithms that handle time variables related to \"lifespan\" formula_1 and \"elapsed time\" formula_2 by introducing two other time variables, \"last event time\", formula_3, and \"next event time\" formula_4 with the following relations: \nformula_5\n\nand\n\nformula_6\n\nwhere formula_7 denotes the \"current time\". And the \"remaining time\",\n\nformula_9, apparently formula_10.\n\nSince the behavior of a given atomic DEVS model can be defined in two different views depending on the total state and the external transition function (refer to Behavior of DEVS), the simulation algorithms are also introduced in two different views as below.\n\nRegardless of two different views of total states, algorithms for initialization and internal transition cases are commonly defined as below.\n\nAs addressed in Behavior of Atomic DEVS, when DEVS receives an input event, right calling formula_24, the last event time,formula_11 is set by the current time,formula_14, thus the elapsed timeformula_27 becomes zero because formula_28.\n\nNotice that as addressed in Behavior of Atomic DEVS, depending on the value of formula_36 return by formula_37, last event time,formula_11, and next event time,formula_12,consequently, elapsed time, formula_27, and lifespanformula_12, are updated (if formula_42) or preserved (if formula_43).\n\n\n"}
{"id": "52337803", "url": "https://en.wikipedia.org/wiki?curid=52337803", "title": "Spoon class theory", "text": "Spoon class theory\n\nThe spoon class theory refers to the idea that individuals in a country can be classified into different socioeconomic classes based on the assets and income level of their parents, and as a consequence, one's success in life depends entirely on being born into a wealthy family. The term appeared in 2015 and was first widely used among online communities in South Korea.\n\nThe theory is thought to originate from the well-known English idiom \"born with a silver spoon in one's mouth\". In the past, European nobility often used silver dishes, and children were fed by nannies using silver spoons, which indicated the wealth of the family. In South Korea, this idea was taken further to establish several categories to classify individuals based on their family's wealth.\n\nThe spoon classes have been identified as follows:\n\nHyo Chan Cho, the author of hyper-reality shock, explained that the meaning of gold spoon, which is common in Korea's society, \"is related to Jean Baudrillard's simulacrum\". He suggested that the gold spoon is included in a simulacrum that doesn't have an origin. Issues of gold spoon celebrities and commercials which made those people idealize changed nonexistence as existence. Regardless of the pros and cons of a gold spoon, it became an important image of their life. He further stated that as our society accepted images like gold spoon which became a hyperreality. We accept media's reproducing images that don't have originals as more than an existence. ‘Simulacrum' means an image without substance, it wields strong influence than an existence.\n\nYoung adults preparing for significant life changes such as college, marriage or employment are concerned that they are at a disadvantage. For example, many corporations in Korea require stellar academic performance and for applicants to speak English fluently. Individuals who come from the upper class can advance themselves due to the wealth and power that their parents or family possess. Young people who come from the middle and lower class are at a disadvantage because often they are expected to work and attend school, coupled with the fact that they are not being provided with the same monetary support as their wealthier peers. This economic polarization not only influences employment but also affects other things like marriage. The causes of inequality in this society are economic and the fact that poverty is passed from generation to generation. This inequality is creating new classes in Korean society. However, some people overcome their parent's low economic class. Some people who were raised in wealthy families criticize this as well. Unfairness in Korean society is becoming a burden for young adults and is making the Korean society similar to the one described in the spoon class theory.\n\nPark Jae-wan, a professor at Sungkyunkwan University analyzed spoon class theory in research on social mobility. \"The distribution of income in Korea is close to that of advanced countries, considering Gini coefficient, relative income share, income share, and relative poverty rate.\" \"The basis of 'Helos' or 'gold spoon' claims is weak\". According to the results of estimating the probability that each income group will remain in the same class for the whole household from 2011 to 12, it is 29.8 per cent for low income class, 38.2 per cent for middle class and 32.0 per cent for high income class. Park Jae-wan suggested, \"As the results of the analysis, Korea is still highly likely to move.\" However, the pace of stratification has been slowing since the financial crisis, mainly because the poverty has been fixed, particularly among the elderly. \" He cited the following five causes of Spoon class theory. 1. Youth unemployment 2. Reinforced pass down 3. Government regulation and vested interest 4. Relative tendency and relative deprivation of Koreans 5. Bad social capital.\n\nHan Jun, professor of sociology at Yeonsei University said, \"Raising the possibility of social mobility is also an important task in terms of social vitality and social integration.\" It requires policy efforts to care for the physical and mental health of low-income vulnerable children, academic aspirations, and cognitive abilities, Emphasis should be given to schools in low-income vulnerable groups and rural and rural areas. \"\n\nLee Byeong-hoon, professor of sociology at Chung-Ang University, said, \"Negative and critical perceptions of opportunistic inequality in our society seem to be influenced by factors such as hierarchy status and experience of discrimination.\" According to the analysis, the subject and parental generations have low subjective status, experience of discrimination and disadvantages, and younger and higher educated people have higher negative and critical perception of opportunity inequality. In addition, the perception of socioeconomic opportunity inequality, the severity of inequality in opportunity, and the negative perception of effort achievement were found to be higher in the order of lower income class, middle class, and higher income class.\n\nIn the 1980s, the share of assets contributed by gifts and inheritance was 27 per cent, age groups spanning 19 to 75 years old: 181 men born between 1940 and 1959, the generation of industrialization; 593 men born between 1960 and 1974, the generation of democracy; and 568 men born between 1975 and 1995, the information generation\".\"\n\nThe results of the poll found that the spoon class theory not only existed in Korea but deepened with each successive generation. The study showed that the persistence of poverty across generations has deepened, with 50.7 per cent in the youngest generation answering that both father and son were in the lower class – an increase of almost 15 per cent from the 36.4 per cent who answered the same in the democracy generation. \n\nIn the oldest generation, only 35.9 per cent answered that both father and son were considered in the lower class\".\"\n\nCEO Score Daily '(2017) analyzed the assets fluctuation of 2007 ~ 2017 among the 160 wealthy stockholders of Korea, the United States, Japan and China (for each of the top 40) announced by Forbes. As a result, There are 48 (30 per cent) rich men in the four countries, and the wealthy self-And 112 patients (70 per cent). In Korea, 25 out of 40 equity holders (62.5 per cent) were 'Gold Spoon' It is much higher than that of the US (25 per cent), Japan (12 per cent) and China (2.5 per cent)\n\n\"As a result of analyzing the percentage of social movements by generation,\" said Han Jun, a professor of sociology at Yonsei University, \"social movements decreased slightly from 85 percent to 81 percent compared to 20 years ago.\" According to the analysis, the younger generation (born in 1987–1994) compared to the young people in the 1990s (1966–75) had a 12 per cent decrease in the percentage of upward movements that gained a better job than their parents, The mobility rate has increased by about 8 percentage points, and social mobility has changed negatively. Professor Han Jun pointed out, \"The problem is that the decrease in the mobility opportunity that I feel subjective is bigger than the actual one.\" According to the National Statistical Office (NSO), the negative perception of upside potential is 22 per cent more than 10 years ago (from 29 to 51 per cent) as of 2015.\n\nKorea's income mobility was the eighth highest among the 17 OECD member countries. Korea's income mobility is not relatively low, said Lee Jin-young, an assistant researcher at the Korea Economic Research Institute. According to the analysis of the inter-generation income elasticity of OECD member countries, Korea's income elasticity is 0.29, similar to New Zealand (0.29) and Sweden (0.27). On the other hand, Japan was higher than Korea with 0.34, 0.47 in the US and 0.32 in Germany.\n\nIn 2015, Korean society entered into a new class era as inequality increased. According to Global Attitudes Survey conducted by Pew Research Center, Korean citizens answered that the most threatening thing for them is inequality. However, the conclusion that inequality has a similar effect for all cultures should not be automatically assumed, since at least \"8 other countries were content with inequality\", including Greece. In contrast, Japan answered that nuclear weapons and radioactivity are most threatening. The results of this survey shows that the awareness of inequality is much higher than nuclear accidents and environmental pollution. \n\nThe suspected unqualified admission of Chung Yoo-ra to Ewha University is an example of nut rage. Chung Yoo-ra is a South Korean female dressage rider. She competed in the 2014 Asian Games, where she won a team gold medal. She is the daughter of Choi Sun-sil, who was a central figure in the 2016 South Korean political scandal. She is suspected of unqualified admission to the Ewha University.\n\n"}
{"id": "47274958", "url": "https://en.wikipedia.org/wiki?curid=47274958", "title": "Ternary equivalence relation", "text": "Ternary equivalence relation\n\nIn mathematics, a ternary equivalence relation is a kind of ternary relation analogous to a binary equivalence relation. A ternary equivalence relation is symmetric, reflexive, and transitive. The classic example is the relation of collinearity among three points in Euclidean space. In an abstract set, a ternary equivalence relation determines a collection of equivalence classes or \"pencils\" that form a linear space in the sense of incidence geometry. In the same way, a binary equivalence relation on a set determines a partition.\n\nA ternary equivalence relation on a set is a relation , written , that satisfies the following axioms:\n\n"}
{"id": "52454538", "url": "https://en.wikipedia.org/wiki?curid=52454538", "title": "The Nonhuman Turn", "text": "The Nonhuman Turn\n\nThe Nonhuman Turn describes a late 20th and 21st Century movement within the arts, humanities and social sciences, as practitioners of these disciplines turn away from the Social Constructivism of the earlier 20th Century, in favour of emergent philosophies which seek to decenter the human, and to emphasize instead the agency of the nonhuman world. The phrase is adopted by Richard Grusin in his introduction to a collection of essays of the same name, based on the proceedings of the 2012 conference on the Nonhuman Turn, hosted by the Center for 21st Century Studies at the University of Wisconsin-Milwaukee.\n\nThe nonhuman turn is not been a homogenous movement. Instead it should be thought of as an umbrella term covering a number of developments in philosophy and critical theory across a range of disciplines. The key constituents of this wider movement include: \n"}
{"id": "26075660", "url": "https://en.wikipedia.org/wiki?curid=26075660", "title": "Torcuato di Tella Institute", "text": "Torcuato di Tella Institute\n\nThe Torcuato di Tella Institute is a non-profit foundation organized for the promotion of Argentine culture.\n\nThe di Tella Foundation and its institute were created on July 22, 1958, the tenth anniversary of the death of industrialist and arts patron Torcuato di Tella. Funding for the project, organized by his sons, Torcuato and Guido di Tella, was raised using the United States model of corporate financing, as well as by the donation of 10% of the Siam di Tella corporation's public stock. Its objective was initially limited to an arts program revolving primarily around the display of the di Tella family's private collections, which prominently included works by Henry Moore, Pablo Picasso, Amedeo Modigliani and Jackson Pollock.\n\nThe board of the foundation consisted of family members, though the institute was directed by a board that included academics and intellectuals from outside the family. Guido di Tella would serve as president, and the post of director of the institute was offered to Enrique Oteiza, whose family were leading Pampas-area landowners. The foundation also received funding in the form of grants from the Ford and Rockefeller Foundations, after which the modest initiative expanded into theater and music, and grew to become the most significant cultural institution in Buenos Aires of the 1960s.\n\nThe institute continued to influence prevailing trends in the history of Argentine culture, however, and it adopted and advanced a modernist trend in various artistic disciplines. Its audiovisual center, established in 1960, and directed by Roberto Villanueva, premiered with a play, \"El Desatino\" (\"The Folly\"). The production's scenery backdrops were projected through slides, and introduced audiences to Nacha Guevara and Les Luthiers. This format would be promoted in subsequent years for its ability to broadcast material through compact and portable media in a way that would stimulate a network of local groups active in the cultural field. \nFollowing its establishment, the di Tella art collection was transferred to the foundation, and Jorge Romero Brest hosted a free show at the National Museum of Fine Arts, which the leading local art critic directed. The activities were transferred to a small office in the Museum of Fine Arts in August 1960, and this was followed by an annual award for national and international artists, many of which sold their works to the di Tella collection. As part of the awards program, the winners were awarded scholarships covering study abroad and an exhibition of works in a North American or European gallery. Growing local interest in Latin American art was accompanied by an initiative to show the di Tella collection across the Argentine hinterland, for which a minibus was purchased in 1963; the experiment, however, ended the following year, when the vehicle crashed in a rural La Rioja road.\nAn initiative by Guido di Tella led to the institute's relocation into a modern, newly completed Florida Street building in August 1963. The offices were rented by SIAM di Tella at the northern end of Florida Street, near Plaza San Martín, a busy pedestrian intersection in the upscale Retiro district that could attract larger audiences. The building was refurbished with the addition of three stage theatres, and interiors designed to be inviting, with a floor-to-ceiling glass panel façade featuring publicity photos taken by Humberto Rivas, and a large lobby. The modern, air-conditioned building was propitious for exhibits and artistic events year-round. Its café, like the gallery, was staffed by attendants who wore no uniforms, and allowed patrons to smoke and take photographs at their leisure.\n\nFounded by classical composer Alberto Ginastera, CLAEM (the Latin American Center for Advanced Musical Studies) was made part of the institute in 1962, yielding numerous productions of dodecaphonic, electronic, and acoustic music; CLAEM attracted prominent international guest lecturers such as Aaron Copland, Luigi Nono, and Iannis Xenakis. A visual arts center (CAV) was also inaugurated at the new address. Directed by Romero Brest, CAV became the leading Buenos Aires center for the display and promotion for avant-garde creations. CAV introduced art patrons to sculptors Juan Carlos Distéfano, Julio Le Parc, and Clorindo Testa, as well as painters Romulo Macció, Luis Felipe Noé, Jorge de la Vega, Ernesto Deira, Antonio Seguí, and conceptual artists such as Edgardo Giménez and Marta Minujín. The latter garnered interest after earning the institute's first National Award in 1964, and became known for her \"happenings.\" Erotic in some aspects, and provocative to conservative local audiences, her early di Tella Institute events included \"Eróticos en technicolor\" and the interactive \"Revuélquese y viva\" (\"Roll Around in Bed and Live\"). She then joined Rubén Santantonín in 1965 to create \"La Menesunda\" (\"Mayhem\"), where participants were asked to go through sixteen chambers, each separated by a human-shaped entry. Led by neon lights, groups of eight visitors would encounter rooms with television sets at full blast, couples making love in bed, a cosmetics counter (complete with an attendant), a dental office from which dialing an oversized rotary phone was required to leave, a walk-in freezer with dangling fabrics (suggesting sides of beef), and a mirrored room with black lighting, falling confetti, and the scent of frying food. The use of advertising throughout suggested the influence of pop art in Minujín's \"mayhem.\" \nAlready established as the leading local center for pop art, the di Tella Institute also became a forum for art as political commentary. This was dramatized by what became the center's most contentious display, sculptor León Ferrari's \"La civilización occidental y cristiana\" (\"Western-Christian Civilization\"), in October 1965. The work displays Christ crucified not by the traditional cross; but by a fighter plane, as a symbolic protest against the Vietnam War. \nA turn of historical events in 1966 proved detrimental to the institute, and to freedom of expression, when the civilian administration of President Arturo Illia was deposed on June 28 by the Argentine Armed Forces, and was replaced with the head of the Joint Chiefs of Staff, General Juan Carlos Onganía. Moderate in comparison with many of his counterparts in other Latin American nations, Onganía was, however, a member of the right-wing Catholic power group, Opus Dei, and as such, found many of the developments in Argentine culture during the 1960s offensive. Sharing his distaste for modern culture was Luis Margaride, whom he named head of the Federal District Police, and who would be remembered for his crusades against nightclubs, long hair, and miniskirts. Facing a government policy backdrop such as this, numerous avant-garde artists (and others, particularly in academia) left Argentina, many never to return.\n\nA self-styled \"mazanana loca\" (\"city block of madness\"), the center's agenda remained active initially, and this new era was marked by the advent of the \"Experience\"a fusion of the more controversial happenings with experimental theatre. The decline in audiences and contributions, as well as the Siam di Tella Corporation's own, mounting financial problems, led to the curtailment of most of its activities by 1969, however. The Ford Foundation continued to support the institute, though these grants were contractually limited to social studies. The director, Enrique Oteiza, and two leading board members, Jorge Sábato and Roberto Cortés Conde, resigned, and in May 1970, the famed Florida Street center hosted its last exhibition, a theatrical production by Marilú Marini. The foundation was bankrupt, and only the sale in 1971 of numerous works from its collection of Gothic, Renaissance, and Baroque art to the federal government for 2.1 million pesos (US$500,000), staved off its liquidation.\n\nRemembered nostalgically by friends of the arts, and particularly during the \"Dirty War\" of the late 1970s, when repression of political terrorism quickly extended to dissidents and controversial artists, the institute's absence became an example of the \"cultural blackout\" described by writer Ernesto Sábato at the time. Following President Néstor Kirchner's appointment of Torcuato Di Tella (jr.) as Secretary of Culture in 2003, the idea of reviving the storied center was first plausibly discussed by the president of the board of regents of Torcuato di Tella University, Manuel Mora y Araujo.\n\nMuch of the 1960s-era documentation and numerous works had been stored by the library at the university, the di Tella family's surviving contribution to local culture, and the material's recataloguing was initiated in 2004. New facilities were developed that year from a former water company building by one of the center's alumni, sculptor and architect Clorindo Testa, and on April 21, 2007, the institute's center for visual arts was reinaugurated at the Figueroa Alcorta Avenue location with the \"Otro Modo\" (\"Another Way\") Festival. Continuing to host media and conceptual art displays, the institute celebrated its 50th anniversary in March 2008.\n\n"}
