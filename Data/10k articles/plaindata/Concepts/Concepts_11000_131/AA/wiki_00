{"id": "1673370", "url": "https://en.wikipedia.org/wiki?curid=1673370", "title": "Adult attention deficit hyperactivity disorder", "text": "Adult attention deficit hyperactivity disorder\n\nAdult attention deficit hyperactivity disorder (also referred to as adult ADHD, adult with ADHD, or simply ADHD in adults, formerly AADD) is the neurobiological condition of attention deficit hyperactivity disorder (ADHD) in adults.\n\nAbout one-third to two-thirds of children with symptoms from early childhood continue to demonstrate notable ADHD symptoms throughout life.\n\nThree types of ADHD are identified in the DSM-5 as:\nIn later life, the hyperactive/impulsive subtype manifests less frequently. The hyperactivity symptoms tend to turn more into \"inner restlessness\", starting in adolescence and carrying on in adulthood.\n\nAdult ADHD is typically marked by inattentiveness, difficulty getting work done, procrastination and organizational problems. Specifically, adults with ADHD present with persistent difficulties in following directions, remembering information, concentrating, organizing tasks, completing work within specified time frames and appearing timely in appointments. These difficulties affect several different areas of an ADHD adult's life, causing emotional, social, vocational, marital, legal, financial and/or academic problems. As a result, low self-esteem is commonly developed. However, given the right guidance and coaching, these traits of ADHD could also lead to career success, and in some cases, unique advantages in critical thinking and creativity.\n\nDiagnosis of the condition follows after one or several assessment interviews by a clinician including:\nas well as evaluation to diagnose additional possible conditions which often coexist with ADHD, called comorbidities or comorbid disorders.\n\nThe condition is highly heritable, and while its exact causes are not fully known, genetic or environmental factors are understood to play a part. ADHD is a childhood-onset condition, usually requiring symptoms to have been present before age 12 for a diagnosis. Children under treatment will migrate to adult health services if necessary as they transit into adulthood, however diagnosis of adults involves full examination of their history.\n\nSuccessful treatment of ADHD is usually based on a combination of medication, cognitive behavioral therapy, and coaching or skills training. Medium-to-high intensity physical exercise, improved sleep and improved and targeted nutrition are also known to have a positive effect. Within school and work, reasonable accommodations may be put in place to help the individual work more efficiently and productively.\n\nThe DSM-5, or Diagnostic and Statistical Manual of Mental Disorders, 2013 edition, defines three types of ADHD:\n\nTo meet the diagnostic criteria of ADHD, an individual must display:\n\nThe symptoms (see below) were required to have been present since before the individual was seven years old, and must have interfered with at least two spheres of his or her functioning (at home and at school or work, for example) over the last six months. The DSM-IV criteria for ADHD were, however, tailored towards the type of symptoms that children would show, and might therefore have underestimated the prevalence of ADHD in adults. In 2013, the newer DSM-5 reviewed some of these criteria, with more lenient requirements for the diagnosis, especially in adults, and the age limit for symptoms first arising raised to twelve years.\nADHD is a chronic condition, beginning in early childhood and persisting throughout a person's lifetime. It is estimated that 33–66% of children with ADHD will continue to have significant ADHD-related symptoms persisting into adulthood, resulting in a significant impact on education, employment, and interpersonal relationships.\n\nIndividuals with ADHD exhibit deficiencies in self-regulation and which in turn foster problematic characteristics such as distractibility, procrastination and disorganization. They are often perceived by others as chaotic, with a tendency to need high stimulation to be less distracted and function effectively. The learning potential and overall intelligence of an adult with ADHD, however, are no different from the potential and intelligence of adults who do not have the disorder.\n\nWhereas teachers and caregivers responsible for children are often attuned to the symptoms of ADHD, employers and others who interact with adults are less likely to regard such behaviors as a symptom. In part, this is because symptoms do change with maturity; adults who have ADHD are less likely to exhibit obvious hyperactive behaviors. Instead, they may report constant mental activity and inner restlessness as their hyperactivity internalizes.\n\nSymptoms of ADHD (see table below) can vary widely between individuals and throughout the lifetime of an individual. As the neurobiology of ADHD is becoming increasingly understood, it is becoming evident that difficulties exhibited by individuals with ADHD are due to problems with the parts of the brain responsible for executive functions (see below: Pathophysiology). These result in problems with sustaining attention, planning, organization, prioritization, time blindness, impulse control and decision making.\n\nThe difficulties generated by these deficiencies can range from moderate to extreme, resulting in the inability to effectively structure their lives, plan daily tasks, or think of and act accordingly even when aware of potential consequences. These lead to poor performance in school and work, followed by underachievement in these areas. In young adults, poor driving record with traffic violations as well as histories of alcoholism or substance abuse may surface. The difficulty is often due to the ADHD person's observed behaviour (e.g. the impulsive types, who may insult their boss for instance, resulting in dismissal), despite genuinely trying to avoid these and knowing that it can get them in trouble. Often, the ADHD person will miss things that an adult of similar age and experience should catch onto or know. These lapses can lead others to label the individuals with ADHD as \"lazy\" or \"stupid\" or \"inconsiderate\".\n\nAs problems accumulate, a negativistic self-view becomes established and a vicious circle of failure is set up. Up to 80% of adults may have some form of psychiatric comorbidity such as depression or anxiety. Many with ADHD also have associated learning disabilities, such as dyslexia, which contributes to their difficulties.\n\nStudies on adults with ADHD have shown that, more than often, they experience self stigma and depression in childhood, commonly resulting from feeling neglected and different from their peers. These problems may play a role to the high levels of depression, substance abuse, and relationship problems that affect adults with ADHD later in life.\n\nOver the last 30 years, research into ADHD has greatly increased. There is no single, unified theory that explains the cause of ADHD. Genetic factors are presumed important, and it has been suggested that environmental factors may affect how symptoms manifest.\n\nIt is becoming increasingly accepted that individuals with ADHD have difficulty with \"executive functioning\". In higher organisms, such as humans, these functions are thought to reside in the frontal lobes. They enable recall of tasks that need accomplishing, organization to accomplish these tasks, assessment of consequences of actions, prioritization of thoughts and actions, keeping track of time, awareness of interactions with surroundings, the ability to focus despite competing stimuli, and adaptation to changing situations.\n\nSeveral lines of research based on structural and/or functional imaging techniques, stimulant drugs, psychological interventions have identified alterations in the dopaminergic and adrenergic pathways of individuals with ADHD. In particular, areas of the prefrontal cortex appear to be the most affected. Dopamine and norepinephrine are neurotransmitters playing an important role in brain function. The uptake transporters for dopamine and norepinephrine are overly active and clear these neurotransmitters from the synapse a lot faster than in normal individuals. This is thought to increase processing latency and salience, and diminished working memory.\n\nThe diagnosis of ADHD in adults requires retrospectively establishing whether the symptoms were also present in childhood, even if not previously recognized. As with other mental disorders such as schizophrenia there is no objective \"test\" that diagnoses ADHD. Rather, it is a combination of a careful history of symptoms up to early childhood, including corroborating evidence from family members, previous report cards, etc. The screening tests also seek to rule out other conditions or differential diagnoses such as depression, anxiety, or substance abuse. Other diseases such as hyperthyroidism may exhibit symptoms similar to those of ADHD, and it is imperative to rule these out as well. Asperger syndrome, a condition on the autism spectrum, is sometimes mistaken for ADHD, due to impairments in executive functioning found in some people with Asperger syndrome. However, Asperger syndrome also typically involves difficulties in social interaction, restricted and repetitive patterns of behavior and interests, and problems with sensory processing, including hypersensitivity. Along with this, the quality of diagnosing an adult with ADHD can often be skewed being that the majority of adults with ADHD also have other complications, ranging from anxiety and depression to substance abuse.\n\nAssessment of adult patients seeking a possible diagnosis can be better than in children due to the adult's greater ability to provide their own history, input, and insight. However, it has been noted that many individuals, particularly those with high intelligence, develop coping strategies that mask ADHD impairments and therefore they do not seek diagnosis and treatment.\n\nFormal tests and assessment instruments such as IQ tests, standardized achievement tests, or neuropsychological tests typically are \"not helpful\" for identifying people with ADHD. Furthermore, no currently available physiological or medical measure is definitive diagnostically. However, psycho-educational and medical tests are helpful in ruling in or out other conditions (e.g. learning disabilities, mental retardation, allergies) that may be associated with ADHD-like behaviors.\n\nUnited States medical and mental health professionals follow the Diagnostic and Statistical Manual of Mental Disorders (DSM) of the American Psychiatric Association; the International Classification of Diseases (ICD) published by the World Health Organisation (WHO) is often used by health professionals elsewhere. Periodic updates incorporate changes in knowledge and treatments. For example, under DSM-IV (published in 1994, with corrections and minor changes in 2000), the diagnostic criteria for ADHD in adults broadly follow the same as in children, but the proposed revision for the DSM-5 differentiates the presentation of ADHD for children and adults for several symptoms.\n\nIt should be noted that every normal individual exhibits ADHD-like symptoms occasionally (when tired or stressed, for example) but for a positive diagnosis to be received, the symptoms should be present from childhood and persistently interfere with functioning in multiple spheres of an individual's life: work, school, and interpersonal relationships. The symptoms that individuals exhibit as children are still present in adulthood, but manifest differently as most adults develop compensatory mechanisms to adapt to their environment.\n\nTreatment for adult ADHD may combine medication and behavioral, cognitive, or vocational interventions. Treatment often begins with medication selected to address the symptoms of ADHD, along with any comorbid conditions that may be present. Medication alone, while effective in correcting the physiological symptoms of ADHD, will not address the paucity of skills which many adults will have failed to acquire because of their ADHD (e.g., one might regain \"ability\" to focus with medication, but skills such as organizing, prioritizing and effectively communicating have taken others time to cultivate).\n\nStimulants, the first line medications in adult ADHD, are typically formulated in immediate and long-acting formulations.\n\nMethylphenidate, a stimulant, with short and long-acting formulations, is often the first-line therapy and appears effective. In the short term, methylphenidate is well tolerated. However, long term studies have not been conducted in adults and concerns about increases in blood pressure have not been established. Methylphenidate increases concentrations of dopamine and norepinephrine in the synaptic cleft, promoting increased neurotransmission. It acts to block the dopamine and norepinephrine reuptake transporters, thus slowing the removal at which these neurotransmitters are cleared from the synapses.\n\nAmphetamine and its derivatives, prototype stimulants, are likewise available in immediate and long-acting formulations. Amphetamines act by multiple mechanisms including reuptake inhibition, displacement of transmitters from vesicles, reversal of uptake transporters and reversible MAO inhibition. Thus amphetamines actively increases the release of these neurotransmitters into the synaptic cleft. They may have a better side-effect profile than methylphenidate cardiovascularly and potentially better tolerated.\n\nThe non-stimulant atomoxetine (Strattera), is also an effective treatment for adult ADHD. Although atomoxetine has a half life similar to stimulants it exhibits delayed onset of therapeutic effects similar to antidepressants. Unlike the stimulants which are controlled substances, atomoxetine lacks abuse potential. It is particularly effective for those with the predominantly inattentive concentration type of attention deficit due to being primarily a norepinephrine reuptake inhibitor. It is often prescribed in adults who cannot tolerate the side effects of amphetamines or methylphenidate. It is also approved for ADHD by the US Food and Drug Administration. A rare but potentially severe side effect includes liver damage and increased suicidal ideation.\n\nBupropion and desipramine are two antidepressants that have demonstrated some evidence of effectiveness in the management of ADHD particularly when there is comorbid major depression, although antidepressants have lower treatment effect sizes.\n\nTreatment of adult ADHD may also include forms of stress management or relaxation training.\n\nResearch has shown that, alongside medication, psychological interventions in adults can be effective in reducing symptomatic deficiencies. Emerging evidence suggests a possible role for cognitive behavioral therapy (CBT) alongside medication in the treatment of adult ADHD.\n\nFor most adults, the psychosocial therapy is not effective. For this reason, medications are the first line of therapies. The medications that are prescribed for adults come in both stimulant and non-stimulant form. Although the drug therapies are effective for adults, the benefits should be discussed with the patient’s physician to ensure the benefits of the medications outweigh the risk. If medication is unwanted or not an option, increasing exercise and changing one’s diet may help alleviate some of the symptoms such as hyperactivity\n\nExercise may alleviate some of the symptoms of ADHD for approximately 45 minutes.\n\nIn North America and Europe, it is estimated that three to five percent of adults have ADHD, but only about ten percent of those have received a formal diagnosis. It has been estimated that 5% of the global population has ADHD (including cases not yet diagnosed). In the context of the World Health Organization World Mental Health Survey Initiative, researchers screened more than 11,000 people aged 18 to 44 years in ten countries in the Americas, Europe and the Middle East. On this basis they estimated the adult ADHD proportion of the population to average 3.5 percent with a range of 1.2 to 7.3 percent, with a significantly lower prevalence in low-income countries (1.9%) compared to high-income countries (4.2%). The researchers concluded that adult ADHD often co-occurs with other disorders, and that it is associated with considerable role disability. Although they found that few adults are treated for ADHD itself, in many instances treatment is given for the co-occurring disorders.\n\nEarly work on disorders of attention was conducted by Alexander Crichton in 1798 writing about \"mental restlessness\". The underlying condition came to be recognized from the early 1900s by Sir George Still. Efficacy of medications on symptoms was discovered during the 1930s and research continued throughout the twentieth century. ADHD in adults began to be studied from the early 1970s and research has increased as worldwide interest in the condition has grown.\n\nIn the 1970s researchers began to realize that the condition now known as ADHD did not always disappear in adolescence, as was once thought. The expansion of the definition for ADHD beyond only being a condition experienced by children was mainly accomplished by refocusing the diagnosis on inattention instead of hyperactivity. At about the same time, some of the symptoms were also noted in many parents of the children under treatment. The condition was formally recognized as affecting adults in 1978, often informally called \"adult ADD\", since symptoms associated with hyperactivity are generally less pronounced.\n\nADHD in adults, as with children, is recognized as an impairment that may constitute a disability under U.S. federal disability nondiscrimination laws, including such laws as the Rehabilitation Act of 1973 and the Americans With Disabilities Act (ADA, 2008 revision), if the disorder substantially limits one or more of an individual's major life activities. For adults whose ADHD does constitute a disability, workplaces have a duty to provide reasonable accommodations, and educational institutions have a duty to provide appropriate academic adjustments or modifications, to help the individual work more efficiently and productively.\n\nIn a 2004 study it was estimated that the yearly income discrepancy for adults with ADHD was $10,791 less per year than high school graduate counterparts and $4,334 lower for college graduate counterparts. The study estimates a total loss in productivity in the United States of over $77 billion USD. By contrast, loss estimations are $58 billion for drug abuse, $85 billion for alcohol abuse and $43 billion for depression.\n\nADHD controversies include concerns about its existence as a disorder, its causes, the methods by which ADHD is diagnosed and treated including the use of stimulant medications in children, possible overdiagnosis, misdiagnosis as ADHD leading to undertreatment of the real underlying disease, alleged hegemonic practices of the American Psychiatric Association and negative stereotypes of children diagnosed with ADHD. These controversies have surrounded the subject since at least the 1970s.\n\n"}
{"id": "18497722", "url": "https://en.wikipedia.org/wiki?curid=18497722", "title": "Ann Dancing", "text": "Ann Dancing\n\nAnn Dancing is an artwork created in 2007 by Julian Opie (born 1958, London) an English artist and former trustee of the Tate. The sculpture was removed from its base on August 20, 2008, for repairs, and was returned on October 31 of that year.\n\nThe sculpture consists of four rectilinear panels of light-emitting diode (LED) screens that each display the same animated image in orange of a woman on all four panels. The woman, \"Ann,\" is wearing a sheath dress and sways from side to side in a dancing motion. Ann either has pointed feet or is wearing high-heeled shoes. She appears to have no clear hairstyle.\n\nIt is probable that the animated image of Ann dancing is computer generated from an internally housed computer located in the red brick base of the sculpture.\n\nThe sculpture was installed at the intersection of Massachusetts Avenue, Alabama Street, and Vermont Street in Indianapolis from January to February, 2008. The sculpture is located directly in front of the Old Point Tavern and was the first artwork installed on the Indianapolis Cultural Trail at a total expense of $150,000.\n\nThe sculpture is visible in the Google Maps Street View, but only from the middle of the intersection .\n\n—Julian Opie, 2008.\n\n\n\n"}
{"id": "2275708", "url": "https://en.wikipedia.org/wiki?curid=2275708", "title": "Authoritarian personality", "text": "Authoritarian personality\n\nAuthoritarian personality is a state of mind or attitude characterized by belief in absolute obedience or submission to someone else’s authority, as well as the administration of that belief through the oppression of one's subordinates. It usually applies to individuals who are known or viewed as having an authoritarian, strict, or oppressive personality towards subordinates.\n\nTheodor W. Adorno, Else Frenkel-Brunswik, Daniel Levinson, and Nevitt Sanford theorized in their 1950 book, \"The Authoritarian Personality\", about a personality type that involved the \"potentially fascistic individual\". They labeled it the \"authoritarian personality\" based on earlier writings by Erich Fromm that used this term. Because the historical influences for their theory included the rise of fascism in the 1930s, World War II, and the Holocaust, a main component of the \"authoritarian personality\" is being susceptible to antisemitic ideology and anti-democratic political beliefs. Their large body of research (known as the Berkeley studies) focused mainly on prejudice within a psychoanalytic/psychosocial theoretical framework (i.e., Freudian and Frommian).\n\nAdorno et al. (1950) viewed the authoritarian personality as having a strict superego that controls a weak ego unable to cope with strong id impulses. The resulting intrapsychic conflicts cause personal insecurities, resulting in that person's superego to adhere to externally imposed conventional norms (conventionalism), and to the authorities who impose these norms (authoritarian submission). The ego-defense mechanism of projection occurs as indicated when that person avoids self-reference of the anxiety-producing id impulse, by displaying them onto \"inferior\" minority groups in the given culture (projectivity), with associated beliefs that are highly evaluative (power and toughness), and rigid (stereotypy). Additionally, there is a cynical view of humanity and a need for power and toughness resulting from the anxieties produced by perceived lapses in society's conventional norms (destructiveness and cynicism). Other characteristics of this personality type are a general tendency to focus upon those who violate conventional values and act harshly towards them (authoritarian aggression), a general opposition to subjective or imaginative tendencies (anti-intraception), a tendency to believe in mystic determination (superstition), and finally, an exaggerated concern with promiscuity.\nIn regards to child development, the formation of the authoritarian type occurs within the first few years of the person's life, strongly shaped by the parents and family structure. \"Hierarchical, authoritarian, exploitative\" parent-child relationships may result in this personality type (Adorno et al., 1950, pp. 482–484). Parents who have a need for domination, and who dominate and threaten the child harshly, and demand obedience to conventional behaviors with threats, foster the characteristics of this personality. In addition, the parents have a preoccupation with social status, and communicate this to the child in terms of rigid and externalized rules. The child then suffers from suppressed feelings of resentment and aggression towards the parents, who are instead, idealized with reverence.\n\nAlfred Adler provided another perspective, linking the \"will to power over others\" as a central neurotic trait, usually emerging as aggressive over-compensation for felt and dreaded feelings of inferiority and insignificance. According to this view, the authoritarian's need to maintain control and prove superiority over others is rooted in a worldview populated by enemies and empty of equality, empathy, and mutual benefit.\n\nThese researchers' most noteworthy measurement for authoritarianism is the \"F-scale\", designed to tap a set of beliefs thought to be associated with authoritarianism without the need for specific out-groups indicated. Kirscht and Dillehay (1967) outlined several problems with the Berkeley studies, including response bias. Response bias results from the F scale being uniformly worded in a confirming direction. Hence, if one tends to respond in agreement with items, regardless of their content, one is rated as an authoritarian by such a test. Several studies have shown that more variance of the F scale can be explained by response bias than the content of the items (Kirscht & Dillehay, 1967).\n\nActual assessment of 16 Nazi criminals at Nuremberg trials (reported in Zillmer, \"et al.\", 1995) conducted by clinicians using the Rorschach inkblots, and in one study, the F scale for authoritarianism, found that these ex-Nazis score high on three dimensions (anti-intraception, superstition and stereotyping, and projectivity), but not all nine dimensions as the theory predicted.\n\nOne of the first applications of the authoritarian scales in academia was by Stern and colleagues, in the early 1950s, at the University of Chicago (as reported in Wiggins, 1980). The hypothesized prediction was that \"authoritarian\" students would have difficulty in the sciences and humanities, and use of an attitudinal scale was a successful predictor.\n\nSoon after the publication of \"The Authoritarian Personality\", the theory became the subject of many criticisms. Theoretical problems involved the psychoanalytic interpretation of personality, and methodological problems focused on the inadequacies of the F-scale. Another criticism is that the theory of the Berkeley group insinuates that authoritarianism exists only on the right of the political spectrum. As a result, some have claimed that the theory is corrupted by political bias. Kreml found that although there were stylistic similarities between authoritarians and anti-authoritarians (dogmatism, rigidity, etc.), construct variables like a) the relative need for order, b) the relative need for power, c) rejection or acceptance of impulse, and d) extroversion versus introversion, differentiated the two types and could underpin a full-spectrum psycho-political theory.\n\nWiggins provided an insightful explanation of how the authoritarian construct is an example of the synthetic approach to personality assessment. In short, in the synthetic approach, the assumption is that those with authoritarian personality characteristics are assessed with researcher's intuitive model of what characteristics fit the criterion role requirements of the predicted situation (support of Fascism). Hence, it is not a completely empirical approach to prediction, but rather based on \"arm chair\" situational analysis of the criteria, and intuited psychological characteristics to be assessed that fit the situation. More recently, Jost, Glaser, Kruglanski, and Sulloway (2003) have presented how the traditional research in authoritarianism or conservatism has confounded the psychological variables (e.g., personality characteristics) with the political criteria (conservative attitudes). Hence the scales measuring individual differences on authoritarianism often include the criteria attitudinal statements of political ideologies.\n\nAlthough the authoritarian is a personality construct, Adorno et al. (1950) proposed that the social environment influenced the expression of prejudice expressed, based upon the \"climate of opinion\" that exists at the time. Hence, ideological beliefs created within the culture and other social forces shape the prejudices of the given authoritarian individual. However, as noted by Taylor (1998), this hypothesized interaction of society and the individual is lost to most of the subsequent research that implemented the F scale in differential psychological studies. Given the science of personality assessment, the variety of methods Adorno, \"et al.\" used are now unsupported, and might explain that lack of empirical studies using the F scale or the other scales developed by Adorno \"et al.\" in subsequent research. An example of the social environment impact is presented by Gibb (1969) in his critique of personality traits and leadership, where a study by Katz suggested that the social situation can override personality differences. In the study, groups of black and white students were formed. Some mixed racial groups had students scoring high authoritarian F scores, and in other mixed groups, low F score students. Comparisons of high authoritarian white students to those not scoring authoritarian indicated that the former student type were more cooperative and less willing to endorse stereotypes towards blacks. Situational norms against prejudicial perceptions might have influenced authoritarian students to act less prejudicial in order to conform to the prescribed norm.\n\nAfter extensive questionnaire research and statistical analysis, Canadian psychologist Bob Altemeyer found in 1981 that only three of the original nine hypothesized components of the model correlated together: authoritarian submission, authoritarian aggression, and conventionalism. Almeyer added: \"The reader familiar with the matter knows that most these criticisms [of the \"Authoritarian Personality\"] are over 25 years old, and now they might be considered little more than flaying a dead horse. Unfortunately the flaying is necessary, for the horse is not dead, but still trotting around—in various introductory psychology and developmental psychology textbooks, for example.\n\nBob Altemeyer conducted a series of studies on what he labeled right-wing authoritarianism (RWA), and presents the most recent analysis of this personality type. The focus of RWA research is political preferences as measured through surveys, that suggest three tendencies as noted in attitudinal clusters. These are: 1) submission to legitimate authorities; 2) aggression towards sanctioned targeted minority groups; and 3) adherence to values and beliefs perceived as endorsed by followed leadership. McCrae & Costa (1997) report that the big 5 dimension of openness to experience is negatively correlated to RWA (r=-0.57) as measured by the NEO-PI-R Openness scale.\n\nLater, Jost, Glaser, Kruglanski, and Sulloway (2003) have proposed that authoritarianism, RWA and other similar constructs of political conservatism are a form of motivated social cognition. These researchers propose that conservatism has characteristics similar to those of authoritarianism, with resistance to change, and justification for inequality as the core components. In addition, conservative individuals have needs to manage uncertainty and threat with both situational motives (e.g., striving for security and dominance in social hierarchies) and dispositional motives (e.g., terror management and self-esteem). Despite its methodological deficiencies, the theory of the authoritarian personality has had a major influence on research in political, personality, and social psychology.\n\nThe research on social cognition and motivation is continued by John Duckitt and Chris Sibley. They distinguish two different aspects of world views that lead to two different kinds of authoritarianism. A view of the social world as dangerous and threatening leads to right wing authoritarianism, while the view of the world as a ruthlessly competitive jungle in which the strong win and the weak lose leads to social dominance orientation.\n\nA recent reinterpretation called regality theory is based on evolutionary psychology. Regality theory sees the authoritarian attitude as a reaction to perceived collective danger. It is argued that this theory adds an evolutionary level of analysis and avoids the political bias that authoritarianism theory is often accused of.\n\n"}
{"id": "58475368", "url": "https://en.wikipedia.org/wiki?curid=58475368", "title": "Automatic Clustering Algorithms", "text": "Automatic Clustering Algorithms\n\nAutomatic Clustering Algorithm refers to algorithms that can perform clustering without prior knowledge of the data sets. In contrast with other Cluster Analysis techniques, automatic clustering algorithms can determine the optimal number of clusters even in the presence of noise and outlier points.\n\nGiven a set of \"n\" objects, centroid-based algorithms create \"k\" partitions based on a dissimilarity function, such that \"k≤n\". A major problem in applying this type of algorithm is determining the appropriate number of clusters for unlabeled data. Therefore, most research in clustering analysis has been focused on the automation of the process. \n\nAutomated selection of \"k\" in a \"K\"-means clustering algorithm, one of the most used centroid-based clustering algorithms, is still a big problem in machine learning. The most accepted solution to this problem is the elbow method. It consists of running \"k\"-means clustering to the data set with a range of values, calculating the sum of squared errors for each, and plotting them in a line chart. If the chart looks like an arm, the best value of \"k\" will be on the \"elbow\". \n\nAnother method that modifies the \"k\"-means algorithm for automatically choosing the optimal number of clusters is the \"G\"-means algorithm. It was developed from the hypothesis that a subset of the data follows a Gaussian distribution. Thus, \"k\" is increased until each \"k\"-means center's data is Gaussian. This algorithm only requires the standard statistical significance level as a parameter and it does not set limits for the covariance of the data.\n\nConnectivity-Based Clustering or Hierarchical Clustering is based on the idea that objects have more similarities to other nearby objects than to those further away. Therefore, the generated clusters from this type of algorithm will be the result of the distance between the analyzed objects. Hierarchical models can either be divisive—where partitions are built from the entire data set available—or agglomerating—where each partition begins with a single object and additional objects are added to the set. Although hierarchical clustering has the advantage of allowing any valid metric to be used as the defined distance, it comes with many problems. For instance, it is sensitive to noise, to fluctuations in the data set, and it is more difficult to automate. \n\nMethods have been developed to improve and automate existing hierarchical clustering algorithms such as an automated version of single linkage hierarchical cluster analysis (HCA). This computerized method bases its success on a self-consistent outlier reduction approach, followed by the building up of a descriptive function, which permits defining natural clusters. Discarded objects can also be assigned to these clusters. Essentially, one needs not to resort to external parameters to identify natural clusters. Information gathered from HCA, automated and reliable, can be resumed in a dendrogram with the number of natural clusters and the corresponding separation (an option not found in classical HCA). This method includes the two following steps: outliers are removed (this is applied in many filtering applications) and an optional classification allows expanding clusters with the whole set of objects. \n\nBIRCH (balanced iterative reducing and clustering using hierarchies) is an algorithm used to perform connectivity-based clustering for large data-sets. It is regarded as one of the fastest clustering algorithms, but it still has limitations because it requires the number of clusters as input. Therefore, new algorithms based on BIRCH have been developed in which there is no need to provide the cluster count from the beginning, but that preserve the quality and speed of the clusters. The main modification is to remove the final step of BIRCH, where the use had to input the cluster count, and improve the rest of the algorithm, referred to as tree-BIRCH, by optimizing a threshold parameter from the data. In this resulting algorithm, the threshold parameter is calculated from the maximum cluster radius and the minimum distance between clusters, which are often known. This method proved to be efficient for data sets of tens of thousands of clusters. If going beyond that amount, a supercluster splitting problem is introduced. For this, other algorithms have been developed, like MDB-BIRCH, which reduces supercluster splitting with relatively high speed.\n\nUnlike partitioning and hierarchical methods, density-based clustering algorithms are able to find clusters of any arbitrary shape, not only spheres. The Density-based clustering algorithm uses autonomous machine learning that identifies patterns regarding geographical location and distance to a particular number of neighbors. It is considered autonomous because a priori knowledge on what is a cluster is not required. This type of algorithm provides different methods to find clusters in the data. The fastest method is DBSCAN and uses a defined distance to differentiate between dense groups of information and sparser noise. Moreover, HDBSCAN can self-adjust by using a range of distances instead of a specified one. Lastly, the method OPTICS creates a reachability plot based on the distance from neighboring features to separate noise from clusters of varying density. \n\nThese methods still require the user to provide the cluster center and cannot be considered automatic. The Automatic Local Density Clustering Algorithm (ALDC) is an example of the new research focused on developing automatic density-based clustering. ALDC works out local density and distance deviation of every point, thus expanding the difference between the potential cluster center and other points. This expansion allows the machine to work automatically. The machine identifies cluster centers and assigns the points that are left by their closest neighbor of higher density. \"\" \n\nIn the automation of data density to identify clusters, research has also been focused on artificially generating the algorithms. For instance, the Estimation of Distribution Algorithms guarantees the generation of valid algorithms by the directed acyclic graph (DAG), in which nodes represent procedures (building block) and edges represent possible execution sequences between two nodes. Building Blocks determine the EDA’s alphabet or, in other words, any generated algorithm. Clustering algorithms artificially generated are compared to DBSCAN, a manual algorithm, in experimental results.\n"}
{"id": "188401", "url": "https://en.wikipedia.org/wiki?curid=188401", "title": "Axiomatic system", "text": "Axiomatic system\n\nIn mathematics, an axiomatic system is any set of axioms from which some or all axioms can be used in conjunction to logically derive theorems. A theory consists of an axiomatic system and all its derived theorems. An axiomatic system that is completely described is a special kind of formal system. A formal theory typically means an axiomatic system, for example formulated within model theory. A formal proof is a complete rendition of a mathematical proof within a formal system.\n\nAn axiomatic system is said to be \"consistent\" if it lacks contradiction, i.e. the ability to derive both a statement and its denial from the system's axioms.\n\nIn an axiomatic system, an axiom is called \"independent\" if it is not a theorem that can be derived from other axioms in the system. A system will be called independent if each of its underlying axioms is independent. Although independence is not a necessary requirement for a system, consistency usually is, but see neutrosophic logic.\n\nAn axiomatic system will be called \"complete\" if for every statement, either itself or its negation is derivable.\n\nBeyond consistency, relative consistency is also the mark of a worthwhile axiom system. This is when the undefined terms of a first axiom system are provided definitions from a second, such that the axioms of the first are theorems of the second.\n\nA good example is the relative consistency of neutral geometry or absolute geometry with respect to the theory of the real number system. Lines and points are undefined terms in absolute geometry, but assigned meanings in the theory of real numbers in a way that is consistent with both axiom systems.\n\nA model for an axiomatic system is a well-defined set, which assigns meaning for the undefined terms presented in the system, in a manner that is correct with the relations defined in the system. The existence of a concrete model proves the consistency of a system. A model is called concrete if the meanings assigned are objects and relations from the real world}, as opposed to an abstract model which is based on other axiomatic systems.\n\nModels can also be used to show the independence of an axiom in the system. By constructing a valid model for a subsystem without a specific axiom, we show that the omitted axiom is independent if its correctness does not necessarily follow from the subsystem.\n\nTwo models are said to be isomorphic if a one-to-one correspondence can be found between their elements, in a manner that preserves their relationship. An axiomatic system for which every model is isomorphic to another is called categorial (sometimes categorical), and the property of categoriality (categoricity) ensures the completeness of a system.\n\nStating definitions and propositions in a way such that each new term can be formally eliminated by the priorly introduced terms requires primitive notions (axioms) to avoid infinite regress. This way of doing mathematics is called the axiomatic method.\n\nA common attitude towards the axiomatic method is logicism. In their book \"Principia Mathematica\", Alfred North Whitehead and Bertrand Russell attempted to show that all mathematical theory could be reduced to some collection of axioms. More generally, the reduction of a body of propositions to a particular collection of axioms underlies the mathematician's research program. This was very prominent in the mathematics of the twentieth century, in particular in subjects based around homological algebra.\n\nThe explication of the particular axioms used in a theory can help to clarify a suitable level of abstraction that the mathematician would like to work with. For example, mathematicians opted that rings need not be commutative, which differed from Emmy Noether's original formulation. Mathematicians decided to consider topological spaces more generally without the separation axiom which Felix Hausdorff originally formulated.\n\nThe Zermelo-Fraenkel axioms, the result of the axiomatic method applied to set theory, allowed the \"proper\" formulation of set-theory problems and helped to avoid the paradoxes of naïve set theory. One such problem was the Continuum hypothesis. Zermelo–Fraenkel set theory with the historically controversial axiom of choice included is commonly abbreviated ZFC, where C stands for choice. Many authors use ZF to refer to the axioms of Zermelo–Fraenkel set theory with the axiom of choice excluded. Today ZFC is the standard form of axiomatic set theory and as such is the most common foundation of mathematics.\n\nMathematical methods developed to some degree of sophistication in ancient Egypt, Babylon, India, and China, apparently without employing the axiomatic method.\n\nEuclid of Alexandria authored the earliest extant axiomatic presentation of Euclidean geometry and number theory. Many axiomatic systems were developed in the nineteenth century, including non-Euclidean geometry, the foundations of real analysis, Cantor's set theory, Frege's work on foundations, and Hilbert's 'new' use of axiomatic method as a research tool. For example, group theory was first put on an axiomatic basis towards the end of that century. Once the axioms were clarified (that inverse elements should be required, for example), the subject could proceed autonomously, without reference to the transformation group origins of those studies.\n\nNot every consistent body of propositions can be captured by a describable collection of axioms. Call a collection of axioms recursive if a computer program can recognize whether a given proposition in the language is an axiom. Gödel's First Incompleteness Theorem then tells us that there are certain consistent bodies of propositions with no recursive axiomatization. Typically, the computer can recognize the axioms and logical rules for deriving theorems, and the computer can recognize whether a proof is valid, but to determine whether a proof exists for a statement is only soluble by \"waiting\" for the proof or disproof to be generated. The result is that one will not know which propositions are theorems and the axiomatic method breaks down. An example of such a body of propositions is the theory of the natural numbers. The Peano Axioms (described below) thus only partially axiomatize this theory.\n\nIn practice, not every proof is traced back to the axioms. At times, it is not clear which collection of axioms a proof appeals to. For example, a number-theoretic statement might be expressible in the language of arithmetic (i.e. the language of the Peano Axioms) and a proof might be given that appeals to topology or complex analysis. It might not be immediately clear whether another proof can be found that derives itself solely from the Peano Axioms.\n\nAny more-or-less arbitrarily chosen system of axioms is the basis of some mathematical theory, but such an arbitrary axiomatic system will not necessarily be free of contradictions, and even if it is, it is not likely to shed light on anything. Philosophers of mathematics sometimes assert that mathematicians choose axioms \"arbitrarily\", but it is possible that although they may appear arbitrary when viewed only from the point of view of the canons of deductive logic, that appearance is due to a limitation on the purposes that deductive logic serves.\n\nThe mathematical system of natural numbers 0, 1, 2, 3, 4, ... is based on an axiomatic system first written down by the mathematician Peano in 1889. He chose the axioms, in the language of a single unary function symbol \"S\" (short for \"successor\"), for the set of natural numbers to be:\n\n\nIn mathematics, axiomatization is the formulation of a system of statements (i.e. axioms) that relate a number of primitive terms in order that a consistent body of propositions may be derived deductively from these statements. Thereafter, the proof of any proposition should be, in principle, traceable back to these axioms.\n\n\n"}
{"id": "8475917", "url": "https://en.wikipedia.org/wiki?curid=8475917", "title": "BIFF", "text": "BIFF\n\nBIFF, later sometimes B1FF, was a pseudonym on, and the prototypical newbie of, Usenet. BIFF was created as and taken up as a satire of a partly amusing, partly annoying, mostly unwelcome intrusion into a then fairly rarefied community. BIFF had a Commodore Vic-20 at first and a Commodore 64 later. BIFF posts were limited to 40 character lines to look like they'd come from those machines.\n\nBIFF was created by Joe Talmadge, who abandoned the character after just two postings. From then on, Richard Sexton took over and was credited by Talmadge as popularising BIFF.\n\"Richard Sexton:\" I make no claim to inventing BIFF. Blame Joe Talmadge. Joe wasn't too busy one year at HP and invented a whole cast of characters such as SYSTEMS ADMINISTRATOR MAN, Bobby Joe (Dedicated Wobegon listener), Joe Supportive (soc.singles reader for 1.5 years) and of course the big bad BIFFSTER.\n\n\"Joe Talmadge:\" Oh. Hey. Don't blame me. It's Webber's fault. Anyway, I may have invented BIFF, but Richard made him famous. Richard is the Roy Crock of BIFF; meanwhile, I fade quietly into obscurity.\nVersions have since been posted for the amusement of the Internet at large.\n\n\"Richard Sexton:\" I posted a few BIFF articles from my account at gryphon, but now there are about 20 people, well connected, posting BIFF forgeries. You probably don't want to hear that the people doing them had to find something to occupy themselves after they dissolved the backbone cabal.\n\nBIFF served as a satire of an exceptional behaviour in a fairly homogeneous environment.\n\nThe explosive growth of the web led to the rapid decline of BIFF, as Biffisms became no longer exceptional and the Internet rapidly became a much larger and much more heterogeneous environment, leading both to newcomers not being aware of BIFF's existence and to BIFF becoming less contrasting as an exception in the increasingly \"noisy\" Internet.\n\nThis was posted on the Usenet Oracle mailing list digest 775-06:\n\nB1ff also became the name of a type of internet slang that was created in the early days of the Internet by groups who felt they were being watched by government officials or corporations. This was a major step towards full 1337 (Leet); however, they originally had different purposes. B1ff only changes words only enough so a program looking for certain words doesn't find them, whereas 1337 was created to prevent non-1337 humans from reading text.\n\nDuring the early days of internet gaming, a new side of b1ff took a significant rise, when scripts edited the content of instant chat conversations. Today, however, most censoring scripts can compensate for the letter-number replacement. Thus, b1ff has mainly evolved into 1337, a more involved language which, although it does include letter-number replacement, also includes letter mixing (e.g. \"pr0n\"), similar-sound substitution (e.g. \"h4xx\" for \"hack\"), 'building' a letter from multiple characters (e.g. x from ><), and the invention of new words as a substitute for common words.\n\nIn the sense of caricaturing the prototypical pre-adolescent or over-self-confident noob on the Internet, b1ff's legacy lives on, in practice if not in name.\n\nBeyond apparently inspiring the naming and style of the precursor to leet, b1ffisms continue to be seen today in many Internet forums and comment threads, both in direct posts and in deliberate ironic imitation. Every Internet user who has used leetspeek ironically (\"the ph34r!\") or mocked pre-teen gamers or IRCers (\"OMG WTF!!!1eleven11 kthxbye\") is essentially using a modernised b1ff. b1ff, like Eliza Doolittle, appears to be a specifically created yet immediately classic shibboleth of some fundamental human behaviour, on the part of both the parodied and the parodying.\n\n\n"}
{"id": "56556388", "url": "https://en.wikipedia.org/wiki?curid=56556388", "title": "Basic Principles for the Treatment of Prisoners", "text": "Basic Principles for the Treatment of Prisoners\n\nThe Basic Principles for the Treatment of Prisoners were adopted and proclaimed by the General Assembly of the United Nations by resolution 45/111 on 14 December 1990. \n\nArticle 1 protects the human dignity. Article 2 bans discrimination.\n"}
{"id": "6709424", "url": "https://en.wikipedia.org/wiki?curid=6709424", "title": "Cabinet noir", "text": "Cabinet noir\n\nIn France, the cabinet noir (French for \"black room\") was the office where the letters of suspected persons were opened and read by public officials before being forwarded to their destination. However, this had to be done with some sophistication, as it was considered undesirable that the subjects of the practice know about it, and \"that the black chamber not interrupt the smooth running of the postal service.\" This practice had been in vogue since the establishment of posts, and was frequently used by the ministers of Louis XIII and Louis XIV; but it was not until the reign of Louis XV that a separate office for this purpose was created. This was called the \"cabinet du secret des postes\", or more popularly the \"cabinet noir\". Although declaimed against at the time of the French Revolution, it was used both by the revolutionary leaders and by Napoleon.\n\nBy the 1700s, cryptanalysis was becoming industrialized, with teams of government cryptanalysts working together to crack the most complex monoalphabetic ciphers. Each European power had its own so called black chamber, a nerve centre for deciphering messages and gathering intelligence. The most celebrated, disciplined and efficient was the Geheime Kabinettskanzlei in Vienna. It operated according to a strict timetable, because it was vital that its activities should not interrupt the smooth running of the postal service. Letters which were supposed to be delivered to embassies in Vienna were first routed via the black chamber, arriving at 7 am. Secretaries melted seals, and a team of stenographers worked in parallel to make copies of the letters. Within three hours the letters had been resealed and returned to the central post office to be delivered to their intended destination. As well as supplying the emperors of Austria with vital intelligence, the Viennese black chamber sold the information it harvested to other European powers. In 1774, for example, an arrangement was made with Abbot Georgel, the secretary in the French embassy, who had access to a biweekly package on information for 1,000 ducats.\n\nBlack chambers were also employed by the Dutch Republic.\n\nIn 1911, the \"Encyclopædia Britannica\" took the view that the \"cabinet noir\" had disappeared, but that the right to open letters in cases of emergency still appeared to be retained by the French government; and a similar right was occasionally exercised in England under the direction of a Secretary of State. In England, this power was frequently employed during the eighteenth century and was confirmed by the Post Office Act 1837; its most notorious use was, perhaps, the opening of Mazzini's letters in 1844.\n\nSuch postal censorship became common during World War I. Governments claimed that the total war which was waged required such censorship to preserve the civilian population's morale from heart-breaking news up from the front. Whatever the justification, this meant that not a single letter sent from a soldier to his family escaped previous reading by a government official, destroying any notion of privacy or of secrecy of correspondence. Post censorship was retained during the interwar period and afterwards, but without being done on such a massive scale.\n\nThe opening of international mail outgoing and incoming from the United States by US Customs under the Trade Act of 2002 occurs under the border search exception to the Fourth Amendment. This practice has had some criticism (including allegations that it adds to the expense of conducting the Postal Service and can thus affect postage rates), of which the USPS apparently informed Congress before passage of the legislation. However, this criticism may be tempered by the fact that the act prohibits agents searching for contraband from reading mail incidentally included in the package or envelope including it, or allowing others to read it. The Intelligence Authorization Act of 2004 has also been characterized as unconstitutionally permitting the opening of domestic mail.\n\n\n"}
{"id": "6602738", "url": "https://en.wikipedia.org/wiki?curid=6602738", "title": "Carian alphabets", "text": "Carian alphabets\n\nThe Carian alphabets are a number of regional scripts used to write the Carian language of western Anatolia. They consisted of some 30 alphabetic letters, with several geographic variants in Caria and a homogeneous variant attested from the Nile delta, where Carian mercenaries fought for the Egyptian pharaohs. They were written left-to-right in Caria (apart from the Carian–Lydian city of Tralleis) and right-to-left in Egypt. Carian was deciphered primarily through Egyptian–Carian bilingual tomb inscriptions, starting with John Ray in 1981; previously only a few sound values and the alphabetic nature of the script had been demonstrated. The readings of Ray and subsequent scholars were largely confirmed with a Carian–Greek bilingual inscription discovered in Kaunos in 1996, which for the first time verified personal names, but the identification of many letters remains provisional and debated, and a few are wholly unknown.\n\nThere is a range of graphic variation between cities in Caria, some of which extreme enough to have separate Unicode characters. The Kaunos alphabet is thought to be complete. There may be other letters in Egyptian cities outside Memphis, but they need to be confirmed. The letters with identified values in the various cities are as follows: \nThe Carian scripts, which have a common origin, have long puzzled scholars. Most of the letters resemble letters of the Greek alphabet, but their sound values are generally unrelated to the values of the Greek letters. This is unusual among the alphabets of Asia Minor, which generally approximate the Greek alphabet fairly well, both in sound and shape, apart from sounds which had no equivalent in Greek. However, the Carian sound values are not completely disconnected: 𐊠 /a/ (Greek Α), 𐊫 /o/ (Greek Ο), 𐊰 /s/ (Greek Ϻ san), and 𐊲 /u/ (Greek Υ) are as close to Greek as any Anatolian alphabet, and 𐊷, which resembles Greek Β, has the similar sound /p/, which it shares with Greek-derived Lydian 𐤡.\n\nAdiego (2007) therefore suggests that the original Carian script was adopted from cursive Greek, and that it was later restructured, perhaps for monumental inscription, by imitating the form of the most graphically similar Greek print letters without considering their phonetic values. Thus a /t/, which in its cursive form may have had a curved top, was modeled after Greek \"qoppa\" (Ϙ) rather than its ancestral \"tau\" (Τ) to become 𐊭. Carian /m/, from archaic Greek 𐌌, would have been simplified and was therefore closer in shape to Greek Ν than Μ when it was remodeled as 𐊪. Indeed, many of the regional variants of Carian letters parallel Greek variants: 𐊥 𐅝 are common graphic variants of digamma, 𐊨 ʘ of theta, 𐊬 Λ of both gamma and lambda, 𐌓 𐊯 𐌃 of rho, 𐊵 𐊜 of phi, 𐊴 𐊛 of chi, 𐊲 V of upsilon, and 𐋏 𐊺 parallel Η 𐌇 eta. This could also explain why one of the rarest letters, 𐊱, has the form of one of the most common Greek letters. However, no such proto-Carian cursive script is attested, so these etymologies are speculative.\n\nFurther developments occurred within each script; in Kaunos, for example, it would seem that 𐊮 /š/ and 𐊭 /t/ both came to resemble a Latin P, and so were distinguished with an extra line in one: 𐌓 /t/, 𐊯 /š/.\n\nNumerous attempts at deciphering the Carian inscriptions were made during the 20th century. After World War II, most of the known Carian inscriptions were collected and published, which provided good basis for decipherment.\n\nIn the 1960s the Russian researcher Vitaly Shevoroshkin showed that earlier assumptions that the script was a syllabic or semisyllabic writing system was false. He devoted many years to his study, and used proper methodology. He made it clear that Carian was indeed alphabetically written, but made few significant advances in the understanding of the language. He took the values of letters resembling those of the Greek alphabet for granted, which proved to be unfounded.\n\nOther researchers of Carian were H. Stoltenberg, O. Masson, Yuri Otkupshchikov, P. Meriggi (1966), and R. Gusmani (1975), but their work was not widely accepted.\n\nStoltenberg, like Shevoroshkin, and most others, generally attributed Greek values to Carian symbols.\n\nIn 1972, an Egyptologist K. Zauzich investigated bilingual texts in Carian and Egyptian (what became known as 'Egyptian approach'). This was an important step in decipherment, that produced good results.\n\nThis method was further developed by T. Kowalski in 1975, which was his only publication on the subject.\n\nThe British Egyptologist John D. Ray apparently worked independently from Kowalski; nevertheless he produced similar results (1981, 1983). He used Carian–Egyptian bilingual inscriptions that had been neglected until then. His big breakthrough was the reading of the name Psammetichus (Egyptian Pharaoh) in Carian.\n\nThe radically different values that Ray assigned to the letters initially met with scepticism. Ignasi-Xavier Adiego, along with Diether Schürr, started to contribute to the project in the early 1990s. In his 1993 book \"Studia Carica\", Adiego offered the decipherment values for letters that are now known as the ‘Ray-Schürr-Adiego system’. This system now gained wider acceptance. The discovery of a new bilingual inscription in 1996 (the Kaunos Carian-Greek bilingual inscription) confirmed the essential validity of their decipherment.\n\nCarian was added to the Unicode Standard in April, 2008 with the release of version 5.1.\nIt is encoded in Plane 1 (Supplementary Multilingual Plane).\n\nThe Unicode block for Carian is U+102A0–U+102DF:\n𐊡𐋊𐋋𐋌𐋍 are graphic variants, as are 𐊤𐋈𐋐, 𐋎𐊦𐋏, 𐊺𐋏, 𐊼𐊽, 𐋂𐋃, 𐋁𐋀, and possibly 𐋇𐊶.\n\n\n"}
{"id": "39304308", "url": "https://en.wikipedia.org/wiki?curid=39304308", "title": "Chữ khoa đẩu", "text": "Chữ khoa đẩu\n\nChữ khoa đẩu is a doubtful obsolete script for the Vietnamese language. It is supposed to have been used in the Hồng Bàng period, and it is believed to have disappeared later during the Chinese domination of Vietnam. However, it is note-worthy that the term \"chữ khoa đẩu\" itself comes from \"tadpole script\" () in Chinese texts produced in Vietnam while the ones produced elsewhere refers to \"kēdǒu zì\" as variants of the ancient Chinese characters.\n\n\"Chữ khoa đẩu\" is also considered a distinct Muong language script similar to Thai alphabet and Lao alphabet. The script has 30 basic consonant signs. In 2013, a book by Đỗ Văn Xuyền was published in which he claimed to have deciphered \"chữ khoa đẩu\" used by the Vietnamese ancient. However, Xuyền's claim, like the earlier ones by Bửu Cầm or Lê Trọng Khánh, lacks of scientific basis, and the script is indeed based on the Brahmic scripts used by Tai peoples in Vietnam.\n"}
{"id": "58267", "url": "https://en.wikipedia.org/wiki?curid=58267", "title": "Conceptual schema", "text": "Conceptual schema\n\nA 'conceptual schema' is a high-level description of a business's informational needs. It typically includes only the main concepts and the main relationships among them. Typically this is a first-cut model, with insufficient detail to build an actual database. This level describes the structure of the whole database for a group of users. The conceptual model is also known as the data model that can be used to describe the conceptual schema when a database system is implemented. It hides the internal details of physical storage and targets on describing entities, datatype, relationships and constraints.\n\nA conceptual schema or conceptual data model is a map of concepts and their relationships used for databases. This describes the semantics of an organization and represents a series of assertions about its nature. Specifically, it describes the things of significance to an organization (\"entity classes\"), about which it is inclined to collect information, and characteristics of (\"attributes\") and associations between pairs of those things of significance (\"relationships\").\n\nBecause a conceptual schema represents the semantics of an organization, and not a database design, it may exist on various levels of abstraction. The original ANSI four-schema architecture began with the set of \"external schema\" that each represent one person's view of the world around him or her. These are consolidated into a single \"conceptual schema\" that is the superset of all of those external views. A data model can be as concrete as each person's perspective, but this tends to make it inflexible. If that person's world changes, the model must change. Conceptual data models take a more abstract perspective, identifying the fundamental things, of which the things an individual deals with are just examples.\n\nThe model does allow for what is called inheritance in object oriented terms. The set of instances of an entity class may be subdivided into entity classes in their own right. Thus, each instance of a \"sub-type\" entity class is also an instance of the entity class's \"super-type\". Each instance of the super-type entity class, then is also an instance of one of the sub-type entity classes.\n\nSuper-type/sub-type relationships may be \"exclusive\" or not. A methodology may require that each instance of a super-type may \"only\" be an instance of \"one\" sub-type. Similarly, a super-type/sub-type relationship may be \"exhaustive\" or not. It is exhaustive if the methodology requires that each instance of a super-type \"must be\" an instance of a sub-type. A sub-type named other is often necessary.\n\n\nA data structure diagram (DSD) is a data model or diagram used to describe conceptual data models by providing graphical notations which document entities and their relationships, and the constraints that bind them.\n\n\n\n"}
{"id": "2419303", "url": "https://en.wikipedia.org/wiki?curid=2419303", "title": "Control moment gyroscope", "text": "Control moment gyroscope\n\nA control moment gyroscope (CMG) is an attitude control device generally used in spacecraft attitude control systems. A CMG consists of a spinning rotor and one or more motorized gimbals that tilt the rotor’s angular momentum. As the rotor tilts, the changing angular momentum causes a gyroscopic torque that rotates the spacecraft.\n\nCMGs differ from reaction wheels. The latter apply torque simply by changing rotor spin speed, but the former tilt the rotor's spin axis without necessarily changing its spin speed. CMGs are also far more power efficient. For a few hundred watts and about 100 kg of mass, large CMGs have produced thousands of newton meters of torque. A reaction wheel of similar capability would require megawatts of power.\n\nThe most effective CMGs include only a single gimbal. When the gimbal of such a CMG rotates, the change in direction of the rotor's angular momentum represents a torque that reacts onto the body to which the CMG is mounted, e.g. a spacecraft. Except for effects due to the motion of the spacecraft, this torque is due to a constraint, so it does no mechanical work (i.e., requires no energy). Single-gimbal CMGs exchange angular momentum in a way that requires very little power, with the result that they can apply very large torques for minimal electrical input.\n\nSuch a CMG includes two gimbals per rotor. As an actuator, it is more versatile than a single-gimbal CMG because it is capable of pointing the rotor's momentum vector in any direction. However, the torque generated by one gimbal's motion must often be reacted by the other gimbal on its way to the spacecraft, requiring more power for a given torque than a single-gimbal CMG. If the goal is simply to store momentum in a mass-efficient way, as in the case of the International Space Station, dual-gimbal CMGs are a good design choice. However, if a spacecraft instead requires large output torque while consuming minimal power, single-gimbal CMGs are a better choice.\n\nMost CMGs hold rotor speed constant using relatively small motors to offset changes due to dynamic coupling and non-conservative effects. Some academic research has focused on the possibility of increasing and decreasing rotor speed while the CMG gimbals. Variable-speed CMGs (VSCMGs) offer few practical advantages when considering actuation capability because the output torque from the rotor is typically much smaller than that caused by the gimbal motion. The primary practical benefit of the VSCMG when compared to the conventional CMG is an additional degree of freedom—afforded by the available rotor torque—which can be exploited for continuous CMG singularity avoidance and VSCMG cluster reorientation. Research has shown that the rotor torques required for these two purposes are very small and within the capability of conventional CMG rotor motors. Thus, the practical benefits of VSCMGs are readily available using conventional CMGs with alterations to CMG cluster steering and CMG rotor motor control laws. The VSCMG also can be used as a mechanical battery to store electric energy as kinetic energy of the flywheels.\n\nAt least three single-axis CMGs are necessary for control of spacecraft attitude. However, no matter how many CMGs a spacecraft uses, gimbal motion can lead to relative orientations that produce no usable output torque along certain directions. These orientations are known as \"singularities\" and are related to the kinematics of robotic systems that encounter limits on the end-effector velocities due to certain joint alignments. Avoiding these singularities is naturally of great interest, and several techniques have been proposed. David Bailey and others have argued (in patents and in academic publications) that merely avoiding the \"divide by zero\" error that is associated with these singularities is sufficient. Two more recent patents summarize competing approaches. See also: Gimbal Lock.\n\nA cluster of CMGs can become saturated, in the sense that it is holding a maximum amount of angular momentum in a particular direction and can hold no more.\n\nAs an example, suppose a spacecraft equipped with two or more dual-gimbal CMGs experiences a transient unwanted torque, perhaps caused by reaction from venting waste gas, tending to make it roll clockwise about its forward axis and thus increase its angular momentum along that axis. Then the CMG control program will command the gimbal motors of the CMGs to slant the rotors' spin axes gradually more and more forward, so that the angular momentum vectors of the rotors point more nearly along the forward axis. While this gradual change in rotor spin direction is in progress, the rotors will be creating gyroscopic torques whose resultant is anticlockwise about the forward axis, holding the spacecraft steady against the unwanted waste gas torque.\n\nWhen the transient torque ends, the control program will stop the gimbal movement, and the rotors will be left pointing more forward than before. The inflow of unwanted forward angular momentum has been routed through the CMGs and dumped into the rotors; the forward component of their total angular momentum vector is now greater than before.\n\nIf these events are repeated, the angular momentum vectors of the individual rotors will bunch more and more closely together round the forward direction. In the limiting case, they will all end up parallel, and the CMG cluster will now be saturated in that direction; it can hold no more angular momentum. If the CMGs were initially holding no angular momentum about any other axes, they will end up saturated exactly along the forward axis. If however (for example) they were already holding a little angular momentum in the \"up\" (yaw left) direction, they will saturate (end up parallel) along an axis pointing forward and slightly up, and so on. Saturation is possible about any axis.\n\nIn the saturated condition attitude control is impossible. Since the gyroscopic torques can now only be created at right angles to the saturation axis, roll control about that axis itself is now non-existent. There will also be major difficulties with control about other axes. For example, an unwanted left yaw can only be countered by storing some \"up\" angular momentum in the CMG rotors. This can only be done by tilting at least one of their axes up, which will slightly reduce the forward component of their total angular momentum. Since they can now store less \"right roll\" forward angular momentum, they will have to release some back into the spacecraft, which will be forced to start an unwanted roll to the right.\n\nThe only remedy for this loss of control is to desaturate the CMGs by removing the excess angular momentum from the spacecraft. The simplest way of doing this is to use Reaction Control System (RCS) thrusters. In our example of saturation along the forward axis, the RCS will be fired to produce an anticlockwise torque about that axis. The CMG control program will then command the rotor spin axes to begin fanning out away from the forward direction, producing gyroscopic torques whose resultant is clockwise about the forward direction, opposing the RCS as long as it is still firing and holding the spacecraft steady. This is continued until a suitable amount of forward angular momentum has been drained out of the CMG rotors; it is transformed into the moment of momentum of the moving matter in the RCS thruster exhausts and carried away from the spacecraft.\n\nIt is worth noting that \"saturation\" can only apply to a cluster of two or more CMGs, since it means that their rotor spins have become parallel. It is meaningless to say that a single constant-speed CMG can become saturated; in a sense it is \"permanently saturated\" in whatever direction the rotor happens to be pointing. This contrasts with a single reaction wheel, which can absorb more and more angular momentum along its fixed axis by spinning faster, until it reaches saturation at its maximum design speed.\n\nThere are other undesirable rotor axis configurations apart from saturation, notably anti-parallel alignments. For example, if a spacecraft with two dual-gimbal CMGs gets into a state in which one rotor spin axis is facing directly forward, while the other rotor spin is facing directly aft (i.e. anti-parallel to the first), then all roll control will be lost. This happens for the same reason as for saturation; the rotors can only produce gyroscopic torques at right angles to their spin axes, and here these torques will have no fore-and-aft components and so no influence on roll. However, in this case the CMGs are not saturated at all; their angular momenta are equal and opposite, so the total stored angular momentum adds up to zero. Just as for saturation, however, and for exactly the same reasons, roll control will become increasingly difficult if the CMGs even approach anti-parallel alignment.\n\nIn the anti-parallel configuration, although roll control is lost, control about other axes still works well (in contrast to the situation with saturation). An unwanted left yaw can be dealt with by storing some \"up\" angular momentum, which is easily done by tilting both rotor spin axes slightly up by equal amounts. Since their fore and aft components will still be equal and opposite, there is no change in fore-and-aft angular momentum (it will still be zero) and therefore no unwanted roll. In fact the situation will be improved, because the rotor axes are no longer quite anti-parallel and some roll control will be restored.\n\nAnti-parallel alignment is therefore not quite as serious as saturation but must still be avoided. It is theoretically possible with any number of CMGs; as long as some rotors are aligned parallel along a particular axis, and all the others point in exactly the opposite direction, there is no saturation but still no roll control about that axis. With three or more CMGs the situation can be immediately rectified simply by redistributing the existing total angular momentum among the rotors (even if that total is zero). In practice the CMG control program will continually redistribute the total angular momentum to avoid the situation arising in the first place.\n\nIf there are only two CMGs in the cluster, as in our first example, then anti-parallel alignment will inevitably occur if the total stored angular momentum reaches zero. The remedy is to keep it away from zero, possibly by using RCS firings. This is not very satisfactory, and in practice all spacecraft using CMGs are fitted with at least three. However it sometimes happens that after malfunctions a cluster is left with only two working CMGs, and the control program must be able to deal with this situation.\n\nOlder CMG models like the ones launched with Skylab in 1973 had limited gimbal travel between fixed mechanical stops. On the Skylab CMGs the limits were plus or minus 80 degrees from zero for the inner gimbals, and from plus 220 degrees to minus 130 degrees for the outer ones (so zero was offset by 45 degrees from the centre of travel). Visualising the inner angle as 'latitude' and the outer as 'longitude', it can be seen that for an individual CMG there were 'blind spots' with radius 10 degrees of latitude at the 'North and South poles', and an additional 'blind strip' of width 10 degrees of 'longitude' running from pole to pole, centred on the line of 'longitude' at plus 135 degrees. These 'blind areas' represented directions in which the rotor's spin axis could never be pointed.\n\nSkylab carried three CMGs, mounted with their casings (and therefore their rotor axes when the gimbals were set to zero) facing in three mutually perpendicular directions. This ensured that the six 'polar blind spots' were spaced 90 degrees apart from each other. The 45 degree zero offset then ensured that the three 'blind strips' of the outer gimbals would pass halfway between neighbouring 'polar blind spots' and at a maximum distance from each other. The whole arrangement ensured that the 'blind areas' of the three CMGs never overlapped, and thus that at least two of the three rotor spins could be pointed in any given direction.\n\nThe CMG control program was responsible for making sure that the gimbals never hit the stops, by redistributing angular momentum between the three rotors to bring large gimbal angles closer to zero. Since the total angular momentum to be stored had only three degrees of freedom, while the control program could change six independent variables (the three pairs of gimbal angles), the program had sufficient freedom of action to do this while still obeying other constraints such as avoiding anti-parallel alignments.\n\nOne advantage of limited gimbal movement such as Skylab's is that singularities are less of a problem. If Skylab's inner gimbals had been able to reach 90 degrees or more away from zero, then the 'North and South poles' could have become singularities; the gimbal stops prevented this.\n\nMore modern CMGs such as the four units installed on the ISS in 2000 have unlimited gimbal travel and therefore no 'blind areas'. Thus they do not have to be mounted facing along mutually perpendicular directions; the four units on the ISS all face the same way. The control program need not concern itself with gimbal stops, but on the other hand it must pay more attention to avoiding singularities.\n\nSkylab, launched in May 1973, was the first spacecraft to be fitted with large CMGs for attitude control.\n\nCMGs were used for attitude control on the Salyut and Mir space stations, where they were called gyrodynes (from the Russian гиродин \"girodin\"; this word is also sometimes used – especially by Russian crew – for the CMGs on the ISS). They were first tested on Salyut 3 in 1974, and introduced as standard components from Salyut 6 onwards. The completed Mir station had twelve gyrodynes altogether, starting with six in the pressurised interior of the Kvant-1 module. These were later supplemented by another six on the unpressurised outside of Kvant-2. According to NPO Energia, putting them outside turned out to be a mistake, as it made gyrodyne replacement much more difficult.\n\nThe ISS employs a total of four CMGs, mounted on Z1 truss as primary actuating devices during normal flight mode operation. The objective of the CMG flight control system is to hold the space station at a fixed attitude relative to the surface of the Earth. In addition, it seeks a Torque Equilibrium Attitude (TEA), in which the combined torque contribution of gravity gradient, atmospheric drag, solar pressure, and geomagnetic interactions are minimized. In the presence of these continual environmental disturbances CMGs absorb momentum in an attempt to maintain the space station at a desired attitude. The CMGs will eventually saturate (absorbing momentum to the point where they can absorb no more), resulting in loss of effectiveness of the CMG array for control. Some kind of momentum management scheme (MMS) is necessary to allow the CMGs to hold a desired attitude and at the same time prevent CMG saturation. Since the CMGs are momentum-exchange devices, external control torques must be used to desaturate the CMGs, that is, bring the momentum back to nominal value. Some methods for unloading CMG momentum include the use of magnetic torques, reaction thrusters, and gravity gradient torque. For the space station, the gravity gradient torque approach is preferred because it requires no consumables or external hardware and because the gravity-gradient torque on the ISS can be very high. CMG saturation has been observed during spacewalks, requiring propellant to be used to maintain desired attitude. In 2006 and 2007, CMG-based experiments demonstrated the viability of zero-propellant maneuvers to adjust attitude of the ISS 90° and 180°. By 2016, four Soyuz undockings had been done using CMG-based attitude adjustment, resulting in considerable propellant savings.\n\nAs of 2016, the Russian Orbital Segment of the ISS carries no CMGs of its own. However, the proposed but as yet unbuilt Science and Power Module (NEM-1) would be fitted with several externally-mounted CMGs. NEM-1 would be installed on one of the lateral ports of the small UM or Nodal Module scheduled for completion and launch at some time within the 2016–25 Russian programme. Its twin NEM-2 (if completed) would later be installed symmetrically on the other lateral UM port.\n\nOn 24 February 2015, the Scientific and Technical Council of Roscosmos announced that after decommissioning of the ISS (then planned for 2024) the newer Russian modules would be detached and form the nucleus of a small all-Russian space station to be called OPSEK. If this plan is carried out, the CMGs on NEM-1 (and NEM-2, if built) would provide attitude control for the new Russian station.\n\n\nCMG applications and fundamental research are undertaken at several institutions.\n"}
{"id": "14285414", "url": "https://en.wikipedia.org/wiki?curid=14285414", "title": "Correlative rights doctrine", "text": "Correlative rights doctrine\n\nThe correlative rights doctrine is a legal doctrine limiting the rights of landowners to a common source of groundwater (such as an aquifer) to a reasonable share, typically based on the amount of land owned by each on the surface above. This doctrine is also applied to oil and gas in some U.S. states.\n\nUnder California law, the owners of overlying land own the subsurface water as tenants in common, and each is allowed a reasonable amount for his/her own use.\n\n\n"}
{"id": "1657953", "url": "https://en.wikipedia.org/wiki?curid=1657953", "title": "Dacke War", "text": "Dacke War\n\nThe Dacke War () was a peasant uprising led by Nils Dacke in Småland, Sweden, in 1542 against the rule of Gustav Vasa. Dacke and his followers were dissatisfied with the heavy tax burden, the introduction of Lutheranism and the confiscation of Church property (the confiscation and taxes were introduced to pay for the Swedish War of Liberation that had brought Gustav Vasa to power). In 1543 the uprising was defeated, and Nils Dacke was killed.\n\nNils Dacke and his peasants were dissatisfied with the policies of the Swedish king Gustav Vasa. In his effort to modernize Sweden and gain more power, the king had instituted a more efficient system for tax collection. The heavy tax burden angered many peasants.\n\nGustav Vasa had also broken relations with Rome and promoted Lutheranism instead of Catholicism in order to confiscate the property of the church (including land), effected by the laws of the Reduction of Gustav I of Sweden. In 1541 the king's men had confiscated many of the belongings of the churches in Småland, such as the church silver and even the church bells, to finance the army. Dacke criticized the new church order and promoted the old faith. He was also supported by many local priests.\n\nThe rebellion was one of many rebellions during the rule of Gustav Vasa. In contrast with other contemporary rebellions in Sweden, this one was led by peasants, and not supported by the local nobility.\n\nThe uprising began in the summer of 1542 when the king's bailiffs were attacked and killed when they came to collect taxes. Gustav Vasa responded by sending a military force led by his own father-in-law . He was defeated by Dacke's constantly growing army of peasants. Other attempts to defeat Dacke militarily also failed.\n\nNext, the Swedish government stopped all supplies of provisions and other necessities to the region. This weakened the rebellions considerably. Defaming propaganda about Dacke was also spread by the government, labeling him a traitor and a heretic.\n\nIn March 1543 Gustav Vasa ordered his army of Swedish recruits and German landsknecht mercenaries to attack Småland. This time larger forces were deployed, and Dacke's forces were attacked from two directions - from Östergötland and Västergötland. The uprising was defeated, and Dacke was wounded but managed to flee.\n\nThe king's revenge on the instigators of the rebellion was hard. The leaders that were caught were executed together with the priests who had supported Dacke. Peasants who had supported the rebellion were deported to Finland, where they had to serve in the army, and the counties where the rebellion had taken place had to pay a large fine to the king.\n\nDacke himself was caught and killed in August 1543 when trying to escape the country. According to legend, his body was taken to Kalmar, where his head was publicly displayed wearing a crown of copper, as a warning to others.\n\nThe rebellion was the most serious threat to the rule of Gustav Vasa, but after having defeated it he managed to consolidate his power, concentrating more and more power in the hands of the monarch.\n\nIn the Swedish language, the idiom \"[something] hasn't happened since the Dacke War\" is used to mean \"[something] hasn't happened for a long time\". This expression is especially common in the southern parts of Sweden but is also used elsewhere.\n\n\n"}
{"id": "50417566", "url": "https://en.wikipedia.org/wiki?curid=50417566", "title": "Data Re-Identification", "text": "Data Re-Identification\n\nData Re-Identification is the practice of matching anonymous data (also known as de-identified data) with publicly available information, or auxiliary data, in order to discover the individual to which the data belongs to. This is a concern because companies with privacy policies, health care providers, and financial institutions may release the data they collect after the data has gone through the de-identification process. The de-identification process involves masking, generalizing or deleting both direct and indirect identifiers; the definition of this process is not universal, however. Information in the public domain, even seemingly anonymized, may thus be re-identified in combination with other pieces of available data and basic computer science techniques. The Common Rule Agencies, a collection of multiple U.S. federal agencies and departments including the U.S. Department of Health and Human Services, speculate that re-identification is becoming gradually easier because of \"big data\" - the abundance and constant collection and analysis of information along the evolution of technologies and the advances of algorithms. However, others have claimed that de-identification is a safe and effective data liberation tool and do not view re-identification as a concern.\n\nA 2000 study found that 87 percent of the U.S. population can be identified using a combination of their gender, birthdate and zip code. Others do not think that re-identification is a serious threat, and call it a \"myth\"; they claim that the combination of zip code, date of birth and gender is rare or partially complete, such as only the year and month birth without the date, or the county name instead of the specific zip code, thus the risk of such re-identification is reduced in many instances.\n\nExisting privacy regulations typically protect information that has been modified, so that the data is deemed anonymized, or de-identified. For financial information, the Federal Trade Commission permits its circulation if it is de-identified and aggregated. The Gramm Leach Bliley Act (GLBA), which mandates financial institutions give consumers the opportunity to opt out of having their information shared with third parties, does not cover de-identified data if the information is aggregate and does not contain personal identifiers, since this data is not treated as personally identifiable information.\n\nIn terms of university records, authorities both on the state and federal level have shown an awareness about issues of privacy in education and a distaste for institutions' disclosure of information. The U.S. Department of Education has provided guidance about data discourse and identification, instructing educational institutions to be sensitive to the risk of re-identification of anonymous data by cross-referencing with auxiliary data, to minimize the amount of data in the public domain by decreasing publication of directory information about students and institutional personnel, and to be consistent in the processes of de-identification.\n\nMedical information of patients are becoming increasingly available on the Internet, on free and publicly accessing platforms such as HealthData.gov and PatientsLikeMe, encouraged by government open data policies and data-sharing initiatives spearheaded by the private sector. While this level of accessibility yields many benefits, concerns regarding discrimination and privacy have been raised. Protections on medical records and consumer data from pharmacies are stronger compared to those for other kinds of consumer data. The Health Insurance Portability and Accountability Act (HIPAA) protects the privacy of identifiable data about health, but authorize information release to third parties if de-identified. In addition, it mandates that patients receive breach notifications should there be more than a low probability that the patient's information was inappropriately disclosed or utilized without sufficient mitigation of the harm to him or her. The likelihood of re-identification is a factor in determining the probability that the patient's information has been compromised. Commonly, pharmacies sell de-identified information to data mining companies that sell to pharmaceutical companies in turn.\nThere have been state laws enacted to ban data mining of medical information, but they were struck down by federal courts in Maine and New Hampshire on First Amendment grounds. Another federal court on another case used \"illusive\" to describe concerns about privacy of patients and did not recognize the risks of re-identification.\n\nThe Notice of Proposed Rule Making, published by the Common Rule Agencies in September 2015, expanded the umbrella term of \"human subject\" in research to include biospecimens, or materials taken from the human body - blood, urine, tissue etc. This mandates that researchers using biospecimens must follow the stricter requirements of doing researcher with human subjects. The rationale for this is the increased risk of re-identification of biospecimen. The final revisions affirmed this regulation.\n\nThere have been a sizable amount of successful attempts of re-identification in different fields. Even if it is not easy for a lay person to break anonymity, once the steps to do so are disclosed and learnt, there is no need for higher level knowledge to access information in a database. Sometimes, technical expertise is not even needed if a population has a unique combination of identifiers.\n\nIn the mid 1990s, a government agency in Massachusetts called Group Insurance Commission (GIC), which purchased health insurance for employees of the state, decided to release records of hospital visits to any researcher who requested the data, at no cost. GIC assured that the patient's privacy was not a concern since it had removed identifiers such as name, addresses, social security numbers. However, information such as zip codes, birth date and sex remained untouched. The GIC assurance was reinforced by the then governor of Massachusetts, William Weld. Latanya Sweeney, a graduate student at the time, put her mind to picking out the governor's records in the GIC data. By combining the GIC data with the voter database of the city Cambridge, which she purchased for 20 dollars, Governor Weld's record was discovered with ease.\n\nIn 1997, a researcher successfully de-anonymized medical records using voter databases.\n\nIn 2001, Professor Latanya Sweeney again successfully matched anonymized hospital visit records in the state of Washington to individual persons using the state's voting records 43% of the time.\n\nThere are existing algorithms used to re-identify patient with prescription drug information.\n\nTwo researchers at the University of Texas, Arvind Narayanan and Professor Vitaly Shmatikov, were able to re-identity some portion of anonymized Netflix movie-ranking data with individual consumers on the streaming website. The data was released by Netflix 2006 after de-identification, which consisted of replacing individual names with random numbers and moving around personal details. The two researchers de-anonymized some of the data by comparing it with non-anonymous IMDb (Internet Movie Database) users’ movie ratings. Very little information from the database, it was found, was needed to identify the subscriber. In the resulting research paper, there were startling revelations of how easy it is to re-identify Netflix users. For example, simply knowing data about only two movies a user has reviewed, including the precise rating and the date of rating give or take three days allows for 68% re-identification success.\n\nIn 2006, after AOL published its users' search queries, data that was anonymized prior to the public release, New York Times reporters successfully carried out re-identification of individuals by taking groups of searches made by anonymized users. AOL had attempted to suppress identifying information, including usernames and IP addresses, but had replaced these with unique identification numbers to preserve the utility of this data for researchers. Bloggers, after the release, pored over the data, either trying to identify specific users with this content, or to point out entertaining, depressing, or shocking search queries, examples of which include \"how to kill you wife,\" \"depression and medical leave,\" \"car crash photos.\" Two reporters, Michael Barbro and Tom Zeller, were able to track down a 62 year old widow named Thelma Arnold from recognizing clues to the identity of User 417729 search histories. Arnold acknowledged that she was the author of the searches, confirming that re-identification is possible.\n\nThe individuals whose data is re-identified is also at risk of having their information, with their identity attached it, sold to organizations they do not want possessing private information about their finances, health or preferences. The release of this data may cause anxiety, shame or embarrassment. Once an individual's privacy has been breached as a result of re-identification, future breaches become much easier: once an link is made between one piece of data and a person's real identity, any association between the data and an anonymous identity breaks anonymity of the person.\n\nRe-identification may expose companies which have pledged to assure anonymity to increased liability to contract or to tort and cause them to violate their privacy policies by having released information to third parties that can identify users after re-identification. Not only will they violate internal policies, institutions may also violate state and federal laws, such laws concerning financial confidentiality or medical privacy.\n\nTo address the drawbacks of re-identification, there have been multiple proposals put forward:\n\nWhile a complete ban on re-identification has been urged, the enforcement of this is impossible. However, there are ways for lawmakers to combat and punish re-identification efforts, if and when they are exposed:pair a ban with harsher penalties and stronger enforcement by the Federal Trade Commission and the Federal Bureau of Investigation, grant victims of re-identification a right of action against those who re-identify them, mandate software audit trails for people who utilize and analyze anonymized data. A small-scale re-identification ban may also be imposed on trusted recipients of particular databases, such as government data miners. This ban would be much easier to enforce and may discourage re-identification in other spheres and in the future.\n\n"}
{"id": "4488653", "url": "https://en.wikipedia.org/wiki?curid=4488653", "title": "Five Virtues", "text": "Five Virtues\n\nIn Sikhism, the Five Virtues are fundamental qualities which one should develop in order to reach Mukti, or to reunite or merge with God. The Sikh Gurus taught that these positive human qualities were Sat (truth), Daya (compassion), Santokh (contentment), Nimrata (humility), and Pyaar (love).\n\nSat is the virtue of truthful living, which means practising \"righteousness, honesty, justice, impartiality and fair play.\"\n\nSantokh, or contentment, is freedom \"from ambition, envy, greed and jealousy. Without contentment, it is impossible to acquire peace of mind.\"\n\nThe exercise of Daya, or compassion, involves \"considering another's difficulty or sorrow as one's own and helping to relieve it as far as possible. Compassion also includes the overlooking of imperfections and mistakes of others, for to err is human.\"\n\nNimrata, translated as \"humility\", \"benevolence\" or \"humbleness\", is the fourth virtue.\n\nPyaar requires Sikhs to be filled with the love of God.\n"}
{"id": "31982810", "url": "https://en.wikipedia.org/wiki?curid=31982810", "title": "Geometrical-optical illusions", "text": "Geometrical-optical illusions\n\nGeometrical-optical illusions are visual illusions, also optical illusions, in which the geometrical properties of what is seen differ from those of the corresponding objects in the visual field.\n\nIn studying geometry one concentrates on the position of points and on the length, orientation and curvature of lines. Geometrical-optical illusions then relate in the first instance to object characteristics as defined by geometry. Though vision is three-dimensional, in many situations depth can be factored out and attention concentrated on a simple view of a two-dimensional tablet with its x and y co-ordinates. '\n\nWhereas their counterparts in the observer's object space are public and have measurable properties, the illusions themselves are private to the observer's (human or animal) experience. Nevertheless, they are accessible to portrayal by verbal and other communication and even to measurement by psychophysics. A nulling technique is particularly useful in which a target is deliberately given an opposing deformation in an effort to cancel the illusion.\n\n \nVisual or Optical Illusions can be categorized according to the nature of the difference between objects and percepts. For example, these can be in brightness or color, called \"intensive\" properties of targets, e.g. Mach bands. Or they can be in their location, size, orientation or depth, called \"extensive\". When an illusion involves properties that fall within the purview of geometry it is \"geometrical-optical\", a term given to it in the first scientific paper devoted to the topic by J.J. Oppel, a German high-school teacher, in 1854. It was taken up by Wilhelm Wundt, widely regarded as the founder of experimental psychology, and is now universally used. That by 1972 the first edition of Robinson's book devotes 100 closely printed pages and over 180 figures to these illusions attests to their popularity.\n\nThe easiest to explore are the geometrical-optical illusions that show up in ordinary black and white line drawings. A few examples are drawn from the list of optical illusions. They illustrate illusions of position (Poggendorff illusion), of length (Müller-Lyer illusion), of orientation (Zöllner illusion, Münsterberg illusion or shifted-chess-board illusion and its café wall illusion variant), of rectilinearity or straightness of lines (Hering illusion), of size (Delboeuf illusion) and of vertical/horizontal anisotropy (Vertical-horizontal illusion), in which the vertical extension appears exaggerated.\n\n Visual illusions proper should be distinguished from some related phenomena. Some simple targets such as the Necker Cube are capable of more than one interpretation, which are usually seen in alternation, one at a time. They may be called ambiguous configurations rather than illusion, because what is seen at any time is not actually illusory. The configurations of the Penrose or Escher type are illusory in the sense that only on a detailed logical analysis it becomes apparent that they are not physically realizable. If one thinks of an \"illusion\" as something out there that is misinterpreted, and of a \"delusion\" when a demonstrable substrate is lacking, the distinction breaks down for such effects as the Kanizsa triangle and \"illusory contours.\"\n\nExplanations of geometrical-optical illusion are based on one of two modes of attack: \n\nThe first stage in the operations that transfer information from a visual target in front of an observer into its neural representation in the brain and then allow a percept to emerge, is the imaging by the eye and the processing by the neural circuits in the retina. Some components of geometrical-optical illusions can be ascribed to aberrations at that level. Even if this does not fully account for an illusion, the step is helpful because it puts elaborate mental theories in a more secure place. The moon illusion is a good example. Before invoking concepts of apparent distance and size constancy, it helps to be sure that the retinal image hasn't changed much when the moon looks larger as it descends to the horizon. \nOnce the signals from the retina enter the visual cortex, a host of local interactions are known to take place. In particular, neurons are tuned to target orientation and their response are known to depend on context. The widely accepted interpretation of, e.g. the Poggendorff and Hering illusions as manifestation of expansion of acute angles at line intersections, is an example of successful implementation of a \"bottom-up,\" physiological explanation of a geometrical-optical illusion.\n\nA scientific study will include the recognition that a representation of the visual word is embodied in the state of the organism's nervous system at the time the illusion is experienced. In the discipline of experimental neuroscience, a top-down influence has the meaning that signals originating in higher neural centers, repository of memory traces, innate patterns and decision operations, travel down to lower neuronal circuits where they cause a shift of the excitation balance in the deviated direction. Such a concept is to be distinguished from the bottom-up approach which would look for aberrations that are imposed on the input in its path through the sensory apparatus. Top-down neural signaling would be a fitting implementation of the gestalt concept enunciated by Max Wertheimer that the \"properties of any of the parts are determined by the intrinsic structural laws of the whole.\"\n\nWhen objects and associated percepts, in their respective spaces, correspond to each other albeit with deformations describable in terms of geometry, the mathematically inclined are tempted to search for transformations, perhaps non-Euclidean, that map them on each other. Application of differential geometry has so far not been notably successful ; the variety and complexity of the phenomena, significant differences between individuals and dependence on context, previous experience and instruction set a high bar for satisfying formulations.\n\n\n"}
{"id": "45990", "url": "https://en.wikipedia.org/wiki?curid=45990", "title": "Grandfather paradox", "text": "Grandfather paradox\n\nThe grandfather paradox is a paradox of time travel in which inconsistencies emerge through changing the past. The name comes from the paradox's common description: a person travels to the past and kills their own grandfather before the conception of their father or mother, which prevents the time traveler's existence. Despite its title, the grandfather paradox does not exclusively regard the contradiction of killing one's own grandfather to prevent one's birth. Rather, the paradox regards any action that alters the past, since there is a contradiction whenever the past becomes different from the way it was.\n\nThe grandfather paradox was alluded to in written stories as early as 1929, and in 1931 it was described as \"the age-old argument of preventing your birth by killing your grandparents\" in a letter to American science fiction magazine Amazing Stories. Early science fiction stories dealing with the paradox are the short story \"Ancestral Voices\" by Nathaniel Schachner, published in 1933, and the 1944 book \"Future Times Three\" by René Barjavel, although a number of other works from the 1930s and 1940s touched upon the topic in various degrees of detail.\n\nThe grandfather paradox encompasses any change to the past, and it's presented in many variations. Physicist John Garrison et al. give a variation of the paradox of an electronic circuit which sends a signal through a time machine to shut itself off, and receives the signal before it sends it. An equivalent paradox is known in philosophy as \"autoinfanticide\", going back in time and killing oneself as a baby.\n\nAnother variant of the grandfather paradox is the \"Hitler paradox\" or \"Hitler's murder paradox\", a fairly frequent trope in science fiction, in which the protagonist travels back in time to murder Adolf Hitler before he can instigate World War II and The Holocaust. Rather than necessarily physically preventing time travel, the action removes any \"reason\" for the travel, along with any knowledge that the reason ever existed, thus removing any point in travelling in time in the first place. Additionally, the consequences of Hitler's existence are so monumental and all-encompassing that for anyone born after the war, it is likely that their birth was influenced in some way by its effects, and thus the lineage aspect of the paradox would directly apply in some way.\n\nSome advocate a parallel universe approach to the grandfather paradox. When the time traveller kills their grandfather, they are actually killing a parallel universe version of their grandfather, and the time traveller's original universe is unaltered; it's been argued that since the traveler arrives in a different universe's history and not their own history, this is not \"genuine\" time travel. In other variants, the actions of the time traveller have no effect outside of their own personal experience, as depicted in Alfred Bester's short story \"The Men Who Murdered Mohammed\".\n\nEven without knowing whether time travel to the past is physically possible, it is possible to show using modal logic that changing the past results in a logical contradiction. If it is necessarily true that the past happened in a certain way, then it is false and impossible for the past to have occurred in any other way. A time traveller would not be able to change the past from the way it \"is\", they would only act in a way that is already consistent with what \"necessarily\" happened.\n\nConsideration of the grandfather paradox has led some to the idea that time travel is by its very nature paradoxical and therefore logically impossible. For example, the philosopher Bradley Dowden made this sort of argument in the textbook \"Logical Reasoning\", arguing that the possibility of creating a contradiction rules out time travel to the past entirely. However, some philosophers and scientists believe that time travel into the past need not be logically impossible provided that there is no possibility of changing the past, as suggested, for example, by the Novikov self-consistency principle. Bradley Dowden himself revised the view above after being convinced of this in an exchange with the philosopher Norman Swartz.\n\nConsideration of the possibility of backwards time travel in a hypothetical universe described by a Gödel metric led famed logician Kurt Gödel to assert that time might itself be a sort of illusion. He suggests something along the lines of the block time view in which time is just another dimension like space, with all events at all times being fixed within this 4-dimensional \"block\".\n\nA variation of Everett's many-worlds interpretation (MWI) of quantum mechanics provides a resolution to the grandfather paradox that involves the time traveler arriving in a different universe than the one they came from; it's been argued that since the traveler arrives in a different universe's history and not their own history, this is not \"genuine\" time travel. Stephen Hawking has argued that even if the MWI is correct, we should expect each time traveler to experience a single self-consistent history, so that time travelers remain within their own world rather than traveling to a different one.\n\nBackwards time travel that does not create a grandfather paradox creates a causal loop. The Novikov self-consistency principle expresses one view on how backwards time travel would be possible without the generation of paradoxes. According to this hypothesis, physics in or near closed timelike curves (time machines) can only be consistent with the universal laws of physics, and thus only self-consistent events can occur. Anything a time traveller does in the past must have been part of history all along, and the time traveller can never do anything to prevent the trip back in time from happening, since this would represent an inconsistency. Novikov et al. used the example given by physicist Joseph Polchinski for the grandfather paradox, of a billiard ball heading towards a time machine: the ball's older self emerges from the time machine and strikes its younger self so its younger self never enters the time machine. Novikov et al. showed how this system can be solved in a self-consistent way which avoids the grandfather paradox, though it creates a causal loop. Some physicists suggest that causal loops only exist in the quantum scale, in a fashion similar to the chronology protection conjecture proposed by Stephen Hawking, so histories over larger scales are not looped. Another conjecture, the cosmic censorship hypothesis, suggests that every closed timelike curve passes through an event horizon, which prevents such causal loops from being observed.\n\nSeth Lloyd and other researchers at MIT have proposed an expanded version of the Novikov principle, according to which probability bends to prevent paradoxes from occurring. Outcomes would become stranger as one approaches a forbidden act, as the universe must favor improbable events to prevent impossible ones.\n\nPhysicist David Deutsch has argued that quantum computation with a negative delay—backwards time travel—produces only self-consistent solutions, and the chronology-violating region imposes constraints that are not apparent through classical reasoning. In 2014, researchers published a simulation validating Deutsch's model with photons. Deutsch uses the terminology of \"multiple universes\" in his paper in an effort to express the quantum phenomena, but notes that this terminology is unsatisfactory. Others have taken this to mean that \"Deutschian time travel\" involves multiple universes in order to resolve the grandfather paradox.\nHowever, it was shown in an article by Tolksdorf and Verch that Deutsch's CTC (closed timelike curve) fixed point condition can be fulfilled to arbitrary precision in any quantum system described according to relativistic quantum field theory on spacetimes where CTCs are excluded, casting doubts on whether Deutsch's condition is really characteristic of quantum processes mimicking CTCs in the sense of general relativity. Daniel Greenberger and Karl Svozil proposed that quantum theory gives a model for time travel where the past must be self-consistent.\n\n"}
{"id": "18679103", "url": "https://en.wikipedia.org/wiki?curid=18679103", "title": "Implicit self-esteem", "text": "Implicit self-esteem\n\nImplicit self-esteem refers to a person's disposition to evaluate themselves in a spontaneous, automatic, or unconscious manner. It contrasts with \"explicit self-esteem\", which entails more conscious and reflective self-evaluation. Both explicit and implicit self-esteem are constituents of self-esteem.\n\nImplicit self-esteem has been specifically defined as \"the introspectively unidentified (or inaccurately identified) effect of the self-attitude on evaluation of self-associated and self-dissociated objects\". Because by definition implicit self-esteem may not be accessible to conscious introspection, measures of implicit do not rely on direct self-reports, but rather infer the valence of associations with the self through other means.\n\nThe vast majority of implicit self-esteem measures suggest that an individual's self-evaluation spills over to self-related objects. Also, these measures reveal that people, on average, have positive self-evaluations. The overestimation of one's traits and abilities is argued to be a spillover of positive affect from the self to objects associated with the self. This \"spillover\" is automatic and unconscious. Implicit self-esteem therefore offers an explanation of positivity bias for things related to the self. Associations are especially important; implicit self-esteem is made up of a series of associations between the self and a positive or negative evaluation of the self. This is especially shown in measures of the Implicit Association Test.\n\nSeveral researchers have suggested that levels of implicit self-esteem can be affected by evaluative conditioning, through pairing of construct of the self with positive or negative stimuli, with the objective of altering attitude towards the self. In addition, social comparison, or more specifically the performance of people in one’s close social circle, can also affect implicit self-esteem. This information suggests that expectancies of social inclusion is a factor in self-evaluation.\n\nThe influence of evaluative conditioning on implicit self-esteem is analogous to the principles of classical conditioning on behavioral responses. Although the latter involves pairing an unconditioned stimulus with a neutral stimulus repeatedly until presence of the neutral stimulus evokes the consequence of the unconditioned stimulus, evaluative conditioning involves pairing positive and negative stimulus with an internal construct- the self- to manipulate levels of implicit self-esteem.\n\nThe effectiveness of evaluative conditioning hinges on the understanding that implicit self-esteem is interpersonally associative in nature, and that there is a causal relationship between the self and positive/negative social feedback. Studies have shown that participants repeatedly exposed to pairings of self-relevant information with smiling faces showed enhanced implicit self-esteem.\n\nIn addition, studies have also found that pairing the word ‘I’ with positive traits heightens implicit self-esteem regardless of the level of temporal self-esteem prior to the conditioning process. Subliminal presentation of the stimuli reflected that implicit self-esteem is altered in the absence of consciousness. Given that evaluative conditioning changes attitude at a fundamental level and the evaluation that is automatically activated on encountering the attitude object, implicit self-esteem could be assessed as attitude towards the self.\n\nThe self-evaluation maintenance theory (SEM) suggests that the success of one's partner or \"significant other\" in areas that are self-relevant can cause people to feel threatened, allowing comparison of one’s self to the self of another, impacting self-evaluation. Intimacy of relationships predicts likelihood of upward social comparison, which inevitably leads to lower implicit self-esteem.\n\nGiven that the SEM is moderated by intimacy of relationship, its impact can be a prominent influential factor between romantic partners. Evidence shows that men tend to have lowered implicit self-esteem when their romantic partner succeeded than when they failed, automatically interpreting their romantic partner's success as their own failure. The underlying explanation might be that self-evaluation is driven by one's expectations around fulfillment of one's role as a man. Another explanation in line with the interpersonal nature of self-evaluation stems from the belief that women are attracted to men’s success. Hence, the perception of failure in a man could trigger his fear associated with acceptance from his significant other, as well as abandonment issues. In general, studies of social comparison on implicit self-esteem has yielded the conclusion that comparisons with other individuals can impact one's self-esteem. In addition, these effects are greater when there is a close psychological identification with the partner with whom one is being compared.\n\nAn individual's level of implicit self-esteem affects him or her in various crucial domains that are relevant to social, emotional, and cognitive well-being. In some cases, discrepancies between the implicit and explicit self-esteem effects affective well-being and are highly associated with clinical symptoms. Implicit self-esteem also determines how individuals approach relational conflicts and social settings. While low levels of implicit self-esteem can be erroneous, boosts in implicit self-esteem through mechanisms involved in narcissism can also impair an individual's performance in cognitive tasks and external representation of competence in occupational settings.\n\nWhen explicit self-esteem is lower, it is called \"damaged self-esteem.\" When the implicit self-esteem is lower it is called \"defensive self-esteem.\"\n\nIt has been found that individuals who tend to have a higher correspondence between implicit and explicit self-esteem, trust their intuition.\n\nIndividuals with a combination of high implicit and low explicit self-esteem possess what psychologists call a \"damaged self-esteem\".\n\nStudy results indicate that, in comparison to individuals with low implicit and low explicit self-esteem, individuals with damaged self-esteem exhibit more optimism and less self-protection as well as higher levels of both maladaptive and adaptive perfectionism.\n\nDamaged self-esteem has also been found to correlate with many clinical symptoms and disorders. In particular, the size of the discrepancy between implicit and explicit self-esteem in the direction of a damaged self-esteem has been found to correlate positively with heightened symptoms of depressive symptoms, suicidal ideation and loneliness. While implicit self-esteem itself is not correlated with these internalizing symptoms, the interaction between implicit and explicit self-esteem does. In particular, when individuals display low explicit self-esteem, their level of implicit self-esteem becomes directly and positively correlated with their level of suicidal ideation. This reflects the crucial role of implicit self-esteem in internalizing problems. We can understand the impact of a damaged self-esteem as an entrapment between goals, which stem from implicit self-esteem, and reality, which mediates explicit self-esteem. Indeed, damaged self-esteem has been found to correlate with a maladaptive pattern of perfectionism, which is hinged upon rigidly high expectations that often contribute to failure.\n\nThe development of damaged self-esteem also showed a relationship to the use of self-defeating humor as a coping strategy, however, the causal direction is unclear. It could be that the frequent use of self-defeating humor lead to the development of damaged self-esteem (e.g., through a downward spiral of social rejection), or, that people with damaged self-esteem are more likely to use self-defeating humor (i.e., in line with their uncomplimentary view of the self). Another alternative is that both self-defeating humor and damaged self-esteem are caused by a third variable, such as neuroticism or alexithymia.\n\nDamaged self-esteem has also been found to correlate positively with internet addiction, the underlying mechanism of which parallels that of clinical conditions such as bulimia nervosa. This occurrence of compulsions may be attributed to an automatic defense mechanism in which the individual avoids anxiety. However, the development of a damaged self-esteem as an avoidance mechanism can also precipitate difficulties in establishing a consistent self-view.\n\nConversely, individuals with a combination of low implicit and high explicit self-esteem have what is called \"defensive self-esteem\" (or synonymously \"fragile self-esteem\"). In a comparative study it was found that individuals with defensive self-esteem tended to be less forgiving than others.\n\nAn important indicator of relationship stability and health is conflict behavior, the way individuals behave during a conflict. Peterson and DeHart found that implicit self-esteem can regulate connection during times of relationship crises. Studies suggest that individuals with high implicit self-esteem tend to engage more in nonverbal positive behaviors during conflict when they perceive their partners to be committed. Positive nonverbal behaviors during conflict is extremely predictive of relationship outcomes such as commitment, satisfaction and stability. Also, implicit self-esteem also predicts sensitivity towards partners’ availability or support, even within a relationship-threat. That is, individuals high in implicit self-esteem tend to be implicitly motivated to consciously correct for connection and sensitivity to their partners’ effort, despite explicitly doubting their investment in the relationship. This ability to overcome relationship-threats as perpetuated by high levels of implicit self-esteem is crucial to relational well-being.\n\nIn addition, low implicit self-esteem has also been found to precipitate uncertainty in self-concept. This instability in grasping the self is especially erroneous in regulation of behaviors in social situations. It has been shown that uncertainty about the self makes people vulnerable to holding and expressing minority opinions, especially those who are susceptible to self-threat (low self-esteem). Individuals with low implicit self-esteem tend to respond defensively to self-threats, and because minority opinions are more self-diagnostic than majority ones, individuals may hold these opinions to shield themselves from threat of uncertainty. They also tend to take extreme views and to over-estimate the social consensus for their views.\n\nGender differences play a vital role in implicit self-esteem in how it is influenced by the performance of the significant other.\n\nAlso, women are more prone to trust their feelings and intuition, in contrast to men. The correlation between explicit and implicit self-esteem is greater for women then for men. Implicit self-esteem contains instinctive and empirical factors; then people who are in touch with their feelings, would report to have higher explicit self-esteem scores, which are consistent to implicit self-esteem scores. There were six studies that supported this idea, and these results were held in three diverse cultures, two unlike measures of implicit self-esteem. These ideas showed the correlation between implicit and explicit self-esteem. This is higher for women than men.\n\nSelf-affirming activities that significantly raises implicit self-esteem, such as viewing one’s own Facebook profile page, has been shown to decrease motivation to do well in cognitive tasks of moderate difficulty. Results like this suggest that a peak in unconscious positivity associated with the self may discount an individual’s efforts to further prove his worth in other areas. Consequently, this leaves an individual unmotivated to perform well in more practical settings.\n\nImplicit self-esteem is assessed using indirect measures of cognitive processing. These include the Name Letter Task and the Implicit Association Test. Such indirect measures are designed to reduce awareness of, or control of, the process of assessment. When used to assess implicit self-esteem, they feature stimuli designed to represent the self, such as personal pronouns (e.g., \"I\") or letters in one's name.\n\nThe so-called Name Letter Task (NLT, also called Initial Preference Task, IPT) relies on the name-letter effect and is one of the widest used measures of implicit self-esteem. Different measures have been proposed in order to improve the psychometrical properties of the name letter task.\n\nThe name-letter effect represents the idea that an individual prefers the letters belonging to their own name and will select these above other letters in choice tasks or rate them as more favourable or attractive than other letters in rating tasks. It seemingly occurs subconsciously, with the mere-exposure effect ruled out as a possible explanation.\n\nThis effect has been found in a vast range of studies. In one such scenario, participants were given a list of letters, one of which contained letters from their own name and the other of which contained other letters, and asked them to circle the preferred letter. This study found that, even when accounting for all other variables, letters belonging to the participants' own names were preferred.\n\nSimilar results have been found in cross-cultural studies, using different alphabets.\n\nIt is important to note the difference between the name-letter effect and 'implicit egotism', the latter being attributed to the way people gravitate towards places, people and situations that reflect themselves, including perhaps similarities with their own name. Indeed, research into the topic has shown similarities between people's names and their future careers; for example, the names Dennis and Denise are overepresented among dentists.\n\nThe implicit-association test is an experimental method used by psychologists to attempt to tap into a person's automatic, or subconscious association between a concept and an attribute. It has been widely used in an attempt to uncover a person's subconscious prejudices against certain members of society, such as those who are overweight, as well as other implicit stereotypes and associations. The test was formatted in order to measure self-esteem. Participants are asked to make rapid responses, co-classifying themselves (\"the self\") and positive attributes, as well as negative attributes. The speed, or ease of these associations made is said to show a subconscious, or implicit preference for one attribute over another, with regards to the self.\n\nMany studies have shown that the vast majority of people's implicit self-esteem is positively biased. That is, people find it a great deal easier to associate themselves with a positive concept than a negative one. Whether this is truly displaying implicit self-esteem is arguable; the findings may instead be linked with illusory superiority, in that people tend to rate themselves as above average on a number of scales.\n\nImplicit self-esteem <br>\nIn the article \"Stalking the perfect measure of implicit self-esteem: The blind men and the elephant revisited?\", the validity and reliability of seven implicit self-esteem measures have been explored. The implicit measures were not correlated with one another. However they did correlate, but only faintly with measures of explicit self-esteem. The implicit self-esteem measurements confirmed partial reliabilities in correlation to good test-retest reliabilities. Nonetheless implicit measures were limited in their ability to calculate standard variables, for the test. Certain evidence explained that measurements of implicit self-esteem are delicate to put in context, which is further argued in later research of implicit self-esteem.\n\nHowever, the validity of the implicit-association test and implicit self-esteem as a measure of self-esteem itself is questionable due to mixed evidence with regards to explicit self-esteem. On the one hand, researchers in a detailed and comprehensive study of implicit self-esteem found the IAT to correlate weakly, yet consistently, with measures of explicit self-esteem. However, more recent research has found measures of explicit self-esteem, such as questionnaires, to be independent of implicit self-esteem, providing an interesting insight into the validity of implicit self-esteem, explicit self-esteem, and the nature of self-esteem itself.\n\n"}
{"id": "1605389", "url": "https://en.wikipedia.org/wiki?curid=1605389", "title": "Infinity symbol", "text": "Infinity symbol\n\nThe infinity symbol (sometimes called the lemniscate) is a mathematical symbol representing the concept of infinity.\n\nThe shape of a sideways figure eight has a long pedigree; for instance, it appears in the cross of Saint Boniface, wrapped around the bars of a Latin cross. However, John Wallis is credited with introducing the infinity symbol with its mathematical meaning in 1655, in his \"De sectionibus conicis\".\nWallis did not explain his choice of this symbol, but it has been conjectured to be a variant form of a Roman numeral for 1,000 (originally CIƆ, also CƆ), which was sometimes used to mean \"many\", or of the Greek letter ω (omega), the last letter in the Greek alphabet.\nLeonhard Euler used an open variant of the symbol in order to denote \"absolutus infinitus\". Euler freely performed various operations on infinity, such as taking its logarithm. This symbol is not used anymore, and is not encoded as a separate character in Unicode.\n\nIn mathematics, the infinity symbol is used more often to represent a potential infinity, rather than to represent an actually infinite quantity such as the ordinal numbers and cardinal numbers (which use other notations). For instance, in the mathematical notation for summations and limits such as\nthe infinity sign is conventionally interpreted as meaning that the variable grows arbitrarily large (towards infinity) rather than actually taking an infinite value.\n\nThe infinity symbol may also be used to represent a point at infinity, especially when there is only one such point under consideration. This usage includes, for instance,\nthe infinite point of a projective line,\nand the point added to a topological space formula_2 to form its one-point compactification formula_3.\n\nIn areas other than mathematics, the infinity symbol may take on other related meanings; for instance, it has been used in bookbinding to indicate that a book is printed on acid-free paper and will therefore be long-lasting.\n\nIn modern mysticism, the infinity symbol has become identified with a variation of the ouroboros, an ancient image of a snake eating its own tail that has also come to symbolize the infinite, and the ouroboros is sometimes drawn in figure-eight form to reflect this identification, rather than in its more traditional circular form.\n\nIn the works of Vladimir Nabokov, including \"The Gift\" and \"Pale Fire\", the figure-eight shape is used symbolically to refer to the Möbius strip and the infinite, for instance in these books' descriptions of the shapes of bicycle tire tracks and of the outlines of half-remembered people. The poem after which \"Pale Fire\" is entitled explicitly refers to \"the miracle of the lemniscate\".\n\nThe well known shape and meaning of the infinity symbol have made it a common typographic element of graphic design. For instance, the Métis flag, used by the Canadian Métis people in the early 19th century, is based around this symbol. In modern commerce, corporate logos featuring this symbol have been used by, among others, Room for PlayStation Portable, Microsoft Visual Studio, Fujitsu, and CoorsTek.\n\nThe symbol is encoded in Unicode at and in LaTeX as codice_1: formula_4.\n\nThe Unicode set of symbols also includes several variant forms of the infinity symbol, that are less frequently available in fonts: , and in block Miscellaneous Mathematical Symbols-B. The acid-free paper symbol mentioned above is encoded separately as .\n\n"}
{"id": "23975565", "url": "https://en.wikipedia.org/wiki?curid=23975565", "title": "Intrinsic value (animal ethics)", "text": "Intrinsic value (animal ethics)\n\nIntrinsic value exists wherever self-valuing beings exist . The intrinsic value of a human, or any other sentient animal, is the value it confers on itself by desiring its own lived experience as an end in itself. Because intrinsic value is self-ascribed, all animals have it, unlike instrumental or extrinsic values. Instrumental value is the value that others confer on an animal (or on any other entity) because of its value as a resource (e.g. as property, labour, food, fibre, 'ecosystem services') or as a source of emotional, recreational, aesthetic or spiritual gratification. Intrinsic values are conferred from within an animal, and are therefore not directly measurable by economists, while extrinsic values are conferred from outside and can, in principle, be measured econometrically. \n\nThe phrase \"intrinsic value\" (often used synonymously with \"inherent value\") has been adopted by animal rights advocates. The Dutch \"Animal Health and Welfare Act\" referred to it in 1981: \"Acknowledgment of the intrinsic value of animals means that animals have value in their own right and as a consequence their interests are no longer automatically subordinate to man's interests.\" This acknowledgement has stirred a debate on what it entails in the context of animal husbandry, animal breeding, vivisection, animal testing and biotechnology. It is also used by environmental advocates and in law to holistically encompass the totality of intrinsic values in an ecosystem. Article 7(d) of New Zealand's \"Resource Management Act\" (RMA), for example, requires particular regard to be to given to \"intrinsic values of ecosystems\" \n\nMoral attitudes towards animals in the west (as expressed in public debate and legislation) have changed considerably over time. Britain's first anti-cruelty laws were introduced in the Cruelty to Animals Act 1835. This was followed by similar laws in many other countries, especially in the second half of the 20th century. These laws did not challenge the idea that other animals are resources for human use and they only limited those acts of cruelty which (a) had few economic or social repercussions; and (b) were offensive to human sensibilities (the so-called Offence principle) or at odds with human dignity. These regulations were anthropocentric in character: they generally gave human economic and recreational interests, such as farming, fishing and blood sports, greater priority than animal suffering - that is they favoured the animals' instrumental values over their intrinsic ones. \n\nDuring the second half of the 20th century, the intensification of cattle breeding, the growth of pig and chicken factory farming and the increased use of animals in harmful laboratory experiments provoked fierce debates in which the negative consequences for the animals themselves became an issue. Notably during the 1960s and 1970s, pressure groups started to argue on behalf of the interests of animals kept in laboratories and farms. They expressed their discontent with laws that protected the institutional cruelty of the animal exploitation industries while only prohibiting selected acts of individual cruelty in certain situations. They called for new forms of legislation that would protect animals for non-anthropocentric reasons.\n\nIn these discussions (of the moral relevance of the animal's welfare) two key issues were involved. To begin with, the Harm principle, rather than the Offence principle, should be the moral foundation for the protection of animals. Secondly, as to the scepticism expressed by scientists regarding the presence of consciousness and self-awareness in animals, they should be granted the benefit of the doubt by adopting the so-called \"analogy postulate\". Applied ethological research into the behaviour of animals in captivity made it clear that the intensive use of animals had negative effects on the animal's health and well-being. Nevertheless, concern for the well-being of animals had to be purged from anthropomorphism and sentimentalism. This point of view is taken for example in a report by the Dutch Federation of Veterinarians in the EEC (FVE, 1978) concerning welfare-problems among domestic animals. This document states that:\nalthough the interests of animals often conflict with the demands of society, society remains responsible for the welfare of the animals involved. Considerations regarding animal welfare ought to be based on veterinary, scientific and ethological norms, but not on sentiment. And although animals do not have fundamental rights, human beings have certain moral obligations towards them.\n\nDuring the 1970s and 1980s, the criticism regarding the living conditions of farm and laboratory animals became mixed up with other social debates, notably the discussions concerning the protection of the (natural) environment and the ones concerning the development of new breeding techniques. Due to this broadening of the issues, other objections against the use of animals for scientific or economic reasons emerged. The instrumental use of the animals, it was said, is hard to reconcile with their \"intrinsic\" (or \"inherent\") value. In 1981 the Dutch government included the intrinsic value-argument in a statement concerning the protection of animals (CRM, 1981). Now a principle was formulated that allowed for the possibility that, in some cases, the interests of animals might prevail over and above those of science and industry. The interests of the animal involved health and well-being as experienced by the animals themselves, independent from considerations concerning their suitability for human use. It was now claimed that animals have an intrinsic value, that is a \"good-of-their-own\", and an interest in their own well-being.\n\nDevelopments within the field of biotechnology broadened the scope of the debate on the moral status of animals even more. After the controversy concerning the transgenic bull Herman and the lactoferrin project of GenePharming, modern biotechnology has almost become a synonym for genetic engineering. In the debate on bull Herman, concern for the intrinsic value of animals became an issue in its own right. Many felt that there was more to intrinsic value than merely the concern for the animal's welfare. Since then, intrinsic value not only refers to the animal's welfare, but also to the moral attitude society takes towards animals (or nature) as such. For some, this stance means a return to the Offence principle, and therefore not helpful in the struggle against anthropocentrism or anthropomorphism. Others however maintain that recognition of the intrinsic value of animals goes beyond animal welfare, since it respects the animal as \"centre of its own being\".\n\nThe cause of much confusion in the discussion over intrinsic value in relation to the moral status of animals, is the diversity of meanings and connotations associated with intrinsic value. Broadly speaking there are 4 main positions in this debate defining intrinsic value. One can adhere to a meaning of intrinsic value of animals in a sense that is:\n\n\nOf the first, behaviouristic interpretation, one can say (since it is \"morally neutral\") that it is useless to ethical theory. Of the fourth, attitudinal or intuitionistic interpretation, one can say that it is indiscriminate of sentience or interests, and could be used for any kind of (natural, cultural or abstract) entity worth protecting (including species, cultures, languages, historical buildings or sites, etc.). The core issue in the debate over intrinsic value of animals remains between utilitarianists and deontologists.\n\nJAAWS\n\nThe ethics of animal research. Talking Point on the use of animals in scientific research\n\nEthical and Scientific Considerations Regarding Animal Testing and Research\n"}
{"id": "47108875", "url": "https://en.wikipedia.org/wiki?curid=47108875", "title": "Jumble algorithm", "text": "Jumble algorithm\n\nEach clue in a Jumble word puzzle is a word that has been “jumbled” by permuting the letters of each word to make an anagram. A dictionary of such anagrams may be used to solve puzzles or verify that a jumbled word is unique when creating puzzles.\n\nAlgorithms have been designed to solve Jumbles, using a dictionary. Common algorithms work by printing all words that can be formed from a set of letters. The solver then chooses the right word.\n\nFirst algorithm:\n\n\nSecond algorithm:\n\n\nAlgorithm to find the permutations of J:\n\n\nJ(1)J(2)\n\nJ(2)J(1)\n\n\nJ(1)J(2)J(3)\n\nJ(1)J(3)J(2)\n\nJ(3)J(1)J(2)\n\nJ(2)J(1)J(3)\n\nJ(2)J(3)J(1)\n\nJ(3)J(2)J(1)\n\n\nThough the algorithm looks complex it is easy to program.\n\nDouglas Hofstadter developed a program called Jumbo that tries to solve Jumble problems as a human mind would.\nThe program doesn't rely on a dictionary and doesn't try to find real English words, but rather words that could be English, exploiting a database of plausibilities for various combinations of letters.\nLetters are combined non-deterministically, following a strategy inspired by chemical reactions and free associations.\n"}
{"id": "184638", "url": "https://en.wikipedia.org/wiki?curid=184638", "title": "Lander (spacecraft)", "text": "Lander (spacecraft)\n\nA lander is a spacecraft which descends toward and comes to rest on the surface of an astronomical body. By contrast with an impact probe, which makes a hard landing and is damaged or destroyed so ceases to function after reaching the surface, a lander makes a soft landing after which the probe remains functional.\n\nFor bodies with atmospheres, the landing occurs after atmospheric entry. In these cases, landers may employ parachutes to slow down and to maintain a low terminal velocity. Sometimes small landing rockets are fired just before impact to reduce the impact velocity. Landing may be accomplished by controlled descent and setdown on landing gear, with the possible addition of a post-landing attachment mechanism for celestial bodies with low gravity. Some missions (for example, Luna 9 and Mars Pathfinder), used inflatable airbags to cushion the lander's impact rather than a more traditional landing gear.\n\nWhen a high velocity impact is planned not for just achieving the surface but for study of consequences of impact, the spacecraft is called an impactor.\n\nSeveral terrestrial bodies have been subject of lander or impactor exploration: among them Earth's Moon, the planets Venus, Mars, and Mercury, the Saturn moon Titan, the asteroids and comets.\n\nBeginning with Luna 2 in 1959, the first few spacecraft to reach the lunar surface were impactors, not landers. They were part of the Soviet Luna program or the American Ranger program.\n\nIn 1966, the Soviet Luna 9 became the first spacecraft to achieve a lunar soft landing and to transmit photographic data to Earth. The American Surveyor program (since 1966) was designed to determine where Apollo could land safely. As a result, these robotic missions required soft landers to sample the lunar soil and determine the thickness of the dust layer, which was unknown before Surveyor.\n\nThe U.S. manned Apollo Lunar Modules (since 1969) with rovers (since 1971) and Soviet unmanned late big landers (since 1969), Lunokhods (since 1970) and sample return missions (since 1970) used a rocket descent engine for a soft landing of astronauts and lunar rovers on the Moon.\n\nThe Altair spacecraft, previously known as the \"Lunar Surface Access Module\" or \"LSAM\", was the planned lander for the Constellation program, prior to the cancellation of Project Constellation.\n\nRussia has plans for Luna-Grunt mission to return samples from the Moon by 2021.\n\nThe Chinese Chang'e 3 mission and its Jade Rabbit rover landed on 14 December 2013. Then China plans to repeat lander with rover in Chang'e 4 mission after 2015 that will followed by Chang'e 5 and Chang'e 6 sample return missions in 2017 and before 2020.\n\nThe Soviet Venera program included a number of Venus landers, some of which were crushed during descent much as Galileo's Jupiter \"lander\" and others of which successfully touched down. Venera 3 in 1966 and Venera 7 in 1970 became the first impact and soft landing on Venus. The Soviet Vega program also placed in 1985 two balloons in the Venusian atmosphere, they were the first aerial tools on other planets.\n\nThe Soviet Union's Mars 1962B was the first Earth based mission intended to reach the surface as impact on Mars in 1962. In 1971, the lander of the Mars 3 probe conducted the first soft landing on Mars, but communication was lost within a minute after touchdown, which occurred during one of the worst global dust storms since the beginning of telescopic observations of the Red Planet. Three other landers, Mars 2 in 1971 and Mars 5, Mars 6 in 1973, either crashed or failed to even enter the planet's atmosphere. All four landers used an aeroshell-like heat shield during atmospheric entry. Mars 2 and Mars 3 landers carried the first small skis-walking Martian rovers that did not work on the planet.\n\nThe Soviet Union planned the heavy Marsokhod Mars 4NM mission (in 1973) Mars sample return Mars 5NM mission (in 1975) but they did not occur due to need of the N1 superlauncher that was never flown successfully. A double-launching Soviet Mars 5M (Mars-79) sample return mission was planned for 1979 but cancelled due to complexity and technical problems.\nViking 1 and 2 were launched respectively in August and September 1975, each comprising an orbiter vehicle and a lander. Viking 1 landed in July 1976 and Viking 2 in September 1976. The Viking rovers were the first successful, working Mars landers. The mission ended in May 1983, after both landers had died.\n\nIn the 1970s, the US planned the Voyager-Mars mission. This would have consisted of two orbiters and two landers, launched by a single Saturn V rocket, but the mission was cancelled.\n\nMars 96 was the first complex post-Soviet Russian mission with an orbiter, lander and penetrators. Planned for 1996, it failed at launch. A planned repeat of this mission, Mars 98, was cancelled due to lack of funding.\nThe U.S. Mars Pathfinder was launched in December 1996 and released the first acting rover on Mars, named \"Sojourner\", in July 1997. It failed in September 1997, probably due to electronics failure caused by the cold temperatures. Mars Pathfinder was part of the cancelled Mars Environmental Survey program with a set of 16 landers planned for 1999–2009.\nThe Mars Polar Lander ceased communication on 3 December 1999, prior to reaching the surface, and is presumed to have crashed.\nThe European Beagle 2 lander deployed successfully from the Mars Express spacecraft but the signal confirming a landing which should have come on 25 December 2003 was not received. Indeed, no communication was ever established and \"Beagle 2\" was declared lost on 6 February 2004. The proposed 2009 British Beagle 3 lander mission to search for life, past or present was not adopted.\n\nThe French/ESA NetLander mission for 2007 or 2009, with an orbiter and 4 landers, was cancelled because it was too expensive. Its successor, Mars MetNet, a multi-lander mission for 2011–2019 was not adopted by the ESA.\n\nThe American Mars Exploration Rovers \"Spirit\" and \"Opportunity\" were launched in June and July 2003. They reached the Martian surface in January 2004 using landers featuring airbags and parachutes to soften impact. \"Spirit\" ceased functioning in 2010, more than five years past its design lifetime. As of January 2017, \"Opportunity\" remains active, having exceeded its three-month design lifetime by well over a decade.\nThe U.S. Phoenix spacecraft successfully achieved soft landing on the surface of Mars on May 25, 2008, using a combination of parachutes and rocket descent engines.\n\nMars Science Laboratory (\"Curiosity\") was successfully launched by NASA on November 26, 2011. It landed in the Aeolis Palus region of Gale Crater on Mars on August 6, 2012.\n\nPlanned for 2018, NASA's Mars Astrobiology Explorer-Cacher lander mission was cancelled due to budget cuts.\n\nExploration of Mars including the use of landers continues to this day. Amongst them, Russia has planned a Mars sample return mission Mars-Grunt for near 2026, and China for near 2030.\n\nWhile several flybys conducted by Mars orbiting probes have provided images and other data about the Martian moons Phobos and Deimos, only few of them intended to land on the surface of these satellites. Two probes under the Soviet Phobos program were successfully launched in 1988, but in 1989 the intended landings on Phobos and Deimos were not conducted due to failures in the spacecraft system. The post-Soviet Russian Fobos-Grunt probe was an intended sample return mission to Phobos in 2012 but failed after launch in 2011.\n\nIn 2007 European Space Agency and EADS Astrium proposed and developed the mission to Phobos to 2016 with lander and sample return, but it stayed as project. Since 2007 the Canadian Space Agency considering the mission to Phobos PRIME (Phobos Reconnaissance and International Mars Exploration) with orbiter and lander. Recent proposals include a 2008 NASA Glenn Research Center Phobos and Deimos sample return mission, the 2013 Phobos Surveyor, and the OSIRIS-REx II mission concept.\n\nThe Japanese Aerospace Exploration Agency (JAXA) plans to launch the Martian Moons Exploration (MMX) in 2024, a sample return mission targeting Phobos. MMX will land and collect samples from Phobos multiple times, along with deploying a rover jointly developed by CNES and DLR. By using a corer sampling mechanism, the spacecraft aims to retrieve a minimum 10 kg amount of samples. MMX will return to Earth in 2029.\n\nRussia plans to repeat Fobos-Grunt mission near 2024.\n\nThe \"Huygens\" probe, carried to Saturn's moon Titan by the \"Cassini\", was specifically designed to survive landing on land and on liquid. It was thoroughly drop-tested to make sure it could withstand impact and continue functioning for at least three minutes. However, due to the low speed impact, it continued providing data for more than two hours after it landed. The landing on Titan in 2005 was the first landing on planet's satellites outside the Moon.\n\nThe proposed U.S. TiME mission considered a lander that would splash down in a lake in Titan's northern hemisphere and float on the surface of the lake for few months. Spain's proposed TALISE mission is similar to TIME lander but has its own propulsion system for controlling shipping.\n\nVesta, the multiaimed Soviet mission, developed in cooperation with European countries for realisation in 1991–1994 but canceled due to the Soviet Union disbanding, included the flyby of Mars with delivering the aerostat and small landers or penetrators followed by flybys of 1 Ceres or 4 Vesta and some other asteroids with impact of large penetrator on the one of them.\n\nThe cancelled NASA's Comet Rendezvous Asteroid Flyby mission considered the launch in 1995 and landing of penetrators on comet's nucleus in 2001.\n\nThe first landing on a similarly small body, the asteroid 433 Eros, was performed in 2001 by the probe NEAR Shoemaker despite the fact that NEAR was not originally designed to be capable of landing.\n\nThe \"Hayabusa\" probe made several attempts to land on 25143 Itokawa in 2005 with mixed success, including a failed attempt to deploy a rover. Meanwhile, it was the second lander on asteroid and in 2010 the first sample returm mission from asteroid.\n\nThe \"Rosetta\" probe, launched 2 March 2004, put the first robotic lander \"Philae\" on comet Churyumov–Gerasimenko on 12 November 2014. Due to the extremely low gravity of such bodies, the landing system includes a harpoon launcher intended to anchor a cable in the surface and pull it down.\n\nJapan (JAXA) launched the Hayabusa 2 asteroid space probe in 2014 to deliver several landing parts (including Minerva II and German MASCOT (Mobile Asteroid Surface Scout) landers and SCI (Small Carry-on Impactor) penetrator) in 2018–2019 and to return samples to Earth by 2020.\n\nNASA launched the OSIRIS-REx sample return mission to asteroid 101955 Bennu in 2016.\n\nAIDA is a mission concept of ESA to investigate the effects of impact crashing a spacecraft into an asteroid.\n\nESA proposed the Marco Polo and then MarcoPolo-R sample return missions in near 2020 from the surface of a Near Earth asteroid but these missions were not adopted.\n\nThe Chinese Space Agency is designing a sample retrieval mission from Ceres that would take place during the 2020s.\n\nScheduled to launch in October 2018 and expected to reach Mercury in December 2025, ESA's BepiColombo mission to Mercury originally included the Mercury Surface Element. The MSE lander would have carried a 7 kg payload consisting of an imaging system (a descent camera and a surface camera), a heat flow and physical properties package, an alpha particle X-ray spectrometer, a magnetometer, a seismometer, a soil penetrating device (mole), and a micro-rover. The MSE aspect of the mission was cancelled in 2003 due to budgetary constraints.\n\nA few Jupiter probes provides many images and other data about its moons. Some proposed missions with landing on Jupiter's moons were cancelled or no adopted. The small nuclear-powered Europa lander was proposed as part of NASA's JIMO mission that cancelled in 2006.\n\nCurrently planning by ESA to launch in 2022 JUICE mission includes Russian Ganymede Lander on Ganymede that would make a soft landing near 2033. NASA proposed ESA to include a lander or impactor on Europa under Europa Clipper mission planning to launch in 2025. As Europa is very interesting and need to be investigate for habitability (by extraterrestrial life) and assess its astrobiological potential by confirming the existence and determining the characteristics of water within and below Europa's icy shell, despite of high radiation environment around itself and Jupiter troubling for any robotic surface mission, the Europa Lander Mission of NASA still considering and have led to steady lobbying for future missions. Russian Europa Lander was proposed to include as part of now cancelled joint NASA/ESA EJSM/Laplace mission and then stays planning for realisation in any other or separate mission. Also, another proposal calls for a large nuclear-powered \"melt probe\" (cryobot) that would melt through the ice until it reached an ocean below where it would deploy an autonomous underwater vehicle (hydrobot) that would gather information.\n\nThe Deep Space 2 impactor probe was to be the first spacecraft to penetrate below the surface of another planet. However the mission failed with the loss of its mother ship, Mars Polar Lander after communication was lost after entry into Mars atmosphere on 3 December 1999.\n\nComet Tempel 1 was visited by NASA's \"Deep Impact\" probe on 4 July 2005. The impact crater formed was approximately 200 m wide and 30–50 m deep, and detected the presence of silicates, carbonates, smectite, amorphous carbon and polycyclic aromatic hydrocarbons.\n\nThe Moon Impact Probe (MIP) developed by the Indian Space Research Organisation (ISRO), India's national space agency, was a lunar probe that was released on 14 November 2008 by ISRO's Chandrayaan-1 lunar remote sensing orbiter which in turn was launched, on 22 October 2008. It discovered the presence of water on the Moon.\n\nThe Lunar Crater Observation and Sensing Satellite (LCROSS) was a robotic spacecraft operated by NASA to perform a lower-cost means of determining the nature of hydrogen detected at the polar regions of the moon. The main LCROSS mission objective was to explore the presence of water ice in a permanently shadowed crater near a lunar polar region. LCROSS was launched together with the Lunar Reconnaissance Orbiter (LRO) on 18 June 2009, as part of the shared Lunar Precursor Robotic Program. LCROSS was designed to collect and relay data from the impact and debris plume resulting from the launch vehicle's spent Centaur upper stage striking the crater Cabeus near the south pole of the Moon. Centaur impacted successfully on 9 October 2009, at 11:31 UTC. The Shepherding Spacecraft descended through Centaur's ejectate plume, collected and relayed data, impacting six minutes later at 11:37 UTC. The project was successful in discovering water in the southern lunar crater Cabeus.\n\nThe NASA \"MESSENGER\" (MErcury Surface, Space ENvironment, GEochemistry, and Ranging) mission to Mercury launched on 3 August 2004 and entered orbit around the planet on 18 March 2011. Following a mapping mission, \"MESSENGER\" was directed to crash into Mercury's surface on April 30, 2015. The spacecraft's impact with Mercury occurred near 3:26 PM EDT on April 30, 2015, leaving a crater estimated to be 16 m (52 ft) in diameter.\n\n"}
{"id": "39784978", "url": "https://en.wikipedia.org/wiki?curid=39784978", "title": "Landstände", "text": "Landstände\n\nThe Landstände (singular \"Landstand\") or Landtage (singular \"Landtag\") were the various territorial estates or diets in the Holy Roman Empire in the Middle Ages and the early modern period, as opposed to their respective territorial lords (the \"Landesherrn\").\n\nThe structure of the \"Landstände\" was highly variable depending on the country and period of history. Furthermore, both the representatives of the older system, the \"Ständeordnung\", where the estates were predominant, and the parliaments of the newer people's representative systems were called \"Landstände\". The term \"Landtag\" was used, both under the \"Ständeordnung\" as well as the newer representative structures, for a general assembly of the estates or the parliament. The totality of the \"Landstände\" in a sovereign territory was also called the \"Landschaft\".\n\nIn the older feudal system the estates originally consisted of the assembly of deputies of the privileged estates of a country, the nobility and the clergy, who had joined together to form an organised body. Later, representatives of the towns were added. In some cases (for example, in Vienna, Württemberg or Mecklenburg) yeomen (\"Freibauer\") were also given the right to participate as representatives of the peasants. An unusual exception were the estates in the land of Hadler, which were formed almost exclusively of the farmers of large farms (\"Großbauer\").\n\nAt the \"Landtage\" the \"Landstände\" were divided into separate \"curiae\" (divisions). As a rule, three \"curiae\" were usually distinguished: the prelates, the knights and the towns. However, the early \"Landstände\" initially only represented the rights of their own estate and could only indirectly be considered to represent the whole population in their domain at the same time. In the \"Ständeordnungen\", unlike absolutist systems of rule, the prince could not raise new taxes or adopt new laws outside his own personal estate (\"chamber goods\" or \"Kämmergüter\") without the consent of the \"Landstände\". In some cases, the estates also shared in the administration of justice and other public affairs. The limits of their powers were not usually accurately determined.\n\nSometimes the term \"Landstände\" was retained even for the constitutional assemblies of newer representative systems, which in many countries took the place of the privileged assemblies of the \"Ständeordnung\" during the 19th century.\n\nThe \"Landstände\" first emerged in the 14th century, although the term itself was not used in Middle High German and was probably first translated later from the French word \"états\". But it was not a new concept. The records of the Roman historian, Tacitus, show that co-determination was already being practised in the classical period at important public occasions. And according to the old Germanic law, public meetings and court hearings - the so-called \"things\" - were held, in the open. Even in the later Frankish Empire, alongside the general assemblies of the nobility and clergy, so-called \"placita\" are recorded; a form of representation of the people.\n\nIn individual tribes, for example the Bavarians and Saxons, there were such meetings too. However, these gatherings did not represent a formal grouping of the estates, as had evolved by the 14th century. Even the imperial and knightly assemblies and the state \"things\" of the 12th and 13th centuries were structures that dealt with the general welfare of the land, but these meetings still lacked the character of an independent body.\n\n"}
{"id": "632487", "url": "https://en.wikipedia.org/wiki?curid=632487", "title": "List of algorithm general topics", "text": "List of algorithm general topics\n\nThis is a list of algorithm general topics. \n\n\n"}
{"id": "27925223", "url": "https://en.wikipedia.org/wiki?curid=27925223", "title": "List of philosophical concepts", "text": "List of philosophical concepts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "6039598", "url": "https://en.wikipedia.org/wiki?curid=6039598", "title": "Looting of the Eastern Mausoleum", "text": "Looting of the Eastern Mausoleum\n\nThe Looting of the Eastern Mausoleum was an incident in which some of the major mausoleums of the Chinese Qing dynasty in the Eastern Qing Tombs were looted by troops under the command of the warlord Sun Dianying.\n\nIn the early hours of June 8, 1928, warlord Sun Dianying led his army into the Eastern Mausoleums of the Qing dynasty in Malanyu, northwest of Zunhua, Hebei. This was the final resting place of the Qing emperors and empresses, and was about 120 kilometers (75 miles) from the Forbidden City of Peking. The 78-square-kilometer (30.1-square-mile) burial site was for five emperors, 15 empresses and 136 imperial concubines within 15 tombs, including the Shunzhi Emperor (1638–61), the Kangxi Emperor (1654–1722), the Qianlong Emperor (1711–99) and Empress Dowager Cixi (1835–1908).\n\nOn June 12, 1928, Sun Dianying ordered a large-scale graverobbing operation that removed almost all the underground funeral objects of the Huifeiling and Yuling Mausoleums and the underground palace of Puxiangyu East Dingling. Ma Futian, Regimental Commander in the 28th Army of Zhang Zuolin, had quietly occupied Malanyu. Sun Dianying ordered Tan Wenjiang, one of his division commanders, to capture the tomb area. At dawn on July 2, Ma Futian was driven away and Tan's army looted the mausoleums in Malanyu. After that, Sun's army went straight to the area of the Eastern Qing Tombs, pretending to engage in war exercises in the area. Tan Wenjiang placed policemen all around, denying access to the area and signs declared the army was \"protecting the Tombs\" to prevent interference.\n\nThe looting operation was directed by Sun Dianying from his car. Trucks were on hand to speed away with the loot as soon as they were loaded. At midnight the engineering corps blew up the entrance, opening the passage leading to the underground palace. The stone door was pried open to give access to the rear room of the grave. Sun gave first priority to officers above battalion commander level to collect treasure for themselves. Ordinary soldiers were eventually allowed to take the leftovers.\n\nThe robbers first took the large treasure objects placed around the remains of Empress Dowager Cixi, such as jadeite watermelons, grasshoppers and vegetables, jade lotus and coral. They snatched objects found beneath the body and ravaged the corpse itself, taking her imperial robe; tearing off her undergarments, shoes and socks, and taking all the pearls and jewels on her body. The looters even pried open her jaws and took the rare pearl from her mouth. Ultimately, they looted the objects under the coffin that had been favorites of Cixi when she was alive.\n\nWhile Tan Wenjiang was robbing Cixi's tomb, Han Dabao, a brigade commander under Sun Dianying, led another group to the Yuling Mausoleum and declared his intention to conduct a war exercise. They blew the entrance and doorways of the underground palace and rushed into the tomb. The coffins of the Qianlong Emperor and his two empresses and three concubines were pried open, all the valuables looted and the skeletons thrown into the mud. The soldiers then rushed to the Yuling Mausoleum and the underground palace of Puxiangyu East Dingling and looted what they could.\n\nNewspapers reported the graverobbing and the news spread throughout China and around the world. People were outraged. China's dethroned last emperor Puyi, who had dismissed Sun from his post, sent telegrams to Chiang Kai-shek; Yan Xishan, Commander of Garrison Force in Beijing; the Central Committee of Kuomintang and local newspapers asking them to punish Sun Dianying severely. Many others also called for punishment. However, Sun Dianying bribed those who were in a position to discipline him and nothing was done.\n\nAfter removing the treasures from the graves, Sun and his army sealed the empty chambers with stones. They carted off some of China's greatest treasures, but some things could not be easily removed and the imposing buildings of the mausoleum still survive.\n"}
{"id": "100416", "url": "https://en.wikipedia.org/wiki?curid=100416", "title": "Lynching", "text": "Lynching\n\nLynching is a premeditated extrajudicial killing by a group. It is most often used to characterize informal public executions by a mob in order to punish an alleged transgressor, or to intimidate a group. It is an extreme form of informal group social control such as \"charivari,\" skimmington, riding the rail, and tarring and feathering, and often conducted with the display of a public spectacle for maximum intimidation. It is to be considered an act of terrorism and punishable by law. Instances of lynchings and similar mob violence can be found in every society.\n\nIn the United States, lynchings of African Americans, typically by hanging, became frequent in the South during the period after the Reconstruction era and especially during the decades on either side of the turn of the 20th century. At the time, Southern states were passing new constitutions and laws to disenfranchise African Americans and impose legal segregation and Jim Crow rule. Most lynchings were conducted by white mobs against black victims, often suspects taken from jail before they were tried by all-white juries, or even before arrest. The political message—the promotion of white supremacy and black powerlessness—was an important element of the ritual. Lynchings were photographed and published as postcards, which were popular souvenirs in the U.S., to expand the intimidation of the acts. Victims were sometimes shot, burned alive, or otherwise tortured and mutilated in the public events. In some cases the mutilated body parts were taken as mementos by the spectators. Particularly in the West, other minorities—Native Americans, Mexicans and Asians—were also lynched. The South had the states with the highest total numbers of lynchings.\n\nThe origins of the word \"lynch\" are obscure, but it likely originated during the American revolution. The verb comes from the phrase \"Lynch Law\", a term for a punishment without trial. Two Americans during this era are generally credited for coining the phrase: Charles Lynch and William Lynch, who both lived in Virginia in the 1780s. Charles Lynch has the better claim, as he was known to have used the term in 1782, while William Lynch is not known to have used the term until much later. There is no evidence that death was imposed as a punishment by either of the two men. In 1782, Charles Lynch wrote that his assistant had administered \"Lynch's law\" to Tories \"for Dealing with Negroes, &c.\"\n\nThe origin of the terms \"lynching\" and \"lynch law\" is traditionally attributed In the United States to Charles Lynch, a Virginia Quaker. Charles Lynch (1736–1796) was a Virginia planter and American Revolutionary who headed a county court in Virginia which incarcerated Loyalist supporters of the British for up to one year during the war. While he lacked proper jurisdiction, he claimed this right by arguing wartime necessity. Subsequently, he prevailed upon his friends in the Congress of the Confederation to pass a law that exonerated him and his associates from wrongdoing. He was concerned that he might face legal action from one or more of those so incarcerated, even though the American Colonies had won the war. This action by the Congress provoked controversy, and it was in connection with this that the term \"Lynch law\", meaning the assumption of extrajudicial authority, came into common parlance in the United States. Lynch was not accused of racist bias. He acquitted blacks accused of murder on three separate occasions. He was accused, however, of ethnic prejudice in his abuse of Welsh miners.\n\nWilliam Lynch (1742–1820) from Virginia claimed that the phrase was first used in a 1780 compact signed by him and his neighbors in Pittsylvania County. While Edgar Allan Poe claimed that he found this document, it was probably a hoax.\n\nA 17th-century legend of James Lynch fitz Stephen, who was Mayor of Galway in Ireland in 1493, says that when his son was convicted of murder, the mayor hanged him from his own house. The story was proposed by 1904 as the origin of the word \"lynch\". It is dismissed by etymologists, both because of the distance in time and place from the alleged event to the word's later emergence, and because the incident did not constitute a lynching in the modern sense.\n\nThe archaic verb \"linch\", to beat severely with a pliable instrument, to chastise or to maltreat, has been proposed as the etymological source; but there is no evidence that the word has survived into modern times, so this claim is also considered implausible.\n\nEvery society has had forms of extrajudicial punishments, including murder. The legal and cultural antecedents of American lynching were carried across the Atlantic by migrants from the British Isles to colonial North America. Collective violence was a familiar aspect of the early modern Anglo-American legal landscape. Group violence in the British Atlantic was usually nonlethal in intention and result. In the seventeenth century, in the context of political turmoil in England and unsettled social and political conditions in the American colonies, there arose rebellions and riots that took multiple lives. In the United States, during the decades before the Civil War (sometimes called the Antebellum era), free Blacks, Latinos in the South West, and runaways were the objects of racial lynching.\n\nBut lynching attacks on U.S. blacks, especially in the South, increased dramatically in the aftermath of Reconstruction, after slavery had been abolished and freed men gained the right to vote. The peak of lynchings occurred in 1892, after southern white Democrats had regained control of state legislatures. Many incidents were related to economic troubles and competition. At the turn of the 20th century, southern states passed new constitutions or legislation which effectively disenfranchised most blacks and many poor whites, established segregation of public facilities by race, and separated blacks from common public life and facilities through Jim Crow rules. Nearly 3,500 African Americans and 1,300 whites were lynched in the United States between 1882 and 1968.\n\nLynching in the British Empire during the 19th century coincided with a period of violence which denied people participation in white-dominated society on the basis of race after the Emancipation Act of 1833.\n\nLynchings took place in the United States both before and after the American Civil War, most commonly in Southern states and Western frontier settlements and most frequently in the late 19th century. It was performed without due process of law by self-appointed commissions, mobs, or vigilantes as a form of punishment for presumed criminal offences. At the first recorded lynching, in St. Louis in 1835, a black man named McIntosh who killed a deputy sheriff while being taken to jail was captured, chained to a tree, and burned to death on a corner lot downtown in front of a crowd of over 1,000 people.\n\nIn the South in the antebellum era, members of the abolitionist movement or other people who opposed slavery were sometimes victims of mob violence. The largest lynching during the war and perhaps the largest lynching in all of U.S. history, was the lynching of 41 men in the Great Hanging at Gainesville, Texas in October 1862. Most of the victims were hanged after an extrajudicial \"trial\" but at least fourteen of them did not receive that formality. The men had been accused of insurrection or treason. Five more men were hanged in Decatur, Texas as part of the same sweep.\n\nAfter the war, southern whites struggled to maintain their social dominance. Secret vigilante and insurgent groups such as the Ku Klux Klan (KKK) instigated extrajudicial assaults and killings in order to keep whites in power and discourage freedmen from voting, working and getting educated. They also sometimes attacked Northerners, teachers, and agents of the Freedmen's Bureau. A study of the period from 1868 to 1871 estimates that the KKK was involved in more than 400 lynchings. The aftermath of the war was a period of upheaval and social turmoil, in which most white men had been war veterans. Mobs usually alleged crimes for which they lynched blacks. In the late 19th century, however, journalist Ida B. Wells showed that many presumed crimes were either exaggerated or had not even occurred.\n\nFrom the 1890s onwards, the majority of those lynched were black, including at least 159 women. Between 1882 and 1968, the Tuskegee Institute recorded 1,297 lynchings of whites and 3,446 lynchings of blacks. However, lynchings of members of other ethnic groups, such as Mexicans and Chinese, were undercounted in the Tuskegee Institute's records. One of the largest mass lynchings in American history occurred in 1891, when a mob lynched eleven Italian immigrants in New Orleans, Louisiana, following their acquittal on charges that they had killed the local police chief. The largest lynching was the Chinese massacre of 1871.\n\nMob violence arose as a means of enforcing white supremacy and it frequently verged on systematic political terrorism. \"The Ku Klux Klan, paramilitary groups, and other whites united by frustration and anger ruthlessly defended the interests of white supremacy. The magnitude of the extralegal violence which occurred during election campaigns reached epidemic proportions, leading the historian William Gillette to label it guerrilla warfare.\"\nDuring Reconstruction, the Ku Klux Klan and others used lynching as a means to control blacks, forcing them to work for planters and preventing them from exercising their right to vote. Federal troops and courts enforcing the Civil Rights Act of 1871 largely broke up the Reconstruction-era Klan.\n\nBy the end of Reconstruction in 1877, with fraud, intimidation and violence at the polls, white Democrats regained nearly total control of the state legislatures across the South. They passed laws to make voter registration more complicated, reducing black voters on the rolls. In the late 19th and early 20th centuries, from 1890 to 1908, ten of eleven Southern legislatures ratified new constitutions and amendments to effectively disenfranchise most African Americans and many poor whites through devices such as poll taxes, property and residency requirements, and literacy tests. Although required of all voters, some provisions were selectively applied against African Americans. In addition, many states passed grandfather clauses to exempt white illiterates from literacy tests for a limited period. The result was that black voters were stripped from registration rolls and without political recourse. Since they could not vote, they could not serve on juries. They were without official political voice.\n\nThe ideology behind lynching, directly connected with the denial of political and social equality, was stated forthrightly by Benjamin Tillman, governor of South Carolina and later a United States Senator:\n\nLynchings declined briefly after the takeover in the 1870s. By the end of the 19th century, with struggles over labor and disenfranchisement, and continuing agricultural depression, lynchings rose again. The number of lynchings peaked at the end of the 19th century, but these kinds of murders continued into the 20th century. Tuskegee Institute records of lynchings between the years 1880 and 1951 show 3,437 African-American victims, as well as 1,293 white victims. Lynchings were concentrated in the Cotton Belt (Mississippi, Georgia, Alabama, Texas and Louisiana).\n\nThe rapid influx of blacks into the North during the Great Migration of the early 20th century disturbed the racial balance within Northern cities, exacerbating hostility between both black and white Northerners. Many whites defended their space with violence, intimidation, or legal tactics toward blacks, while many other whites migrated to more racially homogeneous regions, a process known as white flight. Overall, blacks in Northern cities experienced systemic discrimination in a plethora of aspects of life. Throughout this period, racial tensions exploded, most violently in Chicago, and lynchings—mob-directed hangings—increased dramatically in the 1920s.\n\nAfrican Americans resisted through protests, marches, lobbying Congress, writing of articles, rebuttals of so-called justifications of lynching, organizing women's groups against lynching, and organizing integrated groups against lynching. African-American playwrights produced 14 anti-lynching plays between 1916 and 1935, ten of them by women.\n\nAfter the release of the movie \"The Birth of a Nation\" (1915), which glorified lynching and the Reconstruction-era Klan, the Klan re-formed. Unlike its earlier form, it was heavily represented among urban populations, especially in the Midwest. In response to the massive immigration of people from Southern and Eastern Europe, the Klan espoused an anti-immigrant, anti-Catholic and anti-Jewish stance, in addition to exercising the oppression of blacks.\n\nMembers of mobs that participated in lynchings often took photographs of what they had done to their victims in order to spread awareness and fear of their power. Some of those photographs were published and sold as postcards. In 2000, James Allen published a collection of 145 lynching photos in book form as well as online, with written words and video to accompany the images.\n\nThe Dyer Anti-Lynching Bill was first introduced to the United States Congress in 1918 by Republican Congressman Leonidas C. Dyer of St. Louis, Missouri. The bill was passed by the United States House of Representatives in 1922, and in the same year it was given a favorable report by the United States Senate Committee. Its passage was blocked by white Democratic senators from the Solid South, the only representatives elected since the southern states had disenfranchised African Americans around the start of the 20th century. The Dyer Bill influenced later anti-lynching legislation, including the Costigan-Wagner Bill. The Dyer and Costigan Wagner bills were blocked by Senator William Borah (R-ID).\n\nAs passed by the House, the Dyer Anti-Lynching Bill stated: \n\"To assure to persons within the jurisdiction of every State the equal protection of the laws, and to punish the crime of lynching... Be it enacted by the Senate and House of Representatives of the United States of America in Congress assembled, That the phrase 'mob or riotous assemblage,' when used in this act, shall mean an assemblage composed of three or more persons acting in concert for the purpose of depriving any person of his life without authority of law as a punishment for or to prevent the commission of some actual or supposed public offense.\"\n\nWhile the frequency of lynching dropped in the 1930s, there was a spike in 1930 during the Great Depression. For example, in North Texas and southern Oklahoma alone, four people were lynched in separate incidents in less than a month. In his book \"Russia Today: What Can We Learn from It?\" (1934), Sherwood Eddy wrote: \"In the most remote villages of Russia today Americans are frequently asked what they are going to do to the Scottsboro Negro boys and why they lynch Negroes.\" A spike in lynchings occurred after World War II, as tensions arose after veterans returned home. Whites tried to re-impose white supremacy over returning black veterans. The last documented mass lynching occurred in Walton County, Georgia, in 1946, when two war veterans and their wives were killed by local white landowners. \n\nBy the 1950s, the Civil Rights Movement was gaining new momentum. It was spurred by the lynching of Emmett Till, a 14-year-old youth from Chicago who was killed while visiting an uncle in Mississippi. His mother insisted on having an open-casket funeral so that people could see how badly her son had been beaten. The black community throughout the U.S. became mobilized. Vann R. Newkirk wrote \"the trial of his killers became a pageant illuminating the tyranny of white supremacy\". The state of Mississippi tried two defendants, but they were acquitted by an all-white jury.\n\nDavid Jackson writes that it was the photograph of the \"child’s ravaged body, that forced the world to reckon with the brutality of American racism.\" Soviet media frequently covered racial discrimination in the U.S. Deeming American criticism of Soviet Union human rights abuses at this time as hypocrisy, the Russians responded with \"And you are lynching Negroes\". In \"Cold War Civil Rights: Race and the Image of American Democracy\" (2001), the historian Mary L. Dudziak wrote that Soviet Communist criticism of racial discrimination and violence in the United States influenced the federal government to support civil rights legislation.\n\nMost, but not all, lynchings ceased by the 1960s. The murder of James Craig Anderson in Mississippi in 2011 was the last recorded fatal lynching in the United States. Jasmine Richards was convicted of \"lynching\" in 2016, but her conviction was for \"taking by means of a riot of any person from the lawful custody of a peace officer\" (California penal code 405a), not for killing anybody.\n\nTitle 18, U.S.C., Section 241, is the civil rights conspiracy statute, which makes it unlawful for two or more persons to conspire to injure, oppress, threaten, or intimidate any person of any state, territory, or district in the free exercise or enjoyment of any right or privilege secured to him/her by the Constitution or the laws of the United States (or because of his/her having exercised the same) and further makes it unlawful for two or more persons to go in disguise on the highway or premises of another person with intent to prevent or hinder his or her free exercise or enjoyment of such rights. Depending upon the circumstances of the crime, and any resulting injury, the offense is punishable by a range of fines and/or imprisonment for any term of years up to life, or the death penalty.\n\nThe term 'felony lynching' was used in California law. It described the act of taking someone out of the custody of a police officer by \"means of riot\". It does not refer to the act of lynching, or murder by physical violence. This statute has been used to charge individuals who have tried to free someone from police custody. There have been several notable cases in the twenty-first century, some controversial, when a black person has attempted to free another black person from police custody. In 2015, Governor Jerry Brown signed legislation by Senator Holly Mitchell removing the word \"lynching\" from the state's criminal code without comment after it received unanimous approval in a vote by state lawmakers. Mitchell stated, \"It's been said that strong words should be reserved for strong concepts, and 'lynching' has such a painful history for African Americans that the law should only use it for what it is – murder by mob.\" The law was otherwise unchanged.\n\nA 2017 study found that exposure to lynchings in the post-Reconstruction South \"reduced local black voter turnout by roughly 2.5 percentage points.\" There was other violence directed at blacks, particularly in campaign season. Most significantly for voting, from 1890 to 1908, southern states passed new constitutions and laws that disenfranchised most blacks due to barriers to voter registration. These actions had major effects, soon reducing black voter turnout in most of the South to insignificant amounts.\n\nAnother 2017 study found supportive evidence of Stewart Tolnay and E. M. Beck's claim that lynchings were \"due to economic competition between African American and white cotton workers\". The study found that lynchings were associated with greater black out-migration from 1920 to 1930, and higher state-level wages.\n\nIn Britain, a series of race riots broke out in several cities in 1919 between whites and black sailors. In Liverpool, after a black sailor had been stabbed by two whites in a pub, his friends attacked the pub in revenge. In response, the police raided lodging houses with black occupants, accompanied by an \"enraged lynch mob\". Charles Wootton, a young black seaman who had not been involved in the attacks, was chased into the river Mersey and drowned after being pelted with missiles thrown by the mob, who chanted \"Let him drown!\" The Charles Wootton College in Liverpool was named in his memory.\n\nIn 1944, Wolfgang Rosterg, a German prisoner of war known to be unsympathetic to the Nazi regime, was lynched by Nazis in POW Camp 21 in Comrie, Scotland. At the end of the war, five of the perpetrators were hanged at Pentonville Prison – the largest multiple execution in 20th-century Britain.\n\nThe situation is less clear with regards to reported \"lynchings\" in Germany. Nazi propaganda sometimes tried to depict state-sponsored violence as spontaneous lynchings. The most notorious instance of this was \"Kristallnacht\", which the government portrayed as the result of \"popular wrath\" against Jews, but it was carried out in an organised and planned manner, mainly by SS men. Similarly, the approximately 150 confirmed murders of surviving crew members of crashed Allied aircraft in revenge for what Nazi propaganda called \"Anglo-American bombing terror\" were chiefly conducted by German officials and members of the police or the Gestapo, although civilians sometimes took part in them. The execution of enemy aircrew without trial in some cases had been ordered by Hitler personally in May 1944. Publicly it was announced that enemy pilots would no longer be protected from \"public wrath\". There were secret orders issued that prohibited policemen and soldiers from interfering in favor of the enemy in conflicts between civilians and Allied forces, or prosecuting civilians who engaged in such acts. In summary,\n\nLynching of members of the Turkish Armed Forces occurred in the aftermath of the 2016 Turkish \"coup d'état\" attempt.\n\nOn November 23, 2004, in the Tlahuac lynching, three Mexican undercover federal agents investigating a narcotics-related crime were lynched in the town of San Juan Ixtayopan (Mexico City) by an angry crowd who saw them taking photographs and suspected that they were trying to abduct children from a primary school. The agents immediately identified themselves but they were held and beaten for several hours before two of them were killed and set on fire. The incident was covered by the media almost from the beginning, including their pleas for help and their murder.\n\nBy the time police rescue units arrived, two of the agents were reduced to charred corpses and the third was seriously injured. Authorities suspect that the lynching was provoked by the persons who were being investigated.\nBoth local and federal authorities had abandoned the agents, saying that the town was too far away for them to try to intervene. Some officials said they would provoke a massacre if the authorities tried to rescue the men from the mob.\n\nIn May 2015, a sixteen-year-old girl was lynched in Rio Bravo by a vigilante mob after being accused of involvement in the killing of a taxi driver earlier in the month.\n\nExtrajudicial punishment, including lynching, of alleged criminals who committed various crimes, ranging from theft to murder, has some endorsement in Dominican society. According to a 2014 Latinobarómetro survey, the Dominican Republic had the highest rate of acceptance in Latin America of such unlawful measures. These issues are particularly evident in the Northern Region.\n\nAfter the 2010 earthquake the slow distribution of relief supplies and the large number of affected people created concerns about civil unrest, marked by looting and mob justice against suspected looters. In a 2010 news story, CNN reported, \"At least 45 people, most of them Vodou priests, have been lynched in Haiti since the beginning of the cholera epidemic by angry mobs blaming them for the spread of the disease, officials said.\n\nThe practice of whipping and necklacing offenders and political opponents evolved in the 1980s during the apartheid era in South Africa. Residents of black townships formed \"people's courts\" and used whip lashings and deaths by necklacing in order to terrorize fellow blacks who were seen as collaborators with the government. Necklacing is the torture and execution of a victim by igniting a kerosene-filled rubber tire that has been forced around the victim's chest and arms. Necklacing was used to punish victims who were alleged to be traitors to the black liberation movement along with their relatives and associates. Sometimes the \"people's courts\" made mistakes, or they used the system to punish those whom the anti-Apartheid movement's leaders opposed. A tremendous controversy arose when the practice was endorsed by Winnie Mandela, then the wife of the then-imprisoned Nelson Mandela and a senior member of the African National Congress.\n\nMore recently, drug dealers and other gang members have been lynched by People Against Gangsterism and Drugs, a vigilante organization.\n\nThe practice of extrajudicial punishments, including lynching, is referred to as 'jungle justice' in Nigeria. The practice is widespread and \"an established part of Nigerian society\", predating the existence of the police. Exacted punishments vary between a \"muddy treatment\", that is, being made to roll in the mud for hours and severe beatings followed by necklacing. The case of the \"Aluu four\" sparked national outrage. The absence of a functioning judicial system and law enforcement, coupled with corruption are blamed for the continuing existence of the practice.\n\nPalestinian lynch mobs have murdered Palestinians suspected of collaborating with Israel. According to a Human Rights Watch report from 2001:\nIn October 2000, two Israeli reservists, serving as drivers, mistakenly entered Ramallah and were killed by a Palestinian crowd. Their bodies were mutilated and dragged to Al-Manara Square in the city center. Palestinian policemen did not prevent, and in some cases actually took part in the lynching.\n\nIn July 2014, three Israeli men kidnapped Mohammed Abu Khdeir while he was waiting for dawn Ramadan prayers outside of his house in Eastern Jerusalem. They forced him into their car and beat him while driving to the deserted forest area new Jerusalem, then poured gasoline on him and set him on fire after being tortured and beat multiple times. On 30 November 2015, the two minors involved were found guilty of Khdeirs' murder, and were respectively sentenced to life and 21 years imprisonment on 4 February. On 3 May 2016, Ben David was sentenced to life in prison and an additional 20 years.\n\nOn March 19, 2015 in Kabul, Afghanistan a large crowd beat a young woman, Farkhunda, after she was accused by a local mullah of burning a copy of the Quran, Islam's holy book. Shortly afterwards, a crowd attacked her and beat her to death. They set the young woman's body on fire on the shore of the Kabul River. Although it was unclear whether the woman had burned the Quran, police officials and the clerics in the city defended the lynching, saying that the crowd had a right to defend their faith at all costs. They warned the government against taking action against those who had participated in the lynching. The event was filmed and shared on social media. The day after the incident six men were arrested on accusations of lynching, and Afghanistan's government promised to continue the investigation. On March 22, 2015, Farkhunda's burial was attended by a large crowd of Kabul residents; many demanded that she receive justice. A group of Afghan women carried her coffin, chanted slogans and demanded justice.\n\nIn India, lynchings may reflect internal tensions between ethnic communities. An example is the 2006 Kherlanji massacre, where four members of a Dalit caste family were slaughtered by Kunbi caste members in Khairlanji, a village in the Bhandara district of Maharashtra. Though this incident was reported as an example of \"upper\" caste violence against members of a \"lower\" caste, it was found to be an example of communal violence. It was retaliation against a family who had opposed the Eminent Domain seizure of its fields so a road could be built that would have benefitted the group who murdered them. The women of the family were paraded naked in public, before being mutilated and murdered. Sociologists and social scientists reject attributing racial discrimination to the caste system and attributed this and similar events to intra-racial ethno-cultural conflicts.\n\nThere have been numerous lynchings in relation to cow vigilante violence in India since 2014, mainly involving Hindu mobs lynching Indian Muslims and Dalits. Some notable examples of such attacks include the 2015 Dadri mob lynching, the 2016 Jharkhand mob lynching, and the 2017 Alwar mob lynching. The Alwar mob lynching repeated again in July 2018, where a group of cow vigilantes killed the 31 year old Muslim man called Rakbar Khan. \n\nIn the 2015 Dimapur mob lynching, a mob in Dimapur, Nagaland, broke into a jail and lynched an accused rapist on 5 March 2015 while he was awaiting trial.\n\nSince May 2017, when seven people were lynched in Jharkhand, India has experienced a spate of mob-related violence and killings known as the Indian Whatsapp lynchings following the spread of fake news, primarily relating to child-abduction and organ harvesting, via the Whatsapp message service.\n\nIn 1937, Abel Meeropol, a Jewish schoolteacher from New York, saw a copy of the photograph of a lynching in Marion, Indiana. He said that the photograph \"haunted me for days\" and inspired him to write the poem \"Strange Fruit\". It was published in the \"New York Teacher\" and later in the magazine \"New Masses\", in both cases under the pseudonym Lewis Allan. This poem was set to music, also written by Meeropol, and the song was performed and popularized by Billie Holiday. The song reached 16th place on the charts in July 1939. The song has been performed by many other singers, including Nina Simone.\n\nThe finale of the 2015 film \"The Hateful Eight\" set in post Civil War America is a detailed and close focused depiction of the lynching of a white woman, prompting some debate about whether it is a political commentary on racism and hate in America or if it was simply created for entertainment value.\n\nDutch politicians Johan de Witt and his brother Cornelis were assassinated by a carefully organised lynch mob in 1672. A Dutch biographial film, \"Michiel de Ruyter\", called \"Admiral\" in the English version, depicts the lynching and the background history.\n\n\n\n\n"}
{"id": "55808872", "url": "https://en.wikipedia.org/wiki?curid=55808872", "title": "Mainland Asian tiger", "text": "Mainland Asian tiger\n\nThe Mainland Asian tiger (\"Panthera tigris tigris\") is a term used for the following tiger populations in continental Asia:\n\n"}
{"id": "231237", "url": "https://en.wikipedia.org/wiki?curid=231237", "title": "Managed intensive rotational grazing", "text": "Managed intensive rotational grazing\n\nIn agriculture, Managed intensive rotational grazing (MIRG), also known as simply as managed grazing or cell grazing, mob grazing and holistic managed planned grazing, describes a variety of closely related systems of forage use in which ruminant and non-ruminant herds and/or flocks are regularly and systematically moved to fresh rested areas with the intent to maximize the quality and quantity of forage growth.\n\nOne primary goal of MIRG is to have a vegetative cover over all grazed areas at all times, and to prevent the complete removal of all vegetation from the grazed areas (\"bare dirt\")\n\nMIRG can be used with cattle, sheep, goats, pigs, chickens, turkeys, ducks and other animals. The herds graze one portion of pasture, or a paddock, while allowing the others to recover. The length of time a paddock is grazed will depend on the size of the herd and the size of the paddock and local environmental factors. Resting grazed lands allows the vegetation to renew energy reserves, rebuild shoot systems, and deepen root systems, with the result being long-term maximum biomass production. MIRG is especially effective because grazers do better on the more tender younger plant stems. MIRG also leave parasites behind to die off minimizing or eliminating the need for de-wormers. Pasture systems alone can allow grazers to meet their energy requirements, and with the increased productivity of MIRG systems, the grazers obtain the majority of their nutritional needs without the supplemental feed sources that are required in continuous grazing systems.\n\nOne key element of this style of animal husbandry is that either each grazed area must contain all elements needed for the animals (water source, for instance) or the feed or water source must be moved each time the animals are moved. Having fixed feeding or watering stations defeats the rotational aspect, leading to degradation of the ground around the water supply or feed supply if additional feed is provided to the animals. Special care must be taken to ensure that high use areas do not become areas where mud, parasites or diseases are spread or communicated\n\nRuminal tympany, also known as bloat, is a common problem in grazing systems for ruminants, although not for pigs or poultry, that if left untreated can lead to animal death. This problem occurs when foam producing compounds in plants are digested by cows, causing foam to form in the rumen of the animal and ultimately prohibiting animals from expelling gas. The risk of bloat can be mitigated by seeding non-bloating legumes with the grasses. Animals are especially susceptible to bloat if they are moved to new pasture sources when they are particularly hungry. It is therefore important to ensure that the herd is eating enough at the end of a rotation when forage will be more scarce, limiting the potential for animals to gorge themselves when turned onto new paddocks.\n\nSeveral problems are related to shade in pasture areas. Although shade provides relief from heat and reduces the risk of heat stress, animals tend to congregate in these areas which leads to nutrient loading, uneven grazing, and potential soil erosion. Taller shade trees move the shade area around as the day progresses minimizing this problem.\n\nHerd health benefits arise from animals having access to both space and fresh air. Freedom of movement within a paddock results in increased physical fitness, which limits the potential for injuries and abrasion, and reduces the potential of exposure to high levels of harmful disease-causing microorganisms and insects.\n\nIn a concentrated animal feeding operation (CAFO), it is considered normal for a large number of animals to continuously occupy a very small area. The aisles that animals use to move around are consequently constantly coated with a moist layer of manure and urine from the many animals, leading to ailments such as foot rot due to the constant wet exposure. The manure and urine is usually just scraped off into gutters below slatted surfaces, and the surfaces and gutters are rarely washed completely clean, so molds, bacteria, and insects can grow and thrive in the potentially infectious waste. Feeding areas in a CAFO are also rarely stripped bare and washed with a disinfectant, as this would reduce the time available for animals to eat continuously, so molds and bacteria are also able to become established where the animals eat. The close confinement and lack of general environmental cleanliness leads to easier spread of infection and increased sickness, requiring the regular feeding of antibiotics to keep the confined animals healthy but which also leads to antibiotic resistance for bacteria constantly present in the CAFO.\n\nBy comparison, with managed grazing, the animals are able to spread out and exist in a natural environment more suited for their natural growth and development. As the animals move to a new paddock, wastes are left behind and allowed to decay without the animals nearby. The animals experience less disease without the need for regular antibiotic dosing, and fewer foot ailments.\n\nIn general, a well managed rotational grazing system has rather low pasture weed establishment because the majority of niches are already filled with established forage species, making it hard for competing weeds to emerge and become established. The use of multiple species in the co-grazing helps to minimize weeds. Established forage plants in rotational grazing pasture systems are healthy and unstressed due to the \"rest\" period, enhancing the competitive advantage of the forage. Additionally, in comparison to cash grain crop production, many plants which would be considered weeds are not problematic in perennial pasture. Many of these plants are actually nutritious to grazers and control of these plants is therefore not necessary in management intensive rotational systems. However, certain species such as thistles and various other weeds, are indigestible and possibly poisonous to grazers. These plant species will not be grazed by the herd and can be recognized for their prevalence in pasture systems.\n\nA key step in managing weeds in any pasture system is identification. Once the undesired species in a pasture system are identified, an integrated approach of management can be implemented to control weed populations. It is important to recognize that no single approach to weed management will result in weed free pastures; therefore, various cultural, mechanical, and chemical control methods can be combined in an integrated weed management plan. Cultural controls include: avoiding spreading manure contaminated with weed seeds, cleaning equipment after working in weed infested areas, and managing weed problems in fencerows and other areas near pastures. Mechanical controls such as repeated mowing, clipping, and hand weeding can also be used to effectively manage weed infestations by weakening the plant. These methods should be implemented when weed flower buds are closed or just starting to open to prevent seed production. Although these methods for managing weeds greatly reduces reliance on herbicides, weed problems may still persist in managed grazing systems and the use of herbicides may become necessary. Use of herbicides may restrict the use of a pasture for some length of time, depending on the type and amount of the chemical used. Frequently, weeds in pasture systems are patchy and therefore spot treatment of herbicides may be used as a least cost method of chemical control.\n\nIf pasture systems are seeded with more than 40% legumes, commercial nitrogen fertilization is unnecessary for adequate plant growth. Legumes are able to fix atmospheric nitrogen, thus providing nitrogen for themselves and surrounding plants. \nAlthough grazers remove nutrient sources from the pasture system when they feed on forage sources, the majority of the nutrients consumed by the herd are returned to the pasture system through manure. At a relatively high stocking rate, or high ratio of animals per hectare, manure will be evenly distributed across the pasture system. The nutrient content in these manure sources should be adequate to meet plant requirements, making commercial fertilization unnecessary. Management intensive rotational grazing systems are often associated with increased soil fertility which arises because manure is a rich source of organic matter that increases the health of soil. In addition, these pasture system are less susceptible to erosion because the land base has continuous ground cover throughout the year.\n\nHigh levels of fertilizers entering waterways are a pertinent environmental concern associated with agricultural systems. However, management intensive rotational grazing systems effectively reduce the amount of nutrients that move off-farm which have the potential to cause environmental degradation. These systems are fertilized with on-farm sources, and are less prone to leaching as compared to commercial fertilizers. Additionally, the system is less prone to excess nutrient fertilization, so the majority of nutrients put into the system by manure sources are utilized for plant growth. Permanent pasture systems also have deeper, well established forage root systems which are more efficient at taking up nutrients from within the soil profile.\n\nAlthough milk yields are often lower in MIRG systems, net farm income per cow is often greater as compared to confinement operations. This is due to the additional costs associated with herd health and purchased feeds are greatly reduced in management intensive rotational grazing systems. Additionally, a transition to management intensive rotational grazing is associated with low start-up and maintenance costs. Another consideration is that while production per cow is less, the number of cows per acre on the pasture can increase. The net effect is more productivity per acre at less cost.\n\nThe main costs associated with transitioning to management intensive rotational grazing are purchasing fencing, fencers, and water supply materials. If a pasture was continuously grazed in the past, likely capital has already been invested in fencing and a fencer system. Cost savings to graziers can also be recognized when one considers that many of the costs associated with livestock operations are transmitted to the grazers. For example, the grazers actively harvest their own sources of food for the portion of the year where grazing is possible. This translates into lower costs for feed production and harvesting, which are fuel intensive endeavors. MIRG systems rely on the grazers to produce fertilizer sources via their excretion. There is also no need for collection, storage, transportation, and application of manure, which are also all fuel intensive. Additionally, external fertilizer use contributes to other costs such as labor, purchasing costs.\n\nIt can also be demonstrated that management intensive rotational grazing system also result in time savings because the majority of work which might otherwise require human labor is transmitted to the herd.\n\nMany pastures undergoing MIRG are less susceptible to soil erosion and are associated with higher soil fertility than continuously grazed pastures, depending on the skill of the manager and the management system they are using. As a result, the paddocks require fewer commercial inputs, which have been associated with negative environmental impacts. In addition, because these systems tend to be more resilient and stable they are more capable of responding to changing environmental conditions and perturbations while not compromising productivity.\n\nAnimals raised on pasture have shown major differences in the nutritional quality of the products they produce for human consumption.\n\nManaged intensive rotational grazing paints a wide brush over many different managed grazing systems. Managers have found that rotational grazing systems can work for diverse management purposes, but scientific experiments have demonstrated that rotational grazing systems do not always necessarily work for specific ecological purposes. This controversy stems from two main categorical differences in rotational grazing, prescribed management and adaptive management. The performance of rangeland grazing strategies are similarly constrained by several ecological variables establishing that differences among them are dependent on the effectiveness of those management models. Depending on the management model, plant production has been shown to be equal or greater in continuous compared to rotational grazing in 87% of the experiments.\n\n\n"}
{"id": "25916838", "url": "https://en.wikipedia.org/wiki?curid=25916838", "title": "Matilda effect", "text": "Matilda effect\n\nThe Matilda effect is a bias against acknowledging the achievements of those women scientists whose work is attributed to their male colleagues. This effect was first described by suffragist and abolitionist Matilda Joslyn Gage (1826–98) in her essay, \"Woman as Inventor\". The term \"Matilda effect\" was coined in 1993 by science historian Margaret W. Rossiter.\n\nRossiter provides several examples of this effect. Trotula (Trota of Salerno), a 12th-century Italian woman physician, wrote books which, after her death, were attributed to male authors. Nineteenth- and twentieth-century cases illustrating the Matilda effect include those of Nettie Stevens, Maria Skłodowska Curie, Lise Meitner, Marietta Blau, Rosalind Franklin, and Jocelyn Bell Burnell.\n\nThe Matilda effect is sometimes compared to the Matthew effect, whereby an eminent scientist often gets more credit than a comparatively unknown researcher, even if their work is shared or similar.\n\nProfessor Ben Barres (born 1955), a neurobiologist at Stanford University Medical School who transitioned from female to male, has spoken of his scientific achievements having been perceived differently, depending on his sex at the time. This offers one account of biases experienced from different identities, as perceived by one individual.\n\nIn 2012, two female researchers from Radboud University Nijmegen showed that in the Netherlands the sex of professorship candidates influences the evaluation made of them. Similar cases are described by two Italian female researchers in a study corroborated further by a Spanish study.\nOn the other hand, several studies found no difference between citations and impact of publications of male authors and those of female authors.\n\nSwiss researcher have indicated that mass media ask male scientists more often to contribute on shows than they do their female fellow scientists.\n\nAccording to one U.S. study, \"although overt gender discrimination generally continues to decline in American society,\" \"women continue to be disadvantaged with respect to the receipt of scientific awards and prizes, particularly for research.\"\n\nExamples of women subjected to the Matilda effect:\nExamples of men scientists favored over women scientists for Nobel Prizes\n\n\n"}
{"id": "34384656", "url": "https://en.wikipedia.org/wiki?curid=34384656", "title": "Metafunction", "text": "Metafunction\n\nThe term \"metafunction\" originates in systemic functional linguistics and is considered to be a property of all languages. Systemic functional linguistics is functional and semantic rather than formal and syntactic in its orientation. As a functional linguistic theory, it claims that both the emergence of grammar and the particular forms that grammars take should be explained “in terms of the functions that language evolved to serve”. While languages vary in how and what they do, and what humans do with them in the contexts of human cultural practice, all languages are considered to be shaped and organised in relation to three functions, or metafunctions. Michael Halliday, the founder of systemic functional linguistics, calls these three functions the \"ideational\", \"interpersonal\", and \"textual\". The ideational function is further divided into the \"experiential\" and \"logical\".\n\nMetafunctions are \"systemic clusters\"; that is, they are groups of semantic systems that make meanings of a related kind. The three metafunctions are mapped onto the structure of the clause. For this reason, systemic linguists analyse a clause from three perspectives. Halliday argues that the concept of metafunction is one of a small set of principles that are necessary to explain how language works; this concept of function in language is necessary to explain the organisation of the semantic system of language. Function is considered to be \"a fundamental property of language itself\".\n\nAccording to Ruqaiya Hasan, the metafunctions in SFL \"are not hierarchised; they have equal status, and each is manifested in every act of language use: in fact, an important task for grammatics is to describe how the three metafunctions are woven together into the same linguistic unit\". Hasan argues that this is one way in which Halliday's account of the functions of language is different from that of Karl Bühler, for example, for whom functions of language are hierarchically ordered, with the referential function the most important of all. For Buhler, the functions were considered to operate one at a time. In SFL, the metafunctions operate simultaneously, and any utterance is a harmony of choices across all three functions.\n\nThe ideational function is language concerned with building and maintaining a theory of experience. It includes the experiential function and the logical function.\n\nThe experiential function refers to the grammatical choices that enable speakers to make meanings about the world around us and inside us:\n\nHalliday argues that it was through this process of humans making meaning from experience that language evolved. Thus, the human species had to “make sense of the complex world in which it evolved: to classify, or group into categories, the objects and events within its awareness”. These categories are not given to us through our senses; they have to be “construed”. In taking this position on the active role of grammar in construing “reality”, Halliday was influenced by Whorf.\n\nHalliday describes the logical function as those systems “which set up logical–semantic relationships between one clausal unit and another” The systems which come under the logical function are and . When two clauses are combined, a speaker chooses whether to give both clauses equal status, or to make one dependent on the other. In addition, a speaker choose some meaning relation in the process of joining or binding clauses together. Halliday argues that the meanings we make in such processes are most closely related to the experiential function. For this reason, he puts the experiential and logical functions together into the ideational function.\n\nThe interpersonal function refers to the grammatical choices that enable speakers to enact their complex and diverse interpersonal relations. This tenet of systemic functional linguistics is based on the claim that a speaker not only talks about something, but is always talking to and with others. Language not only construes experience, but simultaneously acts out “the interpersonal encounters that are essential to our survival”. Halliday argues that these encounters:\n\nThe grammatical systems that relate to the interpersonal function include Mood, Modality, and Polarity.\n\nHalliday argues that both experiential and interpersonal functions are intricately organized, but that between the two “there is comparatively very little constraint”. This means that “by and large, you can put any interactional ‘spin’ on any representational content”. What allows meanings from these two modes to freely combine is the intercession of a third, distinct mode of meaning that Halliday refers to as the textual function. The term encompasses all of the grammatical systems responsible for managing the flow of discourse. These systems “create coherent text – text that coheres within itself and with the context of situation” They are both structural (involving choices relating to the ordering of elements in the clause), and non-structural (involving choices that create cohesive ties between units that have no structural bond). The relevant grammatical systems include Theme, Given and New, as well as the systems of cohesion, such as Reference, Substitution, and Ellipsis. Halliday argues that the textual function is distinct from both the experiential and interpersonal because its object is language itself. Through the textual function, language “creates a semiotic world of its own: a parallel universe, or ‘virtual reality’ in modern terms”.\n"}
{"id": "30865852", "url": "https://en.wikipedia.org/wiki?curid=30865852", "title": "O-minimal theory", "text": "O-minimal theory\n\nIn mathematical logic, and more specifically in model theory, an infinite structure (\"M\",<...) which is totally ordered by < is called an o-minimal structure if and only if every definable subset \"X\" ⊂ \"M\" (with parameters taken from \"M\") is a finite union of intervals and points.\n\nO-minimality can be regarded as a weak form of quantifier elimination. A structure \"M\" is o-minimal if and only if every formula with one free variable and parameters in \"M\" is equivalent to a quantifier-free formula involving only the ordering, also with parameters in \"M\". This is analogous to the minimal structures, which are exactly the analogous property down to equality.\n\nA theory \"T\" is an o-minimal theory if every model of \"T\" is o-minimal. It is known that the complete theory \"T\" of an o-minimal structure is an o-minimal theory. This result is remarkable because, in contrast, the complete theory of a minimal structure need not be a strongly minimal theory, that is, there may be an elementarily equivalent structure which is not minimal.\n\nO-minimal structures can be defined without recourse to model theory. Here we define a structure on a nonempty set \"M\" in a set-theoretic manner, as a sequence \"S\" = (\"S\"), \"n\" = 0,1,2... such that\n\nIf \"M\" has a dense linear order without endpoints on it, say <, then a structure \"S\" on \"M\" is called o-minimal if it satisfies the extra axioms\n\nThe \"o\" stands for \"order\", since any o-minimal structure requires an ordering on the underlying set.\n\nO-minimal structures originated in model theory and so have a simpler — but equivalent — definition using the language of model theory. Specifically if \"L\" is a language including a binary relation <, and (\"M\",<...) is an \"L\"-structure where < is interpreted to satisfy the axioms of a dense linear order, then (\"M\",<...) is called an o-minimal structure if for any definable set \"X\" ⊆ \"M\" there are finitely many open intervals \"I\"..., \"I\" with endpoints in \"M\" ∪ {±∞} and a finite set \"X\" such that\n\nExamples of o-minimal theories are:\n\nIn the case of RCF, the definable sets are the semialgebraic sets. Thus the study of o-minimal structures and theories generalises real algebraic geometry. A major line of current research is based on discovering expansions of the real ordered field that are o-minimal. Despite the generality of application, one can show a great deal about the geometry of set definable in o-minimal structures. There is a cell decomposition theorem, Whitney and Verdier stratification theorems and a good notion of dimension and Euler characteristic.\n\n\n\n"}
{"id": "9315395", "url": "https://en.wikipedia.org/wiki?curid=9315395", "title": "Parametricity", "text": "Parametricity\n\nIn programming language theory, parametricity is an abstract uniformity property enjoyed by parametrically polymorphic functions, which captures the intuition that all instances of a polymorphic function act the same way.\n\nConsider this example, based on a set \"X\" and the type \"T\"(\"X\") = [\"X\" → \"X\"] of functions from \"X\" to itself. The higher-order function \"twice\" : \"T\"(\"X\") → \"T\"(\"X\") given by \"twice\"(\"f\") = \"f\" ∘ \"f\", is intuitively independent of the set \"X\". The family of all such functions \"twice\", parametrized by sets \"X\", is called a \"parametrically polymorphic function\". We simply write twice for the entire family of these functions and write its type as formula_1\"X\". \"T\"(\"X\") → \"T\"(\"X\"). The individual functions \"twice\" are called the \"components\" or \"instances\" of the polymorphic function. Notice that all the component functions \"twice\" act \"the same way\" because they are given by the same rule. Other families of functions obtained by picking one arbitrary function from each \"T\"(\"X\") → \"T\"(\"X\") would not have such uniformity. They are called \"\"ad hoc\" polymorphic functions\". \"Parametricity\" is the abstract property enjoyed by the uniformly acting families such as twice, which distinguishes them from \"ad hoc\" families. With an adequate formalization of parametricity, it is possible to prove that the parametrically polymorphic functions of type formula_1\"X\". \"T\"(\"X\") → \"T\"(\"X\") are one-to-one with natural numbers. The function corresponding to the natural number \"n\" is given by the rule \"f\" formula_3 \"f\", i.e., the polymorphic Church numeral for \"n\". In contrast, the collection of all \"ad hoc\" families would be too large to be a set.\n\nThe \"parametricity theorem\" was originally stated by John C. Reynolds, who called it the \"abstraction theorem\". In his paper \"Theorems for free!\", Philip Wadler described an application of parametricity to derive theorems about parametrically polymorphic functions based on their types.\n\nParametricity is the basis for many program transformations implemented in compilers for the Haskell programming language. These transformations were traditionally thought to be correct in Haskell because of Haskell's non-strict semantics. Despite being a lazy programming language, Haskell does support certain primitive operations—such as the operator codice_1—that enable so-called \"selective strictness\", allowing the programmer to force the evaluation of certain expressions. In their paper \"Free theorems in the presence of \"seq\"\", Patricia Johann and Janis Voigtlaender showed that because of the presence of these operations, the general parametricity theorem does not hold for Haskell programs; thus, these transformations are unsound in general.\n\n\n"}
{"id": "54102859", "url": "https://en.wikipedia.org/wiki?curid=54102859", "title": "Pavlovian-instrumental transfer", "text": "Pavlovian-instrumental transfer\n\nPavlovian-instrumental transfer (PIT) is a psychological phenomenon that occurs when a conditioned stimulus (CS, also known as a \"cue\") that has been associated with rewarding or aversive stimuli via classical conditioning alters motivational salience and operant behavior. Two distinct forms of Pavlovian-instrumental transfer have been identified in humans and other animals – specific PIT and general PIT – with unique neural substrates mediating each type. In relation to rewarding stimuli, specific PIT occurs when a CS is associated with a specific rewarding stimulus through classical conditioning and subsequent exposure to the CS enhances an operant response that is directed toward the same reward with which it was paired (i.e., it promotes approach behavior). General PIT occurs when a CS is paired with one reward and it enhances an operant response that is directed toward a different rewarding stimulus.\n\nAn example of specific PIT, as described by a neuroscience review from 2013 on Pavlovian-instrumental transfer, is as follows: \"in a typical experimental scenario a rat is trained to associate a sound (CS) with the delivery of food. Later, the rat undergoes an instrumental training where it learns to press a lever to get some food (without the sound being present). Finally, the rat is presented again with the opportunity to press the lever, this time both in the presence and absence of the sound. The results show that the rat will press the lever more in the presence of the sound than without, even if the sound has not been previously paired with lever pressing. The Pavlovian sound-food association learned in the first phase has somehow transferred to the instrumental situation, hence the name 'Pavlovian-instrumental transfer.'\"\n\nIn relation to rewarding stimuli, specific PIT occurs when a CS is associated with a specific rewarding stimulus through classical conditioning and subsequent exposure to the CS enhances an operant response that is directed toward the same reward with which it was paired (i.e., it promotes approach behavior). General PIT occurs when a CS is paired with one reward and it enhances an operant response that is directed toward a different rewarding stimulus. Neurobiological state factors (e.g., appetite and satiety states, stress level, drug states such as intoxication and withdrawal, etc.), and particularly the motivational state of an animal, strongly affect the amount of appetitive motivational salience (i.e., incentive salience) that a \"reward cue\" confers to an associated rewarding stimulus via Pavlovian-instrumental transfer. Acute stress amplifies the motivational salience that reward cues confer to rewarding stimuli through both specific and general PIT; however, chronic stress reduces the motivational impact reward cues.\n\nSpecific PIT and general PIT also occur with aversive stimuli and are defined analogously. Specific PIT with an aversive stimulus occurs when a CS is paired with an aversive stimulus and subsequent exposure to the CS enhances an operant response that is directed away from the aversive stimulus with which it was paired (i.e., it promotes escape and avoidance behavior). General PIT with an aversive stimulus occurs when a CS is paired with one aversive stimulus and it enhances an operant response that is directed away from a different aversive stimulus.\n\nBased upon studies on rats that involved PIT with rewards, specific PIT is mediated by the nucleus accumbens shell and basolateral amygdala, while general PIT is mediated by the nucleus accumbens core and central amygdala. Studies on humans which employed neuroimaging during PIT experiments with rewards appear to be consistent with these findings.\n\nDue to the effect of reward cues and Pavlovian-instrumental transfer on the amplification of incentive salience for rewarding stimuli, PIT is believed to be one of the mechanisms responsible for producing \"cue-triggered wanting\", or craving, for a drug that occurs when an individual with a drug addiction is exposed to drug cues even after long periods of abstinence. For example, anti-drug agencies previously used posters with images of drug paraphernalia – which is a type of drug cue – as an attempt to show the dangers of drug use. However, such posters are no longer used because of the effect of incentive salience in causing cravings and relapse upon sight of the stimuli illustrated in the posters.\n\nThe sight or smell of food which one has consumed and enjoyed in the past can elicit hunger (i.e., the motivation to eat) in humans, an effect which is presumably mediated through PIT. In PIT experiments with rats, the presentation of a conditioned stimulus which has been paired with food has been shown to increase instrumental actions that have been reinforced by food, such as pressing a lever which leads to the delivery of a food pellet.\n"}
{"id": "17645570", "url": "https://en.wikipedia.org/wiki?curid=17645570", "title": "Philosophical theory", "text": "Philosophical theory\n\nA philosophical theory or philosophical position is a set of beliefs that explains or accounts for a general philosophy or specific branch of philosophy. The use of the term theory here is a statement of colloquial English and not reflective of the term theory. While any sort of thesis or opinion may be termed a position, in analytic philosophy it is thought best to reserve the word \"theory\" for systematic, comprehensive attempts to solve problems.\n\nThe elements that comprise a philosophical position consist of statements which are believed to be true by the thinkers who accept them, and which may or may not be empirical. The sciences have a very clear idea of what a theory is; however in the arts such as philosophy, the definition is more hazy. Philosophical positions are not necessarily scientific theories, although they may consist of both empirical and non-empirical statements.\n\nIn essence, the collective statements of all philosophical movements, schools of thought, and belief systems consist of philosophical positions. Also included among philosophical positions are many principles, hypotheses, rules, paradoxes, laws, as well as 'ologies, 'isms, 'sis's, and effects.\n\nSome examples of philosophical positions include:\n\nPhilosophical positions may also take the form of a life stance, religion, world view, or ideology.\n\n"}
{"id": "9641126", "url": "https://en.wikipedia.org/wiki?curid=9641126", "title": "Posthegemony", "text": "Posthegemony\n\nPosthegemony or post-hegemony is a period or a situation in which hegemony is no longer said to function as the organizing principle of a national or post-national social order, or of the relationships between and amongst nation states within the global order. The concept has different meanings within the fields of political theory, cultural studies, and international relations.\n\nIn the field of cultural studies, posthegemony has been developed as a concept by a number of critics whose work engages with and critiques the use of cultural hegemony theory within the writings of Ernesto Laclau and within subaltern studies. George Yúdice, in 1995, was one of the first commentators to summarize the background to the emergence of this concept:\n\nFlexible accumulation, consumer culture, and the \"new world information order\" are produced or distributed (made to flow) globally, to occupy the space of the nation, but are no longer \"motivated\" by any essential connections to a state, as embodied, for example, in a \"national-popular\" formation. Their motivations are both infra- and supranational. We might say that, from the purview of the national proscenium, a posthegemonic situation holds. That is, the \"compromise solution\" that culture provided for Gramsci is not now one that pertains to the national level but to the local and transnational. Instead, the \"culture-ideology of consumerism\" serves to naturalize global capitalism everywhere [emphasis added].\n\nThe concept of posthegemony is related to the rise of the \"multitude\" as a social force which, unlike the \"people\", cannot be captured by hegemony, together with the roles of affect and habitus in mechanisms of social control and agency. Posthegemony and its related terms are influenced by Gilles Deleuze and Félix Guattari, Pierre Bourdieu and Michael Hardt and Antonio Negri’s accounts of the supra- and infra-national forces that are said to have rendered obsolete the national-popular forms of coercion and consent through which, for Antonio Gramsci, hegemony structured and constituted society.\n\nThe features of posthegemony as a concept correspond closely to those of postmodernity. Thus, posthegemony theory argues that ideology is no longer a political driving force in mechanisms of social control and that the modernist theory of hegemony, which depends on ideology, therefore no longer accurately reflects the social order. Some commentators also argue that history is not, as Karl Marx described it, a class struggle, but rather a \"struggle to produce class\".\n\nThe concept of posthegemony also resonates with the work of post-Foucauldian theorists such as Giorgio Agamben. Nicholas Thoburn, drawing on Agamben's discussion on the \"state of exception\", writes that \"it is, perhaps, with the recasting of the relationship between law and politico-military and economic crises and interventions that is instituted in the state of exception that the time of hegemony is most revealed to have passed.\"\n\nIn international relations, \"posthegemony\" refers to the decline of the US unilateral hegemony. This has likely been the result of the difficulties that have arisen out of the unilateral style foreign policy. These difficulties predominantly include disdain from; those directly affected by the, sometimes forceful, hegemonic actions of the US, those who spectated the actions, and even Americans themselves who view the actions of their government as immoral. For example, after the Vietnam War, in 1978, 72 per cent of Americans thought the war was not a mistake but fundamentally wrong and immoral. This exemplifies the hegemonic decline: how could the US maintain the legitimacy of their interventions if their own citizens find them wrong and immoral?\n\nThe dominant power(s) of the world is/are fluid, the initial period of US unilateralism can be loosely pinpointed to their interventions during the world wars. Following this period of rising US dominance on the world stage,\n\nThe predictions of these individuals represent the fluidity of power over time, through the idea that, during the period where the US was unequivocally dominant, people could still see the inevitable future, of a change in power and authority, on the world stage.\n\nAmong the criticisms of the theory of posthegemony is Richard Johnson's, that it involves \"a marked reduction of social complexity.\" Johnson concedes that \"one considerable achievement of 'the post-hegemony project' is to draw many observable post-9/11 features into a single imaginative picture, while also synthesizing different currents in contemporary social theory.\" But he argues that \"it is strange, however, that the result is viewed as the end of hegemony rather than as a new hegemonic moment.\" He, therefore, calls for a rejuvenation of the concept of hegemony, rather than its abandonment. \n\n\n"}
{"id": "44829416", "url": "https://en.wikipedia.org/wiki?curid=44829416", "title": "RedditGifts", "text": "RedditGifts\n\nRedditGifts is an online user-to-user gift exchange service that connects Reddit users around the world with one another. Free to participate in, RedditGifts was first created by Reddit user \"kickme444\" (the alias of Dan McComas), while working on freelance projects. The service involves signing up, filling out a profile based on the user's preferences, and agreeing to send a gift to a randomly assigned user.\n\nThe service is a gift exchange designed for Reddit users. It matches a Reddit user with another Reddit user, and each user must then send a gift to the other user according to that user's tastes and preferences listed on their profile. Participation is free, although extra perks can be purchased with the optional \"RedditGifts Elves\" membership, and goods are usually priced between $10 and $25 USD. The service also requests that users leave \"thank-you notes\" for their gift-giver on the website.\n\nSince starting as a Secret Santa exchange, RedditGifts has added a variety of different themed exchanges that run throughout the year, such as Nintendo and Halloween. The service also runs Arbitrary Day, \"a celebration of nothing in particular\". Up to 2011, approximately $1.5 million had been spent by Reddit users on the project.\n\nMicrosoft chairman and philanthropist Bill Gates has participated in the service for five years under the username \"thisisbillgates\", most notably in 2013 and in 2014, The Bill and Melinda Gates Foundation confirmed this. Gates was noted to send objects to Reddit users like \"Calid7\" (a 25-year-old Californian woman), such as a travel book and a stuffed cow. He has also been known to donate to charities in the name of the user he has been matched with.\n\nRedditGifts was originally created by Reddit user Dan McComas, or \"kickme444\" as known on the site, and \"5days\". McComas first conceived the idea while working on freelance projects in 2009.\n\nThe site was launched in 2009 with 4,500 participants; the website reports that 212,894 people worldwide participated in the event during 2014.\n\nThe site is a three-time Guinness World Record holder for the World's Largest Gift Exchange.\n\nIn August 2011, Reddit bought the RedditGifts site. The two site-runners reportedly could not afford to continue maintaining the site while simultaneously working their day jobs. Both site-runners continued to run the website, although they are not involved with the daily operations of Reddit. The company takes a 15 to 20 percent cut of every purchase. Reddit operates the website as part of its ongoing plans to monetise their website. As of December 2013, approximately 14 percent of Reddit's revenue came from running the service, although McComas said that those sales alone could \"put Reddit firmly in the black\", and that the company may choose to reinvest funds in e-commerce customer service and infrastructure.\n\nIn December 2013, while expressing his desire for the Reddit business to become self-sustaining, then-Chief Executive Yishan Wong stated that the RedditGifts service \"provides real value\", and called it \"promising\" for the business. Wong increased the staff count dedicated to the project to eight after realising the service's potential.\n\n"}
{"id": "981682", "url": "https://en.wikipedia.org/wiki?curid=981682", "title": "Reductio ad Hitlerum", "text": "Reductio ad Hitlerum\n\n' (; pseudo-Latin for \"reduction to Hitler\"; sometimes argumentum ad Hitlerum, \"argument to Hitler\", ', \"to Nazism\"), or playing the Nazi card, is an attempt to invalidate someone else's position on the basis that the same view was held by Adolf Hitler or the Nazi Party, for example: \"Hitler was against tobacco smoking, X is against tobacco smoking, therefore X is a Nazi\".\n\nCoined by Leo Strauss in 1953, borrows its name from the term used in logic, (reduction to the absurd). According to Strauss, is a form of , , or a . The suggested rationale is one of guilt by association. It is a tactic often used to derail arguments, because such comparisons tend to distract and anger the opponent, as Hitler and Nazism have been condemned in the modern world.\n\n is a form of association fallacy. The argument is that a policy leads to or is the same as one advocated or implemented by Adolf Hitler or the Third Reich and so \"proves\" that the original policy is undesirable.\n\nAnother instance of is asking a question of the form \"You know who else...?\" with the deliberate intent of impugning a certain idea or action by implying Hitler held that idea or performed such action.\n\nAn invocation of Hitler or Nazism is not a when it illuminates the argument instead of causing distraction from it.\n\nThe phrase is first known to have been used in an article written by University of Chicago professor Leo Strauss for \"Measure: A Critical Journal\" in spring 1951; it was made famous in a book by the same author published in 1953 \"Natural Right and History\", Chapter II:\n\nIn following this movement towards its end we shall inevitably reach a point beyond which the scene is darkened by the shadow of Hitler. Unfortunately, it does not go without saying that in our examination we must avoid the fallacy that in the last decades has frequently been used as a substitute for the : the . A view is not refuted by the fact that it happens to have been shared by Hitler.\n\nThe phrase was derived from the legitimate logical argument called . The variant takes its form from the names of many classic fallacies, such as . The variant may be further derived, humorously, from .\n\nIn 2000, Thomas Fleming described its use against traditional values:\nLeo Strauss called it the . If Hitler liked neoclassical art, that means that classicism in every form is Nazi; if Hitler wanted to strengthen the German family, that makes the traditional family (and its defenders) Nazi; if Hitler spoke of the \"nation\" or the \"folk\", then any invocation of nationality, ethnicity, or even folkishness is Nazi ...\n\nSome historians studying the Holocaust argue that not all comparisons to Hitler and Nazism are logical fallacies since if they all were, there would be nothing to learn from the events that led to the Holocaust. That would undermine the entire significance of studying the Holocaust to avoid future genocides.\n\nProponents of this point do make distinctions between many different categories of comparison(s) to Hitler, some of which are fallacies, some of which are not, and some of which may or may not be fallacies. This approach is also critical to strict refusal to acknowledge similarities to the Holocaust, since there were early stages leading up to it and policies today can be comparable to those without being comparable to the \"final solution\". Advocates of this approach argue that such early stages are those where something can really be done about it, before it is too late. Categories include:\n\nAlthough named for and formalized around Hitler, the logical fallacy existed prior to the Second World War. There were other individuals from history who were used as stand-ins for pure evil. In the 18th, 19th and early 20th centuries the Pharaoh of the Book of Exodus was commonly seen as the most villainous person in history. In the years prior to the Civil War, abolitionists referred to slaveholders as modern-day Pharaohs. After VE Day, Pharaoh continued to appear in the speeches of social reformers like Martin Luther King Jr. Judas Iscariot and Pontius Pilate were also commonly held up as pure evil. However, there was no universal Hitler-like person and different regions and times used different stand-ins. In the years after the American Revolution, King George III was often vilified in the United States. Andrew Jackson was also called King Andrew the First. During the Civil War, some Southerners spoke of Lincoln in Hitler-like terms. Some Confederates even called Lincoln a \"modern Pharaoh\".\n\nIn 1991, Michael André Bernstein alleged over a full-page advertisement placed in \"The New York Times\" by the Lubavitch community, following the Crown Heights Riot, under the heading \"This Year \"Kristallnacht\" Took Place on August 19th Right Here in Crown Heights.\" Henry Schwarzschild, who had witnessed \"Kristallnacht\", wrote to the \"New York Times\" that \"however ugly were the anti-Semitic slogans and the assaultive behavior of people in the streets [during the Crown Heights riots]... one thing that clearly did not take place was a \"Kristallnacht\".\"\n\nAmerican conservative radio and television host Glenn Beck is often criticized for his frequent use of , including a controversial statement comparing the victims of the 2011 Norway attacks to members of the Hitler Youth. Beck has also compared the National Endowment for the Arts to Joseph Goebbels and ACORN to Hitler's \"Brown Shirts\".\n\n\"The American Conservative\" accused Jonah Goldberg's book \"Liberal Fascism\" of employing the fallacy:\nThat Nazism and contemporary liberalism both promote healthy living is as meaningless a finding as that bloody marys and martinis may both be made with gin. Repeatedly, Goldberg fails to recognize a . ... In no case does Goldberg uncover anything more ominous than a coincidence.\n\n\n"}
{"id": "48673882", "url": "https://en.wikipedia.org/wiki?curid=48673882", "title": "Roy model", "text": "Roy model\n\nThe Roy model is one of the earliest works in economics on self-selection due to A.D. Roy. The basic model considers two types of workers that choose occupation in one of two sectors. \n\nRoy's original paper deals with workers selecting into fishing and hunting professions, where there is no uncertainty about the amount of goods (fish or rabbits) that will be caught in a given period, but fishing is more costly as it requires more skill. The central question that Roy tries to answer in the original paper is whether the best hunters will hunt and the best fishermen will fish. While the discussion is non-mathematical, it is observed that choices will depend on the distribution of skills, the correlation between these skills in the population, and the technology available to use these skills.\n\nGeorge Borjas was the first to formalize the model of Roy in a mathematical sense and apply it to self-selection in immigration. Specifically, assume source country 0 and destination country 1, with log earnings in a country \"i\" given by \"w= a + e\", where \"e∼N(0, formula_1 )\". Additionally, assume there is a cost \"C\" associated with migrating from country 0 to country 1 and workers know all parameters and their own realization of e and e. Borjas then uses the implications of the Roy model to infer something about what wages for immigrants in country 1 would have been had they stayed in country 0 and what wages for non-immigrants in country 0 would have been had they migrated. The third, and final, element needed for this is the correlation between the wages in the two countries, \"ρ\". A worker will choose to immigrate if \"a - a - C + e - e > 0\" which will happen with probability \n\"1 - Φ ( v )\" where \"v\" is formula_2 , \"s\" is the standard deviation of \"e – e\", and \"Φ\" is the standard normal cdf. This leads to the famous central result that the expected wage for immigrants depends on the selection mechanism, as shown in equation (1), where \"ϕ\" is the standard normal pdf and, like before, \"Φ\" is the standard normal cdf.\n\n\"E[w\" |\"Immigrate\"] = a +ρs formula_3 (1)\n\nWhile Borjas was the first to mathematically formalize the Roy model, it has guided thinking in other fields of research as well. A famous example by James Heckman and Bo Honoré who study labor market participation using the Roy model, where the choice equation leads to the Heckman correction procedure. More generally, Heckman and Vytlacil propose the Roy model as an alternative to the LATE framework proposed by Angrist and Imbens.\n"}
{"id": "5151535", "url": "https://en.wikipedia.org/wiki?curid=5151535", "title": "Secrecy of correspondence", "text": "Secrecy of correspondence\n\nThe secrecy of correspondence (, ) or literally translated as secrecy of letters, is a fundamental legal principle enshrined in the constitutions of several European countries. It guarantees that the content of sealed letters is never revealed and letters in transit are not opened by government officials or any other third party. It is thus the main legal basis for the assumption of privacy of correspondence.\n\nThe principle has been naturally extended to other forms of communication, including telephony and electronic communications on the Internet as the constitutional guarantees are generally thought to cover also these forms of communication. However, national telecommunications privacy laws may allow lawful interception, i.e. wiretapping and monitoring of electronic communications in cases of suspicion of crime. Paper letters have in most jurisdictions remained outside the legal scope of law enforcement surveillance, even in cases of \"reasonable searches and seizures\".\n\nWhen applied to electronic communication, the principle protects not only the content of the communication, but also the information on when and to whom any messages (if any) have been sent (see: Call detail records), and in the case of mobile communication, the location information of the mobile units. As a consequence, in jurisdictions with a safeguard on secrecy of letters, location data collected from mobile phone networks has a higher level of protection than data collected by vehicle telematics or transport tickets.\n\nIn the United States there is no specific constitutional guarantee on the privacy of correspondence. The secrecy of letters and correspondence is derived through litigation from the Fourth Amendment to the United States Constitution. In an 1877 case the U.S. Supreme Court stated:\nNo law of Congress can place in the hands of officials connected with the Postal Service any authority to invade the secrecy of letters and such sealed packages in the mail; and all regulations adopted as to mail matter of this kind must be in subordination to the great principle embodied in the fourth amendment of the Constitution.\n\nThe protection of the Fourth Amendment has been extended beyond the home in other instances. A protection similar to that of correspondence has even been argued to extend to the contents of trash cans outside one's house, although unsuccessfully. Like all rights derived through litigation, the secrecy of correspondence is subject to interpretations. By Supreme Court precedent, rights derived from the Fourth Amendment are limited by the legal test of a \"reasonable expectation of privacy\".\n\n\n\n"}
{"id": "1600053", "url": "https://en.wikipedia.org/wiki?curid=1600053", "title": "Self-replicating machine", "text": "Self-replicating machine\n\nA self-replicating machine is a type of autonomous robot that is capable of reproducing itself autonomously using raw materials found in the environment, thus exhibiting self-replication in a way analogous to that found in nature. The concept of self-replicating machines has been advanced and examined by Homer Jacobsen, Edward F. Moore, Freeman Dyson, John von Neumann and in more recent times by K. Eric Drexler in his book on nanotechnology, \"Engines of Creation\" (coining the term clanking replicator for such machines) and by Robert Freitas and Ralph Merkle in their review \"Kinematic Self-Replicating Machines\" which provided the first comprehensive analysis of the entire replicator design space. The future development of such technology is an integral part of several plans involving the mining of moons and asteroid belts for ore and other materials, the creation of lunar factories, and even the construction of solar power satellites in space. The possibly misnamed von Neumann probe is one theoretical example of such a machine. Von Neumann also worked on what he called the universal constructor, a self-replicating machine that would operate in a cellular automata environment.\n\nA self-replicating machine is an artificial self-replicating system that relies on conventional large-scale technology and automation. Certain idiosyncratic terms are occasionally found in the literature. For example, the term clanking replicator was once used by Drexler to distinguish macroscale replicating systems from the microscopic nanorobots or \"assemblers\" that nanotechnology may make possible, but the term is informal and is rarely used by others in popular or technical discussions. Replicators have also been called \"von Neumann machines\" after John von Neumann, who first rigorously studied the idea. However, the term \"von Neumann machine\" is less specific and also refers to a completely unrelated computer architecture that von Neumann proposed and so its use is discouraged where accuracy is important. Von Neumann himself used the term universal constructor to describe such self-replicating machines.\n\nHistorians of machine tools, even before the numerical control era, sometimes figuratively said that machine tools were a unique class of machines because they have the ability to \"reproduce themselves\" by copying all of their parts. Implicit in these discussions is that a human would direct the cutting processes (later planning and programming the machines), and would then be assembling the parts. The same is true for RepRaps, which are another class of machines sometimes mentioned in reference to such non-autonomous \"self-replication\". In contrast, machines that are \"truly autonomously\" self-replicating (like biological machines) are the main subject discussed here.\n\nThe general concept of artificial machines capable of producing copies of themselves dates back at least several hundred years. An early reference is an anecdote regarding the philosopher René Descartes, who suggested to Queen Christina of Sweden that the human body could be regarded as a machine; she responded by pointing to a clock and ordering \"see to it that it reproduces offspring.\" Several other variations on this anecdotal response also exist. Samuel Butler proposed in his 1872 novel \"Erewhon\" that machines were already capable of reproducing themselves but it was man who made them do so, and added that \"machines which reproduce machinery do not reproduce machines after their own kind\". In George Eliot's 1879 book \"Impressions of Theophrastus Such\", a series of essays that she wrote in the character of a fictional scholar named Theophrastus, the essay \"Shadows of the Coming Race\" speculated about self-replicating machines, with Theophrastus asking \"how do I know that they may not be ultimately made to carry, or may not in themselves evolve, conditions of self-supply, self-repair, and reproduction\".\n\nIn 1802 William Paley formulated the first known teleological argument depicting machines producing other machines, suggesting that the question of who originally made a watch was rendered moot if it were demonstrated that the watch was able to manufacture a copy of itself. Scientific study of self-reproducing machines was anticipated by John Bernal as early as 1929 and by mathematicians such as Stephen Kleene who began developing recursion theory in the 1930s. Much of this latter work was motivated by interest in information processing and algorithms rather than physical implementation of such a system, however. In the course of the 1950s, suggestions of several increasingly simple mechanical systems capable of self-reproduction were made—notably by Lionel Penrose.\n\nA detailed conceptual proposal for a physical non-biological self-replicating system was first put forward by mathematician John von Neumann in lectures delivered in 1948 and 1949, when he proposed a kinematic self-reproducing automaton model as a thought experiment. Von Neumann's concept of a physical self-replicating machine was dealt with only abstractly, with the hypothetical machine using a \"sea\" or stockroom of spare parts as its source of raw materials. The machine had a program stored on a memory tape that directed it to retrieve parts from this \"sea\" using a manipulator, assemble them into a duplicate of itself, and then copy the contents of its memory tape into the empty duplicate's. The machine was envisioned as consisting of as few as eight different types of components; four logic elements that send and receive stimuli and four mechanical elements used to provide a structural skeleton and mobility. While qualitatively sound, von Neumann was evidently dissatisfied with this model of a self-replicating machine due to the difficulty of analyzing it with mathematical rigor. He went on to instead develop an even more abstract model self-replicator based on cellular automata. His original kinematic concept remained obscure until it was popularized in a 1955 issue of \"Scientific American\".\n\nIn 1956 mathematician Edward F. Moore proposed the first known suggestion for a practical real-world self-replicating machine, also published in \"Scientific American\". Moore's \"artificial living plants\" were proposed as machines able to use air, water and soil as sources of raw materials and to draw its energy from sunlight via a solar battery or a steam engine. He chose the seashore as an initial habitat for such machines, giving them easy access to the chemicals in seawater, and suggested that later generations of the machine could be designed to float freely on the ocean's surface as self-replicating factory barges or to be placed in barren desert terrain that was otherwise useless for industrial purposes. The self-replicators would be \"harvested\" for their component parts, to be used by humanity in other non-replicating machines.\n\nThe next major development of the concept of self-replicating machines was a series of thought experiments proposed by physicist Freeman Dyson in his 1970 Vanuxem Lecture. He proposed three large-scale applications of machine replicators. First was to send a self-replicating system to Saturn's moon Enceladus, which in addition to producing copies of itself would also be programmed to manufacture and launch solar sail-propelled cargo spacecraft. These spacecraft would carry blocks of Enceladean ice to Mars, where they would be used to terraform the planet. His second proposal was a solar-powered factory system designed for a terrestrial desert environment, and his third was an \"industrial development kit\" based on this replicator that could be sold to developing countries to provide them with as much industrial capacity as desired. When Dyson revised and reprinted his lecture in 1979 he added proposals for a modified version of Moore's seagoing artificial living plants that was designed to distill and store fresh water for human use and the \"Astrochicken.\"\n\nIn 1980, inspired by a 1979 \"New Directions Workshop\" held at Wood's Hole, NASA conducted a joint summer study with ASEE entitled to produce a detailed proposal for self-replicating factories to develop lunar resources without requiring additional launches or human workers on-site. The study was conducted at Santa Clara University and ran from June 23 to August 29, with the final report published in 1982. The proposed system would have been capable of exponentially increasing productive capacity and the design could be modified to build self-replicating probes to explore the galaxy.\n\nThe reference design included small computer-controlled electric carts running on rails inside the factory, mobile \"paving machines\" that used large parabolic mirrors to focus sunlight on lunar regolith to melt and sinter it into a hard surface suitable for building on, and robotic front-end loaders for strip mining. Raw lunar regolith would be refined by a variety of techniques, primarily hydrofluoric acid leaching. Large transports with a variety of manipulator arms and tools were proposed as the constructors that would put together new factories from parts and assemblies produced by its parent.\n\nPower would be provided by a \"canopy\" of solar cells supported on pillars. The other machinery would be placed under the canopy.\n\nA \"casting robot\" would use sculpting tools and templates to make plaster molds. Plaster was selected because the molds are easy to make, can make precise parts with good surface finishes, and the plaster can be easily recycled afterward using an oven to bake the water back out. The robot would then cast most of the parts either from nonconductive molten rock (basalt) or purified metals. A carbon dioxide laser cutting and welding system was also included.\n\nA more speculative, more complex microchip fabricator was specified to produce the computer and electronic systems, but the designers also said that it might prove practical to ship the chips from Earth as if they were \"vitamins.\"\n\nA 2004 study supported by NASA's Institute for Advanced Concepts took this idea further. Some experts are beginning to consider self-replicating machines for asteroid mining.\n\nMuch of the design study was concerned with a simple, flexible chemical system for processing the ores, and the differences between the ratio of elements needed by the replicator, and the ratios available in lunar regolith. The element that most limited the growth rate was chlorine, needed to process regolith for aluminium. Chlorine is very rare in lunar regolith.\n\nIn 1995, inspired by Dyson's 1970 suggestion of seeding uninhabited deserts on Earth with self-replicating machines for industrial development, Klaus Lackner and Christopher Wendt developed a more detailed outline for such a system. They proposed a colony of cooperating mobile robots 10–30 cm in size running on a grid of electrified ceramic tracks around stationary manufacturing equipment and fields of solar cells. Their proposal didn't include a complete analysis of the system's material requirements, but described a novel method for extracting the ten most common chemical elements found in raw desert topsoil (Na, Fe, Mg, Si, Ca, Ti, Al, C, O and H) using a high-temperature carbothermic process. This proposal was popularized in Discover Magazine, featuring solar-powered desalination equipment used to irrigate the desert in which the system was based. They named their machines \"Auxons\", from the Greek word \"auxein\" which means \"to grow.\"\n\nEarly experimentation with rapid prototyping in 1997-2000 was not expressly oriented toward reproducing rapid prototyping systems themselves, but rather extended simulated \"evolutionary robotics\" techniques into the physical world. Later developments in rapid prototyping have given the process the ability to produce a wide variety of electronic and mechanical components, making this a rapidly developing frontier in self-replicating system research.\n\nIn 1998 Chris Phoenix informally outlined a design for a hydraulically powered replicator a few cubic feet in volume that used ultraviolet light to cure soft plastic feedstock and a fluidic logic control system, but didn't address most of the details of assembly procedures, error rates, or machining tolerances.\n\nIn 2005, Adrian Bowyer of the University of Bath started the RepRap Project to develop a rapid prototyping machine which would be able to manufacture some or most of its own components, making such machines cheap enough for people to buy and use in their homes. The project is releasing its designs and control programs under the GNU GPL. The RepRap approach uses fused deposition modeling to manufacture plastic components, possibly incorporating conductive pathways for circuitry. Other components, such as steel rods, nuts and bolts, motors and separate electronic components, would be supplied externally. In 2006 the project produced a basic functional prototype and in May 2008 the machine succeeded in producing all of the plastic parts required to make a 'child' machine.\n\nSome researchers have proposed a microfactory of specialized machines that support recursion—nearly all of the parts of all of the machines in the factory can be manufactured by the factory.\n\nIn the spirit of the 1980 \"Advanced Automation for Space Missions\" study, the NASA Institute for Advanced Concepts began several studies of self-replicating system design in 2002 and 2003. Four phase I grants were awarded:\n\n\nIn 2012, NASA researchers Metzger, Muscatello, Mueller, and Mantovani argued for a bootstrapping approach to start self-replicating factories in space. They developed this concept on the basis of In Situ Resource Utilization (ISRU) technologies that NASA has been developing to \"live off the land\" on the Moon or Mars. Their modeling showed that in just 20 to 40 years this industry could become self-sufficient then grow to large size, enabling greater exploration in space as well as providing benefits back to Earth. In 2014, Thomas Kalil of the White House Office of Science and Technology Policy published on the White House blog an interview with Metzger on bootstrapping solar system civilization through self-replicating space industry. Kalil requested the public submit ideas for how \"the Administration, the private sector, philanthropists, the research community, and storytellers can further these goals.\" Kalil connected this concept to what former NASA Chief technologist Mason Peck has dubbed \"Massless Exploration\", the ability to make everything in space so that you do not need to launch it from Earth. Peck has said, \"...all the mass we need to explore the solar system is already in space. It's just in the wrong shape.\" In 2016, Metzger argued that fully self-replicating industry can be started over several decades by astronauts at a lunar outpost for a total cost (outpost plus starting the industry) of about a third of the space budgets of the International Space Station partner nations, and that this industry would solve Earth's energy and environmental problems in addition to providing massless exploration.\n\nIn 2005, a team of researchers at Cornell University, including Hod Lipson, implemented a self-assembling machine. The machine is composed of a tower of four articulated cubes, known as \"molecubes\", which can revolve about a triagonal. This enables the tower to function as a robotic arm, collecting nearby molecubes and assembling them into a copy of itself. The arm is directed by a computer program, which is contained within each molecube, analogous to how each animal cell contains an entire copy of its DNA. However, the machine cannot manufacture individual molecubes, nor do they occur naturally, so its status as a self-replicator is debatable.\n\nIn 2011, a team of scientists at New York University created a structure called 'BTX' (bent triple helix) based around three double helix molecules, each made from a short strand of DNA. Treating each group of three double-helices as a code letter, they can (in principle) build up self-replicating structures that encode large quantities of information.\n\nIn 2001 Jarle Breivik at University of Oslo created a system of magnetic building blocks, which in response to temperature fluctuations, spontaneously form self-replicating polymers.\n\nIn 1968 Zellig Harris wrote that \"the metalanguage is in the language,\" suggesting that self-replication is part of language. In 1977 Niklaus Wirth formalized this proposition by publishing a self-replicating deterministic context-free grammar. Adding to it probabilities, Bertrand du Castel published in 2015 a self-replicating stochastic grammar and presented a mapping of that grammar to neural networks, thereby presenting a model for a self-replicating neural circuit.\n\nPartial construction is the concept that the constructor creates a partially constructed (rather than fully formed) offspring, which is then left to complete its own construction.\n\nThe von Neumann model of self-replication envisages that the mother automaton should construct all portions of daughter automatons, without exception and prior to the initiation of such daughters. Partial construction alters the construction relationship between mother and daughter automatons, such that the mother constructs but a portion of the daughter, and upon initiating this portion of the daughter, thereafter retracts from imparting further influence upon the daughter. Instead, the daughter automaton is left to complete its own development. This is to say, means exist by which automatons may develop via the mechanism of a zygote.\n\nThe idea of an automated spacecraft capable of constructing copies of itself was first proposed in scientific literature in 1974 by Michael A. Arbib, but the concept had appeared earlier in science fiction such as the 1967 novel \"Berserker\" by Fred Saberhagen or the 1950 novellette trilogy \"The Voyage of the Space Beagle\" by A. E. van Vogt. The first quantitative engineering analysis of a self-replicating spacecraft was published in 1980 by Robert Freitas, in which the non-replicating Project Daedalus design was modified to include all subsystems necessary for self-replication. The design's strategy was to use the probe to deliver a \"seed\" factory with a mass of about 443 tons to a distant site, have the seed factory replicate many copies of itself there to increase its total manufacturing capacity, and then use the resulting automated industrial complex to construct more probes with a single seed factory on board each.\n\n\nAs the use of industrial automation has expanded over time, some factories have begun to approach a semblance of self-sufficiency that is suggestive of self-replicating machines. However, such factories are unlikely to achieve \"full closure\" until the cost and flexibility of automated machinery comes close to that of human labour and the manufacture of spare parts and other components locally becomes more economical than transporting them from elsewhere. As Samuel Butler has pointed out in Erewhon, replication of partially closed universal machine tool factories is already possible. Since safety is a primary goal of all legislative consideration of regulation of such development, future development efforts may be limited to systems which lack either control, matter, or energy closure. Fully capable machine replicators are most useful for developing resources in dangerous environments which are not easily reached by existing transportation systems (such as outer space).\n\nAn artificial replicator can be considered to be a form of artificial life. Depending on its design, it might be subject to evolution over an extended period of time. However, with robust error correction, and the possibility of external intervention, the common science fiction scenario of robotic life run amok will remain extremely unlikely for the foreseeable future.\n\n\nThe power source might be solar or possibly radioisotope based given that new liquid based compounds can generate substantial power from radioactive decay.\n"}
{"id": "274908", "url": "https://en.wikipedia.org/wiki?curid=274908", "title": "Ship's bell", "text": "Ship's bell\n\nA ship's bell is used to indicate the time aboard a ship and hence to regulate the sailors' duty watches. The bell itself is usually made of brass or bronze, and normally has the ship's name engraved or cast on it.\n\nUnlike civil clock bells, the strikes of a ship's bell do not accord to the number of the hour. Instead, there are eight bells, one for each half-hour of a four-hour watch. In the age of sailing, watches were timed with a 30-minute hourglass. Bells would be struck every time the glass was turned, and in a pattern of pairs for easier counting, with any odd bells at the end of the sequence.\n\nThe classical, or traditional, system was:\n\nMost of the crew of a ship would be divided into two to four groups, called watches. Each watch would take its turn with the essential activities of manning the helm, navigating, trimming sails, and keeping a lookout.\n\nThe hours between 16:00 and 20:00 are so arranged because that watch (the \"dog watch\") was divided in two. The odd number of watches aimed to give each man a different watch each day; it also allowed the entire crew of a vessel to eat an evening meal, the normal time being at 17:00 with first dog watchmen eating at 18:00.\n\nSome \"ship's bell\" clocks use a simpler system:\n\n\nThe ship's name is traditionally engraved or cast onto the surface of the bell, often with the year the ship was launched, as well. Occasionally (especially on more modern ships) the bell will also carry the name of the shipyard that built the ship. If a ship's name is changed, maritime tradition is that the original bell carrying the original name will remain with the vessel. A ship's bell is a prized possession when a ship is broken up and often provides the only positive means of identification in the case of a shipwreck.\n\nMost United States Navy ships of the post–World War II era have actually carried two ship's bells: the official bell on deck and a smaller one in the pilot house and at the quarterdeck at the 1MC (public address) station, used when the ship is underway.\n\nAccording to seafaring legend, the ship's cooks and boatswain's mates had a duty arrangement to give the cooks more sleep. The boatswain's mates, who worked 24 hours a day on watches, would build the fire in the stove, so the cook could get up a little while later and the fire would be already going so he could begin preparing breakfast. In return, between meals, the cooks would shine the bell, which was traditionally the boatswain's mates' responsibility.\n\nIt is a naval tradition to baptize children using the ship's bell as a baptismal font and to engrave the names of the children on the bell afterwards. Christening information from the bells held by the Canadian Forces Base Esquimalt Museum has been entered into a searchable data archive.\n\n\n"}
{"id": "11824035", "url": "https://en.wikipedia.org/wiki?curid=11824035", "title": "Signal-to-interference ratio", "text": "Signal-to-interference ratio\n\nThe signal-to-interference ratio (SIR or S/I), also known as the carrier-to-interference ratio (CIR or C/I), is the quotient between the average received modulated carrier power \"S\" or \"C\" and the average received co-channel interference power \"I\", i.e. cross-talk, from other transmitters than the useful signal. \n\nThe CIR resembles the carrier-to-noise ratio (CNR or \"C/N\"), which is the signal-to-noise ratio (SNR or \"S/N\") of a modulated signal before demodulation. A distinction is that interfering radio transmitters contributing to \"I\" may be controlled by radio resource management, while \"N\" involves noise power from other sources, typically additive white gaussian noise (AWGN).\n\nThe CIR ratio is studied in interference limited systems, i.e. where \"I\" dominates over \"N\", typically in cellular radio systems and broadcasting systems where frequency channels are reused in view to achieve high level of area coverage. The \"C/N\" is studied in noise limited systems. If both situations can occur, the carrier-to-noise-and-interference ratio (CNIR or C/(N+I)) may be studied.\n\n"}
{"id": "730244", "url": "https://en.wikipedia.org/wiki?curid=730244", "title": "Social engineering (political science)", "text": "Social engineering (political science)\n\nSocial engineering is a discipline in social science that refers to efforts to influence particular attitudes and social behaviors on a large scale, whether by governments, media or private groups in order to produce desired characteristics in a target population. Social engineering can also be understood philosophically as a deterministic phenomenon where the intentions and goals of the architects of the new social construct are realized.\n\nSocial engineers use the scientific method to analyze and understand social systems in order to design the appropriate methods to achieve the desired results in the human subjects.\n\nDecision-making can affect the safety and survival of billions of people. The scientific theory expressed by German sociologist Ferdinand Tönnies in his study \"The Present Problems of Social Structure\", proposes that society can no longer operate successfully using outmoded methods of social management. To achieve the best outcomes, all conclusions and decisions must use the most advanced techniques and include reliable statistical data, which can be applied to a social system. According to this, social engineering is a data-based scientific system used to develop a sustainable design so as to achieve the intelligent management of Earth’s resources and human capital with the highest levels of freedom, prosperity, and happiness within a population.\n\nAs a result of abuse by authoritarian regimes and other non-inclusive attempts at social engineering, the term has in cases been imbued with a negative connotation. In British and Canadian jurisprudence, changing public attitudes about a behaviour is accepted as one of the key functions of laws prohibiting the behaviour. Governments also influence behavior more subtly through incentives and disincentives built into economic policy and tax policy, for instance, and have done so for centuries.\n\nR. D. Ingthorsson states that a human being is a biological creature from birth but is from then on shaped as a person through social influences (upbringing/socialisation) and is in that sense a social construction, a product of society.\n\nThe Dutch industrialist J.C. Van Marken () introduced the term \"sociale ingenieurs\" in an essay in 1894. The idea was that modern employers needed the assistance of specialists—\"social engineers\"—in handling the \"human\" challenges, just as they needed technical expertise (traditional engineers) to deal with non-human challenges (materials, machines, processes). The term came to America in 1899, when the notion of \"social engineering\" was also launched as the name of the task of the social engineer in this sense. \"Social engineering\" was the title of a small journal in 1899 (from 1900 named \"Social Service\"), and in 1909 the title of a book by its former editor, William H. Tolman (translated into French in 1910), marking the end of the usage of the terminology in the sense of Van Marken. With the Social Gospel sociologist Edwin L. Earp's \"The Social Engineer\", published during the \"efficiency craze\" of 1911 in the U.S., the usage of the term was launched that has since then been standard: the one building on a metaphor of social relations as \"machineries\", to be dealt with in the manner of the technical engineer.\n\nA prerequisite of social engineering is a body of reliable information about the society that is to be engineered and effective tools to carry out the engineering. The availability of which has dramatically increased within the past one hundred years. Prior to the invention of the printing press, it was difficult for groups outside of the wealthy to gain access to a reliable body of information, as the media for conveying the information was prohibitively expensive. With the rise of the information age, information can be distributed and produced on an unprecedented scale. Similarly digital technology has increased the variety and access of effective tools. However, it has also created questionably reliable bodies of information.\n\nSocial engineering can be carried out by any organization, without regard to scale, or sponsorship in the public or private sector. Some of the most comprehensive, and most pervasive campaigns of social engineering are those initiated by powerful central governments with the systems of authority to widely affect the individuals and cultures within their purview.\n\nExtremely intensive social engineering campaigns occurred in countries with authoritarian governments. In the 1920s the government of the Soviet Union embarked on a campaign to fundamentally alter the behavior and ideals of Soviet citizens, to replace the old social frameworks of the Russian Empire with a new Soviet culture, to create the New Soviet man. The Soviets used newspapers, books, film, mass relocations, and even architectural-design tactics to serve as a \"social condenser\" and to change personal values and private relationships. In a less positive manner, political executions (for example the Night of the Murdered Poets in Moscow in 1952), and arguably fear of becoming a victim of mass murder with the Mass killings under Communist regimes, played an influential role in the social engineering frameworks in Soviet Russia. Similar examples include the Chinese \"Great Leap Forward\" (1958–1961) and \"Cultural Revolution\" (1966–1976) programs and the Khmer Rouge's deurbanization of Cambodia (1975–1979). In Singapore, the government's housing policies attempt to promote a mix of all races within each subsidized housing district in order to foster social cohesion and national loyalty while providing citizens with affordable housing. In Tanzania in the 1970s, the government pursued a policy of enforced villagisation under Operation Vijiji in order to promote collective farming.\n\nNon-authoritarian regimes tend to rely on more sustained social engineering campaigns that create more gradual, but ultimately far-reaching, change. Examples include the \"War on Drugs\" in the United States, the increasing reach of intellectual-property rights and copyright, and the promotion of elections as a political tool. The campaign for promoting elections, which is by far the most successful of the three examples, has been in place for over two centuries. Social theorists of the Frankfurt School in Weimar Germany like Theodor Adorno had observed the new phenomenon of mass culture and commented on its new manipulative power, when the rise of the National Socialists drove them out of the country around 1930 (many of them became connected with the Institute for Social Research in the United States). National Socialists themselves were no strangers to the idea of influencing political attitudes and redefining personal relationships. The national-socialist propaganda machine under Joseph Goebbels was a synchronized, sophisticated and effective tool for shaping public opinion.\n\nIn a similar vein the Greek military junta of 1967–1974 attempted to steer Greek public opinion not only by propaganda but also by inventing new words and slogans such as \"palaiokommatismos\" (old-partyism), \"Ellas Ellinon Christianon\" (Greece of Christian Greeks), and \"Ethnosotirios Epanastasis\" (nation-saving revolution, meaning \"coup d'état\").\n\nSocial engineering can be used as a means to achieve a wide variety of different results, as illustrated by the different governments and other organizations that have employed it. Discussion of the possibilities for such manipulation became especially active following World War II, with the advent of mass television, and continuing discussion of techniques of social engineering, particularly in advertising, and bias-based journalism, remains quite pertinent in the western model of consumer capitalism. Journalism, when the intent is not to report objectively, but to report with an intent to sway popular attitudes and social behaviors or to \"shape public opinion\", comes under the scope of social engineering. This also applies when information that would bring into question the viewpoints and social goals of a journalistic establishment is withheld in favor of other information. Within ethical journalism the knowledge of both personal and establishment/producer bias allows the journalist to avoid social engineering by correcting it and by reporting factual evidence in a way which does not promote or oppose attitudes and social behaviors, and thereby portray or deny them as the \"popular\" attitude and preferable social behavior by virtue of the establishment's authority or possession of a national or international platform.\n\nNote that social engineering practiced in exclusion of cultural elements and interacting societies has led to pogroms and to mass murders, particularly when employed by authoritarian regimes. Often this occurs because these cultures or societies are perceived as possessing \"undesirable\" traits. The acting engineers have used the simple \"effective\" tool of violence rather than the difficult and time-consuming methods of persuasion and logic.\n\nCaution in social engineering methods includes consideration of the inherent incompleteness of their body of information and how it affects their utilization of tools at hand. Analysis of social engineering goals and their desirability—which includes the desires of the community which they desire to engineer—answer the question of the ethics of disclosure. Social engineering without consent is a violation of the culture, and constitutes an assault tantamount to a rape, or seizing by force of that culture raptio. Consent, full disclosure and involvement, presents additional difficulties which help to avoid marginalization and feelings of violation within the culture. Long-term attempts at social engineering in the Middle East may be considered to have extreme backlash, as a result of being non inclusive of the cultural values, body of reliable information, or utilization of effective tools.\n\nIn defense of the comparison to rape, consider the article Raptio, which describes the origin of the word, which meant \"seize prey, take by force\", from \"raper\", an Old French legal term for \"to seize\", in turn from Latin \"rapere\"—\"seize, carry off by force, abduct\". Social engineering is an exercise of removing an attitude or behavior and replacing it with another. Which is done with force, when done without consent, that constitutes a violation through abducting an individual or societies culture and replacing it with the engineers' culture.\n\nIn Egypt, social engineering is being practiced by the current authoritarian regime and by the media controlled by the Egyptian Intelligence, Military since July 2013. The coup d'état engineered by them to overthrow the first democratic elected president, former president Mohamed Morsi. The media had been criticizing the every move of the Morsi during this one year presidency which fueled many opposition against him and resulted in demonstrations against him in Tahrir Square in June 30 of 2013. Despite many supporters of the Morsi also came to the street to support him. They were later all killed and imprisoned in the Rabaa Massacre and were labeled as terrorists when General Abdel-fatah el Sisi called to give him support to end terrorism.\nSince then Sisi has been using social engineering by controlling the media and falsifying evidences and news to gain support, blocking many news agency that opposes him and VPN providers to prevent citizens from gaining any other source of news other than the one controlled by him.\nSocial Engineering is also being used on children in schools by ordering children to repeat sentences in support of Sisi like \"Long live president Sisi\". The video was shared on social media Facebook and viewed by 1.5 million viewers. It is believed by many Egyptians it is an act to imprint on children in order to shift their behavior and thinking fearing another revolution in the future like the one that ousted former president Mubarak that was led by youth and young adults in January 2011. Something a little similar to Nazi Germany when Hitler was using children.\n\nIn India, social engineering was effectively done in the state of Bihar, on a grander scale, to unify different castes after 2005. The coherency of voting allegiances based on social extremes among upper castes and Dalits were challenged by this vote (Poll in Indian reference).\n\nIn his classic political science book, \"The Open Society and Its Enemies\", volume I, \"The Spell of Plato\" (1945), Karl Popper examined the application of the critical and rational methods of science to the problems of the open society. In this respect, he made a crucial distinction between the principles of \"democratic\" social engineering (what he called \"piecemeal social engineering\") and \"Utopian\" social engineering.\n\nPopper wrote:\nAccording to Popper, the difference between \"piecemeal social engineering\" and \"Utopian social engineering\" is:\n\n"}
{"id": "508377", "url": "https://en.wikipedia.org/wiki?curid=508377", "title": "Tathāgata", "text": "Tathāgata\n\nTathāgata () is a Pali and Sanskrit word; Gautama Buddha uses it when referring to himself in the Pāli Canon. The term is often thought to mean either \"one who has thus gone\" (\"tathā-gata\") or \"one who has thus come\" (\"tathā-āgata\"). This is interpreted as signifying that the Tathāgata is beyond all coming and going – beyond all transitory phenomena. There are, however, other interpretations and the precise original meaning of the word is not certain.\n\nThe Buddha is quoted on numerous occasions in the Pali Canon as referring to himself as \"the Tathāgata\" instead of using the pronouns \"me\", \"I\" or \"myself\". This may be meant to emphasize by implication that the teaching is uttered by one who has transcended the human condition, one beyond the otherwise endless cycle of rebirth and death, i.e. beyond dukkha.\n\nThe term Tathāgata has a number of possible meanings.\n\nThe word's original significance is not known and there has been speculation about it since at least the time of Buddhaghosa, who gives eight interpretations of the word, each with different etymological support, in his commentary on the Digha Nikaya, the \"SUMAṄGALAVILĀSINĪ\":\n\nModern scholarly opinion generally opines that Sanskrit grammar offers at least two possibilities for breaking up the compound word: either \"tathā\" and \"āgata\" (via a sandhi rule<br>ā + ā → ā), or \"tathā\" and \"gata.\" \"Tathā\" means \"thus\" in Sanskrit and Pali, and Buddhist thought takes this to refer to what is called \"reality as-it-is\" (\"yathābhūta\"). This reality is also referred to as \"thusness\" or \"suchness\" (tathatā), indicating simply that it (reality) is what it is.\n\nTathāgata is defined as someone who \"knows and sees reality as-it-is\" (\"yathā bhūta ñāna dassana\"). \"Gata\" \"gone\" is the past passive participle of the verbal root \"gam\" \"go, travel\". \"Āgata\" \"come\" is the past passive participle of the verb meaning \"come, arrive\". In this interpretation, Tathāgata means literally either “the one who has gone to suchness” or \"the one who has arrived at suchness\".\n\nAnother interpretation, proposed by the scholar Richard Gombrich, is based on the fact that, when used as a suffix in compounds, \"-gata\" will often lose its literal meaning and signifies instead \"being\". Tathāgata would thus mean \"one like that\", with no motion in either direction.\n\nAccording to Fyodor Shcherbatskoy, the term has a non-Buddhist origin, and is best understood when compared to its usage in non-Buddhist works such as the \"Mahabharata\". Shcherbatskoy gives the following example from the \"Mahabharata\" (\"Shantiparva\", 181.22): \"Just as the footprints of birds (flying) in the sky and fish (swimming) in water cannot be seen, Thus (\"tātha\") is going (\"gati\") of those who have realized the Truth.\"\n\nA number of passages affirm that a Tathāgata is \"immeasurable\", \"inscrutable\", \"hard to fathom\", and \"not apprehended\". A tathāgata has abandoned that clinging to the \"skandhas\" (personality factors) that render \"citta\" (the mind) a bounded, measurable entity, and is instead \"freed from being reckoned by\" all or any of them, even in life. The aggregates of form, feeling, perception, mental formations, and cognizance that comprise personal identity have been seen to be \"dukkha\" (a burden), and an enlightened individual is one with \"burden dropped\".The Buddha explains \"that for which a monk has a latent tendency, by that is he reckoned, what he does not have a latent tendency for, by that is he not reckoned. These tendencies are ways in which the mind becomes involved in and clings to conditioned phenomena. Without them, an enlightened person cannot be \"reckoned\" or \"named\"; he or she is beyond the range of other beings, and cannot be \"found\" by them, even by gods, or Mara. In one passage, Sariputta states that the mind of the Buddha cannot be \"encompassed\" even by him.\n\nThe Buddha and Sariputta, in similar passages, when confronted with speculation as to the status of an arahant after death, bring their interlocutors to admit that they cannot even apprehend an arahant that is alive. As Sariputta puts it, his questioner Yamaka \"can't pin down the Tathagata as a truth or reality even in the present life.\" These passages imply that condition of the arahant, both before and after parinirvana, lies beyond the domain where the descriptive powers of ordinary language are at home; that is, the world of the skandhas and the greed, hatred, and delusion that are \"blown out\" with nirvana.\n\nIn the Aggi-Vacchagotta Sutta, an ascetic named Vaccha questions the Buddha on a variety of metaphysical issues. When Vaccha asks about the status of a tathagata after death, the Buddha asks him in which direction a fire goes when it has gone out. Vaccha replies that the question \"does not fit the case ... For the fire that depended on fuel ... when that fuel has all gone, and it can get no other, being thus without nutriment, it is said to be extinct.\" The Buddha then explains: \"In exactly the same way ..., all form by which one could predicate the existence of the saint, all that form has been abandoned, uprooted, pulled out of the ground like a palmyra-tree, and become non-existent and not liable to spring up again in the future. The saint ... who has been released from what is styled form is deep, immeasurable, unfathomable, like the mighty ocean.\" The same is then said of the other aggregates. A variety of similar passages make it clear that the metaphor \"gone out, he cannot be defined\" (\"atthangato so na pamanam eti\") refers equally to liberation in life. In the Aggi-Vacchagotta Sutta itself, it is clear that the Buddha is the subject of the metaphor, and the Buddha has already \"uprooted\" or \"annihilated\" the five aggregates. In Sn 1074, it is stated that the sage cannot be \"reckoned\" because he is freed from the category \"name\" or, more generally, concepts. The absence of this precludes the possibility of reckoning or articulating a state of affairs; \"name\" here refers to the concepts or apperceptions that make propositions possible.\n\nNagarjuna expressed this understanding in the nirvana chapter of his Mulamadhyamakakarika: \"It is not assumed that the Blessed One exists after death. Neither is it assumed that he does not exist, or both, or neither. It is not assumed that even a living Blessed One exists. Neither is it assumed that he does not exist, or both, or neither.\"\n\nSpeaking within the context of Mahayana Buddhism (specifically the Perfection of Wisdom sutras), Edward Conze writes that the term 'tathagata' denotes inherent true selfhood within the human being:\n\n"}
{"id": "407247", "url": "https://en.wikipedia.org/wiki?curid=407247", "title": "Three marks of existence", "text": "Three marks of existence\n\nIn Buddhism, the three marks of existence are three characteristics (Pali: \"tilakkhaa\"; Sanskrit: \"trilakaa\") of all existence and beings, namely impermanence (\"anicca\"), unsatisfactoriness or suffering (\"dukkha\"), and non-self (\"anattā\"). These three characteristics are mentioned in verses 277, 278 and 279 of the \"Dhammapada\". That humans are subject to delusion about the three marks, that this delusion results in suffering, and that removal of that delusion results in the end of suffering, is a central theme in the Buddhist Four Noble Truths and Noble Eightfold Path.\n\nThe three marks are:\n\nImpermanence (Pali \"anicca\", Sanskrit \"anitya\") means that all conditioned things (\"saṅkhāra\") are in a constant state of flux. Buddhism states that all physical and mental events come into being and dissolve. Human life embodies this flux in the aging process, the cycle of repeated birth and death (Samsara), nothing lasts, and everything decays. This is applicable to all beings and their environs, including beings who are reborn in deva (god) and naraka (hell) realms. This is in contrast to nirvana, the reality that is \"nicca\", or knows no change, decay or death.\n\n\"Dukkha\" (Sanskrit \"duhkha\") means \"unsatisfactoriness, suffering, pain\". The dukkha includes the physical and mental sufferings that follows each rebirth, aging, illness, dying; dissatisfaction from getting what a being wishes to avoid or not getting the desired, and no satisfaction from Sankhara dukkha, in which everything is conditioned and conditioning, or because all things are not experienced as impermanent and without any essence.\n\n\"Anatta\" (Sanskrit \"anatman\") refers to the doctrine of \"non-self\", that there is no unchanging, permanent Self or soul in living beings and no abiding essence in anything or phenomena.\n\nWhile anicca and dukkha apply to \"all conditioned phenomena\" (saṅkhārā), anattā has a wider scope because it applies to all dhammā without \"conditioned, unconditioned\" qualification. Thus, \"nirvana\" too is a state of \"without Self\" or anatta. The phrase \"sabbe dhamma anatta\" includes within its scope each skandha (aggregate, heap) that compose any being, and the belief \"I am\" is a mark of conceit which must be destroyed to end all Dukkha. The \"Anattā\" doctrine of Buddhism denies that there is anything called a 'Self' in any person or anything else, and that a belief in 'Self' is a source of \"Dukkha\". Some Buddhist traditions and scholars, however, interpret the anatta doctrine to be strictly in regard to the five aggregates rather than a universal truth. Religious studies scholar Alexander Wynne calls anattā a \"not-self\" teaching rather than a \"no-self\" teaching.\n\nIn Buddhism, ignorance of (avidyā, or moha; i.e. a failure to grasp directly) the three marks of existence is regarded as the first link in the overall process of saṃsāra whereby a being is subject to repeated existences in an endless cycle of suffering. As a consequence, dissolving that ignorance through direct insight into the three marks is said to bring an end to saṃsāra and, as a result, to that suffering (\"dukkha nirodha\" or \"nirodha sacca\", as described in the third of the Four Noble Truths).\n\nGautama Buddha taught that all beings conditioned by causes (\"saṅkhāra\") are impermanent (\"anicca\") and suffering (\"dukkha\"), and that not-self (\"anattā\") characterises all dhammas, meaning there is no \"I\", \"me\", or \"mine\" in either the conditioned or the unconditioned (i.e. nibbāna). The teaching of three marks of existence in the Pali Canon is credited to the Buddha.\n\n"}
{"id": "46787935", "url": "https://en.wikipedia.org/wiki?curid=46787935", "title": "Triple top line", "text": "Triple top line\n\nTriple top line (abbreviated as TTL or 3TL) is first mentioned by McDonough and Braungart (2002). The Triple bottom line, an accounting framework coined by John Elkington in 1994, focuses on aligning sustainability and the intentions of a business when it comes to profit, whereas Triple top line is a focus to align sustainability and business profitability from the inception of a product. The Triple top Line approach is an integral part of the process from the beginning of a product's development through its future development and marketing strategic planning. \n\n"}
{"id": "31504168", "url": "https://en.wikipedia.org/wiki?curid=31504168", "title": "Women's Brigade (Broken Hill)", "text": "Women's Brigade (Broken Hill)\n\nThe Women's Brigade was a labour protest organisation for women formed during the first of several strikes to occur in the mining town of Broken Hill, NSW, Australia between 1889-1920.\n\nAs early as 1874, Australian women were engaging in direct action in mining disputes in Australia. During the 1874 strikes at the Wallaroo and Moonta mines in South Australia, women entered the mines armed with brooms, forced the men out and closed down the engines.\n\nThe notice for the first meeting read:\nA number of Broken Hill Women are very anxious to do something towards supporting the men now out on strike and a meeting for women only will be held at the Masonic Hotel tonight, at 7:30. The president will be the chair. Mr T.C. Tait, the proprietor of the Masonic Hotel, has kindly given the use of his new hall, erected at the site of the hotel, for the use of the women's brigade. All matrons and maids who are in sympathy with the union are requested to attend.\n\nAccording to Unbroken Spirit, 'Poor living conditions and negligent managerial policy on the mines at Broken Hill fed into a strong union presence from the very early days'. Women and children found life in the mining town harsh, yet fought alongside the miners in favour of the union goals despite the hardship prolonged strikes caused to family welfare.\n\nAccording to Carroll (1986), and Blainey (1968) there was particular concern in the town regarding lead poisoning from the mine dust. Due to a long delay in developing the technical process for extracting value from the low-lying ores that were mined at Broken Hill, 40 ft high hills of toxic tailings were created about the mines and left without processing until the opening of the Newcastle BHP Steelworks in 1915.\n\nBroken Hill's first mining strike occurred in 1889 as a result of the trade union ultimatum that members not be made to work with non-unionised workers. The strike lasted a week and during this time the Women's Brigade was formed.\n\nBy the time of the arrival of BHP directors from Melbourne on 13 November, the Women's Brigade had set up as 'sentinels' in order to catch any non-union labour, including shift bosses who were 'tarred and whitewashed' the following day while the 2500 miners met to negotiate the dispute with the directors.\n\nA small number of men were injured by the Women's Brigade on 14 November. As a result of this Richard Sleath on behalf of the union distanced himself from their actions while mentioning the resolve of the women who had a week's worth of provisions and that they intended to remain on picket for the week. An appeal was made for the miners to use their influence with the women to calm their behaviour.\n\nThe brigade now numbers 400 members, many of whom took part in the famous sweeping affair at the Moonta and Wallaroo mines 15 years ago. Two hundred women spent the night in the mines and two of them have been locked up for trespassing.\n\nThe Women's Brigade was very active in using direct action methods against the imported contract labour during the 1892 Broken Hill miners' strike. According to Bloodworth (1996), there were daily mass rallies and the Barrier United Females' Strike Protest Committee was formed in the initial weeks. The 500-strong Brigade passed a resolution to join the 24 August union demonstration but this was later abandoned in favour of a women's rally on the 25th.\n\nProminent suffragette, Mary Lee publicly supported the women of Broken Hill claiming 'that the women of Broken Hill are the first great body of working women who have raised their voices in united protest against the glaring injustice that \"the present constitution will not allow them a voice in framing the laws\"...' and that this rendered the 1892 Broken Hill miners' strike 'more profoundly interesting' than any before.\n\nThe womenfolk that evening at least got some of their own back, for wherever a 'cop' was observed during the march, he either received a 'back-hander' from a woman on passing or was spat upon. This happened not once, but hundreds of times during that memorable tramp through the city's streets. And, under the circumstances, show to me the workingman that did not, and does not, applaud such acts, and I will show you a creature who lives but to creep and to cringe, and knows but little of the history of his class, or the damnable actions of its historic enemies.\n"}
