{"id": "31485329", "url": "https://en.wikipedia.org/wiki?curid=31485329", "title": "Accidental Adversaries", "text": "Accidental Adversaries\n\nAccidental Adversaries is one of the ten system archetypes used in system dynamics modelling, or systems thinking. This archetype describes the degenerative pattern that develops when two subjects cooperating for a common goal, accidentally take actions that undermine each other's success. It is similar to the escalation system archetype in terms of pattern behaviour that develops over time.\n\nThe archetype describes a pattern where two subjects have decided to work together because they will benefit from the alliance. Each take actions believing that it will bring benefit to the other and if the cooperation works, they will both benefit from it. Problems start arising when one or both of the subjects need to fix a local gap in performance, maybe due to external pressure. They initiate action to fix the gap and accidentally undermine each other's success. The result of these activities may produce a sense of resentment or frustration between the subjects or it may even turn the subjects into adversaries (hence the archetype name), thereby destroying the alliance.\n\nThe original set of system archetypes were published in The Fifth Discipline by Peter Senge. The exact source of these generic structures is not known. However \"Accidental Adversaries\" has a clear origin. It is derived from observations made by Jennifer Kemeny, a colleague of Senge's and a contributor to the original archetype descriptions. In her consulting work in the late 1980s she was intrigued at how often potential strategic alliances were unsuccessful, or devolved into outright hostility. Such a recurring phenomenon suggested a structural cause. The essential elements of the archetype were first described during a session Kemeny facilitated. It was the first meeting of the first ever alliance between Walmart and Procter and Gamble. Employees diagrammed how their protective business policies caused unintentional damage to the other company – which responded with similarly defensive actions. The net effect of these \"back and forth\" behaviors was lower profit and less goodwill – which for a long period of time had overshadowed the possibility of mutual benefit between the organizations. An early description of \"Accidental Adversaries\" is in The Fifth Discipline: Fieldbook, by Senge, \"et al.\"\n\nLet's consider two parties A and B. They have realised that by uniting forces, they can increase their success, whatever that may be. At a certain point in time, say B, takes corrective actions in order to counteract a decrease in performance due to external pressure. Although B's actions are important for B, their impact on A are not understood or not considered. The result is that now A feels that he is being betrayed and reacts to diminish the negative effects of B's actions. Ironically, A's new actions now obstruct B's success. If this situation persists and the results worsen, the alliance breaks down. A vicious circle is created and each partner has now even forgotten the original purpose of the alliance. A and B's actions now only focus on counteracting the hostile actions taken by the other. A and B thus 'accidentally' become adversaries.\n\nThe causal loop diagram in Figure 1 shows the pattern dynamics of the system.\nThe pattern of behaviour begins with the outer reinforcing loop R1 where A and B have formed a synergistic alliance that benefits both. An action taken by A in favour of B increases B's success and vice versa. At some point in time, though, say B, initiates a series of corrective actions in order to adjust its performance. The actions taken increase performance, whose effects balance the number of corrective actions required. This results in the creation of a balancing loop, B2. These actions also unintentionally obstruct the other party's success, and result in the formation of the negative reinforcing loop R2 that undermines the virtuous reinforcing loop R1. Now each corrective action taken by B causes A to start taking corrective measures as well, thus activating its own balancing loop B1. The formation of R2 is the critical point at which the dynamics of the system go out of control, resembling the Escalation archetype.\n\nFigure 2 shows the stock and flow diagram for this archetype.\n\nAn alternative form of the model is shown in Figure 3. The differences are in the two reinforcing loops R3 and R4. Here, A and B specifically take actions to improve their growth, not, as before, to adjust their performance to a pre-determined target. By seeking improvement through R3 and R4, A and B suppress the effects of R1 and establish the negative-effect reinforcing loop R2, that in turn, completely takes over B5 and B6. The two balancing loops B5 and B6 are formed by, respectively,\n\nThe archetype behaviour over time is shown in Figure 4.Both parties show a similar trend in direction and rate of change of success, with one trailing behind the other one due to system delays. Even though the pattern shows stable periods, the overall trend follows a downward direction. The above simulation can be run from InsightMaker.\n\nIt is possible to achieve leverage by introducing or re-emphasising a link between each party's success, thus re-establishing the outer virtuous reinforcing loop shown in Figure 3. Kemeny proposes a list of seven action steps to deal with the unintended consequences of each party's actions, given their mental models:\n\n\nA classic example of the Accidental Adversaries system archetype is that of Procter and Gamble supplying Wal-Mart. When Procter and Gamble's profits decline, they introduce promotions. This results in extra costs and decreasing profitability for Wal-Mart. Wal-Mart's responds by stocking-up – buying large quantities of products during the discount period – and then selling them at regular prices when promotions end, thereby increasing its margins. Procter and Gamble's production now faces great swings in volume because Wal-Mart does not need to order products for months, which adds to P and G's costs. To improve their situation, Procter and Gamble pushes even more on promotions, blaming at the same time Wal-Mart for their problems. Wal-Mart reacts by stocking-up even more. Eventually, Procter and Gamble finds itself putting a lot of effort into promotions, at the expense of new product development, while Wal-Mart concentrates solely on buying and storing promoted products rather than improving their operations. The situation is described by the causal loop diagram in Figure 5.\nIn the Procter and Gamble/Wal-Mart case, leverage was achieved by bringing both parties together to understand the structure they had unintentionally created, even though each party 's action was completely rational and understandable from their local perspective.\n\nSenge, Peter (1990). The Fifth Discipline. Currency. .\nSenge, P. \"et al.\" (1994). The Fifth Discipline Fieldbook. New York: Doubleday Currency.\nForrester, J. W. (1971, 1973). World Dynamics. Cambridge, MA: Wright Allen Press.\nForrester, Jay W. (1969). Urban Dynamics. Pegasus Communications. .\nForrester, J. W. (1975). Collected Papers of Jay W. Forrester. Cambridge, MA: Wright-Allen Press.\nKim D, Kim C, A Pocket Guide to Using the Archetypes, Pegasus Communications, May 1994.\nRamsey P, Wells R, Managing the Archetypes: Accidental Adversaries, Pegasus Communications, 2001.\nKim D, Anderson V, Systems Archetype Basics: From Story to Structure, Pegasus Communications, 1998.\n\n"}
{"id": "775", "url": "https://en.wikipedia.org/wiki?curid=775", "title": "Algorithm", "text": "Algorithm\n\nIn mathematics and computer science, an algorithm () is an unambiguous specification of how to solve a class of problems. Algorithms can perform calculation, data processing and automated reasoning tasks.\n\nAs an effective method, an algorithm can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input. \n\nThe concept of algorithm has existed for centuries. Greek mathematicians used algorithms in, for example, the sieve of Eratosthenes for finding prime numbers and the Euclidean algorithm for finding the greatest common divisor of two numbers. \n\nThe word \"algorithm\" itself derives from the 9th century mathematician Muḥammad ibn Mūsā al-Khwārizmī, Latinized \"Algoritmi\". A partial formalization of what would become the modern concept of algorithm began with attempts to solve the Entscheidungsproblem (decision problem) posed by David Hilbert in 1928. Later formalizations were framed as attempts to define \"effective calculability\" or \"effective method\". Those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's Formulation 1 of 1936, and Alan Turing's Turing machines of 1936–37 and 1939.\n\nThe word 'algorithm' has its roots in Latinizing the name of Muhammad ibn Musa al-Khwarizmi in a first step to \"algorismus\". Al-Khwārizmī (, c. 780–850) was a Persian mathematician, astronomer, geographer, and scholar in the House of Wisdom in Baghdad, whose name means 'the native of Khwarezm', a region that was part of Greater Iran and is now in Uzbekistan.\n\nAbout 825, al-Khwarizmi wrote an Arabic language treatise on the Hindu–Arabic numeral system, which was translated into Latin during the 12th century under the title \"Algoritmi de numero Indorum\". This title means \"Algoritmi on the numbers of the Indians\", where \"Algoritmi\" was the translator's Latinization of Al-Khwarizmi's name. Al-Khwarizmi was the most widely read mathematician in Europe in the late Middle Ages, primarily through another of his books, the Algebra. In late medieval Latin, \"algorismus\", English 'algorism', the corruption of his name, simply meant the \"decimal number system\". In the 15th century, under the influence of the Greek word ἀριθμός 'number' (\"cf.\" 'arithmetic'), the Latin word was altered to \"algorithmus\", and the corresponding English term 'algorithm' is first attested in the 17th century; the modern sense was introduced in the 19th century.\n\nIn English, it was first used in about 1230 and then by Chaucer in 1391. English adopted the French term, but it wasn't until the late 19th century that \"algorithm\" took on the meaning that it has in modern English.\n\nAnother early use of the word is from 1240, in a manual titled \"Carmen de Algorismo\" composed by Alexandre de Villedieu. It begins thus:\nwhich translates as:\nThe poem is a few hundred lines long and summarizes the art of calculating with the new style of Indian dice, or Talibus Indorum, or Hindu numerals.\n\nAn informal definition could be \"a set of rules that precisely defines a sequence of operations.\" which would include all computer programs, including programs that do not perform numeric calculations. Generally, a program is only an algorithm if it stops eventually.\n\nA prototypical example of an algorithm is the Euclidean algorithm to determine the maximum common divisor of two integers; an example (there are others) is described by the flowchart above and as an example in a later section.\n\nNo human being can write fast enough, or long enough, or small enough† ( †\"smaller and smaller without limit ...you'd be trying to write on molecules, on atoms, on electrons\") to list all members of an enumerably infinite set by writing out their names, one after another, in some notation. But humans can do something equally useful, in the case of certain enumerably infinite sets: They can give \"explicit instructions for determining the nth member of the set\", for arbitrary finite \"n\". Such instructions are to be given quite explicitly, in a form in which \"they could be followed by a computing machine\", or by a \"human who is capable of carrying out only very elementary operations on symbols.\"\n\nAn \"enumerably infinite set\" is one whose elements can be put into one-to-one correspondence with the integers. Thus, Boolos and Jeffrey are saying that an algorithm implies instructions for a process that \"creates\" output integers from an \"arbitrary\" \"input\" integer or integers that, in theory, can be arbitrarily large. Thus an algorithm can be an algebraic equation such as \"y = m + n\" – two arbitrary \"input variables\" \"m\" and \"n\" that produce an output \"y\". But various authors' attempts to define the notion indicate that the word implies much more than this, something on the order of (for the addition example):\n\nThe concept of \"algorithm\" is also used to define the notion of decidability. That notion is central for explaining how formal systems come into being starting from a small set of axioms and rules. In logic, the time that an algorithm requires to complete cannot be measured, as it is not apparently related to our customary physical dimension. From such uncertainties, that characterize ongoing work, stems the unavailability of a definition of \"algorithm\" that suits both concrete (in some sense) and abstract usage of the term.\n\nAlgorithms are essential to the way computers process data. Many computer programs contain algorithms that detail the specific instructions a computer should perform (in a specific order) to carry out a specified task, such as calculating employees' paychecks or printing students' report cards. Thus, an algorithm can be considered to be any sequence of operations that can be simulated by a Turing-complete system. Authors who assert this thesis include Minsky (1967), Savage (1987) and Gurevich (2000):\n\nTypically, when an algorithm is associated with processing information, data can be read from an input source, written to an output device and stored for further processing. Stored data are regarded as part of the internal state of the entity performing the algorithm. In practice, the state is stored in one or more data structures.\n\nFor some such computational process, the algorithm must be rigorously defined: specified in the way it applies in all possible circumstances that could arise. That is, any conditional steps must be systematically dealt with, case-by-case; the criteria for each case must be clear (and computable).\n\nBecause an algorithm is a precise list of precise steps, the order of computation is always crucial to the functioning of the algorithm. Instructions are usually assumed to be listed explicitly, and are described as starting \"from the top\" and going \"down to the bottom\", an idea that is described more formally by \"flow of control\".\n\nSo far, this discussion of the formalization of an algorithm has assumed the premises of imperative programming. This is the most common conception, and it attempts to describe a task in discrete, \"mechanical\" means. Unique to this conception of formalized algorithms is the assignment operation, setting the value of a variable. It derives from the intuition of \"memory\" as a scratchpad. There is an example below of such an assignment.\n\nFor some alternate conceptions of what constitutes an algorithm see functional programming and logic programming.\n\nAlgorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in natural language statements. Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer but are often used as a way to define or document algorithms.\n\nThere is a wide variety of representations possible and one can express a given Turing machine program as a sequence of machine tables (see more at finite-state machine, state transition table and control table), as flowcharts and drakon-charts (see more at state diagram), or as a form of rudimentary machine code or assembly code called \"sets of quadruples\" (see more at Turing machine).\n\nRepresentations of algorithms can be classed into three accepted levels of Turing machine description:\n\nFor an example of the simple algorithm \"Add m+n\" described in all three levels, see Algorithm#Examples.\n\nAlgorithm design refers to a method or mathematical process for problem-solving and engineering algorithms. The design of algorithms is part of many solution theories of operation research, such as dynamic programming and divide-and-conquer. Techniques for designing and implementing algorithm designs are also called algorithm design patterns, such as the template method pattern and decorator pattern.\n\nOne of the most important aspects of algorithm design is creating an algorithm that has an efficient run-time, also known as its Big O.\n\nTypical steps in the development of algorithms:\n\nMost algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain implementing arithmetic or an insect looking for food), in an electrical circuit, or in a mechanical device.\n\nIn computer systems, an algorithm is basically an instance of logic written in software by software developers, to be effective for the intended \"target\" computer(s) to produce \"output\" from given (perhaps null) \"input\". An optimal algorithm, even running in old hardware, would produce faster results than a non-optimal (higher time complexity) algorithm for the same purpose, running in more efficient hardware; that is why algorithms, like computer hardware, are considered technology.\n\n\"\"Elegant\" (compact) programs, \"good\" (fast) programs \": The notion of \"simplicity and elegance\" appears informally in Knuth and precisely in Chaitin:\n\nChaitin prefaces his definition with: \"I'll show you can't prove that a program is 'elegant'\"—such proof would solve the Halting problem (ibid).\n\n\"Algorithm versus function computable by an algorithm\": For a given function multiple algorithms may exist. This is true, even without expanding the available instruction set available to the programmer. Rogers observes that \"It is . . . important to distinguish between the notion of \"algorithm\", i.e. procedure and the notion of \"function computable by algorithm\", i.e. mapping yielded by the procedure. The same function may have several different algorithms\".\n\nUnfortunately, there may be a tradeoff between goodness (speed) and elegance (compactness)—an elegant program may take more steps to complete a computation than one less elegant. An example that uses Euclid's algorithm appears below.\n\n\"Computers (and computors), models of computation\": A computer (or human \"computor\") is a restricted type of machine, a \"discrete deterministic mechanical device\" that blindly follows its instructions. Melzak's and Lambek's primitive models reduced this notion to four elements: (i) discrete, distinguishable \"locations\", (ii) discrete, indistinguishable \"counters\" (iii) an agent, and (iv) a list of instructions that are \"effective\" relative to the capability of the agent.\n\nMinsky describes a more congenial variation of Lambek's \"abacus\" model in his \"Very Simple Bases for Computability\". Minsky's machine proceeds sequentially through its five (or six, depending on how one counts) instructions, unless either a conditional IF–THEN GOTO or an unconditional GOTO changes program flow out of sequence. Besides HALT, Minsky's machine includes three \"assignment\" (replacement, substitution) operations: ZERO (e.g. the contents of location replaced by 0: L ← 0), SUCCESSOR (e.g. L ← L+1), and DECREMENT (e.g. L ← L − 1). Rarely must a programmer write \"code\" with such a limited instruction set. But Minsky shows (as do Melzak and Lambek) that his machine is Turing complete with only four general \"types\" of instructions: conditional GOTO, unconditional GOTO, assignment/replacement/substitution, and HALT.\n\n\"Simulation of an algorithm: computer (computor) language\": Knuth advises the reader that \"the best way to learn an algorithm is to try it . . . immediately take pen and paper and work through an example\". But what about a simulation or execution of the real thing? The programmer must translate the algorithm into a language that the simulator/computer/computor can \"effectively\" execute. Stone gives an example of this: when computing the roots of a quadratic equation the computor must know how to take a square root. If they don't, then the algorithm, to be effective, must provide a set of rules for extracting a square root.\n\nThis means that the programmer must know a \"language\" that is effective relative to the target computing agent (computer/computor).\n\nBut what model should be used for the simulation? Van Emde Boas observes \"even if we base complexity theory on abstract instead of concrete machines, arbitrariness of the choice of a model remains. It is at this point that the notion of \"simulation\" enters\". When speed is being measured, the instruction set matters. For example, the subprogram in Euclid's algorithm to compute the remainder would execute much faster if the programmer had a \"modulus\" instruction available rather than just subtraction (or worse: just Minsky's \"decrement\").\n\n\"Structured programming, canonical structures\": Per the Church–Turing thesis, any algorithm can be computed by a model known to be Turing complete, and per Minsky's demonstrations, Turing completeness requires only four instruction types—conditional GOTO, unconditional GOTO, assignment, HALT. Kemeny and Kurtz observe that, while \"undisciplined\" use of unconditional GOTOs and conditional IF-THEN GOTOs can result in \"spaghetti code\", a programmer can write structured programs using only these instructions; on the other hand \"it is also possible, and not too hard, to write badly structured programs in a structured language\". Tausworthe augments the three Böhm-Jacopini canonical structures: SEQUENCE, IF-THEN-ELSE, and WHILE-DO, with two more: DO-WHILE and CASE. An additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical induction.\n\n\"Canonical flowchart symbols\": The graphical aide called a flowchart, offers a way to describe and document an algorithm (and a computer program of one). Like the program flow of a Minsky machine, a flowchart always starts at the top of a page and proceeds down. Its primary symbols are only four: the directed arrow showing program flow, the rectangle (SEQUENCE, GOTO), the diamond (IF-THEN-ELSE), and the dot (OR-tie). The Böhm–Jacopini canonical structures are made of these primitive shapes. Sub-structures can \"nest\" in rectangles, but only if a single exit occurs from the superstructure. The symbols, and their use to build the canonical structures are shown in the diagram.\n\nOne of the simplest algorithms is to find the largest number in a list of numbers of random order. Finding the solution requires looking at every number in the list. From this follows a simple algorithm, which can be stated in a high-level description of English prose, as:\n\n\"High-level description:\"\n\n\"(Quasi-)formal description:\"\nWritten in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in pseudocode or pidgin code:\n\nEuclid's algorithm to compute the greatest common divisor (GCD) to two numbers appears as Proposition II in Book VII (\"Elementary Number Theory\") of his \"Elements\". Euclid poses the problem thus: \"Given two numbers not prime to one another, to find their greatest common measure\". He defines \"A number [to be] a multitude composed of units\": a counting number, a positive integer not including zero. To \"measure\" is to place a shorter measuring length \"s\" successively (\"q\" times) along longer length \"l\" until the remaining portion \"r\" is less than the shorter length \"s\". In modern words, remainder \"r\" = \"l\" − \"q\"×\"s\", \"q\" being the quotient, or remainder \"r\" is the \"modulus\", the integer-fractional part left over after the division.\n\nFor Euclid's method to succeed, the starting lengths must satisfy two requirements: (i) the lengths must not be zero, AND (ii) the subtraction must be “proper”; i.e., a test must guarantee that the smaller of the two numbers is subtracted from the larger (alternately, the two can be equal so their subtraction yields zero).\n\nEuclid's original proof adds a third requirement: the two lengths must not be prime to one another. Euclid stipulated this so that he could construct a reductio ad absurdum proof that the two numbers' common measure is in fact the \"greatest\". While Nicomachus' algorithm is the same as Euclid's, when the numbers are prime to one another, it yields the number \"1\" for their common measure. So, to be precise, the following is really Nicomachus' algorithm.\n\nOnly a few instruction \"types\" are required to execute Euclid's algorithm—some logical tests (conditional GOTO), unconditional GOTO, assignment (replacement), and subtraction.\n\nThe following algorithm is framed as Knuth's four-step version of Euclid's and Nicomachus', but, rather than using division to find the remainder, it uses successive subtractions of the shorter length \"s\" from the remaining length \"r\" until \"r\" is less than \"s\". The high-level description, shown in boldface, is adapted from Knuth 1973:2–4:\n\nINPUT:\n\nE0: [Ensure \"r\" ≥ \"s\".]\n\nE1: [Find remainder]: Until the remaining length \"r\" in R is less than the shorter length \"s\" in S, repeatedly subtract the measuring number \"s\" in S from the remaining length \"r\" in R.\n\nE2: [Is the remainder zero?]: EITHER (i) the last measure was exact, the remainder in R is zero, and the program can halt, OR (ii) the algorithm must continue: the last measure left a remainder in R less than measuring number in S.\n\nE3: [Interchange \"s\" and \"r\"]: The nut of Euclid's algorithm. Use remainder \"r\" to measure what was previously smaller number \"s\"; L serves as a temporary location.\n\nOUTPUT:\n\nDONE:\n\nThe following version of Euclid's algorithm requires only six core instructions to do what thirteen are required to do by \"Inelegant\"; worse, \"Inelegant\" requires more \"types\" of instructions. The flowchart of \"Elegant\" can be found at the top of this article. In the (unstructured) Basic language, the steps are numbered, and the instruction is the assignment instruction symbolized by ←.\n\nThe following version can be used with Object Oriented languages:\n\n\"How \"Elegant\" works\": In place of an outer \"Euclid loop\", \"Elegant\" shifts back and forth between two \"co-loops\", an A > B loop that computes A ← A − B, and a B ≤ A loop that computes B ← B − A. This works because, when at last the minuend M is less than or equal to the subtrahend S ( Difference = Minuend − Subtrahend), the minuend can become \"s\" (the new measuring length) and the subtrahend can become the new \"r\" (the length to be measured); in other words the \"sense\" of the subtraction reverses.\n\nDoes an algorithm do what its author wants it to do? A few test cases usually suffice to confirm core functionality. One source uses 3009 and 884. Knuth suggested 40902, 24140. Another interesting case is the two relatively prime numbers 14157 and 5950.\n\nBut exceptional cases must be identified and tested. Will \"Inelegant\" perform properly when R > S, S > R, R = S? Ditto for \"Elegant\": B > A, A > B, A = B? (Yes to all). What happens when one number is zero, both numbers are zero? (\"Inelegant\" computes forever in all cases; \"Elegant\" computes forever when A = 0.) What happens if \"negative\" numbers are entered? Fractional numbers? If the input numbers, i.e. the domain of the function computed by the algorithm/program, is to include only positive integers including zero, then the failures at zero indicate that the algorithm (and the program that instantiates it) is a partial function rather than a total function. A notable failure due to exceptions is the Ariane 5 Flight 501 rocket failure (June 4, 1996).\n\n\"Proof of program correctness by use of mathematical induction\": Knuth demonstrates the application of mathematical induction to an \"extended\" version of Euclid's algorithm, and he proposes \"a general method applicable to proving the validity of any algorithm\". Tausworthe proposes that a measure of the complexity of a program be the length of its correctness proof.\n\n\"Elegance (compactness) versus goodness (speed)\": With only six core instructions, \"Elegant\" is the clear winner, compared to \"Inelegant\" at thirteen instructions. However, \"Inelegant\" is \"faster\" (it arrives at HALT in fewer steps). Algorithm analysis indicates why this is the case: \"Elegant\" does \"two\" conditional tests in every subtraction loop, whereas \"Inelegant\" only does one. As the algorithm (usually) requires many loop-throughs, \"on average\" much time is wasted doing a \"B = 0?\" test that is needed only after the remainder is computed.\n\n\"Can the algorithms be improved?\": Once the programmer judges a program \"fit\" and \"effective\"—that is, it computes the function intended by its author—then the question becomes, can it be improved?\n\nThe compactness of \"Inelegant\" can be improved by the elimination of five steps. But Chaitin proved that compacting an algorithm cannot be automated by a generalized algorithm; rather, it can only be done heuristically; i.e., by exhaustive search (examples to be found at Busy beaver), trial and error, cleverness, insight, application of inductive reasoning, etc. Observe that steps 4, 5 and 6 are repeated in steps 11, 12 and 13. Comparison with \"Elegant\" provides a hint that these steps, together with steps 2 and 3, can be eliminated. This reduces the number of core instructions from thirteen to eight, which makes it \"more elegant\" than \"Elegant\", at nine steps.\n\nThe speed of \"Elegant\" can be improved by moving the \"B=0?\" test outside of the two subtraction loops. This change calls for the addition of three instructions (B = 0?, A = 0?, GOTO). Now \"Elegant\" computes the example-numbers faster; whether this is always the case for any given A, B, and R, S would require a detailed analysis.\n\nIt is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, the sorting algorithm above has a time requirement of O(\"n\"), using the big O notation with \"n\" as the length of the list. At all times the algorithm only needs to remember two values: the largest number found so far, and its current position in the input list. Therefore, it is said to have a space requirement of \"O(1)\", if the space required to store the input numbers is not counted, or O(\"n\") if it is counted.\n\nDifferent algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm (with cost O(log n) ) outperforms a sequential search (cost O(n) ) when used for table lookups on sorted lists or arrays.\n\nThe analysis, and study of algorithms is a discipline of computer science, and is often practiced abstractly without the use of a specific programming language or implementation. In this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation. Usually pseudocode is used for analysis as it is the simplest and most general representation. However, ultimately, most algorithms are usually implemented on particular hardware/software platforms and their algorithmic efficiency is eventually put to the test using real code. For the solution of a \"one off\" problem, the efficiency of a particular algorithm may not have significant consequences (unless n is extremely large) but for algorithms designed for fast interactive, commercial or long life scientific usage it may be critical. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.\n\nEmpirical testing is useful because it may uncover unexpected interactions that affect performance. Benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization.\nEmpirical tests cannot replace formal analysis, though, and are not trivial to perform in a fair manner.\n\nTo illustrate the potential improvements possible even in well-established algorithms, a recent significant innovation, relating to FFT algorithms (used heavily in the field of image processing), can decrease processing time up to 1,000 times for applications like medical imaging. In general, speed improvements depend on special properties of the problem, which are very common in practical applications. Speedups of this magnitude enable computing devices that make extensive use of image processing (like digital cameras and medical equipment) to consume less power.\n\nThere are various ways to classify algorithms, each with its own merits.\n\nOne way to classify algorithms is by implementation means.\n\n\nAnother way of classifying algorithms is by their design methodology or paradigm. There is a certain number of paradigms, each different from the other. Furthermore, each of these categories includes many different types of algorithms. Some common paradigms are:\n\n\nFor optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following:\n\n\nEvery field of science has its own problems and needs efficient algorithms. Related problems in one field are often studied together. Some example classes are search algorithms, sorting algorithms, merge algorithms, numerical algorithms, graph algorithms, string algorithms, computational geometric algorithms, combinatorial algorithms, medical algorithms, machine learning, cryptography, data compression algorithms and parsing techniques.\n\nFields tend to overlap with each other, and algorithm advances in one field may improve those of other, sometimes completely unrelated, fields. For example, dynamic programming was invented for optimization of resource consumption in industry but is now used in solving a broad range of problems in many fields.\n\nAlgorithms can be classified by the amount of time they need to complete compared to their input size:\n\n\nSome problems may have multiple algorithms of differing complexity, while other problems might have no algorithms or no known efficient algorithms. There are also mappings from some problems to other problems. Owing to this, it was found to be more suitable to classify the problems themselves instead of the algorithms into equivalence classes based on the complexity of the best possible algorithms for them.\n\nThe adjective \"continuous\" when applied to the word \"algorithm\" can mean:\n\nAlgorithms, by themselves, are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute \"processes\" (USPTO 2006), and hence algorithms are not patentable (as in Gottschalk v. Benson). However practical applications of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is highly controversial, and there are highly criticized patents involving algorithms, especially data compression algorithms, such as Unisys' LZW patent.\n\nAdditionally, some cryptographic algorithms have export restrictions (see export of cryptography).\n\nAlgorithms were used in ancient Greece. Two examples are the Sieve of Eratosthenes, which was described in Introduction to Arithmetic by Nicomachus, and the Euclidean algorithm, which was first described in Euclid's Elements (c. 300 BC). Babylonian clay tablets describe and employ algorithmic procedures to compute the time and place of significant astronomical events.\n\nTally-marks: To keep track of their flocks, their sacks of grain and their money the ancients used tallying: accumulating stones or marks scratched on sticks or making discrete symbols in clay. Through the Babylonian and Egyptian use of marks and symbols, eventually Roman numerals and the abacus evolved (Dilson, p. 16–41). Tally marks appear prominently in unary numeral system arithmetic used in Turing machine and Post–Turing machine computations.\n\nThe work of the ancient Greek geometers (Euclidean algorithm), the Indian mathematician Brahmagupta, and the Persian mathematician Al-Khwarizmi (from whose name the terms \"algorism\" and \"algorithm\" are derived), and Western European mathematicians culminated in Leibniz's notion of the calculus ratiocinator (ca 1680):\n\"The clock\": Bolter credits the invention of the weight-driven clock as \"The key invention [of Europe in the Middle Ages]\", in particular, the verge escapement that provides us with the tick and tock of a mechanical clock. \"The accurate automatic machine\" led immediately to \"mechanical automata\" beginning in the 13th century and finally to \"computational machines\"—the difference engine and analytical engines of Charles Babbage and Countess Ada Lovelace, mid-19th century. Lovelace is credited with the first creation of an algorithm intended for processing on a computer – Babbage's analytical engine, the first device considered a real Turing-complete computer instead of just a calculator – and is sometimes called \"history's first programmer\" as a result, though a full implementation of Babbage's second device would not be realized until decades after her lifetime.\n\n\"Logical machines 1870—Stanley Jevons' \"logical abacus\" and \"logical machine\"\": The technical problem was to reduce Boolean equations when presented in a form similar to what is now known as Karnaugh maps. Jevons (1880) describes first a simple \"abacus\" of \"slips of wood furnished with pins, contrived so that any part or class of the [logical] combinations can be picked out mechanically . . . More recently, however, I have reduced the system to a completely mechanical form, and have thus embodied the whole of the indirect process of inference in what may be called a \"Logical Machine\"\" His machine came equipped with \"certain moveable wooden rods\" and \"at the foot are 21 keys like those of a piano [etc] . . .\". With this machine he could analyze a \"syllogism or any other simple logical argument\".\n\nThis machine he displayed in 1870 before the Fellows of the Royal Society. Another logician John Venn, however, in his 1881 \"Symbolic Logic\", turned a jaundiced eye to this effort: \"I have no high estimate myself of the interest or importance of what are sometimes called logical machines ... it does not seem to me that any contrivances at present known or likely to be discovered really deserve the name of logical machines\"; see more at Algorithm characterizations. But not to be outdone he too presented \"a plan somewhat analogous, I apprehend, to Prof. Jevon's \"abacus\" ... [And] [a]gain, corresponding to Prof. Jevons's logical machine, the following contrivance may be described. I prefer to call it merely a logical-diagram machine ... but I suppose that it could do very completely all that can be rationally expected of any logical machine\".\n\n\"Jacquard loom, Hollerith punch cards, telegraphy and telephony—the electromechanical relay\": Bell and Newell (1971) indicate that the Jacquard loom (1801), precursor to Hollerith cards (punch cards, 1887), and \"telephone switching technologies\" were the roots of a tree leading to the development of the first computers. By the mid-19th century the telegraph, the precursor of the telephone, was in use throughout the world, it's discrete and distinguishable encoding of letters as \"dots and dashes\" a common sound. By the late 19th century the ticker tape (ca 1870s) was in use, as was the use of Hollerith cards in the 1890 U.S. census. Then came the teleprinter (ca. 1910) with its punched-paper use of Baudot code on tape.\n\n\"Telephone-switching networks\" of electromechanical relays (invented 1835) was behind the work of George Stibitz (1937), the inventor of the digital adding device. As he worked in Bell Laboratories, he observed the \"burdensome' use of mechanical calculators with gears. \"He went home one evening in 1937 intending to test his idea... When the tinkering was over, Stibitz had constructed a binary adding device\".\n\nDavis (2000) observes the particular importance of the electromechanical relay (with its two \"binary states\" \"open\" and \"closed\"):\n\n\"Symbols and rules\": In rapid succession, the mathematics of George Boole (1847, 1854), Gottlob Frege (1879), and Giuseppe Peano (1888–1889) reduced arithmetic to a sequence of symbols manipulated by rules. Peano's \"The principles of arithmetic, presented by a new method\" (1888) was \"the first attempt at an axiomatization of mathematics in a symbolic language\".\n\nBut Heijenoort gives Frege (1879) this kudos: Frege's is \"perhaps the most important single work ever written in logic. ... in which we see a \" 'formula language', that is a \"lingua characterica\", a language written with special symbols, \"for pure thought\", that is, free from rhetorical embellishments ... constructed from specific symbols that are manipulated according to definite rules\". The work of Frege was further simplified and amplified by Alfred North Whitehead and Bertrand Russell in their Principia Mathematica (1910–1913).\n\n\"The paradoxes\": At the same time a number of disturbing paradoxes appeared in the literature, in particular, the Burali-Forti paradox (1897), the Russell paradox (1902–03), and the Richard Paradox. The resultant considerations led to Kurt Gödel's paper (1931)—he specifically cites the paradox of the liar—that completely reduces rules of recursion to numbers.\n\n\"Effective calculability\": In an effort to solve the Entscheidungsproblem defined precisely by Hilbert in 1928, mathematicians first set about to define what was meant by an \"effective method\" or \"effective calculation\" or \"effective calculability\" (i.e., a calculation that would succeed). In rapid succession the following appeared: Alonzo Church, Stephen Kleene and J. B. Rosser's λ-calculus a finely honed definition of \"general recursion\" from the work of Gödel acting on suggestions of Jacques Herbrand (cf. Gödel's Princeton lectures of 1934) and subsequent simplifications by Kleene. Church's proof that the Entscheidungsproblem was unsolvable, Emil Post's definition of effective calculability as a worker mindlessly following a list of instructions to move left or right through a sequence of rooms and while there either mark or erase a paper or observe the paper and make a yes-no decision about the next instruction. Alan Turing's proof of that the Entscheidungsproblem was unsolvable by use of his \"a- [automatic-] machine\"—in effect almost identical to Post's \"formulation\", J. Barkley Rosser's definition of \"effective method\" in terms of \"a machine\". S. C. Kleene's proposal of a precursor to \"Church thesis\" that he called \"Thesis I\", and a few years later Kleene's renaming his Thesis \"Church's Thesis\" and proposing \"Turing's Thesis\".\n\nHere is a remarkable coincidence of two men not knowing each other but describing a process of men-as-computers working on computations—and they yield virtually identical definitions.\n\nEmil Post (1936) described the actions of a \"computer\" (human being) as follows:\n\nHis symbol space would be\n\nAlan Turing's work preceded that of Stibitz (1937); it is unknown whether Stibitz knew of the work of Turing. Turing's biographer believed that Turing's use of a typewriter-like model derived from a youthful interest: \"Alan had dreamt of inventing typewriters as a boy; Mrs. Turing had a typewriter, and he could well have begun by asking himself what was meant by calling a typewriter 'mechanical'\". Given the prevalence of Morse code and telegraphy, ticker tape machines, and teletypewriters we might conjecture that all were influences.\n\nTuring—his model of computation is now called a Turing machine—begins, as did Post, with an analysis of a human computer that he whittles down to a simple set of basic motions and \"states of mind\". But he continues a step further and creates a machine as a model of computation of numbers.\n\nTuring's reduction yields the following:\n\"It may be that some of these change necessarily invoke a change of state of mind. The most general single operation must, therefore, be taken to be one of the following:\n\nA few years later, Turing expanded his analysis (thesis, definition) with this forceful expression of it:\n\nJ. Barkley Rosser defined an 'effective [mathematical] method' in the following manner (italicization added):\n\nRosser's footnote No. 5 references the work of (1) Church and Kleene and their definition of λ-definability, in particular Church's use of it in his \"An Unsolvable Problem of Elementary Number Theory\" (1936); (2) Herbrand and Gödel and their use of recursion in particular Gödel's use in his famous paper \"On Formally Undecidable Propositions of Principia Mathematica and Related Systems I\" (1931); and (3) Post (1936) and Turing (1936–37) in their mechanism-models of computation.\n\nStephen C. Kleene defined as his now-famous \"Thesis I\" known as the Church–Turing thesis. But he did this in the following context (boldface in original):\n\nA number of efforts have been directed toward further refinement of the definition of \"algorithm\", and activity is on-going because of issues surrounding, in particular, foundations of mathematics (especially the Church–Turing thesis) and philosophy of mind (especially arguments about artificial intelligence). For more, see Algorithm characterizations.\n\n\n"}
{"id": "33428711", "url": "https://en.wikipedia.org/wiki?curid=33428711", "title": "Analysis Situs (book)", "text": "Analysis Situs (book)\n\nAnalysis Situs is a book by the Princeton mathematician Oswald Veblen, published in 1922. It is based on his 1916 lectures at the Cambridge Colloquium of the American Mathematical Society. The book, which went into a second edition in 1931, was the first English-language textbook on topology, and served for many years as the standard reference for the domain. Its contents were based on the work of Henri Poincaré as well as Veblen's own work with his former student and colleague, James Alexander.\n\nAmong the many innovations in the book was the first definition of a topological manifold, and systematisations of Betti number, torsion, the fundamental group, and the topological classification problem.\n\n"}
{"id": "33431597", "url": "https://en.wikipedia.org/wiki?curid=33431597", "title": "Attentional control", "text": "Attentional control\n\nAttentional control refers to an individual's capacity to choose what they pay attention to and what they ignore. It is also known as endogenous attention or executive attention. In lay terms, attentional control can be described as an individual's ability to concentrate. Primarily mediated by the frontal areas of the brain including the anterior cingulate cortex, attentional control is thought to be closely related to other executive functions such as working memory.\n\nSources of attention in our brain create a system of three networks: alertness (maintaining awareness), orientation (information from sensory input), and executive control (resolving conflict). These three networks have been studied using experimental designs involving adults, children, and monkeys, with and without abnormalities of attention. Research designs include the Stroop task\n\nEarly researchers studying the development of the frontal cortex thought that it was functionally silent during the first year of life. Similarly, early research suggested that infants aged one year or younger are completely passive in the allocation of their attention, and have no capacity to choose what they pay attention to and what they ignore. This is shown, for example, in the phenomenon of 'sticky fixation', whereby infants are incapable of disengaging their attention from a particularly salient target. Other research has suggested, however, that even very young infants do have some capacity to exercise control over their allocation of attention, albeit in a much more limited sense.\n\nAs the frontal lobes mature, children's capacity to exercise attentional control increases, although attentional control abilities remain much poorer in children than they do in adults. Some children show impaired development of attentional control abilities, thought to arise from the relatively slower development of frontal areas of the brain, which sometimes results in a diagnosis of Attention Deficit Hyperactivity Disorder (ADHD).\n\nSome studies of aging and cognition focus on working memory processes and declines in attentional control. One study used fMRI measures during a Stroop task comparing neural activity of attentional control in younger (21–27 years) and older participants (60–75 years). Conditions included increased competition and increased conflict. Results showed evidence of decreases in responsiveness in brain areas associated with attentional control for the older group. This result suggests that older people may have decreases in their ability to utilize attentional control in their everyday lives.\n\nDisrupted attentional control has been noted not just in the early development of conditions for which the core deficit is related to attention such as ADHD, but also in conditions such as autism and anxiety. Disrupted attentional control has also been reported in infants born preterm, as well as in infants with genetic disorders such as Down syndrome and Williams syndrome. Several groups have also reported impaired attentional control early in development in children from lower socioeconomic status families.\n\nThe patterns of disrupted attentional control relate to findings of disrupted performance on executive functions tasks such as working memory across a wide number of different disorder groups. The question of why the executive functions appear to be disrupted across so many different disorder groups remains, however, poorly understood.\n\nStudies have shown that there is a high probability that those who suffer from low attentional control also experience other mental conditions. Low attentional control is more common among those with attention deficit hyperactivity disorder (ADHD),\"a disorder with persistent age-inappropriate symptoms of inattention, hyperactivity, and impulsivity that are sufficient to cause impairment in major life activities\". Also low attentional control is common in individuals with Schizophrenia and Alzheimer's disease, those with social anxiety, trait anxiety, and depression, and attention difficulties following a stroke. Individuals also respond quicker, and have better overall executive control when they have low levels of anxiety and depression. Low levels of attentional control are also thought to increase chances of developing a psychopathology because the ability to shift one’s focus away from threat information is important in processing emotions. More researchers are also accounting for attentional control in studies that might not necessarily focus on attention by having participants fill out an Attentional Control Scale (ACS) or a Cognitive Attentional Syndrome-1 (CAS1), both of which are self-reporting questionnaires measuring attention focusing and attention shifting. Researchers are also suggesting others in the field use experimental and longitudinal designs to address the relationship between ACS, emotional functioning, CAS, and attention to threat. This is due to the increasing problematic occurrences experts are seeing in the field regarding attentional control in relation to other mental illnesses.\n\nAttention problems are also characteristic of anxiety disorders like PTSD. Attentional bias causes a person to processes emotionally negative information preferentially over emotionally positive information. Participants were selected after being measured on scales for PTSD, anxiety proneness, attentional control, and attentional bias. Results indicated attentional control was inversely related to attentional bias. PTSD patients with higher attentional control exhibited less attentional bias. Individual differences in attentional control had an effect on anxiety problems in PTSD.\n\nAttentional control theory focuses on anxiety and cognitive performance. The assumption of this theory is that the effects of anxiety on attentional control are key to understanding the relationship between anxiety and performance. In general, anxiety inhibits attentional control on a specific task by impairing processing efficiency. There are three functions associated with this theory. The inhibition function prevents stimuli unrelated to a task and responses from disrupting performance. The shifting function is used to allocate attention to the stimuli that are most relevant to the task. The updating function is used to update and monitor information in working memory. There are three main hypotheses associated with attentional control theory. First, the efficiency of the central executive is impaired by anxiety. Second, anxiety impairs the inhibition function, and third, anxiety impairs the shifting function.\n\nEven four days of mindfulness meditation training can significantly improve visuo-spatial processing, working memory and executive functioning. However, research has indicated that mindfulness does not affect attentional control directly. Participants did tasks of sustained attention, inhibition, switching, and object detection. These tasks were done before and after an 8-week mindfulness based stress reduction course (MBSR), and were compared to a control group. There were no significant differences between the groups, meaning that the MBSR course did not affect attentional control. Mindfulness influences non-directed attention and other things like emotional well-being.\n\nModular approaches view cognitive development as a mosaic-like process, according to which cognitive faculties develop separately according to genetically predetermined maturational timetables. Prominent authors who take a modular approach to cognitive development include Jerry Fodor, Elizabeth Spelke and Steven Pinker. In contrast, other authors such as Annette Karmiloff-Smith, Mark Johnson and Linda Smith have instead advocated taking a more interactive or dynamical systems approaches to cognitive development. According to these approaches, which are known as neuroconstructivist approaches, cognitive systems interact over developmental time as certain cognitive faculties are required for the subsequent acquisition of other faculties in other areas.\n\nAmongst authors who take neuroconstructivist approaches to development, particular importance has been attached to attentional control, since it is thought to be a domain-general process that may influence the subsequent acquisition of other skills in other areas. The ability to regulate and direct attention releases the child from the constraints of only responding to environmental events, and means they are able actively to guide their attention towards the information-rich areas key for learning. For example, a number of authors have looked at the relationship between an infants' capacity to exercise attentional control and their subsequent performance during language acquisition.\nWorking memory capacity has been studied to understand how memory functions. The ability to predict the effectiveness of someone’s working memory capacity comes from attentional control mechanisms. These mechanisms help with the regulation of goals, behavior, and outside distractions, which are all important for effective learning.\n\nOur brains have distinct attention systems that have been shaped throughout time by evolution. Visual attention operates mainly on three different representations: location\n, feature, and object-based. The spatial separation between two objects has an effect on attention. People can selectively pay attention to one of two objects in the same general location. Research has also been done on attention to non-object based things like motion. When directing attention to a feature like motion, neuronal activity increases in areas specific for the feature. When visually searching for a non-spatial feature or a perceptual feature, selectively enhancing the sensitivity to that specific feature plays a role in directing attention. When people are told to look for motion, then motion will capture their attention, but attention is not captured by motion if they are told to look for color.\n\nAccording to fMRI studies of the brain and behavioral observations, visual attention can be moved independently of moving eye position. Studies have had participants fixate their eyes on a central point and measured brain activity as stimuli were presented outside the visual fixation point. fMRI findings show changes in brain activity correlated with the shift in spatial attention to the various stimuli. Behavioral studies have also shown that when a person knows where a stimulus is likely to appear, their attention can shift to it more rapidly and process it better.\n\nOther studies have demonstrated that perceptual and cognitive load affect spatial focusing of attention. These two mechanisms interact oppositely so that when cognitive load is decreased, perceptual load must be high to increase spatial attention focusing.\n\nThe cocktail party effect is the phenomenon that a person hears his or her name even when not attending to the conversation. To study this, a screening measure for attentional control was given that tested a person’s ability to keep track of words while also doing math problems. Participants were separated into two groups---low and high span attentional control ability groups. They listened to two word lists read simultaneously by a male and a female voice and were told to ignore the male voice. Their name was read by the “ignored” male voice. Low span people were more likely to hear their name compared to high span people. This result suggests that people with lower attentional control ability have more trouble inhibiting information from the surrounding environment.\n\n"}
{"id": "1900877", "url": "https://en.wikipedia.org/wiki?curid=1900877", "title": "Ayatana", "text": "Ayatana\n\nĀyatana (Pāli; Sanskrit: आयतन) is a Buddhist term that has been translated as \"sense base\", \"sense-media\" or \"sense sphere.\" In Buddhism, there are six \"internal\" sense bases (Pali: \"ajjhattikāni āyatanāni\"; also known as, \"organs\", \"gates\", \"doors\", \"powers\" or \"roots\") and six \"external\" sense bases (\"bāhirāni āyatanāni\" or \"sense objects\"; also known as \"vishaya\" or \"domains\"). Thus, there are six internal-external (organ-object) pairs of sense bases:\n\nBuddhism and other Indian epistemologies identify six \"senses\" as opposed to the Western identification of five. In Buddhism, \"mind\" denotes an internal sense organ which interacts with sense objects that include sense impressions, feelings, perceptions and volition.\n\nIn the Four Noble Truths, the Buddha identifies that the origin of suffering (Pali, Skt.: \"dukkha\") is craving (Pali: \"\"; Skt.: \"\"). In the chain of Dependent Origination, the Buddha identifies that craving arises from sensations that result from contact at the six sense bases. (See Figure 2 below.) Therefore, to overcome craving and its resultant suffering, one should develop restraint of and insight into the sense bases.\n\nThroughout the Pali Canon, the sense bases are referenced in hundreds of discourses. In these diverse discourses, the sense bases are contextualized in different ways including:\n\n\nIn \"The Vipers\" discourse (\"Asivisa Sutta\", SN 35.197), the Buddha likens the internal sense bases to an \"empty village\" and the external sense bases to \"village-plundering bandits.\" Using this metaphor, the Buddha characterizes the \"empty\" sense organs as being \"attacked by agreeable & disagreeable\" sense objects.\n\nElsewhere in the same collection of discourses (SN 35.191), the Buddha's Great Disciple Sariputta clarifies that the actual suffering associated with sense organs and sense objects is not \"inherent\" to these sense bases but is due to the \"fetters\" (here identified as \"desire and lust\") that arise when there is contact between a sense organ and sense object.\n\nIn the \"Fire Sermon\" (\"Adittapariyaya Sutta\", SN 35.28), delivered several months after the Buddha's awakening, the Buddha describes all sense bases and related mental processes in the following manner:\n\nThe Buddha taught that, in order to escape the dangers of the sense bases, one must be able to apprehend the sense bases without defilement. In \"Abandoning the Fetters\" (SN 35.54), the Buddha states that one abandons the fetters \"when one knows and sees ... as impermanent\" (Pali: \"anicca\") the six sense organs, objects, sense-consciousness, contact and sensations. Similarly, in \"Uprooting the Fetters\" (SN 35.55), the Buddha states that one uproots the fetters \"when one knows and sees ... as nonself\" (\"anatta\") the aforementioned five sextets.\n\nTo foster this type of penetrative knowing and seeing and the resultant release from suffering, in the Satipatthana Sutta (MN 10) the Buddha instructs monks to meditate on the sense bases and the dependently arising fetters as follows:\n\nThe \"Vimuttimagga\", the \"Visuddhimagga\", and associated Pali commentaries and subcommentaries all contribute to traditional knowledge about the sense bases.\n\nWhen the Buddha speaks of \"understanding\" the eye, ear, nose, tongue and body, what is meant?\n\nAccording to the first-century CE Sinhalese meditation manual, \"Vimuttimagga\", the sense organs can be understood in terms of the object sensed, the consciousness aroused, the underlying \"sensory matter,\" and an associated primary or derived element that is present \"in excess.\" These characteristics are summarized in the table below.\nThe compendious fifth-century CE \"Visuddhimagga\" provides similar descriptors, such as \"the size of a mere louse's head\" for the location of the eye's \"sensitivity\" (Pali: \"pasāda\"; also known as, \"sentient organ, sense agency, sensitive surface\"), and \"in the place shaped like a goat's hoof\" regarding the nose sensitivity (Vsm. XIV, 47–52). In addition, the Visuddhimagga describes the sense organs in terms of the following four factors:\nThus, for instance, it describes the eye as follows:\n\nIn regards to the sixth internal sense base of mind (\"mano\"), Pali subcommentaries (attributed to Dhammapāla Thera) distinguish between consciousness arising from the five physical sense bases and that arising from the primarily post-canonical notion of a \"life-continuum\" or \"unconscious mind\" (\"bhavaga-mana\"):\n\nIn the fifth-century CE exegetical \"Visuddhimagga\", Buddhaghosa identifies knowing about the sense bases as part of the \"soil\" of liberating wisdom. Other components of this \"soil\" include the aggregates, the faculties, the Four Noble Truths and Dependent Origination.\n\n\n\n<div col||20em|small=yes>\n<div col end>\n\n\n"}
{"id": "8733398", "url": "https://en.wikipedia.org/wiki?curid=8733398", "title": "Ben Franklin effect", "text": "Ben Franklin effect\n\nThe Ben Franklin effect is a proposed psychological phenomenon: a person who has already performed a favor for another is more likely to do another favor for the other than if they had \"received\" a favor from that person. An explanation for this is cognitive dissonance. People reason that they help others \"because\" they like them, even if they do not, because their minds struggle to maintain logical consistency between their actions and perceptions.\n\nThe Benjamin Franklin effect, in other words, is the result of one's concept of self coming under attack. Every person develops a persona, and that persona persists because inconsistencies in one's personal narrative get rewritten, redacted, and misinterpreted.\n\nBenjamin Franklin, after whom the effect is named, quoted what he described as an \"old maxim\" in his autobiography: \"He that has once done you a kindness will be more ready to do you another, than he whom you yourself have obliged.\"\n\nIn his autobiography, Franklin explains how he dealt with the animosity of a rival legislator when he served in the Pennsylvania legislature in the 18th century:\n\nThe initial study of the effect was done by Jecker and Landy in 1969; in which students were invited to take part in a Q&A competition run by the researcher in which they could win sums of money. After this competition was over, one-third of the students who had \"won\" were approached by the researcher, who asked them to return the money on the grounds that he had used his own funds to pay the winners and was running short; another third were asked by a secretary to return the money because it was from the psychology department and funds were low; another third were not approached. All three groups were then asked how much they liked the researcher. The second group liked him the least, the first group the most – suggesting that a refund request by an intermediary had decreased their liking, while a direct request had increased their liking.\n\nIn 1971, University of North Carolina psychologists John Schopler and John Compere carried out the following experiment:\n\nIn short, the subjects' own conduct toward the accomplices shaped their perception of them – \"You tend to like the people to whom you are kind and dislike the people to whom you are rude.\"\n\nResults were mimicked in a more recent but smaller study by psychologist Yu Niiya with Japanese and American subjects.\n\nThis perception of Franklin has been cited as an example within cognitive dissonance theory, which says that people change their attitudes or behavior to resolve tensions, or \"dissonance\", between their thoughts, attitudes, and actions. In the case of the Ben Franklin effect, the dissonance is between the subject's negative attitudes to the other person and the knowledge that they did that person a favor. One science blogger accounts for the phenomenon in the following way: \"Current self-perception theory tells us that our brains behave like an outside observer, continually watching what we do and then contriving explanations for those actions, which subsequently influence our beliefs about ourselves...Our observing brain doesn't like it when our actions don't match the beliefs we have about ourselves, a situation commonly referred to as cognitive dissonance. So, whenever your behavior is in conflict with your beliefs (for example if you do a favor for someone you may not like very much or vice versa, when you do something bad to someone you are supposed to care about), this conflict immediately sets off alarm bells in your brain. The brain has a clever response – it goes about changing how you feel in order to reduce the conflict and turn off the alarms.\"\n\nPsychologist Yu Niiya attributes the phenomenon to the requestee reciprocating a perceived attempt by the requester to ignite friendly relations. This theory would explain the Ben Franklin effect's absence when an intermediary is used.\n\nSome have observed that the Ben Franklin effect can be useful for improving relationships among coworkers.\n\nIn the sales field, the Ben Franklin effect can be used to build rapport with a client. Instead of offering to help the potential client, a salesperson can instead ask the potential client for assistance: \"For example, ask them to share with you what product benefits they find most compelling, where they think the market is headed, or what products may be of interest several years from now. This pure favor, left unrepaid, can build likability that will enhance your ability to earn that client's time and investment in the future.\"\n\nThe Benjamin Franklin effect can also be observed in successful mentor-protege relationships. Such relationships, one source points out, \"are defined by their fundamental imbalance of knowledge and influence. Attempting to proactively reciprocate favors with a mentor can backfire, as the role reversal and unsolicited assistance may put your mentor in an unexpected, awkward situation\".\nThe Ben Franklin effect was cited in Dale Carnegie's bestselling book \"How to Win Friends and Influence People\". Carnegie interprets the request for a favor as \"a subtle but effective form of flattery\".\n\nPsychologist Yu Niiya suggests that the Ben Franklin effect vindicates the theory of \"amae\" (甘え). It states that dependent, childlike behavior can induce a parent-child bond where one partner sees themselves as the caretaker. In effect, \"amae\" creates a relationship where one person feels responsible for the other, who is then free to act immaturely and make demands.\n\nOne commentator has discussed the Ben Franklin effect in connection with dog training, thinking \"more about the human side of the relationship rather than about the dogs themselves.\" While trainers often distinguish between the impact of positive and negative reinforcement-based training methods on the dogs, it can also be relevant to \"consider the effects that these two approaches may have upon the trainer. The Ben Franklin Effect suggests that how we treat our dogs during training influences how we think about them as individuals – specifically, how much we like (or dislike) them. When we do nice things for our dogs in the form of treats, praise, petting and play to reinforce desired behaviors, such treatment may result in our liking them more. And, if we use harsh words, collar jerks or hitting in an attempt to change our dog's behavior, then...we will start to like our dog less.\"\n\nThe opposite case is also believed to be true, namely that we come to hate a person whom we did wrong to. We de-humanize them to justify the bad things we did to them.\n\nIt has been suggested that if soldiers who have killed enemy servicemen in combat situations later come to hate them, it is because this psychological maneuver helps to \"decrease the dissonance of killing\". Such a phenomenon might also \"explain long-standing grudges like Hatfield vs. McCoy\" or vendetta situations in various cultures: \"Once we start, we may not be able to stop and engage in behavior we would normally never allow.\" As one commentator has put it, \"Jailers come to look down on inmates; camp guards come to dehumanize their captives; soldiers create derogatory terms for their enemies. It's difficult to hurt someone you admire. It's even more difficult to kill a fellow human being. Seeing the casualties you create as something less than you, something deserving of damage, makes it possible to continue seeing yourself as a good and honest person, to continue being sane.\"\n\n\n"}
{"id": "41628373", "url": "https://en.wikipedia.org/wiki?curid=41628373", "title": "Bhuman", "text": "Bhuman\n\nBhuman (Sanskrit:भूमन) means fullness or abundance; It is a synonym of Brahman. The word, \"Bhuman\", is derived from the word, \"Bahu\", meaning much or many, with the suffix – \"imam\", added after it by dropping – \"i\", to impart the sense of the abstract noun. This word refers directly to the Supreme Self who is superior to \"Prana\" though \"Prana\" is \"Bhuman\" because of proximity where the vow of \"Prana\", consisting in transcending all other thing is alluded to.\n\nBrahman has \"avyama\" ('unlimited extension in terms of space'); it is \"sarvagata\" ('omnipresent'), \"ananta\" ('infinite'); it is called \"Bhuman\" ('plenitude') and is \"nirvayava\" or ('without parts') and \"arupvad\" ('formless'), and eternal because it is \"aksara\" ('imperishable'). Brahman is Bhuman, the plenitude which transcends the sum of its parts and yet fully inheres in them. The quality of being the Bhuman (plenitude) etc.; ascribed by the \"Śrutis\" agrees well with the highest Self, which is the cause of everything. Brahman resides in its entirety within the smallest particle imaginable and yet remains the \"ayatana\", the receptacle, or \"bhuman\", the totality which transcends the sum of these parts.\n\nBhuman is not the human soul, the companion of Prana; Bhuman is Ananda (Bliss), immense joy. \"Samprasada\" also refers to the released soul. The consideration of happiness induces and impels man to perform actions, had he experienced unhappiness in his pursuit he would not have gone in for the actions at all; but real happiness is that which one enjoys in the vision of the Infinite. The Doctrine of Ananda (Bliss) is central to the philosophy of Vedanta. Ananda conveys a sense of infinite, eternal, absolute happiness and not selfish pleasure which is transitory gratification but a state of absolute expansion called Bhuman. Bhuman is illimitation. Ananda, the supreme pleasure, is in illimitation and not in the limited (\"alpa\").\n\nBrahman is the Absolute to be distinguished from Prajapati and from Prana, the Vital force, directly identified with life (\"ayuh\") and consciousness (\"Prajna\"). Badarayana declares -\nPanini explains that the word, Bhuman, is derived from the word \"Bahu\" meaning much; many, with the suffix \"imam\" added after it by dropping – \"i\" to impart the sense of the abstract noun. Adi Sankara in his \"Bhasya\" states that this word is introduced without discarding Prana to mean Prana is Brahman. But, here the instruction is that Bhuman is superior to Prana therefore Prana cannot be Brahman because from Brahman proceeds Prana (Chandogya Upanishad VII.xxvi.1). According to Madhava, Bhuman is that which is beyond all and not merely beyond Prana alone; if it is something subsequent to Prana it cannot be Brahman.\n"}
{"id": "1115098", "url": "https://en.wikipedia.org/wiki?curid=1115098", "title": "Business ecosystem", "text": "Business ecosystem\n\nStarting in the early 1990s, James F. Moore originated the strategic planning concept of a business ecosystem, now widely adopted in the high tech community. The basic definition comes from Moore's book, \"\".\n\nThe concept first appeared in Moore's May/June 1993 \"Harvard Business Review\" article, titled \"Predators and Prey: A New Ecology of Competition\", and won the McKinsey Award for article of the year.\n\nMoore defined \"business ecosystem\" as:\nAn economic community supported by a foundation of interacting organizations and individuals—the organisms of the business world. The economic community produces goods and services of value to customers, who are themselves members of the ecosystem. The member organisms also include suppliers, lead producers, competitors, and other stakeholders. Over time, they coevolve their capabilities and roles, and tend to align themselves with the directions set by one or more central companies. Those companies holding leadership roles may change over time, but the function of ecosystem leader is valued by the community because it enables members to move toward shared visions to align their investments, and to find mutually supportive roles.\nMoore used several ecological metaphors, suggesting that the firm is embedded in a (business) environment, that it needs to coevolve with other companies, and that “the particular niche a business occupies is challenged by newly arriving species.” This meant that companies need to become proactive in developing mutually beneficial (\"symbiotic\") relationships with customers, suppliers, and even competitors.\n\nUsing ecological metaphors to describe business structure and operations is increasingly common especially within the field of information technology (IT). For example, J. Bradford DeLong, a professor of economics at the University of California, Berkeley, has written that \"business ecosystems\" describe “the pattern of launching new technologies that has emerged from Silicon Valley”. He defines business ecology as “a more productive set of processes for developing and commercializing new technologies” that is characterized by the “rapid prototyping, short product-development cycles, early test marketing, options-based compensation, venture funding, early corporate independence”. DeLong also has expressed that the new way is likely to endure “because it's a better business ecology than the legendarily lugubrious model refined at Xerox Parc—a more productive set of processes for rapidly developing and commercializing new technologies”.\n\nMangrove Software, The Montague Institute, Kenneth L. Kraemer, director of the University of California, Irvine’s Center for Research on Information Technology and Organizations and Stephen Abram, Vice President of Micromedia, Ltd., Tom Gruber, co-founder and CTO of Intraspect Software, Vinod K. Dar, Managing Director of Dar & Company, have all advocated this approach.\n\nGruber explains that over a century ago, Ford Motors did well using methods of mass production, an assembly line, and insourcing. However, Ford began to outsource its production “[w]hen the ecology evolved.” Gruber (n.d.) has stated that such evolution in the ecology of the business world is “punctuated now and then by radical changes in the environment” and that “globalization and the Internet are the equivalents of large-scale climate change. Globalization is eliminating the traditional advantages of the large corporation: access to capital, access to markets, and economies of scale”.\n\nThe application service provider (ASP) industry is moving toward relationship networks and focusing on core competencies. “According to the gospel of Cisco Systems, companies inclined to exist together within an “ecosystem” facilitate the imminence of Internet-based application delivery”.\n\nBooks also use natural systems metaphors without discussing the interfaces between human business and biological ecosystems.\n\nAnother work defines business ecology as “a new field for sustainable organizational management and design,” one “that is based on the principle that organizations, as living organisms, are most successful when their development and behavior are aligned with their core purpose and values – what we call “social DNA’”.\n\nThe need for companies to attend to ecological health is indicated by the following: “Business ecology is based on the elegant structure and principles of natural systems. It recognizes that to develop healthy business ecosystems, leaders and their organizations must see themselves, and their environments, through an “ecological lens”.\n\nSome environmentalists have used \"business ecosystems\" as a way to talk about environmental issues as they relate to business rather than as a metaphor to describe the increasing complexity of relationships among companies. According to Townsend, business ecology is the study of the reciprocal relationship between business and organisms and their environments. The goal of this \"business ecology\" is sustainability through the complete ecological synchronization and integration of a business with the sites that it inhabits, uses, and affects.\n\nOther environmentalists believe that the ecosystem metaphor is just a way for business to appear 'Green'. According to author Alan Marshall, the metaphor is used to make out that somehow business operates using natural principles which should be left to run without interference by governments.\n\n"}
{"id": "37053189", "url": "https://en.wikipedia.org/wiki?curid=37053189", "title": "Concealed 360", "text": "Concealed 360\n\nConcealed 360 is an interactive short film which uses 360-degree audio and visual exploration to explore how one’s personal perspective affects their interpretation of narrative. Filmed with a 360-degree camera-rig consisting of six 1080p cameras, the film enables users to pan 360-degrees, while taking in the action of each scene as it unfolds in order to solve the murder mystery.\n\nDillon Wagner, an amateur drug trafficker, emerges from a coma and tries to figure out how he ended up in a hospital. In addition to his memory loss, he finds out that his closest friend and drug connection, Joey Valeri, was murdered. Not knowing why, or by whom, Wagner desperately tries to piece together the last few days leading up to the murder.\n\n\nPre-screenings of Concealed 360 were conducted on May 19 and May 20 at the Maker Faire 2012 in San Mateo. The reception was over 65,000 people in attendance as the user interactivity user tested. The final interactive screening was held on June 13 at CSU Eastbay.\n"}
{"id": "31127943", "url": "https://en.wikipedia.org/wiki?curid=31127943", "title": "Conflict trap", "text": "Conflict trap\n\nConflict trap is a term to describe the pattern when civil wars repeat themselves.\n\nScholars have offered a few reasons for it, after Paul Collier and Nicholas Sambanis (2002) noticed this pattern and coined the term ‘conflict trap’. Firstly, a civil war can aggravate the causal political and economic conditions which lead to a repeat civil war. Other reasons can be because the combatants are fighting over stakes that is valuable to both the parties, combatants are unable to defeat each other or are engaged in a long duration serious war.\n\nPolitical and Legal Institutions play an important role in inhibiting a repeat civil war, hence preventing a country from entering a conflict trap. Strong institutions can put a check on the incumbent’s power, hence ensuring public welfare and therefore rebels have fewer reasons to restart a civil war. Also, this removes the need for the rebels to maintain an army or a threat of war. A strong political and legal system also gives a non-violent platform to rebels to bring about desired changes, so they may not need to adopt violent ways to protest.\n\nThere can be other reasons why a civil war may start and repeat itself. There may be intense grievances held by the groups, which motivate them to fight. Inferior health conditions, lower GDP and poor human development create conditions for dissatisfaction, and hence civil wars. If a country is poor, it may be easier for rebels to organize an army and start a civil war. Weaker states serve as one of reasons for an easier rebellion, since they lack capacity to effectively keep rebels in check. A country’s geography may also contribute in wars since rebels can easily escape.\n\nThe Bottom Billion\n"}
{"id": "22203751", "url": "https://en.wikipedia.org/wiki?curid=22203751", "title": "Cortex System", "text": "Cortex System\n\nThe Cortex System is a generic RPG system based on the \"Sovereign Stone System\", and was developed by Margaret Weis Productions, Ltd for the Serenity Role Playing Game. It was subsequently used for their licensed \"Battlestar Galactica\" and Supernatural RPGs, and brought out as a stand-alone system in the \"Cortex System Role Playing Game\" book (also called the \"Cortex Classic System Role Playing Game\"). Serenity, using the Cortex System, was the 2005 Origins Award Gamer's Choice Role Playing Game of the Year.\n\nNote that despite the similar name, the Cortex System is not the Cortex Plus system, also produced by Margaret Weis Productions.\n\nThe system uses dice with 2, 4, 6, 8, 10, and 12 sides, described with the standard role-playing game notation of d2, d4, d6, d8, and so on, and the basic resolution system involves adding up the total on all your relevant dice and comparing it to a static target number. The three parts of your dice pool are your relevant Attributes (Agility, Strength, Vitality, Alertness, Intelligence, and Willpower) which range from d4 to d12+d4, your relevant Skill (from a list of 22) which range from 0 to d12, and any relevant Assets (positive character traits) or Complications (negative traits). Situational advantages can be represented either by changing the target difficulty number or increasing or reducing the number of faces on the dice rolled.\n\nThe \"Cortex System\" also uses Plot Points, which increase characters' survivability and give players greater control over events in the game. Players can spend Plot Points to gain extra dice when making a die roll, reduce the damage from an attack, or even make changes to the storyline. Some Assets also require the expenditure of Plot Points. At the end of a game session, excess Plot Points are converted to Advancement Points, which a player spends to improve his or her character's abilities.\n\nThe game uses a point-buy system with a default initial spread between attributes, skills, and traits (the collective name for advantages and complications) based on the intended power level of the game. This is similar to the siloed point buy and special abilities and character flaws used in a variety of other role-playing games, such as Merits and Flaws in White Wolf Publishing's original \"World of Darkness\", Qualities and Drawbacks in Eden Unisystem, and Edges and Hindrances in Pinnacle Entertainment's \"Deadlands\" and \"Savage Worlds\" games (to which the Serenity system bears a strong resemblance).\n\n\nThe initial reception to the Cortex System was good, with the Serenity Role Playing Game winning the 2005 Origins Award for Gamer's Choice: Best Roleplaying Game of the Year. The Battlestar Galactica Role Playing Game gained praise from SF Weekly for the Plot Point mechanisms and the way they reproduced the all-or-nothing moments of the Battlestar Galactica series. The Journal of Transformative Works said that the system itself could be considered transformative given that the rules were meant to evolve through play sessions. By the time of the Smallville Roleplaying Game, RPGamer commented that \"Smallville does something rare in a licensed game: not only does it deliver an experience that captures the feel of the original, but also comes up with a set of mechanics that create an entirely new dynamic for roleplaying.\" \n"}
{"id": "11239314", "url": "https://en.wikipedia.org/wiki?curid=11239314", "title": "Cyphonism", "text": "Cyphonism\n\nCyphonism (Gr , from , \"bent, crooked\") was a form of punishment by the (\"kyphon\"), a sort of wooden pillory by which the neck of the malefactor was bent or weighed downward. Formerly, this term was widely believed to refer to a form of punishment in which the criminal's naked body was smeared with honey, and exposed him to flies, wasps, etc.\n\n"}
{"id": "368361", "url": "https://en.wikipedia.org/wiki?curid=368361", "title": "Downshifting (lifestyle)", "text": "Downshifting (lifestyle)\n\nThe term down-shifting in the English language refers to the act of reducing the gear of a motor vehicle while driving a manual transmission. This title or term has now been re purposed and applied to describe a social behavior or trend in which individuals live simpler lives to escape from what critics call the rat race.\n\nThe long-term effect of downshifting can include an escape from what has been described as economic materialism, as well as reduce the \"stress and psychological expense that may accompany economic materialism\". This new social trend emphasizes finding an improved balance between leisure and work, while also focusing life goals on personal fulfillment, as well as building personal relationships instead of the all-consuming pursuit of economic success.\n\nDown-shifting, as a concept, shares many characteristics with simple living. However, it is distinguished as an alternative form by its focus on moderate change and concentration on an individual comfort level and a gradual approach to living. In the 1990s, this new form of simple living began appearing in the mainstream media, and has continually grown in popularity among populations living in industrial societies, especially the United States, the United Kingdom, New Zealand, and Australia.\n\nDown-shifters are people that adopt long-term voluntary simplicity in their lives. A few of the main practices of down-shifters include accepting less money for fewer hours worked, while placing an emphasis on consuming less in order to reduce their ecological footprint. One of the main results of these practices is being able to enjoy more leisure time in the company of others, especially loved ones. \n\nThe primary motivations for downshifting are gaining leisure time, escaping from work-and-spend cycle, and removing the clutter of unnecessary possessions. The personal goals of downshifting are simple: To reach a holistic self-understanding and satisfying meaning in life.\n\nBecause of its personalized nature and emphasis on many minor changes, rather than complete lifestyle overhaul, it attracts downshifters or participants across the socioeconomic spectrum. An intrinsic consequence of downshifting is increased time for non-work-related activities, which, combined with the diverse demographics of downshifters, cultivates higher levels of civic engagement and social interaction.\n\nThe scope of participation is limitless, because all members of society — adults, children, businesses, institutions, organizations, and governments — are able to downshift.\n\nIn practice, down-shifting involves a variety of behavioral and lifestyle changes. The majority of these down-shifts are voluntary choices. Natural life course events, such as the loss of a job, or birth of a child can prompt involuntary down-shifting. There is also a temporal dimension, because a down-shift could be either temporary or permanent.\n\nThe most common form of down-shifting is work (or income) down-shifting. Down-shifting is fundamentally based on dissatisfaction with the conditions and consequences of the workplace environment. The philosophy of work-to-live replaces the social ideology of live-to-work. Reorienting economic priorities shifts the work–life balance away from the workplace.\n\nEconomically, work downshifts are defined in terms of reductions in either actual or potential income, work hours, and spending levels. Following a path of earnings that is lower than the established market path is a downshift in potential earnings in favor of gaining other non-material benefits.\n\nOn an individual level, work downshifting is a voluntary reduction in annual income. Downshifters desire meaning in life outside of work and, therefore, will opt to decrease the amount of time spent at work or work hours. Reducing the number of hours of work, consequently, lowers the amount earned. Simply not working overtime or taking a half-day a week for leisure time, are work downshifts.\n\nCareer downshifts are another way of downshifting economically and entail lowering previous aspirations of wealth, a promotion or higher social status. Quitting a job to work locally in the community, from home or to start a business are examples of career downshifts. Although more radical, these changes do not mean stopping work altogether.\n\nMany reasons are cited by workers for this choice and usually center on a personal cost–benefit analysis of current working situations and the quality extracurricular activities. High stress, pressure from employers to increase productivity, and long commutes can be factors that contribute to the costs of being employed. If the down-shifter wants more non-material benefits like leisure time, a healthy family life, or personal freedom, then, switching jobs could be a desirable option.\n\nWork down-shifting may also be a key to considerable health benefits as well as a healthy retirement. People are retiring later in life than previous generations. We can see by looking at The Health and Retirement Study, done by the Health and Retirement Study Survey Research Center, women can show the long term health benefits of down-shifting their work lives by working part time hours over a long period of years. Men however prove to be more unhealthy if they work part time from middle age till retirement. Men that down-shift their work life to part time hours at the age of 60 to 65 however benefit from continuing to work a part time job through a semi retirement even over the age of 70. This is an example of how flexible working policies can be a key to being healthy while in retirement. \n\nAnother aspect of down-shifting is being a conscious consumer or actively practicing alternative forms of consumption. Proponents of down-shifting point to consumerism as a primary source of stress and dissatisfaction because it creates a society of individualistic consumers who measure both social status and general happiness by an unattainable quantity of material possessions. Instead of buying goods for personal satisfaction, consumption down-shifting, purchasing only the necessities, is a way to focus on quality of life rather than quantity.\n\nThis realignment of spending priorities promotes the functional utility of goods over their ability to convey status which is evident in downshifters being, in general, less brand-conscious. These consumption habits also facilitate the option of working and earning less because annual spending is proportionally lower. Reducing spending is less demanding than more extreme downshifts in other areas, like employment, as it requires only minor lifestyle changes.\n\nUnions, business, and governments could implement more flexible working hours, part-time work, and other non-traditional work arrangements that enable people to work less, while still maintaining employment. Small business legislation, reduced filing requirements and reduced tax rates encourage small-scale individual entrepreneurship and therefore help individuals quit their jobs altogether and work for themselves on their own terms.\n\nThe catch-phrase of International Downshifting Week is \"Slow Down and Green Up\". Whether intentional or unintentional, generally, the choices and practices of down-shifters nurture environmental health because they reject the fast-paced lifestyle fueled by fossil fuels and adopt more sustainable lifestyles. The latent function of consumption down-shifting is to reduce, to some degree, the carbon footprint of the individual down-shifter. An example is to shift from a corporate suburban rat race lifestyle to a small eco friendly farming lifestyle.\n\nAs a response the hectic pace of life and stresses in urban areas, downshifting geographically is a relocation to a smaller, rural, or more slow-paced community. This is a more drastic change, but because the access to the internet is widespread and possible, downshifting geographically does not bring total removal from mainstream culture.\n\nAlthough downshifting is primarily motivated by personal desire and not by a conscious political stance, it does define societal overconsumption as the source of much personal discontent. By redefining life satisfaction in non-material terms, downshifters assume an alternative lifestyle but continue to coexist in a society and political system preoccupied with the economy. In general, downshifters are politically apathetic because mainstream politicians mobilize voters through the \"hip-pocket nerve\", proposing governmental solutions to periods of financial hardship and economic recessions. This economic rhetoric is meaningless to downshifters who have forgone worrying about money.\n\nAlthough consumers do hold the majority in the United States, the UK, and Australia, a significant minority, approximately 20 to 25 percent, of these countries' citizens identify themselves in some respect as downshifters. Downshifting is not an isolated or unusual choice. Politics still centers around consumerism and unrestricted growth, but downshifting value, such as family priorities and workplace regulation, are appearing in political debates and campaigns.\n\nLike downshifters, the Cultural Creatives is another social movement whose ideology and practices diverge from mainstream consumerism and according to Paul Ray, are followed by at least a quarter of U.S. citizens.\n\nIn his book \"In Praise of Slowness\", Carl Honoré relates followers of downshifting and simple living to the global Slow Movement.\n\nThe emergence of a large and diverse class of downshifters challenges the economically bias ideas for improving society. Downshifting and similar, post-materialist ideologies are rising in popularity, but as a result of their grassroots nature, and relatively inconspicuous, non-confrontational subcultures, they represent unorganized social movements without political aspirations or motivating grievances.\n\n\n"}
{"id": "18855029", "url": "https://en.wikipedia.org/wiki?curid=18855029", "title": "Envy", "text": "Envy\n\nEnvy (from Latin \"invidia\") is an emotion which \"occurs when a person lacks another's superior quality, achievement, or possession and either desires it or wishes that the other lacked it\".\n\nAristotle defined envy as pain at the sight of another’s good fortune, stirred by “those who have what we ought to have.” Bertrand Russell said that envy was one of the most potent causes of unhappiness. Not only is the envious person rendered unhappy by his or her envy, Russell explained, but that person may also wish to inflict misfortune on others, in forms of emotional abuse and violent acts of criminality. Although envy is generally seen as something negative, Russell also believed that envy was a driving force behind the movement of economies and must be endured to achieve the \"keep up with Jones\" system. He believed this is what helps to maintain democracy, a system where no one can achieve more than anyone else. \n\nPsychologists have recently suggested that there are two types of envy: malicious envy and benign envy—malicious envy being proposed as a sick force that ruins a person and his/her mind and causes the envious person to blindly want the \"hero\" to suffer; on the other hand, benign envy being proposed as a type of positive motivational force that causes the person to aspire to be as good as the \"hero\"—but only if benign envy is used in a right way. However, Sherry Turkle considers that the advent of social media and selfie culture is creating an alienating sense of “self-envy” psyche in users, and posits this further affects problem areas attached to attachments. Envy and gloating have parallel structures as emotions.\n\nThe only type of envy that can have positive effects is benign envy. According to researchers, benign envy can provide emulation, improvement motivation, positive thoughts about the other person, and admiration.This type of envy, if dealt with correctly, can positively effect a persons future by motivating them to be a better person and to succeed. Our human instinct is to avoid negative aspects in life such as the negative emotion, envy. However, it is possible to turn this negative emotional state into a motivational tool that can help a person to become successful in the future.\n\nOne theory that helps explain envy and its effects on human behavior is the Socioevolutionary theory. Based upon (Charles) Darwin's (1859) theory of evolution through natural selection, socioevolutionary theory predicts that humans behave in ways that enhance individual survival and the reproduction of their genes. Thus, this theory provides a framework for understanding social behavior and experiences, such as the experience and expression of envy, as rooted in biological drives for survival and procreation. Recent studies have demonstrated that inciting envy actually changes cognitive function, boosting mental persistence and memory.\n\n\"Schadenfreude\" means taking pleasure in the misfortune of others and can be understood as an outgrowth of envy in certain situations.\n\nEnvy and Schadenfreude are very similar and are linked emotional states. Both emotions are considered very complex and often times looked down upon, which is understandable considering they are both antisocial behaviors. Given the detrimental states of these emotions, it is very important to understand their development in the early stages of childhood.\n\nThe two social emotions, envy and schadenfreude act together as sister emotions during the developmental stages in adolescents. Both emotions have a strong presence during these early stages in a child's life. However, as a person ages ,the more they begin to conceal these “bad” emotions and learn to regulate. \n\nIn previous studies, it has been shown that envy becomes less prevalent as a child gets older. Researchers believe that this results from an increase in the regulation of envious type emotions as a person ages and gains more wisdom about life. Depending on age, the correlation of envy and schadenfreude increases. For example a younger child is more likely to make spiteful decisions when they feel envious towards a person. \n\nOften, envy involves a motive to \"outdo or undo the rival's advantages\". In part, this type of envy may be based on materialistic possessions rather than psychological states. Basically, people find themselves experiencing an overwhelming emotion due to someone else owning or possessing desirable items that they do not. For example, your next door neighbor just bought a brand new ocarina—a musical instrument you've been infatuated with for months now but can't afford. Feelings of envy in this situation would occur in the forms of emotional pain, a lack of self-worth, and a lowered self-esteem/well-being.\n\nIn Nelson Aldrich's \"Old Money\", he states that \"envy is so integral and painful a part of what animates human behavior in market societies that many people have forgotten the full meaning of the word, simplifying it into one of the symptoms of desire. It is that (a symptom of desire), which is why it flourishes in market societies: democracies of desire, they might be called, with money for ballots, stuffing permitted. But envy is more or less than desire. It begins with the almost frantic sense of emptiness inside oneself, as if the pump of one's heart were sucking on air. One has to be blind to perceive the emptiness, of course, but that's what envy is, a selective blindness. \"Invidia\", Latin for envy, translates as \"nonsight,\" and Dante had the envious plodding along under cloaks of lead, their eyes sewn shut with leaden wire. What they are blind to is what they have, God-given and humanly nurtured, in themselves\".\n\nEnvy may negatively affect the closeness and satisfaction of relationships. Overcoming envy might be similar to dealing with other negative emotions (anger, resentment, etc.). Individuals experiencing anger often seek professional treatment (anger management) to help understand why they feel the way they do and how to cope. Subjects experiencing envy often have a skewed perception on how to achieve true happiness. By helping people to change these perceptions, they will be more able to understand the real meaning of fortune and satisfaction with what they do have. According to Lazarus \"coping is an integral feature of the emotion process\". There are very few theories that emphasize the coping process for emotions as compared to the information available concerning the emotion itself.\n\nThere are numerous styles of coping, of which there has been a significant amount of research done, for example, avoidant versus approach. Coping with envy can be similar to coping with anger. The issue must be addressed cognitively in order to work through the emotion. According to the research done by Salovey and Rodin (1988), \"more effective strategies for reducing initial envy appear to be stimulus focused rather than self-focused.\". Salovey and Rodin (1988) also suggest \"self-bolstering (e.g., \"thinking about my good qualities\") may be an effective strategy for moderating these self-deprecating thoughts and muting negative affective reactions\". Further research needs to be done in order to better understand envy, as well as to help people cope with this emotion.\n\nEnvy can be a painful emotion that can result from an unflattering social comparison of someone who is perceived to be a superior person. Aristotle defined envy as the pain a person experiences at the sight of another person’s good fortune. While envy is seen to be negative, it is the driving force behind the “keeping up with the Jones” philosophy. This means that there is always something to work toward, and to never become complacent with life.\n\nChildren show evidence of envy at an early age. Adults can be just as envious, however, they tend to be better at concealing the emotion. Envy plays a significant role in the development of adolescents. Comparing oneself is a universal aspect of human nature. No matter the age or culture, social comparison happens all over the globe. Comparison can range from physical attributes, material possessions and intelligence. However,children are more likely to envy over material objects such as shoes, video games, iPhone ect. Kids believe these material objects are correlated to their status.\n\nAs children get older they develop stronger non-materialistic envy such as romantic relationships, physical appearance, achievement, and popularity. Sometimes envious feelings are internalized in children, having a negative impact on their self-esteem. Envy comes from comparing, these comparisons can serve as a reminder that they have failed social norms and do not fit in with their peers. A feeling of inadequacy can arise and become destructive to a child’s happiness and cause further internal damage.\n\nA child's identity is formed during their early years. Identity development is considered the central task during adolescence. When children grow up understanding who they are, they are able to better define what their strengths and weaknesses are while comparing themselves to others. Comparison can have two outcomes, it can be healthy in aiding in self-improvement or it can be unhealthy and result in envy/jealousy which can develop into depression. This is why self-exploration and identity development are critical in adolescent years.\n\nIt is important to identify healthy and unhealthy envy in a child at an early age. If a child is showing signs of unhealthy envy, it is best to teach the child productive ways to handle these emotions. It is much easier to teach a child how to control their emotions while they are young rather than allowing them to develop a habit that is hard to break when they are older.\n\nThe things that drive people mad with envy changes throughout their lifetime. Studies have shown that the younger the person, the more likely they are to be envious of others. Adults under the age of 30 are more likely to experience envy compared to those 30 years and older. However, what people become envious over differs across adulthood. \n\nYounger adults, under the age of 30, have been found to envy others social status, relationships, and attractiveness. This starts to fade when a person hits there 30's, typically, at this point in life the person begins to accept who they are as an individual and compare themselves to others less often. However, they still envy others, just over different aspects in life, such as, career or salary . Studies have shown a decrease in envy as a person ages, however, envious feelings over money was the only thing that consistently increased as a person got older .\n\nAs a person ages, they begin to accept their social status. Nonetheless, envious feelings will be present throughout a persons life. It is up to the individual on whether they will let these envious feelings motivate or destroy them.\nIndividuals with narcissistic personality disorder are often envious of others or believe others are envious of them.\n\nA narcissist may secure a sense of superiority in the face of another person's ability by using contempt to minimize the other person.\n\nAristotle (in Rhetoric) defined envy (φθόνος \"phthonos\") \"as the pain caused by the good fortune of others\", while Kant defined it as \"a reluctance to see our own well-being overshadowed by another's because the standard we use to see how well off we are is not the intrinsic worth of our own well-being but how it compares with that of others\" (in Metaphysics of Morals).\n\nIn Buddhism, the term irshya is commonly translated as either envy or jealousy. \"Irshya\" is defined as a state of mind in which one is highly agitated to obtain wealth and honor for oneself, but unable to bear the excellence of others.\n\nThe term \"mudita\" (sympathetic joy) is defined as taking joy in the good fortune of others. This virtue is considered the antidote to envy and the opposite of schadenfreude.\n\nBoth in the Old and New Testament there are various descriptions of envy and events related to it, mostly with a dramatic outcome.\n\nEnvy is one of the Seven deadly sins in Roman Catholicism. In the \"Book of Genesis\" envy is said to be the motivation behind Cain murdering his brother, Abel, as Cain envied Abel because God favored Abel's sacrifice over Cain's.\n\nEnvy was regarded by Paul of Tarsus to be a sin of the flesh. Envy is among the things that comes from the heart, defiling a person. The whole body is full of darkness when the eye, the lamp of body, is bad. He who is glad at calamity will not go unpunished, said Solomon. Envy ruins the body's health, making bones rot and prohibiting the inheritance of the kingdom of God. Sometimes, as a punishment, people are left in their sins, falling prey to envy and other heavy sins.\n\nEnvy is credited as the basis of all toil and skills of people. For example, mankind will choose occupations to gain wealth, fame and pleasures to equal or exceed their neighbours. Envy is, therefore, a sin deeply ingrained in human nature. It comes into being when man lacks certain things, a circumstance that exists when God is not approached for provision or when the provision is used for one's own selfish passions and pleasures.\n\nEnvy may be caused by wealth (Isaac, envied the Philistines), by the brightness of wealth, power and beauty (Assyria kingdom envied of other kingdoms), by political and military rising (Saul eyed David from the moment he heard the women song of joy), fertility (Leah, envied of Rachel), social ascent (Joseph whom his brothers were jealous of), countless miracles and healings (the apostles envied of high priest and the Sadducees), popularity (Paul and Barnabas, envied of unfaithful Jewish from Antioch), the success of Christianization of many Thessalonians (Paul and Silas, envied of unfaithful Jews from Thessalonica), virtues and true power to heal, to make miracles and to teach people (Jesus envied of the chief priests).\n\nIn the NT, Jewish Christians are admonished to not look with evil eye at the last converts (\"Gentiles\" or Pagan Christians) to avoid therefore becoming the last ones, missing the kingdom of God. They should be happy for anyone saved, like Christ, who came to save the lost, as the shepherd seeking the lost sheep. Zacchaeus, the chief tax collector, was among the lost ones and he succeeded in bringing salvation to him and to his house.\n\nSometimes arisen out of sophistry, envy cannot coexist with true and spiritual wisdom, but with false, earthly, unspiritual, demonic wisdom.\n\nThrowing away envy is a crucial condition in our path to salvation. Envy was seen by the Apostle Paul as a real danger even within the first Christian communities. Envy should remain a sin of the past, defeated by God's teaching, which, as in the tenth commandment, forbids us from coveting our neighbour's things, woman, and servants, and urges us to rejoice with those who rejoice and weep with those who weep, as Apostle Paul said, and to love our neighbours as ourselves. Because brotherly, Christian love banishes definitively envy from our hearts.\n\n\"One who does not envy but is a compassionate friend to all ... such a devotee is very dear to Me.\" - Lord Krishna in the Bhagavad Gita, Chapter 12, Verse 15.\n\nIn Hinduism, envy is considered a disastrous emotion. Hinduism maintains that anything which causes the mind to lose balance with itself leads to misery. This concept is put forth in the epic Mahabharata, wherein Duryodhana launches the Kurukshetra war out of envy of the perceived prosperity of his cousins. He is known to have remarked:\n\n\"Father! The prosperity of the Pandavas (cousins) is burning me deeply! I cannot eat, sleep or live in the knowledge that they are better off than me!\"\n\nThus, Hinduism teaches that envy can be overcome simply by recognizing that the man or woman who is the object of one's envy is merely enjoying the fruits of their past karmic actions and that one should not allow such devious emotions to take control of their mind, lest they suffer the same fate as the antagonists of the Mahabharata.\n\nIn Islam, envy (Hassad حسد in Arabic) is an impurity of the heart and can destroy one's good deeds. One must be content with what God has willed and believe in the justice of the creator. A Muslim should not allow his envy to inflict harm upon the envied person. \n\nMuhammad said, \"Do not envy each other, do not hate each other, do not oppose each other, and do not cut relations, rather be servants of Allah as brothers. It is not permissible for a Muslim to disassociate from his brother for more than three days such that they meet and one ignores the other, and the best of them is the one who initiates the salaam.\" Sahih al-Bukhari [Eng. Trans. 8/58 no. 91], Sahih Muslim [Eng. Trans. 4/1360 no. 6205, 6210]\n\nA Muslim may wish for himself a blessing like that which someone else has, without wanting it to be taken away from the other person. This is permissible and is not called hasad. Rather, it is called ghibtah.\n\n\"There is to be no envy except in two cases: (towards) a person to whom Allah has granted wisdom, and who rules by this and teaches it to the people, and (towards) a person to whom Allah has granted wealth and property along with the power to spend it in the cause of the Truth.\" [Al-Bukhaari & Muslim]\n\nIn Judaism (in the Hebrew Bible 'jealousy', is a key feature of God's personality – He is furious in jealousy (for His own people's undivided worship). YeHoVaH is jealous for His own. \nThe God of Israel is, \"slow to anger and great in compassion\" (Exodus 34:6) but when His jealousy and anger had accumulated there was an outburst of punishment. (Exodus 34:6; Numbers 14:18; Psalm 103:8; Ps 86:15; Ps 145:8; Jonah 4:2; Nahum 1:3; Nehemiah 9:17; Joel 2:13 etc.)\nWhile jealousy is branded as a negative and unwanted emotion generally in society today and also in Christianity, which had developed out of Judaism, in the Biblical (so-called Old Testament) context it is a strong aspect of God's character and therefor not a flawed characteristic – unlike envy, which God does denounce. (Exodus 20:14; Deuteronomy 5:9 and verse18) \nWe envy people when we want what they have. We are jealous when we want to keep for ourselves what belongs exclusively to us. Therefore we see the frightening permission God gave husbands who became jealous of their wives, to make them take a curse upon themselves, in case they had slept with another man while they belonged to their husband. (Numbers 5:11 – 31)\nThis points to the intimacy and exclusivity He is interested in, from His own people. Ephraim 'committed harlotry' against YHVH and thereby defiled the nation of Israel. Therefore He withdrew Himself from them, to their detriment: \"Woe to those when I depart from them!\" (Hosea 9:12), He warns. \"They will cry to YHVH, but he will hide His face from them\". (Micah 3:4) A wounded Lover speaking. \"You paid, but were not paid; for your harlotry. Therefore, oh harlot, hear the Word of YHVH: I shall set My jealousy against you and they will deal furiously with you.\" (Ezekiel 23:25)\nYHVH showed Ezekiel how the people in Jerusalem set up 'an image that provokes jealousy'. (Ezekiel 8:11, 12, 1 Kings 14:22, 2 Chronicles 14:2)\nGod also loves like a jealous lover: He told Moses to make a breastplate for Aaron the priest, to wear when he goes into the Most Holy Place. On the breastplate he had to display the names of all the tribes of Israel, so He could see it whenever Aaron went in to work where YHVH's Presence was (Exodus 28:29). Somewhere else He says, I have your name engraved in the palm of My hand. (Isaiah 49:16)\nThe God of Israel wants with His people a marriage of faithfulness, fairness, kindness and compassion – and that they should know Him. (Hosea 2:21,22)\nHe is even jealous for the land itself, the land of Israel. (Joel 2:18)\n\"I am jealous for Jerusalem and for Zion with a great jealousy.. YHVH will yet comfort Zion and will yet choose Jerusalem.. For I will be to her a wall of fire all around; and the glory in her midst.\" (Zechariah 1:14, 17, Zechariah 2:9)\nThe Hebrew Bible says Judah provoked YHVH to jealousy with all their sins and their false gods. (1 Kings 14:22) There is a notable difference in meaning between jealousy (of something that is one's own) and envy (which is covetousness of another one's possessions). (Exodus 20: 14; Proverbs 27:4)\n\nIn English-speaking cultures, envy is often associated with the color green, as in \"green with envy\", and yellow. Yellow is the color of ambivalence and contradiction; a color associated with optimism and amusement; but also with betrayal, duplicity, and jealousy. The phrase \"green-eyed monster\" refers to an individual whose current actions appear motivated by jealousy not envy. This is based on a line from Shakespeare's \"Othello\". Shakespeare mentions it also in \"The Merchant of Venice\" when Portia states: \"How all the other passions fleet to air, as doubtful thoughts and rash embraced despair and shuddering fear and green-eyed jealousy!\"\n\nThe character Envy from Fullmetal Alchemist series is one of the seven homunculi named after the seven deadly sins.\n\nThe character of Zelena on ABC's \"Once Upon a Time\", takes on the title \"The Wicked Witch of the West\" after envy itself dyes her skin in the episode \"It's Not Easy Being Green\".\n\nIn the parable \"Garden of Statues\", a character goes mad with envy because of all the attention his sculptor neighbor is getting.\n\nIn Nelson W. Aldrich Jr.'s \"Old Money\", he states that people who suffer from a case of malicious envy are blind to what good things they already have, thinking they have nothing, causing them to feel emptiness and despair.\n\n\n\n"}
{"id": "30858332", "url": "https://en.wikipedia.org/wiki?curid=30858332", "title": "Evander Holyfield vs. Mike Tyson II", "text": "Evander Holyfield vs. Mike Tyson II\n\nEvander Holyfield vs. Mike Tyson II, billed as \"The Sound and the Fury\" and afterwards infamously referred to as \"The Bite Fight\", was a professional boxing match contested on June 28, 1997 for the WBA Heavyweight Championship. It achieved notoriety as one of the most bizarre fights in boxing history, after Tyson bit off part of Holyfield's ear. Tyson was disqualified from the match and lost his boxing license, though it was later reinstated. \n\nThe fight took place at the MGM Grand Garden Arena in Las Vegas, Nevada. The referee officiating the fight was Mills Lane, who was brought in as a late replacement when Tyson's camp protested the original selection of Mitch Halpern (who officiated the first fight) as the referee.\n\nTyson and Holyfield fought seven months earlier in Las Vegas. Tyson was making his first defense of the WBA championship he had won from Bruce Seldon in a first round knockout. Holyfield, despite being a former champion, was a significant underdog entering the match as he had been rather lackluster in several fights since he returned to fighting in 1995 after a brief retirement. However, Holyfield surprised Tyson by controlling the contest and knocked him down in the sixth round. Halpern stopped the fight in the eleventh round, giving Holyfield an upset victory.\n\nThe fight began with Holyfield dominating Tyson. Holyfield won the first three rounds. At 2:19 of the first round, an overhand right from Holyfield stunned Tyson, but Tyson fought back immediately pushing Holyfield backwards. At 32 seconds into the second round, Holyfield ducked under a right from Tyson. In doing so, he head-butted Tyson, opening a large cut over the latter's right eye (although trainer Ritchie Giachetti believed the cut happened in the first round). Tyson had repeatedly complained about head-butting in the first bout between the two fighters. Upon reviewing replays, referee Mills Lane stated the headbutts were unintentional and nonpunishable.\n\nAs the third round was about to begin, Tyson came out of his corner without his mouthpiece. Lane ordered Tyson back to his corner to insert it. Tyson inserted his mouthpiece, got back into position and the match resumed. Tyson began the third round with a furious attack. With forty seconds remaining in the round, Holyfield got Tyson in a clinch, and Tyson rolled his head above Holyfield's shoulder and bit Holyfield on his right ear. The bite avulsed a one-inch piece of cartilage from the top of the ear, and Tyson spit out the piece of ear onto the ring floor. As Holyfield shrieked in pain and jumped in circles, he managed to push Tyson away, at which point Lane called for a time-out. As Holyfield turned to walk to his corner, Tyson shoved him from behind. Lane sent Tyson to a neutral corner as an enraged Holyfield gestured for Mills Lane to look at his bitten ear, which was rapidly bleeding.\n\nThe fight was delayed for several minutes as Lane debated what to do. Lane's original decision was to immediately disqualify Tyson, but after the ringside doctor determined that Holyfield was able to continue despite the massive bite, Lane announced he would be deducting two points from Tyson and the fight would continue. Bobby Czyz, who was calling the fight with Steve Albert for Showtime, said, \"I wonder how this would have played in Mitch Halpern's eyes,\" and Albert told Czyz, \"That's a thought, Bobby.\" As Lane explained the decision to Tyson and his cornermen, Tyson asserted that the injury to Holyfield's ear was the result of a punch. \"Bullshit,\" Lane retorted. The fight was resumed.\n\nDuring another clinch, Tyson bit Holyfield's left ear. Holyfield threw his hands around to get out of the clinch and jumped back. Tyson's second bite just scarred Holyfield's ear. Lane did not stop the fight this time, so the two men continued fighting until time expired. The men walked back to their respective corners, and when the second bite was discovered, the fight was stopped.\n\nAfter the fight was stopped, Tyson went on a rampage at Holyfield and his trainer Brooks while they were still in their corner. Mills Lane told Tyson's corner that he was disqualifying Tyson for biting Holyfield. To protect Holyfield, security surrounded him in his corner and Tyson was taken back to his corner by security. Mills Lane was interviewed, and said he knew from experience that the bites were intentional. He had told Tyson not to bite anymore, and said Tyson asked to be disqualified by disobeying that order. Holyfield left the ring seconds after the interview, which gave the fans and audience the hint that the fight was over. Holyfield told the press afterward that Tyson bit him because he knew he was going to get knocked out and he chose to lose in a disqualification instead.\n\nReporters then interviewed Mike Tyson's instructor, who was upset about Lane's decision. The instructor said, \"They will have to explain that. I do not agree with it but it is what it is...all I know is Mike Tyson has a cut in his eye.\"\n\nTwenty-five minutes after the brawl ended, announcer Jimmy Lennon Jr. read the decision: \"Ladies and gentlemen, this bout has been stopped at the end of round number three. The referee in charge, Mills Lane, disqualifies Mike Tyson for biting Evander Holyfield in both ears, the winner by way of disqualification and still the WBA Champion of the world, Evander 'The Real Deal' Holyfield!\" As a result Holyfield remained the WBA Heavyweight champion.\n\nTyson said it was retaliation for the headbutts.\n\nLater, during post-match interviews, Tyson was walking back to his locker room when a fan from the stadium tossed a bottle of water in his direction. Tyson, his instructor and a pain manager climbed over a temporary railing and up into the stands, made obscene gestures to the crowd, and made their way up the side of a stairway. Tyson had to be restrained as he was led off. When interviewed about his championship and the incident with Tyson, Holyfield said he already forgave Tyson for biting him since he has 100% faith in God and Jesus.\n\nNonetheless, Tyson was sentenced with a permanent suspension from boxing and his license cancelled indefinitely. Tyson was also fined $3,000,000 and had to serve a period of community service. After a year and an appeal in court, Tyson's license was reinstated. When Tyson and Holyfield retired from boxing they befriended each other and are now close friends today.\n\nThe fight generated a total revenue of $180,000,000 ($), from live gate, pay-per-view, closed-circuit telecasts, foreign television rights, and casino profits.\n\nAs a result of biting Holyfield on both ears and other behavior, Tyson's boxing license was revoked by the Nevada State Athletic Commission and he was fined $3 million plus legal costs. The revocation was not permanent; a little more than a year later on October 18, 1998, the commission voted 4–1 to restore Tyson's boxing license.\n\nIn the 2008 documentary \"Tyson\", the boxer claimed he did it as retaliation for the headbutting.\n\nOn October 16, 2009 on \"The Oprah Winfrey Show\", Tyson apologized to Holyfield. Holyfield accepted his apology and forgave Tyson.\n\nA book entitled \"The Bite Fight\" was made in 2013 by George Willis, illustrating the lives of Tyson and Holyfield before, during, and after the fight.\n"}
{"id": "39095", "url": "https://en.wikipedia.org/wiki?curid=39095", "title": "Extension (semantics)", "text": "Extension (semantics)\n\nIn any of several studies that treat the use of signs—for example, in linguistics, logic, mathematics, semantics, and semiotics—the extension of a concept, idea, or sign consists of the things to which it applies, in contrast with its comprehension or intension, which consists very roughly of the ideas, properties, or corresponding signs that are implied or suggested by the concept in question.\n\nIn philosophical semantics or the philosophy of language, the 'extension' of a concept or expression is the set of things it extends to, or applies to, if it is the sort of concept or expression that a single object by itself can satisfy. Concepts and expressions of this sort are monadic or \"one-place\" concepts and expressions.\n\nSo the extension of the word \"dog\" is the set of all (past, present and future) dogs in the world: the set includes Fido, Rover, Lassie, Rex, and so on. The extension of the phrase \"Wikipedia reader\" includes each person who has ever read Wikipedia, including \"you\".\n\nThe extension of a whole statement, as opposed to a word or phrase, is defined (since Frege 1892) as its truth value. So the extension of \"Lassie is famous\" is the logical value 'true', since Lassie is famous.\n\nSome concepts and expressions are such that they don't apply to objects individually, but rather serve to relate objects to objects. For example, the words \"before\" and \"after\" do not apply to objects individually—it makes no sense to say \"Jim is before\" or \"Jim is after\"—but to one thing in relation to another, as in \"The wedding is before the reception\" and \"The reception is after the wedding\". Such \"relational\" or \"polyadic\" (\"many-place\") concepts and expressions have, for their extension, the set of all sequences of objects that satisfy the concept or expression in question. So the extension of \"before\" is the set of all (ordered) pairs of objects such that the first one is before the second one.\n\nIn mathematics, the 'extension' of a mathematical concept is the set that is specified by that concept.\n\nFor example, the extension of a function is a set of ordered pairs that pair up the arguments and values of the function; in other words, the function's graph. The extension of an object in abstract algebra, such as a group, is the underlying set of the object. The extension of a set is the set itself. That a set can capture the notion of the extension of anything is the idea behind the axiom of extensionality in axiomatic set theory.\n\nThis kind of extension is used so constantly in contemporary mathematics based on set theory that it can be called an implicit assumption.\n\nIn computer science, some database textbooks use the term 'intension' to refer to the schema of a database, and 'extension' to refer to particular instances of a database.\n\nThere is an ongoing controversy in metaphysics about whether or not there are, in addition to actual, existing things, non-actual or nonexistent things. If there are—if, for instance, there are possible but non-actual dogs (dogs of some non-actual but possible species, perhaps) or nonexistent beings (like Sherlock Holmes, perhaps)—then these things might also figure in the extensions of various concepts and expressions. If not, only existing, actual things can be in the extension of a concept or expression. Note that \"actual\" may not mean the same as \"existing\". Perhaps there exist things that are merely possible, but not actual. (Maybe they exist in other universes, and these universes are other \"possible worlds\"—possible alternatives to the actual world.) Perhaps some actual things are nonexistent. (Sherlock Holmes seems to be an \"actual\" example of a fictional character; one might think there are many other characters Arthur Conan Doyle \"might\" have invented, though he actually invented Holmes.)\n\nA similar problem arises for objects that no longer exist. The extension of the term \"Socrates\", for example, seems to be a (currently) non-existent object. Free logic is one attempt to avoid some of these problems.\n\nSome fundamental formulations in the field of general semantics rely heavily on a valuation of extension over intension. See for example extension, and the extensional devices.\n\n\n"}
{"id": "36894282", "url": "https://en.wikipedia.org/wiki?curid=36894282", "title": "Eye poke", "text": "Eye poke\n\nAn eye poke, eye jab, eye stab, eye strike or poke in the eye is a strike at the eye or eyes of a human or animal. It is typically made with the fingers which may either be forked to jab both eyes or held together, like a bird's beak, to strike with force and protect the fingers from damage. The attack became better known among the public due to its use in comedy; the idea of using it to entertain was likely invented by the vaudeville duo of Joe Weber and Lew Fields.\n\nIn sporting events, a losing fighter will sometimes break the rules, and poke someone in the eye leading to the fight be ended as \"no decision,\" thus preventing them from taking a loss. Some events try to prevent eye poking by having fighters wear gloves with webbing over the fingers.\n\nIn a street fight situation, when one's opponent may be trying to inflict serious harm, martial arts expert Kelly McCann advises that the eyes should be a \"persistent primary target\". An eye poke needs little power to be effective, and it can stop even highly determined attackers. If the hand is kept at an approximately 45° angle to the opponent's face during the strike, there is less risk of hurting ones fingers, and even if they do not connect with the eye, the palm can impact on the opponent's face. \n\nThe eye poke was a signature move in the slapstick antics of the comedy and vaudeville act The Three Stooges, who mastered the technique. In reality, the Stooges poked each other on the eyebrows to avoid actual injury. The form of attack was well known among children who watched the show.\n\n\n"}
{"id": "16006394", "url": "https://en.wikipedia.org/wiki?curid=16006394", "title": "Food vs. fuel", "text": "Food vs. fuel\n\nFood versus fuel is the dilemma regarding the risk of diverting farmland or crops for biofuels production to the detriment of the food supply. The biofuel and food price debate involves wide-ranging views, and is a long-standing, controversial one in the literature. There is disagreement about the significance of the issue, what is causing it, and what can or should be done to remedy the situation. This complexity and uncertainty is due to the large number of impacts and feedback loops that can positively or negatively affect the price system. Moreover, the relative strengths of these positive and negative impacts vary in the short and long terms, and involve delayed effects. The academic side of the debate is also blurred by the use of different economic models and competing forms of statistical analysis.\n\nBiofuel production has increased in recent years. Some commodities like maize (corn), sugar cane or vegetable oil can be used either as food, feed, or to make biofuels. For example, since 2006, a portion of land that was also formerly used to grow other crops in the United States is now used to grow corn for biofuels, and a larger share of corn is destined to ethanol production, reaching 25% in 2007. Second generation biofuels could potentially combine farming for food and fuel and moreover, electricity could be generated simultaneously, which could be beneficial for developing countries and rural areas in developed countries.\nWith global demand for biofuels on the increase due to the oil price increases taking place since 2003 and the desire to reduce oil dependency as well as reduce GHG emissions from transportation, there is also fear of the potential destruction of habitats by being converted into farmland. Environmental groups have raised concerns about this trade-off for several years, but the debate reached a global scale due to the 2007–2008 world food price crisis. On the other hand, several studies do show that biofuel production can be significantly increased without increased acreage. Therefore, stating that the crisis in hand relies on the food scarcity.\n\nBiofuels are not a new phenomenon. Before the industrialisation, horses were the primary (and humans probably the secondary) source of power for transportation and physical work, requiring food. The growing of crops for horses (typically oat) for carrying out physical work is of course comparable to the growing of crops for biofuels for engines, albeit on a smaller scale, because production since then has increased.\n\nBrazil has been considered to have the world's first sustainable biofuels economy and its government claims Brazil's sugar cane based ethanol industry has not contributed to the 2008 food crisis. A World Bank policy research working paper released in July 2008 concluded that \"...large increases in biofuels production in the United States and Europe are the main reason behind the steep rise in global food prices\", and also stated that \"Brazil's sugar-based ethanol did not push food prices appreciably higher\". However, a 2010 study also by the World Bank concluded that their previous study may have overestimated the contribution of biofuel production, as \"the effect of biofuels on food prices has not been as large as originally thought, but that the use of commodities by financial investors (the so-called \"financialisation of commodities\") may have been partly responsible for the 2007/08 spike.\" A 2008 independent study by OECD also found that the impact of biofuels on food prices are much smaller.\n\nFrom 1974 to 2005 real food prices (adjusted for inflation) dropped by 75%. Food commodity prices were relatively stable after reaching lows in 2000 and 2001. Therefore, recent rapid food price increases are considered extraordinary. A World Bank policy research working paper published in July 2008 found that the increase in food commodities prices was led by grains, with sharp price increases in 2005 despite record crops worldwide. From January 2005 until June 2008, maize prices almost tripled, wheat increased 127 percent, and rice rose 170 percent. The increase in grain prices was followed by increases in fats and oil prices in mid-2006. On the other hand, the study found that sugar cane production has increased rapidly, and it was large enough to keep sugar price increases small except for 2005 and early 2006. The paper concluded that biofuels produced from grains have raised food prices in combination with other related factors between 70 to 75 percent, but ethanol produced from sugar cane has not contributed significantly to the recent increase in food commodities prices.\n\nAn economic assessment report published by the OECD in July 2008 found that \"...the impact of current biofuel policies on world crop prices, largely through increased demand for cereals and vegetable oils, is significant but should not be overestimated. Current biofuel support measures alone are estimated to increase average wheat prices by about 5 percent, maize by around 7 percent and vegetable oil by about 19 percent over the next 10 years.\"\n\nCorn is used to make ethanol and prices went up by a factor of three in less than 3 years (measured in US dollars). Reports in 2007 linked stories as diverse as food riots in Mexico due to rising prices of corn for tortillas, and reduced profits at Heineken the large international brewer, to the increasing use of corn (maize) grown in the US Midwest for ethanol production. (In the case of beer, the barley area was cut in order to increase corn production. Barley is not currently used to produce ethanol.) Wheat is up by almost a factor of 3 in 3 years, while soybeans are up by a factor of 2 in 2 years (both measured in US dollars).\n\nAs corn is commonly used as feed for livestock, higher corn prices lead to higher prices in animal source foods. Vegetable oil is used to make biodiesel and has about doubled in price in the last couple years. The price is roughly tracking crude oil prices. The 2007–2008 world food price crisis is blamed partly on the increased demand for biofuels. During the same period rice prices went up by a factor of 3 even though rice is not directly used in biofuels.\n\nThe USDA expects the 2008/2009 wheat season to be a record crop and 8% higher than the previous year. They also expect rice to have a record crop. Wheat prices have dropped from a high over $12/bushel in May 2008 to under $8/bushel in May. Rice has also dropped from its highs.\n\nAccording to a 2008 report from the World Bank the production of biofuel pushed food prices up. These conclusions were supported by the Union of Concerned Scientists in their September 2008 newsletter in which they remarked that the World Bank analysis \"contradicts U.S. Secretary of Agriculture Ed Schaffer's assertion that biofuels account for only a small percentage of rising food prices\".\n\nAccording to the October Consumer Price Index released Nov. 19, 2008, food prices continued to rise in October 2008 and were 6.3 percent higher than October 2007. Since July 2008 fuel costs dropped by nearly 60 percent.\n\nThe demand for ethanol fuel produced from field corn was spurred in the U.S. by the discovery that methyl tertiary butyl ether (MTBE) was contaminating groundwater. MTBE use as an oxygenate additive was widespread due to mandates of the Clean Air Act amendments of 1992 to reduce carbon monoxide emissions. As a result, by 2006 MTBE use in gasoline was banned in almost 20 states. There was also concern that widespread and costly litigation might be taken against the U.S. gasoline suppliers, and a 2005 decision refusing legal protection for MTBE, opened a new market for ethanol fuel, the primary substitute for MTBE. At a time when corn prices were around US$2 a bushel, corn growers recognized the potential of this new market and delivered accordingly. This demand shift took place at a time when oil prices were already significantly rising.\n\nThat food prices went up at the same time fuel prices went up is not surprising and should not be entirely blamed on biofuels. Energy costs are a significant cost for fertilizer, farming, and food distribution. Also, China and other countries have had significant increases in their imports as their economies have grown. Sugar is one of the main feedstocks for ethanol and prices are down from 2 years ago. Part of the food price increase for international food commodities measured in US dollars is due to the dollar being devalued. Protectionism is also an important contributor to price increases. 36% of world grain goes as fodder to feed animals, rather than people.\n\nOver long time periods population growth and climate change could cause food prices to go up. However, these factors have been around for many years and food prices have jumped up in the last 3 years, so their contribution to the current problem is minimal.\n\nFrance, Germany, the United Kingdom and the United States governments have supported biofuels with tax breaks, mandated use, and subsidies. These policies have the unintended consequence of diverting resources from food production and leading to surging food prices and the potential destruction of natural habitats.\n\nFuel for agricultural use often does not have fuel taxes (farmers get duty-free petrol or diesel fuel). Biofuels may have subsidies and low/no retail fuel taxes. Biofuels compete with retail gasoline and diesel prices which have substantial taxes included. The net result is that it is possible for a farmer to use more than a gallon of fuel to make a gallon of biofuel and still make a profit. There have been thousands of scholarly papers analyzing how much energy goes into making ethanol from corn and how that compares to the energy in the ethanol.\n\nA World Bank policy research working paper concluded that food prices have risen by 35 to 40 percent between 2002 and 2008, of which 70 to 75 percent is attributable to biofuels. The \"month-by-month\" five-year analysis disputes that increases in global grain consumption and droughts were responsible for significant price increases, reporting that this had only a marginal impact. Instead the report argues that the EU and US drive for biofuels has had by far the biggest impact on food supply and prices, as increased production of biofuels in the US and EU were supported by subsidies and tariffs on imports, and considers that without these policies, price increases would have been smaller. This research also concluded that Brazil's sugar cane based ethanol has not raised sugar prices significantly, and recommends removing tariffs on ethanol imports by both the US and EU, to allow more efficient producers such as Brazil and other developing countries, including many African countries, to produce ethanol profitably for export to meet the mandates in the EU and the US.\n\nAn economic assessment published by the OECD in July 2008 agrees with the World Bank report recommendations regarding the negative effects of subsidies and import tariffs, but found that the estimated impact of biofuels on food prices are much smaller. The OECD study found that trade restrictions, mainly through import tariffs, protect the domestic industry from foreign competitors but impose a cost burden on domestic biofuel users and limits alternative suppliers. The report is also critical of limited reduction of GHG emissions achieved from biofuels based on feedstocks used in Europe and North America, finding that the current biofuel support policies would reduce greenhouse gas emissions from transport fuel by no more than 0.8% by 2015, while Brazilian ethanol from sugar cane reduces greenhouse gas emissions by at least 80% compared to fossil fuels. The assessment calls for the need for more open markets in biofuels and feedstocks in order to improve efficiency and lower costs.\n\nOil price increases since 2003 resulted in increased demand for biofuels. Transforming vegetable oil into biodiesel is not very hard or costly so there is a profitable arbitrage situation if vegetable oil is much cheaper than diesel. Diesel is also made from crude oil, so vegetable oil prices are partially linked to crude oil prices. Farmers can switch to growing vegetable oil crops if those are more profitable than food crops. So all food prices are linked to vegetable oil prices, and in turn to crude oil prices. A World Bank study concluded that oil prices and a weak dollar explain 25–30% of total price rise between January 2002 until June 2008.\n\nDemand for oil is outstripping the supply of oil and oil depletion is expected to cause crude oil prices to go up over the next 50 years. Record oil prices are inflating food prices worldwide, including those crops that have no relation to biofuels, such as rice and fish.\n\nIn Germany and Canada it is now much cheaper to heat a house by burning grain than by using fuel derived from crude oil. With oil at $120/barrel a savings of a factor of 3 on heating costs is possible. When crude oil was at $25/barrel there was no economic incentive to switch to a grain fed heater.\n\nFrom 1971 to 1973, around the time of the 1973 oil crisis, corn and wheat prices went up by a factor of 3. There was no significant biofuel usage at that time.\n\nSome argue that the US government policy of encouraging ethanol from corn is the main cause for food price increases. US Federal government ethanol subsidies total $7 billion per year, or $1.90 per gallon. Ethanol provides only 55% as much energy as gasoline per gallon, realizing about a $3.45 per gallon gasoline trade off. Corn is used to feed chickens, cows, and pigs, so higher corn prices lead to higher prices for chicken, beef, pork, milk, cheese, etc.\n\nU.S. Senators introduced the \"BioFuels Security Act\" in 2006. \"It's time for Congress to realize what farmers in America's heartland have known all along - that we have the capacity and ingenuity to decrease our dependence on foreign oil by growing our own fuel,\" said U.S. Senator for Illinois Barack Obama.\n\nTwo-thirds of U.S. oil consumption is due to the transportation sector. The Energy Independence and Security Act of 2007 has a significant impact on U.S. Energy Policy. With the high profitability of growing corn, more and more farmers switch to growing corn until the profitability of other crops goes up to match that of corn. So the ethanol/corn subsidies drive up the prices of other farm crops.\n\nThe US - an important export country for food stocks - will convert 18% of its grain output to ethanol in 2008. Across the US, 25% of the whole corn crop went to ethanol in 2007. The percentage of corn going to biofuel is expected to go up.\n\nSince 2004 a US subsidy has been paid to companies that blend biofuel and regular fuel. The European biofuel subsidy is paid at the point of sale. Companies import biofuel to the US, blend 1% or even 0.1% regular fuel, and then ship the blended fuel to Europe, where it can get a second subsidy. These blends are called B99 or B99.9 fuel. The practice is called \"splash and dash\". The imported fuel may even come from Europe to the US, get 0.1% regular fuel, and then go back to Europe. For B99.9 fuel the US blender gets a subsidy of $0.999 per gallon. The European biodiesel producers have urged the EU to impose punitive duties on these subsidized imports. In 2007, US lawmakers were also looking at closing this loophole.\n\nThe prospects for the use of biofuels could change in a relatively dramatic way in 2014. Petroleum trade groups petitioned the EPA in August 2013 to take into consideration a reduction of renewable biofuel content in transportation fuels. On November 15, 2013 the United States EPA announced a review of the proportion of ethanol that should be required by regulation. The standards established by the Energy Independence and Security Act of 2007 could be modified significantly. The announcement allows sixty days for the submission of commentary about the proposal. Journalist George Monbiot has argued for a 5-year freeze on biofuels while their impact on poor communities and the environment is assessed. \n\nA 2007 UN report on biofuel also raises issues regarding food security and biofuel production. Jean Ziegler, then UN Special Rapporteur on food, concluded that while the argument for biofuels in terms of energy efficiency and climate change are legitimate, the effects for the world's hungry of transforming wheat and maize crops into biofuel are \"absolutely catastrophic\", and terms such use of arable land a \"crime against humanity\". Ziegler also calls for a 5-year moratorium on biofuel production. Ziegler's proposal for a five-year ban was rejected by the U.N. Secretary Ban Ki-moon, who called for a comprehensive review of the policies on biofuels, and said that \"just criticising biofuel may not be a good solution\".\n\nFood surpluses exist in many developed countries. For example, the UK wheat surplus was around 2 million tonnes in 2005. This surplus alone could produce sufficient bioethanol to replace around 2.5% of the UK's petroleum consumption, without requiring any increase in wheat cultivation or reduction in food supply or exports. However, above a few percent, there would be direct competition between first generation biofuel production and food production. This is one reason why many view second generation biofuels as increasingly important.\n\nThere are different types of biofuels and different feedstocks for them, and it has been proposed that only non-food crops be used for biofuel. This avoids direct competition for commodities like corn and edible vegetable oil. However, as long as farmers are able to derive a greater profit by switching to biofuels, they will. The law of supply and demand predicts that if fewer farmers are producing food the price of food will rise.\n\nSecond generation biofuels use lignocellulosic raw material such as forest residues (sometimes referred to as brown waste and black liquor from Kraft process or sulfite process pulp mills). Third generation biofuels (biofuel from algae) use non-edible raw materials sources that can be used for biodiesel and bioethanol. \n\nIt has long been recognized that the huge supply of agricultural cellulose, the lignocellulosic material commonly referred to as \"Nature's polymer\", would be an ideal source of material for biofuels and many other products. Composed of lignin and monomer sugars such as glucose, fructose, arabinose, galactose, and xylose, these constituents are very valuable in their own right. To this point in history, there are some methods commonly used to coax \"recalcitrant\" cellulose to separate or hydrolyse into its lignin and sugar parts, treatment with; steam explosion, supercritical water, enzymes, acids and alkalines. All these methods involve heat or chemicals, are expensive, have lower conversion rates and produce waste materials. In recent years the rise of \"mechanochemistry\" has resulted in the use of ball mills and other mill designs to reduce cellulose to a fine powder in the presence of a catalyst, a common bentonite or kaolinite clay, that will hydrolyse the cellulose quickly and with low energy input into pure sugar and lignin. Still currently only in pilot stage, this promising technology offers the possibility that any agricultural economy might be able to get rid of its requirement to refine oil for transportation fuels. This would be a major improvement in carbon neutral energy sources and allow the continued use of internal combustion engines on a large scale.\n\nSoybean oil, which only represents half of the domestic raw materials available for biodiesel production in the United States, is one of many raw materials that can be used to produce biodiesel.\n\nNon-food crops like Camelina, Jatropha, seashore mallow and mustard, used for biodiesel, can thrive on marginal agricultural land where many trees and crops won't grow, or would produce only slow growth yields. Camelina is virtually 100 percent efficient. It can be harvested and crushed for oil and the remaining parts can be used to produce high quality omega-3 rich animal feed, fiberboard, and glycerin. Camelina does not take away from land currently being utilized for food production. Most camelina acres are grown in areas that were previously not utilized for farming. For example, areas that receive limited rainfall that can not sustain corn or soybeans without the addition of irrigation can grow camelina and add to their profitability.\n\nJatropha cultivation provides benefits for local communities:\n\nCultivation and fruit picking by hand is labour-intensive and needs around one person per hectare. In parts of rural India and Africa this provides much-needed jobs - about 200,000 people worldwide now find employment through jatropha. Moreover, villagers often find that they can grow other crops in the shade of the trees. Their communities will avoid importing expensive diesel and there will be some for export too.\nNBB’s Feedstock Development program is addressing production of arid variety crops, algae, waste greases, and other feedstocks on the horizon to expand available material for biodiesel in a sustainable manner.\n\nCellulosic ethanol is a type of biofuel produced from lignocellulose, a material that comprises much of the mass of plants. Corn stover, switchgrass, miscanthus and woodchip are some of the more popular non-edible cellulosic materials for ethanol production. Commercial investment in such second-generation biofuels began in 2006/2007, and much of this investment went beyond pilot-scale plants. Cellulosic ethanol commercialization is moving forward rapidly. The world’s first commercial wood-to-ethanol plant began operation in Japan in 2007, with a capacity of 1.4 million liters/year. The first wood-to-ethanol plant in the United States is planned for 2008 with an initial output of 75 million liters/year.\n\nOther second generation biofuels may be commercialized in the future and compete less with food. Synthetic fuel can be made from coal or biomass and may be commercialized soon.\n\nProtein rich feed for cattle/fish/poultry can be produced from biogas/natural gas which is presently used as fuel source. Cultivation of \"Methylococcus capsulatus\" bacteria culture by consuming natural gas produces high protein rich feed with tiny land and water foot print. The carbon dioxide gas produced as by product from these plants can also be put to use in cheaper production of algae oil or spirulina from algaculture which can displace the prime position of crude oil in near future. With these proven technologies, abundant natural gas/ biogas availability can impart full global food security by producing highly nutrient food products with out any water pollution or green house gas (GHG) emissions.\n\nBiofuels can also be produced from the waste byproducts of food-based agriculture (such as citrus peels or used vegetable oil) to manufacture an environmentally sustainable fuel supply, and reduce waste disposal cost.\n\nA growing percentage of U.S. biodiesel production is made from waste vegetable oil (recycled restaurant oils) and greases.\n\nCollocation of a waste generator with a waste-to-ethanol plant can reduce the waste producer's operating cost, while creating a more-profitable ethanol production business. This innovative collocation concept is sometimes called holistic systems engineering. Collocation disposal elimination may be one of the few cost-effective, environmentally sound, biofuel strategies, but its scalability is limited by availability of appropriate waste generation sources. For example, millions of tons of wet Florida-and-California citrus peels cannot supply billions of gallons of biofuels. Due to the higher cost of transporting ethanol, it is a local partial solution, at best.\n\nSome people have claimed that ending subsidies and tariffs would enable sustainable development of a global biofuels market. Taxing biofuel imports while letting petroleum in duty-free does not fit with the goal of encouraging biofuels. Ending mandates, subsidies, and tariffs would end the distortions that current policy is causing. The US ethanol tariff and some US ethanol subsidies are currently set to expire over the next couple years. The EU is rethinking their biofuels directive due to environmental and social concerns. On January 18, 2008 the UK House of Commons Environmental Audit Committee raised similar concerns, and called for a moratorium on biofuel targets. Germany ended their subsidy of biodiesel on Jan 1 2008 and started taxing it.\n\nTo avoid overproduction and to prop up farmgate prices for agricultural commodities, the EU has for a long time have had farm subsidy programs to encourage farmers not to produce and leave productive acres fallow. The 2008 crisis prompted proposals to bring some of the reserve farmland back into use, and the used area increased actually with 0.5% but today these areas are once again out of use. According to Eurostat, 18 million hectares has been abandoned since 1990, 7,4 millions hectares are currently set aside, and the EU has recently decided to set aside another 5–7% in so called Ecological Focus Areas, corresponding to 10–12 million hectares. In spite of this reduction of used land, the EU is a net exporter of e.g. wheat.\n\nThe American Bakers Association has proposed reducing the amount of farmland held in the US Conservation Reserve Program. Currently the US has in the program.\n\nIn Europe about 8% of the farmland is in set aside programs. Farmers have proposed freeing up all of this for farming. Two-thirds of the farmers who were on these programs in the UK are not renewing when their term expires.\n\nSecond generation biofuels are now being produced from the cellulose in dedicated energy crops (such as perennial grasses), forestry materials, the co-products from food production, and domestic vegetable waste. Advances in the conversion processes will almost certainly improve the sustainability of biofuels, through better efficiencies and reduced environmental impact of producing biofuels, from both existing food crops and from cellulosic sources.\n\nLord Ron Oxburgh suggests that responsible production of biofuels has several advantages:\n\nProduced responsibly they are a sustainable energy source that need not divert any land from growing food nor damage the environment; they can also help solve the problems of the waste generated by Western society; and they can create jobs for the poor where previously were none. Produced irresponsibly, they at best offer no climate benefit and, at worst, have detrimental social and environmental consequences. In other words, biofuels are pretty much like any other product.\nFar from creating food shortages, responsible production and distribution of biofuels represents the best opportunity for sustainable economic prospects in Africa, Latin America and impoverished Asia. Biofuels offer the prospect of real market competition and oil price moderation. Crude oil would be trading 15 per cent higher and gasoline would be as much as 25 per cent more expensive, if it were not for biofuels. A healthy supply of alternative energy sources will help to combat gasoline price spikes.\n\nAn additional policy option is to continue the current trends of government incentive for these types of crops to further evaluate the effects on food prices over a longer period of time due to the relatively recent onset of the biofuel production industry. Additionally, by virtue of the newness of the industry we can assume that like other startup industries techniques and alternatives will be cultivated quickly if there is sufficient demand for the alternative fuels and biofuels. What could result from the shock to food prices is a very quick move toward some of the non-food biofuels as are listed above amongst the other policy alternatives.\n\nDemand for fuel in rich countries is now competing against demand for food in poor countries. The increase in world grain consumption in 2006 happened due to the increase in consumption for fuel, not human consumption. The grain required to fill a fuel tank with ethanol will feed one person for a year.\n\nSeveral factors combine to make recent grain and oilseed price increases impact poor countries more:\n\n\nThe impact is not all negative. The Food and Agriculture Organization (FAO) recognizes the potential opportunities that the growing biofuel market offers to small farmers and aquaculturers around the world and has recommended small-scale financing to help farmers in poor countries produce local biofuel.\n\nOn the other hand, poor countries that do substantial farming have increased profits due to biofuels. If vegetable oil prices double, the profit margin could more than double. In the past rich countries have been dumping subsidized grains at below cost prices into poor countries and hurting the local farming industries. With biofuels using grains the rich countries no longer have grain surpluses to get rid of. Farming in poor countries is seeing healthier profit margins and expanding.\n\nInterviews with local farmers in southern Ecuador provide strong anecdotal evidence that the high price of corn is encouraging the burning of tropical forests in order to grow more. The destruction of tropical forests now account for 20% of all greenhouse gas emmisons.\n\nUS government subsidies for making ethanol from corn have been attacked as the main cause of the food vs fuel problem. To defend themselves, the National Corn Growers Association has published their views on this issue. They consider the \"food vs fuel\" argument to be a fallacy that is \"fraught with misguided logic, hyperbole and scare tactics.\"\n\nClaims made by the NCGA include:\n\n\nSince reaching record high prices in June 2008, corn prices fell 50% by October 2008, declining sharply together with other commodities, including oil. According to a Reuters article, \"Analysts, including some in the ethanol sector, say ethanol demand adds about 75 cents to $1.00 per bushel to the price of corn, as a rule of thumb. Other analysts say it adds around 20 percent, or just under 80 cents per bushel at current prices. Those estimates hint that $4 per bushel corn might be priced at only $3 without demand for ethanol fuel.\". These industry sources consider that a speculative bubble in the commodity markets holding positions in corn futures was the main driver behind the observed hike in corn prices affecting food supply.\n\nThe United States and Brazil lead the industrial world in global ethanol production, with Brazil as the world's largest exporter and biofuel industry leader. In 2006 the U.S. produced 18.4 billion liters (4.86 billion gallons), closely followed by Brazil with 16.3 billion liters (4.3 billion gallons), producing together 70% of the world's ethanol market and nearly 90% of ethanol used as fuel. These countries are followed by China with 7.5%, and India with 3.7% of the global market share.\n\nSince 2007, the concerns, criticisms and controversy surrounding the food vs biofuels issue has reached the international system, mainly heads of states, and inter-governmental organizations (IGOs), such as the United Nations and several of its agencies, particularly the Food and Agriculture Organization (FAO) and the World Food Programme (WFP); the International Monetary Fund; the World Bank; and agencies within the European Union.\n\nIn March 2007, \"ethanol diplomacy\" was the focus of President George W. Bush's Latin American tour, in which he and Brazil's president, Luiz Inácio Lula da Silva, were seeking to promote the production and use of sugar cane based ethanol throughout Latin America and the Caribbean. The two countries also agreed to share technology and set international standards for biofuels. The Brazilian sugar cane technology transfer will permit various Central American countries, such as Honduras, Nicaragua, Costa Rica and Panama, several Caribbean countries, and various Andean Countries tariff-free trade with the U.S. thanks to existing concessionary trade agreements. Even though the U.S. imposes a USD 0.54 tariff on every gallon of imported ethanol, the Caribbean nations and countries in the Central American Free Trade Agreement are exempt from such duties if they produce ethanol from crops grown in their own countries. The expectation is that using Brazilian technology for refining sugar cane based ethanol, such countries could become exporters to the United States in the short-term. In August 2007, Brazil's President toured Mexico and several countries in Central America and the Caribbean to promote Brazilian ethanol technology.\n\nThis alliance between the U.S. and Brazil generated some negative reactions. While Bush was in São Paulo as part of the 2007 Latin American tour, Venezuela's President Hugo Chavez, from Buenos Aires, dismissed the ethanol plan as \"a crazy thing\" and accused the U.S. of trying \"to substitute the production of foodstuffs for animals and human beings with the production of foodstuffs for vehicles, to sustain the American way of life.\" Chavez' complaints were quickly followed by then Cuban President Fidel Castro, who wrote that \"you will see how many people among the hungry masses of our planet will no longer consume corn.\" \"Or even worse,\" he continued, \"by offering financing to poor countries to produce ethanol from corn or any other kind of food, no tree will be left to defend humanity from climate change.\"' Daniel Ortega, Nicaragua's President, and one of the preferential recipients of Brazil technical aid, said that \"we reject the gibberish of those who applaud Bush's totally absurd proposal, which attacks the food security rights of Latin Americans and Africans, who are major corn consumers\", however, he voiced support for sugar cane based ethanol during Lula's visit to Nicaragua.\n\nAs a result of the international community's concerns regarding the steep increase in food prices, on April 14, 2008, Jean Ziegler, the United Nations Special Rapporteur on the Right to Food, at the Thirtieth Regional Conference of the Food and Agriculture Organization (FAO) in Brasília, called biofuels a \"crime against humanity\", a claim he had previously made in October 2007, when he called for a 5-year ban for the conversion of land for the production of biofuels. The previous day, at their Annual International Monetary Fund and World Bank Group meeting at Washington, D.C., the World Bank's President, Robert Zoellick, stated that \"While many worry about filling their gas tanks, many others around the world are struggling to fill their stomachs. And it's getting more and more difficult every day.\"\n\nLuiz Inácio Lula da Silva gave a strong rebuttal, calling both claims \"fallacies resulting from commercial interests\", and putting the blame instead on U.S. and European agricultural subsidies, and a problem restricted to U.S. ethanol produced from maize. He also said that \"biofuels aren't the villain that threatens food security\". In the middle of this new wave of criticism, Hugo Chavez reaffirmed his opposition and said that he is concerned that \"so much U.S.-produced corn could be used to make biofuel, instead of feeding the world's poor\", calling the U.S. initiative to boost ethanol production during a world food crisis a \"crime\".\n\nGerman Chancellor Angela Merkel said the rise in food prices is due to poor agricultural policies and changing eating habits in developing nations, not biofuels as some critics claim. On the other hand, British Prime Minister Gordon Brown called for international action and said Britain had to be \"selective\" in supporting biofuels, and depending on the UK's assessment of biofuels' impact on world food prices, \"we will also push for change in EU biofuels targets\". Stavros Dimas, European Commissioner for the Environment said through a spokeswoman that \"there is no question for now of suspending the target fixed for biofuels\", though he acknowledged that the EU had underestimated problems caused by biofuels.\n\nOn April 29, 2008, U.S. President George W. Bush declared during a press conference that \"85 percent of the world's food prices are caused by weather, increased demand and energy prices\", and recognized that \"15 percent has been caused by ethanol\". He added that \"the high price of gasoline is going to spur more investment in ethanol as an alternative to gasoline. And the truth of the matter is it's in our national interests that our farmers grow energy, as opposed to us purchasing energy from parts of the world that are unstable or may not like us.\" Regarding the effect of agricultural subsidies on rising food prices, Bush said that \"Congress is considering a massive, bloated farm bill that would do little to solve the problem. The bill Congress is now considering would fail to eliminate subsidy payments to multi-millionaire farmers\", he continued, \"this is the right time to reform our nation's farm policies by reducing unnecessary subsidies\".\n\nJust a week before this new wave of international controversy began, U.N. Secretary General Ban Ki-moon had commented that several U.N. agencies were conducting a comprehensive review of the policy on biofuels, as the world food price crisis might trigger global instability. He said \"We need to be concerned about the possibility of taking land or replacing arable land because of these biofuels\", then he added \"While I am very much conscious and aware of these problems, at the same time you need to constantly look at having creative sources of energy, including biofuels. Therefore, at this time, just criticising biofuel may not be a good solution. I would urge we need to address these issues in a comprehensive manner.\" Regarding Jean Ziegler's proposal for a five-year ban, the U.N. Secretary rejected that proposal.\n\nA report released by Oxfam in June 2008 criticized biofuel policies of high-income countries as neither a solution to the climate crisis nor the oil crisis, while contributing to the food price crisis. The report concluded that from all biofuels available in the market, Brazilian sugarcane ethanol is not very effective, but it is the most favorable biofuel in the world in term of cost and greenhouse gas balance. The report discusses some existing problems and potential risks, and asks the Brazilian government for caution to avoid jeopardizing its environmental and social sustainability. The report also says that: \"Rich countries spent up to $15 billion last year supporting biofuels while blocking cheaper Brazilian ethanol, which is far less damaging for global food security.\"\n\nA World Bank research report published in July 2008 found that from June 2002 to June 2008 \"biofuels and the related consequences of low grain stocks, large land use shifts, speculative activity and export bans\" pushed prices up by 70 percent to 75 percent. The study found that higher oil prices and a weak dollar explain 25–30% of total price rise. The study said that \"...large increases in biofuels production in the United States and Europe are the main reason behind the steep rise in global food prices\" and also stated that \"Brazil's sugar-based ethanol did not push food prices appreciably higher\". The Renewable Fuels Association (RFA) published a rebuttal based on the version leaked before its formal release. The RFA critique considers that the analysis is highly subjective and that the author \"estimates the impact of global food prices from the weak dollar and the direct and indirect effect of high petroleum prices and attributes everything else to biofuels\".\n\nAn economic assessment by the OECD also published in July 2008 agrees with the World Bank report regarding the negative effects of subsidies and trade restrictions, but found that the impact of biofuels on food prices are much smaller. The OECD study is also critical of the limited reduction of GHG emissions achieved from biofuels produced in Europe and North America, concluding that the current biofuel support policies would reduce greenhouse gas emissions from transport fuel by no more than 0.8 percent by 2015, while Brazilian ethanol from sugar cane reduces greenhouse gas emissions by at least 80 percent compared to fossil fuels. The assessment calls on governments for more open markets in biofuels and feedstocks in order to improve efficiency and lower costs. The OECD study concluded that \"...current biofuel support measures alone are estimated to increase average wheat prices by about 5 percent, maize by around 7 percent and vegetable oil by about 19 percent over the next 10 years.\"\n\nAnother World Bank research report published in July 2010 found their previous study may have overestimated the contribution of biofuel production, as the paper concluded that \"the effect of biofuels on food prices has not been as large as originally thought, but that the use of commodities by financial investors (the so-called \"financialization of commodities\") may have been partly responsible for the 2007/08 spike\".\n\n"}
{"id": "15233213", "url": "https://en.wikipedia.org/wiki?curid=15233213", "title": "Funnel plot", "text": "Funnel plot\n\nA funnel plot is a graph designed to check for the existence of publication bias; funnel plots are commonly used in systematic reviews and meta-analyses. In the absence of publication bias, it assumes that studies with high precision will be plotted near the average, and studies with low precision will be spread evenly on both sides of the average, creating a roughly funnel-shaped distribution. Deviation from this shape can indicate publication bias.\n\nFunnel plots, introduced by Light and Pillemer in 1984\nand discussed in detail by Matthias Egger and colleagues,\nare useful adjuncts to meta-analyses. A funnel plot is a scatterplot of treatment effect against a measure of study precision. It is used primarily as a visual aid for detecting bias or systematic heterogeneity. A symmetric inverted funnel shape arises from a ‘well-behaved’ data set, in which publication bias is unlikely. An asymmetric funnel indicates a relationship between treatment effect estimate and study precision. This suggests the possibility of either publication bias or a systematic difference between studies of higher and lower precision (typically ‘small study effects’). Asymmetry can also arise from use of an inappropriate effect measure. Whatever the cause, an asymmetric funnel plot leads to doubts over the appropriateness of a simple meta-analysis and suggests that there needs to be investigation of possible causes.\n\nA variety of choices of measures of ‘study precision’ is available, including total sample size, standard error of the treatment effect, and inverse variance of the treatment effect (weight). Sterne and Egger have compared these with others, and conclude that the standard error is to be recommended.\nWhen the standard error is used, straight lines may be drawn to define a region within which 95% of points might lie in the absence of both heterogeneity and publication bias.\n\nIn common with confidence interval plots, funnel plots are conventionally drawn with the treatment effect measure on the horizontal axis, so that study precision appears on the vertical axis, breaking with the general rule. Since funnel plots are principally visual aids for detecting asymmetry along the treatment effect axis, this makes them considerably easier to interpret.\n\nThe funnel plot is not without problems. \nIf high precision studies are different from low precision studies with respect to effect size (e.g., due to different populations examined) a funnel plot may give a wrong impression of publication bias.\nThe appearance of the funnel plot can change quite dramatically depending on the scale on the y-axis — whether it is the inverse square error or the trial size.\n\n\n"}
{"id": "48870697", "url": "https://en.wikipedia.org/wiki?curid=48870697", "title": "Gamma-object", "text": "Gamma-object\n\nIn mathematics, a Γ-object of a pointed category \"C\" is a contravariant functor from Γ to \"C\".\n\nThe basic example is Segal's so-called Γ-space, which may be thought of as a generalization of simplicial abelian group (or simplicial abelian monoid). More precisely, one can define a Gamma space as an O-monoid object in an infinity-category. The notion plays a role in the generalization of algebraic K-theory that replaces an abelian group by something higher.\n"}
{"id": "2885832", "url": "https://en.wikipedia.org/wiki?curid=2885832", "title": "Geschwind–Galaburda hypothesis", "text": "Geschwind–Galaburda hypothesis\n\nThe Geschwind–Galaburda hypothesis was proposed by Norman Geschwind and Albert Galaburda to explain sex differences in cognitive abilities by relating them to lateralization of brain function. The basic idea is that differences in maturation rates between the cerebral hemispheres are mediated by circulating testosterone levels, and that sexual maturation acts to fix the hemispheres at different relative stages of development after puberty.\n\nAccording to the theory, male brains mature later than females, and the left hemisphere matures later than the right.\n\n\n"}
{"id": "5149972", "url": "https://en.wikipedia.org/wiki?curid=5149972", "title": "H-Prize", "text": "H-Prize\n\nThe H-Prize program is a series of inducement prizes intended to encourage research into the use of hydrogen as an energy carrier in a hydrogen economy. The program is sponsored by the United States Department of Energy and administered by the Hydrogen Education Foundation. In 2014, an H-Prize competition was launched to develop a small-scale hydrogen generation and refueling station for hydrogen fuel cell electric vehicles. The prize, named the H2 Refuel H-Prize and worth $1 million, was awarded in January 2017, to the consortium \"SimpleFuel\". \n\nLegislation for the prize, introduced by Rep. Bob Inglis, passed the United States House of Representatives in May 2006 as , but did not receive a vote in the United States Senate. After being reintroduced by Rep. Dan Lipinski at the beginning of a new term in January 2007, it was eventually folded into the Energy Independence and Security Act of 2007, which passed both the House of Representatives and the Senate and was then signed into law in December 2007.\n\nThe bill authorized $50 million in prize money, for the period from fiscal year 2008 to fiscal year 2017. However, in order to be spent, these funds must also be included in Congress's annual appropriation bills. The law provides for three categories of prizes:\n\n\nIn October 2008, the Hydrogen Education Foundation of Washington, D.C. was selected to administer the H-Prize along with SCRA, a South Carolina research foundation.\n\nIn August 2009, the Office of Energy Efficiency and Renewable Energy (EERE) of the U.S. Department of Energy announced a $1 million prize for innovative hydrogen storage materials which would be awarded in February 2011. However, the prize was never awarded as no competitor met the competition requirements.\n\nIn March 2014, EERE announced plans for a $1 million H2 Refuel H-Prize, and the Prize competition was launched in October 2014. The competition focused on developing small hydrogen refueling stations that can generate hydrogen from electricity or natural gas and dispense it to vehicles at least 1 kg at a time. The winner, SimpleFuel, was announced in January 2017, after building a prototype system and successfully completing the testing period. SimpleFuel was a consortium of three companies, PDC Machines, Ivys, Inc, and McPhy Energy North America. \n\n\n"}
{"id": "53527471", "url": "https://en.wikipedia.org/wiki?curid=53527471", "title": "Hangprinter", "text": "Hangprinter\n\nHangprinter is an open-source fused deposition modeling delta 3D printer notable for its unique frameless design, it was created by Torbjörn Ludvigsen. The Hangprinter uses relatively low cost parts and can be constructed for around $250 USD. The printer is part of the RepRap project where many of the parts of the printer are able to be produced on the printer itself (partially self replicating). The files for the printer are available on Github for download, modification and redistribution.\n\nThe Hangprinter v0, also called the Slideprinter, is a 2D plotter. It was designed solely to test if a 3D version could realistically be created.\n\nThe Hangprinter v1 uses counter weights to stay elevated.\n\nAll parts of the Hangprinter Version 2 are contained within a single unit which uses cables to suspend the printer within a room, allowing it to create extremely large objects over 4 meters tall. \n\nVersion 3 of the Hangprinter has the motors and gears attached to the ceiling, making the carriage lighter.\n\n\n"}
{"id": "8311927", "url": "https://en.wikipedia.org/wiki?curid=8311927", "title": "Harvard–Yenching Classification", "text": "Harvard–Yenching Classification\n\nThe Harvard–Yenching Classification System is a library classification system for Chinese language materials in the United States of America. It was devised by Alfred Kaiming Chiu (1898–1977). The system was primarily created for the classification of Chinese language materials in the Harvard–Yenching Library which was founded in 1927 at the Harvard–Yenching Institute.\n\nDuring that early period other systems, such as the early edition of the Library of Congress Classification, did not consist of appropriate subject headings to classify the Chinese language materials, particularly the ancient published materials. As many American libraries started to collect the ancient and contemporary published materials from China, a number of American libraries subsequently followed Harvard University to adopt Harvard–Yenching classification system, such as the East Asian Library of the University of California in Berkeley, Columbia University, The University of Chicago, Washington University in St. Louis etc.\n\nIn addition to American libraries, the libraries of other universities in the world including England, Australia, New Zealand, Hong Kong, Singapore etc. also followed Harvard University to adopt the system. During the period from the 1930s to the 1970s, the use of the system became popular for classifying not only Chinese language materials but also other East Asian materials including Korean and Japanese language materials.\n\nDuring the period from the 1970s to the 1980s, a comprehensive subset of subject headings for Chinese language materials was gradually established in the Library of Congress Classification System so that almost a full spectrum of ancient and contemporary Chinese topics can be widely covered. As a result of this, the Library of Congress Classification System eventually replaced the Harvard–Yenching Classification System for all Chinese language materials acquired after the 1970s in many American Libraries.\n\nThough the system has largely been phased out, the system is still being used in some libraries for Chinese language materials acquired prior to the Library of Congress update. Such previously acquired books are normally stored in separate stacks in libraries. However, some of the university libraries in the Commonwealth countries of the United Kingdom such as England, Australia and New Zealand still continue to use the Harvard–Yenching system; for example, the Institute for Chinese Studies Library of the University of Oxford, University of Sydney, and University of Auckland.\n\nThe key classes of the system are listed as follows:\n\n\n\n\n\n\n\n\n\n\n\nThe official library classification in China is:\n\n\nThe other library classifications for Chinese materials outside China are:\n\n\n"}
{"id": "7058047", "url": "https://en.wikipedia.org/wiki?curid=7058047", "title": "History of Lorentz transformations", "text": "History of Lorentz transformations\n\nThe history of Lorentz transformations comprises the development of linear transformations forming the Lorentz group or Poincaré group preserving the Lorentz interval formula_1 and the Minkowski inner product formula_2.\n\nIn mathematics, transformations equivalent to what was later known as Lorentz transformations in various dimensions were discussed in the 19th century in relation to the theory of quadratic forms, hyperbolic geometry, Möbius geometry, and sphere geometry, which is connected to the fact that the group of motions in hyperbolic space, the Möbius group or projective special linear group, and the Laguerre group are isomorphic to the Lorentz group.\n\nIn physics, Lorentz transformations became known at the beginning of the 20th century, when it was discovered that they exhibit the symmetry of Maxwell's equations. Subsequently, they became fundamental to all of physics, because they formed the basis of special relativity in which they exhibit the symmetry of Minkowski spacetime, making the velocity of light invariant between different inertial frames. They relate the spacetime coordinates, which specify the position \"x,y,z\" and time \"t\" of an event, relative to a particular inertial frame of reference (the \"rest system\"), and the coordinates \"x′,y′,z′\" and \"t′\" of the same event relative to another coordinate system moving in the positive \"x\"-direction at a constant speed \"v\", relative to the rest system.\n\nThe general quadratic form \"q(x)\" with coefficients of a symmetric matrix A, the associated bilinear form \"b(x,y)\", and the linear transformations of \"q(x)\" and \"b(x,y)\" into \"q(x′)\" and \"b(x′,y′)\" using the transformation matrix g, can be written as\n\nThe case \"n=1\" is the binary quadratic form introduced by Lagrange (1773) and Gauss (1798/1801), \"n=2\" is the ternary quadratic form introduced by Gauss (1798/1801), \"n=3\" is the quaternary quadratic form etc.\n\nThe general Lorentz transformation follows from () by setting A=A′=diag(-1,1...,1) and det g=1. It forms an indefinite orthogonal group called the Lorentz group , the quadratic form \"q(x)\" becomes the Lorentz interval in terms of an indefinite quadratic form in terms of pseudo-Euclidean space, and the associated bilinear form \"b(x)\" becomes the Minkowski inner product:\n\nSuch general Lorentz transformations () for various dimensions were used by Gauss (1818), Jacobi (1827, 1833), Lebesgue (1837), Bour (1856), Somov (1863), Hill (1882) in order to simplify computations of elliptic functions and integrals. They were also used by Poincaré (1881), Cox (1881/82), Picard (1882, 1884), Killing (1885, 1893), Gérard (1892), Hausdorff (1899), Woods (1901, 1903), Liebmann (1904/05) to describe hyperbolic motions (i.e. rigid motions in the hyperbolic plane or hyperbolic space), which were expressed in terms of Weierstrass coordinates of the hyperboloid model satisfying the relation formula_3 or in terms of the Cayley–Klein metric of projective geometry using the \"absolute\" form formula_4. In addition, infinitesimal transformations related to the Lie algebra of the group of hyperbolic motions were given in terms of Weierstrass coordinates formula_3 by Killing (1888-1897).\n\nIf \"x, x′\" in () are interpreted as homogeneous coordinates, then the corresponding inhomogenous coordinates \"u, u′\" follow by\n\nso that the Lorentz transformation becomes a homography leaving invariant the equation of the unit sphere, which John Lighton Synge called “the most general formula for the composition of velocities” in terms of special relativity (the transformation matrix g stays the same as in ()):\n\n\\hline -x_{0}^{2}+\\cdots+x_{n}^{2}=-x_{0}^{\\prime2}+\\dots+x_{n}^{\\prime2}=0 & \\rightarrow & -1+u_{1}^{2}+\\cdots+u_{n}^{2}=-1+u_{1}^{\\prime2}+\\cdots+u_{n}^{\\prime2}=0\n\\end{matrix}\\\\\n\\hline \\begin{align}u_{s} & =\\frac{g_{s0}+\\sum_{j=1}^{n}g_{sj}u_{j}^{\\prime}}{g_{00}+\\sum_{j=1}^{n}g_{0j}u_{j}^{\\prime}}\\\\\nu_{s}^{\\prime} & =\\frac{g_{s0}^{(-1)}+\\sum_{j=1}^{n}g_{sj}^{(-1)}u_{j}}{g_{00}^{(-1)}+\\sum_{j=1}^{n}g_{0j}^{(-1)}u_{j}}\\\\\n\\left|\\begin{align}\\sum_{i=1}^{n}g_{ij}g_{ik}-g_{0j}g_{0k} & =\\left\\{ \\begin{align}-1\\quad & (j=k=0)\\\\\n1\\quad & (j=k>0)\\\\\n0\\quad & (j\\ne k)\n\\right.\\\\\n\\sum_{j=1}^{n}g_{ij}g_{kj}-g_{i0}g_{k0} & =\\left\\{ \\begin{align}-1\\quad & (i=k=0)\\\\\n1\\quad & (i=k>0)\\\\\n0\\quad & (i\\ne k)\n\\right.\n\\right.\n\nSuch Lorentz transformations for various dimensions were directly used by Gauss (1818), Jacobi (1827–1833), Lebesgue (1837), Bour (1856), Somov (1863), Hill (1882), Callandreau (1885) in order to simplify computations of elliptic functions and integrals, by Picard (1882-1884) in relation to Hermitian quadratic forms, or by Woods (1901, 1903) in terms of the Beltrami–Klein model of hyperbolic geometry. In addition, infinitesimal transformations related to the Lie algebra of the group of hyperbolic motions leaving invariant the unit sphere formula_7 were given by Lie (1885-1893) and Werner (1889) and Killing (1888-1897).\n\nParticular forms of Lorentz transformations or relativistic velocity additions, mostly restricted to 2, 3 or 4 dimensions, have been formulated by many authors using:\n\n\nBy using the imaginary quantities formula_8 in x as well as formula_9 \"(s=1,2...n)\" in g, the Lorentz transformation () assumes the form of an orthogonal transformation, the Lorentz interval becomes the Euclidean norm, and the Minkowski inner product becomes the dot product:\n\nThe cases \"n=1,2,3,4\" of this quadratic form with real numbers and its transformation was discussed by Euler (1771) and in \"n\" dimensions by Cauchy (1829). Its interpretation as leaving invariant the equation of the sphere with imaginary radius was given by Lie (1871), its interpretation as a Lorentz transformation with \"n=3\" using one imaginary coordinate was given by Minkowski (1907) and Sommerfeld (1909).\n\nA well known example of an orthogonal transformation is spatial rotation:\n\nThis quadratic form and its transformation with real numbers and real angle was discussed by Euler (1771), its interpretation as a Lorentz transformation using one imaginary coordinate and imaginary angle was given by Minkowski (1907) and Sommerfeld (1909).\n\nThe case of a Lorentz transformation without spatial rotation is called a Lorentz boost. The simplest case can be given, for instance, by setting \"n=1\" in ():\n\nwhich resembles precisely the relations of hyperbolic functions by setting \"g=g\"=cosh(η) and \"g=g\"=sinh(η), with η as the hyperbolic angle. Thus by adding an unchanged \"x\"-axis, a Lorentz boost for \"n=2\" representing a translation in the hyperbolic plane in terms of Weierstrass coordinates of the hyperboloid model along one axis (being the same as a rotation around an imaginary angle \"iη=φ\" in ()) is given by\n\n\\frac{\\tanh\\eta}{\\sqrt{1-\\tanh^{2}\\eta}} & =\\sinh\\eta & (e)\\\\\n\\frac{\\tanh\\eta+\\tanh\\zeta}{1+\\tanh\\eta\\tanh\\zeta} & =\\tanh\\left(\\eta+\\zeta\\right) & (f)\n}\\right.\n\nwhich can also be expressed as squeeze mappings in terms of exponential functions:\n\nAll hyperbolic relations (a,b,c,d,e,f) on the right of () were given by Lambert (1768–1770). The Lorentz transformations (, see ) were given by Cox (1882), Lindemann (1890/91), Gérard (1892), Killing (1893, 1897/98), Whitehead (1897/98), Woods (1903/05) and Liebmann (1904/05). Lorentz transformations (, 1) were given by Lindemann (1890/91) and Herglotz (1909), while formulas equivalent to (, 2) by Klein (1871).\n\nIn line with equation () one can use coordinates formula_10, which in terms of hyperbolic geometry can be interpreted as changing the above Weierstrass coordinates into Beltrami coordinates inside the unit circle formula_11, thus the corresponding Lorentz transformations () obtain the form:\n\n\\hline -x_{0}^{2}+x_{1}^{2}+x_{2}^{2}=-x_{0}^{\\prime2}+x_{1}^{\\prime2}+x_{2}^{\\prime2}=0 & \\rightarrow & -1+u_{x}^{2}+u_{y}^{2}=-1+u_{x}^{\\prime2}+u_{y}^{\\prime2}=0\n\\end{matrix}\\\\\n\\hline {\\scriptstyle \\begin{align}\\frac{\\sinh\\eta}{\\cosh\\eta} & =\\tanh\\eta=v\\\\\n\\cosh\\eta & =\\frac{1}{\\sqrt{1-\\tanh^{2}\\eta}}\\\\\nu_{1} & =\\tanh\\zeta_{1}\\\\\nu_{2} & =\\tanh\\zeta_{2}\\\\\nu_{1}^{\\prime} & =\\tanh\\zeta_{1}^{\\prime}\\\\\n}\\left|\\begin{align}u_{1} & =\\frac{\\sinh\\eta+u_{1}^{\\prime}\\cosh\\eta}{\\cosh\\eta+u_{1}^{\\prime}\\sinh\\eta} & & =\\frac{\\tanh\\zeta_{1}^{\\prime}+\\tanh\\eta}{1+\\tanh\\zeta_{1}^{\\prime}\\tanh\\eta} & & =\\frac{u_{1}^{\\prime}+v}{1+u_{1}^{\\prime}v}\\\\\nu_{2} & =\\frac{u_{2}^{\\prime}}{\\cosh\\eta+u_{1}^{\\prime}\\sinh\\eta} & & =\\frac{\\tanh\\zeta_{2}^{\\prime}\\sqrt{1-\\tanh^{2}\\eta}}{1+\\tanh\\zeta_{1}^{\\prime}\\tanh\\eta} & & =\\frac{u_{2}^{\\prime}\\sqrt{1-v^{2}}}{1+u_{1}^{\\prime}v}\\\\\nu_{1}^{\\prime} & =\\frac{-\\sinh\\eta+u_{1}\\cosh\\eta}{\\cosh\\eta-u_{1}\\sinh\\eta} & & =\\frac{\\tanh\\zeta{}_{1}-\\tanh\\eta}{1-\\tanh\\zeta{}_{1}\\tanh\\eta} & & =\\frac{u_{1}-v}{1-u_{1}v}\\\\\n\\right.\n\nThese Lorentz transformations were given by Escherich (1874) and Killing (1898) (on the left), as well as Beltrami (1868) or Schur (1885/86, 1900/02) (on the right). By using the scalar product of \"[u, u]\", the resulting Lorentz transformation can be seen as equivalent to the hyperbolic law of cosines:\n\nu^{\\prime2}=u_{1}^{\\prime2}+u_{2}^{\\prime2},\\ \\tan\\alpha'=\\frac{u_{2}^{\\prime}}{u_{1}^{\\prime}}\\\\\n\\downarrow\\\\\nu'=\\frac{\\sqrt{-v^{2}-u^{2}+2vu\\cos\\alpha+\\left(vu\\sin\\alpha\\right){}^{2}}}{1-vu\\cos\\alpha},\\quad u=\\frac{\\sqrt{v^{2}+u^{\\prime2}+2vu'\\cos\\alpha'-\\left(vu'\\sin\\alpha'\\right){}^{2}}}{1+vu'\\cos\\alpha'}\\\\\n\\downarrow\\\\\n\\frac{1}{\\sqrt{1-u^{\\prime2}}}=\\frac{1}{\\sqrt{1-v^{2}}}\\frac{1}{\\sqrt{1-u^{2}}}-\\frac{v}{\\sqrt{1-v^{2}}}\\frac{u}{\\sqrt{1-u^{2}}}\\cos\\alpha & (b)\\\\\n\\downarrow\\\\\n\\frac{1}{\\sqrt{1-\\tanh^{2}\\xi}}=\\frac{1}{\\sqrt{1-\\tanh^{2}\\eta}}\\frac{1}{\\sqrt{1-\\tanh^{2}\\zeta}}-\\frac{\\tanh\\eta}{\\sqrt{1-\\tanh^{2}\\eta}}\\frac{\\tanh\\zeta}{\\sqrt{1-\\tanh^{2}\\zeta}}\\cos\\alpha\\\\\n\\downarrow\\\\\n\\cosh\\xi=\\cosh\\eta\\cosh\\zeta-\\sinh\\eta\\sinh\\zeta\\cos\\alpha & (a)\n\nThe hyperbolic law of cosines (a) was given by Taurinus (1826) and Lobachevsky (1829/30) and others, while variant (b) was given by Schur (1900/02).\n\nIn the theory of relativity, Lorentz transformations exhibit the symmetry of Minkowski spacetime by using a constant \"c\" as the speed of light, and a parameter \"v\" as the relative velocity between two inertial reference frames. In particular, the hyperbolic angle η in () can be interpreted as the velocity related rapidity η=atanh(β) with β=\"v/c\", so that γ=cosh(η) is the Lorentz factor, βγ=sinh(η) the proper velocity, \"v=c\"·tanh(η) the relative velocity of two inertial frames, \"u′=c\"·tanh(ζ) the velocity of another object, \"u=c\"·tanh(η+ζ) the velocity-addition formula, thus () becomes:\n\n} & =\\gamma & (d)\\\\\n\\frac{\\beta}{\\sqrt{1-\\beta^{2}}} & =\\beta\\gamma & (e)\\\\\n\\frac{u'+v}{1+\\frac{u'v}{c^{2}}} & =u & (f)\n}\\right.\n\nOr in four dimensions and by setting \"x=ct, x=x, x=y\" and adding an unchanged \"z\" the familiar form follows\n\n\\right)\\\\\nx & =\\gamma(x'+vt')\\\\\ny & =y'\\\\\nz & =z'\n\\right|\\begin{align}t' & =\\gamma\\left(t-x\\frac{v}{c^{2}}\\right)\\\\\nx' & =\\gamma(x-vt)\\\\\ny' & =y\\\\\nz' & =z\n\nSimilar transformations were introduced by Voigt (1887) and by Lorentz (1892, 1895) who analyzed Maxwell's equations, they were completed by Larmor (1897, 1900) and Lorentz (1899, 1904), and brought into their modern form by Poincaré (1905) who gave the transformation the name of Lorentz. Eventually, Einstein (1905) showed in his development of special relativity that the transformations follow from the principle of relativity and constant light speed alone by modifying the traditional concepts of space and time, without requiring a mechanical aether in contradistinction to Lorentz and Poincaré. Minkowski (1907–1908) used them to argue that space and time are inseparably connected as spacetime. Minkowski (1907–1908) and Varićak (1910) showed the relation to imaginary and hyperbolic functions. Important contributions to the mathematical understanding of the Lorentz transformation were also made by other authors such as Herglotz (1909/10), Ignatowski (1910), Noether (1910) and Klein (1910), Borel (1913–14).\n\nIn line with equation (), one can substitute formula_12 in () or (), producing the Lorentz transformation of velocities (or velocity addition formula) in analogy to Beltrami coordinates of ():\n\n\\hline -x_{0}^{2}+x_{1}^{2}+x_{2}^{2}=-x_{0}^{\\prime2}+x_{1}^{\\prime2}+x_{2}^{\\prime2}=0 & \\rightarrow & -c^{2}+u_{x}^{2}+u_{y}^{2}=-c^{2}+u_{x}^{\\prime2}+u_{y}^{\\prime2}=0\n\\end{matrix}\\\\\n\\hline {\\scriptstyle \\begin{align}\\frac{\\sinh\\eta}{\\cosh\\eta} & =\\tanh\\eta=\\frac{v}{c}\\\\\n\\cosh\\eta & =\\frac{1}{\\sqrt{1-\\tanh^{2}\\eta}}\\\\\nu_{x} & =c\\tanh\\zeta_{x}\\\\\nu_{y} & =c\\tanh\\zeta_{y}\\\\\nu_{x}^{\\prime} & =c\\tanh\\zeta_{x}^{\\prime}\\\\\n}\\left|\\begin{align}u_{x} & =\\frac{c^{2}\\sinh\\eta+u_{x}^{\\prime}c\\cosh\\eta}{c\\cosh\\eta+u_{x}^{\\prime}\\sinh\\eta} & & =\\frac{c\\tanh\\zeta_{x}^{\\prime}+c\\tanh\\eta}{1+\\tanh\\zeta_{x}^{\\prime}\\tanh\\eta} & & =\\frac{u_{x}^{\\prime}+v}{1+\\frac{v}{c^{2}}u_{x}^{\\prime}}\\\\\nu_{y} & =\\frac{cy'}{c\\cosh\\eta+u_{x}^{\\prime}\\sinh\\eta} & & =\\frac{c\\tanh\\zeta_{y}^{\\prime}\\sqrt{1-\\tanh^{2}\\eta}}{1+\\tanh\\zeta_{x}^{\\prime}\\tanh\\eta} & & =\\frac{u_{y}^{\\prime}\\sqrt{1-\\frac{v^{2}}{c^{2}}}}{1+\\frac{v}{c^{2}}u_{x}^{\\prime}}\\\\\nu_{x}^{\\prime} & =\\frac{-c^{2}\\sinh\\eta+u_{x}c\\cosh\\eta}{c\\cosh\\eta-u_{x}\\sinh\\eta} & & =\\frac{c\\tanh\\zeta{}_{x}-c\\tanh\\eta}{1-\\tanh\\zeta{}_{x}\\tanh\\eta} & & =\\frac{u_{x}-v}{1-\\frac{v}{c^{2}}u{}_{x}}\\\\\n\\right.\n\nor more generally expressed as hyperbolic law of cosines in terms of ():\n\nu'^{2}=u_{x}^{\\prime2}+u_{y}^{\\prime2},\\ \\tan\\alpha'=\\frac{u_{y}^{\\prime}}{u_{x}^{\\prime}}\\\\\n\\downarrow\\\\\nu'=\\frac{\\sqrt{-v^{2}-u^{2}+2vu\\cos\\alpha+\\left(\\frac{vu\\sin\\alpha}{c}\\right){}^{2}}}{1-\\frac{v}{c^{2}}u\\cos\\alpha},\\quad u=\\frac{\\sqrt{v^{2}+u^{\\prime2}+2vu'\\cos\\alpha'-\\left(\\frac{vu'\\sin\\alpha'}{c}\\right){}^{2}}}{1+\\frac{v}{c^{2}}u'\\cos\\alpha'}\\\\\n\\downarrow\\\\\n\\frac{1}{\\sqrt{1-\\frac{u^{\\prime2}}{c^{2}}}}=\\frac{1}{\\sqrt{1-\\frac{v^{2}}{c^{2}}}}\\frac{1}{\\sqrt{1-\\frac{u^{2}}{c^{2}}}}-\\frac{v/c}{\\sqrt{1-\\frac{v^{2}}{c^{2}}}}\\frac{u/c}{\\sqrt{1-\\frac{u^{2}}{c^{2}}}}\\cos\\alpha\\\\\n\\downarrow\\\\\n\\frac{1}{\\sqrt{1-\\tanh^{2}\\xi}}=\\frac{1}{\\sqrt{1-\\tanh^{2}\\eta}}\\frac{1}{\\sqrt{1-\\tanh^{2}\\zeta}}-\\frac{\\tanh\\eta}{\\sqrt{1-\\tanh^{2}\\eta}}\\frac{\\tanh\\zeta}{\\sqrt{1-\\tanh^{2}\\zeta}}\\cos\\alpha\\\\\n\\downarrow\\\\\n\\cosh\\xi=\\cosh\\eta\\cosh\\zeta-\\sinh\\eta\\sinh\\zeta\\cos\\alpha\n\nThe velocity addition formula was given by Einstein (1905), while the relations to trigonometric and hyperbolic functions were given by Sommerfeld (1909) and Varićak (1910).\n\nAlso Lorentz boosts for arbitrary directions in line with () can be given as:\n\nor in vector notation\n\nSuch transformations were formulated by Herglotz (1911) and Silberstein (1911) and others.\n\nA general sphere transformation preserving the quadratic form formula_14 is the group Con(p,1) of spacetime conformal transformations in terms of inversions or special conformal transformations, which has the property of changing spheres into spheres. One can switch between the representations by using an imaginary radius coordinate \"x=iR\" which gives formula_15 (conformal transformation), or by using a real radius coordinate \"x=R\" which gives formula_14 (spherical wave transformation). This group was studied by Lie (1871) and others in terms of contact transformations, in which \"x\" is related to the radius \"R\".\n\nIt turns out that Con(3,1) is isomorphic to the special orthogonal group SO(4,2), and contains the Lorentz group SO(3,1) as a subgroup by setting λ=1. More generally, Con(p,q) is isomorphic to SO(p+1,q+1) and contains SO(p,q) as subgroup. This implies that Con(p,0) is isomorphic to the Lorentz group of arbitrary dimensions SO(p+1,1). Consequently, the conformal group in the plane Con(2,0) – known as the group of Möbius transformations – is isomorphic to the Lorentz group SO(3,1). This can be seen using tetracyclical coordinates satisfying the form formula_17, which were discussed by Pockels (1891), Klein (1893), Bôcher (1894).\n\nA special case of Lie's geometry of oriented spheres is the Laguerre group, transforming oriented planes and lines into each other. It's generated by the Laguerre inversion introduced by Laguerre (1882) and discussed by Darboux (1887) leaving invariant \"x+y+z-R\" with \"R\" as radius, thus the Laguerre group is isomorphic to the Lorentz group\n\nStephanos (1883) argued that Lie's geometry of oriented spheres in terms of contact transformations, as well as the special case of the transformations of oriented planes into each other (such as by Laguerre), provides a geometrical interpretation of Hamilton's biquaternions.\n\nThe relation between Lie's sphere transformations and the Lorentz transformation was noted by Bateman & Cunningham (1909–1910) and others. Furthermore, the group isomorphism between the Laguerre group and Lorentz group was pointed out by Bateman (1910), Cartan (1912, 1915/55), Poincaré (1912/21) and others.\n\nThe general transformation () of any quadratic form into itself can also be given using \"arbitrary\" parameters based on the Cayley transform (I-T)·(I+T) as follows:\n\n\\cdot\\mathbf{A}\\cdot\\mathbf{x}=q'=\\mathbf{x}^{\\mathrm{\\prime T}}\\cdot\\mathbf{A}\\cdot\\mathbf{x}'\\\\\n\\hline \\\\\n\\mathbf{x}=(\\mathbf{I}-\\mathbf{T}\\cdot\\mathbf{A})^{-1}\\cdot(\\mathbf{I}+\\mathbf{T}\\cdot\\mathbf{A})\\cdot\\mathbf{x}'\\\\\n\\text{or}\\\\\n\\mathbf{x}=\\mathbf{A}^{-1}\\cdot(\\mathbf{A}-\\mathbf{T})\\cdot(\\mathbf{A}+\\mathbf{T})^{-1}\\cdot\\mathbf{A}\\cdot\\mathbf{x}'\n\nwhere A is, as above, a symmetric matrix defining the quadratic form (there is no primed A' because the coefficients are assumed to be the same on both sides), I the identity matrix, and T an arbitrary antisymmetric matrix. After Cayley (1846) introduced transformations related to sums of positive squares, Hermite (1853/54, 1854) derived transformations for arbitrary quadratic forms, whose result was reformulated in terms of matrices () by Cayley (1855a, 1855b). For instance, the choice A=diag(1,1,1) gives an orthogonal transformation which can be used to describe spatial rotations corresponding to the Euler-Rodrigues parameters \"[a,b,c,d]\" discovered by Euler (1771) and Rodrigues (1840), which can be interpreted as the coefficients of quaternions. Setting \"d=1\", the equations have the form:\n\nAlso the Lorentz interval and the Lorentz transformation can be produced by the Cayley–Hermite formalism. The Lorentz transformation in 2 dimensions follows from () by setting A=diag(-1,1):\n\n\\left[\\begin{matrix}1+a^{2} & -2a\\\\\n\\end{matrix}\\right|\\left\\{ \\mathbf{T}=\\begin{vmatrix}0 & a\\\\\n-a & 0\n\nor with A=diag(-1,1,1):\n\nor with A=diag(-1,1,1,1):\n\nEquations containing the Lorentz transformations (, , ) as special cases were given by Cayley (1855), while Lorentz transformation () was explicitly given by Bachmann (1869). In relativity, equations similar to (, ) were first employed by Borel (1913) to represent Lorentz transformations.\n\nAs described in equation (), the Lorentz interval is closely connected to the alternative form formula_18, which in terms of the Cayley–Hermite parameters is invariant under the transformation:\n\nThis transformation was given by Cayley (1884), even though he didn't relate it to the Lorentz interval but rather to formula_19. As shown in the next section in equation (), many authors (some before Cayley) expressed the invariance of formula_18 and its relation to the Lorentz interval by using the alternative Cayley–Klein parameters and Möbius transformations.\n\nThe previously mentioned Euler-Rodrigues parameter \"a,b,c,d\" (i.e. Cayley-Hermite parameter in equation () with \"d=1\") are closely related to Cayley–Klein parameter α,β,γ,δ introduced by Helmholtz (1866/67), Cayley (1879) and Klein (1884) to connect Möbius transformations formula_21 and rotations:\n\nthus () becomes:\n\nAlso the Lorentz transformation can be expressed with variants of the Cayley–Klein parameters: One relates these parameters to a spin-matrix D, the spin transformations of variables formula_23 (the overline denotes complex conjugate), and the Möbius transformation of formula_24. When defined in terms of isometries of hyperblic space (hyperbolic motions), the Hermitian matrix u associated with these Möbius transformations produces an invariant determinant formula_25 identical to the Lorentz interval. Therefore, these transformations were described by John Lighton Synge as being a \"factory for the mass production of Lorentz transformations\". It also turns out that the related spin group Spin(3, 1) or special linear group SL(2, C) acts as the double cover of the Lorentz group (one Lorentz transformation corresponds to two spin transformations of different sign), while the Möbius group Con(2, 0) or projective special linear group PSL(2, C) is isomorphic to both the Lorentz group and the group of isometries of hyperbolic space.\n\nIn four dimensions, these Lorentz transformations can be represented as follows:\n\n\\eta' & =\\gamma\\xi+\\delta\\eta\n\\right.\\\\\n\\hline \\left.\\begin{matrix}\\mathbf{u}=\\left(\\begin{matrix}X_{1} & X_{2}\\\\\n\\end{matrix}\\right)=\\left(\\begin{matrix}\\bar{\\xi}\\xi & \\xi\\bar{\\eta}\\\\\n\\bar{\\xi}\\eta & \\bar{\\eta}\\eta\n\\end{matrix}\\right)=\\left(\\begin{matrix}x_{0}+x_{3} & x_{1}-ix_{2}\\\\\n\\end{matrix}\\right)\\\\\n\\end{matrix}\\right|\\begin{matrix}\\mathbf{D}=\\left(\\begin{matrix}\\alpha & \\beta\\\\\n\\gamma & \\delta\n\\end{matrix}\\right)\\\\\n\\end{matrix}\\\\\n\\hline \\mathbf{u}'=\\mathbf{D}\\cdot\\mathbf{u}\\cdot\\bar{\\mathbf{D}}^{\\mathrm{T}}=\\begin{align}X_{1}^{\\prime} & =X_{1}\\alpha\\bar{\\alpha}+X_{2}\\alpha\\bar{\\beta}+X_{3}\\bar{\\alpha}\\beta+X_{4}\\beta\\bar{\\beta}\\\\\nX_{2}^{\\prime} & =X_{1}\\bar{\\alpha}\\gamma+X_{2}\\bar{\\alpha}\\delta+X_{3}\\bar{\\beta}\\gamma+X_{4}\\bar{\\beta}\\delta\\\\\nX_{3}^{\\prime} & =X_{1}\\alpha\\bar{\\gamma}+X_{2}\\alpha\\bar{\\delta}+X_{3}\\beta\\bar{\\gamma}+X_{4}\\beta\\bar{\\delta}\\\\\n\\hline \\begin{align}X_{3}^{\\prime}X_{2}^{\\prime}-X_{1}^{\\prime}X_{4}^{\\prime} & =X_{3}X_{2}-X_{1}X_{4}=0\\\\\n\nor expressing u′ in terms of \"x...\" it follows:\n\nThe general transformation u′ in () was given by Cayley (1854). The Möbius transformation in () was used as a Lorentz transformation by Poincaré (1881) and Hausdorff (1899) for the special case \"x\"=-1 and the resulting Lorentz interval formula_26 (hyperboloid in three dimensions). The general relation between Möbius transformations and transformation u′ leaving invariant the generalized circle was pointed out by Poincaré (1883) in relation to Kleinian groups, and by Klein (1884) in relation to surfaces of second degree and the invariance of the unit sphere. The invariance of the complete Lorentz interval using u′ as Lorentz transformation was demonstrated by Klein (1889-1893), 1896/97), Bianchi (1893), Fricke (1893, 1897). Its reformulation as Lorentz transformation () was provided by Bianchi (1893) and Fricke (1893, 1897). In relativity, () was first employed by Herglotz (1909/10).\n\nIn the case of three dimensions (\"x\"=0, while \"x\" becomes the new \"x\", no complex conjugate) it simplifies to:\n\n\\eta' & =\\gamma\\xi+\\delta\\eta\n\\right.\\\\\n\\hline \\left.\\begin{matrix}\\mathbf{u}=\\left(\\begin{matrix}X_{1} & X_{2}\\\\\n\\end{matrix}\\right)=\\left(\\begin{matrix}\\xi^{2} & \\xi\\eta\\\\\n\\end{matrix}\\right)=\\left(\\begin{matrix}x_{0}+x_{2} & x_{1}\\\\\n\\end{matrix}\\right)\\\\\n\\end{matrix}\\right|\\begin{matrix}\\mathbf{D}=\\left(\\begin{matrix}\\alpha & \\beta\\\\\n\\gamma & \\delta\n\\end{matrix}\\right)\\\\\n\\end{matrix}\\\\\n\\hline \\mathbf{u}'=\\mathbf{D}\\cdot\\mathbf{u}\\cdot\\mathbf{D}^{\\mathrm{T}}=\\begin{align}X_{1}^{\\prime} & =X_{1}\\alpha^{2}+X_{2}2\\alpha\\beta+X_{3}\\beta^{2}\\\\\nX_{2}^{\\prime} & =X_{1}\\alpha\\gamma+X_{2}(\\alpha\\delta+\\beta\\gamma)+X_{3}\\beta\\delta\\\\\n\\hline \\begin{align}X_{2}^{\\prime2}-X_{1}^{\\prime}X_{3}^{\\prime} & =X_{2}^{2}-X_{1}X_{3}=0\\\\\n\nthus\n\nThe general transformation u′ and its invariant formula_18 in () was already used by Lagrange (1773) and Gauss (1798/1801) in the theory of integer binary quadratic forms. The invariant formula_18 was also studied by Klein (1871) in connection to hyperbolic plane geometry (see equation ()), while the connection between u′ and formula_18 with the Möbius transformation was analyzed by Poincaré (1886) in relation to Fuchsian groups. The adaptation to the Lorentz interval by which () becomes a Lorentz transformation was given by Bianchi (1888) and Fricke (1891). The Lorentz Transformation in the form of () was explicitly stated by Gauss around 1800 (posthumously published 1863), as well as Selling (1873), Bianchi (1888), Fricke (1891) in relation to integer indefinite ternary quadratic forms.\n\nThe Lorentz transformations can also be expressed in terms of biquaternions having one real part \"xe+xe+xe\" and one purely imaginary part \"ix\" (some authors use the opposite convention). Its general form (on the left) and the corresponding boost (on the right) are as follows (where the overline denotes Hamiltonian conjugation and * complex conjugation):\n\nCayley (1854, 1855) derived quaternion transformations leaving invariant the sum of squares formula_30. Cox (1882/83) discussed the Lorentz interval in terms of Weierstrass coordinates formula_31 in the course of adapting William Kingdon Clifford's biquaternions \"a+ωb\" to hyperbolic geometry (ω=-1 for hyperbolic geometry, ω=1 elliptic, ω=0 parabolic). Stephanos (1883) related the imaginary part of William Rowan Hamilton's biquaternions to the radius of spheres, and introduced a homography leaving invariant the equations of oriented spheres or oriented planes in terms of Lie sphere geometry. Buchheim (1884/85) discussed the Cayley absolute formula_32 and adapted Clifford's biquaternions to hyperbolic geometry similar to Cox by using all three values of ω. Eventually, the modern Lorentz transformation using biquaternions with ω=-1 as in hyperbolic geometry was given by Noether (1910), Klein (1910), Conway (1911), Silberstein (1911).\n\nOften connected with quaternionic systems is the hyperbolic number ε=1, which also allows to formulate the Lorentz transformations:\n\nAfter the trigonometric expression e=cos(x)+i·sin(x) (Euler's formula) was given by Euler (1748) and the hyperbolic analogue e by Cockle (1848) in the framework of tessarines, it was shown by Cox (1882/83) that one can identify ww′=e with associative quaternion multiplication. Here, e is the hyperbolic versor with ε=1, the elliptic one follows with ε=-1, and parabolic with ε=0 (this should not be confused with the expression ω in Clifford's biquaternions also used by Cox, in which ω=-1 is hyperbolic). The hyperbolic versor was also discussed by Macfarlane (1892, 1894, 1900) in terms of hyperbolic quaternions. The expression ε=1 for hyperbolic motions (and ε=-1 for elliptic, ε=0 for parabolic motions) also appear in \"biquaternions\" defined by Vahlen (1901/02, 1905).\n\nMore extended forms of complex and (bi-)quaternionic systems in terms of Clifford algebra can also be used to express the Lorentz transformations. For instance, using a system \"a\" of Clifford numbers one can transform the following general quadratic form into itself, in which the individual values of formula_33 can be set to +1 or -1 at will:\n\nThe Lorentz interval follows if the sign of one i differs from all others. The general definite form formula_35 as well as the general indefinite form formula_36 and their invariance under transformation (1) was discussed by Lipschitz (1885/86), while hyperbolic motions were discussed by Vahlen (1901/02, 1905) by setting ε=1 in transformation (2), while elliptic motions follow with ε=-1 and parabolic motions ε=0, all of which he also related to biquaternions.\n\nA summary of historical Lorentz boost formulas consistent with (, , , ) and velocity additions consistent with (, , , ).\n\nAfter Vincenzo Riccati introduced hyperbolic functions in 1757, Johann Heinrich Lambert (read 1767, published 1768) gave the following relations, in which \"tang \"φ or abbreviated \"tφ\" was equated by Lambert to the tangens hyperbolicus formula_37 of a variable \"u\", or in modern notation \"tφ=tanh(u)\":\n\nIn (1770) he rewrote the addition law for the hyperbolic tangens (f) or (g) as:\n\nLeonhard Euler (1771) demonstrated the invariance of quadratic forms in terms of sum of squares under a linear substitution and its coefficients, now known as orthogonal transformation, as well as under rotations using Euler angles. The case of two dimensions is given by\n\nor three dimensions\n\nThese coefficiens \"A,B,C,D,E,F,G,H,I\" were related by Euler to four arbitrary parameter \"p,q,r,s\", which where rediscovered by Olinde Rodrigues (1840) who related them to rotation angles now called Euler–Rodrigues parameters in line with equation ():\n\nThe orthogonal transformation in four dimensions was given by him as\n\nAfter the invariance of the sum of squares under linear substitutions was discussed by Euler (1771), the general expressions of a binary quadratic form and its transformation was formulated by Lagrange (1773) as follows\n\nwhich is equivalent to () \"(n=1)\". The theory of binary quadratic forms was considerably expanded by Carl Friedrich Gauss (1798, published 1801) in his Disquisitiones Arithmeticae. He rewrote Lagrange's formalism as follows using integer coefficients α,β,γ,δ:\n\nwhich is equivalent to () \"(n=1)\". As pointed out by Gauss, \"F\" and \"F' \" are called \"proper equivalent\" if αδ-βγ=1, so that \"F\" is contained in \"F' \" as well as \"F' \" is contained in \"F\". In addition, if another form \"F\"\" is contained by the same procedure in \"F' \" it is also contained in \"F\" and so forth.\n\nGauss (1798/1801) also discussed ternary quadratic forms with the general expression\n\nwhich is equivalent to () \"(n=2)\". Gauss called these forms definite when they have the same sign such as \"x+y+z\", or indefinite in the case of different signs such as \"x+y-z\". While discussing the classification of ternary quadratic forms formula_47, Gauss (1801) presented twenty special cases, among them these six variants:\n\nThe determination of all transformations of the Lorentz interval (as a special case of an integer ternary quadratic form) into itself was explicitly worked out by Gauss around 1800 (posthumously published in 1863), for which he provided a coefficient system α,β,γ,δ:\n\nGauss' result was cited by Bachmann (1869), Selling (1873), Bianchi (1888), Leonard Eugene Dickson (1923). The parameters α,β,γ,δ, when applied to spatial rotations, were later called Cayley–Klein parameters.\n\nGauss (1818) discussed planetary motions together with formulating elliptic functions. In order to simplify the integration, he transformed the expression\n\ninto\n\nin which cos \"T\", sin \"T\" are related to cos \"E\", sin \"E\" by the following transformation including an arbitrary constant \"k\", which Gauss then rewrote by setting \"k\"=1:\n\nSubsequently he showed that these relations can be reformulated using three variables \"x,y,z\" and \"u,u′,u″\", so that\n\ncan be transformed into\n\nin which \"x,y,z\" and \"u,u′,u″\" are related by the transformation:\n\nAfter the addition theorem for the tangens hyperbolicus was given by Lambert (1868), hyperbolic geometry was used by Franz Taurinus (1826), and later by Nikolai Lobachevsky (1829/30) and others, to formulate the hyperbolic law of cosines:\n\nFollowing Gauss (1818), Carl Gustav Jacob Jacobi formulated Gauss' transformation using 3 dimensions in 1827 and in 1833:\n\nIn 1832 he used the orthogonal transformation in order to derive the case of 2 dimensions:\n\nAfter Cauchy (1829) formulated the orthogonal transformation for arbitrary dimensions, Jacobi in 1833 used this result to extend his previous formulas:\n\nHe also stated the following linear substitution leaving invariant the Lorentz interval:\n\nAugustin-Louis Cauchy (1829) extended the orthogonal transformation of Euler (1771) to arbitrary dimensions\n\nVictor-Amédée Lebesgue (1837) summarized the previous work of Gauss (1818), Jacobi (1827, 1833), Cauchy 1829. He started with the orthogonal transformation\n\nIn order to achieve the invariance of the Lorentz interval\n\nhe gave the following instructions as to how the previous equations shall be modified: In equation (9) change the sign of the last term of each member. In the first \"n-1\" equations of (10) change the sign of the last term of the left-hand side, and in the one which satisfies α=\"n\" change the sign of the last term of the left-hand side as well as the sign of the right-hand side. In all equations (11) the last term will change sign. In equations (12) the last terms of the right-hand side will change sign, and so will the left-hand side of the \"n\"-th equation. In equations (13) the signs of the last terms of the left-hand side will change, moreover in the \"n\"-th equation change the sign of the right-hand side. In equations (14) the last terms will change sign.\n\nHe went on to redefine the variables of the Lorentz interval and its transformation:\n\nThe Euler–Rodrigues parameters discovered by Euler (1871) and Rodrigues (1840) leaving invariant formula_19 were extended to formula_66 by Arthur Cayley (1846) as a byproduct of what is now called the Cayley transform using the method of skew–symmetric coefficients. Following Cayley's methods, a general transformation for quadratic forms into themselves in three (1853) and arbitrary (1854) dimensions was provided by Hermite (1853, 1854). Hermite's formula was simplified and brought into matrix form equivalent to () by Cayley (1855a)\n\nwhich he abbreviated in 1858, where formula_68 is any skew-symmetric matrix:\n\nUsing the parameters of (1855a), Cayley in a subsequent paper (1855b) particularly discussed several special cases, such as:\n\nor:\n\nor:\n\nAlready in 1854, Cayley published an alternative method of transforming quadratic forms by using certain parameters α,β,γ,δ in relation to a homographic transformation of a surface of second order into itself:\n\nIn the same paper, Cayley also introduced four different parameters \"M=a+b+c+d\" in order to demonstrate the invariance of formula_74, and subsequently showed the relation to quaternions. Fricke & Klein (1897) credited Cayley by calling the above transformation the most general (real or complex) space collineation of first kind of an absolute surface of second kind into itself. Parameters α,β,γ,δ are similar to what was later called Cayley–Klein parameters in relation to spatial rotations (which was done by Cayley in 1879 and before by Hermann von Helmholtz (1866/67)).\n\nIn 1845, Cayley showed that the Euler-Rodrigues parameters in equation () representing rotations can be related to William Rowan Hamilton's quaternions by using a pre- and a postfactor\n\nand in 1848 he used the abbreviated form\n\nIn 1854 he showed how to transform the sum of four squares into itself:\n\nor in 1855:\n\nIn 1859, Cayley found out that a quadratic form or projective quadric can be used as an \"absolute\", serving as the basis of a projective metric (the Cayley–Klein metric). For instance, using the absolute \"x+y+z=0\", he defined the distance of two points as follows\n\nand he also alluded to the case of the unit sphere \"x+y+z=1\". In the hands of Klein (1871), all of this became essential for the discussion of non-Euclidean geometry (in particular the Cayley–Klein or Beltrami–Klein model of hyperbolic geometry) and associated quadratic forms and transformations, including the Lorentz interval and Lorentz transformation.\n\nCayley (1884) himself also discussed some properties of the Beltrami–Klein model and the pseudosphere, and formulated coordinate transformations using the Cayley-Hermite formalism:\n\nCharles Hermite (1853) extended the work of Gauss (1801) and others (including himself) on \"definite\" ternary quadratic forms that can be transformed into ±\"(x+y+z)\", by additionally analyzing \"indefinite\" ternary quadratic forms that can be transformed into the Lorentz interval ±\"(x+y-z)\". By using Cayley's (1846) method of skew–symmetric coefficients, he derived a transformation leaving invariant almost all types of ternary quadratic forms. This was generalized by him in 1854 to all dimensions, so Hermite arrived at a transformation scheme leaving invariant almost all types of quadratic forms:\n\nThis result was subsequently expressed in matrix form by Cayley (1855). Later, Ferdinand Georg Frobenius (1877) added some modifications in order to include some special cases of quadratic forms that cannot be dealt with by the Cayley–Hermite transformation.\n\nFollowing Gauss (1818), Edmond Bour (1856) wrote the transformations:\n\nFollowing Gauss (1818), Jacobi (1827, 1833), and Bour (1856), Osip Ivanovich Somov (1863) wrote the transformation systems:\n\nEugenio Beltrami (1868a) introduced coordinates of the Beltrami–Klein model of hyperbolic geometry, and formulated the corresponding transformations in terms of homographies:\n\nPaul Gustav Heinrich Bachmann (1869) adapted Hermite's (1853/54) transformation of ternary quadratic forms to the case of integer transformations. He particularly analyzed the Lorentz interval and its transformation, and also alluded to the analogue result of Gauss (1800) in terms of Cayley–Klein parameters, while Bachmann formulated his result in terms of the Cayley–Hermite transformation:\n\nHe described this transformation in 1898 in the first part of his \"arithmetics of quadratic forms\" as well.\n\nElaborating on Cayley's (1859) definition of an \"absolute\" (Cayley–Klein metric), Felix Klein (1871) defined a \"fundamental conic section\" in order to discuss motions such as rotation and translation in the non-Euclidean plane, and another fundamental form by using homogeneous coordinates \"x,y\" related to a circle with radius \"2c\" with measure of curvature formula_87. When \"c\" is positive, the measure of curvature is negative and the fundamental conic section is real, thus the geometry becomes hyperbolic (Beltrami–Klein model):\n\nIn (1873) he pointed out that hyperbolic geometry in terms of a surface of constant negative curvature can be related to a quadratic equation, which can be transformed into a sum of squares of which one square has a different sign, and can also be related to the interior of a surface of second degree corresponding to an ellipsoid or two-sheet hyperboloid.\n\nIn (1872) while devising the Erlangen program, Klein discussed the general relation between projective metrics, binary forms and conformal geometry transforming a sphere into itself in terms of linear transformations of the complex variable \"x+iy\". Following Klein, these relations were discussed by Ludwig Wedekind (1875) using formula_92. Klein (1875) then showed that all finite groups of motions follow by determining all finite groups of such linear transformations of \"x+iy\" into itself. In (1878), Klein classified the substitutions of formula_93 with αδ-βγ=1 into hyperbolic, elliptic, parabolic, and in (1882) he added the loxodromic substitution as the combination of elliptic and hyperbolic ones. (In 1890, Robert Fricke in his edition of Klein's lectures of elliptic functions and Modular forms, referred to the analogy of this treatment to the theory of quadratic forms as given by Gauss and in particular Dirichlet.)\n\nIn (1884) Klein related the linear fractional transformations (interpreted as rotations around the \"x+iy\"-sphere) to Cayley–Klein parameters [α,β,γ,δ], to Euler–Rodrigues parameters \"[a,b,c,d]\", and to the unit sphere by means of stereographic projection, and also discussed transformations preserving surfaces of second degree equivalent to the transformation given by Cayley (1854):\n\n\\xi^{2}+\\eta^{2}+\\zeta^{2}-1 & =x_{1}^{2}+x_{2}^{2}+x_{3}^{2}-x_{0}^{2}=0\n</math>.\n\nThe formulas on the right can be related to those on the left by setting\n\nand become equivalent to Lorentz transformation () by setting\n\nIn his lecture in the winter semester of 1889/90 (published 1892–93), he discussed the hyperbolic plane by using (as in 1871) the Lorentz interval in terms of a circle with radius \"2k\" as the basis of hyperbolic geometry, and another quadratic form to discuss the \"kinematics of hyperbolic geometry\" consisting of motions and congruent displacements of the hyperbolic plane into itself:\n\nIn his lecture in the summer semester of 1890 (published 1892–93), he discussed general surfaces of second degree, including an \"oval\" surface corresponding to hyperbolic space and its motions:\n\nIn (1896/97), Klein again defined hyperbolic motions and explicitly used \"t\" as time coordinate:\n\nKlein's work was summarized and extended Fricke (1893, 1897) (supported by Klein).\n\nIn 1890 Klein discussed a general type of Euclidean or Non-Euclidean motion in relation to a problem posed by Helmholtz (1868), with the following transformation\n\nIn 1893 he called the special case with \"a=b=0\" a \"spiral transformation\":\n\nIn relation to line geometry, Klein (1871/72) used coordinates satisfying the condition formula_102. They were introduced in 1868 (belatedly published in 1872/73) by Gaston Darboux as a system of five coordinates in \"R\" (later called \"pentaspherical\" coordinates) in which the last coordinate is imaginary. Sophus Lie (1871) more generally used \"n+2\" coordinates in \"R\" (later called \"polyspherical\" coordinates) satisfying formula_103 in which the last coordinate is imaginary, as a means to discuss conformal transformations generated by inversions. These simultaneous publications can be explained by the fact that Darboux, Lie, and Klein corresponded with each other by letter.\n\nWhen the last coordinate is defined as real, the corresponding polyspherical coordinates satisfy the form of a sphere. Initiated by lectures of Klein between 1889–1890, his student Friedrich Pockels (1891) used such real coordinates, emphasizing that all of these coordinate systems remain invariant under conformal transformations generated by inversions:\n\nSpecial cases were described by Klein (1893):\n\nBoth systems were also described by Maxime Bôcher (1894) in an expanded version of a thesis supervised by Klein.\n\nIn several papers between 1847 and 1850 it was shown by Joseph Liouville that the relation \"λ(δx+δy+δz)\" is invariant under the group of conformal transformations generated by inversions transforming spheres into spheres, which can be related special conformal transformations or Möbius transformations. (The conformal nature of the linear fractional transformation formula_107 of a complex variable formula_108 was already discussed by Euler (1777)).\n\nLiouville's theorem was extended to all dimensions by Sophus Lie (1871a). In addition, Lie described a manifold whose elements can be represented by spheres, where the last coordinate \"y\" can be related to an imaginary radius by \"iy\":\n\nIf the second equation is satisfied, two spheres \"y′\" and \"y″\" are in contact. Lie then defined the correspondence between contact transformations in \"R\" and conformal point transformations in \"R\": The sphere of space \"R\" consists of \"n+1\" parameter (coordinates plus imaginary radius), so if this sphere is taken as the element of space \"R\", it follows that \"R\" now corresponds to \"R\". Therefore, any transformation (to which he counted orthogonal transformations and inversions) leaving invariant the condition of contact between spheres in \"R\", corresponds to the transformation of points in \"R\".\n\nEventually, Lie (1871/72) pointed out that the conformal point transformations of \"R\" consist of motions (such as rigid transformations and orthogonal transformations), similarity transformations, and inversions.\n\nIn (1885/86), Lie identified the projective group of a general surface of second degree formula_110 with the group of non-Euclidean motions. In a thesis guided by Lie, Hermann Werner (1889) discussed this projective group by using the equation of a unit sphere as the surface of second degree, and also gave the corresponding infinitesimal projective transformations:\n\nformula_111\n\nMore generally, Lie (1890) defined non-Euclidean motions in terms of two forms formula_112 in which the imaginary form with formula_113 denotes the group of elliptic motions (in Klein's terminology), the real form with −1 the group of hyperbolic motions, with the latter having the same form as Werner's transformation:\n\nSummarizing, Lie (1893) discussed the real continuous groups of the conic sections representing non-Euclidean motions, which in the case of hyperbolic motions have the form:\n\nContinuing the work of Gauss (1801) on definite ternary quadratic forms and Hermite (1853) on indefinite ternary quadratic forms, Eduard Selling (1873) used the auxiliary coefficients ξ,η,ζ by which a definite form formula_118 and an indefinite form \"f\" can be rewritten in terms of three squares:\n\nIn addition, Selling showed that auxiliary coefficients ξηζ can be geometrically interpreted as point coordinates which are in motion upon one sheet of a two-sheet hyperboloid, which is related to Selling's formalism for the reduction of indefinite forms by using definite forms.\n\nSelling also reproduced the Lorentz transformation given by Gauss (1800/63), to whom he gave full credit, and called it the only example of a particular indefinite ternary form known to him that has ever been discussed:\n\nGustav von Escherich (1874) discussed the plane of constant negative curvature based on the Beltrami–Klein model of hyperbolic geometry by Beltrami (1868), as well as Christoph Gudermann's (1830) rectangular coordinates \"x\"=tan(a) and \"y\"=tan(b) and coordinate transformations using trigonometric functions in the cases of rotation and translation related to sphere geometry. By using hyperbolic functions \"x\"=tanh(a/k) and \"y\"=tanh(b/k), Escherich gave the corresponding coordinate transformations for the hyperbolic plane, which for the case of translation has the form:\n\nWilhelm Killing (1878–1880) described non-Euclidean geometry by using Weierstrass coordinates (named after Karl Weierstrass who described them in lectures in 1872 which Killing attended) obeying the form\n\nor\n\nwhere \"k\" is the reciprocal measure of curvature, \"k\"=∞ denotes Euclidean geometry, \"k\">0 elliptic geometry, and \"k\"<0 hyperbolic geometry. In (1877/78) he pointed out the possibility and some characteristics of a transformation (indicating rigid motions) preserving the above form. In (1879/80) he wrote the corresponding transformations as a general rotation matrix\n\nIn (1885) he wrote the Weierstrass coordinates and their transformation as follows:\n\nIn (1885) he also gave the transformation for \"n\" dimensions:\n\nIn (1885) he applied his transformations to mechanics and defined four-dimensional vectors of velocity and force. Regarding the geometrical interpretation of his transformations, Killing argued in (1885) that by setting \"k\"=-1 and using \"p,x,y\" as rectangular space coordinates, the hyperbolic plane is mapped on one side of a two-sheet hyperboloid \"p-x-y=1\" (known as hyperboloid model), by which the previous formulas become equivalent to Lorentz transformations and the geometry becomes that of Minkowski space. Finally, in (1893) he wrote:\n\nand for \"n\" dimensions\n\nThe case of translation was given by Killing (1893) in the form\n\nIn 1898, Killing wrote that relation in a form similar to Escherich (1874), and derived the corresponding Lorentz transformation for the two cases were \"v\" is unchanged or \"u\" is unchanged:\n\nKilling (1887/88) defined the infinitesimal projective transformations in relation to the following quadratic form of second degree by:\n\nand in (1892) he defined the infinitesimal transformation for non-Euclidean motions in terms of Weierstrass coordinates:\n\nIn (1897/98) he pointed out (see the following formulas on the left) that the corresponding group of non-Euclidean motions in terms of Weierstrass coordinates is intransitive when related to form (1) and transitive when related to form (2), and he also showed (on the right) the relation of Weierstrass coordinates to the notation of Killing (1887/88) and Werner (1889), Lie (1890):\n\nHenri Poincaré (1881) published a work which connected the work of Hermite (1853) and Selling (1873) on indefinite quadratic forms with non-Euclidean geometry (Poincaré already discussed such relations in an unpublished manuscript in 1880). He used two indefinite ternary forms in terms of three squares and then defined them in terms of Weierstrass coordinates (without using that expression) connected by a transformation with integer coefficients:\n\nHe went on to describe the properties of \"hyperbolic coordinates\". Poincaré mentioned the hyperboloid model also in (1887).\n\nPoincaré (1881a) also demonstrated the connection of his above formulas to Möbius transformations:\n\nPoincaré (1881b) also used the Möbius transformation formula_138 in relation to Fuchsian functions and the discontinuous Fuchsian group, being a special case of the hyperbolic group leaving invariant the “fundamental circle” (Poincaré disk model and Poincaré half-plane model of hyperbolic geometry). He then extended Klein's (1878-1882) study on the relation between Möbius transformations and hyperbolic, elliptic, parabolic, and loxodromic substitutions, and while formulating Kleinian groups (1883) he used the following transformation leaving invariant the generalized circle:\n\nIn 1886, Poincaré investigated the relation between indefinite ternary quadratic forms and Fuchsian functions and groups:\n\nHomersham Cox (1881/82) – referring to similar rectangular coordinates used by Gudermann (1830) and George Salmon (1862) on a sphere, and to Escherich (1874) as reported by Johannes Frischauf (1876) in the hyperbolic plane – defined the Weierstrass coordinates (without using that expression) and their transformation:\n\nCox also gave the Weierstrass coordinates and their transformation in hyperbolic space:\n\nThe case of translation was also given by him, where the \"y\"-axis remains unchanged:\n\nSubsequently, Cox (1882/83) also described hyperbolic geometry in terms of an analogue to quaternions and Hermann Grassmann's exterior algebra. To that end, he used hyperbolic numbers, which were first introduced by James Cockle (1848) in the framework of his tessarine algebra as follows:\n\nIn the hands of Cox (who doesn't mention Cockle) this expression becomes a means to transfer point P to point Q in the hyperbolic plane, which he wrote in the form:\n\nIn (1882/83a) he showed the equivalence of \"PQ\"=-cosh(θ)+ι·sinh(θ) with \"quaternion multiplication\", and in (1882/83b) he described \"QP\"=cosh(θ)+ι·sinh(θ) as being \"associative quaternion multiplication\". He also showed that the position of point P in the hyperbolic plane may be determined by three quantities in terms of Weierstrass coordinates obeying the relation \"z-x-y=1\".\n\nCox went on to develop an algebra for hyperbolic space analogous to Clifford's biquaternions. While Clifford (1873) used biquaternions of the form \"a+ωb\" in which ω=0 denotes parabolic space and ω=1 elliptic space, Cox discussed hyperbolic space using the imaginary quantity formula_148 and therefore ω=-1. He also obtained relations of quaternion multiplication in terms of Weierstrass coordinates:\n\nFollowing Gauss (1818), George William Hill (1882) formulated the equations\n\nAfter previous work by Albert Ribaucour (1870), a transformation which transforms oriented spheres into oriented spheres, oriented planes into oriented planes, and oriented lines into oriented lines, was explicitly formulated by Edmond Laguerre (1882) as \"transformation by reciprocal directions\" which was later called „Laguerre inversion/transformation\". It can be seen as a special case of the conformal group in terms of Lie's transformations of oriented spheres. In two dimensions the transformation or oriented lines has the form (\"R\" being the radius):\n\nÉmile Picard (1882) analyzed the invariance of indefinite ternary Hermitian quadratic forms with integer coefficients and their relation to discontinuous groups, extending Poincaré's Fuchsian functions of one complex variable related to a circle, to \"hyperfuchsian\" functions of two complex variables related to a hypersphere. He formulated the following special case of an Hermitian form:\n\nOr in (1884a) in relation to indefinite binary Hermitian quadratic forms:\n\nOr in (1884b):\n\nOr in (1884c):\n\nCyparissos Stephanos (1883) showed that Hamilton's biquaternion \"a+aι+aι+aι\" can be interpreted as an oriented sphere in terms of Lie's sphere geometry (1871), having the vector \"aι+aι+aι\" as its center and the scalar formula_158 as its radius. Its norm formula_159 is thus equal to the power of a point of the corresponding sphere. In particular, the norm of two quaternions \"N(Q-Q)\" (the corresponding spheres are in contact with \"N(Q-Q)=0\") is equal to the tangential distance between two spheres. The general contact transformation between two spheres then can be given by a homography using 4 arbitrary quaternions \"A,B,C,D\" and two variable quaternions \"X,Y\":\n\nStephanos pointed out that the special case \"A=0\" denotes transformations of oriented planes (see Laguerre's transformation of oriented planes (1882)).\n\nArthur Buchheim (1884, published 1885) applied Clifford's biquaternions and their operator ω to different forms of geometries (Buchheim mentions Cox (1882) as well). He defined the scalar \"ω=e\" which in the case -1 denotes hyperbolic space, 1 elliptic space, and 0 parabolic space. He derived the following relations consistent with the Cayley–Klein absolute:\n\nFollowing Gauss (1818) and Hill (1882), Octave Callandreau (1885) formulated the equations\n\nClifford algebra (which includes quaternions as special cases) was used by Rudolf Lipschitz (1885/86) who introduced the orthogonal transformation ΛX=YΛ of a definite quadratic form as a sum or squares formula_164 into itself, which he discussed both for real as well as imaginary expressions. He then discussed the general indefinite form and its transformation by using \"m\" real and \"n-m\" imaginary quantities:\n\nFriedrich Schur (1885/86) discussed spaces of constant Riemann curvature, and by following Beltrami (1868) he used the transformation\n\nIn (1900/02) he derived basic formulas of non-Eucliden geometry, including the case of translation for which he obtained the transformation similar to his previous one:\n\nwhere formula_168 can have values >0, <0 or ∞.\n\nHe also defined the triangle\n\nFollowing Laguerre (1882), Gaston Darboux (1887) presented the Laguerre inversions in four dimensions using coordinates \"x,y,z,R\":\n\nRelated to Klein's (1871) and Poincaré's (1881-1887) work on non-Euclidean geometry and indefinite quadratic forms, Luigi Bianchi (1888) analyzed the differential Lorentz interval \"ds=dx+dy-dz\", alluded to the Möbius transformations and its parameters α,β,γ,δ in order to preserve the Lorentz interval, and gave credit to Gauss (1800/63) who obtained the same coefficient system:\n\nIn 1893, Bianchi gave the coefficients in the case of four dimensions:\n\nSolving for formula_175 Bianchi obtained:\n\nFerdinand von Lindemann discussed hyperbolic geometry in his (1890/91) edition of the lectures on geometry of Alfred Clebsch. Citing Killing (1885) and Poincaré (1887) in relation to the hyperboloid model in terms of Weierstrass coordinates for the hyperbolic plane and space, he set\n\nIn addition, following Klein (1871) he employed the Cayley absolute related to surfaces of second degree, by using the following quadratic form and its transformation\n\ninto which he put\n\nFrom that, he obtained the following Cayley absolute and the corresponding most general motion in hyperbolic space comprising ordinary rotations (\"a\"=0) or translations (α=0):\n\nRobert Fricke (1891) – following the work of his teacher Klein (1878–1882) as well as Poincaré (1881–1887) on automorphic functions and group theory – obtained the following transformation for an integer ternary quadratic form\n\nAnd the general case of four dimensions in 1893:\n\nSupported by Felix Klein, Fricke summarized his and Klein's work in a treatise concerning automorphic functions (1897). Using a sphere as the absolute, in which the interior of the sphere is denoted as hyperbolic space, they defined hyperbolic motions, and stressed that any hyperbolic motion corresponds to \"circle relations\" (now called Möbius transformations):\n\nLouis Gérard (1892) – in a thesis examined by Poincaré – discussed Weierstrass coordinates (without using that name) in the plane using the following invariant and its Lorentz transformation equivalent to () \"(n=2)\":\n\nHe gave the case of translation as follows:\n\nAlexander Macfarlane (1892, 1893) – similar to Cockle (1848) and Cox (1882/83) – defined the hyperbolic versor in terms of hyperbolic numbers\n\nand in 1894 he defined the \"exspherical\" versor\n\nand used them to analyze hyperboloids of one- or two sheets. This was further extended by him in (1900) in order to express trigonometry in terms of hyperbolic quaternions \"re\", with β=+1 and formula_189, the hyperbolic number \"x+yβ\", and the hyperbolic versor \"e\".\n\nAlfred North Whitehead (1898) discussed the kinematics of hyperbolic space as part of his study of universal algebra, and obtained the following transformation:\n\nFelix Hausdorff (1899) – citing Killing (1885) – discussed Weierstrass coordinates in the plane using the following invariant and its transformation:\n\nHausdorff (1899) also discussed the relation of the above coordinates to conformal Möbius transformations:\n\nModifying Lipschitz's (1885/86) variant of Clifford numbers, Theodor Vahlen (1901/02) formulated Möbius transformations (which he called vector transformations) and biquaternions in order to discuss motions in n-dimensional non-Euclidean space, leaving the following quadratic form invariant (where \"j\"=1 represents hyperbolic motions, \"j\"=-1 elliptic motions, \"j\"=0 parabolic motions):\n\nFrederick S. Woods (1901/02) defined the following invariant quadratic form and its projective transformation (he pointed out that this can be connected to hyperbolic geometry by setting formula_194 with \"R\" as real quantity):\n\nAlternatively, Woods (1903, published 1905) – citing Killing (1885) – used the invariant quadratic form in terms of Weierstrass coordinates and its transformation (with formula_196 for hyperbolic space):\n\nand the case of translation:\n\nand the loxodromic substitution for hyperbolic space:\n\nHeinrich Liebmann (1904/05) – citing Killing (1885), Gérard (1892), Hausdorff (1899) – used the invariant quadratic form and its Lorentz transformation equivalent to () \"(n=2)\"\n\nand the case of translation:\n\nWoldemar Voigt (1887) developed a transformation in connection with the Doppler effect and an incompressible medium, being in modern notation:\n\nIf the right-hand sides of his equations are multiplied by γ they are the modern Lorentz transformation (). In Voigt's theory the speed of light is invariant, but his transformations mix up a relativistic boost together with a rescaling of space-time. Optical phenomena in free space are scale, conformal (using the factor λ discussed above), and Lorentz invariant, so the combination is invariant too. For instance, Lorentz transformations can be extended by using formula_203:\n\n\"l\"=1/γ gives the Voigt transformation, \"l\"=1 the Lorentz transformation. But scale transformations are not a symmetry of all the laws of nature, only of electromagnetism, so these transformations cannot be used to formulate a principle of relativity in general. It was demonstrated by Poincaré and Einstein that one has to set \"l\"=1 in order to make the above transformation symmetric and to form a group as required by the relativity principle, therefore the Lorentz transformation is the only viable choice.\n\nAlso Hermann Minkowski said in 1908 that the transformations which play the main role in the principle of relativity were first examined by Voigt in 1887. Voigt responded in the same paper by saying that his theory was based on an elastic theory of light, not an electromagnetic one. However, he concluded that some results were actually the same.\n\nIn 1888, Oliver Heaviside investigated the properties of charges in motion according to Maxwell's electrodynamics. He calculated, among other things, anisotropies in the electric field of moving bodies represented by this formula:\n\nConsequently, Joseph John Thomson (1889) found a way to substantially simplify calculations concerning moving charges by using the following mathematical transformation (like other authors such as Lorentz or Larmor, also Thomson implicitly used the Galilean transformation \"z-vt\" in his equation):\n\nThereby, inhomogeneous electromagnetic wave equations are transformed into a Poisson equation. Eventually, George Frederick Charles Searle noted in (1896) that Heaviside's expression leads to a deformation of electric fields which he called \"Heaviside-Ellipsoid\" of axial ratio\n\nIn order to explain the aberration of light and the result of the Fizeau experiment in accordance with Maxwell's equations, Lorentz in 1892 developed a model (\"Lorentz ether theory\") in which the aether is completely motionless, and the speed of light in the aether is constant in all directions. In order to calculate the optics of moving bodies, Lorentz introduced the following quantities to transform from the aether system into a moving system (it's unknown whether he was influenced by Voigt, Heaviside, and Thomson)\n\nwhere \"x\" is the Galilean transformation \"x-vt\". Except the additional γ in the time transformation, this is the complete Lorentz transformation (). While \"t\" is the \"true\" time for observers resting in the aether, \"t′\" is an auxiliary variable only for calculating processes for moving systems. It is also important that Lorentz and later also Larmor formulated this transformation in two steps. At first an implicit Galilean transformation, and later the expansion into the \"fictitious\" electromagnetic system with the aid of the Lorentz transformation. In order to explain the negative result of the Michelson–Morley experiment, he (1892b) introduced the additional hypothesis that also intermolecular forces are affected in a similar way and introduced length contraction in his theory (without proof as he admitted). The same hypothesis was already made by George FitzGerald in 1889 based on Heaviside's work. While length contraction was a real physical effect for Lorentz, he considered the time transformation only as a heuristic working hypothesis and a mathematical stipulation.\n\nIn 1895, Lorentz further elaborated on his theory and introduced the \"theorem of corresponding states\". This theorem states that a moving observer (relative to the ether) in his „fictitious\" field makes the same observations as a resting observers in his „real\" field for velocities to first order in \"v/c\". Lorentz showed that the dimensions of electrostatic systems in the ether and a moving frame are connected by this transformation:\n\nFor solving optical problems Lorentz used the following transformation, in which the modified time variable was called \"local time\" () by him:\n\nWith this concept Lorentz could explain the Doppler effect, the aberration of light, and the Fizeau experiment.\n\nIn 1897, Larmor extended the work of Lorentz and derived the following transformation\n\nLarmor noted that if it is assumed that the constitution of molecules is electrical then the FitzGerald–Lorentz contraction is a consequence of this transformation, explaining the Michelson–Morley experiment. It's notable that Larmor was the first who recognized that some sort of time dilation is a consequence of this transformation as well, because \"individual electrons describe corresponding parts of their orbits in times shorter for the [rest] system in the ratio 1/γ\". Larmor wrote his electrodynamical equations and transformations neglecting terms of higher order than \"(v/c)\" – when his 1897 paper was reprinted in 1929, Larmor added the following comment in which he described how they can be made valid to all orders of \"v/c\":\n\nIn line with that comment, in his book Aether and Matter published in 1900, Larmor used a modified local time \"t″=t′-εvx′/c\" instead of the 1897 expression \"t′=t-vx/c\" by replacing \"v/c\" with \"εv/c\", so that \"t″\" is now identical to the one given by Lorentz in 1892, which he combined with a Galilean transformation for the \"x′, y′, z′, t′\" coordinates:\n\nLarmor knew that the Michelson–Morley experiment was accurate enough to detect an effect of motion depending on the factor \"(v/c)\", and so he sought the transformations which were \"accurate to second order\" (as he put it). Thus he wrote the final transformations (where \"x′=x-vt\" and \"t″\" as given above) as:\n\nby which he arrived at the complete Lorentz transformation (). Larmor showed that Maxwell's equations were invariant under this two-step transformation, \"to second order in \"v/c\"\" – it was later shown by Lorentz (1904) and Poincaré (1905) that they are indeed invariant under this transformation to all orders in \"v/c\".\n\nLarmor gave credit to Lorentz in two papers published in 1904, in which he used the term \"Lorentz transformation\" for Lorentz's first order transformations of coordinates and field configurations:\n\nAlso Lorentz extended his theorem of corresponding states in 1899. First he wrote a transformation equivalent to the one from 1892 (again, \"x\"* must be replaced by \"x-vt\"):\n\nThen he introduced a factor formula_216 of which he said he has no means of determining it, and modified his transformation as follows (where the above value of \"t′\" has to be inserted):\n\nThis is equivalent to the complete Lorentz transformation () when solved for \"x″\" and \"t″\" and with ε=1. Like Larmor, Lorentz noticed in 1899 also some sort of time dilation effect in relation to the frequency of oscillating electrons \"\"that in \"S\" the time of vibrations be \"kε\" times as great as in \"S\"\"\", where \"S\" is the aether frame.\n\nIn 1904 he rewrote the equations in the following form by setting \"l\"=1/ε (again, \"x\"* must be replaced by \"x-vt\"):\n\nUnder the assumption that \"l=1\" when \"v\"=0, he demonstrated that \"l=1\" must be the case at all velocities, therefore length contraction can only arise in the line of motion. So by setting the factor \"l\" to unity, Lorentz's transformations now assumed the same form as Larmor's and are now completed. Unlike Larmor, who restricted himself to show the covariance of Maxwell's equations to second order, Lorentz tried to widen its covariance to all orders in \"v/c\". He also derived the correct formulas for the velocity dependence of electromagnetic mass, and concluded that the transformation formulas must apply to all forces of nature, not only electrical ones. However, he didn't achieve full covariance of the transformation equations for charge density and velocity. When the 1904 paper was reprinted in 1913, Lorentz therefore added the following remark:\n\nLorentz's 1904 transformation was cited and used by Alfred Bucherer in July 1904:\n\nor by Wilhelm Wien in July 1904:\n\nor by Emil Cohn in November 1904 (setting the speed of light to unity):\n\nor by Richard Gans in February 1905:\n\nNeither Lorentz or Larmor gave a clear physical interpretation of the origin of local time. However, Henri Poincaré in 1900 commented on the origin of Lorentz’s \"wonderful invention\" of local time. He remarked that it arose when clocks in a moving reference frame are synchronised by exchanging signals which are assumed to travel with the same speed formula_223 in both directions, which lead to what is nowadays called relativity of simultaneity, although Poincaré's calculation does not involve length contraction or time dilation. In order to synchronise the clocks here on Earth (the \"x*, t\"* frame) a light signal from one clock (at the origin) is sent to another (at \"x\"*), and is sent back. It's supposed that the Earth is moving with speed \"v\" in the \"x\"-direction (= \"x\"*-direction) in some rest system (\"x, t\") (\"i.e.\" the luminiferous aether system for Lorentz and Larmor). The time of flight outwards is\n\nand the time of flight back is\n\nThe elapsed time on the clock when the signal is returned is formula_226 and the time formula_227 is ascribed to the moment when the light signal reached the distant clock. In the rest frame the time formula_228 is ascribed to that same instant. Some algebra gives the relation between the different time coordinates ascribed to the moment of reflection. Thus\n\nidentical to Lorentz (1892). By dropping the factor γ under the assumption that formula_230, Poincaré gave the result formula_231, which is the form used by Lorentz in 1895.\n\nSimilar physical interpretations of local time were later given by Emil Cohn (1904) and Max Abraham (1905).\n\nOn June 5, 1905 (published June 9) Poincaré simplified the equations which are algebraically equivalent to those of Larmor and Lorentz and gave them the modern form ():\n\nApparently Poincaré was unaware of Larmor's contributions, because he only mentioned Lorentz and therefore used for the first time the name \"Lorentz transformation\". Poincaré set the speed of light to unity, pointed out the group characteristics of the transformation by setting \"l\"=1, and modified/corrected Lorentz's derivation of the equations of electrodynamics in some details in order to fully satisfy the principle of relativity, \"i.e.\" making them fully Lorentz covariant.\n\nIn July 1905 (published in January 1906) Poincaré showed in detail how the transformations and electrodynamic equations are a consequence of the principle of least action; he demonstrated in more detail the group characteristics of the transformation, which he called Lorentz group, and he showed that the combination formula_233 is invariant. He noticed that the Lorentz transformation is merely a rotation in four-dimensional space about the origin by introducing formula_234 as a fourth imaginary coordinate, and he used an early form of four-vectors.\n\nOn June 30, 1905 (published September 1905) Einstein published what is now called special relativity and gave a new derivation of the transformation, which was based only on the principle on relativity and the principle of the constancy of the speed of light. While Lorentz considered \"local time\" to be a mathematical stipulation device for explaining the Michelson-Morley experiment, Einstein showed that the coordinates given by the Lorentz transformation were in fact the inertial coordinates of relatively moving frames of reference. For quantities of first order in \"v/c\" this was also done by Poincaré in 1900, while Einstein derived the complete transformation by this method. Unlike Lorentz and Poincaré who still distinguished between real time in the aether and apparent time for moving observers, Einstein showed that the transformations concern the nature of space and time.\n\nThe notation for this transformation is equivalent to Poincaré's of 1905 and (), except that Einstein didn't set the speed of light to unity:\n\nEinstein also defined the velocity addition formula () (which also has been done by Poincaré in May 1905 in unpublished letters to Lorentz):\n\nThe work on the principle of relativity by Lorentz, Einstein, Planck, together with Poincaré's four-dimensional approach, were further elaborated and combined with the hyperboloid model by Hermann Minkowski in 1907 and 1908. Minkowski particularly reformulated electrodynamics in a four-dimensional way (Minkowski spacetime). For instance, he wrote \"x, y, z, it\" in the form \"x, x, x, x\". By defining formula_237 as the angle of rotation around the formula_108-axis, the Lorentz transformation assumes a form (with \"c\"=1) in agreement with ():\n\nEven though Minkowski used the imaginary number iψ, he for once directly used the tangens hyperbolicus in the equation for velocity\n\nMinkowski's expression can also by written as ψ=atanh(q) and was later called rapidity. He also wrote the Lorentz transformation in matrix form equivalent to () formula_242:\n\nAs a graphical representation of the Lorentz transformation he introduced the Minkowski diagram, which became a standard tool in textbooks and research articles on relativity:\n\nUsing an imaginary rapidity such as Minkowski, Arnold Sommerfeld (1909) formulated a transformation equivalent to Lorentz boost (), and the relativistc velocity addition () in terms of trigonometric functions and the spherical law of cosines:\n\nformula_244\n\nIn line with Lie's (1871) research on the relation between sphere transformations with an imaginary radius coordinate and 4D conformal transformations, it was pointed out by Bateman and Cunningham (1909–1910), that by setting \"u=ict\" as the imaginary fourth coordinates one can produce spacetime conformal transformations. Not only the quadratic form \"λ(δx+δy+δz+δu)\", but also Maxwells equations are covariant with respect to these transformations, irrespective of the choice of λ. These variants of conformal or Lie sphere transformations were called spherical wave transformations by Bateman. However, this covariance is restricted to certain areas such as electrodynamics, whereas the totality of natural laws in inertial frames is covariant under the Lorentz group. In particular, by setting λ=1 the Lorentz group can be seen as a 10-parameter subgroup of the 15-parameter spacetime conformal group .\n\nBateman (1910/12) also alluded to the identity between the Laguerre inversion and the Lorentz transformations. In general, the isomorphism between the Laguerre group and the Lorentz group was pointed out by Élie Cartan (1912, 1915/55), Henri Poincaré (1912/21) and others.\n\nFollowing Klein (1889–1897) and Fricke & Klein (1897) concerning the Cayley absolute, hyperbolic motion and its transformation, Gustav Herglotz (1909/10) classified the one-parameter Lorentz transformations as loxodromic, hyperbolic, parabolic and elliptic. The general case (on the left) equivalent to Lorentz transformation () and the hyperbolic case (on the right) equivalent to Lorentz transformation () are as follows:\n\nFollowing Sommerfeld (1909), hyperbolic functions were used by Vladimir Varićak in several papers starting from 1910, who represented the equations of special relativity on the basis of hyperbolic geometry in terms of Weierstrass coordinates. For instance, by setting \"l=ct\" and \"v/c=tanh(u)\" with \"u\" as rapidity he wrote the Lorentz transformation in agreement with ():\n\nVarićak also related the velocity addition to the hyperbolic law of cosines:\n\nSubsequently, other authors such as E. T. Whittaker (1910) or Alfred Robb (1911, who coined the name rapidity) used similar expressions, which are still used in modern textbooks.\n\nWhile earlier derivations and formulations of the Lorentz transformation relied from the outset on optics, electrodynamics, or the invariance of the speed of light, Vladimir Ignatowski (1910) showed that it is possible to use the principle of relativity (and related group theoretical principles) alone, in order to derive the following transformation between two inertial frames:\n\nThe variable \"n\" can be seen as a space-time constant whose value has to be determined by experiment or taken from a known physical law such as electrodynamics. For that purpose, Ignatowski used the above-mentioned Heaviside ellipsoid representing a contraction of electrostatic fields by \"x\"/γ in the direction of motion. It can be seen that this is only consistent with Ignatowski's transformation when \"n=1/c\", resulting in \"p\"=γ and the Lorentz transformation (). With \"n\"=0, no length changes arise and the Galilean transformation follows. Ignatowski's method was further developed and improved by Philipp Frank and Hermann Rothe (1911, 1912), with various authors developing similar methods in subsequent years.\n\nIn an appedix to Klein's and Sommerfeld's \"Theory of the top\" (1910), Fritz Noether showed how to formulate hyperbolic rotations using biquaternions with formula_249, which he also related to the speed of light by setting ω=-\"c\". He concluded that this is the principal ingredient for a rational representation of the group of Lorentz transformations equivalent to ():\n\nformula_250\n\nBesides citing quaternion related standard works such as Cayley (1854), Noether referred to the entries in Klein's encyclopedia by Eduard Study (1899) and the French version by Élie Cartan (1908). Cartan's version contains a description of Study's dual numbers, Clifford's biquaternions (including the choice formula_249 for hyperbolic geometry), and Clifford algebra, with references to Stephanos (1883), Buchheim (1884/85), Vahlen (1901/02) and others.\n\nAlready in 1908, while describing \"Drehstreckungen\" (orthogonal substitutions in terms of rotations leaving invariant a quadratic form up to a factor) by using Cayley's (1854) quaternion multiplication formalism, Felix Klein pointed out that the modern principle of relativity as provided by Minkowski is essentially only the consequent application of such Drehstreckungen, even though he didn't provide details. Citing Noether, in August 1910 Klein published the following quaternion substitutions forming the group of Lorentz transformations:\n\nor in March 1911\n\nIndependently, also Arthur W. Conway in February 1911 succeeded in combining quaternions and relativity (where formula_254 is the force and formula_255 the charge)\n\nAlso Ludwik Silberstein in November 1911 as well as in 1914, succeeded in combining quaternions and relativity\n\nSilberstein cites Cayley (1854, 1855) and Study's encyclopedia entry (in the extended French version of Cartan in 1908), as well as the appendix of Klein's and Sommerfeld's book.\n\nGustav Herglotz (1911) showed how to formulate the transformation equivalent to () in order to allow for arbitrary velocities and coordinates formula_258 and formula_259:\n\nThis was simplified using vector notation by Ludwik Silberstein (1911 on the left, 1914 on the right):\n\nEquivalent formulas were also given by Wolfgang Pauli (1921), with Erwin Madelung (1922) providing the matrix form\n\nThese formulas were called \"general Lorentz transformation without rotation\" by Christian Møller (1952), who in addition gave an even more general Lorentz transformation in which the Cartesian axes have different orientations, using a rotation operator formula_263. In this case, formula_264 is not equal to formula_265, but the relation formula_266 holds instead, with the result\n\nBorel (1913) started by demonstrating Euclidean motions using Euler-Rodrigues parameter in three dimensions, and Cayley's (1846) parameter in four dimensions. Then he demonstrated the connection to indefinite quadratic forms expressing hyperbolic motions and Lorentz transformations. In three dimensions equivalent to ():\n\nIn four dimensions equivalent to ():\n\nIn pursuing the history in years before Lorentz enunciated his expressions, one looks to the essence of the concept. In mathematical terms, Lorentz transformations are squeeze mappings, the linear transformations that turn a square into a rectangles of the same area. Before Euler, the squeezing was studied as quadrature of the hyperbola and led to the hyperbolic logarithm. In 1748 Euler issued his precalculus textbook where the number e is exploited for trigonometry in the unit circle. The first volume of Introduction to the Analysis of the Infinite had no diagrams, allowing teachers and students to draw their own illustrations.\n\nThere is a gap in Euler's text where Lorentz transformations arise. A feature of natural logarithm is its interpretation as area in hyperbolic sectors. In relativity the classical concept of velocity is replaced with rapidity, a hyperbolic angle concept built on hyperbolic sectors. A Lorentz transformation is a hyperbolic rotation which preserves differences of rapidity, just as the circular sector area is preserved with a circular rotation. Euler’s gap is the lack of hyperbolic angle and hyperbolic functions, later developed by Johann H. Lambert. Euler described some transcendental functions: exponetiation and circular functions. He used the exponential series formula_270 With the imaginary unit i = – 1, and splitting the series into even and odd terms, he obtained\nThis development misses the alternative:\nHere Euler could have noted split-complex numbers along with complex numbers.\n\nFor physics, one space dimension is insufficient. But to extend split-complex arithmetic to four dimensions leads to hyperbolic quaternions, and opens the door to abstract algebra’s \nhypercomplex numbers. Reviewing the expressions of Lorentz and Einstein, one observes that the Lorentz factor is an algebraic function of velocity. For readers uncomfortable with transcendental functions cosh and sinh, algebraic functions may be more to their liking.\n\n\n\n\n\n"}
{"id": "35473263", "url": "https://en.wikipedia.org/wiki?curid=35473263", "title": "Human rights movement", "text": "Human rights movement\n\nHuman rights movement refers to a nongovernmental social movement engaged in activism related to the issues of human rights. The foundations of the global human rights movement involve resistance to: colonialism, imperialism, slavery, racism, apartheid, patriarchy, and oppression of indigenous peoples.\n\nA key principle of the human rights movement is its appeal to universality: the idea that all human beings should struggle in solidarity for a common set of basic conditions that has to be followed by all.\n\nHuman rights activism predates the 20th century, and includes, for example, the anti-slavery movement. Historical movements were usually concerned with a limited set of issues, and they were more local than global. One account identifies the 1899 Hague Convention as a starting point for the idea that humans have rights independent of the states that control them.\n\nThe activities of the International Federation for Human Rights (originally the International Labor Organization)—founded in France by the international labor movement in the 1920s—can be seen as a precursor to the modern movements. This organization was quickly embraced by the United States and European powers, perhaps as a way to counteract the Bolshevik call for global solidarity among workers.\n\nAnother major global human rights movement grew out of resistance to colonialism. The Congo Reform Association, founded in 1904, has also been described as a foundational modern human rights movement. This group used photographs to document terror wrought by Belgians in the course of demanding rubber production in the Congo. These photographs were passed among sympathetic Europeans and Americans, including Edmund Morel, Joseph Conrad, and Mark Twain—who wrote satirically as King Leopold:\n\n...oh well, the pictures get sneaked around everywhere, in spite of all we can do to ferret them out and suppress them. Ten thousand pulpits and ten thousand presses are saying the good word for me all the time and placidly and convincingly denying the mutilations. Then that trivial little kodak, that a child can carry in its pocket, gets up, uttering never a word, and knocks them dumb!\n\nThe photos and subsequent literature triggered international outrage at Belgian crimes committed against the Congolese.\n\nAs the century went on, African Americans including W. E. B. Du Bois, Walter White, and Paul Robeson joined with leaders of the African diaspora (from Haiti, Liberia, the Philippines, and elsewhere) to make a global demand for basic rights. Although the origins of this movement were multifaceted (owing strength both to the capitalist Marcus Garvey and to the more left-wing African Blood Brotherhood), a definitive moment of international solidarity came after Italy's annexation of Ethiopia in 1935.\n\nIn the aftermath of World War II, the Pan-Africanist contingent played a major role in causing the United Nations to explicitly protect \"human rights\" in its founding documents. Du Bois compared colonies across the world to ghettos in the United States and called for a world document affirming the human rights of all people.\n\nRepresentatives of small countries (particularly from Latin America), as well as Du Bois and other activists, were unhappy with the version of human rights envisioned for the UN Charter at Dumbarton Oaks in 1944. Du Bois stated at the time that, evidently, \"the only way to human equality is through the philanthropy of the masters\". However, the US government supported powerful domestic organizations willing to promote its concept of human rights, such as the American Bar Association and the American Jewish Committee. These organizations won public approval of the United Nations and the human rights concept.\n\nThe concept of human rights was indeed built into United Nations with institutions such as the United Nations Commission on Human Rights and the Universal Declaration of Human Rights. Active diplomacy by Latin American countries was instrumental to the process of promoting these ideas and drafting the relevant agreements. As a result of this pressure, more human rights language was adopted at the 1945 San Francisco Conference to create the UN Charter. Revelations about the Holocaust, followed by the Nuremberg Trials, also had a major influence on the movement, particularly among Jewish and Christian lobbying groups. Some NGO's represented the UN charter as a victory for the human rights movement, while other activists argued that it paid lipservice to human rights while basically serving the interests of the great powers.\n\nEarly in the Cold War, the \"human rights\" concept was used to promote the ideological agendas of the superpowers. The Soviet Union argued that people in colonized lands around the world had been exploited by Western powers. A large percentage of Soviet propaganda to the Third World centered on charges of racism and human rights violations. The United States countered with its own propaganda, describing its own society as free and the Soviet Union's as unfree. Human rights language became an international standard, which could be used by great powers or by people's movements to make demands.\n\nWithin the United States, participants in the Civil Rights Movement called for human rights in addition to civil rights. Du Bois, the National Negro Congress (NNC), the National Association for the Advancement of Colored People (NAACP), the Civil Rights Congress (CRC), and other activists, soon began charging the U.S. with human rights violations at the U.N. In 1951, Du Bois, William Patterson and the CRC presented a document called \"We Charge Genocide\" which accused the US of complicity with ongoing systematic violence against African Americans.\n\nAn Appeal for Human Rights, published by Atlanta students in 1960, is cited as a key moment in beginning the wave of nonviolent direct actions that swept the American South. In 1967, less than a year before he was assassinated, Martin Luther King, Jr. began to argue that the concept of \"civil rights\" was laden with isolating, individualistic capitalist values. He said: \"It is necessary for us to realize that we have moved from the era of civil rights to the era of human rights. When you deal with human rights you are not dealing with something clearly defined in the Constitution. They are rights that are clearly defined by the mandates of a humanitarian concern.\" For King, who began to organize the multi-racial Poor People's Campaign at the end of his life, human rights required economic justice in addition to de jure equality.\n\nAfter the decolonization of Africa and of Asia, former colonies gained majority status in the UN's Commission on Human Rights, and focused their attention on global white supremacy and economic inequality—in doing so, choosing to admit other types of human rights abuses. Some of these nations argued that focusing on civil rights, as opposed to human rights, was a privilege available only to the wealthy nations that had benefited from colonialism. Demands for human rights in the Third World increased throughout the 1960s, even as the global superpowers turned their attention elsewhere.\n\nSince the 1970s the human rights movement has played an increasingly important role on the international scene. Although government support for human rights decreased, international organizations increased in strength and number. Some of the events of the 1970s, which gave global prominence to the human movements issue, included the abuses of Chilean Augusto Pinochet and American Richard Nixon administrations; the signing of the Helsinki Accords (1975) between the West and the USSR; the Soweto riots in South Africa; awarding of the Nobel Peace Prize to Amnesty International (1977); and the emergence of the Democracy Wall movement in China. Nixon was succeeded by the Jimmy Carter administration, much more supportive of the human rights issues.\n\nPressure from the international human rights movement brought human rights increasingly to the political agenda of numerous countries and diplomatic negotiations. As the issue of human rights became important for dissidents in the Eastern Bloc (Soviet human rights movement, Charter 77, Workers' Defence Committee), this period also saw a growing reframing of the struggle between the West and USSR from the economic terms (\"communism versus free market\") into a struggle for human rights (\"totalitarianism versus liberty\"). Since the end of the Cold War, the issues of human rights have been present in a number of major political and military conflicts, debated by global public opinion, from Kosovo to Iraq, Afghanistan, Congo and Darfur.\n\nOriginally, most international human rights organizations came from France and the UK; since the 1970s American organizations moved beyond rights for Americans to partake in the international scene, and around the turn of the century, as noted by Neier, \"the movement became so global in character that it is no longer possible to ascribe leadership to any particular [national or regional] segment\". However, others, like Ibhawoh, point out that there still is a gap between regions, particularly as most of the international human rights movement organizations are located in the global North, and thus continuous concerns are raised about their understanding of the situations in the global South.\n\nThe global human rights movement has become more expansive since the 1990s, including greater representation of women's rights and economic justice as part of the human rights umbrella. Economic, social and cultural (ESC) rights gained new prominence.\n\nAdvocates for women's human rights (sometimes identifying as part of the feminist movement), criticized the early human rights movement for focusing on male concerns and artificially excluding women's issues from the public sphere. Women's rights have nevertheless gained prominence in the international human rights movement, particularly insofar as they include protection from gender-based violence. In Latin America, the issue of women's human rights intersects with the struggle against authoritarian governments. In many cases (see: the Mothers of the Plaza de Mayo) women's groups were some of the most prominent advocates of human rights in general. Mainstream acceptance of women's human rights within the international human rights movement has increased since 1989.\n\nThe authority of the United Nations human rights framework diminished in the 1990s, partly due to the emphasis on economic liberalization that followed the Cold War.\n\nThe 1990s also saw a call to \"defend the defenders\" of human rights—to protect human rights activists from violence and repression. Unfortunately, there has been an increase in the number of attacks on the activists The movement has come to a standstill as individuals continue to push for liberation but are unable to report their findings out of fear of harm or death. The number of female activists has been growing since the beginning of the Feminist movement however, there have been increased number of attacks on women. Recently, the Taliban targeted the female activists to send a message.\n\nThe internet has expanded the power of the human rights movement by improving communication between activists in different physical locations. This is known as mediated mobilization. Individuals who are using their voices to communicate about the injustices are now able to communicate with like minded people who use their voices through participatory journalism.\n\nThe human rights movement has historically focused on abuses by states, and some have argued that it has not attended closely enough to the actions of corporations. In the 1990s, some first steps were taken towards holding corporations accountable for human rights abuses. For example, the Parliament of Britain approved a resolution to censure British Petroleum for funding Colombian death squads.\n\nThe international human rights movement is concerned with issues such as deprivation of life and liberty, deprivation of the right of free and peaceful expressions, gatherings and worship, equal treatment regardless of individual background, and opposition to unjust and cruel practices such as torture.<ref name=\"Neier2012-2-3\"/ \nOther issues include opposition to the death penalty and to child labor.\n\nMuch of the human rights movement is local in nature, concerned with human rights violations in their own countries, but they rely on an international network of support. The international nature of the movement allows local activists to broadcast their concerns, sometimes generating international pressure on their home government. The movement generally espouses the principle that sovereignty ends where human rights begin. This principle justifies intervention across borders to rectify perceived violations.\n\nThe human rights movement is also credited with supplying local activists with a vocabulary to use in support of their claims.\n\nOne major schism within the international human rights movement has been between NGOs and activists from the First and Third Worlds. Critics of the mainstream movement have argued that it suffers from systemic biases and is unwilling to confront inequality on a global scale. In particular, some critique the role of neoliberal capitalism in creating economic conditions that engender 'human rights violations', arguing that the dominant human rights movement is blind to these dynamics. (\"See also:\" structural adjustment.) Makau Mutua has written:\n\nAs currently constituted and deployed, the human rights movement will ultimately fail because it is perceived as an alien ideology in non-Western societies. The movement does not deeply resonate in the cultural fabrics of non-Western states, except among hypocritical elites steeped in Western ideas. In order ultimately to prevail, the human rights movement must be moored in the cultures of all peoples.\n\nDavid Kennedy has criticized a tendency of the international human rights movement to \"treat human rights as an object of devotion rather than calculation\", arguing that human rights language is vague and may impede utilitarian assessments of a situation. Kennedy also argues that this vocabulary can be \"misused, distorted, or co-opted\", and that framing issues in terms of human rights may narrow the field of possibility and exclude other narratives. Others have also critiqued the movement and its language as vague.\n\nSome have argued that the human rights movement has a tendency to subtly debase people by portraying them as victims of abuse.\n\nParticularly since the 1970s, the international human rights movement has been mediated by nongovernmental organizations (NGOs).\n\nMajor international human rights organizations include Amnesty International and Human Rights Watch.\n\nHistorically, the influence of the International Federation for Human Rights is seen as highly important on the movement.\n\nThe creation of the International Criminal Court at the turn of the 21st century is seen as another achievement of the international human rights activists.\n\n\n"}
{"id": "341299", "url": "https://en.wikipedia.org/wiki?curid=341299", "title": "Ilse Koch", "text": "Ilse Koch\n\nIlse Koch (; née Margarete Ilse Köhler; 22 September 1906 – 1 September 1967, known as the Witch of Buchenwald) was the wife of Karl-Otto Koch, commandant of the Nazi concentration camps Buchenwald (1937–1941) and Majdanek (1941–1943). In 1947, she became one of the first prominent Nazis tried by the U.S. military.\n\nAfter the trial received worldwide media attention, survivor accounts of her actions resulted in other authors describing her abuse of prisoners as sadistic, and the image of her as \"the concentration camp murderess\" was current in post-war German society. \n\nShe was accused of taking souvenirs from the skin of murdered inmates with distinctive tattoos, although those claims were rejected at both of her trials. She was known as \"The Witch of Buchenwald\" (\"Die Hexe von Buchenwald\") by the inmates because of her cruelty and toward prisoners. In English, she is referred to as: \"The Beast of Buchenwald\", \"Queen of Buchenwald\", \"Red Witch of Buchenwald\", \"Butcher Widow\", and, more commonly, \"The Bitch of Buchenwald\".\n\nKoch was born in Dresden, Germany, the daughter of a factory foreman. She was known as a polite and happy child in her elementary school. At the age of 15, she entered an accountancy school. Later, she went to work as a bookkeeping clerk. At the time the economy of Germany had not yet recovered from Germany's defeat in World War I. In 1932, she became a member of the rising Nazi Party. Through some friends in the SA and SS, she met Karl-Otto Koch in 1934, marrying him two years later.\n\nIn 1936, she began working as a guard and secretary at the Sachsenhausen concentration camp near Berlin, which her fiancé commanded, and was married the same year. In 1937 she came to Buchenwald when her husband was made Commandant. While at Buchenwald, Koch allegedly engaged in a gruesome experiment, where it was claimed that she ordered selected tattooed prisoners to be murdered and skinned to retrieve the tattooed parts of their bodies. It was allegedly done to help a prison doctor, , in his dissertation on tattooing and criminality.\n\nIn 1940, she built an indoor sports arena, which cost over 250,000 reichsmarks (approximately $62,500), most of which had been seized from the inmates. In 1941 Karl Otto Koch was transferred to Lublin, where he helped establish the Majdanek concentration and extermination camp. Ilse Koch remained at Buchenwald until 24 August 1943, when she and her husband were arrested on the orders of Josias von Waldeck-Pyrmont, SS and Police Leader for Weimar, who had supervisory authority over Buchenwald. The charges against the Kochs comprised private enrichment, embezzlement, and the murder of prisoners to prevent them from giving testimony.\n\nIlse Koch was imprisoned until 1944 when she was acquitted for lack of evidence. Her husband was found guilty and sentenced to death by an SS court in Munich, and was executed by firing squad on 5 April 1945 in the court of the camp he once commanded. She went to live with her surviving family in the town of Ludwigsburg, where she was arrested by U.S. authorities on 30 June 1945.\n\nKoch and 30 other accused were arraigned before the American military court at Dachau (General Military Government Court for the Trial of War Criminals) in 1947. Prosecuting her was future United States Court of Claims Judge Robert L. Kunzig. She was charged with \"participating in a criminal plan for aiding, abetting and participating in the murders at Buchenwald\".\n\nKoch announced in the courtroom that she was eight months pregnant but on 19 August 1947, she was sentenced to life imprisonment for \"violation of the laws and customs of war\".\n\nGen. Lucius D. Clay was the interim military governor of the American Zone in Germany, and he reduced the judgment to four years' imprisonment on 8 June 1948, after she had served two years of her sentence, on the grounds that \"there was no convincing evidence that she had selected inmates for extermination in order to secure tattooed skins, or that she possessed any articles made of human skin\". News of the reduced sentence did not become public until 16 September 1948, and Clay stood firm despite the ensuing uproar. Jean Edward Smith reported in his biography \"Lucius D. Clay: An American Life\" that the general maintained that the leather lamp shades were really made out of goat skin. The book quotes a statement made by Clay years later:\n\nUnder the pressure of public opinion Koch was re-arrested in 1949 and tried before a West German court. The hearing opened on 27 November 1950 before the District Court at Augsburg and lasted seven weeks, during which 250 witnesses were heard, including 50 for the defense. Koch collapsed and had to be carried from the court in late December 1950, and again on 11 January 1951. At least four witnesses for the prosecution testified that they had seen Koch choose tattooed prisoners, who were then killed, or had seen or been involved in the process of making human-skin lampshades from tattooed skin. However, this charge was dropped by the prosecution when they could not prove lampshades or any other items were actually made from human skin.\n\nOn 15 January 1951, the Court pronounced its verdict, in a 111-page-long decision, for which Koch was not present in court. It was concluded that the previous trials in 1944 and 1947 were not a bar to proceedings under the principle of \"ne bis in idem\", as at the 1944 trial Koch had only been charged with receiving, while in 1947 she had been accused of crimes against foreigners after 1 September 1939, and not with crimes against humanity of which Germans and Austrians had been defendants both before and after that date. She was convicted of charges of incitement to murder, incitement to attempted murder and incitement to the crime of committing grievous bodily harm, and on 15 January 1951 was sentenced to life imprisonment and permanent forfeiture of civil rights.\n\nKoch appealed to have the judgment quashed, but the appeal was dismissed on 22 April 1952 by the Federal Court of Justice. She later made several petitions for a pardon, all of which were rejected by the Bavarian Ministry of Justice. Koch protested her life sentence, to no avail, to the International Human Rights Commission.\n\nKarl and Ilse Koch had two sons, one of whom committed suicide after the war \"because he couldn't live with the shame of the crimes of his parents.\" Another son, Uwe, conceived in her prison cell at Dachau with a fellow German prisoner, was born in the Aichach prison near Dachau where Koch was sent to serve her life sentence and was immediately taken from her. At the age of 19, Uwe Köhler learned that Koch was his mother and began visiting her regularly at Aichach.\n\nKoch hanged herself at Aichach women's prison on 1 September 1967 at age 60. She suffered from delusions and had become convinced that concentration camp survivors would abuse her in her cell.\n\nIn 1971, her son Uwe sought posthumous rehabilitation for his mother.\n\n\n\n\n"}
{"id": "26929473", "url": "https://en.wikipedia.org/wiki?curid=26929473", "title": "Internarrative identity", "text": "Internarrative identity\n\nInternarrative identity is building upon the notion of narrative Identity, the idea that our identities are shaped by the accounts we give of our lives. The central tenet of internarrative identity is that the ability of individuals to shape their lives is extended by multiple autobiographical narratives with associative principles beyond temporality. This concept was developed in 1997 by Ajit K. Maan in the central text Internarrative Identity.\n\nWhile the foundation of Narrative Identity Theory is the holding together of life experiences in a unified structure, Internarrative Identity Theory celebrates what have previously been considered problematic areas of experience - conflict, marginalization, disruption, subversion, deviation - as places of possibility for self creation.\nInternarrative Identity Theory locates the solution of narrative conflict within the problem itself. Existence in-between authoritarian discourses and dominant cultures enables an extended form of agency wherein a subject is able to undermine traditional associations, assumptions, concepts, and at the same time, create links between otherwise incommensurable world views. Rather than being a passive recipient of dominant discourses the Internarrative subject is uniquely able to subvert regulatory identity practices.\n\n\n"}
{"id": "22352470", "url": "https://en.wikipedia.org/wiki?curid=22352470", "title": "Invertible module", "text": "Invertible module\n\nIn mathematics, particularly commutative algebra, an invertible module is intuitively a module that has an inverse with respect to the tensor product. Invertible modules form the foundation for the definition of invertible sheaves in algebraic geometry.\n\nFormally, a finitely generated module \"M\" over a ring \"R\" is said to be invertible if it is locally a free module of rank 1. In other words, formula_1 for all primes \"P\" of \"R\". Now, if \"M\" is an invertible \"R\"-module, then its dual is its inverse with respect to the tensor product, i.e. formula_2.\n\nThe theory of invertible modules is closely related to the theory of codimension one varieties including the theory of divisors.\n\n\n"}
{"id": "780287", "url": "https://en.wikipedia.org/wiki?curid=780287", "title": "Juan Marichal", "text": "Juan Marichal\n\nJuan Antonio Marichal Sánchez (born October 20, 1937) is a Dominican former professional baseball player. He played as a right-handed pitcher in Major League Baseball, most notably for the San Francisco Giants. Marichal was known for his high leg kick, pinpoint control and intimidation tactics, which included aiming pitches directly at the opposing batters' helmets.\n\nMarichal also played for the Boston Red Sox and Los Angeles Dodgers for the final two seasons of his career. Although he won more games than any other pitcher during the 1960s, he appeared in only one World Series game and he was often overshadowed by his contemporaries Sandy Koufax and Bob Gibson in post-season awards. Marichal was inducted into the Baseball Hall of Fame in 1983.\n\nJuan Marichal was born on October 20, 1937, in the small farming village of Laguna Verde, Dominican Republic, the youngest of Francisco and Natividad Marichal's four children. He has two brothers, Gonzalo and Rafael, and a sister, Maria. His father died of an unknown illness when Marichal was three years old. His house did not have electricity, but food was plentiful since his family owned a farm. As a child, Marichal worked on the farm daily and was responsible for taking care of his family's horses, donkeys, and goats. He lived near the Yaque del Norte River and often spent time swimming and fishing. One day while Marichal was playing by the river, he fell unconscious owing to poor digestion and was in a coma for nine days. Doctors did not expect him to survive, but he slowly regained consciousness after his family gave him steam baths by doctor's orders.\n\nHis older brother Gonzalo instilled a love of baseball in young Marichal and taught him the fundamentals of pitching, fielding, and batting. Every weekend, Marichal played the sport with his brother and friends. For their games, they found golf balls and paid the local shoemaker one peso to sew thick cloth around the ball to make it the proper size. They employed branches from a wassama tree for bats and canvas tarps for gloves. Among his childhood playmates were the Alou brothers, Felipe, Jesús, and Matty, who all later played with Marichal on the San Francisco Giants. From the age of six, Marichal aspired to become a professional baseball player, but his mother discouraged this, instead urging him to get an education. At the time, there were no players from the Dominican Republic in Major League Baseball, and his goal was viewed to be unrealistic. At age 11, he briefly held a job cutting sugar cane for the J.W. Tatem Shipping conglomerate.\n\nIn 1954, sixteen-year-old Marichal joined a summer league in Monte Cristi, playing for a team called Las Flores. Although he began at shortstop, Marichal switched to pitcher after taking inspiration from Bombo Ramos of the Dominican national team. He left high school after being recruited to play for the United Fruit Company team in 1956.\n\nMarichal's delivery was renowned for one of the fullest windups in modern baseball, with a high kick of his left leg that went nearly vertical (even more so than Warren Spahn's delivery). Marichal maintained this delivery his entire career, and photographs taken near his retirement show the vertical kick only diminished. The windup was the key to his delivery in that he was consistently able to conceal the type of pitch until it was on its way.\n\nMarichal was discovered by Ramfis Trujillo, the son of late Dominican dictator Rafael Leónidas Trujillo. Ramfis was the primary sponsor of the Dominican Air Force Baseball Team (Aviación Dominicana), against which Marichal pitched a 2–1 victory game in his native Monte Cristi. From the very moment the game ended, Marichal was a member of Aviación Dominicana team, enlisted to the Air Force right on the spot by Ramfis's orders.\n\nMarichal entered the major leagues on July 19, with the San Francisco Giants as the second native pitcher to come from the Dominican Republic. He made an immediate impression: in his debut, on July 19, 1960 against the Philadelphia Phillies, he took a no hitter into the eighth inning only to surrender a two-out single to Clay Dalrymple. He ended up with a one-hit shutout, walking one and striking out 12. He started 10 more games that season, finishing at 6–2 with a 2.66 ERA. He improved his victory totals to 13 and 18 over the following two seasons, respectively, before finally cracking the 20-victory plateau in 1963, when he went 25–8 with 248 strikeouts and a 2.41 ERA. He appeared in every All-Star game of the 1960s beginning in 1962. In May 1966, he was named NL Player of the Month with a 6-0 record, a 0.97 ERA, and 42 SO. On July 14, 1967, he surrendered the 500th Home Run of Eddie Mathews' career.\n\nMarichal enjoyed similar success through the 1969 season, posting more than 20 victories in every season except 1967, and never posting an ERA higher than 2.76. He led the league in victories in 1963 and 1968 when he won 26 games. In 1968, he also finished in the highest rank of his career in MVP voting, finishing fifth behind Bob Gibson, Pete Rose, Willie McCovey, and Curt Flood. He and Sandy Koufax were the only two Major League pitchers in the post-war era (1946–present) to have more than one season of 25 or more wins; both pitchers had three such seasons in their careers.\n\nMarichal won more games during the decade of the 1960s (191) than any other major league pitcher, but did not receive any votes for the Cy Young Award until 1970, when baseball writers started voting for the top three pitchers in each league rather than one per league (or, until 1967, only the top pitcher in MLB). Marichal finished in the top 10 in ERA seven consecutive years, starting in 1963 and culminating in 1969, in which year he led the league. During his career, he also finished in the top 10 in strikeouts six times, top 10 in innings pitched eight times (leading the league twice), and top 10 in complete games 10 times, with a career total of 244. He led the league twice in shutouts, throwing 10 of them in 1965.\n\nMarichal exhibited exceptional control. He had 2,303 strikeouts with only 709 walks, a strikeout-to-walk ratio of 3.25 to 1. This ranks among the top 20 pitchers of all time, ahead of such notables as Bob Gibson, Nolan Ryan, Steve Carlton, Sandy Koufax, Don Drysdale, Walter Johnson and Roger Clemens, who each have strikeout-to-walk ratios of less than 3:1. Over his career, he led the league in the fewest walks per nine innings four times, and finished second three times – totaling eleven years in which he finished in the top 10, all while also finishing in the top 10 for strikeouts six years.\n\nOne regular-season game in Marichal's career deserves mention, involving him and Milwaukee Braves' Hall of Famer Warren Spahn in a night contest played July 2, 1963, before almost 16,000 at Candlestick Park in San Francisco. The two great pitchers matched scoreless innings until Giants outfielder Willie Mays homered off Spahn to win the game 1–0 in the 16th inning. Both Spahn and Marichal tossed 15-plus inning complete games, something that had not happened before or since in the big leagues. Marichal allowed eight hits (all singles except for a double hit by Spahn) in the 16 innings, striking out 10, and saddling eventual career home run king Hank Aaron with an 0-for-6 collar. Spahn permitted nine hits in 15 innings, walking just one (Mays intentionally, in the 14th, after Harvey Kuenn's leadoff double) and striking out two. The game, almost the innings-duration of two contests, lasted only 4 hours, 10 minutes. By coincidence, future Major League Baseball commissioner Bud Selig attended the game as a fan.\n\nMarichal is also remembered for a notorious incident that occurred with John Roseboro during a game between the Giants and Los Angeles Dodgers at Candlestick Park on August 22, . The Giant-Dodger rivalry was, at the time, the fiercest in baseball - a rivalry which began when both teams played in the New York City market. As the 1965 season neared its climax, the Giants were involved in a tight pennant race, entering the game trailing the Dodgers by a game and a half while the Milwaukee Braves were one game behind the Dodgers.\n\nMaury Wills led off the game with a bunt single off Marichal and scored when Ron Fairly hit a double. Marichal, a fierce competitor, viewed the bunt as a cheap way to get on base and took umbrage with Wills. When Wills came up to bat in the second inning, Marichal threw a pitch directly at Wills sending him sprawling to the ground. Willie Mays then led off the bottom of the second inning for the Giants and Dodgers' pitcher Sandy Koufax threw a pitch over Mays' head as a token form of retaliation. In the top of the third inning with two outs, Marichal threw a fastball that came close to hitting Fairly, prompting him to dive to the ground. Marichal's act angered the Dodgers and home plate umpire Shag Crawford warned both teams any further retaliations would not be tolerated.\n\nMarichal came to bat in the third inning expecting Koufax to throw at him. Instead, Marichal was startled when, after the second pitch, Roseboro's return throw to Koufax either brushed his ear or came close enough for Marichal to feel the breeze off the ball. When Marichal confronted Roseboro, Roseboro came out of his crouch with his fists clenched. Marichal later said he thought Roseboro was about to attack him. Marichal raised his bat, striking Roseboro at least twice on the head, opening a two-inch gash that sent blood flowing down the catcher's face. Roseboro later required 14 stitches. Koufax raced in from the mound to attempt to separate them and was joined by the umpires, players and coaches from both teams.\n\nA 14-minute brawl ensued on the field before Koufax, Giants captain Willie Mays and other peacemakers restored order. Marichal was ejected from the game and afterwards, National League president Warren Giles suspended him for eight games (two starts), fined him a then-NL record US$1,750 (), and also forbade him from traveling to Dodger Stadium for the final, crucial two-game series of the season. Roseboro filed a $110,000 damage suit against Marichal one week after the incident but, eventually settled out of court for $7,500.\n\nMany people protested the apparently light punishment meted out, since it would cost Marichal only two starts. The Giants were in a tight pennant race with the Dodgers (as well as the Pirates, Reds, and Braves) and the race was decided with only two games to play. The Giants, who ended up winning the August 22 game and were down only game afterward, eventually lost the pennant to the Dodgers by 2 games. Ironically, the Giants went on a 14-game win streak that started during Marichal's absence and by then it was a two-team race as the Pirates, Reds, and Braves fell further behind. But then the Dodgers won 15 of their final 16 games (after Marichal had returned) to win the pennant. Marichal won in his first game back, 2–1 vs. the Astros on September 9 (the same day Koufax pitched his perfect game vs. the Cubs), but lost his last three decisions as the Giants slumped in the season's final week.\n\nMarichal didn't face the Dodgers again until spring training in April 3, . In his first at bat against Marichal since the incident, Roseboro hit a three-run home run. San Francisco General Manager Chub Feeney approached Dodgers General Manager Buzzy Bavasi to attempt to arrange a handshake between Marichal and Roseboro however, Roseboro declined the offer.\n\nYears later, Roseboro stated that he was retaliating for Marichal having thrown at Wills. He explained that Koufax would not throw at batters for fear of hurting them due to the velocity of his pitches. He further stated that his throwing close to Marichal's ear was, \"standard operating procedure\", as a form of retribution. After years of bitterness, Roseboro and Marichal became close friends in the 1980s, getting together occasionally at Old-Timers games, golf tournaments and charity events.\n\nIn 1970, Marichal experienced a severe reaction to penicillin which led to back pain and chronic arthritis. Marichal's career stumbled in , when he only posted 12 wins and his ERA shot up to 4.12, before straightening itself out with a stellar 1971 season in which he won 18 games and his ERA dropped below 3.00. It was the only season in which Marichal earned any consideration for the Cy Young Award, finishing in 8th place. It was his final great season (and his final of nine All-Star appearances), however, as he posted 6–16 and 11–15 records in 1972 and 1973 respectively.\n\nAfter the 1973 season, the Giants sold Marichal to the Boston Red Sox. He had a fairly solid 1974, going 5–1 in 11 starts, but was released after the season. He then signed with the Dodgers as a free agent. Dodger fans had never forgiven Marichal for his attack on Roseboro 10 years earlier, and it took a personal appeal from Roseboro to calm them down. However, Marichal's 1975 didn't last long; he was lit up for nine runs, 11 hits and a 13.50 ERA in only two starts before retiring. He finished his career with 243 victories, 142 losses, 244 complete games, 2,303 strikeouts and a 2.89 ERA over 3,507 innings pitched. He played in the 1962 World Series against the New York Yankees (one start, a no decision) and the 1971 National League Championship Series against the Pittsburgh Pirates (losing his only start). Between 1962 and 1971, the Giants averaged 90 wins a season, and Marichal averaged 20 wins a year.\n\nMarichal pitched a no-hitter on June 15, , and was named to nine All-Star teams. He was selected the Most Valuable Player of the 1965 game in Minneapolis, in which he pitched three shutout innings and faced the minimum nine batters, giving up one hit. His overall All-Star Game record was 2–0 with a 0.50 ERA in eight appearances facing 62 batters in 18 total innings, second-most in innings pitched only to Don Drysdale (19.1 innings; 2-1, 1.40 ERA and 69 batters faced).\n\nMarichal was passed over for election to the Baseball Hall of Fame during his first two years of eligibility, by all accounts because the Baseball Writers' Association of America voters still held his attack on Roseboro against him. However, after a personal appeal by Roseboro, Marichal was elected in 1983, and thanked Roseboro in his induction speech. When Roseboro died in 2002, Marichal served as an honorary pallbearer at his funeral and told the gathered, \"Johnny's forgiving me was one of the best things that happened in my life. I wish I could have had John Roseboro as my catcher.\"\n\nMarichal's uniform number 27 has been retired by the Giants. In , Marichal, who was working as a broadcaster for Spanish radio, was on hand to see his son-in-law at the time, José Rijo, win the World Series Most Valuable Player Award. In , he ranked #71 on \"The Sporting News\"' list of the 100 Greatest Baseball Players, and was a finalist for the Major League Baseball All-Century Team. He was honored before a game between the Giants and Oakland Athletics with a statue outside AT&T Park in 2005, and was named one of the three starting pitchers on Major League Baseball's Latino Legends Team. In 1976, sportswriter Harry Stein published an \"All Time All-Star Argument Starter\", consisting of five ethnic baseball teams. Marichal was the right-handed pitcher on Stein's Latin team. The Giants also honored him by wearing jerseys that said \"Gigantes.\" Marichal was inducted into the Hispanic Heritage Baseball Museum Hall of Fame on July 20, 2003 in pregame on field ceremony at Pac Bell Park.\nIn 2015 the Estadio Quisqueya in his home country was renamed \"Quisqueya stadium Juan Marichal\" after him.\n\nIn 2008, Marichal was filmed at a cockfight in the Dominican Republic along with New York Mets pitcher Pedro Martínez. The incident caused controversy in the United States, but Martinez defended their attendance at the cockfight by saying, \"I understand that people are upset, but that is part of our Dominican culture and is legal in the Dominican Republic\". He added, \"I was invited by my idol, Juan Marichal, to attend the event as a spectator, not as a participant.\"\n\n\n\n"}
{"id": "1272976", "url": "https://en.wikipedia.org/wiki?curid=1272976", "title": "Karma in Hinduism", "text": "Karma in Hinduism\n\nKarma is a concept in Puranic time/era which explains causality through a system where beneficial effects are derived from past beneficial actions and harmful effects from past harmful actions, creating a system of actions and reactions throughout a soul's (Atman's) reincarnated lives forming a cycle of rebirth. The causality is said to be applicable not only to the material world but also to our thoughts, words, actions and actions that others do under our instructions.\n\nThe earliest appearance of the word \"karman\" is found in the Rigveda. The term \"karman\" also appears significantly in the Veda. According to Brahmana, \"as his wife man is born to the world he has made\" and one is placed in a balance in the other world for an estimate of one's good and evil deed. It also declares that as a man is 'constituted' by his desires, he is born in the other world with reference to these.\nScholars have generally agreed that the earliest formulation of the Karma doctrine occurs in the Brhadaranyaka Upanisad, which is the earliest of the Upanisads. The doctrine occurs here in the context of a discussion of the fate of the individual after death.\n\nThe doctrine of transmigration of the soul, with respect to fateful retribution for acts committed, does not appear in the Rig Veda. The belief in rebirth is, suggests Radhakrishnan, evident in the Brahmanas, where words like \"punar-mrtyu\"(re-death), \"punar-asu\" (coming to life again) and \"punarajati\" (rebirth) are used to denote it. Radhakrishnan acknowledges that other scholars interpret certain \"punar-mrtyu\" verses of Rigveda to be discussing \"repeated deaths\"; however, he suggests that it might also be re-interpreted to imply rebirth, as in \"come home once again\".\n\nThe concept of karma first appears strongly in the Bhagavad Gita. The topic of karma is mentioned in the Puranas.\n\nThe word ‘karma’ has originated from the Sanskrit root ‘kri’ which means ‘to do’ or ‘to ‘act and react’\n\n\"Karma\" literally means \"action\" or \"reincarnation\", and more broadly names the universal principle of cause and effect, action and reaction, which Hindus believe governs all consciousness. Karma is not fate, for we act with what can be described as a conditioned free will creating our own destinies. Karma refers to the totality of our actions and their concomitant reactions in this and previous lives, all of which determine our future. The conquest of karma lies in intelligent action and dispassionate reaction. Not all karmas rebound immediately. Some accumulate and return unexpectedly in this or other lifetimes. Human beings are said to produce karma in four ways:\n\nEverything that we have ever thought, spoken, done or caused is karma, as is also that which we think, speak or do this very moment. Hindu scriptures divide karma into three kinds:\n\n\nSome believe that only human beings who can distinguish right from wrong can do (kriyamana) karma. Therefore, animals and young children are considered incapable of creating new karma (and thus cannot affect their future destinies) as they are incapable of discriminating between right and wrong.\n\nTulsidas, a Hindu saint, said: \"Our destiny was shaped long before the body came into being.\" As long as the stock of \"sanchita karma\" lasts, a part of it continues to be taken out as \"prarabdha karma\" for being enjoyed in one lifetime, leading to the cycle of birth and death. A Jiva cannot attain moksha (liberation) from the cycle of birth and death, until the accumulated \"sanchita karmas\" are completely exhausted.\n\nUnkindness yields spoiled fruits, called \"paap\", and good deeds bring forth sweet fruits, called \"punya\". As one acts, so does one become: one becomes virtuous by virtuous action, and evil by evil action.\n\nSeveral different views exist in Hinduism, some extant today and some historical, regarding the role of divine beings in controlling the effects of karma or the lack thereof.\n\nIn the theistic side of Vedanta,the creator Ishvara rules over the world through the law of karma.\n\nIn non-dualistic (Advaita) school of Vedanta, the creator is not the ultimate reality, \"I am God\" is the supreme truth, the pursuit of self-knowledge is spirituality, and it shares the general concepts of karma-rebirth-samsara ideas found in Buddhism with some important differences.\n\nIn a commentary to Brahma Sutras (III, 2, 38, and 41), a Vedantic text, Adi Sankara, an Indian philosopher who consolidated the doctrine of Advaita Vedanta, a sub-school of Vedanta, argues that the original karmic actions themselves cannot bring about the proper results at some future time; neither can super sensuous, non-intelligent qualities like adrsta—an unseen force being the metaphysical link between work and its result—by themselves mediate the appropriate, justly deserved pleasure and pain. The fruits, according to him, then, must be administered through the action of a conscious agent, namely, a supreme being (Ishvara).\n\nA human's karmic acts result in merits and demerits. Since unconscious things generally do not move except when caused by an agent (for example, the ax moves only when swung by an agent), and since the law of karma is an unintelligent and unconscious law, Sankara argues there must be a conscious God who knows the merits and demerits which persons have earned by their actions, and who functions as an instrumental cause [a \"judge and police-force\" working for \"the law\"] in helping individuals reap their appropriate fruits. Thus, God affects the person's environment, even to its atoms, and for those souls who reincarnate, produces the appropriate rebirth body, all in order that the person might have the karmically appropriate experiences. Since a data-system (or computer) is needed to discern different \"just\" consequences for actions, there is suggested to be a sentient theistic administrator or supervisor for karma, i.e., Vishnu (King of Kings).\n\n\nSwami Sivananda, an Advaita scholar, reiterates the same views in his commentary synthesising Vedanta views on the Brahma Sutras. In his commentary on Chapter 3 of the Brahma Sutras, Sivananda notes that karma is insentient and short-lived, and ceases to exist as soon as a deed is executed. Hence, karma cannot bestow the fruits of actions at a future date according to one's merit. Furthermore, one cannot argue that karma generates apurva or punya, which gives fruit. Since apurva is non-sentient, it cannot act unless moved by an intelligent being such as God. It cannot independently bestow reward or punishment.\n\nThere is a passage from Swami Sivananda's translation of the Svetasvatara Upanishad (4:6) illustrating this concept:\n\nIn his commentary, the first bird represents the individual soul, while the second represents Brahman or God. The soul is essentially a reflection of Brahman. The tree represents the body. The soul identifies itself with the body, reaps the fruits of its actions, and undergoes rebirth. The Lord alone stands as an eternal witness, ever contented, and does not eat, for he is the director of both the eater and the eaten.\n\nSwami Sivananda also notes that God is free from charges of partiality and cruelty which are brought against him because of social inequality, fate, and universal suffering in the world. According to the Brahma Sutras, individual souls are responsible for their own fate; God is merely the dispenser and witness with reference to the merit and demerit of souls.\n\nIn his commentary on Chapter 2 of the Brahma Sutras, Sivananda further notes that the position of God with respect to karma can be explained through the analogy of rain. Although rain can be said to bring about the growth of rice, barley and other plants, the differences in various species is due to the diverse potentialities lying hidden in the respective seeds. Thus, Sivananda explains that differences between classes of beings are due to different merits belonging to individual souls. He concludes that God metes rewards and punishments only in consideration of the specific actions of beings.\n\nRamanuja of the Vishishtadvaita school, another sub-school of Vedanta, addresses the problem of evil by attributing all evil things in life to the accumulation of evil karma of jivas (souls in bondage to a corporeal form) and maintains that God is \"amala,\" or without any stain of evil.\nIn Sri Bhasya, Ramanuja's interpretation of the Brahma sutras from a Vaishnavite theistic view, Brahman, whom he conceives as Vishnu, arranges the diversity of creation in accordance with the different karma of individual souls.\n\nRamanuja, reiterates that inequality and diversity in the world are due to the fruits of karma of different souls and the omnipresent energy of the soul suffers pain or pleasure due to its karma. Unlike the Semitic religions, e.g., Abrahamic religions, which believe that God created the soul and the world out of ‘nothing,’ Ramanuja believed that creation is an eternally recurring cyclic process and hence God is free from the responsibility of starting it and causing the evils accruing from it. Instead he believed that karma, the result of the actions of Jivas (souls) in previous embodiments, causes the good and evil, enjoyments and sufferings of karma which have to be necessarily to be enjoyed or suffered by the Jivas themselves who are responsible for the fruits.\n\nAlthough souls alone have the freedom and responsibility for their acts and thus reap the fruits of karma, i.e., good and evil karma, God as Vishnu, is the supreme Enforcer of karma, by acting as the Sanctioner (Anumanta) and the Overseer (Upadrasta). According to Ramanuja, all jivas are burdened with their load of Karma, which gives them only enjoyments and sufferings, but also desires and tendencies to act in particular ways; although the moral responsibility accrues only to the Jiva, as he acts according to the tendencies and deserts he has acquired by his karma, Ramanuja believes that God wills only their fructification. According to the foregoing concept, God is \"compared to light which may be used for forging or for reading scriptures,\" but the merits or demerit \"devolves entirely on the persons concerned and not on the light.\"\n\nFurthermore, Ramanuja believes that Vishnu wishing to do a favour to those who are resolved on acting so as fully to please Him, engenders in their minds a tendency towards highly virtuous actions, such as means to attain to Him; while on the other hand, in order to punish those who are resolved on lines of action altogether displeasing to Him, He engenders in their minds a delight in such actions as have a downward tendency and are obstacles in the way of the attainment of God.\n\nMadhva, the founder of the Dvaita school, another sub-school of Vedanta, on the other hand, believes that there must be a root cause for variations in karma even if karma is accepted as having no beginning and being the cause of the problem of evil. Since jivas have different kinds of karma, from good to bad, all must not have started with same type of karma from the beginning of time. Thus, Madhva concludes that the jivas (souls) are not God's creation as in the Christian doctrine, but are rather entities co-existent with Vishnu, although under His absolute control. Souls are thus dependent on Him in their pristine nature and in all transformations that they may undergo.\n\nAccording to Madhva, God, although He has control, does not interfere with Man's free will; although He is omnipotent, that does not mean that He engages in extraordinary feats. Rather, God enforces a rule of law and, in accordance with the just deserts of jivas, gives them freedom to follow their own nature. Thus, God functions as the sanctioner or as the divine accountant, and accordingly jivas are free to work according to their innate nature and their accumulated karma, good and bad. Since God acts as the sanctioner, the ultimate power for everything comes from God and the jiva only utilizes that power, according to his/her innate nature. However, like Shankara's interpretation of the Brahma Sutras as mentioned earlier, Madhva, agrees that the rewards and punishments bestowed by God are regulated by Him in accordance with the good and sinful deeds performed by them, and He does so of out of His own will to keep himself firm in justice and he cannot be controlled in His actions by karma of human beings nor can He be accused of partiality or cruelty to anyone.\n\nSwami Tapasyananda further explains the Madhva view by illustrating the doctrine with this analogy: the power in a factory comes from the powerhouse (God), but the various cogs (\"jivas\") move in a direction in which they are set. Thus he concludes that no charge of partiality and cruelty can be brought against God. The jiva is the actor and also the enjoyer of the fruits of his/her own actions.\n\nMadhva differed significantly from traditional Hindu beliefs, owing to his concept of eternal damnation. For example, he divides souls into three classes: one class of souls which qualify for liberation (Mukti-yogyas), another subject to eternal rebirth or eternal transmigration (Nitya-samsarins), and a third class that is eventually condemned to eternal hell or Andhatamas (Tamo-yogyas).\n\nEarlier historical traditions of Hinduism such as Mimamsakas, reject any such notions of divinity being responsible and see karma as acting independently, considering the natural laws of causation sufficient to explain the effects of karma.\n\nAccording to the Mimamsakas it is useless to set up a God for that purpose, since Karma itself can give the result at a future time.\n\nIn the later writings in the Nyaya, school of philosophy, Udayana's \"Nyayakusumanjali\" used Karma for the last of his nine proofs of the existence of a creative God: See Nyaya on God and Salvation.\n\nSwami Sivananda Saraswati puts it like this:\n\nSivananda concludes that God metes rewards and punishments only in consideration of the specific actions of beings.\n\nSambandar of the Shaiva Siddhanta school, in the 7th century C.E., writes about karma in his outline of Shaivism. He explains the concept of karma in Hinduism by distinguishing it from that of Buddhism and Jainism, which do not require the existence of an external being like God. In their beliefs, just as a calf among a large number of cows can find its mother at suckling time, so also does karma find the specific individual it needs to attach to and come to fruition. However, theistic Hindus posit that karma, unlike the calf, is an unintelligent entity. Hence, karma cannot locate the appropriate person by itself. Sambantha concludes that an intelligent Supreme Being with perfect wisdom and power (Shiva, for example) is necessary to make karma attach to the appropriate individual. In such sense, God is the Divine Accountant.\n\nAppayya Dikshita, a Shaiva theologian and proponent of Shiva Advaita, states that Shiva only awards happiness and misery in accordance with the law of karma. Thus persons themselves perform good or evil actions according to their own inclinations as acquired in past creations, and in accordance with those deeds, a new creation is made for the fulfilment of the law of karma. Shaivas believe that there are cycles of creations in which souls gravitate to specific bodies in accordance with karma, which as an unintelligent object depends on the will of Siva alone.\n\nSrikantha, another Saivite theologian and proponent of Siva Advaita, believes that individual souls themselves do things which may be regarded as the cause of their particular actions, or desisting from particular actions, in accordance with the nature of the fruition of their past deeds. Srikantha further believes that Siva only helps a person when he wishes to act in a particular way or to desist from a particular action. Regarding the view that karma produce their own effects directly, Srikantha holds that karma being without any intelligence cannot be expected to produce manifold effects through various births and various bodies; rather fruits of one's karma can be performed only by the will of God operating in consonance with man's free will, or as determined in later stages by man's own karma so the prints of all karma are distributed in the proper order by the grace of God Shiva. In this way, God is ultimately responsible on one hand for our actions, and on the other for enjoyment and suffering in accordance with our karmas, without any prejudice to humans' moral responsibility as expressed through free will or as determined later by our own deeds. A good summary of his view is that \"man is responsible, free to act as he wills to, for Siva only fulfills needs according to the soul's karma.\" \n\nIn Chapter 1 of 10th book of the Bhagavata Purana, Vasudeva, the father of Krishna, exhorts Kamsa to refrain from killing his wife, Devaki, the mother of Krishna, by stating that death is certain for those who are born and when the body returns to the five elements, the soul leaves the body and helplessly obtains another form in accordance with the laws of karma, citing passages from Brihadaranyaka Upanishad, IV:4:3. Moreover, he adds and states that the soul materializes into an appropriate body whatever the state of the mind one remembers at the time of death; i.e., at the time of the death, the soul and its subtle body of mind, intelligence and ego, is projected into the womb of a creature, human or non-human that can provide a gross body that is most suitable for the dominant state of the mind of the particular person at the time of death; note that this passage is similar in meaning as Bhagavad Gita, VIII, verse 6 Edwin Bryant, Associate Professor of religion at Rutgers University, New Jersey provided the foregoing commentaries on the discussion of Vasudeva in the Bhagavata Purana.\n\nMany names in the Vishnu Sahasranama, the thousand names of Vishnu allude to the power of God in controlling karma. For example, the 135th name of Vishnu, Dharmadhyaksha, in the Advaita philosopher Sankara's interpretation means, \"One who directly sees the merits (Dharma) and demerits (Adharma), of beings by bestowing their due rewards on them.\" \n\nOther names of Vishnu alluding to this nature of God are Bhavanah, the 32nd name, Vidhata, the 44th name, Apramattah, the 325th name, Sthanadah, the 387th name and Srivibhavanah, the 609th name. Bhavanah, according to Sankara's interpretation, means \"One who generates the fruits of Karmas of all Jivas (souls) for them to enjoy.\" The Brahma Sutra (3.2.28) \"Phalmatah upapatteh\" speaks of the Lord's function as the bestower of the fruits of all actions of the jivas.\n\nSee above discussion of karma under the Vedanta sections of Ramanuja (Vishishtadvaita) and Madhva (Dvaita) for treatment of karma under the two Vaishnavite teachers.\n\n\"\"According to their karma\", all living entities are wandering throughout the entire universe. Some of them are being elevated to the upper planetary systems, and some are going down into the lower planetary systems. \"Out of many millions of wandering living entities\", one who is very fortunate gets an opportunity to associate with a bona fide spiritual master by the grace of Krishna. By the mercy of both Krsna and the spiritual master, such a person receives the seed of the creeper of devotional service.\"\n\nKulashekhara Alwar, a Vaishnava devotee, says in his \"Mukundamala Stotra\": 'yad yad bhavyam bhavatu bhagavan purva-karma-anurupam'. And purva-karma or bhaagya or daiva is unseen adrsta by us, and is known only to God as Vidhaataa. God created the law of karma, and God will not violate it. God does, however, give courage and strength if asked.\n\nThe Nyaya school, one of six orthodox schools of Hindu philosophy, states that one of the proofs of the existence of God is karma; It is seen that some people in this world are happy, some are in misery. Some are rich and some poor. The Naiyanikas explain this by the concept of karma and reincarnation. The fruit of an individual's actions does not always lie within the reach of the individual who is the agent; there ought to be, therefore, a dispenser of the fruits of actions, and this supreme dispenser is God. This belief of Nyaya, accordingly, is the same as that of Vedanta and Vaiśeṣika Sūtra.\nThus the Nyaya school provides the moral argument for the existence of God.\n\nIn Hinduism, more particularly the Dharmaśāstras, Karma is a principle in which \"cause and effect are as inseparably linked in the moral sphere as assumed in the physical sphere by science. A good action has its reward and a bad action leads to retribution. If the bad actions do not yield their consequences in this life, the soul begins another existence and in the new environment undergoes suffering for its past deeds\". Thus it is important to understand that karma does not go away, one must either reap the benefits or suffer the consequences of his past actions. The Brihadaranyaka Upanishad states, \"According as a man acts and according as he believes so will he be; a man of meritorious acts will be meritorious, a man of evil deeds sinful. He becomes pure by pure deeds and evil by evil deeds. And here they say that person consists of desires. An as is his desire so is his will; and as is his will, so is his deed; and whatever deeds he does that he will reap\". The doctrine of karma dates from ancient times and besides the above author is mentioned in the Gautama dharma-sutra, Shatapatha Brahmana, Kathaaka-grhya-sutra, Chandogya Upanishad, Markandeya Purana and many others.\n\nThe shastras written about karma go into some detail about possible consequences of karma. There is often talk about coming back as a variety of different object when it comes to reincarnation and pasts lives. In this case, it holds true, or at least insofar as the texts state. The Kathaaka-grhya-sutra states, \"some human beings enter the womb in order to have an embodied existence; others go into inorganic matter (the stump of a tree and the like) according to their deeds and according to their knowledge\".\n\nMore extensively discussed is the consequences of karma in relation to sin. \"Karmavipaka means the ripening (or fruition) of evil actions or sins. This fruition takes three forms, as stated in the Yogasutra II. 3, i.e., jati (birth as a worm or animal), ayuh (life i.e. living for a short period such as five or ten years) and bhoga (experiencing the torments of Hell\".\n\nAccording to a theistic view, the effects of one's bad karma may be mitigated. Examples of how bad karma can be mitigated include following, or living virtuously; performing good deeds, such as helping others; yoga, or worshiping God in order to receive grace; and conducting pilgrimages to sacred places, such as or to get grace of God. Editors of Hinduism Today Magazine, What is Hinduism? In another example, Ganesha can unweave his devotees from their karma, simplifying and purifying their lives, but this only happens after they have established a personal relationship with Him.\n\nExamples of getting God's grace are further illustrated below.\n\nShvetashvatara Upanishad 7 and 12 aver that the doer of the deeds wanders about and obtains rebirth according to his deeds but postulates an omnipotent creator, i.e., Isvara and the doctrine of grace. Isvara is the great refuge of all and a person attains immortality when blessed by Isvara or at Isvara's pleasure.\n\nA person can be free from sorrow through the grace of Isvara. Therefore, the Shvetashvatara Upanishad postulates a supreme Being whose grace to devotees provides a way of escape from the law of karma.\nAs Adi Sankara stated in his commentary on Shvetashvatara Upanishad VI:4, \"If we dedicate all our works to Ishvara, we will not be subject to the law of karma.\" \n\nTheistic schools believe in cycles of creations where souls gravitate to specific bodies in accordance with karma, which as an unintelligent object depends on the will of God alone. For example, Kaushitaki Upanishad 1.2 asserts that birth in different forms of existence as a worm, insect, fish, bird, lion, boar, snake or a human, is determined by a person's deeds and knowledge.\n\nChandogya Upanishad 5.10.7 distinguishes between good birth such as birth in a spiritual family, i.e., (brahmin caste) or an evil birth, such as birth as a dog or hog.) Thus, the doctrine of karma comes to explain why different life forms manifest, into widely various levels of biological development such as characterization into different species from plants to various types of animals, and to even differences between members of the same species, such as humans.\n\nSwami Nikilananda comments: As the rivers, following their different courses, ultimately merge in the ocean and give up their names and forms, so the devotees, losing their names and forms, become one with the Supreme Reality.\n\nCharles Keyes, professor emeritus at the University of Washington and E. Valentine Daniel, professor of anthropology at Columbia University state that many Hindus believe that heavenly bodies, including the planets, have an influence throughout the life of a human being, and these planetary influences are the \"fruit of karma.\" \n\nThe Navagraha, planetary deities, including Shani (Saturn), are considered subordinate to Ishvara (i.e., the Supreme Being) and are believed by many to assist in the administration of justice. Thus, these planets can influence earthly life.\n\nSuch planetary influences are believed by many to be measurable using astrological methods including Jyotiṣa, the Hindu system of astrology.\n\nBesides narrow meaning of karma as the reaction or suffering being due to karma of their past lives and that one would have to transmigrate to another body in their next life, it is often used in the broader sense as action or reaction.\n\nThus, karma in Hinduism may mean an activity, an action or a materialistic activity. Often with the specific combination it takes specific meanings, such as \"karma-yoga\" or \"karma-kanda\" means \"yoga or actions\" and \"path of materialistic activity\" respectively. Yet another example is Nitya karma, which describes rituals which have to be performed daily by Hindus, such as the Sandhyavandanam which involves chanting of the Gayatri Mantra.\n\nOther uses include such expressions such as \"ugra-karma\", meaning bitter, unwholesome labor.\n\nIt has also been argued that Karma has a role in Hindu society as a whole. When one abides by their caste duty good Karma is earned and vice versa; and the Karma one collects is reflected in the next life as movement within the Caste system. The promise of upward mobility appealed to people, and was made plausible through Karma. This effectively \"tamed\" the lower castes into passive acceptance of the status quo. Thus, the Karma doctrine discouraged actual social mobility.\n\n\n"}
{"id": "25779246", "url": "https://en.wikipedia.org/wiki?curid=25779246", "title": "Kleshas (Hinduism)", "text": "Kleshas (Hinduism)\n\nKleśa (sanskrit क्लेश, also \"klesha\" ) is a term from Indian philosophy and yoga, meaning a \"poison\". The third \"śloka\" of the second chapter of Patañjali's \"Yogasūtra\" explicitly identifies Five Poisons (Sanskrit: \"pañcakleśā\"):\n\nTranslated into English, these five (pañca) Kleśa-s or Afflictions () are:\n\n\nInternational Nath Order (INO) perspectives:\n"}
{"id": "510791", "url": "https://en.wikipedia.org/wiki?curid=510791", "title": "List of cognitive biases", "text": "List of cognitive biases\n\nCognitive biases are systematic patterns of deviation from norm or rationality in judgment, and are often studied in psychology and behavioral economics.\n\nAlthough the reality of these biases is confirmed by replicable research, there are often controversies about how to classify these biases or how to explain them. Some are effects of information-processing rules (i.e., mental shortcuts), called \"heuristics\", that the brain uses to produce decisions or judgments. Biases have a variety of forms and appear as cognitive (\"cold\") bias, such as mental noise, or motivational (\"hot\") bias, such as when beliefs are distorted by wishful thinking. Both effects can be present at the same time.\n\nThere are also controversies over some of these biases as to whether they count as useless or irrational, or whether they result in useful attitudes or behavior. For example, when getting to know others, people tend to ask leading questions which seem biased towards confirming their assumptions about the person. However, this kind of confirmation bias has also been argued to be an example of social skill: a way to establish a connection with the other person.\n\nAlthough this research overwhelmingly involves human subjects, some findings that demonstrate bias have been found in non-human animals as well. For example, hyperbolic discounting has been observed in rats, pigeons, and monkeys.\n\nMany of these biases affect belief formation, business and economic decisions, and human behavior in general.\n\nMost of these biases are labeled as attributional biases.\n\nIn psychology \"and\" cognitive science, a memory bias is a cognitive bias that either enhances or impairs the recall of a memory (either the chances that the memory will be recalled at all, or the amount of time it takes for it to be recalled, or both), or that alters the content of a reported memory. There are many types of memory bias, including:\n\n\nA 2012 \"Psychological Bulletin\" article suggested that at least eight seemingly unrelated biases can be produced by the same information-theoretic generative mechanism that assumes noisy information processing during storage and retrieval of information in human memory.\n\nPeople do appear to have stable individual differences in their susceptibility to decision biases such as overconfidence, temporal discounting, and bias blind spot. That said, these stable levels of bias within individuals are possible to change. Participants in experiments who watched training videos and played debiasing games showed medium to large reductions both immediately and up to three months later in the extent to which they exhibited susceptibility to six cognitive biases: anchoring, bias blind spot, confirmation bias, fundamental attribution error, projection bias, and representativeness.\n\nDebiasing is the reduction of biases in judgment and decision making through incentives, nudges, and training. Cognitive bias mitigation and cognitive bias modification are forms of debiasing specifically applicable to cognitive biases and their effects. Reference class forecasting is a method for systematically debiasing estimates and decisions, based on what Daniel Kahneman has dubbed the outside view.\n\n\n"}
{"id": "29076341", "url": "https://en.wikipedia.org/wiki?curid=29076341", "title": "Matthew effect", "text": "Matthew effect\n\nThe Matthew effect, Matthew principle, or Matthew effect of accumulated advantage can be observed in many aspects of life and fields of activity. It is sometimes summarized by the adage \"the rich get richer and the poor get poorer.\" The concept is applicable to matters of fame or status, but may also be applied literally to cumulative advantage of economic capital.\n\nThe term was coined by sociologist Robert K. Merton in 1968 and takes its name from the Parable of the talents or minas in the biblical Gospel of Matthew. Merton credited his collaborator and wife, sociologist Harriet Zuckerman, as co-author of the concept of the Matthew effect.\n\nThe concept is named according to two of the Parables of Jesus in the synoptic Gospels (Table 2, of the Eusebian Canons).\n\nThe concept concludes both synoptic versions of the parable of the talents:\n\nThe concept concludes two of the three synoptic versions of the parable of the lamp under a bushel (absent in the version of Matthew):\n\nThe concept is presented again in Matthew outside of a parable during Christ's explanation to his disciples of the purpose of parables:\n\nIn the sociology of science, \"Matthew effect\" was a term coined by Robert K. Merton to describe how, among other things, eminent scientists will often get more credit than a comparatively unknown researcher, even if their work is similar; it also means that credit will usually be given to researchers who are already famous. For example, a prize will almost always be awarded to the most senior researcher involved in a project, even if all the work was done by a graduate student. This was later formulated by Stephen Stigler as Stigler's law of eponymy – \"No scientific discovery is named after its original discoverer\"  – with Stigler explicitly naming Merton as the true discoverer, making his \"law\" an example of itself.\n\nMerton furthermore argued that in the scientific community the Matthew effect reaches beyond simple reputation to influence the wider communication system, playing a part in social selection processes and resulting in a concentration of resources and talent. He gave as an example the disproportionate visibility given to articles from acknowledged authors, at the expense of equally valid or superior articles written by unknown authors. He also noted that the concentration of attention on eminent individuals can lead to an increase in their self-assurance, pushing them to perform research in important but risky problem areas.\n\nAs credit is valued in science, specific claims of the Matthew effect are contentious. Many examples below exemplify more famous scientists getting credit for discoveries due to their fame, even as other less notable scientists had preempted their work.\n\n\n\nIn science, dramatic differences in the productivity may be explained by three phenomena: sacred spark, cumulative advantage, and search costs minimization by journal editors. The sacred spark paradigm suggests that scientists differ in their initial abilities, talent, skills, persistence, work habits, etc. that provide particular individuals with an early advantage. These factors have a multiplicative effect which helps these scholars succeed later. The cumulative advantage model argues that an initial success helps a researcher gain access to resources (e.g., teaching release, best graduate students, funding, facilities, etc.), which in turn results in further success. Search costs minimization by journal editors takes place when editors try to save time and effort by consciously or subconsciously selecting articles from well-known scholars. Whereas the exact mechanism underlying these phenomena is yet unknown, it is documented that a minority of all academics produce the most research output and attract the most citations.\n\nIn education, the term \"Matthew effect\" has been adopted by psychologist Keith Stanovich to describe a phenomenon observed in research on how new readers acquire the skills to read: early success in acquiring reading skills usually leads to later successes in reading as the learner grows, while failing to learn to read before the third or fourth year of schooling may be indicative of lifelong problems in learning new skills.\n\nThis is because children who fall behind in reading would read less, increasing the gap between them and their peers. Later, when students need to \"read to learn\" (where before they were learning to read), their reading difficulty creates difficulty in most other subjects. In this way they fall further and further behind in school, dropping out at a much higher rate than their peers.\n\nIn the words of Stanovich:\nIn network science, the Matthew effect is used to describe the preferential attachment of earlier nodes in a network, which explains that these nodes tend to attract more links early on. \"Because of preferential attachment, a node that acquires more connections than another one will increase its connectivity at a higher rate, and thus an initial difference in the connectivity between two nodes will increase further as the network grows, while the degree of individual nodes will grow proportional with the square root of time.\" The Matthew Effect therefore explains the growth of some nodes in vast networks such as the Internet.\n\nProduct recommendations and information about past purchases have been shown to influence consumers' choices significantly - whether for music, movie, book, technological, and other types of products. Social influence often induces a rich-get-richer phenomenon where popular products tend to become even more popular.\n\n\n"}
{"id": "1707802", "url": "https://en.wikipedia.org/wiki?curid=1707802", "title": "Mortido", "text": "Mortido\n\nMortido is a term used in Freudian psychoanalysis to refer to the energy of the death instinct, formed on analogy to the term libido. \n\nIn the early 21st century, the term has been used more rarely, but still designates the destructive side of psychic energy.\n\nMortido was introduced by Freud's pupil Paul Federn to cover the psychic energy of the death instinct, something left open by Freud himself: Edoardo Weiss preferred to use destrudo. Providing what he saw as clinical proof of the reality of the death instinct in 1930, Federn reported on the self-destructive tendencies of severely melancholic patients as evidence of what he would later call inwardly-directed mortido.\n\nHowever, Freud himself favoured neither term – mortido or destrudo. This worked against either of them gaining widespread popularity in the psychoanalytic literature.\n\nEric Berne, who was a pupil of Federn's, made extensive use of the term mortido in his pre-transactional analysis study, \"The Mind in Action\" (1947). As he wrote in the Foreword to the third edition of 1967, \"the historical events of the last thirty years...become much clearer by introducing Paul Federn's concept of mortido\". \n\nBerne saw mortido as activating such forces as hate and cruelty, blinding anger and social hostilities; and considered that inwardly directed mortido underlay the phenomena of guilt and self-punishment, as well as their clinical exacerbations in the form of depression or melancholia.\n\nBerne saw sexual acts as gratifying mortido at the same time as libido; and recognised that on occasion the former becomes more important sexually than the latter, as in sadomasochism and destructive emotional relationships.\n\nBerne's concern with the role of mortido in individuals and groups, social formations and nations, arguably continued throughout all his later writings.\n\nJean Laplanche has explored repeatedly the question of mortido, and of how far a distinctive instinct of destruction can be identified in parallel to the forces of libido.\n\nThe importance for the individual of integrating mortido in their life, as opposed to splitting it off and disowning it, has been taken up by figures like Robert Bly in the men's movement.\n\nThe term has also been applied in contemporary expositions of the Cabbala.\n\n"}
{"id": "7760245", "url": "https://en.wikipedia.org/wiki?curid=7760245", "title": "Mutant (Marvel Comics)", "text": "Mutant (Marvel Comics)\n\nIn American comic books published by Marvel Comics, a mutant is a human being that possesses a genetic trait called the X-gene. It causes the mutant to develop superhuman powers that manifest at puberty. Human mutants are sometimes referred to as a human subspecies Homo sapiens superior, or simply Homo superior. Mutants are the evolutionary progeny of \"Homo sapiens\", and are generally assumed to be the next stage in human evolution. The accuracy of this is the subject of much debate in the Marvel Universe.\n\nUnlike Marvel's mutates, which are characters who develop their powers only after exposure to outside stimuli or energies (such as the Hulk, Spider-Man, The Fantastic Four, and Absorbing Man), mutants have actual genetic mutations.\n\nA March 1952 story in \"Amazing Detective Cases\" #11 called \"The Weird Woman\" tells of a woman describing herself as a mutant who seeks a similarly superhuman mate.\n\nRoger Carstairs, a mutant who can create illusions, is shown in \"Man Comics\" #28, dated September 1953.\n\nA character with superhuman powers, born from a radiation-exposed parent, was seen in \"The Man with the Atomic Brain!\" in \"Journey into Mystery\" #52 in May 1959; although not specifically called a \"mutant\", his origin is consistent with one.\n\nA little-known story in \"Tales of Suspense\" #6 (November 1959) titled \"The Mutants and Me!\" was one of the first Marvel (then known as Atlas) stories to feature a named \"mutant\".\n\nTad Carter, a mutant with telekinetic powers, is shown in Amazing Adult Fantasy #14, dated July 1962.\n\nThe modern concept of mutants as an independent subspecies was created and utilized by Marvel editor/writer Stan Lee in the early 1960s, as a means to create a large number of superheroes and supervillains without having to think of a separate origin for each one. As part of the concept, Lee decided that these mutant teenagers should, like ordinary ones, attend school in order to better cope with the world, in this case Xavier's School for Gifted Youngsters. These mutants first appeared in the superhero series \"X-Men\", which debuted in 1963. Marvel later introduced several additional mutant superhero teams, including The New Mutants, X-Factor, Excalibur, X-Force, and Generation X.\n\nOfficially, Namor the Sub-Mariner is considered the first mutant superhero whom Marvel Comics ever published, debuting in 1939. However, Namor was not actually described as a mutant until decades after his first appearance. The same is true of Toro, partner of the android Human Torch introduced in 1940.\n\nAn Omega-level mutant is one with the most powerful genetic potential of their mutant abilities. The term was first seen in the 1986 issue \"Uncanny X-Men\" #208, but was completely unexplained beyond the obvious implication of it referring to an exceptional level of power. The term was not seen again until the 2001 limited series \"X-Men Forever\". Some abilities depicted by mutants described as Omega-level include immortality, reality warping, highly destructive energy projection, extreme manipulation of matter, energy, time and space, high psionic ability with strong telepathy and telekinesis, and the potential to exist beyond the boundaries of the known physical universe. No firm definition has been offered in comics. Examples of mutants that have been confirmed as Omega-level include Franklin Richards, Jean Grey, Hyperstorm, Nate Grey, Stryfe, Mister M, Exodus, Quentin Quire, Vulcan, Rachel Summers, Iceman, Proteus, Legion, Elixir and Psylocke.\n\nIntroduced in Chris Claremont's \"X-Treme X-Men\", a character known as Vargas claims to be humanity's natural response to mutants. Vargas was born at the epitome of peak physical skill, having superhuman levels of strength, speed, reflexes, agility, stamina, and durability. Vargas also seems to be immune to various mutant abilities (such as Rogue's absorption and Psylocke's telekinetic blast).\n\nCreated by Rob Liefeld, Externals are immortal mutants whose powers have allowed them to exist for centuries. Eventually, most of the Externals are killed by Selene. Gideon, Selene, and Apocalypse are examples of Externals.\n\nCheyarafim and Neyaphem first appear in \"Uncanny X-Men\" #429. According to the character Azazel, the Cheyarafim are a group of angel-like mutants who were the traditional enemies of the Neyaphem, a demonic-looking group of mutants who lived in Biblical times. The Cheyarafim were fanatics who had a strict, absolutist view of morality which led them into conflict with the Neyaphem. This escalated into a holy war, causing the Neyaphem to be exiled into an alternate dimension. What happened to the Cheyarafim after this has not been revealed.\n\nThe X-Man Angel is said to be descended from Cheyarafim, while Nightcrawler is supposedly the son of a Neyaphem, Azazel. It is unknown if Joshua Guthrie, known as Icarus, is a descendant of the Cheyarafim.\n\nMaximus Lobo claims to be a part of a mutant sub-species of feral, wolf-like mutants, whom he calls the Dominant Species. He later tries to recruit Wolf Cub into his ranks, to no avail. A few years later, another mutant, Romulus, claims that some human mutants evolved from canines instead of primates. Mutants who are a part of this group include Romulus, Wolverine, Daken, Sabretooth, Wolfsbane, Wild Child, Thornn, Feral, and Wolf Cub, with X-23 and the Native as other likely candidates. These groups appear to be one and the same.\n\nIntroduced in the second series of \"X-Factor\", a changeling is a mutant whose powers manifest at birth. Jamie Madrox (Multiple Man) and Damian Tryp are examples of this sub-class.\n\nHumans are not the only species to have mutant subspecies. Ariel, Longshot, Cerise, Ultra Girl, and Warlock are examples of mutant aliens.\n\nAs a fictional oppressed minority, mutants are often used as extended metaphors for real-world people and situations. In 1982, \"X-Men\" writer Chris Claremont said, \"[mutants] are hated, feared and despised collectively by humanity for no other reason than that they are mutants. So what we have here, intended or not, is a book that is about racism, bigotry and prejudice.\"\n\nDanny Fingeroth writes extensively in his book \"Superman on the Couch\" about the appeal of mutants and their meaning to society:\nAn obvious parallel between homosexuality and mutation is drawn in the feature film \"X2\", where Iceman's mother asks, \"Have you tried \"not\" being a mutant?\" This question (or various forms thereof) is common among parents who find out their children are gay. In the 2011 film \"\", Hank McCoy (later known as Beast), upon being outed to a colleague as a mutant, responds, \"You didn't ask, so I didn't tell.\"\nIn his article \"Super Heroes, a Modern Mythology\", Richard Reynolds writes:\n\nWithin the \"Earth X\" universe, the powers of the vast majority of Marvel's human superheroes were revealed to have been the result of genetic manipulation by the Celestials millions of years in the past.\n\nIn the Ultimate Marvel universe within the pages of the \"Ultimate Origins\" #1, it is revealed that super-powered \"mutants\" were artificially created via genetic modification by the Weapon X program in a laboratory in Alberta, Canada in October 1943. The project was an attempt to produce a supersoldier, inspired by the existence of Captain America. James Howlett was the first individual to be so modified. At some later point, possibly during a confrontation between Magneto and his parents, the mutant trigger was released into the environment worldwide, leading to the appearance of mutants in the general population. Following the events of the \"Ultimatum\" storyline, information concerning the origins of mutancy was made public and steps were taken in the US to make being a mutant illegal. While the move apparently has majority support among the non-mutant population, a vocal minority has voiced concern that it will lead to witch-hunts and genocide.\n\n"}
{"id": "29584884", "url": "https://en.wikipedia.org/wiki?curid=29584884", "title": "Pedro Déniz", "text": "Pedro Déniz\n\nPedro J. Déniz Acosta (born 1964, Santa Brígida, Las Palmas de Gran Canaria, Canary Islands, Spain) is a Spanish interdisciplinary artist who has developed art projects and experiences ranging from the objectual art to installation art, from art photography to video art, cultivating art intervention, performance art, visual poetry and graphic design.\n\nIn 1998 inaugurated the project \"La Puente\", which represents a turning point in his work, opening channels of communication and artistic action by launching bottles at the height of the zero meridian message wrapper made by different artists. \nIn 2002 his first exhibition in the Hall of San Antonio Abad from Centro Atlántico de Arte Moderno, CAAM. That same year, presented along the Moroccan artist Mounir Fatmi the performance \"Power line - Imagen y poder\" in Espacio C Contemporary Art, Camargo, Santander.\nThe two artists collaborate again a year later at the Second International Contemporary Art Hybrid Spaces, Osorio, Teror, Gran Canaria with the performance \"Hybrid Spaces\", both actions intended to place the viewer at a point of reflection on the cultural frictions. As part of Performando, meeting of performances held in Las Palmas de Gran Canaria running performance \"Paper Memories\" by building a wall through fragments of the press in order to question the information that we relegate the media.\nIn October 2003, includes a photographic installation called \"Welcome\" in Contemporary African Photography Biennale, 5èmes Rencontres Africaines de la Photographie, Bamako, Mali. The same year, \"Dulce-sal\" exhibits a photographic installation based on migration issues included in the collective project of the Aula del Mestizaje, University of Las Palmas de Gran Canaria. The artist Ricardo Basbaum (Brazil), Robin Rhode (South Africa) and Pedro Déniz showed the group exhibition called \"3 Scenarios\", the installation was from March to April in the Hall of San Antonio Abad del Centro Atlántico de Arte Moderno CAAM in 2005.\nOne of his visual poems selected for the exhibition \"Disagreements. About art, politics and public sphere in the Spanish State\", experimental poetry section at the Barcelona Museum of Contemporary Art MACBA. That same year he presented his video \"May day\" in the context of the 6th Biennial Photography in Bamako, Mali. During the months of April and May 2006 participated in the second part of \"Violencias Urban(istic)a\" to the MAPFRE Foundation in Las Palmas de Gran Canaria and Santa Cruz de Tenerife, Canary Islands Government. That year also presented at the Biennial of Dakar his work \"Ajuy\" within the multimedia project Meeting Point.\nOctober to December 2007 presented in the Gallery of the Metropolitan Autonomous University (UAM) in Mexico City \"Transit, the territories of the reality\" with artists Jose Domingo Diaz and Luzardo who seek challenge his works the concept of the reality imposed by the dominant culture. In April 2008 he participated with the performance \"Jappy New Year\" in the Circo Project, an international event and Audiovisual Performance, Havana, Cuba.\nUnder the Interaction Meeting organized by Gran Canaria Espacio Digital introduces its performance \"Transit/Blindness\" where it investigates the loss of the root causes by migration. In 2009 exhibited at the 10th Havana Biennial interactive installation \"Trench of thought\" where Déniz reflected through his work the fragility of thought plural. In 2016 the Galerie St. Gertrude, Hamburg, realized in cooperation with the artist the first exhibition in Germany.\n\n\n"}
{"id": "526609", "url": "https://en.wikipedia.org/wiki?curid=526609", "title": "Physical abuse", "text": "Physical abuse\n\nPhysical abuse is any intentional act causing injury or trauma to another person or animal by way of bodily contact. In most cases, children are the victims of physical abuse, but adults can also be victims, as in cases of domestic violence or workplace aggression. Alternative terms sometimes used include physical assault or physical violence, and may also include sexual abuse. Physical abuse may involve more than one abuser, and more than one victim.\n\nPhysical abuse means any non-accidental act or behavior causing injury, trauma, or other physical suffering or bodily harm. Abusive acts toward children can often result from parents' attempts at child discipline through excessive corporal punishment.\n\nA number of causes of physical abuse against children have been identified, the most common of which, according to Mash and Wolfe, being:\n\nPhysically abused children are at risk for later interpersonal problems involving aggressive behavior, and adolescents are at a much greater risk for substance abuse. In addition, symptoms of depression, emotional distress, and suicidal ideation are also common features of people who have been physically abused. Studies have also shown that children with a history of physical abuse may meet DSM-IV-TR criteria for posttraumatic stress disorder (PTSD). As many as one-third of children who experience physical abuse are also at risk to become abusive as adults \n\nResearchers have pointed to other potential psycho-biological effects of child physical abuse on parenting, when abused children become adults. These recent findings may, at least in part, be carried forward by epigenetic changes that impact the regulation of stress physiology. Many other potentially important consequences of childhood physical abuse on adolescent and adult physical and mental health and development have been documented via the Adverse Childhood Experiences (ACE) studies.\n\nSeeking treatment is unlikely for a majority of people that are physically abused, and the ones who are seeking treatment are usually under some form of legal constraint. The prevention and treatment options for physically abused children include: enhancing positive experiences early in the development of the parent-child relationship, as well as changing how parents teach, discipline, and attend to their children. \n\nEvidence-based interventions include cognitive behavioral therapy (CBT) as well as video-feedback interventions and child-parent psychodynamic psychotherapy; all of which specifically target anger patterns and distorted beliefs, and offer training and/or reflection, support, and modelling that focuses on parenting skills and expectations, as well as increasing empathy for the child by supporting the parent's taking the child's perspective. \n\nThese forms of treatment may include training in social competence and management of daily demands in an effort to decrease parental stress, which is a known risk factor for physical abuse. Although these treatment and prevention strategies are to help children and parents of children who have been abused, some of these methods can also be applied to adults who have physically abused.\n\nPhysical abuse has been described among Adélie penguins in Antarctica.\n"}
{"id": "573313", "url": "https://en.wikipedia.org/wiki?curid=573313", "title": "Physical disability", "text": "Physical disability\n\nA physical disability is a limitation on a person's physical functioning, mobility, dexterity or stamina. Other physical disabilities include impairments which limit other facets of daily living, such as respiratory disorders, blindness, epilepsy and sleep disorders.\n\nPrenatal disabilities are acquired before birth. These may be due to diseases or substances that the mother has been exposed to during pregnancy, embryonic or fetal developmental accidents or genetic disorders.\n\nPerinatal disabilities are acquired between some weeks before to up to four weeks after birth in humans. These can be due to prolonged lack of oxygen or obstruction of the respiratory tract, damage to the brain during birth (due to the accidental misuse of forceps, for example) or the baby being born prematurely. These may also be caused due to genetic disorders or accidents.\n\nPost-natal disabilities are gained after birth. They can be due to accidents, injuries, obesity, infection or other illnesses. These may also be caused due to genetic disorders.\n\nMobility impairment includes physical defects, including upper or lower limb loss or impairment, poor manual dexterity, and damage to one or multiple organs of the body. Disability in mobility can be a congenital or acquired problem or a consequence of disease. People who have a broken skeletal structure also fall into this category.\n\nVisual impairment is another type of physical impairment. There are hundreds of thousands of people who suffer greatly from minor to various serious vision injuries or impairments. These types of injuries can also result in severe problems or diseases such as blindness and ocular trauma. Some other types of vision impairment include scratched cornea, scratches on the sclera, diabetes-related eye conditions, dry eyes and corneal graft, macular degeneration in old age and retinal detachment. \n\nHearing loss is a partial or total inability to hear. Deaf and hard of hearing people have a rich culture and benefit from learning sign language for communication purposes. People who are only partially deaf can sometimes make use of hearing aids to improve their hearing ability.\n\nPhysical impairment can also be attributed to disorders causing, among others, sleep deficiency, chronic fatigue, chronic pain, and seizures.\n\n"}
{"id": "1005874", "url": "https://en.wikipedia.org/wiki?curid=1005874", "title": "Principle", "text": "Principle\n\nA principle is a concept or value that is a guide for behavior or evaluation. In law, it is a rule that has to be or usually is to be followed, or can be desirably followed, or is an inevitable consequence of something, such as the laws observed in nature or the way that a system is constructed. The principles of such a system are understood by its users as the essential characteristics of the system, or reflecting system's designed purpose, and the effective operation or use of which would be impossible if any one of the principles was to be ignored. A system may be explicitly based on and implemented from a document of principles as was done in IBM's 360/370 \"Principles of Operation\".\n\nExamples of principles are, entropy in a number of fields, least action in physics, those in descriptive comprehensive and fundamental law: doctrines or assumptions forming normative rules of conduct, separation of church and state in statecraft, the central dogma of molecular biology, fairness in ethics, etc.\n\nIn common English, it is a substantive and collective term referring to rule governance, the absence of which, being \"unprincipled\", is considered a character defect. It may also be used to declare that a reality has diverged from some ideal or norm as when something is said to be true only \"in principle\" but not in fact.\n\nA principle represents values that orient and rule the conduct of persons in a particular society. To \"act on principle\" is to act in accordance with one's moral ideals. Principles are absorbed in childhood through a process of socialization. There is a presumption of liberty of individuals that is restrained. Exemplary principles include First, do no harm, the golden rule and the doctrine of the mean.\n\nIt represents a set of values that inspire the written norms that organize the life of a society submitting to the powers of an authority, generally the State. The law establishes a legal obligation, in a coercive way; it therefore acts as principle conditioning of the action that limits the liberty of the individuals. See, for examples, the territorial principle, homestead principle, and precautionary principle.\n\nArchimedes principle, relating buoyancy to the weight of displaced water, is an early example of a law in science. Another early one developed by Malthus is the \"population principle\", now called the Malthusian principle. Freud also wrote on principles, especially the reality principle necessary to keep the id and pleasure principle in check. Biologists use the principle of priority and principle of Binominal nomenclature for precision in naming species. There are many principles observed in physics, notably in cosmology which observes the mediocrity principle, the anthropic principle, the principle of relativity and the cosmological principle. Other well-known principles include the uncertainty principle in quantum mechanics and the pigeonhole principle and superposition principle in mathematics.\n\nThe principle states that every event has a rational explanation. The principle has a variety of expressions, all of which are perhaps best summarized by the following:\n\nHowever, one realizes that in every sentence there is a direct relation between the predicate and the subject. To say that \"the Earth is round\", corresponds to a direct relation between the subject and the predicate.\n\nAccording to Aristotle, “It is impossible for the same thing to belong and not to belong at the same time to the same thing and in the same respect.” For example, it is not possible that in exactly the same moment and place, it rains and doesn't rain.\n\nThe principle of the excluding third or \"principium tertium exclusum\" is a principle of the traditional logic formulated canonically by Leibniz as: either \"A\" is \"B\" or \"A\" isn't \"B\". It is read the following way: either \"P\" is true, or its denial ¬\"P\" is.\nIt is also known as \"tertium non datur\" ('A third (thing) is not). Classically it is considered to be one of the most important fundamental principles or laws of thought (along with the principles of identity, no contradiction and sufficient reason).\n"}
{"id": "31392139", "url": "https://en.wikipedia.org/wiki?curid=31392139", "title": "Prisoner", "text": "Prisoner\n\nA prisoner, (also known as an inmate or detainee) is a person who is deprived of liberty against his or her will. This can be by confinement, captivity, or by forcible restraint. The term applies particularly to serving a prison sentence in a prison. This term does not apply to defendants who are pre-trial.\n\n\"Prisoner\" is a legal term for a person who is imprisoned.\n\nIn section 1 of the Prison Security Act 1992, the word \"prisoner\" means any person for the time being in a prison as a result of any requirement imposed by a court or otherwise that he be detained in legal custody.\n\n\"Prisoner\" was a legal term for a person prosecuted for felony. It was not applicable to a person prosecuted for misdemeanour. The abolition of the distinction between felony and misdemeanour by section 1 of the Criminal Law Act 1967 has rendered this distinction obsolete.\n\nGlanville Williams described as \"invidious\" the practice of using the term \"prisoner\" in reference to a person who had not been convicted.\n\nThe earliest evidence of the existence of the prisoner dates back to 8,000 BC from prehistoric graves in Lower Egypt. This evidence suggests that people from Libya enslaved a San-like tribe.\n\nAmong the most extreme adverse effects suffered by prisoners, appear to be caused by solitary confinement for long durations. When held in \"Special Housing Units\" (SHU), prisoners are subject to sensory deprivation and lack of social contact that can have a severe negative impact on their mental health.\n\nLong durations may lead to depression and changes to brain physiology. In the absence of a social context that is needed to validate perceptions of their environment, prisoners become highly malleable, abnormally sensitive, and exhibit increased vulnerability to the influence of those controlling their environment. Social connection and the support provided from social interaction are prerequisite to long-term social adjustment as a prisoner.\n\nPrisoners exhibit the paradoxical effect of social withdrawal after long periods of solitary confinement. A shift takes place from a craving for greater social contact, to a fear of it. They may grow lethargic and apathetic, and no longer be able to control their own conduct when released from solitary confinement. They can come to depend upon the prison structure to control and limit their conduct.\n\nLong-term stays in solitary confinement can cause prisoners to develop clinical depression, and long-term impulse control disorder. Those with pre-existing mental illnesses are at a higher risk for developing psychiatric symptoms. Some common behaviours are self-mutilation, suicidal tendencies, and psychosis.\n\nA psychopathological condition identified as \"SHU syndrome\" has been observed among such prisoners. Symptoms are characterized as problems with concentration and memory, distortions of perception, and hallucinations. Most convicts suffering from SHU syndrome exhibit extreme generalized anxiety and panic disorder, with some suffering amnesia.\n\nThe psychological syndrome known as Stockholm syndrome, describes a paradoxical phenomenon where, over time, hostages develop positive feelings towards their captors.\n\nThe founding of ethnographic prison sociology as a discipline, from which most of the meaningful knowledge of prison life and culture stems, is commonly credited to the publication of two key texts: Donald Clemmer's \"The Prison Community\", which was first published in 1940 and republished in 1958; and Gresham Sykes classic study \"The Society of Captives\", which was also published in 1958. Clemmer's text, based on his study of 2,400 convicts over three years at the Menard Branch of the Illinois State Penitentiary where he worked as a clinical sociologist, propagated the notion of the existence of a distinct inmate culture and society with values and norms antithetical to both the prison authority and the wider society.\nIn this world, for Clemmer, these values, formalized as the \"inmate code\", provided behavioural precepts that unified prisoners and fostered antagonism to prison officers and the prison institution as a whole. The process whereby inmates acquired this set of values and behavioural guidelines as they adapted to prison life he termed \"prisonization\", which he defined as the \"taking on, in greater or lesser degree, the folkways, mores, customs and general culture of the penitentiary'. However, while Clemmer argued that all prisoners experienced some degree of prisonization this was not a uniform process and factors such as the extent to which a prisoner involved himself in primary group relations in the prison and the degree to which he identified with the external society all had a considerable impact.\n\nPrisonization as the inculcation of a convict culture was defined by identification with primary groups in prison, the use of prison slang and argot, the adoption of specified rituals and a hostility to prison authority in contrast to inmate solidarity and was asserted by Clemmer to create individuals who were acculturated into a criminal and deviant way of life that stymied all attempts to reform their behaviour.\n\nOpposed to these theories, several European sociologists have shown that inmates were often fragmented and the links they have with society are often stronger than those forged in prison, particularly through the action of work on time perception\n\nThe convict code was theorized as a set of tacit behavioural norms which exercised a pervasive impact on the conduct of prisoners. Competency in following the routines demanded by the code partly determined the inmate’s identity as a convict. As a set of values and behavioural guidelines, the convict code referred to the behaviour of inmates in antagonising staff members and to the mutual solidarity between inmates as well as the tendency to the non-disclosure to prison authorities of prisoner activities and to resistance to rehabilitation programmes. Thus, it was seen as providing an expression and form of communal resistance and allowed for the psychological survival of the individual under extremely repressive and regimented systems of carceral control.\n\nSykes outlined some of the most salient points of this code as it applied in the post-war period in the United States:\n\nBoth federal and state laws govern the rights of prisoners. Prisoners in the United States do not have full rights under the Constitution, however, they are protected by Amendment VIII which prohibits cruel and unusual punishment.\n\nGrowing research associates education with a number of positive outcomes for prisoners, the institution, and society. Although at the time of the ban’s enactment there was limited knowledge about the relationship between education and recidivism, there is growing merit to idea that education in prison is a preventative to re-incarceration. Several studies help illustrate the point. For example, one study in 1997 that focused on 3,200 prisoners in Maryland, Minnesota, and Ohio, showed that simply attending school behind bars reduced the likelihood of re-incarceration by 29 percent. In 2000, the Texas Department of Education conducted a longitudinal study of 883 men and women who earned college degrees while incarcerated, finding recidivism rates between 27.2 percent (completion of an AA degree) and 7.8 percent (completion of a BA degree), compared to a system-wide recidivism rate between 40 and 43 percent.10 One report, sponsored by the Correctional Education Association, focused on recidivism in three states, concluding that education prevented crime. More recently, a 2013 Department of Justice funded study from the RAND Corporation found that incarcerated individuals who participated in correctional education were 43% less likely to return to prison within 3 years than prisoners who did not participate in such programs. The research implies that education has the potential to impact recidivism rates positively by lowering them.\n\n\nOther types of prisoners can include those under police arrest, house arrest, those in psychiatric institutions, internment camps, and peoples restricted to a specific area such as Jews in the Warsaw ghetto.\n\n\nLight\n\n\n"}
{"id": "4392514", "url": "https://en.wikipedia.org/wiki?curid=4392514", "title": "Proportional reasoning", "text": "Proportional reasoning\n\n\"Proportional reasoning \"is ...\n\nIn mathematics and in physics, proportionality is a mathematical relation between two quantities; it can be expressed as an equality of two ratios:\n\nFunctionally, proportionality can be a relationship between variables in a mathematical equation. For example, given the following equation for the force of gravity (according to Newton):\n\nthe force of gravity between two masses is directly proportional to the product of the two masses and inversely proportional to the square of the distance between the two masses.\n\nIn Piaget’s model of intellectual development, the fourth and final stage is the formal operational stage. In the classic book “The Growth of Logical Thinking from Childhood to Adolescence” by Jean Piaget and Barbel Inhelder formal operational reasoning takes many forms, including propositional reasoning, deductive logic, separation and control of variables, combinatorial reasoning, and proportional reasoning. Robert Karplus, a science educator in the 1960s and 1970s, investigated all these forms of reasoning in adolescents & adults. Mr. Tall-Mr.Short was one of his studies.\n\nComparable reasoning patterns exist for inverse proportion.\n\n Consider a container of colored liquid inside a right triangle where the triangle can be tilted and the water levels on the left and right side can be measured on a built-in scale. This is called a “water triangle”:\n\nSomeone with knowledge about the area of triangles might reason: “Initially the area of the water forming the triangle is 12 since ½ * 4 * 6 = 12. The amount of water doesn’t change so the area won’t change. So the answer is 3 because ½ * 3 * 8 = 12.”\n\nA correct multiplicative answer is relatively rare. By far the most common answer is something like: “2 units because the water level on the right side increased by two units so the water level on the left side must decrease by two units and 4 – 2 = 2.” Less frequently the reason for two units is: “Before there is a total of 10 units because 4 + 6 = 10. The total number of units must stay the same so the answer is 2 because 2 + 8 = 10.”\n\nSo again there are individuals who are not at the formal operational level apply an additive strategy rather than a multiplicative strategy to solve an inverse proportion. And, like the direct proportion, this incorrect strategy appears to be logical to the individual and appears to give a reasonable answer. Students are very surprised when they actually carry out the experiment and tilt the triangle to find the answer is 3 and not 2 as they so confidently predicted.\n\nLet T be the height of Mr. Tall and S be the height of Mr. Short, then the correct multiplicative strategy can be expressed as T/S = 3/2; this is a constant ratio relation. The incorrect additive strategy can be expressed as T – S = 2; this is a constant difference relation. Here is the graph for these two equations. For the numeric values involved in the problem statement, these graphs are “similar” and it is easy to see why individuals consider their incorrect answers perfectly reasonable.\n\nNow consider our inverse proportion using the “water triangle.” Let L be the height of the water on the left side and R be the height of the water on the right side, then the correct multiplicative strategy can be expressed as L * R = 24; this is a constant product relation. The incorrect additive strategy can be expressed as L + R = 10; this is a constant sum relation. Here is the graph for these two equations. For the numeric values involved in the problem statement, these graphs are “similar” and it is easy to see why individuals consider their incorrect answers perfectly reasonable.\n\nAs any experienced teacher will attest, it is not sufficient to simply tell a student his/her answer is incorrect and then instruct the student to use the correct solution. The incorrect strategy has not been “unwired in the brain” and would re-emerge after the current lesson has been completed.\n\nAlso the additive strategies noted above cannot simply be labeled as “incorrect” since they do correctly match other real world situations. For example, consider the following problem:\n\n\"On Independence Day this year Mr. Tall was 6 years old and Mr. Short was 4 years old. On a future Independence Day Mr. Short is 6 years old. How old will Mr. Tall be on that Independence Day?\"\n\nSimilarly the constant sum relation can be correct for some situations. Consider the following problem.\n\n\"There are four beavers on the left side of a river and six beavers on the right side of the river. At a later time with the same group of beavers there are eight beavers on the right side of the river. How many beavers will there be on the left side?\"\n\nSo there are situations where the additive relations (constant difference and constant sum) are correct and other situations where the multiplicative relations (constant ratio and constant product) are correct.\n\nIt is critically important that students on their own recognize that their current mode of reasoning, say that it is additive, is inappropriate for a multiplicative problem they are trying to solve. Robert Karplus developed a model of learning he called the learning cycle that facilitates the acquisition of new reasoning skills.\n\n\nHands-on activities are extremely useful in the learning cycle. After making predictions about the height of Mr. Tall in paper clips, the measuring tools can be introduced and the students can test their strategies. For the student using a constant difference relation, actual measurement will show that Mr. Tall is actually nine paper clips high and this will set up some cognitive dissonance.\n\nThe same is true for the inverse relations. Here is a picture of two students working with the “water triangle.” Given the problem noted above, most students predict the water level on the left side will drop to two units when the water triangle is tilted. When they carry out the experiment and see that the answer is 3 units, this establishes some cognitive dissonance. This is a prime time for the teacher to move the lesson into the second stage of the learning cycle.\n\nIt is important that the students not over apply the multiplicative strategies they learn. Therefore, some of the hands-on activities might not be based on a multiplicative relation. Here is a picture of two students working with an apparatus where the constant sum relation is correct.\n\nIt is not always possible or feasible to put carefully designed hands-on activities into the hands of students. Also, older audiences do not always react well to using hands-on experimentation. However, it is often possible to introduce cognitive dissonance through thought experiments.\n\nIn all the experiments noted above there are two variables whose values change based on a fixed relation. Consider the following problem that is similar to the Mr. Tall and Mr. Short problem.\n\n\"Here is a photograph of a father and a daughter. In this picture the daughter is 4 cm high and the father is 6 cm high. They decided to enlarge the picture and in the bigger picture the daughter is 6 cm high. How high is the father in the larger picture?\"\n\nA very common answer for an individual using an additive relation is 8 cm because the father is always 2 cm higher than his daughter. So now ask this student the following question:\nSuppose they made a very small version of the original picture and in this small picture the father is 2 cm high. How high will the daughter be in this small picture?\"\n\nThe student quickly realizes that the strategy “the father is always 2 cm higher than his daughter” is not correct. This can also be achieved by exploring the other extreme where the original picture is blown up to poster size and the daughter is 100 cm high. How high will the father be in this poster? A student answering 102 cm realizes that the father and daughter are almost the same height which cannot be right. Once cognitive dissonance is present, the teacher can introduce the correct relation, constant ratio.\n\nThe student can also be encouraged to conduct their own thought experiments, such as “what if the height of the daughter doubles in an enlargement, what will happen to the height of the father?” Most students, including those still at the concrete operational stage, will quickly answer that the father’s height must also double. The abstract thought experiment is: “Suppose that one of the variables is doubled in value, how will the other variable change?” If the answer is “double”, then this may be a constant ratio problem. But if the answer is not double, such as for the age problem with Mr. Tall and Mr. Short given above, then it is not a constant ratio problem.\n\nFor inverse relations, such as the “water triangle”, limiting cases can also introduce cognitive dissonance. For example:\n\n\"Given the initial conditions with the water level on the left at 4 units and the water level on the right at 6 units, predict what is the water level on the left if the triangle is tilted until the water level on the right is 10 units.\"\n\nStudents will abandon the additive strategy at this point realizing that 0 cannot be the correct answer. A thought experiment can be performed for inverse relations. If one variable doubles in value, what happens to the other variable? If the answer is ½ then this might be a constant product relation (that is, an inverse proportion).\n\nPlotting the values of variables can also be a valuable tool for identifying whether two variables are directly proportional or not. If they are directly proportional, then the values should be on a straight line and that line should intersect the origin.\n\nThe four functional relations noted above, constant sum, constant difference, constant product, and constant ratio, are based on the four arithmetic operations students are most familiar with, namely, addition, subtraction, multiplication and division. Most relations in the real world do not fall into one of these categories. However, if students learn simple techniques such as thought experiments and plotting graphs, they will be able to apply these techniques to more complex situations.\n\nAgain, consider Newton’s equation for the force of gravity:\nIf a student understands the functional relation between the variables, then he/she should be able to answer the following thought experiments.\n\nWhat would happen to the force of gravitational attraction if:\n\nGenerally, thought experiments must be confirmed by experimental results. Many children and adults when asked to perform a thought experiment on the mass of an object and the velocity with which it falls to the earth might say that when the mass is doubled then the object will fall twice as fast. However, experimental results do not back up this “logical” thought experiment so it is always essential that theoretical results agree with experimental data.\n"}
{"id": "23359426", "url": "https://en.wikipedia.org/wiki?curid=23359426", "title": "Quino checkerspot", "text": "Quino checkerspot\n\nThe Quino checkerspot (\"Euphydryas editha quino\") is a butterfly native to southern California and northwestern Mexico. It is a subspecies of the common Edith's checkerspot (\"Euphydryas editha\") and the second such subspecies to be listed under the federal Endangered Species Act. This species, like many others, has undergone several changes in nomenclature and classification. It was originally described as \"Melitaea quino\" in 1863 and then in 1929 it was reduced to a subspecies of \"Euphydryas chalcedona\". In 1998 it was concluded through Hans Hermann Behr's 1863 description that it should be classified as \"E. editha\", not \"E. chalcedona\". The species was synonymous with \"E. editha wrighti\", a junior synonym for \"E. e. quino\", thus becoming scientifically accepted as \"E. editha quino\".\n\nA member of the brush-footed butterfly family, Nymphalidae, the Quino checkerspot is a medium-sized butterfly with a wingspan of approximately 3 cm. The dorsal wing surfaces are a colorful checkerboard of brown, red and yellow spots. The Quino differs from other \"E. editha\" subspecies in that its spots tend to be a darker red.\n\nIt also differentiates itself through its size and larval and pupal phenotypes. The ventral side of the butterfly are dominated by a checkered red and cream pattern. Its abdomen has red stripes across the dorsal side. After a second molt, the Quino checkerspot is recognized by the dark black coloration and row of 8 to 9 orange tubercles on their back. Before the larvae first molt they are mostly yellowish. After first molt and before their second molt they are gray with black markings. The pupae are mottled black on a blue-gray background.\n\nThe Quino checkerspot is easily confused in the field by inexperienced butterfly searchers. It is generally confused with three other co-occurring butterfly species, the Chalcedon or variable checkerspot, (\"Euphydryas chalcedona\"), Gabb's checkerspot (\"Chlosyne gabbii\") and Wright's checkerspot (\"Thessalia leonira wrighti\").\n\nThe life cycle of the Quino checkerspot closely mirrors that of the close Bay checkerspot. They share the same host plant and similar chronology of developmental stages.\n\nThe obvious factor in the decline of the Quino checkerspot is urban development. Much of the historic scrub land that it occupied, much like the Mission blue butterfly, also endangered, has been built over. The persisting habitat faces other threats. Invasive species, in the form of non-native plant life and overgrazing are just two of the hurdles facing the recovery of the Quino checkerspot. Today, there are eight populations of the Quino known.\n\nHabitat declined and, thus, distribution and population of the Quino checkerspot has been greatly reduced during the last 100 years. Nearly all of the blame lies in agricultural and urban development in southern California. The other impactors to the decline include non-native grasses and fire suppression practices as well as grazing. The Quino checkerspot became the second subspecies of \"Euphydryas editha\" to be listed under the Endangered Species Act (ESA). The first was the Bay checkerspot.\n\nCurrently, the Quino checkerspot is only found in a very few locales. Western Riverside County, southern San Diego County and northern Baja California, Mexico. The animal's historic range once included much of coastal California south of Ventura County as well as the inland valleys south of the Tehachapi Mountains. Regardless, more than 75% of the butterfly's original range has been lost. The range loss translates directly into population decline. Quino checkerspot butterfly populations appear to have decreased by more than 95% range wide. \n"}
{"id": "1809033", "url": "https://en.wikipedia.org/wiki?curid=1809033", "title": "Stereotype threat", "text": "Stereotype threat\n\nStereotype threat is a situational predicament in which people are or feel themselves to be at risk of conforming to stereotypes about their social group. Since its introduction into the academic literature, stereotype threat has become one of the most widely studied topics in the field of social psychology.\nStereotype threat has been argued to show a reduction in the performance of individuals who belong to negatively stereotyped groups. According to the theory, if negative stereotypes are present regarding a specific group, group members are likely to become anxious about their performance, which may hinder their ability to perform to their full potential. Importantly, the individual does not need to subscribe to the stereotype for it to be activated. It is hypothesized that the mechanism through which anxiety (induced by the activation of the stereotype) decreases performance is by depleting working memory (especially the phonological aspects of the working memory system).\n\nHowever, studies have cautioned that stereotype threat should not be interpreted as a factor in real-world performance gaps. One review has voiced concerns that the effect has been over-estimated for schoolgirls and that the field likely suffers from publication bias.\n\nStereotype threat is purportedly a contributing factor to long-standing racial and gender gaps in academic performance. It may occur whenever an individual's performance might confirm a negative stereotype because stereotype threat is thought to arise from a particular situation, rather than from an individual's personality traits or characteristics. Since most people have at least one social identity which is negatively stereotyped, most people are vulnerable to stereotype threat if they encounter a situation in which the stereotype is relevant. Situational factors that increase stereotype threat can include the difficulty of the task, the belief that the task measures their abilities, and the relevance of the stereotype to the task. Individuals show higher degrees of stereotype threat on tasks they wish to perform well on and when they identify strongly with the stereotyped group. These effects are also increased when they expect discrimination due to their identification with a negatively stereotyped group. Repeated experiences of stereotype threat can lead to a vicious circle of diminished confidence, poor performance, and loss of interest in the relevant area of achievement.\n\nThe opposite of stereotype threat is stereotype boost, which is when people perform better than they otherwise would have, because of exposure to positive stereotypes about their social group. A variant of stereotype boost is stereotype lift, which is people achieving better performance because of exposure to negative stereotypes about other social groups.\n\nIn 1995, Claude Steele and Joshua Aronson performed the first experiments demonstrating that stereotype threat can undermine intellectual performance. They had African-American and European-American college students take a difficult verbal portion of the Graduate Record Examination test. As would be expected based on national averages, the African-American students did not perform as well on the test. Steele and Aronson split students into three groups: stereotype-threat (in which the test was described as being \"diagnostic of intellectual ability\"), non-stereotype threat (in which the test was described as \"a laboratory problem-solving task that was nondiagnostic of ability\"), and a third condition (in which the test was again described as nondiagnostic of ability, but participants were asked to view the difficult test as a challenge). All three groups received the same test.\n\nAdjusted for previous SAT scores, subjects in the non-diagnostic-challenge condition performed significantly better than those in the non-diagnostic-only condition and those in the diagnostic condition. In the first experiment, the race-by-condition interaction was marginally significant. However, the second study reported in the same paper found a significant interaction effect of race and condition. This suggested that placement in the diagnostic condition significantly impacted African Americans compared with European Americans.\n\nSteele and Aronson concluded that changing the instructions on the test could reduce African-American students' concern about confirming a negative stereotype about their group. Supporting this conclusion, they found that African-American students who regarded the test as a measure of intelligence had more thoughts related to negative stereotypes of their group. Steele and Aronson measured this through a word completion task. They found that African Americans who thought the test measured intelligence were more likely to complete word fragments using words associated with relevant negative stereotypes (e.g., completing \"__mb\" as \"dumb\" rather than as \"numb\").\n\nMore than 300 published papers show the effects of stereotype threat on performance in a variety of domains. The strength of the stereotype threat that occurs depends on how the task is framed. If a task is framed to be neutral, stereotype threat is not likely to occur; however, if tasks are framed in terms of active stereotypes, participants are likely to perform worse on the task. For example, a study on chess players revealed that female players performed more poorly than expected when they were told they would be playing against a male opponent. In contrast, women who were told that their opponent was female performed as would be predicted by past ratings of performance. Female participants who were made aware of the stereotype of females performing worse at chess than males performed worse in their chess games.\n\nResearchers Vishal Gupta, Daniel Turban, and Nachiket Bhawe extended stereotype threat research to entrepreneurship, a traditionally male-stereotyped profession. Their study revealed that stereotype threat can depress women's entrepreneurial intentions while boosting men's intentions. However, when entrepreneurship is presented as a gender-neutral profession, men and women express a similar level of interest in becoming entrepreneurs. Another experiment involved a golf game which was described as a test of \"natural athletic ability\" or of \"sports intelligence\". When it was described as a test of athletic ability, European-American students performed worse, but when the description mentioned intelligence, African-American students performed worse.\n\nOther studies have demonstrated how stereotype threat can negatively affect the performance of European Americans in athletic situations as well as the performance of men who are being tested on their social sensitivity. Although the framing of a task can produce stereotype threat in most individuals, certain individuals appear to be more likely to experience stereotype threat than others. Individuals who highly identify with a particular group appear to be more vulnerable to experiencing stereotype threat than individuals who do not identify strongly with the stereotyped group.\n\nThe mere presence of other people can evoke stereotype threat. In one experiment, women who took a mathematics exam along with two other women got 70% of the answers right, whereas women who took the same exam in the presence of two men got an average score of 55%.\n\nThe goal of a study conducted by Desert, Preaux, and Jund in 2009 was to see if children from lower socioeconomic groups are affected by stereotype threat. The study compared children that were 6–7 years old with children that were 8–9 years old from multiple elementary schools. These children were presented with the Raven's Matrices test, which is an intellectual ability test. Separate groups of children were given directions in an evaluative way and other groups were given directions in a non-evaluative way. The \"evaluative\" group received instructions that are usually given with the Raven Matrices test, while the \"non-evaluative\" group was given directions which made it seem as if the children were simply playing a game. The results showed that third graders performed better on the test than the first graders did, which was expected. However, the lower socioeconomic status children did worse on the test when they received directions in an evaluative way than the higher socioeconomic status children did when they received directions in an evaluative way. These results suggested that the framing of the directions given to the children may have a greater effect on performance than socioeconomic status. This was shown by the differences in performance based on which type of instructions they received. This information can be useful in classroom settings to help improve the performance of students of lower socioeconomic status.\n\nThere have been studies on the effects of stereotype threat based on age. A study was done on 99 senior citizens ranging in age from 60–75 years. These seniors were given multiple tests on certain factors and categories such as memory and physical abilities, and were also asked to evaluate how physically fit they believe themselves to be. Additionally, they were asked to read articles that contained both positive and negative outlooks about seniors, and they watched someone reading the same articles. The goal of this study was to see if priming the participants before the tests would affect performance. The results showed that the control group performed better than those that were primed with either negative or positive words prior to the tests. The control group seemed to feel more confident in their abilities than the other two groups.\n\nMany psychological experiments carried out on Stereotype Threat focus on the physiological effects of negative stereotype threat on performance, looking at both high and low status groups. Scheepers and Ellemers tested the following hypothesis: when assessing a performance situation on the basis of current beliefs the low status group members would show a physiological threat response, and high-status members would also show a physiological threat response when examining a possible alteration of the status quo(Scheepers & Ellemers, 2005). The results of this experiment were in line with expectations. As predicted, participants in the low status condition showed higher blood pressure immediately after the status feedback, while participants in the high-status condition showed a spike in blood pressure while anticipating the second round of the task.\n\nIn 2012, Scheepers et al. hypothesized that when high social power is stimulated 'an efficient cardiovascular pattern (challenge)' is produced, whereas, 'an inefficient cardiovascular pattern' or threat is caused by the activation of low social power (Scheepers, de Wit, Ellemers & Sassenberg, 2012). Two experiments were carried out in order to test this hypothesis. The first experiment looked at power priming and the second experiment related to role play. Both results from these two experiments provided evidence in support for the hypothesis.\n\nCleopatra Abdou and Adam Fingerhut were the first to develop experimental methods to study stereotype threat in a health care context, including the first study indicating that health care stereotype threat is linked with adverse health outcomes and disparities.\n\nStereotype threat concerns how stereotype cues can harm performance. However, in certain situations, stereotype activation can also lead to performance enhancement through stereotype lift or stereotype boost. Stereotype lift increases performance when people are exposed to negative stereotypes about another group. This enhanced performance has been attributed to increases in self-efficacy and decreases in self-doubt as a result of negative outgroup stereotypes. Stereotype boost suggests that positive stereotypes may enhance performance. Stereotype boost occurs when a positive aspect of an individual's social identity is made salient in an identity-relevant domain. Although stereotype boost is similar to stereotype lift in enhancing performance, stereotype lift is the result of a negative outgroup stereotype, whereas stereotype boost occurs due to activation of a positive ingroup stereotype.\n\nConsistent with the positive racial stereotype concerning their superior quantitative skills, Asian American women performed better on a math test when their Asian identity was primed compared to a control condition where no social identity was primed. Conversely, these participants did worse on the math test when instead their gender identity—which is associated with stereotypes of inferior quantitative skills—was made salient, which is consistent with stereotype threat. Two replications of this result have been attempted. In one case, the effect was only reproduced after excluding participants who were unaware of stereotypes about the mathematical abilities of Asians or women, while the other replication failed to reproduce the original results even considering several moderating variables.\n\nAlthough numerous studies demonstrate the effects of stereotype threat on performance, questions remain as to the specific cognitive factors that underlie these effects. Steele and Aronson originally speculated that attempts to suppress stereotype-related thoughts lead to anxiety and the narrowing of attention. This could contribute to the observed deficits in performance. In 2008, Toni Schmader, Michael Johns, and Chad Forbes published an integrated model of stereotype threat that focused on three interrelated factors:\nSchmader et al. suggest that these three factors summarize the pattern of evidence that has been accumulated by past experiments on stereotype threat. For example, stereotype threat has been shown to disrupt working memory and executive function, increase arousal, increase self-consciousness about one's performance, and cause individuals to try to suppress negative thoughts as well as negative emotions such as anxiety. People have a limited amount of cognitive resources available. When a large portion of these resources are spent focusing on anxiety and performance pressure, the individual is likely to perform worse on the task at hand.\n\nA number of studies looking at physiological and neurological responses support Schmader and colleagues' integrated model of the processes that produce stereotype threat. Supporting an explanation in terms of stress arousal, one study found that African Americans under stereotype threat exhibit larger increases in arterial blood pressure. One study found increased cardiovascular activation amongst women who watched a video in which men outnumbered women at a math and science conference. Other studies have similarly found that individuals under stereotype threat display increased heart rates. Stereotype threat may also activate a neuroendocrine stress response, as measured by increased levels of cortisol while under threat. The physiological reactions that are induced by stereotype threat can often be subconscious, and can distract and interrupt cognitive focus from the task.\n\nWith regard to performance monitoring and vigilance, studies of brain activity have supported the idea that stereotype threat increases both of these processes. Forbes and colleagues recorded electroencephalogram (EEG) signals that measure electrical activity along the scalp, and found that individuals experiencing stereotype threat were more vigilant for performance-related stimuli.\n\nAnother study used functional magnetic resonance imaging (fMRI) to investigate brain activity associated with stereotype threat. The researchers found that women experiencing stereotype threat while taking a math test showed heightened activation in the ventral stream of the anterior cingulate cortex (ACC), a neural region thought to be associated with social and emotional processing. Wraga and colleagues found that women under stereotype threat showed increased activation in the ventral ACC and that the amount of this activation predicted performance decrements on the task. When individuals were made aware of performance-related stimuli, they were more likely to experience stereotype threat.\n\nA study conducted by Boucher, Rydell, Loo, and Rydell has shown that stereotype threat not only affects performance, but can also affect the ability to learn new information. In the study, undergraduate men and women had a session of learning followed by an assessment of what they learned. Some participants were given information intended to induce stereotype threat, and some of these participants were later given \"gender fair\" information, which it was predicted would reduce or remove stereotype threat. As a result, participants were split into four separate conditions: control group, stereotype threat only, stereotype threat removed before learning, and stereotype threat removed after learning. The results of the study showed that the women who were presented with the \"gender fair\" information performed better on the math related test than the women who were not presented with this information. This study also showed that it was more beneficial to women for the \"gender fair\" information to be presented prior to learning rather than after learning. These results suggest that eliminating stereotype threat prior to taking mathematical tests can help women perform better, and that eliminating stereotype threat prior to mathematical learning can help women learn better.\n\nDecreased performance is the most recognized consequence of stereotype threat. However, research has also shown that stereotype threat can cause individuals to blame themselves for perceived failures, self-handicap, discount the value and validity of performance tasks, distance themselves from negatively stereotyped groups, and disengage from situations that are perceived as threatening.\n\nIn the long run, the chronic experience of stereotype threat may lead individuals to disidentify with the stereotyped group. For example, a woman may stop seeing herself as \"a math person\" after experiencing a series of situations in which she experienced stereotype threat. This disidentification is thought to be a psychological coping strategy to maintain self-esteem in the face of failure. Repeated exposure to anxiety and nervousness can lead individuals to choose to distance themselves from the stereotyped group.\n\nAlthough much of the research on stereotype threat has examined the effects of coping with negative stereotype on academic performance, recently there has been an emphasis on how coping with stereotype threat could \"spillover\" to dampen self-control and thereby affect a much broader category of behaviors, even in non-stereotyped domains. Research by Michael Inzlicht and colleagues suggest that, when women cope with negative stereotypes about their math ability, they perform worse on math tests, and that, after completing the math test, women may continue to show deficits even in unrelated domains. For example, women might overeat, be more aggressive, make more risky decisions, and show less endurance during physical exercise.\n\nThe perceived discrimination associated with stereotype threat can also have negative long-term consequences on individuals' mental health. Perceived discrimination has been extensively investigated in terms of its effects on mental health, with a particular emphasis on depression. Cross-sectional studies involving diverse minority groups, including those relating to internalized racism, have found that individuals who experience more perceived discrimination are more likely to exhibit depressive symptoms. Additionally, perceived discrimination has also been found to predict depressive symptoms in children and adolescents. Other negative mental health outcomes associated with perceived discrimination include a reduced general well-being, post-traumatic stress disorder, anxiety, and rebellious behavior. A meta-analysis conducted by Pascoe and Smart Richman has shown that the strong link between perceived discrimination and negative mental health persists even after controlling for factors such as education, socioeconomic status, and employment.\n\nAdditional research seeks ways to boost the test scores and academic achievement of students in negatively stereotyped groups. There are many ways to combat the effects of stereotype threat.\n\n\nThe stereotype threat explanation of achievement gaps has attracted criticism. According to Paul R. Sackett, Chaitra M. Hardison, and Michael J. Cullen, both the media and scholarly literature have wrongly concluded that eliminating stereotype threat could completely eliminate differences in test performance between European Americans and African Americans. Sackett et al. have pointed out that, in Steele and Aronson's (1995) experiments where stereotype threat was removed, an achievement gap of approximately one standard deviation remained between the groups, which is very close in size to that routinely reported between African American and European Americans' average scores on large-scale standardized tests such as the SAT. In subsequent correspondence between Sackett et al. and Steele and Aronson, Sackett et al. wrote that \"They [Steele and Aronson] agree that it is a misinterpretation of the Steele and Aronson (1995) results to conclude that eliminating stereotype threat eliminates the African American-White test-score gap.\"\n\nArthur R. Jensen criticised stereotype threat theory on the basis that it invokes an additional mechanism to explain effects which could be, according to him, explained by other, well-known, and well-established theories, such as test anxiety and especially the Yerkes–Dodson law. In Jensen's view, the effects which are attributed to stereotype threat may simply reflect \"the interaction of ability level with test anxiety as a function of test complexity\". However Diamond \"et al\" state \"that one problem with the Yerkes-Dodson law is that it invokes an ill-defined distinction between 'simple' versus 'complex' tasks.\" They further articulate that, \"Yerkes and Dodson may have the dubious distinction to be the most highly cited, but largely unread, paper in the history of science.\"\n\nIn 2009, Wei examined real-world testing over a broad population (rather than lab assessments with questionable external validity), and found a reverse stereotype threat: a randomly assigned question actually raised female students' scores by 0.05 standard deviations. An earlier experiment with Advanced Placement exams found no effects that were 'practically significant,' but does show 'statistically significant' effect.\n\nGijsbert Stoet and David C. Geary reviewed the evidence for the stereotype threat explanation of the achievement gap in mathematics between men and women. They concluded that the relevant stereotype threat research has many methodological problems, such as not having a control group, and that the stereotype threat literature on this topic misrepresents itself as \"well established\". They concluded that the evidence is in fact very weak.\n\nThe strength and type of the effect has also been questioned. Flore and Wicherts concluded the reported effect is small, but also that the field is inflated by publication bias. They argue that, correcting for this, the most likely true effect size is near zero (see meta-analytic plot, highlighting both the restriction of large effect to low-powered studies, and the plot asymmetry which occurs when publication bias is active).\n\nEarlier meta-analyses reached similar conclusions. For instance, Ganley et al. (2013) examined math stereotype threat in a well-powered (total N ~ 1000) multi-experiment study. This allowed examination of potential moderators such as age and implicit vs explicit methods. Significant gender differences in math were found, but \"no evidence that the mathematics performance of school-age girls was impacted by stereotype threat\" was found. Further, they found that evidence for stereotype threat in children reflects publication bias: large, well-controlled studies find smaller or non-significant effects, while among the many underpowered studies run, researchers selectively published those in which false-positive effects reached significance:\n\nnonsignificant findings were almost always reported in an article along with some significant stereotype threat effects found either at another age (Ambady et al., 2001; Muzzatti & Agnoli, 2007), only with certain students (Keller, 2007), on certain items (Keller, 2007; Neuville & Croizet, 2007), or in certain contexts (Huguet & Regner, 2007, Study 2; Picho & Stephens, 2012; Tomasetto et al., 2011). Importantly, none of the three unpublished dissertations showed a stereotype threat effect. This observation suggests the possibility that publication bias is occurring. Publication bias refers to the fact that studies with null results are often not written up for publication or accepted for publication (Begg, 1994). This bias is a serious concern, especially if these results are being used to make recommendations for interventions.\n\n\n\n"}
{"id": "18113340", "url": "https://en.wikipedia.org/wiki?curid=18113340", "title": "Student Peace Alliance", "text": "Student Peace Alliance\n\nThe Student Peace Alliance is a nonpartisan student action organization advocating peace across the United States of America. Founded at Brandeis University in the Spring of 2006 by Aaron Voldman and Julia Simon-Mishel, Student Peace Alliance is a 501(c)4 organization, and has 100 chapters in the United States at high schools, colleges, and community youth organizations.\n\nThe Student Peace Alliance calls for a U.S. Department of Peace to effectively and efficiently implement policies that would centralize and unify peace building efforts domestically and abroad. With a legislative bill in the U.S. House of representatives (H.R. 808) that currently has 70 co-sponsors, Student Peace Alliance coordinates supportive efforts on a local and national level through various actions including, policy suggestions, research, citizen lobbying efforts, and peaceful non-violent demonstrations.\n\nValentine's Day Action : On Valentine's Day congressional offices all over the nation are visited by Student Peace Alliance members with Valentine's letters from constituents of their congressional district.\n\nPeace of the Pie Action : The annual Mother's Day \"Peace of the Pie\" National Action Day! On May 11 (the Friday before Mother's Day), members of SPA and Peace Alliance take pies to their local Congressional offices, letting members of Congress know that \"Peace wants a Piece of the Pie, which is 2% of the Federal Defence Budget.\n\nThe Greater Alliance, all of which have the goal of promoting peace and policies for peace, consists of the Peace Alliance, Student Peace Alliance, and Global Alliance for Ministries and Departments of Peace. Founded in 2004 by Maryanne Williamson, the Peace Alliance is an organization dedicated to, in coordination with national efforts, raising grassroots awareness in local communities about violence prevention and peace programs in order to influence congressional decisions. Established in 2005, and operating in over 30 countries around the world, The Global Alliance for Ministries and Departments of Peace works on an international level promoting peaceful dialogue between nations through peace summits and global initiatives to establish Departments and Ministries of Peace.\n\n"}
{"id": "2768085", "url": "https://en.wikipedia.org/wiki?curid=2768085", "title": "Sudarsky's gas giant classification", "text": "Sudarsky's gas giant classification\n\nSudarsky's classification of gas giants for the purpose of predicting their appearance based on their temperature was outlined by David Sudarsky and colleagues in the paper \"Albedo and Reflection Spectra of Extrasolar Giant Planets\" and expanded on in \"Theoretical Spectra and Atmospheres of Extrasolar Giant Planets\", published before any successful direct or indirect observation of an extrasolar planet atmosphere was made. It is a broad classification system with the goal of bringing some order to the likely rich variety of extrasolar gas-giant atmospheres.\n\nGas giants are split into five classes (numbered using Roman numerals) according to their modeled physical atmospheric properties. In the Solar System, only Jupiter and Saturn are within the Sudarsky classification, and both are Class I.\nThe appearance of planets that are not gas giants cannot be predicted by the Sudarsky system, for example terrestrial planets such as Earth and Venus, HD 85512 b (3.6 Earth masses) and OGLE-2005-BLG-390Lb (5.5 Earth masses), or ice giants such as Uranus (14 Earth masses) and Neptune (17 Earth masses).\n\nThe appearance of extrasolar planets is largely unknown because of the difficulty in making direct observations of extrasolar planets. In addition, analogies with planets in the Solar System can apply for few of the extrasolar planets known; because most are wholly unlike any of our planets, for example the hot Jupiters.\n\nBodies that transit their star can be spectrographically mapped, for instance HD 189733 b. \nThat planet has further been shown to be blue with an albedo greater (brighter) than 0.14. Most transiting planets are hot Jupiters.\n\nSpeculation on the appearances of \"unseen\" extrasolar planets currently relies upon computational models of the likely atmosphere of such a planet, for instance how the atmospheric temperature–pressure profile and composition would respond to varying degrees of insolation.\n\nPlanets in this class have appearances dominated by ammonia clouds. These planets are found in the outer regions of a planetary system. They exist at temperatures less than about . The predicted Bond albedo of a class I planet around a star like the Sun is 0.57, compared with a value of 0.343 for Jupiter and 0.342 for Saturn. The discrepancy can be partially accounted for by taking into account non-equilibrium condensates such as tholins or phosphorus, which are responsible for the coloured clouds in the Jovian atmosphere, and are not modelled in the calculations.\n\nThe temperatures for a class I planet require a cool star or a distant orbit. The former stars might be too dim for us even to know about them, and the latter orbits might be too unpronounced for notice until several observations of those \"orbits\"' \"years\" (cf. Kepler's third law). Superjovians would have enough mass to improve these observations; but a superjovian of comparable age to Jupiter will have more internal heating than said planet, which could push it to a higher class.\n\nAs of 2015, 47 Ursae Majoris c and d could be Class I planets. Upsilon Andromedae e and 55 Cancri d may also be Class I planets.\n\nPlanets in class II are too warm to form ammonia clouds; instead their clouds are made up of water vapor. These characteristics are expected for planets with temperatures below around 250 K (-23°C; -10°F). Water clouds are more reflective than ammonia clouds, and the predicted Bond albedo of a class II planet around a Sun-like star is 0.81. Even though the clouds on such a planet would be similar to those of Earth, the atmosphere would still consist mainly of hydrogen and hydrogen-rich molecules such as methane.\n\nExamples of possible class II planets: HD 45364 b and HD 45364 c, HD 28185 b, Gliese 876 b and c, Upsilon Andromedae d, 55 Cancri f, 47 Ursae Majoris b, PH2b, Kepler-90 h, HD 10180 g.\n\nPlanets with equilibrium temperatures between about 350 K (170 °F, 80 °C) and 800 K (980 °F, 530 °C) do not form global cloud cover, because they lack suitable chemicals in the atmosphere to form clouds. These planets would appear as featureless azure-blue globes because of Rayleigh scattering and absorption by methane in their atmospheres, appearing like Jovian-mass versions of Uranus and Neptune. Because of the lack of a reflective cloud layer, the Bond albedo is low, around 0.12 for a class-III planet around a Sun-like star. They exist in the inner regions of a planetary system, roughly corresponding to the location of Mercury.\n\nPossible class-III planets are HD 37124 b, HD 18742 b, HD 178911 Bb, 55 Cancri c, Upsilon Andromedae c, Kepler-89e, COROT-9b and HD 205739 b. Above 700 K (800 °F, 430 °C), sulfides and chlorides might provide cirrus-like clouds.\n\nAbove 900 K (630 °C/1160 °F), carbon monoxide becomes the dominant carbon-carrying molecule in a gas giant's atmosphere (rather than methane). Furthermore, the abundance of alkali metals, such as sodium substantially increase, and spectral lines of sodium and potassium are predicted to be prominent in a gas giant's spectrum. These planets form cloud decks of silicates and iron deep in their atmospheres, but this is not predicted to affect their spectrum. The Bond albedo of a class IV planet around a Sun-like star is predicted to be very low, at 0.03 because of the strong absorption by alkali metals. Gas giants of classes IV and V are referred to as hot Jupiters.\n\n55 Cancri b was listed as a class IV planet.\n\nHD 209458 b at 1300 K (1000 °C) would be another such planet, with a geometric albedo of, within error limits, zero; and in 2001, NASA witnessed atmospheric sodium in its transit, though less than predicted. This planet hosts an upper cloud deck absorbing so much heat that below it is a relatively cool stratosphere. The composition of this dark cloud, in the models, is assumed to be titanium/vanadium oxide (sometimes abbreviated \"TiVO\"), by analogy with red dwarfs, but its true composition is yet unknown; it could well be as per Sudarsky.\n\nHD 189733 b, with measured temperatures 920–1200 K (650–930 °C), also qualifies as class IV. However, in late 2007 it was measured as deep blue, with an albedo over 0.14 (possibly due to the brighter glow of its \"hot spot\"). No stratosphere has been conclusively proven for it as yet.\n\nTrES-2b was measured with the lowest albedo, and therefore listed as class IV.\n\nFor the very hottest gas giants, with temperatures above 1400 K (2100 °F, 1100 °C) or cooler planets with lower gravity than Jupiter, the silicate and iron cloud decks are predicted to lie high up in the atmosphere. The predicted Bond albedo of a class V planet around a Sun-like star is 0.55, due to reflection by the cloud decks. At such temperatures, a gas giant may glow red from thermal radiation but the reflected light generally overwhelms thermal radiation. For stars of visual apparent magnitude under 4.50, such planets are theoretically visible to our instruments. Examples of such planets might include 51 Pegasi b and Upsilon Andromedae b. HAT-P-11b and those other extrasolar gas giants found by the Kepler telescope might be possible class V planets, such as Kepler-7b, HAT-P-7b, or Kepler-13 b.\n\n\n"}
{"id": "34983797", "url": "https://en.wikipedia.org/wiki?curid=34983797", "title": "Taylor's law", "text": "Taylor's law\n\nTaylor's law (also known as Taylor's power law) is an empirical law in ecology that relates the variance of the number of individuals of a species per unit area of habitat to the corresponding mean by a power law relationship. It is named after the ecologist who first proposed it in 1961, Lionel Roy Taylor (1924–2007). Taylor's original name for this relationship was the law of the mean.\n\nThis law was originally defined for ecological systems, specifically to assess the spatial clustering of organisms. For a population count \"Y\" with mean \"µ\" and variance var(\"Y\"), Taylor's law is written,\n\nwhere \"a\" and \"b\" are both positive constants. Taylor proposed this relationship in 1961, suggesting that the exponent \"b\" be considered a species specific index of aggregation. This power law has subsequently been confirmed for many hundreds of species.\n\nTaylor's law has also been applied to assess the time dependent changes of population distributions. Related variance to mean power laws have also been demonstrated in several non-ecological systems: \n\nThe first use of a double log-log plot was by Reynolds in 1879 on thermal aerodynamics. Pareto used a similar plot to study the proportion of a population and their income.\nThe term \"variance\" was coined by Fisher in 1918.\n\nFisher in 1921 proposed the equation\n\nNeyman studied the relationship between the sample mean and variance in 1926. Barlett proposed a relationship between the sample mean and variance in 1936\n\nSmith in 1938 while studying crop yields proposed a relationship similar to Taylor's. This relationship was\n\nwhere \"V\" is the variance of yield for plots of \"x\" units, \"V\" is the variance of yield per unit area and \"x\" is the size of plots. The slope (\"b\") is the index of heterogeneity. The value of \"b\" in this relationship lies between 0 and 1. Where the yield are highly correlated \"b\" tends to 0; when they are uncorrelated \"b\" tends to 1.\n\nBliss in 1941, Fracker and Brischle in 1941 and Hayman & Lowe in 1961 also described what is now known as Taylor's law, but in the context of data from single species.\n\nL. R. Taylor (1924–2007) was an English entomologist who worked on the Rothamsted Insect Survey for pest control. His 1961 paper used data from 24 papers published between 1936 and 1960. These papers considered a variety of biological settings: virus lesions, macro-zooplankton, worms and symphylids in soil, insects in soil, on plants and in the air, mites on leaves, ticks on sheep and fish in the sea. In these papers the \"b\" value lay between 1 and 3. Taylor proposed the power law as a general feature of the spatial distribution of these species. He also proposed a mechanistic hypothesis to explain this law. Among the papers cited were those of Bliss and Yates and Finney.\n\nInitial attempts to explain the spatial distribution of animals had been based on approaches like Bartlett's stochastic population models and the negative binomial distribution that could result from birth-death processes. Taylor's novel explanation was based the assumption of a balanced migratory and congregatory behavior of animals. His hypothesis was initially qualitative, but as it evolved it became semi-quantitative and was supported by simulations. In proposing that animal behavior was the principal mechanism behind the clustering of organisms, Taylor though appeared to have ignored his own report of clustering seen with tobacco necrosis virus plaques.\n\nFollowing Taylor's initial publications several alternative hypotheses for the power law were advanced. Hanski proposed a random walk model, modulated by the presumed multiplicative effect of reproduction. Hanski's model predicted that the power law exponent would be constrained to range closely about the value of 2, which seemed inconsistent with many reported values.\n\nAnderson \"et al\" formulated a simple stochastic birth, death, immigration and emigration model that yielded a quadratic variance function. As a response to this model Taylor argued that such a Markov process would predict that the power law exponent would vary considerably between replicate observations, and that such variability had not been observed.\n\nAbout this time concerns were, however, raised regarding the statistical variability with measurements of the power law exponent, and the possibility that observations of a power law might reflect more mathematical artifact than a mechanistic process. Taylor \"et al\" responded with an additional publication of extensive observations which he claimed refuted Downing's concerns.\n\nIn addition, Thórarinsson published a detailed critique of the animal behavioral model, noting that Taylor had modified his model several times in response to concerns raised, and that some of these modifications were inconsistent with earlier versions. Thórarinsson also claimed that Taylor confounded animal numbers with density and that Taylor had incorrectly interpreted simulations that had been constructed to demonstrate his models as validation.\n\nKemp reviewed a number of discrete stochastic models based on the negative binomial, Neyman type A, and Polya–Aeppli distributions that with suitable adjustment of parameters could produce a variance to mean power law. Kemp, however, did not explain the parameterizations of his models in mechanistic terms. Other relatively abstract models for Taylor's law followed.\n\nA number of additional statistical concerns were raised regarding Taylor's law, based on the difficulty with real data in distinguishing between Taylor's law and other variance to mean functions, as well the inaccuracy of standard regression methods.\n\nReports also began to accumulate where Taylor's law had been applied to time series data. Perry showed how simulations based on chaos theory could yield Taylor's law, and Kilpatrick & Ives provided simulations which showed how interactions between different species might lead to Taylor's law.\n\nOther reports appeared where Taylor's law had been applied to the spatial distribution of plants and bacterial populations As with the observations of Tobacco necrosis virus mentioned earlier, these observations were not consistent with Taylor's animal behavioral model.\n\nEarlier it was mentioned that variance to mean power function had been applied to non-ecological systems, under the rubric of Taylor's law. To provide a more general explanation for the range of manifestations of the power law a hypothesis was proposed based on the Tweedie distributions, a family of probabilistic models that express an inherent power function relationship between the variance and the mean. Details regarding this hypothesis will be provided in the next section.\n\nA further alternative explanation for Taylor's law was proposed by Cohen \"et al\", derived from the Lewontin Cohen growth model. This model was successfully used to describe the spatial and temporal variability of forest populations.\n\nAnother paper by Cohen and Xu that random sampling in blocks where the underling distribution is skewed with the first four moments finite gives rise to Taylor's law. Approximate formulae for the parameters and their variances were also derived. These estimates were tested again data from the Black Rock Forest and found to be in reasonable agreement.\n\nFollowing Taylor's initial publications several alternative hypotheses for the power law were advanced. Hanski proposed a random walk model, modulated by the presumed multiplicative effect of reproduction. Hanski's model predicted that the power law exponent would be constrained to range closely about the value of 2, which seemed inconsistent with many reported values. Anderson \"et al\" formulated a simple stochastic birth, death, immigration and emigration model that yielded a quadratic variance function. The Lewontin Cohen growth model. is another proposed explanation. The possibility that observations of a power law might reflect more mathematical artifact than a mechanistic process was raised.\n\nIn the physics literature Taylor's law has been referred to as \"fluctuation scaling\". Eisler \"et al\", in a further attempt to find a general explanation for fluctuation scaling, proposed a process they called \"impact inhomogeneity\" in which frequent events are associated with larger impacts. In appendix B of the Eisler article, however, the authors noted that the equations for impact inhomogeneity yielded the same mathematical relationships as found with the Tweedie distributions.\n\nAnother group of physicists, Fronczak and Fronczak, derived Taylor's power law for fluctuation scaling from principles of equilibrium and non-equilibrium statistical physics. Their derivation was based on assumptions of physical quantities like free energy and an external field that caused the clustering of biological organisms. Direct experimental demonstration of these postulated physical quantities in relationship to animal or plant aggregation has yet to be achieved, though. Shortly thereafter, an analysis of Fronczak and Fronczak's model was presented that showed their equations directly lead to the Tweedie distributions, a finding that suggested that Fronczak and Fronczak had possibly provided a maximum entropy derivation of these distributions.\n\nTaylor's law has been shown to hold for prime numbers not exceeding a given real number. This result has been shown to hold for the first 11 million primes. If the Hardy–Littlewood twin primes conjecture is true then this law also holds for twin primes.\n\nThe law itself is named after the ecologist Lionel Roy Taylor (1924–2007). The name 'Taylor's law' was coined by Southwood in 1966. Taylor's original name for this relationship was the law of the mean\n\nAbout the time that Taylor was substantiating his ecological observations, MCK Tweedie, a British statistician and medical physicist, was investigating a family of probabilistic models that are now known as the Tweedie distributions. As mentioned above, these distributions are all characterized by a variance to mean power law mathematically identical to Taylor's law.\n\nThe Tweedie distribution most applicable to ecological observations is the compound Poisson-gamma distribution, which represents the sum of \"N\" independent and identically distributed random variables with a gamma distribution where \"N\" is a random variable distributed in accordance with a Poisson distribution. In the additive form its cumulant generating function (CGF) is:\n\nwhere \"κ\"(\"θ\") is the cumulant function,\n\nthe Tweedie exponent\n\n\"s\" is the generating function variable, and \"θ\" and \"λ\" are the canonical and index parameters, respectively.\n\nThese last two parameters are analogous to the scale and shape parameters used in probability theory. The cumulants of this distribution can be determined by successive differentiations of the CGF and then substituting \"s=0\" into the resultant equations. The first and second cumulants are the mean and variance, respectively, and thus the compound Poisson-gamma CGF yields Taylor's law with the proportionality constant\n\nThe compound Poisson-gamma cumulative distribution function has been verified for limited ecological data through the comparison of the theoretical distribution function with the empirical distribution function. A number of other systems, demonstrating variance to mean power laws related to Taylor's law, have been similarly tested for the compound Poisson-gamma distribution.\n\nThe main justification for the Tweedie hypothesis rests with the mathematical convergence properties of the Tweedie distributions. The Tweedie convergence theorem requires the Tweedie distributions to act as foci of convergence for a wide range of statistical processes. As a consequence of this convergence theorem, processes based on the sum of multiple independent small jumps will tend to express Taylor's law and obey a Tweedie distribution. A limit theorem for independent and identically distributed variables, as with the Tweedie convergence theorem, might then be considered as being fundamental relative to the \"ad hoc\" population models, or models proposed on the basis of simulation or approximation.\n\nThis hypothesis remains controversial; more conventional population dynamic approaches seem preferred amongst ecologists, despite the fact that the Tweedie compound Poisson distribution can be directly applied to population dynamic mechanisms.\n\nOne difficulty with the Tweedie hypothesis is that the value of \"b\" does not range between 0 and 1. Values of \"b\" < 1 are rare but have been reported.\n\nIn symbols\n\nwhere \"s\" is the variance of the density of the \"i\"th sample, \"m\" is the mean density of the \"i\"th sample and \"a\" and \"b\" are constants.\n\nIn logarithmic form\n\nTaylor's law is scale invariant. If the unit of measurement is changed by a constant factor \"c\", the exponent (\"b\") remains unchanged.\n\nTo see this let \"y\" = \"cx\". Then\n\nformula_11\n\nformula_12\n\nformula_13\n\nformula_14\n\nTaylor's law expressed in the original variable (\"x\") is\n\nformula_15\n\nand in the rescaled variable (\"y\") it is\n\nformula_16\n\nIt has been shown that Taylor's law is the only relationship between the mean and variance that is scale invariant.\n\nA refinement in the estimation of the slope \"b\" has been proposed by Rayner.\n\nwhere \"r\" is the Pearson moment correlation coefficient between log(\"s\") and log \"m\", \"f\" is the ratio of sample variances in log(\"s\") and log \"m\" and \"φ\" is the ratio of the errors in log(\"s\") and log \"m\".\n\nOrdinary least squares regression assumes that \"φ\" = ∞. This tends to underestimate the value of \"b\" because the estimates of both log(\"s\") and log \"m\" are subject to error.\n\nAn extension of Taylor's law has been proposed by Ferris \"et al\" when multiple samples are taken\n\nwhere \"s\" and \"m\" are the variance and mean respectively, \"b\", \"c\" and \"d\" are constants and \"n\" is the number of samples taken. To date, this proposed extension has not been verified to be as applicable as the original version of Taylor's law.\n\nAn extension to this law for small samples has been proposed by Hanski. For small samples the Poisson variation (\"P\") - the variation that can be ascribed to sampling variation - may be significant. Let \"S\" be the total variance and let \"V\" be the biological (real) variance. Then\n\nAssuming the validity of Taylor's law, we have\n\nBecause in the Poisson distribution the mean equals the variance, we have\n\nThis gives us\n\nThis closely resembles Barlett's original suggestion.\n\nSlope values (\"b\") significantly > 1 indicate clumping of the organisms.\n\nIn Poisson-distributed data, \"b\" = 1. If the population follows a lognormal or gamma distribution, then \"b\" = 2.\n\nFor populations that are experiencing constant per capita environmental variability, the regression of log( variance ) versus log( mean abundance ) should have a line with \"b\" = 2.\n\nMost populations that have been studied have \"b\" < 2 (usually 1.5–1.6) but values of 2 have been reported. Occasionally cases with \"b\" > 2 have been reported. \"b\" values below 1 are uncommon but have also been reported ( \"b\" = 0.93 ).\n\nIt has been suggested that the exponent of the law (\"b\") is proportional to the skewness of the underlying distribution. This proposal has criticised: additional work seems to be indicated.\n\nThe origin of the slope (\"b\") in this regression remains unclear. Two hypotheses have been proposed to explain it. One suggests that \"b\" arises from the species behavior and is a constant for that species. The alternative suggests that it is dependent on the sampled population. Despite the considerable number of studies carried out on this law (over 1000), this question remains open.\n\nIt is known that both \"a\" and \"b\" are subject to change due to age-specific dispersal, mortality and sample unit size.\n\nThis law may be a poor fit if the values are small. For this reason an extension to Taylor's law has been proposed by Hanski which improves the fit of Taylor's law at low densities.\n\nA form of Taylor's law applicable to binary data in clusters (e.q., quadrats) has been proposed. In a binomial distribution, the theoretical variance is\n\nwhere \"(var)\" is the binomial variance, \"n\" is the sample size per cluster, and \"p\" is the proportion of individuals with a trait (such as disease), an estimate of the probability of an individual having that trait.\n\nOne difficulty with binary data is that the mean and variance, in general, have a particular relationship: as the mean proportion of individuals infected increases above 0.5, the variance deceases.\n\nIt is now known that the observed variance \"(var)\" changes as a power function of \"(var)\".\n\nHughes and Madden noted that if the distribution is Poisson, the mean and variance are equal. As this is clearly not the case in many observed proportion samples, they instead assumed a binomial distribution. They replaced the mean in Taylor's law with the binomial variance and then compared this theoretical variance with the observed variance. For binomial data, they showed that \"var = var\" with overdispersion, \"var\" > \"var\".\n\nIn symbols, Hughes and Madden's modification to Tyalor's law was\n\nIn logarithmic form this relationship is\n\nThis latter version is known as the binary power law.\n\nA key step in the derivation of the binary power law by Hughes and Madden was the observation made by Patil and Stiteler that the variance-to-mean ratio used for assessing over-dispersion of unbounded counts in a single sample is actually the ratio of two variances: the observed variance and the theoretical variance for a random distribution. For unbounded counts, the random distribution is the Poisson. Thus, the Taylor power law for a collection of samples can be considered as a relationship between the observed variance and the Poisson variance.\n\nMore broadly, Madden and Hughes considered the power law as the relationship between two variances, the observed variance and the theoretical variance for a random distribution. With binary data, the random distribution is the binomial (not the Poisson). Thus the Taylor power law and the binary power law are two special cases of a general power-law relationships for heterogeneity.\n\nWhen both \"a\" and \"b\" are equal to 1, then a small-scale random spatial pattern is suggested and is best described by the binomial distribution. When \"b\" = 1 and \"a\" > 1, there is over-dispersion (small scale aggregation). When \"b\" is > 1, the degree of aggregation varies with \"p\". Turechek \"et al\" have showed that the binary power law describes numerous data sets in plant pathology. In general, \"b\" is greater than 1 and less than 2.\n\nThe fit of this law has been tested by simulations. These results suggest that rather than a single regression line for the data set, a segmental regression may be a better model for genuinely random distributions. However, this segmentation only occurs for very short-range dispersal distances and large quadrat sizes. The break in the line occurs only at \"p\" very close to 0.\n\nAn extension to this law has been proposed. The original form of this law is symmetrical but it can be extended to an asymmetrical form. Using simulations the symmetrical form fits the data when there is positive correlation of disease status of neighbors. Where there is a negative correlation between the likelihood of neighbours being infected, the asymmetrical version is a better fit to the data.\n\nBecause of the ubiquitous occurrence of Taylor's law in biology it has found a variety of uses some of which are listed here.\n\nIt has been recommended based on simulation studies in applications testing the validity of Taylor's law to a data sample that:\n\n(1) the total number of organisms studied be > 15\n(2) the minimum number of groups of organisms studied be > 5 \n(3) the density of the organisms should vary by at least 2 orders of magnitude within the sample\n\nIt is common assumed (at least initially) that a population is randomly distributed in the environment. If a population is randomly distributed then the mean ( \"m\" ) and variance ( \"s\" ) of the population are equal and the proportion of samples that contain at least one individual ( \"p\" ) is\n\nWhen a species with a clumped pattern is compared with one that is randomly distributed with equal overall densities, p will be less for the species having the clumped distribution pattern. Conversely when comparing a uniformly and a randomly distributed species but at equal overall densities, \"p\" will be greater for the randomly distributed population. This can be graphically tested by plotting \"p\" against \"m\".\n\nWilson and Room developed a binomial model that incorporates Taylor's law. The basic relationship is\n\nwhere the log is taken to the base \"e\".\n\nIncorporating Taylor's law this relationship becomes\n\nThe common dispersion parameter (\"k\") of the negative binomial distribution is\n\nwhere \"m\" is the sample mean and \"s\" is the variance. If 1 / \"k\" is > 0 the population is considered to be aggregated; 1 / \"k\" = 0 ( \"s\" = \"m\" ) the population is considered to be randomly (Poisson) distributed and if 1 / \"k\" is < 0 the population is considered to be uniformly distributed. No comment on the distribution can be made if \"k\" = 0.\n\nWilson and Room assuming that Taylor's law applied to the population gave an alternative estimator for \"k\":\n\nwhere \"a\" and \"b\" are the constants from Taylor's law.\n\nJones using the estimate for \"k\" above along with the relationship Wilson and Room developed for the probability of finding a sample having at least one individual\n\nderived an estimator for the probability of a sample containing \"x\" individuals per sampling unit. Jones's formula is\n\nwhere \"P\"( \"x\" ) is the probability of finding \"x\" individuals per sampling unit, \"k\" is estimated from the Wilon and Room equation and \"m\" is the sample mean. The probability of finding zero individuals \"P\"( 0 ) is estimated with the negative binomial distribution\n\nJones also gives confidence intervals for these probabilities.\n\nwhere \"CI\" is the confidence interval, \"t\" is the critical value taken from the t distribution and \"N\" is the total sample size.\n\nKatz proposed a family of distributions (the Katz family) with 2 parameters ( \"w\", \"w\" ). This family of distributions includes the Bernoulli, Geometric, Pascal and Poisson distributions as special cases. The mean and variance of a Katz distribution are\n\nwhere \"m\" is the mean and \"s\" is the variance of the sample. The parameters can be estimated by the method of moments from which we have\n\nFor a Poisson distribution \"w\" = 0 and \"w\" = \"λ\" the parameter of the Possion distribution. This family of distributions is also sometimes known as the Panjer family of distributions.\n\nThe Katz family is related to the Sundt-Jewel family of distributions:\n\nformula_39\n\nThe only members of the Sundt-Jewel family are the Poisson, binomial, negative binomial (Pascal), extended truncated negative binomial and logarithmic series distributions.\n\nIf the population obeys a Katz distribution then the coefficients of Taylor's law are\n\nKatz also introduced a statistical test\n\nwhere \"J\" is the test statistic, \"s\" is the variance of the sample, \"m\" is the mean of the sample and \"n\" is the sample size. \"J\" is asymptotically normally distributed with a zero mean and unit variance. If the sample is Poisson distributed \"J\" = 0; values of \"J\" < 0 and > 0 indicate under and over dispersion respectively. Overdispersion is often caused by latent heterogeneity - the presence of multiple sub populations within the population the sample is drawn from.\n\nThis statistic is related to the Neyman-Scott statistic\n\nwhich is known to be asymptotically normal and the conditional chi-squared statistic (Poisson dispersion test)\n\nwhich is known to have an asymptotic chi squared distribution with \"n\" − 1 degrees of freedom when the population is Poisson distributed.\n\nIf the population obeys Taylor's law then\n\nIf Taylor's law is assumed to apply it is possible to determine the mean time to local extinction. This model assumes a simple random walk in time and the absence of density dependent population regulation.\n\nLet formula_46 where \"N\" and \"N\" are the population sizes at time \"t\" + 1 and \"t\" respectively and \"r\" is parameter equal to the annual increase (decrease in population). Then\n\nwhere \"var\"( \"r\" ) is the variance of \"r\".\n\nLet \"K\" be a measure of the species abundance (organisms per unit area). Then\n\nwhere T is the mean time to local extinction.\n\nThe probability of extinction by time \"t\" is\n\nIf a population is lognormally distributed then the harmonic mean of the population size (\"H\") is related to the arithmetic mean (\"m\")\n\nGiven that \"H\" must be > 0 for the population to persist then rearranging we have\n\nis the minimum size of population for the species to persist.\n\nThe assumption of a lognormal distribution appears to apply to about half of a sample of 544 species. suggesting that it is at least a plausible assumption.\n\nThe degree of precision (\"D\") is defined to be \"s\" / \"m\" where \"s\" is the standard deviation and \"m\" is the mean. The degree of precision is known as the coefficient of variation in other contexts. In ecology research it is recommended that \"D\" be in the range 10-25%. The desired degree of precision is important in estimating the required sample size where an investigator wishes to test if Taylor's law applies to the data. The required sample size has been estimated for a number of simple distributions but where the population distribution is not known or cannot be assumed more complex formulae may needed to determine the required sample size.\n\nWhere the population is Poisson distributed the sample size (\"n\") needed is\n\nwhere \"t\" is critical level of the t distribution for the type 1 error with the degrees of freedom that the mean (\"m\") was calculated with.\n\nIf the population is distributed as a negative binomial distribution then the required sample size is\n\nwhere \"k\" is the parameter of the negative binomial distribution.\n\nA more general sample size estimator has also been proposed\n\nwhere a and b are derived from Taylor's law.\n\nAn alternative has been proposed by Southwood\n\nwhere \"n\" is the required sample size, \"a\" and \"b\" are the Taylor's law coefficients and \"D\" is the desired degree of precision.\n\nKarandinos proposed two similar estimators for \"n\". The first was modified by Ruesink to incorporate Taylor's law.\n\nwhere \"d\" is the ratio of half the desired confidence interval (\"CI\") to the mean. In symbols\n\nThe second estimator is used in binomial (presence-absence) sampling. The desired sample size (\"n\") is\n\nformula_58\n\nwhere the \"d\" is ratio of half the desired confidence interval to the proportion of sample units with individuals, \"p\" is proportion of samples containing individuals and \"q\" = 1 - \"p\". In symbols\n\nFor binary (presence/absence) sampling, Schulthess \"et al\" modified Karandinos' equation\n\nformula_60\n\nwhere \"N\" is the required sample size, \"p\" is the proportion of units containing the organisms of interest, \"t\" is the chosen level of significance and \"D\" is a parameter derived from Taylor's law.\n\nSequential analysis is a method of statistical analysis where the sample size is not fixed in advance. Instead samples are taken in accordance with a predefined stopping rule. Taylor's law has been used to derive a number of stopping rules.\n\nA formula for fixed precision in serial sampling to test Taylor's law was derived by Green in 1970.\n\nwhere \"T\" is the cumulative sample total, \"D\" is the level of precision, \"n\" is the sample size and \"a\" and \"b\" are obtained from Taylor's law.\n\nAs an aid to pest control Wilson \"et al\" developed a test that incorporated a threshold level where action should be taken. The required sample size is\n\nwhere \"a\" and \"b\" are the Taylor coefficients, || is the absolute value, \"m\" is the sample mean, \"T\" is the threshold level and \"t\" is the critical level of the t distribution. The authors also provided a similar test for binomial (presence-absence) sampling\n\nwhere \"p\" is the probability of finding a sample with pests present and \"q\" = 1 - \"p\".\n\nGreen derived another sampling formula for sequential sampling based on Taylor's law\n\nwhere \"D\" is the degree of precision, \"a\" and \"b\" are the Taylor's law coefficients, \"n\" is the sample size and \"T\" is the total number of individuals sampled.\n\nSerra \"et al\" have proposed a stopping rule based on Taylor's law.\n\nformula_65\n\nwhere \"a\" and \"b\" are the parameters from Taylor's law, \"D\" is the desired level of precision and \"T\" is the total sample size.\n\nSerra \"et al\" also proposed a second stopping rule based on Iwoa's regression\n\nformula_66\n\nwhere \"α\" and \"β\" are the parameters of the regression line, \"D\" is the desired level of precision and \"T\" is the total sample size.\n\nThe authors recommended that \"D\" be set at 0.1 for studies of population dynamics and \"D\" = 0.25 for pest control.\n\nIt is considered to be good practice to estimate at least one additional analysis of aggregation (other than Taylor's law) because the use of only a single index may be misleading. Although a number of other methods for detecting relationships between the variance and mean in biological samples have been proposed, to date none have achieved the popularity of Taylor's law. The most popular analysis used in conjunction with Taylor's law is probably Iowa's Patchiness regression test but all the methods listed here have been used in the literature.\n\nBarlett in 1936 and later Iawo independently in 1968 both proposed an alternative relationship between the variance and the mean. In symbols\n\nwhere \"s\" is the variance in the \"i\"th sample and \"m\" is the mean of the \"i\"th sample\n\nWhen the population follows a negative binomial distribution, \"a\" = 1 and \"b\" = \"k\" (the exponent of the negative binomial distribution).\n\nThis alternative formulation has not been found to be as good a fit as Taylor's law in most studies.\n\nNachman proposed a relationship between the mean density and the proportion of samples with zero counts:\n\nwhere \"p\" is the proportion of the sample with zero counts, \"m\" is the mean density, \"a\" is a scale parameter and \"b\" is a dispersion parameter. If \"a\" = \"b\" = 0 the distribution is random. This relationship is usually tested in its logarithmic form\n\nAllsop used this relationship along with Taylor's law to derive an expression for the proportion of infested units in a sample\n\nwhere\n\nwhere \"D\" is the degree of precision desired, \"z\" is the upper α/2 of the normal distribution, \"a\" and \"b\" are the Taylor's law coefficients, \"c\" and \"d\" are the Nachman coefficients, \"n\" is the sample size and \"N\" is the number of infested units.\n\nBinary sampling is not uncommonly used in ecology. In 1958 Kono and Sugino derived an equation that relates the proportion of samples without individuals to the mean density of the samples.\n\nwhere \"p\" is the proportion of the sample with no individuals, \"m\" is the mean sample density, \"a\" and \"b\" are constants. Like Taylor's law this equation has been found to fit a variety of populations including ones that obey Taylor's law. Unlike the negative binomial distribution this model is independent of the mean density.\n\nThe derivation of this equation is straightforward. Let the proportion of empty units be \"p\" and assume that these are distributed exponentially. Then\n\nformula_74\n\nTaking logs twice and re arranging, we obtain the equation above. This model is the same as that proposed by Nachman.\n\nThe advantage of this model is that it does not require counting the individuals but rather their presence or absence. Counting individuals may not be possible in many cases particularly where insects are the matter of study.\n\n\nThe equation was derived while examining the relationship between the proportion \"P\" of a series of rice hills infested and the mean severity of infestation \"m\". The model studied was\n\nwhere \"a\" and \"b\" are empirical constants. Based on this model the constants \"a\" and \"b\" were derived and a table prepared relating the values of \"P\" and \"m\"\n\n\nThe predicted estimates of \"m\" from this equation are subject to bias and it is recommended that the adjusted mean ( \"m\" ) be used instead\n\nwhere var is the variance of the sample unit means \"m\" and \"m\" is the overall mean.\n\nAn alternative adjustment to the mean estimates is\n\nwhere MSE is the mean square error of the regression.\n\nThis model may also be used to estimate stop lines for enumerative (sequential) sampling. The variance of the estimated means is\n\nwhere\n\nwhere MSE is the mean square error of the regression, \"α\" and \"β\" are the constant and slope of the regression respectively, \"s\" is the variance of the slope of the regression, \"N\" is the number of points in the regression, \"n\" is the number of sample units and \"p\" is the mean value of \"p\" in the regression. The parameters \"a\" and \"b\" are estimated from Taylor's law:\n\nHughes and Madden have proposed testing a similar relationship applicable to binary observations in cluster, where each cluster contains from 0 to n individuals.\n\nwhere \"a\", \"b\" and \"c\" are constants, \"var\" is the observed variance, and p is the proportion of individuals with a trait (such as disease), an estimate of the probability of an individual with a trait. In logarithmic form, this relationship is\n\nIn most cases, it is assumed that \"b = c\", leading to a simple model\n\nThis relationship has been subjected to less extensive testing than Taylor's law. However, it has accurately described over 100 data sets, and there are no published examples reporting that it does not works.\n\nA variant of this equation was proposed by Shiyomi et al. () who suggested testing the regression\n\nwhere var is the variance, \"a\" and \"b\" are the constants of the regression, \"n\" here is the sample size (not sample per cluster) and \"p\" is the probability of a sample containing at least one individual.\n\nA negative binomial model has also been proposed. The dispersion parameter (\"k\") using the method of moments is \"m\" / ( \"s\" – \"m\" ) and \"p\" is the proportion of samples with counts > 0. The \"s\" used in the calculation of \"k\" are the values predicted by Taylor's law. \"p\" is plotted against 1 - ( \"k\" ( \"k\" + \"m\" ) ) and the fit of the data is visually inspected.\n\nPerry and Taylor have proposed an alternative estimator of \"k\" based on Taylor's law.\n\nA better estimate of the dispersion parameter can be made with the method of maximum likelihood. For the negative binomial it can be estimated from the equation\n\nwhere \"A\" is the total number of samples with more than \"x\" individuals, \"N\" is the total number of individuals, \"x\" is the number of individuals in a sample, \"m\" is the mean number of individuals per sample and \"k\" is the exponent. The value of \"k\" has to be estimated numerically.\n\nGoodness of fit of this model can be tested in a number of ways including using the chi square test. As these may be biased by small samples an alternative is the \"U\" statistic – the difference between the variance expected under the negative binomial distribution and that of the sample. The expected variance of this distribution is \"m\" + \"m\" / \"k\" and\n\nwhere \"s\" is the sample variance, \"m\" is the sample mean and \"k\" is the negative binomial parameter.\n\nThe variance of U is\n\nwhere \"p\" = \"m\" / \"k\", \"q\" = 1 + \"p\", \"R\" = \"p\" / \"q\" and \"N\" is the total number of individuals in the sample. The expected value of \"U\" is 0. For large sample sizes \"U\" is distributed normally.\n\nNote: The negative binomial is actually a family of distributions defined by the relation of the mean to the variance\n\nformula_91\n\nwhere \"a\" and \"p\" are constants. When \"a\" = 0 this defines the Poisson distribution. With \"p\" = 1 and \"p\" = 2, the distribution is known as the NB1 and NB2 distribution respectively.\n\nThis model is a version of that proposed earlier by Barlett.\n\nThe dispersion parameter (\"k\") is\n\nwhere \"m\" is the sample mean and \"s\" is the variance. If \"k\" is > 0 the population is considered to be aggregated; \"k\" = 0 the population is considered to be random; and if \"k\" is < 0 the population is considered to be uniformly distributed.\nSouthwood has recommended regressing \"k\" against the mean and a constant\n\nwhere \"k\" and \"m\" are the dispersion parameter and the mean of the ith sample respectively to test for the existence of a common dispersion parameter (\"k\"). A slope (\"b\") value significantly > 0 indicates the dependence of \"k\" on the mean density.\n\nAn alternative method was proposed by Elliot who suggested plotting ( \"s\" − \"m\" ) against ( \"m\" − \"s\" / \"n\" ). \"k\" is equal to 1/slope of this regression.\n\nThis coefficient (\"C\") is defined as\n\nformula_94\n\nIf the population can be assumed to be distributed in a negative binomial fashion, then \"C\" = 100 (1/\"k\") where \"k\" is the dispersion parameter of the distribution.\n\nThis index (\"I\") is defined as\n\nThe usual interpretation of this index is as follows: values of \"I\" < 1, 1, > 1 are taken to mean a uniform distribution, a random distribution or an aggregated distribution.\n\nBecause \"s\" = Σ x − (Σx), the index can also be written\n\nIf Taylor's law can be assumed to hold, then\n\nLloyd's index of mean crowding (\"IMC\") is the average number of other points contained in the sample unit that contains a randomly chosen point.\n\nwhere \"m\" is the sample mean and \"s\" is the variance.\n\nLloyd's index of patchiness (\"IP\") is\n\nIt is a measure of pattern intensity that is unaffected by thinning (random removal of points). This index was also proposed by Pielou in 1988 and is sometimes known by this name also.\n\nBecause an estimate of the variance of \"IP\" is extremely difficult to estimate from the formula itself, LLyod suggested fitting a negative binomial distribution to the data. This method gives a parameter \"k\"\n\nThen\n\nwhere \"SE\"(\"IP\") is the standard error of the index of patchiness,\"var\"(\"k\") is the variance of the parameter \"k\" and \"q\" is the number of quadrats sampled..\n\nIf the population obeys Taylor's law then\n\nIwao proposed a patchiness regression to test for clumping\n\nLet\n\n\"y\" here is Lloyd's index of mean crowding. Perform an ordinary least squares regression of \"m\" against \"y\".\n\nIn this regression the value of the slope (\"b\") is an indicator of clumping: the slope = 1 if the data is Poisson-distributed. The constant (\"a\") is the number of individuals that share a unit of habitat at infinitesimal density and may be < 0, 0 or > 0. These values represent regularity, randomness and aggregation of populations in spatial patterns respectively. A value of \"a\" < 1 is taken to mean that the basic unit of the distribution is a single individual.\n\nWhere the statistic \"s\" / \"m\" is not constant it has been recommended to use instead to regress Lloyd's index against \"am\" + \"bm\" where \"a\" and \"b\" are constants.\n\nThe sample size (\"n\") for a given degree of precision (\"D\") for this regression is given by\n\nwhere \"a\" is the constant in this regression, \"b\" is the slope, \"m\" is the mean and \"t\" is the critical value of the t distribution.\n\nIawo has proposed a sequential sampling test based on this regression. The upper and lower limits of this test are based on critical densities m where control of a pest requires action to be taken.\n\nwhere \"N\" and \"N\" are the upper and lower bounds respectively, \"a\" is the constant from the regression, \"b\" is the slope and \"i\" is the number of samples.\n\nKuno has proposed an alternative sequential stopping test also based on this regression.\n\nwhere \"T\" is the total sample size, \"D\" is the degree of precision, \"n\" is the number of samples units, a is the constant and b is the slope from the regression respectively.\n\nKuno's test is subject to the condition that \"n\" ≥ (\"b\" − 1) / \"D\"\n\nParrella and Jones have proposed an alternative but related stop line\n\nwhere \"a\" and \"b\" are the parameters from the regression, \"N\" is the maximum number of sampled units and \"n\" is the individual sample size.\n\nMorisita's index of dispersion ( \"I\" ) is the scaled probability that two points chosen at random from the whole population are in the same sample. Higher values indicate a more clumped distribution.\n\nAn alternative formulation is\n\nwhere \"n\" is the total sample size, \"m\" is the sample mean and \"x\" are the individual values with the sum taken over the whole sample.\nIt is also equal to\n\nwhere \"IMC\" is Lloyd's index of crowding.\n\nThis index is relatively independent of the population density but is affected by the sample size. Values > 1 indicate clumping; values < 1 indicate a uniformity of distribution and a value of 1 indicates a random sample.\n\nMorisita showed that the statistic\n\nis distributed as a chi squared variable with \"n\" − 1 degrees of freedom.\n\nAn alternative significance test for this index has been developed for large samples.\n\nwhere \"m\" is the overall sample mean, \"n\" is the number of sample units and \"z\" is the normal distribution abscissa. Significance is tested by comparing the value of \"z\" against the values of the normal distribution.\n\nA function for its calculation is available in the statistical R language. R function\n\nSmith-Gill developed a statistic based on Morisita's index which is independent of both sample size and population density and bounded by −1 and +1. This statistic is calculated as follows\n\nFirst determine Morisita's index ( \"I\" ) in the usual fashion. Then let \"k\" be the number of units the population was sampled from. Calculate the two critical values\n\nwhere χ is the chi square value for \"n\" − 1 degrees of freedom at the 97.5% and 2.5% levels of confidence.\n\nThe standardised index ( \"I\" ) is then calculated from one of the formulae below\n\nWhen \"I\" ≥ \"M\" > 1\n\nWhen \"M\" > \"I\" ≥ 1\n\nWhen 1 > \"I\" ≥ \"M\"\n\nWhen 1 > \"M\" > \"I\"\n\n\"I\" ranges between +1 and −1 with 95% confidence intervals of ±0.5. \"I\" has the value of 0 if the pattern is random; if the pattern is uniform, \"I\" < 0 and if the pattern shows aggregation, \"I\" > 0.\n\nSouthwood's index of spatial aggregation (\"k\") is defined as\n\nwhere \"m\" is the mean of the sample and \"m\"* is Lloyd's index of crowding.\n\nFisher's index of dispersion is\n\nThis index may be used to test for over dispersion of the population. It is recommended that in applications n > 5 and that the sample total divided by the number of samples is > 3. In symbols\n\nwhere \"x\" is an individual sample value. The expectation of the index is equal to \"n\" and it is distributed as the chi-square distribution with \"n\" − 1 degrees of freedom when the population is Poisson distributed. It is equal to the scale parameter when the population obeys the gamma distribution.\n\nIt can be applied both to the overall population and to the individual areas sampled individually. The use of this test on the individual sample areas should also include the use of a Bonferroni correction factor.\n\nIf the population obeys Taylor's law then\n\nThe index of cluster size (\"ICS\") was created by David and Moore. Under a random (Poisson) distribution \"ICS\" is expected to equal 0. Positive values indicate a clumped distribution; negative values indicate a uniform distribution.\n\nwhere \"s\" is the variance and \"m\" is the mean.\n\nIf the population obeys Taylor's law\n\nThe \"ICS\" is also equal to Katz's test statistic divided by ( \"n\" / 2 ) where \"n\" is the sample size. It is also related to Clapham's test statistic. It is also sometimes referred to as the clumping index.\n\nGreen's index (\"GI\") is a modification of the index of cluster size that is independent of \"n\" the number of sample units.\n\nThis index equals 0 if the distribution is random, 1 if it is maximally aggregated and −1 / ( \"nm\" − 1 ) if it is uniform.\n\nThe distribution of Green's index is not currently known so statistical tests have been difficult to devise for it.\n\nIf the population obeys Taylor's law\n\nBinary sampling (presence/absence) is frequently used where it is difficult to obtain accurate counts. The dispersal index (\"D\") is used when the study population is divided into a series of equal samples ( number of units = \"N\": number of units per sample = \"n\": total population size = \"n\" x \"N\" ). The theoretical variance of a sample from a population with a binomial distribution is\n\nwhere \"s\" is the variance, \"n\" is the number of units sampled and \"p\" is the mean proportion of sampling units with at least one individual present. The dispersal index (\"D\") is defined as the ratio of observed variance to the expected variance. In symbols\n\nwhere \"var\" is the observed variance and \"var\" is the expected variance. The expected variance is calculated with the overall mean of the population. Values of \"D\" > 1 are considered to suggest aggregation. \"D\"( \"n\" − 1 ) is distributed as the chi squared variable with \"n\" - 1 degrees of freedom where \"n\" is the number of units sampled.\n\nAn alternative test is the \"C\" test.\n\nwhere \"D\" is the dispersal index, \"n\" is the number of units per sample and \"N\" is the number of samples. C is distributed normally. A statistically significant value of C indicates overdispersion of the population.\n\n\"D\" is also related to intraclass correlation ( ρ ) which is defined as\n\nwhere \"T\" is the number of organisms per sample, \"p\" is the likelihood of the organism having the sought after property (diseased, pest free, \"etc\"), and x is the number of organism in the \"i\"th unit with this property. \"T\" must be the same for all sampled units. In this case with \"n\" constant\n\nIf the data can be fitted with a beta-binomial distribution then\n\nwhere \"θ\" is the parameter of the distribution.\n\nMa has proposed a parameter (\"m\") − the population aggregation critical density - to relate population density to Taylor's law.\n\nA number of statistical tests are known that may be of use in applications.\n\nA related statistic suggested by de Oliveria is the difference of the variance and the mean. If the population is Poisson distributed then\n\nwhere \"t\" is the Poisson parameter, \"s\" is the variance, \"m\" is the mean and \"n\" is the sample size. The expected value of \"s\" - \"m\" is zero. This statistic is distributed normally.\n\nIf the Poisson parameter in this equation is estimated by putting \"t\" = \"m\", after a little manipulation this statistic can be written\n\nThis is almost identical to Katz's statistic with ( \"n\" - 1 ) replacing \"n\". Again \"O\" is normally distributed with mean 0 and unit variance for large \"n\". This statistic is the same as the Neyman-Scott statistic.\n\n\nde Oliveria actually suggested that the variance of \"s\" - \"m\" was ( 1 - 2\"t\" + 3\"t\" ) / \"n\" where \"t\" is the Poisson parameter. He suggested that \"t\" could be estimated by putting it equal to the mean (\"m\") of the sample. Further investigation by Bohning showed that this estimate of the variance was incorrect. Bohning's correction is given in the equations above.\n\nIn 1936 Clapham proposed using the ratio of the variance to the mean as a test statistic (the relative variance). In symbols\n\nFor a Possion distribution this ratio equals 1. To test for deviations from this value he proposed testing its value against the chi square distribution with \"n\" degrees of freedom where \"n\" is the number of sample units. The distribution of this statistic was studied further by Blackman who noted that it was approximately normally distributed with a mean of 1 and a variance ( \"V\" ) of\n\nThe derivation of the variance was re analysed by Bartlett who considered it to be\n\nFor large samples these two formulae are in approximate agreement. This test is related to the later Katz's \"J\" statistic.\n\nIf the population obeys Taylor's law then\n\n\nA refinement on this test has also been published These authors noted that this test tends to detect overdispersion at higher scales even when this was not present in the data. They noted that the use of the multinomial distribution may be more appropriate than the use of a Poisson distribution for such data. The statistic \"θ\" is distributed\n\nwhere \"N\" is the number of sample units, \"n\" is the total number of samples examined and \"x\" are the individual data values.\n\nThe expectation and variance of \"θ\" are\n\nFor large \"N\" \"E\"(θ) is approximately 1 and\n\nIf the number of individuals sampled ( \"n\" ) is large this estimate of the variance is in agreement with those derived earlier. However, for smaller samples these latter estimates are more precise and should be used.\n\n"}
{"id": "735430", "url": "https://en.wikipedia.org/wiki?curid=735430", "title": "Timestamp", "text": "Timestamp\n\nA timestamp is a sequence of characters or encoded information identifying when a certain event occurred, usually giving date and time of day, sometimes accurate to a small fraction of a second. The term derives from rubber stamps used in offices to stamp the current date, and sometimes time, in ink on paper documents, to record when the document was received. Common examples of this type of timestamp are a postmark on a letter or the \"in\" and \"out\" times on a time card.\nIn modern times usage of the term has expanded to refer to digital date and time information attached to digital data. For example, computer files contain timestamps that tell when the file was last modified, and digital cameras add timestamps to the pictures they take, recording the date and time the picture was taken.\n\nA timestamp is the time at which an event is recorded by a computer, not the time of the event itself. In many cases, the difference may be inconsequential: the time at which an event is recorded by a timestamp (e.g., entered into a log file) should be close to the time of the event.\n\nThis data is usually presented in a consistent format, allowing for easy comparison of two different records and tracking progress over time; the practice of recording timestamps in a consistent manner along with the actual data is called timestamping. The sequential numbering of events is sometimes called timestamping.\n\nTimestamps are typically used for logging events or in a sequence of events (SOE), in which case each event in the log or SOE is marked with a timestamp.\n\nPractically all computer file systems store one or more timestamps in the per-file metadata.\nIn particular, most modern operating systems support the POSIX stat (system call), so each file has 3 timestamps associated with it:\ntime of last access (atime: ls -lu),\ntime of last modification (mtime: ls -l), and\ntime of last status change (ctime: ls -lc).\n\nSome file archivers and some version control software, when they copy a file from some remote computer to the local computer, adjust the timestamps of the local file to show the date/time in the past when that file was created or modified on that remote computer, rather than the date/time when that file was copied to the local computer.\n\nExamples of timestamps:\n\nISO 8601 standardizes the representation of dates and times. These standard representations are often used to construct timestamp values.\n\nTimestamp can also refer to:\n\n\n"}
{"id": "47385882", "url": "https://en.wikipedia.org/wiki?curid=47385882", "title": "Yishai Schlissel", "text": "Yishai Schlissel\n\nYishai Schlissel (also spelled Shlisel; ; born 10 December 1975) is an Israeli convicted criminal. He stabbed marchers during the Jerusalem gay pride parade in 2005, for which he served ten years in prison. On 30 July 2015, during the 2015 Jerusalem gay pride, he stabbed 16-year-old Shira Banki to death, and wounded six other people. The incident occurred just three weeks after his release from prison. On 24 August, Schlissel was indicted for murder, six counts of attempted murder, and wounding under aggravating circumstances, and detained until the end of proceedings. On June 26 2016, he was sentenced to life plus 31 years in prison, as well as pay NIS 2,064,000 (around $650,000) in damages. \n\nSchlissel was born on 10 December 1975 in Yad Binyamin. He was the eldest of ten children. His parents are the children of Holocaust survivors. He studied Talmud, and is very religious. He later studied in a kolel in Jerusalem.\n\nIn 2005, Schlissel stabbed three marchers during the gay pride parade in Jerusalem. As a result, he was convicted of attempted murder and aggravated assault, and sentenced to twelve years in prison. In 2007, his sentence was reduced to ten years on appeal. Schlissel served his sentence at Maasiyahu Prison. In 2008, he was hospitalized for a month and a half over mental health issues and diagnosed with a paranoid psychiatric condition. Schlissel was released in June 2015.\n\nShortly after his release, he distributed a homophobic letter in his hometown which read, \"It is the obligation of every Jew to keep his soul from punishment and stop this giant desecration of God's name next Thursday.\" He was not tracked by the Judea and Samaria Police District because his previous crime had occurred in Jerusalem, outside of their jurisdiction. Moreover, Moshe Edry, the chief of the Jerusalem District Police, was not warned that Schlissel would be coming to Jerusalem.\n\nOn 30 July 2015, only three weeks after being released, he stabbed six marchers during the Jerusalem gay pride parade. The act was widely condemned, including by Prime Minister Benjamin Netanyahu. One of the victims, 16-year-old Shira Banki, died of her wounds at the Hadassah Medical Center three days later, on 2 August 2015. Shortly after, Prime Minister Netanyahu offered his condolences, adding \"We will deal with the murderer to the fullest extent of the law.\"\n\nA psychiatric evaluation by the Israel Prison Service, with which Schlissel refused to cooperate, found him fit to stand trial. However, after state psychiatrists argued that his refusal to cooperate rendered the evaluation inconclusive, the court ordered that he be hospitalized for a 48-hour observation period for reevaluation. He was again found fit to stand trial. On 24 August 2015, Shlissel was charged with one count of murder and six counts of attempted murder and aggravated assault at the Jerusalem District Court. He refused to recognize the authority of the court to try him due to it not abiding by Jewish religious law. He announced that he would refuse to be represented by an attorney, and did not cooperate with the public defender assigned to represent him. On April 19, 2016, he was convicted. On June 26, 2016, he was sentenced to life plus 31 years in prison, and was ordered to pay NIS 2,064,000 in compensation to the families of his victims. \n\nSchlissel is serving his sentence in Ayalon Prison. In August 2016, Schlissel tore up pictures of another inmate's daughters due to them being \"immodestly dressed\", and was subsequently beaten by the inmate until guards separated them. Schlissel was hospitalized as a result of the beating he sustained, and was temporarily placed in solitary confinement as a disciplinary measure. After prison authorities carried out an evaluation, it was decided to separate him from other inmates due to his volatile behavior, and he was given his own cell in a protected area of the prison. One month later, Schlissel was again attacked and had to be hospitalized after an argument between him and two convicted mobsters in the prison yard escalated to the mobsters beating him until guards intervened.\n\nBefore his prison sentence, Schlissel resided in Modi'in Illit, a Haredi Israeli settlement and city in the West Bank. He is an Israeli ultra-Orthodox Jew. Prior to his crime, he was married, with four children. He divorced his wife at the beginning of his imprisonment in 2005.\n"}
