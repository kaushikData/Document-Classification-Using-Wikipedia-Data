{"id": "56337393", "url": "https://en.wikipedia.org/wiki?curid=56337393", "title": "125th anniversary of the overthrow of the Kingdom of Hawaii", "text": "125th anniversary of the overthrow of the Kingdom of Hawaii\n\nThe 125th anniversary of the overthrow of the Kingdom of Hawaii was marked by a march of thousands of people from Mauna Ala (the Hawaiian Royal Mausoleum) to the Iolani Palace to commemorate Liliʻuokalani's forced removal from the throne and raise the Hawaiian Kingdom Flag at the exact moment it was lowered in 1893.\n\nOn July 6, 1846, U.S. Secretary of State John C. Calhoun, on behalf of President Tyler, afforded formal recognition of Hawaiian independence under the reign of Kamehameha III. As a result of the recognition of Hawaiian independence, the Hawaiian Kingdom entered into treaties with the major nations of the world and established over ninety legations and consulates in multiple seaports and cities. The Kamehameha Dynasty was the reigning monarchy of the Kingdom of Hawaii, beginning with its founding by Kamehameha I in 1795, until the death of Kamehameha V in 1872 and Lunalilo in 1874. The kingdom would continue under the House of Kalākaua for another 21 years until its overthrow in 1893 when a coup d'état against Queen Liliuokalani was supported by U.S. Marines.\n\nOn January 17, 1993 a march was held from the Aloha Tower to the Iolani Palace to commemorate the 100th anniversary of the event. The march of 15,000 people was led by the Ka Lāhui and was part of the \"ʻOnipaʻa\", an observance of the queen's overthrow. The 1993 observance took its name from Queen Liliuokalani's motto, \"ʻOnipaʻa\" (to remain steadfast). 20,000 people are estimated to have met at the palace for a series of events that included a re-enactment of the overthrow and speeches from activists and educators like Haunani-Kay Trask who said; \"We are not Americans! We are not Americans! We will die as Hawiians! We will never be American!\" Trask and others from the University of Hawaii at Manoa worked closely with film-makers Puhipau and John Lander of the production company \"Nā Maka o ka 'Āina\" to create the film; \"Act of War:The overthrow of the Hawaiian Nation\". The film helped bring the findings of Hawaiian historians like Trask, Jonathan Kamakawiwo'ole Osorio and Lilikalā Kameʻeleihiwa to a larger and broader audience. In 1993, Trask also released her well known book; \"From a Native Daughter: Colonialism and Sovereignty in Hawaii\" that dealt with such topics as corporate tourism, academic exploitation, suppression of Native epistemology and histories as well as the high number of ancient sites including burials that have been destroyed An iconic image from the cover of her book shows the 1993 march as it neared the palace. The book's cover photo was taken by photographer Ed Greevy however, Honolulu-Star Advertiser photographer Bruce Asato also captured the moment and appeared on the front page of the newspaper. Both image depict the organizers of the march stopping to allow elders to enter the grounds of the palace first.\n\nMarking the 122nd anniversary of the overthrow, activists organized the \"Queen Liliuokalani Kingdom Restoration Spiritual Walk\" in 2015, beginning the march from Mauna Ala (the Royal Mausoleum) down to the Iolani Palace, to Liliuokalani's statue and then to the capital rotunda. Organizer's concerns included crown lands being sold illegally, the desecration of sacred grounds and the moving of ancient burials for the ongoing rail project.\n\nWhen a false nuclear ballistic missile alert went off 4 days before the event, one of the organizers, Kaukaohu Wahilani, mentioned how the US military is linked to colonialism; \"It was only through the might of the American military that the overthrow was successful\". The event, \"ʻOnipaʻa Kākou\", made a solemn day a memorable event featuring a rally after the march with speeches and hula and ceremonial protocols and prayers. Beginning one week before the event, Hinaleimoana Wong began holding the Mana Ka Lahui Mele Workshops for the community to learn songs being used during the day. University of Hawaii at Manoa students participated and helped print specially designed T-shirts created by Tita Coloma just for the day.\n\nThe full event began with protocol observances at Mauna Ala with a march to the palace for the flag raising and then to the statue of Queen Liliʻuokalani where ceremonial offerings were made. The crowd entered through the main gates of the palace grounds as oli (chants) were performed and participants carried torches, Hawaiian flags and two purple Kāhili which were carried alongside a marcher carrying Queen Liliuokalani's portrait.\n"}
{"id": "34840", "url": "https://en.wikipedia.org/wiki?curid=34840", "title": "6th century BC", "text": "6th century BC\n\nThe 6th century BC started the first day of 600 BC and ended the last day of 501 BC.\n\nThis century represents the peak of a period in human history popularly known as Axial Age. This period saw the emergence of five major thought streams springing from five great thinkers in different parts of the world: Buddha and Mahavira in India, Zoroaster in Persia, Pythagoras in Greece and Confucius in China.\nPāṇini, in India, composed a grammar for Sanskrit, in this century or slightly later. This is the oldest still known grammar of any language.\n\nIn Western Asia, the first half of this century was dominated by the Neo-Babylonian or Chaldean empire, which had risen to power late in the previous century after successfully rebelling against Assyrian rule. The Kingdom of Judah came to an end in 586 BC when Babylonian forces under Nebuchadnezzar II captured Jerusalem, and removed most of its population to their own lands. Babylonian rule was ended in the 540s by Cyrus, who founded the Persian Empire in its place. The Persian Empire continued to expand and grew into the greatest empire the world had known at the time.\n\nIn Iron Age Europe, the Celtic expansion was in progress. China was in the Spring and Autumn period.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee: List of sovereign states in the 6th century BC.\n"}
{"id": "57165078", "url": "https://en.wikipedia.org/wiki?curid=57165078", "title": "Alexandra Club", "text": "Alexandra Club\n\nThe Alexandra Club was a private members club for women in Edwardian London. It was based at 12 Grosvenor Street, at the east end of the street on the north side, in London's Mayfair district. The club was founded in 1884, and closed in 1939. The club had 600 members by 1888.\n\nMembership of the club was only available to women eligible to attend the Queen's Drawing Rooms. Amy Levy in her 1888 novel, \"The Romance of a Shop\" considered the merits of the Alexandra Club against other clubs for women and concluded that the phrase \"who has been or who would probably be precluded from Her Majesty's Drawing Rooms\" to be \"full of the sound and fury of exclusiveness and signifying not so much after all\". Smoking was forbidden at the club, and members were not permitted to entertain men. Accommodation was available.\n\nThe entrance fee was 5 guineas, with the annual subscription fee 4 guineas for members from the country, and 5 guineas for those in town. The popularity of the club led to disputes between members over the best tables those in which according to the historian Anne de Courcy, the \"lunchers could be viewed in all their glory from the street\". The Prince of Wales, visiting his wife, Princess Alexandra, was once denied entrance by the footman of the club. The prince's satisfaction over this caused him amusement and led him to say that the club was entitled to bear his wife's name as a result.\n"}
{"id": "43343961", "url": "https://en.wikipedia.org/wiki?curid=43343961", "title": "Arab Winter", "text": "Arab Winter\n\nThe Arab Winter is a term for the resurgence of authoritarianism and Islamic extremism evolving in the aftermath of the Arab Spring protests in Arab countries. The term \"Arab Winter\" refers to the events across Arab League countries in the Mid-East and North Africa, including the Syrian Civil War, the Iraqi insurgency and the following civil war, the Egyptian Crisis, the Libyan Crisis and the Crisis in Yemen. Events referred to as the Arab Winter include those in Egypt that led to the removal of Mohamed Morsi and the seizure of power by General Abdel Fattah el-Sisi in an anti-Muslim Brotherhood campaign.\n\nAccording to scholars of the University of Warsaw, the Arab Spring fully devolved into the Arab Winter four years after its onset. The Arab Winter is characterized by the emergence of multiple regional civil wars, mounting regional instability, economic and demographic decline of Arab countries, and ethno-religious sectarian strife. According to a study by the American University of Beirut, as of the summer of 2014 the Arab Winter had resulted in nearly a quarter of a million deaths and millions of refugees. Perhaps the most significant event in the Arab Winter was the rise of the extreme ISIL group, which controlled large swathes of land from 2014.\n\nThe term \"Arab Winter\" refers to the events across Arab League countries in the Mid-East and North Africa, including the Syrian Civil War, the Iraqi insurgency and the following civil war, the Egyptian Crisis, the Libyan Crisis and the Crisis in Yemen. Events referred to as the Arab Winter include those in Egypt that led to the removal of Mohamed Morsi and the seizure of power by General Abdel Fattah el-Sisi in an anti-Muslim Brotherhood military coup. Political developments, particularly the restoration of authoritarianism and suppression of civil liberties in Egypt since July 3, 2013, have been described as constituting a \"military winter\" that functioned in opposition to the goals of the Arab Spring. Various militias and tribes have started fighting in Libya after a breakdown in negotiations. The arenas of Lebanon and Bahrain were also identified as areas of the Arab Winter. Libya was named as a scene of the Arab Winter, together with Syria, by Professor Sean Yom. The Northern Mali conflict was often described as part of the \"Islamist Winter\". Political changes which occurred in Tunisia, involving a change in government, as well as an ISIL insurgency, were also indicated by some as a possible \"heading towards Arab Winter\".\n\nChinese professor Zhang Weiwei first predicted \"Arab Winter\" in his June 2011 debate with Francis Fukuyama, who believed the movement might be spread to China. \"My understanding of the Middle East leads me to conclude that the west should not be too happy. It will bring enormous problems to American interest. It is called \"Arab Spring\" for now, and I guess it will soon turn to be the winter for the Middle East.\"\n\nAccording to scholars of the University of Warsaw, the Arab Spring fully devolved into the Arab Winter four years after its onset. This view was also supported by Prof. James Y. Simms Jr. in his 2017 opinion article for the \"Richmond Times\". In early 2016, \"The Economist\" marked the situation across Arab world countries as \"worse than ever\", marking it as the ongoing Arab Winter.\n\nAccording to the Moshe Dayan Center for Middle Eastern and African Studies, as of January 2014, the cost of Arab Winter upheaval across the Arab World was some 800 billion USD. Some 16 million people in Syria, Egypt, Iraq, Jordan and Lebanon were expected to require humanitarian assistance in 2014.\n\nAccording to \"The Economist\", Malta has \"benefited\" from the Arab Winter, as tourists who might otherwise be in Egypt or Tunisia opt for a safer alternative.\n\nAccording to a study by the American University of Beirut, as of the summer of 2014 the Arab Winter had resulted in nearly a quarter of a million deaths and millions of refugees.\n\nPolitical columnist and commentator George Will reported that as of early 2017, over 30,000 lives had been lost in Libya, 220,000-320,000 had been killed in Syria and 4 million refugees had been produced by the Syrian Civil War alone.\n\nThe political turmoil and violence in the Middle East and North Africa resulted in massive population displacement in the region. As a result, “boat-people”, which was once commonly referred to Vietnamese boat people, became frequently used, including internally displaced persons and asylum-seekers and refugees who had previously been residing in Libya, have headed towards the European Union. The attempts by some Libyans and Tunisians to seek safety from the violence by crossing the Mediterranean sea have triggered fears among European politicians and populations of arrivals that might \"flood\" their shores. This has spurred a flurry of legislative activity and patrolling of the waters to manage arrivals.\n\n"}
{"id": "24754777", "url": "https://en.wikipedia.org/wiki?curid=24754777", "title": "Ben Segenreich", "text": "Ben Segenreich\n\nBen Segenreich (born 3 March 1952 in Vienna) is a journalist, correspondent and an expert for the Arab–Israeli conflict at the ORF in Israel (Tel Aviv).\n\nBen Segenreich finished school at the French Lyzeum in Vienna and studied Physics and Mathematics in Paris and Vienna. After gaining his doctoral degree in Physics he worked in the software development and was at the same time the Austrian correspondent of the Israeli daily newspaper Maariv.\n\nIn 1983 he emigrated to Israel and continued his work in the software development. Since 1989 he was correspondent for Austrian, German and Swiss daily newspapers and magazines. Since 1990 Ben Segenreich is TV and radio correspondent of the ORF in Israel.\n\nSince 2009 Ben Segenreich is member of the International Council of the Austrian Service Abroad.\n\nHe is married and is father of two daughters.\n\n"}
{"id": "1895197", "url": "https://en.wikipedia.org/wiki?curid=1895197", "title": "British Book Awards", "text": "British Book Awards\n\nThe British Book Awards or Nibbies are literary awards for the best UK writers and their works, administered by \"The Bookseller\". The awards have had several previous names, owners and sponsors since being launched in 1990, including the National Book Awards from 2010-2014.\n\nThe British Book Awards or Nibbies ran from 1990–2009 and founded by the editor of \"Publishing News\". The award was then acquired by Agile Marketing which renamed it the National Book Awards with headline sponsors Galaxy National Book Awards (2010–11) (sponsored by Galaxy) and Specsavers National Book Awards (2012-2014) (sponsored by Specsavers). There were no National Book Awards after 2014. In 2017 the award was acquired by \"The Bookseller\" and renamed to the original British Book Awards or Nibbies..\n\nIn 2005, \"The Bookseller\" launched a separate scheme, The Bookseller Retail Awards (winners not listed in this article). In 2010, running parallel to the National Book Awards, \"The Bookseller\" combined The Nibbies with its retail awards to produce The Bookseller Industry Awards (winners not listed in this article). In 2017 The British Book Industry Awards were renamed as The British Book Awards after it acquired the National Book Awards from Agile Marketing.\n\nIt is known as the \"Nibbies\" because of the golden nib-shaped trophy given to winners.\n\nPrior to 2010 the Best was a unique winner. Starting in 2010, the Best was chosen by the public via open internet vote from among one of the winning books in the other categories. \n\nPreviously called British Children's Book of the Year. Renamed to Children's Book of the Year in 2010.\n\nPreviously called the \"Newcomer of the Year\". Name changed to \"New Writer of the Year\" in 2010. Name changed to \"Début Book of the Year\" in 2017.\n\nPreviously called Popular Fiction Award. Name changed to Popular Fiction Book of the Year in 2010. Name changed to Fiction Book of the Year in 2017.\n\nPreviously called the Crime Thriller of the Year. Name changed to Thriller & Crime Novel of the Year in 2011. Name changed to Crime & Thriller Book of the Year in 2017.\n\n\n\nNamed \"Bestseller of the Year\" in 1991. Renamed \"Bestseller Award\" in 2017.\n\nThe following awards are no longer active.\n\nPreviously called Biography of the Year. Name changed to Biography/Autobiography of the Year in 2010.\n\n\n\n\n\nPreviously called the Lifetime Achievement Award (1993–2009). Renamed to Outstanding Achievement Award in 2010.\n\nPreviously called Author of the Year. Renamed to UK Author of the Year in 2010 notwithstanding the fact the award has been given to non UK authors.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "28754767", "url": "https://en.wikipedia.org/wiki?curid=28754767", "title": "C.V. Starr Center for the Study of the American Experience", "text": "C.V. Starr Center for the Study of the American Experience\n\nThe C.V. Starr Center for the Study of the American Experience is an institute at Washington College, in Chestertown, Maryland, that promotes the research and study of American history and culture. Founded in 2000, the C.V. Starr Center at Washington College is one of many educational initiatives funded by the Starr Foundation, a private foundation with assets of over $1.25 billion. The inaugural director of the C.V. Starr Center, Edward L. Widmer, served under Bill Clinton as special assistant to the president for national security affairs; among other accomplishments, he wrote foreign policy speeches and advised the president on topics related to history and scholarship as senior advisor to the president for special projects. Since 2006, Adam Goodheart, a historian, journalist and author of \"1861: The Civil War Awakening\", has served as director of the Center. In addition to its academic components, the C.V. Starr Center works closely with external groups to sponsor events of public interest, such as the Poplar Grove Project, a recovery and recordation project in collaboration with the Maryland State Archives, and hosts readings and lectures often focused on topics of local interest, such as Chesapeake Bay history.\n\nThe C.V. Starr Center for the Study of the American Experience is headquartered in Chestertown’s Custom House, a building constructed in the 1740s by Samuel Massey as a residence for the Ringgold family and known for its detailed Flemish bond brickwork with glazed headers., The location, beside the public dock at the intersection of High and Water Streets, has always been central to much of Chestertown’s daily activity in commerce, industry, and tourism. Over its 200+ year history, the Custom House has changed hands several times. Senator James Alfred Pearce, who later chaired the Joint Library Committee of Congress and served on the Smithsonian Institution’s Board of Regents, lived in the Custom House in the late 19th century. One of the building’s most influential owners, Wilbur Ross Hubbard, carried out a major renovation and restoration project in the 1970s before bequeathing the house to Washington College. The front portico is another recent addition, designed by consultant Michael Bourne to integrate local motifs. Buildings of this style, size, and period are now rare in the territory that once comprised the original 13 colonies. In 1969, the National Park Service recognized the Custom House on the National Survey of Historic Sites and Buildings. In addition to the C.V. Starr Center, the Custom House serves as home to the Center for Environment & Society and the Washington College Archaeology Lab.\n\nThe Starr Center hosts numerous public events, including talks by visiting authors, museum programs, concerts, panel discussions and public conversations on American culture. Recent speakers have included Senators Birch Bayh and Richard Lugar, filmmaker John Waters, actress Anna Deavere Smith, journalist Michael Meyer, civil rights attorney Sherrilyn Ifill, novelist James McBride and historians James McPherson, Annette Gordon-Reed and David Blight.\n\nThe “History on the Waterfront” multimedia program, launched in 2009, is a free thirty-minute audio-guided tour recreating the sights and sounds of an 18th-century working waterfront. The tour provides a walk back in time into an era when local streets bustled with revolutionaries and convicts, slave traders, British soldiers and heroes of the Underground Railroad. “History on the Waterfront” was researched, written and performed by Washington College students, faculty and staff, along with members of the Chestertown community, and includes narrative, music, reenactments and firsthand accounts of life in the colonial port. The program was orchestrated by the Starr Center's associate director, Jill Ogline Titus, and narrated by the Center’s director, Adam Goodheart. The Center’s program manager, Michael Buckley, who also produces the weekly radio series \"Voices of the Chesapeake Bay\" on 103.1 WRNR, oversaw the technical aspects of the production. The “History on the Waterfront” tour is free of charge and available year-round.\n\nThe Riverfront Concert Series, which debuted in 2010, offers free musical performances throughout the summer on the riverfront lawn of the Custom House. Performers in the 2010 series included singer-songwriter Bob Zentz and acoustic guitar duo Mac Walter & John Cronin. The series builds on the Starr Center’s longstanding interest in the musical traditions of Chesapeake Bay and its rich heritage of storytelling.\n\nThe Poplar Grove Plantation, near Centreville, Maryland, is the location of the Poplar Grove Project, an ongoing exploration of the Emory family’s papers, some dating back as far as the 17th century. Starr Center director Adam Goodheart began initial excavations at Poplar Grove in 2003 with the Archaeology Field School at Washington College, and in 2008, one of his students discovered the collection of papers. In conjunction with the Maryland State Archives, the Poplar Grove project seeks to preserve, document and study this large collection. Current Poplar Grove owner James Wood, a descendant of the Emory family, has been a source of information for Goodheart’s students as they participate in the project. His mother, Mary Wood, published a book about the Emory women called \"My Darling Alice: Based on Letters and Legends of an Eastern Shore of Maryland Farm – 1837 – 1935\". To date, over 28,406 documents have been recovered and scanned into digital files that will be published online.\n\nThe George Washington Book Prize recognizes the year's best books on the nation's founding era, especially those with the potential to advance broad public understanding of American history. Sponsored by Washington College, the Gilder Lehrman Institute of American History, and George Washington’s Mount Vernon, and administered by the Starr Center, the $50,000 award is one of the largest literary prizes in America. In 2005, the inaugural book prize was awarded to Ron Chernow for his biography \"Alexander Hamilton\". Other winners have included Richard Beeman (2010) for \"Plain, Honest Men: The Making of the American Constitution\", Annette Gordon-Reed (2009) for \"The Hemingses of Monticello\", Marcus Rediker (2008) for \"The Slave Ship: A Human History\", and Stacy Schiff (2006) for \"A Great Improvisation: Franklin, France, and the Birth of America\". In 2011, Pauline Maier was awarded the prize for her book \"Ratification: The People Debate the Constitution. 1787–1788\".\n\nThe Patrick Henry Writing Fellowship offers a nine-month residency to authors doing innovative work on America’s founding era and its legacy. Launched by the Starr Center in 2008, the fellowship is permanently endowed as part of a $2.5 million challenge grant from the National Endowment for the Humanities’ “We the People” initiative. The Henry Fellowship includes a $45,000 stipend and residency in the restored circa-1735 Patrick Fellows’ Residence. Co-sponsored by the Rose O’Neill Literary House, Washington College’s center for literature and the literary arts, the fellowship aims to encourage reflection on the links between American history and contemporary culture, and to foster the literary art of historical writing. The annual application deadline is February 15. Inaugural Patrick Henry Writing Fellow, historian and author Henry Wiencek, spent his residency working on an upcoming book about Thomas Jefferson and his slaves and teaching a class at Washington College. The 2009–10 fellowship recipient, Marla Miller, recently published a biography of flag-maker Betsy Ross, \"Betsy Ross and the Making of America\", completed during her residence.\n\nThe Patrick Henry Fellows’ Residence, known traditionally as the Buck-Chambers House, has strong connections to three centuries of American history. Its past owners include a British merchant active in the convict trade to Chesapeake Bay, a Revolutionary War officer who served with George Washington in the famous New York campaign of 1776, and a U.S. senator who led a Kent County militia company into battle during the War of 1812. Purchased by Washington College in 2007, the house underwent a year-long restoration before welcoming the first Patrick Henry Visiting Fellow in September 2008.\n\nThe Hodson Trust – John Carter Brown Fellowship supports work by academics, independent scholars and writers working on significant projects relating to the literature, history, culture or art of the Americas before 1830. The fellowship is also open to filmmakers, novelists, creative and performing artists, and others working on projects that draw on this period of history. The award supports two months of research at the John Carter Brown Library in Providence, RI, and two months of writing at the Starr Center, and comes with a stipend of $20,000, plus housing and university privileges at both institutions. The annual application deadline is March 15.\n\n"}
{"id": "39866732", "url": "https://en.wikipedia.org/wiki?curid=39866732", "title": "Carl A. Trocki", "text": "Carl A. Trocki\n\nCarl A. Trocki is an American historian, an expert in Southeast Asia and China. He was professor of Asian Studies at the Queensland University of Technology, director of the Centre for Community and Cross-Cultural Studies of the QUT, Fellow of the Australian Academy of the Humanities. Trocki previously was Jacobson Visiting Associate Professor of Southeast Asian History at Georgetown University and also taught at Thomas More College. He served in the US Peace Corps in Malaysia, and received the BA from Cleveland State University. He is a native of Erie Pennsylvania.\n\nHe holds Ph.D. in Southeast Asian history from the Cornell University.\n\nHe has publications on Thailand, Singapore, Malaysia, Chinese diaspora, and drug trade in Asia.\n\n"}
{"id": "8663808", "url": "https://en.wikipedia.org/wiki?curid=8663808", "title": "Caroline Dormon", "text": "Caroline Dormon\n\nCaroline Coroneos Dormon, also known as Carrie Dormon (July 19, 1888 – November 21, 1971), was an American botanist, horticulturist, ornithologist, historian, archeologist, preservationist, naturalist, conservationist, and author from Louisiana.\n\nShe was born in modest circumstances at Briarwood, the family home in northern Natchitoches Parish, to James L. Dormon and the former Caroline Trotti. She was reared a Southern Baptist in Arcadia, the parish seat of government of Bienville Parish, in northern Louisiana. She never married.\n\nAs a child, Dormon developed a great interest in plants and wildlife. She was educated at the Baptist-affiliated Judson College, Marion, Alabama, from which she received a bachelor's degree in literature and art. She taught for several years in Louisiana schools, and then re-established her home at Briarwood in 1918. She began to collect and preserve native trees and shrubs.\n\nIn 1921, she became a public relations representative for the Louisiana Forestry Department. She attended a Southern Forestry Congress in 1922 and persuaded the United States Forest Service to establish a national forest in Louisiana. U.S. Representative James B. Aswell of Natchitoches worked with Dormon to bring to fruition the Kisatchie National Forest, which was designated in 1930 during President Herbert Hoover's administration.\n\nIn 1941, during the administration of Governor Sam Houston Jones, Dormon joined the Louisiana Highway Department (later the Louisiana Department of Transportation and Development) as beautification consultant. She was later a landscape consultant for the Huey P. Long Charity Hospital in Pineville, Louisiana east of the Red River. She also served as consultant for Hodges Gardens.\n\nDormon also proposed what became the Louisiana State Arboretum, located some eight miles (13 km) north of Ville Platte, the seat of Evangeline Parish, as part of nearby Chicot State Park. The site was dedicated in 1964. The Caroline Dormon Lodge opened in 1965, serving as a visitor center, library, and herbarium of native plants which grow within the boundaries of the arboretum.\n\nHer published works include: \"Wild Flowers of Louisiana\" (1934), \"Forest Trees of Louisiana\" (1941), \"Flowers Native to the Deep South\" (1958), \"Natives Preferred\" (1965), \"Southern Indian Boy\" (1967), and \"Bird Talk\" (1969).\n\nDormon was the only woman member of the De Soto Commission established by Congress in 1935 to commemorate the 400th anniversary of Hernando de Soto's expedition across the American Southeast, which crossed northern Louisiana.\n\nIn 1965, she was presented with an honorary Doctor of Science award from Louisiana State University in Baton Rouge. The Dormon Collection is located at the Eugene P. Watson Memorial Library of Northwestern State University in Natchitoches.\n\nIn August 2012, the Rapides Parish School Board opened Caroline Dormon Junior High School Woodworth. The school is located on a site, off U.S. Highway 165. Land for the school was donated by the United States Forest Service from the Kisatchie National Forest.\n\nThe K-8th grade school cost $6.5 million and is a “green” school with energy saving tweaks such as solar panels, building and roofing colors, and efficiency of the mechanical and electrical systems. The school is expected to open with approximately three hundred pupils.\n\nDormon died in Shreveport. She is interred at the Briarwood Baptist Church Cemetery near her home. Dormon willed her home, Briarwood, to the public. Located near Saline in southern Bienville Parish, it is the headquarters of the Caroline Dormon Nature Preserve.\n\nNatchitoches attorney and philanthropist Arthur C. Watson organized the Foundation for the Preservation of the Caroline Dormon Nature Preserve and served as its treasurer until his death in 1984. The Caroline Dormon Trail extending in the Kisatchie Bayou Recreation Complex within the national forest. The trail starts at the Longleaf Scenic Byway.\n\nIn 1972, an art show named in Dormon's honor was formed in Shreveport by Emmett Elmo Rhodes. For twenty-six years, this annual art show and festival ran to promote nature art in \"Miss Carrie's\" name which eventually received entries from all over the state. The show had two divisions, Children's Division and Adult Division. It was held at different venues located in Shreveport and was first held at the R.S. Barnwell Art and Garden Center, then at Mall St. Vincent, then at the Louisiana State Exhibit Museum.\n\n"}
{"id": "18778180", "url": "https://en.wikipedia.org/wiki?curid=18778180", "title": "Celtic settlement of Eastern Europe", "text": "Celtic settlement of Eastern Europe\n\nGallic groups began an eastern movement into the Balkan peninsula from the 4th century BC. Although Celtic settlements were concentrated in the western half of the Carpathian Basin, there were notable incursions, and settlements, within the Balkan Peninsula itself. \n\nFrom their new bases in northern Illyria and Pannonia, the Gallic invasions climaxed in the early 3rd century BC, with the invasion of Greece. The 279 BC invasion of Greece proper was preceded by a series of other military campaigns waged in the southern Balkans and against the kingdom of Macedonia, favoured by the state of confusion ensuing from the disputed succession after Alexander the Great's death. A part of the invasion crossed over to Anatolia and eventually settled in the area that came to be named after them, Galatia.\n\nFrom the 4th century BC, Celtic groups pushed into the Carpathian region and the Danube basin, coinciding with their movement into Italy. The \"Boii\" and \"Volcae\" were two large Celtic confederacies who generally cooperated in their campaigns. Splinter groups moved south via two major routes: one following the Danube river, another eastward from Italy. According to legend, 300,000 Celts moved into Italy and Illyria. By the 3rd century, the native inhabitants of Pannonia were almost completely Celticized. La Tène remains are found widely in Pannonia, but finds westward beyond the Tisza river and south beyond the Sava are rather sparse. These finds are deemed to have been locally produced \"Norican-Pannonian\" variation of Celtic culture. Nevertheless, features are encountered that suggest ongoing contacts with distant provinces such as Iberia. The fertile lands around the Pannonian rivers enabled the Celts to establish themselves easily, developing their agriculture and pottery, and at the same time exploiting the rich mines of modern Poland. Thus, it appears that the Celts had created a new homeland for themselves in the southern part of Central Europe; in a region stretching from Poland to the river Danube.\n\nThe political situation in the northern Balkans was in constant flux with various tribes dominant over their neighbours at any one time. Within tribes, military expeditions were conducted by \"an enterprising and mobile warrior class able from time to time to conquer large areas and to exploit their population\". The political situation in the Balkans during the 4th century BC played to the Celts' advantage. The Illyrians had been waging war against the Greeks, leaving their western flank weak. While Alexander ruled Greece, the Celts dared not to push south near Greece. Therefore, early Celtic expeditions were concentrated against Illyrian tribes.\n\nThe first Balkan tribe to be defeated by the Celts was the Illyric Autariatae, who, during the 4th century BC, had enjoyed a hegemony over much of the central Balkans, centred on the Morava valley. An account of Celtic tactics is revealed in their attacks on the Ardiaei.\n\nIn 335 BC, the Celts sent representatives to pay homage to Alexander the Great, while Macedon was engaged in wars against Thracians on its northern border. Some historians suggest that this 'diplomatic' act was actually an evaluation of Macedonian military might. After the death of Alexander the Great, Celtic armies began to bear down on the southern regions, threatening the Greek kingdom of Macedonia and the rest of Greece. In 310 BC, the Celtic general Molistomos attacked deep into Illyrian territory, subduing the Dardanians, Paeonians and Triballi. The new Macedonian king Cassander felt compelled to take his old Illyrian enemies under his protection. In 298 BC, the Celts attempted a penetrating attack into Thrace and Macedon, where they suffered a heavy defeat near Haemus Mons at the hands of Cassander. However, another body of Celts led by the general Cambaules marched on Thrace, capturing large areas. The Celtic tribe of the Serdi lived in Thrace and founded the city of Serdica, present day Sofia.\n\nThe Celtic military pressure toward Greece in the southern Balkans reached its turning point in 281 BC. The collapse of Lysimachus' successor kingdom in Thrace opened the way for the migration. The cause for this is explained by Pausanias as greed for loot, by Justin as a result of overpopulation, and by Memnon as the result of famine. According to Pausanias, an initial probing raid led by Cambaules withdrew when they realized they were too few in numbers. In 280 BC, a great army comprising about 85,000 warriors left Pannonia, split into three divisions, and marched south in a \"great expedition\" to Macedon and central Greece. Under the leadership of Cerethrius, 20,000 men moved against the Thracians and Triballi. Another division, led by Brennus and Acichorius moved against the Paionians, while the third division, headed by Bolgios, aimed for the Macedonians and Illyrians.\nBolgios inflicted heavy losses on the Macedonians, whose young king, Ptolemy Keraunos, was captured and decapitated. However, Bolgios' contingent was repulsed by the Macedonian nobleman Sosthenes, and satisfied with the loot they had won, Bolgios' contingents turned back. Sosthenes, in turn, was attacked and defeated by Brennus and his division, who were then free to ravage the country.\n\nAfter these expeditions returned home, Brennus urged and persuaded them to mount a third united expedition against central Greece, led by himself and Acichorius. The reported strength of the army of 152,000 infantry and 24,400 cavalry is impossibly large. The actual number of horsemen has to be intended half as big: Pausanias describes how they used a tactic called \"trimarcisia\", where each cavalryman was supported by two mounted servants, who could supply him with a spare horse should he have to be dismounted, or take his place in the battle, should he be killed or wounded.\n\nA Greek coalition made up of Aetolians, Boeotians, Athenians, Phocians, and other Greeks north of Corinth took up quarters at the narrow pass of Thermopylae, on the east coast of central Greece. During the initial assault, Brennus' forces suffered heavy losses. Hence he decided to send a large force under Acichorius against Aetolia. The Aetolian detachment, as Brennus hoped, left Thermopylae to defend their homes. The Aetolians joined the defence \"en masse\" – the old and women joining the fight. Realizing that the Gallic sword was dangerous only at close quarters, the Aetolians resorted to skirmishing tactics. According to Pausanias, only half the number that had set out for Aetolia returned.\n\nEventually, Brennus found a way around the pass at Thermopylae, but by then the Greeks had escaped by sea.\n\nBrennus pushed on to Delphi, where he was defeated and forced to retreat, after which he died of wounds sustained in the battle. His army fell back to the river Spercheios, where it was routed by the Thessalians and Malians.\n\nBoth historians who relate the attack on Delphi, Pausanias and Junianus Justinus, say that the Gauls were defeated and driven off. They were overtaken by a violent thunderstorm, which made it impossible to manoeuvre or even hear their orders. The night that followed was frosty, and in the morning the Greeks attacked them from both sides. Brennus was wounded and the Gauls fell back, killing those of their own wounded who were unable to retreat. That night, a panic fell on the camp, as the Gauls divided into factions and fought amongst themselves. They were joined by Acichorius and the rest of the army, but the Greeks forced them into a full-scale retreat. Brennus took his own life by drinking neat wine according to Pausanias, or by stabbing himself according to Justinus. Pressed by the Aetolians, the Gauls fell back to the Spercheios, where the waiting Thessalians and Malians destroyed them.\n\nIn spite of the Greek accounts about the defeat of the Gauls, the Roman literary tradition preferred a far different version. Strabo reports a story told in his time of a semi-legendary treasure – the \"aurum Tolosanum\", fifteen thousand talents of gold and silver – supposed to have been the cursed gold looted during the sack of Delphi and brought back to Tolosa (modern Toulouse, France) by the Tectosages, who were said to have been part of the invading army.\n\nMore than a century and a half after the alleged sack, the Romans ruled Gallia Narbonensis. In 105 BC, while marching to Arausio, the Proconsul of Cisalpine Gaul Quintus Servilius Caepio plundered the sanctuaries of the town of Tolosa, whose inhabitants had joined the Cimbri, finding over 50,000 15 lb. bars of gold and 10,000 15 lb. bars of silver. The riches of Tolosa were shipped back to Rome, but only the silver made it: the gold was stolen by a band of marauders, who were believed to have been hired by Caepio himself and to have killed the legion guarding it. The Gold of Tolosa was never found, and was said to have been passed all the way down to the last heir of the Servilii Caepiones, Marcus Junius Brutus.\n\nIn 105 BC, Caepio refused to co-operate with his superior officer, Gnaeus Mallius Maximus, because he thought of him as a \"novus homo\", deciding by himself to engage in battle against the Cimbri, on the Rhone. There, the Roman army suffered a crushing defeat and complete destruction, in the so-called Battle of Arausio (modern Orange).\n\nUpon his return to Rome, Caepio was tried for \"the loss of his Army\" and embezzlement. He was convicted and given the harshest sentence allowable; he was stripped of his Roman citizenship, forbidden fire and water within 800 miles of Rome, fined 15,000 talents (about 825,000 lb) of gold, and forbidden from seeing or speaking to his friends or family until he had left for exile.\n\nHe spent the rest of his life in exile in Smyrna in Asia Minor. His defeat and ensuing ruin were looked upon as a punishment for his sacrilegious theft.\n\nStrabo distances himself from this account, arguing that the defeated Gauls were in no position to carry off such spoils, and that, in any case, Delphi had already been despoiled of its treasure by the Phocians during the Third Sacred War in the previous century. However, Brennus' legendary pillage of Delphi is presented as fact by some popular modern historians.\n\nMost scholars deem the Greek campaign a disaster for the Celts.\n\nSome of the survivors of the Greek campaign, led by Comontoris (one of Brennus' generals) settled in Thrace. In 277 BC, Antigonus II Gonatas defeated the Gauls at the Battle of Lysimachia and the survivors retreated, founding a short-lived city-state named Tyle. Another group of Gauls, who split off from Brennus' army in 281 BC, were transported over to Asia Minor by Nicomedes I to help him defeat his brother and secure the throne of Bithynia. They eventually settled in the region that came to be named after them, Galatia. They were defeated by Antiochus I, and as a result, they were confined to barren highlands in the centre of Anatolia.\n\nCeltic groups were still the pre-eminent political units in the northern Balkans from the 4th to the 1st century BC. The \"Boii\" controlled most of northern Pannonia during the 2nd century BC, and are also mentioned as having occupied the territory of modern Slovakia. We learn of other tribes of the Boian confederation inhabiting Pannonia. There were the \"Taurisci\" in the upper Sava valley, west of Sisak, as well as the \"Anarti\", \"Osi\" and \"Cotini\" in the Carpathian basin. In the lower Sava valley, the \"Scordisci\" wielded much power over their neighbours for over a century.\n\nThe later half of the 1st century BC brought much change to the power relations of barbarian tribes in Pannonia. The defeat of the Boian confederation by the Geto-Dacian king Burebista significantly curtailed Celtic control of the Carpathian basin, and some of the Celticization was reversed. Yet, more Celtic tribes appear in sources. The Hercuniates and Latobici migrated from the northern regions (Germania). Altogether new tribes are encountered, bearing Latin names (such as the Arabiates), possibly representing new creations carved out of the defeated Boian confederation. To further weaken Celtic hegemony in Pannonia, the Romans moved the Pannonian-Illyrian \"Azali\" to northern Pannonia. The political dominance previously enjoyed by the Celts was overshadowed by newer barbarian confederations, such the Marcomanni and Iazyges. Their ethnic independence was gradually lost as they were absorbed by the surrounding Dacian, Illyrian and Germanic peoples, although Celtic names survive until the 3rd century AD.\n"}
{"id": "51176602", "url": "https://en.wikipedia.org/wiki?curid=51176602", "title": "CenterPOS Malware", "text": "CenterPOS Malware\n\nCenterPOS (also known as \"Cerebrus\") is a point of sale (POS) malware discovered Cyber Security Experts. It was discovered in September 2015 along with other kinds of POS malware, such as NewPOSThings, BlackPOS, and Alina. There are two versions which have been released by the developer responsible: version 1.7 and version 2.0. CenterPOS 2.0 has similar functionality to CenterPOS version 1.7. The 2.0 variant of CenterPOS malware added some more effective features, such as the addition of a configuration file for storing information in its command and control server.\n\nCenterPOS has been used to target retailers in order to illegally obtain payment card information using a memory scraper. It uses two distinct modes to scrape and store information: a \"smart scan\" and a \"normal scan\". At the normal scan mode, the malware looks at all of the processes on a device and determines which ones are not currently running processes, are not named \"system\", \"system idle process\" or \"idle\", and do not contain keywords such as Microsoft or Mozilla. If the process meets the criteria list, the malware will search all memory regions within the process, searching for credit card data with regular expressions in the regular expression list. In smart scan mode, the malware starts by performing a normal scan, and any process that has a regular expression match will be added to the smart scan list. After the first pass, the malware will only search the processes that are in the smart scan list. The malware contains functionality that allows cybercriminals to create a configuration file.\n\nCenterPOS malware searches for the configuration file that contains the C&C information. If unable to find the configuration file, it asks for a password. If the password entered is correct, then it payloads the functions to create a configuration file. This malware is very different from other point of sale system malware in that it has a separate component called builder to create a payload.\n\nThe CenterPOS malware looks for the credit and debit card information through smart scan mode and then encrypts all the scraped data using Triple DES encryption. Then the memory scraped data is sent to the operator of the malware through a separate HTTP POST request.\n\n"}
{"id": "28715145", "url": "https://en.wikipedia.org/wiki?curid=28715145", "title": "Centum gravamina teutonicae nationis", "text": "Centum gravamina teutonicae nationis\n\nThe Centum gravamina teutonicae nationis, or Gravamina for short, was a list of \"one hundred grievances [see \"gravamen\"] of the German nation\" directed at the Catholic Church in Germany, brought forward by the German princes, \"Fürsten\", assembled at the Diet of Nuremberg in 1522–23. They were in fact the second book of grievances (\"Secundum Gravaminum Libellus\"), the first being the Gravamina Nationis Germanicae et Sacri Romani Imperii Decem (ten grievances of the German nation and the Holy Roman Empire) that had been circulating in manuscript in the years leading up to the Protestant Reformation since 1455, when first presented by Dietrich von Erbach, the Archbishop of Mainz. Their first English editor and translator writes of them:\n"}
{"id": "1779389", "url": "https://en.wikipedia.org/wiki?curid=1779389", "title": "Cosmas of Prague", "text": "Cosmas of Prague\n\nCosmas of Prague (; ; – October 21, 1125) was a priest, writer and historian born in a noble family in Bohemia. Between 1075 and 1081, he studied in Liège. After his return to Bohemia, he became a priest and married Božetěcha, with whom he probably had a son. In 1086, Cosmas was appointed prebendary (\"canonicus\") of Prague, a prestigious position. As prebendary he also travelled through Europe on official matters.\n\nHis \"magnum opus,\" written in Latin, is called Chronica Boemorum (\"Chronicle of Bohemians\" or \"The chronicle of the Czechs\"). The \"Chronica\" is divided into three books:\n\nThe continuation of Cosmas's chroncicle was followed by Cosmas's Followers, a group of chroniclers who wrote about the proceeding years.\n\n"}
{"id": "10896972", "url": "https://en.wikipedia.org/wiki?curid=10896972", "title": "Demetrios Constantelos", "text": "Demetrios Constantelos\n\nDemetrios J. Constantelos was a researcher in Byzantology and a professor emeritus of history and religious studies at Stockton University, Pomona, NJ. He was born in Spilia, Messenia, Greece. He was ordained a priest of the Greek Orthodox Archdiocese of America in 1955 and earned a PhD in Byzantine Civilization at Rutgers University in 1965.\n\nPublications by Demetrios Constantelos include:\n\n"}
{"id": "3078177", "url": "https://en.wikipedia.org/wiki?curid=3078177", "title": "Edward Kenealy", "text": "Edward Kenealy\n\nEdward Vaughan Hyde Kenealy QC (2 July 1819 – 16 April 1880) was an Irish barrister and writer. He is best remembered as counsel for the Tichborne claimant and the eccentric and disturbed conduct of the trial that led to his ruin.\n\nHe was born at Cork, the son of a local merchant. He was educated at Trinity College Dublin, and was called to the Irish Bar in 1840 and to the English Bar in 1847. He obtained a fair practice in criminal cases. In 1868 he became a QC and a bencher of Gray's Inn.\n\nHe practised on the Oxford circuit and in the Central Criminal Court and his most famous cases included:\n\nKenealy suffered from diabetes and an erratic temperament has sometimes been attributed to poor control of the symptoms. In 1850 he was sentenced to one month imprisonment for punishing his six-year-old illegitimate son with undue severity. He married Elizabeth Nicklin of Tipton, Staffordshire in 1851 and they had eleven children, including novelist Arabella Kenealy (1864–1938). The Kenealy family lived in Portslade, East Sussex, from the 1852 until 1874. Edward Kenealy commuted to London and Oxford for his law practice but returned at weekends and other times to be with his family.\n\nIn 1850, he published an eccentric poem inspired by Johann Wolfgang von Goethe, \"Goethe, a New Pantomime\". He also published a large amount of poetry in journals such as \"Fraser's Magazine\". He published translations from Latin, Greek, German, Italian, Portuguese, Russian, Irish, Persian, Arabic, Hindustani and Bengali. It is unlikely that he was fluent in all these languages.\n\nIn 1866, Kenealy wrote \"The Book of God: the Apocalypse of Adam-Oannes\", an unorthodox theological work in which he claimed that he was the \"twelfth messenger of God\", descended from Jesus Christ and Genghis Khan.\n\nHe also published a more conventional biography of Edward Wortley Montagu in 1869.\n\nDuring the trial, Kenealy abused witnesses, made scurrilous allegations against various Roman Catholic institutions, treated the judges with disrespect, and protracted the trial until it became the longest in English legal history. His violent conduct of the case became a public scandal and, after rejecting his client's claim, the jury censured his behaviour.\n\nHe started a newspaper, \"The Englishman\", to plead his cause, and to attack the judges. His behaviour was so extreme that in 1874 he was disbenched and disbarred by his Inn. He formed the Magna Charta Association and went on a nationwide tour to protest his cause.\n\nAt a by-election in 1875, he was elected to Parliament for Stoke-upon-Trent with a majority of 2000 votes. However, no other Member of Parliament would introduce him when he took his seat. Benjamin Disraeli forced a motion to dispense with this convention.\n\nIn Parliament, Kenealy called for a Royal Commission into his conduct in the Tichborne case, but lost a vote on this by 433–3. One vote was Kenealy's, another that of his teller, George Hammond Whalley. The third \"aye\" was by Purcell O'Gorman of Waterford City. During this period, he also wrote a nine-volume account of the case.\n\nDr Kenealy, as he was always called, gradually ceased to attract attention, lost his seat at the 1880 general election and died in London later in the year aged 60. He is buried in the churchyard of St Helen's Church, Hangleton, East Sussex.\n\n"}
{"id": "51165786", "url": "https://en.wikipedia.org/wiki?curid=51165786", "title": "Ellen DuBois", "text": "Ellen DuBois\n\nEllen Carol DuBois is a professor of history and gender studies at the University of California, Los Angeles. In 1998 she won the Joan Kelly Memorial Prize of the American Historical Association for her book \"Harriot Stanton Blatch and the Winning of Woman Suffrage\" (Yale University Press, 1997).\n\nShe earned a B.A. from Wellesley College in 1968 and a Ph.D. from Northwestern University in 1975. After teaching at the University at Buffalo for 16 years, she moved to Los Angeles to continue teaching at UCLA.\n\n"}
{"id": "25108767", "url": "https://en.wikipedia.org/wiki?curid=25108767", "title": "Gazarta (Chaldean Diocese)", "text": "Gazarta (Chaldean Diocese)\n\nThe Chaldean diocese of Gazarta, named for the town of Jezira (modern Cizre), known in Syriac as Gazarta d'Beth Zabdai, was established on a stable basis in the early nineteenth century. Many of the Assyrians of the Gazarta region, including the bishop of Gazarta Pilipus Yaqub Orahim, were killed during the Assyrian Genocide during 1914-1918, and the diocese lapsed after the First World War.\n\nThe first nineteenth-century bishop of Gazarta was Giwargis Peter di Natale, who was consecrated for Gazarta in 1833 by the patriarch Yohannan VIII Hormizd and transferred to the diocese of Amadiya in 1842.\nThe patriarch Nicholas I Zaya appointed Basil Asmar, previously administrator of the diocese of Amid, to the diocese of Gazarta in 1842. Basil was metropolitan of Gazarta for nine years, and Badger noted in 1850 that 'he seldom resides at Jezeerah'. He retired due to old age and ill health in 1851. \n\nThe patriarch Joseph VI Audo sent the priest Abdisho Dosho (later metropolitan of Amadiya) to Gazarta in his place for a year, and appointed Jerome Paul Hindi metropolitan of Gazarta in 1852. Hindi was metropolitan of Gazarta until 1873, and was succeeded by Eliya Peter Abulyonan (1874–8), who became patriarch in 1879 after Audo's death.\n\nIn 1875, the monk Philip Abraham of the monastery of Notre Dame des Semences, who was born in Telkepe in 1848 and became a priest in 1873, was consecrated metropolitan for India by Joseph VI Audo, who was then challenging the authority of the \"Padroado\" jurisdiction in India. He was recalled from India in 1881 by Eliya XII Abulyonan on instructions from the Vatican, and was transferred to the diocese of Gazarta in 1882, taking the name Mar Yaqob. He was among the many Chaldean clergy murdered in the Armenian massacres of 1915. He was the last metropolitan of Gazarta. The Assyrian population of the Gazarta district was greatly reduced in the First World War in the Assyrian Genocide, and the diocese of Gazarta was not revived afterwards.\n\nIn 1867 the diocese of Gazarta contained 20 villages and had a population of 7,000 Assyrians, served by 10 \"qashe\" (priests). In 1896 the diocese had a population of 5,500 Assyrians, and contained 16 parishes or stations, 20 churches, and 14 \"qashe\" (priests), assisted by a Dominican mission and a mission of the Sisters of Preservation at Gazarta (Chabot). The largest villages in the diocese were Gazarta, Hoz and Mer, Harbol, Peshabur, Taqian, Girik Bedro, Wasta and Tel Qabin.\n\nIn 1913 the diocese included 17 villages, and had 17 \"qashe\" (priests) and 6,400 civilians (Tfinkdji).\n\nThe diocese was ruined in the First World War during the Assyrian Genocide, and in 1928 there were only 1,600 Assyrians remaining in its former territory, without a \"qasha\" (priest) (Tisserant). In 1937 there were 2,250 Assyrians, with a \"qasha\" (priest) and a church (Kajo).\n\n"}
{"id": "2068726", "url": "https://en.wikipedia.org/wiki?curid=2068726", "title": "History of Earth", "text": "History of Earth\n\nThe history of Earth concerns the development of planet Earth from its formation to the present day. Nearly all branches of natural science have contributed to understanding of the main events of Earth's past, characterized by constant geological change and biological evolution.\n\nThe geological time scale (GTS), as defined by international convention, depicts the large spans of time from the beginning of the Earth to the present, and its divisions chronicle some definitive events of Earth history. (In the graphic: Ga means \"billion years ago\"; Ma, \"million years ago\".) Earth formed around 4.54 billion years ago, approximately one-third the age of the universe, by accretion from the solar nebula. Volcanic outgassing probably created the primordial atmosphere and then the ocean, but the early atmosphere contained almost no oxygen. Much of the Earth was molten because of frequent collisions with other bodies which led to extreme volcanism. While Earth was in its earliest stage (Early Earth), a giant impact collision with a planet-sized body named Theia is thought to have formed the Moon. Over time, the Earth cooled, causing the formation of a solid crust, and allowing liquid water on the surface.\n\nThe Hadean eon represents the time before a reliable (fossil) record of life; it began with the formation of the planet and ended 4.0 billion years ago. The following Archean and Proterozoic eons produced the beginnings of life on Earth and its earliest evolution. The succeeding eon is the Phanerozoic, divided into three eras: the Palaeozoic, an era of arthropods, fishes, and the first life on land; the Mesozoic, which spanned the rise, reign, and climactic extinction of the non-avian dinosaurs; and the Cenozoic, which saw the rise of mammals. Recognizable humans emerged at most 2 million years ago, a vanishingly small period on the geological scale.\n\nThe earliest undisputed evidence of life on Earth dates at least from 3.5 billion years ago, during the Eoarchean Era, after a geological crust started to solidify following the earlier molten Hadean Eon. There are microbial mat fossils such as stromatolites found in 3.48 billion-year-old sandstone discovered in Western Australia. Other early physical evidence of a biogenic substance is graphite in 3.7 billion-year-old metasedimentary rocks discovered in southwestern Greenland as well as \"remains of biotic life\" found in 4.1 billion-year-old rocks in Western Australia. According to one of the researchers, \"If life arose relatively quickly on Earth … then it could be common in the universe.\"\n\nPhotosynthetic organisms appeared between 3.2 and 2.4 billion years ago and began enriching the atmosphere with oxygen. Life remained mostly small and microscopic until about 580 million years ago, when complex multicellular life arose, developed over time, and culminated in the Cambrian Explosion about 541 million years ago. This sudden diversification of life forms produced most of the major phyla known today, and divided the Proterozoic Eon from the Cambrian Period of the Paleozoic Era. It is estimated that 99 percent of all species that ever lived on Earth, over five billion, have gone extinct. Estimates on the number of Earth's current species range from 10 million to 14 million, of which about 1.2 million are documented, but over 86 percent have not been described. However, it was recently claimed that 1 trillion species currently live on Earth, with only one-thousandth of one percent described.\n\nThe Earth's crust has constantly changed since its formation, as has life has since its first appearance. Species continue to evolve, taking on new forms, splitting into daughter species, or going extinct in the face of ever-changing physical environments. The process of plate tectonics continues to shape the Earth's continents and oceans and the life they harbor. Human activity is now a dominant force affecting global change, harming the biosphere, the Earth's surface, hydrosphere, and atmosphere with the loss of wild lands, over-exploitation of the oceans, production of greenhouse gases, degradation of the ozone layer, and general degradation of soil, air, and water quality.\nIn geochronology, time is generally measured in mya (megayears or million years), each unit representing the period of approximately 1,000,000 years in the past. The history of Earth is divided into four great eons, starting 4,540 mya with the formation of the planet. Each eon saw the most significant changes in Earth's composition, climate and life. Each eon is subsequently divided into eras, which in turn are divided into periods, which are further divided into epochs.\n\nThe history of the Earth can be organized chronologically according to the geologic time scale, which is split into intervals based on stratigraphic analysis.\n\nThe standard model for the formation of the Solar System (including the Earth) is the solar nebula hypothesis. In this model, the Solar System formed from a large, rotating cloud of interstellar dust and gas called the solar nebula. It was composed of hydrogen and helium created shortly after the Big Bang 13.8 Ga (billion years ago) and heavier elements ejected by supernovae. About 4.5 Ga, the nebula began a contraction that may have been triggered by the shock wave from a nearby supernova. A shock wave would have also made the nebula rotate. As the cloud began to accelerate, its angular momentum, gravity, and inertia flattened it into a protoplanetary disk perpendicular to its axis of rotation. Small perturbations due to collisions and the angular momentum of other large debris created the means by which kilometer-sized protoplanets began to form, orbiting the nebular center.\n\nThe center of the nebula, not having much angular momentum, collapsed rapidly, the compression heating it until nuclear fusion of hydrogen into helium began. After more contraction, a T Tauri star ignited and evolved into the Sun. Meanwhile, in the outer part of the nebula gravity caused matter to condense around density perturbations and dust particles, and the rest of the protoplanetary disk began separating into rings. In a process known as runaway accretion, successively larger fragments of dust and debris clumped together to form planets. Earth formed in this manner about 4.54 billion years ago (with an uncertainty of 1%) and was largely completed within 10–20 million years. The solar wind of the newly formed T Tauri star cleared out most of the material in the disk that had not already condensed into larger bodies. The same process is expected to produce accretion disks around virtually all newly forming stars in the universe, some of which yield planets.\n\nThe proto-Earth grew by accretion until its interior was hot enough to melt the heavy, siderophile metals. Having higher densities than the silicates, these metals sank. This so-called \"iron catastrophe\" resulted in the separation of a primitive mantle and a (metallic) core only 10 million years after the Earth began to form, producing the layered structure of Earth and setting up the formation of Earth's magnetic field. J. A. Jacobs was the first to suggest that the inner core—a solid center distinct from the liquid outer core—is freezing and growing out of the liquid outer core due to the gradual cooling of Earth's interior (about 100 degrees Celsius per billion years).\n\nThe first eon in Earth's history, the \"Hadean\", begins with the Earth's formation and is followed by the \"Archean\" eon at 3.8 Ga. The oldest rocks found on Earth date to about 4.0 Ga, and the oldest detrital zircon crystals in rocks to about 4.4 Ga, soon after the formation of the Earth's crust and the Earth itself. The giant impact hypothesis for the Moon's formation states that shortly after formation of an initial crust, the proto-Earth was impacted by a smaller protoplanet, which ejected part of the mantle and crust into space and created the Moon.\n\nFrom crater counts on other celestial bodies, it is inferred that a period of intense meteorite impacts, called the \"Late Heavy Bombardment\", began about 4.1 Ga, and concluded around 3.8 Ga, at the end of the Hadean. In addition, volcanism was severe due to the large heat flow and geothermal gradient. Nevertheless, detrital zircon crystals dated to 4.4 Ga show evidence of having undergone contact with liquid water, suggesting that the Earth already had oceans or seas at that time.\n\nBy the beginning of the Archean, the Earth had cooled significantly. Present life forms could not have survived at Earth's surface, because the Archean atmosphere lacked oxygen hence had no ozone layer to block ultraviolet light. Nevertheless, it is believed that primordial life began to evolve by the early Archean, with candidate fossils dated to around 3.5 Ga. Some scientists even speculate that life could have begun during the early Hadean, as far back as 4.4 Ga, surviving the possible Late Heavy Bombardment period in hydrothermal vents below the Earth's surface.\n\nEarth's only natural satellite, the Moon, is larger relative to its planet than any other satellite in the solar system. During the Apollo program, rocks from the Moon's surface were brought to Earth. Radiometric dating of these rocks shows that the Moon is 4.53 ± 0.01 billion years old, formed at least 30 million years after the solar system. New evidence suggests the Moon formed even later, 4.48 ± 0.02 Ga, or 70–110 million years after the start of the Solar System.\n\nTheories for the formation of the Moon must explain its late formation as well as the following facts. First, the Moon has a low density (3.3 times that of water, compared to 5.5 for the earth) and a small metallic core. Second, there is virtually no water or other volatiles on the moon. Third, the Earth and Moon have the same oxygen isotopic signature (relative abundance of the oxygen isotopes). Of the theories proposed to account for these phenomena, one is widely accepted: The \"giant impact hypothesis\" proposes that the Moon originated after a body the size of Mars (sometimes named Theia) struck the proto-Earth a glancing blow.\n\nThe collision released about 100 million times more energy than the more recent Chicxulub impact that is believed to have caused the extinction of the dinosaurs. It was enough to vaporize some of the Earth's outer layers and melt both bodies. A portion of the mantle material was ejected into orbit around the Earth. The giant impact hypothesis predicts that the Moon was depleted of metallic material, explaining its abnormal composition. The ejecta in orbit around the Earth could have condensed into a single body within a couple of weeks. Under the influence of its own gravity, the ejected material became a more spherical body: the Moon.\n\nMantle convection, the process that drives plate tectonics, is a result of heat flow from the Earth's interior to the Earth's surface. It involves the creation of rigid tectonic plates at mid-oceanic ridges. These plates are destroyed by subduction into the mantle at subduction zones. During the early Archean (about 3.0 Ga) the mantle was much hotter than today, probably around , so convection in the mantle was faster. Although a process similar to present-day plate tectonics did occur, this would have gone faster too. It is likely that during the Hadean and Archean, subduction zones were more common, and therefore tectonic plates were smaller.\n\nThe initial crust, formed when the Earth's surface first solidified, totally disappeared from a combination of this fast Hadean plate tectonics and the intense impacts of the Late Heavy Bombardment. However, it is thought that it was basaltic in composition, like today's oceanic crust, because little crustal differentiation had yet taken place. The first larger pieces of continental crust, which is a product of differentiation of lighter elements during partial melting in the lower crust, appeared at the end of the Hadean, about 4.0 Ga. What is left of these first small continents are called cratons. These pieces of late Hadean and early Archean crust form the cores around which today's continents grew.\n\nThe oldest rocks on Earth are found in the North American craton of Canada. They are tonalites from about 4.0 Ga. They show traces of metamorphism by high temperature, but also sedimentary grains that have been rounded by erosion during transport by water, showing that rivers and seas existed then. Cratons consist primarily of two alternating types of terranes. The first are so-called greenstone belts, consisting of low-grade metamorphosed sedimentary rocks. These \"greenstones\" are similar to the sediments today found in oceanic trenches, above subduction zones. For this reason, greenstones are sometimes seen as evidence for subduction during the Archean. The second type is a complex of felsic magmatic rocks. These rocks are mostly tonalite, trondhjemite or granodiorite, types of rock similar in composition to granite (hence such terranes are called TTG-terranes). TTG-complexes are seen as the relicts of the first continental crust, formed by partial melting in basalt.\n\nEarth is often described as having had three atmospheres. The first atmosphere, captured from the solar nebula, was composed of light (atmophile) elements from the solar nebula, mostly hydrogen and helium. A combination of the solar wind and Earth's heat would have driven off this atmosphere, as a result of which the atmosphere is now depleted of these elements compared to cosmic abundances. After the impact which created the moon, the molten Earth released volatile gases; and later more gases were released by volcanoes, completing a second atmosphere rich in greenhouse gases but poor in oxygen. Finally, the third atmosphere, rich in oxygen, emerged when bacteria began to produce oxygen about 2.8 Ga.\n\nIn early models for the formation of the atmosphere and ocean, the second atmosphere was formed by outgassing of volatiles from the Earth's interior. Now it is considered likely that many of the volatiles were delivered during accretion by a process known as \"impact degassing\" in which incoming bodies vaporize on impact. The ocean and atmosphere would, therefore, have started to form even as the Earth formed. The new atmosphere probably contained water vapor, carbon dioxide, nitrogen, and smaller amounts of other gases.\n\nPlanetesimals at a distance of 1 astronomical unit (AU), the distance of the Earth from the Sun, probably did not contribute any water to the Earth because the solar nebula was too hot for ice to form and the hydration of rocks by water vapor would have taken too long. The water must have been supplied by meteorites from the outer asteroid belt and some large planetary embryos from beyond 2.5 AU. Comets may also have contributed. Though most comets are today in orbits farther away from the Sun than Neptune, computer simulations show that they were originally far more common in the inner parts of the solar system.\n\nAs the Earth cooled, clouds formed. Rain created the oceans. Recent evidence suggests the oceans may have begun forming as early as 4.4 Ga. By the start of the Archean eon, they already covered much of the Earth. This early formation has been difficult to explain because of a problem known as the faint young Sun paradox. Stars are known to get brighter as they age, and at the time of its formation the Sun would have been emitting only 70% of its current power. Thus, the Sun has become 30% brighter in the last 4.5 billion years. Many models indicate that the Earth would have been covered in ice. A likely solution is that there was enough carbon dioxide and methane to produce a greenhouse effect. The carbon dioxide would have been produced by volcanoes and the methane by early microbes. Another greenhouse gas, ammonia, would have been ejected by volcanos but quickly destroyed by ultraviolet radiation.\n\nOne of the reasons for interest in the early atmosphere and ocean is that they form the conditions under which life first arose. There are many models, but little consensus, on how life emerged from non-living chemicals; chemical systems created in the laboratory fall well short of the minimum complexity for a living organism.\n\nThe first step in the emergence of life may have been chemical reactions that produced many of the simpler organic compounds, including nucleobases and amino acids, that are the building blocks of life. An experiment in 1953 by Stanley Miller and Harold Urey showed that such molecules could form in an atmosphere of water, methane, ammonia and hydrogen with the aid of sparks to mimic the effect of lightning. Although atmospheric composition was probably different from that used by Miller and Urey, later experiments with more realistic compositions also managed to synthesize organic molecules. Computer simulations show that extraterrestrial organic molecules could have formed in the protoplanetary disk before the formation of the Earth.\n\nAdditional complexity could have been reached from at least three possible starting points: self-replication, an organism's ability to produce offspring that are similar to itself; metabolism, its ability to feed and repair itself; and external cell membranes, which allow food to enter and waste products to leave, but exclude unwanted substances.\n\nEven the simplest members of the three modern domains of life use DNA to record their \"recipes\" and a complex array of RNA and protein molecules to \"read\" these instructions and use them for growth, maintenance, and self-replication.\n\nThe discovery that a kind of RNA molecule called a ribozyme can catalyze both its own replication and the construction of proteins led to the hypothesis that earlier life-forms were based entirely on RNA. They could have formed an RNA world in which there were individuals but no species, as mutations and horizontal gene transfers would have meant that the offspring in each generation were quite likely to have different genomes from those that their parents started with. RNA would later have been replaced by DNA, which is more stable and therefore can build longer genomes, expanding the range of capabilities a single organism can have. Ribozymes remain as the main components of ribosomes, the \"protein factories\" of modern cells.\n\nAlthough short, self-replicating RNA molecules have been artificially produced in laboratories, doubts have been raised about whether natural non-biological synthesis of RNA is possible. The earliest ribozymes may have been formed of simpler nucleic acids such as PNA, TNA or GNA, which would have been replaced later by RNA. Other pre-RNA replicators have been posited, including crystals and even quantum systems.\n\nIn 2003 it was proposed that porous metal sulfide precipitates would assist RNA synthesis at about and at ocean-bottom pressures near hydrothermal vents. In this hypothesis, the proto-cells would be confined in the pores of the metal substrate until the later development of lipid membranes.\n\nAnother long-standing hypothesis is that the first life was composed of protein molecules. Amino acids, the building blocks of proteins, are easily synthesized in plausible prebiotic conditions, as are small peptides (polymers of amino acids) that make good catalysts. A series of experiments starting in 1997 showed that amino acids and peptides could form in the presence of carbon monoxide and hydrogen sulfide with iron sulfide and nickel sulfide as catalysts. Most of the steps in their assembly required temperatures of about and moderate pressures, although one stage required and a pressure equivalent to that found under of rock. Hence, self-sustaining synthesis of proteins could have occurred near hydrothermal vents.\n\nA difficulty with the metabolism-first scenario is finding a way for organisms to evolve. Without the ability to replicate as individuals, aggregates of molecules would have \"compositional genomes\" (counts of molecular species in the aggregate) as the target of natural selection. However, a recent model shows that such a system is unable to evolve in response to natural selection.\n\nIt has been suggested that double-walled \"bubbles\" of lipids like those that form the external membranes of cells may have been an essential first step. Experiments that simulated the conditions of the early Earth have reported the formation of lipids, and these can spontaneously form liposomes, double-walled \"bubbles\", and then reproduce themselves. Although they are not intrinsically information-carriers as nucleic acids are, they would be subject to natural selection for longevity and reproduction. Nucleic acids such as RNA might then have formed more easily within the liposomes than they would have outside.\n\nSome clays, notably montmorillonite, have properties that make them plausible accelerators for the emergence of an RNA world: they grow by self-replication of their crystalline pattern, are subject to an analog of natural selection (as the clay \"species\" that grows fastest in a particular environment rapidly becomes dominant), and can catalyze the formation of RNA molecules. Although this idea has not become the scientific consensus, it still has active supporters.\n\nResearch in 2003 reported that montmorillonite could also accelerate the conversion of fatty acids into \"bubbles\", and that the bubbles could encapsulate RNA attached to the clay. Bubbles can then grow by absorbing additional lipids and dividing. The formation of the earliest cells may have been aided by similar processes.\n\nA similar hypothesis presents self-replicating iron-rich clays as the progenitors of nucleotides, lipids and amino acids.\n\nIt is believed that of this multiplicity of protocells, only one line survived. Current phylogenetic evidence suggests that the last universal ancestor (LUA) lived during the early Archean eon, perhaps 3.5 Ga or earlier. This LUA cell is the ancestor of all life on Earth today. It was probably a prokaryote, possessing a cell membrane and probably ribosomes, but lacking a nucleus or membrane-bound organelles such as mitochondria or chloroplasts. Like modern cells, it used DNA as its genetic code, RNA for information transfer and protein synthesis, and enzymes to catalyze reactions. Some scientists believe that instead of a single organism being the last universal common ancestor, there were populations of organisms exchanging genes by lateral gene transfer.\n\nThe Proterozoic eon lasted from 2.5 Ga to 542 Ma (million years) ago. In this time span, cratons grew into continents with modern sizes. The change to an oxygen-rich atmosphere was a crucial development. Life developed from prokaryotes into eukaryotes and multicellular forms. The Proterozoic saw a couple of severe ice ages called snowball Earths. After the last Snowball Earth about 600 Ma, the evolution of life on Earth accelerated. About 580 Ma, the Ediacaran biota formed the prelude for the Cambrian Explosion.\n\nThe earliest cells absorbed energy and food from the surrounding environment. They used fermentation, the breakdown of more complex compounds into less complex compounds with less energy, and used the energy so liberated to grow and reproduce. Fermentation can only occur in an \"anaerobic\" (oxygen-free) environment. The evolution of photosynthesis made it possible for cells to derive energy from the Sun.\n\nMost of the life that covers the surface of the Earth depends directly or indirectly on photosynthesis. The most common form, oxygenic photosynthesis, turns carbon dioxide, water, and sunlight into food. It captures the energy of sunlight in energy-rich molecules such as ATP, which then provide the energy to make sugars. To supply the electrons in the circuit, hydrogen is stripped from water, leaving oxygen as a waste product. Some organisms, including purple bacteria and green sulfur bacteria, use an anoxygenic form of photosynthesis that uses alternatives to hydrogen stripped from water as electron donors; examples are hydrogen sulfide, sulfur and iron. Such extremophile organisms are restricted to otherwise inhospitable environments such as hot springs and hydrothermal vents.\n\nThe simpler anoxygenic form arose about 3.8 Ga, not long after the appearance of life. The timing of oxygenic photosynthesis is more controversial; it had certainly appeared by about 2.4 Ga, but some researchers put it back as far as 3.2 Ga. The latter \"probably increased global productivity by at least two or three orders of magnitude\". Among the oldest remnants of oxygen-producing lifeforms are fossil stromatolites.\n\nAt first, the released oxygen was bound up with limestone, iron, and other minerals. The oxidized iron appears as red layers in geological strata called banded iron formations that formed in abundance during the Siderian period (between 2500 Ma and 2300 Ma). When most of the exposed readily reacting minerals were oxidized, oxygen finally began to accumulate in the atmosphere. Though each cell only produced a minute amount of oxygen, the combined metabolism of many cells over a vast time transformed Earth's atmosphere to its current state. This was Earth's third atmosphere.\n\nSome oxygen was stimulated by solar ultraviolet radiation to form ozone, which collected in a layer near the upper part of the atmosphere. The ozone layer absorbed, and still absorbs, a significant amount of the ultraviolet radiation that once had passed through the atmosphere. It allowed cells to colonize the surface of the ocean and eventually the land: without the ozone layer, ultraviolet radiation bombarding land and sea would have caused unsustainable levels of mutation in exposed cells.\n\nPhotosynthesis had another major impact. Oxygen was toxic; much life on Earth probably died out as its levels rose in what is known as the \"oxygen catastrophe\". Resistant forms survived and thrived, and some developed the ability to use oxygen to increase their metabolism and obtain more energy from the same food.\n\nThe natural evolution of the Sun made it progressively more luminous during the Archean and Proterozoic eons; the Sun's luminosity increases 6% every billion years. As a result, the Earth began to receive more heat from the Sun in the Proterozoic eon. However, the Earth did not get warmer. Instead, the geological record suggests it cooled dramatically during the early Proterozoic. Glacial deposits found in South Africa date back to 2.2 Ga, at which time, based on paleomagnetic evidence, they must have been located near the equator. Thus, this glaciation, known as the Huronian glaciation, may have been global. Some scientists suggest this was so severe that the Earth was frozen over from the poles to the equator, a hypothesis called Snowball Earth.\n\nThe Huronian ice age might have been caused by the increased oxygen concentration in the atmosphere, which caused the decrease of methane (CH) in the atmosphere. Methane is a strong greenhouse gas, but with oxygen it reacts to form CO, a less effective greenhouse gas. When free oxygen became available in the atmosphere, the concentration of methane could have decreased dramatically, enough to counter the effect of the increasing heat flow from the Sun.\n\nHowever, the term Snowball Earth is more commonly used to describe later extreme ice ages during the Cryogenian period. There were four periods, each lasting about 10 million years, between 750 and 580 million years ago, when the earth is thought to have been covered with ice apart from the highest mountains, and average temperatures were about . The snowball may have been partly due to the location of the supercontintent Rodinia straddling the Equator. Carbon dioxide combines with rain to weather rocks to form carbonic acid, which is then washed out to sea, thus extracting the greenhouse gas from the atmosphere. When the continents are near the poles, the advance of ice covers the rocks, slowing the reduction in carbon dioxide, but in the Cryogienian the weathering of Rodinia was able to continue unchecked until the ice advanced to the tropics. The process may have finally been reversed by the emission of carbon dioxide from volcanoes or the destabilization of methane gas hydrates. According to the alternative Slushball Earth theory, even at the height of the ice ages there was still open water at the Equator.\n\nModern taxonomy classifies life into three domains. The time of their origin is uncertain. The Bacteria domain probably first split off from the other forms of life (sometimes called Neomura), but this supposition is controversial. Soon after this, by 2 Ga, the Neomura split into the Archaea and the Eukarya. Eukaryotic cells (Eukarya) are larger and more complex than prokaryotic cells (Bacteria and Archaea), and the origin of that complexity is only now becoming known. \n\nAround this time, the first proto-mitochondrion was formed. A bacterial cell related to today's \"Rickettsia\", which had evolved to metabolize oxygen, entered a larger prokaryotic cell, which lacked that capability. Perhaps the large cell attempted to digest the smaller one but failed (possibly due to the evolution of prey defenses). The smaller cell may have tried to parasitize the larger one. In any case, the smaller cell survived inside the larger cell. Using oxygen, it metabolized the larger cell's waste products and derived more energy. Part of this excess energy was returned to the host. The smaller cell replicated inside the larger one. Soon, a stable symbiosis developed between the large cell and the smaller cells inside it. Over time, the host cell acquired some genes from the smaller cells, and the two kinds became dependent on each other: the larger cell could not survive without the energy produced by the smaller ones, and these, in turn, could not survive without the raw materials provided by the larger cell. The whole cell is now considered a single organism, and the smaller cells are classified as organelles called mitochondria.\n\nA similar event occurred with photosynthetic cyanobacteria entering large heterotrophic cells and becoming chloroplasts. Probably as a result of these changes, a line of cells capable of photosynthesis split off from the other eukaryotes more than 1 billion years ago. There were probably several such inclusion events. Besides the well-established endosymbiotic theory of the cellular origin of mitochondria and chloroplasts, there are theories that cells led to peroxisomes, spirochetes led to cilia and flagella, and that perhaps a DNA virus led to the cell nucleus, though none of them are widely accepted.\n\nArchaeans, bacteria, and eukaryotes continued to diversify and to become more complex and better adapted to their environments. Each domain repeatedly split into multiple lineages, although little is known about the history of the archaea and bacteria. Around 1.1 Ga, the supercontinent Rodinia was assembling. The plant, animal, and fungi lines had split, though they still existed as solitary cells. Some of these lived in colonies, and gradually a division of labor began to take place; for instance, cells on the periphery might have started to assume different roles from those in the interior. Although the division between a colony with specialized cells and a multicellular organism is not always clear, around 1 billion years ago, the first multicellular plants emerged, probably green algae. Possibly by around 900 Ma true multicellularity had also evolved in animals.\n\nAt first, it probably resembled today's sponges, which have totipotent cells that allow a disrupted organism to reassemble itself. As the division of labor was completed in all lines of multicellular organisms, cells became more specialized and more dependent on each other; isolated cells would die.\n\nReconstructions of tectonic plate movement in the past 250 million years (the Cenozoic and Mesozoic eras) can be made reliably using fitting of continental margins, ocean floor magnetic anomalies and paleomagnetic poles. No ocean crust dates back further than that, so earlier reconstructions are more difficult. Paleomagnetic poles are supplemented by geologic evidence such as orogenic belts, which mark the edges of ancient plates, and past distributions of flora and fauna. The further back in time, the scarcer and harder to interpret the data get and the more uncertain the reconstructions.\n\nThroughout the history of the Earth, there have been times when continents collided and formed a supercontinent, which later broke up into new continents. About 1000 to 830 Ma, most continental mass was united in the supercontinent Rodinia. Rodinia may have been preceded by Early-Middle Proterozoic continents called Nuna and Columbia.\n\nAfter the break-up of Rodinia about 800 Ma, the continents may have formed another short-lived supercontinent around 550 Ma. The hypothetical supercontinent is sometimes referred to as Pannotia or Vendia. The evidence for it is a phase of continental collision known as the Pan-African orogeny, which joined the continental masses of current-day Africa, South America, Antarctica and Australia. The existence of Pannotia depends on the timing of the rifting between Gondwana (which included most of the landmass now in the Southern Hemisphere, as well as the Arabian Peninsula and the Indian subcontinent) and Laurentia (roughly equivalent to current-day North America). It is at least certain that by the end of the Proterozoic eon, most of the continental mass lay united in a position around the south pole.\n\nThe end of the Proterozoic saw at least two Snowball Earths, so severe that the surface of the oceans may have been completely frozen. This happened about 716.5 and 635 Ma, in the Cryogenian period. The intensity and mechanism of both glaciations are still under investigation and harder to explain than the early Proterozoic Snowball Earth.\nMost paleoclimatologists think the cold episodes were linked to the formation of the supercontinent Rodinia. Because Rodinia was centered on the equator, rates of chemical weathering increased and carbon dioxide (CO) was taken from the atmosphere. Because CO is an important greenhouse gas, climates cooled globally.\nIn the same way, during the Snowball Earths most of the continental surface was covered with permafrost, which decreased chemical weathering again, leading to the end of the glaciations. An alternative hypothesis is that enough carbon dioxide escaped through volcanic outgassing that the resulting greenhouse effect raised global temperatures. Increased volcanic activity resulted from the break-up of Rodinia at about the same time.\n\nThe Cryogenian period was followed by the Ediacaran period, which was characterized by a rapid development of new multicellular lifeforms. Whether there is a connection between the end of the severe ice ages and the increase in diversity of life is not clear, but it does not seem coincidental. The new forms of life, called Ediacara biota, were larger and more diverse than ever. Though the taxonomy of most Ediacaran life forms is unclear, some were ancestors of groups of modern life. Important developments were the origin of muscular and neural cells. None of the Ediacaran fossils had hard body parts like skeletons. These first appear after the boundary between the Proterozoic and Phanerozoic eons or Ediacaran and Cambrian periods.\n\nThe Phanerozoic is the current eon on Earth, which started approximately 542 million years ago. It consists of three eras: The Paleozoic, Mesozoic, and Cenozoic, and is the time when multi-cellular life greatly diversified into almost all the organisms known today.\n\nThe Paleozoic (\"old life\") era was the first and longest era of the Phanerozoic eon, lasting from 542 to 251 Ma. During the Paleozoic, many modern groups of life came into existence. Life colonized the land, first plants, then animals. Two major extinctions occurred. The continents formed at the break-up of Pannotia and Rodinia at the end of the Proterozoic slowly moved together again, forming the supercontinent Pangaea in the late Paleozoic.\n\nThe Mesozoic (\"middle life\") era lasted from 251 Ma to  Ma. It is subdivided into the Triassic, Jurassic, and Cretaceous periods. The era began with the Permian–Triassic extinction event, the most severe extinction event in the fossil record; 95% of the species on Earth died out. It ended with the Cretaceous–Paleogene extinction event that wiped out the dinosaurs..\n\nThe Cenozoic (\"new life\") era began at  Ma, and is subdivided into the Paleogene, Neogene, and Quaternary periods. These three periods are further split into seven sub-divisions, with the Paleogene composed of The Paleocene, Eocene, and Oligocene, the Neogene divided into the Miocene, Pliocene, and the Quaternary composed of the Pleistocene, and Holocene. Mammals, birds, amphibians, crocodilians, turtles, and lepidosaurs survived the Cretaceous–Paleogene extinction event that killed off the non-avian dinosaurs and many other forms of life, and this is the era during which they diversified into their modern forms.\n\nAt the end of the Proterozoic, the supercontinent Pannotia had broken apart into the smaller continents Laurentia, Baltica, Siberia and Gondwana. During periods when continents move apart, more oceanic crust is formed by volcanic activity. Because young volcanic crust is relatively hotter and less dense than old oceanic crust, the ocean floors rise during such periods. This causes the sea level to rise. Therefore, in the first half of the Paleozoic, large areas of the continents were below sea level.\n\nEarly Paleozoic climates were warmer than today, but the end of the Ordovician saw a short ice age during which glaciers covered the south pole, where the huge continent Gondwana was situated. Traces of glaciation from this period are only found on former Gondwana. During the Late Ordovician ice age, a few mass extinctions took place, in which many brachiopods, trilobites, Bryozoa and corals disappeared. These marine species could probably not contend with the decreasing temperature of the sea water.\n\nThe continents Laurentia and Baltica collided between 450 and 400 Ma, during the Caledonian Orogeny, to form Laurussia (also known as Euramerica). Traces of the mountain belt this collision caused can be found in Scandinavia, Scotland, and the northern Appalachians. In the Devonian period (416–359 Ma) Gondwana and Siberia began to move towards Laurussia. The collision of Siberia with Laurussia caused the Uralian Orogeny, the collision of Gondwana with Laurussia is called the Variscan or Hercynian Orogeny in Europe or the Alleghenian Orogeny in North America. The latter phase took place during the Carboniferous period (359–299 Ma) and resulted in the formation of the last supercontinent, Pangaea.\n\nBy 180 Ma, Pangaea broke up into Laurasia and Gondwana.\n\nThe rate of the evolution of life as recorded by fossils accelerated in the Cambrian period (542–488 Ma). The sudden emergence of many new species, phyla, and forms in this period is called the Cambrian Explosion. The biological fomenting in the Cambrian Explosion was unpreceded before and since that time. Whereas the Ediacaran life forms appear yet primitive and not easy to put in any modern group, at the end of the Cambrian most modern phyla were already present. The development of hard body parts such as shells, skeletons or exoskeletons in animals like molluscs, echinoderms, crinoids and arthropods (a well-known group of arthropods from the lower Paleozoic are the trilobites) made the preservation and fossilization of such life forms easier than those of their Proterozoic ancestors. For this reason, much more is known about life in and after the Cambrian than about that of older periods. Some of these Cambrian groups appear complex but are seemingly quite different from modern life; examples are \"Anomalocaris\" and \"Haikouichthys\". More recently, however, these seem to have found a place in modern classification. \n\nDuring the Cambrian, the first vertebrate animals, among them the first fishes, had appeared. A creature that could have been the ancestor of the fishes, or was probably closely related to it, was \"Pikaia\". It had a primitive notochord, a structure that could have developed into a vertebral column later. The first fishes with jaws (Gnathostomata) appeared during the next geological period, the Ordovician. The colonisation of new niches resulted in massive body sizes. In this way, fishes with increasing sizes evolved during the early Paleozoic, such as the titanic placoderm \"Dunkleosteus\", which could grow long.\n\nThe diversity of life forms did not increase greatly because of a series of mass extinctions that define widespread biostratigraphic units called \"biomeres\". After each extinction pulse, the continental shelf regions were repopulated by similar life forms that may have been evolving slowly elsewhere. By the late Cambrian, the trilobites had reached their greatest diversity and dominated nearly all fossil assemblages.\n\nOxygen accumulation from photosynthesis resulted in the formation of an ozone layer that absorbed much of the Sun's ultraviolet radiation, meaning unicellular organisms that reached land were less likely to die, and prokaryotes began to multiply and become better adapted to survival out of the water. Prokaryote lineages had probably colonized the land as early as 2.6 Ga even before the origin of the eukaryotes. For a long time, the land remained barren of multicellular organisms. The supercontinent Pannotia formed around 600 Ma and then broke apart a short 50 million years later. Fish, the earliest vertebrates, evolved in the oceans around 530 Ma. A major extinction event occurred near the end of the Cambrian period, which ended 488 Ma.\n\nSeveral hundred million years ago, plants (probably resembling algae) and fungi started growing at the edges of the water, and then out of it. The oldest fossils of land fungi and plants date to 480–460 Ma, though molecular evidence suggests the fungi may have colonized the land as early as 1000 Ma and the plants 700 Ma. Initially remaining close to the water's edge, mutations and variations resulted in further colonization of this new environment. The timing of the first animals to leave the oceans is not precisely known: the oldest clear evidence is of arthropods on land around 450 Ma, perhaps thriving and becoming better adapted due to the vast food source provided by the terrestrial plants. There is also unconfirmed evidence that arthropods may have appeared on land as early as 530 Ma.\n\nAt the end of the Ordovician period, 443 Ma, additional extinction events occurred, perhaps due to a concurrent ice age. Around 380 to 375 Ma, the first tetrapods evolved from fish. Fins evolved to become limbs that the first tetrapods used to lift their heads out of the water to breathe air. This would let them live in oxygen-poor water, or pursue small prey in shallow water. They may have later ventured on land for brief periods. Eventually, some of them became so well adapted to terrestrial life that they spent their adult lives on land, although they hatched in the water and returned to lay their eggs. This was the origin of the amphibians. About 365 Ma, another period of extinction occurred, perhaps as a result of global cooling. Plants evolved seeds, which dramatically accelerated their spread on land, around this time (by approximately 360 Ma).\n\nAbout 20 million years later (340 Ma), the amniotic egg evolved, which could be laid on land, giving a survival advantage to tetrapod embryos. This resulted in the divergence of amniotes from amphibians. Another 30 million years (310 Ma) saw the divergence of the synapsids (including mammals) from the sauropsids (including birds and reptiles). Other groups of organisms continued to evolve, and lines diverged—in fish, insects, bacteria, and so on—but less is known of the details.\nAfter yet another, the most severe extinction of the period (251~250 Ma), around 230 Ma, dinosaurs split off from their reptilian ancestors. The Triassic–Jurassic extinction event at 200 Ma spared many of the dinosaurs, and they soon became dominant among the vertebrates. Though some mammalian lines began to separate during this period, existing mammals were probably small animals resembling shrews.\n\nThe boundary between avian and non-avian dinosaurs is not clear, but \"Archaeopteryx\", traditionally considered one of the first birds, lived around 150 Ma.\n\nThe earliest evidence for the angiosperms evolving flowers is during the Cretaceous period, some 20 million years later (132 Ma).\n\nThe first of five great mass extinctions was the Ordovician-Silurian extinction. Its possible cause was the intense glaciation of Gondwana, which eventually led to a snowball earth. 60% of marine invertebrates became extinct and 25% of all families.\n\nThe second mass extinction was the Late Devonian extinction, probably caused by the evolution of trees, which could have led to the depletion of greenhouse gases (like CO2) or the eutrophication of water. 70% of all species became extinct.\n\nThe third mass extinction was the Permian-Triassic, or the Great Dying, event was possibly caused by some combination of the Siberian Traps volcanic event, an asteroid impact, methane hydrate gasification, sea level fluctuations, and a major anoxic event. Either the proposed Wilkes Land crater in Antarctica or Bedout structure off the northwest coast of Australia may indicate an impact connection with the Permian-Triassic extinction. But it remains uncertain whether either these or other proposed Permian-Triassic boundary craters are either real impact craters or even contemporaneous with the Permian-Triassic extinction event. This was by far the deadliest extinction ever, with about 57% of all families and 83% of all genera killed.\n\nThe fourth mass extinction was the Triassic-Jurassic extinction event in which almost all synapsids and archosaurs became extinct, probably due to new competition from dinosaurs.\n\nThe fifth and most recent mass extinction was the K-T extinction. In 66 Ma, a asteroid struck Earth just off the Yucatán Peninsula – somewhere in the south western tip of then Laurasia – where the Chicxulub crater is today. This ejected vast quantities of particulate matter and vapor into the air that occluded sunlight, inhibiting photosynthesis. 75% of all life, including the non-avian dinosaurs, became extinct, marking the end of the Cretaceous period and Mesozoic era.\n\nThe first true mammals evolved in the shadows of dinosaurs and other large archosaurs that filled the world by the late Triassic. The first mammals were very small, and were probably nocturnal to escape predation. Mammal diversification truly began only after the Cretaceous-Paleogene extinction event. By the early Paleocene the earth recovered from the extinction, and mammalian diversity increased. Creatures like \"Ambulocetus\" took to the oceans to eventually evolve into whales, whereas some creatures, like primates, took to the trees. This all changed during the mid to late Eocene when the circum-Antarctic current formed between Antarctica and Australia which disrupted weather patterns on a global scale. Grassless savannas began to predominate much of the landscape, and mammals such as \"Andrewsarchus\" rose up to become the largest known terrestrial predatory mammal ever, and early whales like \"Basilosaurus\" took control of the seas. \n\nThe evolution of grass brought a remarkable change to the Earth's landscape, and the new open spaces created pushed mammals to get bigger and bigger. Grass started to expand in the Miocene, and the Miocene is where many modern- day mammals first appeared. Giant ungulates like \"Paraceratherium\" and \"Deinotherium\" evolved to rule the grasslands. The evolution of grass also brought primates down from the trees, and started human evolution. The first big cats evolved during this time as well. The Tethys Sea was closed off by the collision of Africa and Europe.\n\nThe formation of Panama was perhaps the most important geological event to occur in the last 60 million years. Atlantic and Pacific currents were closed off from each other, which caused the formation of the Gulf Stream, which made Europe warmer. The land bridge allowed the isolated creatures of South America to migrate over to North America, and vice versa. Various species migrated south, leading to the presence in South America of llamas, the spectacled bear, kinkajous and jaguars.\n\nThree million years ago saw the start of the Pleistocene epoch, which featured dramatic climactic changes due to the ice ages. The ice ages led to the evolution of modern man in Saharan Africa and expansion. The mega-fauna that dominated fed on grasslands that, by now, had taken over much of the subtropical world. The large amounts of water held in the ice allowed for various bodies of water to shrink and sometimes disappear such as the North Sea and the Bering Strait. It is believed by many that a huge migration took place along Beringia which is why, today, there are camels (which evolved and became extinct in North America), horses (which evolved and became extinct in North America), and Native Americans. The ending of the last ice age coincided with the expansion of man, along with a massive die out of ice age mega-fauna. This extinction, nicknamed \"the Sixth Extinction\", has been going ever since.\n\nA small African ape living around 6 Ma was the last animal whose descendants would include both modern humans and their closest relatives, the chimpanzees. Only two branches of its family tree have surviving descendants. Very soon after the split, for reasons that are still unclear, apes in one branch developed the ability to walk upright. Brain size increased rapidly, and by 2 Ma, the first animals classified in the genus \"Homo\" had appeared. Of course, the line between different species or even genera is somewhat arbitrary as organisms continuously change over generations. Around the same time, the other branch split into the ancestors of the common chimpanzee and the ancestors of the bonobo as evolution continued simultaneously in all life forms.\n\nThe ability to control fire probably began in \"Homo erectus\" (or \"Homo ergaster\"), probably at least 790,000 years ago but perhaps as early as 1.5 Ma. The use and discovery of controlled fire may even predate \"Homo erectus\". Fire was possibly used by the early Lower Paleolithic (Oldowan) hominid \"Homo habilis\" or strong australopithecines such as \"Paranthropus.\"\n\nIt is more difficult to establish the origin of language; it is unclear whether \"Homo erectus\" could speak or if that capability had not begun until \"Homo sapiens\". As brain size increased, babies were born earlier, before their heads grew too large to pass through the pelvis. As a result, they exhibited more plasticity, and thus possessed an increased capacity to learn and required a longer period of dependence. Social skills became more complex, language became more sophisticated, and tools became more elaborate. This contributed to further cooperation and intellectual development. Modern humans (\"Homo sapiens\") are believed to have originated around 200,000 years ago or earlier in Africa; the oldest fossils date back to around 160,000 years ago.\n\nThe first humans to show signs of spirituality are the Neanderthals (usually classified as a separate species with no surviving descendants); they buried their dead, often with no sign of food or tools. However, evidence of more sophisticated beliefs, such as the early Cro-Magnon cave paintings (probably with magical or religious significance) did not appear until 32,000 years ago. Cro-Magnons also left behind stone figurines such as Venus of Willendorf, probably also signifying religious belief. By 11,000 years ago, \"Homo sapiens\" had reached the southern tip of South America, the last of the uninhabited continents (except for Antarctica, which remained undiscovered until 1820 AD). Tool use and communication continued to improve, and interpersonal relationships became more intricate.\n\nThroughout more than 90% of its history, \"Homo sapiens\" lived in small bands as nomadic hunter-gatherers. As language became more complex, the ability to remember and communicate information resulted, according to a theory proposed by Richard Dawkins, in a new replicator: the meme. Ideas could be exchanged quickly and passed down the generations. Cultural evolution quickly outpaced biological evolution, and history proper began. Between 8500 and 7000 BC, humans in the Fertile Crescent in the Middle East began the systematic husbandry of plants and animals: agriculture. This spread to neighboring regions, and developed independently elsewhere, until most \"Homo sapiens\" lived sedentary lives in permanent settlements as farmers. Not all societies abandoned nomadism, especially those in isolated areas of the globe poor in domesticable plant species, such as Australia. However, among those civilizations that did adopt agriculture, the relative stability and increased productivity provided by farming allowed the population to expand.\n\nAgriculture had a major impact; humans began to affect the environment as never before. Surplus food allowed a priestly or governing class to arise, followed by increasing division of labor. This led to Earth's first civilization at Sumer in the Middle East, between 4000 and 3000 BC. Additional civilizations quickly arose in ancient Egypt, at the Indus River valley and in China. The invention of writing enabled complex societies to arise: record-keeping and libraries served as a storehouse of knowledge and increased the cultural transmission of information. Humans no longer had to spend all their time working for survival, enabling the first specialized occupations (e.g. craftsmen, merchants, priests, etc...). Curiosity and education drove the pursuit of knowledge and wisdom, and various disciplines, including science (in a primitive form), arose. This in turn led to the emergence of increasingly larger and more complex civilizations, such as the first empires, which at times traded with one another, or fought for territory and resources.\n\nBy around 500 BC, there were advanced civilizations in the Middle East, Iran, India, China, and Greece, at times expanding, at times entering into decline. In 221 BC, China became a single polity that would grow to spread its culture throughout East Asia, and it has remained the most populous nation in the world. The fundamentals of Western civilization were largely shaped in Ancient Greece, with the world's first democratic government and major advances in philosophy, science, and mathematics, and in Ancient Rome in law, government, and engineering. The Roman Empire was Christianized by Emperor Constantine in the early 4th century and declined by the end of the 5th. Beginning with the 7th century, Christianization of Europe began. In 610, Islam was founded and quickly became the dominant religion in Western Asia. The House of Wisdom was established in Abbasid-era Baghdad, Iraq. It is considered to have been a major intellectual center during the Islamic Golden Age, where Muslim scholars in Baghdad and Cairo flourished from the ninth to the thirteenth centuries until the Mongol sack of Baghdad in 1258 AD. In 1054 AD the Great Schism between the Roman Catholic Church and the Eastern Orthodox Church led to the prominent cultural differences between Western and Eastern Europe.\n\nIn the 14th century, the Renaissance began in Italy with advances in religion, art, and science. At that time the Christian Church as a political entity lost much of its power. In 1492, Christopher Columbus reached the Americas, initiating great changes to the new world. European civilization began to change beginning in 1500, leading to the scientific and industrial revolutions. That continent began to exert political and cultural dominance over human societies around the world, a time known as the Colonial era (also see Age of Discovery). In the 18th century a cultural movement known as the Age of Enlightenment further shaped the mentality of Europe and contributed to its secularization. From 1914 to 1918 and 1939 to 1945, nations around the world were embroiled in world wars. Established following World War I, the League of Nations was a first step in establishing international institutions to settle disputes peacefully. After failing to prevent World War II, mankind's bloodiest conflict, it was replaced by the United Nations. After the war, many new states were formed, declaring or being granted independence in a period of decolonization. The United States and Soviet Union became the world's dominant superpowers for a time, and they held an often-violent rivalry known as the Cold War until the dissolution of the latter. In 1992, several European nations joined in the European Union. As transportation and communication improved, the economies and political affairs of nations around the world have become increasingly intertwined. This globalization has often produced both conflict and cooperation.\n\nChange has continued at a rapid pace from the mid-1940s to today. Technological developments include nuclear weapons, computers, genetic engineering, and nanotechnology. Economic globalization, spurred by advances in communication and transportation technology, has influenced everyday life in many parts of the world. Cultural and institutional forms such as democracy, capitalism, and environmentalism have increased influence. Major concerns and problems such as disease, war, poverty, violent radicalism, and recently, human-caused climate change have risen as the world population increases.\n\nIn 1957, the Soviet Union launched the first artificial satellite into orbit and, soon afterward, Yuri Gagarin became the first human in space. Neil Armstrong, an American, was the first to set foot on another astronomical object, the Moon. Unmanned probes have been sent to all the known planets in the solar system, with some (such as Voyager) having left the solar system. Five space agencies, representing over fifteen countries, have worked together to build the International Space Station. Aboard it, there has been a continuous human presence in space since 2000. The World Wide Web became a part of everyday life in the 1990s, and since then has become an indispensable source of information in the developed world.\n\n"}
{"id": "9033874", "url": "https://en.wikipedia.org/wiki?curid=9033874", "title": "J. Mordaunt Crook", "text": "J. Mordaunt Crook\n\nJoseph Mordaunt Crook (born 27 February 1937), generally known as J. Mordaunt Crook, is an English architectural historian and specialist on the Georgian and Victorian periods. He is an authority on the life and work of the Victorian architect William Burges.\n\n\n"}
{"id": "55949472", "url": "https://en.wikipedia.org/wiki?curid=55949472", "title": "John Chartres", "text": "John Chartres\n\nJohn Anthony Chartres (born January 1946) is the former professor of economic and social history at the University of Leeds. He is a specialist in the economic history of agriculture in England.\n\n\n"}
{"id": "19495159", "url": "https://en.wikipedia.org/wiki?curid=19495159", "title": "Jožko Šavli", "text": "Jožko Šavli\n\nJožko Šavli (March 22, 1943March 11, 2011) was a Slovene author, self-declared historian and high school teacher in economic sciences from Italy.\n\nŠavli was born in Tolmin, then part of the Kingdom of Italy (now in Slovenia). He obtained a degree in Business Management at the University of Ljubljana in 1967. Then he continued his studies at the Vienna School of International Trade (\"Hochschule für Welthandel\"), where in 1975 he obtained a doctorate in social and economic sciences with a thesis on the economic structure and regional economic development in the district of Horn in Lower Austria.\nFrom 1978 he taught business subjects at the Slovene language Technical School of Commerce in Gorizia, Italy.\n\nŠavli became known in Slovenia in the mid 1980s, when he advanced the Venetic theory, together with the poet Matej Bor. According to the theory, the Slovenes were not descended from the Slavs that settled the region in the 6th century, but that they were descended from a proto-Slavic speaking people known as the Veneti. The theory was rejected in academic circles, but nevertheless gained widespread popularity in Slovenia, as well as some interest abroad.\n\nAlready in the early 1980s, Šavli began to publish articles and essays that concerned in particular traditions of the Medieval state of Carantania. In his books, Šavli emphasized the political and cultural continuity between the proto-Slovene state of Carantania and the later Duchy of Carinthia. Šavli claims to have discovered the Black Panther as the coat of Arms or symbol of Carantania. This discovery is\ndisputed by academic historians. Šavli lived and worked in Gorizia, where he died in March 2011.\n\n\n"}
{"id": "30010614", "url": "https://en.wikipedia.org/wiki?curid=30010614", "title": "Karum (trade post)", "text": "Karum (trade post)\n\nKarum (Akkadian: \"kārum\" \"quay, port, commercial district\", plural \"kārū\", from Sumerian \"kar\" \"fortification (of a harbor), break-water\") is the name given to ancient Assyrian trade posts \nin Anatolia (modern Turkey) from the 20th to 18th centuries BC. The main centre of \"karum\" trading was at the ancient town of Kanesh.\n\nEarly references to \"karu\" come from the Ebla tablets; in particular, a vizier known as Ebrium concluded the earliest treaty fully known to archaeology, known variously as the \"Treaty between Ebla and Aššur\" or the \"Treaty with Abarsal\" (scholars have disputed whether the text refers to Aššur or to Abarsal, an unknown location). In either case, the other city contracted to establish \"karu\" in Eblaite territory (Syria), among other things.\n\nSargon the Great, who likely destroyed Ebla soon after this, is said in a much later Hittite account to have invaded Anatolia to punish Nurdaggal the king of Burushanda for mistreating the Akkadian and Assyrian merchant class in the \"karu\" there. However, this is not given the weight of a contemporary source.\n\nDuring the second millennium BC, Anatolia was under the sovereignty of Hatti city states and, later, the Hittites. By 1960 BC, Assyrian merchants had established the \"karu\", small colonial settlements next to Anatolian cities which paid taxes to the rulers of the cities. There were also smaller trade stations which were called \"mabartū\" (singular \"mabartum\"). The number of \"karu\" and \"mabartu\" was probably around twenty. Among them were Kültepe (Kanesh in antiquity) in modern Kayseri Province; Alişar Hüyük (Ankuva (?) in antiquity) in modern Yozgat Province; and Boğazköy (Hattusa in antiquity) in modern Çorum Province. (However, Alişar Hüyük was probably a \"mabartum\".) But after the establishment of the Hittite Empire, the \"karu\" disappeared from Anatolian history.\n\nIn the second millennium BC, money was not yet invented, and Assyrian merchants used gold for wholesale trade and silver for retail trade. Gold was considered eight times more valuable than silver. But there was one more metal, \"amutum\", which was even more valuable than gold. \"Amutum\" is thought to be the newly discovered iron and was forty times more valuable than silver.\nThe most important Anatolian export was copper, and the Assyrian merchants sold tin and clothing to Anatolia.\n\nThe name \"Karum\" is given to an upscale shopping mall in Çankaya district of modern day Ankara, Turkey. This is a reference to the presence of \"karu\" in Asia minor, since the very early days of history.\nAnother mall in Ankara's Bilkent district is also given the name \"Ankuva\". This is also a reference to archaeological discoveries of various \"karu\" in Central Anatolia.\n"}
{"id": "3065193", "url": "https://en.wikipedia.org/wiki?curid=3065193", "title": "Kuroko", "text": "Kuroko\n\nIn kabuki, the \"kuroko\" serve many of the same purposes as running crew. They move scenery and props on stage, aiding in scene changes and costume changes. They will also often play the role of animals, will-o-the-wisps, or other roles which are played not by an actor in full costume, but by holding a prop. \"Kuroko\" wear all black, head to toe, in order to imply that they are invisible and not part of the action onstage.\n\nThe convention of wearing black to imply that the wearer is invisible on stage is a central element in \"bunraku\" puppet theatre as well. \"Kuroko\" will wear white or blue in order to blend in with the background in a scene set, for example, in a snowstorm, or at sea, in which case they are referred to as or respectively. As this convention was extended to kabuki actors depicting stealthy ninja, historian Stephen Turnbull suggested that the stereotypical image of a ninja dressed all in black derived from kabuki. The theatrical convention of dressing ninja characters as apparent stagehands to imply stealth and to surprise audiences contributed to this popular image, in contrast to the historical reality that real ninjas usually dressed like civilians.\n\nIn Noh theatre, a \"kōken\", wearing black but no mask, serves much the same purpose.\n\n\n"}
{"id": "57578038", "url": "https://en.wikipedia.org/wiki?curid=57578038", "title": "Leila Fawaz", "text": "Leila Fawaz\n\nLeila Fawaz is a Lebanese historian and academician. She is the founding director of The Fares Center for Eastern Mediterranean Studies from 2001 to 2012. Fawaz was born in Sudan to Greek-Orthodox Lebanese parents and raised in Lebanon. She took two degrees at the American University of Beirut between 1967 and 1968 and pursued graduate studies in history at Harvard University between 1972 and 1979.\n\nShe became a member of the Tufts University faculty in 1979, and became a full professor in 1994. She chaired the History Department from 1994 to 1996. From 1996 to 2001, she served as dean of arts and humanities and as associate dean of the faculty. At Tufts, Fawaz became the Issam M. Fares Professor of Lebanese and Eastern Mediterranean Studies, where she also held appointments as professor of diplomacy at the Fletcher School of Law and Diplomacy and as professor of history.\n\nFrom 1990 to 1994, she Fawaz was the editor-in-chief of \"The International Journal of Middle East Studies\", there she advanced the notion of conducting analytical and comparative research, with an international and cross-disciplinary approach. She bemoaned the overspecialization within the field, the neglect of attention to humanities/arts and, uninteresting writing, and linked these problems to Middle East studies as a whole, because researchers were \"still a long way from being pathfinders in the world of scholarship generally.\"\n\nFawaz also served on the editorial board of \"The American Historical Review\". A former president of the Middle East Studies Association of North America, a member of the Council on Foreign Relations, and a member of the Comité Scientifique of the Maison Méditerranéenne des Sciences de l’Homme at the Université de Provence. Fawaz served on the board of overseers of Harvard University from 1996 to 2012.\n\nIn 2012 She was named a \"chevalier\" in the French National Order of the Legion of Honor. She was honored for her \"exemplary personal commitment to French-American relations\" and for her efforts to promote French academic research and thought at prestigious American universities.\"\n"}
{"id": "55483831", "url": "https://en.wikipedia.org/wiki?curid=55483831", "title": "Libby Connors", "text": "Libby Connors\n\nLibby Connors (born 1960, Brisbane) is Associate Professor of History at the University of Southern Queensland. In 2015 she received the Queensland Premier's Award for a work of State Significance for \"Warrior: A Legendary Leader's Dramatic Life and Violent Death on the Colonial Frontier\" \",\" which followed Dalla lawman Dundalli from his life in southeast Queensland to his execution outside Brisbane gaol on 5 January 1855.\n"}
{"id": "35720163", "url": "https://en.wikipedia.org/wiki?curid=35720163", "title": "Marguerite Dupire", "text": "Marguerite Dupire\n\nMarguerite Dupire (12 October 1920 – 4 March 2015) was a French ethnologist who specialises on African people, and had worked extensively on the Fulani of Niger, Cameroon, Guinea, Senegal, and then after a mission in Ivory Coast, on the Serer people of Sine (in Senegal) since 1965.\n\nDupire gained a degree in philosophy in 1943. She then completed her training by studying psychology and ethnology at the University of Paris, then in the United States, at the Northwestern University and the Philadelphia University, where she was the student of notable anthropologists such as Melville Herskovits and Alfred Irving Hallowell in the late 1940s.\n\nMarguerite Dupire has authored numerous scientific articles (see below).\n\nHer principal works are :\n\n\n"}
{"id": "2289787", "url": "https://en.wikipedia.org/wiki?curid=2289787", "title": "Marine isotope stage", "text": "Marine isotope stage\n\nMarine isotope stages (MIS), marine oxygen-isotope stages, or oxygen isotope stages (OIS), are alternating warm and cool periods in the Earth's paleoclimate, deduced from oxygen isotope data reflecting changes in temperature derived from data from deep sea core samples. Working backwards from the present, which is MIS 1 in the scale, stages with even numbers have high levels of oxygen-18 and represent cold glacial periods, while the odd-numbered stages are troughs in the oxygen-18 figures, representing warm interglacial intervals. The data are derived from pollen and foraminifera (plankton) remains in drilled marine sediment cores, sapropels, and other data that reflect historic climate; these are called proxies.\n\nThe MIS timescale was developed from the pioneering work of Cesare Emiliani in the 1950s, and is now widely used in archaeology and other fields to express dating in the Quaternary period (the last 2.6 million years), as well as providing the fullest and best data for that period for paleoclimatology or the study of the early climate of the Earth, representing \"the standard to which we correlate other Quaternary climate records\". Emiliani's work in turn depended on Harold Urey's prediction in a paper of 1947 that the ratio between oxygen-18 and oxygen-16 isotopes in calcite, the main chemical component of the shells and other hard parts of a wide range of marine organisms, should vary depending on the prevailing water temperature in which the calcite was formed.\n\nOver 100 stages have been identified, going currently back some 6 million years, and the scale may in future reach back up to 15 mya. Some stages, in particular MIS 5, are divided into sub-stages, such as \"MIS 5a\", with 5 a, c, and e being warm and b and d cold. A numeric system for referring to \"horizons\" (events rather than periods) may also be used, with for example MIS 5.5 representing the peak point of MIS 5e, and 5.51, 5.52 etc. representing the peaks and troughs of the record at a still more detailed level. For more recent periods, increasingly precise resolution of timing continues to be developed.\n\nIn 1957 Emiliani moved to the University of Miami to have access to core-drilling ships and equipment, and began to drill in the Caribbean and collect core data. A further important advance came in 1967, when Nicholas Shackleton suggested that the fluctuations over time in the marine isotope ratios that had become evident by then were caused not so much by changes in water temperature, as Emiliani thought, but mainly by changes in the volume of ice-sheets, which when they expanded took up the lighter oxygen-16 isotope in preference to the heavier oxygen-18. The cycles in the isotope ratio were found to correspond to terrestrial evidence of glacials and interglacials. A graph of the entire series of stages then revealed unsuspected advances and retreats of ice and also filled in the details of the stadials and interstadials.\n\nMore recent ice core samples of today's glacial ice substantiated the cycles through studies of ancient pollen deposition. Currently a number of methods are making additional detail possible. Matching the stages to named periods proceeds as new dates are discovered and new regions are explored geologically. The marine isotopic records appear more complete and detailed than any terrestrial equivalents, and have enabled a timeline of glaciation for the Plio-Pleistocene to be identified. It is now believed that changes in the size of the major ice sheets such as the historical Laurentide ice sheet of North America are the main factor governing variations in the oxygen isotope ratios.\n\nThe MIS data also matches the astronomical data of Milankovitch cycles of orbital forcing or the effects of variations in insolation caused by cyclical slight changes in the tilt of the earth's axis of rotation – the \"orbital theory\". Indeed, that the MIS data matched Milankovich's theory, which he formed during World War I, so well was a key factor in the theory gaining general acceptance, despite some remaining problems at certain points, notably the so-called 100,000-year problem. For relatively recent periods data from radiocarbon dating and dendrochronology also support the MIS data. The sediments also acquire depositional remanent magnetization which allows them to be correlated with earth's geomagnetic reversals. For older core samples, individual annual depositions cannot usually be distinguished, and dating is taken from the geomagnetic information in the cores. Other information, especially as to the ratios of gases such as carbon dioxide in the atmosphere, is provided by analysis of ice cores.\n\nThe SPECMAP Project, funded by the US National Science Foundation, has produced one standard chronology for oxygen isotope records, although there are others. This high resolution chronology was derived from several isotopic records, the composite curve was then smoothed, filtered and tuned to the known cycles of the astronomical variables. The use of a number of isotopic profiles was designed to eliminate 'noise' errors, that could have been contained within a single isotopic record. Another large research project funded by the US government in the 1970s and 1980s was (CLIMAP), which to a large degree succeeded in its aim of producing a map of the global climate at the Last Glacial Maximum, some 18,000 years ago, with some of the research also directed at the climate some 120,000 years ago, during the last interglacial. The theoretical advances and greatly improved data available by the 1970s enabled a \"grand synthesis\" to be made, best known from the 1976 paper \"Variations in the earth’s orbit: pacemaker of the ice ages\" (in \"Science\"), by J.D. Hays, Shackleton and John Imbrie, which is still very widely accepted today, and covers the MIS timescale and the causal effect of the orbital theory.\n\nIn 2010 the Subcommission on Quaternary Stratigraphy of the International Commission on Stratigraphy dropped other lists of MIS dates and started using the Lisiecki & Raymo (2005) LR04 Benthic Stack, as updated. This was compiled by Lorraine Lisiecki and Maureen Raymo.\n\nThe following are the start dates (apart from MIS 5 sub-stages) of the most recent MIS (Lisiecki & Raymo 2005, LR04 Benthic Stack). The figures, in thousands of years ago, are from Lisiecki's website.. Numbers for substages in MIS 5 denote peaks of substages rather than boundaries.\n\n\nThe list continues to MIS 104, beginning 2.614 million years ago.\n\nThe following are the start dates of the most recent MIS, in kya (thousands of years ago). The first figures are derived by Aitken & Stokes from Bassinot et al. (1994), with the figures in parentheses alternative estimates from Martinson et al. for stage 4 and for the others the SPECMAP figures in Imbrie et al. (1984). For stages 1–16 the SPECMAP figures are within 5 kya of the figures given here. All figures up to MIS 21 are taken from Aitken & Stokes, Table 1.4, except for the sub-stages of MIS 5, which are from Wright's Table 1.1.\n\n\nSome older stages, in mya (millions of years ago):\n\n\n\n\n"}
{"id": "17731455", "url": "https://en.wikipedia.org/wiki?curid=17731455", "title": "Martti Häikiö", "text": "Martti Häikiö\n\nMartti Johannes Häikiö (b. 1 October 1949 in Mikkeli), is a Finnish historian and writer. In 1978 Häikiö became an associate professor in political history at Helsinki University. He has also been editor in chief and columnist in many journals and newspapers.\n\nHis doctoral dissertation concerns the British politics on Finland before and during the Winter War and has thereafter published a lot of popular works on modern Finnish history and corporate history, such as \"Nokia, the inside story\" (2002).\n\n"}
{"id": "31792937", "url": "https://en.wikipedia.org/wiki?curid=31792937", "title": "Medieval renaissances", "text": "Medieval renaissances\n\nThe medieval renaissances were periods characterised by significant cultural renewal across medieval Western Europe. These are effectively seen as occurring in three phases - the Carolingian Renaissance (8th and 9th centuries), Ottonian Renaissance (10th century) and the renaissance of the 12th century.\n\nThe term was first used by medievalists in the 19th century, by analogy with the historiographical concept of the 15th and 16th century Italian Renaissance. This was notable since it marked a break with the dominant historiography of the time, which saw the Middle Ages as a Dark Age. The term has always been a subject of debate and criticism, particularly on how widespread such renewal movements were and on the validity of comparing them with the Italian Renaissance.\n\nThe term 'renaissance' was first used as a name for a period in medieval history in the 1830s, with the birth of medieval studies. It was coined by Jean-Jacques Ampère.\n\nThe Carolingian Renaissance was a period of intellectual and cultural revival in the Carolingian Empire occurring from the late eighth century to the ninth century, as the first of three medieval renaissances. It occurred mostly during the reigns of the Carolingian rulers Charlemagne and Louis the Pious. It was supported by the scholars of the Carolingian court, notably Alcuin of York For moral betterment the Carolingian renaissance reached for models drawn from the example of the Christian Roman Empire of the 4th century. During this period there was an increase of literature, writing, the arts, architecture, jurisprudence, liturgical reforms and scriptural studies. Charlemagne's \"Admonitio generalis\" (789) and his \"Epistola de litteris colendis\" served as manifestos. The effects of this cultural revival, however, were largely limited to a small group of court \"literati\": \"it had a spectacular effect on education and culture in Francia, a debatable effect on artistic endeavors, and an immeasurable effect on what mattered most to the Carolingians, the moral regeneration of society,\" John Contreni observes. Beyond their efforts to write better Latin, to copy and preserve patristic and classical texts and to develop a more legible, classicizing script, the Carolingian minuscule that Renaissance humanists took to be Roman and employed as humanist minuscule, from which has developed early modern Italic script, the secular and ecclesiastical leaders of the Carolingian Renaissance for the first time in centuries applied rational ideas to social issues, providing a common language and writing style that allowed for communication across most of Europe.\n\nSir Kenneth Clark was of the view that by means of the Carolingian Renaissance, Western civilization survived by the skin of its teeth. The use of the term \"renaissance\" to describe this period is contested due to the majority of changes brought about by this period being confined almost entirely to the clergy, and due to the period lacking the wide-ranging social movements of the later Italian Renaissance. Instead of being a rebirth of new cultural movements, the period was more an attempt to recreate the previous culture of the Roman Empire. The Carolingian Renaissance in retrospect also has some of the character of a false dawn, in that its cultural gains were largely dissipated within a couple of generations, a perception voiced by Walahfrid Strabo (died 849), in his introduction to Einhard's \"Life of Charlemagne\".\n\nThe Ottonian Renaissance was a limited \"renaissance\" of economy and art in central and southern Europe that accompanied the reigns of the first three emperors of the Saxon Dynasty, all named Otto: Otto I (936–973), Otto II (973–983), and Otto III (983–1002), and which in large part depended upon their patronage. The Ottonian Renaissance began after Otto's marriage to Adelaide (951) united the kingdoms of Italy and Germany and thus brought the West closer to Byzantium and furthered the cause of Christian (political) unity with his imperial coronation in 963. The period is sometimes extended to cover the reign of Henry II as well, and, rarely, the Salian dynasts. The term is generally confined to Imperial court culture conducted in Latin in Germany. - it is sometimes also known as the Renaissance of the 10th century, so as to include developments outside Germania, or as the Year 1000 Renewal, due to coming right at the end of the 10th century. It was shorter than the preceding Carolingian Renaissance and to a large extent a continuation of it - this has led historians such as Pierre Riché to prefer evoking it as a 'third Carolingian renaissance', covering the 10th century and running over into the 11th century, with the 'first Carolingian renaissance' occurring during Charlemagne's own reign and the 'second Carolingian renaissance' happening under his successors.\n\nThe Ottonian Renaissance is recognized especially in the arts and architecture, invigorated by renewed contact with Constantinople, in some revived cathedral schools, such as that of Bruno of Cologne, in the production of illuminated manuscripts from a handful of elite scriptoria, such as Quedlinburg, founded by Otto in 936, and in political ideology. The Imperial court became the center of religious and spiritual life, led by the example of women of the royal family: Matilda the literate mother of Otto I, or his sister Gerberga of Saxony, or his consort Adelaide, or Empress Theophanu.\n\nThe Renaissance of the 12th century was a period of many changes at the outset of the High Middle Ages. It included social, political and economic transformations, and an intellectual revitalization of Western Europe with strong philosophical and scientific roots. For some historians these changes paved the way to later achievements such as the literary and artistic movement of the Italian Renaissance in the 15th century and the scientific developments of the 17th century.\n\nAfter the collapse of the Western Roman Empire, Western Europe had entered the Middle Ages with great difficulties. Apart from depopulation and other factors, most classical scientific treatises of classical antiquity, written in Greek, had become unavailable. Philosophical and scientific teaching of the Early Middle Ages was based upon the few Latin translations and commentaries on ancient Greek scientific and philosophical texts that remained in the Latin West.\n\nThis scenario changed during the renaissance of the 12th century. The increased contact with the Islamic world in Spain and Sicily, the Crusades, the Reconquista, as well as increased contact with Byzantium, allowed Europeans to seek and translate the works of Hellenic and Islamic philosophers and scientists, especially the works of Aristotle.\n\nThe development of medieval universities allowed them to aid materially in the translation and propagation of these texts and started a new infrastructure which was needed for scientific communities. In fact, the European university put many of these texts at the center of its curriculum, with the result that the \"medieval university laid far greater emphasis on science than does its modern counterpart and descendent.\"\n\nIn Northern Europe, the Hanseatic League was founded in the 12th century, with the foundation of the city of Lübeck in 1158–1159. Many northern cities of the Holy Roman Empire became Hanseatic cities, including Hamburg, Stettin, Bremen and Rostock. Hanseatic cities outside the Holy Roman Empire were, for instance, Bruges, London and the Polish city of Danzig (Gdańsk). In Bergen and Novgorod the league had factories and middlemen. In this period the Germans started colonizing Eastern Europe beyond the Empire, into Prussia and Silesia. In the late 13th century, the Venetian explorer Marco Polo became one of the first Europeans to travel the Silk Road to China. Westerners became more aware of the Far East when Polo documented his travels in \"Il Milione\". He was followed by numerous Christian missionaries to the East, such as William of Rubruck, Giovanni da Pian del Carpini, Andrew of Longjumeau, Odoric of Pordenone, Giovanni de Marignolli, Giovanni di Monte Corvino, and other travelers such as Niccolò da Conti.\n\nThe translation of texts from other cultures, especially ancient Greek works, was an important aspect of both this Twelfth-Century Renaissance and the latter Renaissance (of the 15th century), the relevant difference being that Latin scholars of this earlier period focused almost entirely on translating and studying Greek and Arabic works of natural science, philosophy and mathematics, while the latter Renaissance focus was on literary and historical texts.\n\nA new method of learning called scholasticism developed in the late 12th century from the rediscovery of the works of Aristotle; the works of medieval Jewish and Islamic thinkers influenced by him, notably Maimonides, Avicenna (see Avicennism) and Averroes (see Averroism); and the Christian philosophers influenced by them, most notably Albertus Magnus, Bonaventure and Abélard. Those who practiced the scholastic method believed in empiricism and supporting Roman Catholic doctrines through secular study, reason, and logic. Other notable scholastics (\"schoolmen\") included Roscelin and Peter Lombard. One of the main questions during this time was the problem of the universals. Prominent non-scholastics of the time included Anselm of Canterbury, Peter Damian, Bernard of Clairvaux, and the Victorines. The most famous of the scholastic practitioners was Thomas Aquinas (later declared a Doctor of the Church), who led the move away from the Platonic and Augustinian and towards the Aristotelian. \n\nDuring the High Middle Ages in Europe, there was increased innovation in means of production, leading to economic growth. These innovations included the windmill, manufacturing of paper, the spinning wheel, the magnetic compass, eyeglasses, the astrolabe, and Hindu-Arabic numerals.\n"}
{"id": "215106", "url": "https://en.wikipedia.org/wiki?curid=215106", "title": "Michael Wood (historian)", "text": "Michael Wood (historian)\n\nMichael David Wood (born 23 July 1948) is an English historian and broadcaster. He has presented numerous well-known television documentary series from the late 1970s right up to the present day. Wood has also written a number of books on English history, including \"In Search of the Dark Ages\", \"The Domesday Quest\", \"The Story of England\", and \"In Search of Shakespeare\". He was appointed Professor of Public History at the University of Manchester in 2013.\n\nWood was born in Moss Side, Manchester. He attended Heald Place Primary School in Rusholme. When he was eight, his family moved to Wythenshawe where he went to Benchill Primary School. At Manchester Grammar School, he developed an interest in theatre, playing Grusha in the first British amateur production of Brecht's \"The Caucasian Chalk Circle\". He took A-levels in English, French and History.\n\nWood studied history and English at Oriel College, Oxford, touring the United States for six weeks in his final year, and graduated with a second class Bachelor of Arts degree. Later, he undertook post-graduate research in Anglo-Saxon history at Oriel. Three years into his research for a DPhil, he left to become a journalist with ITV.\n\nIn the 1970s Wood worked for the BBC in Manchester. He was first a reporter and then an assistant producer on current affairs programmes, before returning to his love of history with his 1979–81 series \"In Search of the Dark Ages\" for BBC2. He quickly became popular with female viewers for his blond good looks (he was humorously dubbed \"the thinking woman's crumpet\" by British newspapers), his deep voice, and his habit of wearing tight jeans and a sheepskin jacket. Wood's work is also well known in the United States, where it receives much airplay on PBS and on various cable television networks. The series \"Legacy\" (1992) is one of his more frequently broadcast documentaries on US television.\n\nSince 1990, Michael Wood has been a director of independent television production company Maya Vision International. In 2006 he joined the British School of Archaeology in Iraq campaign, which aimed to train and encourage new Iraqi archaeologists, and he has lectured on the subject. In 2013, Wood joined the University of Manchester as Professor of Public History.\n\nHis partner for ten years, in the late 1970s and early 1980s, was journalist and broadcaster Pattie Coldwell. He currently lives in north London with his wife, television producer Rebecca Ysabel Dobbs and two daughters, Minakshi and Jyoti.\n\nWood was a Fellow of the Royal Historical Society until 2007. In 2009, he was awarded an Honorary Doctor of Arts by Sunderland University. This was followed by an honorary Doctorate of Letters from the University of Leicester in 2011, and in 2015 he was awarded the President's Medal by the British Academy. Having previously acted as President of the Leicestershire Archaeological and Historical Society, in 2017 he accepted the position of Honorary Life Vice President, offered in recognition of his work on the documentary series Michael Wood's Story of England.\n\n\n\n\n\n"}
{"id": "46426078", "url": "https://en.wikipedia.org/wiki?curid=46426078", "title": "Milad Doueihi", "text": "Milad Doueihi\n\nMilad Doueihi (born 1959) is an American cultural and intellectual historian. He is Professor of Digital Humanities at Paris-Sorbonne University.\n\nDoueihi was Directeur d'Études Associé at the École des Hautes Études en Sciences Sociales in Paris. In 2000 he was a Prize Fellow at the American Academy in Berlin.\n\n\n"}
{"id": "49885462", "url": "https://en.wikipedia.org/wiki?curid=49885462", "title": "Modern French shield", "text": "Modern French shield\n\nThe shield samnitic, also called Modern French shield, appears in the 16th c. is a shield of rectangular shape whose lower corners are rounded as arcs of a circle with a radius of half module. According to some authors it is normally 8 modules high and 7 wide, as the tournament shield, while others report a height of 9 modules and width of 7. The center of the lower side is provided with a tip formed by two arcs of a circle of radius also of the middle module. This shield is also frequently used in heraldry Spanish along with the Spanish shield: some authors, such as the Marquis of Avilés, reported the proportion of a height of 6 modules to 5 modules wide for standard Spanish. The Samnite shield has also been adopted as the official support civic heraldry and Italian military and in this case its official size is 9 × 7.\n\n"}
{"id": "57443291", "url": "https://en.wikipedia.org/wiki?curid=57443291", "title": "Oxford Handbook of Latin American History", "text": "Oxford Handbook of Latin American History\n\nThe Oxford Handbook of Latin American History is a reference work, primarily of historiography, with narrative discussions of publications on particular topics with select bibliography. Essays analyze the recent historiography, periodization, themes and trends in the field. Essays are by region and theme.   The articles include treatment of both Spanish America and Brazil.  They include: “Historiography of New Spain” Kevin Terraciano and Lisa Sousa; “Colonial Spanish South America”, Lyman L. Johnson and Susan M. Socolow; “The Historiography of Early Modern Brazil”, Stuart B. Schwartz; “Sexuality in Colonial Spanish America”, Asunción Lavrin; “Independence in Latin America,” Jeremy Adelman; “Slavery in Brazil,” João Reis and Herbert S. Klein; “Slavery in Brazil,” Barbara Weinstein; “Race in Post-abolition Afro-Latin America,” Kim D. Butler and Aline Helg; “Indigenous Peoples and Nation-States in Spanish America, 1780-2000,” Florencia Mallon; “Rural History,” Eric Van Young, “Latin American Labor History,” James P. Brennan; “Gender and Sexuality in Latin America,” Donna J. Guy; “The Historiography of Latin American Families,” Nara Milanich; “The New Economic History of Latin America: Evolution and Recent Contributions,” John H. Coatsworth and William R. Summerhill; “Disease, Medicine, and Health,” Diego Armus and Adrián López Denis; and “Popular Religion in Latin American Histoririography,” Reinaldo L. Román and Pamela Voekel.\n\nThe work has been favorably reviewed in a number of scholarly journals.\n"}
{"id": "501400", "url": "https://en.wikipedia.org/wiki?curid=501400", "title": "Patani", "text": "Patani\n\nPatani (in Malay (derived from Jawi: ڤتاني), also sometimes Patani Raya, or \"Greater Patani\") is a historical region in the northern part of the Malay peninsula. It includes the southern Thai provinces of Pattani, Yala (Jala), Narathiwat (Menara), and parts of Songkhla (Singgora).\n\nThe Patani region has historical affinities with the Singgora (Songkhla), Ligor (Nakhon Si Thammarat), and Lingga (near Surat Thani) sultanates dating back to the time when the Patani Kingdom was a semi-independent Malay sultanate paying tribute to the Siamese kingdoms of Sukhothai and Ayutthaya. After Ayutthaya fell to the Burmese in 1767, the Sultanate of Patani gained full independence, but under King Rama I, it again came under Siam's control.\n\nIn recent years a secessionist movement has sought the establishment of a Malay Islamic state, Patani Darussalam, encompassing the three southern Thai provinces. This campaign has taken a particularly violent turn after 2001, resulting in an intractable insurgency problem across southern Thailand and the imposition of martial law.\n\nIn Thailand's southernmost provinces, \"Patani\" has become a controversial term used to refer to the area encompassing the provinces of Pattani (with two t's), Yala, Narathiwat, and parts of Songkhla, mostly inhabited by Malay Muslims. When written in Thai, \"Patani\"\n(ปาตานี) \"paa-ta-ni\" sounds markedly different from \"Pattani\" (ปัตตานี) \"pat-ta-ni\". Malays say P’tani (ปตานี), pronounced as \"pa-ta-ni\" with a very brief first syllable and stress on the second syllable. \"P'tani\", the original Malay word for the region, has been used for a long time and is usually never written in Thai. So while technically being the same word as \"P'tani\", \"Patani\" has a certain newness and a separatist political connotation.\n\nFrom the cultural point of view the term \"Patani\" may refer to the territories of the historical Sultanate of Patani, as well as to the wider areas that were once under its rule.\n\nThe inhabitants of the Patani region have been traditionally part of the Malay culture, having a historical background in which Islam has constituted a major influence.\n\nThe Patani people speak a form of the Malay language locally known as Jawi. Patani had a complex and distinct culture that included a rich oral literature, rice harvest ceremonies, colourful paintings on the hulls of \"Korlae\" boats, and the performances of a kind of \"Wayang\" theatre. Living in a borderland at the northern end of the Malay peninsula, over the centuries the Patani people adapted themselves to a life of harmony with the local Chinese, Buddhist, Indian, Arab and Orang Asli communities.\n\nDespite the ethnic affinity of the Patani with their Malay neighbours to the south, The Patani Kingdom was led by sultans who historically preferred to pay tribute to the distant Siamese kings in Bangkok. For many centuries the King of Siam restricted himself to exacting a periodic tribute in the form of Bunga mas, ritual trees with gold leaves and flowers that were a symbolic acknowledgment of Siamese suzerainty, leaving the Patani rulers largely alone.\n\nUntil well into the 20th century, the government in Bangkok had relied on local officials in the implementation of policies within the Patani region, including the exemption in implementing Thai Civil Law, which had allowed Muslims to continue their observance of local laws based on Islam regarding issues on inheritance and family. However, by 1934 Marshall Plaek Phibunsongkhram set in motion of a process of Thaification which had as its objective the cultural assimilation of the Patani people, among other ethnic groups in Thailand.\n\nThe National Culture Act was enforced as a result of the Thaification process, promoting the concept of 'Thai-ness' and its centralist aims. Its \"Mandate 3\" was directly aimed at the Patani people. By 1944, Thai civil law was enforced throughout the land including the Patani region over-riding the earlier concessions to local Islamic administrative practices. The school curriculum was revised to that of a Thai-centric one with all lessons in the Thai language. Traditional Muslim courts that were used to handle civil cases were removed and replaced with civil courts run and approved by the central government in Bangkok. This forced assimilation process and the perceived imposition of Thai-Buddhist cultural practices upon their society became an irritant for the harmonious relationship of the ethnic Malay Patani people and the Thai state.\n\nDenied recognition as a culturally separate ethnic minority, Patani leaders reacted against the Thai government policy towards them and a nationalist movement began to grow, leading to the South Thailand insurgency. Initially the goal of the nationalist movement such as the Patani United Liberation Organisation (PULO) was secession, pursuing an armed struggle towards an independent state where Patani people could live with dignity without having alien cultural values imposed on them.\n\nAfter 2001, the Patani insurgency was taken over by groups whose leaders are mainly Salafist religious teachers who have promoted religion, rejecting the nation-building ideology of the early secessionist movements.\nCurrent insurgent groups proclaim militant jihadism and are not separatist any more. They have extreme and transnational religious goals, such as an Islamic Caliphate, to the detriment of a constructive cultural or nationalistic Patani identity. Salafi-based groups are hostile to the heritage and practices of traditional Malay Muslims, accusing them of being un-Islamic. They are not concerned about Patani cultural values, instead their immediate aim is to make the Patani region ungovernable.\n\nSo far, and in the present circumstances, to preserve an identity free of the influence of Militant Islam has been next to impossible for the people of the hapless Patani Region. The activity of the present-day insurgents has changed the face of Patani society by the imposition of extreme religious undercurrents and the enforcement of the stern Salafi rules on local people.\n\nThe area was home to the Hindu-Buddhist kingdom of Langkasuka as early as the second century, as accounts from Chinese travellers attest. Langkasuka reached its peak in the sixth and seventh centuries, and then declined as a major trade center. Pattani subsequently became part of the Hindu-Buddhist Empire of Srivijaya, a maritime confederation based in Palembang, which spanned the seventh to the thirteenth centuries. Regional influence during these early centuries also came from the developing Khmer, Siamese and Malay cultures.\n\nThe founding of the Islamic kingdom of Patani is thought to have been around the mid-13th century, with folklore suggesting it was named after an exclamation made by Sultan Ismail Shah, \"Pantai Ini!\" (\"This beach\" in the local Malay language). However, some think it was the same country known to the Chinese as Pan Pan.\n\nPatani came under Thai rule briefly during the Sukhothai period, and more extensively during the later Ayuthaya period.\n\nIn 1791 and 1808, there were rebellions within Pattani against Thai rule, following which Pattani was divided into 7 largely autonomous states (\"Mueang\"): Pattani, Nongchik, Saiburi (Teluban), Yala (Jala), Yaring (Jambu), Ra-ngae (Legeh) and Raman. All were ruled by the King of Ligor.\n\nIn 1909 Great Britain and Thailand signed the Bangkok Treaty of 1909. The British recognised Thailand's sovereignty over Pattani, and, in return, Thailand gave up the kingdoms of Kedah, Kelantan, Perlis and Terengganu to the British. All seven mueang were reunited into a monthon and incorporated into the kingdom. Later, the central government in Bangkok renamed certain localities with Thai versions of their names and merged some of the mueang.\n\nWhen the monthon system was dissolved in 1933, three provinces remained: Pattani, Yala and Narathiwat.\n\nOn 8 December 1941, during the Second World War, the Japanese invaded Thailand, and crossed Pattani to invade British Malaya. The Thai government, led by Marshall Plaek Phibunsongkhram became an active ally of Japan by promising to help Thailand retake some territorial claims back from the British and the French. This included Sirat Malai, the former Malay dependencies of Kelantan, Trengganu, Kedah, and Perlis. It is arguable that this move not only gave more territory to the Thai state but on the contrary, it strengthened the old Malay ties between the Pattani region and the northern Malayan peninsula states.\n\nTengku Mahmud Mahyuddin, a prominent Pattani leader and the son of the last Raja of Pattani, allied himself with the British in the hopes that Pattani would be granted independence after an Allied victory. His main support came from ethnic Malays displeased by the nationalistic policies of the Phibun regime, which were seen by the southern Malays as forcing them to give up their own language and culture and the economic hardship that ensued as a result of alleged mismanagement. According to Ockey, even leading Thai politicians such as Pridi Phanomyong, Seni Pramot were among those that \"overtly or covertly\" supported this resistance against the Japanese. During this time the electoral seats for in this region were mainly held by non-Muslim representatives except Satun. Mahyuddin assisted the British by launching guerrilla attacks against the Japanese. In 1945, a petition by Malay leaders led by Tengku Abdul Jalal demanded that Britain guarantee independence for the southernmost provinces of Thailand. At the war's end, the Greater Malay Pattani State (Negara Melayu Patani Raya) flag did fly briefly in Pattani. However, since the British had no power over Thailand, the Thai continued to rule over Pattani, while the British kept Thailand stable as a counterweight to the communist insurgency in Malaya. This led to the formation of several insurgent groups seeking the independence of Pattani.\n\nAfter World War 2 had ended, the US had wanted to treat Thailand as an ally because of the resistance movement against the Japanese during the war but the British on the other hand had wanted to treat it as a defeated enemy. With this notion in the balance, the newly elected government led by Pridi had to address the issue of the South. With the aid of advisor Chaem Phromyong, a Muslim, the policies of accommodation and integration of the South. With the closure of the war, the government also approved the Patronage of Islam Act. This Act recognised the work and role of religious figures in the South and gave them authority once more in the affairs of the Muslims in that region. Coincidentally, this Act also paved the way for Haji Sulong to become the president of the Pattani Islamic Committee in 1945. It was from this appointment that Sulong began to take an interest in the restoration of the Islamic courts which had earlier been abolished by the Phibun regime. Ockey had pointed out that Sulong was not entirely pleased with the restoration process because the authority in the Islamic courts was still preceded by the presence of a judge from the Ministry of Justice alongside the Muslim judge in cases. However, the negotiations over this supposed unhappiness was in the form of meetings to discuss terms and not open confrontation. This peaceful attempts at resolution would all come to an end in November 1947 when Pridi was forced out of power by opposing army personnel.\n\nDuring World War II, along with the Greater Patani Malay Movement led by Tengku Mahmud Mahyuddin, another resistance force under the leadership of Islamic scholar Haji Sulong Tokmina also fought against the Japanese. Their stated goal was to create an Islamic republic in Patani, which frequently put it at odds with Prince Tengku Mahmud, who wanted to reestablish the Pattanese Sultanate.\n\nHaji Sulong had emerged at a time when the region of Pattani was in need of new political direction. His appearance supposedly gave new light to the nationalistic intents of the Malays in the region based on Islamic principles. Haji Sulong was born in 1895 to a family in Kampong Anak Rhu. He had completed his Islamic studies locally before being sent by his father to further education in Mecca, Saudi Arabia. There in Mecca, he apparently met up with and studied with famous Islamic scholars and teachers. He even opened a school in Mecca and had students from all over the world study under him. Eventually he married and settled in Mecca. During this period, there was a wave of nationalism sweeping the world in the earlier part of the 20th century and Sulong himself was exposed to Arab nationalism. Sulong's return to Pattani happened almost by chance due to the death of his first infant son to alleviate the grief of his family.\n\nUpon his return, he had apparently looked upon the plight of the Pattani region being a distant shadow of its former glory as the 'cradle of Islam' in Southeast Asia. According to Thanet, the Thai historian, this notion set him into action. From teaching in a khru (small Muslim village school), he eventually opened up a pondok to spread his teachings due to his popularity amongst villagers. Even Pridi Phanomyong visited Haji Sulong at his school and it soon became the most popular Islamic school in Thailand. According to Thanet, the rise of Islamic nationalism in the South is not only attributed to Haji Sulong himself but also the religious students who 'were inclined towards modernism' from the northern states in Malaya. Haji Sulong was an ulama who openly distrusted the government's involvement in the religious affairs of the community. His conviction in the matter stemmed from his ideal that a community cannot be established in the south as long as it is solely under Thai rule.\n\nOn 3 April 1947, a commission of inquiry by the Bangkok government was sent to the four Muslim states in the South to check on the apparent plight of the Muslims living there. It was during this time that the Provincial Islamic Council of Pattani drafted out a seven-point demand to the central government in Bangkok which sought for the betterment of Muslims in the region. According to Thanet, the commission of inquiry pointed to Haji Sulong as the leader behind the conceptualisation of this demands. Prime Minister Thamrong brought the seven point demand for consideration with his cabinet and then decided that they could not be met because he felt that the existing structure of the Bangkok government was still (then) adequate to govern the Pattani region. He concluded that an exclusive reorganisation of the governmental structure in the area to suit the people there would then \"divide the country\".\n\nBy the late part of 1947, Haji Sulong and his supporters realised that their efforts with the government to negotiate better terms for the Muslims in the south was not working. They then decided to adopt a policy of non-co-operation and this included the boycott of the January 1948 elections. Moreover, at this time, the Pridi government had been ousted in a coup by the military which meant a return to the old rigid ways towards the southern states. During this period, Haji Sulong and his associates were arrested and charged for treason. This was a move purported by Phibun even though he was not back yet in power according to Thanet's research (Phibun returned to power a few months after in April 1948). This arrests were soon followed by clashes between Muslim villagers and the police/military forces at Duson Nyior. This event was known was the Dusun Nyior Incident which was led by a religious leader by the name of Haji Abdul Rahman. The ensuing violence involved over 1000 men in open battle and led to the deaths of an estimated 400 Malay Muslim peasants and 30 policemen. After the clashes had been settled, there was an exodus of Malays across the border to Malaya. There was even a petition by the Pattani Muslims to the United Nations to step in to broker the separation of the southern states to join the Federation of Malaya.\n\nWhile on trial, the court provided many descriptions of the alleged activities that Haji Sulong had carried out to incite unrest amongst the people. This include accusations that state that he held meetings which invoked \"arousing rebellious feelings among the people almost to the point of creating unrest in the kingdom\". According to Thanet, the most serious accusation was that he incited the people to seek for self-government with the intention of inviting Tengku Mahmud Mayuddin, who was the son of the last raja of Pattani, to preside as leader over the four southern provinces. On top of this, the prosecution also stated that Haji Sulong had recommended that Islamic law be implemented and if the government did not comply to the said demands, Haji Sulong would then urge the Malay population to make their complaints heard until the government did something about it. Haji Sulong was convicted and spent time in jail until 1952 and after his release, he was ordered by the government to give up his public activities and subject himself to checks by the police on demand. The prosecutor in that trial wanted to press for a heavier sentence, citing evidence that Haji Sulong was planning something bigger to the scale of a rebellion. Originally as a result of the state's prosecution citing these evidences, the sentence had been longer but due to the willingness of Haji Sulong to co-operate in the investigations, the court reduced the sentence to four years and eight months. In 1954, Haji Sulong, his eldest son Ahmad and some of his associates were told to report to a police station at Songkhla. Upon leaving to report to the station, they were never seen or heard from again. After his disappearance, his family inquired about his whereabouts but never got any answers. In 1957, his son Ameen Tokmina ran for parliament in the elections in the hope that a position in the government could help him look for more information.\n\nToday, the goals and ideas of Haji Sulong Tokmina are still carried on by minor resistance groups interested in creating an Islamic republic. After the war, though, British and Thai policies essentially removed the possibility of an independent republic in Pattani. The British originally had the intention to deliver on their promise to the anti-Japanese Malay leaders in South Thailand that they would either create an independent territory out of the Malay-based southern Thai states or incorporate them in British Malaya as a punishment of Thailand for aiding the Japanese conquest of Malaya and profiting from it. However, they were dissuaded by the USA who needed Thai rice to feed the war-ravaged region and Thai friendship to face up to the possibility of China's fall to the Chinese communist army.\n\nPatani separatist groups, most notably the Barisan Revolusi Nasional-Koordinasi (BRN-C), began to use increasingly violent tactics after 2001. There have been suggestions of links between the BRN-C and foreign Islamist groups, such as Jemaah Islamiyah, however the strength of these relationships remains unknown.\n\n\n"}
{"id": "51335261", "url": "https://en.wikipedia.org/wiki?curid=51335261", "title": "Pierre-Nicolas Chantreau", "text": "Pierre-Nicolas Chantreau\n\nPierre Nicolas Chantreau, called don Chantreau, (1741, Paris – 25 October 1808, Auch) was an 18th-century French historian, journalist, grammarian and lexicographer.\n\nAround 1762, at the age of twenty or twenty-one, he traveled to Spain to become a teacher of French at the Royal School of Ávila. He published a French grammar for use by Hispanics which earned him to enter the Real Academia Española and receive the title of \"don\" Chantreau. Back in France in 1782, he joined the revolutionary ideas and became an employee at the libraries section of the . In 1792, he was appointed responsible for an investigation to the Spanish border, secret mission whose purpose was to ensure the feelings of Catalans to the French Revolution. In 1794,he proposed the departmental director of Gers the creation of an educational newspaper \"Les Documents de la raison, feuille antifanatique\", then wrote the \"Courrier du département du Gers\". He was later a teacher of history at the école centrale in Auch in 1796, then at the l'École militaire, then based in Fontainebleau, in 1803\n\nWhile his historical charts and chronologies quickly fell into oblivion, his lexicon of the words of the Revolution inspired Louis-Sébastien Mercier a \"Néologie ou vocabulaire de mots nouveaux\" and his French grammar, of which several editions followed one another until 1926, was a milestone in the history of language teaching in Spain.\n\n\n"}
{"id": "536059", "url": "https://en.wikipedia.org/wiki?curid=536059", "title": "Qanat", "text": "Qanat\n\nA qanāt () is a gently sloping underground channel to transport water from an aquifer or water well to surface for irrigation and drinking. This is an old system of water supply from a deep well with a series of vertical access shafts. The qanats still create a reliable supply of water for human settlements and irrigation in hot, arid, and semi-arid climates. The qanat technology was developed in ancient Iran by the Persian people sometime in the early 1st millennium BC, and spread from there slowly westward and eastward. This view is disputed by Tikrit, who argues a South-East Arabian origin for the technology. A pre-Archemaed Empire Arabian origin is also argued by the Underground Aqueducts Handbook. \n\nThe Archemaed era Qanats of Gonabad is one of the oldest and largest qanats in the world built between 700 BC to 500 BC, and is still in use today.\n\nCommon variants of \"qanat\" in English include \"kanat\", \"khanat\", \"kunut\", \"kona\", \"konait\", \"ghanat\", \"ghundat\".\n\n\"Qanāt\" () is the Arabicized of Persian \"Kanāt\", and literally means \"channel\". In Persian, the words for \"qanat\" are kārīz (or kārēz; ), and is derived from earlier word \"kāhrēz\" (). The word \"qanāt\" () is also used in Persian. Other names for qanat include \"kahan\" (), \"Kahn\" (Balochi), \"kahriz/kəhriz\" (Azerbaijan); \"khettara\" (Morocco); \"Canal\" (Spain); \"falaj\" () (United Arab Emirates and Oman), \"foggara/fughara\" (North Africa). Alternative terms for qanats in Asia and North Africa are \"kakuriz\", \"chin-avulz\", and \"mayun\".\n\nCotton is indigenous to South Asia and has been cultivated in India for a long time. Cotton appears in the \"Inquiry into Plants\" by Theophrastus and is mentioned in the Laws of Manu. As transregional trade networks expanded and intensified, cotton spread from its homeland to India and into the Middle East where it devastated the agricultural systems already in place there. Much of Persia was initially too hot for the crop to be cultivated; to solve that problem, the qanat was developed first in modern-day Iran, where it doubled the amount of available water for irrigation and urban use. Because of this, Persia enjoyed larger surpluses of agriculture thus increasing urbanization and social stratification. The qanat technology subsequently spread from Persia westward and eastward.\n\nThe original ancient engineered design of the Qanat and its multiple aligned bore-holes are thought to have controlled desert endorheic basin flooding without destroying the salt mirror playa or causing erosion of the flat evaporation fields. The Qanat water was primarily needed to extract salt, rather than for simple domestic irrigation. Additionally considerable quantities of subsoil brines existing in such basin water tables would ensure brine supplies, as is demonstrated by the new potash plants in the Tarim basin using the ancient Qanat technology.\nThe surface crust of an inland Sabkha endorheic basin typically is made up of layers of salts that have re-crystallized and settled or precipitated during the evaporation process of controlled Qanat system flood waters. Leached Salts dissolve quickly in a desert endorheic basin, and over a short intensely hot period, the process of re-crystallizing the salts can produce purer and more concentrated, layered playa cakes. The dissolved salts leached out of the underlying layers in such vast desert basin flats, are intermittently precipitated back onto the basin surface, predominantly sodium chloride crystals, one after the other. It is thought that the many Qanat systems in the Taklamakan desert basin (Tarim basin) were primarily built to produce and trade salt along the Silk Road. The position of the Silk Road skirting these endorheic basins may well have resulted due to efficient and pure salt leaching technique still producing salt cake crust in similar deserts.\n\nQanats are constructed as a series of well-like vertical shafts, connected by gently sloping tunnels. Qanats efficiently deliver large amounts of subterranean water to the surface without need for pumping. The water drains by gravity, typically from an upland aquifer, with the destination lower than the source. Qanats allow water to be transported over long distances in hot dry climates without much water loss to evaporation.\n\nThe qanat should not be confused with the spring-flow tunnel typical to the mountainous area around Jerusalem. Although both are excavated tunnels designed to extract water by gravity flow, there are crucial differences. Firstly, the origin of the qanat was a well that was turned into an artificial spring. In contrast, the origin of the spring-flow tunnel was the development of a natural spring to renew or increase flow following a recession of the water table. Secondly, the shafts essential for the construction of qanats are not essential to spring-flow tunnels.\n\nIt is very common for a qanat to start below the foothills of mountains, where the water table is closest to the surface. From this source, the qanat tunnel slopes gently downward, slowly converging with the steeper slope of the land surface above, and the water finally flows out above ground where the two levels meet. To connect a populated or agricultural area with an aquifer, qanats must often extend for long distances.\n\nQanats are sometimes split into an underground distribution network of smaller canals called kariz. Like qanats, these smaller canals are below ground to avoid contamination and evaporation. In some cases water from a qanat is stored in a reservoir, typically with night flow stored for daytime use. An ab anbar is an example of a traditional Persian qanat-fed reservoir for drinking water.\n\nThe qanat system has the advantage of being resistant to natural disasters such as earthquakes and floods, and to deliberate destruction in war. Furthermore, it is almost insensitive to the levels of precipitation, delivering a flow with only gradual variations from wet to dry years. From a sustainability perspective, qanats are powered only by gravity, and thus have low operation & maintenance costs once built. Qanats transfer freshwater from the mountain plateau to the lower-lying plains with saltier soil. This helps to control soil salinity and prevent desertification.\n\nThe value of the qanat is directly related to the quality, volume, and regularity of the water flow. Much of the population of Iran and other arid countries in Asia and North Africa historically depended upon the water from qanats; the areas of population corresponded closely to the areas where qanats are possible. Although a qanat was expensive to construct, its long-term value to the community, and thereby to the group that invested in building and maintaining it, was substantial.\n\nThe qanat technology is used most extensively in areas with the following characteristics: \n\n\nA typical town or city in Iran, and elsewhere where the qanat is used, has more than one qanat. Fields and gardens are located both over the qanats a short distance before they emerge from the ground and below the surface outlet. Water from the qanats defines both the social regions in the city and the layout of the city.\n\nThe water is freshest, cleanest, and coolest in the upper reaches and more prosperous people live at the outlet or immediately upstream of the outlet. When the qanat is still below ground, the water is drawn to the surface via water wells or animal driven Persian wells. Private subterranean reservoirs could supply houses and buildings for domestic use and garden irrigation as well. Further, air flow from the qanat is used to cool an underground summer room (shabestan) found in many older houses and buildings.\n\nDownstream of the outlet, the water runs through surface canals called jubs (\"jūbs\") which run downhill, with lateral branches to carry water to the neighborhood, gardens and fields. The streets normally parallel the jubs and their lateral branches. As a result, the cities and towns are oriented consistent with the gradient of the land; this is a practical response to efficient water distribution over varying terrain.\n\nThe lower reaches of the canals are less desirable for both residences and agriculture. The water grows progressively more polluted as it passes downstream. In dry years the lower reaches are the most likely to see substantial reductions in flow.\n\nTraditionally qanats are built by a group of skilled laborers, \"muqannīs\", with hand labor. The profession historically paid well and was typically handed down from father to son.\n\nThe critical, initial step in qanat construction is identification of an appropriate water source. The search begins at the point where the alluvial fan meets the mountains or foothills; water is more abundant in the mountains because of orographic lifting and excavation in the alluvial fan is relatively easy. The \"muqannīs\" follow the track of the main water courses coming from the mountains or foothills to identify evidence of subsurface water such as deep-rooted vegetation or seasonal seeps. A trial well is then dug to determine the location of the water table and determine whether a sufficient flow is available to justify construction. If these prerequisites are met, the route is laid out aboveground.\n\nEquipment must be assembled. The equipment is straightforward: containers (usually leather bags), ropes, reels to raise the container to the surface at the shaft head, hatchets and shovels for excavation, lights, spirit levels or plumb bobs and string. Depending upon the soil type, qanat liners (usually fired clay hoops) may also be required.\n\nAlthough the construction methods are simple, the construction of a qanat requires a detailed understanding of subterranean geology and a degree of engineering sophistication. The gradient of the qanat must be carefully controlled: too shallow a gradient yields no flow and too steep a gradient will result in excessive erosion, collapsing the qanat. And misreading the soil conditions leads to collapses, which at best require extensive rework and at worst are fatal for the crew.\n\nConstruction of a qanat is usually performed by a crew of 3–4 \"muqannīs\". For a shallow qanat, one worker typically digs the horizontal shaft, one raises the excavated earth from the shaft and one distributes the excavated earth at the top.\n\nThe crew typically begins from the destination to which the water will be delivered into the soil and works toward the source (the test well). Vertical shafts are excavated along the route, separated at a distance of 20–35 m. The separation of the shafts is a balance between the amount of work required to excavate them and the amount of effort required to excavate the space between them, as well as the ultimate maintenance effort. In general, the shallower the qanat, the closer the vertical shafts. If the qanat is long, excavation may begin from both ends at once. Tributary channels are sometimes also constructed to supplement the water flow.\n\nMost qanats in Iran run less than 5 km, while some have been measured at ~70 km in length near Kerman. The vertical shafts usually range from 20 to 200 meters in depth, although qanats in the province of Khorasan have been recorded with vertical shafts of up to 275 m. The vertical shafts support construction and maintenance of the underground channel as well as air interchange. Deep shafts require intermediate platforms to simplify the process of removing soil.\n\nThe construction speed depends on the depth and nature of the ground. If the earth is soft and easy to work, at 20 meters depth a crew of four workers can excavate a horizontal length of 40 meters per day. When the vertical shaft reaches 40 meters, they can excavate only 20 meters horizontally per day and at 60 meters in depth this drops below 5 horizontal meters per day. In Algeria, a common speed is just 2 m per day at 15 m depth. Deep, long qanats (which many are) require years and even decades to construct.\n\nThe excavated material is usually transported by means of leather bags up the vertical shafts. It is mounded around the vertical shaft exit, providing a barrier that prevents windblown or rain driven debris from entering the shafts. These mounds may be covered to provide further protection to the qanat. From the air, these shafts look like a string of bomb craters.\n\nThe qanat's water-carrying channel must have a sufficient downward slope that water flows easily. However the downward gradient must not be so great as to create conditions under which the water transitions between supercritical and subcritical flow; if this occurs, the waves that result can result in severe erosion that can damage or destroy the qanat. The choice of the slope is a trade off between erosion and sedimentation. Highly sloped tunnels are subject to more erosion as water flows at a higher speed. On the other hand, less sloped tunnels need frequent\nmaintenance due to the problem of sedimentation. A lower downward gradient also contributes to reducing the solid contents and contamination in water. In shorter qanats the downward gradient varies between 1:1000 and 1:1500, while in longer qanats it may be almost horizontal. Such precision is routinely obtained with a spirit level and string.\n\nIn cases where the gradient is steeper, underground waterfalls may be constructed with appropriate design features (usually linings) to absorb the energy with minimal erosion. In some cases the water power has been harnessed to drive underground mills. If it is not possible to bring the outlet of the qanat out near the settlement, it is necessary to run a \"jub\" or canal overground. This is avoided when possible to limit pollution, warming and water loss due to evaporation.\n\nThe vertical shafts may be covered to minimize blown-in sand. The channels of qanats must be periodically inspected for erosion or cave-ins, cleaned of sand and mud and otherwise repaired. For safety, air flow must be assured before entry.\n\nSome damaged qanats have been restored. To be sustainable, restoration needs to take into account many nontechnical factors beginning with the process of selecting the qanat to be restored. In Syria, three sites were chosen based on a national inventory conducted in 2001. One of them, the Drasiah qanat of Dmeir, was completed in 2002. Selection criteria included the availability of a steady groundwater flow, social cohesion and willingness to contribute of the community using the qanat, and the existence of a functioning water-rights system.\n\nThe primary applications of qanats are for irrigation, providing cattle with water, and drinking water supply. Other applications include cooling and ice storage.\n\nQanats used in conjunction with a wind tower can provide cooling as well as a water supply. A wind tower is a chimney-like structure positioned above the house; of its four openings, the one opposite the wind direction is opened to move air out of the house. Incoming air is pulled from a qanat below the house. The air flow across the vertical shaft opening creates a lower pressure (see Bernoulli effect) and draws cool air up from the qanat tunnel, mixing with it. The air from the qanat is drawn into the tunnel at some distance away and is cooled both by contact with the cool tunnel walls/water and by the transfer of latent heat of evaporation as water evaporates into the air stream. In dry desert climates this can result in a greater than 15 °C reduction in the air temperature coming from the qanat; the mixed air still feels dry, so the basement is cool and only comfortably moist (not damp). Wind tower and qanat cooling have been used in desert climates for over 1000 years.\n\nBy 400 BC, Persian engineers had mastered the technique of storing ice in the middle of summer in the desert.\n\nThe ice could be brought in during the winters from nearby mountains. But in a more usual and sophisticated method they built a wall in the east–west direction near the yakhchal (ice pit). In winter, the qanat water would be channeled to the north side of the wall, whose shade made the water freeze more quickly, increasing the ice formed per winter day. Then the ice was stored in yakhchals—specially designed, naturally cooled refrigerators. A large underground space with thick insulated walls was connected to a qanat, and a system of windcatchers or wind towers was used to draw cool subterranean air up from the qanat to maintain temperatures inside the space at low levels, even during hot summer days. As a result, the ice melted slowly and was available year-round.\n\nThe Qanats are called Kariz in Dari (Persian) and Pashto and have been in use since the pre-Islamic period. It is estimated that more than 20,000 Karizes were in use in the 20th century. The oldest \"functional\" Kariz which is more than 300 years old and 8 kilometers long is located in Wardak province and is still providing water to nearly 3000 people. The incessant war for the last 30 years has destroyed a number of these ancient structures. In these troubled times maintenance has not always been possible. To add to the troubles, as of 2008 the cost of labour has become very high and maintaining the Kariz structures is no longer possible. Lack of skilled artisans who have the traditional knowledge also poses difficulties. A number of the large farmers are abandoning their Kariz which has been in their families sometimes for centuries, and moving to tube and dug wells backed by diesel pumps.\n\nHowever, the government of Afghanistan is aware of the importance of these structures and all efforts are being made to repair, reconstruct and maintain (through the community) the kariz. The Ministry of Rural Rehabilitation and Development along with National and International NGOs is making the effort.\n\nThere are still functional qanat systems in 2009. American forces are reported to have unintentionally destroyed some of the channels during expansion of a military base, creating tensions between them and the local community. Some of these tunnels have been used to store supplies, and to move men and equipment underground.\n\nQanats have been preserved in Armenia in the community of Shvanidzor, in the southern province of Syunik, bordering with Iran. Qanats are named \"kahrezes\" in Armenian. There are 5 kahrezes in Shvanidzor. Four of them were constructed in XII-XIVc, even before the village was founded. The fifth kahrez was constructed in 2005. Potable water runs through I, II and V kahrezs. Kahrez III and IV are in quite poor condition. In the summer, especially in July and August, the amount of water reaches its minimum, creating a critical situation in the water supply system. Still, kahrezes are the main source of potable and irrigation water for the community.\n\nThe territory of Azerbaijan was home to numerous kahrizes many centuries ago. Archaeological findings suggest that long before the ninth century AD, kahrizes by which the inhabitants brought potable and irrigation water to their settlements were in use in Azerbaijan. Traditionally, kahrizes were built and maintained by a group of masons called ‘Kankans’ with manual labour. The profession was handed down from father to son.\n\nIt is estimated that until the 20th century, nearly 1500 kahrizes, of which as many as 400 were in the Nakhichevan Autonomous Republic, existed in Azerbaijan.\nHowever, following the introduction of electric and fuel-pumped wells during Soviet times, kahrizes were neglected.\n\nToday, it is estimated that 800 are still functioning in Azerbaijan. These operational kahrizes are key to the life of many communities.\n\nIn 1999, upon the request of the communities in Nakhichevan, taking into consideration the needs and priorities of the communities, especially women as the main beneficiaries, IOM began implementing a pilot programme to rehabilitate the kahrizes.\nBy 2018 IOM rehabilitated more than 163 kahrizes with funds from the United Nations Development Programme (UNDP), European Commission (EC), Canadian International Development Agency (CIDA), Swiss Agency for Development and Cooperation (SDC) and the Bureau of Population, Refugees, and Migration, US State Department (BPRM) and the self-contribution of the local communities.\n\nIn 2010, IOM began a kahriz rehabilitation project with funds from the Korea International Cooperation Agency (KOICA). During the First Phase of the action which lasted until January 2013, a total of 20 kahrizes in the mainland of Azerbaijan have been renovated. In June 2018, the Second Phase has been launched and by 2022, IOM and KOICA aim to renovate fully a total of 40 kahrizes.\n\nThe oasis of Turpan, in the deserts of Xinjiang in northwestern China, uses water provided by qanat (locally called \"karez\"). The number of karez systems in the area is slightly below 1,000, and the total length of the canals is about 5,000 kilometers.\n\nTurpan has long been the center of a fertile oasis and an important trade center along the Northern Silk Road, at which time it was adjacent to the kingdoms of Korla and Karashahr to the southwest. The historical record of the karez extends back to the Han Dynasty. The Turfan Water Museum is a Protected Area of the People's Republic of China because of the importance of the Turpan karez water system to the history of the area.\n\nIn India, there are karez (qanat) systems. These are located at Bidar, Bijapur, Burhanpur \"(Kundi Bhandara)\" and Aurgangabad. The Karez does exist few other places as well, but investigations are in progress to determine the reality.The Bidar karez systems where probably the first one to have ever been dug in India. It dates to the Bahmani period. Valliyil Govindankutty Assistant Professor in Geography Government College Chittur was responsible for unraveling Karez Systems of Bidar and has been supporting District Administration with research outputs towards conservation of the Karez system. He was responsible for mapping these wonderful water system. Bidar is having three karez systems as per Gulam Yazdani's documentation. Detailed documentation of the Naubad karez system was dome by Valliyil Govindankutty in August 2013. A report was submitted to District Administration of Bidar and highlights many new facts which do not exist in previous documentations. The research support provided by Valliyil Govindankutty to the District Administration has led to the initiation of cleaning the debris and collapsed sections paving the way to its rejuvenation. The cleaning of karez has led to bringing water to higher areas of the plateau, and it has in turn recharged the wells in the vicinity. Other than Naubad there are two more karez systems in Bidar, \"Shukla Theerth\" and \"Jamna Mori\". The Shukla theerth is the longest karez system in Bidar. The mother well of this karez has been discovered by Valliyil Govindankutty and Team YUVAA during survey near Gornalli Kere, a historic embankment. The third system called Jamna mori is more of a distribution system within the old city area with many channels crisscrossing the city lanes.\n\nThe Bijapur karez system is much complicated. The study done by Valliyil Govindankutty reveals that it has surface water and groundwater connections. The Bijapur karez is a network of shallow masonry aqueducts, terracotta/ceramic pipes, embankments and reservoirs, tanks etc. All weave together a network to ensure water reaches the old city. The system starts at Torwi and extends as shallow aqueducts and further as pipes; further it becomes deeper from the Sainik school area onward which exists as a tunnel dug through the geology. The system can be clearly traced up to Ibrahim Roja.\n\nIn Aurangabad the karez systems are called nahars. These are shallow aqueducts running through the city. There are 14 aqueducts in Aurangabad. The Nahar-i-Ambari is the oldest and longest. Its again a combination of shallow aqueduts, open channels, pipes, cisterns, etc. The source of water is a surface water body. The karez has been constructed right below the bed of lake. The lake water seeps through the soil into the Karez Gallery.\n\nIn Burhanpur the karez is called \"Kundi-Bhandara\", sometimes wrongly referred to as\"Khuni Bhandara\". The system is approx 6 km long starts from the alluvial fans of Satpura hills in the north of the town. Unlike Bidar, Bijapur and Aurgangabad the System airvents are round in shape. Inside the Karez one could see lime depositions on the walls. The Systems ends to carry water further to palaces and public fountains through pipe line.\n\nIt has been suggested that underground temples at Gua Made in Java reached by shafts, in which masks of a green metal were found, originated as a qanat.\n\nIn the middle of the twentieth century, an estimated 50,000 \"qanats\" were in use in Iran, each commissioned and maintained by local users. Of these, only 37,000 remain in use as of 2015.\n\nOne of the oldest and largest known qanats is in the Iranian city of Gonabad, and after 2,700 years still provides drinking and agricultural water to nearly 40,000 people. Its main well depth is more than 360 meters and its length is 45 kilometers. Yazd, Khorasan and Kerman are zones known for their dependence on an extensive system of \"qanats\".\n\nIn 2016, UNESCO inscribed the Persian Qanat as a World Heritage Site, listing the following eleven qanats: Qasebeh Qanat, Qanat of Baladeh, Qanat of Zarch, Hasan Abad-e Moshir Qanat, Ebrāhim Ābād Qanat in Markazi Province, Qanat of Vazvān in Esfahan Province, Mozd Ābād Qanat in Esfahan Province, Qanat of the Moon in Esfahan Province, Qanat of Gowhar-riz in Kerman Province, Jupār – Ghāsem Ābād Qanat in Kerman Province, and Akbar Ābād Qanat in Kerman Province. Since 2002, UNESCO's International Hydrological Programme (IHP) Intergovernmental Council began investigating the possibility of an international qanat research center to be located in Yazd, Iran.\n\nThe Qanats of Gonabad also called kariz Kai Khosrow is one of the oldest and largest qanats in the world built between 700 BC to 500 BC. It is located at Gonabad, Razavi Khorasan Province, Iran. This property contains 427 water wells with total length of 33113 meters.\n\nAccording to Callisthenes, the Persians were using water clocks in 328 BCE to ensure a just and exact distribution of water from qanats to their shareholders for agricultural irrigation. The use of water clocks in Iran, especially in Qanats of Gonabad and kariz Zibad, dates back to 500BCE. Later they were also used to determine the exact holy days of pre-Islamic religions, such as the \"Nowruz\", \"Chelah\", or \"Yaldā\" – the shortest, longest, and equal-length days and nights of the years.\nThe Water clock, or \"Fenjaan\", was the most accurate and commonly used timekeeping device for calculating the amount or the time that a farmer must take water from Qanats of Gonabad until it was replaced by more accurate current clocks.\nMany of the Iranian qanats bear some characteristics which allow us to call them feat of engineering, considering the intricate techniques used in their construction. The eastern and central regions of Iran hold the most qanats due to low precipitation and lack of permanent surface streams, whereas a small number of qanats can be found in the northern and western parts which receive more rainfall and enjoy some permanent rivers. Respectively the provinces Khorasan Razavi, Southern Khorasan, Isfahan and Yazd accommodate the most qanats, but from the viewpoint of water discharge the provinces Isfahan, Khorasan Razavi, Fars and Kerman are ranked first to fourth.\n\nHenri Golbot, explored the genesis of the qanat in his 1979 publication, \"Les Qanats. Une technique d'acquisition de l'eau\" (The Qanats. a Technique for Obtaining Water), He argues that the ancient Iranians made use of the water that the miners wished to get rid of it, and founded a basic system named qanat or Kariz to supply the required water to their farm lands. According to Goblot, this innovation took place in the northwest of the present Iran somewhere bordering Turkey and later was introduced to the neighboring Zagros Mountains.\nAccording to an inscription left by Sargon II, the king of Assyria, In 714 BC he invaded the city of Uhlu lying in the northwest of Uroomiye lake that lay in the territory of Urartu empire, and then he noticed that the occupied area enjoyed a very rich vegetation even though there was no river running across it. So he managed to discover the reason why the area could stay green, and realized that there were some qanats behind the matter. In fact it was Ursa, the king of the region, who had rescued the people from thirst and turned Uhlu into a prosperous and green land. Goblot believes that the influence of the Medeans and Achaemenids made the technology of qanat spread from Urartu (in the western north of Iran and near the present border between Iran and Turkey) to all over the Iranian plateau. \nIt was an Achaemenid ruling that in case someone succeeded in constructing a qanat and bringing groundwater to the surface in order to cultivate land, or in renovating an abandoned qanat, the tax he was supposed to pay the government would be waived not only for him but also for his successors for up to 5 generations. During this period, the technology of qanat was in its heyday and it even spread to other countries. For example, following Darius’s order, Silaks the naval commander of the Persian army and Khenombiz the royal architect managed to construct a qanat in the oasis of Kharagha in Egypt. Beadnell believes that qanat construction dates back to two distinct periods: they were first constructed by the Persianse, and later the Romans dug some other qanats during their reign in Egypt from 30 BC to 395 AD. The magnificent temple built in this area during Darius’s reign shows that there was a considerable population depending on the water of qanats. Ragerz has estimated this population to be 10,000 people. The most reliable document confirming the existence of qanats at this time was written by Polybius who states that: “the streams are running down from everywhere at the base of Alborz mountain, and people have transferred too much water from a long distance through some subterranean canals by spending much cost and labor”.\n\nDuring the Seleucid Era, which began after the occupation of Iran by Alexander, it seems that the qanats were abandoned.\n\nIn terms of the situation of qanats during this era, some historical records have been found. In a study by Russian orientalist scholars it has been mentioned that: the Persians used the side branches of rivers, mountain springs, wells and qanats to supply water. The subterranean galleries excavated to obtain groundwater were named as qanat. These galleries were linked to the surface through some vertical shafts which were sunk in order to get access to the gallery to repair it if necessary.\n\nAccording to the historical records, the Parthian kings did not care about the qanats the way the Achaemenid kings and even Sassanid kings did. As an instance, Arsac III, one of the Parthian kings, destroyed some qanats in order to make it difficult for Seleucid Antiochus to advance further while fighting him.\nThe historical records from this time indicate a perfect regulation on both water distribution and farmlands. All the water rights were recorded in a special document which was referred to in case of any transaction. The lists of farmlands – whether private or governmental – were kept at the tax department. During this period there were some official rulings on qanats, streams, construction of dam, operation and maintenance of qanats, etc. The government proceeded to repair or dredge the qanats that were abandoned or destroyed for any reason, and construct the new qanats if necessary. A document written in the Pahlavi language pointed out the important role of qanats in developing the cities at that time.\nIn Iran, the advent of Islam, which coincided with the overthrow of the Sassanid dynasty, brought about a profound change in religious, political, social and cultural structures. But the qanats stayed intact, because the economic infrastructure, including qanats was of great importance to the Arabs. As an instance, M. Lombard reports that the Moslem clerics who lived during Abbasid period, such as Abooyoosef Ya’qoob (death 798 AD) stipulated that whoever can bring water to the idle lands in order to cultivate, his tax would be waived and he would be entitled to the lands cultivated. Therefore, this policy did not differ from that of the Achaemenids in not getting any tax from the people who revived abandoned lands. The Arabs’ supportive policy on qanats was so successful that even the holy city of Mecca gained a qanat too. The Persian historian Hamdollah Mostowfi writes: “Zobeyde Khatoon (Haroon al-Rashid’s wife) constructed a qanat in Mecca. After the time of Haroon al-Rashid, during the caliph Moghtader’s reign this qanat fell into decay, but he rehabilitated it, and the qanat was rehabilitated again after it collapsed during the reign of two other caliphs named Ghaem and Naser. After the era of the caliphs this qanat completely fell into ruin because the desert sand filled it up, but later Amir Choopan repaired the qanat and made it flow again in Mecca.”\n\nThere are also other historical texts proving that the Abbasids were concerned about qanats. For example, according to the “Incidents of Abdollah bin Tahir’s Time” written by Gardizi, in 830 AD a terrible earthquake struck the town of Forghaneh and reduced many homes to rubble. The inhabitants of Neyshaboor used to come to Abdollah bin Tahir in order to request him to intervene, for they fought over their qanats and found the relevant instruction or law on qanat as a solution neither in the prophet’s quotations nor in the clerics’ writings. So Abdollah bin Tahir managed to bring together all the clergymen from throughout Khorasan and Iraq to compile a book entitled \"Alghani\" (The Book of Qanat). This book collected all the rulings on qanats which could be of use to whoever wanted to judge a dispute over this issue. Gardizi added that this book was still applicable to his time, and everyone made references to this book.\n\nOne can deduce from these facts that during the above-mentioned period the number of qanats was so considerable that the authorities were prompted to put together some legal instructions concerning them. Also it shows that from the ninth to eleventh centuries the qanats that were the hub of the agricultural systems were also of interest to the government. Apart from The Book of Alghani, which is considered as a law booklet focusing on qanat-related rulings based on Islamic principles, there is another book about groundwater written by Karaji in 1010. This book, entitled \"Extraction of Hidden Waters\", examines just the technical issues associated with the qanat and tries to answer the common questions such as how to construct and repair a qanat, how to find a groundwater supply, how to do leveling, etc.. Some of the innovations described in this book were introduced for the first time in the history of hydrogeology, and some of its technical methods are still valid and can be applied in qanat construction. The content of this book implies that its writer (Karaji) did not have any idea that there was another book on qanats compiled by the clergymen.\n\nThere are some records dating back to that time, signifying their concern about the legal vicinity of qanats. For example, Mohammad bin Hasan quotes Aboo-Hanifeh that in case someone constructs a qanat in abandoned land, someone else can dig another qanat in the same land on the condition that the second qanat is 500 zera’ (375 meters) away from the first one.\n\nMs. Lambton quotes Moeen al-din Esfarzi who wrote the book \"Rowzat al-Jannat\" (the garden of paradise) that Abdollah bin Tahir (from the Taherian dynasty) and Ismaeel Ahmed Samani (from the Samani dynasty) had several qanats constructed in Neyshaboor. Later, in the 11th century, a writer named Nasir Khosrow acknowledged all those qanats with the following words: “Neyshaboor is located in a vast plain at a distance of 40 Farsang (~240 km) from Serakhs and 70 Farsang (~420 km) from Mary (Marv) … all the qanats of this city run underground, and it is said that an Arab who was offended by the people of Neyshaboor has complained that; what a beautiful city Neyshaboor could have become if its qanats would have flowed on the ground surface and instead its people would have been underground”.\nThese documents all certify the importance of qanats during the Islamic history within the cultural territories of Iran.\nIn the 13th century, the invasion of Iran by Mongolian tribes reduced many qanats and irrigation systems to ruin, and many qanats were deserted and dried up. Later, in the era of the Ilkhanid dynasty especially at the time of Ghazan Khan and his Persian minister Rashid al-Din Fazl-Allah, some measures were taken to revive the qanats and irrigation systems. There is a 14th-century book entitled \"Al-Vaghfiya Al-Rashidiya\" (Rashid’s Deeds of Endowment) that names all the properties located in Yazd, Shiraz, Maraghe, Tabriz, Isfahan and Mowsel that Rashid Fazl-Allah donated to the public or religious places. This book mentions many qanats running at that time and irrigating a considerable area of farmland. At the same time, another book, entitled \"Jame’ al-Kheyrat\", was written by Seyyed Rokn al-Din on the same subject as Rashid’s book. In this book, Seyyed Rokn al-Din names the properties he donated in the region of Yazd. These deeds of endowment indicate that much attention was given to the qanats during the reign of Ilkhanids, but it is attributable to their Persian ministers, who influenced them.\n\nIn the years 1984–1985 the ministry of energy took a census of 28,038 qanats whose total discharge was 9 billion cubic meters. In the years 1992–1993 the census of 28,054 qanats showed a total discharge of 10 billion cubic meters. 10 years later in 2002–2003 the number of the qanats was reported as 33,691 with a total discharge of 8 billion cubic meters.\n\nIn the restricted regions there are 317,225 wells, qanats and springs that discharge 36,719 million cubic meters water a year, out of which 3,409 million cubic meters is surplus to the aquifer capacity. in 2005, in the country as a whole, there were 130,008 deep wells with a discharge of 31,403 million cubic meter, 33,8041 semi deep wells with a discharge of 13,491 million cubic meters, 34,355 qanats with a discharge of 8,212 million cubic meters, and 55,912 natural springs with a discharge of 21,240 million cubic meters.\n\nA survey of qanat systems in the Kurdistan region of Iraq conducted by the Department of Geography at Oklahoma State University (USA) on behalf of UNESCO in 2009 found that out of 683 karez systems, some 380 were still active in 2004, but only 116 in 2009. Reasons for the decline of qanats include \"abandonment and neglect\" prior to 2004, \"excessive pumping from wells\" and, since 2005, drought. Water shortages are said to have forced, since 2005, over 100,000 people who depended for their livelihoods on karez systems to leave their homes. The study says that a single karez has the potential to provide enough household water for nearly 9,000 individuals and irrigate over 200 hectares of farmland. UNESCO and the government of Iraq plan to rehabilitate the karez through a Karez Initiative for Community Revitalization to be launched in 2010. Most of the karez are in Sulaymaniyah Governorate (84%). A large number are also found in Erbil Governorate (13%), especially on the broad plain around and in Erbil city.\n\nIn Japan there are several dozen qanat-like structures, locally known as 'mambo' or 'manbo', most notably in the Mie- and Gifu Prefectures. Whereas some link their origin clearly to the Chinese karez, and therefore to the Iranian source, a Japanese conference in 2008 found insufficient scientific studies to evaluate the origins of the mambo.\n\nAmong the qanats built in the Roman Empire, the 94 km long Gadara Aqueduct in northern Jordan was possibly the longest continuous qanat ever built. Partly following the course of an older Hellenistic aqueduct, excavation work arguably started after a visit by emperor Hadrian in 129–130 AD. The Gadara Aqueduct was never quite finished and was put in service only in sections.\n\nIn Pakistan qanat irrigation system is endemic only in Balochistan. The major concentration is in the north and northwest along the Pakistan-Afghanistan border and oasis of Makoran division.\n\nThe acute shortage of water resources give water a decisive role in the regional conflicts arose in the course of history of Balochistan. Therefore, in Balochistan, the possession of water resources is more important than ownership of land itself. Hence afterward a complex system for the collection, channeling and distribution of water were developed in Balochistan. Similarly, the distribution and unbiased flow of water to different stockholders also necessitate the importance of different societal classes in Balochistan in general and particularly in Makoran. For instance, sarrishta (literally, head of the chain) is responsible for administration of channel. He normally owns the largest water quota. Under sarrishta, there are several heads of owners issadar who also possessed larger water quotas. The social hierarchy within Baloch society of Makoran depends upon the possession of largest quotas of water. The role of sarrishta in some cases hierarchical and passing from generations within the family and he must have the knowledge of the criteria of unbiased distribution of water among different issadar.\n\nThe sharing of water is based on a complex indigenous system of measurement depends upon time and space particularly to the phases of moon; the hangams. Based on seasonal variations and share of water the hangams are apportioned among various owners over period of seven or fourteen days. However, in some places, instead of hangam, anna used which is based on twelve-hour period for each quota. Therefore, if a person own 16 quotas it means that he is entitled for water for eight days in high seasons and 16 days in winter when water level went down as well as expectation of winter rain (Baharga) in Makran region. The twelve-hour water quota again subdivided into several sub-fractions of local measuring scales such as tas or pad (Dr Gul Hasan Pro VC LUAWMS, 2 day National conference on Kech).\n\nThe Chagai district is in the north west corner of Balochistan, Pakistan, bordering with Afghanistan and Iran. Qanats, locally known as Kahn, are found more broadly in this region. They are spread from Chaghai district all the way up to Zhob district.\n\nQanats were found over much of Syria. The widespread installation of groundwater pumps has lowered the water table and qanat system. Qanats have gone dry and been abandoned across the country.\n\nIn Oman from the Iron Age Period (found in Salut, Bat and other sites) a system of underground aqueducts called Falaj were constructed, a series of well-like vertical shafts, connected by gently sloping horizontal tunnels.\nThere are three types of Falaj: Daudi (داوودية) with underground aqueducts, Ghaili (الغيلية) requiring a dam to collect the water, and Aini (العينية) whose source is a water spring. These enabled large scale agriculture to flourish in a dryland environment.\nAccording to UNESCO, some 3,000 \"aflaj\" (plural) or \"falaj\" (singular), are still in use in Oman today. Nizwa, the former capital city of Oman, was built around a \"falaj\" which is in use to this day. These systems date to before the Iron Age in Oman. In July 2006, five representative examples of this irrigation system were inscribed as a World Heritage Site.\n\nThe oasis of Al Ain in the United Arab Emirates continues traditional \"falaj\" (qanat) irrigations for the palm groves and gardens.\n\nThere are four main oases in the Egyptian desert. The Kharga Oasis is one that has been extensively studied. There is evidence that as early as the second half of the 5th century BC water brought in qanats was being used. The qanats were excavated through water-bearing sandstone rock, which seeps into the channel, with water collected in a basin behind a small dam at the end. The width is approximately 60 cm, but the height ranges from 5 to 9 meters; it is likely that the qanat was deepened to enhance seepage when the water table dropped (as is also seen in Iran). From there the water was used to irrigate fields.\n\nThere is another instructive structure located at the Kharga oasis. A well that apparently dried up was improved by driving a side shaft through the easily penetrated sandstone (presumably in the direction of greatest water seepage) into the hill of Ayn-Manâwîr to allow collection of additional water. After this side shaft had been extended, another vertical shaft was driven to intersect the side shaft. Side chambers were built, and holes bored into the rock—presumably at points where water seeped from the rocks—are evident.\n\nDavid Mattingly reports foggara extending for hundreds of miles in the Garamantes area near Germa in Libya: \"The channels were generally very narrow – less than 2 feet wide and 5 high – but some were several miles long, and in total some 600 foggara extended for hundreds of miles underground. The channels were dug out and maintained using a series of regularly spaced vertical shafts, one every 30 feet or so, 100,000 in total, averaging 30 feet in depth, but sometimes reaching 130.\"\n\nThe foggara water management system in Tunisia, used to create oases, is similar to that of the Iranian qanat. The foggara is dug into the foothills of a fairly steep mountain range such as the eastern ranges of the Atlas mountains. Rainfall in the mountains enters the aquifer and moves toward the Saharan region to the south. The foggara, 1 to 3 km in length, penetrates the aquifer and collects water. Families maintain the foggara and own the land it irrigates over a ten-meter width, with length reckoned by the size of plot that the available water will irrigate.\n\nQanats (designated foggaras in Algeria) are the source of water for irrigation in large oases like that at Gourara. The foggaras are also found at Touat (an area of Adrar 200 km from Gourara). The length of the foggaras in this region is estimated to be thousands of kilometers.\n\nAlthough sources suggest that the foggaras may have been in use as early as 200 AD, they were clearly in use by the 11th century after the Arabs took possession of the oases in the 10th century and the residents embraced Islam.\n\nThe water is metered to the various users through the use of distribution weirs that meter flow to the various canals, each for a separate user.\n\nThe humidity of the oases is also used to supplement the water supply to the foggara. The temperature gradient in the vertical shafts causes air to rise by natural convection, causing a draft to enter the foggara. The moist air of the agricultural area is drawn into the foggara in the opposite direction to the water run-off. In the foggara it condenses on the tunnel walls and the air passes out of the vertical shafts. This condensed moisture is available for reuse.\n\nIn southern Morocco, the qanat (locally \"khettara\") is also used. On the margins of the Sahara Desert, the isolated oases of the Draa River valley and Tafilalt have relied on qanat water for irrigation since the late 14th century. In Marrakech and the Haouz plain, the qanats have been abandoned since the early 1970s, having dried up. In the Tafilaft area, half of the 400 khettaras are still in use. The Hassan Adahkil Dam's impact on local water tables is said to be one of the many reasons for the loss of half of the khettara.\n\nThe black berbers (\"haratin\") of the south were the hereditary class of qanat diggers in Morocco who build and repair these systems. Their work was hazardous.\n\nThe Tunnel of Eupalinos on Samos runs for 1 kilometre through a hill to supply water to the town of Pythagorion. It was built on the order of the Tyrant Polycrates around 550 BC. At either end of the tunnel proper, shallow \"qanat\"-like tunnels carried the water from the spring and to the town.\n\nThe 5,653 m long Claudius Tunnel, intended to drain the largest Italian inland water, Fucine Lake, was constructed using the qanat technique. It featured shafts up to 122 m deep. The entire ancient town of Palermo in Sicily was equipped with a huge qanat system built during the Arab period (827–1072). Many of the qanats are now mapped and some can be visited. The famous Scirocco room has an air-conditioning system cooled by the flow of water in a qanat and a \"wind tower\", a structure able to catch the wind and use it to draw the cooled air up into the room.\n\nThe Raschpëtzer near Helmsange in southern Luxembourg is a particularly well preserved example of a Roman qanat. It is probably the most extensive system of its kind north of the Alps. To date, some 330 m of the total tunnel length of 600 m have been explored. Thirteen of the 20 to 25 shafts have been investigated. The qanat appears to have provided water for a large Roman villa on the slopes of the Alzette valley. It was built during the Gallo-Roman period, probably around the year 150 and functioned for about 120 years thereafter.\n\nThere are still many examples of \"galeria\" or qanat systems in Spain, most likely brought to the area by the Moors during their rule of the Iberian peninsula. Turrillas in Andalusia on the north facing slopes of the Sierra de Alhamilla has evidence of a qanat system. Granada is another site with an extensive qanat system. In Madrid they were called \"viajes de agua\" and were used until relatively recently. See and in Spanish.\n\nQanats in the Americas, usually referred to as puquios or filtration galleries, can be found in the Nazca region of Peru and in northern Chile. The Spanish introduced qanats into Mexico in 1520 AD.\n\nIn an August 21, 1906 letter written from Teheran, Florence Khanum, the American wife of Persian diplomat Ali Kuli Khan, described the use of qanats for the garden at the home of her brother-in-law, General Husayn Kalantar,\nJanuary 1, 1913\n\nOne of the oldest and strangest traditions in Iran was to hold wedding ceremonies between widows and underground water tunnels called qanats.\n\n\n\n"}
{"id": "36279495", "url": "https://en.wikipedia.org/wiki?curid=36279495", "title": "Ralf van Bühren", "text": "Ralf van Bühren\n\nRalf van Bühren (born 3 February 1962) is a German art historian, theologian, and Church historian, who teaches at the Pontifical University of Santa Croce in Rome. His publications specialize on the History of Christian Art and Architecture in general, as well as on the rhetorics and visual communication of modern art, on the liturgical space after the Council of Trent and the Vatican Council II, on Religious Tourism, and on the pastoral concern for contemporary artists in particular.\n\nVan Bühren was born in Bad Kreuznach. At the Max-Planck-Gymnasium in Trier, he finished his secondary school education in 1982. Between 1984 and 1991 van Bühren studied Art history at the University of Trier and the Ludwig Maximilian University of Munich. In Munich in 1988 he converted to the Roman Catholic Church.\n\nIn 1994 he received the PhD in Art history at the University of Cologne. The dissertation was published in 1998 as \"The works of mercy in the Art from the 12th-18th centuries. Iconographic changes caused by the modern reception of Rhetorics\". It explains art theory and rhetorics as origins of a persuasive mode of representation in early modern art.\n\nBetween 1992 and 1995 van Bühren worked as pedagogical assistant in the \"Museumsdienst Köln\" at the Wallraf-Richartz Museum and Museum Ludwig in Cologne, in the data processing service of the \"Bildarchiv Foto Marburg\", and as freelance collaborator in the \"Domforum Köln\" at the Cologne Cathedral and the romanesque churches of Cologne.\n\nFrom 1996 to 1998 he was chief copy Editor at the German publishing house \"Verlag Schnell & Steiner\" in Regensburg, whose founders (Hugo Schnell, Johannes Steiner) in 1934 invented the species of small church guidebooks, which today are produced a million times.\n\nIn 2006 van Bühren was awarded the doctorate degree in Theology at the Pontifical University of Santa Croce in Rome. The dissertation he published in 2008 in the series \"Konziliengeschichte\" (ed. by Walter Brandmüller) as \"Art and Church in the 20th century. The reception of the Second Vatican Council\". The prologue has written Friedhelm Hofmann, Bishop of Würzburg and at that time a member of the Pontifical Commission for the Cultural Heritage of the Church as well as of the Commission for Science and Culture of the German Bishops' Conference.\n\nSince 2006 van Bühren is teaching Art History as Associate Professor at the Pontifical University of Santa Croce in Rome. The focus of his research and his lectures at the School of Church Communications is on \"Art and Architecture as Communication Media\", at the School of Theology on \"Liturgical Art from Antiquity to the Present\" and \"Christian Art History\". At many universities, these subjects do not rate among the required courses within the teaching program of the studies of Catholic theology, although the Second Vatican Council claimed the consideration of art. The University of Santa Croce tries to counter this deficit inside of today’s theological education.\n\nOn 1 July 2014 van Bühren was appointed as consultant to the Pontifical Council for Culture.\n\nSince 2014, he is Editorial Board member of the peer-reviewed journal “Church, Communication and Culture”, edited by Santa Croce’s School of Communications and published by Routledge (Taylor & Francis Group). Recently, van Bühren's communication studies explain the importance of art history for Religious Tourism, cultural journalism, religious correspondents and Church media relations.\n\nHis current lectures include courses on \"Christian Art and Architecture in Rome. From Antiquity to the Present\" (in English), open to students of other international universities. These courses intersperse classroom sessions with site visits.\n\n\n"}
{"id": "54015318", "url": "https://en.wikipedia.org/wiki?curid=54015318", "title": "Robert Jones (artilleryman)", "text": "Robert Jones (artilleryman)\n\nRobert Jones was a lieutenant in the artillery corps of the British army. He was known for popularising figure skating in Great Britain and his 1772 trial for sodomy.\n\nHe wrote a book, \"A Treatise on Skating\" which was published in London in 1772, which went through several print runs. He also wrote \"A New Treatise on Artificial Fireworks\", which was published in 1765.\n"}
{"id": "55898626", "url": "https://en.wikipedia.org/wiki?curid=55898626", "title": "Rohat Nabieva", "text": "Rohat Nabieva\n\nRohat Abduvahobona Nabieva () (November 6, 1936 – March 4, 2017) was a Tajikistani historian, active in the Soviet era and after. She was among the first historians in her country to consider history through the lens of gender.\n\nBorn into a working-class family in Khujand, Nabieva took her degree in history at Tajikistan State University, from whose Department of History and Philology she graduated in 1959. She remained associated with the same institution for the bulk of her academic career, first as a postgraduate student, then as an instructor, and then as an assistant professor of history in the Department of Soviet History. It was during this time, in 1967, that she joined the Communist Party of the Soviet Union. In 1975 she received her doctorate; she then became head of the Department of Tajik History at the University. Nabieva has focused, in her research, on the role played by Tajik women in various aspects of Soviet society, among them the labor force, agriculture, and industry. Among her writings are \"Women of Soviet Tajikistan\" (1967), \"The Patriotism of the Tajik Youth\" (1969), and \"The Share of Women in Society\" (1999). Later in her career she was commissioned by the Ministry of Education of the Republic of Tajikistan to work with philologist Farxod Zikriyoyev to write textbooks covering the post-independence period of Tajikistan's history. The 9th grade text was published in 2001; that for the 11th grade followed in 2006. Nabieva received numerous awards for her work over the course of her career; among these are the Order of the Presidium of the Academy of Sciences of Tajikistan and the Honorary Order of the Presidium of the Supreme Soviet of Tajikistan.\n\n \n"}
{"id": "253264", "url": "https://en.wikipedia.org/wiki?curid=253264", "title": "Slavery in the United States", "text": "Slavery in the United States\n\nSlavery in the United States was the legal institution of human chattel enslavement, primarily of Africans and African Americans, that existed in the United States of America in the 18th and 19th centuries. Slavery had been practiced in British America from early colonial days, and was legal in all Thirteen Colonies at the time of the Declaration of Independence in 1776. It lasted in about half the states until 1865, when it was prohibited nationally by the Thirteenth Amendment. As an economic system, slavery was largely replaced by sharecropping.\n\nBy the time of the American Revolution (1775–1783), the status of slave had been institutionalized as a racial caste associated with African ancestry. When the United States Constitution was ratified (1789), a relatively small number of free people of color were among the voting citizens (male property owners). During and immediately following the Revolutionary War, abolitionist laws were passed in most Northern states and a movement developed to abolish slavery. Northern states depended on free labor and all had abolished slavery by 1805. The rapid expansion of the cotton industry in the Deep South after the invention of the cotton gin greatly increased demand for slave labor to pick cotton when it all ripened at once, and the Southern states continued as slave societies. Those states attempted to extend slavery into the new Western territories to keep their share of political power in the nation. Southern leaders also wanted to annex Cuba as a slave territory. The United States became polarized over the issue of slavery, split into slave and free states, in effect divided by the Mason–Dixon line which delineated (free) Pennsylvania from (slave) Maryland and Delaware.\n\nCongress during the Jefferson administration prohibited the importation of slaves, effective 1808, although smuggling (illegal importing) via Spanish Florida was not unusual. Domestic slave trading, however, continued at a rapid pace, driven by labor demands from the development of cotton plantations in the Deep South. More than one million slaves were sold from the Upper South, which had a surplus of labor, and taken to the Deep South in a forced migration, splitting up many families. New communities of African-American culture were developed in the Deep South, and the total slave population in the South eventually reached 4 million before liberation.\n\nAs the West was developed for settlement, the Southern state governments wanted to keep a balance between the number of slave and free states to maintain a political balance of power in Congress. The new territories acquired from Britain, France, and Mexico were the subject of major political compromises. By 1850, the newly rich cotton-growing South was threatening to secede from the Union, and tensions continued to rise. Many white Southern Christians, including church ministers, attempted to justify their support for slavery as modified by Christian paternalism. The largest denominations, the Baptist, Methodist, and Presbyterian churches, split over the slavery issue into regional organizations of the North and South. When Abraham Lincoln won the 1860 election on a platform of halting the expansion of slavery, seven states broke away to form the Confederacy. The first six states to secede held the greatest number of slaves in the South. Shortly after, the Civil War began when Confederate forces attacked the US Army's Fort Sumter. Four additional slave states then seceded. Due to Union measures such as the Confiscation Acts and Emancipation Proclamation in 1863, the war effectively ended slavery, even before ratification of the Thirteenth Amendment in December 1865 formally ended the legal institution throughout the United States.\n\nIn the early years of the Chesapeake Bay settlements, colonial officials found it difficult to attract and retain laborers under the harsh frontier conditions, and there was a high mortality rate. Most laborers came from Britain as indentured laborers, signing contracts of indenture to pay with work for their passage, their upkeep and training, usually on a farm. The colonies had agricultural economies. These indentured laborers were often young people who intended to become permanent residents. In some cases, convicted criminals were transported to the colonies as indentured laborers, rather than being imprisoned. The indentured laborers were not slaves, but were required to work for four to seven years in Virginia to pay the cost of their passage and maintenance. Many Germans, Scots-Irish, and Irish came to the colonies in the 18th century, settling in the backcountry of Pennsylvania and further south.\n\nThe first 19 or so Africans to reach the English colonies arrived in Jamestown, Virginia in 1619, brought by Dutch traders who had seized them from a captured Spanish slave ship. The Spanish usually baptized slaves in Africa before embarking them. As English custom then considered baptized Christians exempt from slavery, colonists treated these Africans as indentured servants, and they joined about 1,000 English indentured servants already in the colony. The Africans were freed after a prescribed period and given the use of land and supplies by their former masters. The historian Ira Berlin noted that what he called the \"charter generation\" in the colonies was sometimes made up of mixed-race men (Atlantic Creoles) who were indentured servants, and whose ancestry was African and Iberian. They were descendants of African women and Portuguese or Spanish men who worked in African ports as traders or facilitators in the slave trade. For example, Anthony Johnson arrived in Virginia in 1621 from Angola as an indentured servant; he became free and a property owner, eventually buying and owning slaves himself. The transformation of the social status of Africans, from indentured servitude to slaves in a racial caste which they could not leave or escape, happened gradually.\n\nThere were no laws regarding slavery early in Virginia's history. But, in 1640, a Virginia court sentenced John Punch, an African, to slavery after he attempted to flee his service. The two whites with whom he fled were sentenced only to an additional year of their indenture, and three years' service to the colony. This marked the first legal sanctioning of slavery in the English colonies and was one of the first legal distinctions made between Europeans and Africans.\n\nIn 1641, Massachusetts became the first colony to authorize slavery through enacted law. Massachusetts passed the Body of Liberties, which prohibited slavery in many instances, but did allow for three legal bases of slavery. Slaves could be held if they were captives of war, if they sold themselves into slavery or were purchased from elsewhere, or if they were sentenced to slavery as punishment by the governing authority. The Body of Liberties used the word \"strangers\" to refer to people bought and sold as slaves; they were generally not English subjects. Colonists came to equate this term with Native Americans and Africans.\n\nIn 1654, John Casor, a black indentured servant in colonial Virginia, was the first man to be declared a slave in a civil case. He had claimed to an officer that his master, Anthony Johnson, himself a free black, had held him past his indenture term. A neighbor, Robert Parker told Johnson that if he did not release Casor, Parker would testify in court to this fact. Under local laws, Johnson was at risk for losing some of his headright lands for violating the terms of indenture. Under duress, Johnson freed Casor. Casor entered into a seven years' indenture with Parker. Feeling cheated, Johnson sued Parker to repossess Casor. A Northampton County, Virginia court ruled for Johnson, declaring that Parker illegally was detaining Casor from his rightful master who legally held him \"for the duration of his life\".\n\nDuring the colonial period, the status of slaves was affected by interpretations related to the status of foreigners in England. England had no system of naturalizing immigrants to its island or its colonies. Since persons of African origins were not English subjects by birth, they were among those peoples considered foreigners and generally outside English common law. The colonies struggled with how to classify people born to foreigners and subjects. In 1656 Virginia, Elizabeth Key Grinstead, a mixed-race woman, successfully gained her freedom and that of her son in a challenge to her status by making her case as the baptized Christian daughter of the free Englishman Thomas Key. Her attorney was an English subject, which may have helped her case. (He was also the father of her mixed-race son, and the couple married after Key was freed.)\n\nShortly after the Elizabeth Key trial and similar challenges, in 1662 the Virginia royal colony approved a law adopting the principle of \"partus sequitur ventrem\" (called \"partus\", for short), stating that any children born in the colony would take the status of the mother. A child of an enslaved mother would be born into slavery, regardless if the father were a freeborn Englishman or Christian. This was a reversal of common law practice in England, which ruled that children of English subjects took the status of the father. The change institutionalized the skewed power relationships between slaveowners and slave women, freed the white men from the legal responsibility to acknowledge or financially support their mixed-race children, and somewhat confined the open scandal of mixed-race children and miscegenation to within the slave quarters.\n\nThe Virginia Slave codes of 1705 further defined as slaves those people imported from nations that were not Christian. Native Americans who were sold to colonists by other Native Americans (from rival tribes), or captured by Europeans during village raids, were also defined as slaves. This codified the earlier principle of non-Christian foreigner enslavement.\nIn 1735, the Georgia Trustees enacted a law to prohibit slavery in the new colony, which had been established in 1733 to enable the \"worthy poor\" as well as persecuted European Protestants to have a new start. Slavery was then legal in the other twelve English colonies. Neighboring South Carolina had an economy based on the use of enslaved labor. The Georgia Trustees wanted to eliminate the risk of slave rebellions and make Georgia better able to defend against attacks from the Spanish to the south, who offered freedom to escaped slaves. James Edward Oglethorpe was the driving force behind the colony, and the only trustee to reside in Georgia. He opposed slavery on moral grounds as well as for pragmatic reasons, and vigorously defended the ban on slavery against fierce opposition from Carolina slave merchants and land speculators.\n\nThe Protestant Scottish highlanders who settled what is now Darien, Georgia added a moral anti-slavery argument, which became increasingly rare in the South, in their 1739 \"Petition of the Inhabitants of New Inverness\". By 1750 Georgia authorized slavery in the state because they had been unable to secure enough indentured servants as laborers. As economic conditions in England began to improve in the first half of the 18th century, workers had no reason to leave, especially to face the risks in the colonies.\n\nDuring most of the British colonial period, slavery existed in all the colonies. People enslaved in the North typically worked as house servants, artisans, laborers and craftsmen, with the greater number in cities. Many men worked on the docks and in shipping. In 1703, more than 42 percent of New York City households held slaves, the second-highest proportion of any city in the colonies after Charleston, South Carolina. But slaves were also used as agricultural workers in farm communities, including in areas of upstate New York and Long Island, Connecticut, and New Jersey.\n\nThe South developed an agricultural economy dependent on commodity crops. Its planters rapidly acquired a significantly higher number and proportion of slaves in the population overall, as its commodity crops were labor-intensive. Early on, enslaved people in the South worked primarily in agriculture, on farms and plantations growing indigo, rice, and tobacco; cotton did not become a major crop until after the American Revolution and after the 1790s. Before then long-staple cotton was cultivated primarily on the Sea Islands of Georgia and South Carolina.\n\nThe invention of the cotton gin in 1793 enabled the cultivation of short-staple cotton in a wide variety of mainland areas, leading in the 19th century to the development of large areas of the Deep South as cotton country. Tobacco was very labor-intensive, as was rice cultivation. In South Carolina in 1720, about 65% of the population consisted of enslaved people. Planters (defined by historians in the Upper South as those who held 20 enslaved people or more) used enslaved workers to cultivate commodity crops. They also worked in the artisanal trades on large plantations and in many southern port cities. Backwoods subsistence farmers, the later wave of settlers in the 18th century who settled along the Appalachian Mountains and backcountry, seldom held enslaved people.\n\nSome of the British colonies attempted to abolish the international slave trade, fearing that the importation of new Africans would be disruptive. Virginia bills to that effect were vetoed by the British Privy Council. Rhode Island forbade the import of enslaved people in 1774. All of the colonies except Georgia had banned or limited the African slave trade by 1786; Georgia did so in 1798. Some of these laws were later repealed.\n\nFewer than 350,000 enslaved people were imported into the Thirteen Colonies and the U.S, constituting less than 5% of the twelve million enslaved people brought from Africa to the Americas. The great majority of enslaved Africans were transported to sugar colonies in the Caribbean and to Brazil. As life expectancy was short, their numbers had to be continually replenished. Life expectancy was much higher in the U.S., and the enslaved population was successful in reproduction. The number of enslaved people in the US grew rapidly, reaching by the 1860 Census. From 1770 until 1860, the rate of natural growth of North American enslaved people was much greater than for the population of any nation in Europe, and it was nearly twice as rapid as that of England.\n\nLouisiana was founded as a French colony. Colonial officials in 1724 implemented Louis XIV of France's \"Code Noir\", which regulated the slave trade and the institution of slavery in New France and French Caribbean colonies. This resulted in a different pattern of slavery in Louisiana, purchased in 1803, compared to the rest of the United States. As written, the \"Code Noir\" gave some rights to slaves, including the right to marry. Although it authorized and codified cruel corporal punishment against slaves under certain conditions, it forbade slave owners to torture them or to separate married couples (or to separate young children from their mothers). It also required the owners to instruct slaves in the Catholic faith.\n\nTogether with a more permeable historic French system that allowed certain rights to \"gens de couleur libres\" (free people of color), often born to white fathers and their mixed-race concubines, a far higher percentage of African Americans in Louisiana were free as of the 1830 census (13.2% in Louisiana compared to 0.8% in Mississippi, whose population was dominated by white Anglo-Americans. Most of Louisiana's \"third class\" of free people of color, situated between the native-born French and mass of African slaves, lived in New Orleans). The Louisiana free people of color were often literate, had gained education, and a significant number owned businesses, properties, and even slaves. The \"Code Noir\" forbade interracial marriages. However, interracial unions were widespread under the system known as \"plaçage\". The mixed-race offspring (creoles of color) from such unions were among those in the intermediate social caste of free people of color. The English colonies insisted on a binary system, in which mulatto and black slaves were treated equally under the law, and discriminated against equally if free. But many free people of African descent were mixed race.\n\nWhen the US took over Louisiana, Americans from the Protestant South entered the territory and began to impose their norms. They officially discouraged interracial relationships (although white men continued to have unions with black women, both enslaved and free.) The \"Americanization\" of Louisiana gradually resulted in a binary system of race, causing free people of color to lose status as they were grouped with the slaves. They lost certain rights as they became classified by American whites as officially \"black\".\n\nWhile a smaller number of African slaves were kept and sold in England, slavery in Great Britain had not been authorized by statute there. In 1772, it was made unenforceable at common law in England and Wales by a legal decision. The large British role in the international slave trade continued until 1807. Slavery flourished in most of Britain's colonies, with many wealthy slave owners living in England and holding considerable power.\n\nIn early 1775 Lord Dunmore, royal governor of Virginia, wrote to Lord Dartmouth of his intent to free slaves owned by Patriots in case of rebellion. On November 7, 1775, Lord Dunmore issued Lord Dunmore's Proclamation which declared martial law and promised freedom for any slaves of American patriots who would leave their masters and join the royal forces. Slaves owned by Loyalist masters, however, were unaffected by Dunmore's Proclamation. About 1500 slaves owned by Patriots escaped and joined Dunmore's forces. Most died of disease before they could do any fighting. Three hundred of these freed slaves made it to freedom in Britain.\n\nMany slaves used the very disruption of war to escape their plantations and fade into cities or woods. For instance, in South Carolina, nearly 25,000 slaves (30% of the total enslaved population) fled, migrated, or died during the war. Throughout the South, losses of slaves were high, with many due to escapes. Slaves also escaped throughout New England and the mid-Atlantic, joining the British who had occupied New York.\n\nIn the closing months of the war, the British evacuated 20,000 freedmen from major coastal cities, transporting more than 3,000 for resettlement in Nova Scotia, where they were registered as Black Loyalists and eventually granted land. They transported others to the Caribbean islands, and some to England.\n\nAt the same time, the British were transporting Loyalists and their slaves, primarily to the Caribbean, but some to Nova Scotia. For example, over 5,000 enslaved Africans owned by Loyalists were transported in 1782 with their owners from Savannah to Jamaica and St. Augustine, Florida (then controlled by Britain). Similarly, over half of the black people evacuated in 1782 from Charleston by the British to the West Indies and Florida were slaves owned by white Loyalists.\n\nSlaves and free blacks also fought on the side of rebels during the Revolutionary War. Washington authorized slaves to be freed who fought with the American Continental Army. Rhode Island started enlisting slaves in 1778, and promised compensation to owners whose slaves enlisted and survived to gain freedom. During the course of the war, about one fifth of the northern army was black. In 1781, Baron Closen, a German officer in the French Royal Deux-Ponts Regiment at the Battle of Yorktown, estimated the American army to be about one-quarter black. These men included both former slaves and free blacks.\n\nIn the 18th century, Britain became the world's largest slave trader. Starting in 1777, the Patriots outlawed the importation of slaves state by state. They all acted to end the international trade but it was later reopened in South Carolina and Georgia. In 1807 Congress acted on President Jefferson's advice and made importing slaves from abroad a federal crime, as the Constitution permitted, starting January 1, 1808.\n\nThe Constitution of the United States took effect in 1789 and included several provisions regarding slavery. Section 9 of Article I forbade the Federal government from preventing states from importing slaves before January 1, 1808. As a protection for slavery, the delegates approved Section 2 of Article IV, which prohibited states from freeing slaves who fled to them from another state, and required the return of chattel property to owners.\n\nIn a section negotiated by James Madison of Virginia, Section 2 of Article I designated \"other persons\" (slaves) to be added to the total of the state's free population, at the rate of three-fifths of their total number, to establish the state's official population for the purposes of apportionment of Congressional representation and federal taxation. The protections afforded slavery in the Constitution disproportionately strengthened the political power of Southern representatives, as three-fifths of the (non-voting) slave population was counted for Congressional apportionment.\n\nIn addition, many parts of the country were tied to the Southern economy. As the historian James Oliver Horton noted, prominent slaveholder politicians and the commodity crops of the South had a strong influence on United States politics and economy. Horton said,\n\nin the 72 years between the election of George Washington and the election of Abraham Lincoln, 50 of those years [had] a slaveholder as president of the United States, and, for that whole period of time, there was never a person elected to a second term who was not a slaveholder.\n\nThis increased the power of southern states in Congress for decades, affecting national policies and legislation. The planter elite dominated the southern Congressional delegations and the United States presidency for nearly 50 years.\n\nThe U.S. Constitution barred the federal government from prohibiting the importation of slaves for 20 years. Various states passed different restrictions on the international slave trade during that period; by 1808, the only state still allowing the importation of African slaves was South Carolina. After 1808, legal importation of slaves ceased, although there was smuggling via lawless Spanish Florida and the disputed Gulf Coast to the west. This route all but ended after Florida became a U.S. territory in 1821 (but see Wanderer and Clotilda).\n\nThe replacement for the importation of slaves from abroad was increased domestic production. Virginia and Maryland had little new agricultural development, and their need for slaves was mostly for replacements for decedents. Normal reproduction more than supplied these: Virginia and Maryland had surpluses of slaves. Their tobacco farms were \"worn out\" and the climate was not suitable for cotton or sugar cane. The surplus was even greater because slaves were encouraged to reproduce (though they could not marry). The white supremacist Virginian Thomas Roderick Dew wrote in 1832 that Virginia was a \"negro-raising state\"; i.e. Virginia \"produced\" slaves.\n\nWhere demand for slaves was the strongest was in what was then the southwest of the country: Alabama, Mississippi, and Louisiana, and later Texas, Arkansas, and Missouri. Here there was abundant land suitable for plantation agriculture, which young men with some capital established. This was expansion of the white, monied population: younger men seeking their fortune.\n\nThe most valuable crop that could be grown on a plantation in that climate was cotton. That crop was labor-intensive, and the least-costly laborers were slaves. Demand for slaves exceeded the supply in the southwest; therefore slaves, never cheap if they were productive, went for a higher price. As portrayed in Uncle Tom's Cabin (the \"original\" cabin was in Maryland), \"selling South\" was greatly feared. A recently (2018) publicized example of the practice of \"selling South\" is the 1838 sale by Jesuits of 272 slaves from Maryland, to plantations in Louisiana, to benefit Georgetown University, which \"owes its existence\" to this transaction.\n\nTraders responded to the demand, including John Armfield and his uncle Isaac Franklin, who were \"reputed to have made over half a million dollars (in 19th-century value)\" in the slave trade. (They did not handle the Jesuit transaction just mentioned.) Setting up an office in what was then the District of Columbia, regional center of the slave trade, in Alexandria, \"a major slave trading port for more than a century\", the two men went into business in 1828 buying slaves in the North and selling them in the South:\n\nThis house on Duke Street houses the Freedom House Museum, with exhibits on the slave trade and the lives of slaves.\n\nMr. Armfield remained in Alexandria doing the purchasing, with agents in Richmond and Warrenton, Virginia, and Baltimore, Frederick, and Easton, Maryland (on Maryland's Eastern Shore, near Delaware). Mr. Franklin handled the selling out of New Orleans and Natchez, Mississippi, with offices in St. Francisville and Vidalia, Louisiana. Their partnership grew to the point that when the partnership was dissolved in 1836 and the business sold, they owned six ships for the sole purpose of transporting slaves, with monthly and then biweekly sailings. (The ships carried miscellaneous cargo on the return trips.) One of them, the \"Isaac Franklin\", was built for them.\n\nFranklin and Armfield's Alexandria site was visited by various abolitionists, who have left us detailed descriptions of it. They concur in that Mr. Armfield, in contrast with Robert Lumpkin among others, was the most scrupulous of the major slave traders, who would not knowingly purchase kidnapped slaves or freedmen, and whose slaves were reasonably well treated while he owned them, at least at the Duke Street facility. Slaves concur in this relatively — relatively — positive picture, asking that if they were to be sold, that they be sold to Mr. Armfield. However, Armfield frequently took children from their parents, and sold them South.\n\nA little-discussed but important aspect of slavery in the United States is that owners of female slaves could freely and legally use them as sexual objects. This follows free use of female slaves on slaving vessels by the crews. \"Fancy\" was a code word that indicated the girl or young woman was suitable for or trained for sexual use. In some cases, children were also abused in this manner. The sale of a 13 year old \"nearly a fancy\" is documented, Zephaniah Kingsley, Jr. bought his wife when she was 13.\n\nFurthermore, females of breeding age were supposed to be kept pregnant, producing more slaves to sell. The variations in skin color found in the United States make it obvious how often black females were impregnated by whites. For example, in the 1850 Census, 75.4% of \"free negros\" in Florida were described as mulattos, of mixed race. Nevertheless, it is only very recently, with DNA studies, that any sort of reliable number can be provided, and the research has only begun. Light-skinned girls, who contrasted with the black field workers, were preferred.\n\nThe sexual use of black slaves by white men, either slave owners or those who could purchase the temporary services of a slave, took various forms. A slaveowner, or his teenage sons, could go to the slave quarters area of the plantation and do what he wanted, usually in front of the rest of the slaves, or with minimal privacy. It was not unusual for a \"house\" female — a housekeeper, maid, cook, laundress, or nanny — to be used by one or more white males of the household for their sexual enjoyment. Houses of prostitution throughout the slave states were largely staffed by female slaves providing sexual services, to their owners' profit. There were a small number of free black females engaged in prostitution, or concubinage, especially in New Orleans.\n\nLight-skinned young girls were sold openly for sexual use; their price was much higher than that of a field hand. Special markets for the fancy girl trade existed in New Orleans and Lexington, Kentucky. We have this on no less an authority than Abraham Lincoln:\n\nThose \"considered educated and refined, were purchased by the wealthiest clients, usually plantation owners, to become personal sexual companions.\" \"There was a great demand in New Orleans for 'fancy girls'.\"\n\nThe terrifying issue which did come up frequently was the exaggerated threat of sexual intercourse between black male and white female. Just as the black girls were perceived as having \"a trace of Africa, that supposedly incited passion and sexual wantonness\", the men were all savages, unable to control their lust, given an opportunity. \n\nA colorful but unique approach to the question was offered by Quaker and Florida planter Zephaniah Kingsley, Jr. He advocated, and personally practiced, deliberate racial mixing through marriage, as part of his proposed solution to the slavery issue: racial integration. In an 1829 \"Treatise\", he stated that mixed-race people were healthier and often more beautiful, that interracial sex was hygienic, and that slavery made it convenient. Because of these views, tolerated in Spanish Florida, he felt it impossible to remain long in Territorial Florida, and moved with his slaves and multiple wives to a plantation in Haiti (now in the Dominican Republic). There were many others who less flagrantly practiced interracial, common-law marriages with slaves (see \"Partus sequitur ventrem\").\n\nIn the 19th century, proponents of slavery often defended the institution as a \"necessary evil\". White people of that time feared that emancipation of black slaves would have more harmful social and economic consequences than the continuation of slavery. In 1820, Thomas Jefferson, one of the Founding Fathers of the United States, wrote in a letter that with slavery:\n\nThe French writer and traveler Alexis de Tocqueville, in his influential \"Democracy in America\" (1835), expressed opposition to slavery while observing its effects on American society. He felt that a multiracial society without slavery was untenable, as he believed that prejudice against blacks increased as they were granted more rights (for example, in northern states). He believed that the attitudes of white Southerners, and the concentration of the black population in the South, were bringing the white and black populations to a state of equilibrium, and were a danger to both races. Because of the racial differences between master and slave, he believed that the latter could not be emancipated.\n\nRobert E. Lee wrote in 1856:\n\nHowever, as the abolitionist movement's agitation increased and the area developed for plantations expanded, apologies for slavery became more faint in the South. Leaders then described slavery as a beneficial scheme of labor control. John C. Calhoun, in a famous speech in the Senate in 1837, declared that slavery was \"instead of an evil, a good—a positive good\". Calhoun supported his view with the following reasoning: in every civilized society one portion of the community must live on the labor of another; learning, science, and the arts are built upon leisure; the African slave, kindly treated by his master and mistress and looked after in his old age, is better off than the free laborers of Europe; and under the slave system conflicts between capital and labor are avoided. The advantages of slavery in this respect, he concluded, \"will become more and more manifest, if left undisturbed by interference from without, as the country advances in wealth and numbers\".\n\nOther Southern writers who also began to portray slavery as a positive good were James Henry Hammond and George Fitzhugh. They presented several arguments to defend the act of slavery in the South. Hammond, like Calhoun, believed that slavery was needed to build the rest of society. In a speech to the Senate on March 4, 1858, Hammond developed his \"Mudsill Theory,\" defending his view on slavery stating, \n\"Such a class you must have, or you would not have that other class which leads progress, civilization, and refinement. It constitutes the very mud-sill of society and of political government; and you might as well attempt to build a house in the air, as to build either the one or the other, except on this mud-sill.\" Hammond believed that in every class one group must accomplish all the menial duties, because without them the leaders in society could not progress. He argued that the hired laborers of the North were slaves too: \"The difference… is, that our slaves are hired for life and well compensated; there is no starvation, no begging, no want of employment,\" while those in the North had to search for employment.\n\nGeorge Fitzhugh used assumptions about white superiority to justify slavery, writing that, \"the Negro is but a grown up child, and must be governed as a child.\" In \"The Universal Law of Slavery\", Fitzhugh argues that slavery provides everything necessary for life and that the slave is unable to survive in a free world because he is lazy, and cannot compete with the intelligent European white race. He states that \"The negro slaves of the South are the happiest, and in some sense, the freest people in the world.\" Without the South, \"He (slave) would become an insufferable burden to society\" and \"Society has the right to prevent this, and can only do so by subjecting him to domestic slavery.\"\n\nOn March 21, 1861, Vice President Alexander Stephens of the Confederacy delivered his Cornerstone Speech. He explained the differences between the constitution of the Confederate Republic and that of the United States, and laid out the cause for the American Civil War, and a defense of slavery.\n\nThe new Constitution has put at rest forever all the agitating questions relating to our peculiar institutions—African slavery as it exists among us—the proper status of the negro in our form of civilization. This was the immediate cause of the late rupture and present revolution. Jefferson, in his forecast, had anticipated this, as the \"rock upon which the old Union would split.\" He was right. What was conjecture with him, is now a realized fact. But whether he fully comprehended the great truth upon which that rock stood and stands, may be doubted. The prevailing ideas entertained by him and most of the leading statesmen at the time of the formation of the old Constitution were, that the enslavement of the African was in violation of the laws of nature; that it was wrong in principle, socially, morally and politically. It was an evil they knew not well how to deal with; but the general opinion of the men of that day was, that, somehow or other, in the order of Providence, the institution would be evanescent and pass away... Those ideas, however, were fundamentally wrong. They rested upon the assumption of the equality of races. This was an error. It was a sandy foundation, and the idea of a Government built upon it—when the \"storm came and the wind blew, it fell.\"Our new Government is founded upon exactly the opposite ideas; its foundations are laid, its cornerstone rests, upon the great truth that the negro is not equal to the white man; that slavery, subordination to the superior race, is his natural and moral condition.\n\nClaims against slaves were allegedly backed by contemporary research. The leading researcher was Dr. Samuel A. Cartwright, inventor of the mental illness of drapetomania — the desire of a slave to run away. The Medical Association of Louisiana set up a committee, of which he was chair, to investigate \"The Diseases and Physical Peculiarities of the Negro Race\". Their report, first delivered to the Medical Association in an address, was published in their journal, and then reprinted in part in the widely circulated DeBow's Review.\n\nBeginning during the revolution and in the first two decades of the postwar era, every state in the North abolished slavery, ending with New Jersey in 1804, although in some cases existing slaves were not liberated immediately. These were the first abolitionist laws in the Atlantic World.\n\nIn Massachusetts, slavery was successfully challenged in court in 1783 in a freedom suit by Quock Walker; he said that slavery was in contradiction to the state's new constitution of 1780 providing for equality of men. Freed slaves were subject to racial segregation and discrimination in the North, and it took decades for some states to extend the franchise to them.\n\nMost northern states passed legislation for gradual abolition, first freeing children born to slave mothers (and requiring them to serve lengthy indentures to their mother's masters, often into their 20s as young adults). As a result of this gradualist approach, New York did not fully free its last ex-slaves until 1827, Rhode Island had seven slaves still listed in the 1840 census. Pennsylvania's last ex-slaves were freed in 1847, Connecticut's in 1848, and New Hampshire and New Jersey in 1865.\n\nNone of the Southern states abolished slavery, but it was common for individual slaveholders in the South to free numerous slaves, often citing revolutionary ideals, in their wills. Methodist, Quaker and Baptist preachers traveled in the South, appealing to slaveholders to manumit their slaves. By 1810, the number and proportion of free blacks in the population of the United States had risen dramatically. Most free blacks resided in the North, but even in the Upper South, the proportion of free blacks went from less than one percent of all blacks to more than 10 percent, even as the total number of slaves was increasing through importation.\n\nThrough the Northwest Ordinance of 1787 under the Congress of the Confederation, slavery was prohibited in the territories northwest of the Ohio River; existing slaves were not freed for years, although they could no longer be sold. This was a compromise. Thomas Jefferson proposed in 1784 to end slavery in all the territories, but his bill lost in the Congress by one vote. The territories south of the Ohio River (and Missouri) had authorized slavery.\nNortherners predominated in the westward movement into the Midwestern territory after the American Revolution; as the states were organized, they voted to prohibit slavery in their constitutions when they achieved statehood: Ohio in 1803, Indiana in 1816, and Illinois in 1818. What developed was a Northern block of free states united into one contiguous geographic area that generally shared an anti-slavery culture. The exceptions were the areas along the Ohio River settled by Southerners, the southern portions of states such as Indiana, Ohio and Illinois. Residents of those areas generally shared in Southern culture and attitudes. In addition, these areas were devoted to agriculture longer than the industrializing northern parts of these states, and some farmers used slave labor. The emancipation of slaves in the North led to the growth in the population of northern free blacks, from several hundred in the 1770s to nearly 50,000 by 1810.\n\nThroughout the first half of the 19th century, abolitionism, a movement to end slavery, grew in strength; most abolitionist societies and supporters were in the North. They worked to raise awareness about the evils of slavery, and to build support for abolition.\n\nThis struggle took place amid strong support for slavery among white Southerners, who profited greatly from the system of enslaved labor. But slavery was entwined with the national economy; for instance, the banking, shipping and manufacturing industries of New York City all had strong economic interests in slavery, as did similar industries in other major port cities in the North. The northern textile mills in New York and New England processed Southern cotton and manufactured clothes to outfit slaves. By 1822 half of New York City's exports were related to cotton.\n\nSlaveholders began to refer to slavery as the \"peculiar institution\" to differentiate it from other examples of forced labor. They justified it as less cruel than the free labor of the North.\n\nThe principal organized bodies to advocate abolition and anti-slavery reforms in the north were the Pennsylvania Abolition Society and the New York Manumission Society. Before the 1830s the antislavery groups called for gradual emancipation. \nBy the late 1820s, under the impulse of religious evangelicals, the sense emerged that owning slaves was a sin and the owner had to immediately free himself from this grave sin by emancipation.\n\nIn the early part of the 19th century, other organizations were founded to take action on the future of black Americans. Some advocated removing free black people from the United States to places where they would enjoy greater freedom; some endorsed colonization in Africa, while others advocated emigration. During the 1820s and 1830s, the American Colonization Society (ACS) was the primary organization to implement the \"return\" of black Americans to Africa. The ACS was made up mostly of Quakers and slaveholders, who found uneasy common ground in support of \"repatriation\". But, by this time, most black Americans were native-born and did not want to emigrate; rather, they wanted full rights in the United States, where their people had lived and worked for generations.\n\nIn 1822 the ACS established the colony of Liberia in West Africa. The ACS assisted thousands of freedmen and free blacks (with legislated limits) to emigrate there from the United States. Many white people considered this preferable to emancipation in the United States. Henry Clay, one of the founders and a prominent slaveholder politician from Kentucky, said that blacks faced\n\nunconquerable prejudice resulting from their color, they never could amalgamate with the free whites of this country. It was desirable, therefore, as it respected them, and the residue of the population of the country, to drain them off.\n\nAfter 1830, abolitionist and minister William Lloyd Garrison promoted emancipation, characterizing slaveholding as a personal sin. He demanded that slaveowners repent and start the process of emancipation. His position increased defensiveness on the part of some southerners, who noted the long history of slavery among many cultures. A few abolitionists, such as John Brown, favored the use of armed force to foment uprisings among the slaves, as he did at Harper's Ferry. Most abolitionists tried to raise public support to change laws and to challenge slave laws. Abolitionists were active on the lecture circuit in the North, and often featured escaped slaves in their presentations. The eloquent Frederick Douglass became an important abolitionist leader after escaping from slavery. Harriet Beecher Stowe's novel \"Uncle Tom's Cabin\" (1852) was an international bestseller and aroused popular sentiment against slavery. It also provoked the publication of numerous anti-Tom novels by Southerners in the years before the American Civil War.\n\nWhile under the Constitution, Congress could not prohibit the import slave trade until 1808, the third Congress regulated it in the Slave Trade Act of 1794, which prohibited shipbuilding and outfitting for the trade. Subsequent acts in 1800 and 1803 sought to discourage the trade by limiting investment in import trading and prohibiting importation into states that had abolished slavery, which most in the North had by that time. The final Act Prohibiting Importation of Slaves was adopted in 1807, effective in 1808. However, illegal importation of African slaves (smuggling) was common.\n\nAfter Great Britain and the United States outlawed the international slave trade in 1807, British slave trade suppression activities began in 1808 through diplomatic efforts and formation of the Royal Navy's West Africa Squadron. From 1819, they were assisted by forces from the United States Navy. With the Webster-Ashburton Treaty of 1842, the relationship with Britain was formalized, and the two countries jointly ran the Blockade of Africa with their navies.\n\nAlthough Virginia, Maryland, and Delaware were slave states, the latter two already had a high proportion of free blacks by the outbreak of war. Following the Revolution, the three legislatures made manumission easier, allowed by deed or will. Quaker and Methodist ministers particularly urged slaveholders to free their slaves. The number and proportion of freed slaves in these states rose dramatically until 1810. More than half of the number of free blacks in the United States were concentrated in the Upper South. The proportion of free blacks among the black population in the Upper South rose from less than one percent in 1792 to more than 10 percent by 1810. In Delaware, nearly 75 percent of blacks were free by 1810.\n\nIn the US as a whole, by 1810 the number of free blacks reached 186,446, or 13.5 percent of all blacks. After that period few slaves were freed, as the development of cotton plantations featuring short-staple cotton in the Deep South drove up the internal demand for slaves in the domestic slave trade and high prices were paid.\n\nThe growing international demand for cotton led many plantation owners further west in search of suitable land. In addition, the invention of the cotton gin in 1793 enabled profitable processing of short-staple cotton, which could readily be grown in the uplands. The invention revolutionized the cotton industry by increasing fifty-fold the quantity of cotton that could be processed in a day. At the end of the War of 1812, fewer than 300,000 bales of cotton were produced nationally. By 1820 the amount of cotton produced had increased to 600,000 bales, and by 1850 it had reached 4,000,000. There was an explosive growth of cotton cultivation throughout the Deep South and greatly increased demand for slave labor to support it. As a result, manumissions decreased dramatically in the South. \n\nMost of the slaves sold from the Upper South were from Maryland, Virginia, and the Carolinas, where changes in agriculture decreased the need for their labor and the demand for slaves. Before 1810, primary destinations for the slaves who were sold were Kentucky and Tennessee, but after 1810 Georgia, Alabama, Mississippi, Louisiana and Texas of the Deep South received the most slaves. This is where cotton became king. Kentucky and Tennessee joined the slave exporting states.\n\nBy 1815, the domestic slave trade had become a major economic activity in the United States; it lasted until the 1860s. Between 1830 and 1840 nearly 250,000 slaves were taken across state lines. In the 1850s more than 193,000 were transported, and historians estimate nearly one million in total took part in the forced migration of this new Middle Passage. By 1860 the slave population in the United States had reached 4 million. Of all 1,515,605 free families in the fifteen slave states in 1860, nearly 400,000 held slaves (roughly one in four, or 25%), amounting to 8% of all American families.\nThe historian Ira Berlin called this forced migration of slaves the \"Second Middle Passage\", because it reproduced many of the same horrors as the Middle Passage (the name given to the transportation of slaves from Africa to North America). These sales of slaves broke up many families and caused much hardship. Characterizing it as the \"central event\" in the life of a slave between the American Revolution and the Civil War, Berlin wrote that whether slaves were directly uprooted or lived in fear that they or their families would be involuntarily moved, \"the massive deportation traumatized black people, both slave and free.\" Individuals lost their connection to families and clans. Added to the earlier colonists combining slaves from different tribes, many ethnic Africans lost their knowledge of varying tribal origins in Africa. Most were descended from families who had been in the United States for many generations.\n\nIn the 1840s, almost 300,000 slaves were transported, with Alabama and Mississippi receiving 100,000 each. During each decade between 1810 and 1860, at least 100,000 slaves were moved from their state of origin. In the final decade before the Civil War, 250,000 were moved. Michael Tadman wrote in \"Speculators and Slaves: Masters, Traders, and Slaves in the Old South\" (1989) that 60–70% of inter-regional migrations were the result of the sale of slaves. In 1820 a child in the Upper South had a 30% chance of being sold south by 1860. The death rate for the slaves on their way to their new destination across the American South was less than that suffered by captives shipped across the Atlantic Ocean, but mortality was higher than the normal death rate.\n\nSlave traders transported two-thirds of the slaves who moved west. Only a minority moved with their families and existing master. Slave traders had little interest in purchasing or transporting intact slave families; in the early years, planters demanded only the young male slaves needed for heavy labor. Later, in the interest of creating a \"self-reproducing labor force\", planters purchased nearly equal numbers of men and women. Berlin wrote:\n\nThe internal slave trade became the largest enterprise in the South outside the plantation itself, and probably the most advanced in its employment of modern transportation, finance, and publicity. The slave trade industry developed its own unique language, with terms such as \"prime hands, bucks, breeding wenches, and \"fancy girls\" coming into common use.\n\nThe expansion of the interstate slave trade contributed to the \"economic revival of once depressed seaboard states\" as demand accelerated the value of slaves who were subject to sale.\n\nSome traders moved their \"chattels\" by sea, with Norfolk to New Orleans being the most common route, but most slaves were forced to walk overland. Others were shipped downriver from such markets as Louisville on the Ohio River, and Natchez on the Mississippi. Traders created regular migration routes served by a network of slave pens, yards, and warehouses needed as temporary housing for the slaves. In addition, other vendors provided clothes, food, and supplies for slaves. As the trek advanced, some slaves were sold and new ones purchased. Berlin concluded, \"In all, the slave trade, with its hubs and regional centers, its spurs and circuits, reached into every cranny of southern society. Few southerners, black or white, were untouched.\"\n\nOnce the trip ended, slaves faced a life on the frontier significantly different from most labor in the Upper South. Clearing trees and starting crops on virgin fields was harsh and backbreaking work. A combination of inadequate nutrition, bad water, and exhaustion from both the journey and the work weakened the newly arrived slaves and produced casualties. New plantations were located at rivers' edges for ease of transportation and travel. Mosquitoes and other environmental challenges spread disease, which took the lives of many slaves. They had acquired only limited immunities to lowland diseases in their previous homes. The death rate was so high that, in the first few years of hewing a plantation out of the wilderness, some planters preferred whenever possible to use rented slaves rather than their own.\n\nThe harsh conditions on the frontier increased slave resistance and led owners and overseers to rely on violence for control. Many of the slaves were new to cotton fields and unaccustomed to the \"sunrise-to-sunset gang labor\" required by their new life. Slaves were driven much harder than when they had been in growing tobacco or wheat back east. Slaves had less time and opportunity to improve the quality of their lives by raising their own livestock or tending vegetable gardens, for either their own consumption or trade, as they could in the east.\n\nIn Louisiana, French colonists had established sugar cane plantations and exported sugar as the chief commodity crop. After the Louisiana Purchase in 1803, Americans entered the state and joined the sugar cultivation. Between 1810 and 1830, planters bought slaves from the North and the number of slaves increased from less than 10,000 to more than 42,000. Planters preferred young males, who represented two-thirds of the slave purchases. Dealing with sugar cane was even more physically demanding than growing cotton. The largely young, unmarried male slave force made the reliance on violence by the owners \"especially savage\".\n\nNew Orleans became nationally important as a slave market and port, as slaves were shipped from there upriver by steamboat to plantations on the Mississippi River; it also sold slaves who had been shipped downriver from markets such as Louisville. By 1840, it had the largest slave market in North America. It became the wealthiest and the fourth-largest city in the nation, based chiefly on the slave trade and associated businesses. The trading season was from September to May, after the harvest.\n\nSlave traders were men of low reputation, even in the South. In the 1828 presidential election, candidate Andrew Jackson was strongly criticized by opponents as a slave trader who transacted in slaves in defiance of modern standards or morality.\n\nThe treatment of slaves in the United States varied widely depending on conditions, times and places. The power relationships of slavery corrupted many whites who had authority over slaves, with children showing their own cruelty. Masters and overseers resorted to physical punishments to impose their wills. Slaves were punished by whipping, shackling, hanging, beating, burning, mutilation, branding and imprisonment. Punishment was most often meted out in response to disobedience or perceived infractions, but sometimes abuse was carried out to re-assert the dominance of the master or overseer of the slave. Treatment was usually harsher on large plantations, which were often managed by overseers and owned by absentee slaveholders, conditions permitting abuses.\n\nWilliam Wells Brown, who escaped to freedom, reported that on one plantation, slave men were required to pick 80 pounds per day of cotton, while women were required to pick 70 pounds; if any slave failed in his or her quota, they were subject to whip lashes for each pound they were short. The whipping post stood next to the cotton scales. A New York man who attended a slave auction in the mid-19th century reported that at least three-quarters of the male slaves he saw at sale had scars on their backs from whipping. By contrast, small slave-owning families had closer relationships between the owners and slaves; this sometimes resulted in a more humane environment but was not a given.\n\nHistorian Lawrence M. Friedman wrote: \"Ten Southern codes made it a crime to mistreat a slave. … Under the Louisiana Civil Code of 1825 (art. 192), if a master was \"convicted of cruel treatment,\" the judge could order the sale of the mistreated slave, presumably to a better master.\" Masters and overseers were seldom prosecuted under these laws.\n\nAccording to Adalberto Aguirre, there were 1,161 slaves executed in the U.S. between the 1790s and 1850s. Quick executions of innocent slaves as well as suspects typically followed any attempted slave rebellions, as white militias overreacted with widespread killings that expressed their fears of rebellions, or suspected rebellions.\n\nAlthough most slaves had lives that were very restricted in terms of their movements and agency, exceptions existed to virtually every generalization; for instance, there were also slaves who had considerable freedom in their daily lives: slaves allowed to rent out their labor and who might live independently of their master in cities, slaves who employed white workers, and slave doctors who treated upper-class white patients. After 1820, in response to the inability to import new slaves from Africa and in part to abolitionist criticism, some slaveholders improved the living conditions of their slaves, to encourage them to be productive and to try to prevent escapes. It was part of a paternalistic approach in the antebellum era that was encouraged by ministers trying to use Christianity to improve the treatment of slaves. Slaveholders published articles in southern agricultural journals to share best practices in treatment and management of slaves; they intended to show that their system was better than the living conditions of northern industrial workers.\n\nMedical care for slaves was limited in terms of the medical knowledge available to anyone. It was generally provided by other slaves or by slaveholders' family members. Many slaves possessed medical skills needed to tend to each other, and used folk remedies brought from Africa. They also developed new remedies based on American plants and herbs.\n\nAccording to Andrew Fede, a master could be held criminally liable for killing a slave only if the slave he killed was \"completely submissive and under the master's absolute control\". For example, in 1791 the North Carolina legislature defined the willful killing of a slave as criminal murder, unless done in resisting or under moderate correction (that is, corporal punishment).\n\nBecause of the power relationships at work, slave women in the United States were at high risk for rape and sexual abuse. Many slaves fought back against sexual attacks, and some died resisting. Others carried psychological and physical scars from the attacks. Sexual abuse of slaves was partially rooted in a patriarchal Southern culture which treated black women as property or chattel. Southern culture strongly policed against sexual relations between white women and black men on the purported grounds of racial purity but, by the late 18th century, the many mixed-race slaves and slave children showed that white men had often taken advantage of slave women. Wealthy planter widowers, notably such as John Wayles and his son-in-law Thomas Jefferson, took slave women as concubines; each had six children with his partner: Elizabeth Hemings and her daughter Sally Hemings (the half-sister of Jefferson's late wife), respectively. Both Mary Chesnut and Fanny Kemble, wives of planters, wrote about this issue in the antebellum South in the decades before the Civil War. Sometimes planters used mixed-race slaves as house servants or favored artisans because they were their children or other relatives. As a result of centuries of slavery and such relationships, DNA studies have shown that the vast majority of African Americans also have historic European ancestry, generally through paternal lines.\nWhile slaves' living conditions were poor by modern standards, Robert Fogel argued that all workers, free or slave, during the first half of the 19th century were subject to hardship.\n\nTo help regulate the relationship between slave and owner, including legal support for keeping the slave as property, states established slave codes, most based on laws existing since the colonial era. The code for the District of Columbia defined a slave as \"a human being, who is by law deprived of his or her liberty for life, and is the property of another\".\n\nWhile each state had its own slave code, many concepts were shared throughout the slave states. According to the slave codes, some of which were passed in reaction to slave rebellions, teaching a slave to read or write was illegal. This prohibition was unique to American slavery, believed to reduce slaves forming aspirations that could lead to escape or rebellion. Informal education occurred when white children taught slave companions what they were learning; in other cases, adult slaves learned from free artisan workers, especially if located in cities, where there was more freedom of movement.\n\nIn Alabama, slaves were not allowed to leave their master's premises without written consent or passes. This was a common requirement in other states as well, and locally run patrols (known to slaves as \"pater rollers\") often checked the passes of slaves who appeared to be away from their plantations. In Alabama slaves were prohibited from trading goods among themselves. In Virginia, a slave was not permitted to drink in public within one mile of his master or during public gatherings. Slaves were not permitted to carry firearms in any of the slave states.\n\nSlaves were generally prohibited by law from associating in groups, with the exception of worship services (a reason why the Black church is such a notable institution in black communities today). Following Nat Turner's rebellion in 1831, which raised white fears throughout the South, some states also prohibited or restricted religious gatherings of slaves, or required that they be officiated by white men. Planters feared that group meetings would facilitate communication among slaves that could lead to rebellion. Slaves held private, secret \"brush meetings\" in the woods.\n\nIn Ohio, an emancipated slave was prohibited from returning to the state in which he or she had been enslaved. Other northern states discouraged the settling of free blacks within their boundaries. Fearing the influence of free blacks, Virginia and other southern states passed laws to require blacks who had been freed to leave the state within a year (or sometimes less time) unless granted a stay by an act of the legislature.\n\nThe United States Constitution, adopted in 1787, prevented Congress from completely banning the importation of slaves until 1808, although Congress regulated it in the Slave Trade Act of 1794, and in subsequent Acts in 1800 and 1803. After the Revolution, numerous states individually passed laws against importing slaves. By contrast, the states of Georgia and South Carolina reopened their trade due to demand by their upland planters, who were developing new cotton plantations: Georgia from 1800 until December 31, 1807, and South Carolina from 1804. In that period, Charleston traders imported about 75,000 slaves, more than were brought to South Carolina in the 75 years before the Revolution. Approximately 30,000 were imported to Georgia.\n\nBy January 1, 1808, when Congress banned further imports, South Carolina was the only state that still allowed importation of slaves. Congress allowed continued trade only in slaves who were descendants of those currently in the United States. In addition, US citizens could participate financially in the international slave trade and the outfitting of ships for that trade. The domestic slave trade became extremely profitable as demand rose with the expansion of cultivation in the Deep South for cotton and sugar cane crops. Slavery in the United States became, more or less, self-sustaining by natural increase among the current slaves and their descendants.\n\nDespite the ban, slave imports continued through smugglers bringing in slaves past the U.S. Navy's African Slave Trade Patrol to South Carolina, and overland from Texas and Florida, both under Spanish control. Congress increased the punishment associated with importing slaves, classifying it in 1820 as an act of piracy, with smugglers subject to harsh penalties, including death if caught. After that, \"it is unlikely that more than 10,000 [slaves] were successfully landed in the United States.\" But, some smuggling of slaves into the United States continued until just before the start of the Civil War; see \"Wanderer\" (slave ship) and \"Clotilde\" (slave ship)\n\nDuring the War of 1812, British Royal Navy commanders of the blockading fleet, based at the Bermuda dockyard, were instructed to offer freedom to defecting American slaves, as the Crown had during the Revolutionary War. Thousands of escaped slaves went over to the Crown with their families. Men were recruited into the Corps of Colonial Marines on occupied Tangier Island, in the Chesapeake Bay.\n\nThe freedmen fought for Britain throughout the Atlantic campaign, including the attack on Washington D.C. and the Louisiana Campaign. Seven hundred of these ex-marines were granted land (they reportedly organised themselves in villages along the lines of their military companies). Many other freed American slaves were recruited directly into existing West Indian regiments, or newly created British Army units The British later resettled a few thousand freed slaves at Nova Scotia, as they had for freedmen after the Revolution. Some of the earlier freedmen had migrated to Sierra Leone in the late 18th century, when it was established as a British colony. Descendants have established the Black Loyalist Heritage Museum and website.\n\nSlaveholders, primarily in the South, had considerable \"loss of property\" as thousands of slaves escaped to British lines or ships for freedom, despite the difficulties. The planters' complacency about slave \"contentment\" was shocked by seeing that slaves would risk so much to be free. Afterward, when some freed slaves had been settled at Bermuda, slaveholders such as Major Pierce Butler of South Carolina tried to persuade them to return to the United States, to no avail.\n\nThe Americans protested that Britain's failure to return all slaves violated the Treaty of Ghent. After arbitration by the Tsar of Russia, the British paid $1,204,960 in damages (about $ million in today's money) to Washington, which reimbursed the slaveowners.\n\nPrior to the American Revolution, masters and revivalists spread Christianity to slave communities, supported by the Society for the Propagation of the Gospel. In the First Great Awakening of the mid-18th century, Baptists and Methodists from New England preached a message against slavery, encouraged masters to free their slaves, converted both slaves and free blacks, and gave them active roles in new congregations. The first independent black congregations were started in the South before the Revolution, in South Carolina and Georgia.\n\nOver the decades and with the growth of slavery throughout the South, Baptist and Methodist ministers gradually changed their messages to accommodate the institution. After 1830, white Southerners argued for the compatibility of Christianity and slavery, with a multitude of both Old and New Testament citations. They promoted Christianity as encouraging better treatment of slaves and argued for a paternalistic approach. In the 1840s and 1850s, the issue of accepting slavery split the nation's largest religious denominations (the Methodist, Baptist and Presbyterian churches) into separate Northern and Southern organizations see Methodist Episcopal Church, South, Southern Baptist Convention, and Presbyterian Church in the Confederate States of America).\n\nSouthern slaves generally attended their masters' white churches, where they often outnumbered the white congregants. They were usually permitted to sit only in the back or in the balcony. They listened to white preachers, who emphasized the obligation of slaves to keep in their place, and acknowledged the slave's identity as both person and property. Preachers taught the masters responsibility and the concept of appropriate paternal treatment, using Christianity to improve conditions for slaves, and to treat them \"justly and fairly\" (Col. 4:1). This included masters having self-control, not disciplining under anger, not threatening, and ultimately fostering Christianity among their slaves by example.\n\nSlaves also created their own religious observances, meeting alone without the supervision of their white masters or ministers. The larger plantations with groups of slaves numbering twenty, or more, tended to be centers of nighttime meetings of one or several plantation slave populations. These congregations revolved around a singular preacher, often illiterate with limited knowledge of theology, who was marked by his personal piety and ability to foster a spiritual environment. African Americans developed a theology related to Biblical stories having the most meaning for them, including the hope for deliverance from slavery by their own Exodus. One lasting influence of these secret congregations is the African-American spiritual.\n\nAccording to Herbert Aptheker, \"there were few phases of ante-bellum Southern life and history that were not in some way influenced by the fear of, or the actual outbreak of, militant concerted slave action.\"\n\nHistorians in the 20th century identified 250 to 311 slave uprisings in U.S. and colonial history. Those after 1776, include:\n\nIn 1831, Nat Turner, a literate slave who claimed to have spiritual visions, organized a slave rebellion in Southampton County, Virginia; it was sometimes called the Southampton Insurrection. Turner and his followers killed nearly 60 white inhabitants, mostly women and children. Many of the men in the area were attending a religious event in North Carolina. Eventually Turner was captured with 17 other rebels, who were subdued by the militia. Turner and his followers were hanged, and Turner's body was flayed. In a frenzy of fear and retaliation, the militia killed more than 100 slaves who had not been involved in the rebellion. Planters whipped hundreds of innocent slaves to ensure resistance was quelled.\n\nThis rebellion prompted Virginia and other slave states to pass more restrictions on slaves and free people of color, controlling their movement and requiring more white supervision of gatherings. In 1835 North Carolina withdrew the franchise for free people of color, and they lost their vote.\n\n\"See also\": Anti-literacy law\n\nAcross the South, white legislatures enacted harsh new laws to curtail the already limited rights of African Americans. Virginia prohibited blacks, free or slave, from practicing preaching, prohibited blacks from owning firearms, and forbade anyone to teach slaves or free blacks how to read. It specified heavy penalties for both student and teacher if slaves were educated, including whippings or jail.\n\n[E]very assemblage of negroes for the purpose of instruction in reading or writing, or in the night time for any purpose, shall be an unlawful assembly. Any justice may issue his warrant to any office or other person, requiring him to enter any place where such assemblage may be, and seize any negro therein; and he, or any other justice, may order such negro to be punished with stripes.\n\nUnlike in the South, slave owners in Utah were required to send their slaves to school. Black slaves did not have to spend as much time in school as Indian slaves.\n\nEli Whitney's invention of the cotton gin in 1793, made processing of short-staple cotton profitable, and it was cultivated throughout the South to satisfy US and international demand. Statistical data shows that 7% of the slaves (680,000 total in 1790 of 720,000 blacks) were in the North, population of 2 million. There had been approximately 15,000 slaves in New England in 1770 of 650,000 inhabitants. 35,000 slaves live in the Mid-Atlantic States of 600,000 inhabitants of whom 19,000 lived in New York where they made up 11% of the population. By 1790 Virginia held 44% (315,000 in a total population of 750,000 the State). It was common in agriculture, with a more massive presence in the South – the region where climate was more propitious for widescale agricultural activity. By 1790 slavery in the New England States was abolished in Massachusetts, New Hampshire and Vermont and phased out in Rhode Island and Connecticut. New York introduced gradual emancipation in 1799 (completed in 1827). Pennsylvania abolished slavery during the War for Independence.\n\nSome economists and historians regard slavery as a profitable system. They do not fully account for the government costs necessary to maintain the institution, nor for human suffering. The transition from indentured servants to slaves is cited to show that slaves offered greater profits to their owners. Thus, it is the near-universal consensus among economic historians and economists that slavery was not \"a system irrationally kept in existence by plantation owners who failed to perceive or were indifferent to their best economic interests\". The relative price of slaves and indentured servants in the antebellum period did decrease. Indentured servants became more costly with the increase in the demand of skilled labor in England. At the same time, slaves were mostly supplied from within the United States and thus language was not a barrier, and the cost of transporting slaves from one state to another was relatively low. In the decades preceding the civil war, the United States experienced a rapid natural increase of black population. The slave population multiplied nearly fourfold between 1810 and 1860, although the international slave trade was banned in 1808. Thus, it is also the universal consensus among modern economic historians and economists that slavery in the United States was not \"economically moribund on the eve of the Civil War\".\n\nRobert Fogel and Stanley Engerman, in their 1974 book \"Time on the Cross\", argued that the rate of return of slavery at the market price was close to 10 percent, a number close to investment in other assets. Fogel's 1989 work, \"Without Consent or Contract: The Rise and Fall of American Slavery\", elaborated on the moral indictment of slavery which ultimately led to its abolition.\n\nScholars disagree on how to quantify efficiency of slavery. In \"Time on the Cross\", Fogel and Engerman equate efficiency to total factor productivity (TFP)—the output per average unit of input on a farm. Using this measurement, southern farms that enslaved black people using the Gang System were 35% more efficient than Northern farms which used free labor. Under the Gang System, groups of slaves perform synchronized tasks under the constant vigilance of an overseer. Each group was like a part of a machine. If perceived to be working below his capacity, a slave could be punished. Fogel argues that this kind of negative enforcement was not frequent and that slaves and free laborers had similar quality of life; however, there is controversy on this last point. A critique of Fogel and Engerman's view was published by Paul A. David in 1976. In 1995, a random survey of 178 members of the Economic History Association sought to study the views of economists and economic historians on the debate. The study found that 72 percent of economists and 65 percent of economic historians would generally agree that \"Slave agriculture was efficient compared with free agriculture. Economies of scale, effective management, and intensive utilization of labor and capital made southern slave agriculture considerably more efficient than nonslave southern farming.\" 48 percent of the economists agreed without provisos, while 24 percent agreed when provisos were included in the statement. On the other hand, 58 percent of economic historians and 42 percent of economists disagreed with Fogel and Engerman's \"proposition that the material (not psychological) conditions of the lives of slaves compared favorably with those of free industrial workers in the decades before the Civil War\".\n\nControlling for inflation, prices of slaves rose dramatically in the six decades prior to Civil War, reflecting demand due to commodity cotton, as well as use of slaves in shipping and industry. Although the prices of slaves relative to indentured servants declined, both got more expensive. Cotton production was rising and relied on the use of slaves to yield high profits. Fogel and Engeman initially argued that if the Civil War had not happened, the slave prices would have increased even more, an average of more than 50 percent by 1890.\n\nPrices reflected the characteristics of the slave—such factors as sex, age, nature, and height were all taken into account to determine the price of a slave. Over the life-cycle, the price of enslaved women was higher than their male counterparts up to puberty age, as they would likely bear children and produce more slaves, in addition to serving as laborers. Men around the age of 25 were the most valued, as they were at the highest level of productivity and still had a considerable life-span. If slaves had a history of fights or escapes, their price was lowered reflecting what planters believed was risk of repeating such behavior. Slave traders and buyers would examine a slave's back for whipping scars—a large number of injuries would be seen as evidence of laziness or rebelliousness, rather than the previous master's brutality, and would lower the slave's price. Taller male slaves were priced at a higher level, as height was viewed as a proxy for fitness and productivity.\n\nThe conditions of the market led to shocks in the supply and demand of slaves, which in turn changed prices. For instance, slaves became more expensive after the decrease in supply caused by the ban on importation of slaves in 1808. The market for the products of their work also affected slaves' economic value: demand for slaves fell with the price of cotton in 1840. Anticipation of changes also had a huge influence on prices. As the civil war progressed, there was great doubt that slavery would continue to be legal, and prime males in New Orleans were sold at $1,116 by 1862 as opposed to $1,381 in 1861.\n\nWhile slavery brought profits in the short run, discussion continues on the economic benefits of slavery in the long-run. In 1995, a random anonymous survey of 178 members of the Economic History Association found that out of the 40 propositions about American economic history that were surveyed, the propositions most disputed by economic historians and economists were those surrounding the postbellum economy of the American South. The only exception was the proposition initially put forward by historian Gavin Wright that the \"modern period of the South's economic convergence to the level of the North only began in earnest when the institutional foundations of the southern regional labor market were undermined, largely by federal farm and labor legislation dating from the 1930s.\" 62 percent of economists (24 percent with and 38 percent without provisos) and 73 percent of historians (23 percent with and 50 percent without provisos) agreed with this statement. Wright has also argued that the private investment of monetary resources in the cotton industry, among others, delayed development in the South of commercial and industrial institutions. There was little public investment in railroads or other infrastructure. Wright argues that agricultural technology was far more developed in the South, representing an economic advantage of the South over the North of the United States. \n\nIn \"Democracy in America\", Alexis de Tocqueville noted that \"the colonies in which there were no slaves became more populous and more rich than those in which slavery flourished.\" Economists Peter H. Lindert and Jeffrey G. Williamson, in a pair of articles published in 2012 and 2013, found that, despite the American South initially having per capita income roughly double that of the North in 1774, incomes in the South had declined 27% by 1800 and continued to decline over the next four decades, while the economies in New England and the Mid-Atlantic states vastly expanded. By 1840, per capita income in the South was well behind the Northeast and the national average. (Note: This is also true of contemporary incomes in the United States in the early 21st century.)\n\nLindert and Williamson argue that this antebellum period is exemplary of what economists Daron Acemoglu, Simon Johnson, and James A. Robinson call \"a reversal of fortune\". Economist Thomas Sowell, in his essay \"The Real History of Slavery,\" confirms the observation made by de Tocqueville, by comparing slavery in the United States to slavery in Brazil. He notes that slave societies reflected similar economic trends in those and other parts of the world, suggesting that the trend Lindert and Williamson identify may have continued until the American Civil War:\n\nSowell also notes in \"Ethnic America: A History\", citing historians Clement Eaton and Eugene Genovese, that three-quarters of Southern white families owned no slaves at all. Most slaveholders lived on farms rather than plantations, and few plantations were as large as the fictional ones depicted in \"Gone with the Wind\". In \"The Real History of Slavery,\" Sowell draws the following conclusion regarding the macroeconomic value of slavery:\n\nIn short, even though some individual slaveowners grew rich and some family fortunes were founded on the exploitation of slaves, that is very different from saying that the whole society, or even its non-slave population as a whole, was more economically advanced than it would have been in the absence of slavery. What this means is that, whether employed as domestic servants or producing crops or other goods, millions suffered exploitation and dehumanization for no higher purpose than the...aggrandizement of slaveowners.\n\nBecause of the three-fifths compromise in the U.S. Constitution, in which slaves counted in the calculation of how many representatives a state had in Congress (though only three-fifths as much as a free person), the planter class had long held power in Congress out of proportion to the total number of free people in the US population as a whole.\n\nIn 1850, Congress passed the Fugitive Slave Act, which required law enforcement and citizens of free states to cooperate in the capture and return of slaves. This met with considerable overt and covert resistance in free states and cities such as Philadelphia, New York, and Boston. Refugees from slavery continued to flee the South across the Ohio River and other parts of the Mason–Dixon line dividing North from South, to the North and Canada via the Underground Railroad. Some white northerners helped hide former slaves from their former owners or helped them reach freedom in Canada.\n\nAs part of the Compromise of 1850, Congress abolished the domestic slave trade (though not the legality of slavery) in the District of Columbia. After 1854, Republicans argued that the Slave Power, especially the pro-slavery Democratic Party, controlled two of the three branches of the Federal government.\n\nThe abolitionists, realizing that the total elimination of slavery was, as an immediate goal, unrealistic, had worked to prevent expansion of slavery into the new states formed out of the Western territories. The Missouri Compromise, the Compromise of 1850, and the Bleeding Kansas crisis dealt with whether new states would be slave or free, or how that was to be decided. Both sides were anxious about effects of these decisions on the balance of power in the Senate.\nAfter the passage of the Kansas–Nebraska Act in 1854, border fighting broke out in Kansas Territory, where the question of whether it would be admitted to the Union as a slave or free state was left to the inhabitants. Migrants from free and slave states moved into the territory to prepare for the vote on slavery. Abolitionist John Brown was active in the fighting in \"Bleeding Kansas,\" but so too were many white Southerners who opposed abolition.\n\nAbraham Lincoln's and the Republicans' political platform in 1860 was to stop slavery's expansion. Historian James McPherson says that in a famous speech in 1858, Lincoln said American republicanism can be purified by restricting the further expansion of slavery as the first step to putting it on the road to 'ultimate extinction.' Southerners took Lincoln at his word. When he won the presidency they left the Union to escape the 'ultimate extinction' of slavery.\"\n\nWith the development of slave and free states after the American Revolution, and far-flung commercial and military activities, new situations arose in which slaves might be taken by masters into free states. Most free states not only prohibited slavery, but ruled that slaves brought and kept there illegally could be freed. Such cases were sometimes known as transit cases.\n\nDred Scott and his wife Harriet Scott each sued for freedom in St. Louis after the death of their master, based on their having been held in a free territory (the northern part of the Louisiana Purchase from which slavery was excluded under the terms of the Missouri Compromise). (Later the two cases were combined under Dred Scott's name.) Scott filed suit for freedom in 1846 and went through two state trials, the first denying and the second granting freedom to the couple (and, by extension, their two daughters, who had also been held illegally in free territories). For 28 years, Missouri state precedent had generally respected laws of neighboring free states and territories, ruling for freedom in such transit cases where slaves had been held illegally in free territory. But in the Dred Scott case, the State Supreme Court ruled against the slaves, saying that \"times were not what they once were\".\n\nAfter Scott and his team appealed the case to the U.S. Supreme Court, the slaveowning Supreme Court Justice Roger B. Taney denied Scott his freedom in a sweeping decision. The 1857 decision, decided 7–2, held that a slave did not become free when taken into a free state; Congress could not bar slavery from a territory; and people of African descent imported into the United States and held as slaves, or their descendants, could never be citizens. A state could not bar slaveowners from bringing slaves into that state. Many Republicans, including Abraham Lincoln, considered the decision unjust and as proof that the Slave Power had seized control of the Supreme Court. Written by Chief Justice Roger B. Taney, the decision effectively barred slaves and their descendants from citizenship. Abolitionists were enraged and slave owners encouraged, contributing to tensions on this subject that led to civil war. Critics note that at the time the Constitution was drafted, five states including North Carolina allowed free blacks to vote.\n\nThe divisions became fully exposed with the 1860 presidential election. The electorate split four ways. The Southern Democrats endorsed slavery, while the Republicans denounced it. The Northern Democrats said democracy required the people to decide on slavery locally, state by state and territory by territory. The Constitutional Union Party said the survival of the Union was at stake and everything else should be compromised.\n\nLincoln, the Republican, won with a plurality of popular votes and a majority of electoral votes. Lincoln, however, did not appear on the ballots of ten southern slave states. Many slave owners in the South feared that the real intent of the Republicans was the abolition of slavery in states where it already existed, and that the sudden emancipation of four million slaves would be disastrous for the slave owners and for the economy that drew its greatest profits from the labor of people who were not paid.\n\nThe slave owners also argued that banning slavery in new states would upset what they saw as a delicate balance of free states and slave states. They feared that ending this balance could lead to the domination of the federal government by the northern free states. This led seven southern states to secede from the Union. When the southern forces attacked a US Army installation at Fort Sumter, the American Civil War began and four additional slave states seceded. Northern leaders had viewed the slavery interests as a threat politically, but with secession, they viewed the prospect of a new Southern nation, the Confederate States of America, with control over the Mississippi River and parts of the West, as politically unacceptable.\n\nThe consequent American Civil War, beginning in 1861, led to the end of chattel slavery in America. Not long after the war broke out, through a legal maneuver credited to Union General Benjamin F. Butler, a lawyer by profession, slaves who came into Union \"possession\" were considered \"contraband of war\". General Butler ruled that they were not subject to return to Confederate owners as they had been before the war. Soon word spread, and many slaves sought refuge in Union territory, desiring to be declared \"contraband\". Many of the \"contrabands\" joined the Union Army as workers or troops, forming entire regiments of the U.S. Colored Troops. Others went to refugee camps such as the Grand Contraband Camp near Fort Monroe or fled to northern cities. General Butler's interpretation was reinforced when Congress passed the Confiscation Act of 1861, which declared that any property used by the Confederate military, including slaves, could be confiscated by Union forces.\nAt the beginning of the war, some Union commanders thought they were supposed to return escaped slaves to their masters. By 1862, when it became clear that this would be a long war, the question of what to do about slavery became more general. The Southern economy and military effort depended on slave labor. It began to seem unreasonable to protect slavery while blockading Southern commerce and destroying Southern production. As Congressman George W. Julian of Indiana put it in an 1862 speech in Congress, the slaves \"cannot be neutral. As laborers, if not as soldiers, they will be allies of the rebels, or of the Union.\" Julian and his fellow Radical Republicans put pressure on Lincoln to rapidly emancipate the slaves, whereas moderate Republicans came to accept gradual, compensated emancipation and colonization. Copperheads, the border states and War Democrats opposed emancipation, although the border states and War Democrats eventually accepted it as part of total war needed to save the Union.\n\nThe Emancipation Proclamation was an executive order issued by President Lincoln on January 1, 1863. In a single stroke it changed the legal status, as recognized by the U.S. government, of 3 million slaves in designated areas of the Confederacy from \"slave\" to \"free\". It had the practical effect that as soon as a slave escaped the control of the Confederate government, by running away or through advances of federal troops, the slave became legally and actually free. Plantation owners, realizing that emancipation would destroy their economic system, sometimes moved their slaves as far as possible out of reach of the Union army. By June 1865, the Union Army controlled all of the Confederacy and had liberated all of the designated slaves.\n\nIn 1861, Lincoln expressed the fear that premature attempts at emancipation would mean the loss of the border states. He believed that \"to lose Kentucky is nearly the same as to lose the whole game.\" At first, Lincoln reversed attempts at emancipation by Secretary of War Simon Cameron and Generals John C. Fremont (in Missouri) and David Hunter (in South Carolina, Georgia and Florida) to keep the loyalty of the border states and the War Democrats.\n\nLincoln mentioned his Emancipation Proclamation to members of his cabinet on July 21, 1862. Secretary of State William H. Seward told Lincoln to wait for a victory before issuing the proclamation, as to do otherwise would seem like \"our last shriek on the retreat\". In September 1862 the Battle of Antietam provided this opportunity, and the subsequent War Governors' Conference added support for the proclamation. Lincoln had already published a letter encouraging the border states especially to accept emancipation as necessary to save the Union. Lincoln later said that slavery was \"somehow the cause of the war\".\n\nLincoln issued his preliminary Emancipation Proclamation on September 22, 1862, and said that a final proclamation would be issued if his gradual plan, based on compensated emancipation and voluntary colonization, was rejected. Only the District of Columbia accepted Lincoln's gradual plan, and Lincoln issued his final Emancipation Proclamation on January 1, 1863. In his letter to Hodges, Lincoln explained his belief that\n\nLincoln's Emancipation Proclamation of January 1, 1863 was a powerful action that promised freedom for slaves in the Confederacy as soon as the Union armies reached them, and authorized the enlistment of African Americans in the Union Army. The Emancipation Proclamation did not free slaves in the Union-allied slave-holding states that bordered the Confederacy. Since the Confederate States did not recognize the authority of President Lincoln, and the proclamation did not apply in the border states, at first the proclamation freed only those slaves who had escaped behind Union lines. The proclamation made the abolition of slavery an official war goal that was implemented as the Union took territory from the Confederacy. According to the Census of 1860, this policy would free nearly four million slaves, or over 12% of the total population of the United States.\n\nBased on the President's war powers, the Emancipation Proclamation applied to territory held by Confederates at the time. However, the Proclamation became a symbol of the Union's growing commitment to add emancipation to the Union's definition of liberty. Lincoln played a leading role in getting the constitutionally-required two-thirds majority of both houses of Congress to vote for the Thirteenth Amendment, which made emancipation universal and permanent.\n\nEnslaved African Americans had not waited for Lincoln before escaping and seeking freedom behind Union lines. From early years of the war, hundreds of thousands of African Americans escaped to Union lines, especially in Union-controlled areas such as Norfolk and the Hampton Roads region in 1862 Virginia, Tennessee from 1862 on, the line of Sherman's march, etc. So many African Americans fled to Union lines that commanders created camps and schools for them, where both adults and children learned to read and write. The American Missionary Association entered the war effort by sending teachers south to such contraband camps, for instance, establishing schools in Norfolk and on nearby plantations.\n\nIn addition, nearly 200,000 African-American men served with distinction in the Union forces as soldiers and sailors. Most were escaped slaves. The Confederacy was outraged by armed black soldiers and refused to treat them as prisoners of war. They murdered many, as at the Fort Pillow Massacre, and re-enslaved others.\n\nThe Arizona Organic Act abolished slavery on February 24, 1863 in the newly formed Arizona Territory. Tennessee and all of the border states (except Kentucky) abolished slavery by early 1865. Thousands of slaves were freed by the operation of the Emancipation Proclamation as Union armies marched across the South. Emancipation came to the remaining southern slaves after the surrender of all Confederate troops in spring 1865.\n\nIn spite of the South's shortage of manpower, until 1865, most Southern leaders opposed arming slaves as soldiers. However, a few Confederates discussed arming slaves. Finally in early 1865 General Robert E. Lee said black soldiers were essential, and legislation was passed. The first black units were in training when the war ended in April.\n\nBooker T. Washington remembered Emancipation Day in early 1863, when he was a boy of nine in Virginia:\n\nThe war ended on June 22, 1865, and following that surrender, the Emancipation Proclamation was enforced throughout remaining regions of the South that had not yet freed the slaves. Slavery officially continued for a couple of months in other locations. Federal troops arrived in Galveston, Texas on June 19, 1865, to enforce the emancipation. That day of gaining freedom in Texas is now celebrated as Juneteenth in many U.S. states.\n\nThe Thirteenth Amendment, abolishing slavery except as punishment for a crime, had been passed by the Senate in April 1864, and by the House of Representatives in January 1865.\nThe amendment did not take effect until it was ratified by three fourths of the states, which occurred on December 6, 1865, when Georgia ratified it. On that date, all remaining slaves became officially free.\n\nLegally, the last 40,000-45,000 slaves were freed in the last two slave states of Kentucky and Delaware by the final ratification of the Thirteenth Amendment to the Constitution on December 18, 1865. Slaves still held in Tennessee, Kentucky, Kansas, New Jersey, Delaware, West Virginia, Maryland, Missouri, Washington, D.C., and twelve parishes of Louisiana also became legally free on this date.\n\nAmerican historian R.R. Palmer opined that the abolition of slavery in the United States without compensation to the former slave owners was an \"annihilation of individual property rights without parallel...in the history of the Western world\". Economic historian Robert E. Wright argues that it would have been much cheaper, with minimal deaths, if the federal government had purchased and freed all the slaves, rather than fighting the Civil War. Another economic historian, Roger Ransom, writes about how Gerald Gunderson compared compensated emancipation to the cost of the war and \"notes that the two are roughly the same order of magnitude — 2.5 to 3.7 billion dollars\". Ransom also writes that compensated emancipation would have tripled federal outlays if paid over the period of 25 years and was a program that had no political support within the United States during the 1860s.\n\nProponents of the 13th Amendment to the Constitution knew that without legislation that codified the 13th Amendment in the form of laws and statutes along with law enforcement agencies to uphold the laws, there would be no true end to slavery, and this is the reason for the inclusion of Section 2 of the 13th Amendment authorizing Congress to establish laws upholding the amendment. The federal government also sent troops to the south to provide protection to the former slaves who were still living among their former masters.\n\nDuring the Reconstruction Era, from January 1, 1863 to March 31, 1877, federal troops were stationed in the south specifically to protect black rights and prevent them from being re-enslaved. However, in the Gilded Age that followed the withdrawal, blacks were left at the mercy of the whites. When African Americans in the South no longer had the protection of federal troops, whites imposed laws to prevent them from voting, restrict their movement, and found other ways to practice involuntary servitude.\n\nThis lasted well into the 20th century. President Lyndon B. Johnson abolished peonage in 1966, which rapidly decreased sharecropping in every plantation nationwide. Journalist Douglas A. Blackmon reported in his Pulitzer Prize-winning book \"Slavery By Another Name\" that many blacks were virtually enslaved under convict leasing programs, which started after the Civil War. Most Southern states had no prisons; they leased convicts to businesses and farms for their labor, and the lessee paid for food and board. The incentives for abuse were satisfied.\n\nThe continued involuntary servitude took various forms, but the primary forms included convict leasing, peonage, and sharecropping, with the latter eventually encompassing poor whites as well. By the 1930s, whites constituted most of the sharecroppers in the South. Mechanization of agriculture had reduced the need for farm labor, and many blacks left the South in the Great Migration.\n\nJurisdictions and states created fines and sentences for a wide variety of minor crimes, and used these as an excuse to arrest and sentence blacks. Under convict leasing programs, African American men, often guilty of no crime at all, were arrested, compelled to work without pay, repeatedly bought and sold, and coerced to do the bidding of the leaseholder. Sharecropping, as it was practiced during this period, often involved severe restrictions on the freedom of movement of sharecroppers, who could be whipped for leaving the plantation. Both sharecropping and convict leasing were legal and tolerated by both the north and south. However, peonage was an illicit form of forced labor. Its existence was ignored by authorities while thousands of African Americans and poor Anglo Americans were subjugated and held in bondage until the mid 1960s to the late 1970s.\n\nWith the exception of cases of peonage, beyond the period of Reconstruction, the federal government took almost no action to enforce the 13th Amendment until December 1941 when President Franklin Delano Roosevelt summoned his attorney general. Five days after Pearl Harbor, at the request of the president Attorney General Francis Biddle issued to all federal prosecutors, instructing them to actively investigate and try any case of involuntary servitude or slavery. Several months later, convict leasing was officially abolished. But aspects have persisted in other forms, while historians argue that other systems of penal labor, were all created in 1865 and convict leasing was simply the most oppressive form.\n\nOver time a large civil rights movement arose to bring full civil rights and equality under the law to all Americans.\n\nWith emancipation a legal reality, white Southerners were concerned with both controlling the newly freed slaves and keeping them in the labor force at the lowest level. The system of convict leasing began during Reconstruction and was fully implemented in the 1880s and officially ending in the last state, Alabama, in 1928. It persisted in various forms until it was abolished in 1942 by President Franklin D. Roosevelt during World War II, several months after the attack on Pearl Harbor involved the U.S. in the conflict. This system allowed private contractors to purchase the services of convicts from the state or local governments for a specific time period. African Americans, due to \"vigorous and selective enforcement of laws and discriminatory sentencing,\" made up the vast majority of the convicts leased. Writer Douglas A. Blackmon writes of the system:\n\nThe constitutional basis for convict leasing is that the Thirteenth Amendment, while abolishing slavery and involuntary servitude generally, expressly permits it as a punishment for crime.\n\nThe anti-literacy laws after 1832 contributed greatly to the problem of widespread illiteracy facing the freedmen and other African Americans after Emancipation and the Civil War 35 years later. The problem of illiteracy and need for education was seen as one of the greatest challenges confronting these people as they sought to join the free enterprise system and support themselves during Reconstruction and thereafter.\n\nConsequently, many black and white religious organizations, former Union Army officers and soldiers, and wealthy philanthropists were inspired to create and fund educational efforts specifically for the betterment of African Americans; some African Americans had started their own schools before the end of the war. Northerners helped create numerous normal schools, such as those that became Hampton University and Tuskegee University, to generate teachers, as well as other colleges for former slaves. Blacks held teaching as a high calling, with education the first priority for children and adults. Many of the most talented went into the field. Some of the schools took years to reach a high standard, but they managed to get thousands of teachers started. As W. E. B. Du Bois noted, the black colleges were not perfect, but \"in a single generation they put thirty thousand black teachers in the South\" and \"wiped out the illiteracy of the majority of black people in the land\".\n\nNorthern philanthropists continued to support black education in the 20th century, even as tensions rose within the black community, exemplified by Booker T. Washington and W. E. B. Du Bois, as to the proper emphasis between industrial and classical academic education at the college level. An example of a major donor to Hampton Institute and Tuskegee was George Eastman, who also helped fund health programs at colleges and in communities. Collaborating with Washington in the early decades of the 20th century, philanthropist Julius Rosenwald provided matching funds for community efforts to build rural schools for black children. He insisted on white and black cooperation in the effort, wanting to ensure that white-controlled school boards made a commitment to maintain the schools. By the 1930s local parents had helped raise funds (sometimes donating labor and land) to create over 5,000 rural schools in the South. Other philanthropists, such as Henry H. Rogers and Andrew Carnegie, each of whom had arisen from modest roots to become wealthy, used matching fund grants to stimulate local development of libraries and schools.\n\nOn February 24, 2007, the Virginia General Assembly passed House Joint Resolution Number 728 acknowledging \"with profound regret the involuntary servitude of Africans and the exploitation of Native Americans, and call for reconciliation among all Virginians\". With the passing of this resolution, Virginia became the first state to acknowledge through the state's governing body their state's negative involvement in slavery. The passing of this resolution was in anticipation of the 400th anniversary commemoration of the founding of Jamestown, Virginia (the first permanent English settlement in North America), which was an early colonial slave port. Apologies have also been issued by Alabama, Florida, Maryland, North Carolina and New Jersey.\n\nOn July 30, 2008, the United States House of Representatives passed a resolution apologizing for American slavery and subsequent discriminatory laws.\n\nThe U.S. Senate unanimously passed a similar resolution on June 18, 2009, apologizing for the \"fundamental injustice, cruelty, brutality, and inhumanity of slavery\". It also explicitly states that it cannot be used for restitution claims.\n\nA 2016 study, published in The Journal of Politics, finds that \"Whites who currently live in Southern counties that had high shares of slaves in 1860 are more likely to identify as a Republican, oppose affirmative action, and express racial resentment and colder feelings toward blacks.\" The study contends that \"contemporary differences in political attitudes across counties in the American South in part trace their origins to slavery's prevalence more than 150 years ago. \" The authors argue that their findings are consistent with the theory that \"following the Civil War, Southern whites faced political and economic incentives to reinforce existing racist norms and institutions to maintain control over the newly freed African American population. This amplified local differences in racially conservative political attitudes, which in turn have been passed down locally across generations.\"\n\nA 2017 study in the \"British Journal of Political Science\" argued that the British American colonies without slavery adopted better democratic institutions in order to attract migrant workers to their colonies.\n\nDuring the 16th, 17th and 18th centuries, Indian slavery, the enslavement of Native Americans by European colonists, was common. Many of these Native slaves were exported to the Northern colonies and to off-shore colonies, especially the \"sugar islands\" of the Caribbean. The exact number of Native Americans who were enslaved is unknown because vital statistics and census reports were at best infrequent. Historian Alan Gallay estimates that from 1670 to 1715, British slave traders sold between 24,000 and 51,000 Native Americans from what is now the southern part of the U.S. Andrés Reséndez estimates that between 147,000 and 340,000 Native Americans were enslaved in North America, excluding Mexico. Even after the Indian Slave Trade ended in 1750 the enslavement of Native Americans continued in the west, and also in the Southern states mostly through kidnappings.\n\nSlavery of Native Americans was organized in colonial and Mexican California through Franciscan missions, theoretically entitled to ten years of Native labor, but in practice maintaining them in perpetual servitude, until their charge was revoked in the mid-1830s. Following the 1847–48 invasion by U.S. troops, the \"loitering or orphaned Indians\" were de facto enslaved in the new state from statehood in 1850 to 1867. Slavery required the posting of a bond by the slave holder and enslavement occurred through raids and a four-month servitude imposed as a punishment for Indian \"vagrancy\".\n\nAfter 1800, some of the Cherokee and the other four civilized tribes of the Southeast started buying and using black slaves as labor. They continued this practice after removal to Indian Territory in the 1830s, when as many as 15,000 enslaved blacks were taken with them.\n\nThe nature of slavery in Cherokee society often mirrored that of white slave-owning society. The law barred intermarriage of Cherokees and enslaved African Americans, but Cherokee men had unions with enslaved women, resulting in mixed-race children. Cherokee who aided slaves were punished with one hundred lashes on the back. In Cherokee society, persons of African descent were barred from holding office even if they were also racially and culturally Cherokee. They were also barred from bearing arms and owning property. The Cherokee prohibited teaching African Americans to read and write.\n\nBy contrast, the Seminole welcomed into their nation African Americans who had escaped slavery (Black Seminoles). Historically, the Black Seminoles lived mostly in distinct bands near the Native American Seminole. Some were held as slaves of particular Seminole leaders. Seminole practice in Florida had acknowledged slavery, though not the chattel slavery model common elsewhere. It was, in fact, more like feudal dependency and taxation. The relationship between Seminole blacks and natives changed following their relocation in the 1830s to territory controlled by the Creek who had a system of chattel slavery. Pro slavery pressure from Creek and pro-Creek Seminole and slave raiding led to many Black Seminoles escaping to Mexico.\n\nThe Haida and Tlingit Indians who lived along southeast Alaska's coast were traditionally known as fierce warriors and slave-traders, raiding as far as California. Slavery was hereditary after slaves were taken as prisoners of war. Among some Pacific Northwest tribes, about a quarter of the population were slaves. Other slave-owning tribes of North America were, for example, Comanche of Texas, Creek of Georgia, the fishing societies, such as the Yurok, that lived along the coast from what is now Alaska to California; the Pawnee, and Klamath.\n\nSome tribes held people as captive slaves late in the 19th century. For instance, \"Ute Woman\", was a Ute captured by the Arapaho and later sold to a Cheyenne. She was kept by the Cheyenne to be used as a prostitute to serve American soldiers at Cantonment in the Indian Territory. She lived in slavery until about 1880. She died of a hemorrhage resulting from \"excessive sexual intercourse\".\n\nSlaveholders included people of African ancestry. An African former indentured servant who settled in Virginia in 1621, Anthony Johnson, became one of the earliest documented slave owners in the mainland American colonies when he won a civil suit for ownership of John Casor. In 1830 there were 3,775 such black slaveholders in the South who owned a total of 12,760 slaves, a small percent, out of a total of over 2 million slaves. 80% of the black slaveholders were located in Louisiana, South Carolina, Virginia, and Maryland.\n\nThere were economic and ethnic differences between free blacks of the Upper South and Deep South, with the latter fewer in number, but wealthier and typically of mixed race. Half of the black slaveholders lived in cities rather than the countryside, with most living in New Orleans and Charleston. Especially New Orleans had a large, relatively wealthy free black population (\"gens de couleur\") composed of people of mixed race, who had become a third social class between whites and enslaved blacks, under French and Spanish colonial rule. Relatively few non-white slaveholders were \"substantial planters\". Of those who were, most were of mixed race, often endowed by white fathers with some property and social capital. For example, Andrew Durnford of New Orleans was listed as owning 77 slaves. According to Rachel Kranz: \"Durnford was known as a stern master who worked his slaves hard and punished them often in his efforts to make his Louisiana sugar plantation a success.\"\n\nThe historians John Hope Franklin and Loren Schweninger wrote:\n\nThe historian Ira Berlin wrote:\n\nAfrican-American history and culture scholar Henry Louis Gates Jr. wrote:\n\nFree blacks were perceived \"as a continual symbolic threat to slaveholders, challenging the idea that 'black' and 'slave' were synonymous\". Free blacks were sometimes seen as potential allies of fugitive slaves and \"slaveholders bore witness to their fear and loathing of free blacks in no uncertain terms.\" For free blacks, who had only a precarious hold on freedom, \"slave ownership was not simply an economic convenience but indispensable evidence of the free blacks' determination to break with their slave past and their silent acceptance – if not approval – of slavery.\"\n\nThe historian James Oakes in 1982 stated that \"[t]he evidence is overwhelming that the vast majority of black slaveholders were free men who purchased members of their families or who acted out of benevolence\". After 1810 Southern states made it increasingly difficult for any slaveholders to free slaves. Often the purchasers of family members were left with no choice but to maintain, on paper, the owner–slave relationship. In the 1850s \"there were increasing efforts to restrict the right to hold bondsmen on the grounds that slaves should be kept 'as far as possible under the control of white men only.'\"\n\nIn his 1985 statewide study of black slaveholders in South Carolina, Larry Koger challenged the benevolent view. He found that the majority of black slaveholders appeared to hold at least some of their slaves for commercial reasons. For instance, he noted that in 1850 more than 80 percent of black slaveholders were of mixed race, but nearly 90 percent of their slaves were classified as black. Koger also noted that many South Carolina free blacks operated small businesses as skilled artisans, and many owned slaves working in those businesses.\n\nBarbary pirates from North Africa began to seize North American colonists as early as 1625, and roughly 700 Americans were held captive in this region as slaves between 1785 and 1815. Some captives used their experiences as a North African slave to criticize slavery in the United States, such as William Ray in his book \"Horrors of Slavery\".\n\nThe Barbary situation led directly to the creation of the United States Navy in March 1794. While the United States managed to secure peace treaties, these obliged it to pay tribute for protection from attack. Payments in ransom and tribute to the Barbary states amounted to 20% of United States government annual expenditures in 1800. The First Barbary War in 1801 and the Second Barbary War in 1815 led to more favorable peace terms ending the payment of tribute.\n\nFor various reasons, the census did not always include all of the slaves, especially in the West. California was admitted as a free state and reported no slaves. However, there were many slaves that were brought to work in the mines during the California Gold Rush. Some Californian communities openly tolerated slavery, such as San Bernardino, which was mostly made up of transplants from the neighboring slave territory of Utah. New Mexico Territory never reported any slaves on the census, yet sued the government for compensation for 600 slaves that were freed when congress outlawed slavery in the territory. Utah was actively trying to hide its slave population from Congress and did not report slaves in several communities. Additionally, the census did not traditionally include Native Americans, and hence did not include Native American slaves or black slaves owned by Native Americans. There were hundreds of Native American slaves in California, Utah and New Mexico that were never recorded in the census.\n\nAs of the 1860 Census, one may compute the following statistics on slaveholding:\nThe historian Peter Kolchin, writing in 1993, noted that until the latter decades of the 20th century, historians of slavery had primarily concerned themselves with the culture, practices and economics of the slaveholders, not with the slaves. This was in part due to the circumstance that most slaveholders were literate and left behind written records, whereas slaves were largely illiterate and not in a position to leave written records. Scholars differed as to whether slavery should be considered a benign or a \"harshly exploitive\" institution.\n\nMuch of the history written prior to the 1950s had a distinctive racist slant to it. By the 1970s and 1980s, historians were using archaeological records, black folklore, and statistical data to develop a much more detailed and nuanced picture of slave life. Individuals were shown to have been resilient and somewhat autonomous in many of their activities, within the limits of their situation and despite its precariousness. Historians who wrote in this era include John Blassingame (\"Slave Community\"), Eugene Genovese (\"Roll, Jordan, Roll\"), Leslie Howard Owens (\"This Species of Property\"), and Herbert Gutman (\"The Black Family in Slavery and Freedom\").\n\n\n\n\n"}
{"id": "47926046", "url": "https://en.wikipedia.org/wiki?curid=47926046", "title": "Spink &amp; Son", "text": "Spink &amp; Son\n\nSpink & Son (established 1666) are an auction and collectibles company known principally for their sales of coins, banknotes, stock and bond certificates and medals. They also deal in philatelic items, wine and spirits, and other collectible items.\n\nJohn Spink founded a goldsmith's and pawnbroker's business near Lombard Street, London, in 1666. The Great Fire of London caused a temporary relocation before Spink returned to the rebuilt Lombard Street. In 1770 the firm moved to 2 Gracechurch Street where they traded in jewellery and coins. The firm of Spink and Son was established in 1666.\n\nIn the 1850s, Spink started to deal in oriental art and in the 1880s they bought the Soho Mint and began to design and manufacture medals.\n\nThe company published the \"Numismatic Circular\" from 1892 until 2014 when it was discontinued in favour of a quarterly coin auction format for selling coins. It was \"the oldest continually published coin and medal catalogue\". In 1996, Spink acquired the Seaby imprint, publishers of the \"Seaby Coin & Medal Bulletin\".\n\nSpink is particularly well known for its annual \"Coins of England\" price guide and handbook, the 50th edition of which was published in 2015. From 2015 onwards the decimal section (coins struck from 1971 onwards) was removed and included into a separate publication whilst the \"Coins of England\" book remains and references coins up until 1968.\n\nThe current chairman and CEO is Olivier D. Stocker.\n"}
{"id": "1227399", "url": "https://en.wikipedia.org/wiki?curid=1227399", "title": "Stig Strömholm", "text": "Stig Strömholm\n\nStig Fredrik Strömholm (born 16 September 1931 in Boden, Sweden), Swedish, former rector magnificus of Uppsala University and past president of Academia Europaea.\n\nStrömholm received his education in Uppsala (B.A. 1952; LL.B. 1957, Licentiate 1957). He completed a doctorate in Law in Uppsala in 1966. He was Professor in Jurisprudence in Uppsala from 1969, and Professor in Private Law and Conflict of Laws there 1982-1997. After an appointment as prorector (deputy vice-chancellor) of Uppsala University 1978-1989, he was rector magnificus (vice-chancellor) of the university 1989-1997. On June 5, 1992 Strömholm received an honorary doctorate from the Faculty of Humanities at Uppsala University, Sweden.\n\nOn his retirement from the rectorial chair in 1997, he received a festschrift, \"Festskrift till Stig Strömholm\" (Uppsala 1997), in two volumes and 925 pages, including a partial bibliography ().\n\nHe holds the Gunnerus Medal. He is a member of the Norwegian Academy of Science and Letters. Strömholm was the president of Academia Europaea from 1997 to 2002.\n\n"}
{"id": "53984688", "url": "https://en.wikipedia.org/wiki?curid=53984688", "title": "Susan Ashbrook Harvey", "text": "Susan Ashbrook Harvey\n\nSusan Ashbrook Harvey (born 1953) is the Royce Family Professor of Teaching Excellence and the Willard Prescott and Annie McClelland Smith Professor of History and Religion at Brown University. She specialises in late antique and Byzantine Christianity, with Syriac studies as her particular focus.\n\nHarvey was born Susan Jean Ashbrook in 1953 in Rochester, New York to a Baptist seminary professor. She cites her Christian upbringing as a source of inspiration for her research. Harvey received her BA in Classics from Grinnell College in 1975. In 1977, she followed this with a Master of Letters in Byzantine Studies from the University of Birmingham and then a PhD at the same institution in 1982. Her thesis was supervised by Sebastian Brock, one of the foremost experts in the Syriac language and a source of inspiration for Harvey's later interest in Syriac Christianity. From 1983–1987 she was Assistant Professor of Religious Studies at the University of Rochester. In 1987, she joined the faculty of Brown University.\n\nHarvey's work focuses on the social aspects of Christianity, particularly issues affecting women. She has researched the variety of roles women played in the ancient church, and highlighted the many women saints emerging from all walks of life. According to Ross Shepard Kraemer, \"Susan Harvey has almost single-handedly established an entire sub-field of studies on women and gender in the Syrian Orient\". Harvey has also published widely on topics relating to asceticism, hagiography, hymnography, homiletics, and piety in late antique Christianity. In recognition of this and other work she was awarded an honorary Doctor of Humane Letters from Grinnell College in 2007. She was also awarded Doctor theologiae, \"honoris causa\", from Lund University in May 2013 and Doctor theologiae, \"honoris causa\", from the University of Bern in December 2009. In 2007–2008 she was the recipient of a Guggenheim fellowship to work on biblical women and women's choirs in Syriac Christianity.\n\n"}
{"id": "6985614", "url": "https://en.wikipedia.org/wiki?curid=6985614", "title": "Synekism", "text": "Synekism\n\nSynekism is a concept in urban studies coined by Edward Soja. It refers to the dynamic formation of the polis state — the union of several small urban settlements under the rule of a \"capital\" city (or so-called city-state or urban system). Soja's definition of synekism, mentioned in \"Writing the city spatially\", is \"the stimulus of urban agglomeration.\"\n\nFrom the social sciences' view, it is also a \"nucleated and hierarchically nested process of political governance, economic development, social order, and cultural identity\" Soja.\nIn densely settled urban places, a critical-mass provides potential for innovation that is not typically available in rural environments, therefore synekism can be thought of as the geographical relationships that create and give importance to cities.\n\n"}
{"id": "27447624", "url": "https://en.wikipedia.org/wiki?curid=27447624", "title": "Teodor Jeske-Choiński", "text": "Teodor Jeske-Choiński\n\nTeodor Jeske-Choiński (27 February 1854 - 14 April 1920 ) was a Polish intellectual, writer and historian, literature critic. \n\nHe was a friend as well as an opponent of Henryk Sienkiewicz. Sienkiewicz' novels were focused rather on Polish history, whereas Jeske-Choińskis were looking at broader, European context. In 1900 he published \"Tiara i korona\", a novel about the dispute between the Emperor Henry IV and Pope Gregory VII. \n\nJoanna Beata Michlic named him \"one of the leading theorists and exponents of Anti-Semitism in Poland\".\nIn 1951 the communist censorship put complete ban on all his books, which made him completely forgotten among polish public.\n"}
{"id": "14948584", "url": "https://en.wikipedia.org/wiki?curid=14948584", "title": "Theodoric Vrie", "text": "Theodoric Vrie\n\nTheodoric Vrie (dates unknown) was a historian of the Council of Constance. \n\nHe describes himself as a brother of the Order of Hermits of St. Augustine, and a lector in sacred theology in the Province of Saxony. From his description of facts it appears that Vrie must have been an eyewitness to the events he records. The history is brought down to the election and consecration of Pope Martin V, 21 Nov., 1417. Vrie was still living in the sumer of 1425, when a general chapter of his order at Rome authorized the republication of his work. Vrie's work is modelled on the \"De consolatione philosophiæ\" of Boethius; this also is its original title. It presents a picture of the facts and disorders of the time, pointing out their source, and the remedy under the form of a series of dialogues in prose and metre between Christ and the Church Militant. The \"De consolatione\" of Vrie was printed in Cologne in 1484 with the works of Jean Gerson (fourth volume), but was not repeated in the Strasbourg edition of Gerson in 1494. It was printed again with a short life of the author in von der-Hardt (see below).\n\n"}
{"id": "46697018", "url": "https://en.wikipedia.org/wiki?curid=46697018", "title": "Timothy Cheek", "text": "Timothy Cheek\n\nTimothy Cheek () is a Canadian historian specializing in the study of intellectuals, the history of the Chinese Communist Party, and the political system in modern China. He is Professor, Louis Cha Chair in Chinese Research and Director, Centre for Chinese Research, Institute of Asian Research,\nat the University of British Columbia. From 2002-2009 he was editor of the journal \"Pacific Affairs\" Before going to University of British Columbia in 2002, he taught at The Colorado College. \n\nAfter taking a B.A. in Asian Studies, with Honours, at Australian National University, in 1978, Cheek earned a Master's Degree in History, University of Virginia in 1980. In 1986 he earned a Ph.D., History and East Asian Languages, at Harvard University, under the supervision of Philip A. Kuhn. \n\nCheek has served on the Board of University of British Columbia Press (2010 – ), Editorial Board, \"Journal of the Canadian Historical Association\" (Ottawa), 2007— ) Editorial Board, \"The China Journal\" (Canberra), 2007— ), Editorial Board, \"Issues and Studies \"(Taipei) (2004— ), Editorial Board, \"Historiography East and West \" (Leiden/Vienna) (2003— ). Editorial Board, \"China Information\" (Leiden) (1998 – ), as well as other executive or advisory positions.\n\nScholars such as Merle Goldman, with whom Cheek has collaborated, have tended to see Chinese intellectuals as dissidents or critics of the regime, while Cheek has tended to assume that the intellectuals he studies see themselves as working within the regime, broadly conceived, that is, as \"establishment intellectuals.\" The introduction to a group of essays he edited with Carol Lee Hamrin comments that \"anti-establishment intellectuals in China have less to gain and more to lose than their American counterparts\", and that since all Chinese intellectuals are state employees, \"by playing assigned roles as supporters of the establishment and servants of the state, they gain patriotic self-esteem, outlets for their publications, power over their peers, and opportunities for scarce commodities such as housing and travel abroad\". \n\nA review of his edited volume, \"Cambridge Companion to Mao\", wrote that the essays in it \"contribute to an understanding of Mao Zedong that is as messy and complex as it is compelling. The text, moreover, encourages readers to engage the problem of knowing the historical Mao, while reminding the reader of the equal importance of Mao’s ahistorical legacy. Sadly, this text will most likely never be sold in airport bookstores alongside popular biographies of Mao, but Cheek’s collection will hopefully spark lively discussion in seminar classrooms. \n\n\n\n"}
{"id": "34467368", "url": "https://en.wikipedia.org/wiki?curid=34467368", "title": "Tukumana Te Taniwha", "text": "Tukumana Te Taniwha\n\nTukumana Te Taniwha (1862–1941) was a notable New Zealand tribal leader and historian of Māori descent. He identified with the Ngāti Maru and Ngāti Whanaunga iwi. He was born in the Coromandel, New Zealand in 1862. His mother, Karukino Te Taniwha, was the daughter of Te Horeta Te Taniwha.\n"}
{"id": "2513120", "url": "https://en.wikipedia.org/wiki?curid=2513120", "title": "Twentieth Air Force", "text": "Twentieth Air Force\n\nThe Twentieth Air Force (Air Forces Strategic) (20th AF) is a numbered air force of the United States Air Force Global Strike Command (AFGSC). It is headquartered at Francis E. Warren Air Force Base, Wyoming.\n\n20 AF's primary mission is Intercontinental Ballistic Missile (ICBM) operations. The Twentieth Air Force commander is also the Commander, Task Force 214 (TF 214), which provides alert ICBMs to the United States Strategic Command (USSTRATCOM).\n\nEstablished on 4 April 1944 at Washington D.C, 20 AF was a United States Army Air Forces combat air force deployed to the Pacific Theater of World War II. Operating initially from bases in India and staging though bases in China, 20 AF conducted strategic bombardment of the Japanese Home Islands. It relocated to the Mariana Islands in late 1944, and continued the strategic bombardment campaign against Japan until the Japanese capitulation in August 1945. The 20 AF 509th Composite Group conducted the atomic bomb attacks on Hiroshima and Nagasaki, Japan, in August 1945.\n\nInactivated on 1 March 1955, the command was reactivated 1 September 1991, as a component of the Strategic Air Command (SAC) and became operationally responsible for all land-based Intercontinental Ballistic Missiles.\n\nTwentieth Air Force headquarters is unique in that it has dual responsibilities to Air Force Global Strike Command and United States Strategic Command. As the missile Numbered Air Force for AFGSC, 20th Air Force is responsible for maintaining and operating the Air Force's ICBM force. Designated as STRATCOM's Task Force 214, 20th Air Force provides on-alert, combat ready ICBMs to the president. Combined with the other two legs of the Triad, bombers and submarines, STRATCOM forces protect the United States with an umbrella of deterrence.\n\n\nThe Twentieth Air Force was brought into existence on 4 April 1944 specifically to perform strategic bombardment missions against Japan. This was done at the insistence of General Henry H. (Hap) Arnold, commander of the USAAF, mainly to avoid having the new B-29 Superfortress being diverted to tactical missions under pressure from the China Burma India Theater commanders. Twentieth Air Force was to be commanded by General Arnold himself at Joint Chiefs of Staff level. Twentieth Air Force was completely autonomous and its B-29s were to be completely independent of other command structures and would be dedicated exclusively against strategic targets in Japan.\n\nIn addition Twentieth Air Force was chosen (secretly) to be the operational component of the Manhattan Project in 1944, and performed the atomic attacks on Japan in August 1945. However, in early 1944, the B-29 was not yet operationally ready. The aircraft had been in development at Boeing since the late 1930s and the first XB-29 (41-0002) flew on 21 September 1942. However, the aircraft suffered from an overwhelming number of development issues, and with engine problems (fires). As a result, most of the first production B-29s were still held up at Air Technical Service Command modification centers, awaiting modifications and conversion to full combat readiness. By March 1944, the B-29 modification program had fallen into complete chaos, with absolutely no bombers being considered as combat ready. The program was seriously hampered by the need to work in the open air in inclement weather, as many hangars were simply too small to house the aircraft indoors; by delays in acquiring the necessary tools and support equipment, and by the USAAF's general lack of experience with the B-29.\n\nGeneral Arnold became alarmed at the situation and directed that his assistant, Major General B. E. Meyer, personally take charge of the entire modification program. The resulting burst of activity that took place between 10 March and 15 April 1944 came to be known as the \"Battle of Kansas\". Beginning in mid-March, technicians and specialists from the Boeing Wichita and Seattle factories were drafted into the modification centers to work around the clock to get the B-29s ready for combat. The mechanics often had to work outdoors in freezing weather. As a result of superhuman efforts on the part of all concerned, 150 B-29s had been handed over to the XX Bomber Command by 15 April 1944.\n\nOperation Matterhorn was the name for the B-29 Superfortress offensive against the Empire of Japan from airfields in China. On 10 April 1944, the Joint Chiefs of Staff (JCS) informally approved Operation Matterhorn. The operational vehicle was to be the 58th Bombardment Wing (Very Heavy) of the XX Bomber Command.\n\nThe headquarters of the XX Bomber Command had been established at Kharagpur India on 28 March 1944. The commander was General Kenneth B. Wolfe. The first B-29 reached its base in India on 2 April 1944. In India, existing airfields at Kharagpur, Chakulia, Piardoba and Dudkhundi had been converted for B-29 use. All of these bases were located in southern Bengal and were not far from port facilities at Calcutta.\n\nThe first B-29 bombing raid from India took place on 5 June 1944. Ninety-eight B-29s took off from bases in eastern India to attack the Makasan railroad yards at Bangkok, Thailand. Bombardment operations against Japan were planned to be carried out from bases in China. There were four sites in the Chengtu area of China that were assigned to the B-29 operation—at Kwanghan, Kuinglai, Hsinching, and Pengshan. The primary flaw in the Operation Matterhorn plan was the fact that all the supplies of fuel, bombs, and spares needed to support the forward bases in China had to be flown in from India over the Hump, since Japanese control of the seas around the Chinese coast made seaborne supply of China impossible.\n\nBy mid-June, enough supplies had been stockpiled at Chinese forward bases to permit the launching of a single attack against targets in Japan. It was a nighttime raid to be carried out on the night of 14/15 June 1944 against the Imperial Iron and Steel Works at Yawata on Kyūshū. Unfortunately, the Japanese had been warned of the approaching raid and the city of Yawata was blacked out and haze and/or smoke helped to obscure the target. Only 15 aircraft bombed visually while 32 bombed by radar. Only one bomb actually hit anywhere near the intended target, and the steel industry was essentially untouched. Although very little damage was actually done, the Yawata raid was hailed as a great victory in the American press, since it was the first time since the Doolittle raid of 1942 that American aircraft had hit the Japanese home islands.\n\nOn the night 10–11 August, 56 B-29s staged through British air bases in Ceylon attacked the Plajdoe oil storage facilities at Palembang on Sumatra in Indonesia. This involved a 4030-mile, 19-hour mission from Ceylon to Sumatra, the longest American air raid of the war. Other B-29s laid mines in the Moesi River. At the same time, a third batch of B-29s attacked targets in Nagasaki. These raids all showed a lack of operational control and inadequate combat techniques, drifting from target to target without a central plan and were largely ineffective.\n\nIn Washington, it was decided that new leadership was needed for Twentieth Air Force. General Wolfe's replacement was Major General Curtis E. LeMay, who arrived in India on 29 August. Supply problems and aircraft accidents were still preventing a fully effective concentration of force and effort. In addition, Japanese defensive efforts were becoming more effective.\n\nBy late 1944, it was becoming apparent that B-29 operations against Japan staged out of bases in China and India were far too expensive in men and materials and would have to be stopped. In December 1944, the Joint Chiefs of Staff made the decision that Operation Matterhorn would be phased out, and the 58th Bombardment Wing's B-29s would be moved to newly captured bases in the Marianas in the central Pacific. The last raid out of China was flown on 15 January 1945, which was an attack on targets in Formosa (Taiwan). The 58th Bombardment Wing then redeployed to new bases in the Marianas in February.\n\nThe Marianas chain of islands, consisting primarily of Saipan, Tinian, and Guam, were considered as being ideal bases from which to launch B-29 Superfortress operations against Japan. The islands were about 1500 miles from Tokyo, a range which the B-29s could just about manage. Most important of all, they could be put on a direct supply line from the United States by ship. The XXI Bombardment Command had been assigned the overall responsibility of the B-29 operations out of the Marianas bases.\n\nThe first B-29 arrived on Saipan on 12 October 1944. It was piloted by General Hansell himself. By 22 November, over 100 B-29s were on Saipan. The XXI Bomber Command was assigned the task of destroying the aircraft industry of Japan in a series of high-altitude, daylight precision attacks.\n\nThe first raid against Japan took place on 24 November 1944. The target was the Nakajima Aircraft Company's Musashi engine plant just outside Tokyo. 111 B-29s took off, Seventeen of them had to abort due to the usual spate of engine failures. The remainder approached the target at altitudes of 27–32,000 feet. For the first time, the B-29 encountered the jet stream, which was a high-speed wind coming out of the west at speeds as high as 200 mph at precisely the altitudes at which the bombers were operating. This caused the bomber formations to be disrupted and made accurate bombing impossible.\n\nConcerned about the relative failure of the B-29 offensive to deal any crippling blows to Japan, General LeMay issued a new directive on 19 February. General LeMay had analyzed the structure of the Japanese economy, which depended heavily on cottage industries housed in cities close to major industrial areas. By destroying these feeder industries, the flow of vital components to the central plants could be slowed, disorganizing production of weapons vital to Japan. He decided to do this by using incendiary bombs rather than purely high-explosive bombs, which would, it was hoped, cause general conflagrations in large cities like Tokyo or Nagoya, spreading to some of the priority targets.\n\nThe first raid to use these new techniques was on the night of 9–10 March against Tokyo. Another wing—the 314th Bombardment Wing (19th, 29th, 39th, and 330th BG) commanded by Brig. Gen. Thomas S. Power—had arrived in the Marianas and was stationed at North Field on Guam. A total of 302 B-29s participated in the raid, with 279 arriving over the target. The raid was led by special pathfinder crews who marked central aiming points. It lasted for two hours. The raid was a success beyond General LeMay's wildest expectations. The individual fires caused by the bombs joined to create a general conflagration due to strong winds of some 17 to 28 mph (27 to 45 km/h) at ground level, that prevented a more specific firestorm event. When it was over, sixteen square miles (41 km.) of the center of Tokyo had gone up in flames and nearly 84,000 people had been killed. Fourteen B-29s were lost. The B-29 was finally beginning to have an effect.\n\nBy mid-June, most of the larger Japanese cities had been gutted, and LeMay ordered new incendiary raids against 58 smaller Japanese cities. By now, the B-29 raids were essentially unopposed by Japanese fighters. In late June, B-29 crews felt sufficiently confident that they began to drop leaflets warning the population of forthcoming attacks, followed three days later by a raid in which the specified urban area was devastated. By the end of June, the civilian population began to show signs of panic, and the Imperial Cabinet first began to consider negotiating an end to the war. However, at that time, the Japanese military was adamant about continuing on to the bitter end.\n\nIn June 1945, the XX and XXI Bombardment Commands were grouped under the U.S. Strategic Air Forces in the Pacific (USASTAF), under the command of General Carl A. Spaatz. The history of XXI Bomber Command terminated on 16 July 1945. On that date the command was redesignated Headquarters and Headquarters Squadron, Twentieth Air Force. This redesignation brought to an end the XXI Bomber Command as a separate establishment, as it was absorbed into the internal organizational structure of Twentieth Air Force and was placed under the command of USASTAF.\n\nA reorganization of United States military commands on 16 July 1945 placed Twentieth Air Force under the command and control of the new United States Strategic Air Forces in the Pacific. Twentieth Air Force would command B-29 wings directly based in the Mariana Islands, while the newly re-deployed Eighth Air Force would command B-29 wings based on Okinawa. This realignment was made in advance of the planned Invasion of Japan (Operation Downfall) set to begin in October 1945. XXI Bomber Command was inactivated, its organization under the direct control of Twentieth Air Force.\n\nBy mid-July 1945, the combat missions over Japan were essentially un-opposed, with VII Fighter Command long range P-51 Mustangs operating from captured Iwo Jima airfields flying escort to the Marianas-based B-29s. Missions primarily consisted of low-level incendiary raids on smaller Japanese cities, both at night as well as daylight on a daily basis. The 315th Bombardment Wing, which became operational at the beginning of July, carried out a series of strikes against oil production facilities which essentially shut down the Japanese oil industry.\n\nThe 509th Composite Group was deployed overseas in the spring of 1945. The 509th was initially a part of XXI Bombardment Command based in the Marianas. By July, the bombers were established at North Field on Tinian, which had just been completed for the 313th Bombardment Wing. It was, however, under the direct operational control of the commander, Twentieth Air Force. The mission of the unit was the operational use of the Atomic Bomb.\n\nIt had only one Bombardment Squadron—the 393rd, commanded by Major Charles W. Sweeney. The 509th Composite Group was a completely self-sufficient unit, with its own engineer, material, and troop squadrons as well as its own military police unit. Since the Manhattan project was carried out in an atmosphere of high secrecy, the vast majority of the officers and men of the 509th Composite Group were completely ignorant of its intended mission.\n\nWith the testing of the Atomic Bomb completed in the United States, the two other bombs (Little Boy, Fat Man) had arrived on Tinian on 26 July, being delivered by the . On 24 July, a directive was sent to General Carl A. Spaatz ordering the 509th to deliver its first atomic bomb as soon as weather would permit. The Japanese cities of Hiroshima, Kokura, Niigata and Nagasaki were potential targets. President Harry S. Truman gave his final go-ahead from the Potsdam Conference on 31 July.\n\nOn 6 August the atomic attack began with a flight of three special reconnaissance F-13As (RB-29s) which took off to report the weather over the primary and secondary targets. Col. Tibbets followed in his B-29 aircraft, \"Enola Gay\", an hour later, accompanied by two other B-29s which would observe the drop. While on the way to Japan, Major Claude Eatherly, flying \"Straight Flush\", radioed that Hiroshima was clear for a visual bomb drop. Navy weapons expert Captain William Parsons armed the bomb while in flight, as it was deemed too dangerous to do this on the ground at North Field, lest an accident happen and the bomb go off, wiping out the entire base. At 8:15 am, the \"Enola Gay\" released Little Boy from an altitude of . The radar fuse on the bomb had been preset to go off at an altitude of above the ground. In the ensuing explosion, yielding about 12 kilotons of TNT in explosive power, about 75,000 people were killed and 48,000 buildings were destroyed.\n\nWith no official statement from the Japanese government, there was no let-up with the conventional B-29 raids. B-29s from the 58th, 73rd, and 313th Bombardment Wings hit the Toyokawa Arsenal the next day. On the night of 7 August, the 525th Bombardment Group dropped 189 tons of mines on several different sea targets. On 8 August, the 58th, 73rd, and 313th Bomb Wings dropped incendiary bombs on targets at Yawata in the southern island of Kyūshū. At the same time, the 314th BW hit an industrial area of Tokyo. The Japanese defenses were still effective enough to down four B-29s during the Yawata raid and three at Tokyo.\n\nSince there was still no official reaction from Japan, the Americans felt that there was no alternative but to prepare a second atomic attack. The plutonium bomb called \"Fat Man\" was loaded into a B-29 known as \"Bockscar\" (Martin-Omaha built B-29-35-MO serial number 44-27297, the name often spelled Bock's Car), named after its usual commander, Captain Frederick C. Bock. However, on this mission, the aircraft was flown by Major Sweeney, with Capt. Bock flying one of the observation planes. The primary target was to be the Kokura Arsenal, with the seaport city of Nagasaki as the alternative.\n\nBockscar took off on 9 August, with Fat Man on board. This time, the primary target of Kokura was obscured by dense smoke left over from the earlier B-29 raid on nearby Yawata, and the bombardier could not pinpoint the specified aiming point despite three separate runs. So Sweeney turned to the secondary target, Nagasaki. There were clouds over Nagasaki as well, and a couple of runs over the target had to be made before the bombardier could find an opening in the clouds. At 11:00 am, Fat Man was released from the aircraft and after a long descent, the bomb exploded. The yield was estimated at 22 kilotons of TNT. Approximately 35,000 people died at Nagasaki from the immediate blast and fire.\n\nAfter releasing the bomb, Sweeney was forced to divert to Okinawa because of a problem with a fuel transfer pump, and because of the long flight with multiple bomb runs and circling for better weather. There was not even enough fuel left to fly to Iwo Jima. After refueling on Okinawa, the B-29 returned to Tinian. The Japanese Emperor ordered that the government accept the Allied terms of surrender at once. It took time for the full details to be worked out, and there was a very real danger that some elements of the Japanese military would still not accept surrender, and might attempt a military coup d'état, even against their Emperor.\n\nIn the meantime, conventional bombing of Japanese targets still continued, with a record number of 804 B-29s hitting targets in Japan on 14 August. On the morning of 15 August, the Emperor broadcast by radio his command of Japan's surrender in an address to his nation. Practically none of his subjects had never heard his voice before. All further offensive operations against Japan ceased after the Emperor's broadcast.\n\nAfter that time, most of the B-29s in the Pacific were diverted to missions of mercy, dropping food and clothing to thousands of Allied prisoners of war held in Japan, China, Manchuria, and Korea. 1066 B-29s participated in 900 missions to 154 camps. Some 63,500 prisoners were provided with 4470 tons of supplies. These flights cost eight B-29s lost by accidents, with 77 crew members aboard.\n\nThe Japanese surrender was formally signed on 2 September 1945, aboard the huge battleship \"USS Missouri\" in Tokyo Bay, bringing the Pacific War to an end.\n\nFollowing the end of World War II, Twentieth Air Force remained in the Pacific, being headquartered on Guam. The vast majority of its fleet of B-29 Superfortreses were returned to the United States as part of \"Operation Sunset\". The United States Strategic Air Forces in the Pacific was inactivated on 6 December 1945, and the remaining assets of the command were placed under the Pacific Air Command, United States Army.\n\nThe last of the World War II combat wings, the 315th Bombardment Wing, returned to the United States on 30 May 1946, with the 19th Bombardment Group, remaining at North Field, Guam as its only operational organization. In 1949, budget reductions forced the realignment and consolidation of Air Force units in the pacific, and the mission of Twentieth Air Force became the defense of the Ryukyu Islands and was reassigned to Kadena AB, Okinawa. It commanded the following units:\n\n\nOn 27 June 1950, the United Nations Security Council voted to assist the South Koreans in resisting the invasion of their nation by North Korea. President Harry S. Truman authorized General Douglas MacArthur (commander of the US occupying forces in Japan) to commit units to the battle. MacArthur ordered General George E. Stratemeyer, CIC of the Far Eastern Air Force (FEAF) to attack attacking North Korean forces between the front lines and the 38th parallel. At that time, the 22 B-29s of the 19th Bombardment Group stationed at Andersen Field on Guam were the only aircraft capable of hitting the Korean peninsula, and this unit was ordered to move to Kadena air base on Okinawa and begin attacks on North Korea. These raids began on 28 June. On 29 June, clearance was given for B-29 attacks on airfields in North Korea. The B-29s were frequently diverted into tactical attacks against advancing North Korean troops.\n\nOn 8 July, a special FEAF Bomber Command was set up under the command of Major General Emmett O'Donnell. On 13 July, the FEAF Bomber Command took over command of the 19th Bombardment Group and of the 22nd and 92nd Bombardment Groups which had been transferred from SAC bases in the United States.\n\nThe other major components of Twentieth Air Force, the 51st Fighter-Interceptor Wing was reassigned to Fifth Air Force at Itazuke AB, Japan in September 1950, where its F-82 Twin Mustangs and F-80 Shooting Stars were used in combat over Korea. The very long-range RB-29s of the 31st Strategic Reconnaissance Squadron (unarmed B-29s fitted with additional internal fuel tanks within the bomb bays and various photo mapping cameras) were also reassigned to Fifth Air Force at Johnson AB, Japan where they were combined with other aerial reconnaissance units.\n\nWith the end of the Korean War in 1953, Far East Air Forces reorganized its forces and Twentieth Air Force units were reassigned. The bombardment units were reassigned to Strategic Air Command in 1954; fighter units to Fifth Air Force in 1955 and used for air defense. It was inactivated on 1 March 1955.\n\nTwentieth Air Force was reactivated on 1 September 1991 as a component of Strategic Air Command and located at Vandenberg AFB, California. Its mission was the responsibility for all land-based Intercontinental Ballistic Missiles (ICBM)s. 20th Air Force's rebirth came at a time when America's nuclear forces were entering a decade of unprecedented force reductions and changes. Spawned by the Cold War's end and the breakup of the Soviet Union, these changes reshaped the basic fabric of the nation's nuclear deterrent forces.\n\nIn the decades since its reactivation, 20th Air Force has experienced four major command identities. After one year in Strategic Air Command and another year in Air Combat Command, 20th Air Force was moved under Air Force Space Command in 1993. December 2009 marked the final transition of 20th Air Force to the newly created Air Force Global Strike Command. Twentieth Air Force Headquarters' changed its location in 1993, moving from Vandenberg AFB, Calif., to its current home at FE Warren Air Force Base, Wyoming. Today 450 Minuteman III missiles remain on alert.\n\nTwentieth Air Force headquarters is unique in that it has dual responsibilities to Air Force Global Strike Command and United States Strategic Command. As the missile Numbered Air Force for AFGSC, 20th Air Force is responsible for maintaining and operating the Air Force's ICBM force. Designated as STRATCOM's Task Force 214, 20th Air Force provides on-alert, combat ready ICBMs to the president. Combined with the other two legs of the Triad, bombers and submarines, STRATCOM forces protect the United States with an umbrella of deterrence.\n\n\n\n\nBombardment Wings\n\nFighter Wings\n\nMissile Wings\n\nOther Wings and Groups\n\n\n\n\n"}
{"id": "25987727", "url": "https://en.wikipedia.org/wiki?curid=25987727", "title": "Wolfram Setz", "text": "Wolfram Setz\n\nWolfram Setz (born July 7, 1941) is a German historian, editor, translator and essayist.\n\nBorn in Stralsund, Setz studied at the universities of Cologne and Tübingen, completing his Ph.D. in 1975 with a dissertation on Lorenzo Valla's exposure of the Donation of Constantine as a hoax. Setz was subsequently employed as an editor at Monumenta Germaniae Historica in Munich. Following retirement in 2004, he relocated to Hamburg.\n\nSetz is editor of the \"Bibliothek rosa Winkel\", a series of over 70 volumes of LGBT literary reprints, cultural studies, and historical works that was launched in 1991, published initially by Verlag rosa Winkel and since 2001 by Männerschwarm Verlag. Several of Setz's publications have contributed to the rediscovery of the gay rights pioneer Karl Heinrich Ulrichs (1825–1895).\n\nSetz was editor of the book series \"Homosexualität und Literatur\" published by Verlag rosa Winkel (12 volumes, 1981-1999) and a coeditor of the scholarly journal \"Forum Homosexualität und Literatur\" (1987-2007). In 1986 he was a founding member of the German LGBT organization \"Bundesverband Homosexualität\" and served for several years as a board member until the organization was dissolved in 1997.\n\n\n\n"}
